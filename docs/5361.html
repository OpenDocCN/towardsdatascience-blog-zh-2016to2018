<html>
<head>
<title>Learning sentence embeddings by Natural Language Inference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过自然语言推理学习句子嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-sentence-embeddings-by-natural-language-inference-a50b4661a0b8?source=collection_archive---------1-----------------------#2018-10-13">https://towardsdatascience.com/learning-sentence-embeddings-by-natural-language-inference-a50b4661a0b8?source=collection_archive---------1-----------------------#2018-10-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="d168" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">无监督学习方法看起来像是构建单词、句子或文档嵌入的正常方式，因为它更一般化，使得预训练的嵌入结果可以转移到其他 NLP 下游问题。例如，单词嵌入中的<a class="ae ko" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a"> skip-gram </a>和句子嵌入中的 skip-through 以及段落嵌入中的分布式词袋。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/d3fe950b1246edd381fa3a6483437936.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jeUZN-RepRTx0DMn"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">“closeup photo of person carrying professional video recorder” by <a class="ae ko" href="https://unsplash.com/@laura_lee?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Laura Lee Moreau</a> on <a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="fae6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Conneau 等人指出，ImageNet(图像分类)中的监督学习在将结果转移到下游问题方面做得很好。有些特征可以以某种方式转移到下游。因此，Conneau 等人使用文本蕴涵数据来训练一个句子嵌入层，称为 InferSent。</p><p id="c167" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">看完这篇文章，你会明白:</p><ul class=""><li id="0484" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn lk ll lm ln bi translated">完美的设计</li><li id="7fdc" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated">体系结构</li><li id="716b" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated">履行</li><li id="56b5" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated">拿走</li></ul><h1 id="bcbd" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">完美的设计</h1><p id="f7ca" class="pw-post-body-paragraph jq jr it js b jt mr jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn im bi translated">该团队使用 SNLI(斯坦福自然语言推理)数据来训练自然语言推理(NLI)问题的模型。NLI 的目标是找出句子 1(前提)和句子 2(假设)之间的关系。有三个范畴，即蕴涵、矛盾和中性。下面是一个非常简单的例子:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mw"><img src="../Images/7c43974f5b25d7a4afdec44aaaf003e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BFLbAi0m30Hs9oU0"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">“two apples and walnuts on white towel” by <a class="ae ko" href="https://unsplash.com/@kotomanov?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Alex Kotomanov</a> on <a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><ol class=""><li id="ee1c" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn mx ll lm ln bi translated">我吃水果。</li><li id="9ca1" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn mx ll lm ln bi translated">我吃苹果。</li></ol><p id="f412" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">直觉上，这种关系是隐含的。作者认为，NLI 是理解句子中语义关系的合适任务，因此它有助于为下游 NLP 问题的句子嵌入建立良好的嵌入。</p><h1 id="a8a5" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">体系结构</h1><p id="bb36" class="pw-post-body-paragraph jq jr it js b jt mr jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn im bi translated">总体思路是，两个句子(前提输入和假设输入)将通过句子编码器(权重相同)进行转换。然后利用 3 种匹配方法来识别前提输入和假设输入之间的关系。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi my"><img src="../Images/388240d9b609eadb0d094cb372320b27.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*efWq1UrOcGy2E-34OxsBHQ.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Conneau et al. (2017)</figcaption></figure><ol class=""><li id="84b8" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn mx ll lm ln bi translated">两个向量的连接</li><li id="7b72" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn mx ll lm ln bi translated">两个向量的逐元素乘积</li><li id="397a" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn mx ll lm ln bi translated">两个向量的绝对元素差</li></ol><p id="0cf4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在概述之后，可能会进入句子编码器的架构。Conneau 等人评估了 7 种不同的架构:</p><ol class=""><li id="16da" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn mx ll lm ln bi translated">标准 LSTM</li><li id="d7b0" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn mx ll lm ln bi translated">标准 GRU</li><li id="bfb8" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn mx ll lm ln bi translated">前向和后向 GRU 的最后隐藏状态的串联</li><li id="c6c8" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn mx ll lm ln bi translated">具有平均轮询的双向 LSTM</li><li id="b1fd" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn mx ll lm ln bi translated">具有最大轮询的双向 LSTM</li><li id="6554" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn mx ll lm ln bi translated">自我关注网络(用 BiLSTM 关注)</li><li id="5102" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn mx ll lm ln bi translated">分层卷积网络</li></ol><p id="312b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在首先得出最佳方法之前，我们可以认为用 BiLSTM 注意应该是最佳方法，因为注意机制有助于识别重要权重。实际上，在迁移学习中使用它可能是有害的。另一方面，采用平均轮询的 BiLSTM 可能由于无法定位重要部分而性能不佳。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/5d95e158f256235b540fb12776bb27e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*78sgGtCOQ8PHHnV6Wg4WNQ.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">#5 Bi-directional LSTM with max polling (Conneau et al., 2017)</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi na"><img src="../Images/6ed84e27232c9e50836dd09ad603ac31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*1Dmuk254qcpMFB3tVi_84Q.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">#6 Self-attentive Network Architecture (Conneau et al., 2017)</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/6a2868b626efb707547d88d4f0656c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*H5POIKHhmD-L_nLylV6MSQ.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Hierarchical convolutional networks (Conneau et al., 2017)</figcaption></figure><p id="766f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从实验结果来看，最大轮询的双向 LSTM 是最好的方法。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/21ae1e60f6b4ede6c9428eacea77efc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*sB0c4eTJThAaPdsp0DjRMA.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Conneau et al. (2017)</figcaption></figure><h1 id="a53b" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">履行</h1><p id="6863" class="pw-post-body-paragraph jq jr it js b jt mr jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn im bi translated">有两种方法可以使用 InferSent。首先是在你的 NLP 问题中使用一个预先训练好的嵌入层。另一个是你自己发出的建筑噪音。</p><p id="116e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="nd">加载预训练嵌入</em> </strong></p><p id="5e5d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">脸书研究小组提供了两个预训练模型，即版本 1(基于手套)和版本 2(基于快速文本)。</p><p id="d103" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">加载推断预训练模型和 GloVe(或 fastText)模型，然后您可以将句子编码为向量。</p><pre class="kq kr ks kt gt ne nf ng nh aw ni bi"><span id="edfd" class="nj lu it nf b gy nk nl l nm nn"># Init InferSent Model<br/>infer_sent_model = InferSent()<br/>infer_sent_model.load_state_dict(torch.load(dest_dir + dest_file))</span><span id="8157" class="nj lu it nf b gy no nl l nm nn"># Setup Word Embedding Model<br/>infer_sent_model.set_w2v_path(word_embs_model_path)</span><span id="d050" class="nj lu it nf b gy no nl l nm nn"># Build Vocab for InferSent model<br/>model.build_vocab(sentences, tokenize=True)</span><span id="7fd0" class="nj lu it nf b gy no nl l nm nn"># Encode sentence to vectors<br/>model.encode(sentences, tokenize=True)</span></pre><p id="0e76" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="nd">列车嵌入</em> </strong></p><p id="47bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">另一种方法是自己训练嵌入。你可以使用你自己的数据或原始数据集。因为这个推理器使用监督学习方法来生成句子嵌入，所以您首先需要有一个带注释的(带标签的)数据。</p><p id="4a96" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是第一种方法的步骤。克隆人将原始的<a class="ae ko" href="https://github.com/facebookresearch/InferSent" rel="noopener ugc nofollow" target="_blank">回购</a>发送到本地。然后在控制台中执行“get_data.bash ”,以便下载和处理 SNLI(斯坦福自然语言推理)和 multi nli(NLI)语料库。确保您必须在当前文件夹而不是其他相对路径中执行以下 shell 脚本</p><pre class="kq kr ks kt gt ne nf ng nh aw ni bi"><span id="a5b7" class="nj lu it nf b gy nk nl l nm nn">./get_data.bash</span></pre><p id="3381" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">之后，下载手套(和/或快速文本)</p><pre class="kq kr ks kt gt ne nf ng nh aw ni bi"><span id="29b3" class="nj lu it nf b gy nk nl l nm nn">mkdir dataset/GloVe<br/>curl -Lo dataset/GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip<br/>unzip dataset/GloVe/glove.840B.300d.zip -d dataset/GloVe/<br/>mkdir dataset/fastText<br/>curl -Lo dataset/fastText/crawl-300d-2M.vec.zip https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip<br/>unzip dataset/fastText/crawl-300d-2M.vec.zip -d dataset/fastText/</span></pre><p id="f7e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正在下载预先训练好的模型。版本 1 通过使用 GloVe 来训练，而版本 2 利用了 fastText。</p><pre class="kq kr ks kt gt ne nf ng nh aw ni bi"><span id="cbf1" class="nj lu it nf b gy nk nl l nm nn">curl -Lo encoder/infersent1.pkl https://s3.amazonaws.com/senteval/infersent/infersent1.pkl<br/>curl -Lo encoder/infersent2.pkl https://s3.amazonaws.com/senteval/infersent/infersent2.pkl</span></pre><p id="f444" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，您可以执行下面的命令来训练嵌入层。</p><pre class="kq kr ks kt gt ne nf ng nh aw ni bi"><span id="3d6a" class="nj lu it nf b gy nk nl l nm nn">python train_nli.py --word_emb_path ./glove.42B.300d.txt</span></pre><p id="1026" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于我的单个 GPU 虚拟机，完成培训大约需要 1 天时间。</p><h1 id="7da7" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">拿走</h1><p id="1c37" class="pw-post-body-paragraph jq jr it js b jt mr jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn im bi translated">要访问所有代码，可以访问我的<a class="ae ko" href="https://github.com/makcedward/nlp/blob/master/sample/nlp-embeddings-sentence-infersent.ipynb" rel="noopener ugc nofollow" target="_blank"> github </a> repo。</p><ul class=""><li id="2a4d" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn lk ll lm ln bi translated">与其他嵌入方法相比，<strong class="js iu"> InferSent 使用监督学习</strong>来计算单词向量。</li><li id="1fb4" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><strong class="js iu"> InferSent 利用单词嵌入</strong> (GloVe/ fastText)构建句子嵌入。</li><li id="6734" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated">预训练模型支持 GloVe(版本 1)和 fasttext(版本 2)</li></ul><h1 id="fb70" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph jq jr it js b jt mr jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以通过<a class="ae ko" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae ko" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ko" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="e154" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">参考</h1><p id="5819" class="pw-post-body-paragraph jq jr it js b jt mr jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn im bi translated">Conneau，D. Kiela，H. Schwenk，L. Barrault，A. Bordes，<a class="ae ko" href="https://arxiv.org/abs/1705.02364" rel="noopener ugc nofollow" target="_blank"> <em class="nd">从自然语言推理数据中监督学习通用语句表示</em> </a></p><p id="21ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://github.com/facebookresearch/InferSent" rel="noopener ugc nofollow" target="_blank">在 Pytorch 中感染</a></p></div></div>    
</body>
</html>