<html>
<head>
<title>Don’t Get Called a Charlatan: Building Credible Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不要被称为江湖骗子:建立可信的模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dont-get-called-a-charlatan-building-credible-models-4f4709eb760c?source=collection_archive---------15-----------------------#2018-02-26">https://towardsdatascience.com/dont-get-called-a-charlatan-building-credible-models-4f4709eb760c?source=collection_archive---------15-----------------------#2018-02-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/6e576155f6902258414f9cfd91d5b231.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*y0gcj-KSJvrHnQSlGjQ8bg.jpeg"/></div></div></figure><div class=""/><p id="26df" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你的客户理解你的模型，他们会更容易相信。</p><p id="5725" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">肖恩·惠勒(Schaun Wheeler)几周前在<a class="ae kx" rel="noopener" target="_blank" href="/an-ethical-code-cant-be-about-ethics-66acaea6f16f">发表的关于数据科学伦理的明智文章</a>触及了数据科学家的一个重要问题——客户可能会认为你是个骗子。虽然惠勒是在不同的背景下说的，但值得考虑的是，像“预测分析”和“机器学习”这样的项目据说正处于炒作的顶峰(根据Gartner的说法)，如果这是正确的，那么反弹可能就在眼前。</p><p id="e92f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这也是不可避免的，尤其是如果你正在生产的东西可以被看作是一个预测(这意味着尽可能广泛的术语)，你将是错误的——“所有的模型都是错误的”，毕竟。那么，显而易见的挑战是让你的观众相信你在做有用的事情，即使你的模型是错误的。</p><p id="cf16" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这种情况下，一个重要的工具就是解释你的模型的能力。如果没有对输入变量如何影响模型输出的某种形式的解释，你就无法对正在发生的事情做出任何假设。如果没有关于数据告诉你什么的假设，你就无法将结果与现有知识进行比较。这反过来又会立即束缚你利用主题专家的知识来改进模型的能力。除此之外，无法在您的模型中直接使用主题专家的知识是一个错失为您的工作赢得盟友的机会，也是一个错失改进您的模型的机会。</p><p id="7d6c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了给模型提供可信度，对它正在做什么的解释需要对用户有意义。如果你把模型呈现给用户，最好的情况是你的模型至少能说明一种他们已经知道的关系，以及另一种向他们展示新东西的关系。第一个确认模型已经找到了真实的关系。第二个说明模型发现了用户以前不知道的新东西。如果你不能提供任何与用户自己观察到的东西相一致的发现，他们不太可能会接受你的模型是可信的。同时，如果你不能给他们提供任何新的东西，他们不太可能接受你的工作是值得付出的。</p><p id="539a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">直觉上，有两种策略可以用来建立既准确又可解释的模型。首先是建立一个内在可解释的模型，并努力确保其准确性。另一个是建立一个准确的和工作来解释它。我们将研究以下任一策略下的一些选项。</p><p id="65d8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">可解释且准确的模型</strong></p><p id="2b56" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">解释你的模型最明显的方法是从一开始就让它可以解释。如果可能，对连续因变量使用线性回归，或对分类因变量使用逻辑回归(二项式或多项式，视情况而定)或其他适当的GLM(例如泊松、负二项式)。</p><p id="4231" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">提高模型预测性能的一种方法是采用收缩方法，如套索或岭回归。这些方法减少了与逐步回归相关的问题，即变量选择过程是谨慎的，因此是贪婪的，这可能导致高方差。作为一个例子，岭回归试图通过防止系数变得太大来减少这个问题，因此在完全丢弃变量和允许它们过度影响之间采取了中间路线。</p><p id="13f9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果放松对线性关系的假设，线性模型可以模拟更复杂的关系。这可以通过使用附加模型来说明有影响的非线性预测来实现。广义加性模型(GAM)使用诸如样条的平滑函数来表示非线性关系。Harrell支持的合理预测模型策略的一个要素是放宽关键变量的线性假设(由主题领域的知识决定)</p><p id="4b8f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这一领域与之前的观察结果紧密相关，即客户需要在他们的模型中看到一些他们已经知道和不知道的东西——展示了一个GAM，该GAM证实了客户之前的想法，即“年龄”是一个重要因素(他们确实知道的东西),但通过展示效果有一个峰值或逐渐减小(他们不知道的东西),扩大了他们的视角，为我带来了出色的客户认同。</p><p id="555f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">黑盒子里的窗户</strong></p><p id="22b3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">另一种选择是有一个表现良好的不透明模型，但提供其他模型来解释它。这种想法的一个扩展是用传统上认为不可解释的随机森林这样的算法建立一个准确的预测模型，并使用先进的技术来解释它。去年年底，Grover王子写了一篇关于如何在Python中解释随机森林的文章——目前作者的观点是，使用r可以有更多的选择。</p><p id="cc2a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，使用随机森林来发现量化关系以及进行预测是一个活跃的研究主题——例如在最近由<a class="ae kx" href="https://arxiv.org/pdf/1406.1845.pdf" rel="noopener ugc nofollow" target="_blank">门奇和胡克</a>发表的论文以及他们<a class="ae kx" href="https://arxiv.org/abs/1404.6473" rel="noopener ugc nofollow" target="_blank">更早的论文</a>中。更早的论文“量化随机森林中的不确定性”特别讨论了基于U-统计估计整个随机森林中特定预测因子对因变量影响的大小和方向的策略。在R <a class="ae kx" href="http://shftan.github.io/surfin/" rel="noopener ugc nofollow" target="_blank">这里</a>有一个这些想法的实验性实现，以及一个使用jacknife对随机森林进行推理的替代方法。</p><p id="6a88" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">R中的<a class="ae kx" href="https://cran.r-project.org/web/packages/inTrees/index.html" rel="noopener ugc nofollow" target="_blank"> inTrees </a>包也借鉴了类似的主题，它创建了一个树集合的规则集概要，是当今R中用于解释随机森林和其他树集合的几个包中的一个很好的例子。inTrees方法是从构成集成的树中提取规则，并根据规则的频率和错误等属性，保留最高质量的规则作为集成的解释或总结。</p><p id="964d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">上述方法仅适用于树的集合，包括随机森林和梯度推进机器。解释最近出现的任何算法的结果的一个选项是使用通用模型解释器，其中LIME(本地可解释模型不可知解释)可能是最突出的例子。</p><p id="970e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">与广义加性模型等方法相比，LIME将根据具体情况提供解释。也就是说，对于代表要评分的情况的一组参数，时间解释代表不同变量如何影响该特定情况；如果将另一种情况呈现给算法，变量的影响可能会非常不同。</p><p id="6c0c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">解释以水平条形图的形式呈现，显示不同变量影响的相对大小，向右延伸的条形代表使分类更有可能的变量，向左延伸的条形代表使结果不太可能的变量。在高层次上，变量的影响来自敏感性分析，该分析检查与感兴趣的病例非常相似的其他病例的分类结果。</p><p id="e584" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是LIME的局部方面——解释是在个案的基础上给出的，而不是作为一个整体为模型提供规则或指南。这是与前面讨论的树集合的方法的显著区别。此外，LIME目前仅适用于分类器，而非回归模型。LIME的一个R实现可从这里获得。</p><p id="966a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在当前环境下，确保用户能够信任模型的可解释模型的重要性日益得到认可；与此同时，幸运的是，不仅可解释模型的预测能力被正则化等技术扩展了，解释黑盒算法的方法也开始激增。</p></div></div>    
</body>
</html>