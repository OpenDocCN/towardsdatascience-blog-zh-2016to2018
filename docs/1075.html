<html>
<head>
<title>Learning Distributions over Rewards leads to State-of-the-art in RL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习奖励分配导致RL的艺术状态</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-distributions-over-rewards-leads-to-state-of-the-art-in-rl-5afbf70672e?source=collection_archive---------11-----------------------#2017-07-26">https://towardsdatascience.com/learning-distributions-over-rewards-leads-to-state-of-the-art-in-rl-5afbf70672e?source=collection_archive---------11-----------------------#2017-07-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="0019" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最近在DeepMind中完成的<a class="ae kl" href="https://arxiv.org/pdf/1707.06887.pdf" rel="noopener ugc nofollow" target="_blank">工作</a>“强化学习的分布式视角”展示了许多Atari游戏中具有特殊技巧的最新成果。他们训练了一个神经网络来提供可能的奖励分布，而不是单一的价值。</p><p id="a5c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基本上这是一个很好的老深度Q网络。但是，它并没有逼近预期的未来回报值，而是生成了可能结果的整体分布。这种变化背后的主要动机是分布可能有几个峰值。仅仅将它们平均为一个期望值可能是不合适的，并导致不充分的结果。</p><p id="1209" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">他们用代表不同奖励值范围的分类变量的概率代替了DQN的输出。他们测试了不同数量的区间，分割了可能的值范围:5、11、21、51和51个区间的表现远远优于其他区间。超出此范围的值被剪切到最后一个容器中。对学习算法进行了第二次修改，以处理分布贝尔曼方程。</p><p id="ebeb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与普通DQN相比，TensorFlow实施所需的计算时间增加了约25 %,但只需要一小部分训练步骤就可以实现卓越的性能。与DQN相比，在没有其他现代RL技术的情况下，这种方法几乎将具有超人性能的游戏数量增加了一倍:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/9ec220b9f871e9bb9011c770df3ca281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LjzoEjFQUQkX2Nsk.jpg"/></div></div></figure><p id="6e31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可以在原始论文中找到所有的数学和实现细节。虽然研究人员使用DQN作为基础，但更先进的模型可能会提供更好的性能。</p><h2 id="bf49" class="ky kz iq bd la lb lc dn ld le lf dp lg jy lh li lj kc lk ll lm kg ln lo lp lq bi translated">它看起来像什么</h2><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi lr"><img src="../Images/8b9cf6d641099cdef14a6c3615de46c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OQZVTsA20QdHestJ.jpg"/></div></div></figure><p id="ad6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">行动中:</p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="ls lt l"/></div></figure></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="c97e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mb">最初发表于</em> <a class="ae kl" href="http://cognitivechaos.com/learning-distributions-rewards-leads-state-art-rl/" rel="noopener ugc nofollow" target="_blank"> <em class="mb">认知混乱</em> </a> <em class="mb">。</em></p></div></div>    
</body>
</html>