<html>
<head>
<title>Multi-Layer Neural Networks with Sigmoid Function— Deep Learning for Rookies (2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有Sigmoid函数的多层神经网络——新手的深度学习(2)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f?source=collection_archive---------0-----------------------#2017-06-27">https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f?source=collection_archive---------0-----------------------#2017-06-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="8ba8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第1章:<a class="ae kl" href="https://medium.com/towards-data-science/introducing-deep-learning-and-neural-networks-deep-learning-for-rookies-1-bd68f9cf5883" rel="noopener">介绍深度学习和神经网络</a></p><p id="8842" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二章:具有Sigmoid函数的多层神经网络</p><p id="f467" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="km">关注我的</em> <a class="ae kl" href="https://twitter.com/nahuakang" rel="noopener ugc nofollow" target="_blank"> <em class="km">推特</em> </a> <em class="km">了解更多关于深度学习创业公司的生活。</em></p></div><div class="ab cl kn ko hu kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ij ik il im in"><p id="37ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大家好。欢迎回到我的第二篇系列文章<strong class="jp ir">菜鸟深度学习(DLFR) </strong>，作者是你忠实的菜鸟；)如果你觉得难以理解的话，请随意参考我在这里的第一篇文章。或者在本页用备注高亮显示或者在下方留下评论！我们也非常感谢您的反馈。</p><p id="36bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这次我们将更深入地研究神经网络，这篇文章将比上一次更具技术性。但是不用担心，我会让你在没有CS/数学背景的情况下，尽可能简单直观地学习基础知识。你很快就能吹嘘你的理解了；)</p><h1 id="d157" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">来自DLFR 1的评论</h1><p id="2413" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">上次，我们介绍了深度学习领域，并检查了一个简单的神经网络——感知器……或恐龙……好吧，说真的，单层感知器。我们还研究了感知器网络如何处理我们输入的输入数据并返回输出。</p><p id="ee62" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关键概念:输入数据，权重，求和加偏，激活函数(具体是阶跃函数)，然后输出。厌倦了吗？别担心:)我保证会有……更多的术语出现！但是你很快就会习惯的。我保证。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi lx"><img src="../Images/8706a7726ca41af93f08e9b8c13c8cb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v88ySSMr7JLaIBjwr4chTw.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk"><strong class="bd mn">Graph 1:</strong> Procedures of a Single-layer Perceptron Network</figcaption></figure><p id="a897" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回到20世纪50年代和60年代，人们没有单层感知器的有效学习算法来学习和识别非线性模式(还记得XOR门问题吗？).公众对感知器失去了兴趣。毕竟，现实世界中的大多数问题都是非线性的，作为个体人类，你和我都非常擅长线性或二元问题的决策，如<strong class="jp ir"> <em class="km">我是否应该研究深度学习</em> </strong>而不需要感知机。好吧，“好”在这里是一个棘手的词，因为我们的大脑真的不是那么理性。但我会把这个问题留给行为经济学家和心理学家。</p><h1 id="f768" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">突破:多层感知器</h1><p id="b329" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">快进近20年，到1986年，Geoffrey Hinton、David Rumelhart和Ronald Williams发表了一篇论文“<em class="km">通过反向传播错误</em>学习表征”，其中介绍了:</p><ol class=""><li id="dc34" class="mo mp iq jp b jq jr ju jv jy mq kc mr kg ms kk mt mu mv mw bi translated"><strong class="jp ir">反向传播</strong>，<strong class="jp ir">到<em class="km">的一个过程，反复调整权重</em>到</strong>以使实际输出和期望输出之间的差异最小化</li><li id="84b4" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated"><strong class="jp ir">隐藏层</strong>，是堆叠在输入和输出 之间的<strong class="jp ir"> <em class="km">神经元节点，允许神经网络学习更复杂的特性(如异或逻辑)</em></strong></li></ol><p id="b3dc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你对DL完全陌生，你应该还记得杰弗里·辛顿(Geoffrey Hinton)，他在DL的发展过程中起着举足轻重的作用。这是一些重大新闻:我们20年来一直认为神经网络没有解决现实世界问题的前景。现在我们看到岸上灯塔发出的光了！让我们来看看这两个新的介绍。</p><p id="114c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嗯，第一个，<strong class="jp ir">反向传播</strong>，上一篇帖子提到过。还记得我们反复强调设计神经网络的重要性，以便网络可以从<strong class="jp ir"> <em class="km">期望输出</em> </strong>(事实是什么)和<strong class="jp ir"> <em class="km">实际输出</em> </strong>(网络返回什么)之间的差异中学习，然后向权重发回信号并要求权重进行自我调整吗？这将使网络的输出在下次运行时更接近于期望的输出。</p><p id="73d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二个呢，隐藏层？什么是隐藏层？一个隐藏层就把单层感知器变成了多层感知器！计划是这样的，由于技术原因，<strong class="jp ir"> <em class="km">我将在这篇文章中重点讨论隐藏层，然后在下一篇文章</em> </strong>中讨论反向传播。</p><p id="c476" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里只是一个随机的有趣事实:我怀疑Geoffrey Hinton保持着谷歌:D最年长实习生的记录。不管怎样，如果你已经熟悉机器学习，我相信他在Coursera上的课程会很适合你。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nc"><img src="../Images/dbd497a48316e3d885f3aef30563d38e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HKfw7SKdWINMZuSUcs7d6g.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Geoffrey Hinton, one of the most important figures in Deep Learning. Source: thestar.com</figcaption></figure><h1 id="f23c" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">具有隐藏层的神经网络</strong></h1><p id="10af" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">神经网络的隐藏层实际上就是在输入和输出层之间增加更多的神经元。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nd"><img src="../Images/07d94764662531941fde2855fcb5fd71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CJEBy3GCaGQKNx7PEy-w5w.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk"><strong class="bd mn">Graph 2:</strong> Left: Single-Layer Perceptron; Right: Perceptron with Hidden Layer</figcaption></figure><p id="60a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输入层中的数据用下标<strong class="jp ir"> <em class="km"> 1，2，3，…，m </em> </strong>标记为<strong class="jp ir"> <em class="km"> x </em> </strong>。隐藏层中的神经元用下标<strong class="jp ir"> <em class="km"> 1，2，3，…，n </em> </strong>标记为<strong class="jp ir"> <em class="km"> h </em> </strong>。注意隐藏层<strong class="jp ir">是<em class="km"> n </em>而不是<em class="km"> m </em> </strong>，因为隐藏层神经元的数量可能与输入数据中的数量不同。并且如下图所示，隐藏层神经元也标有上标<strong class="jp ir"> <em class="km"> 1 </em> </strong>。这是为了当你有几个隐藏层时，你可以识别它是哪个隐藏层:<em class="km">第一个隐藏层有上标1，第二个隐藏层有上标2，以此类推，像在</em> <strong class="jp ir"> <em class="km">图3 </em> </strong>中。输出用一个<strong class="jp ir"> <em class="km">帽子</em> </strong>标注为<strong class="jp ir"> <em class="km"> y </em> </strong>。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ne"><img src="../Images/63a582813bdbb5799bd8e43fffa0863d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7mZxWa3zvTzdtFZFoHSqBg.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk"><strong class="bd mn">Graph 3:</strong> We label input layer as x with subscripts 1, 2, …, m; hidden layer as h with subscripts 1, 2, …, n; output layer with a hat</figcaption></figure><p id="6e57" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了使生活更容易，我们将使用一些术语来使事情变得清晰一些。我知道，术语可能很烦人但是你会习惯的:)首先，如果我们有<strong class="jp ir"> <em class="km"> m </em> </strong>输入数据(<strong class="jp ir"> <em class="km"> x1，x2，…，xm </em> </strong>)，我们称这个<strong class="jp ir"> <em class="km"> m特性</em> </strong>。特征只是我们认为对特定结果有影响的一个变量。就像我们关于<strong class="jp ir"> <em class="km">结果的例子如果你决定学不学DL，</em> </strong>我们有3个特点:1。学了DL，2以后会赚更多的钱吗？数学/编程难吗，3。你需要一个GPU来开始吗？</p><p id="2ec4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其次，当我们将m个特征中的每一个乘以一个权重(<strong class="jp ir"> <em class="km"> w1，w2，…，wm </em> </strong>)并将它们加在一起时，这就是一个<strong class="jp ir">点积</strong>:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nf"><img src="../Images/93194ff95f24590e6086e4efa5b8b84b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FyWj1TIC42Lf98wc7XyY6w.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Definition of a Dot Product</figcaption></figure><p id="072a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以现在我们来看看几点:</p><ol class=""><li id="2d4b" class="mo mp iq jp b jq jr ju jv jy mq kc mr kg ms kk mt mu mv mw bi translated">用<strong class="jp ir"> <em class="km"> m特性</em> </strong>在输入<strong class="jp ir"><em class="km"/></strong>X时，需要用<strong class="jp ir"><em class="km"/></strong>m权重来执行点积</li><li id="69d5" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated">用<strong class="jp ir"> <em class="km"> n </em> </strong>隐层中的隐神经元，你需要<strong class="jp ir"> <em class="km"> n </em> </strong>套权值(<strong class="jp ir"> <em class="km"> W1，W2，… Wn </em> </strong>)来进行点积</li><li id="88b8" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated">有了1个隐藏层，你执行<strong class="jp ir"> <em class="km"> n </em> </strong>点积得到隐藏输出<strong class="jp ir"><em class="km">h:</em></strong><em class="km"/>(<strong class="jp ir"><em class="km">h1，h2，…，hn </em> </strong>)</li><li id="9c47" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated">然后就像单层感知器一样，我们用隐藏输出<strong class="jp ir"> <em class="km"> h: </em> </strong> ( <strong class="jp ir"> <em class="km"> h1，h2，…，hn </em> </strong>)作为具有<strong class="jp ir"> n个特征的输入数据，</strong>用1组<strong class="jp ir"><em class="km"/></strong>权重(<strong class="jp ir"> <em class="km"> w1，w2，…，wn </em> </strong>)执行<strong class="jp ir"> </strong>点积，得到你的最终输出<strong class="jp ir"/></li></ol><p id="c395" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输入值如何<em class="km">前向传播</em>到隐含层，然后从隐含层到输出的过程与<strong class="jp ir">图1 </strong>相同。下面我提供了如何做到这一点的描述，在图4 中使用了下面的神经网络。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ng"><img src="../Images/7283836f286e1ce0e5f4509ca3a9a37b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_M4bZyuwaGby6KMiYVYXvg.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Graph 4: Example</figcaption></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nh"><img src="../Images/e856d356305ec6d304e23fe0836e19f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MpP9dnU9xR5mKcCs9RG1nw.jpeg"/></div></div></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ni"><img src="../Images/7a44da4c80c49ad5f45cc068e10317e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M6NfCYovYOcFG2d5-L2MDQ.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Graph 5: Procedure to Hidden Layer Outputs</figcaption></figure><p id="1059" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在隐藏层的输出被计算出来了，我们用它们作为输入来计算最终的输出。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nj"><img src="../Images/90e5f98bdb5e6d3014dcc47d5079b9e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2YiX_-go-mHQBRB9-jpA5Q.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Graph 6: Procedure to Final Output</figcaption></figure><p id="5f45" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">耶！现在你完全理解多层感知器是如何工作的了:)它就像一个单层感知器，除了在这个过程中你有更多的权重。当你在具有更多更多特征的大型数据集上训练神经网络时(比如自然语言处理中的word2vec)，这个过程会耗尽你计算机中的大量内存。这就是为什么深度学习直到过去几年才起飞的一个原因，当时我们开始生产更好的硬件，可以处理消耗内存的深度神经网络。</p><h1 id="271f" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">乙状结肠神经元:介绍</h1><p id="66c1" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">所以现在我们有了一个更复杂的带有隐藏层的神经网络。但是我们还没有用阶跃函数解决激活问题。</p><p id="f273" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上一篇文章中，我们谈到了阶跃函数线性的局限性。需要记住的一点是:<strong class="jp ir">如果激活函数是线性的，那么你可以在神经网络中任意堆叠多个隐层，最终输出的仍然是原始输入数据</strong>的一个 <a class="ae kl" href="https://stats.stackexchange.com/a/192073/166513" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">线性组合</strong> </a> <strong class="jp ir">。如果概念难以理解，请<a class="ae kl" href="https://www.quora.com/Why-does-deep-learning-architectures-use-only-non-linear-activation-function-in-the-hidden-layers/answer/David-Kobilnyk?srid=Lfu1" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">务必阅读此链接</strong> </a> <strong class="jp ir"> </strong>以获得<strong class="jp ir"> </strong>的解释。这种线性意味着它不能真正掌握非线性问题的复杂性，如XOR逻辑或由曲线或圆分隔的模式。</strong></p><p id="8c66" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同时，阶跃函数也没有有用的导数(它的导数在x轴上的0点处处处为0或未定义)。对于<strong class="jp ir">反向传播</strong>不起作用，这个我们在下一篇帖子里一定会讲到！</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d2becb57cba6e1cf109176c2f0c78f8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*xtRZobdx2L0CYYecyLFWEg.jpeg"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Graph 7: Step Function</figcaption></figure><p id="8ab4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">好吧，这里有另一个问题:具有阶跃函数的感知器作为神经网络的“关系候选者”不是很“稳定”。想想看:这个女孩(或男孩)有严重的躁郁症！有一天(对于<em class="km"> z </em> <em class="km"> &lt; 0 </em>)，(s)他都是“安静”“下来”，给你零反应。然后又过了一天(对于<em class="km"> z </em> <em class="km"> ≥ 0 </em>)，他突然“健谈”“活泼”，跟你说个不停。巨大的变化！她/他的心情是没有过渡的，你不知道什么时候是下降还是上升。是的…那是阶梯函数。</p><p id="bfc2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，基本上，我们感知机网络输入层中任何权重的微小变化都可能导致一个神经元突然从0翻转到1，这可能再次影响隐藏层的行为，然后影响最终结果。就像我们已经说过的，我们想要一种学习算法，它可以通过逐渐改变权重来改善我们的神经网络，而不是通过平坦的无响应或突然的跳跃。如果我们不能使用阶跃函数来逐渐改变权重，那么它就不应该是选择。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nl"><img src="../Images/9b6c51763ab974f16f521be397f038eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jyHcwrLJlSjQ7QfypGl1YA.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Graph 8: We Want Gradual Change in Weights to Gradually Change Outputs</figcaption></figure><p id="5ff5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在告别带阶跃函数的感知器。我们正在为我们的神经网络寻找一个新的合作伙伴，<strong class="jp ir"> sigmoid神经元</strong>，它具有sigmoid功能(duh)。但是不要担心:唯一会改变的是激活函数，到目前为止我们所了解的关于神经网络的一切仍然适用于这种新型神经元！</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nm"><img src="../Images/92a71ce0e6b963213d55a44fe953d78b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TdlBJnoH3GcKwAIFv1ASxg.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Sigmoid Function</figcaption></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/063aff2233c5f138c7f9fbed8718a1aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*sOtpVYq2Msjxz51XMn1QSA.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Graph 9: Sigmoid Function using Matplotlib</figcaption></figure><p id="350d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果函数对你来说看起来很抽象或者很奇怪，那么不要太在意像欧拉数<strong class="jp ir"> <em class="km"> e </em> </strong>之类的细节或者最初是怎么有人想出这个疯狂的函数的。对于那些不精通数学的人来说，关于图9 中的sigmoid函数，唯一重要的事情首先是它的曲线，其次是它的导数。以下是一些更多的细节:</p><ol class=""><li id="bfcb" class="mo mp iq jp b jq jr ju jv jy mq kc mr kg ms kk mt mu mv mw bi translated">Sigmoid函数产生与阶跃函数相似的结果，输出介于0和1之间。曲线在<strong class="jp ir"> <em class="km"> z=0 </em> </strong>处过0.5，我们可以为激活函数设置规则，比如:如果乙状结肠神经元的输出大于等于0.5，则输出1；如果输出小于0.5，则输出0。</li><li id="8539" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated">Sigmoid函数的曲线上没有加加速度。它是光滑的，它有一个非常好的简单的导数<strong class="jp ir"><em class="km">【σ(z)*(1-σ(z)】,</em></strong><em class="km"/>，在曲线上处处可微。导数的微积分推导可以在栈溢出<a class="ae kl" href="https://math.stackexchange.com/a/1225116/425514" rel="noopener ugc nofollow" target="_blank">这里</a>找到如果你想看的话。但是不一定要知道怎么推导。这里没有压力。</li><li id="097f" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated">如果<strong class="jp ir"> <em class="km"> z </em> </strong>非常负，那么输出大约为0；如果<strong class="jp ir"> <em class="km"> z </em> </strong>非常正，则输出约为1；但在<strong class="jp ir"> <em class="km"> z=0 </em> </strong>左右，其中<strong class="jp ir"> <em class="km"> z </em> </strong>既不太大也不太小(在<strong class="jp ir">图9 </strong>中两条外侧垂直网格虚线之间)，随着<strong class="jp ir"><em class="km"/></strong>的变化，我们会有相对更多的偏差。</li></ol><p id="7223" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，这似乎是我们神经网络的测年材料:)Sigmoid函数，不同于阶跃函数，将非线性引入到我们的神经网络模型中。非线性只是指我们从神经元得到的输出，是一些输入<strong class="jp ir"> <em class="km"> x (x1，x2，…，xm) </em> </strong>和权重<strong class="jp ir"> <em class="km"> w (w1，w2，…，wm) </em> </strong>加偏置然后放入一个sigmoid函数的点积，不能用输入<strong class="jp ir"><em class="km">【x(x1，x2，…，XM)</em><em class="km">的一个<a class="ae kl" href="https://stats.stackexchange.com/a/192073/166513" rel="noopener ugc nofollow" target="_blank">线性组合来表示</a></em></strong></p><p id="4ff7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当被多层神经网络中的每个神经元使用时，该非线性激活函数产生原始数据的新的“<a class="ae kl" href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" rel="noopener ugc nofollow" target="_blank">表示</a>，并且最终允许非线性判定边界，例如XOR。因此，在XOR的情况下，如果我们在隐藏层中添加两个sigmoid神经元，我们可以在另一个空间中，将原始2D图重塑为类似下面图10 左侧的3D图像。因此，该脊允许对异或门进行分类，它代表了<strong class="jp ir">图10 </strong>右侧2D异或门的浅黄色区域。所以如果我们的输出值在山脊较高的区域上，那么应该是真或者1(就像天气冷但不热，或者天气热但不冷)；如果我们的输出值在两个角上较低的平坦区域，那么它为假或0，因为说天气既热又冷或既不热也不冷是不对的(好吧，我想天气可能既不热也不冷…你明白我的意思吧…对吗？).</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/0eb2955ec0c03dea19f88325aaca25b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Eb12oyNamqFduS2LN75yuQ.jpeg"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Graph 10. Representation of Neural Networks with Hidden Layers to Classify XOR Gate. Source: <a class="ae kl" href="http://colinfahey.com/" rel="noopener ugc nofollow" target="_blank">http://colinfahey.com/</a></figcaption></figure><p id="c89a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我知道这些关于非线性的谈话可能会令人困惑，所以请在这里阅读更多关于线性和非线性的内容<a class="ae kl" href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" rel="noopener ugc nofollow" target="_blank"/>(Christopher Olah的一个很棒的博客中的动画直观的帖子)，在这里阅读<a class="ae kl" href="https://medium.com/@vivek.yadav/how-neural-networks-learn-nonlinear-functions-and-classify-linearly-non-separable-data-22328e7e5be1" rel="noopener"/>(由<a class="no np ep" href="https://medium.com/u/b783495cc56b?source=post_page-----bf464f09eb7f--------------------------------" rel="noopener" target="_blank"> Vivek Yadav </a>带ReLU激活功能)，在这里阅读<a class="ae kl" href="https://www.quora.com/What-is-the-role-of-the-activation-function-in-a-neural-network/answer/Sebastian-Raschka-1?srid=Lfu1" rel="noopener ugc nofollow" target="_blank"/>(由Sebastian Raschka)。希望你已经意识到为什么非线性激活函数是重要的，如果没有，放轻松，给自己一些时间来消化它。</p><p id="6616" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">问题解决了……暂时的；)我们将在不久的将来看到一些不同类型的激活函数，因为sigmoid函数也有自己的问题！比较流行的有<strong class="jp ir"> <em class="km"> tanh </em> </strong>和<strong class="jp ir"> <em class="km"> ReLU </em> </strong>。然而，这是另一篇文章。</p><h1 id="d776" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">多层神经网络:直观的方法</h1><p id="0fae" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">好吧。因此，我们在神经网络中引入了隐藏层，并用sigmoid神经元取代了感知器。我们还介绍了非线性激活函数允许对我们的数据中的非线性决策边界或模式进行分类的思想。你可以记住这些要点，因为它们是事实，但我鼓励你在互联网上搜索一下，看看你是否能更好地理解这个概念(我们自然需要一些时间来理解这些概念)。</p><p id="964e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们从未讨论过一个非常重要的问题:首先，我们究竟为什么要在神经网络中设置隐藏层？隐藏层如何神奇地帮助我们解决单层神经元无法解决的复杂问题？</p><p id="28ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从上面的XOR示例中，您已经看到了在1个隐藏层中添加两个隐藏神经元可以将我们的问题重新塑造到一个不同的空间中，这神奇地为我们创建了一种用脊对XOR进行分类的方法。因此，隐藏层以某种方式扭曲了问题，使神经网络很容易对问题或模式进行分类。现在，我们将使用一个经典的教科书示例:手写数字的识别，来帮助您直观地理解隐藏层的作用。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b3ed538a825bc79a8d0ebba75762e693.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*g50LVDcK9w8Gs_CXiX3kVw.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Graph 11. MNIST dataset of Hand-written Digits. Source: <a class="ae kl" href="http://neuralnetworksanddeeplearning.com/" rel="noopener ugc nofollow" target="_blank">http://neuralnetworksanddeeplearning.com/</a></figcaption></figure><p id="8552" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图11 中的数字<strong class="jp ir">属于一个名为<a class="ae kl" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST </a>的数据集。它包含了70，000个人手书写的数字的例子。这些数字中的每一个都是28x28像素的图片。所以一个数字的每个图像总共有28*28=784个像素。每个像素取0到255之间的值(RGB颜色代码)。0表示颜色是白色，255表示颜色是黑色。</strong></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/465e8c9e15e605661675bf31268964b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:62/format:webp/1*5awqgI4YrAK6DwmyBS__Tw.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Graph 12. MNIST digit 5, which consist of 28x28 pixel values between 0 and 255. Source: <a class="ae kl" href="http://neuralnetworksanddeeplearning.com/" rel="noopener ugc nofollow" target="_blank">http://neuralnetworksanddeeplearning.com/</a></figcaption></figure><p id="80fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，计算机无法像我们人类一样真正“看到”一个数字，但如果我们将图像分解为一个由784个数字组成的数组，如[0，0，180，16，230，…，4，77，0，0，0，0]，那么我们可以将这个数组输入到我们的神经网络中。计算机不能通过“看”来理解图像，但它可以理解和分析代表图像的像素数量。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ns"><img src="../Images/22939f90cfc73a6da7995a2b9c564b1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J7RUyPQ-9UdHAiFMmWY9ww.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Graph 13: Multi-Layer Sigmoid Neural Network with 784 input neurons, 16 hidden neurons, and 10 output neurons</figcaption></figure><p id="8a2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，让我们建立一个类似上面<strong class="jp ir">图13 </strong>中的神经网络。它有784个28×28像素值的输入神经元。假设它有16个隐藏神经元和10个输出神经元。10个输出神经元以数组的形式返回给我们，每个神经元负责对一个数字从0到9进行分类。所以如果神经网络认为手写数字是零，那么我们应该得到一个输出数组[1，0，0，0，0，0，0，0，0，0]，这个数组中第一个感觉数字是零的输出被我们的神经网络“激发”为1，其余的都是0。如果神经网络认为手写数字是5，那么我们应该得到[0，0，0，0，0，1，0，0，0，0，0，0]。负责分类5的第6个元素被触发，而其他元素没有被触发。如此等等。</p><p id="8bdf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还记得我们提到过，神经网络通过对数据进行重复训练来变得更好，这样它们就可以调整网络每一层的权重，以使最终结果/实际输出更接近所需输出吗？因此，当我们实际上用MNIST数据集中的所有训练样本训练这个神经网络时，我们不知道应该给每一层分配什么权重。所以我们只是随机要求计算机在每一层分配权重。(我们不希望所有的权重都是0，如果空间允许，我会在下一篇文章中解释)。</p><p id="9a42" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种随机初始化权重的概念很重要，因为每次训练深度学习神经网络时，你都在为权重初始化不同的数字。所以本质上，在网络被训练之前，你和我都不知道神经网络中发生了什么。一个经过训练的神经网络具有在特定值优化的权重，这些权重对我们的问题做出最佳预测或分类。从字面上看，这是一个黑匣子。并且每次经过训练的网络将具有不同的权重集。</p><p id="df82" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了便于论证，让我们想象一下<strong class="jp ir">图14 </strong>中的以下情况，这是我从迈克尔·尼尔森的<a class="ae kl" href="http://neuralnetworksanddeeplearning.com/chap1.html" rel="noopener ugc nofollow" target="_blank">在线书籍</a>中借来的:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nt"><img src="../Images/6de3540dd5e4c2f0088337eca8bff00c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TbkzovkGpy7SbhqBAPnICw.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Graph 14. An Intuitive Example to Understand Hidden Layers</figcaption></figure><p id="81fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在监督学习中用一轮又一轮的标记数据训练神经网络后，假设前4个隐藏神经元学会识别上面<strong class="jp ir">图14 </strong>左侧的模式。然后，如果我们向神经网络输入一个手写数字0的数组，网络应该正确地触发隐藏层中的前4个隐藏神经元，而其他隐藏神经元保持沉默，然后再次触发第一个输出神经元，而其余的神经元保持沉默。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nt"><img src="../Images/e1ec6d3ea6de64531b2f5f92f1e3f915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p1Y_HIMPD3WjVkZXOxnsiA.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Graph 15. Neural Networks are Black Boxes. Each Time is Different.</figcaption></figure><p id="75e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你用一组新的随机权重来训练神经网络，它可能会产生下面的网络(<strong class="jp ir">比较图15和图14 </strong>)，因为权重是随机的，我们永远不知道哪个会学习哪个或什么模式。但是，如果训练得当，网络仍然应该触发正确的隐藏神经元，然后产生正确的输出。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nu"><img src="../Images/fc314e99d43101df5d2f0452473177f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FT5wX5ssFrZ8fI2qDQo-yQ.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Our quote this week ;)</figcaption></figure><p id="4cd6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后要提一点:在多层神经网络中，第一个隐藏层将能够学习一些非常简单的模式。每增加一层隐藏层，就能逐渐学习更复杂的模式。请看来自《科学美国人》的<strong class="jp ir">图16 </strong>中的人脸识别示例:)</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nv"><img src="../Images/cb15ca35be0d825b818534fa5ccdafac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3YHA_Hyx-3n0xvgKnwRUIQ.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Graph 16: Each Hidden Layer Learns More Complex Features. Source: Scientific American</figcaption></figure><p id="0e8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一些很棒的人制作了下面的网站，让你玩玩神经网络，看看隐藏层是如何工作的。试试看。真的很好玩！</p><div class="nw nx gp gr ny nz"><a href="http://playground.tensorflow.org/" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">张量流-神经网络游乐场</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">这是一种构建从数据中学习的计算机程序的技术。它非常松散地基于我们如何思考…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">playground.tensorflow.org</p></div></div><div class="oi l"><div class="oj l ok ol om oi on mh nz"/></div></div></a></div><p id="9803" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Ophir Samson 写了一篇很好的文章<a class="ae kl" href="https://medium.com/towards-data-science/deep-learning-weekly-piece-whats-a-neural-network-aa0df888d8a2" rel="noopener">，也解释了什么是神经网络</a>，有很好的可视化效果，而且很简洁！</p><div class="nw nx gp gr ny nz"><a href="https://medium.com/towards-data-science/deep-learning-weekly-piece-whats-a-neural-network-aa0df888d8a2" rel="noopener follow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">深度学习周刊:什么是神经网络？</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">对于本周的文章，我想通过我收集的一个简单的例子来阐明什么是神经网络…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">medium.com</p></div></div><div class="oi l"><div class="oo l ok ol om oi on mh nz"/></div></div></a></div><h1 id="fb9e" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">概述</h1><p id="ab51" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">在这篇文章中，我们回顾了感知器的局限性，介绍了具有新激活函数的sigmoid神经元，称为sigmoid function。我们还讨论了多层神经网络如何工作，以及神经网络中隐藏层背后的直觉。</p><p id="540b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在几乎完成了理解基本神经网络的全部课程；)呵呵，还没完呢！在下一篇文章中，我将会谈到一种叫做损失函数的东西，以及这种神秘的反向传播，我们只提到过，但从来没有访问过！如果你等得不耐烦，请查看以下链接:</p><div class="nw nx gp gr ny nz"><a href="http://cs231n.github.io/optimization-2/" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">用于视觉识别的CS231n卷积神经网络</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">斯坦福CS231n课程材料和笔记:视觉识别的卷积神经网络。</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">cs231n.github.io</p></div></div></div></a></div><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="op oq l"/></div></figure><div class="nw nx gp gr ny nz"><a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">一个逐步反向传播的例子</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">背景技术反向传播是训练神经网络的常用方法。网上不缺论文说…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">mattmazur.com</p></div></div><div class="oi l"><div class="or l ok ol om oi on mh nz"/></div></div></a></div><p id="e368" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">敬请关注，最重要的是，享受学习:D的乐趣</p><p id="d733" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你喜欢这次阅读吗？别忘了关注我的 <a class="ae kl" href="https://twitter.com/nahuakang" rel="noopener ugc nofollow" target="_blank"> <em class="km">推特</em> </a> <em class="km">！</em></p></div></div>    
</body>
</html>