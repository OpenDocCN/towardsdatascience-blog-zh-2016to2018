<html>
<head>
<title>Text Classification in Keras (Part 1) — A Simple Reuters News Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras 中的文本分类(第一部分)——一个简单的路透社新闻分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-classification-in-keras-part-1-a-simple-reuters-news-classifier-9558d34d01d3?source=collection_archive---------8-----------------------#2018-08-23">https://towardsdatascience.com/text-classification-in-keras-part-1-a-simple-reuters-news-classifier-9558d34d01d3?source=collection_archive---------8-----------------------#2018-08-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a297" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Keras 中教授 NLP 和文本分类系列的第 1 部分</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/933fd967e4e56f22b67fc3adceca9a8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HgXA9v1EsqlrRDaC_iORhQ.png"/></div></figure></div><div class="ab cl kn ko hu kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ij ik il im in"><h1 id="ea95" class="ku kv iq bd kw kx ky kz la lb lc ld le jw lf jx lg jz lh ka li kc lj kd lk ll bi translated">教程视频</h1><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lm ln l"/></div></figure><p id="6458" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">如果你喜欢这个视频或者觉得它有任何帮助，如果你给我一两美元来资助我的机器学习教育和研究，我会永远爱你！每一美元都让我离成功更近一步，我永远心存感激。</p><h1 id="fd4b" class="ku kv iq bd kw kx ml kz la lb mm ld le jw mn jx lg jz mo ka li kc mp kd lk ll bi translated">代码</h1><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="40ba" class="mv kv iq mr b gy mw mx l my mz">import keras<br/>from keras.datasets import reuters</span><span id="8477" class="mv kv iq mr b gy na mx l my mz">Using TensorFlow backend.</span><span id="eb37" class="mv kv iq mr b gy na mx l my mz">(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)<br/>word_index = reuters.get_word_index(path="reuters_word_index.json")<br/><br/>print('# of Training Samples: {}'.format(len(x_train)))<br/>print('# of Test Samples: {}'.format(len(x_test)))<br/><br/>num_classes = max(y_train) + 1<br/>print('# of Classes: {}'.format(num_classes))</span><span id="20f7" class="mv kv iq mr b gy na mx l my mz"># of Training Samples: 8982<br/># of Test Samples: 2246<br/># of Classes: 46</span><span id="320b" class="mv kv iq mr b gy na mx l my mz">index_to_word = {}<br/>for key, value in word_index.items():<br/>    index_to_word[value] = key</span><span id="241a" class="mv kv iq mr b gy na mx l my mz">print(' '.join([index_to_word[x] for x in x_train[0]]))<br/>print(y_train[0])</span><span id="8c70" class="mv kv iq mr b gy na mx l my mz">the wattie nondiscriminatory mln loss for plc said at only ended said commonwealth could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 psbr oils several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed largely april 0 are 2 states will billion total and against 000 pct dlrs<br/>3</span><span id="7fe0" class="mv kv iq mr b gy na mx l my mz">from keras.preprocessing.text import Tokenizer<br/><br/>max_words = 10000<br/><br/>tokenizer = Tokenizer(num_words=max_words)<br/>x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')<br/>x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')<br/><br/>y_train = keras.utils.to_categorical(y_train, num_classes)<br/>y_test = keras.utils.to_categorical(y_test, num_classes)</span><span id="86d0" class="mv kv iq mr b gy na mx l my mz">print(x_train[0])<br/>print(len(x_train[0]))<br/><br/>print(y_train[0])<br/>print(len(y_train[0]))</span><span id="2bca" class="mv kv iq mr b gy na mx l my mz">[0. 1. 0. ... 0. 0. 0.]<br/>10000<br/>[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.<br/> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]<br/>46</span><span id="512d" class="mv kv iq mr b gy na mx l my mz">from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, Activation<br/><br/>model = Sequential()<br/>model.add(Dense(512, input_shape=(max_words,)))<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(num_classes))<br/>model.add(Activation('softmax'))</span><span id="e643" class="mv kv iq mr b gy na mx l my mz">model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>print(model.metrics_names)</span><span id="394f" class="mv kv iq mr b gy na mx l my mz">['loss', 'acc']</span><span id="064b" class="mv kv iq mr b gy na mx l my mz">batch_size = 32<br/>epochs = 3<br/><br/>history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)<br/>score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)<br/>print('Test loss:', score[0])<br/>print('Test accuracy:', score[1])</span><span id="8fb2" class="mv kv iq mr b gy na mx l my mz">Train on 8083 samples, validate on 899 samples<br/>Epoch 1/3<br/>8083/8083 [==============================] - 13s 2ms/step - loss: 1.3051 - acc: 0.7192 - val_loss: 0.9643 - val_acc: 0.7931<br/>Epoch 2/3<br/>8083/8083 [==============================] - 12s 2ms/step - loss: 0.5136 - acc: 0.8841 - val_loss: 0.8800 - val_acc: 0.8165<br/>Epoch 3/3<br/>8083/8083 [==============================] - 13s 2ms/step - loss: 0.2873 - acc: 0.9344 - val_loss: 0.9045 - val_acc: 0.8065<br/>2246/2246 [==============================] - 0s 175us/step<br/>Test loss: 0.8878143835789586<br/>Test accuracy: 0.7983081033478224</span></pre></div><div class="ab cl kn ko hu kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ij ik il im in"><p id="728a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">原贴于 hunterheidenreich.com 的<a class="ae mk" href="http://hunterheidenreich.com/blog/keras-text-classification-part-1/" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>