# 神经网络:误差预测层

> 原文：<https://towardsdatascience.com/neural-networks-error-prediction-layers-fd8181dc33cd?source=collection_archive---------4----------------------->

早在 2005 年，杰夫·霍金斯就写过“[论智力](https://www.amazon.com/Intelligence-Understanding-Creation-Intelligent-Machines/dp/0805078533)”——关于人类神经科学中的一个奇特发现，*尚未被深度学习利用*。它值得仔细看看。

人类、海豚和猴子拥有不同于其他生物的大脑:在我们的额叶，我们有许多**相邻的多种*类型*神经元的堆叠**——就像一张覆盖着盘子的早餐桌上，每个盘子上都有自己的一堆煎饼，点缀着配料。这些神经元以一种特殊的方式发挥作用:

*   最下面的*层*被赋予**预测下一时刻**的任务；每当它知道未来时，多巴胺就会激增，并加强它的联系。
*   下一个层*的任务是**预测下层何时出错**；每当它知道下层*的未来*时，它就会感到兴奋，并加强它的学习。*
*   *进一步的*层也被赋予预测其下层何时**出错**的任务；他们学会成为错误检测者。

人类在我们的“煎饼堆”中有六层，我们有一个有数百万个盘子的“早餐桌”，以形成我们更高层次的推理和自我反思的智能。这个模型与今天的人工神经网络完全不同。

**但是，深度神经网络工作得很好……**

是的，他们有。甘斯和 LSTM 网络公司也是如此。这些方法很快就找到了将数据压缩成特征的好方法。然而，我们的大脑做的不止这些。

让我们比较一下:一个循环神经网络，和人脑。rnn 接收环境的当前状态(其“输入”是屏幕上的像素)并在该环境中产生一个动作(“输出”在可能动作的空间中，而“输入”在可能像素排列的空间*)。然而，人类的额叶有一个最底层的“煎饼”，它取系统的当前状态(即“输入”)，并试图**预测系统的未来状态**(“输出”是与“输入”在同一空间上的*，**损失函数是两个**之差)。人脑不会像 RNN 那样试图“选择游戏中的最佳走法”！我们实际上是在试图**预测接下来**会发生什么。**

*此外，一个 RNN 可能有许多“层”神经元，但它只代表单一“层”的*功能*。像素输入→动作输出。我们自己的“煎饼”栈实际上就像多个神经网络的*栈*一样运行。每个“煎饼”接受状态输入→状态预测输出。更高层的“煎饼”起着**错误检测器**的作用，因为它们观察到的*状态*是它们更低层的基本事实与其预测之间的**差异**的映射。**较高层只看到较低层做出*错误预测*** 的像素。*

*如果我们希望神经网络的行为像额叶一样，它需要映射像素输入→像素*预测*输出，然后将该输出与随后时刻的地面真实像素进行比较，以查看*哪里的预测是错误的*。那将是最低级的“煎饼”。然后，被*误预测的像素*将成为下一个“煎饼”的**输入**，并且该“煎饼”试图预测下一个时刻的误差将在**的哪里。第二个“煎饼”将是一个新的神经网络。***

*我们将需要六个这样的神经网络，一个堆叠在另一个之上——让事情变得更复杂的是，更高层的“煎饼”也将*接收来自多个特征检测器神经元的信号。我们的第六个“煎饼”将接收第五个“煎饼”的**误差**作为输入，以及来自第一个、第二个、第三个和第四个煎饼的一些**特征检测器信号**！这与现有的人工神经网络不同，我认为这种差异很重要。**

***有什么帮助？***

*目前，研究人员期望单个神经网络每次都能正确处理问题。大脑不是这样工作的。在我们六个‘煎饼’中，第一个‘煎饼’神经网络出错*往往*。如果我们将一个人工神经网络训练成最低的“煎饼”，我们将需要尽早降低学习速度，并在过度拟合之前**停止。我们的网络仍然会得到许多错误的答案。***

*然后，我们需要第二个“煎饼”，第二个神经网络，接收下层的错误。那一层也会很早就停止。很可能*仍然会**错误地判断最低层何时会出错**。只有当许多这样的“煎饼”堆叠在一起时，错误率才会显著下降。**

**对于注重数字的人来说:当前的 NLP 网络在大约 4%的时间里是错误的。同时，假设一个有六个“煎饼”的“额叶”有一个最低的“煎饼”有 50%的时间是错误的。就其本身而言，这个“煎饼”比我们目前的网络要糟糕得多。然而，它的错误实例被传递给第二个“煎饼”。这个煎饼只寻找 50%的错误，我们可以假设它纠正了 50%的错误。到目前为止，这两个“煎饼”加在一起，有 75%的正确率。有了这些“煎饼”中的六个“煎饼”，每个“煎饼”只纠正一半剩余的错误，组合的准确度是 98.4375%！因此，即使每个错误检测器都“有故障”,错误检测器的堆栈也能很快胜过端到端网络。**

****通过种植煎饼预测误差****

**人类有更强的理性和反思能力，我们的盘子里也有更多的“煎饼”!海豚有四个，猿和猴子更少。我预计，如果一台机器有比我们更多的*【煎饼】，每个“煎饼”都试图预测它下面的“煎饼”的误差，那么这台机器将会比我们更有能力。这为机器智能开辟了一个新的方向，*在前进中学习*。***

**边学边做:机器将从一个单一的深度神经网络开始，并被赋予预测下一时刻的任务。当它的成功率超过某个阈值时，在顶部添加一个新的深度神经网络。这个新网络将接收下层网络的错误预测，并负责预测下一个错误将发生在哪里。当网络的成功率超过阈值时，添加一个新的深度神经网络。继续这个过程，以逐步改善网络的组合。**

**通过这种“煎饼”范式，网络通过在所有旧网络之上生长另一个深度神经网络“煎饼”*来响应**新信息**。学习永远不会停止。“煎饼”堆得越来越高。当与专家混合的各种神经网络相结合时，这个概念变得更加重要。***

****专家混合****

**在混合专家神经网络中，神经元像树莓一样“聚集”成密集连接的束。而且，就像树莓酱一样，有一些长距离的连接将所有的树莓“粘合”在一起。目前，专家模型混合就此打住。当一个输入进入果酱堆的底部时，它只激活几个“树莓”，每个“树莓”执行一点点特征检测。这些特征激活了果酱堆中较高的一些“覆盆子”，在那里检测到更高水平的特征。**

**沿着果酱堆向上移动，这些专家覆盆子能够发现不同输入的特征；对于输入的每个子集，不同的一组专家开始工作。专家混合网络的行为就像许多稀疏网络的联合，每个覆盆子都是其中一些网络的 T2 交集。**

**(在稀疏网络的拥挤维恩图中，每个网络都有特征检测器，这些特征检测器*与任何其他网络有一点点*重叠；因为有如此多的网络在一起，组合图允许大多数稀疏网络与其他人共享大多数特征检测器。例如，稀疏网络#1 可能利用特征检测器“树莓”A、B 和 C。同时，稀疏网络#2 使用 A、D 和 e。网络#3 需要检测特征 C、D 和 e 的“树莓”。因此，您可以将这三个稀疏网络组合成检测所有特征的专家混合物:A、B、C、D 和 e。每个稀疏网络仅在少数地方与其他稀疏网络重叠。但是，综合起来看，所有的专家都有重叠之处。)**

****所有浇头****

**回到构成我们额叶的“煎饼”。为了与我们自己的大脑相匹配，煎饼的比喻需要更加精细:我们的神经元显示出从一个盘子到另一个盘子的跨越堆叠的链接。这就像一个覆盖着盘子的早餐桌，每个盘子上堆放着六个“煎饼”……并且*覆盆子果酱涂抹在所有的盘子上*，沿着盘子的侧面滴落，并且每个盘子都接触到其他的盘子！我们的大脑很混乱。**

**如果我们想要一个像我们一样学习和成长的人工神经网络，它将需要多个错误预测深度神经网络的“煎饼”，其中每个深度神经网络都由专家的分层混合物组成。每个“煎饼”网络接收其下方“煎饼”的**错误**、*以及由相邻“煎饼”检测到的一些**特征**作为输入。这与当前的深度神经网络架构大相径庭。而且，值得一试。***