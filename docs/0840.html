<html>
<head>
<title>Functional programming for deep learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的函数式编程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/functional-programming-for-deep-learning-bc7b80e347e9?source=collection_archive---------0-----------------------#2017-06-29">https://towardsdatascience.com/functional-programming-for-deep-learning-bc7b80e347e9?source=collection_archive---------0-----------------------#2017-06-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e0decee385eaaea520665a5e0b993c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iXuDGtF8YeH0VPPbwb539w.png"/></div></div></figure><p id="5ecf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我开始在<a class="ae kw" href="http://thinktopic.com/" rel="noopener ugc nofollow" target="_blank"> ThinkTopic </a>的最新工作之前，“函数式编程”和“机器学习”的概念完全属于两个不同的世界。一个是随着世界转向简单性、可组合性和不变性以维护复杂的扩展应用程序，编程范式<a class="ae kw" href="http://blog.salsitasoft.com/why-now/" rel="noopener ugc nofollow" target="_blank">越来越受欢迎</a>；另一个是教电脑<a class="ae kw" href="https://www.theverge.com/2017/6/26/15877020/google-ai-experiment-sketch-rnn-doodles-quick-draw" rel="noopener ugc nofollow" target="_blank">自动完成涂鸦</a>和<a class="ae kw" href="http://www.wired.co.uk/article/how-ai-and-machine-learning-are-shaping-the-future-of-music" rel="noopener ugc nofollow" target="_blank">制作音乐</a>的工具。重叠在哪里？</p><p id="c6c4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我与这两位合作得越多，就越开始意识到这种重叠既是实际的，也是理论上的。首先，机器学习不是一项独立的努力；它需要迅速融入工业中复杂的缩放应用。其次，机器学习——尤其是深度学习——在设计上是有功能的。给定正确的生态系统，以完全功能化的方式执行深度学习有几个令人信服的理由:</p><ul class=""><li id="eaba" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka ir">深度学习模型是组合的。</strong>函数式编程就是组合高阶函数链来操作简单的数据结构。神经网络以同样的方式设计，将一层到下一层的函数变换链接在一起，对输入数据的简单矩阵进行操作。事实上，深度学习的整个过程可以被视为优化一组组合函数，这意味着模型本身具有内在功能。</li><li id="8a3f" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated"><strong class="ka ir">深度学习组件是不可变的。</strong>当函数对输入数据进行操作时，数据不会<em class="ll">改变</em>，一组新值被输出并传递。此外，当权重被更新时，它们不需要被“变异”——它们可以被新值替换。理论上，对权重的更新可以以任何顺序应用(即，它们不相互依赖)，因此不需要跟踪连续的可变状态。</li><li id="d094" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated"><strong class="ka ir">函数式编程提供了简单的并行性</strong>。最重要的是，纯函数和可组合函数很容易并行化。并行意味着更快的速度和更强的计算能力。函数式编程为我们提供了基本上免费的并发性和并行性，使得在深度学习中使用大型分布式模型变得更加容易。</li></ul><p id="c857" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">关于函数式编程和深度学习的结合，有许多理论和观点，从<a class="ae kw" href="http://colah.github.io/posts/2015-09-NN-Types-FP/" rel="noopener ugc nofollow" target="_blank">数学论证</a>到<a class="ae kw" href="http://www.kdnuggets.com/2015/04/functional-programming-big-data-machine-learning.html" rel="noopener ugc nofollow" target="_blank">实践概述</a>，但有时只有在实践中才能看到它，这才是最有说服力的(也是最有用的)。在ThinkTopic，我们一直在开发一个叫做<a class="ae kw" href="https://github.com/thinktopic/cortex" rel="noopener ugc nofollow" target="_blank"> Cortex </a>的开源机器学习库。在这篇文章的其余部分，我将介绍函数式编程背后的一些想法，并将它们用于异常检测的Cortex深度学习模型。</p><h1 id="6111" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">Clojure基础</h1><p id="1a62" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated">在继续我们的Cortex教程之前，我想介绍一些Clojure的基础知识。Clojure是一种函数式编程语言，它擅长两件事:<strong class="ka ir">并发</strong>和<strong class="ka ir">数据处理</strong>。对我们来说幸运的是，这两件事对机器学习都非常有用。事实上，我们使用Clojure进行机器学习的一个主要原因是，每天都要为训练准备数据集(数据操作、处理等)。)很容易超过实现算法的工作，特别是当我们有一个像Cortex这样的坚实的学习库时。使用Clojure和。edn(而不是C++和protobuf)，我们可以在ML项目上获得杠杆和速度。</p><p id="1f43" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要更深入地了解这门语言，请看这里的社区指南<a class="ae kw" href="http://clojure-doc.org/articles/tutorials/introduction.html" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="412d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从基础开始:Clojure代码由一堆在运行时计算的<em class="ll">表达式</em>组成。这些表达式包含在括号中，通常被视为函数调用。</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="68ca" class="my ln iq mu b gy mz na l nb nc">(+ 2 3)          ; =&gt; 5<br/>(if false 1 0)   ; =&gt; 0</span></pre><p id="6901" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有4种基本的集合数据结构:向量、列表、散列映射和集合。逗号被视为空白，所以通常会被省略。</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="6702" class="my ln iq mu b gy mz na l nb nc">[1 2 3]            ; vector (ordered)<br/>'(1 2 3)           ; list (ordered)<br/>{:a 1 :b 2 :c 3}   ; hashmap or map (unordered)<br/>#{1 2 3}           ; set (unordered, unique values)</span></pre><p id="7f85" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">列表前面的单引号只是防止它被当作表达式来计算。</p><p id="c2e0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Clojure还附带了很多内置函数来操作这些数据结构。Clojure的部分优点在于，它被设计为针对很少的数据类型提供很多函数，而不是针对很多数据类型中的每一种都提供一些专门的函数。作为一种FP语言，Clojure支持高阶函数，这意味着函数可以作为参数传递给其他函数。</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="785a" class="my ln iq mu b gy mz na l nb nc">(count [a b c])              ; =&gt; 3</span><span id="42e9" class="my ln iq mu b gy nd na l nb nc">(range 5)                    ; =&gt; (0 1 2 3 4)</span><span id="e62c" class="my ln iq mu b gy nd na l nb nc">(take 2 (drop 5 (range 10))) ; =&gt; (5 6)</span><span id="43a3" class="my ln iq mu b gy nd na l nb nc">(:b {:a 1 :b 2 :c 3})        ; use keyword as function =&gt; 2</span><span id="98e7" class="my ln iq mu b gy nd na l nb nc">(map inc [1 2 3])            ; map and increment =&gt; (2 3 4)</span><span id="2ec1" class="my ln iq mu b gy nd na l nb nc">(filter even? (range 5))     ; filter collection based off predicate =&gt; (0 2 4)</span><span id="e011" class="my ln iq mu b gy nd na l nb nc">(reduce + [1 2 3 4])         ; apply + to first two elements, then apply + to that result and the 3rd element, and so forth =&gt; 10</span></pre><p id="58b1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当然，我们也可以用Clojure写自己的函数，用<code class="fe ne nf ng mu b">defn</code>。Clojure函数定义遵循形式<code class="fe ne nf ng mu b">(defn fn-name [params*] expressions)</code>，它们总是返回主体中最后一个表达式的值。</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="a889" class="my ln iq mu b gy mz na l nb nc">(defn add2<br/>  [x]<br/>  (+ x 2))</span><span id="922d" class="my ln iq mu b gy nd na l nb nc">(add2 5)     ; =&gt; 7</span></pre><p id="3d76" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe ne nf ng mu b">let</code>表达式在“let”的词法范围内创建和绑定变量。也就是说，在表达式<code class="fe ne nf ng mu b">(let [a 4] (...))</code>中，变量“a”在内括号内(且仅在内括号内)取值为4。这些变量被称为“局部变量”</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="6317" class="my ln iq mu b gy mz na l nb nc">(defn square-and-add<br/>  [a b]<br/>  (let [a-squared (* a a)<br/>        b-squared (* b b)]<br/>    (+ a-squared b-squared)))</span><span id="17f0" class="my ln iq mu b gy nd na l nb nc">(square-and-add 3 4)       ; =&gt; 25</span></pre><p id="1d0c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，有几种方法可以创建匿名函数，既可以赋给局部函数，也可以传递给高阶函数。</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="a689" class="my ln iq mu b gy mz na l nb nc">(fn [x] (* 5 x))          ; anonymous function</span><span id="360f" class="my ln iq mu b gy nd na l nb nc">#(* 5 %)                  ; equivalent anonymous function, where the % represents the function's argument</span><span id="9e26" class="my ln iq mu b gy nd na l nb nc">(map #(* 5 %) [1 2 3])    ; =&gt; (5 10 15)</span></pre><p id="1373" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">基本就是这样！现在我们已经学习了一些Clojure，让我们把乐趣放在函数式编程上，回到一些ML上。</p><h1 id="234e" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">皮质</h1><p id="5ba0" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated"><a class="ae kw" href="https://github.com/thinktopic/cortex" rel="noopener ugc nofollow" target="_blank"> Cortex </a>是用Clojure编写的，是目前规模最大、发展最快的使用函数式编程语言的机器学习库之一。本文的其余部分将介绍如何在Cortex中构建最先进的分类模型，以及这样做所需的函数式编程范例和数据扩充技术。</p><h2 id="4e6f" class="my ln iq bd lo nh ni dn ls nj nk dp lw kj nl nm ma kn nn no me kr np nq mi nr bi translated">数据预处理</h2><p id="a51c" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated">我们的数据集将是由Kaggle <a class="ae kw" href="https://www.kaggle.com/dalpozz/creditcardfraud" rel="noopener ugc nofollow" target="_blank">在这里</a>提供的信用卡欺诈检测数据。事实证明，这个数据集非常不平衡，在284，807个案例中，只包含492个正面欺诈案例。也就是0.172%。这稍后会给我们带来问题，但首先让我们看看数据，看看这个模型做得如何。</p><p id="72e7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了确保个人数据的匿名性，除了“时间”和“数量”之外的所有原始特征都已经被转换成PCA成分(其中每个条目代表一个新变量，该变量包含来自原始数据的最相关的信息)。稍微研究一下数据就会发现，第一个“时间”变量相当不具信息性，所以我们在读取数据时会忽略它。下面是我们的初始代码:</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="e825" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">皮层神经网络期望以图谱的形式输入数据，这样每个图谱代表一个单独的标记数据点。例如，一个分类数据集可能看起来像<code class="fe ne nf ng mu b">[{:data [12 10 38] :label “cat”} {:data [20 39 3] :label “dog“} ... ]</code>。在我们的create-dataset函数中，我们读入csv数据文件，将除最后一列之外的所有列指定为“数据”(或特征)，并将最后一列指定为标签。在该过程中，我们基于分类类别将标签转变为独热向量(例如<code class="fe ne nf ng mu b">[0 1 0 0]</code>)，因为我们的神经网络的最后一个softmax层返回类别概率的向量，而不是实际的标签。最后，我们从这两个变量创建一个映射，并将其作为数据集返回。</p><h2 id="b094" class="my ln iq bd lo nh ni dn ls nj nk dp lw kj nl nm ma kn nn no me kr np nq mi nr bi translated">模型描述</h2><p id="bdab" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated">在Cortex中创建模型相当简单。首先，我们将定义一个超参数图，供以后在训练中使用。然后，为了定义一个模型，我们简单地将这些层串在一起:</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="09ad" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe ne nf ng mu b">network-description</code>是神经网络的矢量层。我们的模型包括:</p><ul class=""><li id="f3e0" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated">输入层</li><li id="81d1" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">具有ReLU激活功能的全连接(线性)层</li><li id="b1aa" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">辍学者</li><li id="3782" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">另一个全连接的ReLU层</li><li id="a872" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">通过softmax函数传递的大小为2的输出层。</li></ul><p id="ebcf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在第一层和最后一层，我们都需要指定一个<code class="fe ne nf ng mu b">:id</code>。这个id指的是我们的网络应该查看的数据图中的键。(回想一下资料图长得像<code class="fe ne nf ng mu b">{:data [...] :label [...]}</code>)。对于我们的输入层，我们传入<code class="fe ne nf ng mu b">:data</code> id来告诉模型为它的前向传递获取训练数据。在我们最终的网络层中，我们提供了<code class="fe ne nf ng mu b">:label</code>作为<code class="fe ne nf ng mu b">:id</code>，因此模型可以使用真实标签来计算我们的误差。</p><h2 id="ba14" class="my ln iq bd lo nh ni dn ls nj nk dp lw kj nl nm ma kn nn no me kr np nq mi nr bi translated">培训和评估</h2><p id="1122" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated">这就是变得有点困难的地方。训练函数本身实际上并不复杂——Cortex为训练提供了一个很好的高级调用，所以我们所要做的就是传入我们的参数(网络、训练和测试数据集等)。).唯一的警告是，系统期望一个有效的“无限”数据集用于训练，但Cortex提供了一个函数(<code class="fe ne nf ng mu b">infinite-class-balanced-dataset</code>)来帮助我们转换它。</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="c12f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">复杂的部分是<code class="fe ne nf ng mu b">f1-test-fn</code>。事情是这样的:在训练期间，<code class="fe ne nf ng mu b">train-n</code>函数期望被提供一个<code class="fe ne nf ng mu b">:test-fn</code>,该函数评估模型的执行情况，并确定它是否应该被保存为“最佳网络”有一个默认的测试函数评估交叉熵损失，但这个损失值不是那么容易解释，它不太适合我们的不平衡数据集。为了解决这个问题，我们将编写自己的测试函数。</p><p id="43ca" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是我们如何测试这个模型的性能呢？分类任务中的标准度量是准确性，但是在像我们这样不平衡的数据集中，准确性是一个相当无用的度量。因为正面(欺诈性)例子仅占我们数据集的0.172%，即使是专门预测负面例子的模型也能达到99.828%的准确率。99.828%是一个相当不错的准确率，但如果亚马逊真的使用了这种模式，我们可能会面临犯罪和信用卡欺诈。</p><p id="531f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">谢天谢地，亚马逊没有使用这种模式，我们也不会。一组更能说明问题的指标是<a class="ae kw" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">精度、回忆</a>和<a class="ae kw" href="https://en.wikipedia.org/wiki/F1_score" rel="noopener ugc nofollow" target="_blank"> F1(或更一般的F-beta)分数</a>。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/f2af01fb560f5c37de08b54d36469d07.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*drkXZxmGjpuFvt9Rkuci0w.png"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Precision and recall visualized. Source: <a class="ae kw" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Precision_and_recall</a></figcaption></figure><p id="bf9b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通俗地说，precision会问这样一个问题:“在我猜测的所有正面例子中，<em class="ll">实际上</em>是正面的占多大比例？”回忆提出了这样一个问题:“在所有实际上是积极的例子中，我正确地猜测为积极的占多大比例？”</p><p id="d74e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">F-beta分数(传统F1分数的推广)是精确度和召回率的加权平均值，也是以0到1的范围来衡量的:</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/8c3088080f55ed7b5f94cc7827aec7e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*7kMk73sYamfBDPRwQwZIIw.png"/></div></figure><p id="ae35" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当beta = 1时，我们得到<code class="fe ne nf ng mu b">2 * (precision * recall) / (precision + recall)</code>的标准F1度量。一般来说，beta代表<em class="ll">召回应该比precision </em>重要多少倍。对于我们的欺诈检测模型，我们将使用F1分数作为我们的最高分数来跟踪，但我们也将记录精确度和召回分数来检查平衡。这是我们的<code class="fe ne nf ng mu b">f1-test-fn</code>:</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="b1d0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该函数在测试集上运行当前网络，计算F1分数，并相应地更新/保存网络。它还打印出我们在每个时期的评估指标。如果我们现在在REPL中运行<code class="fe ne nf ng mu b">(train)</code>，我们会得到一个高分，它看起来像这样:</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="d52a" class="my ln iq mu b gy mz na l nb nc">Epoch: 30<br/>Precision: 0.2515923566878981<br/>Recall: 0.9186046511627907<br/>F1: 0.395</span></pre><p id="0ab2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">哈哈。这太糟糕了。</p><h2 id="22c7" class="my ln iq bd lo nh ni dn ls nj nk dp lw kj nl nm ma kn nn no me kr np nq mi nr bi translated">数据扩充</h2><p id="80ab" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated">问题来了。还记得我说过我们高度不平衡的数据集会给我们带来问题吗？该模型目前没有足够多的正面例子可供借鉴。当我们在train函数中调用<code class="fe ne nf ng mu b">experiment-util/infinite-class-balanced-dataset</code>时，我们实际上创建了每个正面训练实例的数百个副本来平衡数据集。结果，该模型有效地记忆了那些特征值，而不是实际学习类别之间的区别。</p><p id="64c2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">解决这个问题的一种方法是通过<em class="ll">数据扩充</em>，在这个过程中，我们根据已经有的例子生成额外的人工数据。为了创建真实的正面训练示例，我们将向每个现有正面示例的特征向量添加随机数量的噪声。我们添加的噪声量将取决于正类中每个特征的<em class="ll">方差</em>，因此方差大的特征将被大量噪声增强，反之亦然。</p><p id="a99d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以下是我们的数据扩充代码:</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="da4c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe ne nf ng mu b">augment-train-ds </code>获取我们的原始训练数据集，计算必须达到50/50类别平衡的扩充数量，并通过基于允许的方差(<code class="fe ne nf ng mu b">get-scaled-variances</code>)添加随机噪声向量(<code class="fe ne nf ng mu b">add-rand-variance</code>)将这些扩充应用于我们现有的样本。最后，我们将扩充的示例连接回原始数据集，并返回平衡的数据集。</p><p id="7434" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在训练期间，模型将会看到大量不切实际的正面例子，而测试集仍然只有0.172%的正面例子。因此，虽然模型可能能够更好地了解两个类之间的差异，但它会在测试过程中过度预测正面的例子。为了解决这个问题，我们可以要求一个更高的确定性阈值来预测测试期间的“肯定”。换句话说，我们可以要求模型至少有70%的把握，而不是要求模型至少有50%的把握来对一个例子进行分类。经过一些测试，我发现最佳值设置为90%。这个代码可以在源代码的<code class="fe ne nf ng mu b">vec-&gt;label</code>函数中找到，并在<code class="fe ne nf ng mu b">f1-test-fn</code>的第31行被调用。</p><p id="1750" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用新的、增强的数据集进行训练，我们的高分看起来像这样:</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="d2bc" class="my ln iq mu b gy mz na l nb nc">Epoch: 25<br/>Precision: 0.8658536585365854<br/>Recall: 0.8255813953488372<br/>F1: 0.8452380952380953</span></pre><p id="2ee0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好多了！</p><h1 id="9435" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">结论</h1><p id="57c4" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated">一如既往，该模型仍可改进。以下是后续步骤的一些想法:</p><ul class=""><li id="761c" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated">所有的PCA特征都是信息性的吗？查看正例与反例的值在特征中的分布，并删除任何无助于区分这两个类别的特征。</li><li id="0c40" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">有没有其他的神经网络架构，激活函数等。表现更好？</li><li id="5abf" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">有没有不同的数据扩充技术可以表现得更好？</li><li id="1b25" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">与Keras/Tensorflow/Theano/Caffe相比，Cortex中的模型性能如何？</li></ul><p id="4958" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该项目的源代码可以在<a class="ae kw" href="https://github.com/joycex99/fraud-detection" rel="noopener ugc nofollow" target="_blank">这里</a>找到。我鼓励你尝试这些后续步骤，测试新的数据集，并探索不同的网络架构(我们在conv网上有一个很棒的<a class="ae kw" href="https://github.com/thinktopic/cortex/tree/master/examples/mnist-classification" rel="noopener ugc nofollow" target="_blank">图像分类示例</a>供参考)。Cortex正在<a class="ae kw" href="http://thinktopic.com/blog/toward-cortex-1-dot-0" rel="noopener ugc nofollow" target="_blank">向它的1.0版本</a>推进，所以如果你有任何想法、建议或反馈，一定要让我们知道。黑客快乐！</p></div></div>    
</body>
</html>