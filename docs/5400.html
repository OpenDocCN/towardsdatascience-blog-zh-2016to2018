<html>
<head>
<title>Building a Logistic Regression in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 构建逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-logistic-regression-in-python-301d27367c24?source=collection_archive---------1-----------------------#2018-10-16">https://towardsdatascience.com/building-a-logistic-regression-in-python-301d27367c24?source=collection_archive---------1-----------------------#2018-10-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1e0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设你得到了不同申请人的两次考试的分数，目标是根据分数将申请人分为两类，即如果申请人可以被大学录取，则分为 1 类；如果申请人不能被大学录取，则分为 0 类。这个问题可以用线性回归解决吗？让我们检查一下。</p><p id="a227" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">注意:</strong>我建议你在继续写这篇博客之前，先阅读一下<a class="ae kl" rel="noopener" target="_blank" href="/linear-regression-using-python-b136c91bf0a2">线性回归</a>。</p><h2 id="5acc" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">目录</h2><ul class=""><li id="6a57" class="lf lg iq jp b jq lh ju li jy lj kc lk kg ll kk lm ln lo lp bi translated">什么是逻辑回归？</li><li id="b258" class="lf lg iq jp b jq lq ju lr jy ls kc lt kg lu kk lm ln lo lp bi translated">数据集可视化</li><li id="0ba5" class="lf lg iq jp b jq lq ju lr jy ls kc lt kg lu kk lm ln lo lp bi translated">假设和成本函数</li><li id="8dfc" class="lf lg iq jp b jq lq ju lr jy ls kc lt kg lu kk lm ln lo lp bi translated">从头开始训练模型</li><li id="ddbe" class="lf lg iq jp b jq lq ju lr jy ls kc lt kg lu kk lm ln lo lp bi translated">模型评估</li><li id="dcf0" class="lf lg iq jp b jq lq ju lr jy ls kc lt kg lu kk lm ln lo lp bi translated">Scikit-learn 实现</li></ul><h2 id="8b0e" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated"><strong class="ak">什么是逻辑回归？</strong></h2><p id="2d4e" class="pw-post-body-paragraph jn jo iq jp b jq lh js jt ju li jw jx jy lv ka kb kc lw ke kf kg lx ki kj kk ij bi translated">如果你记得线性回归，它是用来确定一个连续因变量的值。逻辑回归通常用于分类目的。与线性回归不同，因变量只能取有限数量的值，即因变量是<strong class="jp ir">分类的</strong>。当可能的结果只有两个时，称为二元逻辑回归<strong class="jp ir">。</strong></p><p id="045f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们<strong class="jp ir"> </strong>看看逻辑回归是如何用于分类任务的。</p><p id="8d8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在线性回归中，输出是输入的加权和。逻辑回归是一种广义线性回归，因为我们不直接输出输入的加权和，而是通过一个可以映射 0 到 1 之间任何实数值的函数来传递。</p><blockquote class="ly lz ma"><p id="1d39" class="jn jo mb jp b jq jr js jt ju jv jw jx mc jz ka kb md kd ke kf me kh ki kj kk ij bi translated"><strong class="jp ir">如果我们像线性回归一样将输入的加权和作为输出，则该值可能大于 1，但我们希望该值介于 0 和 1 之间。这就是为什么线性回归不能用于分类任务</strong>的原因。</p></blockquote><p id="12e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从下图中我们可以看到，线性回归的输出通过一个<strong class="jp ir">激活</strong>函数传递，该函数可以映射 0 和 1 之间的任何实数值。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/e46e5305346fd6804e9ce6e2e424727c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*8q9ztX9dGVCv7e0DmH_IVA.png"/></div></figure><p id="4ab1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所使用的激活功能被称为<strong class="jp ir">s 形</strong>功能。sigmoid 函数的图形如下所示</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/c9b371b0ff36b7d21cb2fcacb0f70d62.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*yKvimZ3MCAX-rwMX2n87nw.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">sigmoid function</figcaption></figure><p id="4624" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，sigmoid 函数的值总是位于 0 和 1 之间。X=0 时，该值正好为 0.5。我们可以使用 0.5 作为概率阈值来确定类别。如果概率大于 0.5，我们将其归类为<strong class="jp ir"> Class-1(Y=1) </strong>或者<strong class="jp ir"> Class-0(Y=0) </strong>。</p><p id="12a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们建立模型之前，让我们看看逻辑回归的假设</p><ul class=""><li id="e6f4" class="lf lg iq jp b jq jr ju jv jy ms kc mt kg mu kk lm ln lo lp bi translated">因变量必须是分类变量</li><li id="65d0" class="lf lg iq jp b jq lq ju lr jy ls kc lt kg lu kk lm ln lo lp bi translated">独立变量(特征)必须是独立的(以避免多重共线性)。</li></ul><h2 id="cbe2" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">资料组</h2><p id="0293" class="pw-post-body-paragraph jn jo iq jp b jq lh js jt ju li jw jx jy lv ka kb kc lw ke kf kg lx ki kj kk ij bi translated">这篇博客中使用的数据取自吴恩达在 Coursera 上的<a class="ae kl" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>课程。数据可以从<a class="ae kl" href="https://github.com/animesh-agarwal/Machine-Learning/blob/master/LogisticRegression/data/marks.txt" rel="noopener ugc nofollow" target="_blank">这里</a>下载。数据由 100 名申请者的两次考试成绩组成。目标值采用二进制值 1，0。1 表示申请人被大学录取，而 0 表示申请人没有被录取。目标是建立一个分类器，可以预测申请是否会被大学录取。</p><p id="d7bc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们使用<code class="fe mv mw mx my b">read_csv</code>函数将数据加载到<code class="fe mv mw mx my b">pandas</code>数据帧中。我们还将把数据分成<code class="fe mv mw mx my b">admitted</code>和<code class="fe mv mw mx my b">non-admitted</code>来可视化数据。</p><figure class="mg mh mi mj gt mk"><div class="bz fp l di"><div class="mz na l"/></div></figure><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/04a29387ed0f35cbc44e182b38beb254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*FgLFfUXXLeEUucwSdYMM8Q.png"/></div></figure><p id="3622" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">既然我们对问题和数据有了清晰的理解，让我们继续构建我们的模型。</p><h2 id="9d65" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">假设和成本函数</h2><p id="0176" class="pw-post-body-paragraph jn jo iq jp b jq lh js jt ju li jw jx jy lv ka kb kc lw ke kf kg lx ki kj kk ij bi translated">到目前为止，我们已经了解了如何使用逻辑回归将实例分类到不同的类别中。在这一节中，我们将定义假设和成本函数。</p><p id="cb2b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">线性回归模型可以用下面的等式来表示。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/f17b6b3aa73b913249e7ed7cda4df437.png" data-original-src="https://miro.medium.com/v2/resize:fit:226/format:webp/1*HsoXveMFMW46v9oKV3QikQ.png"/></div></figure><p id="300b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我们将 sigmoid 函数应用于线性回归的输出</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/17c244f2cb50e7de76c875be99f3568b.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*AlIJXuiC19cucDZ_1kE1pg.png"/></div></figure><p id="c644" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中 sigmoid 函数由下式表示:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/db8d66b9c75f006db904edc50ea09f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*IO5RjkmyCq6t1VmZkW8Sxg.png"/></div></figure><p id="63f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">逻辑回归的假设就变成了，</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/38389b4367394bf7cc3adf06bbe6a04d.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*L9a6phB1ZzjRhb-VI3W1YQ.png"/></div></figure><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/845d6abb5f9eaaf15b2626b2f112f256.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*jStEeKa6l6KgQxbS8iGzrw.png"/></div></figure><p id="7933" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果输入的加权和大于零，则预测类为 1，反之亦然。因此，通过将输入的加权和设置为 0，可以找到分隔两个类的决策边界。</p><h2 id="62e8" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated"><strong class="ak">成本函数</strong></h2><p id="9e2b" class="pw-post-body-paragraph jn jo iq jp b jq lh js jt ju li jw jx jy lv ka kb kc lw ke kf kg lx ki kj kk ij bi translated">像线性回归一样，我们将为我们的模型定义一个成本函数，目标是最小化成本。</p><p id="303f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">单个训练示例的成本函数可以由下式给出:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d5effceb4a7e7a16d53c1bca31b28315.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*qKfAYUsI0VPcIXVBbEdPEg.png"/></div></figure><p id="a171" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">成本函数直觉</strong></p><p id="216d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果实际类是 1，而模型预测是 0，我们应该高度惩罚它，反之亦然。从下图中可以看出，对于图<code class="fe mv mw mx my b">-log(h(x))</code>来说，当 h(x)接近 1 时，成本为 0，当 h(x)接近 0 时，成本为无穷大(也就是说，我们对模型的惩罚很重)。类似地，对于图<code class="fe mv mw mx my b">-log(1-h(x))</code>，当实际值为 0 并且模型预测为 0 时，成本为 0，并且随着 h(x)接近 1，成本变得无穷大。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/b999318bfd4bb610d9567603ba17197c.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*yWzKLQhWITQ4bR2aMSVVuw.png"/></div></figure><p id="10e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以将这两个方程结合起来使用:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/2526b047449517cc53b4c0b875585007.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*-qOesmejYBqeifLZHXrvtw.png"/></div></figure><p id="be22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由<em class="mb"> J(θ) </em>表示的所有训练样本的成本可以通过对所有训练样本的成本取平均值来计算</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/0ed0ed76069bf7da3d3fffa10a7b1f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*Capcm4gWSZFZY6Str-wSRQ.png"/></div></figure><p id="21bf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<strong class="jp ir"> <em class="mb"> m </em> </strong>为训练样本数。</p><p id="e241" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用梯度下降来最小化成本函数。任何参数的梯度可由下式给出</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/671ba2a913e07b55d5ab29fceaef0ed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*3-s2EtTHU0qJs7pA_iGT_w.png"/></div></figure><p id="82fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该方程类似于我们在线性回归中得到的结果，只有 h(x)在两种情况下是不同的。</p><h2 id="d035" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">训练模型</h2><p id="9f23" class="pw-post-body-paragraph jn jo iq jp b jq lh js jt ju li jw jx jy lv ka kb kc lw ke kf kg lx ki kj kk ij bi translated">现在我们已经准备好了构建模型所需的一切。让我们用代码实现它。</p><p id="fe56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们首先为我们的模型准备数据。</p><pre class="mg mh mi mj gt nm my nn no aw np bi"><span id="c4e6" class="km kn iq my b gy nq nr l ns nt">X = np.c_[np.ones((X.shape[0], 1)), X]<br/>y = y[:, np.newaxis]<br/>theta = np.zeros((X.shape[1], 1))</span></pre><p id="4c5c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将定义一些用于计算成本的函数。</p><pre class="mg mh mi mj gt nm my nn no aw np bi"><span id="1b04" class="km kn iq my b gy nq nr l ns nt">def sigmoid(x):<br/>    # Activation function used to map any real value between 0 and 1<br/>    return 1 / (1 + np.exp(-x))<br/><br/>def net_input(theta, x):<br/>    # Computes the weighted sum of inputs<br/>    return np.dot(x, theta)<br/><br/>def probability(theta, x):<br/>    # Returns the probability after passing through sigmoid<br/>    return sigmoid(net_input(theta, x))</span></pre><p id="83bc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们定义<code class="fe mv mw mx my b">cost</code>和<code class="fe mv mw mx my b">gradient</code>函数。</p><pre class="mg mh mi mj gt nm my nn no aw np bi"><span id="1619" class="km kn iq my b gy nq nr l ns nt">def cost_function(self, theta, x, y):<br/>    # Computes the cost function for all the training samples<br/>    m = x.shape[0]<br/>    total_cost = -(1 / m) * np.sum(<br/>        y * np.log(probability(theta, x)) + (1 - y) * np.log(<br/>            1 - probability(theta, x)))<br/>    return total_cost<br/><br/>def gradient(self, theta, x, y):<br/>    # Computes the gradient of the cost function at the point theta<br/>    m = x.shape[0]<br/>    return (1 / m) * np.dot(x.T, sigmoid(net_input(theta,   x)) - y)</span></pre><p id="d07b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们也定义一下<code class="fe mv mw mx my b">fit</code>函数，它将被用来寻找最小化成本函数的模型参数。在这篇博客中，我们编写了梯度下降法来计算模型参数。这里，我们将使用<code class="fe mv mw mx my b">scipy</code>库中的<code class="fe mv mw mx my b">fmin_tnc</code>函数。它可以用来计算任何函数的最小值。它将参数视为</p><ul class=""><li id="9c23" class="lf lg iq jp b jq jr ju jv jy ms kc mt kg mu kk lm ln lo lp bi translated">func:最小化的函数</li><li id="a168" class="lf lg iq jp b jq lq ju lr jy ls kc lt kg lu kk lm ln lo lp bi translated">x0:我们要寻找的参数的初始值</li><li id="640e" class="lf lg iq jp b jq lq ju lr jy ls kc lt kg lu kk lm ln lo lp bi translated">fprime:由“func”定义的函数的梯度</li><li id="47e2" class="lf lg iq jp b jq lq ju lr jy ls kc lt kg lu kk lm ln lo lp bi translated">args:需要传递给函数的参数。</li></ul><pre class="mg mh mi mj gt nm my nn no aw np bi"><span id="d7e3" class="km kn iq my b gy nq nr l ns nt">def fit(self, x, y, theta):<em class="mb"><br/>    </em>opt_weights = fmin_tnc(func=cost_function, x0=theta,<br/>                  fprime=gradient,args=(x, y.flatten()))<br/>    return opt_weights[0]</span><span id="4a33" class="km kn iq my b gy nu nr l ns nt">parameters = fit(X, y, theta)</span></pre><p id="d143" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型参数为<code class="fe mv mw mx my b">[-25.16131856 0.20623159 0.20147149]</code></p><p id="f9d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了了解我们的模型表现有多好，我们将绘制决策边界。</p><h2 id="d37d" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated"><strong class="ak">绘制决策边界</strong></h2><p id="4b45" class="pw-post-body-paragraph jn jo iq jp b jq lh js jt ju li jw jx jy lv ka kb kc lw ke kf kg lx ki kj kk ij bi translated">因为我们的数据集中有两个要素，所以线性方程可以表示为:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/9201c8238088b451c4eef3eb29e4d5d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*p4JFHHlPo5PVZYc3uUhpAg.png"/></div></figure><p id="7daa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如前所述，可以通过将输入的加权和设置为 0 来找到决策边界。使 h(x)等于 0 得到我们，</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/3fc80fc0c776a76024f1a3fed96e12d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*oTv8jjq4-ojOEzilU3oSfg.png"/></div></figure><p id="23ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将在用于可视化数据集的图的顶部绘制决策边界。</p><pre class="mg mh mi mj gt nm my nn no aw np bi"><span id="a70c" class="km kn iq my b gy nq nr l ns nt">x_values = [np.min(X[:, 1] - 5), np.max(X[:, 2] + 5)]<br/>y_values = - (parameters[0] + np.dot(parameters[1], x_values)) / parameters[2]<br/><br/>plt.plot(x_values, y_values, label='Decision Boundary')<br/>plt.xlabel('Marks in 1st Exam')<br/>plt.ylabel('Marks in 2nd Exam')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/7077d66d2745ddad04864fb07e3cb47e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*bcWJyX3iAIr0MGNIxUnQxg.png"/></div></figure><p id="438e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看起来我们的模型在预测课程方面做得不错。但是有多准确呢？让我们找出答案。</p><h2 id="6913" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">模型的准确性</h2><pre class="mg mh mi mj gt nm my nn no aw np bi"><span id="c6d9" class="km kn iq my b gy nq nr l ns nt">def predict(self, x):<em class="mb"><br/>    </em>theta = parameters[:, np.newaxis]<br/>    return probability(theta, x)</span><span id="6fa0" class="km kn iq my b gy nu nr l ns nt">def accuracy(self, x, actual_classes, probab_threshold=0.5):<br/>    predicted_classes = (predict(x) &gt;= <br/>                         probab_threshold).astype(int)<br/>    predicted_classes = predicted_classes.flatten()<br/>    accuracy = np.mean(predicted_classes == actual_classes)<br/>    return accuracy * 100</span><span id="5edb" class="km kn iq my b gy nu nr l ns nt">accuracy(X, y.flatten())</span></pre><p id="9000" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型的准确率为 89%。</p><p id="956d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们使用 scikit-learn 实现我们的分类器，并将其与我们从头构建的模型进行比较。</p><h2 id="2cec" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">scikit-learn 实现</h2><pre class="mg mh mi mj gt nm my nn no aw np bi"><span id="9591" class="km kn iq my b gy nq nr l ns nt">from sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import accuracy_score </span><span id="0cb7" class="km kn iq my b gy nu nr l ns nt">model = LogisticRegression()<br/>model.fit(X, y)<br/>predicted_classes = model.predict(X)<br/>accuracy = accuracy_score(y.flatten(),predicted_classes)<br/>parameters = model.coef_</span></pre><p id="17dc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型参数为<code class="fe mv mw mx my b">[[-2.85831439, 0.05214733, 0.04531467]]</code>，准确率为 91%。</p><p id="36ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="mb">为什么模型参数与我们从零开始实现的模型有显著差异？</em> </strong>如果你看一下 sk-learn 的逻辑回归实现的<a class="ae kl" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">文档</a>，它考虑到了正则化。基本上，正则化用于防止模型过度拟合数据。我不会在这篇博客中深入讨论正规化的细节。但是现在，就这样了。感谢阅读！！</p><p id="22d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇博客中使用的完整代码可以在这个 GitHub <a class="ae kl" href="https://github.com/animesh-agarwal/Machine-Learning/tree/master/LogisticRegression" rel="noopener ugc nofollow" target="_blank"> repo </a>中找到。</p><p id="40ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你被困在任何地方或有任何反馈，请给我留言。</p><h2 id="dc76" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated"><strong class="ak">下一步是什么？</strong></h2><p id="1bf9" class="pw-post-body-paragraph jn jo iq jp b jq lh js jt ju li jw jx jy lv ka kb kc lw ke kf kg lx ki kj kk ij bi translated">在下一篇博客中，我们将使用在这篇博客中学习到的概念，在来自 UCI 机器学习知识库的<a class="ae kl" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank">成人数据</a>上构建一个分类器。该数据集包含近 49K 个样本，包括分类值、数值值和缺失值。这将是一个值得探索的有趣数据集。</p><p id="53f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请继续关注这个空间。</p></div></div>    
</body>
</html>