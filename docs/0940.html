<html>
<head>
<title>Getting started with OpenAI gym</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenAI 健身房入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-openai-gym-932c7311d26c?source=collection_archive---------7-----------------------#2017-07-11">https://towardsdatascience.com/getting-started-with-openai-gym-932c7311d26c?source=collection_archive---------7-----------------------#2017-07-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/1097940f0b2b0eff7b49821c67ee9eed.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*nQFn14K79VUslu_V.png"/></div></figure><p id="4d0c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">OpenAI 健身房环境是了解更多机器学习的最有趣的方式之一。特别是强化学习和神经网络可以完美地应用到基准和雅达利游戏集合中。每个环境都有多种特色解决方案，并且您经常可以找到关于如何获得相同分数的文章。通过观察别人的方法和想法，你可以以一种有趣的方式快速提高自己。我注意到开始健身有点困难。虽然网上有很多算法教程，但第一步是理解你正在工作的编程环境。为了让新人更容易适应这个环境，我决定用 docker 容器和 jupyter 笔记本制作一个小教程。</p><figure class="kt ku kv kw gt jr gh gi paragraph-image"><div class="gh gi ks"><img src="../Images/e84c46b355a20207073fb0474b2b7ae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*VvnS8OdVdU1LF32e.png"/></div></figure><h2 id="78e7" class="kx ky iq bd kz la lb dn lc ld le dp lf kf lg lh li kj lj lk ll kn lm ln lo lp bi translated">你需要什么</h2><p id="0619" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">开始之前，<a class="ae lv" href="https://docs.docker.com/engine/installation/#supported-platforms" rel="noopener ugc nofollow" target="_blank">安装 Docker </a>。Docker 是一个让你在电脑上运行虚拟机的工具。我创建了一个“图像”,其中包含了几个你想要的东西:tensorflow、健身房环境、numpy、opencv 和一些其他有用的工具。</p><p id="a84d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">安装 docker 后，运行以下命令下载我准备好的 Docker 映像:</p><pre class="kt ku kv kw gt lw lx ly lz aw ma bi"><span id="a6c9" class="kx ky iq lx b gy mb mc l md me">docker run -p 8888:8888 rmeertens/tensorflowgym</span></pre><p id="39cf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在浏览器中，导航到:localhost:8888，并打开 TRADR 文件夹中的 OpenAI Universe 笔记本。</p><h2 id="fe40" class="kx ky iq bd kz la lb dn lc ld le dp lf kf lg lh li kj lj lk ll kn lm ln lo lp bi translated">自己玩个游戏</h2><p id="db96" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">让我们从自己玩翻筋斗游戏开始。你控制一个上面有杆子的酒吧。“游戏”的目标是尽可能长时间地保持横杠直立。在这个游戏中，你可以做两个动作:向左用力，或者向右用力。要手动玩这个游戏，请执行代码的第一部分。</p><p id="d564" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">通过左右点击你施加一个力，你会看到新的状态。请注意，我将游戏编程为当您“丢失”游戏时自动重置。</p><pre class="kt ku kv kw gt lw lx ly lz aw ma bi"><span id="f692" class="kx ky iq lx b gy mb mc l md me">%matplotlib notebook<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/>from ipywidgets import widgets<br/>from IPython.display import display<br/><br/>import gym<br/><br/>from matplotlib import animation<br/>from JSAnimation.IPython_display import display_animation<br/><br/><br/><br/>def leftclicked(something):<br/>    """ Apply a force to the left of the cart"""<br/>    onclick(0)<br/><br/>def rightclicked(something):<br/>    """ Apply a force to the right of the cart"""<br/>    onclick(1)<br/>    <br/>def display_buttons():<br/>    """ Display the buttons you can use to apply a force to the cart """<br/>    left = widgets.Button(description="&lt;")<br/>    right = widgets.Button(description="&gt;")<br/>    display(left, right)<br/>    <br/>    left.on_click(leftclicked)<br/>    right.on_click(rightclicked)<br/><br/># Create the environment and display the initial state<br/>env = gym.make('CartPole-v0')<br/>observation = env.reset()<br/>firstframe = env.render(mode = 'rgb_array')<br/>fig,ax = plt.subplots()<br/>im = ax.imshow(firstframe) <br/><br/># Show the buttons to control the cart<br/>display_buttons()<br/><br/><br/># Function that defines what happens when you click one of the buttons<br/>frames = []<br/>def onclick(action):<br/>    global frames<br/>    observation, reward, done, info = env.step(action)<br/>    frame = env.render(mode = 'rgb_array')<br/>    im.set_data(frame)<br/>    frames.append(frame)<br/>    if done:<br/>        env.reset()</span></pre><figure class="kt ku kv kw gt jr gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/b881b410423d42bde80044b2d5302d59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*PNfK03G64UCIytKb.gif"/></div></figure><h2 id="fc97" class="kx ky iq bd kz la lb dn lc ld le dp lf kf lg lh li kj lj lk ll kn lm ln lo lp bi translated">重播</h2><p id="ba4c" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">既然你已经玩过了，你可能想看重播。我们保存了游戏的每一次按钮点击状态，你可以在你的浏览器中显示出来:</p><pre class="kt ku kv kw gt lw lx ly lz aw ma bi"><span id="42ba" class="kx ky iq lx b gy mb mc l md me">def display_frames_as_gif(frames, filename_gif = None):<br/>    """<br/>    Displays a list of frames as a gif, with controls<br/>    """<br/>    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)<br/>    patch = plt.imshow(frames[0])<br/>    plt.axis('off')<br/><br/>    def animate(i):<br/>        patch.set_data(frames[i])<br/><br/>    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)<br/>    if filename_gif: <br/>        anim.save(filename_gif, writer = 'imagemagick', fps=20)<br/>    display(display_animation(anim, default_mode='loop'))<br/><br/>display_frames_as_gif(frames, filename_gif="manualplay.gif")</span></pre><h2 id="fb36" class="kx ky iq bd kz la lb dn lc ld le dp lf kf lg lh li kj lj lk ll kn lm ln lo lp bi translated">表现</h2><p id="993f" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">在<a class="ae lv" href="https://gym.openai.com/envs/CartPole-v0" rel="noopener ugc nofollow" target="_blank"> OpenAI 网站</a>上描述了弹球环境。观察参数中的值显示位置(x)、速度(x_dot)、角度(theta)和角速度(theta_dot)。如果杆的角度超过 15 度，或者手推车从中心移动超过 2.4 个单位，游戏“结束”。然后可以通过调用 env.reset()来重置环境。</p><h2 id="129b" class="kx ky iq bd kz la lb dn lc ld le dp lf kf lg lh li kj lj lk ll kn lm ln lo lp bi translated">开始学习</h2><p id="ca97" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">如果没有简单的“学习”机制，这篇博文将是不完整的。凯文·弗朗斯写了一篇关于简单算法的博文，你可以应用在这个问题上:【http://kvfrans.com/simple-algoritms-for-solving-cartpole/<a class="ae lv" href="http://kvfrans.com/simple-algoritms-for-solving-cartpole/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="7925" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">实现起来最简单的就是他的随机搜索算法。通过将参数与观察参数相乘，推车决定向左或向右施力。现在的问题是:什么是最好的参数？随机搜索随机定义它们，查看购物车在这些参数下可以持续多长时间，并记住找到的最佳参数。</p><pre class="kt ku kv kw gt lw lx ly lz aw ma bi"><span id="f86c" class="kx ky iq lx b gy mb mc l md me">def run_episode(env, parameters):  <br/>    """Runs the env for a certain amount of steps with the given parameters. Returns the reward obtained"""<br/>    observation = env.reset()<br/>    totalreward = 0<br/>    for _ in xrange(200):<br/>        action = 0 if np.matmul(parameters,observation) &lt; 0 else 1<br/>        observation, reward, done, info = env.step(action)<br/>        totalreward += reward<br/>        if done:<br/>            break<br/>    return totalreward<br/><br/># Random search: try random parameters between -1 and 1, see how long the game lasts with those parameters<br/>bestparams = None  <br/>bestreward = 0  <br/>for _ in xrange(10000):  <br/>    parameters = np.random.rand(4) * 2 - 1<br/>    reward = run_episode(env,parameters)<br/>    if reward &gt; bestreward:<br/>        bestreward = reward<br/>        bestparams = parameters<br/>        # considered solved if the agent lasts 200 timesteps<br/>        if reward == 200:<br/>            break<br/>            <br/>def show_episode(env, parameters):  <br/>    """ Records the frames of the environment obtained using the given parameters... Returns RGB frames"""<br/>    observation = env.reset()<br/>    firstframe = env.render(mode = 'rgb_array')<br/>    frames = [firstframe]<br/>    <br/>    for _ in xrange(200):<br/>        action = 0 if np.matmul(parameters,observation) &lt; 0 else 1<br/>        observation, reward, done, info = env.step(action)<br/>        frame = env.render(mode = 'rgb_array')<br/>        frames.append(frame)<br/>        if done:<br/>            break<br/>    return frames<br/><br/>frames = show_episode(env, bestparams)<br/>display_frames_as_gif(frames, filename_gif="bestresultrandom.gif")</span></pre><figure class="kt ku kv kw gt jr gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/1943165a28647433196ce297fbe593ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*jILB6eaDs8n1vlit.gif"/></div></figure><h2 id="04da" class="kx ky iq bd kz la lb dn lc ld le dp lf kf lg lh li kj lj lk ll kn lm ln lo lp bi translated">练习以了解更多关于 OpenAI 健身房的信息</h2><p id="f89f" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">接下来就是自己玩自己学了。以下是一些建议:</p><ul class=""><li id="2706" class="mg mh iq jw b jx jy kb kc kf mi kj mj kn mk kr ml mm mn mo bi translated">继续凯文·弗兰斯的教程:<a class="ae lv" href="http://kvfrans.com/simple-algoritms-for-solving-cartpole/" rel="noopener ugc nofollow" target="_blank">http://kvfrans.com/simple-algoritms-for-solving-cartpole/</a></li><li id="2283" class="mg mh iq jw b jx mp kb mq kf mr kj ms kn mt kr ml mm mn mo bi translated">上传并分享您的结果。比较随机算法的效果，或者你自己实现的算法与其他人相比的效果。如何做到这一点可以在这个页面找到:【https://gym.openai.com/docs#recording-and-uploading-results】T2 标题下的“记录和上传结果”</li><li id="fd33" class="mg mh iq jw b jx mp kb mq kf mr kj ms kn mt kr ml mm mn mo bi translated">看看其他环境:【https://gym.openai.com/envs】T4。如果你能解决 cartpole 环境，你当然也能解决钟摆问题(注意，你必须调整你的算法，因为这个只有 3 个变量在观察中)。</li></ul><h2 id="c475" class="kx ky iq bd kz la lb dn lc ld le dp lf kf lg lh li kj lj lk ll kn lm ln lo lp bi translated">结论</h2><p id="ca55" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">恭喜你！你在开放的健身房里制作了你的第一个自动平衡杆。既然这样做了，是时候要么改进你的算法，要么开始尝试不同的环境了。这个 Jupyter 笔记本跳过了很多关于你实际在做什么的基础知识，在<a class="ae lv" href="https://gym.openai.com/docs" rel="noopener ugc nofollow" target="_blank"> OpenAI 网站</a>上有一篇关于这个的很棒的文章。</p><h2 id="be56" class="kx ky iq bd kz la lb dn lc ld le dp lf kf lg lh li kj lj lk ll kn lm ln lo lp bi translated">下一步</h2><p id="fb66" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">除非你决定把自己的算法作为练习，否则你不会在本教程中做很多机器学习(我不认为寻找随机参数是“学习”)。请看看:</p><ul class=""><li id="5769" class="mg mh iq jw b jx jy kb kc kf mi kj mj kn mk kr ml mm mn mo bi translated"><a class="ae lv" href="http://www.pinchofintelligence.com/introduction-openai-gym-part-2-building-deep-q-network/" rel="noopener ugc nofollow" target="_blank">第二部分:哪里会看深度 q 网络:预测每个动作的回报的神经网络。</a></li><li id="7126" class="mg mh iq jw b jx mp kb mq kf mr kj ms kn mt kr ml mm mn mo bi translated"><a class="ae lv" href="http://www.pinchofintelligence.com/openai-gym-part-3-playing-space-invaders-deep-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">第 3 部分:我们将使用深度 q 网络，以雅达利游戏的图片作为输入</a></li></ul><h2 id="947e" class="kx ky iq bd kz la lb dn lc ld le dp lf kf lg lh li kj lj lk ll kn lm ln lo lp bi translated">感谢</h2><p id="ec13" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">这篇博客是我的 TRADR summerschool 关于在强化学习算法中使用人类输入的研讨会的第一部分。更多信息可以在他们的主页上找到。</p></div></div>    
</body>
</html>