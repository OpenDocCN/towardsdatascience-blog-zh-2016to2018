# 深度学习的硬件

> 原文：<https://towardsdatascience.com/hardware-for-deep-learning-8d9b03df41a?source=collection_archive---------3----------------------->

# 我喜欢深度学习…

深度学习最近的成功是不可阻挡的。从对图像和语音识别中的对象进行分类，到为图像添加字幕，理解视觉场景，总结视频，翻译语言，绘画，甚至制作图像，语音，声音和音乐！

# …我想跑得更快！

结果是惊人的，因此需求将会上升。想象你是谷歌或脸书或 Twitter:在你找到一种方法来“阅读”图像和视频的内容，以更好地了解你的用户，他们喜欢什么，他们谈论什么，他们推荐什么，他们分享什么之后。你会怎么做？你可能会想做更多！

也许你运行了一个版本的 ResNet / Xception / denseNet，把用户图片分成上千个类别。如果你是互联网巨头之一，你有很多服务器和服务器场，所以理想情况下你想在这个现有的基础设施上运行深度学习算法。它可以工作一段时间…直到你意识到你用来解析文本的服务器，现在要做的操作是以前对单个图像进行分类的一百万倍以上。来自使用的数据越来越快。[现实生活每一分钟 300 小时视频！](http://fortunelords.com/youtube-statistics/)。

[服务器农场消耗大量电力](https://www.cloudyn.com/blog/10-facts-didnt-know-server-farms/)如果我们需要使用 100 万个以上的基础设施来处理图像和视频，我们将需要建立大量的发电厂，或者使用更有效的方法在云中进行深度学习。权力来之不易，所以我们最好走效率路线前进。

但数据中心只是我们需要更优化的微芯片和硬件来实现深度学习解决方案的领域之一。在自动驾驶汽车中，放置 1000 瓦的计算系统可能是可以的(尽管也将使用电池/燃料)，但在许多其他应用中，功率是一个硬限制。想想无人机、机器人、手机、平板电脑和其他移动设备。这些都需要在几瓦的功率预算下运行，如果不低于 1 W 的话。

还有许多消费产品，如智能相机、增强现实护目镜和设备，也需要消耗很少的电力，并且可能不想使用云计算解决方案来解决隐私问题。

随着我们的家庭变得越来越智能，人们可以看到许多设备将需要使用深度学习应用程序，持续收集和处理数据。

# 所以…你需要新的硬件，啊？

因此，我们需要新的硬件，比基于英特尔至强处理器的服务器更高效的硬件。一个英特尔服务器 CPU 可能消耗 100–150 W 的功率，并且可能需要一个带冷却功能的大型系统来支持性能。

其他选择是什么？

*   图形处理器，GPU
*   现场可编程逻辑器件
*   定制微芯片、专用集成电路、专用集成电路或片上系统
*   数字信号处理器
*   我们可能从未来、外星人或晦涩难懂的新物理定律中获得的一些其他技术

# 绘图处理器

GPU 是设计用于生成基于多边形的计算机图形的处理器。近年来，考虑到最近的计算机游戏和图形引擎的复杂性和对真实性的需求，GPU 已经积累了巨大的处理能力。NVIDIA 在这一领域处于领先地位，它生产的处理器拥有几千个内核，计算效率几乎达到 100%。事实证明，这些处理器也非常适合执行神经网络的计算，[矩阵乘法](https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/)。请注意，矩阵向量乘法被认为是[“令人尴尬的并行”](https://en.wikipedia.org/wiki/Embarrassingly_parallel)，因为它们可以通过简单的算法扩展来并行化(它们没有分支，因此几乎没有[缓存未命中](https://en.wikipedia.org/wiki/CPU_cache))。

泰坦 X 是一种最受欢迎的训练深度学习模型的工具。凭借超过 3500 个内核，它可以提供[超过 11 万亿次触发器](https://blogs.nvidia.com/blog/2016/07/21/titan-x/)。有关测试性能的更多信息，请点击查看[。](https://github.com/soumith/convnet-benchmarks)

英特尔 CPU 和 NVIDIA GPUs 之间的竞争[有利于后者](https://blogs.nvidia.com/blog/2016/08/16/correcting-some-mistakes/)，因为 GPU 的核心数量巨大(英特尔至强处理器的核心数量约为 3500 个，至强处理器的核心数量为 16 个，至强处理器的核心数量为 32 个)，抵消了 CPU 时钟速度快 2 到 3 倍的影响。GPU 核心是更复杂的(分支预测和流水线)CPU 核心的简化版本，但是拥有如此多的 GPU 核心可以实现更高水平的并行性，从而提高性能。

目前，GPU 是训练深度学习系统的标准，无论是卷积/ CNN 还是递归神经网络/ RNN。他们可以在几毫秒内一次对 128 或 256 幅图像的大批量图像进行训练。但它们的功耗约为 250 W，并且需要一台完整的 PC 来支持它们，还需要额外的 150 W 功率。不低于 400 W 可能进入高性能 GPU 系统。

这不是增强现实护目镜、无人机、手机、移动设备和小型机器人的选项。即使在未来的消费级自动驾驶汽车中，这种功率预算也是不可接受的。

NVIDIA 正在努力开发更节能的设备，如 [Tegra TX1](http://www.nvidia.com/object/jetson-tx1-dev-kit.html) 和 [TX2](https://www.engadget.com/2017/03/07/nvidia-launches-jetson-tx2-platform-for-drones-and-robots/) (12 W 和大约 100 G-flops/s TX1 在深度神经网络上的性能，更多的是 TX2)和更强大的 [Drive PX](http://www.nvidia.com/object/drive-px.html) (250 W，像 Titan X)。

还要注意的是，在自动驾驶汽车和智能摄像机的情况下，实时视频是必要的，图像批处理是不可能的，因为视频必须实时处理才能及时响应。

一般来说，GPU 每瓦功率可提供约 5 G-flops/s 的性能。如果我们希望移动系统部署深度学习解决方案，我们需要做得更好！

# 现场可编程门阵列 （Field Programmable Gata Array 的缩写）

现代 FPGA 设备，如 Xilinx 公司的[产品](https://www.xilinx.com/)是电子产品的乐高积木。人们可以用他们的电路作为构建模块来构建完整的定制微处理器和复杂的异构系统。近年来，FPGA 开始支持越来越多的乘累加计算模块。这些 DSP 模块可以执行乘法运算，并且可以排列在一起并行执行许多运算。

10 多年来，我们一直致力于将 FPGA 用于神经网络。我们的工作始于 NYU Yann le Cun 团队最初的开拓性工作，特别是 [Clement Farabet](http://yann.lecun.com/exdb/publis/pdf/farabet-fpl-09.pdf) 。我们的合作产生了用于运行神经网络的复杂数据流处理器。

在 2011 年至 2015 年初期间，我们完善了一个名为 [nn-X](http://ieeexplore.ieee.org/document/6910056/?tp=&arnumber=6910056) 的全新设计。这项工作由 Berin Martini 和 Vinayak Gokhale(来自我们实验室)领导。该系统在 4 W 的预算下提供了高达 200 G-ops/s 的速度，实际上是 50 G-ops/s/W，几乎是 GPU 的 10 倍。

但是 nn-X 遇到了两个主要问题:

*   不使用固定卷积引擎时利用率低
*   高内存带宽第一个问题是由于 nn-X 采用 10x10 的固定卷积引擎，当执行 3x3 卷积时，只有 9%的 DSP 单元得到有效利用。后来，通过将 12x12 的网格划分为 4x4 的 3x3 卷积器单元，这种情况得到了改善。不幸的是，该系统还需要高内存带宽，因为它没有使用数据缓存，并且需要从内存中获取输入并将结果直接保存到内存中。因此，nn-X 无法扩展，其 DPS 单元的利用率从未超过 75–80%。

具有类似设计约束的系统在性能上也会受到限制。

所需要的是具有数据高速缓存的系统，该系统可以使用任意组的 DPS 单元来有效地使用接近 100%的资源。一个这样的系统是[微软的 Catapult](https://www.microsoft.com/en-us/research/project/project-catapult/) 和我们自己的[雪花加速器](https://medium.com/@culurciello/snowflake-ae51c238ead6)，利用率接近 100%。

微软使用 Altera 设备在执行深度神经网络方面实现了创纪录的性能。不幸的是，这不是一个商业系统，而是微软数据中心的资产之一，因此还没有向公众开放。中国互联网巨头百度也走了这条路线。

# 定制 SoC

高通、AMD、ARM、Intel、NVIDIA 都在努力将定制微芯片集成到他们现有的解决方案中。Nervana 和 Movidius(现在都在英特尔公司工作)已经或正在开发集成解决方案。在相同的技术节点上，SoC 可以提供比 FPGA 系统好~ 10 倍的性能，在某些特定的架构上甚至更多。随着 SoC 和处理器的功耗越来越低，这种差异将来自新的集成内存系统，以及对外部存储器带宽的有效利用。在这一领域，集成为系统级封装的 3D 内存是将功耗降低至少 10 倍的一种方法。

# 死后无子女。

DSP 已经存在很长时间了，它生来就是用来执行矩阵运算的。但是到目前为止，还没有一个 DSP 真正提供任何有用的性能或可以与 GPU 竞争的设备。这是为什么呢？主要原因是内核数量。DSP 主要用于电信系统，不需要超过 16 或 32 个内核。他们的工作量不需要它。相反，GPU 工作负载在最近 10-15 年间持续增长，因此需要越来越多的内核。最后，自 2006 年左右，NVIDIA GPUs 在性能上优于 DSP。

德州仪器继续开发它们，但我们还没有看到它们的任何竞争性能。许多 DSP 也已经被 FPGAs 取代。

高通在他们的 SoC 中使用了 DSP，它们提供了加速，但是目前还没有足够的细节将它们与其他解决方案进行比较。

# 今后

是我们要做的。保持兴趣、专注和活跃！

# 关于作者

我在硬件和软件方面都有将近 20 年的神经网络经验(一个罕见的组合)。在这里看关于我:[传媒](https://medium.com/@culurciello/)、[网页](https://e-lab.github.io/html/contact-eugenio-culurciello.html)、[学者](https://scholar.google.com/citations?user=SeGmqkIAAAAJ)、 [LinkedIn](https://www.linkedin.com/in/eugenioculurciello/) 等等…