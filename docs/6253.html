<html>
<head>
<title>The Near Future of Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的不远的将来</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-near-future-of-deep-learning-deb9fb9e811e?source=collection_archive---------18-----------------------#2018-12-03">https://towardsdatascience.com/the-near-future-of-deep-learning-deb9fb9e811e?source=collection_archive---------18-----------------------#2018-12-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e9d8666c32d069ee75ca15f919e92c05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LNxNDGvSHNswuLqUa7q_hQ.png"/></div></div></figure><p id="4182" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在所有最新的深度学习研究中保持领先是非常困难的。下载 Arxiv 移动应用并启用通知将向您展示深度学习研究的疯狂步伐。这么多论文被公开，很难区分信号和噪音。这篇文章将强调我认为将对深度学习的未来产生最大影响的 3 个研究领域。</p><h1 id="05f5" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">一、建筑搜索</h1><p id="fc7c" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">事实上，设计神经网络架构与其说是科学，不如说是艺术。我们大多数人只是从货架上抓取一个流行的网络架构，如 VGG-16/19 [1]或 ResNet [2]。这些前沿架构是如何被发现的？使用强大的 GPU 计算机进行简单的试错。</p><p id="3b39" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">何时应用最大池、使用何种卷积滤波器大小、在何处添加漏失层，这些决策大多只是随机猜测。</p><p id="66d5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">回车，<strong class="ka ir">神经架构搜索</strong> [3]。NAS 使用强化学习来搜索卷积网络架构。</p><p id="fe7f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">与敌手对 NAS，<strong class="ka ir">衰老进化为图像分类器搜索</strong>【4】。该策略使用遗传算法的修改来搜索卷积网络架构。</p><p id="b5ef" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这两种算法都能够设计出优于人类的 CNN 架构。Keras 或 PyTorch 等库中的未来神经网络 API 将只要求您定义网络中所需的一系列层或参数。这些算法将设计架构。</p><h1 id="ff2b" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated"><strong class="ak">二世。压缩神经网络</strong></h1><p id="96d1" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">训练深度学习网络是熟悉计算机内存的好方法。典型的笔记本电脑大约有 8 到 16 GB 的内存。iPhone 8 plus 有大约 3 GB 的内存。VGG-16 图像分类网络有大约 1.44 亿个参数，大约 500 MB。由于这些网络的巨大规模，很难建立移动人工智能应用程序和使用多个网络的应用程序。将网络加载到 RAM 内存中可以加快计算速度。</p><p id="21ca" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">压缩这些网络的研究比如<strong class="ka ir">深度压缩</strong>【5】工作原理非常类似 JPEG 图像压缩；量化和霍夫曼编码。深度压缩能够将 VGG-16 从 552 MB 减少到 11.3 MB，而不损失准确性。</p><h1 id="4d37" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated"><strong class="ak">三世。基于 GAN 的数据增强</strong></h1><p id="bfc2" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">构建深度学习模型的最大挑战之一是构建数据集。深度学习模型需要大量的数据。GANs 是一个很有前途的生成式建模框架，可以从数据集中推断出新的数据点。这可用于从小数据集创建大数据集。</p><p id="013c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">甘斯与训练稳定性作斗争。在图像域，很难实现高分辨率输出，如 GANs 的 360 x 360 x 3。这导致了诸如 DCGAN [6]、StackGAN [7]和逐渐增长的 GANs [8]等架构的发展。</p><h1 id="cb4e" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated"><strong class="ak">结论</strong></h1><p id="4bcb" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">总而言之，我认为架构搜索、压缩神经网络和使用 GANs 建立深度学习数据集都将成为深度学习未来的基础部分。关于这些主题，有很多令人兴奋的研究论文，我推荐大家去看看。</p><p id="8fc8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">感谢阅读！希望这有助于引导你对深度学习的兴趣！</p><h1 id="a819" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated"><strong class="ak">参考文献</strong></h1><p id="9f96" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">[1]卡伦·西蒙扬，安德鲁·齐泽曼。用于大规模图像识别的非常深的卷积网络。2014.</p><p id="947d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[2]何、、任、。用于图像识别的深度残差学习。2015.</p><p id="7b36" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[3] Barret Zoph，Quoc 诉 Le。强化学习的神经结构搜索。2017.</p><p id="3a9b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[4] Esteban Real，Alok Aggarwal，黄雁萍，Quoc 诉 Le。图像分类器结构搜索的正则化进化。2018.</p><p id="fffa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[5]宋汉，毛，威廉 j .日报。深度压缩:通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。2015.</p><p id="2d38" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[6]亚历克·拉德福德，卢克·梅斯，苏密特·钦塔拉。深度卷积生成对抗网络的无监督表示学习。2015.</p><p id="d7cd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[7]张寒、徐涛、李洪生、张少婷、王晓刚、黄小蕾、季米特里斯·梅塔克萨斯。StackGAN:使用堆叠生成式对抗网络进行文本到照片级逼真图像合成。2016.</p><p id="9bee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[8]泰罗·卡拉斯、蒂莫·艾拉、萨穆利·莱恩、贾科科·莱蒂宁。为了提高质量、稳定性和变化性而逐步种植甘蔗。2017.</p></div></div>    
</body>
</html>