<html>
<head>
<title>Generating text using a Recurrent Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用递归神经网络生成文本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-text-using-a-recurrent-neural-network-1c3bfee27a5e?source=collection_archive---------5-----------------------#2018-10-29">https://towardsdatascience.com/generating-text-using-a-recurrent-neural-network-1c3bfee27a5e?source=collection_archive---------5-----------------------#2018-10-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/008c0148a1dec36b9139c16bc6c7c6b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qcil7N8lcUzAMWZL"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 1: person writing on paper by <a class="ae jd" href="https://unsplash.com/@rawpixel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">rawpixel</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="cb9b" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">如何使用 Keras 创建亚瑟·柯南·道尔风格的文本</h2></div><p id="0b73" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">深度学习可以用于许多有趣的事情，但通常它可能会觉得只有最聪明的工程师才能创建这样的应用程序。但这根本不是真的。</p><p id="de32" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过<a class="ae jd" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>和其他高级深度学习库，每个人都可以创建和使用深度学习模型，无论他对理论和算法内部工作的理解如何。</p><p id="f89f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我们将看看如何使用递归神经网络来创建亚瑟·柯南·道尔爵士风格的新文本，使用他的书“夏洛克·福尔摩斯的冒险”作为我们的数据集。</p><p id="8b0e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以从<a class="ae jd" href="http://www.gutenberg.org/cache/epub/1661/pg1661.txt" rel="noopener ugc nofollow" target="_blank">古腾堡网站</a>获得数据。我们只需要将其保存为文本(。txt)文件并删除文本中嵌入的 Gutenberg 页眉和页脚。如果你不想自己做这些，你可以从<a class="ae jd" href="https://github.com/TannerGilbert/Tutorials/blob/master/Keras-Tutorials/4.%20LSTM%20Text%20Generation/Keras%20LSTM%20Text%20Generation.ipynb" rel="noopener ugc nofollow" target="_blank">我的 Github </a>获得本文的文本和所有代码。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="3045" class="ly lz jg bd ma mb mc md me mf mg mh mi km mj kn mk kp ml kq mm ks mn kt mo mp bi translated">递归神经网络</h1><figure class="mr ms mt mu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mq"><img src="../Images/7b42344525227b8f3d39c94928aacaaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w8iP14nHFYd1s15a5D7Ftw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 2: Unfold RNN by <a class="ae jd" href="https://commons.wikimedia.org/w/index.php?curid=60109157" rel="noopener ugc nofollow" target="_blank">François Deloche</a></figcaption></figure><p id="a849" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">递归神经网络(RNN)是用于顺序数据的最先进的算法。这是因为它们可以通过内部记忆记住以前的输入。</p><p id="532c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我不会深入探讨递归神经网络是如何工作的，但如果你感兴趣，你可以查看我的视频<a class="ae jd" href="https://www.youtube.com/watch?v=2GHcSDDZYGg" rel="noopener ugc nofollow" target="_blank">解释 RNN 是如何工作的，或者你可以查看<a class="ae jd" href="https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912" rel="noopener">Suvro baner JEE</a>的这篇伟大文章</a>。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="b9e3" class="ly lz jg bd ma mb mc md me mf mg mh mi km mj kn mk kp ml kq mm ks mn kt mo mp bi translated">创建我们的数据集</h1><p id="919b" class="pw-post-body-paragraph kv kw jg kx b ky mv kh la lb mw kk ld le mx lg lh li my lk ll lm mz lo lp lq ij bi translated">像往常一样，我们将开始创建我们的数据集。为了能够在 RNN 中使用文本数据，我们需要将其转换为数值。然后，我们将创建一个字符序列作为我们的 X 数据，并使用下面的字符作为我们的 Y 值。最后，我们将把数据转换成一个布尔数组。</p><p id="3568" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，我们将加载数据，并创建从字符到整数和整数到字符的映射:</p><pre class="mr ms mt mu gt na nb nc nd aw ne bi"><span id="522f" class="nf lz jg nb b gy ng nh l ni nj">with open('sherlock_homes.txt', 'r') as file:<br/>    text = file.read().lower()<br/>print('text length', len(text))</span><span id="3594" class="nf lz jg nb b gy nk nh l ni nj">chars = sorted(list(set(text))) # getting all unique chars<br/>print('total chars: ', len(chars))</span><span id="d0e8" class="nf lz jg nb b gy nk nh l ni nj">char_indices = dict((c, i) for i, c in enumerate(chars))<br/>indices_char = dict((i, c) for i, c in enumerate(chars))</span></pre><p id="21ad" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了获得有价值的数据，我们可以用它来训练我们的模型，我们将把我们的数据分成长度为 40 个字符的子序列。然后我们将把数据转换成一个布尔数组。</p><pre class="mr ms mt mu gt na nb nc nd aw ne bi"><span id="3970" class="nf lz jg nb b gy ng nh l ni nj">maxlen = 40<br/>step = 3<br/>sentences = []<br/>next_chars = []<br/>for i in range(0, len(text) - maxlen, step):<br/>    sentences.append(text[i: i + maxlen])<br/>    next_chars.append(text[i + maxlen])</span><span id="e84e" class="nf lz jg nb b gy nk nh l ni nj">x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)<br/>y = np.zeros((len(sentences), len(chars)), dtype=np.bool)<br/>for i, sentence in enumerate(sentences):<br/>    for t, char in enumerate(sentence):<br/>        x[i, t, char_indices[char]] = 1<br/>    y[i, char_indices[next_chars[i]]] = 1</span></pre></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="1d41" class="ly lz jg bd ma mb mc md me mf mg mh mi km mj kn mk kp ml kq mm ks mn kt mo mp bi translated">递归神经网络模型</h1><p id="a24e" class="pw-post-body-paragraph kv kw jg kx b ky mv kh la lb mw kk ld le mx lg lh li my lk ll lm mz lo lp lq ij bi translated">虽然创建一个 RNN 听起来很复杂，但是使用<a class="ae jd" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>实现起来非常容易。我们将创建一个简单的 RNN，其结构如下:</p><ol class=""><li id="c754" class="nl nm jg kx b ky kz lb lc le nn li no lm np lq nq nr ns nt bi translated"><strong class="kx jh"> LSTM 层:</strong>将学习序列</li><li id="f5c2" class="nl nm jg kx b ky nu lb nv le nw li nx lm ny lq nq nr ns nt bi translated"><strong class="kx jh">密集(全连接)层:</strong>每个独特字符一个输出神经元</li><li id="f7e9" class="nl nm jg kx b ky nu lb nv le nw li nx lm ny lq nq nr ns nt bi translated"><strong class="kx jh"> Softmax 激活:</strong>将输出转换为概率值</li></ol><p id="f151" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将使用<a class="ae jd" href="https://keras.io/optimizers/#rmsprop" rel="noopener ugc nofollow" target="_blank"> RMSprop </a>优化器和<a class="ae jd" href="https://keras.io/losses/#categorical_crossentropy" rel="noopener ugc nofollow" target="_blank">分类交叉熵</a>损失函数。</p><pre class="mr ms mt mu gt na nb nc nd aw ne bi"><span id="ea30" class="nf lz jg nb b gy ng nh l ni nj">from keras.models import Sequential<br/>from keras.layers import Dense, Activation<br/>from keras.layers import LSTM<br/>from keras.optimizers import RMSprop</span><span id="77b8" class="nf lz jg nb b gy nk nh l ni nj">model = Sequential()<br/>model.add(LSTM(128, input_shape=(maxlen, len(chars))))<br/>model.add(Dense(len(chars)))<br/>model.add(Activation('softmax'))</span><span id="afb8" class="nf lz jg nb b gy nk nh l ni nj">optimizer = RMSprop(lr=0.01)<br/>model.compile(loss='categorical_crossentropy', optimizer=optimizer)</span></pre></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="e2a9" class="ly lz jg bd ma mb mc md me mf mg mh mi km mj kn mk kp ml kq mm ks mn kt mo mp bi translated">助手功能</h1><p id="a3d0" class="pw-post-body-paragraph kv kw jg kx b ky mv kh la lb mw kk ld le mx lg lh li my lk ll lm mz lo lp lq ij bi translated">为了看到我们的模型在训练时的改进，我们将创建两个辅助函数。这两个函数来自 Keras 团队的<a class="ae jd" href="https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py" rel="noopener ugc nofollow" target="_blank">官方 LSTM 文本生成示例</a>。</p><p id="29b7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第一个辅助函数将从输出(概率数组)中抽取一个索引。它有一个名为 temperature 的参数，这个参数定义了函数在创建文本时的自由度。第二个将在每个时期结束时生成具有四个不同温度的文本，以便我们可以看到我们的模型做得如何。</p><pre class="mr ms mt mu gt na nb nc nd aw ne bi"><span id="dafb" class="nf lz jg nb b gy ng nh l ni nj">def sample(preds, temperature=1.0):<br/>    # helper function to sample an index from a probability array<br/>    preds = np.asarray(preds).astype('float64')<br/>    preds = np.log(preds) / temperature<br/>    exp_preds = np.exp(preds)<br/>    preds = exp_preds / np.sum(exp_preds)<br/>    probas = np.random.multinomial(1, preds, 1)<br/>    return np.argmax(probas)</span><span id="92c9" class="nf lz jg nb b gy nk nh l ni nj">def on_epoch_end(epoch, logs):<br/>    # Function invoked at end of each epoch. Prints generated text.<br/>    print()<br/>    print('----- Generating text after Epoch: %d' % epoch)<br/><br/>    start_index = random.randint(0, len(text) - maxlen - 1)<br/>    for diversity in [0.2, 0.5, 1.0, 1.2]:<br/>        print('----- diversity:', diversity)<br/><br/>        generated = ''<br/>        sentence = text[start_index: start_index + maxlen]<br/>        generated += sentence<br/>        print('----- Generating with seed: "' + sentence + '"')<br/>        sys.stdout.write(generated)<br/><br/>        for i in range(400):<br/>            x_pred = np.zeros((1, maxlen, len(chars)))<br/>            for t, char in enumerate(sentence):<br/>                x_pred[0, t, char_indices[char]] = 1.<br/><br/>            preds = model.predict(x_pred, verbose=0)[0]<br/>            next_index = sample(preds, diversity)<br/>            next_char = indices_char[next_index]<br/><br/>            generated += next_char<br/>            sentence = sentence[1:] + next_char<br/><br/>            sys.stdout.write(next_char)<br/>            sys.stdout.flush()<br/>        print()<br/>print_callback = LambdaCallback(on_epoch_end=on_epoch_end)</span></pre><p id="504c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们还将定义另外两个回调函数。第一个名为<em class="nz"> ModelCheckpoint。这将节省我们的模型每一个时代的损失减少。</em></p><pre class="mr ms mt mu gt na nb nc nd aw ne bi"><span id="e7dc" class="nf lz jg nb b gy ng nh l ni nj">from keras.callbacks import ModelCheckpoint<br/><br/>filepath = "weights.hdf5"<br/>checkpoint = ModelCheckpoint(filepath, monitor='loss',<br/>                             verbose=1, save_best_only=True,<br/>                             mode='min')</span></pre><p id="08b4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另一个回调会在每次学习停滞时降低学习速度。</p><pre class="mr ms mt mu gt na nb nc nd aw ne bi"><span id="669f" class="nf lz jg nb b gy ng nh l ni nj">rom keras.callbacks import ReduceLROnPlateau<br/>reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,<br/>                              patience=1, min_lr=0.001)</span><span id="6a0e" class="nf lz jg nb b gy nk nh l ni nj">callbacks = [print_callback, checkpoint, reduce_lr]</span></pre></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="110c" class="ly lz jg bd ma mb mc md me mf mg mh mi km mj kn mk kp ml kq mm ks mn kt mo mp bi translated">训练模型并生成新文本</h1><p id="93da" class="pw-post-body-paragraph kv kw jg kx b ky mv kh la lb mw kk ld le mx lg lh li my lk ll lm mz lo lp lq ij bi translated">对于训练，我们需要选择一个 batch_size 和我们想要训练的时期数。对于 batch_size，我选择 128，这只是一个任意的数字。我只训练了 5 个时期的模型，所以我不需要等这么久，但如果你想你可以训练它更多。</p><pre class="mr ms mt mu gt na nb nc nd aw ne bi"><span id="9d67" class="nf lz jg nb b gy ng nh l ni nj">model.fit(x, y, batch_size=128, epochs=5, callbacks=callbacks)</span></pre><p id="5b55" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">培训产出:</p><pre class="mr ms mt mu gt na nb nc nd aw ne bi"><span id="e4b9" class="nf lz jg nb b gy ng nh l ni nj">Epoch 1/5<br/>187271/187271 [==============================] - 225s 1ms/step - loss: 1.9731</span><span id="f0e3" class="nf lz jg nb b gy nk nh l ni nj">----- Generating text after Epoch: 0<br/>----- diversity: 0.2<br/>----- Generating with seed: "lge<br/>on the right side of his top-hat to "<br/>lge<br/>on the right side of his top-hat to he wise as the bore with the stor and string that i was a bile that i was a contion with the man with the hadd and the striet with the striet in the stries in the struttle and the striet with the strange with the man with the struttle with the stratter with the striet with the street with the striet which when she with the strunt of the stright of my stright of the string that i shall had been whi<br/>----- diversity: 0.5<br/>----- Generating with seed: "lge<br/>on the right side of his top-hat to "<br/>lge<br/>on the right side of his top-hat to he had putting the stratce, and that is street in the striet man would not the stepe which we had been of the strude<br/>in our in my step withinst in some with the hudied that in had a had become and the corted to a give with his right with a comon was and confice my could to my sule i was and shugher. i little which a sitter and the site my dippene with a chair drive to be but the some his site with</span></pre><p id="d288" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了自己生成文本，我们将创建一个类似于 on_epoch_end 函数的函数。它将采用一个随机的起始索引，从文本中取出接下来的 40 个字符，然后用它们来进行预测。作为参数，我们将传递给它我们想要生成的文本的长度和生成的文本的多样性。</p><pre class="mr ms mt mu gt na nb nc nd aw ne bi"><span id="6199" class="nf lz jg nb b gy ng nh l ni nj">def generate_text(length, diversity):<br/>    # Get random starting text<br/>    start_index = random.randint(0, len(text) - maxlen - 1)<br/>    generated = ''<br/>    sentence = text[start_index: start_index + maxlen]<br/>    generated += sentence<br/>    for i in range(length):<br/>            x_pred = np.zeros((1, maxlen, len(chars)))<br/>            for t, char in enumerate(sentence):<br/>                x_pred[0, t, char_indices[char]] = 1.<br/><br/>            preds = model.predict(x_pred, verbose=0)[0]<br/>            next_index = sample(preds, diversity)<br/>            next_char = indices_char[next_index]<br/><br/>            generated += next_char<br/>            sentence = sentence[1:] + next_char<br/>    return generated</span></pre><p id="e1dc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们可以通过调用 generate_text 函数来创建文本:</p><pre class="mr ms mt mu gt na nb nc nd aw ne bi"><span id="af83" class="nf lz jg nb b gy ng nh l ni nj">print(generate_text(500, 0.2)</span></pre><p id="474d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">生成的文本:</p><pre class="mr ms mt mu gt na nb nc nd aw ne bi"><span id="85ae" class="nf lz jg nb b gy ng nh l ni nj">of something akin to fear had begun<br/>to be a sount of his door and a man in the man of the compants and the commins of the compants of the street. i could he could he married him to be a man which i had a sound of the compant and a street in the compants of the companion, and the country of the little to come and the companion and looked at the street. i have a man which i shall be a man of the comminstance to a some of the man which i could he said to the house of the commins and the man of street in the country and a sound and the c</span></pre></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="6a41" class="ly lz jg bd ma mb mc md me mf mg mh mi km mj kn mk kp ml kq mm ks mn kt mo mp bi translated">结论</h1><p id="1d90" class="pw-post-body-paragraph kv kw jg kx b ky mv kh la lb mw kk ld le mx lg lh li my lk ll lm mz lo lp lq ij bi translated">递归神经网络是一种处理顺序数据的技术，因为它们可以通过内部存储器记住最后的输入。它们在几乎每一个顺序问题上都达到了最先进的性能，并被大多数大公司所采用。RNN 可用于以特定作者的风格生成文本。</p><p id="c667" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">创建文本生成 RNN 的步骤如下:</p><ol class=""><li id="b927" class="nl nm jg kx b ky kz lb lc le nn li no lm np lq nq nr ns nt bi translated">创建或收集数据集</li><li id="7331" class="nl nm jg kx b ky nu lb nv le nw li nx lm ny lq nq nr ns nt bi translated">建立 RNN 模式</li><li id="6dd5" class="nl nm jg kx b ky nu lb nv le nw li nx lm ny lq nq nr ns nt bi translated">以一个随机的句子为起点创建新的文本</li></ol><p id="416e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个项目的细节可以在<a class="ae jd" href="https://github.com/TannerGilbert/Keras-Tutorials/tree/master/4.%20LSTM%20Text%20Generation" rel="noopener ugc nofollow" target="_blank">这里</a>找到。我鼓励任何人试验一下代码，也许可以改变数据集和预处理步骤，看看会发生什么。</p><p id="e504" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了获得更好的输出，您还可以对模型进行许多改进。其中一些是:</p><ol class=""><li id="4464" class="nl nm jg kx b ky kz lb lc le nn li no lm np lq nq nr ns nt bi translated">使用更复杂的网络结构(更多 LSTM 层、密集层)</li><li id="b23d" class="nl nm jg kx b ky nu lb nv le nw li nx lm ny lq nq nr ns nt bi translated">为更多时代而训练</li><li id="5858" class="nl nm jg kx b ky nu lb nv le nw li nx lm ny lq nq nr ns nt bi translated">摆弄批处理大小</li></ol><p id="d28b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你喜欢这篇文章，可以考虑订阅我的<a class="ae jd" href="https://www.youtube.com/channel/UCBOKpYBjPe2kD8FSvGRhJwA" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>，在社交媒体上关注我。</p><p id="2d59" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你有任何问题或批评，可以通过<a class="ae jd" href="https://twitter.com/Tanner__Gilbert" rel="noopener ugc nofollow" target="_blank">推特</a>或评论区联系我。</p></div></div>    
</body>
</html>