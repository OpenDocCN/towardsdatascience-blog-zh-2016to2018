<html>
<head>
<title>How to create SnapChat lenses using pix2pix</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用 pix2pix 创建 SnapChat 镜头</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-pix2pix-to-create-snapchat-lenses-e9520f17bad1?source=collection_archive---------12-----------------------#2018-12-30">https://towardsdatascience.com/using-pix2pix-to-create-snapchat-lenses-e9520f17bad1?source=collection_archive---------12-----------------------#2018-12-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7332" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们都喜欢 SnapChat 镜头/滤镜，但有没有想过如何自己制作？</h2></div><p id="134c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">是的，我知道副标题来自我的<a class="ae lb" href="https://hackernoon.com/how-to-make-snapchat-lenses-f9eae861b5db" rel="noopener ugc nofollow" target="_blank">上一篇文章</a>，在那里我展示了如何使用 dlib 和 openCV 使 SnapChat 像 live filters 一样。但今天我想展示我们如何使用名为 pix2pix 的深度学习网络来创建相同的过滤器。这种方法有助于从以前的方法中消除手动特征工程步骤，并且仅通过神经网络的一次推理就可以直接输出目标图像。那么，我们开始吧。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/fa5f90c2fad73e914e9677e24d39beb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*QCLxc399RxKLyFIhBUHc9A.gif"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Demo</figcaption></figure></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="c740" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">pix2pix 是什么？</h1><p id="e1ed" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">它发表在论文<a class="ae lb" href="https://arxiv.org/abs/1611.07004" rel="noopener ugc nofollow" target="_blank">用条件对抗网络进行图像到图像的翻译</a>(俗称 pix2pix)。</p><p id="4d5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">名称“pix2pix”来自于网络被训练成从输入像素映射到输出像素，其中输出是输入的某种转换。你可以在下图中看到一些例子，</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ms"><img src="../Images/16945c221b80ed1bcc7dedfa909adb0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t_lDqP-1u4C5z9Pq9PltBQ.jpeg"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Source <a class="ae lb" href="https://arxiv.org/pdf/1611.07004.pdf" rel="noopener ugc nofollow" target="_blank">Paper</a></figcaption></figure><p id="1dfb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">pix2pix 真正吸引人的地方在于它是一种通用的<em class="mx">图像到图像的转换。它不是为上面的每个任务设计定制的网络，而是处理所有任务的同一模型——只是在每个任务的不同数据集上进行训练。Pix2pix 可以用更少的训练图像和更少的训练时间产生有效的结果。给定一个只有 400 个(外观，图像)对的训练集，并且在单个 Pascal Titan X GPU 上进行不到两个小时的训练，pix2pix 可以做到这一点:</em></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi my"><img src="../Images/2f4e87c1fcbbd0214ccda074d2151cd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GOuWXcbVNMr8T0NJCz_0gw.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Source <a class="ae lb" href="https://arxiv.org/pdf/1611.07004.pdf" rel="noopener ugc nofollow" target="_blank">Paper</a></figcaption></figure><h1 id="5df1" class="lv lw iq bd lx ly mz ma mb mc na me mf jw nb jx mh jz nc ka mj kc nd kd ml mm bi translated">pix2pix 网络如何工作？</h1><p id="f186" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">pix2pix 网络使用 2 个网络进行训练。</p><ol class=""><li id="a453" class="ne nf iq kh b ki kj kl km ko ng ks nh kw ni la nj nk nl nm bi translated">发电机</li><li id="aaff" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">鉴别器</li></ol><p id="2f83" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，生成器将尝试从输入训练数据生成输出图像，鉴别器将尝试识别输出是假还是真。因此，两个网络都将努力提高精度，最终形成一个真正优秀的发电机。所以，你可以把网络的基本结构想象成编码器-解码器。</p><p id="1e02" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，生成器被输入一个实际的图像(输入)，我们想把它“翻译”成另一个结构相似的图像(实际输出)。我们的生成器现在产生假输出，我们希望它与真实输出无法区分。因此，我们简单地收集大量“真实事物”的例子，并要求网络生成无法与它们区分的图像。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="9555" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">方法</h1><p id="9b66" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">今天，我们将使用上述模型来创建 SnapChat 过滤器。我将使用下面的图像来显示结果。所有的图像都是 256x256 因为我是在同样的尺寸上训练的(是的，我没有足够的 GPU 能力来训练高清)</p><p id="c0ef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们收集训练数据！为此我将使用我以前的<a class="ae lb" href="https://github.com/smitshilu/SnapChatFilterExample" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">教程代码</strong> </a> <strong class="kh ir">。</strong>我下载了一堆人脸图像数据集，还用谷歌下载了人脸图像。</p><p id="59f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我将<strong class="kh ir">“暴徒生活眼镜”</strong>应用到所有的图像上，并将它们并排放置，因为训练数据需要这种格式。我使用下面的 pix2pix 训练库，它使用 tensorflow 来训练和显示结果。</p><div class="ns nt gp gr nu nv"><a href="https://github.com/affinelayer/pix2pix-tensorflow" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd ir gy z fp oa fr fs ob fu fw ip bi translated">affini layer/pix2pix-tensor flow</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">有条件对抗网络的图像到图像翻译的张量流端口 https://phillipi.github.io/pix2pix/…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">github.com</p></div></div><div class="oe l"><div class="of l og oh oi oe oj li nv"/></div></div></a></div><p id="4815" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦您完成生成训练数据，它将如下所示:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ok"><img src="../Images/f85fc31801a0b4b1b56eabd8837f67cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V120RAHR64e1R4FrCRNkeg.jpeg"/></div></div></figure><p id="166c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们开始训练。为了训练，我们应该在 pix2pix-tensorflow 存储库中使用以下命令</p><pre class="ld le lf lg gt ol om on oo aw op bi"><span id="a4cb" class="oq lw iq om b gy or os l ot ou">python pix2pix.py --mode train --output_dir dir_to_save_checkpoint --max_epochs 200 --input_dir dir_with_training_data --which_direction AtoB</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/3bf284c1e60175982208f746f8c64185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*ufDLCKCB97qTKh7CjBBTcg.png"/></div></figure><p id="757a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，AtoB 定义了训练模型的方向。例如，在上面的图像中，AtoB 表示模型将学习将正常的脸转换为戴眼镜的脸。</p><p id="9fbd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以在 tensorboard 中的训练数据和图形上看到结果，您可以通过以下命令启动 tensor board:</p><pre class="ld le lf lg gt ol om on oo aw op bi"><span id="e3d6" class="oq lw iq om b gy or os l ot ou">tensorboard --logdir=dir_to_save_checkpoint</span></pre><p id="bc89" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦你开始看到你的模型有不错的结果，就停止训练，用评估数据来检查实时表现。如果您认为结果对于实时性能来说不够好，可以从最后一个检查点开始训练。</p><pre class="ld le lf lg gt ol om on oo aw op bi"><span id="fe37" class="oq lw iq om b gy or os l ot ou">python pix2pix.py --mode train --output_dir dir_to_save_checkpoint --max_epochs 200 --input_dir dir_with_training_data --which_direction AtoB --checkpoint dir_of_saved_checkpoint</span></pre><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="ov ow l"/></div></figure><h1 id="f7f1" class="lv lw iq bd lx ly mz ma mb mc na me mf jw nb jx mh jz nc ka mj kc nd kd ml mm bi translated">结论</h1><p id="082a" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">条件对抗网络是许多图像到图像翻译任务的有前途的方法。你需要适当地训练，请使用一个好的 GPU，因为我得到了以下输出，训练较少，没有太多的训练数据差异。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/99fa5bd8912977773c5eddb138954dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*fy_PlKzRyL_IsIG0Xfsy7w.gif"/></div></figure></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><p id="783d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你喜欢这篇文章请在<a class="ae lb" href="https://medium.com/@smitshilu" rel="noopener"> <strong class="kh ir"> <em class="mx">上关注我</em></strong></a><strong class="kh ir"><em class="mx"/></strong>或者<strong class="kh ir"><em class="mx"/></strong><a class="ae lb" href="https://github.com/smitshilu" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir"><em class="mx">Github</em></strong></a><strong class="kh ir"><em class="mx"/></strong>或者订阅我的<a class="ae lb" href="http://www.youtube.com/c/SmitShilu" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> <em class="mx"> YouTube 频道</em> </strong> </a>。</p></div></div>    
</body>
</html>