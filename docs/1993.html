<html>
<head>
<title>Generating Digits and Sounds with Artificial Neural Nets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用人工神经网络生成数字和声音</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-digits-and-sounds-with-artificial-neural-nets-ca1270d8445f?source=collection_archive---------1-----------------------#2017-11-30">https://towardsdatascience.com/generating-digits-and-sounds-with-artificial-neural-nets-ca1270d8445f?source=collection_archive---------1-----------------------#2017-11-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/17e57b12578605258fb3f8d55d6ccb4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*piD4Hte70aovq7DNNKDh2A.png"/></div></div></figure><h2 id="5108" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">为乐趣而实验…</h2><p id="8340" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">在 Cisco Emerge，我们喜欢尝试新旧工具和想法。最近，我们开始修补生成模型。<br/>在<a class="ae lp" href="https://github.com/ciscoemerge/emergeX/tree/master/denoising-VAEs" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上查看这些实验的源代码</p><h2 id="0bee" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">什么是生成模型？</h2><p id="4983" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">生成模型是人工神经网络，能够创建人类从未见过的“假”数据，目标是使其与真实数据无法区分。</p><p id="4f94" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">这里有一个例子，说明了一组特定的生成模型——称为生成对抗网络——在观察了成千上万张人脸图像后能够创造出什么:</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lv"><img src="../Images/cd13b125b1d3239a71fd97e0f8f84045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePYb9pVEqUL9IuZBubCrAQ.jpeg"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Source: <a class="ae lp" href="https://arxiv.org/pdf/1711.09020.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1711.09020.pdf</a></figcaption></figure><p id="ff0d" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">这就好像神经网络学会了创造一个他们观察到的世界的替代现实。</p><h2 id="95e9" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">我们探索了变型自动编码器</h2><p id="194a" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">一组特殊的生成模型被称为变分自动编码器。<br/>除了花哨的名字，这个概念非常简单，下图可以帮助解释我们可以用它们实现什么。</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/66e3d034db1f709b489741ba04bcfa3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OSVtihnWZxzLJeeHjut84g.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Variational Autoencoder</figcaption></figure><ul class=""><li id="1a48" class="mf mg iq kw b kx lq lb lr kh mh kl mi kp mj lo mk ml mm mn bi translated"><strong class="kw ir">编码器</strong>:神经网络的第一部分(称为编码器)读取输入——在这个例子中是一个数字——并试图将信息压缩成一个比原始输入更小的向量。</li><li id="7961" class="mf mg iq kw b kx mo lb mp kh mq kl mr kp ms lo mk ml mm mn bi translated">潜在向量(Latent Vector):这个向量被“强制”匹配一个特定的概率分布——通常是高斯分布(Gaussian Distribution)<a class="ae lp" href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution" rel="noopener ugc nofollow" target="_blank">,但是实际上你可以选择任何东西。<br/>不赘述，之所以会这样，是因为我们希望能够从这个分布中采样一个向量，然后将其馈送到神经网络的第二部分，以生成新数据。</a></li><li id="8207" class="mf mg iq kw b kx mo lb mp kh mq kl mr kp ms lo mk ml mm mn bi translated"><strong class="kw ir">解码器</strong>:网络的第二部分，用来产生新数据，称为解码器。</li></ul><p id="b215" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">在训练期间，编码器试图以“损失最小”的方式压缩输入，而解码器试图在只给定潜在向量作为输入的情况下尽可能地匹配原始输入。</p><h2 id="f073" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">损坏的数据</h2><p id="65d0" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">如果我们在编码器中输入损坏的输入，并要求解码器重建未损坏的输入，会发生什么情况？</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/6a82b7177110c798ce5535b1aeae08e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jH4gFFBv0AUNFgzHW7HqoQ.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Denoising Variational Autoencoder</figcaption></figure><p id="06e9" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">神奇的事情发生了:解码器的输出是一个完整的图像，整个数字被精确地绘制出来。<br/>数字明显比原始数字更平滑，也更模糊，因为关于“缺陷”的信息没有通过网络传输。只有使数字成为数字的信息会保留下来。<br/>这一切都意味着:</p><ol class=""><li id="d07e" class="mf mg iq kw b kx lq lb lr kh mh kl mi kp mj lo mu ml mm mn bi translated">编码器能够学习构建一个数字的表示，即使是在数字本身的一部分丢失(在上面的例子中是 20%)的情况下。</li><li id="67ef" class="mf mg iq kw b kx mo lb mp kh mq kl mr kp ms lo mu ml mm mn bi translated">解码器能够学习如何从编码器学习的表示中重建完整的图像。</li></ol><h2 id="6a95" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">让我们试试声音</h2><p id="c21e" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">人工神经网络非常通用:可以执行对象识别、语音生成、自然语言处理等</p><p id="fc9d" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">出于这个原因，我们保持了相同的神经网络结构，并尝试了相同的声音实验。</p><h2 id="320e" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">嘈杂的 c 大调音阶</h2><p id="1ef1" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">我们没有使用 10 个不同的数字图像，而是使用了 7 个音符——c 大调音阶。更准确地说，它们是正弦曲线(因此它们听起来或多或少像笛子)加上了白噪声(因此它们听起来像笛子，演奏者使用了大量空气)。</p><figure class="lw lx ly lz gt jr"><div class="bz fp l di"><div class="mv mw l"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Click to listen to the sound</figcaption></figure><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mx"><img src="../Images/c73d928578b71392f8951bbb8f6a44cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*efilBygFx4NpN3emRdA6Qg.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Sound1: Non corrupted noisy sinusoid</figcaption></figure><p id="a8ef" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">与我们之前的实验类似，我们向编码器输入斩波输入。</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi my"><img src="../Images/fd838a92ccd001a8a44d08be641badf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZGjjueMR9PLRLAxbUILMzw.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Sound2: This is the input of sounds we fed into the Encoder with, while the image “Sound1” is the reference that the Decoder tries to reconstruct</figcaption></figure><h2 id="bf5b" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">培养</h2><ul class=""><li id="c4d3" class="mf mg iq kw b kx ky lb lc kh mz kl na kp nb lo mk ml mm mn bi translated"><strong class="kw ir"> 0 次迭代<br/> </strong>最初自动编码器只能产生随机噪声:</li></ul><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/c4822d851fddb490b66a6ff448e2bf72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SW04URyB-otxMNFXFENDBw.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk"><strong class="bd nd">0 iterations — </strong>Line1: Spectogram of Reconstructed audio. Line 2: Waveform of Reconstructed Audio. Line 3: Spectogram of Chopped Audio fed into the Autoencoder. Line 4: Waveform of Chopped Audio</figcaption></figure><ul class=""><li id="db49" class="mf mg iq kw b kx lq lb lr kh mh kl mi kp mj lo mk ml mm mn bi translated"><strong class="kw ir"> 100 次迭代</strong></li></ul><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/56043d972e370be1f7a412545c7e15a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*--6D6GeSs4zmXE6oObM8NA.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk"><strong class="bd nd">100 iterations — </strong>Line1: Spectogram of Reconstructed audio. Line 2: Waveform of Reconstructed Audio. Line 3: Spectogram of Chopped Audio fed into the Autoencoder. Line 4: Waveform of Chopped Audio</figcaption></figure><ul class=""><li id="4e0a" class="mf mg iq kw b kx lq lb lr kh mh kl mi kp mj lo mk ml mm mn bi translated"><strong class="kw ir"> 200 次迭代<br/></strong></li></ul><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nf"><img src="../Images/eb577b8c2d9c28a28d0bda7d9deefa4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1vIPcnLf8-o6iwFc5get2w.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk"><strong class="bd nd">200 iterations — </strong>Line1: Spectogram of Reconstructed audio. Line 2: Waveform of Reconstructed Audio. Line 3: Spectogram of Chopped Audio fed into the Autoencoder. Line 4: Waveform of Chopped Audio</figcaption></figure><ul class=""><li id="323d" class="mf mg iq kw b kx lq lb lr kh mh kl mi kp mj lo mk ml mm mn bi translated"><strong class="kw ir"> 400 次迭代<br/>这似乎类似于数字图像实验中发生的情况，输出是平滑和模糊的。<br/>声谱图还显示，除主频率外，所有其他频率的能量都较弱。</strong></li></ul><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/ce7befa46b0aa4d488a530b10cdf85f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zvzM4Hdz-OsUwBT53GXM6g.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk"><strong class="bd nd">400 iterations — </strong>Line1: Spectogram of Reconstructed audio. Line 2: Waveform of Reconstructed Audio. Line 3: Spectogram of Chopped Audio fed into the Autoencoder. Line 4: Waveform of Chopped Audio</figcaption></figure><ul class=""><li id="3dcc" class="mf mg iq kw b kx lq lb lr kh mh kl mi kp mj lo mk ml mm mn bi translated"><strong class="kw ir"> 800 次迭代:<br/> </strong>自动编码器学习重建完整信号，并忽略原始输入中添加的白噪声。</li></ul><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/38b4df0a756be6b6b57771122015c4d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-4kQdP4QP1pvHFb_sMm9tQ.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk"><strong class="bd nd">800 iterations — </strong>Line1: Spectogram of Reconstructed audio. Line 2: Waveform of Reconstructed Audio. Line 3: Spectogram of Chopped Audio fed into the Autoencoder. Line 4: Waveform of Chopped Audio</figcaption></figure><h2 id="0d79" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">听起来是这样的</h2><p id="e59c" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">这是被破坏的输入的声音。请记住，神经网络是用一个只有一部分丢失的短输入来训练的，而完整的声音是一个有许多随机丢失部分的较长输入。</p><figure class="lw lx ly lz gt jr"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="e798" class="pw-post-body-paragraph ku kv iq kw b kx lq kz la lb lr ld le kh ls lg lh kl lt lj lk kp lu lm ln lo ij bi translated">重建的输入听起来像这样:</p><figure class="lw lx ly lz gt jr"><div class="bz fp l di"><div class="mv mw l"/></div></figure><h2 id="e2c7" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">用更复杂的输入自己尝试一下</h2><p id="000b" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">你可以在这个<a class="ae lp" href="https://github.com/ciscoemerge/emergeX/tree/master/denoising-VAEs" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>中找到我们用来运行这些实验并产生这些结果的所有源代码，这样你就可以尝试为数字、声音以及可能更复杂的输入复制它们。随意留下评论，叉回购。<br/>您不需要任何特定的数据集或硬件，只需要安装一台计算机和 TensorFlow(MNIST 数据集附带 tensor flow)。<br/>如果你得到了什么有趣的结果，请在评论中分享。</p><h2 id="24c7" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">谷歌洋红色</h2><p id="796d" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">你可能想看看谷歌的这个项目:<a class="ae lp" href="https://magenta.tensorflow.org/nsynth-fastgen" rel="noopener ugc nofollow" target="_blank">用 NSynth </a>生成你自己的声音。<br/>与我们刚刚展示的实验类似，他们建立了能够学习并直接生成原始音频样本的神经网络。</p><h2 id="0f97" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">结论</h2><p id="449a" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">正如上面提到的 Google Magenta 的项目，已经有人在努力用人工神经网络生成音频，他们取得的结果令人印象深刻。<br/>我们试图让这个实验尽可能简单，这样任何人都可以在不需要庞大数据集和配备 GPU 的昂贵机器的情况下进行尝试，并深入了解可变自动编码器如何对图像和原始声音进行处理。</p></div></div>    
</body>
</html>