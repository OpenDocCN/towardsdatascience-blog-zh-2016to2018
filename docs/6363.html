<html>
<head>
<title>Understanding the concept of Hierarchical clustering Technique</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解层次聚类技术的概念</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec?source=collection_archive---------0-----------------------#2018-12-10">https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec?source=collection_archive---------0-----------------------#2018-12-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="c304" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">层次聚类技术</strong>是机器学习<strong class="jw ir">中流行的聚类技术之一。</strong>在我们试图理解层次聚类技术的概念之前<strong class="jw ir"> </strong>让我们先了解一下聚类…</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ks"><img src="../Images/6653cc0f1bf2a64a1b773cae5a96b102.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/1*0ZuEyPVPm7cESoDm33clfA.gif"/></div></figure><p id="b639" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">什么是聚类？？</strong></p><p id="738d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">聚类</strong>基本上是一种对相似数据点进行分组的技术，使得同一组中的点比其他组中的点更加相似。一组相似的数据点被称为<strong class="jw ir">簇。</strong></p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi la"><img src="../Images/19630925830032bf688b35f7e0f1aaa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aZdqqvSkDcZj4SE3PK9j_Q.png"/></div></div></figure><p id="4a3c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">聚类和分类/回归模型的区别:</strong></p><h2 id="ccdf" class="lf lg iq bd lh li lj dn lk ll lm dp ln kf lo lp lq kj lr ls lt kn lu lv lw lx bi translated">在分类和回归模型中，我们得到一个数据集(D ),其中包含数据点(Xi)和类别标签(易)。其中，Yi 属于分类模型的{0，1}或{0，1，2，…，n ), Yi 属于回归模型的真实值。</h2><p id="0ad9" class="pw-post-body-paragraph ju jv iq jw b jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr ij bi translated">说到聚类，我们得到的数据集只包含数据点(Xi)。这里我们<strong class="jw ir">没有</strong>提供的类标签(Yi)。</p><p id="aa27" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在，让我们回到最初的主题，即<strong class="jw ir">层次聚类技术。</strong></p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi md"><img src="../Images/d351d46def75fb02ff473ed985ad064b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*AmI9wRbXrfIWGESx6eEiTw.gif"/></div></figure><p id="fc6b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这里有一个小礼物，特别送给我的读者，作为善意的表示:)亚马逊音乐无限<strong class="jw ir">免费</strong>3 个月:<a class="ae me" href="https://amzn.to/2ZBlWI7" rel="noopener ugc nofollow" target="_blank">【https://amzn.to/2ZBlWI7】</a><strong class="jw ir"/>(注册支持)感谢。</p><p id="dc0b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">层次聚类技术:</strong></p><p id="a8d8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">层次聚类是一种流行且易于理解的聚类技术。这种聚类技术分为两种类型:</p><ol class=""><li id="1265" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mk ml mm mn bi translated">结块的</li><li id="2398" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">分裂的</li></ol><p id="227b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">点击此处申领免费麦当劳礼品卡</p><p id="725e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">凝聚层次聚类技术:</strong>在该技术中，最初每个数据点被认为是一个单独的聚类。在每次迭代中，相似的聚类与其他聚类合并，直到形成一个聚类或 K 个聚类。</p><p id="a2ea" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">凝聚的基本算法是简单明了的。</p><ul class=""><li id="e4cf" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated">计算邻近矩阵</li><li id="cffb" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mt ml mm mn bi translated">让每个数据点成为一个集群</li><li id="35e2" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mt ml mm mn bi translated">重复:合并两个最近的聚类并更新邻近矩阵</li><li id="241a" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mt ml mm mn bi translated">直到只剩下一个集群</li></ul><p id="2264" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">关键操作是计算两个聚类的接近度</p><p id="af34" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了更好地理解，让我们看一个凝聚层次聚类技术的图示。假设我们有六个数据点{A，B，C，D，E，F}。</p><ul class=""><li id="47bf" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated">步骤 1:在初始步骤中，我们计算各个点的接近度，并将所有六个数据点视为独立的聚类，如下图所示。</li></ul><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/d56185eb04eb4841e36eed2008e51cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*3pMZjFiiaaLcfSZBKDjbXA.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Agglomerative Hierarchical Clustering Technique</figcaption></figure><ul class=""><li id="de7a" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated">步骤 2:在步骤 2 中，相似的聚类被合并在一起并形成单个聚类。让我们考虑 B、C 和 D、E 是在第二步中合并的相似集群。现在，我们剩下四个集群，分别是 A，BC，DE，f。</li><li id="e472" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mt ml mm mn bi translated">步骤 3:我们再次计算新聚类的接近度，并合并相似的聚类以形成新的聚类 A、BC、DEF。</li><li id="9faf" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mt ml mm mn bi translated">步骤 4:计算新群的接近度。聚类 DEF 和 BC 是相似的，并且合并在一起以形成新的聚类。我们现在剩下两个星团 A，BCDEF。</li><li id="7477" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mt ml mm mn bi translated">第 5 步:最后，所有的集群合并在一起，形成一个单一的集群。</li></ul><p id="2d21" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">可以使用<strong class="jw ir">树状图来可视化分层聚类技术。</strong></p><p id="330a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">树状图</strong>是一个<strong class="jw ir"> </strong>树形图，记录了合并或拆分的顺序。</p><div class="kt ku kv kw gt ab cb"><figure class="mz kx na nb nc nd ne paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><img src="../Images/cddf1f8faf2415a0f4a9eb8a771aff22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*JPQRbJDw2E1_HEvwzVTDDw.jpeg"/></div></figure><figure class="mz kx na nb nc nd ne paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><img src="../Images/41234f37b3f1179ec609f11fef7891b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*fw1vlNtq2vPFmAXsBy1_dA.jpeg"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk nf di ng nh">Dendrogram representation</figcaption></figure></div><p id="7339" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> 2。分裂式</strong> <strong class="jw ir">层次聚类技术:</strong>由于分裂式层次聚类技术在现实世界中使用不多，我就简单介绍一下分裂式层次聚类技术。</p><p id="9356" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">简单来说，我们可以说分裂式层次聚类与<strong class="jw ir">凝聚式层次聚类正好相反。</strong>在分裂层次聚类中，我们将所有数据点视为单个聚类，并且在每次迭代中，我们从聚类中分离不相似的数据点。每个被分离的数据点被认为是一个单独的聚类。最终，我们会剩下 n 个集群。</p><p id="cf51" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">由于我们将单个聚类分成 n 个聚类，因此称之为<strong class="jw ir">分裂式</strong> <strong class="jw ir">层次聚类。</strong></p><p id="b757" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因此，我们已经讨论了两种类型的层次聚类技术。</p><p id="b1f6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">但是等等！！我们还剩下<strong class="jw ir"> </strong>层次聚类的<strong class="jw ir">重要部分</strong>。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/42f3baf8c6f1ed8376fac2be846e0491.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/1*dpL-5neZyXbiX30zPNc5rQ.gif"/></div></figure><p id="9c58" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">“我们如何计算两个聚类之间的相似度？？?"</strong></p><p id="b1cf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">计算两个聚类之间的相似性对于合并或划分聚类是重要的。有一些方法可用于计算两个聚类之间的相似性:</p><ul class=""><li id="0a55" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated">部</li><li id="787f" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mt ml mm mn bi translated">马克斯(男子名ˌ等于 Maximilian)</li><li id="63fd" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mt ml mm mn bi translated">群体平均值</li><li id="220f" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mt ml mm mn bi translated">质心之间的距离</li><li id="be42" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mt ml mm mn bi translated">沃德方法</li><li id="b77b" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mt ml mm mn bi translated"><strong class="jw ir"> MIN: </strong>也称为单链算法可以定义为<strong class="jw ir"> </strong>两个聚类 C1 和 C2 的相似度等于点 Pi 和 Pj 之间的相似度的最小值<strong class="jw ir"/>使得 Pi 属于 C1，Pj 属于 C2。</li></ul><p id="10cd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">从数学上讲，这可以写成:</p><p id="db7d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">Sim(C1，C2) = Min Sim(Pi，Pj)使得 Pi ∈ C1 &amp; Pj ∈ C2</p><p id="a990" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">简而言之，选取两个最近的点，使得一个点位于聚类 1 中，另一个点位于聚类 2 中，并获取它们的相似性，将其声明为两个聚类之间的相似性。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/2bcc87884eafa1cfed1097901731059f.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*mtDL2TynaiwpJlhLdecFYQ.jpeg"/></div></figure><p id="db4d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">敏的优点:</strong></p><ul class=""><li id="dcb9" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated">只要两个簇之间的间隙不小，这种方法就可以分离非椭圆形状。</li></ul><div class="kt ku kv kw gt ab cb"><figure class="mz kx na nb nc nd ne paragraph-image"><img src="../Images/637d2a01f514c43af64db53ef646cb02.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*C-BNL_MDzJ6uXKgvhBy0Pg.jpeg"/></figure><figure class="mz kx na nb nc nd ne paragraph-image"><img src="../Images/181e10f1a5badfbedee3662633ff89b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*iQDFke1lM13R82JPG3GqNg.jpeg"/></figure></div><div class="ab cb"><figure class="mz kx na nb nc nd ne paragraph-image"><img src="../Images/27544737f1e1b5818a78ff0bc4e7cb73.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*dcukVWU3Ny2VhhUug5J0rg.jpeg"/></figure><figure class="mz kx na nb nc nd ne paragraph-image"><img src="../Images/e1212b059dda49d3e06628d24cc72e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*uNf5CCVkiwQBzjIJu4gLEg.jpeg"/><figcaption class="mv mw gj gh gi mx my bd b be z dk nf di ng nh">Original data vs Clustered data using MIN approach</figcaption></figure></div><p id="47bb" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">MIN 的缺点:</strong></p><ul class=""><li id="b465" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated">如果聚类之间存在噪声，最小方法不能正确地分离聚类。</li></ul><div class="kt ku kv kw gt ab cb"><figure class="mz kx na nb nc nd ne paragraph-image"><img src="../Images/582017f3af30466c635ee6b463b26d43.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*ViZVqbai7fDLZZ4kpfXyUg.jpeg"/></figure><figure class="mz kx na nb nc nd ne paragraph-image"><img src="../Images/0ffe958e8b4cdc126436be5f58311cb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*zTG9v1NQlUYD5N6NM70wRg.jpeg"/><figcaption class="mv mw gj gh gi mx my bd b be z dk nf di ng nh">Original data vs Clustered data using MIN approach</figcaption></figure></div><ul class=""><li id="19bb" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated"><strong class="jw ir"> MAX: </strong>也称为完全联动算法，这与<strong class="jw ir"> MIN </strong>方法正好相反。两个群集 C1 和 C2 的相似性等于点 Pi 和 Pj 之间的相似性的最大值<strong class="jw ir"/>,使得 Pi 属于 C1，Pj 属于 C2。</li></ul><p id="c985" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">从数学上讲，这可以写成:</p><p id="cbad" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">Sim(C1，C2) = Max Sim(Pi，Pj)使得 Pi ∈ C1 &amp; Pj ∈ C2</p><p id="5653" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">简而言之，选取两个最远的点，使得一个点位于聚类 1 中，另一个点位于聚类 2 中，并获取它们的相似性，将其声明为两个聚类之间的相似性。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/a80eb2d1c11931c83c440e05f8380d0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*nRYZyjoT1ZRzlWp3oP0_QQ.jpeg"/></div></figure><p id="a71c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">MAX 的优点:</strong></p><ul class=""><li id="2db3" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated">如果聚类之间存在噪声，最大值方法在分离聚类方面表现良好。</li></ul><div class="kt ku kv kw gt ab cb"><figure class="mz kx na nb nc nd ne paragraph-image"><img src="../Images/2d08dbf02883f992aec67d3af86b2d10.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*xpR9pkp5Z4P7ZUBtoF-eWA.jpeg"/></figure><figure class="mz kx na nb nc nd ne paragraph-image"><img src="../Images/381ada61383463e477aeb07974e58a0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*pjGdCFuhMD_DWjxYkN8QRA.jpeg"/><figcaption class="mv mw gj gh gi mx my bd b be z dk nf di ng nh">Original data vs Clustered data using MAX approach</figcaption></figure></div><p id="0318" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">Max 的缺点:</strong></p><ul class=""><li id="63de" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated">Max 方法偏向球状星团。</li><li id="a23b" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mt ml mm mn bi translated">最大值方法倾向于打破大的集群。</li></ul><div class="kt ku kv kw gt ab cb"><figure class="mz kx na nb nc nd ne paragraph-image"><img src="../Images/ceff397026cfd0c176df1509ad87293c.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*hpzH_kxtv_YGzi8cSwIXzA.jpeg"/></figure><figure class="mz kx na nb nc nd ne paragraph-image"><img src="../Images/1a0e487c557f858dfb90d06282c3226c.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*itIRbzJFd4_5AUNs7G0zdw.jpeg"/><figcaption class="mv mw gj gh gi mx my bd b be z dk nf di ng nh">Original data vs Clustered data using MAX approach</figcaption></figure></div><ul class=""><li id="3105" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated"><strong class="jw ir">组平均:</strong>取所有的点对，计算它们的相似度，并计算相似度的平均值。</li></ul><p id="fe6e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">从数学上讲，这可以写成:</p><p id="93f7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">sim(C1，C2) = <a class="ae me" href="https://en.wiktionary.org/wiki/%E2%88%91" rel="noopener ugc nofollow" target="_blank"> ∑ </a> sim(Pi，Pj)/|C1|*|C2|</p><p id="d6cf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">其中，π∈C1 &amp; Pj∈C2</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/a420a9eb322e5229ef4b1830a861d536.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*CMHO0wpT8hCkR_xCQW2ggQ.jpeg"/></div></figure><p id="3767" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">组平均的优点:</strong></p><ul class=""><li id="37a5" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated">如果聚类之间存在噪声，则组平均方法在分离聚类方面表现良好。</li></ul><p id="e281" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">组平均的缺点:</strong></p><ul class=""><li id="37f0" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated">群平均法偏向球状星团。</li><li id="0582" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mt ml mm mn bi translated"><strong class="jw ir">质心距离:</strong>计算两个聚类 C1 &amp; C2 的质心，将两个质心之间的相似度作为两个聚类之间的相似度。这是现实世界中不太流行的技术。</li></ul><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/3c8f3268c803ed84495e8dccee027032.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*2AYd0CXANWsM8MLwmrJzYQ.jpeg"/></div></figure><ul class=""><li id="56e2" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated"><strong class="jw ir">沃德方法:</strong>除了沃德方法计算距离 Pi 和 PJ 的平方和之外，这种计算两个聚类之间相似性的方法与组平均完全相同。</li></ul><p id="c88a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">从数学上讲，这可以写成:</p><p id="ad0d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">sim(C1，C2) = <a class="ae me" href="https://en.wiktionary.org/wiki/%E2%88%91" rel="noopener ugc nofollow" target="_blank"> ∑ </a> (dist(Pi，Pj)) /|C1|*|C2|</p><p id="1fb8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">沃德方法的优点:</strong></p><ul class=""><li id="b399" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated">如果聚类之间存在噪声，Ward 的方法在分离聚类方面也做得很好。</li></ul><p id="271d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">沃德方法的缺点:</strong></p><ul class=""><li id="2ab1" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mt ml mm mn bi translated">沃德的方法也偏向球状星团。</li></ul><p id="a234" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">层次聚类技术的空间和时间复杂度:</strong></p><p id="3486" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">空间复杂度:</strong>当数据点的数量很大时，层次聚类技术所需的空间非常大，因为我们需要将相似性矩阵存储在 RAM 中。空间复杂度是 n 的平方的数量级。</p><p id="ac75" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">空间复杂度= O(n)其中 n 是数据点的数量。</p><p id="05eb" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">时间复杂度:</strong>由于我们要进行 n 次迭代，并且在每次迭代中，我们需要更新相似度矩阵和恢复矩阵，所以时间复杂度也很高。时间复杂度是 n 的立方的数量级。</p><p id="39cd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">时间复杂度= O(n)其中 n 是数据点的数量。</p><p id="86c3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"/><strong class="jw ir">层次聚类技术的局限性:</strong></p><ol class=""><li id="f116" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mk ml mm mn bi translated">分层聚类没有数学目标。</li><li id="dbda" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">所有计算聚类之间相似度的方法都有其自身的缺点。</li><li id="b009" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">层次聚类的高空间和时间复杂度。因此，当我们有大量数据时，不能使用这种聚类算法。</li></ol><h2 id="f256" class="lf lg iq bd lh li lj dn lk ll lm dp ln kf lo lp lq kj lr ls lt kn lu lv lw lx bi translated">相关文章:<a class="ae me" href="https://bit.ly/37pQ2yy" rel="noopener ugc nofollow" target="_blank">K 近邻入门</a></h2><div class="nk nl gp gr nm nn"><a href="https://medium.com/datadriveninvestor/machine-learning-getting-started-with-k-nearest-neighbours-6851280d4c93" rel="noopener follow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd ir gy z fp ns fr fs nt fu fw ip bi translated">机器学习:K 近邻入门。</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">我们将在本文中了解到:</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">medium.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob ky nn"/></div></div></a></div><p id="994f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">参考文献:</strong></p><ol class=""><li id="0340" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mk ml mm mn bi translated"><a class="ae me" href="https://cs.wmich.edu/alfuqaha/summer14/cs6530/lectures/ClusteringAnalysis.pdf" rel="noopener ugc nofollow" target="_blank">https://cs . wmich . edu/alfuqaha/summer 14/cs 6530/lectures/clustering analysis . pdf</a></li><li id="2b58" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated"><a class="ae me" href="http://www.appliedaicourse.com" rel="noopener ugc nofollow" target="_blank">www.appliedaicourse.com</a></li><li id="cf77" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated"><a class="ae me" href="https://en.wikipedia.org/wiki/Hierarchical_clustering" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Hierarchical_clustering</a></li></ol><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/06185495aa6bcea9a6707788a8b3d204.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*MFRoW7QCEE-z_PwNy-WRiA.gif"/></div></figure></div></div>    
</body>
</html>