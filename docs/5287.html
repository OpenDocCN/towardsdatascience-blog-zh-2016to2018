<html>
<head>
<title>How Active Learning can help you train your models with less Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主动学习如何帮助您用更少的数据训练模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-active-learning-can-help-you-train-your-models-with-less-data-389da8a5f7ea?source=collection_archive---------5-----------------------#2018-10-09">https://towardsdatascience.com/how-active-learning-can-help-you-train-your-models-with-less-data-389da8a5f7ea?source=collection_archive---------5-----------------------#2018-10-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="75bf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated">即使有大量的计算资源，在大型数据集上训练一个机器学习模型也可能需要几个小时、几天甚至几周的时间，这是昂贵的，并且是你生产力的负担。但是，在大多数情况下，您不需要所有可用的数据来训练您的模型。在本文中，我们将比较数据子集化策略以及它们对模型性能的影响(训练时间和准确性)。我们将在对<a class="ae ku" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> MNIST </a>数据集子集的 SVM 分类器的训练中实现它们。</p><h1 id="9adc" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">通过主动学习构建子集</h1><p id="0ff3" class="pw-post-body-paragraph jn jo iq jp b jq lt js jt ju lu jw jx jy lv ka kb kc lw ke kf kg lx ki kj kk ij bi translated">我们将使用<a class="ae ku" href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)" rel="noopener ugc nofollow" target="_blank">主动学习</a>来构建我们的数据子集。</p><blockquote class="ly lz ma"><p id="b94a" class="jn jo mb jp b jq jr js jt ju jv jw jx mc jz ka kb md kd ke kf me kh ki kj kk ij bi translated">主动学习是机器学习的一种特殊情况，在这种情况下，学习算法能够交互式地询问用户，以在新的数据点获得所需的输出。</p></blockquote><p id="30ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">子集化数据的过程是由一个主动学习者完成的，该学习者将基于一个策略进行学习，该策略的训练子集适合于最大化我们的模型的准确性。我们将考虑 4 种不同的策略来从原始训练集中构建这些数据子集:</p><ul class=""><li id="e05e" class="mf mg iq jp b jq jr ju jv jy mh kc mi kg mj kk mk ml mm mn bi translated"><strong class="jp ir">随机采样</strong>:随机采样数据点</li><li id="70ba" class="mf mg iq jp b jq mo ju mp jy mq kc mr kg ms kk mk ml mm mn bi translated"><strong class="jp ir">不确定性采样</strong>:我们选择我们最不确定其类别的点。</li><li id="196c" class="mf mg iq jp b jq mo ju mp jy mq kc mr kg ms kk mk ml mm mn bi translated"><strong class="jp ir">熵采样</strong>:选择类概率熵最大的点</li><li id="cf9e" class="mf mg iq jp b jq mo ju mp jy mq kc mr kg ms kk mk ml mm mn bi translated"><strong class="jp ir">边缘采样</strong>:我们选择最有可能和第二有可能类别之间的差异最小的点。</li></ul><p id="5210" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些策略中的概率与 SVM 分类器的预测相关联。</p><p id="121d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于这项研究，我们将构建 5000 个子集(8%的数据)；原始训练集 60，000 点中的 10，000 点(数据的 17%)和 15，000 点(数据的 25%)。</p><h1 id="8438" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">结果</h1><p id="21b6" class="pw-post-body-paragraph jn jo iq jp b jq lt js jt ju lu jw jx jy lv ka kb kc lw ke kf kg lx ki kj kk ij bi translated">为了测量我们在子集上训练的性能，我们将测量<strong class="jp ir"> <em class="mb">训练准确度</em> </strong>和<strong class="jp ir"> <em class="mb">训练时间</em>比率</strong>计算如下:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi mt"><img src="../Images/2ce84fce6a5a1a5e8861e2797fb1d309.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JQhIpzN5Q4F3bg7D1FgwVA.png"/></div></div></figure><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nf"><img src="../Images/4dfe0c9c2e3b53a0474905aa5add7618.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ADTGRHe-T7KRfzXD5OTlmw.png"/></div></div></figure><p id="7e90" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以为测试数据集计算相同的比率。结果总结在下图中。每个策略的 3 个数据点对应于子集的大小(5，000；一万和一万五)。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ng"><img src="../Images/50ddebff53057595eeae610f709bdcf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zhJir81HDyZEl0vXyWJfkQ.png"/></div></div></figure><p id="1e0b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们所看到的，使用不确定性采样策略，我们可以在 15，000 个点的子集上实现 99%以上的性能，而在完整数据集上训练 SVM 所需的时间仅为 35%。这清楚地表明，我们可以获得与使用完整数据集相当的结果，但只需要 25%的数据和 35%的时间。随机抽样是所有策略中最快的，但就准确率而言也是最差的。</p><p id="041e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，对数据子集进行处理是一种合理的方法，可以用更少的计算量显著减少训练时间，并且不会影响准确性。子集化数据适用于大多数分类数据集，但需要扩展以适用于时间序列数据和您正在训练的模型。</p><h1 id="0389" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">我们需要多少数据？</h1><p id="61d9" class="pw-post-body-paragraph jn jo iq jp b jq lt js jt ju lu jw jx jy lv ka kb kc lw ke kf kg lx ki kj kk ij bi translated">既然我们已经证明了在数据子集上训练模型的价值和可行性，我们如何知道最佳的子集大小应该是多少呢？一种叫做 FABOLAS [Klein et al.]的方法在这里实现<a class="ae ku" href="https://automl.github.io/RoBO/tutorials.html#fabolas" rel="noopener ugc nofollow" target="_blank"/>可以推荐你应该使用的子集的大小。它通过学习上下文变量(要使用的数据集的大小)和最终得分的可靠性之间的关系来做到这一点。这意味着，通过在子集上训练模型，它可以推断模型在完整数据集上的性能。</p><h1 id="a8fa" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">贝叶斯优化扩展</h1><p id="abc5" class="pw-post-body-paragraph jn jo iq jp b jq lt js jt ju lu jw jx jy lv ka kb kc lw ke kf kg lx ki kj kk ij bi translated">如果我们想更进一步，我们可以通过使用贝叶斯优化来更有效地优化子集上的超参数的训练。我在以前的帖子中已经写了很多关于它的内容:</p><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/the-intuitions-behind-bayesian-optimization-with-gaussian-processes-7e00fcc898a0"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd ir gy z fp np fr fs nq fu fw ip bi translated">高斯过程贝叶斯优化背后的直觉</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">在某些应用中，目标函数是昂贵的或难以评估的。在这些情况下，一般…</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nt l"><div class="nu l nv nw nx nt ny nd nk"/></div></div></a></div><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/demystifying-hyper-parameter-tuning-acb83af0258f"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd ir gy z fp np fr fs nq fu fw ip bi translated">揭开超参数调谐的神秘面纱</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">它是什么，为什么是自然的</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nt l"><div class="nz l nv nw nx nt ny nd nk"/></div></div></a></div><p id="93ae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 Mind Foundry，我们正在努力通过贝叶斯优化和主动学习来实现最优和高效的机器学习。如果您有任何问题或想尝试我们的产品，请随时给<a class="ae ku" href="http://Charles.brecque@mindfoundry.ai" rel="noopener ugc nofollow" target="_blank">发电子邮件</a>给我！</p><p id="4b8d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">【<strong class="jp ir">更新</strong>:我开了一家科技<a class="ae ku" href="http://www.legislate.tech/" rel="noopener ugc nofollow" target="_blank">公司</a>。你可以在这里找到更多的<a class="ae ku" href="https://medium.com/legislate/eliminate-the-work-in-paperwork-c053bfb0188c" rel="noopener"/></p><p id="fa54" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1:<a class="ae ku" href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Active _ learning _(machine _ learning)</a></p><p id="7bad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2: <a class="ae ku" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Klein%2C+A" rel="noopener ugc nofollow" target="_blank">亚伦·克莱因</a>，<a class="ae ku" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Falkner%2C+S" rel="noopener ugc nofollow" target="_blank">斯特凡·福克纳</a>，<a class="ae ku" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bartels%2C+S" rel="noopener ugc nofollow" target="_blank">西蒙·巴特尔</a>，<a class="ae ku" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hennig%2C+P" rel="noopener ugc nofollow" target="_blank">菲利普·亨宁</a>，<a class="ae ku" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hutter%2C+F" rel="noopener ugc nofollow" target="_blank">弗兰克·赫特</a>，大型数据集上机器学习超参数的快速贝叶斯优化，<a class="ae ku" href="https://arxiv.org/abs/1605.07079" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir">arXiv:1605.07079</strong></a><strong class="jp ir">【cs。LG] </strong></p></div></div>    
</body>
</html>