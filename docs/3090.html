<html>
<head>
<title>Coding Neural Network — Gradient Checking</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">编码神经网络—梯度检验</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/coding-neural-network-gradient-checking-5222544ccc64?source=collection_archive---------4-----------------------#2018-04-08">https://towardsdatascience.com/coding-neural-network-gradient-checking-5222544ccc64?source=collection_archive---------4-----------------------#2018-04-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ce1f424e32de9ce7c07790a33628f0e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D-_BDtYmueIITNt9iEfTOA.png"/></div></div></figure><p id="9e54" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在之前的帖子<a class="ae kw" rel="noopener" target="_blank" href="/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76">编码神经网络——前向传播和反向传播</a>中，我们在<code class="fe kx ky kz la b">numpy</code>中实现了前向传播和反向传播。然而，从头开始实现反向传播通常更能减少错误。因此，在对训练数据运行神经网络之前，有必要检查我们的反向传播实现是否正确。在我们开始之前，让我们重温一下什么是反向传播:我们从最后一个节点开始，以相反的拓扑顺序遍历节点，计算成本相对于每条边的节点尾部的导数。换句话说，我们计算成本函数相对于所有参数的导数，即∂J/∂θ，其中θ表示模型的参数。</p><p id="d970" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">测试我们实现的方法是计算数值梯度，并与反向传播(分析)的梯度进行比较。有两种计算数值梯度方法:</p><ul class=""><li id="eb23" class="lb lc iq ka b kb kc kf kg kj ld kn le kr lf kv lg lh li lj bi translated">右侧表格:</li></ul><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lk"><img src="../Images/bedfea0c778d5a46290b4f986c41f3cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NXBG9YMx2vullfvBgi_NAw.png"/></div></div></figure><ul class=""><li id="c602" class="lb lc iq ka b kb kc kf kg kj ld kn le kr lf kv lg lh li lj bi translated">双面形式(见图1):</li></ul><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lp"><img src="../Images/79d4bba61c659b406065e548b15e8210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0u5WeZaYu1oDikwstpw3Bw.png"/></div></div></figure><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lq"><img src="../Images/5a312673a2231b7d5df096670e8d1f47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RyRy2pd7UcpynLvR.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk"><strong class="bd lv">Figure 1:</strong> Two-sided numerical gradients</figcaption></figure><p id="82b4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">近似导数的双侧形式比右侧形式更接近。让我们用下面的例子来说明，使用函数<em class="lw"> f(x) = x </em>，在<em class="lw">对x = 3 </em>求导。</p><ul class=""><li id="6249" class="lb lc iq ka b kb kc kf kg kj ld kn le kr lf kv lg lh li lj bi translated">分析导数:</li></ul><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/30b885ea2748aaaa2facdc6532f35a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*r0HbB20Nn_IDz7EEFMGYIg.png"/></div></figure><ul class=""><li id="c16b" class="lb lc iq ka b kb kc kf kg kj ld kn le kr lf kv lg lh li lj bi translated">双侧数值导数；</li></ul><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/be13b9f33eb16e3246b98e29fd13e07a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*tn6tiYqYG6M8hUMgzjEwhg.png"/></div></figure><ul class=""><li id="0dc2" class="lb lc iq ka b kb kc kf kg kj ld kn le kr lf kv lg lh li lj bi translated">右手数值导数:</li></ul><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/8a20d4843899be92181cc005bec20903.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*vk4NYEFJIF6MrxfeGuuryQ.png"/></div></figure><p id="2444" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如上所述，解析导数和双边数值梯度之间的差异几乎为零；但是解析导数和右侧导数相差0.01。因此，我们将使用双边ε方法来计算数值梯度。</p><p id="be34" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，我们将标准化数值之间的差异。使用以下公式的梯度和分析梯度:如果差值≤ 10e-7，那么我们的实现是好的；否则，我们会在某处出错，不得不返回并重新访问反向传播代码。</p><p id="fc98" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以下是实施梯度检查所需的步骤:</p><ol class=""><li id="c3a0" class="lb lc iq ka b kb kc kf kg kj ld kn le kr lf kv ma lh li lj bi translated">在计算数值梯度和分析梯度时，从训练数据中选择随机数量的示例来使用它。</li></ol><ul class=""><li id="f89c" class="lb lc iq ka b kb kc kf kg kj ld kn le kr lf kv lg lh li lj bi translated">不要使用训练数据中的所有示例，因为梯度检查非常慢。</li></ul><p id="14cf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2.初始化参数。</p><p id="f33e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">3.计算前向传播和交叉熵成本。</p><p id="fcbf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">4.使用我们的反向传播实现来计算梯度。</p><p id="b601" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">5.使用双边ε方法计算数值梯度。</p><p id="59fc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">6.计算数值梯度和分析梯度之间的差异。</p><p id="3124" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将使用我们在<em class="lw">“编码神经网络-前向传播和后向传播”</em>帖子中编写的函数来初始化参数，计算前向传播和后向传播以及交叉熵成本。</p><p id="a4bf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们先导入数据。</p><pre class="ll lm ln lo gt mb la mc md aw me bi"><span id="4740" class="mf mg iq la b gy mh mi l mj mk"># Loading packages<br/>import sys<br/>import h5py<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>from numpy.linalg import norm</span><span id="5dd5" class="mf mg iq la b gy ml mi l mj mk">import seaborn as sns sys.path.append("../scripts/")<br/>from coding_neural_network_from_scratch import (initialize_parameters, L_model_forward, L_model_backward, compute_cost)</span><span id="8018" class="mf mg iq la b gy ml mi l mj mk"># Import the data<br/>train_dataset = h5py.File("../data/train_catvnoncat.h5")<br/>X_train = np.array(train_dataset["train_set_x"]).T<br/>y_train = np.array(train_dataset["train_set_y"]).T<br/>X_train = X_train.reshape(-1, 209)<br/>y_train = y_train.reshape(-1, 209)</span><span id="fbec" class="mf mg iq la b gy ml mi l mj mk">X_train.shape, y_train.shape</span><span id="0b48" class="mf mg iq la b gy ml mi l mj mk">((12288, 209), (1, 209))</span></pre><p id="c2f5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我们将编写帮助器函数，帮助将参数和梯度字典转换成向量，然后再将它们转换回字典。</p><pre class="ll lm ln lo gt mb la mc md aw me bi"><span id="37f8" class="mf mg iq la b gy mh mi l mj mk">def dictionary_to_vector(params_dict):<br/>L = len(layers_dims)<br/>parameters = {}<br/>k = 0<br/>for l in range(1, L):<br/># Create temp variable to store dimension used on each layer<br/>w_dim = layers_dims[l] * layers_dims[l - 1]<br/>b_dim = layers_dims[l]<br/># Create temp var to be used in slicing parameters vector<br/>temp_dim = k + w_dim<br/># add parameters to the dictionary<br/>parameters["W" + str(l)] = vector[ k:temp_dim].reshape(layers_dims[l], layers_dims[l - 1]) <br/>parameters["b" + str(l)] = vector[ temp_dim:temp_dim + b_dim].reshape(b_dim, 1)<br/>k += w_dim + b_dim<br/>return parameters</span><span id="8c75" class="mf mg iq la b gy ml mi l mj mk">def gradients_to_vector(gradients):<br/># Get the number of indices for the gradients to iterate over valid_grads = [key for key in gradients.keys() if not key.startswith("dA")]<br/>L = len(valid_grads)// 2<br/>count = 0<br/># Iterate over all gradients and append them to new_grads list<br/>for l in range(1, L + 1):<br/>if count == 0:<br/>new_grads = gradients["dW" + str(l)].reshape(-1, 1)<br/>new_grads = np.concatenate((new_grads, gradients["db" + str(l)].reshape(-1, 1)))<br/>else:<br/>new_grads = np.concatenate((new_grads, gradients["dW" + str(l)].reshape(-1, 1)))<br/>new_grads = np.concatenate( (new_grads, gradients["db" + str(l)].reshape(-1, 1)))<br/>count += 1<br/>return new_grads</span></pre><p id="b247" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，我们将编写梯度检查函数，它将计算解析梯度和数值梯度之间的差异，并告诉我们反向传播的实现是否正确。我们将随机选择一个例子来计算差异。</p><pre class="ll lm ln lo gt mb la mc md aw me bi"><span id="a179" class="mf mg iq la b gy mh mi l mj mk">def forward_prop_cost(X, parameters, Y, hidden_layers_activation_fn="tanh"):<br/># Compute forward prop<br/>AL, _ = L_model_forward(X, parameters, hidden_layers_activation_fn) # Compute cost<br/>cost = compute_cost(AL, Y)<br/>return cost </span><span id="7e4c" class="mf mg iq la b gy ml mi l mj mk">def gradient_check( parameters, gradients, X, Y, layers_dims, epsilon=1e-7, hidden_layers_activation_fn="tanh"):<br/># Roll out parameters and gradients dictionaries<br/>parameters_vector = dictionary_to_vector(parameters) gradients_vector = gradients_to_vector(gradients)<br/># Create vector of zeros to be used with epsilon<br/>grads_approx = np.zeros_like(parameters_vector)<br/>for i in range(len(parameters_vector)):<br/># Compute cost of theta + epsilon<br/>theta_plus = np.copy(parameters_vector)<br/>theta_plus[i] = theta_plus[i] + epsilon<br/>j_plus = forward_prop_cost( X, vector_to_dictionary(theta_plus, layers_dims), Y, hidden_layers_activation_fn)<br/># Compute cost of theta - epsilon<br/>theta_minus = np.copy(parameters_vector)<br/>theta_minus[i] = theta_minus[i] - epsilon<br/>j_minus = forward_prop_cost( X, vector_to_dictionary(theta_minus, layers_dims), Y, hidden_layers_activation_fn)<br/># Compute numerical gradients<br/>grads_approx[i] = (j_plus - j_minus) / (2 * epsilon)</span><span id="95f2" class="mf mg iq la b gy ml mi l mj mk"># Compute the difference of numerical and analytical gradients numerator = norm(gradients_vector - grads_approx)<br/>denominator = norm(grads_approx) + norm(gradients_vector)<br/>difference = numerator / denominator</span><span id="745c" class="mf mg iq la b gy ml mi l mj mk">if difference &gt; 10e-7:<br/>print ("\033[31mThere is a mistake in back-propagation " +\ "implementation. The difference is: {}".format(difference))<br/>else:<br/>print ("\033[32mThere implementation of back-propagation is fine! "+\ "The difference is: {}".format(difference))</span><span id="fbfd" class="mf mg iq la b gy ml mi l mj mk">return difference</span><span id="ed23" class="mf mg iq la b gy ml mi l mj mk"># Set up neural network architecture<br/>layers_dims = [X_train.shape[0], 5, 5, 1]</span><span id="c3c4" class="mf mg iq la b gy ml mi l mj mk"># Initialize parameters parameters = initialize_parameters(layers_dims)</span><span id="9d62" class="mf mg iq la b gy ml mi l mj mk"># Randomly selecting 1 example from training data<br/>perms = np.random.permutation(X_train.shape[1])<br/>index = perms[:1]</span><span id="5fbb" class="mf mg iq la b gy ml mi l mj mk"># Compute forward propagation<br/>AL, caches = L_model_forward(X_train[:, index], parameters, "tanh") </span><span id="0121" class="mf mg iq la b gy ml mi l mj mk"># Compute analytical gradients<br/>gradients = L_model_backward(AL, y_train[:, index], caches, "tanh") </span><span id="5b83" class="mf mg iq la b gy ml mi l mj mk"># Compute difference of numerical and analytical gradients difference = gradient_check(parameters, gradients, X_train[:, index], y_train[:, index], layers_dims)</span><span id="6682" class="mf mg iq la b gy ml mi l mj mk">There implementation of back-propagation is fine! The difference is: 3.0220555297630148e-09</span></pre><p id="6a71" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">恭喜你！我们的实现是正确的:)</p><h1 id="1632" class="mm mg iq bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">结论</h1><p id="bc1e" class="pw-post-body-paragraph jy jz iq ka b kb nj kd ke kf nk kh ki kj nl kl km kn nm kp kq kr nn kt ku kv ij bi translated">以下是一些关键要点:</p><ol class=""><li id="b66c" class="lb lc iq ka b kb kc kf kg kj ld kn le kr lf kv ma lh li lj bi translated">双侧数值梯度比右侧形式更接近解析梯度。</li></ol><p id="b9f3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2.因为梯度检查非常慢:</p><ul class=""><li id="a226" class="lb lc iq ka b kb kc kf kg kj ld kn le kr lf kv lg lh li lj bi translated">将它应用到一个或几个训练例子中。</li><li id="bd6b" class="lb lc iq ka b kb no kf np kj nq kn nr kr ns kv lg lh li lj bi translated">在确定反向传播的实现是正确的之后，在训练神经网络时关闭它。</li></ul><p id="b367" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">3.应用退出方法时，梯度检查不起作用。使用keep-prob = 1检查梯度检查，然后在训练神经网络时更改它。</p><p id="ddda" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">4.ε= 10e-7是用于分析梯度和数值梯度之差的常用值。如果差值小于10e-7，则反向传播的实现是正确的。</p><p id="d1f0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">5.感谢Tensorflow和Pytorch等<em class="lw">深度学习</em>框架，我们可能会发现自己很少实现反向传播，因为这样的框架为我们计算；然而，要成为一名优秀的深度学习实践者，了解幕后发生的事情是一个很好的实践。</p><p id="457b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">创建这篇文章的源代码可以在<a class="ae kw" href="http://(https://github.com/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Gradient-Checking.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><p id="cd93" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="lw">原载于2018年4月8日</em><a class="ae kw" href="https://imaddabbura.github.io/posts/coding-nn/gradient-checking/Coding-Neural-Network-Gradient-Checking.html" rel="noopener ugc nofollow" target="_blank"><em class="lw">imaddabbura . github . io</em></a><em class="lw">。</em></p></div></div>    
</body>
</html>