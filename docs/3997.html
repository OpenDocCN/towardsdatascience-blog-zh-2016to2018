<html>
<head>
<title>[ NVIDIA / Paper Summary ] Stochastic Layer-Wise Precision in Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[ NVIDIA /论文摘要]深度神经网络中的随机分层精度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nvidia-paper-summary-stochastic-layer-wise-precision-in-deep-neural-networks-882c43de4526?source=collection_archive---------7-----------------------#2018-07-08">https://towardsdatascience.com/nvidia-paper-summary-stochastic-layer-wise-precision-in-deep-neural-networks-882c43de4526?source=collection_archive---------7-----------------------#2018-07-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/5c0832eb2a29cefdb576132b250e98c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*9nUXYO-NYiQOe_11co-mUw.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://media.giphy.com/media/IYgMHLSLoWtvW/giphy.gif" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="52be" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我的一个朋友向我推荐了这篇论文，我觉得它很有趣。</p><blockquote class="kx ky kz"><p id="582e" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated"><strong class="kb ir">请注意，这篇帖子是为了我未来的自己复习这篇论文上的材料，而不是从头再看一遍。</strong></p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="bcee" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">摘要</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/4a54228f36ba54d913c0d77820a4bca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*UoMjYeq5eo3vtE5BOJAOjQ.png"/></div></figure><p id="3838" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">低精度权重已经被提出作为使深度神经网络更有记忆效率以及对敌对攻击更鲁棒的方法。并且许多网络是通过跨网络的所有层的统一精度来实现的，本文的作者引入了一种学习方案，其中 DNN 通过学习随机地探索多个精度配置，因此网络能够学习最优精度配置。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="3ca2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">简介</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ls"><img src="../Images/0bf25bf72e72dde2decb6ccb0153b16c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uu1KZS5--Ww8mv0R3RQ07g.png"/></div></div></figure><p id="8cea" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">由于最近在深度学习方面的进步，许多与计算机视觉和自然语言处理(以及更多)相关的领域都有了很大的突破。然而，训练这些网络通常是昂贵的，因此对 GPU 的需求很高。(比如像网上的 CNN)。许多旨在减小网络规模的研究集中在使权重为二进制或三进制权重和激活。(这占用更少的存储器，并且可以通过位操作实现计算效率。).在本文中，作者引入了训练网络时的一个额外的超参数，即每例精度的配置。(网络将通过 back prop 找到最佳点。)</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="f04b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">相关工作</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/91110c499a244d1b5e1c3facd043f3fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*RehwMKk8tWXBKoGRiZNZpg.png"/></div></figure><p id="1f07" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">有两种方法来减小网络的规模，1)网络压缩，其中预训练的网络被最小化，同时不妨碍准确性，2)网络修剪技术。(这两种方法各有优劣。)作者的工作主要集中在降低网络中权/激活的精度，本文的三个贡献是..<br/> <strong class="kb ir"> <em class="la"> 1)确认低精度运算的总位数和加速比之间的线性关系。<br/> 2)引入基于梯度的学习精度方法<br/> 3)实证证明作者端到端训练策略的优势</em> </strong></p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="e6fb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">高效低精度网络</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ly"><img src="../Images/44bc641470dc08cdb091eda36e822a73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NFkw0N-ylFivF4pJzCOMdQ.png"/></div></div></figure><p id="371d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">低精度学习是一个过程，其中存储在浮点 32 中的权重减少到 1/2 位整数值。(而且大部分研究都集中在量化权重和激活上。)一般来说，量化会导致较低的精度，然而，情况并非总是如此，量化可以作为正则化，并且实际上实现更好的性能。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi lz"><img src="../Images/bcbee2310bc70023352fcde29a70c61e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cTb_EstS6dX4zTZE1ezjOw.png"/></div></div></figure><p id="0e51" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，由于使用了较少的比特数，我们能够获得更高的速度。看到这个数字，人们自然会问，网络的最佳精度是多少？本文作者对此问题进行了研究。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="3e60" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">学习精度</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ma"><img src="../Images/ba4ff4aed291e194a1bed9139ba022ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TXtjmzlPkz0Ksy9WGO_NWg.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">(e.g. 444444 indicates 6 quantized layers, all assigned 4 bits).</figcaption></figure><p id="7e3d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，当每一层都统一调整其精度时，我们可以看到该模型能够实现最低的误差。然而，当涉及到学习最佳精度的两个问题是…</p><p id="df9a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">1)如果对最大精度没有限制，网络将只会增长，因为它导致更低的损耗<br/>。2)量化涉及离散运算，其是不可微分的，因此不适合简单的反向传播。</p><p id="624a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">第一个问题通过固定总净精度来解决，第二个问题通过采用能够通过离散随机操作反向传播梯度的技术来解决。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ls"><img src="../Images/aa14b1498c5e084bd32fb70405acc35c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6r2vb5AaL5dcvVRgvpP0qg.png"/></div></div></figure><p id="e970" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">由于量化过程包括离散化连续值，为了成功地训练网络，需要一些数学技巧。(注意:在使用 Gumbel-Softmax(也称为具体分布)的这一节中，实际上涉及了相当多的数学计算，但是，我不打算详细讨论)。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="f9ed" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">精确分配层</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mb"><img src="../Images/df6e425281ca4d9d06b2c21858f1fa82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q0JnrCcEd4UK0PAMncvbfQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Example training Process</figcaption></figure><p id="0f0a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">为了实现精确分配网络，作者引入了一种新的层，其中存在一个可学习的超参数 pi，它定义了 Gumbel-Softmax 分布。(并且每个分布与每个不同的层相关联。).作者指出，在训练的开始阶段允许温度降得太低会导致高方差梯度，同时还会鼓励很大程度上不均匀的比特分配，从而损害网络的泛化能力。为了解决这个问题，作者们对精度位进行了硬赋值。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="047e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">实验</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mc"><img src="../Images/9deb47fbb10d6c2619c7524cab6f5214.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kKWf7fZroULigmdGYgs3Sg.png"/></div></div></figure><p id="7dd0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">本文作者在 MNIST 和 ILSVRC12 两个数据集上尝试了两种不同的 CNN 网络。本文作者比较了均匀分配网络权值的情况和通过反向传播学习精度的情况。如上所述，当网络本身能够学习精度时，它会提供更好的性能。训练错误如下所示。(对于 MNIST 数据集)</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi md"><img src="../Images/4df7051456fd26183b5dcb7612683ac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*aqLDFF-mtRt8vMKt1we62Q.png"/></div></figure><p id="e743" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">作者发现一个有趣的事实是，网络分配更多的比特给更后面的层。(并且对于早期层，仅分配几个比特。)</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="4f47" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结论与未来工作</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ma"><img src="../Images/6c8fb39539a31596755305f57bee6bfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bhGzYjk92ePZGozBNLk3Tg.png"/></div></div></figure><p id="6dc1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">本文作者介绍了 DNN 的精度分配层，并表明它比均匀分配的低精度模型具有更好的性能。(这是通过网络的能力来实现的，网络能够通过 back prop 找到最佳精度。).作者写道，他们将把这个想法扩展到对抗性攻击以及所提出的模型的变体。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="0047" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">遗言</strong></p><p id="1a5c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我从来没有想过控制网络的精度会作为一个正则化方案。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="2cdf" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="87a5" class="me mf iq kb b kc kd kg kh kk mg ko mh ks mi kw mj mk ml mm bi translated">莱西、泰勒和阿雷比(2018 年)。深度神经网络中的随机分层精度。Arxiv.org。检索于 2018 年 7 月 8 日，来自 https://arxiv.org/abs/1807.00942<a class="ae jy" href="https://arxiv.org/abs/1807.00942" rel="noopener ugc nofollow" target="_blank"/></li></ol></div></div>    
</body>
</html>