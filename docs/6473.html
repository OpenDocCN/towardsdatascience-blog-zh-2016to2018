<html>
<head>
<title>Demystifying ‘Confusion Matrix’ Confusion</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭开“混乱矩阵”混乱的神秘面纱</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/demystifying-confusion-matrix-confusion-9e82201592fd?source=collection_archive---------4-----------------------#2018-12-15">https://towardsdatascience.com/demystifying-confusion-matrix-confusion-9e82201592fd?source=collection_archive---------4-----------------------#2018-12-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3b48" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你对混淆矩阵感到困惑，那么我希望这篇文章可以帮助你理解它！快乐阅读。</p><p id="5264" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用 UCI 钞票认证数据集来揭开混淆矩阵背后的混乱。我们将预测和评估我们的模型，并在此过程中发展我们的概念理解。也将在需要的地方提供进一步阅读的链接。</p><h1 id="c81e" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">了解数据</strong></h1><p id="9d18" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">数据集包含钞票 400x400 像素小波变换图像的属性，可以在<a class="ae lo" href="https://archive.ics.uci.edu/ml/datasets/banknote+authentication" rel="noopener ugc nofollow" target="_blank">这里</a>找到。建议读者下载数据集并跟随学习。进一步参考，可以在这里找到 Kaggle 笔记本<a class="ae lo" href="https://www.kaggle.com/ritesaluja/binaryclassification-and-modelevaluation-example/" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="c209" class="ly km iq lu b gy lz ma l mb mc"><em class="md">#Skipping the necessary Libraries import</em><strong class="lu ir"><br/>#Reading the Data File<br/></strong>df = pd.read_csv('../input/BankNote_Authentication.csv')<br/>df.head(5)</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi me"><img src="../Images/a73e9e430b61d008f24a2b8b1bc7b9bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*vUXW-mRTTKahFjVAt2gxFQ.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Sample Data (using Head)</figcaption></figure><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="e890" class="ly km iq lu b gy lz ma l mb mc"><strong class="lu ir">#To check if the data is equally balanced between the target classes<br/></strong>df['class'].value_counts()</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/280297bd6db876846c375d6598b5c6eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*bFD0X9L0wvzfZopVdHoa8A.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Target Class is balanced enough</figcaption></figure><h1 id="ea31" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">建立模型</strong></h1><p id="7a6d" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">将数据分为训练集和测试集，我们将在训练集上训练我们的模型，评估将在测试集上执行，由于简单和缺乏足够的数据，我们在这里跳过验证集。总的来说数据分为三组训练、测试和验证，这里阅读更多<a class="ae lo" rel="noopener" target="_blank" href="/train-validation-and-test-sets-72cb40cba9e7"/>。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="f179" class="ly km iq lu b gy lz ma l mb mc"><strong class="lu ir">#Defining features and target variable</strong><br/>y = df['class'] #target variable we want to predict <br/>X = df.drop(columns = ['class']) #set of required features, in this case all</span><span id="e014" class="ly km iq lu b gy mn ma l mb mc"><strong class="lu ir">#Splitting the data into train and test set </strong><br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)</span></pre><p id="f139" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们将为我们的预测建立一个简单的逻辑回归模型。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="0027" class="ly km iq lu b gy lz ma l mb mc"><strong class="lu ir">#Predicting using Logistic Regression for Binary classification </strong><br/>from sklearn.linear_model import LogisticRegression<br/>LR = LogisticRegression()<br/>LR.fit(X_train,y_train) <em class="md">#fitting the model </em><br/>y_pred = LR.predict(X_test) <em class="md">#prediction</em></span></pre><h1 id="c25c" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">模型评估</strong></h1><p id="122e" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">我们来绘制最混乱的混淆矩阵？开个玩笑，让我们有一个简单的混淆矩阵(<a class="ae lo" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn 文档</a>用于下面的代码)。</p><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/79962c231b3b6f0165fde5fed4ff6127.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*BAAk374bKlraxnJvV3_hyg.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Confusion Matrix for Binary Classification</figcaption></figure><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="6fd8" class="ly km iq lu b gy lz ma l mb mc"><strong class="lu ir">#Evaluation of Model - Confusion Matrix Plot</strong><br/>def plot_confusion_matrix(cm, classes,<br/>                          normalize=False,<br/>                          title='Confusion matrix',<br/>                          cmap=plt.cm.Blues):<br/>    <em class="md">"""</em><br/><em class="md">    This function prints and plots the confusion matrix.</em><br/><em class="md">    Normalization can be applied by setting `normalize=True`.</em><br/><em class="md">    """</em><br/>    if normalize:<br/>        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]<br/>        print("Normalized confusion matrix")<br/>    else:<br/>        print('Confusion matrix, without normalization')<br/><br/>    print(cm)<br/><br/>    plt.imshow(cm, interpolation='nearest', cmap=cmap)<br/>    plt.title(title)<br/>    plt.colorbar()<br/>    tick_marks = np.arange(len(classes))<br/>    plt.xticks(tick_marks, classes, rotation=45)<br/>    plt.yticks(tick_marks, classes)<br/><br/>    fmt = '.2f' if normalize else 'd'<br/>    thresh = cm.max() / 2.<br/>    for i, j <strong class="lu ir">in</strong> itertools.product(range(cm.shape[0]), range(cm.shape[1])):<br/>        plt.text(j, i, format(cm[i, j], fmt),<br/>                 horizontalalignment="center",<br/>                 color="white" if cm[i, j] &gt; thresh else "black")<br/><br/>    plt.ylabel('True label')<br/>    plt.xlabel('Predicted label')<br/>    plt.tight_layout()<br/><br/><br/><strong class="lu ir"># Compute confusion matrix</strong><br/>cnf_matrix = confusion_matrix(y_test, y_pred)<br/>np.set_printoptions(precision=2)<br/><br/><strong class="lu ir"># Plot non-normalized confusion matrix</strong><br/>plt.figure()<br/>plot_confusion_matrix(cnf_matrix, classes=['Forged','Authorized'],<br/>                      title='Confusion matrix, without normalization')</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/a2ced8e2b0c74534acc93c1b0383db98.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*onA1BafYfvgG7gPCIZANVA.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Confusion Matrix for the Binary Classification performed</figcaption></figure><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="94a0" class="ly km iq lu b gy lz ma l mb mc"><strong class="lu ir">#extracting true_positives, false_positives, true_negatives, false_negatives</strong><br/>tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()<br/>print("True Negatives: ",tn)<br/>print("False Positives: ",fp)<br/>print("False Negatives: ",fn)<br/>print("True Positives: ",tp)</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mq"><img src="../Images/22a39e2e5e802680de48162c36b3dc03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mFfB-K25tv5KLpcNuwI38A.png"/></div></div></figure><p id="1e29" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">我们的模型有多精确？</strong></p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="ae68" class="ly km iq lu b gy lz ma l mb mc"><strong class="lu ir">#Accuracy</strong><br/>Accuracy = (tn+tp)*100/(tp+tn+fp+fn) <br/>print("Accuracy {:0.2f}%:",.format(Accuracy))</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mv"><img src="../Images/60f0d35768079be9885ec239554329c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ebie2TMGSmN6JgLKTe2_xA.png"/></div></div></figure><p id="bbc3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">准确性重要吗？</strong></p><p id="a411" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不总是这样，有时这可能不是正确的衡量标准，尤其是当你的目标阶层不平衡时(数据是有偏差的)。然后，您可能会考虑其他指标，如精确度、召回率、F 分数(组合指标)，但在深入讨论之前，让我们先退后一步，理解构成这些指标基础的术语。</p><p id="7168" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">一些基本术语</strong></p><p id="b99f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="md">真阳性</em> </strong> —被预测为阳性(在我们的场景中是认证的钞票)并且实际上是阳性(即属于阳性‘授权’类)的标签。</p><p id="ff5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="md">真阴性</em> </strong> —被预测为阴性(在我们的场景中是伪造的钞票)而实际上是阴性(即属于阴性‘伪造’类)的标签。</p><p id="e48b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="md">假阳性</em> </strong> —被预测为阳性，但实际上是阴性的标签，或者简单地说，被我们的模型错误地预测为真实，但实际上是伪造的票据。在假设检验中，它也被称为 1 型错误或对零假设的不正确拒绝，<a class="ae lo" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/" rel="noopener ugc nofollow" target="_blank">参考此</a>阅读更多关于假设检验的信息。</p><p id="4912" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="md">假阴性</em> </strong> —预测为阴性，但实际为阳性的标签(预测为伪造的真实票据)。这也被称为第二类错误，它导致拒绝零假设的失败。</p><p id="a7c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们看看每个机器学习从业者应该知道的最常见的评估指标！</p><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/1daefdd0574445c5f02f2ed51a4eddce.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*B0UVzVktFU9uQ1t6_hfN-A.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Mathematical Definitions (Formulas)</figcaption></figure><h1 id="7c6e" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">指标超出准确度</strong></h1><p id="95e1" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir">精度</strong></p><p id="1b6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它是“精确性”，即模型只返回相关实例的能力。如果您的用例/问题陈述涉及最小化误报，即在当前场景中，如果您不希望伪造的纸币被模型标记为真实的，那么精确度是您需要的。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="5df2" class="ly km iq lu b gy lz ma l mb mc"><strong class="lu ir">#Precision </strong><br/>Precision = tp/(tp+fp) <br/>print("Precision {:0.2f}".format(Precision))</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mq"><img src="../Images/808ba5fb2d57e7d4f6b0acc55f0684bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2gQjmn9mJxswF5mghC2zg.png"/></div></div></figure><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/829b6a6b5b33da61ff82407d579635d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*BSv1c0QCtCiqrDeJKap7uw.jpeg"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Precision is about Repeatability &amp; Consistency</figcaption></figure><p id="8dd6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">召回</strong></p><p id="67fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它是“完整性”，模型识别所有相关实例的能力，真正的阳性率，也称为敏感度。在目前的情况下，如果你的重点是最大限度地减少假阴性，也就是说，你不会把真钞错误地归类为伪钞，那么召回就能帮你一把。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="ed84" class="ly km iq lu b gy lz ma l mb mc"><strong class="lu ir">#Recall</strong> <br/>Recall = tp/(tp+fn) <br/>print("Recall {:0.2f}".format(Recall))</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi my"><img src="../Images/1e589d63a1a60184bf9e08a045eb66e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rwNE9wER1z2lQ-ZiEbKGw.png"/></div></div></figure><p id="3471" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> F1 测量</strong></p><p id="5463" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">精确度和召回率的调和平均值，用于表示精确度和召回率之间的平衡，提供相等的权重，范围从 0 到 1。F1 分数在 1 时达到最佳值(完美的精确度和召回率)，在 0 时达到最差值，在此阅读更多<a class="ae lo" href="https://stats.stackexchange.com/questions/49226/how-to-interpret-f-measure-values" rel="noopener ugc nofollow" target="_blank"/>。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="006d" class="ly km iq lu b gy lz ma l mb mc"><strong class="lu ir">#F1 Score</strong><br/>f1 = (2*Precision*Recall)/(Precision + Recall)<br/>print("F1 Score {:0.2f}".format(f1))</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mz"><img src="../Images/af8bc15776890c0d4800adc30f9d5054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*onaPadRK7HiErPJaHsa-XA.png"/></div></div></figure><p id="9bf5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> F-beta 测量</strong></p><p id="b0cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是 F 值的一般形式——β0.5 和 2 通常用作度量，0.5 表示倾向于精确，而 2 表示倾向于回忆，给予它两倍于精确的权重。</p><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/ec623067f70e8dad4badeaaee90376a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*5HgVbqXE3tejNUNI3tnoNg.png"/></div></figure><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="6feb" class="ly km iq lu b gy lz ma l mb mc"><strong class="lu ir">#F-beta score calculation</strong><br/>def fbeta(precision, recall, beta):<br/>    return ((1+pow(beta,2))*precision*recall)/(pow(beta,2)*precision + recall)<br/>            <br/>f2 = fbeta(Precision, Recall, 2)<br/>f0_5 = fbeta(Precision, Recall, 0.5)<br/><br/>print("F2 {:0.2f}".format(f2))<br/>print("\nF0.5 {:0.2f}".format(f0_5))</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mv"><img src="../Images/bd31e25ae8b2493c7f4de16e17b94f94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KEZiuQwDkwWoB6FGHA98GQ.png"/></div></div></figure><p id="5ac9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">特异性</strong></p><p id="bb44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它也被称为“真阴性率”(正确识别的实际阴性的比例)，即数据包含的真阴性越多，其特异性就越高。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="b436" class="ly km iq lu b gy lz ma l mb mc"><strong class="lu ir">#Specificity </strong><br/>Specificity = tn/(tn+fp)<br/>print("Specificity {:0.2f}".format(Specificity))</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nb"><img src="../Images/b2924865a363975d2bad0f4f229a9089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xPIP0fxNOCOzXHfJU8RYRQ.png"/></div></div></figure><p id="46ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> ROC(受试者工作特性曲线)</strong></p><p id="fd7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不同分类阈值下“真阳性率”(灵敏度/召回率)与“假阳性率”(1-特异性)的关系图。</p><p id="28c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ROC 曲线下面积(AUC)测量曲线下的整个二维面积。这是一个衡量参数如何区分两个诊断组的指标。通常用作分类模型质量的度量。</p><p id="812a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随机分类器的曲线下面积为 0.5，而完美分类器的 AUC 等于 1。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="9738" class="ly km iq lu b gy lz ma l mb mc"><strong class="lu ir">#ROC</strong><br/>import scikitplot as skplt #to make things easy<br/>y_pred_proba = LR.predict_proba(X_test)<br/>skplt.metrics.plot_roc_curve(y_test, y_pred_proba)<br/>plt.show()</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/3285e015c5abf6da12cd2a4118fb6ac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*1C5VleXvE3Qz08pYwcjxfg.png"/></div></figure><h1 id="a666" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">结论</strong></h1><p id="022e" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">由于选择用来说明混淆矩阵和相关指标的使用的问题很简单，您可以找到更高水平(98%或以上)的每个值，无论是精确度、召回率还是准确度；通常情况下不会这样，您将需要关于数据的领域知识来在一个度量或另一个度量(通常是度量的组合)之间进行选择。</p><p id="f1b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如:如果是关于发现“你邮箱里的垃圾邮件”，你的模型的<em class="md">高精度</em>将是非常重要的(因为你不希望这个邮件被贴上垃圾邮件的标签)，它将告诉我们被我们归类为垃圾邮件的邮件中有多少是垃圾邮件。真阳性(分类为垃圾邮件的单词，实际上是垃圾邮件)与所有阳性(分类为垃圾邮件的所有单词，无论其分类是否正确)的比率。在欺诈检测中，您可能希望您的<em class="md">回忆</em>更高，以便您可以正确地分类/识别欺诈，即使您没有将一些非欺诈活动分类为欺诈，也不会造成任何重大损失。</p></div></div>    
</body>
</html>