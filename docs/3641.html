<html>
<head>
<title>Implement Gradient Descent in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Python 中实现梯度下降</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implement-gradient-descent-in-python-9b93ed7108d1?source=collection_archive---------0-----------------------#2018-06-03">https://towardsdatascience.com/implement-gradient-descent-in-python-9b93ed7108d1?source=collection_archive---------0-----------------------#2018-06-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="0b26" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">什么是梯度下降？</strong></h1><p id="787f" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这是一种寻找函数最小值的优化算法。我们从函数上的一个随机点开始，沿着函数的<strong class="kn ir">梯度的<strong class="kn ir">负方向</strong>移动，到达<strong class="kn ir">局部/全局最小值</strong>。</strong></p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/78c0f8d2103221b201bb8fa823c63209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*Wj0IlRPf3cSL-k-_WLy8hQ.gif"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Homer descending !</figcaption></figure><h1 id="e057" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">手动示例:</strong></h1><p id="85e4" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ir">问题</strong>:求从点 x=3 开始的函数 y=(x+5)的局部极小值</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lv"><img src="../Images/0c65f52e984e93bf9d99fc597ebe61ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5-56UEwcZHgzqIAtlnsLog.png"/></div></div></figure><p id="614a" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated"><strong class="kn ir">解法:</strong>我们只要看图就知道答案了。当 x = -5 时，y = (x+5)达到最小值(即当 x=-5，y=0 时)。因此 x=-5 是函数的局部和全局最小值。</p><p id="05e2" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">现在，让我们看看如何使用梯度下降获得相同的数值。</p><p id="dcbc" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated"><strong class="kn ir">第一步</strong>:初始化 x =3。然后，求函数的梯度，dy/dx = 2*(x+5)。</p><p id="7c25" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated"><strong class="kn ir">第二步</strong>:向渐变的负方向移动(<a class="ae mf" href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/why-the-gradient-is-the-direction-of-steepest-ascent" rel="noopener ugc nofollow" target="_blank">为什么？</a>)。但是等等，要移动多少？为此，我们需要一个学习率。让我们假设<strong class="kn ir">学习率→ 0.01 </strong></p><p id="493f" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated"><strong class="kn ir">步骤 3 </strong>:让我们执行 2 次梯度下降迭代</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mg"><img src="../Images/df0258c2b7001e623df7c9d46e3fd2d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YkU1u_Px_FprYjKL1xtUwg.png"/></div></div></figure><p id="b834" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated"><strong class="kn ir">第四步</strong>:我们可以观察到 X 值在慢慢减小，应该收敛到-5(局部最小值)。然而，我们应该执行多少次迭代呢？</p><p id="dade" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">让我们在算法中设置一个精度变量，用于计算两个连续“x”值之间的差值。如果连续两次迭代的 x 值之差小于我们设置的精度，停止算法！</p><h1 id="5a9a" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">Python 中的渐变下降:</strong></h1><p id="e820" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ir">第一步</strong>:初始化参数</p><pre class="lk ll lm ln gt mh mi mj mk aw ml bi"><span id="45e9" class="mm jo iq mi b gy mn mo l mp mq">cur_x = 3 # The algorithm starts at x=3<br/>rate = 0.01 # Learning rate<br/>precision = 0.000001 #This tells us when to stop the algorithm<br/>previous_step_size = 1 #<br/>max_iters = 10000 # maximum number of iterations<br/>iters = 0 #iteration counter<br/>df = lambda x: 2*(x+5) #Gradient of our function </span></pre><p id="098f" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated"><strong class="kn ir">步骤 2 </strong>:运行一个循环执行梯度下降:</p><p id="c38f" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">I .当两次连续迭代的 x 值之差小于 0.000001 或迭代次数超过 10，000 时，停止循环</p><pre class="lk ll lm ln gt mh mi mj mk aw ml bi"><span id="4292" class="mm jo iq mi b gy mn mo l mp mq">while previous_step_size &gt; precision and iters &lt; max_iters:<br/>    prev_x = cur_x #Store current x value in prev_x<br/>    cur_x = cur_x - rate * df(prev_x) #Grad descent<br/>    previous_step_size = abs(cur_x - prev_x) #Change in x<br/>    iters = iters+1 #iteration count<br/>    print("Iteration",iters,"\nX value is",cur_x) #Print iterations<br/>    <br/>print("The local minimum occurs at", cur_x)</span></pre><p id="7753" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated"><strong class="kn ir">输出</strong>:从下面的输出中，我们可以观察到前 10 次迭代的 x 值——这可以与我们上面的计算进行交叉检查。该算法在终止之前运行 595 次迭代。下面嵌入了代码和解决方案以供参考。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/e7cffee0f6ea6d593da3063a0b7440dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*5xzAFDVNNOrIR5sPlTZOWg.png"/></div></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><p id="5d1a" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">在<a class="ae mf" href="https://www.linkedin.com/in/rohan-joseph-b39a86aa/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上连接。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mz na l"/></div></figure></div></div>    
</body>
</html>