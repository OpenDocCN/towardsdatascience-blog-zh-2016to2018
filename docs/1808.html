<html>
<head>
<title>Going Sideways in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在神经网络中走向侧面</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/going-sideways-in-neural-networks-7e15b3c4cc63?source=collection_archive---------2-----------------------#2017-10-26">https://towardsdatascience.com/going-sideways-in-neural-networks-7e15b3c4cc63?source=collection_archive---------2-----------------------#2017-10-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="ccbe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经网络有几个致命的问题。其中最主要的是过度拟合——给定足够的训练时间，神经网络将准确预测训练数据，同时失去理解新数据的能力。它完全不能概括它所学到的东西。例如，一个图像分类器最终可以完美地预测它是否正在看一只猫的照片，在你已经手动分类的图像中<em class="kl"/>。然而，这种特殊性使得对所有<em class="kl">不完全</em>像它所学习的图像的分类成为一场掷硬币的游戏。图像分类器已经学会记忆答案。</p><p id="d4ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">研究人员通过提前停止来避免过度适应。他们根据一组不存在于训练数据中的输入<em class="kl">测试网络，这组输入</em>被称为“保持”组。网络在保留集上的精确度是对网络在新数据<em class="kl">上表现如何的估计。一旦这个坚持组的准确性变得更差，研究人员就停止训练网络。网络已经学习了足够的知识来进行归纳，任何额外的训练都会导致记忆(即过拟合训练数据)。</em></p><p id="a23c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从根本上说，我们不想要一个与我们的训练数据完全匹配的网络。提前停止是一种组装，一种补偿研究人员优化错误事物的方式。我们<strong class="jp ir">想要</strong>一个能够<strong class="jp ir">归纳</strong>的网络，这样它就可以预测<em class="kl">不完全</em>像它以前看到的事情。然而，我们优化了网络的成本函数，该网络仅准确预测了它所看到的。我们有错误的成本函数。</p><p id="d85f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">出路</strong></p><p id="06f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想象成本函数的最小值是碗的中心。优化类似于把球滚下碗，直到它停在中间。然而，通过提前停止，我们希望我们的球滚到靠近中心的<em class="kl">点，而不是正好在中心</em>的<em class="kl">点。当球滚下碗边时，它提高了预测的准确性。在接近底部的某个地方，网络对陌生输入的准确性开始恶化。网络开始记忆，而不是归纳。我们在某一点上停止了球，这就是提前停止。</em></p><p id="04bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">重要的是要认识到，如果我们多次滚动球，并找到我们提前停止的位置(没有记忆就概括的位置)，<strong class="jp ir">所有这些提前停止的点形成一个围绕碗中心</strong>的<em class="kl">连续环</em>。我们甚至可以<em class="kl">把碗</em>切割成这个更小的环！每一个“好的概括”网络都将是碗唇上的<strong class="jp ir"/>，而任何进一步进入碗中的<strong class="jp ir"/>都将是一个“记忆陷阱”网络。对于这个更小的碗，<em class="kl">我们不想再滚下</em>——我们应该<em class="kl">而不是</em>顺着斜坡的方向；我们不应该使用梯度下降！</p><p id="5025" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相反，我们希望<strong class="jp ir">沿着这个碗</strong>的边缘迁移，停留在“良好概括”网络的边缘，<strong class="jp ir">永远不要偏离到所有“记忆陷阱”所在的碗</strong>的中心。重要的一点是，碗的某些面可能会很早就开始记忆，因为他们从一个糟糕的概括开始。我们想在碗的边缘走来走去，尝试不同的‘好的概括’，<strong class="jp ir">因为其中一些概括可能比其他的更好</strong>！沿着边缘行进是找到更好的概括的最好方法——远胜于将球滚向不同的方向数百次，摆弄初始化！</p><p id="b92f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">横着走</strong></p><p id="99d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从数学上来说，我们如何像那样沿着边缘滚动？</p><p id="49b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简单—我们垂直于梯度移动。梯度向量是我们碗中最陡下降的方向；碗的一些区域凸起或起皱，最陡的方向并不总是朝着中心。然而，无论最陡的方向是什么，边缘总是垂直于它！在更高维度中，那个边缘仍然是垂直的，尽管有许多垂直的方向。轮圈为<em class="kl">子空间</em>。并且，通过测量网络在保留输入上的准确性，我们有了在这个子空间中进行优化的度量。</p><p id="bee2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，当你第一次训练网络并在泛化时提前停止时，你然后运行<strong class="jp ir">新的优化</strong>:最小化子空间边缘上的损失函数，该损失函数与梯度正交<em class="kl">，其中损失是网络在保持集</em>上的<em class="kl">不准确性。在每一步，你都要测量<em class="kl">训练集</em>的倾斜度，就像常规的 SGD 一样，但是你要移动<em class="kl">垂直于那个倾斜度</em>。高维空间中有许多垂直方向，因此<strong class="jp ir">您为<em class="kl">保持输入</em> </strong>选择梯度最大的方向！你实际上是在说“我不想记住我的训练数据；我只想改进<em class="kl">这个新数据</em>。”</em></p><p id="d372" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那不会导致对隐藏数据的记忆吗？没错。该子空间边缘遭受相同的过度配合问题。<strong class="jp ir">沿篮筐移动，你保证不会记住旧的训练数据，但你沿篮筐的移动<em class="kl">可以记住保持数据</em> </strong>。轮辋需要自己的早停，创造自己的碗轮辋！你的<em class="kl">新的</em>边缘是一个更小的子空间，它被约束为与训练数据的梯度<em class="kl">和与保持数据的梯度</em>正交。有了许多坚持的设定，这可能会永远继续下去…这实际上是计划！</p><p id="a47d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">抵制的层次</strong></p><p id="bbaa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设你有一百万张图片，一半是猫，一半是其他的东西，都有正确的标签。你可以给你的神经网络 80 万张这样的图片用于训练，保留 10 万张作为确定提前停止的保留集，另外 10 万张作为测量预期真实世界准确度的验证集。那是正常的做事方式。</p><p id="4bde" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，有了对碗状边缘和提前停止的这种认识，一种不同的方法出现了:将图像分块到箱中，每个箱有 100k 张图像；在<em class="kl">第一个</em> 100k 图像上训练您的网络，使用<em class="kl">第二个</em> 100k 作为确定提前停止的保留；当撑出精度开始下降时，及早停止；将<em class="kl">垂直于<strong class="jp ir">训练数据</strong>的坡度，并在<strong class="jp ir">保持数据</strong>的坡度的<em class="kl">方向</em>上移动，从而移动轮辋；使用第三个 100k 作为支撑，以确定这一新梯度的早期停止。</em></p><p id="8c4f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">重复这一过程，仔细消化 10 万批新数据，而不去记忆旧数据。通过沿着每个连续箱的边缘移动，我们正在搜索一个越来越小的子空间。每个箱子都有自己的限制，说“不要沿着我的梯度移动——那只会导致记忆”。结合所有这些限制，我们沿着碗的边缘缩小了可用的方向，使得搜索更容易。(但是，当我们沿着边缘移动一步时，我们仍然需要检查我们的新位置是否对应于一个提前停止点。这意味着我们检查训练数据的梯度，并查看在该方向上的移动是否会导致保持精度下降。)</p><p id="c9eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">一切都是坚持</strong></p><p id="0052" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在具有一百万个图像的示例中，我们形成了 100k 个图像的连续箱，并且迭代梯度下降、提前停止和正交子空间。如果我们把这些箱子做得更小，每个箱子只有 100 张图片，会怎么样？或者，如果每个图像都是自己的 bin，并且可以随时添加新的图像，会怎么样？这就是<em class="kl">在线学习</em>——当每个新图像到达时，网络执行额外的训练，而不牺牲先前的归纳！传统的网络是在实验室中训练出来的，不能从新的经验中学习。通过迭代正交梯度技巧，我们的“rim”网络可以在操作期间继续学习<em class="kl">。(DARPA 正在资助对人工智能的研究，这种研究可以边进行边学习，所以这可能很重要。)</em></p><p id="a6ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这可能有助于拟人化这个正交的把戏，它在“滚下碗”和“沿着边缘滚”之间摆动。网络在碗的梯度方向(最陡的方向)上滚动一小段距离，询问“如果我以这种方式改变一点，我对新信息的准确性会不会降低？”如果精度<strong class="jp ir">提高</strong>，网络向那个方向移动，这是普通的梯度下降。然而，如果新信息的准确性下降，网络会对自己说“如果我从碗的最陡部分往下走，我会失去一般性，并开始记忆；沿着边缘的哪个方向，<em class="kl">垂直于最陡方向</em>，反而会提高拒绝数据的准确性？”在每一步之后，该过程重复进行。</p><p id="d443" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，每个输入都是一个拒绝集合，我们将<em class="kl">向下移动到拒绝的梯度</em>的分量，该分量与所有先前输入的梯度<em class="kl">正交</em>，遍历碗的边缘。一旦我们沿着那个边缘走了一步，我们也检查沿着<em class="kl">的一步是否所有先前输入的梯度</em>会降低保持精度(这决定我们是否已经‘远离边缘’了)。如果保持精度会增加，那么网络不在“边缘上”，所以我们沿着所有先前输入的梯度移动一步<em class="kl">；如果精度会降低，我们将搜索限制在该梯度的<em class="kl">正交子空间</em>，而不是沿着<em class="kl">保持</em>的梯度移动。每一个新的输入都会触发这一过程，学习新的数据而不失一般性。</em></p><p id="e51d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我希望这有助于我们寻找一种真正的可以终生学习的普遍智能。有诱人的证据表明，我们的大脑使用一种学习技术，这种技术相当于梯度下降的反向传播(在<a class="ae km" href="https://www.youtube.com/watch?v=FhRW77rZUS8" rel="noopener ugc nofollow" target="_blank"> 19:30 这里是</a>)——如果我们自己的大脑也使用类似于我上面描述的“边缘”搜索的方法来避免过度拟合，我不会感到惊讶。</p></div></div>    
</body>
</html>