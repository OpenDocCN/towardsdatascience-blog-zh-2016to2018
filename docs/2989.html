<html>
<head>
<title>Getting Started with PyTorch Part 1: Understanding how Automatic Differentiation works</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch入门第1部分:了解自动微分的工作原理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec?source=collection_archive---------1-----------------------#2018-03-28">https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec?source=collection_archive---------1-----------------------#2018-03-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3f43" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">更新:这个帖子是2018年初写回来的。PyTorch已经走过了漫长的道路，更新版本的帖子可以在</strong><a class="ae kl" href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir"/></a><strong class="jp ir">这里找到。</strong></p><p id="96e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我开始编写神经网络代码时，我最终使用了我周围所有人都在使用的东西。张量流。</p><p id="54a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是最近，PyTorch已经成为深度学习框架之王的主要竞争者。真正吸引人的是它的动态计算图范例。如果最后一行对你来说没有意义，也不要担心。在这篇文章的结尾，它会的。但请相信我的话，它使调试神经网络的方式更容易。</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="kr ks l"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">If you’re wondering why your energy has been low lately, switch to PyTorch!</figcaption></figure><h1 id="826b" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">先决条件</h1><p id="3ddb" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">在我们开始之前，我必须指出，你至少应该有这样的基本概念:</p><ul class=""><li id="5b7d" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">与神经网络训练相关的概念，特别是反向传播和梯度下降。</li><li id="6d38" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">应用链式法则计算导数。</li><li id="a9b1" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">Python中类的工作原理。(或者关于面向对象编程的一般概念)</li></ul><p id="59f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你错过了以上任何一个，我在文章的最后提供了链接来指导你。</p></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><p id="e07c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，是时候开始使用PyTorch了。这是PyTorch系列教程的第一篇。</p><p id="deb0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是第1部分，我将描述基本的构建模块，以及<em class="mv">亲笔签名的</em>。</p><blockquote class="mw mx my"><p id="215b" class="jn jo mv jp b jq jr js jt ju jv jw jx mz jz ka kb na kd ke kf nb kh ki kj kk ij bi translated"><strong class="jp ir">注意:</strong>需要注意的一点是，本教程是为PyTorch 0.3及更低版本制作的。提供的最新版本是0.4。我决定坚持使用0.3，因为到目前为止，0.3是在Conda和pip渠道中发布的版本。此外，开源中使用的大部分PyTorch代码还没有更新到包含0.4中提出的一些更改。然而，我会指出，在某些地方，事情在0.3和0.4不同。</p></blockquote><h1 id="b3c0" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">积木#1:张量</h1><p id="383e" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">如果你曾经用python做过机器学习，你很可能遇到过NumPy。我们使用Numpy的原因是因为它在做矩阵运算时比Python列表快得多。为什么？因为它完成了c语言中的大部分繁重工作。</p><p id="5e21" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是，在训练深度神经网络的情况下，NumPy阵列根本不能满足它。我懒得在这里做实际的计算(Google for“FLOPS in a iteration of ResNet to get a idea”)，但是仅使用NumPy数组的代码就需要几个月的时间来训练一些最先进的网络。</p><p id="2260" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这就是<strong class="jp ir">张量</strong>发挥作用的地方。PyTorch为我们提供了一种叫做<em class="mv">张量</em>的数据结构，与NumPy的<em class="mv"> ndarray非常相似。</em>但与后者不同的是，<strong class="jp ir">张量可以利用GPU的资源来显著加快矩阵运算的速度。</strong></p><p id="32c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是你做张量的方法。</p><pre class="km kn ko kp gt nc nd ne nf aw ng bi"><span id="e7d8" class="nh ky iq nd b gy ni nj l nk nl">In [1]: import torch </span><span id="27a5" class="nh ky iq nd b gy nm nj l nk nl">In [2]: import numpy as np</span><span id="576b" class="nh ky iq nd b gy nm nj l nk nl">In [3]: arr = np.random.randn((3,5))</span><span id="7083" class="nh ky iq nd b gy nm nj l nk nl">In [4]: arr<br/>Out[4]:</span><span id="e1b4" class="nh ky iq nd b gy nm nj l nk nl">array([[-1.00034281, -0.07042071,  0.81870386],<br/>       [-0.86401346, -1.4290267 , -1.12398822],<br/>       [-1.14619856,  0.39963316, -1.11038695],<br/>       [ 0.00215314,  0.68790149, -0.55967659]])</span><span id="8fe1" class="nh ky iq nd b gy nm nj l nk nl">In [5]: tens = torch.from_numpy(arr)</span><span id="086a" class="nh ky iq nd b gy nm nj l nk nl">In [6]: tens<br/>Out[6]:<br/> <br/>-1.0003 -0.0704  0.8187<br/>-0.8640 -1.4290 -1.1240<br/>-1.1462  0.3996 -1.1104<br/>0.0022  0.6879 -0.5597<br/>[torch.DoubleTensor of size 4x3]</span><span id="8652" class="nh ky iq nd b gy nm nj l nk nl">In [7]: another_tensor = torch.LongTensor([[2,4],[5,6]])</span><span id="5963" class="nh ky iq nd b gy nm nj l nk nl">In [7]: another_tensor<br/>Out[13]: </span><span id="3cb9" class="nh ky iq nd b gy nm nj l nk nl"> 2  4<br/> 5  6<br/>[torch.LongTensor of size 2x2]</span><span id="10a8" class="nh ky iq nd b gy nm nj l nk nl">In [8]: random_tensor = torch.randn((4,3))</span><span id="54c2" class="nh ky iq nd b gy nm nj l nk nl">In [9]: random_tensor<br/>Out[9]:</span><span id="0861" class="nh ky iq nd b gy nm nj l nk nl">1.0070 -0.6404  1.2707<br/>-0.7767  0.1075  0.4539<br/>-0.1782 -0.0091 -1.0463<br/> 0.4164 -1.1172 -0.2888<br/>[torch.FloatTensor of size 4x3]</span></pre><h1 id="61ed" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">构建模块#2:计算图</h1><p id="9576" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">现在，我们在商业方面的事情。当训练神经网络时，我们需要计算损失函数相对于每个权重和偏差的梯度，然后使用梯度下降来更新这些权重。</p><p id="c499" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着神经网络达到数十亿个权重，高效地执行上述步骤可以决定训练的可行性。</p><h2 id="2788" class="nh ky iq bd kz nn no dn ld np nq dp lh jy nr ns ll kc nt nu lp kg nv nw lt nx bi translated">构建模块#2.1:计算图表</h2><p id="7188" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">计算图是现代深度学习网络工作方式的核心，PyTorch也不例外。让我们先了解一下它们是什么。</p><p id="47d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设，你的模型是这样描述的:</p><pre class="km kn ko kp gt nc nd ne nf aw ng bi"><span id="48d5" class="nh ky iq nd b gy ni nj l nk nl"><em class="mv">b = w1 * a<br/>c = w2 * a <br/>d = (w3 * b) + (w4 * c)<br/>L = f(d)</em></span></pre><p id="110e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我真的画出计算图，它可能会像这样。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nz oa di ob bf oc"><div class="gh gi ny"><img src="../Images/7d3f21a9eda76ad7906fed8fbacf1a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FDL9Se9otGzz83F3rofQuA.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Computation Graph for our Model</figcaption></figure><p id="f7b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">现在</strong>，你必须注意到，上图并不完全是PyTorch在引擎盖下的图表的准确表示。然而，就目前而言，这足以证明我们的观点。</p><p id="1c91" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们可以顺序执行计算输出所需的操作时，为什么要创建这样的图形呢？</p><p id="ce27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想象一下，如果你不仅要计算输出，还要训练网络，会发生什么。你必须计算所有紫色节点标记的权重的梯度。这将需要你找到自己的方式，然后更新权重。</p><p id="39e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">计算图是一种简单的数据结构，允许您有效地应用链规则来计算所有参数的梯度。</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nz oa di ob bf oc"><div class="gh gi ny"><img src="../Images/2f85dc8fd497bd8bc01aef1aef2119a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EWpoG5KayZSqkWmwM_wMFQ.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Applying the chain rule using computation graphs</figcaption></figure><p id="5306" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里有一些需要注意的事情。首先，图中箭头的方向现在颠倒了。那是因为我们是反向传播，箭头标记的是梯度反向流动。</p><p id="9d97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二，为了这些例子，你可以把我写的渐变想象成<em class="mv">边缘权重</em>。注意，这些梯度不需要计算链式法则。</p><p id="e206" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，为了计算任意节点的<em class="mv">梯度，比如L，相对于任意其他节点，比如c ( dL / dc) </em>我们要做的就是。</p><ol class=""><li id="8065" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk of mg mh mi bi translated">追踪从L到c的路径<em class="mv"/>。这将是<em class="mv">L→d→c</em></li><li id="a70f" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk of mg mh mi bi translated">沿着这条路径遍历时，将所有的<em class="mv">边权重</em>相乘。你最终得到的数量是:(<em class="mv">dL/DD)*</em>(<em class="mv">DD/DC)=(dL/DC)</em></li><li id="340f" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk of mg mh mi bi translated">如果有多条路径，将它们的结果相加。例如，在<em class="mv"> dL/da的情况下，</em>我们有两条路径。<em class="mv"> L → d → c → a和L→d→b→a</em>我们将它们的贡献相加得到<em class="mv">L w r t a</em>的梯度</li></ol><p id="ec4f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mv"/>(<em class="mv">dL/DD)*</em>(<em class="mv">DD/DC)*</em>(<em class="mv">DC/da)】</em>+<em class="mv">[</em>(<em class="mv">dL/DD)*</em>(<em class="mv">DD/db)*</em>(<em class="mv">db/da)】</em></p><p id="b852" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">原则上，可以从<em class="mv"> L </em>开始，开始向后遍历图形，计算沿途每个节点的梯度。</p><h1 id="bafa" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">构建块#3:变量和亲笔签名</h1><p id="5692" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">PyTorch使用<em class="mv">亲笔签名的</em>包完成了我们上面描述的任务。</p><p id="9784" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，关于<em class="mv">亲笔签名的</em>是如何工作的，基本上有三件重要的事情需要理解。</p><h2 id="08bb" class="nh ky iq bd kz nn no dn ld np nq dp lh jy nr ns ll kc nt nu lp kg nv nw lt nx bi translated">构建模块#3.1:变量</h2><p id="7b0e" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated"><em class="mv">变量</em>，就像<em class="mv">张量</em>一样，是一个用来保存数据的类。然而，它的不同之处在于它的使用方式。<strong class="jp ir"> <em class="mv">变量</em>专门用于保存在神经网络训练期间变化的值，即我们网络的可学习参数。</strong>另一方面，张量用于存储不需要学习的值。例如，张量可用于存储每个示例产生的损失值。</p><pre class="km kn ko kp gt nc nd ne nf aw ng bi"><span id="8ac2" class="nh ky iq nd b gy ni nj l nk nl">from torch.autograd import Variable</span><span id="dbb6" class="nh ky iq nd b gy nm nj l nk nl">var_ex = Variable(torch.randn((4,3))   #creating a Variable</span></pre><p id="e5a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个<em class="mv">变量</em>类包装了一个张量。你可以通过调用<strong class="jp ir"> <em class="mv">来访问这个张量。数据</em>变量的</strong>属性。</p><p id="65e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mv">变量</em>也存储标量(比如说损耗)相对于它所保存的参数的梯度。这个渐变可以通过调用<strong class="jp ir"> <em class="mv">来访问。grad </em> </strong>属性。这基本上是直到这个特定节点计算的梯度，并且每个后续节点的梯度可以通过将<em class="mv">边权重</em>乘以在它之前的节点计算的梯度来计算。</p><p id="4ef8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个<em class="mv">变量</em>持有的第三个属性是一个<strong class="jp ir"> <em class="mv"> grad_fn </em> </strong>，一个<em class="mv">函数</em>对象，它创建了这个变量。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nz oa di ob bf oc"><div class="gh gi og"><img src="../Images/d6c0f5c7bc9c1b202964d226683b92fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wE1f2i7L8QRw8iuVx5mOpw.png"/></div></div></figure><blockquote class="mw mx my"><p id="8594" class="jn jo mv jp b jq jr js jt ju jv jw jx mz jz ka kb na kd ke kf nb kh ki kj kk ij bi translated"><strong class="jp ir">注:</strong> PyTorch 0.4将变量和张量类合二为一，张量可以通过一个开关做成一个“变量”而不是实例化一个新的对象。但是既然，我们在本教程中做的是v 0.3，我们就继续吧。</p></blockquote><h2 id="07b8" class="nh ky iq bd kz nn no dn ld np nq dp lh jy nr ns ll kc nt nu lp kg nv nw lt nx bi translated">构建模块#3.2:功能</h2><p id="90eb" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">我上面说<em class="mv">功能</em>了吗？它基本上是一个函数的抽象。接受输入并返回输出的东西。比如我们有两个变量，<em class="mv"> a </em>和<em class="mv"> b </em>，那么如果，</p><p id="84c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mv"> c = a + b </em></p><p id="a027" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后<em class="mv"> c </em>是一个新变量，它的<em class="mv"> grad_fn </em>是一个叫做<em class="mv"> AddBackward </em> (PyTorch内置的两个变量相加的函数)<em class="mv">，</em>这个函数以<em class="mv"> a </em>和<em class="mv"> b </em>为输入，创建了<em class="mv"> c </em>。</p><p id="f7b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么，你可能会问，既然python确实提供了定义函数的方法，为什么还需要一个全新的类呢？</p><p id="d9e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练神经网络时，有两个步骤:前向传递和后向传递。通常，如果您使用python函数来实现它，您将必须定义两个函数。一个是计算正向传递期间的输出，另一个是计算要传播的梯度。</p><p id="5edd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PyTorch将编写两个独立函数(用于向前传递和向后传递)的需求抽象成一个名为<em class="mv">torch . autograded . function .</em>T3】的类的两个函数成员</p><p id="7e95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PyTorch结合了<em class="mv">变量</em>和<em class="mv">函数</em>来创建一个计算图。</p><h2 id="6f52" class="nh ky iq bd kz nn no dn ld np nq dp lh jy nr ns ll kc nt nu lp kg nv nw lt nx bi translated">积木#3.3:亲笔签名</h2><p id="6eac" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">现在让我们深入研究PyTorch如何创建计算图。首先，我们定义变量。</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="oh ks l"/></div></figure><p id="5a86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面几行代码的结果是，</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="oh ks l"/></div></figure><p id="a9df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们分析一下刚刚到底发生了什么。如果您查看源代码，事情是这样的。</p><ul class=""><li id="440a" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated"><strong class="jp ir">定义图形的<em class="mv">叶</em>变量(第5-9行)。</strong>我们从定义一堆“变量”开始(正常的，python语言的用法，不是pytorch <em class="mv">变量</em>)。如果您注意到，我们定义的值是我们的计算图中的叶节点。只有我们必须定义它们才有意义，因为这些节点不是任何计算的结果。现在，这些家伙占用了我们Python名称空间中的内存。意味着，它们是百分之百真实的。我们<strong class="jp ir">必须</strong>将<strong class="jp ir"><em class="mv">requires _ grad</em></strong><em class="mv"/>属性设置为True，否则，这些变量将不会包含在计算图形中，并且不会为它们计算任何梯度(以及依赖于这些特定变量进行梯度流的其他变量)。</li><li id="70e7" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><strong class="jp ir">创建图表(第12-15行)</strong>。到目前为止，我们的记忆中还没有计算图之类的东西。只有叶节点，但是只要您写下第12–15行，一个图就会被动态生成<strong class="jp ir">。确定这个细节非常重要。在飞行中。</strong>当你写<em class="mv"> b =w1*a </em>时，就是图形创建开始的时候，一直持续到第15行。当从输入中计算输出时，这正是我们模型的正向传递。每个变量的<em class="mv">正向</em>函数可以缓存一些输入值，以便在计算反向传递的梯度时使用。(例如，如果我们的forward函数计算出<em class="mv"> W*x </em>，那么<em class="mv"> d(W*x)/d(W) </em>就是<em class="mv"> x </em>，需要缓存的输入)</li><li id="6a6c" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">现在，我告诉你我之前画的图不准确的原因？因为PyTorch做图的时候，并不是<em class="mv">变量</em>对象才是图的节点。它是一个<em class="mv">函数</em>对象，确切地说，是构成图形节点的每个<em class="mv">变量</em>的<em class="mv"> grad_fn </em>。PyTorch图应该是这样的。</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nz oa di ob bf oc"><div class="gh gi oi"><img src="../Images/73cf78291718fcf755a3c30a8fe35351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*40LF-3EKdsZsbTP5JmzVjQ.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Each Function is a node in the PyTorch computation graph.</figcaption></figure><ul class=""><li id="4cf0" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">我用名字表示了叶节点，但是它们也有自己的<em class="mv">grad _ fn’</em>(返回None值。这是有意义的，因为您不能反向传播到叶节点之外)。剩下的节点现在被它们的<em class="mv">grad _ fn’</em>所代替，我们看到单个节点<em class="mv"> d </em>被三个函数代替，两个乘法，一个加法，而loss被一个<em class="mv">减</em>函数代替。</li><li id="c3ca" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><strong class="jp ir">计算梯度(第18行)。</strong>我们现在通过调用<em class="mv">来计算梯度。<em class="mv"> L </em>上的backward() </em>功能。这到底是怎么回事？首先，L处的梯度就是1 ( <em class="mv"> dL / dL </em>)。<strong class="jp ir">然后，我们调用它的<em class="mv">向后</em>函数，它的基本工作是计算<em class="mv">函数</em>对象的输出到<em class="mv">函数</em>对象的输入的梯度。</strong>这里L是10 — d的结果，也就是说，逆向函数会将梯度(<em class="mv"> dL/dd) </em>计算为-1。</li><li id="52be" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">现在，这个计算的梯度乘以累积的梯度(存储在当前节点对应的<em class="mv">变量</em>的<em class="mv"> grad </em>属性中，在我们的例子中是<em class="mv"> dL/dL = 1 </em>)，然后发送到输入节点，存储在输入节点对应的变量<strong class="jp ir"> <em class="mv"> grad </em>属性中。</strong>技术上，我们所做的是应用链式法则(<em class="mv">dL/dL</em>)*(<em class="mv">dL/DD</em>)=<em class="mv">dL/DD。</em></li><li id="dba9" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">现在，让我们了解梯度是如何传播给<em class="mv">变量</em> <em class="mv"> d的。d </em>是从它的输入(w3，w4，b，c)计算出来的。在我们的图中，它由3个节点、2个乘法和1个加法组成。</li><li id="0e81" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">首先，函数<em class="mv"> AddBackward ( </em>在我们的图中表示节点<em class="mv"> d </em>的加法运算)计算它的输出(<em class="mv"> w3*b + w4*c </em>)相对于它的输入(<em class="mv"> w3*b和w4*c </em>)的梯度，这是(两者都是1)。现在，这些<em class="mv">局部</em>梯度乘以累积梯度(两者都是<em class="mv"> dL/dd </em> x <em class="mv"> 1 </em> = -1)，结果保存在各自输入节点的<em class="mv"> grad </em>属性中。</li><li id="b0b4" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">然后，函数<em class="mv"> MulBackward ( </em>代表<em class="mv"> w4*c </em>的乘法运算)分别计算其输入输出w.r.t到其输入(<em class="mv"> w4和c) </em> a <em class="mv"> s (c </em>和<em class="mv"> w4) </em>的梯度。局部梯度乘以累积梯度(<em class="mv"> dL/d(w4*c) </em> = -1)。结果值(<em class="mv"> -1 </em> x <em class="mv"> c </em>和-1 x <em class="mv"> w4 </em>)然后分别存储在<em class="mv">变量</em> <em class="mv"> w4 </em>和<em class="mv"> c </em>的<em class="mv"> grad </em>属性中。</li><li id="0acb" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">所有节点的梯度以相似的方式计算。</li><li id="2d3a" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><em class="mv"> L </em> w.r.t任何节点的梯度都可以通过调用来访问。<em class="mv"> grad </em>在对应于那个节点的变量上，<strong class="jp ir">假设它是一个叶节点</strong> (PyTorch的默认行为不允许你访问非叶节点的渐变。稍后会有更多相关内容)。现在我们已经得到了梯度，我们可以使用SGD或任何你喜欢的优化算法来更新我们的权重。</li></ul><pre class="km kn ko kp gt nc nd ne nf aw ng bi"><span id="0680" class="nh ky iq nd b gy ni nj l nk nl">w1 = w1 — (learning_rate) * w1.grad    #update the wieghts using GD</span></pre><p id="9b81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">诸如此类。</p><h1 id="140b" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">亲笔签名的一些俏皮细节</h1><p id="5efa" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">所以，我不是告诉过你不能访问非叶子<em class="mv">变量</em>的<em class="mv"> grad </em>属性吗。是啊，这是默认行为。您可以通过调用来重写它。<em class="mv"> retain_grad() </em>对<em class="mv">变量</em>进行定义，然后你就可以访问它的<em class="mv"> grad </em>属性。但是说真的，到底发生了什么事。</p><h2 id="1aa7" class="nh ky iq bd kz nn no dn ld np nq dp lh jy nr ns ll kc nt nu lp kg nv nw lt nx bi translated">动态计算图</h2><p id="5c71" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">PyTorch创建了一个叫做<strong class="jp ir">的动态计算图</strong>，这意味着这个图是动态生成的。<strong class="jp ir">在变量的<em class="mv"> forward </em>函数被调用之前，图中不存在<em class="mv">变量的节点(它是grad_fn) </em>。</strong>该图是调用多个<em class="mv">变量</em>的<em class="mv">前进</em>功能的结果。只有这样，缓冲区才会分配给图形和中间值(用于以后计算梯度)。当您调用<em class="mv"> backward() </em>时，随着梯度的计算，这些缓冲区基本上被释放，图形被破坏。你可以尝试在一个图形上多次向后调用<em class="mv"/>()，你会看到PyTorch会给你一个错误。这是因为图形在第一次调用<em class="mv"> backward() </em>时被破坏，因此，第二次调用时没有图形可以向后调用。</p><p id="bc19" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果再次调用<em class="mv"> forward </em>，会生成一个全新的图形。分配了新的内存。</p><p id="1913" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">默认情况下，只保存叶节点的渐变(<em class="mv"> grad </em>属性)，非叶节点的渐变被破坏。</strong>但是这种行为是可以改变的，如上所述。</p><p id="0e2e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这与TensorFlow使用的<strong class="jp ir">静态计算图</strong>形成对比，tensor flow在运行程序的 之前声明了<strong class="jp ir"> <em class="mv">。动态图范例允许您在运行时对网络架构进行更改，因为只有在运行一段代码时才会创建一个图。这意味着一个图可以在程序的生命周期中被重新定义。然而，这对于静态图形是不可能的，在静态图形中，图形是在运行程序之前创建的，只是在以后执行。动态图也使调试更容易，因为错误的来源很容易追踪。</em></strong></p></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h1 id="7e3a" class="kx ky iq bd kz la oj lc ld le ok lg lh li ol lk ll lm om lo lp lq on ls lt lu bi translated">一些贸易技巧</h1><h2 id="72ab" class="nh ky iq bd kz nn no dn ld np nq dp lh jy nr ns ll kc nt nu lp kg nv nw lt nx bi translated">要求_grad</h2><p id="2e2e" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">这是<em class="mv">变量</em>类的一个属性。默认情况下，它是False。当你不得不冻结一些层，并阻止他们在训练时更新参数时，这是很方便的。您可以简单地将<em class="mv"> requires_grad </em>设置为False，这些<em class="mv">变量</em>不会包含在计算图中。因此，没有梯度会传播到它们，或者传播到依赖这些层进行梯度流动的那些层。<em class="mv"> requires_grad </em>，<strong class="jp ir">设置为True时</strong> <strong class="jp ir">会传染</strong>，意思是即使一个运算的一个操作数<em class="mv"> requires_grad </em>设置为True，结果也会如此。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nz oa di ob bf oc"><div class="gh gi oo"><img src="../Images/75875f24f7a65a15e9385ec61ff268d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aEo6hqBUhN-_2YmIBETB-w.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk"><strong class="bd op">b </strong>is not included in the graph. No gradient is backpropagated through <strong class="bd op"><em class="oq">b</em></strong> now. <strong class="bd op">a</strong> only gets gradients from <strong class="bd op">c </strong>now. Even if <strong class="bd op">w1 </strong>has requires_grad = True, there is no way it can receive gradients.</figcaption></figure><h2 id="a4f8" class="nh ky iq bd kz nn no dn ld np nq dp lh jy nr ns ll kc nt nu lp kg nv nw lt nx bi translated">不稳定的</h2><p id="f6d9" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">这也是一个<em class="mv">变量</em>类的属性，当变量<em class="mv">被设置为真时，它会导致变量</em>被排除在计算图之外。这可能看起来与<em class="mv">要求_grad </em>非常相似，因为当设置为真时<strong class="jp ir">也会传染。但是比<em class="mv">要求_ grad</em>T32】优先级高。带有<em class="mv"> requires_grad </em>等于真且<em class="mv"> volatile </em>等于真的变量将不包括在计算图中。</strong></p><p id="67f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可能会想，当我们可以简单地将<em class="mv"> requires_grad </em>设置为False时，为什么还需要另一个开关来覆盖<em class="mv"> requires_grad </em>？我暂时跑题一下。</p><p id="86c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们进行推理时，不创建图形是非常有用的，并且不需要梯度。首先，消除了创建计算图的开销，提高了速度。第二，如果我们创建一个图形，由于没有<em class="mv">向后</em>被调用，用于缓存值的缓冲区永远不会被释放，这可能会导致内存耗尽。</p><p id="dd8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通常，我们在神经网络中有许多层，我们可能在训练时将<em class="mv"> requires_grad </em>设置为True。为了防止在推断时制作图表，我们可以做两件事中的任何一件。设置r <em class="mv"> equires_grad </em> False在<strong class="jp ir">所有</strong>层上(也许，152层？).或者，只在输入端设置<em class="mv"> volatile </em>为真，我们保证没有任何结果操作会产生一个图形。你的选择。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nz oa di ob bf oc"><div class="gh gi ny"><img src="../Images/99aaaae3f0553e3103f3ffb632bba4ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gR7elwFSFrnvvao0d1XmGw.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">No graph is created for <strong class="bd op">b or </strong>any node that depends on<strong class="bd op"> b.</strong></figcaption></figure><blockquote class="mw mx my"><p id="a4aa" class="jn jo mv jp b jq jr js jt ju jv jw jx mz jz ka kb na kd ke kf nb kh ki kj kk ij bi translated"><strong class="jp ir">注意:</strong> PyTorch 0.4没有组合张量/变量类的可变参数。相反，推理代码应该放在torch.no_grad()上下文管理器中。</p></blockquote><pre class="km kn ko kp gt nc nd ne nf aw ng bi"><span id="1ac9" class="nh ky iq nd b gy ni nj l nk nl">with torch.no_grad():<br/>    -----  your inference code goes here ----</span></pre><h1 id="1c17" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">结论</h1><p id="8c48" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">所以，那是<em class="mv">给你的签名</em>。理解<em class="mv">签名</em>是如何工作的，可以在你被困在某个地方时，或者在你开始时处理错误时，帮你省去很多头痛。感谢阅读到目前为止。我打算在PyTorch上写更多的教程，讨论如何使用内置函数快速创建复杂的架构(或者，可能没有这么快，但比一个块一个块地编码要快)。所以，敬请期待！</p><h1 id="0169" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">进一步阅读</h1><ol class=""><li id="921f" class="ma mb iq jp b jq lv ju lw jy or kc os kg ot kk of mg mh mi bi translated"><a class="ae kl" href="http://neuralnetworksanddeeplearning.com/chap2.html" rel="noopener ugc nofollow" target="_blank">理解反向传播</a></li><li id="3451" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk of mg mh mi bi translated"><a class="ae kl" href="https://www.youtube.com/watch?v=MKWBx78L7Qg" rel="noopener ugc nofollow" target="_blank">理解链式法则</a></li><li id="6c67" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk of mg mh mi bi translated">Python中的类<a class="ae kl" href="https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-i/tutorial/" rel="noopener ugc nofollow" target="_blank">第一部分</a>和<a class="ae kl" href="https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-ii-inheritance-and-composition/tutorial/" rel="noopener ugc nofollow" target="_blank">第二部分</a></li><li id="f27c" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk of mg mh mi bi translated"><a class="ae kl" href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" rel="noopener ugc nofollow" target="_blank"> PyTorch官方教程</a></li></ol></div></div>    
</body>
</html>