<html>
<head>
<title>CycleGANS and Pix2Pix</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CycleGANS和Pix2Pix</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cyclegans-and-pix2pix-5e6a5f0159c4?source=collection_archive---------0-----------------------#2017-08-15">https://towardsdatascience.com/cyclegans-and-pix2pix-5e6a5f0159c4?source=collection_archive---------0-----------------------#2017-08-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="6405" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">演职员表</strong>:展示这些博客的节略版，解释pix2pix和cycleGANs背后的思想和概念。</p><p id="2bd1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://twitter.com/christophrhesse" rel="noopener ugc nofollow" target="_blank"> <em class="kp">克里斯多夫·黑塞</em> </a> <em class="kp">博客:</em></p><div class="kq kr gp gr ks kt"><a href="https://affinelayer.com/pix2pix/" rel="noopener  ugc nofollow" target="_blank"><div class="ku ab fo"><div class="kv ab kw cl cj kx"><h2 class="bd iu gy z fp ky fr fs kz fu fw is bi translated">张量流——仿射层中的像到像转换</h2><div class="la l"><h3 class="bd b gy z fp ky fr fs kz fu fw dk translated">确保你已经安装了Tensorflow 0.12.1首先python -c "导入tensor flow；打印(tensorflow。__version__)" …</h3></div><div class="lb l"><p class="bd b dl z fp ky fr fs kz fu fw dk translated">affinelayer.com</p></div></div></div></a></div><p id="ed22" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Olga Liakhovich博客:</p><div class="kq kr gp gr ks kt"><a href="https://www.microsoft.com/developerblog/2017/06/12/learning-image-image-translation-cyclegans/" rel="noopener  ugc nofollow" target="_blank"><div class="ku ab fo"><div class="kv ab kw cl cj kx"><h2 class="bd iu gy z fp ky fr fs kz fu fw is bi translated">用CycleGANs学习图像到图像的翻译</h2><div class="la l"><h3 class="bd b gy z fp ky fr fs kz fu fw dk translated">我们最近与全球图片社Getty Images合作，探索图像到图像的翻译…</h3></div><div class="lb l"><p class="bd b dl z fp ky fr fs kz fu fw dk translated">www.microsoft.com</p></div></div><div class="lc l"><div class="ld l le lf lg lc lh li kt"/></div></div></a></div><h1 id="4fcc" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">像素2像素:</h1><p id="e242" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">论文:<a class="ae ko" href="https://phillipi.github.io/pix2pix/" rel="noopener ugc nofollow" target="_blank">https://phillipi.github.io/pix2pix/</a></p><p id="c51c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">pix2pix使用条件生成对抗网络(cGAN)来学习从输入图像到输出图像的映射。</p><p id="e9ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据集的一个例子是输入图像是黑白图片，而目标图像是图片的彩色版本。在这种情况下，生成器试图学习如何给黑白图像着色。鉴别器正在查看生成器的彩色化尝试，并试图了解生成器提供的彩色化与数据集中提供的真实彩色化目标图像之间的差异。</p><p id="acbc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">发生器的结构称为“编码器-解码器”，在pix2pix中，编码器-解码器大致如下:</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mm"><img src="../Images/2cf5389033ce95ac454def72b9d963d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*grPpbT-8fwA4twYZkAHwmw.png"/></div></div></figure><p id="9bb5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些体积是为了给你一种感觉，它们旁边的张量维度的形状。本例中的输入是一个256x256的图像，具有3个颜色通道(红色、绿色和蓝色，对于黑白图像都是相同的)，输出是相同的。</p><p id="e13c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">生成器获取一些输入，并试图用一系列编码器(卷积+激活函数)将其减少为更小的表示。这个想法是，通过以这种方式压缩它，我们有希望在最终编码层之后获得数据的更高级表示。解码层执行相反的操作(去卷积+激活功能),并反转编码器层的动作。</p><p id="acd2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了提高图像到图像变换的性能，作者使用了一个“U-Net”来代替编译码器。这是同样的事情，但是“跳过连接”直接将编码器层连接到解码器层:</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mm"><img src="../Images/e85b3af5a7eb3ffdd0aea6ca4b1118cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kpWvVdQOmbMuX2ls-d78TA.png"/></div></div></figure><p id="7995" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果不需要编码/解码部分，跳过连接为网络提供了绕过编码/解码部分的选项。</p><p id="f50e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些图表是一个轻微的简化。例如，网络的第一层和最后一层没有批范数层，而中间的几层有丢失单元。</p><p id="2bca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">鉴别器</strong></p><p id="ecdb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">鉴别器的工作是获取两个图像，一个输入图像和一个未知图像(它将是来自生成器的目标或输出图像)，并决定第二个图像是否是由生成器生成的。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mx"><img src="../Images/f338f808140e5b6db41dfdb913b3b6cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-iPXj4C0sCK0UzW1aPzJZg.png"/></div></div></figure><p id="6476" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该结构看起来很像发生器的编码器部分，但工作方式略有不同。输出是30×30的图像，其中每个像素值(0到1)表示未知图像的相应部分的可信度。在pix2pix实现中，这个30x30图像的每个像素对应于输入图像的70x70补丁的可信度(由于输入图像是256x256，所以补丁重叠很多)。这种架构被称为“PatchGAN”。</p><p id="a028" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">训练</strong></p><p id="c1ab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">训练这个网络有两个步骤:训练鉴别器和训练生成器。</p><p id="5e31" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了训练鉴别器，生成器首先生成输出图像。鉴别器查看输入/目标对和输入/输出对，并猜测它们看起来有多真实。然后基于输入/输出对和输入/目标对的分类误差来调整鉴别器的权重。</p><p id="5cfd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后基于鉴别器的输出以及输出和目标图像之间的差异来调整发生器的权重。</p><div class="mn mo mp mq gt ab cb"><figure class="my mr mz na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><img src="../Images/0a02685581f81cdc8ea6743b9eb2443d.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*EUUrcoQ9nBGyNzIeqzCDJQ.png"/></div></figure><figure class="my mr ne na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><img src="../Images/769d248d3d7a7308ab873e63dd1eb060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*vwCOla_RiuTD6pEEMitTgQ.png"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk nj di nk nl">Discriminator and Generator training</figcaption></figure></div><h1 id="b41c" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">CycleGANs</h1><p id="2e76" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">原创<a class="ae ko" href="https://arxiv.org/pdf/1703.10593.pdf" rel="noopener ugc nofollow" target="_blank"> CycleGAN论文</a></p><p id="1356" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然PIX2PIX可以产生真正神奇的结果，但挑战在于训练数据。您想要学习转换的两个图像空间需要预先格式化为一个包含两个紧密相关图像的X/Y图像。根据您尝试在两种图像类型之间进行转换，这可能会很耗时、不可行，甚至是不可能的(例如，如果两个图像配置文件之间没有一对一的匹配)。这就是CycleGAN的用武之地。</p><p id="9a4c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">CycleGANs背后的关键思想是，他们可以建立在PIX2PIX架构的能力上，但允许你将模型指向两个离散的、<em class="kp">不成对的图像集合</em>。例如，一个图像集合，组X，将充满阳光海滩照片，而组Y将是阴天海滩照片的集合。CycleGAN模型可以学习在这两种美学之间转换图像，而无需将紧密相关的匹配合并到单个X/Y训练图像中。</p><p id="8161" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">CycleGANs能够在没有明确的X/Y训练图像的情况下学习如此出色的翻译的方法包括引入一个<em class="kp">完整翻译周期</em>的概念，以确定整个翻译系统有多好，从而同时改进两个生成器。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/fd626f2ef9a68699c3673e9a327456f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*D5yQU7v0NHXsb1Ep.jpg"/></div></figure><p id="7bb8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这种方法是CycleGANs为图像到图像的翻译带来的巧妙力量，以及它如何在非配对图像风格之间实现更好的翻译。</p><p id="5d9e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最初的CycleGANs论文<a class="ae ko" href="https://arxiv.org/pdf/1703.10593.pdf" rel="noopener ugc nofollow" target="_blank">“使用循环一致对抗网络的不成对图像到图像的翻译”</a>由朱俊彦等人发表。</p><h1 id="28ca" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">损失函数</h1><p id="96d2" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">CycleGANs的强大之处在于他们如何设置损失函数，并使用全周期损失作为额外的优化目标。</p><p id="baa2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为复习:我们正在处理2个生成器和2个鉴别器。</p><h2 id="4f97" class="nn lk it bd ll no np dn lp nq nr dp lt kb ns nt lx kf nu nv mb kj nw nx mf ny bi translated">发电机损耗</h2><p id="39b0" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">让我们从发电机的损耗函数开始，它由两部分组成。</p><p id="d17b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第1部分</strong>:如果伪造(生成)的图像非常好，鉴别器无法区分这些图像与真实图像，则生成器成功。换句话说，鉴别器对假图像的输出应该尽可能接近1。在张量流术语中，发电机希望最小化:</p><pre class="mn mo mp mq gt nz oa ob oc aw od bi"><span id="bb48" class="nn lk it oa b gy oe of l og oh">g_loss_G_disc = tf.reduce_mean((discY_fake — tf.ones_like(discY_fake)) ** 2)</span><span id="3505" class="nn lk it oa b gy oi of l og oh">g_loss_F_dicr = tf.reduce_mean((discX_fake — tf.ones_like(discX_fake)) ** 2)</span></pre><p id="7a57" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意:上面的“**”符号是Python中的幂运算符。</p><p id="3868" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第2部分</strong>:我们需要捕捉<strong class="js iu">循环损耗</strong>:当我们使用另一个发生器从一个发生器回到图像的原始空间时，原始图像(我们开始循环的地方)和循环图像之间的差异应该被最小化。</p><pre class="mn mo mp mq gt nz oa ob oc aw od bi"><span id="bc74" class="nn lk it oa b gy oe of l og oh">g_loss_G_cycle = tf.reduce_mean(tf.abs(real_X — genF_back)) + tf.reduce_mean(tf.abs(real_Y — genG_back))</span><span id="eed8" class="nn lk it oa b gy oi of l og oh">g_loss_F_cycle = tf.reduce_mean(tf.abs(real_X — genF_back)) + tf.reduce_mean(tf.abs(real_Y — genG_back))</span></pre><p id="e876" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，发电机损耗是这两项之和:</p><p id="78fd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">G _ loss _ G = G _ loss _ G _ disc+G _ loss _ G _循环</p><p id="2361" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为循环损耗非常重要，所以我们想增加它的影响。我们为这个乘数使用了一个L1λ常数(在论文中使用了值10)。</p><p id="db33" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在发电机损耗的大结局看起来像:</p><p id="97bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">G _ loss _ G = G _ loss _ G _ disc+L1 _λ* G _ loss _ G _ cycle</p><p id="1a98" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">g _ loss _ F = g _ loss _ F _ disc+L1 _拉姆达* g_loss_F_cycle</p><p id="9e60" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">鉴频器损耗</strong></p><p id="88ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">鉴别器需要做出两个决定:</p><ol class=""><li id="518d" class="oj ok it js b jt ju jx jy kb ol kf om kj on kn oo op oq or bi translated">真实图像应标记为真实(推荐值应尽可能接近1)</li><li id="2236" class="oj ok it js b jt os jx ot kb ou kf ov kj ow kn oo op oq or bi translated">鉴别器应该能够识别生成的图像，从而预测假图像为0。</li></ol><pre class="mn mo mp mq gt nz oa ob oc aw od bi"><span id="e572" class="nn lk it oa b gy oe of l og oh">DY_loss_real = tf.reduce_mean((DY — tf.ones_like(DY))** 2)</span><span id="16d4" class="nn lk it oa b gy oi of l og oh">DY_loss_fake = tf.reduce_mean((DY_fake_sample — tf.zeros_like(DY_fake_sample)) ** 2)</span><span id="ba67" class="nn lk it oa b gy oi of l og oh">DY_loss = (DY_loss_real + DY_loss_fake) / 2</span><span id="dcfa" class="nn lk it oa b gy oi of l og oh">DX_loss_real = tf.reduce_mean((DX — tf.ones_like(DX)) ** 2)</span><span id="0c1c" class="nn lk it oa b gy oi of l og oh">DX_loss_fake = tf.reduce_mean((DX_fake_sample — tf.zeros_like(DX_fake_sample)) ** 2)</span><span id="9188" class="nn lk it oa b gy oi of l og oh">DX_loss = (DX_loss_real + DX_loss_fake) / 2</span></pre></div></div>    
</body>
</html>