<html>
<head>
<title>When and How to use Weighted Least Squares (WLS) Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">何时以及如何使用加权最小二乘(WLS)模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/when-and-how-to-use-weighted-least-squares-wls-models-a68808b1a89d?source=collection_archive---------0-----------------------#2018-08-24">https://towardsdatascience.com/when-and-how-to-use-weighted-least-squares-wls-models-a68808b1a89d?source=collection_archive---------0-----------------------#2018-08-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h1 id="093e" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">WLS，OLS 被忽视的表弟</h1><p id="acaf" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在<a class="ae lt" href="https://www.thisismetis.com" rel="noopener ugc nofollow" target="_blank"> Metis </a>，我教的第一批机器学习模型之一就是<em class="lu">大部分人</em>在高中学习的素简<a class="ae lt" href="https://en.wikipedia.org/wiki/Ordinary_least_squares" rel="noopener ugc nofollow" target="_blank">普通最小二乘(OLS) </a>模型。Excel 有办法去除 OLS 建模的魅力；学生们经常假设有一个散点图，一些神奇的数学方法可以画出一条最佳拟合线，然后在角落里有一个 r，我们希望它接近 1。事实是，OLS 远不止看上去那么简单，大约一周后，学生们哭着求饶(免责声明:没有学生实际上受到伤害！)当我们深入一个最初看起来如此简单的领域时。</p><p id="411a" class="pw-post-body-paragraph kv kw it kx b ky lv la lb lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls im bi translated">理解了这个广泛适用的模型后，一个自然的反应是将 OLS 用于任何事情。这并不是一个糟糕的想法:尽管 OLS <a class="ae lt" href="https://www.albert.io/blog/key-assumptions-of-ols-econometrics-review/" rel="noopener ugc nofollow" target="_blank">需要四个</a>——有人说是五个或六个——假设来满足原始或“经过处理”的数据，但是建模范例是相当健壮的，并且通常可以很好地执行，只要我们规范并使用适当的复杂性、对数变换、截取等。</p><p id="34bb" class="pw-post-body-paragraph kv kw it kx b ky lv la lb lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls im bi translated">然而，OLS 只是一个杰出的家谱:</p><blockquote class="ma"><p id="ed77" class="mb mc it bd md me mf mg mh mi mj ls dk translated">加权最小二乘法(WLS)是安静平方的表亲，但她有一套独特的技巧，与某些数据集完全一致！</p></blockquote><h1 id="301f" class="jx jy it bd jz ka mk kc kd ke ml kg kh ki mm kk kl km mn ko kp kq mo ks kt ku bi translated">输入异方差</h1><p id="56be" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我的学生们最喜欢的另一个术语是<em class="lu">异性恋</em>，这也是“数据科学刽子手”或其他欢乐时光庆祝活动的常见特征。来自古希腊语<em class="lu"> hetero，</em>意为<em class="lu"> </em>“不同”，而<em class="lu"> skedasis，</em>意为<em class="lu"> </em>“分散”，也可以在英语化的“异方差”(注意附加的‘c’)形式中找到。简而言之，异方差数据具有随输入而变化的可变性。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mp"><img src="../Images/f665d9cc70aabd836ea2928972b76e08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8sgKYUIEC6IrgOvKO7GK-Q.jpeg"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Source: sarupub.org</figcaption></figure><p id="d2c3" class="pw-post-body-paragraph kv kw it kx b ky lv la lb lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls im bi translated">事实是，许多数据显示了这种“异方差”。列举一些特征-反应关系的例子，我们通常可以直观地解释为什么:</p><ul class=""><li id="2f6e" class="nf ng it kx b ky lv lc lw lg nh lk ni lo nj ls nk nl nm nn bi translated">随着年龄的增长，净值趋向分化</li><li id="6cf9" class="nf ng it kx b ky no lc np lg nq lk nr lo ns ls nk nl nm nn bi translated">随着公司规模的扩大，收入趋向分化</li><li id="e269" class="nf ng it kx b ky no lc np lg nq lk nr lo ns ls nk nl nm nn bi translated">或者，随着婴儿身高的增加，体重往往会出现差异</li></ul><p id="08f3" class="pw-post-body-paragraph kv kw it kx b ky lv la lb lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls im bi translated">OLS 的一个主要假设是数据——以及残差——是本土的。啊哦！好消息是 OLS 可以处理一定程度的异方差。在网上搜索，你可能会发现不同的经验法则，比如“最高可变性不应超过最低可变性的四倍”。还有一个<a class="ae lt" href="https://en.wikipedia.org/wiki/Heteroscedasticity#Detection" rel="noopener ugc nofollow" target="_blank">数量的测试</a>来统计确定你问题的严重程度。</p><p id="03c0" class="pw-post-body-paragraph kv kw it kx b ky lv la lb lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls im bi translated">幸运的是，OLS 的假设不是非黑即白、二元对立的。有一个灰色区域，在那里模型仍然工作得相当好。</p><blockquote class="ma"><p id="9387" class="mb mc it bd md me mf mg mh mi mj ls dk translated">"但是如果我有可怕的——超过 4 倍的异方差——回归怎么办，大师？"</p><p id="1d47" class="mb mc it bd md me mf mg mh mi mj ls dk translated">"那我们就去 WLS 吧，年轻的学徒！"</p></blockquote><figure class="nu nv nw nx ny mu gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d29ea6a2fea4de4b20a7bade22c72d1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*h-X0ryZWVL8ycI_YIJuqrA.jpeg"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Yoda taught me everything I know about machine learning!!</figcaption></figure><h1 id="f240" class="jx jy it bd jz ka mk kc kd ke ml kg kh ki nz kk kl km oa ko kp kq ob ks kt ku bi translated">WLS 来救援了！</h1><p id="416d" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">让我们看看 WLS 是如何在我最喜欢的机器学习环境之一<a class="ae lt" href="http://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>中实现的。</p><h2 id="3371" class="oc jy it bd jz od oe dn kd of og dp kh lg oh oi kl lk oj ok kp lo ol om kt on bi translated">数据设置</h2><p id="0a17" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">让我们生成一些假数据:</p><pre class="mq mr ms mt gt oo op oq or aw os bi"><span id="bb55" class="oc jy it op b gy ot ou l ov ow">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import statsmodels.api as sm<br/>%matplotlib inline</span><span id="3a52" class="oc jy it op b gy ox ou l ov ow"># generate random data<br/>np.random.seed(24)<br/>x = np.random.uniform(-5,5,25)<br/>ϵ = 2*np.random.randn(25)<br/>y = 2*x+ϵ</span></pre></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><pre class="oo op oq or aw os bi"><span id="23fc" class="oc jy it op b gy oy oz pa pb pc ou l ov ow"># alternate error as a function of x<br/>ϵ2 = ϵ*(x+5)<br/>y2 = 2*x+ϵ2</span><span id="4c67" class="oc jy it op b gy ox ou l ov ow">sns.regplot(x,y);<br/>sns.regplot(x,y2);</span></pre><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi pd"><img src="../Images/ff36da1fc543657d3092aaf5e328069b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ff7i7Cm-Hv-ZwzVc9ZvCVg.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Blue: Regular and Orange: Heteroskedastic</figcaption></figure></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="eac6" class="pw-post-body-paragraph kv kw it kx b ky lv la lb lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls im bi translated">请注意，这些集合来自相同的地面真值函数，但是作为<code class="fe pe pf pg op b">x</code>的函数而增加的方差导致橙色模型拟合不同于蓝色的线。在另一次随机抽取中，斜率可能比蓝色低，但总体上更不稳定。</p><pre class="mq mr ms mt gt oo op oq or aw os bi"><span id="2a12" class="oc jy it op b gy ot ou l ov ow"># add a strong outlier for high x<br/>x_high = np.append(x,5)<br/>y_high = np.append(y2,160)</span><span id="fe59" class="oc jy it op b gy ox ou l ov ow"># add a strong outlier for low x<br/>x_low = np.append(x,-4)<br/>y_low = np.append(y2,160)</span></pre><p id="e953" class="pw-post-body-paragraph kv kw it kx b ky lv la lb lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls im bi translated">上面的第一个附录模拟了一个常见的场景，在这个场景中，一个高方差的区域(预期的)会出现一个极端的观察结果。这将对 OLS 产生比 WLS 更大的影响，因为 WLS 将减少差额及其“罚金”的权重。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi pd"><img src="../Images/65f43619e69b3f5b9891cccecc0418d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IZgqvMObHBOBk90OtY8yeQ.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">High outlier, on the top right</figcaption></figure><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi pd"><img src="../Images/60a3c2911583afaefe2ab066c58c6376.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SuyHjP4iF00ys6zHXWgBTA.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Low outlier, on the left</figcaption></figure></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h1 id="8217" class="jx jy it bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">SKLearn 中的 WLS</h1><p id="66fe" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">为了计算样本重量，记住我们添加的误差作为<code class="fe pe pf pg op b">(x+5)</code>的函数而变化；我们可以用这个来反过来衡量这些值。只要相对权重一致，就不需要绝对的基准。</p><pre class="mq mr ms mt gt oo op oq or aw os bi"><span id="56c9" class="oc jy it op b gy ot ou l ov ow"># calculate weights for sets with low and high outlier<br/>sample_weights_low = [1/(x+5) for x in x_low]<br/>sample_weights_high = [1/(x+5) for x in x_high]</span></pre></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><pre class="oo op oq or aw os bi"><span id="0e88" class="oc jy it op b gy oy oz pa pb pc ou l ov ow"># reshape for compatibility<br/>X_low = x_low.reshape(-1, 1)<br/>X_high = x_high.reshape(-1, 1)</span><span id="0769" class="oc jy it op b gy ox ou l ov ow">---------<br/># import and fit an OLS model, check coefficients<br/>from sklearn.linear_model import LinearRegression</span><span id="e0d0" class="oc jy it op b gy ox ou l ov ow">model = LinearRegression()<br/>model.fit(X_low, ymod)</span><span id="ea4f" class="oc jy it op b gy ox ou l ov ow"># fit WLS using sample_weights<br/>WLS = LinearRegression()<br/>WLS.fit(X_low, ymod, sample_weight=sample_weights_low)</span><span id="368d" class="oc jy it op b gy ox ou l ov ow">print(model.intercept_, model.coef_)<br/>print('WLS')<br/>print(WLS.intercept_, WLS.coef_)</span><span id="5930" class="oc jy it op b gy ox ou l ov ow"># run this yourself, don't trust every result you see online =)</span></pre><p id="f984" class="pw-post-body-paragraph kv kw it kx b ky lv la lb lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls im bi translated">请注意 WLS 的斜率是如何受到低异常值的更多影响的，这是应该的。低区域应该具有较低的可变性，因此异常值被放大到高于 OLS 值，从而将斜率推得更负。下面让我们看看 WLS 是如何抑制高异常值的。</p><pre class="mq mr ms mt gt oo op oq or aw os bi"><span id="da78" class="oc jy it op b gy ot ou l ov ow">model = LinearRegression()<br/>model.fit(X_high, ymod)</span><span id="63a0" class="oc jy it op b gy ox ou l ov ow">WLS.fit(X_high, ymod, sample_weight=sample_weights_high)</span><span id="f9ec" class="oc jy it op b gy ox ou l ov ow">print(model.intercept_, model.coef_)<br/>print('WLS')<br/>print(WLS.intercept_, WLS.coef_)</span></pre><p id="545d" class="pw-post-body-paragraph kv kw it kx b ky lv la lb lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls im bi translated">您将会注意到，在预期方差区域中的异常值是如何减少对参数估计的影响的。请记住，当异常值不完全相等时，请使用 WLS！</p><h1 id="0d9a" class="jx jy it bd jz ka mk kc kd ke ml kg kh ki nz kk kl km oa ko kp kq ob ks kt ku bi translated">结论</h1><p id="adff" class="pw-post-body-paragraph kv kw it kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">你有它！实现 WLS 可能有些棘手；<code class="fe pe pf pg op b">sklearn</code>没有独特的 WLS 模型，因为论证功能(也用于决策树和其他模型)暗中支持我们的需求。</p><p id="df90" class="pw-post-body-paragraph kv kw it kx b ky lv la lb lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls im bi translated">这是对 WLS 的基本介绍，在这个领域还有很多可以探索的，包括有前途的<a class="ae lt" href="http://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors" rel="noopener ugc nofollow" target="_blank"> Huber-White“三明治”估计方法</a>。</p><p id="20c0" class="pw-post-body-paragraph kv kw it kx b ky lv la lb lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls im bi translated">最后，道格拉斯·c·蒙哥马利、伊丽莎白·a·佩克和 g·杰弗里·维宁在《线性回归分析简介》一书中推荐了一种加权方法。例如:</p><ol class=""><li id="b8e6" class="nf ng it kx b ky lv lc lw lg nh lk ni lo nj ls ph nl nm nn bi translated">建模时总是寻求使用经验或先验信息。</li><li id="d0a8" class="nf ng it kx b ky no lc np lg nq lk nr lo ns ls ph nl nm nn bi translated">使用模型的残差—例如如果<code class="fe pe pf pg op b">var(εi)=σ2x_i*var(εi)=σ2x_i</code> —那么我们可能决定使用<code class="fe pe pf pg op b">w_i=1/x_i</code>。</li><li id="f368" class="nf ng it kx b ky no lc np lg nq lk nr lo ns ls ph nl nm nn bi translated">如果回应是<code class="fe pe pf pg op b">n</code>观察的平均值，类似于<code class="fe pe pf pg op b">var(y_i)=var(ε_i)=σ2/n_i*var(y_i)=var(ε_i)=σ2/n_i</code>，那么我们可能会决定使用<code class="fe pe pf pg op b">w_i=n_i</code>。</li><li id="c706" class="nf ng it kx b ky no lc np lg nq lk nr lo ns ls ph nl nm nn bi translated">有时我们知道不同的观测值是由不同的仪器测量的，这些仪器具有一定的(已知的或估计的)准确性。在这种情况下，我们可以决定使用与测量误差方差成反比的权重。</li></ol><p id="1675" class="pw-post-body-paragraph kv kw it kx b ky lv la lb lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls im bi translated">与大多数数据科学工作一样，您的方法必须灵活适应您所拥有的数据类型。</p><p id="0ab8" class="pw-post-body-paragraph kv kw it kx b ky lv la lb lc lw le lf lg lx li lj lk ly lm ln lo lz lq lr ls im bi translated">快乐造型！一如既往，感谢您的阅读、联系和分享！</p></div></div>    
</body>
</html>