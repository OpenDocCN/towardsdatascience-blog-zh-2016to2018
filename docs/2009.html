<html>
<head>
<title>Solving A Simple Classification Problem with Python — Fruits Lovers’ Edition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 解决简单的分类问题——水果爱好者版</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/solving-a-simple-classification-problem-with-python-fruits-lovers-edition-d20ab6b071d2?source=collection_archive---------1-----------------------#2017-12-04">https://towardsdatascience.com/solving-a-simple-classification-problem-with-python-fruits-lovers-edition-d20ab6b071d2?source=collection_archive---------1-----------------------#2017-12-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/abc69e44f6e4487e2b98ea8cfabd0aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GnNVxoh_H8n7Z2d7ceCBUw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo credit: Pixabay</figcaption></figure><p id="52fa" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在这篇文章中，我们将使用 Python 最流行的机器学习工具<a class="ae la" href="http://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>在 Python 中实现几个机器学习算法。使用简单的数据集来训练分类器以区分不同类型的水果。</p><p id="e00f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这篇文章的目的是确定最适合手头问题的机器学习算法；因此，我们希望比较不同的算法，选择性能最好的算法。我们开始吧！</p><h1 id="62b2" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据</h1><p id="e940" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">水果数据集是由爱丁堡大学的<a class="ae la" href="http://homepages.inf.ed.ac.uk/imurray2/" rel="noopener ugc nofollow" target="_blank">伊恩·默里</a>博士创建的。他买了几十个不同品种的橘子、柠檬和苹果，并把它们的尺寸记录在一个表格里。然后密执安大学的教授将水果数据稍微格式化，可以从<a class="ae la" href="https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/fruit_data_with_colors.txt" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><p id="c6dd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们看看前几行数据。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="e461" class="mn lc iq mj b gy mo mp l mq mr">%matplotlib inline<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span><span id="5e7e" class="mn lc iq mj b gy ms mp l mq mr">fruits = pd.read_table('fruit_data_with_colors.txt')<br/>fruits.head()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/cd28d3860af21ab7be88b1e059525b2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*FEXFrLSZj7h7tuxm3ewx1A.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1</figcaption></figure><p id="b47c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">数据集的每一行都代表一片水果，由表的列中的几个要素表示。</p><p id="ca7f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">数据集中有 59 种水果和 7 个特征:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="33be" class="mn lc iq mj b gy mo mp l mq mr">print(fruits.shape)</span></pre><p id="e2be" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mu"> (59，7) </em> </strong></p><p id="d90a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">数据集中有四种水果:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="91c6" class="mn lc iq mj b gy mo mp l mq mr">print(fruits['fruit_name'].unique())</span></pre><p id="5e85" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mu"> ['苹果' '橘子' '橘子' '柠檬]]</em></strong></p><p id="8f31" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">除了普通话，其他数据都很平衡。我们只能顺其自然了。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="d299" class="mn lc iq mj b gy mo mp l mq mr">print(fruits.groupby('fruit_name').size())</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/018cd93cf4fc8b1746c7fc01a77117fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*nTwprHtWbeMqlqmPzTQtyw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2</figcaption></figure><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9b5b" class="mn lc iq mj b gy mo mp l mq mr">import seaborn as sns<br/>sns.countplot(fruits['fruit_name'],label="Count")<br/>plt.show()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/2999fb0ba2e48c3ccf8c50bb076aa744.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*KEvibV4Hf6eJBh5XUp2ilg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3</figcaption></figure><h1 id="d4ef" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">形象化</h1><ul class=""><li id="a761" class="mx my iq ke b kf lz kj ma kn mz kr na kv nb kz nc nd ne nf bi translated">每个数值变量的箱线图将使我们对输入变量的分布有一个更清晰的概念:</li></ul><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="2541" class="mn lc iq mj b gy mo mp l mq mr">fruits.drop('fruit_label', axis=1).plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False, figsize=(9,9), <br/>                                        title='Box Plot for each input variable')<br/>plt.savefig('fruits_box')<br/>plt.show()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/79635c187f6a3c1225a0ce841c5a1165.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*kOdjdtiA0Uj-beupp-ZIvA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4</figcaption></figure><ul class=""><li id="1455" class="mx my iq ke b kf kg kj kk kn nh kr ni kv nj kz nc nd ne nf bi translated">看起来颜色分数可能具有近似高斯分布。</li></ul><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="079e" class="mn lc iq mj b gy mo mp l mq mr">import pylab as pl<br/>fruits.drop('fruit_label' ,axis=1).hist(bins=30, figsize=(9,9))<br/>pl.suptitle("Histogram for each numeric input variable")<br/>plt.savefig('fruits_hist')<br/>plt.show()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/e6e823abb6608af79bdbacf517e9b593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*qsBl9JybuuGMG5kMg30RXg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 5</figcaption></figure><ul class=""><li id="6240" class="mx my iq ke b kf kg kj kk kn nh kr ni kv nj kz nc nd ne nf bi translated">一些属性对是相关的(质量和宽度)。这表明了高度的相关性和可预测的关系。</li></ul><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="7249" class="mn lc iq mj b gy mo mp l mq mr">from pandas.tools.plotting import scatter_matrix<br/>from matplotlib import cm</span><span id="d25a" class="mn lc iq mj b gy ms mp l mq mr">feature_names = ['mass', 'width', 'height', 'color_score']<br/>X = fruits[feature_names]<br/>y = fruits['fruit_label']</span><span id="8fc2" class="mn lc iq mj b gy ms mp l mq mr">cmap = cm.get_cmap('gnuplot')<br/>scatter = pd.scatter_matrix(X, c = y, marker = 'o', s=40, hist_kwds={'bins':15}, figsize=(9,9), cmap = cmap)<br/>plt.suptitle('Scatter-matrix for each input variable')<br/>plt.savefig('fruits_scatter_matrix')</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/d0e3a4741538b9581e4bcb4a4c615bfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*d4jrCyebgFzQH-8BROnWEw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 6</figcaption></figure><h1 id="475a" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">统计摘要</h1><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/ba4e8bfd389b2d16b503ef9c96d5976d.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*OAIHQFz0dvYXuVlNWbv7cw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 7</figcaption></figure><p id="0820" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们可以看到数值没有相同的标度。我们需要对我们为训练集计算的测试集应用缩放。</p><h1 id="a4c5" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">创建训练集和测试集并应用缩放</h1><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="cbe7" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.model_selection import train_test_split</span><span id="d3f5" class="mn lc iq mj b gy ms mp l mq mr">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span><span id="555e" class="mn lc iq mj b gy ms mp l mq mr">from sklearn.preprocessing import MinMaxScaler<br/>scaler = MinMaxScaler()<br/>X_train = scaler.fit_transform(X_train)<br/>X_test = scaler.transform(X_test)</span></pre><h1 id="ce27" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">构建模型</h1><h2 id="5948" class="mn lc iq bd ld nl nm dn lh nn no dp ll kn np nq lp kr nr ns lt kv nt nu lx nv bi translated"><strong class="ak">逻辑回归</strong></h2><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="869a" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.linear_model import LogisticRegression</span><span id="b48a" class="mn lc iq mj b gy ms mp l mq mr">logreg = LogisticRegression()<br/>logreg.fit(X_train, y_train)</span><span id="234f" class="mn lc iq mj b gy ms mp l mq mr">print('Accuracy of Logistic regression classifier on training set: {:.2f}'<br/>     .format(logreg.score(X_train, y_train)))<br/>print('Accuracy of Logistic regression classifier on test set: {:.2f}'<br/>     .format(logreg.score(X_test, y_test)))</span></pre><p id="2fc1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mu">逻辑回归分类器对训练集的准确率:0.70 <br/>逻辑回归分类器对测试集的准确率:0.40 </em> </strong></p><h2 id="8aa7" class="mn lc iq bd ld nl nm dn lh nn no dp ll kn np nq lp kr nr ns lt kv nt nu lx nv bi translated"><strong class="ak">决策树</strong></h2><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1d1e" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.tree import DecisionTreeClassifier</span><span id="997b" class="mn lc iq mj b gy ms mp l mq mr">clf = DecisionTreeClassifier().fit(X_train, y_train)</span><span id="43b7" class="mn lc iq mj b gy ms mp l mq mr">print('Accuracy of Decision Tree classifier on training set: {:.2f}'<br/>     .format(clf.score(X_train, y_train)))<br/>print('Accuracy of Decision Tree classifier on test set: {:.2f}'<br/>     .format(clf.score(X_test, y_test)))</span></pre><p id="e5ef" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mu">决策树分类器在训练集上的准确率:1.00 <br/>决策树分类器在测试集上的准确率:0.73 </em> </strong></p><h2 id="5a0c" class="mn lc iq bd ld nl nm dn lh nn no dp ll kn np nq lp kr nr ns lt kv nt nu lx nv bi translated"><strong class="ak">K-最近邻</strong></h2><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="c1c0" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.neighbors import KNeighborsClassifier</span><span id="4498" class="mn lc iq mj b gy ms mp l mq mr">knn = KNeighborsClassifier()<br/>knn.fit(X_train, y_train)<br/>print('Accuracy of K-NN classifier on training set: {:.2f}'<br/>     .format(knn.score(X_train, y_train)))<br/>print('Accuracy of K-NN classifier on test set: {:.2f}'<br/>     .format(knn.score(X_test, y_test)))</span></pre><p id="e947" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mu">K-NN 分类器在训练集上的准确率:0.95<br/>K-NN 分类器在测试集上的准确率:1.00 </em> </strong></p><h2 id="ee45" class="mn lc iq bd ld nl nm dn lh nn no dp ll kn np nq lp kr nr ns lt kv nt nu lx nv bi translated">线性判别分析</h2><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="fe6e" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis</span><span id="09ed" class="mn lc iq mj b gy ms mp l mq mr">lda = LinearDiscriminantAnalysis()<br/>lda.fit(X_train, y_train)<br/>print('Accuracy of LDA classifier on training set: {:.2f}'<br/>     .format(lda.score(X_train, y_train)))<br/>print('Accuracy of LDA classifier on test set: {:.2f}'<br/>     .format(lda.score(X_test, y_test)))</span></pre><p id="6b28" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mu">LDA 分类器在训练集上的准确率:0.86<br/>LDA 分类器在测试集上的准确率:0.67 </em> </strong></p><h2 id="00b2" class="mn lc iq bd ld nl nm dn lh nn no dp ll kn np nq lp kr nr ns lt kv nt nu lx nv bi translated">高斯朴素贝叶斯</h2><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="3f35" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.naive_bayes import GaussianNB</span><span id="8bd1" class="mn lc iq mj b gy ms mp l mq mr">gnb = GaussianNB()<br/>gnb.fit(X_train, y_train)<br/>print('Accuracy of GNB classifier on training set: {:.2f}'<br/>     .format(gnb.score(X_train, y_train)))<br/>print('Accuracy of GNB classifier on test set: {:.2f}'<br/>     .format(gnb.score(X_test, y_test)))</span></pre><p id="5fe3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mu">GNB 分类器在训练集上的准确率:0.86<br/>GNB 分类器在测试集上的准确率:0.67 </em> </strong></p><h2 id="9dec" class="mn lc iq bd ld nl nm dn lh nn no dp ll kn np nq lp kr nr ns lt kv nt nu lx nv bi translated">支持向量机</h2><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="2b62" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.svm import SVC</span><span id="463f" class="mn lc iq mj b gy ms mp l mq mr">svm = SVC()<br/>svm.fit(X_train, y_train)<br/>print('Accuracy of SVM classifier on training set: {:.2f}'<br/>     .format(svm.score(X_train, y_train)))<br/>print('Accuracy of SVM classifier on test set: {:.2f}'<br/>     .format(svm.score(X_test, y_test)))</span></pre><p id="000e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mu">SVM 分类器在训练集上的准确率:0.61<br/>SVM 分类器在测试集上的准确率:0.33 </em> </strong></p><p id="5a7d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">KNN 算法是我们尝试过的最精确的模型。混淆矩阵提供了对测试集没有错误的指示。然而，测试集非常小。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="ce19" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.metrics import classification_report<br/>from sklearn.metrics import confusion_matrix<br/>pred = knn.predict(X_test)<br/>print(confusion_matrix(y_test, pred))<br/>print(classification_report(y_test, pred))</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/eaf5a5ab8d91e1cad128d1c3fac64367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*z4myp0MNbspQgAcD9x4oug.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 7</figcaption></figure><h1 id="01c8" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">绘制 k-NN 分类器的决策边界</h1><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4850" class="mn lc iq mj b gy mo mp l mq mr">import matplotlib.cm as cm<br/>from matplotlib.colors import ListedColormap, BoundaryNorm<br/>import matplotlib.patches as mpatches<br/>import matplotlib.patches as mpatches</span><span id="f39f" class="mn lc iq mj b gy ms mp l mq mr">X = fruits[['mass', 'width', 'height', 'color_score']]<br/>y = fruits['fruit_label']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span><span id="354e" class="mn lc iq mj b gy ms mp l mq mr">def plot_fruit_knn(X, y, n_neighbors, weights):<br/>    X_mat = X[['height', 'width']].as_matrix()<br/>    y_mat = y.as_matrix()</span><span id="a144" class="mn lc iq mj b gy ms mp l mq mr"># Create color maps<br/>    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF','#AFAFAF'])<br/>    cmap_bold  = ListedColormap(['#FF0000', '#00FF00', '#0000FF','#AFAFAF'])</span><span id="9017" class="mn lc iq mj b gy ms mp l mq mr">clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)<br/>    clf.fit(X_mat, y_mat)</span><span id="b829" class="mn lc iq mj b gy ms mp l mq mr"># Plot the decision boundary by assigning a color in the color map<br/>    # to each mesh point.<br/>    <br/>    mesh_step_size = .01  # step size in the mesh<br/>    plot_symbol_size = 50<br/>    <br/>    x_min, x_max = X_mat[:, 0].min() - 1, X_mat[:, 0].max() + 1<br/>    y_min, y_max = X_mat[:, 1].min() - 1, X_mat[:, 1].max() + 1<br/>    xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step_size),<br/>                         np.arange(y_min, y_max, mesh_step_size))<br/>    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><span id="dcd4" class="mn lc iq mj b gy ms mp l mq mr"># Put the result into a color plot<br/>    Z = Z.reshape(xx.shape)<br/>    plt.figure()<br/>    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)</span><span id="adf1" class="mn lc iq mj b gy ms mp l mq mr"># Plot training points<br/>    plt.scatter(X_mat[:, 0], X_mat[:, 1], s=plot_symbol_size, c=y, cmap=cmap_bold, edgecolor = 'black')<br/>    plt.xlim(xx.min(), xx.max())<br/>    plt.ylim(yy.min(), yy.max())</span><span id="3877" class="mn lc iq mj b gy ms mp l mq mr">patch0 = mpatches.Patch(color='#FF0000', label='apple')<br/>    patch1 = mpatches.Patch(color='#00FF00', label='mandarin')<br/>    patch2 = mpatches.Patch(color='#0000FF', label='orange')<br/>    patch3 = mpatches.Patch(color='#AFAFAF', label='lemon')<br/>    plt.legend(handles=[patch0, patch1, patch2, patch3])</span><span id="6925" class="mn lc iq mj b gy ms mp l mq mr">plt.xlabel('height (cm)')<br/>plt.ylabel('width (cm)')<br/>plt.title("4-Class classification (k = %i, weights = '%s')"<br/>           % (n_neighbors, weights))    <br/>plt.show()</span><span id="4056" class="mn lc iq mj b gy ms mp l mq mr">plot_fruit_knn(X_train, y_train, 5, 'uniform')</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/957a8aae6788ff6bcd76ef18c7d4e7f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*ph9H_P2w29I6rAxR3a_UvA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 8</figcaption></figure><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9670" class="mn lc iq mj b gy mo mp l mq mr">k_range = range(1, 20)<br/>scores = []</span><span id="e447" class="mn lc iq mj b gy ms mp l mq mr">for k in k_range:<br/>    knn = KNeighborsClassifier(n_neighbors = k)<br/>    knn.fit(X_train, y_train)<br/>    scores.append(knn.score(X_test, y_test))<br/>plt.figure()<br/>plt.xlabel('k')<br/>plt.ylabel('accuracy')<br/>plt.scatter(k_range, scores)<br/>plt.xticks([0,5,10,15,20])</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/a126357516c01a33cfdb2b5179bc0454.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*BEpn_3r5yvCKEXaDA3RMMw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 9</figcaption></figure><p id="452d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于这个特定的日期集，当 k=5 时，我们获得最高的精确度。</p><h1 id="0c7d" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">摘要</h1><p id="30ce" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">在这篇文章中，我们关注的是预测的准确性。我们的目标是学习一个具有良好泛化性能的模型。这种模型最大限度地提高了预测精度。我们确定了最适合手头问题(即水果类型分类)的机器学习算法；因此，我们比较了不同的算法，并选择了性能最好的算法。</p><p id="6cc0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">创建这篇文章的源代码可以在<a class="ae la" href="https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Solving%20A%20Simple%20Classification%20Problem%20with%20Python.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。我将很高兴收到关于上述任何反馈或问题。</p></div></div>    
</body>
</html>