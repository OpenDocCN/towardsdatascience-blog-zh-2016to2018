<html>
<head>
<title>K-means Clustering Python Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-means 聚类 Python 示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-algorithms-part-9-k-means-example-in-python-f2ad05ed5203?source=collection_archive---------0-----------------------#2018-12-28">https://towardsdatascience.com/machine-learning-algorithms-part-9-k-means-example-in-python-f2ad05ed5203?source=collection_archive---------0-----------------------#2018-12-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1a6a9e0407f1893e97be02004e5b2221.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xL1tenKdF7n2j7vV.jpeg"/></div></div></figure><div class=""/><p id="0894" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">K-Means 聚类是一种无监督的机器学习算法。与传统的监督机器学习算法相比，K-Means 试图在没有首先用标记数据训练的情况下对数据进行分类。一旦运行了算法并定义了组，任何新数据都可以很容易地分配到最相关的组。</p><p id="45f6" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">K-Means 的实际应用包括:</p><ul class=""><li id="58db" class="kz la je kd b ke kf ki kj km lb kq lc ku ld ky le lf lg lh bi translated">客户特征分析</li><li id="e3d0" class="kz la je kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">市场分割</li><li id="f94a" class="kz la je kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">计算机视觉</li><li id="9f81" class="kz la je kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">搜索引擎</li><li id="7758" class="kz la je kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">天文学</li></ul><h1 id="0a61" class="ln lo je bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">它是如何工作的</h1><ol class=""><li id="eaef" class="kz la je kd b ke ml ki mm km mn kq mo ku mp ky mq lf lg lh bi translated">选择<strong class="kd jf"> K </strong>(即 2) <strong class="kd jf"> </strong>个随机点作为称为质心的聚类中心</li></ol><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mr"><img src="../Images/136558a108e67ec044067af1fc29a04b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EW-5MNBE3mxHS90KIBEAtQ.png"/></div></div></figure><p id="1f76" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">2.通过计算每个数据点相对于每个质心的距离，将每个数据点分配给最近的聚类</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mw"><img src="../Images/b5a81064d8de4d53e1e0d8dfbd1689eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6jruJmmnxtSOj4cuwv91Hg.png"/></div></div></figure><p id="539a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">3.通过计算指定点的平均值来确定新的聚类中心</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mx"><img src="../Images/f50c8de7afc0989b89128aed49804532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*blMy7iRv9R2ceDD2XILjbQ.png"/></div></div></figure><p id="c12e" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">4.重复第 2 步和第 3 步，直到集群分配没有任何变化</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/63734e75850771b450baf8a3a64fcae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GVQnG8FnP4L6jbHsO3oWOQ.png"/></div></div></figure><h1 id="ae06" class="ln lo je bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">选择正确的集群数量</h1><p id="89bd" class="pw-post-body-paragraph kb kc je kd b ke ml kg kh ki mm kk kl km mz ko kp kq na ks kt ku nb kw kx ky im bi translated">通常情况下，您要处理的数据会有多个维度，因此很难可视化。因此，最佳聚类数不再明显。幸运的是，我们有办法从数学上确定这一点。</p><p id="0e28" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们绘制了聚类数和聚类平方和(WCSS)之间的关系，然后我们选择了 WCSS 变化开始变平的聚类数(肘形法)。</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/e31dc9a8fa2c179a04933c88d3d1a673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vLTnh9xdgHvyC8WDNwcQQw.png"/></div></div></figure><p id="6fe2" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">WCSS 被定义为集群的每个成员与其质心之间的平方距离之和。</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/e8cebb77302c90da389fd0a8412ccfc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*bgpKrYZIVBuDirYk0JMnGg.png"/></div></figure><p id="3f28" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">例如，为图 1 的<em class="ne">计算的 WCSS 将大于为图 2 </em>的<em class="ne">计算的 WCSS。</em></p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nf"><img src="../Images/d7074c2e62c5cb4aa4cccc157b38beca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0naSz4RFw_m5VqiRXo2SRw.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">Figure 1</figcaption></figure><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/714073693875f9e84b6c6bfc24132274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vNsFrDUvGn9yTjlnXLgW8A.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">Figure 2</figcaption></figure><h1 id="5e49" class="ln lo je bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">密码</h1><p id="363a" class="pw-post-body-paragraph kb kc je kd b ke ml kg kh ki mm kk kl km mz ko kp kq na ks kt ku nb kw kx ky im bi translated">让我们看看如何使用 python 的 K-Means 算法对数据进行分类。和往常一样，我们需要从导入所需的库开始。</p><pre class="ms mt mu mv gt nl nm nn no aw np bi"><span id="3020" class="nq lo je nm b gy nr ns l nt nu">import numpy as np<br/>import pandas as pd<br/>from matplotlib import pyplot as plt<br/>from sklearn.datasets.samples_generator import make_blobs<br/>from sklearn.cluster import KMeans</span></pre><p id="1367" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在本教程中，我们将使用来自<code class="fe nv nw nx nm b">sklearn.datasets</code>模块的<code class="fe nv nw nx nm b">make_blobs</code>函数生成我们自己的数据。<code class="fe nv nw nx nm b">centers</code>参数指定了集群的数量。</p><pre class="ms mt mu mv gt nl nm nn no aw np bi"><span id="20a8" class="nq lo je nm b gy nr ns l nt nu">X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)</span><span id="a915" class="nq lo je nm b gy ny ns l nt nu">plt.scatter(X[:,0], X[:,1])</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/6451e08b49a32b065232a2272d9048a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xq-cgzxLvbQGtbG4Aasskg.png"/></div></figure><p id="c61d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">即使我们已经知道了最佳的集群数量，我认为我们仍然可以从使用<strong class="kd jf">肘方法</strong>来确定它中获益。为了获得图表中使用的值，我们使用不同数量的分类来训练多个模型，并且每次都存储<code class="fe nv nw nx nm b">intertia_</code>属性(WCSS)的值。</p><pre class="ms mt mu mv gt nl nm nn no aw np bi"><span id="4dcf" class="nq lo je nm b gy nr ns l nt nu">wcss = []</span><span id="4c75" class="nq lo je nm b gy ny ns l nt nu">for i in range(1, 11):<br/>    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)<br/>    kmeans.fit(X)<br/>    wcss.append(kmeans.inertia_)<br/>plt.plot(range(1, 11), wcss)<br/>plt.title('Elbow Method')<br/>plt.xlabel('Number of clusters')<br/>plt.ylabel('WCSS')<br/>plt.show()</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/110578507d3cd7099705d5a844badff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kns0UCH5o0p40y8H_nY1SQ.png"/></div></div></figure><p id="cb79" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">接下来，我们将使用上一步中确定的最佳聚类数(4)对数据进行分类。<code class="fe nv nw nx nm b">k-means++</code>确保你得到的东西不会落入随机初始化陷阱。</p><pre class="ms mt mu mv gt nl nm nn no aw np bi"><span id="93b0" class="nq lo je nm b gy nr ns l nt nu">kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, n_init=10, random_state=0)<br/>pred_y = kmeans.fit_predict(X)</span><span id="8f9c" class="nq lo je nm b gy ny ns l nt nu">plt.scatter(X[:,0], X[:,1])<br/>plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')<br/>plt.show()</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/ad5c419ac68b90a5b3ad31a3a1bb9eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*a6D6TJ3SxROEalT8Yn3G4w.png"/></div></figure></div></div>    
</body>
</html>