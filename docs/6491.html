<html>
<head>
<title>How To Design A Spam Filtering System with Machine Learning Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用机器学习算法设计垃圾邮件过滤系统</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/email-spam-detection-1-2-b0e06a5c0472?source=collection_archive---------2-----------------------#2018-12-16">https://towardsdatascience.com/email-spam-detection-1-2-b0e06a5c0472?source=collection_archive---------2-----------------------#2018-12-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6919" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">探索、绘制和可视化您的数据</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9051e2bb481b1a6c73871b18c42d8eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZoXV9KzoK8E8Ih0Ekfm1Pg.png"/></div></div></figure><p id="9c97" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">作为软件开发人员，电子邮件是非常重要的沟通工具之一。为了进行有效的通信，垃圾邮件过滤是一个重要的功能。</p><p id="49ea" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">那么垃圾邮件过滤系统实际上是如何工作的呢？我们能从零开始设计类似的东西吗？</p><h1 id="c5ca" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">概述</h1><p id="89f5" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">本文这两部分的主要目的是展示如何从头开始设计垃圾邮件过滤系统。</p><p id="af65" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">本文概述如下:</p><ol class=""><li id="a741" class="mk ml iq kt b ku kv kx ky la mm le mn li mo lm mp mq mr ms bi translated">探索性数据分析</li><li id="6ac9" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">数据预处理</li><li id="202f" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">特征抽出</li><li id="91a8" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">评分和指标</li><li id="06f3" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">利用嵌入+神经网络进行改进(下)</li><li id="9726" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">最大似然算法与深度学习的比较(下)</li></ol><p id="9dda" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">我们开始吧！</strong></p><h1 id="03bc" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">探索性数据分析</h1><p id="c02d" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">探索性数据分析是数据科学中一个非常重要的过程。它帮助数据科学家理解手头的数据，并将其与业务上下文联系起来。</p><p id="0dbf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我将在可视化和分析我的数据中使用的开源工具是<strong class="kt ir"> Word Cloud。</strong></p><p id="f1b5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">Word Cloud 是一个用于表示文本数据的数据可视化工具。图像中文本的<strong class="kt ir">大小</strong>代表训练数据中单词的<strong class="kt ir">频率或重要性</strong>。</p><p id="273e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这一部分要采取的步骤:</p><ol class=""><li id="0fa6" class="mk ml iq kt b ku kv kx ky la mm le mn li mo lm mp mq mr ms bi translated">获取电子邮件数据</li><li id="b5c7" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">探索和分析数据</li><li id="c931" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">使用文字云和条形图可视化培训数据</li></ol><p id="92f3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">获取垃圾邮件数据</strong></p><p id="47ba" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我们开发任何有意义的算法之前，数据是必不可少的成分。知道从哪里获取数据可能是一个非常方便的工具，尤其是当你只是一个初学者的时候。</p><p id="bb94" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面是几个著名的仓库，在那里你可以很容易地免费获得上千种数据集</p><ol class=""><li id="2f1c" class="mk ml iq kt b ku kv kx ky la mm le mn li mo lm mp mq mr ms bi translated"><a class="ae my" href="https://archive.ics.uci.edu/ml/index.php" rel="noopener ugc nofollow" target="_blank">加州大学欧文分校机器学习知识库</a></li><li id="0c07" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated"><a class="ae my" href="https://www.kaggle.com/datasets" rel="noopener ugc nofollow" target="_blank"> Kaggle 数据集</a></li><li id="9e88" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated"><a class="ae my" href="https://registry.opendata.aws/" rel="noopener ugc nofollow" target="_blank">自动气象站数据集</a></li></ol><p id="8a60" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于这个邮件垃圾数据集，是垃圾刺客分发的，可以点击这个<a class="ae my" href="https://spamassassin.apache.org/old/publiccorpus/" rel="noopener ugc nofollow" target="_blank">链接</a>去数据集。有几类数据，你可以阅读<em class="mz">readme.html</em>来获得更多关于这些数据的背景信息。</p><p id="3b10" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">简而言之，这个存储库中存在两种类型的数据，即<strong class="kt ir">垃圾数据</strong>(非垃圾数据)和<strong class="kt ir">垃圾数据</strong>。此外，在垃圾邮件数据中，有简单和困难，这意味着有一些非垃圾邮件数据与垃圾邮件数据具有非常高的相似性。这可能会给我们的系统做出决定带来困难。</p><p id="7a40" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果您使用的是 Linux 或 Mac，只需在终端中这样做，<em class="mz"> wget </em>只是一个命令，它可以帮助您下载给定 url 的文件:</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="7b4e" class="nf lo iq nb b gy ng nh l ni nj">wget <a class="ae my" href="https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2" rel="noopener ugc nofollow" target="_blank">https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2</a></span><span id="aec4" class="nf lo iq nb b gy nk nh l ni nj">wget <a class="ae my" href="https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2" rel="noopener ugc nofollow" target="_blank">https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2</a></span><span id="2acb" class="nf lo iq nb b gy nk nh l ni nj">wget <a class="ae my" href="https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2" rel="noopener ugc nofollow" target="_blank">https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2</a></span><span id="51d0" class="nf lo iq nb b gy nk nh l ni nj">wget <a class="ae my" href="https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2" rel="noopener ugc nofollow" target="_blank">https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2</a></span><span id="3f87" class="nf lo iq nb b gy nk nh l ni nj">wget <a class="ae my" href="https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2" rel="noopener ugc nofollow" target="_blank">https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2</a></span></pre><p id="e365" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们运行一些代码，看看所有这些电子邮件的内容！</p><p id="e79c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">探索和分析数据</strong></p><p id="4a2d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们看一下邮件内容，对数据有一个基本的了解</p><h2 id="d966" class="nf lo iq bd lp nl nm dn lt nn no dp lx la np nq lz le nr ns mb li nt nu md nv bi translated">火腿</h2><p id="e072" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">这看起来像是给另一个人的普通邮件回复，这不难归类为一个火腿:</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="4b5c" class="nf lo iq nb b gy ng nh l ni nj">This is a bit of a messy solution but might be useful -<br/><br/>If you have an internal zip drive (not sure about external) and<br/>you bios supports using a zip as floppy drive, you could <br/>use a bootable zip disk with all the relevant dos utils.</span></pre><h2 id="bef6" class="nf lo iq bd lp nl nm dn lt nn no dp lx la np nq lz le nr ns mb li nt nu md nv bi translated">硬火腿(火腿电子邮件是棘手的)</h2><p id="146c" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">硬火腿确实更难从垃圾数据中区分出来，因为它们包含一些关键词，如<em class="mz">限时订单、特殊“返校”优惠</em>，这使其非常可疑！</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="81ff" class="nf lo iq nb b gy ng nh l ni nj">Hello Friends!<br/><br/>We hope you had a pleasant week. Last weeks trivia questions was:<br/><br/><br/>What do these 3 films have in common: One Crazy Summer, Whispers in the =<br/>Dark, Moby Dick?=20<br/><br/>Answer: Nantucket Island<br/><br/><br/><br/>Congratulations to our Winners:<br/><br/>Caitlin O. of New Bedford, Massachusetts<br/><br/>Brigid M. of Marblehead, Massachusetts</span><span id="7b39" class="nf lo iq nb b gy nk nh l ni nj"><br/><br/>Special "Back to School" Offer!<br/><br/>For a limited time order our "Back to School" Snack Basket and receive =<br/>20% Off &amp; FREE SHIPPING!</span></pre><h2 id="2ac6" class="nf lo iq bd lp nl nm dn lt nn no dp lx la np nq lz le nr ns mb li nt nu md nv bi translated">罐头猪肉</h2><p id="6c12" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">其中一个垃圾邮件训练数据看起来确实像垃圾邮件文件夹中的垃圾广告邮件:</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="757b" class="nf lo iq nb b gy ng nh l ni nj">IMPORTANT INFORMATION:<br/><br/>The new domain names are finally available to the general public at discount prices. Now you can register one of the exciting new .BIZ or .INFO domain names, as well as the original .COM and .NET names for just $14.95. These brand new domain extensions were recently approved by ICANN and have the same rights as the original .COM and .NET domain names. The biggest benefit is of-course that the .BIZ and .INFO domain names are currently more available. i.e. it will be much easier to register an attractive and easy-to-remember domain name for the same price.  Visit: <a class="ae my" href="http://www.affordable-domains.com" rel="noopener ugc nofollow" target="_blank">http://www.affordable-domains.com</a> today for more info.<br/></span></pre><h1 id="635f" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated"><strong class="ak">可视化</strong></h1><p id="d483" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated"><strong class="kt ir"> Wordcloud </strong></p><p id="ae63" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">Wordcloud 是一个非常有用的可视化工具，可以让你粗略估计出在你的数据中出现频率最高的单词。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/9a5740bcf7267b1d82a3b70f30a93302.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QcsZSm5Bj4Vo-6DTJUizwQ.png"/></div></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">Visualization for spam email</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/8f7a3cc7c02cb18c66bb619c49e557c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yJ-ODGRYXoUIiVatjKMlRg.png"/></div></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">Visualization for non spam email</figcaption></figure><p id="6dd6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">从这个图像中，您可以注意到垃圾邮件的一些有趣之处。他们中的许多人有大量的“垃圾”词，如:免费，金钱，产品等。在设计垃圾邮件检测系统时，有了这种意识可能有助于我们做出更好的决策。</p><p id="b049" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">需要注意的一点是，单词云只显示单词的频率，不一定显示单词的重要性。因此，在可视化数据之前，有必要进行一些数据清理，例如<strong class="kt ir">从数据中删除停用词、标点符号</strong>等。</p><p id="11fd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> N 元模型可视化</strong></p><p id="275c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">另一种可视化技术是利用条形图并显示出现频率最高的单词。N-gram 的意思是，当你计算单词的频率时，你把多少个单词作为一个单位来考虑。</p><p id="7b20" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在本文中，我展示了 1-gram 和 2-gram 的例子。你绝对可以尝试更大的 n-gram 模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/3e0f49a7d324ca2ed85ca6b141828645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4r0AUyzW6qG0YJReV6vEEA.png"/></div></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">Bar chart visualization of 1-gram model</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/e842fa866e3f09fb8a4b0d16986a3d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pIuHGMk40M6xkHHB8o1dIw.png"/></div></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">Bar chart visualization of 2-gram model</figcaption></figure><h1 id="cb88" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated"><strong class="ak">列车试分裂</strong></h1><p id="abb5" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">将您的数据集分为<strong class="kt ir">训练集</strong>和<strong class="kt ir">测试集</strong>非常重要，这样您就可以在将模型部署到生产环境之前使用测试集来评估模型的性能。</p><p id="d3da" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在进行训练测试分割时，需要注意的一件重要事情是确保训练集和测试集之间的数据分布是相似的。</p><p id="4199" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这种情况下，这意味着垃圾邮件在训练集和测试集中的百分比应该相似。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/4f2159ad2f9f3344ad30d770147408ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gf8EwdBiWdxZHY4j1r9wqA.png"/></div></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">Target Count For Train Data</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/97f8e5114e43b2982268bdc3675754f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*d1gosJDGJmD5i9l6mW9Gzw.png"/></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">Train Data Distribution</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/0a0caf92c05131230e80ae25c9414481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EQ9PERbEp4iUj8CngjFEoA.png"/></div></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">Target Count For Test Data</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/d43316c234b387e349fa29b89ca007a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*7IkW7FhbhHUkO0uehirjXQ.png"/></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">Test Data Distribution</figcaption></figure><p id="3fd7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">训练数据和测试数据之间的分布非常相似，大约为 20–21%，所以我们可以开始处理我们的数据了！</p><h1 id="d63f" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">数据预处理</h1><p id="c1bf" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated"><strong class="kt ir">文字清理</strong></p><p id="ece4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">文本清理是机器学习中非常重要的一步，因为你的数据可能包含许多噪音和不需要的字符，如标点符号、空白、数字、超链接等。</p><p id="9040" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">人们通常使用的一些标准程序是:</p><ul class=""><li id="57ef" class="mk ml iq kt b ku kv kx ky la mm le mn li mo lm og mq mr ms bi translated">将所有字母转换为小写/大写</li><li id="2c22" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm og mq mr ms bi translated">移除数字</li><li id="81d5" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm og mq mr ms bi translated">删除标点符号</li><li id="f4cc" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm og mq mr ms bi translated">删除空白</li><li id="305c" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm og mq mr ms bi translated">删除超链接</li><li id="7dfa" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm og mq mr ms bi translated">删除停用词，如<em class="mz"> a、about、above、down、doing </em>等等</li><li id="5691" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm og mq mr ms bi translated"><strong class="kt ir">词干</strong></li><li id="30ee" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm og mq mr ms bi translated"><strong class="kt ir">单词词条化</strong></li></ul><p id="d996" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对大多数人来说，这两种技术可能是陌生的，它们是<strong class="kt ir">单词词干</strong>和<strong class="kt ir">单词词条化</strong>。这两种技术都试图将单词简化为最基本的形式，但采用的方法不同。</p><ul class=""><li id="7a2a" class="mk ml iq kt b ku kv kx ky la mm le mn li mo lm og mq mr ms bi translated">词干提取—词干提取算法通过使用该语言中常见的前缀和后缀列表来删除单词的结尾或开头。英语单词词干的示例如下:</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oh oi l"/></div></figure><ul class=""><li id="b81a" class="mk ml iq kt b ku kv kx ky la mm le mn li mo lm og mq mr ms bi translated">单词词汇化——词汇化是利用特定语言的词典，并试图将单词转换回其基本形式。它会试着考虑动词的意思，然后 conv 把它还原成最合适的基本形式。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="6d76" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">实现这两种算法可能很棘手，需要大量的思考和设计来处理不同的边缘情况。</p><p id="c6b2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">幸运的是<strong class="kt ir"> NLTK </strong>库已经提供了这两个算法的实现，所以我们可以从库中开箱即用！</p><p id="3619" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">导入库，开始设计一些函数，帮助我们理解这两个算法的基本工作原理。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="1971" class="nf lo iq nb b gy ng nh l ni nj"># Just import them and use it</span><span id="5153" class="nf lo iq nb b gy nk nh l ni nj">from nltk.stem import PorterStemmer<br/>from nltk.stem import WordNetLemmatizer</span><span id="94b7" class="nf lo iq nb b gy nk nh l ni nj">stemmer = PorterStemmer()<br/>lemmatizer = WordNetLemmatizer()</span><span id="2bea" class="nf lo iq nb b gy nk nh l ni nj">dirty_text = "He studies in the house yesterday, unluckily, the fans breaks down"</span><span id="5ef7" class="nf lo iq nb b gy nk nh l ni nj">def word_stemmer(words):<br/>    stem_words = [stemmer.stem(o) for o in words]<br/>    return " ".join(stem_words)</span><span id="45f6" class="nf lo iq nb b gy nk nh l ni nj">def word_lemmatizer(words):<br/>   lemma_words = [lemmatizer.lemmatize(o) for o in words]<br/>   return " ".join(lemma_words)</span></pre><p id="117d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">词干分析器的输出非常明显，一些词尾被砍掉了</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="219b" class="nf lo iq nb b gy ng nh l ni nj">clean_text = word_stemmer(dirty_text.split(" "))<br/>clean_text</span><span id="4db3" class="nf lo iq nb b gy nk nh l ni nj">#Output<br/>'He studi in the hous yesterday, unluckily, the fan break down'</span></pre><p id="1ec5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">词汇化已转换为研究-&gt;研究，休息-&gt;休息</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="d371" class="nf lo iq nb b gy ng nh l ni nj">clean_text = word_lemmatizer(dirty_text.split(" "))<br/>clean_text</span><span id="9ee2" class="nf lo iq nb b gy nk nh l ni nj">#Output<br/>'I study in the house yesterday, unluckily, the fan break down'</span></pre><h1 id="f25b" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">特征抽出</h1><p id="ac69" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">我们的算法总是期望输入是整数/浮点数，所以我们需要在中间有一些特征提取层来将单词转换成整数/浮点数。</p><p id="2dc0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">有几种方法可以做到这一点，今天我将向大家介绍:</p><ol class=""><li id="3c95" class="mk ml iq kt b ku kv kx ky la mm le mn li mo lm mp mq mr ms bi translated">计数矢量器</li><li id="1d61" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">tfidf 矢量器</li><li id="7844" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">单词嵌入</li></ol><p id="47d6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">计数矢量器</strong></p><p id="9a54" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">首先，我们需要将所有训练数据输入到 CountVectorizer 中，CountVectorizer 将保存每个单词及其各自 id 的字典，该 id 将与整个训练集中该单词的字数相关。</p><p id="a99b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">比如像<em class="mz">‘我喜欢吃苹果，喝苹果汁’这样的句子</em></p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="139e" class="nf lo iq nb b gy ng nh l ni nj">from sklearn.feature_extraction.text import CountVectorizer</span><span id="9d5d" class="nf lo iq nb b gy nk nh l ni nj"># list of text documents</span><span id="c4bf" class="nf lo iq nb b gy nk nh l ni nj">text = ["I like to eat apple and drink apple juice"]</span><span id="d7ac" class="nf lo iq nb b gy nk nh l ni nj"># create the transform</span><span id="c5a5" class="nf lo iq nb b gy nk nh l ni nj">vectorizer = CountVectorizer()</span><span id="e255" class="nf lo iq nb b gy nk nh l ni nj"># tokenize and build vocab</span><span id="cc1d" class="nf lo iq nb b gy nk nh l ni nj">vectorizer.fit(text)</span><span id="e4a4" class="nf lo iq nb b gy nk nh l ni nj"># summarize</span><span id="8b4f" class="nf lo iq nb b gy nk nh l ni nj">print(vectorizer.vocabulary_)</span><span id="eb87" class="nf lo iq nb b gy nk nh l ni nj"># encode document</span><span id="fc35" class="nf lo iq nb b gy nk nh l ni nj">vector = vectorizer.transform(text)</span><span id="955d" class="nf lo iq nb b gy nk nh l ni nj"># summarize encoded vector</span><span id="27de" class="nf lo iq nb b gy nk nh l ni nj">print(vector.shape)</span><span id="bff8" class="nf lo iq nb b gy nk nh l ni nj">print(type(vector))</span><span id="57c9" class="nf lo iq nb b gy nk nh l ni nj">print(vector.toarray())</span><span id="dede" class="nf lo iq nb b gy nk nh l ni nj"># Output</span><span id="7336" class="nf lo iq nb b gy nk nh l ni nj"># The number follow by the word are the index of the word<br/>{'like': 5, 'to': 6, 'eat': 3, 'apple': 1, 'and': 0, 'drink': 2, 'juice': 4}</span><span id="c6ab" class="nf lo iq nb b gy nk nh l ni nj"># The index relates to the position of the word count array below<br/># "I like to eat apple and drink apple juice" -&gt; [1 2 1 1 1 1 1]</span><span id="6d5a" class="nf lo iq nb b gy nk nh l ni nj"># apple which has the index 1 correspond to the word count of 2 in the array</span></pre><p id="bc4f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">tfidf 矢量器</strong></p><p id="7591" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">字数不错，但我们能做得更好吗？简单字数统计的一个问题是，像“the”、“and”这样的词会出现很多次，它们并没有真正增加太多有意义的信息。</p><p id="5cde" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">另一个流行的选择是 TfidfVectorizer。除了计算每个单词(经常出现在多个文档或句子中的单词)的字数，矢量器还会尝试缩小它们的大小。</p><p id="d387" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">关于 CountVectorizer 和 TfidfVectorizer 的更多信息，请阅读这篇伟大的<a class="ae my" href="https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/" rel="noopener ugc nofollow" target="_blank">文章</a>，这也是我获得大部分理解的地方。</p><p id="caaa" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">字嵌入</strong></p><p id="dc72" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">网上有很多很棒的文章解释了单词嵌入的细节和生成它们的算法。所以在这里，我将跳过其中的大部分，试着给你一个大概的概念。</p><p id="c0cf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">单词嵌入试图将单词转换成矢量化格式，该矢量表示该单词在高维空间中的位置。</p><p id="0bff" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于具有相似含义的单词，这两个单词向量的余弦距离会更短，并且它们会彼此更接近。</p><p id="a88f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">事实上，这些单词是向量，所以你甚至可以对它们进行数学运算！这些操作的最终结果将是映射到单词的另一个向量。出乎意料的是，那些操作产生了一些惊人的结果！</p><blockquote class="oj"><p id="00a4" class="ok ol iq bd om on oo op oq or os lm dk translated">例 1:国王-男人+女人=王后</p><p id="2d1a" class="ok ol iq bd om on oo op oq or os lm dk translated">示例 2:马德里-西班牙+法国=巴黎</p><p id="793b" class="ok ol iq bd om on oo op oq or os lm dk translated">例 3:步行-游泳+游泳=步行</p></blockquote><p id="3ef8" class="pw-post-body-paragraph kr ks iq kt b ku ot jr kw kx ou ju kz la ov lc ld le ow lg lh li ox lk ll lm ij bi translated">简而言之，单词嵌入是单词的一种非常强大的表示，生成这种嵌入的一种众所周知的技术是<em class="mz"> Word2Vec。</em></p><p id="89d2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">呼！将所有的句子转换成某种形式的向量后，就到了我们文章最精彩的部分→ <strong class="kt ir">算法实现！</strong></p><h1 id="32d1" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">算法实现</h1><p id="2103" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated"><strong class="kt ir">tfidf 矢量器+朴素贝叶斯算法</strong></p><p id="64a8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我采用的第一种方法是使用 tfidf 矢量器作为特征提取工具，并使用朴素贝叶斯算法进行预测。朴素贝叶斯是一种简单的概率型传统机器学习算法。</p><p id="97b2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">即使在过去，它在解决垃圾邮件检测等问题时也非常流行。朴素贝叶斯的细节可以在这篇由<strong class="kt ir"> Devi Soni </strong>撰写的<a class="ae my" rel="noopener" target="_blank" href="/introduction-to-naive-bayes-classification-4cffabb1ae54">文章</a>中查阅，这篇文章简明清晰地解释了朴素贝叶斯算法的理论。</p><p id="9641" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用<strong class="kt ir"> sklearn </strong>库提供的朴素贝叶斯库，让我们自己实现这个算法省去了很多麻烦。我们可以用几行代码轻松完成这项工作</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="993a" class="nf lo iq nb b gy ng nh l ni nj">from sklearn.naive_bayes import GaussianNB</span><span id="6270" class="nf lo iq nb b gy nk nh l ni nj">clf.fit(x_train_features.toarray(),y_train)</span><span id="e27f" class="nf lo iq nb b gy nk nh l ni nj"># Output of the score is the accuracy of the prediction<br/># Accuracy: 0.995<br/>clf.score(x_train_features.toarray(),y_train)</span><span id="0232" class="nf lo iq nb b gy nk nh l ni nj"># Accuracy: 0.932<br/>clf.score(x_test_features.toarray(),y_test)</span></pre><p id="6793" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们达到了 93.2%的准确率。但是准确性并不是评价算法性能的唯一指标。让我们尝试其他评分标准，这可能有助于我们彻底了解该模型的表现。</p><h1 id="59ab" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated"><strong class="ak">评分&amp;指标</strong></h1><p id="f291" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated"><strong class="kt ir">准确度的缺点</strong></p><p id="700d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在评估数据科学模型的性能时，有时准确性可能不是最佳指标。</p><p id="1ff4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们在现实生活中解决的一些问题可能有一个非常不平衡的类，使用准确性可能不会给我们足够的信心来理解算法的性能。</p><p id="67d3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我们试图解决的垃圾邮件问题中，垃圾邮件数据约占我们数据的 20%。如果我们的算法预测所有的电子邮件都是非垃圾邮件，它将达到 80%的准确率。</p><p id="5ba6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于一些只有 1%正面数据的问题，预测所有样本都是负面的会给他们 99%的准确率，但我们都知道这种模型在现实生活中是没有用的。</p><p id="0d9a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">精度&amp;召回</strong></p><p id="14cf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">查准率和查全率是人们评价类不平衡分类模型时常用的评价指标。</p><p id="3a8a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们试着理解 Precision &amp; Recall 试图回答什么问题，</p><p id="e0db" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">精度</strong>:实际上有多少比例的肯定识别是正确的？</p><p id="5e22" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">回忆</strong>:实际阳性中有多少比例被正确识别？</p><p id="5154" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">所以，<strong class="kt ir"> precision </strong>是评估，当一个模型预测某件事是肯定的，这个模型有多精确。另一方面，<strong class="kt ir"> recall </strong>正在评估一个模型在寻找所有正样本方面做得如何。</p><p id="4af1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">精确率和召回率的数学等式分别是</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/e2aafed6ec6076f83ab733b9d7261381.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*zUDKhseb7L-xST-J4V7kAw.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/dc158b1a6e6008c57248ed64ac8368d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*YqW2ZoBP_l0fMfjX7km_KA.png"/></div></figure><p id="2fc4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">TP:真阳性</p><p id="e18b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">FP:假阳性</p><p id="3502" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">TN:真阴性</p><p id="3a77" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">FN:假阴性</p><p id="db28" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">混淆矩阵</strong></p><p id="5961" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">混淆矩阵是理解像真阳性、假阳性、真阴性等结果的一个非常好的方法。</p><p id="f1ec" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">Sklearn 文档提供了一个示例代码，说明如何绘制好看的混淆矩阵来可视化您的结果。你可以在这里查看<a class="ae my" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html" rel="noopener ugc nofollow" target="_blank">，</a>或者你可以在我在文章末尾分享的笔记本中找到代码。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/b3e6c673b2bec18562e3b063149004c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*P1WX3fHgSbgnt-sZljITDA.png"/></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">Confusion Matrix of the result</figcaption></figure><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="a493" class="nf lo iq nb b gy ng nh l ni nj">Precision: 87.82%<br/>Recall: 81.01%</span></pre><p id="ad95" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该模型的召回率相当低，可能在发现垃圾邮件方面做得不够好。<strong class="kt ir">我们如何做得比这更好？</strong></p><h1 id="21af" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated"><strong class="ak">总结</strong></h1><p id="573d" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">在本文中，我向您展示了设计垃圾邮件检测算法所需的所有必要步骤。简单回顾一下:</p><ol class=""><li id="8238" class="mk ml iq kt b ku kv kx ky la mm le mn li mo lm mp mq mr ms bi translated">探索和理解您的数据</li><li id="18f2" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">将手头的数据可视化以获得更好的直觉——word cloud，N-gram 条形图</li><li id="c4be" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">文本清理—单词斯特梅尔和单词词条化</li><li id="d78f" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">特征提取—计数矢量器、Tfidf 矢量器、单词嵌入</li><li id="8e97" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">算法—朴素贝叶斯</li><li id="2b48" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">评分和指标——准确度、精确度、召回率</li></ol><p id="5954" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">垃圾邮件检测算法设计演示的第一部分到此结束。</p><p id="1470" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">还有这篇文章的第 2 部分！我将展示如何通过使用<strong class="kt ir">单词嵌入</strong>和<strong class="kt ir">深度学习模型来提高模型的准确度、精确度和召回率。</strong>有兴趣的话在这里  <strong class="kt ir"> </strong>阅读文章<a class="ae my" rel="noopener" target="_blank" href="/spam-filtering-system-with-deep-learning-b8070b28f9e0">！</a></p><p id="349e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">你可以从<a class="ae my" href="https://github.com/huai99/Email-Spam-Detection-Python" rel="noopener ugc nofollow" target="_blank"> Github </a>克隆笔记本，或者直接从<a class="ae my" href="https://colab.research.google.com/drive/1nh4bWccs7cgOx7kSalBmUMJzK3h7UEvU" rel="noopener ugc nofollow" target="_blank"> Colab </a>运行。请在下面的评论区留下你的任何问题。</p></div></div>    
</body>
</html>