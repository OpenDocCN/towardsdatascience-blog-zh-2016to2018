<html>
<head>
<title>Principal Component Analysis: Your Tutorial and Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析:你的教程和代码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-your-tutorial-and-code-9719d3d3f376?source=collection_archive---------5-----------------------#2018-11-09">https://towardsdatascience.com/principal-component-analysis-your-tutorial-and-code-9719d3d3f376?source=collection_archive---------5-----------------------#2018-11-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/7c9c92e839188c1a55626a85d56eaff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*30eLSgygLAQAyBxHrjC9sQ.png"/></div></figure><blockquote class="jx jy jz"><p id="2dc9" class="ka kb kc kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">想获得灵感？快来加入我的<a class="ae kz" href="https://www.superquotes.co/?utm_source=mediumtech&amp;utm_medium=web&amp;utm_campaign=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu">超级行情快讯</strong> </a>。😎</p></blockquote><p id="d7e9" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">你的数据是你的机器学习模型的生命燃料。总是有许多 ML 技术可供选择并应用于特定的问题，但是没有大量的好数据，你不会走得很远。在机器学习应用程序中，数据通常是大多数性能提升背后的驱动因素。</p><p id="cf34" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">有时这些数据可能很复杂。你拥有太多，以至于理解它的全部含义以及哪些部分是真正重要的<em class="kc"/>可能很有挑战性。简而言之，降维是一种帮助我们从宏观层面更好地理解数据的技术。它减少了数据集的特征数量，这样我们只剩下最重要的部分。</p><p id="f837" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">主成分分析(PCA)是一种简单而强大的降维技术。通过它，我们可以直接减少特征变量的数量，从而缩小重要特征的范围并节省计算量。从高层次来看，PCA 有三个主要步骤:</p><p id="7a27" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">(1)计算数据的协方差矩阵</p><p id="1d9a" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">(2)计算该协方差矩阵的特征值和向量</p><p id="26a8" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">(3)使用特征值和向量只选择最重要的特征向量，然后将数据转换到这些向量上以减少维数！</p><p id="366f" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">整个过程如上图所示，其中我们的数据已经从 1000 个点的三维空间转换到 100 个点的二维空间。这在计算上节省了 10 倍！</p><h1 id="fbb6" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">(1)计算协方差矩阵</h1><p id="0627" class="pw-post-body-paragraph ka kb it kd b ke mb kg kh ki mc kk kl la md ko kp lb me ks kt lc mf kw kx ky im bi translated">PCA 产生一个特征子空间，该特征子空间使沿着特征向量的方差最大化。因此，为了适当地测量这些特征向量的方差，它们必须被适当地平衡。为了实现这一点，我们首先归一化我们的数据，使其具有零均值和单位方差，这样在我们的计算中每个特征将被同等地加权。假设我们的数据集叫做<em class="kc"> X </em>:</p><figure class="mg mh mi mj gt ju"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="b221" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">两个变量的协方差衡量它们的“相关”程度。如果两个变量有一个正的协方差，那么当一个变量增加时，另一个也增加；对于负协方差，特征变量的值将在相反的方向上变化。协方差矩阵只是一个数组，其中每个值基于矩阵中的 x-y 位置指定两个特征变量之间的协方差。公式是:</p><figure class="mg mh mi mj gt ju gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/b800ce2766b3a0fdee8ecbdd0a503e06.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*kWwfHg0cbL1-4_8yX0VhlA.png"/></div></figure><p id="948b" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">其中顶部有线的<em class="kc"> x </em>是 x 的每个特征的平均值向量。请注意，当我们将转置矩阵乘以原始矩阵时，我们最终会将每个数据点的每个特征相乘！在 numpy 代码中，它看起来像这样:</p><figure class="mg mh mi mj gt ju"><div class="bz fp l di"><div class="mk ml l"/></div></figure><h1 id="22c9" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">(2)计算特征值和向量</h1><p id="801e" class="pw-post-body-paragraph ka kb it kd b ke mb kg kh ki mc kk kl la md ko kp lb me ks kt lc mf kw kx ky im bi translated">我们的协方差矩阵的本征向量(主分量)表示新特征空间的向量方向，本征值表示这些向量的幅度。因为我们在看我们的<em class="kc">协方差矩阵</em>，特征值<em class="kc">量化</em>每个向量的贡献方差。</p><p id="8bf0" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">如果一个特征向量具有一个相应的高量值的特征值，这意味着我们的数据在特征空间中沿着该向量具有高方差。因此，这个向量包含了我们数据的大量信息，因为沿着这个向量的任何移动都会导致很大的“差异”。另一方面，具有小特征值的向量具有低方差，因此当沿着该向量移动时，我们的数据不会变化很大。因为当沿着特定的特征向量移动时没有什么变化，即改变该特征向量的值不会对我们的数据产生很大影响，所以我们可以说该特征不是很重要，并且我们可以移除它。</p><p id="6502" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">这就是 PCA 中特征值和向量的全部本质。找出在表示我们的数据时最重要的向量，并丢弃其余的。在 numpy 中，计算我们的协方差矩阵的特征向量和值是一个简单的一行程序。之后，我们将根据特征值对特征向量进行降序排序。</p><figure class="mg mh mi mj gt ju"><div class="bz fp l di"><div class="mk ml l"/></div></figure><h1 id="65da" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">(3)到新向量的投影</h1><p id="4f2c" class="pw-post-body-paragraph ka kb it kd b ke mb kg kh ki mc kk kl la md ko kp lb me ks kt lc mf kw kx ky im bi translated">在这一点上，我们有一个特征向量列表，根据它们的特征值按照对数据集的“重要性”排序。现在我们要做的是选择我们需要的最重要的特征向量，并丢弃其余的。我们可以通过查看向量的<em class="kc">解释的方差百分比</em>来以一种聪明的方式做到这一点。这个百分比量化了有多少信息(方差)可以归因于总的 100%中的每个主成分。</p><p id="d349" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">我们举个例子来说明。假设我们有一个数据集，它最初有 10 个特征向量。计算协方差矩阵后，我们发现特征值为:</p><p id="b761" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi">[12, 10, 8, 7, 5, 1, 0.1, 0.03, 0.005, 0.0009]</p><p id="9816" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">这个数组的总和= 43.1359。但是第一个<strong class="kd iu">6</strong>T8】值代表:</p><p id="8bbf" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">42 / 43.1359 =总数的 99.68%！这意味着我们的前 5 个特征向量有效地保存了关于我们数据集的 99.68%的<em class="kc">方差</em>或<em class="kc">信息</em>。因此，我们可以丢弃最后 4 个特征向量，因为它们只包含 0.32%的信息，这对于节省 40%的计算来说是值得的牺牲！</p><p id="ff9d" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">因此，我们可以简单地定义一个阈值，根据该阈值我们可以决定是保留还是丢弃每个特征向量。在下面的代码中，我们简单地根据所选的 97%的阈值来计算我们想要保留的特征向量的数量。</p><figure class="mg mh mi mj gt ju"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="7880" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">最后一步是将我们的数据实际投影到我们决定保留的向量上。我们通过构建一个<em class="kc">投影矩阵</em>来做到这一点:这只是一个矩阵的花哨词，我们将乘以它来将我们的数据投影到新的向量上。为了创建它，我们简单地连接所有我们决定保留的特征向量。我们的最后一步是简单地取原始数据和投影矩阵之间的点积。</p><p id="b591" class="pw-post-body-paragraph ka kb it kd b ke kf kg kh ki kj kk kl la kn ko kp lb kr ks kt lc kv kw kx ky im bi translated">瞧啊。尺寸缩小！</p><figure class="mg mh mi mj gt ju"><div class="bz fp l di"><div class="mk ml l"/></div></figure></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="bcbb" class="ld le it bd lf lg mu li lj lk mv lm ln lo mw lq lr ls mx lu lv lw my ly lz ma bi translated">喜欢学习？</h1><p id="aed7" class="pw-post-body-paragraph ka kb it kd b ke mb kg kh ki mc kk kl la md ko kp lb me ks kt lc mf kw kx ky im bi translated">在<a class="ae kz" href="https://twitter.com/GeorgeSeif94" rel="noopener ugc nofollow" target="_blank">推特</a>上关注我，我会在那里发布所有最新最棒的人工智能、技术和科学！也在<a class="ae kz" href="https://www.linkedin.com/in/georgeseif/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系</p></div></div>    
</body>
</html>