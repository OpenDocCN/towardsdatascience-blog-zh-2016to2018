<html>
<head>
<title>Spam or Ham: Introduction to Natural Language Processing Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">垃圾邮件还是火腿:自然语言处理介绍第 2 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spam-or-ham-introduction-to-natural-language-processing-part-2-a0093185aebd?source=collection_archive---------2-----------------------#2018-09-30">https://towardsdatascience.com/spam-or-ham-introduction-to-natural-language-processing-part-2-a0093185aebd?source=collection_archive---------2-----------------------#2018-09-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/37e7610af07f5f2dd273f2a3bfac2723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1P2SlI-3rXc30CkY9PwX3A.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Spam … or ham?</figcaption></figure><p id="11a0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这是我的自然语言处理基础系列的第二部分。如果你还没看第一部分，可以在这里找到<a class="ae ld" href="https://medium.com/analytics-vidhya/introduction-to-natural-language-processing-part-1-777f972cc7b3" rel="noopener">。</a></p><p id="aaee" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在这一部分中，我们将从头到尾地介绍如何用 Python 3 构建一个非常简单的文本分类器。我们将使用<a class="ae ld" href="https://www.kaggle.com/uciml/sms-spam-collection-dataset/home" rel="noopener ugc nofollow" target="_blank">垃圾短信收集数据集</a>，该数据集根据短信是“垃圾短信”还是“垃圾短信”(非垃圾短信)来标记 5574 条短信。</p><p id="ef6b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们的目标是建立一个预测模型，它将确定一个文本消息是垃圾邮件还是火腿。代码见<a class="ae ld" href="https://github.com/happilyeverafter95/Medium/blob/master/spam_or_ham.py" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="6581" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">我们开始吧！</h1><p id="8e07" class="pw-post-body-paragraph kf kg it kh b ki mj kk kl km mk ko kp kq ml ks kt ku mm kw kx ky mn la lb lc im bi translated">导入数据后，我更改了列名，使其更具描述性。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="7aff" class="mx lm it mt b gy my mz l na nb">import pandas as pd</span><span id="a87a" class="mx lm it mt b gy nc mz l na nb">data = pd.read_csv("spam.csv", encoding = "latin-1")<br/>data = data[['v1', 'v2']]<br/>data = data.rename(columns = {'v1': 'label', 'v2': 'text'})</span></pre><p id="653e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们数据集的前几个条目如下所示:</p><figure class="mo mp mq mr gt ju gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/1e3ad7542dc71a505876b40dcb65076a.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*nLF2vsOMqC54p3YwfihvCw.png"/></div></figure><p id="de58" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">通过简单地浏览我们的数据，我们对我们正在处理的文本有了一些了解:口语。这个特定的数据集也有 87%的邮件被标记为“垃圾邮件”，13%的邮件被标记为“垃圾邮件”。当稍后评估我们的分类器的强度时，类别不平衡将变得重要。</p><h1 id="cb64" class="ll lm it bd ln lo ne lq lr ls nf lu lv lw ng ly lz ma nh mc md me ni mg mh mi bi translated">数据清理</h1><p id="df1e" class="pw-post-body-paragraph kf kg it kh b ki mj kk kl km mk ko kp kq ml ks kt ku mm kw kx ky mn la lb lc im bi translated">清理文本数据与常规数据清理略有不同。文本规范化比去除离群值或杠杆点更受重视。</p><p id="5f56" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">根据维基百科:</p><blockquote class="nj nk nl"><p id="f39e" class="kf kg nm kh b ki kj kk kl km kn ko kp nn kr ks kt no kv kw kx np kz la lb lc im bi translated">文本规范化是将文本转换成以前可能没有的单一规范形式的过程。</p></blockquote><p id="9e5e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">正确使用时，它可以减少噪音，将具有相似语义的术语分组，并通过给我们一个更小的矩阵来降低计算成本。</p><p id="567b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我将介绍几种常见的标准化方法，但是请记住，使用它们并不总是好主意。保留判断何时使用数据科学中人的因素的权利。</p><p id="40f4" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><a class="ae ld" href="https://en.wikipedia.org/wiki/No_free_lunch_theorem" rel="noopener ugc nofollow" target="_blank"> <strong class="kh iu">没有免费的午餐定理:</strong> </a>从来没有一个解决方案对所有的事情都有效。用您的数据集尝试它，以确定它是否适合您的特殊用例。</p><p id="a92e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">案例规范化</strong></p><p id="20d3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">大小写是否提供了关于文本消息是垃圾邮件还是垃圾邮件的任何相关信息？<em class="nm"> HELLO </em>和<em class="nm"> hello </em>或者<em class="nm"> Hello </em>语义相同吗？你可以根据以前的知识认为垃圾邮件倾向于使用更多的大写字母来吸引读者的注意力。或者，你可以争辩说这没有什么区别，所有的单词都应该简化为相同的大小写。</p><p id="14e2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">删除停止字</strong></p><p id="d9e4" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">停用词是没有增加预测价值的常用词，因为它们随处可见。它们类似于电影《T2:超人总动员》中的反派评论:</p><blockquote class="nj nk nl"><p id="17d9" class="kf kg nm kh b ki kj kk kl km kn ko kp nn kr ks kt no kv kw kx np kz la lb lc im bi translated"><em class="it">当每个人都是超级的时候，没有人会是【超级】</em>。</p></blockquote><figure class="mo mp mq mr gt ju gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/f014cb9b683529db25e7878aa82a084d.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*8uy16EOWCj23T1zESfCfxw.jpeg"/></div></figure><p id="cfb8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">英语中一些常见的停用词:</p><ul class=""><li id="98ba" class="nr ns it kh b ki kj km kn kq nt ku nu ky nv lc nw nx ny nz bi translated">我</li><li id="0d32" class="nr ns it kh b ki oa km ob kq oc ku od ky oe lc nw nx ny nz bi translated">a</li><li id="807b" class="nr ns it kh b ki oa km ob kq oc ku od ky oe lc nw nx ny nz bi translated">因为</li><li id="027c" class="nr ns it kh b ki oa km ob kq oc ku od ky oe lc nw nx ny nz bi translated">到</li></ul><p id="51e5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">关于什么时候删除停用词是个好主意，有很多争论。这种做法在许多信息检索任务(如搜索引擎查询)中使用，但在需要对语言进行语法理解时可能是有害的。</p><p id="f929" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">删除标点符号、特殊符号</strong></p><p id="0713" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们必须再次考虑标点符号和特殊符号对分类器预测能力的重要性。我们还必须考虑每个符号功能的重要性。例如，撇号允许我们定义缩写，并区分像<em class="nm"> it's </em>和<em class="nm"> its 这样的单词。</em></p><p id="edc3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">词干匹配/词干匹配</strong></p><p id="084e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这两种技术都减少了变形形式，使具有相同词条的单词规范化。词汇匹配和词干匹配的区别在于，词汇匹配通过考虑单词的上下文来进行这种归约，而词干匹配则不会。缺点是目前没有准确率非常高的 lemmatiser 或 stemmer。</p><figure class="mo mp mq mr gt ju gh gi paragraph-image"><div class="gh gi of"><img src="../Images/6a68ed98e62b4753e31d19de308ec995.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*ADy6WIGL9U9JA1jiDmVSUw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Examples of stemming from Stanford NLP</figcaption></figure></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><p id="2793" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">不太常见的规范化技术包括纠错、将单词转换为其词性或使用同义词词典映射同义词。除了错误纠正和同义词映射之外，还有许多用于其他规范化技术的预建工具，所有这些工具都可以在<a class="ae ld" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> <em class="nm"> nltk </em>库</a>中找到。</p><p id="daa7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于这个特殊的分类问题，我们将只使用案例规范化。基本原理是很难将词干分析器或词尾分析器应用到口语中，而且由于文本消息非常短，删除停用词可能不会给我们留下太多工作。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="66c2" class="mx lm it mt b gy my mz l na nb">def review_messages(msg):</span><span id="6db8" class="mx lm it mt b gy nc mz l na nb">    # converting messages to lowercase</span><span id="0399" class="mx lm it mt b gy nc mz l na nb">    msg = msg.lower()</span><span id="89ef" class="mx lm it mt b gy nc mz l na nb">    return msg</span></pre><p id="39a8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">作为参考，这个函数进行了大小写规范化，删除了停用词和词干。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="bfda" class="mx lm it mt b gy my mz l na nb">from nltk import stem</span><span id="7f92" class="mx lm it mt b gy nc mz l na nb">from nltk.corpus import stopwords</span><span id="0de3" class="mx lm it mt b gy nc mz l na nb">stemmer = stem.SnowballStemmer('english')</span><span id="a543" class="mx lm it mt b gy nc mz l na nb">stopwords = set(stopwords.words('english'))</span><span id="6097" class="mx lm it mt b gy nc mz l na nb"><br/>def alternative_review_messages(msg):</span><span id="f317" class="mx lm it mt b gy nc mz l na nb">    # converting messages to lowercase</span><span id="bdb9" class="mx lm it mt b gy nc mz l na nb">    msg = msg.lower()</span><span id="d15b" class="mx lm it mt b gy nc mz l na nb">    # removing stopwords</span><span id="e6b1" class="mx lm it mt b gy nc mz l na nb">    msg = [word for word in msg.split() if word not in stopwords]</span><span id="572e" class="mx lm it mt b gy nc mz l na nb">    # using a stemmer</span><span id="77ea" class="mx lm it mt b gy nc mz l na nb">    msg = " ".join([stemmer.stem(word) for word in msg])</span><span id="4a3b" class="mx lm it mt b gy nc mz l na nb">    return msg</span></pre><p id="6eea" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们应用第一个函数来归一化文本消息。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="ecdd" class="mx lm it mt b gy my mz l na nb">data['text'] = data['text'].apply(review_messages)</span></pre><h1 id="89cd" class="ll lm it bd ln lo ne lq lr ls nf lu lv lw ng ly lz ma nh mc md me ni mg mh mi bi translated">向量化文本</h1><p id="b3fa" class="pw-post-body-paragraph kf kg it kh b ki mj kk kl km mk ko kp kq ml ks kt ku mm kw kx ky mn la lb lc im bi translated">在本系列的第一部分中，我们探索了最基本类型的单词矢量器，单词袋模型，由于其简单性，它对于我们的垃圾邮件或业余爱好者分类器来说不是很好。</p><p id="0671" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">相反，我们将使用 TF-IDF 矢量器(术语频率—逆文档频率)，这是一种类似的嵌入技术，它考虑了每个术语对文档的重要性。</p><p id="6d13" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">虽然大多数矢量器都有其独特的优势，但使用哪一种并不总是很清楚。在我们的例子中，选择 TF-IDF 矢量器是因为它在对文本消息等文档进行矢量处理时简单而高效。</p><p id="6be5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">TF-IDF 通过计算文档和词汇表中每个术语之间的 TF-IDF 统计量来对文档进行矢量化。文档向量是通过使用每个统计信息作为向量中的一个元素来构建的。</p><figure class="mo mp mq mr gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/de8f8278747b87aa0aacf09c5e29f22c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n4s0LZS1Qi46pF3aaYzE0A.png"/></div></div></figure><p id="c0d4" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">文件<em class="nm"> j </em>中术语<em class="nm"> i </em>的 TF-IDF 统计量计算如下:</p><figure class="mo mp mq mr gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/fc5579c483f0e67fe0b24747205af5ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*eIDZG3Ot5DP8SKXAvBVALQ.png"/></div></figure><p id="2e0b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在解决了 TF-IDF 之后，我们必须决定矢量器的粒度。将每个单词指定为自己的术语的一种流行的替代方法是使用分词器。记号赋予器根据空白和特殊字符将文档分割成<em class="nm">记号</em>(从而将每个记号分配给它自己的术语)。</p><p id="54bb" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">例如，短语<em class="nm">what ' s how on</em>可能被拆分为<em class="nm"> what，s，going，on </em>。</strong></p><p id="878f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">标记器能够提取比单词级分析器更多的信息。然而，分词器不能很好地处理口语，可能会遇到拆分 URL 或电子邮件的问题。因此，我们将使用单词级分析器，它将每个单词分配给它自己的术语。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><p id="7414" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在训练矢量器之前，我们将数据分为训练集和测试集。我们 10%的数据用于测试。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="fa07" class="mx lm it mt b gy my mz l na nb">from sklearn.model_selection import train_test_split</span><span id="5d74" class="mx lm it mt b gy nc mz l na nb">X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size = 0.1, random_state = 1)</span><span id="d841" class="mx lm it mt b gy nc mz l na nb"># training the vectorizer </span><span id="6cb4" class="mx lm it mt b gy nc mz l na nb">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="818a" class="mx lm it mt b gy nc mz l na nb">vectorizer = TfidfVectorizer()</span><span id="a355" class="mx lm it mt b gy nc mz l na nb">X_train = vectorizer.fit_transform(X_train)</span></pre><h1 id="6432" class="ll lm it bd ln lo ne lq lr ls nf lu lv lw ng ly lz ma nh mc md me ni mg mh mi bi translated">构建和测试分类器</h1><p id="b4be" class="pw-post-body-paragraph kf kg it kh b ki mj kk kl km mk ko kp kq ml ks kt ku mm kw kx ky mn la lb lc im bi translated">下一步是选择要使用的分类器类型。通常在这一步中，我们会选择几个候选分类器，并根据测试集对它们进行评估，看看哪一个效果最好。为了保持事物，我们可以假设支持向量机足够好地工作。</p><p id="8a47" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">SVM 的目标是找到</p><p id="a8c6" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="nm"> C </em>项用作影响目标函数的正则化。</p><p id="4be0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">较大的 C 值通常会导致超平面具有较小的裕度，因为它更强调精度而不是裕度宽度。诸如此类的参数可以通过网格搜索精确调整。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="d41f" class="mx lm it mt b gy my mz l na nb">from sklearn import svm</span><span id="89df" class="mx lm it mt b gy nc mz l na nb">svm = svm.SVC(C=1000)</span><span id="1113" class="mx lm it mt b gy nc mz l na nb">svm.fit(X_train, y_train)</span></pre><p id="7e13" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，让我们来测试一下。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="743e" class="mx lm it mt b gy my mz l na nb">from sklearn.metrics import confusion_matrix</span><span id="e464" class="mx lm it mt b gy nc mz l na nb">X_test = vectorizer.transform(X_test)</span><span id="48ab" class="mx lm it mt b gy nc mz l na nb">y_pred = svm.predict(X_test)</span><span id="20ac" class="mx lm it mt b gy nc mz l na nb">print(confusion_matrix(y_test, y_pred))</span></pre><figure class="mo mp mq mr gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/d80e4495eaa06db7aed606e676b45bfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MRsj2dZpdbqSoxs9LoAadg.png"/></div></div></figure><p id="aedf" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">成绩一点都不差！我们没有假阳性，大约 15%的假阴性。</p><p id="1a6d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们用几个新例子来测试一下。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="6024" class="mx lm it mt b gy my mz l na nb">def pred(msg):</span><span id="a49e" class="mx lm it mt b gy nc mz l na nb">    msg = vectorizer.transform([msg])</span><span id="0937" class="mx lm it mt b gy nc mz l na nb">    prediction = svm.predict(msg)</span><span id="b28d" class="mx lm it mt b gy nc mz l na nb">    return prediction[0]</span></pre><figure class="mo mp mq mr gt ju gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3d1407c8aefe4e9e83413e90fa3e57e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*_uvlqo1cXGP2_GrgJ4u6Hw.png"/></div></figure><p id="dc61" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在我看来是对的:)</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><p id="bd9f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我希望你喜欢本教程的第 2 部分。同样，完整的代码可以在这里找到<a class="ae ld" href="https://github.com/happilyeverafter95/Medium/blob/master/spam_or_ham.py" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="5ec1" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">检查第三部分！</p><h1 id="6274" class="ll lm it bd ln lo ne lq lr ls nf lu lv lw ng ly lz ma nh mc md me ni mg mh mi bi translated">感谢您的阅读！</h1><p id="3246" class="pw-post-body-paragraph kf kg it kh b ki mj kk kl km mk ko kp kq ml ks kt ku mm kw kx ky mn la lb lc im bi translated">如果你喜欢这篇文章，可以看看我关于数据科学、数学和编程的其他文章。<a class="ae ld" href="https://medium.com/@mandygu" rel="noopener">通过 Medium </a>关注我的最新动态。😃</p><p id="c4c4" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">作为一个业余爱好项目，我还在<a class="ae ld" href="http://www.dscrashcourse.com/" rel="noopener ugc nofollow" target="_blank">www.dscrashcourse.com</a>建立了一套全面的<strong class="kh iu">免费</strong>数据科学课程和练习题。</p><p id="fb9b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果你想支持我的写作，下次你报名参加 Coursera 课程时，可以考虑使用我的会员链接。完全公开—我从每一次注册中获得佣金，但不会对您产生额外费用。</p><p id="99bf" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">再次感谢您的阅读！📕</p></div></div>    
</body>
</html>