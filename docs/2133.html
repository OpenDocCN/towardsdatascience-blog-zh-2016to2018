<html>
<head>
<title>Machine learning fundamentals (II): Neural networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习基础(二):神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-fundamentals-ii-neural-networks-f1e7b2cb3eef?source=collection_archive---------3-----------------------#2017-12-21">https://towardsdatascience.com/machine-learning-fundamentals-ii-neural-networks-f1e7b2cb3eef?source=collection_archive---------3-----------------------#2017-12-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/0005b9cb0ab281d8991e8f51955ae320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*DW0Ccmj1hZ0OvSXi7Kz5MQ.jpeg"/></div></figure><div class=""/><p id="e255" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在我之前的文章中，我通过展示成本函数和梯度下降在学习过程中的核心作用，概述了机器学习的工作原理。这篇文章通过探索神经网络和深度学习如何工作来建立这些概念。这篇文章很少解释，很多代码。原因是，我想不出任何方式能比三个蓝色一个棕色的不可思议的视频放在一起更清楚地阐明神经网络的内部工作方式——见完整播放列表<a class="ae kv" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="83e1" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这些视频展示了如何向神经网络输入原始数据——如数字图像——并以惊人的精度输出这些图像的标签。这些视频以一种非常容易理解的方式强调了神经网络的基础数学，这意味着即使那些没有深厚数学背景的人也可以开始理解深度学习的本质。</p><figure class="kx ky kz la gt iv gh gi paragraph-image"><div class="gh gi kw"><img src="../Images/f842ebc3d591315215246788dcb2807c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*7HmSJOABTcRzWMVOB3fJlA.png"/></div></figure><p id="cf4c" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这篇文章旨在作为这些视频的“代码”补充(完整的 Tensorflow 和 Keras 脚本可在文章末尾获得)。目的是演示如何在 Tensorflow 中定义和执行神经网络，使其能够识别如上所示的数字。</p><p id="b1d4" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">TensorFlow(对于那些不知道的人来说)是谷歌的深度学习库，虽然它很低级(我通常在我的深度学习项目中使用更高级的 Keras 库)，但我认为它是一种很好的学习方式。这仅仅是因为，尽管它在幕后做了大量不可思议的事情，但它需要你(是的，你！)来明确定义 NN 的架构。这样你会更好地理解这些网络是如何工作的。</p><h1 id="3046" class="lb lc ja bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">神经网络</h1><p id="a6b2" class="pw-post-body-paragraph jx jy ja jz b ka lz kc kd ke ma kg kh ki mb kk kl km mc ko kp kq md ks kt ku im bi translated">神经网络是大脑中发生的生物过程的数学和计算抽象。具体来说，它们粗略地模拟了相互连接的中子对刺激的反应——例如新的输入信息。我没有发现生物学类比对理解神经网络特别有帮助，所以我不会继续沿着这条路走下去。</p><p id="1572" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">神经网络通过计算输入向量的加权总和来工作，然后输入向量通过非线性激活函数，从而通过非线性转换层创建从输入到输出的映射。变换层或<em class="me">隐藏层</em>中的权重(由中子表示)被反复调整，以表示输入到输出的数据关系。</p><h1 id="28a0" class="lb lc ja bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">定义层和激活</h1><p id="394b" class="pw-post-body-paragraph jx jy ja jz b ka lz kc kd ke ma kg kh ki mb kk kl km mc ko kp kq md ks kt ku im bi translated">第一步，我们定义网络的架构。我们将创建一个四层网络，包括一个输入层，两个隐藏层和一个输出层。请注意一层的输出如何成为下一层的输入。就神经网络而言，这个模型非常简单，它由密集的<em class="me">或</em>完全连接的层组成，但仍然非常强大。</p><figure class="kx ky kz la gt iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/0005b9cb0ab281d8991e8f51955ae320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*DW0Ccmj1hZ0OvSXi7Kz5MQ.jpeg"/></div></figure><p id="4720" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz jb">输入层</strong>——有时也被称为<strong class="jz jb">可见层</strong>——是以原始形式表示数据的模型层。例如，对于数字分类任务，可见图层由对应于像素值的数字表示。</p><figure class="kx ky kz la gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mf"><img src="../Images/2a0548a34458661c0e830ce6f5376eb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pHAL_ij9FJVhQBNp."/></div></div></figure><p id="234a" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在 TensorFlow <em class="me">(所有代码如下)</em>中我们需要创建一个占位符变量来表示这个输入数据，我们还将为每个输入对应的正确标签创建一个占位符变量。这有效地设置了训练数据——我们将用于训练神经网络的<em class="me"> X </em>值和<em class="me"> y </em>标签。</p><p id="c15b" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz jb">隐藏层</strong>使神经网络能够创建输入数据的新表示，模型使用它来学习数据和标签之间的复杂和抽象关系。每个隐藏层由神经元组成，每个神经元代表一个标量值。此标量值用于计算输入加上偏差的加权和(本质上是 y1 ~ wX + b)，从而创建线性(或更具体地说是仿射)变换。</p><p id="ae5e" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在 Tensorflow 中，您必须明确定义构成该层的权重和偏差的变量。我们通过将它们包装在<em class="me"> tf 中来做到这一点。变量</em>函数——这些被包装为<em class="me">变量</em>,因为参数会随着模型学习最能代表数据关系的权重和偏差而更新。我们用方差非常低的随机值来实例化权重，并用零填充偏差变量。然后，我们定义发生在该层的矩阵乘法。</p><figure class="kx ky kz la gt iv"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="49fa" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这个变换然后通过一个激活函数，(这里我使用<strong class="jz jb"> ReLU </strong>或整流线性单元)使线性变换的输出变成非线性。这使得神经网络能够模拟输入和输出之间复杂的非线性 T21 关系——点击这里查看 Siraj Raval 关于激活函数的精彩视频讲解。</p><p id="e698" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz jb">输出层</strong>是模型中的最后一层，在这种情况下，大小为 10，每个标签一个节点。我们将一个<strong class="jz jb"> softmax 激活</strong>应用到这一层，以便它跨最终层节点输出介于 0 和 1 之间的值——代表跨标签的概率。</p><h1 id="52ba" class="lb lc ja bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">成本函数和优化</h1><p id="d357" class="pw-post-body-paragraph jx jy ja jz b ka lz kc kd ke ma kg kh ki mb kk kl km mc ko kp kq md ks kt ku im bi translated">既然定义了神经网络架构，我们就设置<strong class="jz jb">成本函数</strong>和<strong class="jz jb">优化器</strong>。对于这个任务，我使用分类交叉熵。我还定义了一个准确性度量，可以用来评估模型的性能。最后，我将优化器设置为随机梯度下降，并在实例化后调用它的 minimise 方法。</p><figure class="kx ky kz la gt iv"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="d2b8" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">最后，可以运行模型——这里运行 1000 次迭代。在每次迭代中，一个小批量的数据被输入到模型中，它进行预测，计算损失，并通过反向传播，更新权重，重复这个过程。</p><figure class="kx ky kz la gt iv gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/2f5d858fd2ab22d65c11651cc5dc6942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*61ZaNNpbpMtZLLpZ."/></div></figure><p id="5874" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="me">* *摘自三蓝一棕的《但是，什么是神经网络视频** </em></p><p id="c8f3" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这个简单的模型在测试集上达到了大约 95.5%的准确率，这还不算太差，但是还可以更好。在下面的图中，您可以看到模型每次迭代的准确性和成本，有一点非常明显，即训练集和测试集的性能之间存在差异。</p><figure class="kx ky kz la gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mf"><img src="../Images/cd49de7588c3330b63d4fa7b3c0537d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9xS5aqm6nI8qF5ZK."/></div></div></figure><p id="f439" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这表明<strong class="jz jb">过度拟合</strong>——也就是说，模型学习训练数据太好，这限制了它的<strong class="jz jb">泛化能力</strong>。我们可以使用<strong class="jz jb">正则化方法</strong>来处理过度拟合，这将是我下一篇文章的主题。</p><p id="0930" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">感谢您的阅读🙂</p><p id="0f2f" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">页（page 的缩写）完整的 Tensorflow 脚本可以在<a class="ae kv" href="https://gist.github.com/conormm/1c82b093c9c6002e7ca6ff6e9fb34f05" rel="noopener ugc nofollow" target="_blank">这里</a>找到，相同的模型在 Keras <a class="ae kv" href="https://gist.github.com/conormm/e1dd2ee37733f4817e09a41d625d9e7f" rel="noopener ugc nofollow" target="_blank">这里</a>定义。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="0c1d" class="pw-post-body-paragraph jx jy ja jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="me">原载于 2017 年 12 月 21 日 dataflume.wordpress.com</em><em class="me">的</em> <a class="ae kv" href="https://dataflume.wordpress.com/2017/12/21/machine-learning-fundamentals-ii-neural-networks/" rel="noopener ugc nofollow" target="_blank"> <em class="me">。</em></a></p></div></div>    
</body>
</html>