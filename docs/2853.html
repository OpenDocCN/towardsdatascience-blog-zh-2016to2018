<html>
<head>
<title>Auto Tagging Stack Overflow Questions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动标记堆栈溢出问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/auto-tagging-stack-overflow-questions-5426af692904?source=collection_archive---------3-----------------------#2018-03-14">https://towardsdatascience.com/auto-tagging-stack-overflow-questions-5426af692904?source=collection_archive---------3-----------------------#2018-03-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/9d6ce3fa8cdbdde042cb3552ec1c4c63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6NWlFPxUh2IdCPoIzkkV0g.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo Credit: Pexels</figcaption></figure><p id="e673" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">自然语言处理最有趣的应用之一是自动推断和标记问题的主题。在这篇文章中，我们将从堆栈溢出问题和答案的探索性分析开始，然后我们将构建一个简单的模型来预测堆栈溢出问题的标签。我们将使用 Scikit-Learn 解决这个文本分类问题。让我们开始吧。</p><h1 id="ca3a" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">数据</h1><p id="ffe2" class="pw-post-body-paragraph kc kd iq ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">对于这个项目，我们将使用 10%关于编程主题的堆栈溢出问答中的<a class="ae md" href="https://www.kaggle.com/stackoverflow/stacksample/data" rel="noopener ugc nofollow" target="_blank">文本，它可以在</a><a class="ae md" href="https://www.kaggle.com/stackoverflow/stacksample/data" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上免费获得。</p><h1 id="0dcd" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">探索性数据分析</h1><p id="1e49" class="pw-post-body-paragraph kc kd iq ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">因为 ggplot 是我们最喜欢的数据可视化工具之一。因此，我们将在 r 中进行 EDA。</p><p id="e57b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">装载必要的包装</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="248b" class="mn lb iq mj b gy mo mp l mq mr">library(readr)<br/>library(dplyr)<br/>library(ggplot2)<br/>library(lubridate)<br/>library(tidytext)<br/>library(tidyverse)<br/>library(broom)<br/>library(purrr)<br/>library(scales)<br/>theme_set(theme_bw())</span></pre><p id="b651" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">问题数据和标签数据是分开存储的，所以我们将分别读取它们。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="79cb" class="mn lb iq mj b gy mo mp l mq mr">questions &lt;- read_csv("Questions.csv")<br/>question_tags &lt;- read_csv("Tags.csv")</span></pre><p id="0d8f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">标签数据</strong></p><p id="b1dc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">那么，最受欢迎的标签有哪些呢？</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="e620" class="mn lb iq mj b gy mo mp l mq mr">question_tags %&gt;%<br/>  count(Tag, sort = TRUE)</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/01810fb977b9f0eb65a9e8886c0266d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*k9clGcOFYjPjYsoWimeA2w.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1</figcaption></figure><p id="dd94" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">问题数据</strong></p><p id="732e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">每周问的问题数量:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5891" class="mn lb iq mj b gy mo mp l mq mr">questions &lt;- questions[ -c(8:29)]<br/>questions %&gt;%<br/>  count(Week = round_date(CreationDate, "week")) %&gt;%<br/>  ggplot(aes(Week, n)) +<br/>  geom_line() + <br/>  ggtitle('The Number of Questions Asked Per Week')</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/4e8a3cf15d15972c869f0645620fc02f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*l4v9jLNZWtRovTskkdOZpA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2</figcaption></figure><p id="cc57" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">比较特定标签随时间的增长或收缩:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="b64a" class="mn lb iq mj b gy mo mp l mq mr">tags &lt;- c("c#", "javascript", "python", "r", "php")</span><span id="3b18" class="mn lb iq mj b gy mu mp l mq mr">q_per_year &lt;- questions %&gt;%<br/>  count(Year = year(CreationDate)) %&gt;%<br/>  rename(YearTotal = n)</span><span id="b429" class="mn lb iq mj b gy mu mp l mq mr">tags_per_year &lt;- question_tags %&gt;%<br/>  filter(Tag %in% tags) %&gt;%<br/>  inner_join(questions) %&gt;%<br/>  count(Year = year(CreationDate), Tag) %&gt;%<br/>  inner_join(q_per_year)</span><span id="76bb" class="mn lb iq mj b gy mu mp l mq mr">ggplot(tags_per_year, aes(Year, n / YearTotal, color = Tag)) +<br/>  geom_line() +<br/>  scale_y_continuous(labels = scales::percent_format()) +<br/>  ylab("% of Stack Overflow questions with this tag") +<br/>  ggtitle('Growth or Shrinking of Particular Tags Overtime')</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/f49bdf351bd402cc52955784ea12afb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*OmwcUlWPTr85i2Fg3yqH_w.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3</figcaption></figure><p id="3255" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">标题中最常见的单词是什么？</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="37ec" class="mn lb iq mj b gy mo mp l mq mr">title_word_counts &lt;- title_words %&gt;%<br/>  anti_join(stop_words, c(Word = "word")) %&gt;%<br/>  count(Word, sort = TRUE)</span><span id="2dbe" class="mn lb iq mj b gy mu mp l mq mr">title_word_counts %&gt;%<br/>  head(20) %&gt;%<br/>  mutate(Word = reorder(Word, n)) %&gt;%<br/>  ggplot(aes(Word, n)) +<br/>  geom_col(fill = "cyan4", alpha = 0.8, width = 0.6) +<br/>  ylab("Number of appearances in question titles") +<br/>  ggtitle('The most common words in the question titles') +<br/>  coord_flip()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/6f6bb156dc0b43d7ca14b9867bb1eeca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*Q43m8yWDm4O6dIo7VnN9PA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4</figcaption></figure><p id="9bb3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">在标签类别中查找 TF-IDF</strong></p><p id="6486" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们预计标签类别在标题内容方面会有所不同，因此它们之间的词频也会有所不同。我们将使用 tf-idf 来查找与特定标签最相关的标题词。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="45cf" class="mn lb iq mj b gy mo mp l mq mr">common_tags &lt;- question_tags %&gt;%<br/>    group_by(Tag) %&gt;%<br/>    mutate(TagTotal = n()) %&gt;%<br/>    ungroup() %&gt;%<br/>    filter(TagTotal &gt;= 100)</span><span id="5ed0" class="mn lb iq mj b gy mu mp l mq mr">tag_word_tfidf &lt;- common_tags %&gt;%<br/>    inner_join(title_words, by = "Id") %&gt;%<br/>    count(Tag, Word, TagTotal, sort = TRUE) %&gt;%<br/>    ungroup() %&gt;%<br/>    bind_tf_idf(Word, Tag, n)</span><span id="df7d" class="mn lb iq mj b gy mu mp l mq mr">tag_word_tfidf %&gt;%<br/>    filter(TagTotal &gt; 1000) %&gt;%<br/>    arrange(desc(tf_idf)) %&gt;%<br/>    head(10)</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/e756a16601fd3bf9b648c83d4a5cef72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*DkFGU3jBNCht1cg7MKEdgQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 5</figcaption></figure><p id="8cea" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们将检查所有标签类别的顶级 tf-idf，以提取特定于这些标签的单词。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="53ec" class="mn lb iq mj b gy mo mp l mq mr">tag_word_tfidf %&gt;%<br/>  filter(Tag %in% c("c#", "python", "java", "php", "javascript", "android")) %&gt;%<br/>  group_by(Tag) %&gt;%<br/>  top_n(12, tf_idf) %&gt;%<br/>  ungroup() %&gt;%<br/>  mutate(Word = reorder(Word, tf_idf)) %&gt;%<br/>  ggplot(aes(Word, tf_idf, fill = Tag)) +<br/>  geom_col(show.legend = FALSE, width = 0.6) +<br/>  facet_wrap(~ Tag, scales = "free") +<br/>  ylab("tf-idf") +<br/>  coord_flip() +<br/>  ggtitle('The 12 terms with the highest tf-idf within each of the top tag categories')</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/e8b0da1fdcf2b3c18dce80cb03bccb81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*Q86HKmeA-PIEZx6fqlpDsg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 6</figcaption></figure><p id="c4ef" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">随时间变化</strong></p><p id="ceb0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">随着时间的推移，哪些单词和术语变得越来越频繁或越来越不频繁？这些可以给我们一种变化的软件生态系统的感觉，并让我们预测哪些词将继续增长的相关性。为了达到这个目的，我们需要得到每个单词的斜率。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="fe9f" class="mn lb iq mj b gy mo mp l mq mr">questions$month&lt;-month(questions$CreationDate)<br/>questions$year &lt;- year(questions$CreationDate)</span><span id="172b" class="mn lb iq mj b gy mu mp l mq mr">titles_per_month &lt;- questions %&gt;%<br/>  group_by(month) %&gt;%<br/>  summarize(month_total = n())</span><span id="f2aa" class="mn lb iq mj b gy mu mp l mq mr">title_words &lt;- questions %&gt;%<br/>  arrange(desc(Score)) %&gt;%<br/>  distinct(Title, .keep_all = TRUE) %&gt;%<br/>  unnest_tokens(word, Title, drop = FALSE) %&gt;%<br/>  distinct(Id, word, .keep_all = TRUE) %&gt;%<br/>  anti_join(stop_words, by = "word") %&gt;%<br/>  filter(str_detect(word, "[^\\d]")) %&gt;%<br/>  group_by(word) %&gt;%<br/>  mutate(word_total = n()) %&gt;%<br/>  ungroup()</span><span id="b598" class="mn lb iq mj b gy mu mp l mq mr">word_month_counts &lt;- title_words %&gt;%<br/>  filter(word_total &gt;= 1000) %&gt;%<br/>  count(word, month, year) %&gt;%<br/>  complete(word, month, year, fill = list(n = 0)) %&gt;%<br/>  inner_join(titles_per_month, by = "month") %&gt;%<br/>  mutate(percent = n / month_total)</span><span id="6308" class="mn lb iq mj b gy mu mp l mq mr">mod &lt;- ~ glm(cbind(n, month_total - n) ~ year, ., family = "binomial")</span><span id="21d3" class="mn lb iq mj b gy mu mp l mq mr">slopes &lt;- word_month_counts %&gt;%<br/>  nest(-word) %&gt;%<br/>  mutate(model = map(data, mod)) %&gt;%<br/>  unnest(map(model, tidy)) %&gt;%<br/>  filter(term == "year") %&gt;%<br/>  arrange(desc(estimate))</span><span id="885c" class="mn lb iq mj b gy mu mp l mq mr">slopes</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/eaff68380921083d0b8b73f2c02d8818.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*qw5skoGw3_yhN8X7sAFYZQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 7</figcaption></figure><p id="d96a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">然后标出增长最快的 16 个单词:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1f32" class="mn lb iq mj b gy mo mp l mq mr">slopes %&gt;%<br/>  head(16) %&gt;%<br/>  inner_join(word_month_counts, by = "word") %&gt;%<br/>  mutate(word = reorder(word, -estimate)) %&gt;%<br/>  ggplot(aes(year, n / month_total, color = word)) +<br/>  geom_point(show.legend = FALSE) +<br/>  geom_smooth(show.legend = FALSE) +<br/>  scale_y_continuous(labels = percent_format()) +<br/>  facet_wrap(~ word, scales = "free_y") +<br/>  expand_limits(y = 0) +<br/>  labs(x = "Year",<br/>       y = "Percentage of titles containing this term",<br/>       title = "16 fastest growing words in Stack Overflow question titles")</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/1a60bd450b14104cad224de21c2a772f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*xTyz5W_d8n8quDBKWAvwuQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 8</figcaption></figure><p id="15c8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">前 16 个收缩最快的单词:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="86d9" class="mn lb iq mj b gy mo mp l mq mr">slopes %&gt;%<br/>  tail(16) %&gt;%<br/>  inner_join(word_month_counts, by = "word") %&gt;%<br/>  mutate(word = reorder(word, -estimate)) %&gt;%<br/>  ggplot(aes(year, n / month_total, color = word)) +<br/>  geom_point(show.legend = FALSE) +<br/>  geom_smooth(show.legend = FALSE) +<br/>  scale_y_continuous(labels = percent_format()) +<br/>  facet_wrap(~ word, scales = "free_y") +<br/>  expand_limits(y = 0) +<br/>  labs(x = "Year",<br/>       y = "Percentage of titles containing this term",<br/>       title = "16 fastest shrinking words in Stack Overflow question titles")</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi na"><img src="../Images/28f449d218cb9371047a6f545daa77a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*WZ79A9lZ57M3qmKA6TT1CA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 9</figcaption></figure><p id="ed3f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> N 元语法分析</strong></p><p id="ae47" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">n 元模型不仅用于开发一元模型，还用于开发二元模型和三元模型。二元模型是一个 n 元模型，表示<em class="nb"> n </em> =2。以下是问题标题中最常见的二元结构。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="3751" class="mn lb iq mj b gy mo mp l mq mr">title_bigrams &lt;- questions %&gt;%<br/>  unnest_tokens(bigram, Title, token = "ngrams", n = 2)</span><span id="b5bb" class="mn lb iq mj b gy mu mp l mq mr">title_bigrams %&gt;%<br/>  count(bigram, sort = TRUE)</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/f612ec8a3993237b1bcc294497c6ee0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*5g2wdI04eJonrB4vWbv8mQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 10</figcaption></figure><p id="a71a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我相信你会觉得它们毫无意义。让我们找到最常见的有意义的二元模型。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="b280" class="mn lb iq mj b gy mo mp l mq mr">bigrams_separated &lt;- title_bigrams %&gt;%<br/>  separate(bigram, c("word1", "word2"), sep = " ")</span><span id="2fda" class="mn lb iq mj b gy mu mp l mq mr">bigrams_filtered &lt;- bigrams_separated %&gt;%<br/>  filter(!word1 %in% stop_words$word) %&gt;%<br/>  filter(!word2 %in% stop_words$word)</span><span id="7083" class="mn lb iq mj b gy mu mp l mq mr">bigram_counts &lt;- bigrams_filtered %&gt;% <br/>  count(word1, word2, sort = TRUE)</span><span id="6ba3" class="mn lb iq mj b gy mu mp l mq mr">bigrams_united &lt;- bigrams_filtered %&gt;%<br/>  unite(bigram, word1, word2, sep = " ")</span><span id="2202" class="mn lb iq mj b gy mu mp l mq mr">bigrams_united %&gt;%<br/>  count(bigram, sort = TRUE)</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/82209d9c0725ef695b1662c3a291ee7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*piQn73PNpAv3aFcHZl8-Ow.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 11</figcaption></figure><p id="fc2f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">和最常见的三元模型:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="0c1f" class="mn lb iq mj b gy mo mp l mq mr">questions %&gt;%<br/>  unnest_tokens(trigram, Title, token = "ngrams", n = 3) %&gt;%<br/>  separate(trigram, c("word1", "word2", "word3"), sep = " ") %&gt;%<br/>  filter(!word1 %in% stop_words$word,<br/>         !word2 %in% stop_words$word,<br/>         !word3 %in% stop_words$word) %&gt;%<br/>  count(word1, word2, word3, sort = TRUE)</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/f64ab44e73d362851c82078a09e8ccaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*fCu0J3M9G8fznV0Dno8BtA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 12</figcaption></figure><p id="b947" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">那很有趣！</p><p id="ed8c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在，我们将开发一个预测模型来自动标记堆栈溢出问题。我们将用 Python 来实现。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4253" class="mn lb iq mj b gy mo mp l mq mr">write.csv(total, file = "/Users/sli/Documents/total.csv", row.names = FALSE)</span></pre><p id="5814" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">以下是问题和标记组合表的前五行:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="dd74" class="mn lb iq mj b gy mo mp l mq mr">import pandas as pd<br/>total = pd.read_csv('total.csv', encoding='latin-1')</span><span id="4fcc" class="mn lb iq mj b gy mu mp l mq mr">total.head()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nf"><img src="../Images/cd50639e3ad8defd4754e5f1bc2e0915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Rik6BOgAYizWvODb3X-0g.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 13</figcaption></figure><p id="46a7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">以下是第一个问题的全文:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="50e7" class="mn lb iq mj b gy mo mp l mq mr">total['Body'][0]</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/550c05d5e1beca52118bdb6c5bfe2620.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QOgzjfcAMD9guF1Umx6Nxg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 14</figcaption></figure><h1 id="3a54" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">文本预处理</strong></h1><p id="d681" class="pw-post-body-paragraph kc kd iq ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">原始的文本数据是杂乱的，需要清理以便进行进一步的分析。我们从数据中排除 HTML 标签、链接和代码片段。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="6f53" class="mn lb iq mj b gy mo mp l mq mr">from collections import Counter<br/>import numpy as np <br/>import string<br/>import re</span><span id="7522" class="mn lb iq mj b gy mu mp l mq mr">def clean_text(text):<br/>    global EMPTY<br/>    EMPTY = ''<br/>    <br/>    if not isinstance(text, str): <br/>        return text<br/>    text = re.sub('&lt;pre&gt;&lt;code&gt;.*?&lt;/code&gt;&lt;/pre&gt;', EMPTY, text)</span><span id="188c" class="mn lb iq mj b gy mu mp l mq mr">def replace_link(match):<br/>        return EMPTY if re.match('[a-z]+://', match.group(1)) else match.group(1)<br/>    <br/>    text = re.sub('&lt;a[^&gt;]+&gt;(.*)&lt;/a&gt;', replace_link, text)<br/>    return re.sub('&lt;[^&gt;]+&gt;', EMPTY, text)</span></pre><p id="73e8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">然后，我们为“Body”列中已清理的文本创建一个新的“Text”列。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="16ee" class="mn lb iq mj b gy mo mp l mq mr">total['Text'] = total['Body'].apply(clean_text).str.lower()<br/>total.Text = total.Text.apply(lambda x: x.replace('"','').replace("\n","").replace("\t",""))</span></pre><p id="c834" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们的数据中有超过 20，000 个独特的标签。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4315" class="mn lb iq mj b gy mo mp l mq mr">total['Tag'].nunique()</span></pre><p id="b3ed" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi">21981</p><p id="d52a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了简化问题，我们将只处理前 10 个最常用的标签，如下所示:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="6d14" class="mn lb iq mj b gy mo mp l mq mr">def plot_tags(tagCount):<br/>    <br/>    x,y = zip(*tagCount)</span><span id="063d" class="mn lb iq mj b gy mu mp l mq mr">    colormap = plt.cm.gist_ncar #nipy_spectral, Set1,Paired  <br/>    colors = [colormap(i) for i in np.linspace(0, 0.8,50)]</span><span id="93dd" class="mn lb iq mj b gy mu mp l mq mr">    area = [i/4000 for i in list(y)]   # 0 to 15 point radiuses<br/>    plt.figure(figsize=(10,6))<br/>    plt.ylabel("Number of question associations")<br/>    for i in range(len(y)):<br/>      plt.plot(i,y[i],marker='o',linestyle='',ms=area[i],label=x[i])   </span><span id="885a" class="mn lb iq mj b gy mu mp l mq mr">    plt.legend(numpoints=1)<br/>    plt.show()</span><span id="499e" class="mn lb iq mj b gy mu mp l mq mr">import collections<br/>import matplotlib.pyplot as plt<br/>tagCount =  collections.Counter(list(total['Tag'])).most_common(10)<br/>print(tagCount)<br/>plot_tags(tagCount)</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/c75e33d7ff89e33ba7dac7871bf4013a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AscIYPbnxqgmN75VmqBdYg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 15</figcaption></figure><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4081" class="mn lb iq mj b gy mo mp l mq mr">total = total[(total.Tag == 'c#') | (total.Tag == 'java') | (total.Tag == 'php') | (total.Tag =='javascript') | (total.Tag =='jquery') | (total.Tag == 'android') | (total.Tag == 'c++') | (total.Tag == 'iphone') | (total.Tag == 'python') | (total.Tag == 'asp.net')]</span></pre><h1 id="7180" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">文本文档的分类</h1><p id="f788" class="pw-post-body-paragraph kc kd iq ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">我们将 scikit-learn 的单词包方法按标签对文本进行分类。所以，我们只对两列感兴趣——“文本”和“标签”。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="97e7" class="mn lb iq mj b gy mo mp l mq mr">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(total['Text'], total['Tag'], random_state=42, test_size=0.2, shuffle=True)</span></pre><p id="e014" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们将尝试各种分类器，这些分类器可以有效地处理已经转换为稀疏矩阵的文本数据。</p><p id="1400" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">条形图显示每个分类器的准确性、训练时间(标准化)和测试时间(标准化)。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="e841" class="mn lb iq mj b gy mo mp l mq mr">from __future__ import print_function</span><span id="55b1" class="mn lb iq mj b gy mu mp l mq mr">from time import time<br/>import matplotlib.pyplot as pltfrom sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.feature_extraction.text import HashingVectorizer<br/>from sklearn.feature_selection import SelectFromModel<br/>from sklearn.feature_selection import SelectKBest, chi2<br/>from sklearn.linear_model import RidgeClassifier<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.svm import LinearSVC<br/>from sklearn.linear_model import SGDClassifier<br/>from sklearn.linear_model import Perceptron<br/>from sklearn.linear_model import PassiveAggressiveClassifier<br/>from sklearn.naive_bayes import BernoulliNB, MultinomialNB<br/>from sklearn.neighbors import NearestCentroid<br/>from sklearn.utils.extmath import density<br/>from sklearn import metrics</span><span id="bb8a" class="mn lb iq mj b gy mu mp l mq mr">target_names=total['Tag'].unique()<br/>def benchmark(clf):<br/>    print('_' * 80)<br/>    print("Training: ")<br/>    print(clf)<br/>    t0 = time()<br/>    clf.fit(X_train_1, y_train)<br/>    train_time = time() - t0<br/>    print("train time: %0.3fs" % train_time)</span><span id="653f" class="mn lb iq mj b gy mu mp l mq mr">t0 = time()<br/>    pred = clf.predict(X_test_1)<br/>    test_time = time() - t0<br/>    print("test time:  %0.3fs" % test_time)</span><span id="3602" class="mn lb iq mj b gy mu mp l mq mr">score = metrics.accuracy_score(y_test, pred)<br/>    print("accuracy:   %0.3f" % score)</span><span id="05da" class="mn lb iq mj b gy mu mp l mq mr">if hasattr(clf, 'coef_'):<br/>        print("dimensionality: %d" % clf.coef_.shape[1])<br/>        print("density: %f" % density(clf.coef_))</span><span id="d202" class="mn lb iq mj b gy mu mp l mq mr">if opts.print_top10 and feature_names is not None:<br/>            print("top 10 keywords per class:")<br/>            for i, label in enumerate(target_names):<br/>                top10 = np.argsort(clf.coef_[i])[-10:]<br/>                print(trim("%s: %s" % (label, " ".join(feature_names[top10]))))<br/>        print()</span><span id="de31" class="mn lb iq mj b gy mu mp l mq mr">if opts.print_report:<br/>        print("classification report:")<br/>        print(metrics.classification_report(y_test, pred,<br/>                                            target_names=target_names))</span><span id="a142" class="mn lb iq mj b gy mu mp l mq mr">if opts.print_cm:<br/>        print("confusion matrix:")<br/>        print(metrics.confusion_matrix(y_test, pred))</span><span id="92ec" class="mn lb iq mj b gy mu mp l mq mr">print()<br/>    clf_descr = str(clf).split('(')[0]<br/>    return clf_descr, score, train_time, test_time</span><span id="5c9e" class="mn lb iq mj b gy mu mp l mq mr">results = []<br/>for clf, name in (<br/>        (RidgeClassifier(tol=1e-2, solver="lsqr"), "Ridge Classifier"),<br/>        (Perceptron(n_iter=50), "Perceptron"),<br/>        (PassiveAggressiveClassifier(n_iter=50), "Passive-Aggressive")):<br/>    print('=' * 80)<br/>    print(name)<br/>    results.append(benchmark(clf))<br/>    <br/>print('=' * 80)<br/>print("Elastic-Net penalty")<br/>results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,<br/>                                       penalty="elasticnet")))<br/>print('=' * 80)<br/>print("NearestCentroid (aka Rocchio classifier)")<br/>results.append(benchmark(NearestCentroid()))</span><span id="2de7" class="mn lb iq mj b gy mu mp l mq mr">print('=' * 80)<br/>print("Naive Bayes")<br/>results.append(benchmark(MultinomialNB(alpha=.01)))<br/>results.append(benchmark(BernoulliNB(alpha=.01)))</span><span id="9107" class="mn lb iq mj b gy mu mp l mq mr">print('=' * 80)<br/>print("LinearSVC with L1-based feature selection")<br/>results.append(benchmark(Pipeline([<br/>  ('feature_selection', SelectFromModel(LinearSVC(penalty="l1", dual=False,<br/>                                                  tol=1e-3))),<br/>  ('classification', LinearSVC(penalty="l2"))])))</span><span id="cf8a" class="mn lb iq mj b gy mu mp l mq mr">indices = np.arange(len(results))</span><span id="7daf" class="mn lb iq mj b gy mu mp l mq mr">results = [[x[i] for x in results] for i in range(4)]</span><span id="a864" class="mn lb iq mj b gy mu mp l mq mr">clf_names, score, training_time, test_time = results<br/>training_time = np.array(training_time) / np.max(training_time)<br/>test_time = np.array(test_time) / np.max(test_time)</span><span id="a01e" class="mn lb iq mj b gy mu mp l mq mr">plt.figure(figsize=(12, 8))<br/>plt.title("Score")<br/>plt.barh(indices, score, .2, label="score", color='navy')<br/>plt.barh(indices + .3, training_time, .2, label="training time",<br/>         color='c')<br/>plt.barh(indices + .6, test_time, .2, label="test time", color='darkorange')<br/>plt.yticks(())<br/>plt.legend(loc='best')<br/>plt.subplots_adjust(left=.25)<br/>plt.subplots_adjust(top=.95)<br/>plt.subplots_adjust(bottom=.05)</span><span id="1048" class="mn lb iq mj b gy mu mp l mq mr">for i, c in zip(indices, clf_names):<br/>    plt.text(-.3, i, c)</span><span id="ac34" class="mn lb iq mj b gy mu mp l mq mr">plt.show()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/375217a6948238d8ddfcefdf6ff97026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dStNbyG6M2jc135zHKlCBA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 16</figcaption></figure><p id="2b0e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">使用岭回归的分类器取得了迄今为止最好的结果。因此，我们打印出每个标签的精度和召回率。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="c724" class="mn lb iq mj b gy mo mp l mq mr">model = RidgeClassifier(tol=1e-2, solver="lsqr")<br/>model.fit(X_train_1, y_train)<br/>predicted = model.predict(X_test_1)<br/>from sklearn.metrics import classification_report</span><span id="37bb" class="mn lb iq mj b gy mu mp l mq mr">print(classification_report(y_test, predicted, target_names=target_names))</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/8a250de8dc15c31f9feab9cd3147c0ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*v7VkN-D8bd8-hIJAhlJoSw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 17</figcaption></figure><p id="8169" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们也许可以通过参数调整来获得更好的结果，但是我把它留给你去做。</p><p id="0526" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">源代码可以在<a class="ae md" href="https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/stack_over_flow_auto_tagging.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>找到。我期待听到任何反馈或问题。</p><p id="ad53" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">参考资料:</p><p id="d11b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae md" href="http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a></p><p id="a6cf" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae md" href="https://www.tidytextmining.com/" rel="noopener ugc nofollow" target="_blank">文字挖掘用 R </a></p></div></div>    
</body>
</html>