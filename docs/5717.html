<html>
<head>
<title>Review: U-Net (Biomedical Image Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:U-Net(生物医学图像分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-u-net-biomedical-image-segmentation-d02bf06ca760?source=collection_archive---------4-----------------------#2018-11-05">https://towardsdatascience.com/review-u-net-biomedical-image-segmentation-d02bf06ca760?source=collection_archive---------4-----------------------#2018-11-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="80ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>这个故事中，<strong class="jp ir"> U-Net </strong>进行了回顾。U-Net 是生物医学图像分割中著名的全卷积网络(FCN)之一，在我写这个故事的时候已经发表在<strong class="jp ir"> 2015 MICCAI </strong>上，引用超过<strong class="jp ir"> 3000 次</strong>。(<a class="ku kv ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----d02bf06ca760--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p><p id="9ea9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在生物医学图像注释领域，我们总是需要获得相关知识的专家来注释每幅图像。而且他们也<strong class="jp ir">消耗大量的时间来注释</strong>。<strong class="jp ir">如果注释过程变得自动化，可以减少人力和降低成本</strong>。或者可以作为辅助角色<strong class="jp ir">减少人为错误</strong>。</p><blockquote class="kw kx ky"><p id="2a3f" class="jn jo kz jp b jq jr js jt ju jv jw jx la jz ka kb lb kd ke kf lc kh ki kj kk ij bi translated">你可能会问:“读生物医学图像分割是不是太狭隘了？”</p><p id="d89a" class="jn jo kz jp b jq jr js jt ju jv jw jx la jz ka kb lb kd ke kf lc kh ki kj kk ij bi translated">然而，我们可以学习它的技术，并将其应用于不同的行业。比方说，<strong class="jp ir">质量控制/自动检测/施工/制造过程中的自动机器人</strong>，或者我们可能想到的任何其他东西。这些活动包括定量诊断。如果我们能使它自动化，就能以更高的精度节省成本。</p></blockquote><p id="70eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇论文中，他们<strong class="jp ir">分割/注释了电子显微镜(EM)图像</strong>。他们还对网络进行了一点点修改，以<strong class="jp ir">分割/注释 2015 年 ISBI </strong>的牙齿 x 光图像。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ld"><img src="../Images/75c31d2b34d4f7fe3bbdfc64f8f021b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*a0ab8c72FHL7rvQk.gif"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">EM Images</strong></figcaption></figure></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h1 id="932e" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">涵盖哪些内容</h1><h2 id="820c" class="mv ly iq bd lz mw mx dn md my mz dp mh jy na nb ml kc nc nd mp kg ne nf mt ng bi translated">A.EM 图像分割</h2><ol class=""><li id="5a6d" class="nh ni iq jp b jq nj ju nk jy nl kc nm kg nn kk no np nq nr bi translated">U-Net 网络体系结构</li><li id="9290" class="nh ni iq jp b jq ns ju nt jy nu kc nv kg nw kk no np nq nr bi translated">重叠平铺策略</li><li id="41ea" class="nh ni iq jp b jq ns ju nt jy nu kc nv kg nw kk no np nq nr bi translated">用于数据扩充的弹性变形</li><li id="add5" class="nh ni iq jp b jq ns ju nt jy nu kc nv kg nw kk no np nq nr bi translated">触摸物体的分离</li><li id="edac" class="nh ni iq jp b jq ns ju nt jy nu kc nv kg nw kk no np nq nr bi translated">结果</li></ol><h2 id="433e" class="mv ly iq bd lz mw mx dn md my mz dp mh jy na nb ml kc nc nd mp kg ne nf mt ng bi translated">B.牙科 X 射线图像分割</h2><ol class=""><li id="5bcb" class="nh ni iq jp b jq nj ju nk jy nl kc nm kg nn kk no np nq nr bi translated">对 U-Net 的一些修改</li><li id="ef3f" class="nh ni iq jp b jq ns ju nt jy nu kc nv kg nw kk no np nq nr bi translated">结果</li></ol></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h1 id="4d58" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">A.1. U-Net 网络架构</h1><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi nx"><img src="../Images/84eabaca8e4ffe2282b0852f5d1404b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O2NbipwBOdTMtj7ThBNTPQ.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">U-Net</strong></figcaption></figure><p id="f290" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">U-net 架构如上图。它由收缩路径和膨胀路径组成。</p><p id="0d0e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">收缩路径</strong></p><ul class=""><li id="04ee" class="nh ni iq jp b jq jr ju jv jy oc kc od kg oe kk of np nq nr bi translated">连续完成<strong class="jp ir">两次 3×3 Conv </strong>和<strong class="jp ir"> 2×2 最大汇集</strong>。这有助于提取更高级的要素，但也会减小要素地图的大小。</li></ul><p id="fd25" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">扩展路径</strong></p><ul class=""><li id="a209" class="nh ni iq jp b jq jr ju jv jy oc kc od kg oe kk of np nq nr bi translated">连续进行<strong class="jp ir"> 2×2 上 conv </strong>和<strong class="jp ir">两次 3×3 Conv </strong>来恢复分割图的大小。然而，上述过程<strong class="jp ir">减少了“哪里”</strong>，尽管它<strong class="jp ir">增加了“什么”</strong>。这意味着，我们可以获得高级功能，但我们也失去了本地化信息。</li><li id="09f2" class="nh ni iq jp b jq ns ju nt jy nu kc nv kg nw kk of np nq nr bi translated">因此，在每个上 conv 之后，我们还有具有相同级别的特征图(灰色箭头)的<strong class="jp ir">串联。这有助于<strong class="jp ir">给出从收缩路径到膨胀路径</strong>的定位信息。</strong></li><li id="89c3" class="nh ni iq jp b jq ns ju nt jy nu kc nv kg nw kk of np nq nr bi translated">最后，<strong class="jp ir"> 1×1 conv </strong>将特征图大小从 64 映射到 2，因为输出的特征图只有 2 类，细胞和膜。</li></ul></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h1 id="c0dc" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">A.2 .重叠平铺策略</h1><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi og"><img src="../Images/6743b042ae8511808a61edd9832a3e41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*GNB3UkI-hErQwvL-jDLU7A.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">Overlap Tile Strategy</strong></figcaption></figure><p id="6eca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于使用了无填充卷积，输出大小小于输入大小。使用重叠瓦片策略，而不是在网络之前缩小尺寸和在网络之后上采样。由此，<strong class="jp ir">整个图像被逐部分预测</strong>，如上图所示。使用蓝色区域预测图像中的黄色区域。<strong class="jp ir">在图像边界，通过镜像</strong>外推图像。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h1 id="bcf7" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">A.3 .用于数据扩充的弹性变形</h1><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/333eb77d993f8a480ee9bc59b02bd126.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*tKP4KxFTzQZiIBdANn1PVA.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">Elastic Deformation</strong></figcaption></figure><p id="eae8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于训练集只能由专家进行注释，因此训练集很小。为了增加训练集的大小，通过随机变形输入图像和输出分割图来进行数据扩充。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h1 id="e9fa" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">A.4 .触摸物体的分离</h1><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/52569d800e945bddaf8dfdb9db7956ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*W-QhMrDyrOMDWqrIZdY99Q.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">Segmentation Map (Left) and Weight Map (Right)</strong></figcaption></figure><p id="66b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于接触物体彼此靠近放置，它们很容易被网络合并，为了分离它们，对网络的输出应用权重图。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/69ae212649b2ceba956a9d8acf81f369.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*m2j8pBBnwjy3Xo3F0fJjzQ.png"/></div></figure><p id="432b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了如上计算权重图，d1(x)是到位置 x 处最近的单元边界的距离，d2(x)是到第二最近的单元边界的距离。因此，在边界处，重量比图中高得多。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/e3f78b2069d62f4517fdb5033d1f6574.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*cqS0Uj2bewTwCXLeE2fzsg.png"/></div></figure><p id="3087" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，交叉熵函数在每个位置被权重图惩罚。这有助于<strong class="jp ir">迫使网络学习接触细胞之间的小分离边界。</strong></p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h1 id="7761" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">A.5 .结果</h1><h2 id="b2c8" class="mv ly iq bd lz mw mx dn md my mz dp mh jy na nb ml kc nc nd mp kg ne nf mt ng bi translated">A.5.1. ISBI 2012 年奥运会</h2><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi ol"><img src="../Images/2b35a8c2cb2dfe18b9e2f9509ab32462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TAonZZgBR0QZ-8HIWXQF1A.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">Some Difficult Parts in EM Images</strong></figcaption></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi om"><img src="../Images/52b9630771e1bc2d644fad5e1d7e4bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*OuUWr6iA8c6NXZAMcqjbGA.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">U-Net has the Rank 1 Result at that moment</strong></figcaption></figure><ul class=""><li id="9b7b" class="nh ni iq jp b jq jr ju jv jy oc kc od kg oe kk of np nq nr bi translated"><strong class="jp ir">扭曲错误</strong>:惩罚拓扑不一致的分段度量。</li><li id="8174" class="nh ni iq jp b jq ns ju nt jy nu kc nv kg nw kk of np nq nr bi translated"><strong class="jp ir"> Rand Error </strong>:两个聚类或分割之间相似性的度量。</li><li id="b029" class="nh ni iq jp b jq ns ju nt jy nu kc nv kg nw kk of np nq nr bi translated"><strong class="jp ir">像素误差</strong>:标准像素误差。</li><li id="4db1" class="nh ni iq jp b jq ns ju nt jy nu kc nv kg nw kk of np nq nr bi translated">培训时间:10 小时</li><li id="c2c5" class="nh ni iq jp b jq ns ju nt jy nu kc nv kg nw kk of np nq nr bi translated">测试速度:每张图片大约 1 秒</li></ul><h2 id="89a2" class="mv ly iq bd lz mw mx dn md my mz dp mh jy na nb ml kc nc nd mp kg ne nf mt ng bi translated"><strong class="ak"> A.5.2. PhC-U373 和 DIC-HeLa 数据集</strong></h2><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi on"><img src="../Images/259620fca4ad74eebfed7dd15a80c671.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X8PJNsGql6yTvdu3t8KvSA.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">PhC-U373 and DIC-HeLa Datasets</strong></figcaption></figure><p id="6566" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">U-Net 获得了这两个数据集的最高 IoU。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h1 id="7dbc" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">b . 1 . U-Net 的一些修改</h1><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi oo"><img src="../Images/44e10ac1ce22d5289b8d6dbd6b34291f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gA69eVDJW4GQbZvFdOa_dw.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">Dental X-Ray Image with 7 classes</strong></figcaption></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi op"><img src="../Images/59560ef360f1e22bcaf30054d489750a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y3RAoPbY_KPbgD3YeMLHcg.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">U-Net for Dental X-Ray Images</strong></figcaption></figure><p id="24d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这次使用<strong class="jp ir"> 4×4 上 conv </strong>和<strong class="jp ir"> 1×1 Conv </strong>来绘制 64 到 7 的特征图，因为每个位置的输出有 7 个等级。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi gj"><img src="../Images/e19b74f985f4c3927ae5cf5590fba00d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G_vzey1WPZVFOuM5ax1YvQ.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">Zero padding instead of mirroring at the image boundary</strong></figcaption></figure><p id="6bae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在重叠平铺策略中，使用<strong class="jp ir">零填充</strong>代替图像边界处的镜像。因为镜像对牙齿没有任何意义。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi oq"><img src="../Images/24bea0dad47f3d9de2f8be4f29221523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*poKkNRQ4Q7v95DGtJR4oSA.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">Loss function at multiple levels</strong></figcaption></figure><p id="1649" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用 softmax loss 的低分辨率特征图有<strong class="jp ir">附加损失层</strong>，以引导深层直接学习分割类。</p><h1 id="d538" class="lx ly iq bd lz ma or mc md me os mg mh mi ot mk ml mm ou mo mp mq ov ms mt mu bi translated">B.2 .结果</h1><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/494823f33af74098b2c1ce7ec28e6ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*SQpV58nhfWBGGHAPJX2Wiw.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">Some Visualization Results</strong></figcaption></figure></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><p id="7a55" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我也复习过<a class="ae ox" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener"> <strong class="jp ir">累计视野 1 </strong> </a>和<a class="ae ox" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener"> <strong class="jp ir">累计视野 2 </strong> </a>。如果感兴趣，请随时访问。</p><h1 id="76be" class="lx ly iq bd lz ma or mc md me os mg mh mi ot mk ml mm ou mo mp mq ov ms mt mu bi translated">参考</h1><ul class=""><li id="d351" class="nh ni iq jp b jq nj ju nk jy nl kc nm kg nn kk of np nq nr bi translated">【2015】【MICCAI】<br/><a class="ae ox" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank">U-Net:用于生物医学图像分割的卷积网络</a></li><li id="3327" class="nh ni iq jp b jq ns ju nt jy nu kc nv kg nw kk of np nq nr bi translated">【2015】【ISBI】<br/><a class="ae ox" href="http://www-o.ntust.edu.tw/~cweiwang/ISBI2015/challenge2/isbi2015_Ronneberger.pdf" rel="noopener ugc nofollow" target="_blank">使用 U 形深度卷积网络的牙齿 x 光图像分割</a></li></ul><h1 id="2785" class="lx ly iq bd lz ma or mc md me os mg mh mi ot mk ml mm ou mo mp mq ov ms mt mu bi translated">我的相关评论</h1><p id="1773" class="pw-post-body-paragraph jn jo iq jp b jq nj js jt ju nk jw jx jy oy ka kb kc oz ke kf kg pa ki kj kk ij bi translated">[ <a class="ae ox" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">累积视频 1 </a> ] [ <a class="ae ox" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">累积视频 2 </a> ] [ <a class="ae ox" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a> ] [ <a class="ae ox" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">解除网络</a></p></div></div>    
</body>
</html>