<html>
<head>
<title>[ Paper Summary / NIPS 2017 ] The (Un)reliability of saliency methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[论文摘要/ NIPS 2017 ]显著性方法的(不)可靠性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-summary-nips-2017-the-un-reliability-of-saliency-methods-8ed7774a69aa?source=collection_archive---------8-----------------------#2018-08-01">https://towardsdatascience.com/paper-summary-nips-2017-the-un-reliability-of-saliency-methods-8ed7774a69aa?source=collection_archive---------8-----------------------#2018-08-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/3e9912b1339fc34572d813e17ad838c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*2oG6dSiCXd0y8-LKXbIniA.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/art-design-blue-3o7TKoYwWhxyeh8kvu" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="85a5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">另一篇伟大的论文解释了如何解释神经网络</p><blockquote class="kx ky kz"><p id="e134" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated"><strong class="kb ir">请注意，这篇帖子是给未来的自己看的，回顾这篇论文上的材料，而不是从头再看一遍。</strong></p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="https://arxiv.org/pdf/1711.00867.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="36d7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">摘要</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/23fd82c1153e44cf90c7c7443d93907b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W5YDuRoyI-C3QlIh6I18VQ.png"/></div></div></figure><p id="d9e5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">许多研究人员正在寻找方法来解释神经网络内部到底发生了什么，显著性方法就是其中之一。然而，事实证明，当解释对无助于模型预测的因素敏感时，这些方法就不那么可靠了。(如简单的常数加法到输入数据中。)并且作者已经发现，不满足输入方差属性的显著性方法会导致误导属性。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="eff6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">简介</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/0bc65ad7c1543b6d58555eb0c96e29fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VRK1rPNqkty6N_-CqB43mA.png"/></div></div></figure><p id="816f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">多重显著性方法旨在做同样的事情，解释神经网络内部发生了什么。(或推断对所学函数 f(x)的见解)。但是在实践中，它们是以各种不同的方式实现的，并且彼此不重叠。因为没有基本的真实数据，所以很难评估不同的方法，但是许多研究人员开发了一些属性来设置指导线，这些属性包括实现方差和输入方差。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="9b3f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">该模型对于输入的恒定偏移是不变的</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/f6c8802f629a5ce4958ed637a9f84e1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*2ht_q2vpPskxoiWcLfaAbw.png"/></div></figure><p id="d708" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">首先让我们有一个函数 f1()，它执行分类任务，输入值为 x1。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/067ee2402a9180a94538a62c799475e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*w_Jm5ytBYjcIPTCEnMvroQ.png"/></div></figure><p id="00c6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">接下来让我们有输入值 x2，其中我们添加一些常数到 x1。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/b1ce39cfe2897abcb9b02aebfb7e0de0.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*6ipeBth6M8QHq9Sa6E4LbA.png"/></div></figure><p id="5357" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们可以像上面这样定义 f1()函数。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/44a27d7eeeeb39e1e053c0d53855a96b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*NHmbmZOaDQZ5AF85DC8XeQ.png"/></div></figure><p id="a2dd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">然后，可以容易地示出函数 f2()接受 x2 的输入值，如果偏移值被相应地设置，则两个函数计算相同的东西，导致每个输入的导数值也相同。(w)。换句话说，f2()的偏置值抵消了变换。作者将使用 CNN 对输入价公理做一些实验。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="7a24" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">显著性方法对均值漂移的(不)敏感性</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mb"><img src="../Images/28449b3ef1629cb58a8b7f6e60308795.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H4JxcyaJpVQoBNoxHJ4jtg.png"/></div></div></figure><p id="910c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">作者考虑了三种显著性方法，并考虑了每种方法的输入方差。(满足方差内移位特性的方法将导致相同的热图。)</p><ol class=""><li id="7c19" class="mc md iq kb b kc kd kg kh kk me ko mf ks mg kw mh mi mj mk bi translated"><strong class="kb ir">梯度</strong> →输入的微小变化如何影响输出</li><li id="8067" class="mc md iq kb b kc ml kg mm kk mn ko mo ks mp kw mh mi mj mk bi translated"><strong class="kb ir">信号</strong> →哪些输入模式刺激神经元激活</li><li id="afd5" class="mc md iq kb b kc ml kg mm kk mn ko mo ks mp kw mh mi mj mk bi translated"><strong class="kb ir">归因</strong> →将许多不同的归因观点及其梯度相加</li></ol><p id="8913" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="la">梯度和信号方法满足输入方差</em></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mq"><img src="../Images/e8fd4b6fe81c6a664c2578dc321fd66c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*Axzpvz0USDED76uTdgC9OA.png"/></div></div></figure><p id="d6f2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，当我们考虑基于梯度的方法以及模式网时，我们可以观察到生成的显著性热图彼此相同。(因为我们在技术上考虑的是同一个功能)。</p><p id="0116" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="la">归因方法的敏感性</em></p><p id="18ab" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于归因方法，作者考虑了三种方法梯度输入(IG)，综合梯度(IG)和深度泰勒分解(DTD)。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mr"><img src="../Images/a3a050b9bd79948b86bf26aa7466b285.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3BiZ2xvrwozxNxItIJwnRA.png"/></div></div></figure><p id="6dec" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从上图中，我们可以看到 GI 对均值漂移很敏感，因此不满足输入方差特性，而其他两个方法参考点非常重要。(结果是 GI 失败的原因是由于输入值 x 的乘法，均值漂移继续存在。)</p><p id="04fc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="la">参考点方法的可靠性取决于参考点的选择</em></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ms"><img src="../Images/31f68e8ceba205fe566fcd8403970b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hq6fhUyL1hX28nLeBxaPAQ.png"/></div></div></figure><p id="3b0f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，根据 IG 和 DTD 的参考点，它会产生不同的显著性映射。对于 IG，当我们将参考点设置为由来自输入数据 x 的最小值组成的图像时，我们得到相同的显著热图，因为输入数据和参考点之间的关系很重要。但是，当我们设置一个恒定的基线参考点(如值为零的图像)时，生成的热图会有所不同。与 DTD 类似，当选择零矢量图像时，产生的热图不同，然而，当通过模式属性选择参考点时，输入方差成立。并且模式属性具有约束，因为它考虑了数据内变化的自然方向。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="e6d7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">选择合适参考点的重要性</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mt"><img src="../Images/8bb3e23485d878b8241c45ec52e3a087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2N0Ru3aHvvnPwxFQhHlgEA.png"/></div></div></figure><p id="ba61" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">到目前为止，我们已经看到，选择正确的参考点是非常重要的，因为与处理音频或文本数据相比，视觉数据选择合适的参考点更容易。如果我们不能确定参考点选择的含义，我们就不能对这种方法的可靠性做出任何评价。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mu"><img src="../Images/5fb70d7b344ed89b4561071cfb451848.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9x0pt5JJATVkc3s5gaYfCg.png"/></div></div></figure><p id="cbbf" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，这再次表明，选择正确的参考点可以帮助更好地理解模型正在做什么。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="8f36" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结论</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mv"><img src="../Images/9e6d49551fff98607154c623be6e6ccb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aLVxgLQ86KRVeFunQbkXgQ.png"/></div></div></figure><p id="a237" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">总之，本文作者提出了另一个称为输入方差的公理，其中显著性方法应反映模型对输入变换的敏感性。并且已经表明一些方法不能满足这一特性。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="2203" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="8e70" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">非常酷的研究…</p><p id="f3ea" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有写作的列表，请在这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的 twitter 上关注我<a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文</a> t。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="4dfb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="5ec2" class="mc md iq kb b kc kd kg kh kk me ko mf ks mg kw mh mi mj mk bi translated">(2018).Arxiv.org。检索于 2018 年 8 月 1 日，来自<a class="ae jy" href="https://arxiv.org/pdf/1711.00867.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1711.00867.pdf</a></li><li id="dd7b" class="mc md iq kb b kc ml kg mm kk mn ko mo ks mp kw mh mi mj mk bi translated">Kindermans，p .，Hooker，s .，Adebayo，j .，Alber，m .，Sch：tt，k . &amp; Dé；hne，s .等人(2017 年)。显著方法的(不)可靠性。Arxiv.org。检索于 2018 年 8 月 1 日，来自<a class="ae jy" href="https://arxiv.org/abs/1711.00867" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1711.00867</a></li></ol></div></div>    
</body>
</html>