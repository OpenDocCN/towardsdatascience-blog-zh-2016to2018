<html>
<head>
<title>Document feature extraction and classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文档特征提取和分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/document-feature-extraction-and-classification-53f0e813d2d3?source=collection_archive---------0-----------------------#2017-03-19">https://towardsdatascience.com/document-feature-extraction-and-classification-53f0e813d2d3?source=collection_archive---------0-----------------------#2017-03-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="fdf8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di"> E </span>非常分类问题在<a class="ae ku" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">自然语言处理</a> (NLP)中被大致归类为<strong class="jp ir">文档</strong>或<strong class="jp ir">令牌</strong>级分类任务。这是关于如何用python实现这一切并理解其背后的理论背景和用例的两部分博客的第一部分。这篇博客的所有<a class="ae ku" href="https://github.com/ishaan007/vector_space_modelling" rel="noopener ugc nofollow" target="_blank">代码、数据和结果</a>都可以在我的<strong class="jp ir"> GITHUB </strong>个人资料中找到。这篇文章专门讨论文档分类，随后的部分将讨论标记级分类，也称为解析。</p><p id="39a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">文档只不过是一组(不止一个)令牌。每一个有监督的机器学习算法都要求每一个文本文档都以向量的形式表示，以开始对这些文档进行训练，这是通过<a class="ae ku" href="https://en.wikipedia.org/wiki/Vector_space_model" rel="noopener ugc nofollow" target="_blank">向量空间建模</a> (VSM)来完成的。</p><h2 id="d172" class="kv kw iq bd kx ky kz dn la lb lc dp ld jy le lf lg kc lh li lj kg lk ll lm ln bi translated">VSM很大程度上可以通过两种独特且截然不同的技术来实现</h2><ol class=""><li id="1f85" class="lo lp iq jp b jq lq ju lr jy ls kc lt kg lu kk lv lw lx ly bi translated">使用TF-IDF的传统词汇方法(在本博客中)</li><li id="b87b" class="lo lp iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">像doc2vec这样的单词嵌入</li></ol><blockquote class="me mf mg"><p id="173e" class="jn jo mh jp b jq jr js jt ju jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj kk ij bi translated">在这个博客中，我使用了路透社新闻分类数据集，每个新闻文件都属于8个类别中的一个。我使用tf-idf和doc2vec进行特征提取，然后在75:25的训练:测试分割上使用逻辑回归和朴素贝叶斯分类器对这些向量进行分类</p></blockquote><p id="5474" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，工作流程是首先将所有文档转换为矢量。保留这些向量中的大部分用于训练，其余的用于测试，然后应用各自的监督分类技术。</p><h1 id="09d8" class="ml kw iq bd kx mm mn mo la mp mq mr ld ms mt mu lg mv mw mx lj my mz na lm nb bi translated">使用BOW :: TF-IDF的特征提取</h1><p id="daa5" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy nc ka kb kc nd ke kf kg ne ki kj kk ij bi translated">术语频率-逆文档频率使用数据集中的所有标记作为词汇。词汇中的标记在每个文档中的出现频率由术语频率组成，标记出现的文档数量决定了逆文档频率。这确保的是，如果一个标记在文档中频繁出现，该标记将具有高TF，但是如果该标记在大多数文档中频繁出现，那么它将减少IDF，因此像<em class="mh"> an、the、i </em>这样频繁出现的停用词将受到惩罚，而包含文档本质的重要词将得到提升。特定文档的这些TF和IDF矩阵相乘并归一化以形成文档的TF-IDF。</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/8edc83558176f6c39eed165fcaec8518.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/1*g9r9kJy71m-Ry_Y6XUYLLA.gif"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk">TF-IDF formulation</figcaption></figure><blockquote class="me mf mg"><p id="a43c" class="jn jo mh jp b jq jr js jt ju jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj kk ij bi translated">你必须浏览一下<a class="ae ku" href="https://twitter.com/tarantulae" rel="noopener ugc nofollow" target="_blank">克里斯蒂安·佩罗尼</a>的这个<a class="ae ku" href="http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/" rel="noopener ugc nofollow" target="_blank">博客</a>，在那里他用实现细节很好地解释了这个概念。</p></blockquote><h1 id="ffc4" class="ml kw iq bd kx mm mn mo la mp mq mr ld ms mt mu lg mv mw mx lj my mz na lm nb bi translated">使用单词嵌入的特征提取::doc2vec</h1><p id="86eb" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy nc ka kb kc nd ke kf kg ne ki kj kk ij bi translated">Doc2vec是与tf-idf完全不同的算法，TF-IDF使用3层浅深度神经网络来测量文档的上下文，并将相似的上下文短语关联在一起。</p><p id="a536" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于doc2vec需要注意的重要一点是，它不是像单词包那样的单片算法，它有两种不同的变体SKIP GRAM和CBOW，它还可以与其他变体一起使用，如带或不带负采样和带或不带分层softmax。</p><p id="05bc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，doc2vec应该在一个足够大和高质量的数据集上进行训练，以使模型生成合理的嵌入，这将导致良好的特征生成。</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nr"><img src="../Images/3750e167e668d6acdc02e70527df500a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*saRPuMtEYdvM3z5Rv1aRYQ.jpeg"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Doc2Vec <a class="ae ku" href="https://www.distilled.net/resources/a-beginners-guide-to-word2vec-aka-whats-the-opposite-of-canada/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="aebe" class="ml kw iq bd kx mm mn mo la mp mq mr ld ms mt mu lg mv mw mx lj my mz na lm nb bi translated">结果</h1><p id="c9c7" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy nc ka kb kc nd ke kf kg ne ki kj kk ij bi translated">整个练习的结果都符合预期。TF-IDF逻辑回归优于doc2vec逻辑回归，因为doc2 vec的训练集不够丰富或大，不足以让模型学习实际的上下文关系来生成合理的嵌入。虽然doc2vec LR的表现比TF-IDF朴素贝叶斯要好。</p><p id="4fac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">值得一提的是，对于tf-idf，实现了多项式朴素贝叶斯，对于doc2vec使用了高斯朴素贝叶斯，因为多项式朴素贝叶斯对于在doc2vec中生成的负值是失败的，并且在tf-idf的情况下特征肯定是非负的，因为出现频率不能是负的。</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/344763c8f1f2b24bb275b08aa01830f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*DjXGuafbefBzreuqiW5Iww.png"/></div></figure><p id="1fc5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在下一篇博客中，我将尝试使用单词包技术进行令牌分类，并通过特征哈希来扩展它，实现word2vec嵌入。</p><blockquote class="nx"><p id="b69d" class="ny nz iq bd oa ob oc od oe of og kk dk translated">请在contact.ishaanarora@gmail.com联系我，或者在这个<a class="ae ku" href="https://github.com/ishaan007/vector_space_modelling" rel="noopener ugc nofollow" target="_blank"> github repo </a>上开一个问题，讨论这个帖子中的任何事情，甚至是一般的生活:P如果你已经到了这一步，你太棒了！</p></blockquote><figure class="oi oj ok ol om nk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/97ebbdc64dd1cc42e3834de90dbacd3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*HslrQIpPEH_n-jFjHaoW2Q.jpeg"/></div></figure></div></div>    
</body>
</html>