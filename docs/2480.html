<html>
<head>
<title>Why, How and When to apply Feature Selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么、如何以及何时应用特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-how-and-when-to-apply-feature-selection-e9c69adfabf2?source=collection_archive---------1-----------------------#2018-01-31">https://towardsdatascience.com/why-how-and-when-to-apply-feature-selection-e9c69adfabf2?source=collection_archive---------1-----------------------#2018-01-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="ac5f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现代数据集的信息非常丰富，数据是从数百万个物联网设备和传感器收集的。这使得数据变得高维，拥有数百个要素的数据集很常见，甚至达到数万个也不罕见。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/c2a266bc98477826c0cea9cca47d464f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*SGai7lOKRn9YhM2p-8SChQ.jpeg"/></div></figure><p id="19cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">特征选择是数据科学家工作流程中的一个非常重要的组成部分。当呈现非常高维度的数据时，模型通常会阻塞，因为</p><ol class=""><li id="7aed" class="kt ku iq jp b jq jr ju jv jy kv kc kw kg kx kk ky kz la lb bi translated"><strong class="jp ir">训练时间</strong>随着特征数量呈指数增长。</li><li id="18b9" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk ky kz la lb bi translated">随着特征数量的增加，模型具有越来越大的<strong class="jp ir">过度拟合</strong>的风险。</li></ol><p id="07d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">特征选择方法有助于解决这些问题，方法是在不损失太多信息的情况下降低维数。这也有助于理解这些特性及其重要性。</p><p id="9fb8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本文中，我将讨论以下特征选择技术及其特点。</p><ol class=""><li id="9e17" class="kt ku iq jp b jq jr ju jv jy kv kc kw kg kx kk ky kz la lb bi translated">过滤方法</li><li id="60c7" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk ky kz la lb bi translated">包装方法和</li><li id="efbe" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk ky kz la lb bi translated">嵌入式方法。</li></ol><h1 id="b71c" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">过滤方法</h1><p id="ada8" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">过滤方法考虑特征和目标变量之间的关系来计算特征的重要性。</p><h2 id="823b" class="mk li iq bd lj ml mm dn ln mn mo dp lr jy mp mq lv kc mr ms lz kg mt mu md mv bi translated">方差比检验</h2><p id="a715" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">f 检验是一种统计检验，用于模型之间的比较，并检查模型之间的差异是否显著。</p><p id="5796" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">f 检验对模型<strong class="jp ir"> X </strong>和<strong class="jp ir"> Y </strong>进行假设检验，其中<strong class="jp ir"> X </strong>是仅由常数创建的模型，而<strong class="jp ir"> Y </strong>是由常数和特征创建的模型。</p><p id="93f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">比较两个模型中的最小二乘误差，并检查模型<strong class="jp ir"> X </strong>和<strong class="jp ir"> Y </strong>之间的误差差异是否显著或偶然引入。</p><p id="5253" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">f 检验在特征选择中是有用的，因为我们知道每个特征在改进模型中的重要性。</p><p id="86d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Scikit learn 提供使用 F-Test 选择 K 个最佳特性的<strong class="jp ir">。</strong></p><pre class="km kn ko kp gt mw mx my mz aw na bi"><span id="2831" class="mk li iq mx b gy nb nc l nd ne">sklearn.feature_selection.f_regression</span></pre><p id="988c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于分类任务</p><pre class="km kn ko kp gt mw mx my mz aw na bi"><span id="1d8f" class="mk li iq mx b gy nb nc l nd ne">sklearn.feature_selection.f_classif</span></pre><p id="674a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用 f 检验来选择特征有一些缺点。F-Test 检查并仅捕获要素和标注之间的线性关系。高度相关的特征被给予较高的分数，而不太相关的特征被给予较低的分数。</p><ol class=""><li id="0bff" class="kt ku iq jp b jq jr ju jv jy kv kc kw kg kx kk ky kz la lb bi translated">相关性具有很强的欺骗性，因为它不能捕捉强非线性关系。</li></ol><p id="bf4e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.使用像相关性这样的汇总统计可能是个坏主意，正如安斯科姆的四重奏所展示的那样。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ng"><img src="../Images/b5442433de045fa7dcdd2959954418ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oMbcPjuDprAu_QAGizFf7g.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Francis Anscombe illustrates how four distinct datasets have same mean, variance and correlation to emphasize ‘summary statistics’ does not completely describe the datasets and can be quite deceptive.</figcaption></figure><h2 id="41f8" class="mk li iq bd lj ml mm dn ln mn mo dp lr jy mp mq lv kc mr ms lz kg mt mu md mv bi translated">交互信息</h2><p id="6224" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">两个变量之间的互信息衡量一个变量对另一个变量的依赖性。如果<strong class="jp ir"> X </strong>和<strong class="jp ir"> Y </strong>是两个变量，并且</p><ol class=""><li id="9b2e" class="kt ku iq jp b jq jr ju jv jy kv kc kw kg kx kk ky kz la lb bi translated">如果<strong class="jp ir"> X </strong>和<strong class="jp ir"> Y </strong>是独立的，那么知道<strong class="jp ir"> X </strong>就无法得到关于<strong class="jp ir"> Y </strong>的任何信息，反之亦然。因此它们的相互信息是<strong class="jp ir"> 0 </strong>。</li><li id="c398" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk ky kz la lb bi translated">如果<strong class="jp ir"> X </strong>是<strong class="jp ir"> Y </strong>的确定性函数，那么我们可以用互信息<strong class="jp ir"> 1 </strong>从<strong class="jp ir"> Y </strong>确定<strong class="jp ir"> X </strong>，从<strong class="jp ir"> X </strong>确定<strong class="jp ir"> Y </strong>。</li><li id="4d4b" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk ky kz la lb bi translated">当我们有 Y = f(X，Z，M，N)时，0 &lt; mutual information &lt; 1</li></ol><p id="23f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">We can select our features from feature space by ranking their mutual information with the target variable.</p><p id="3c42" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Advantage of using mutual information over F-Test is, it does well with the non-linear relationship between feature and target variable.</p><p id="d38a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Sklearn offers feature selection with Mutual Information for regression and classification tasks.</p><pre class="km kn ko kp gt mw mx my mz aw na bi"><span id="c6a2" class="mk li iq mx b gy nb nc l nd ne">sklearn.feature_selection.mututal_info_regression <br/>sklearn.feature_selection.mututal_info_classif</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi np"><img src="../Images/2ad909182f7137950b534c8f25ebba25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kr8bbyVaQNLCZBPUXs8b1g.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">F-Test captures the linear relationship well. Mutual Information captures any kind of relationship between two variables. <a class="ae nf" href="http://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html" rel="noopener ugc nofollow" target="_blank">http://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html</a></figcaption></figure><h2 id="7bbf" class="mk li iq bd lj ml mm dn ln mn mo dp lr jy mp mq lv kc mr ms lz kg mt mu md mv bi translated">Variance Threshold</h2><p id="7323" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">This method removes features with variation below a certain cutoff.</p><p id="93b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">The idea is when a feature doesn’t vary much within itself, it generally has very little predictive power.</p><pre class="km kn ko kp gt mw mx my mz aw na bi"><span id="717e" class="mk li iq mx b gy nb nc l nd ne">sklearn.feature_selection.VarianceThreshold</span></pre><p id="ddaf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Variance Threshold doesn’t consider the relationship of features with the target variable.</p></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><h1 id="5a33" class="lh li iq bd lj lk nx lm ln lo ny lq lr ls nz lu lv lw oa ly lz ma ob mc md me bi translated">Wrapper Methods</h1><p id="9e54" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">Wrapper Methods generate models with a subsets of feature and gauge their model performances.</p><h2 id="8506" class="mk li iq bd lj ml mm dn ln mn mo dp lr jy mp mq lv kc mr ms lz kg mt mu md mv bi translated">Forward Search</h2><p id="47b7" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">This method allows you to search for the best feature w.r.t model performance and add them to your feature subset one after the other.</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi oc"><img src="../Images/e5bdbec7e7cfda34659534dc63b6e412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YnXZS86uR2HibB3jlJB10A.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Forward Selection method when used to select the best 3 features out of 5 features, Feature 3, 2 and 5 as the best subset.</figcaption></figure><p id="0b2c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">For data with n features,</p><p id="446e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">-&gt;在第一轮中，创建具有单独特征的“N”个模型，并选择最佳预测特征。</p><p id="5c61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">-&gt;在第二轮中，用每个特征和先前选择的特征创建“n-1”个模型。</p><p id="5a7a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">-&gt;重复这一过程，直到选择了“m”个特征的最佳子集。</p><h2 id="b047" class="mk li iq bd lj ml mm dn ln mn mo dp lr jy mp mq lv kc mr ms lz kg mt mu md mv bi translated">递归特征消除</h2><p id="ae64" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">顾名思义，这种方法一个接一个地消除特定模型上表现最差的特征，直到知道特征的最佳子集。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi od"><img src="../Images/a3eba30cc347cec91dc6e3a57d4add83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qXqx7_hDtsO9ez7_nxSXOw.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Recursive elimination eliminates the least explaining features one after the other. Feature 2,3 and 5 are the best subset of features arrived by Recursive elimination.</figcaption></figure><p id="5b27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于具有 n 个特征的数据，</p><p id="6060" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">-&gt;在第一轮中,“n-1”个模型结合了除一个特征之外的所有特征。性能最低的特征被移除</p><p id="5b60" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">-&gt;在第二轮中，通过移除另一个特征来创建“n-2”模型。</p><p id="22a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">包装器方法通过广泛的贪婪搜索向您承诺一组最佳的特性。</p><p id="cf9d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是包装方法的主要缺点是需要训练的模型数量庞大。它在计算上非常昂贵，并且对于大量特征是不可行的。</p></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><h1 id="2b49" class="lh li iq bd lj lk nx lm ln lo ny lq lr ls nz lu lv lw oa ly lz ma ob mc md me bi translated"><strong class="ak">嵌入方法</strong></h1><p id="70ea" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">特征选择也可以通过一些机器学习模型提供的洞察力来实现。</p><p id="6d9f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">套索线性回归</strong>可用于特征选择。套索回归是通过在线性回归的成本函数中增加一个额外的项来实现的。这除了防止过度拟合之外，还将不太重要的特征的系数降低到零。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi oe"><img src="../Images/50c70529c20c3e9e31d63b32b682c98d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m15OB5BljiCFlDDXBqflYA.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">The highlighted term has been added additionally to Cost Function of Linear Regression for the purpose of Regularization.</figcaption></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi of"><img src="../Images/774505cf29e73b8fec4c17d9c522bb85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jb_Zlb85QsArbwC6PEjrHg.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">As we vary ƛ in the cost function, the coefficients have been plotted in this graph. We observe that for ƛ ~=0, the coefficients of most of the features side towards zero. In the above graph, we can see that only ‘lcavol’, ’svi’, and ‘lweight’ are the features with non zero coefficients when ƛ = 0.4.</figcaption></figure><p id="ba21" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">基于树的模型</strong>计算特性的重要性，因为它们需要将性能最佳的特性保持在靠近树根的位置。构建决策树包括计算最佳预测特征。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi og"><img src="../Images/149054273deae87533825abd51ba6d61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2236JElUGuc6eLA3Miidlw.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Decision Trees keep the most important features near the root. In this decision tree, we find that Number of legs is the most important feature, followed by if it hides under the bed and it is delicious and so on. <a class="ae nf" href="https://www.safaribooksonline.com/library/view/data-science-from/9781491901410/ch17.html" rel="noopener ugc nofollow" target="_blank">https://www.safaribooksonline.com/library/view/data-science-from/9781491901410/ch17.html</a></figcaption></figure><p id="4fd7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基于树的模型中的特征重要性基于<strong class="jp ir">基尼指数</strong>、<strong class="jp ir">熵</strong>或<strong class="jp ir">卡方</strong>值来计算。</p></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><p id="3022" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与数据科学中的大多数事情一样，特征选择高度依赖于上下文和数据，并且对于特征选择没有一站式解决方案。最好的方法是了解每种方法的机制，并在需要时使用。</p><p id="e306" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我主要使用特征选择技术来获得关于特征及其与目标变量的相对重要性的见解。请在下面评论你使用的特征选择技术。</p></div></div>    
</body>
</html>