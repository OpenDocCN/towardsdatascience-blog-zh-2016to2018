<html>
<head>
<title>Understanding Neural Networks Using Excel</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Excel 了解神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-convolutions-using-excel-886ca0a964b7?source=collection_archive---------1-----------------------#2017-11-19">https://towardsdatascience.com/understanding-convolutions-using-excel-886ca0a964b7?source=collection_archive---------1-----------------------#2017-11-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/5612b04713d77a436cdeb105a8858d82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TyhFaDwV_o2BcPSmRoi3xw.png"/></div></div></figure><div class=""/><div class=""><h2 id="1d94" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">熟悉深度学习的 TL: DR 方法</h2></div><p id="8d32" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">为了简化卷积神经网络的概念，我将尝试解释在开发您的深度学习模型时会发生什么。要了解更多信息，我建议在网上搜索<a class="ae lm" href="http://neuralnetworksanddeeplearning.com/chap6.html" rel="noopener ugc nofollow" target="_blank"/>，因为这里有大量的信息(比如这个<a class="ae lm" href="https://www.youtube.com/watch?v=Oqm9vsf_hvU&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank">视频</a>)。这个解释来源于<a class="ae lm" href="https://github.com/fastai/fastai/tree/master/courses/dl1/excel" rel="noopener ugc nofollow" target="_blank"> fast.ai </a>库。</p><p id="d1ba" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这张简单神经网络的图片基本上代表了这个例子中发生的事情。</p><figure class="lo lp lq lr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ln"><img src="../Images/92dde12201dd6afc67b9ef98d9b09dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LaEgAU-vdsR_pClMcgbikQ.jpeg"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Simple Neural Network</figcaption></figure><h2 id="f914" class="lw lx jb bd ly lz ma dn mb mc md dp me kz mf mg mh ld mi mj mk lh ml mm mn mo bi translated">输入层</h2><p id="b59a" class="pw-post-body-paragraph kq kr jb ks b kt mp kc kv kw mq kf ky kz mr lb lc ld ms lf lg lh mt lj lk ll ij bi translated">7 号图像数据来自<a class="ae lm" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST </a>数据库，我们假设您使用<a class="ae lm" href="https://medium.com/p/dfdcaf559cba/edit" rel="noopener">预训练模型</a>进行分类。</p><figure class="lo lp lq lr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mu"><img src="../Images/46383587c8592398b2f7c66f26908597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jlGhrU3IvHhkl51z-z3aMA.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">You can see the 7 by the larger numbers in the data</figcaption></figure><h2 id="ae0f" class="lw lx jb bd ly lz ma dn mb mc md dp me kz mf mg mh ld mi mj mk lh ml mm mn mo bi translated">隐藏层 1</h2><p id="dd3a" class="pw-post-body-paragraph kq kr jb ks b kt mp kc kv kw mq kf ky kz mr lb lc ld ms lf lg lh mt lj lk ll ij bi translated">隐藏图层是对输入进行转换，以便从输出图层的数据中识别更复杂的要素，从而做出更好的评估。</p><p id="7464" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">两个过滤器将代表不同的形状-第一个过滤器设计用于检测水平边缘，第二个过滤器检测垂直边缘。这个 3x3 滤波器被称为<a class="ae lm" href="http://setosa.io/ev/image-kernels/" rel="noopener ugc nofollow" target="_blank">卷积核</a>。针对输入中的水平边缘激活滤波器 1。Conv1 显示了对输入的 3x3 部分进行处理并乘以卷积内核后两者的激活情况。下面的一张图给了你一个更好的想法。</p><figure class="lo lp lq lr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mv"><img src="../Images/c4c9abd56a106bb0429b4a07374f80a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6SNDI-H2X26GUkFIx4uG5A.png"/></div></div></figure><p id="7fb2" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">*虽然这在 2d 数组中表示，但它们应该作为张量<a class="ae lm" href="https://en.wikipedia.org/wiki/Tensor" rel="noopener ugc nofollow" target="_blank">堆叠在一起</a>。其中每个矩阵代表张量中的一个切片。这些本质上都是正在发生的行操作(<a class="ae lm" rel="noopener" target="_blank" href="/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c">线性代数</a>)。</p><p id="e71b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">=SUM(F11:H13*$AD$11:$AF$13)是发生的卷积。</p><p id="0af3" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">该求和将导致输入中特定 3×3 点的激活数为 3。</p><figure class="lo lp lq lr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mw"><img src="../Images/e5567e26a1dedb8a3b8fd0c9b7b2c6ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F96jmLp1Ljx4UjtMmhlLmQ.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">This this would represent a single layer.</figcaption></figure><h2 id="2da0" class="lw lx jb bd ly lz ma dn mb mc md dp me kz mf mg mh ld mi mj mk lh ml mm mn mo bi translated">激活功能</h2><figure class="lo lp lq lr gt is gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/ad9ff0b6df71d9852c2a12f7152fdd20.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*QindKA4Dt7Ol3CbICMSxWw.png"/></div></figure><p id="a58c" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">接下来，我们使用非线性单元，通过使用<a class="ae lm" href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener"> RELU </a>作为我们的激活函数来消除负面影响。接下来我们可以看到底片在下一张图中消失了。</p><h2 id="e22e" class="lw lx jb bd ly lz ma dn mb mc md dp me kz mf mg mh ld mi mj mk lh ml mm mn mo bi translated"><strong class="ak">隐藏第二层</strong></h2><p id="6d49" class="pw-post-body-paragraph kq kr jb ks b kt mp kc kv kw mq kf ky kz mr lb lc ld ms lf lg lh mt lj lk ll ij bi translated">接下来，我们做另一个卷积。Conv2 将是下一个隐藏层。这将对 Conv1 中的两个矩阵进行加权，取其和积。这里的卷积核将表示一个 2X3X3 张量。</p><p id="9223" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">使用 RELU 后，我们现在已经创建了我们的第二层。</p><figure class="lo lp lq lr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi my"><img src="../Images/67afd2fe5d0f93b12a783270d9c02542.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qckjchubsvhp65t_vplhgg.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Layer 1 and 2</figcaption></figure><h2 id="4664" class="lw lx jb bd ly lz ma dn mb mc md dp me kz mf mg mh ld mi mj mk lh ml mm mn mo bi translated">最大池化</h2><figure class="lo lp lq lr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mz"><img src="../Images/c7b974cc0b66dffc5ffb901447f43951.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Feiexqhmvh9xMGVVJweXhg.gif"/></div></div></figure><p id="aae3" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">通过只取 Conv2 中 2x2 部分的最大值，Max pooling 将达到高度和宽度分辨率的一半。在 Maxpool 矩阵中，我们可以看到 Conv2 的 2x2 部分的最大值，即 33。池的计算速度比卷积快。同样，它给了你一些平移不变性。</p><figure class="lo lp lq lr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi na"><img src="../Images/4a2b512efc435c4409977dc63b639f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zbhp3yCWPvHOdCj-3Ic-og.png"/></div></div></figure><h2 id="b49e" class="lw lx jb bd ly lz ma dn mb mc md dp me kz mf mg mh ld mi mj mk lh ml mm mn mo bi translated">输出层</h2><p id="45ae" class="pw-post-body-paragraph kq kr jb ks b kt mp kc kv kw mq kf ky kz mr lb lc ld ms lf lg lh mt lj lk ll ij bi translated">接下来，我们通过在 Maxpool 中获取所有激活并给它们一个权重来构建我们的全连接层。这是通过做一个矩阵乘积来实现的。在 excel 中，我们将获取激活和权重的和积。因此，与之前解析卷积层中的每个部分不同，全连接层(密集层)将对卷积层提取并由最大池层缩减采样的特征执行分类。</p><figure class="lo lp lq lr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nb"><img src="../Images/d70bb5129341a19a301a19fe08665905.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J4GnbqQbGKeGX354ieMjPA.png"/></div></div></figure><p id="1472" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这个例子只代表一个类，也就是一个数字。我们还得把剩下的数字分类。</p></div></div>    
</body>
</html>