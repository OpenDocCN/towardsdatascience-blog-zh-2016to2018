<html>
<head>
<title>Pre-Processing in Natural Language Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言机器学习中的预处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pre-processing-in-natural-language-machine-learning-898a84b8bd47?source=collection_archive---------1-----------------------#2017-11-28">https://towardsdatascience.com/pre-processing-in-natural-language-machine-learning-898a84b8bd47?source=collection_archive---------1-----------------------#2017-11-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/1e59353edecd31dc122bbd570e11f367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GKHXT8SImSHCUBw0Z5FMcw.jpeg"/></div></div></figure><p id="9e0e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们很容易忘记我们每天的对话中存储了多少数据。随着数字景观的演变，挖掘文本或自然语言处理(NLP)是人工智能和机器学习中一个不断增长的领域。本文涵盖了应用于 NLP 问题的常见预处理概念。</p><p id="66b5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">文本可以有多种形式，从单个单词的列表，到句子，再到包含特殊字符的多个段落(比如 tweets)。像任何数据科学问题一样，理解被问的问题将告知可以采用什么步骤来将单词转换为与机器学习算法一起工作的数字特征。</p><h1 id="a81b" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">不良贷款的历史</h1><p id="6b5c" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">当我还是个孩子的时候，科幻小说几乎总是有一台电脑，你可以吠叫命令，让他们理解，有时，但不总是，执行。当时，这项技术似乎还很遥远，但今天我口袋里装着一部手机，它比任何人想象的都要小，而且功能更强大。语音转文本的历史复杂而漫长，但却是 NPL 的萌芽。</p><p id="9a34" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">早期的努力需要大量手工编码的词汇和语言规则。1954 年在乔治敦大学首次实现的从英语到俄语的自动翻译仅限于少数几个句子。</p><p id="0fff" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">1964 年，第一个聊天机器人伊莱扎在麻省理工学院诞生。它建立在模式匹配和替代的基础上，通过提出开放式问题来模拟治疗过程。虽然它似乎复制了意识，但它没有真正的对话背景。尽管能力有限，许多人还是对这种人性化的互动感到惊讶。</p><p id="1d63" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该领域的大部分发展实际上始于 20 世纪 80 年代，当时引入了机器学习算法。研究人员从更严格的<a class="ae lz" href="https://en.wikipedia.org/wiki/Transformational_grammar" rel="noopener ugc nofollow" target="_blank">转换语法模型</a>转向了在<a class="ae lz" href="https://en.wikipedia.org/wiki/Cache_language_model" rel="noopener ugc nofollow" target="_blank">缓存语言模型</a>中描述的更宽松的概率关系，这允许更快速的缩放，并更轻松地处理不熟悉的输入。</p><p id="c2d9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">整个 90 年代，计算能力的指数级增长推动了科技的进步，但直到 2006 年 IBM 的沃森(Watson)上了《危险边缘》(Jeopardy)节目，公众才看到计算机智能的进步。对我来说，是 2011 年 iPhone 引入 Siri 让我意识到了它的潜力。</p><p id="97fe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当前的 NLP 前景很容易成为它自己的文章。私营公司前所未有的投资和普遍的开源态度已经扩展了一些很大程度上为更多的受众和应用所专有的东西。一个有趣的例子是在翻译领域，谷歌正在努力翻译任何语言(即使用户体验中的一些错误需要解决)。</p><div class="ma mb gp gr mc md"><a href="https://betanews.com/2017/10/05/google-pixel-buds/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ir gy z fp mi fr fs mj fu fw ip bi translated">谷歌的即时翻译 Pixel Buds 是其迄今为止最令人难以置信的版本</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">谷歌昨天宣布了许多激动人心的发布，但也许最激动人心的当然是…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">betanews.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr jw md"/></div></div></a></div><h1 id="d244" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">预处理的重要性</h1><p id="5e81" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">如果没有大量的后端工作，上面描述的魔术就不会发生。将文本转换成某种算法可以消化的东西是一个复杂的过程。有四个不同的部分:</p><ul class=""><li id="6f18" class="ms mt iq ka b kb kc kf kg kj mu kn mv kr mw kv mx my mz na bi translated"><strong class="ka ir">清理</strong>包括通过去除停用词来去除文本中不太有用的部分，处理大写和字符以及其他细节。</li><li id="4e73" class="ms mt iq ka b kb nb kf nc kj nd kn ne kr nf kv mx my mz na bi translated"><strong class="ka ir">注释</strong>包括对文本应用一个方案。注释可能包括结构标记和<a class="ae lz" href="https://en.wikipedia.org/wiki/Lexical_category" rel="noopener ugc nofollow" target="_blank">词性</a>标注。</li><li id="68e3" class="ms mt iq ka b kb nb kf nc kj nd kn ne kr nf kv mx my mz na bi translated"><strong class="ka ir">标准化</strong>包括通过词干化、词汇化和其他形式的标准化来翻译(映射)方案中的术语或语言简化。</li><li id="8a26" class="ms mt iq ka b kb nb kf nc kj nd kn ne kr nf kv mx my mz na bi translated"><strong class="ka ir">分析</strong>包括对数据集进行统计探测、操作和归纳，以进行特征分析。</li></ul><h1 id="3233" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">工具</h1><p id="bbf2" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">有多种预处理方法。下面的列表并不全面，但它确实给出了从哪里开始的想法。重要的是要认识到，就像所有的数据问题一样，将任何东西转换成机器学习的格式都会将其简化为一种一般化的状态，这意味着在此过程中会损失一些数据的保真度。真正的艺术是了解每一种方法的利弊，从而谨慎地选择正确的方法。</p><h1 id="faaf" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">资本化</h1><p id="8663" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">文本常常有各种反映句子开头、专有名词强调的大写形式。为了简单起见，最常见的方法是将所有内容都简化为小写，但重要的是要记住，一些单词，如“us”到“US”，在简化为小写时可能会改变含义。</p><h1 id="784e" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">无用词</h1><p id="b38e" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">给定文本中的大多数单词是句子的连接部分，而不是显示主语、宾语或意图。像“the”或“and”这样的词可以通过将文本与停用词列表进行比较来删除。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="0079" class="np kx iq nl b gy nq nr l ns nt">IN:<br/>['He', 'did', 'not', 'try', 'to', 'navigate', 'after', 'the', 'first', 'bold', 'flight', ',', 'for', 'the', 'reaction', 'had', 'taken', 'something', 'out', 'of', 'his', 'soul', '.']</span><span id="e34c" class="np kx iq nl b gy nu nr l ns nt">OUT:<br/>['try', 'navigate', 'first', 'bold', 'flight', ',', 'reaction', 'taken', 'something', 'soul', '.']</span></pre><p id="ad90" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在上面的例子中，它将 23 个单词减少到了 11 个，但是需要注意的是，单词“not”被删除了，这取决于我在做什么，这可能是一个大问题。根据所需的敏感度，用户可以手动创建自己的停用词词典或利用预建的库。</p><h1 id="34a5" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">标记化</h1><p id="f22f" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">标记化描述了将段落分割成句子，或者将句子分割成单个单词。对于前一个<a class="ae lz" href="https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation" rel="noopener ugc nofollow" target="_blank">句子，可以应用边界歧义消除</a> (SBD)来创建单个句子的列表。这依赖于预先训练的语言特定算法，如 NLTK 的 Punkt 模型。</p><p id="57e5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过类似的过程，可以将句子拆分成单个单词和标点符号。最常见的是这种跨越空格的拆分，例如:</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="7bba" class="np kx iq nl b gy nq nr l ns nt">IN:</span><span id="1d96" class="np kx iq nl b gy nu nr l ns nt">"He did not try to navigate after the first bold flight, for the reaction had taken something out of his soul."</span><span id="bd24" class="np kx iq nl b gy nu nr l ns nt">OUT:</span><span id="d6c2" class="np kx iq nl b gy nu nr l ns nt">['He', 'did', 'not', 'try', 'to', 'navigate', 'after', 'the', 'first', 'bold', 'flight', ',', 'for', 'the', 'reaction', 'had', 'taken', 'something', 'out', 'of', 'his', 'soul', '.']</span></pre><p id="3854" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当一个单词被缩写、删节或被所有格时，这有时会引起问题。专有名词在使用标点符号的情况下也会受到影响(比如奥尼尔)。</p><h1 id="9eb7" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">词性标注</h1><p id="c4c4" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">理解词类对确定句子的意思有很大的影响。词性(POS)通常需要查看前面和后面的单词，并结合基于规则或随机的方法。然后，它可以与其他过程相结合，以实现更多的功能工程。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="be1d" class="np kx iq nl b gy nq nr l ns nt">IN:<br/>['And', 'from', 'their', 'high', 'summits', ',', 'one', 'by', 'one', ',', 'drop', 'everlasting', 'dews', '.']</span><span id="e004" class="np kx iq nl b gy nu nr l ns nt">OUT:<br/>[('And', 'CC'),<br/> ('from', 'IN'),<br/> ('their', 'PRP$'),<br/> ('high', 'JJ'),<br/> ('summits', 'NNS'),<br/> (',', ','),<br/> ('one', 'CD'),<br/> ('by', 'IN'),<br/> ('one', 'CD'),<br/> (',', ','),<br/> ('drop', 'NN'),<br/> ('everlasting', 'VBG'),<br/> ('dews', 'NNS'),<br/> ('.', '.')]</span><span id="5d20" class="np kx iq nl b gy nu nr l ns nt">Definitions of Parts of Speech<br/>('their', 'PRP$') PRP$: pronoun, possessive<br/>    her his mine my our ours their thy your</span></pre><h1 id="0c0c" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">堵塞物</h1><p id="d582" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">很多自然语言机器学习都是关于文本的情感。词干化是通过去掉不必要的字符(通常是后缀)来去除词尾变化，从而将单词简化为词根的过程。有几个词干模型，包括波特和雪球。结果可用于识别大型数据集之间的关系和共性。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="fd02" class="np kx iq nl b gy nq nr l ns nt">IN:<br/>["It never once occurred to me that the fumbling might be a mere mistake."]</span><span id="2e44" class="np kx iq nl b gy nu nr l ns nt">OUT:<br/> ['it', 'never',  'onc',  'occur',  'to',  'me',  'that',  'the', 'fumbl',  'might', 'be', 'a', 'mere',  'mistake.'],</span></pre><p id="86b2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">显而易见，缩减可能会产生一个不是真正单词的“根”单词。这不一定会对其效率产生负面影响，但如果像“universe”和“university”这样的词被简化为“univers”的同一个词根，就有“越界”的危险。</p><h1 id="48d8" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">词汇化</h1><p id="f6ee" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">词干变化是从词干变化到消除词形变化的另一种方法。通过确定词性和利用 WordNet 的英语词汇库，词汇化可以得到更好的结果。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="2baa" class="np kx iq nl b gy nq nr l ns nt">The stemmed form of leafs is: leaf<br/>The stemmed form of leaves is: leav</span><span id="8c88" class="np kx iq nl b gy nu nr l ns nt">The lemmatized form of leafs is: leaf<br/>The lemmatized form of leaves is: leaf</span></pre><p id="1d2b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Lemmazation 是一个更密集，因此更慢的过程，但更准确。词干在数据库查询中可能更有用，而词汇化在试图确定文本情感时可能工作得更好。</p><h1 id="ac73" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">计数/密度</h1><p id="c11d" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">也许特征工程的更基本的工具之一，增加字数、句子数、标点数和行业特定的字数可以极大地帮助预测或分类。有多种统计方法，其相关性严重依赖于上下文。</p><figure class="ng nh ni nj gt jr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/523ce54bfd6b8d9ebe964fcdcd1cad14.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*IGhqvs3ueoX2blQMJGRs2Q.png"/></div></figure><h1 id="26f9" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">单词嵌入/文本向量</h1><p id="194c" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">单词嵌入是将单词表示为向量的现代方法。单词嵌入的目的是将高维单词特征重新定义为低维特征向量。换句话说，它表示 X 和 Y 向量坐标上的单词，其中基于关系语料库的相关单词被更紧密地放置在一起。<a class="ae lz" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>和<a class="ae lz" href="http://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe </a>是最常见的将文本转换为矢量的模型。</p><h1 id="69bd" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">结论</h1><p id="4886" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">虽然这远不是一个全面的列表，但准备文本是一门复杂的艺术，需要在给定数据和问题的情况下选择最佳的工具。许多预建的库和服务可以提供帮助，但有些可能需要手动映射术语和单词。</p><p id="1b1a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦数据集准备就绪，就可以应用有监督和无监督的机器学习技术。从我最初的实验(这将是我自己的文章)来看，对单个字符串应用预处理技术与对大型数据帧应用预处理技术有很大的不同。调整步骤以获得最佳效率将是面对扩展保持灵活性的关键。</p><p id="d3dc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你喜欢这篇文章，请鼓掌，如果你有兴趣看更多关于自然语言处理的文章，请关注！</p><h1 id="af1e" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">额外资源</h1><div class="ma mb gp gr mc md"><a href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ir gy z fp mi fr fs mj fu fw ip bi translated">自然语言处理-维基百科</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">自然语言处理是计算机科学、人工智能和计算科学的一个领域</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">en.wikipedia.org</p></div></div><div class="mm l"><div class="nw l mo mp mq mm mr jw md"/></div></div></a></div><div class="ma mb gp gr mc md"><a href="https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ir gy z fp mi fr fs mj fu fw ip bi translated">理解和实现自然语言处理的终极指南(带 Python 代码)</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">根据行业估计，只有 21%的可用数据以结构化形式存在。正在生成数据…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">www.analyticsvidhya.com</p></div></div><div class="mm l"><div class="nx l mo mp mq mm mr jw md"/></div></div></a></div><div class="ma mb gp gr mc md"><a href="http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ir gy z fp mi fr fs mj fu fw ip bi translated">深入 NLTK，第一部分:NLTK 入门</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">第一部分:NLTK 入门(本文)第二部分:句子标记化和单词标记化第三部分:词性…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">textminingonline.com</p></div></div><div class="mm l"><div class="ny l mo mp mq mm mr jw md"/></div></div></a></div><div class="ma mb gp gr mc md"><a href="https://www.theatlantic.com/technology/archive/2014/06/when-parry-met-eliza-a-ridiculous-chatbot-conversation-from-1972/372428/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ir gy z fp mi fr fs mj fu fw ip bi translated">当帕里遇到伊莱扎时:1972 年的一段可笑的聊天机器人对话</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">他们可能没有通过图灵测试，但他们赢得了这场古怪之战。</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">www.theatlantic.com</p></div></div><div class="mm l"><div class="nz l mo mp mq mm mr jw md"/></div></div></a></div></div></div>    
</body>
</html>