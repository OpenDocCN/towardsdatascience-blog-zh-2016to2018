<html>
<head>
<title>Review: DenseNet — Dense Convolutional Network (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:DenseNet 密集卷积网络(图像分类)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=collection_archive---------2-----------------------#2018-11-25">https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=collection_archive---------2-----------------------#2018-11-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="ba60" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>这个故事中，<strong class="jp ir"> DenseNet(密集卷积网络)</strong>进行了回顾。这是在<strong class="jp ir"> 2017 CVPR </strong>获得<strong class="jp ir">最佳论文奖</strong>的论文，引用超过<strong class="jp ir"> 2000 次</strong>。它是由康维尔大学、清华大学和脸书人工智能研究所(FAIR)联合发明的。(<a class="ku kv ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----b6631a8ef803--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p><p id="87a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">密集连接，与<a class="ae kw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <em class="kx"> ResNet </em> </a>和<a class="ae kw" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e"> <em class="kx">预激活 ResNet </em> </a>相比，参数少，精度高。那么，让我们来看看它是如何工作的。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><h1 id="346b" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">涵盖哪些内容</h1><ol class=""><li id="f6c0" class="md me iq jp b jq mf ju mg jy mh kc mi kg mj kk mk ml mm mn bi translated"><strong class="jp ir">密集街区</strong></li><li id="9009" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk mk ml mm mn bi translated"><strong class="jp ir"> DenseNet 架构</strong></li><li id="61b3" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk mk ml mm mn bi translated"><strong class="jp ir">dense net 的优势</strong></li><li id="e2b4" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk mk ml mm mn bi translated"><strong class="jp ir"> CIFAR &amp; SVHN 小规模数据集结果</strong></li><li id="133f" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk mk ml mm mn bi translated"><strong class="jp ir"> ImageNet 大规模数据集结果</strong></li><li id="a898" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk mk ml mm mn bi translated"><strong class="jp ir">特征复用的进一步分析</strong></li></ol></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><h1 id="887c" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated"><strong class="ak"> 1。密集块</strong></h1><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi mt"><img src="../Images/65920cf9f467c2f6512ee69c03ab3a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RfyAoe6Wlv4aLip2Y5Aw-Q.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">Standard ConvNet Concept</strong></figcaption></figure><p id="7548" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<strong class="jp ir">标准 ConvNet </strong>中，输入图像经过多重卷积，获得高层特征。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nk"><img src="../Images/d7ae4d3d1b3a7ec7191d37d128bec740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4wx7szWCBse9-7eemGQJSw.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">ResNet Concept</strong></figcaption></figure><p id="d7e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae kw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="jp ir"><em class="kx">ResNet</em></strong></a>中，提出了身份映射来促进梯度传播。<strong class="jp ir">使用元素加法</strong>。它可以被看作是具有从一个 ResNet 模块传递到另一个 ResNet 模块的状态的算法。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nl"><img src="../Images/630a194de63115b96d9c4057254ee50d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rmHdoPjGUjRek6ozH7altw.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">One Dense Block in DenseNet</strong></figcaption></figure><p id="7a61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<strong class="jp ir"> DenseNet </strong>中，每一层从所有前面的层获得额外的输入，并将自己的特征映射传递给所有后面的层。<strong class="jp ir">使用串联</strong>。<strong class="jp ir">每一层都从所有前面的层接收“集体知识”</strong>。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/f92f54f0784282e9dc608b34a020cf10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*P7tcHlzxm9Afg0Cejftv0g.png"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">Dense Block in DenseNet with Growth Rate k</strong></figcaption></figure><p id="44f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">由于每一层都从所有前面的层接收特征图，网络可以更细更紧凑，即通道数量可以更少</strong>。增长率<em class="kx"> k </em>是每层增加的通道数。</p><p id="650c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，它具有较高的计算效率和存储效率。下图显示了正向传播过程中串联的概念:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e6ad01bd8de109a276c856ef42c5fcbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*9ysRPSExk0KvXR0AhNnlAA.gif"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">Concatenation during Forward Propagation</strong></figcaption></figure></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><h1 id="ae15" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">2.<strong class="ak"> DenseNet 架构</strong></h1><h2 id="963e" class="no lg iq bd lh np nq dn ll nr ns dp lp jy nt nu lt kc nv nw lx kg nx ny mb nz bi translated">2.1.基本 DenseNet 成分层</h2><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/78eb4572777f691ceef76b50c53c46af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*IwvJGTxBAcb1H5tSJR6Lng.gif"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">Composition Layer</strong></figcaption></figure><p id="79cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于每一个构图层，<strong class="jp ir">预激活</strong> <a class="ae kw" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> <strong class="jp ir">批范数(BN) </strong> </a> <strong class="jp ir">和 ReLU，然后 3×3 Conv </strong>用<em class="kx"> k </em>通道的输出特征图完成，比如变换<em class="kx"> x </em> 0，<em class="kx"> x </em> 1，<em class="kx"> x </em> 2，<em class="kx"> x </em> 3 到<em class="kx"> x </em> 4。这是来自<a class="ae kw" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e"> <em class="kx">的创意，预激活 ResNet </em> </a> <em class="kx">。</em></p><h2 id="f734" class="no lg iq bd lh np nq dn ll nr ns dp lp jy nt nu lt kc nv nw lx kg nx ny mb nz bi translated">2.2.DenseNet-B(瓶颈层)</h2><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi oa"><img src="../Images/364e895a2c5931e5426ee9da60096849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dniz8zK2ClBY96ol7YGnJw.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">DenseNet-B</strong></figcaption></figure><p id="6476" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了降低模型的复杂度和尺寸，<strong class="jp ir"> BN-ReLU-1×1 Conv </strong>在<strong class="jp ir"> BN-ReLU-3×3 Conv </strong>之前完成。</p><h2 id="20ac" class="no lg iq bd lh np nq dn ll nr ns dp lp jy nt nu lt kc nv nw lx kg nx ny mb nz bi translated">2.4.具有过渡层的多个密集块</h2><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ob"><img src="../Images/10680d41aad356ee198af1d8e30aa587.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BJM5Ht9D5HcP5CFpu8bn7g.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">Multiple Dense Blocks</strong></figcaption></figure><p id="b414" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1×1 Conv，然后是 2×2 平均池，用作两个相邻密集区块之间的过渡层。</p><p id="cd95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在密集块中，要素地图的大小是相同的，因此可以很容易地将它们连接在一起。</p><p id="9c43" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在最后一个密集块的末尾，执行全局平均池化，然后附加 softmax 分类器。</p><h2 id="35ff" class="no lg iq bd lh np nq dn ll nr ns dp lp jy nt nu lt kc nv nw lx kg nx ny mb nz bi translated">2.3.DenseNet-BC(进一步压缩)</h2><p id="cb3e" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy oc ka kb kc od ke kf kg oe ki kj kk ij bi translated"><strong class="jp ir">如果一个密集块包含<em class="kx"> m </em>个特征图，过渡层生成<em class="kx"> θm </em>个输出特征图</strong>，其中 0 &lt; <em class="kx"> θ </em> ≤1 称为压缩因子。</p><p id="f1ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当<em class="kx"> θ </em> =1 时，跨过渡层的特征映射的数量保持不变。<em class="kx"> θ </em> &lt;为 1 的 DenseNet 称为<strong class="jp ir"> DenseNet-C </strong>，实验中<strong class="jp ir"> <em class="kx"> θ </em> =0.5 </strong>。</p><p id="410b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当使用<strong class="jp ir">θ<em class="kx"/>&lt;1 的瓶颈层和过渡层时</strong>，该型号称为<strong class="jp ir"> DenseNet-BC </strong>。</p><p id="6a99" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后训练有/无 B/C 和不同<strong class="jp ir"> <em class="kx"> L </em>层</strong>和<strong class="jp ir"> <em class="kx"> k </em>生长率</strong>的 DenseNets <strong class="jp ir">。</strong></p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><h1 id="7799" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">3.<strong class="ak">dense net 的优势</strong></h1><h2 id="1184" class="no lg iq bd lh np nq dn ll nr ns dp lp jy nt nu lt kc nv nw lx kg nx ny mb nz bi translated">3.1.强梯度流</h2><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi of"><img src="../Images/cac7e29f6f26a27d874bb45876c0b0b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9atnQFu8ncrqFqZdB_LNVg.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">Implicit “Deep Supervision”</strong></figcaption></figure><p id="a445" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">误差信号可以容易地更直接地传播到更早的层。这是一种隐式深度监督，因为较早的层可以从最终分类层获得直接监督。</p><h2 id="50d8" class="no lg iq bd lh np nq dn ll nr ns dp lp jy nt nu lt kc nv nw lx kg nx ny mb nz bi translated">3.2.参数和计算效率</h2><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi og"><img src="../Images/44ecda1e8513ed01a29b812383656e59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*03pZkWqHN7A3pd81Pi-cIQ.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">Number of Parameters for ResNet and DenseNet</strong></figcaption></figure><p id="e6f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于每一层，<a class="ae kw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <em class="kx"> ResNet </em> </a>中的参数个数与<em class="kx"> C </em> ×C 成正比，而 DenseNet 中的参数个数与<em class="kx"> l </em> × <em class="kx"> k </em> × <em class="kx"> k </em>成正比。</p><p id="ca87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于<em class="kx"> k </em> &lt; &lt; <em class="kx"> C </em>，DenseNet 的尺寸比<a class="ae kw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <em class="kx"> ResNet </em> </a>小很多。</p><h2 id="2083" class="no lg iq bd lh np nq dn ll nr ns dp lp jy nt nu lt kc nv nw lx kg nx ny mb nz bi translated">3.3.更加多样化的功能</h2><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi oh"><img src="../Images/0ed8475b68032da05064698bfa86d7f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VoaoQpASmgyaxEISacP44Q.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">More Diversified Features in DenseNet</strong></figcaption></figure><p id="4200" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于 DenseNet 中的每一层都接收所有前面的层作为输入，因此更多样化的特性和模式更丰富。</p><h2 id="1e69" class="no lg iq bd lh np nq dn ll nr ns dp lp jy nt nu lt kc nv nw lx kg nx ny mb nz bi translated">3.4.保持低复杂性特征</h2><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi oi"><img src="../Images/b410fb8873bf9dae72f344730ea96344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PZHH8YgkG4nYlIpicCo1Ww.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">Standard ConvNet</strong></figcaption></figure><p id="6687" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在标准的 ConvNet 中，分类器使用最复杂的特征。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi oj"><img src="../Images/0ba652b77c649174b0cac4491ed6fe4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t_orlp67H-odvgMa4LTzzw.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">DenseNet</strong></figcaption></figure><p id="ea00" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 DenseNet 中，分类器使用所有复杂级别的特征。它倾向于给出更平滑的决策边界。也解释了为什么 DenseNet 在训练数据不足的情况下表现良好。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><h1 id="687a" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">4.<strong class="ak"> CIFAR &amp; SVHN 小规模数据集结果</strong></h1><h2 id="ca9c" class="no lg iq bd lh np nq dn ll nr ns dp lp jy nt nu lt kc nv nw lx kg nx ny mb nz bi translated">4.1.CIFAR-10</h2><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ok"><img src="../Images/ed6d6c58871ff328558e53956ca249af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DVC7dxFxs7ozyG6vJTOTsQ.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">CIFAR-10 Results</strong></figcaption></figure><p id="f385" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kw" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e"> <em class="kx">预激活 ResNet </em> </a>用于详细比较。</p><p id="9448" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用数据扩充(C10+)，测试误差:</p><ul class=""><li id="21e4" class="md me iq jp b jq jr ju jv jy ol kc om kg on kk oo ml mm mn bi translated">小型 ResNet-110: 6.41%</li><li id="e70b" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk oo ml mm mn bi translated">大尺寸 ResNet-1001(10.2 米参数):4.62%</li><li id="993e" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk oo ml mm mn bi translated">最先进的(SOTA) 4.2%</li><li id="e913" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk oo ml mm mn bi translated">小型 DenseNet-BC ( <em class="kx"> L </em> =100，<em class="kx"> k </em> =12)(仅 0.8M 参数):4.5%</li><li id="dbd8" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk oo ml mm mn bi translated">大尺寸 DenseNet ( <em class="kx"> L </em> =250，<em class="kx"> k </em> =24): 3.6%</li></ul><p id="32c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">无数据扩充(C10)，测试误差:</p><ul class=""><li id="b738" class="md me iq jp b jq jr ju jv jy ol kc om kg on kk oo ml mm mn bi translated">小型 ResNet-110: 11.26%</li><li id="61a2" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk oo ml mm mn bi translated">大尺寸 ResNet-1001(10.2 米参数):10.56%</li><li id="2a8b" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk oo ml mm mn bi translated">最先进的(SOTA) 7.3%</li><li id="6064" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk oo ml mm mn bi translated">小型 DenseNet-BC ( <em class="kx"> L </em> =100，<em class="kx"> k </em> =12)(仅 0.8M 参数):5.9%</li><li id="e7d1" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk oo ml mm mn bi translated">大尺寸 DenseNet ( <em class="kx"> L </em> =250，<em class="kx"> k </em> =24): 4.2%</li></ul><p id="e496" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">严重的过拟合出现在<a class="ae kw" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e"> <em class="kx">预激活 ResNet </em> </a> <em class="kx"> </em>中，而 DenseNet 在训练数据不足时表现良好，因为 DenseNet 使用所有复杂度级别的特征。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi op"><img src="../Images/7b5a68069b6a3e6f4c3265b039d221e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f-wGcH50Bdx864n_biyjqg.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">C10+: Different DenseNet Variants (Left), DenseNet vs ResNet (Middle), Training and Testing Curves of DenseNet and ResNet (Right)</strong></figcaption></figure><p id="e289" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">左</strong> : DenseNet-BC 获得最佳结果。</p><p id="8880" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">中间</strong> : <a class="ae kw" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e"> <em class="kx">预激活 ResNet </em> </a>已经比<a class="ae kw" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener"><em class="kx">Alex net</em></a><em class="kx"/>和<a class="ae kw" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> <em class="kx"> VGGNet </em> </a>得到的参数少，DenseNet-BC ( <em class="kx"> k </em> =12)在相同测试误差下比<a class="ae kw" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e"> <em class="kx">预激活 ResNet </em> </a>得到的参数少 3 倍。</p><p id="02b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">右</strong>:参数为 0.8 的 DenseNet-BC-100 与参数为 10.2M 的<a class="ae kw" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e"> <em class="kx">预激活 ResNet-1001 </em> </a>测试误差相近。</p><h2 id="cb6c" class="no lg iq bd lh np nq dn ll nr ns dp lp jy nt nu lt kc nv nw lx kg nx ny mb nz bi translated">4.2.西发尔-100</h2><p id="5ec8" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy oc ka kb kc od ke kf kg oe ki kj kk ij bi translated">CIFAR-100 的类似趋势如下:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi oq"><img src="../Images/3dab759709db8525229c1f911cc28f90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HkFC5vvy9Q9-kpMaZRcO5w.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">CIFAR-100 Results</strong></figcaption></figure><h2 id="c9b0" class="no lg iq bd lh np nq dn ll nr ns dp lp jy nt nu lt kc nv nw lx kg nx ny mb nz bi translated">4.3.详细结果</h2><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi or"><img src="../Images/e68d4827fb05138416bd1fb6cceb6376.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DyhtAHXX-kvuMtf421aSZg.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">Detailed Results, + means data augmentation</strong></figcaption></figure><p id="d6a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">SVHN 是街景门牌号码数据集。蓝色意味着最好的结果。DenseNet-BC 不能获得比基本 DenseNet 更好的结果，作者认为 SVHN 是一个相对容易的任务，并且非常深的模型可能会过度适应训练集。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><h1 id="1f74" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">5.ImageNet <strong class="ak">大规模数据集</strong>结果</h1><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi os"><img src="../Images/a2a1dc6bd0e9f207248ab67376b3df33.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*HkVjuZm9vGh6GZmUi_o7AQ.png"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">Different DenseNet Top-1 and Top-5 Error Rates with Single-Crop (10-Crop) Results</strong></figcaption></figure><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ot"><img src="../Images/fffc01bac857e0e329faefc2b2dda1e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k75kOqrISzAEgtS15WWSZA.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">ImageNet Validation Set Results Compared with Original ResNet</strong></figcaption></figure><p id="f3cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kw" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">在<a class="ae kw" href="https://github.com/facebook/fb.resnet.torch" rel="noopener ugc nofollow" target="_blank">中实现的原始 ResNet </a>该链接</a>用于详细比较。</p><p id="33a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">左</strong>:参数为 20M 的 DenseNet-201 与参数超过 40M 的 ResNet-101 产生相似的验证误差。</p><p id="6452" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">右</strong>:类似的计算次数趋势(GFLOPs)</p><p id="76fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">底:dense net-264(<em class="kx">k</em>= 48)</strong>得到了 20.27% Top-1 误差和 5.17% Top-5 误差的最好结果。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><h1 id="0cfc" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">6.特征重用的进一步分析</h1><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ou"><img src="../Images/d32725e8a80edaff994d3390ed942772.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dY47b5vX8S0MQcJbvIcFnQ.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><strong class="bd nj">Heat map on the average absolute weights of how Target layer (<em class="ov">l</em>) reuses the source layer (s)</strong></figcaption></figure><ul class=""><li id="37bb" class="md me iq jp b jq jr ju jv jy ol kc om kg on kk oo ml mm mn bi translated">由非常早期的层提取的特征被遍及同一密集块的更深的层直接使用。</li><li id="48ef" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk oo ml mm mn bi translated">过渡层的权重也会将其权重分布到所有前面的层。</li><li id="5d23" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk oo ml mm mn bi translated">第二和第三密集块内的层一致地将最小权重分配给过渡层的输出。(第一排)</li><li id="a8fb" class="md me iq jp b jq mo ju mp jy mq kc mr kg ms kk oo ml mm mn bi translated">在最终分类层，权重似乎是对最终特征地图的集中。一些更高级的功能在网络后期产生。</li></ul></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="562f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作者还发表了多尺度 DenseNet。希望我以后也能报道它。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><h1 id="9400" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">参考</h1><p id="ded4" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy oc ka kb kc od ke kf kg oe ki kj kk ij bi translated">【2017 CVPR】【dense net】<br/><a class="ae kw" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank">密集连接的卷积网络</a></p><h1 id="8b42" class="lf lg iq bd lh li ow lk ll lm ox lo lp lq oy ls lt lu oz lw lx ly pa ma mb mc bi translated">我的相关评论</h1><p id="5c2d" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy oc ka kb kc od ke kf kg oe ki kj kk ij bi translated">[<a class="ae kw" href="https://medium.com/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17" rel="noopener">LeNet</a>][<a class="ae kw" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener">AlexNet</a>][<a class="ae kw" href="https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103" rel="noopener">ZFNet</a>][<a class="ae kw" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener">VGGNet</a>][<a class="ae kw" href="https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679" rel="noopener">SPPNet</a>][<a class="ae kw" href="https://medium.com/coinmonks/review-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617" rel="noopener">PReLU-Net</a>][<a class="ae kw" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener">Google Net/Inception-v1</a>][<a class="ae kw" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">BN-Inception/Inception-v2</a>][<a class="ae kw" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener">Inception-v3</a>][<a class="ae kw" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-v4</a></p></div></div>    
</body>
</html>