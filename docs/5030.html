<html>
<head>
<title>Reinforcement Learning: An Introduction to the Concepts, Applications and Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习:概念、应用和代码介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-an-introduction-to-the-concepts-applications-and-code-ced6fbfd882d?source=collection_archive---------5-----------------------#2018-09-23">https://towardsdatascience.com/reinforcement-learning-an-introduction-to-the-concepts-applications-and-code-ced6fbfd882d?source=collection_archive---------5-----------------------#2018-09-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="51b0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">第 1 部分:强化学习的介绍，解释常见的术语、概念和应用。</h2></div><p id="b4a0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这一系列强化学习的博客文章中，我将尝试对理解强化学习及其应用所需的概念进行简单的解释。在这第一篇文章中，我强调了强化学习中的一些主要概念和术语。这些概念将在未来的博客文章中进一步解释，并结合实际问题中的应用和实现。</p><p id="692c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">零件:1<a class="ae lb" rel="noopener" target="_blank" href="/getting-started-with-markov-decision-processes-reinforcement-learning-ada7b4572ffb">2</a>3<a class="ae lb" href="https://medium.com/@taggatle/planning-by-dynamic-programming-reinforcement-learning-ed4924bbaa4c" rel="noopener">3</a>4<a class="ae lb" rel="noopener" target="_blank" href="/model-free-prediction-reinforcement-learning-507297e8e2ad">4</a>…</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/e8086e6dc8322a4d74a2b31a4591d3f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*j7vrfZuwdJfcNJ34"/></div></div></figure><h1 id="b69d" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated"><em class="mg">强化学习</em></h1><p id="b4ca" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">强化学习(RL)可以被视为一种介于<em class="mm">监督</em>和<em class="mm">非监督</em>学习之间的方法。它不是严格监督的，因为它不仅仅依赖于一组标记的训练数据，但也不是无监督的学习，因为我们有一个回报，我们希望我们的代理最大化。代理需要找到在不同情况下采取的“正确”行动，以实现其总体目标。</p><blockquote class="mn"><p id="45c5" class="mo mp iq bd mq mr ms mt mu mv mw la dk translated">强化学习是决策的科学。</p></blockquote><p id="a20f" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">强化学习不涉及主管，只有一个<strong class="kh ir"> <em class="mm">奖励</em> </strong> <em class="mm">信号</em>用于代理确定他们做得好还是不好。<strong class="kh ir"><em class="mm"/></strong>是 RL 中的关键组成部分，其中流程是<strong class="kh ir"><em class="mm"/></strong><strong class="kh ir"><em class="mm"/></strong>。代理做出的每个<strong class="kh ir"> <em class="mm">动作</em> </strong>都会影响它接收的下一个数据。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nc nd l"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Reinforcement Learning applied to Atari games by DeepMind</figcaption></figure><h2 id="f2c0" class="ni lp iq bd lq nj nk dn lu nl nm dp ly ko nn no ma ks np nq mc kw nr ns me nt bi translated">强化学习问题是什么？</h2><p id="c05b" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">到目前为止，我们已经说过代理需要找到“正确的”动作。正确的行动取决于<strong class="kh ir">奖励。</strong></p><blockquote class="nu nv nw"><p id="77aa" class="kf kg mm kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">奖励:</em> </strong> <em class="iq">奖励</em> Rₜ <em class="iq">是一个标量反馈信号，它指示代理在步骤时间</em> t <em class="iq">做得有多好。</em></p></blockquote><p id="5c09" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在强化学习中，我们需要定义我们的问题，以便它可以被应用来满足我们的<strong class="kh ir">奖励假设</strong>。一个例子是下一盘棋，代理人赢了一局得到正奖励，输了一局得到负奖励。</p><blockquote class="nu nv nw"><p id="82d7" class="kf kg mm kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">报酬假设</em> </strong> : <em class="iq">所有的目标都可以用期望累积报酬的最大化来描述。</em></p></blockquote><p id="c27c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于我们的过程涉及到<em class="mm">系列决策</em>任务，我们早期的行动可能会对我们的总体<strong class="kh ir"> <em class="mm">目标</em> </strong>产生长期影响。有时候牺牲<em class="mm">即时奖励</em>(时间步<em class="mm"> Rₜ </em>奖励)来获得更多<em class="mm">长期奖励可能更好。</em>一个应用于国际象棋的例子是牺牲一个卒在稍后阶段夺取一辆车。</p><blockquote class="nu nv nw"><p id="c624" class="kf kg mm kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated"><strong class="kh ir">目标</strong> <em class="iq">:目标是选择行动以最大化未来总报酬。</em></p></blockquote></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><h2 id="90cc" class="ni lp iq bd lq nj nk dn lu nl nm dp ly ko nn no ma ks np nq mc kw nr ns me nt bi translated">设置强化学习问题</h2><p id="ddfa" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">在强化学习中，<strong class="kh ir">代理</strong>决定在每个时间步<em class="mm"> Aₜ </em>采取哪些<em class="mm">动作</em>。代理基于其接收的标量<em class="mm">奖励 Rₜ </em>和<em class="mm">观察到的环境</em> <em class="mm"> Oₜ </em>做出这些决定。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/25fad91deccf449a2587d4fef262ab5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*sElhdJmOibT5XnxTD-PJfA.jpeg"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Reinforcement learning process diagram</figcaption></figure><p id="5823" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">环境</strong>接收代理的动作并发出新的观察<em class="mm"> Oₜ </em>和标量奖励<em class="mm"> Rₜ </em>。接下来发生的环境取决于<strong class="kh ir"> <em class="mm">历史</em> </strong>。</p><blockquote class="nu nv nw"><p id="42db" class="kf kg mm kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">历史</em> </strong> <em class="iq">:历史</em> Hₜ <em class="iq">是到时间 t 为止的一系列观察、行动和奖励</em></p></blockquote><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/743b8b3c67300182dda78f425a95f618.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*74WWa0cRo8eZSwuyZmhjJg.png"/></div></figure><blockquote class="nu nv nw"><p id="701b" class="kf kg mm kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">状态:</em> </strong> <em class="iq">状态是用来决定接下来发生什么的信息。</em></p></blockquote><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/36c83ff2bf9eb92e17039844319768d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/format:webp/1*gaZcxgbDcN5kBoGeVuAG9g.png"/></div></figure><p id="4ad3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mm">历史</em>和<em class="mm">状态</em>的主要区别在于状态是历史的函数。这些状态可以分为三种主要类型:</p><ul class=""><li id="9f7e" class="ok ol iq kh b ki kj kl km ko om ks on kw oo la op oq or os bi translated"><strong class="kh ir"> <em class="mm">环境状态</em></strong><em class="mm">(sₜᵉ</em>)<em class="mm">—</em>环境的私有表示，可能对代理不可见。它用于选择下一个观察值。</li><li id="1d31" class="ok ol iq kh b ki ot kl ou ko ov ks ow kw ox la op oq or os bi translated"><strong class="kh ir"> <em class="mm">代理状态</em></strong><em class="mm">(sₜᵃ</em>)<em class="mm">——</em>代理的内部表示，被代理用来挑选下一个动作。</li><li id="2451" class="ok ol iq kh b ki ot kl ou ko ov ks ow kw ox la op oq or os bi translated"><strong class="kh ir"> <em class="mm">信息状态</em> </strong> <em class="mm"> / </em> <strong class="kh ir"> <em class="mm">马氏状态</em></strong><em class="mm">(sₜ</em>)<em class="mm">—</em>包含了来自历史的有用信息。因此，给定这种状态，将有足够的信息来模拟未来，并且可以丢弃历史。</li></ul><blockquote class="nu nv nw"><p id="62e0" class="kf kg mm kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated">马尔可夫状态:一个状态 Sₜ是马尔可夫的当且仅当</p></blockquote><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/7a2fbf6c99008c8c5292ede41647aea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*djdmkTcQvIo7xCrGh_O4FQ.png"/></div></figure><blockquote class="mn"><p id="7d75" class="mo mp iq bd mq mr oz pa pb pc pd la dk translated">我们相信接下来会发生什么取决于代理的状态表示。</p></blockquote><p id="000f" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">环境又分为<em class="mm">完全可观测环境</em>和<em class="mm">部分可观测环境</em>。</p><ul class=""><li id="ac01" class="ok ol iq kh b ki kj kl km ko om ks on kw oo la op oq or os bi translated"><strong class="kh ir"> <em class="mm">完全可观测环境</em> </strong> <em class="mm">(马尔可夫决策过程):</em>智能体直接观测环境状态。<em class="mm"> Oₜ=Sₜᵃ=Sₜᵉ </em></li><li id="5c63" class="ok ol iq kh b ki ot kl ou ko ov ks ow kw ox la op oq or os bi translated"><strong class="kh ir"> <em class="mm">部分可观测环境</em> </strong> <em class="mm">(部分可观测马尔可夫决策过程):</em>智能体间接观测环境。<em class="mm"> Sₜᵃ≠Sₜᵉ </em></li></ul></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><h2 id="907b" class="ni lp iq bd lq nj nk dn lu nl nm dp ly ko nn no ma ks np nq mc kw nr ns me nt bi translated">强化学习代理</h2><p id="d798" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">到目前为止，我们已经定义了如何设置 RL 问题，但没有定义 RL 代理如何学习或代理由什么组成。RL 代理可以具有三个主要组件中的一个或多个:</p><ul class=""><li id="a76d" class="ok ol iq kh b ki kj kl km ko om ks on kw oo la op oq or os bi translated"><strong class="kh ir"> <em class="mm">策略:</em> </strong> Agent 的行为函数，是从状态到动作的映射。它可以是<em class="mm">确定性政策</em>或<em class="mm">随机政策。</em></li></ul><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/d44a6fe5047b2fccb48d1f285b8d99b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:220/format:webp/1*Wd1P34L_jOrDXLAgemS5nA.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Deterministic policy function</figcaption></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/30ef858eeaad5d0dcedc44913e090480.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*nWsom6_lz0JaJy8PdPVb3A.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Stochastic policy function</figcaption></figure><ul class=""><li id="5bc1" class="ok ol iq kh b ki kj kl km ko om ks on kw oo la op oq or os bi translated"><strong class="kh ir"> <em class="mm">价值函数:</em> </strong>代表每个状态和/或动作有多好。这是对未来回报的预测。</li></ul><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/16c3148737f59b7eb2d828fd5bbe580e.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*ZtZLefYl5Xqkm2ysfCoMEA.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Example value function</figcaption></figure><ul class=""><li id="9b4c" class="ok ol iq kh b ki kj kl km ko om ks on kw oo la op oq or os bi translated"><strong class="kh ir"> <em class="mm">模型:</em> </strong>智能体对环境的表征。它预测环境下一步会做什么。预测的是下一个状态和下一个直接奖励。</li></ul><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/7d6d2b1c6626d83908134ef7f0934ae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*N5J_h9bQsh6zPoItEVLdDg.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Example equation to predict of the next state</figcaption></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/73f98fc830839be942be56bb47e4ac9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*BybJBMaA4ZJVtWyhiq87QQ.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Example equation to predict the next immediate reward</figcaption></figure><p id="9a16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">RL 代理可分为以下几类:</p><ul class=""><li id="d04d" class="ok ol iq kh b ki kj kl km ko om ks on kw oo la op oq or os bi translated"><strong class="kh ir">基于值:</strong>没有策略，基于状态值贪婪地选择动作。</li><li id="ad9a" class="ok ol iq kh b ki ot kl ou ko ov ks ow kw ox la op oq or os bi translated"><strong class="kh ir">基于策略:</strong>没有值函数，使用策略函数选择动作。</li><li id="dbab" class="ok ol iq kh b ki ot kl ou ko ov ks ow kw ox la op oq or os bi translated"><strong class="kh ir">演员评论家:</strong>同时使用价值和政策功能。</li><li id="0bb5" class="ok ol iq kh b ki ot kl ou ko ov ks ow kw ox la op oq or os bi translated"><strong class="kh ir">无模型:</strong>使用策略和/或值函数，但没有<em class="mm">无</em>模型。</li><li id="4dfe" class="ok ol iq kh b ki ot kl ou ko ov ks ow kw ox la op oq or os bi translated"><strong class="kh ir">基于模型:</strong>使用策略和/或值函数，并有一个模型。</li></ul><p id="4a1d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些代理类别中的每一个都将在以后的博客中详细描述。</p><h2 id="372a" class="ni lp iq bd lq nj nk dn lu nl nm dp ly ko nn no ma ks np nq mc kw nr ns me nt bi translated"><strong class="ak"> <em class="mg">问题内部强化学习</em> </strong></h2><p id="b929" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">在顺序决策中有两个基本问题:</p><ul class=""><li id="ca2c" class="ok ol iq kh b ki kj kl km ko om ks on kw oo la op oq or os bi translated"><strong class="kh ir"> <em class="mm">学习:</em> </strong>环境最初是<em class="mm">未知的</em>并且代理需要与环境交互以改进其策略</li><li id="bf47" class="ok ol iq kh b ki ot kl ou ko ov ks ow kw ox la op oq or os bi translated"><strong class="kh ir"> <em class="mm">规划:</em> </strong>如果环境是<em class="mm">已知的</em>代理用它的模型执行计算，然后改进它的策略。</li></ul><p id="72d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于强化学习就像一种试错学习方法，代理需要从环境中的经验中学习，以便在不损失太多回报的情况下做出决策。它需要考虑<em class="mm">开发</em>和<em class="mm">探索</em>，在探索新信息和利用已知信息之间取得平衡，以实现回报最大化。</p><h1 id="57d5" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">参考</h1><ul class=""><li id="7a30" class="ok ol iq kh b ki mh kl mi ko pj ks pk kw pl la op oq or os bi translated"><a class="ae lb" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf" rel="noopener ugc nofollow" target="_blank">关于 RL 的 UCL 课程——第一讲</a></li><li id="c033" class="ok ol iq kh b ki ot kl ou ko ov ks ow kw ox la op oq or os bi translated">《强化学习导论》，萨顿和巴尔托，1998 年</li></ul></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><p id="f3dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你喜欢这篇文章，并想看到更多，不要忘记关注和/或留下掌声。</p></div></div>    
</body>
</html>