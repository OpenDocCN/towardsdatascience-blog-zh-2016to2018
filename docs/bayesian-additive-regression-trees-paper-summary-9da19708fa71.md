# “贝叶斯可加回归树”论文摘要

> 原文：<https://towardsdatascience.com/bayesian-additive-regression-trees-paper-summary-9da19708fa71?source=collection_archive---------0----------------------->

*本文原载于*【blog.zakjost.com】

# *一.背景*

*[本文](http://www-stat.wharton.upenn.edu/~edgeorge/Research_papers/BART%20June%2008.pdf)开发了一种贝叶斯方法来集成树。对于一篇学术论文来说，它非常具有可读性，如果你觉得这个主题有趣，我建议你花时间去读一读。*

*贝叶斯加法回归树(BART)类似于梯度推进树(GBT)方法，因为它们对连续弱学习者的贡献进行求和。这与随机森林相反，随机森林平均许多独立的估计。但是贝叶斯方法不是像 GBT 那样将每个顺序树乘以一个小常数(学习速率)，而是使用一个先验。*

*通过使用先验和似然来获得预测的后验分布，我们得到了比经典回归方法的点估计更丰富的信息。此外，贝叶斯框架有一个内置的复杂性惩罚，这意味着我们不再需要对正则化、最大树深度和我们通常通过交叉验证调整的其他选项进行经验选择。*

*在评估的 42 个不同的数据集上，该方法的性能也优于所有其他被比较的方法，包括 GBM 和 Random Forests。*

# *二。有什么新消息*

*这篇论文的新颖之处实际上是之前三部作品的结合:在[贝叶斯树模型论文](http://www-stat.wharton.upenn.edu/~edgeorge/Research_papers/treed-models.pdf)中的单个树的贝叶斯框架；梯度推进树的想法；以及使用 [*贝叶斯回填*](http://projecteuclid.org/download/pdf_1/euclid.ss/1009212815) 对一般可加模型的后验分布进行 MCMC 采样。如果你能做出一个贝叶斯树，并且你知道如何从任何一个基本学习者的模型中取样，那么你就有了 BART。*

# *三。方法*

*以一种可以连贯地定义先验参数分布的方式构建树问题的任务并不简单。这项工作大部分是在先前链接的树形模型论文中完成的。本质上，整个先验被分成三个子先验:一个用于树结构(深度、分割标准)，另一个用于以树结构为条件的终端节点中的值，最后一个用于剩余噪声的标准偏差。提出一些聪明但直截了当的论点，以得出合理的默认建议。例如，有人认为函数的均值很可能在训练数据的 y_min 和 y_max 之间，所以它被设计成使得大部分先验质量都在这个区域。*

*有趣的是，在树的数量上的选择， *m* ，没有给出先验，这是由于计算的考虑。实际情况是，这个参数对结果的鲁棒性令人难以置信，所以他建议将其设置为 200，然后继续。事实上，结果似乎对合理的先验选择非常稳健，他主要推荐使用指定的默认值。*

*第二个重要部分是从模型的后部取样。像许多问题一样，这是通过 Metropolis-Hastings 算法来完成的，在该算法中，您从一个分布中生成一个样本，然后根据它的表现来保留/拒绝它。在这种情况下，它主要归结为:在加法序列中选择一棵树，通过在一些规则(修剪、生长等)中随机选择来变形它，从这个新树中，从终端节点值分布中采样，然后根据它们的后验概率比选择是保留这个新树还是原始树。通过这种方式，树被不断地改变以平衡它们的复杂性和解释数据的能力。选择先验自然有利于更简单的树，所以只有在有必要解释数据时才选择更深的先验。*

# *四。结果*

*该框架应用于 42 个不同的真实数据集，并与其他流行的方法进行比较:Lasso/L1 正则化的线性回归、梯度推进树、随机森林和具有一个隐藏层的神经网络。这些模型通过各种参数选择进行了调整。笑点是:巴特赢了。如果对超参数调整执行交叉验证，您将获得最佳性能，这类似于您对所有其他模型调整参数的方式，但您可以避免所有这些，只需使用 BART 和默认值即可获得极具竞争力的结果。*

*此外，结果表明，高度稳健的变化选择的先验。贝叶斯理论的辉煌之处在于，它表明，即使当预测器中加入了许多无用的随机变量时(当我们在不清楚哪些变量最重要的情况下建立模型时，这种情况经常发生)，与其他方法相比，它的表现也非常好。*

*作者还提出了一种变量选择技术，该技术涉及人工限制树的数量，然后在生成的树的分裂规则中计算变量的流行度。虽然这可能是一个很好的启发，但变量选择的问题是一个复杂的问题，它没有经过任何严格的探索。*

# *动词 （verb 的缩写）讨论*

*这是一篇超级酷的论文，给出了令人印象深刻的结果。虽然我很难理解贝叶斯回填算法的细节以及吉布斯采样实际上是如何实现的，但基本原理似乎与其他 Metropolis-Hastings 方法基本一致。*

*最终，这种方法提供了一组“开箱即用”的默认值，这些默认值在统计上非常稳健，具有同类最佳的性能，并且能够适应超参数选择的变化。我认为这很好地展示了贝叶斯概率框架如何避免传统的、特别的方法中的许多问题，这些方法主要依赖于最大似然法。*

*![](img/6d486890ffb139bf51efa08c42f612ff.png)*

# *不及物动词你试试*

*最令人失望的事实是，我找不到这个算法的 Python 实现。作者创建了一个 R 包( [BayesTrees](https://cran.r-project.org/web/packages/BayesTree/BayesTree.pdf) )，它有一些明显的问题——主要是缺少“预测”功能——另一个更广泛使用的实现叫做 [bartMachine](https://cran.r-project.org/web/packages/bartMachine/vignettes/bartMachine.pdf) 被创建。*

*如果你有实现这种技术的经验或者知道一个 Python 库，请在评论中留下链接！*