<html>
<head>
<title>TPOT Automated Machine Learning in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的 TPOT 自动机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tpot-automated-machine-learning-in-python-4c063b3e5de9?source=collection_archive---------4-----------------------#2018-08-22">https://towardsdatascience.com/tpot-automated-machine-learning-in-python-4c063b3e5de9?source=collection_archive---------4-----------------------#2018-08-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/94aceb0109a4cc629f49913f2d7a33d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*dCD9QwVjhVnKKz6U.jpg"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">TPOT graphic from the <a class="ae jy" href="http://epistasislab.github.io/tpot/" rel="noopener ugc nofollow" target="_blank">docs</a></figcaption></figure><p id="28e9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这篇文章中，我分享了我和 Python 中的自动机器学习(autoML)工具<a class="ae jy" href="http://epistasislab.github.io/tpot/" rel="noopener ugc nofollow" target="_blank"> TPOT </a>的一些探索。目标是看看 TPOT 能做什么，以及它是否值得成为你机器学习工作流程的一部分。</p><p id="ea69" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">自动化机器学习不会取代数据科学家，(至少现在不会)，但它可能会帮助你更快地找到好的模型。TPOT 自称是你的数据科学助手。</p><blockquote class="kx ky kz"><p id="cb7a" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated">TPOT 旨在成为一个助手，通过探索你可能从未考虑过的管道配置，给你提供如何解决特定机器学习问题的想法，然后将微调留给更受约束的参数调整技术，如网格搜索。</p></blockquote><p id="105d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">所以 TPOT 帮助你找到好的算法。请注意，它不是为自动化深度学习而设计的——类似 AutoKeras 的东西可能会有所帮助。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi le"><img src="../Images/cc958da9f9f183593e6c024d3349782d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iYQTbI4WVGUF1_F1.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk"><a class="ae jy" href="http://epistasislab.github.io/tpot/" rel="noopener ugc nofollow" target="_blank"><strong class="bd ln">An example machine learning pipeline (source: TPOT docs)</strong></a></figcaption></figure><p id="f60d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">TPOT 建立在 scikit learn 库的基础上，并严格遵循 scikit learn API。它可以用于回归和分类任务，并为医学研究提供特殊的实现。</p><p id="99d6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">TPOT 是开源的，有很好的文档记录，正在积极开发中。它的开发是由宾夕法尼亚大学的研究人员带头进行的。TPOT 似乎是最受欢迎的 autoML 库之一，截至 2018 年 8 月，有近<a class="ae jy" href="https://github.com/EpistasisLab/tpot/" rel="noopener ugc nofollow" target="_blank">4500 个 GitHub star</a>。</p><h1 id="47d3" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">TPOT 是如何工作的？</h1><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mm"><img src="../Images/435471a6488ee34b3a208561d11aa533.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vqs4500HEIa4eN4D.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk"><a class="ae jy" href="http://epistasislab.github.io/tpot/" rel="noopener ugc nofollow" target="_blank">An example TPOT Pipeline</a> (source: TPOT docs)</figcaption></figure><p id="7ff3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">TPOT 拥有其开发者所称的遗传搜索算法来寻找最佳参数和模型集合。它也可以被认为是一种自然选择或进化算法。TPOT 尝试了一条流水线，评估了它的性能，并随机改变了流水线的某些部分，以寻找性能更好的算法。</p><blockquote class="kx ky kz"><p id="72b5" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated">AutoML 算法不像在数据集上拟合一个模型那么简单；他们正在考虑多种机器学习算法(随机森林、线性模型、支持向量机等。)在具有多个预处理步骤(缺失值插补、缩放、PCA、特征选择等)的流水线中。)，所有模型和预处理步骤的超参数，以及在流水线内集成或堆叠算法的多种方式。<em class="iq">(来源:</em><a class="ae jy" href="http://epistasislab.github.io/tpot/using/" rel="noopener ugc nofollow" target="_blank"><em class="iq">【TPOT 文档】</em> </a> <em class="iq"> ) </em></p></blockquote><p id="67d4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">TPOT 的这种能力来自于自动高效地评估各种可能的管道。手动完成这项工作既麻烦又慢。</p><h1 id="eb6d" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">运行 TPOT</h1><p id="717e" class="pw-post-body-paragraph jz ka iq kb b kc mn ke kf kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ij bi translated">TPOT 分类器的实例化、拟合和评分类似于任何其他 sklearn 分类器。格式如下:</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="5b37" class="mx lp iq mt b gy my mz l na nb">tpot = TPOTClassifier()<br/>tpot.fit(X_train, y_train)<br/>tpot.score(X_test, y_test)</span></pre><p id="12c6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">TPOT 有自己的一键编码变体。注意，它可以自动将其添加到管道中，因为它将具有少于 10 个唯一值的特性视为分类特性。如果你想使用自己的编码策略，你可以对你的数据进行编码，然后<em class="la">再</em>把它输入 TPOT。</p><p id="d227" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">您可以为 tpot.score 选择评分标准(尽管 Jupyter 和多处理器内核的一个 bug 使您无法在 Jupyter 笔记本中拥有一个多处理器内核的自定义评分标准)。</p><p id="461a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">看来你不能改变 TPOT 在内部搜索最佳管道时使用的评分标准，只能改变 TPOT 选择最佳算法后在测试集上使用的评分标准。这是一些用户可能想要更多控制的领域。也许在未来的版本中会增加这个选项。</p><p id="e43d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">TPOT 使用 tpot.export()将有关最佳性能算法及其准确度分数的信息写入文件。您可以选择 TPOT 运行时希望看到的冗长程度，并让它在运行时将管道写入输出文件，以防由于某种原因提前终止(例如，您的 Kaggle 内核崩溃)。</p><h1 id="6e93" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">TPOT 要跑多长时间？</h1><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nc"><img src="../Images/323c09b60b648f70d4628bec103bce1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aZPBqIana8u1uf3RjxlUlA.jpeg"/></div></div></figure><p id="0cf3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">简而言之，这要视情况而定。</p><p id="1c90" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">TPOT 被设计为运行一段时间——几个小时甚至一天。尽管使用较小数据集的不太复杂的问题可以在几分钟内看到很好的结果。您可以调整 TPOT 的几个参数，以便更快地完成搜索，但代价是对最佳管道的搜索不够彻底。它并不是为了对预处理步骤、特性选择、算法和参数进行全面的搜索而设计的，但是如果您将它的参数设置得更详尽一些，它可能会更接近。</p><p id="77b5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">正如医生解释的那样:</p><blockquote class="kx ky kz"><p id="1843" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated">…TPOT 将需要一段时间才能在较大的数据集上运行，但了解原因很重要。使用默认的 TPOT 设置(100 代，100 人口规模)，TPOT 将在完成前评估 10，000 个管道配置。为了将这个数字放入上下文中，考虑机器学习算法的 10，000 个超参数组合的网格搜索，以及该网格搜索需要多长时间。也就是说，有 10，000 个模型配置要用 10 重交叉验证来评估，这意味着在一次网格搜索中，大约有 100，000 个模型适合并评估了训练数据。</p></blockquote><p id="431f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们将在下面看到的一些数据集只需要几分钟就可以找到得分较高的算法；其他人可能需要几天。</p><p id="9510" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">以下是默认的 TPOTClassifier 参数:</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="100d" class="mx lp iq mt b gy my mz l na nb">generations=100, <br/>population_size=100, <br/>offspring_size=None  # Jeff notes this gets set to population_size<br/>mutation_rate=0.9, <br/>crossover_rate=0.1, <br/>scoring="Accuracy",  # for Classification<br/>cv=5, <br/>subsample=1.0, <br/>n_jobs=1,<br/>max_time_mins=None, <br/>max_eval_time_mins=5,<br/>random_state=None, <br/>config_dict=None,<br/>warm_start=False, <br/>memory=None,<br/>periodic_checkpoint_folder=None, <br/>early_stop=None<br/>verbosity=0<br/>disable_update_check=False</span></pre><p id="dd56" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">每个参数的描述可以在<a class="ae jy" href="http://epistasislab.github.io/tpot/api/" rel="noopener ugc nofollow" target="_blank">文档</a>中找到。以下是几个决定 TPOT 将搜索的管道数量的关键因素:</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="8ecc" class="mx lp iq mt b gy my mz l na nb"><strong class="mt ir">generations:</strong> int, optional (default: 100)           <br/>Number of iterations to the run pipeline optimization process.            Generally, TPOT will work better when you give it more generations(and therefore time) to optimize the pipeline. </span><span id="65d9" class="mx lp iq mt b gy nd mz l na nb"><strong class="mt ir">TPOT will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total </strong>(emphasis mine).</span><span id="e290" class="mx lp iq mt b gy nd mz l na nb"><strong class="mt ir">population_size:</strong> int, optional (default: 100)            <br/>Number of individuals to retain in the GP population every generation. <br/>Generally, TPOT will work better when you give it more individuals (and therefore time) to optimize the pipeline. </span><span id="ca8d" class="mx lp iq mt b gy nd mz l na nb"><strong class="mt ir">offspring_size:</strong> int, optional (default: None)<br/>Number of offspring to produce in each GP generation.            <br/>By default, offspring_size = population_size.</span></pre><p id="2d40" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从 TPOT 开始，设置<em class="la"> verbosity=3 </em>和<em class="la">periodic _ check point _ folder = " any _ string _ you _ like "</em>是值得的，这样你就可以看到模型的发展和训练分数的提高。当管道元素的一些组合不兼容时，您会看到一些错误，但是不要担心。</p><p id="b1e8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果您正在多核上运行，并且没有使用自定义计分功能，请设置 n_jobs=-1 以使用所有可用的内核并加速 TPOT。</p><h2 id="294c" class="mx lp iq bd lq ne nf dn lu ng nh dp ly kk ni nj mc ko nk nl mg ks nm nn mk no bi translated">搜索空间</h2><p id="d957" class="pw-post-body-paragraph jz ka iq kb b kc mn ke kf kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ij bi translated">以下是 TPOT 从版本 0.9 开始选择的分类算法和参数:</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="f433" class="mx lp iq mt b gy my mz l na nb">‘sklearn.naive_bayes.<strong class="mt ir">BernoulliNB</strong>’: { ‘alpha’: [1e-3, 1e-2, 1e-1, 1., 10., 100.], ‘fit_prior’: [True, False] }, </span><span id="80a6" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.naive_bayes.<strong class="mt ir">MultinomialNB</strong>’: { ‘alpha’: [1e-3, 1e-2, 1e-1, 1., 10., 100.], ‘fit_prior’: [True, False] }, </span><span id="1d86" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.tree.<strong class="mt ir">DecisionTreeClassifier</strong>’: { ‘criterion’: [“gini”, “entropy”], ‘max_depth’: range(1, 11), ‘min_samples_split’: range(2, 21), ‘min_samples_leaf’: range(1, 21) }, </span><span id="81e4" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.ensemble.<strong class="mt ir">ExtraTreesClassifier</strong>’: { ‘n_estimators’: [100], ‘criterion’: [“gini”, “entropy”], ‘max_features’: np.arange(0.05, 1.01, 0.05), ‘min_samples_split’: range(2, 21), ‘min_samples_leaf’: range(1, 21), ‘bootstrap’: [True, False] },</span><span id="7e15" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.ensemble.<strong class="mt ir">RandomForestClassifier</strong>’: { ‘n_estimators’: [100], ‘criterion’: [“gini”, “entropy”], ‘max_features’: np.arange(0.05, 1.01, 0.05), ‘min_samples_split’: range(2, 21), ‘min_samples_leaf’: range(1, 21), ‘bootstrap’: [True, False] }, </span><span id="987d" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.ensemble.<strong class="mt ir">GradientBoostingClassifier</strong>’: { ‘n_estimators’: [100], ‘learning_rate’: [1e-3, 1e-2, 1e-1, 0.5, 1.], ‘max_depth’: range(1, 11), ‘min_samples_split’: range(2, 21), ‘min_samples_leaf’: range(1, 21), ‘subsample’: np.arange(0.05, 1.01, 0.05), ‘max_features’: np.arange(0.05, 1.01, 0.05) },</span><span id="fe30" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.neighbors.<strong class="mt ir">KNeighborsClassifier</strong>’: { ‘n_neighbors’: range(1, 101), ‘weights’: [“uniform”, “distance”], ‘p’: [1, 2] }, </span><span id="8b1c" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.svm.<strong class="mt ir">LinearSVC</strong>’: { ‘penalty’: [“l1”, “l2”], ‘loss’: [“hinge”, “squared_hinge”], ‘dual’: [True, False], ‘tol’: [1e-5, 1e-4, 1e-3, 1e-2, 1e-1], ‘C’: [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.] }, </span><span id="f11c" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.linear_model.<strong class="mt ir">LogisticRegression</strong>’: { ‘penalty’: [“l1”, “l2”], ‘C’: [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.], ‘dual’: [True, False] }, </span><span id="3a48" class="mx lp iq mt b gy nd mz l na nb">‘xgboost.<strong class="mt ir">XGBClassifier</strong>’: { ‘n_estimators’: [100], ‘max_depth’: range(1, 11), ‘learning_rate’: [1e-3, 1e-2, 1e-1, 0.5, 1.], ‘subsample’: np.arange(0.05, 1.01, 0.05), ‘min_child_weight’: range(1, 21), ‘nthread’: [1] }</span></pre><p id="e5b6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">并且 TPOT 可以堆叠分类器，包括多次相同的分类器。TPOT 的核心开发者之一在本期<a class="ae jy" href="https://github.com/EpistasisLab/tpot/issues/360" rel="noopener ugc nofollow" target="_blank">中解释了它是如何工作的</a>:</p><blockquote class="kx ky kz"><p id="66df" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated">流水线<code class="fe np nq nr mt b"><em class="iq">ExtraTreesClassifier(ExtraTreesClassifier(input_matrix, True, 'entropy', 0.10000000000000001, 13, 6), True, 'gini', 0.75, 17, 4)</em></code>执行以下操作:</p><p id="17e7" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated">使用提取树分类器拟合所有原始特征</p><p id="20b4" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated">从提取树分类器中获取预测，并使用这些预测创建新要素</p><p id="1dc1" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated">将原始特征加上新的“预测特征”传递给第二个提取树分类器，并将其预测用作管道的最终预测</p><p id="0288" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated">这个过程被称为堆叠分类器，这是机器学习中一种相当常见的策略。</p></blockquote><p id="cac6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这里是 TPOT 从 0.9 版本开始可以应用的 11 个预处理程序。</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="d8a6" class="mx lp iq mt b gy my mz l na nb">‘sklearn.preprocessing.<strong class="mt ir">Binarizer</strong>’: { ‘threshold’: np.arange(0.0, 1.01, 0.05) }, </span><span id="0959" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.decomposition.<strong class="mt ir">FastICA</strong>’: { ‘tol’: np.arange(0.0, 1.01, 0.05) }, </span><span id="652e" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.cluster.<strong class="mt ir">FeatureAgglomeration</strong>’: { ‘linkage’: [‘ward’, ‘complete’, ‘average’], ‘affinity’: [‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’, ‘cosine’] }, </span><span id="a716" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.preprocessing.<strong class="mt ir">MaxAbsScaler</strong>’: { }, </span><span id="8af9" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.preprocessing.<strong class="mt ir">MinMaxScaler</strong>’: { }, </span><span id="5224" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.preprocessing.<strong class="mt ir">Normalizer</strong>’: { ‘norm’: [‘l1’, ‘l2’, ‘max’] }, </span><span id="41ee" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.kernel_approximation.<strong class="mt ir">Nystroem</strong>’: { ‘kernel’: [‘rbf’, ‘cosine’, ‘chi2’, ‘laplacian’, ‘polynomial’, ‘poly’, ‘linear’, ‘additive_chi2’, ‘sigmoid’], ‘gamma’: np.arange(0.0, 1.01, 0.05), ‘n_components’: range(1, 11) }, </span><span id="5f67" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.decomposition.<strong class="mt ir">PCA</strong>’: { ‘svd_solver’: [‘randomized’], ‘iterated_power’: range(1, 11) }, ‘sklearn.preprocessing.PolynomialFeatures’: { ‘degree’: [2], ‘include_bias’: [False], ‘interaction_only’: [False] }, </span><span id="3bdc" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.kernel_approximation.<strong class="mt ir">RBFSampler</strong>’: { ‘gamma’: np.arange(0.0, 1.01, 0.05) }, ‘sklearn.preprocessing.RobustScaler’: { }, </span><span id="2b61" class="mx lp iq mt b gy nd mz l na nb">‘sklearn.preprocessing.<strong class="mt ir">StandardScaler</strong>’: { }, ‘tpot.builtins.ZeroCount’: { }, </span><span id="624b" class="mx lp iq mt b gy nd mz l na nb">‘<strong class="mt ir">tpot.builtins.OneHotEncoder</strong>’: { ‘minimum_fraction’: [0.05, 0.1, 0.15, 0.2, 0.25], ‘sparse’: [False] } (emphasis mine)</span></pre><p id="11a7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这是一个非常全面的 sklearn ml 算法列表，甚至包括一些您可能没有用于预处理的算法，包括 Nystroem 和 RBFSampler。最后列出的预处理算法就是之前提到的自定义 OneHotEncoder。请注意，该列表不包含神经网络算法。</p><p id="ad49" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">组合的数量几乎是无限的——您可以堆叠算法，包括相同算法的实例。管道中的步骤数量可能有一个内部上限，但可以说可能有太多的管道。</p><p id="97a4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果您运行 TPOT 两次，它可能不会导致相同的算法选择(我发现，即使设置了 random_state，也可能不会，如下所述)。正如<a class="ae jy" href="http://epistasislab.github.io/tpot/using/" rel="noopener ugc nofollow" target="_blank">文档</a>所解释的:</p><blockquote class="kx ky kz"><p id="22d3" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated">如果您正在处理相当复杂的数据集或短时间运行 TPOT，不同的 TPOT 运行可能会产生不同的管道建议。TPOT 的优化算法在本质上是随机的，这意味着它使用随机性(部分)来搜索可能的管道空间。当两个 TPOT 运行推荐不同的管道时，这意味着 TPOT 运行由于缺少时间而没有收敛<em class="iq">或</em>多个管道在数据集上执行或多或少相同的操作。</p></blockquote><p id="36f2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">少说多做。让我们用一些数据来检验一下 TPOT！</p><h1 id="cda4" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">数据集 1: MNIST 数字分类</h1><p id="b2c7" class="pw-post-body-paragraph jz ka iq kb b kc mn ke kf kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ij bi translated">首先我们来看一个分类任务 sklearn 数据集中包含的来自<a class="ae jy" href="http://scikit-learn.org/stable/datasets/index.html" rel="noopener ugc nofollow" target="_blank"> MNIST 的流行手写数字分类任务。MNIST 数据库包含 70，000 张 28x28 像素的手写阿拉伯数字图像，从 0 到 9。</a></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e80648eb2ea56a71eba35104a6892127.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*4PGiye98rEGY3yc3B-_xCg.png"/></div></figure><p id="5a8e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">TPOT 是 Kaggle Docker 镜像的标准配置，所以你只需要在使用 Kaggle 的时候导入它——你不需要安装它。</p><p id="f4bb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这是我的代码——可以在<a class="ae jy" href="https://www.kaggle.com/discdiver/tpot-with-mnist-digit-classification/" rel="noopener ugc nofollow" target="_blank">这个 Kaggle 内核</a>上获得，形式略有不同，可能有一些修改。</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="b4cc" class="mx lp iq mt b gy my mz l na nb"># import the usual stuff<br/>import numpy as np <br/>import pandas as pd <br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>import os</span><span id="f425" class="mx lp iq mt b gy nd mz l na nb"># import TPOT and sklearn stuff<br/>from tpot import TPOTClassifier<br/>from sklearn.datasets import load_digits<br/>from sklearn.model_selection import train_test_split<br/>import sklearn.metrics</span><span id="a73d" class="mx lp iq mt b gy nd mz l na nb"># create train and test sets<br/>digits = load_digits()<br/>X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, train_size=0.75, test_size=0.25, random_state=34)</span><span id="b957" class="mx lp iq mt b gy nd mz l na nb">tpot = TPOTClassifier(verbosity=3, <br/>                      scoring="balanced_accuracy", <br/>                      random_state=23, <br/>                      periodic_checkpoint_folder="tpot_mnst1.txt", <br/>                      n_jobs=-1, <br/>                      generations=10, <br/>                      population_size=100)</span><span id="2514" class="mx lp iq mt b gy nd mz l na nb"># run three iterations and time them</span><span id="cc3b" class="mx lp iq mt b gy nd mz l na nb">for x in range(3):<br/>    start_time = timeit.default_timer()<br/>    tpot.fit(X_train, y_train)<br/>    elapsed = timeit.default_timer() - start_time<br/>    times.append(elapsed)<br/>    winning_pipes.append(tpot.fitted_pipeline_)<br/>    scores.append(tpot.score(X_test, y_test))<br/>    tpot.export('tpot_mnist_pipeline.py')</span><span id="91c8" class="mx lp iq mt b gy nd mz l na nb">times = [time/60 for time in times]<br/>print('Times:', times)<br/>print('Scores:', scores)   <br/>print('Winning pipelines:', winning_pipes)</span></pre><p id="cbb1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，管道总数等于人口 _ 大小+世代 x 后代 _ 大小。</p><p id="3308" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">例如，如果设置 population_size=20，generations=5，那么 subject _ size = 20(因为默认情况下 subject _ size 等于 population_size。因为 20 + (5 * 20 ) = 120，所以总共有 120 条管道。</p><p id="cd6c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">您可以看到，运行这个数据集根本不需要太多代码——这包括一个计时和重复测试的循环。</p><p id="1dbd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">有 10 个可能的类别，没有理由选择一个结果而不是另一个，准确性——TPOT 分类的默认——是这项任务的一个很好的衡量标准。</p><p id="d630" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">下面是相关的代码部分。</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="d17f" class="mx lp iq mt b gy my mz l na nb">digits = load_digits()<br/>X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, train_size=0.75, test_size=0.25, random_state=34)</span><span id="efdf" class="mx lp iq mt b gy nd mz l na nb">tpot = TPOTClassifier(verbosity=3, <br/> scoring=”accuracy”, <br/> random_state=32, <br/> periodic_checkpoint_folder=”tpot_results.txt”, <br/> n_jobs=-1, <br/> generations=5, <br/> population_size=10,<br/> early_stop=5)</span></pre><p id="3e4d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">结果如下:</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="ffa2" class="mx lp iq mt b gy my mz l na nb">Times: [4.740584810283326, 3.497970838083226, 3.4362493358499098]<br/>Scores: [0.9733333333333334, 0.9644444444444444, 0.9666666666666667]</span><span id="e2bf" class="mx lp iq mt b gy nd mz l na nb">Winning pipelines: [</span><span id="c481" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('gradientboostingclassifier', GradientBoostingClassifier(criterion='friedman_mse', init=None,<br/>              learning_rate=0.1, loss='deviance', max_depth=7,<br/>              max_features=0.15000000000000002, max_leaf_nodes=None,<br/>              min_impurity_decrease=0.0, min_impurity_split=None,<br/>...auto', random_state=None,<br/>              subsample=0.9500000000000001, verbose=0, warm_start=False))]), </span><span id="0978" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('gradientboostingclassifier', GradientBoostingClassifier(criterion='friedman_mse', init=None,<br/>              learning_rate=0.5, loss='deviance', max_depth=2,<br/>              max_features=0.15000000000000002, max_leaf_...auto', random_state=None,<br/>              subsample=0.9500000000000001, verbose=0, warm_start=False))]), </span><span id="fa29" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('gradientboostingclassifier', GradientBoostingClassifier(criterion='friedman_mse', init=None,<br/>              learning_rate=0.5, loss='deviance', max_depth=2,<br/>              max_features=0.15000000000000002, max_leaf_...auto', random_state=None,<br/>              subsample=0.9500000000000001, verbose=0, warm_start=False))])]</span></pre><p id="31ad" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">请注意，只有 60 个管道——远远少于 TPOT 建议的——我们能够看到相当好的分数——在一个案例中，测试集的准确率超过 97%。</p><h2 id="f75c" class="mx lp iq bd lq ne nf dn lu ng nh dp ly kk ni nj mc ko nk nl mg ks nm nn mk no bi translated">再现性</h2><p id="ecd1" class="pw-post-body-paragraph jz ka iq kb b kc mn ke kf kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ij bi translated">TPOT 每次都用相同的 random_state 集合找到相同的获胜管道吗？不一定。像 RandomForrestClassifier()这样的算法都有自己的 random_state 参数，这些参数没有被设置。</p><p id="d125" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果你实例化一个分类器，然后像我们在上面代码中的<em class="la"> for </em>循环中所做的那样反复拟合，TPOT 并不总是找到相同的结果。我在 random_state 设置和 Kaggle 的 GPU 设置打开的情况下运行了三组非常小的 60 条管道。请注意，我们得到的管道略有不同，因此三个测试集的测试集得分也略有不同。</p><p id="41ca" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">下面是另一个例子，有少量的管道设置了随机状态，并使用了 Kaggle 的 CPU 设置。</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="7e29" class="mx lp iq mt b gy my mz l na nb">Times: [2.8874817832668973, 0.043678393283335025, 0.04388708711679404]<br/>Scores: [0.9622222222222222, 0.9622222222222222, 0.9622222222222222]<br/>Winning pipelines: [</span><span id="6d65" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('gradientboostingclassifier', GradientBoostingClassifier(criterion='friedman_mse', init=None,<br/>              learning_rate=0.5, loss='deviance', max_depth=2,<br/>              max_features=0.15000000000000002, max_leaf_nodes=None,<br/>              min_impurity_decrease=0.0, min_impurity_split=None,<br/>....9500000000000001, tol=0.0001,<br/>              validation_fraction=0.1, verbose=0, warm_start=False))]), </span><span id="f2fe" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('gradientboostingclassifier', GradientBoostingClassifier(criterion='friedman_mse', init=None,<br/>              learning_rate=0.5, loss='deviance', max_depth=2,<br/>              max_features=0.15000000000000002, max_leaf_nodes=None,<br/>              min_impurity_decrease=0.0, min_impurity_split=None,<br/>....9500000000000001, tol=0.0001,<br/>              validation_fraction=0.1, verbose=0, warm_start=False))]), </span><span id="3976" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('gradientboostingclassifier', GradientBoostingClassifier(criterion='friedman_mse', init=None,<br/>              learning_rate=0.5, loss='deviance', max_depth=2,<br/>              max_features=0.15000000000000002, max_leaf_nodes=None,<br/>              min_impurity_decrease=0.0, min_impurity_split=None,<br/>....9500000000000001, tol=0.0001,<br/>              validation_fraction=0.1, verbose=0, warm_start=False))])]</span></pre><p id="46a1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">三次都发现了相同的管道。</p><p id="13fc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">注意，在第一次迭代之后，运行时间要快得多。TPOT 似乎确实记得它什么时候见过一个算法，并且不会重新运行它，即使它是第二次拟合，并且你已经设置了 memory=False。如果设置 verbosity=3，当它找到这样一个以前评估过的管道时，您将看到以下内容:</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="d0ca" class="mx lp iq mt b gy my mz l na nb">Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.</span></pre><h2 id="b225" class="mx lp iq bd lq ne nf dn lu ng nh dp ly kk ni nj mc ko nk nl mg ks nm nn mk no bi translated">运行时间更长，精确度更高</h2><p id="712e" class="pw-post-body-paragraph jz ka iq kb b kc mn ke kf kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ij bi translated">如果制造大量管道，TPOT 怎么办？要真正看到 TPOT 在 MNIST 数字任务中的威力，你需要运行 500 多条管道。如果在 Kaggle 上运行，这至少需要一个小时。然后，您将看到更高的准确度分数，并可能看到更复杂的模型。</p><p id="0a32" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果你有大量的管道和一项重要的任务，你可能会看到一个机器学习算法的输出进入另一个算法的链式或堆叠式集合。</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="9503" class="mx lp iq mt b gy my mz l na nb">0.9950861171999883</span><span id="f738" class="mx lp iq mt b gy nd mz l na nb">knn = KNeighborsClassifier(<br/>        DecisionTreeClassifier(<br/>            OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False), <br/>            DecisionTreeClassifier__criterion=gini, <br/>            DecisionTreeClassifier__max_depth=5, <br/>            DecisionTreeClassifier__min_samples_leaf=20, <br/>            DecisionTreeClassifier__min_samples_split=17), <br/>         KNeighborsClassifier__n_neighbors=1, <br/>         KNeighborsClassifier__p=2, <br/>         KNeighborsClassifier__weights=distance)</span></pre><p id="379a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在运行了一个多小时并生成了 600 多条管道后，这是 0.995 的平均内部 CV 准确度分数。内核在完成之前崩溃了，所以我没有看到测试集的分数，也没有得到输出的模型，但是这对 TPOT 来说看起来很有希望。</p><p id="9fd0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">该算法使用决策树分类器，将 TPOT 定制的 OneHotEncoder 分类编码输入 KNeighborsClassifier。</p><p id="3bd3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这是一个类似的内部分数，不同的管道来自于近 800 个管道后的一个不同的 random_state。</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="0e51" class="mx lp iq mt b gy my mz l na nb">0.9903723557310828 </span><span id="6b57" class="mx lp iq mt b gy nd mz l na nb">KNeighborsClassifier(Normalizer(OneHotEncoder(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=15, RandomForestClassifier__n_estimators=100), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False), Normalizer__norm=max), KNeighborsClassifier__n_neighbors=4, KNeighborsClassifier__p=2, KNeighborsClassifier__weights=distance)</span></pre><p id="f1c9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">TPOT 发现了一个 KNN 管道，一个热编码，规范化和随机森林。花了两个半小时。前一个更快，得分更高，但有时这就是 TPOT 遗传搜索算法的随机性质。😉</p><h2 id="4862" class="mx lp iq bd lq ne nf dn lu ng nh dp ly kk ni nj mc ko nk nl mg ks nm nn mk no bi translated">MNIST 数字分类任务的要点</h2><ol class=""><li id="f8d4" class="nt nu iq kb b kc mn kg mo kk nv ko nw ks nx kw ny nz oa ob bi translated">如果你给它足够的时间，TPOT 可以很好地完成这个图像识别任务。</li><li id="b507" class="nt nu iq kb b kc oc kg od kk oe ko of ks og kw ny nz oa ob bi translated">TPOT 管道越多越好。</li><li id="e615" class="nt nu iq kb b kc oc kg od kk oe ko of ks og kw ny nz oa ob bi translated">如果你需要任务的再现性，TPOT 不是你想要的工具。</li></ol><h1 id="1379" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">数据集 2:蘑菇分类</h1><p id="f878" class="pw-post-body-paragraph jz ka iq kb b kc mn ke kf kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ij bi translated">对于第二个数据集，我选择了流行的<a class="ae jy" href="https://www.kaggle.com/mokosan/mushroom-classification/data" rel="noopener ugc nofollow" target="_blank">蘑菇分类任务。</a>目标是根据标签正确确定蘑菇是否有毒。这不是图像分类任务。它被设置为一个二元任务，这样所有潜在危险的蘑菇被归为一类，而安全食用蘑菇则归为另一类。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi oh"><img src="../Images/eb93bed5ba407c0d2834df387ad697a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IifgV9ZEX02IPaXZJ5y-zg.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Yummy or deadly? Ask TPOT.</figcaption></figure><p id="9f07" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我的代码可以在<a class="ae jy" href="https://www.kaggle.com/discdiver/tpot-mushroom-classification-task/" rel="noopener ugc nofollow" target="_blank">这个 Kaggle 内核</a>上获得。</p><p id="34c0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">TPOT 通常可以在这个数据集上快速拟合出一个完美的模型。它在两分钟之内就完成了。这比我在没有 TPOT 的情况下使用许多 scikit-learn 分类算法、各种名义数据编码和无参数调整测试该数据集时的性能和速度好得多。</p><p id="a558" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在使用相同的 TPOTClassifier 实例和相同的随机状态集的三次运行中，TPOT 发现了以下情况:</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="54ba" class="mx lp iq mt b gy my mz l na nb">Times: [1.854785452616731, 1.5694829618000463, 1.3383520993001488]<br/>Scores: [1.0, 1.0, 1.0]</span></pre><p id="d949" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">有趣的是，它每次都找到不同的最佳算法。它找到了 DecisionTreeClassifier，然后是 KNeighorsClassifier，然后是带有 BernoulliNB 的堆叠 RandomForestClassifier。</p><p id="9c0b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">让我们更深入地探讨一下可再现性。让我们用完全相同的设置再运行一次。</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="713e" class="mx lp iq mt b gy my mz l na nb">Times: [1.8664863013502326, 1.5520636909670429, 1.3386059726501116]<br/>Scores: [1.0, 1.0, 1.0]</span></pre><p id="ac95" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们看到同一组三个管道，非常相似的时间，测试集上相同的分数。</p><p id="89e2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在让我们试着将这个单元分成多个不同的单元，并在每个单元中实例化一个 TPOT 实例。代码如下:</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="bfd1" class="mx lp iq mt b gy my mz l na nb">X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25, random_state=25)</span><span id="c887" class="mx lp iq mt b gy nd mz l na nb">tpot = TPOTClassifier(<br/>  verbosity=3, <br/>  scoring=”accuracy”, <br/>  random_state=25, <br/>  periodic_checkpoint_folder=”tpot_mushroom_results.txt”, <br/>  n_jobs=-1, <br/>  generations=5, <br/>  population_size=10,<br/>  early_stop = 5<br/>)</span></pre><p id="18f1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">第二次运行的结果现在与第一次运行的结果相匹配，并且花费了几乎相同的时间(分数= 1.0，时间= 1.9 分钟，管道=决策树分类器)。更高再现性的关键是我们在每个单元中实例化 TPOT 分类器的一个新实例。</p><p id="5357" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">下面是 10 组 30 个流水线的时间结果，其中随机状态在训练测试分割上，TPOT 设置为 10。所有管道正确地对测试集上的所有蘑菇进行了分类。TPOT 在这项简单易学的任务上速度很快。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a248f2da0492ab4e5f0e33baf06e98e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*ox3f9mTLTPKaF67yJLTL4Q.png"/></div></figure><h1 id="d7a2" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">蘑菇任务的收获</h1><p id="8102" class="pw-post-body-paragraph jz ka iq kb b kc mn ke kf kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ij bi translated">对于这个基本的分类任务，TPOT 表现得又好又快。</p><p id="cc8a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">作为比较，<a class="ae jy" href="https://www.kaggle.com/mokosan/mushroom-classification/notebook" rel="noopener ugc nofollow" target="_blank">R 中蘑菇集上的这个 Kaggle 内核</a>非常好，探索了各种算法，非常接近完美的精度。但是，它并没有完全达到 100%,与我们实施 TPOT 相比，它确实花费了更多的准备和培训时间。</p><p id="8dfb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我强烈认为 TPOT 可以为将来这样的任务节省时间，至少作为第一步。</p><h1 id="5303" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">数据集 3:埃姆斯住宅预测</h1><p id="9792" class="pw-post-body-paragraph jz ka iq kb b kc mn ke kf kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ij bi translated">接下来，我们转向回归任务，看看 TPOT 的表现。我们将使用流行的<a class="ae jy" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" rel="noopener ugc nofollow" target="_blank">爱荷华州艾姆斯房价预测数据集</a>来预测房产销售价值。我的代码可以在<a class="ae jy" href="https://www.kaggle.com/discdiver/tpot-on-ames-housing-regression/" rel="noopener ugc nofollow" target="_blank">这个 Kaggle 内核</a>上获得。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi oj"><img src="../Images/53390d3938504400591d2e6e31bf372b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*US5XGt86xaf0qlEFhQMcpA.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">This house could be in Ames, Iowa, right?</figcaption></figure><p id="baaa" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于这个任务，我首先做了一些基本的缺失值插补。我用该列中最常见的值来填充缺失的数字列值，因为其中一些列包含序号数据。随着时间的推移，我会对列进行分类，并根据区间、序数或名义数据类型使用不同的插补策略。</p><p id="2d52" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在顺序编码之前，字符串列缺失值用“缺失”标签填充，因为并非所有列都有最常见的值。TPOT 的 one hot 编码算法将为每个特征增加一个维度，这将表明该数据缺少该特征的值。</p><p id="a6ed" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">TPOTRegressor 默认使用均方误差评分。</p><p id="f0a3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这里有一个只有 60 条管道的运行。</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="61f6" class="mx lp iq mt b gy my mz l na nb">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 33)</span><span id="1ad6" class="mx lp iq mt b gy nd mz l na nb"># instantiate tpot <br/>tpot = TPOTRegressor(verbosity=3,  <br/>                      random_state=25, <br/>                      n_jobs=-1, <br/>                      generations=5, <br/>                      population_size=10,<br/>                      early_stop = 5,<br/>                      memory = None)<br/>times = []<br/>scores = []<br/>winning_pipes = []</span><span id="7de2" class="mx lp iq mt b gy nd mz l na nb"># run 3 iterations<br/>for x in range(3):<br/>    start_time = timeit.default_timer()<br/>    tpot.fit(X_train, y_train)<br/>    elapsed = timeit.default_timer() - start_time<br/>    times.append(elapsed)<br/>    winning_pipes.append(tpot.fitted_pipeline_)<br/>    scores.append(tpot.score(X_test, y_test))<br/>    tpot.export('tpot_ames.py')</span><span id="429c" class="mx lp iq mt b gy nd mz l na nb"># output results<br/>times = [time/60 for time in times]<br/>print('Times:', times)<br/>print('Scores:', scores)   <br/>print('Winning pipelines:', winning_pipes)</span></pre><p id="5492" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这三次小试的结果。</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="8cfa" class="mx lp iq mt b gy my mz l na nb">Times: [3.8920086714831994, 1.4063017464330188, 1.2469199204002508]<br/>Scores: [-905092886.3009057, -922269561.2683483, -949881926.6436856]<br/>Winning pipelines: [</span><span id="9815" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('zerocount', ZeroCount()), ('xgbregressor', XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,<br/>       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,<br/>       max_depth=9, min_child_weight=18, missing=None, n_estimators=100,<br/>       n_jobs=1, nthread=1, objective='reg:linear', random_state=0,<br/>       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,<br/>       silent=True, subsample=0.5))]), </span><span id="f0ea" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('xgbregressor', XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,<br/>       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,<br/>       max_depth=9, min_child_weight=11, missing=None, n_estimators=100,<br/>       n_jobs=1, nthread=1, objective='reg:linear', random_state=0,<br/>       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,<br/>       silent=True, subsample=0.5))]), </span><span id="06ee" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('stackingestimator', StackingEstimator(estimator=RidgeCV(alphas=array([ 0.1,  1. , 10. ]), cv=None, fit_intercept=True,<br/>    gcv_mode=None, normalize=False, scoring=None, store_cv_values=False))), ('maxabsscaler-1', MaxAbsScaler(copy=True)), ('maxabsscaler-2', MaxAbsScaler(copy=True)), ('xgbr...      reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,<br/>       silent=True, subsample=0.5))])]</span></pre><p id="10fb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">运行很快完成，每次都找到不同的获胜管道。取分数的平方根得到均方根误差(RMSE)。RMSE 的平均价格在 3 万美元左右。</p><p id="2739" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于 train_test_split 和 TPOTRegressor，尝试使用 60 个管道和 random_state = 20。</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="aff0" class="mx lp iq mt b gy my mz l na nb">Times: [9.691357856966594, 1.8972856383004304, 2.5272325469001466]<br/>Scores: [-1061075530.3715296, -695536167.1288683, -783733389.9523941]</span><span id="7c0d" class="mx lp iq mt b gy nd mz l na nb">Winning pipelines: [</span><span id="92e8" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('stackingestimator-1', StackingEstimator(estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,<br/>           max_features=0.7000000000000001, max_leaf_nodes=None,<br/>           min_impurity_decrease=0.0, min_impurity_split=None,<br/>           min_samples_leaf=12, min_sample...0.6000000000000001, tol=0.0001,<br/>             validation_fraction=0.1, verbose=0, warm_start=False))]), </span><span id="a0c8" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('xgbregressor', XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,<br/>       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,<br/>       max_depth=7, min_child_weight=3, missing=None, n_estimators=100,<br/>       n_jobs=1, nthread=1, objective='reg:linear', random_state=0,<br/>       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,<br/>       silent=True, subsample=1.0))]), </span><span id="ab79" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,<br/>           max_features=0.7000000000000001, max_leaf_nodes=None,<br/>           min_impurity_decrease=0.0, min_impurity_split=None,<br/>           min_samples_leaf=12, min_samples_...ators=100, n_jobs=None,<br/>          oob_score=False, random_state=None, verbose=0, warm_start=False))])]</span></pre><p id="6f84" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">导致了截然不同的管道和分数。</p><p id="45bb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">让我们用 720 条管道试一次更长的运行</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="ab76" class="mx lp iq mt b gy my mz l na nb">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 20)</span><span id="0b5f" class="mx lp iq mt b gy nd mz l na nb">tpot = TPOTRegressor(verbosity=3, <br/> random_state=10, <br/> #scoring=rmsle,<br/> periodic_checkpoint_folder=”any_string”,<br/> n_jobs=-1, <br/> generations=8, <br/> population_size=80,<br/> early_stop=5)</span></pre><p id="6152" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">结果:</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="8e5d" class="mx lp iq mt b gy my mz l na nb">Times: [43.206709423016584]<br/>Scores: [-644910660.5815958] <br/>Winning pipelines: [Pipeline(memory=None,<br/>     steps=[('xgbregressor', XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,<br/>       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,<br/>       max_depth=8, min_child_weight=3, missing=None, n_estimators=100,<br/>       n_jobs=1, nthread=1, objective='reg:linear', random_state=0,<br/>       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,<br/>       silent=True, subsample=0.8500000000000001))])]</span></pre><p id="0f4e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">RMSE 是迄今为止最好的。收敛花了大半个小时，我们仍然在运行比推荐的小得多的管道。🤔</p><p id="c378" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">接下来，让我们尝试使用均方根对数误差，这是 Kaggle 在本次比赛中使用的自定义评分参数。这是在另一个非常小的迭代中运行的，30 个管道分三次运行，random_state=20。我们不能使用一个以上的 CPU 核心，因为在 Jupyter 的一些 TPOT 算法中有一个自定义评分参数的错误。</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="7800" class="mx lp iq mt b gy my mz l na nb">Times: [1.6125734224997965, 1.2910610851162345, 0.9708147236000514]<br/>Scores: [-0.15007242511943228, -0.14164770517342357, -0.15506057088945932]<br/>Winning pipelines: [</span><span id="d154" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('maxabsscaler', MaxAbsScaler(copy=True)), ('stackingestimator', StackingEstimator(estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,<br/>           max_features=0.7000000000000001, max_leaf_nodes=None,<br/>           min_impurity_decrease=0.0, min_impurity_split=None,<br/> ...0.6000000000000001, tol=0.0001,<br/>             validation_fraction=0.1, verbose=0, warm_start=False))]), </span><span id="4641" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('extratreesregressor', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,<br/>          max_features=0.6500000000000001, max_leaf_nodes=None,<br/>          min_impurity_decrease=0.0, min_impurity_split=None,<br/>          min_samples_leaf=7, min_samples_split=10,<br/>          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,<br/>          oob_score=False, random_state=None, verbose=0, warm_start=False))]), </span><span id="e103" class="mx lp iq mt b gy nd mz l na nb">Pipeline(memory=None,<br/>     steps=[('ridgecv', RidgeCV(alphas=array([ 0.1,  1. , 10. ]), cv=None, fit_intercept=True,<br/>    gcv_mode=None, normalize=False, scoring=None, store_cv_values=False))])]</span></pre><p id="024f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">那些分数并不可怕。这个小运行的 tpot.export 的输出文件如下。</p><pre class="lf lg lh li gt ms mt mu mv aw mw bi"><span id="3080" class="mx lp iq mt b gy my mz l na nb">import numpy as np <br/>import pandas as pd <br/>from sklearn.linear_model import ElasticNetCV, LassoLarsCV <br/>from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline, make_union <br/>from tpot.builtins import StackingEstimator  </span><span id="8dcc" class="mx lp iq mt b gy nd mz l na nb"># NOTE: Make sure that the class is labeled 'target' in the data file <br/>tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64) </span><span id="795a" class="mx lp iq mt b gy nd mz l na nb">features = tpot_data.drop('target', axis=1).values </span><span id="f054" class="mx lp iq mt b gy nd mz l na nb">training_features, testing_features, training_target, testing_target = train_test_split(features, tpot_data['target'].values, random_state=42)  </span><span id="4587" class="mx lp iq mt b gy nd mz l na nb"># Score on the training set was:-0.169929041242275 </span><span id="2d8f" class="mx lp iq mt b gy nd mz l na nb">exported_pipeline = make_pipeline(     StackingEstimator(estimator=LassoLarsCV(normalize=False)),     ElasticNetCV(l1_ratio=0.75, tol=0.01) </span><span id="a199" class="mx lp iq mt b gy nd mz l na nb">exported_pipeline.fit(training_features, training_target) </span><span id="c067" class="mx lp iq mt b gy nd mz l na nb">results = exported_pipeline.predict(testing_features)</span></pre><p id="8ca6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">将来，我想在这个数据集上对 TPOT 做一些更长时间的运行，看看它的表现如何。我还想看看一些手动特征工程和各种编码策略如何提高我们的模型性能。</p><h1 id="ba72" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">抓到 TPOT 和卡格尔了</h1><p id="34a2" class="pw-post-body-paragraph jz ka iq kb b kc mn ke kf kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ij bi translated">我喜欢 Kaggle 的内核，但如果你想运行几个小时的算法，如 TPOT，这可能会超级令人沮丧。内核在运行时经常崩溃，你有时无法判断你尝试的提交是否被挂起，并且你不能像你希望的那样控制你的环境。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ok"><img src="../Images/df7f1d251b9da93b2f8848a37792ec59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ja80FanFt0zx4_zvYq9qlw.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Wall for banging head</figcaption></figure><p id="9858" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">没有什么比 720 次管道迭代中的 700 次更好的了。我的 Kaggle CPU 利用率经常显示为 400%+并且在这个练习中需要多次重启。</p><p id="c676" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">其他一些需要注意的事项:</p><ul class=""><li id="54ae" class="nt nu iq kb b kc kd kg kh kk ol ko om ks on kw oo nz oa ob bi translated">我发现我需要将我的 Pandas 数据帧转换成 Numpy 数组，以避免回归任务中的 XGBoost 问题。这是熊猫和 XGBoost 的一个已知问题。</li><li id="cb9e" class="nt nu iq kb b kc oc kg od kk oe ko of ks og kw oo nz oa ob bi translated">一个 Kaggle 内核正在运行一个 Jupyter 笔记本。当 Jupyter 笔记本中的 n _ jobs &gt; 1 时，TPOT 的自定义评分分类器不起作用。这是一个已知的<a class="ae jy" href="https://github.com/EpistasisLab/tpot/issues/645" rel="noopener ugc nofollow" target="_blank">问题</a>。</li><li id="2ab0" class="nt nu iq kb b kc oc kg od kk oe ko of ks og kw oo nz oa ob bi translated">当你提交代码时，Kaggle 只会让你的内核代码写到一个输出文件。你看不到 TPOT 提交时的临时输出。确保文件名用引号括起来，不要用斜线。该文件将显示在 O <em class="la">输出</em>选项卡上。</li><li id="cd85" class="nt nu iq kb b kc oc kg od kk oe ko of ks og kw oo nz oa ob bi translated">在 Kaggle 上打开 GPU 设置并不能加快大多数分析的速度，但可能会加快深度学习的速度。</li><li id="ef07" class="nt nu iq kb b kc oc kg od kk oe ko of ks og kw oo nz oa ob bi translated">ka ggle 6 小时的可能运行时间和 GPU 设置使得在非大型数据集上无需配置即可免费实验 TPOT 成为可能。很难拒绝免费的。</li></ul><p id="8436" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">为了更多的时间和速度，你可以使用类似于<a class="ae jy" href="https://www.paperspace.com/" rel="noopener ugc nofollow" target="_blank"> Paperspace </a>的东西。我在 Paperspace 上建立了 TPOT，虽然不是免费的，但也很轻松。如果你需要一个云解决方案来运行 TPOT，我建议你先在 Kaggle 上使用它，如果你需要几个小时以上的运行时间或更高的功率，就离开 Kaggle。</p><h1 id="9892" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">未来方向</h1><p id="e91d" class="pw-post-body-paragraph jz ka iq kb b kc mn ke kf kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ij bi translated">TPOT 和 autoML 有很多有趣的方向可以探索。我想将 TPOT 与 autoSKlearn、MLBox、Auto-Keras 和其他公司进行比较。我还想看看它在处理更多种类的数据、其他插补策略和其他编码策略时的表现。与 LightGBM、CatBoost 和深度学习算法的比较也将是有趣的。机器学习的这个时刻令人兴奋的是，有这么多领域可以探索。关注<a class="ae jy" href="https://medium.com/@jeffhale" rel="noopener"> me </a>确保不会错过未来的分析。</p><p id="4fef" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于大多数数据集来说，仍然有大量的数据清理、特征工程和最终模型选择要做——更不用说预先提出正确问题这一最重要的步骤。那么你可能需要生产你的模型。TPOT 还没有进行彻底的搜索。所以 TPOT 不会取代数据科学家的角色——但这个工具可能会让你最终的机器学习算法更好更快。</p><p id="e5af" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果你使用过 TPOT 或其他 autoML 工具，请在评论中分享你的经验。</p><p id="4b1b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我希望这篇介绍 TPOT 的文章对你有所帮助。如果你有，请在你最喜欢的社交媒体上分享，这样其他人也可以找到它。😀</p><p id="bb33" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我写关于<a class="ae jy" href="https://memorablepython.com" rel="noopener ugc nofollow" target="_blank"> Python </a>、<a class="ae jy" href="https://memorablesql.com" rel="noopener ugc nofollow" target="_blank"> SQL </a>和其他技术主题的文章。如果你对这些感兴趣，请注册我的<a class="ae jy" href="https://dataawesome.com" rel="noopener ugc nofollow" target="_blank">邮件列表，那里有很棒的数据科学资源</a>，点击阅读更多内容，帮助你提高技能<a class="ae jy" href="https://medium.com/@jeffhale" rel="noopener">。👍</a></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><a href="https://dataawesome.com"><div class="ab gu cl op"><img src="../Images/ba32af1aa267917812a85c401d1f7d29.png" data-original-src="https://miro.medium.com/v2/format:webp/1*oPkqiu1rrt-hC_lDMK-jQg.png"/></div></a></figure><p id="a7b9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">投票快乐！🚀</p></div></div>    
</body>
</html>