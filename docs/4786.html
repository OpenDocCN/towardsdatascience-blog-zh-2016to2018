<html>
<head>
<title>Why GPUs?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么选择 GPU？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-gpus-4af616f15ab6?source=collection_archive---------12-----------------------#2018-09-06">https://towardsdatascience.com/why-gpus-4af616f15ab6?source=collection_archive---------12-----------------------#2018-09-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/2390b88b18fc7ead4791290d09bf7a73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QY1GTx_UVf2l236UTaYuHw.jpeg"/></div></figure><p id="e0d1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在深度学习社区中，GPU 支持的机器和集群将大大加快训练神经网络的时间，这已经不是秘密。在本文中，我们将研究梯度下降的运行时间，以及 GPU 在哪些方面降低了时间复杂度。</p><p id="7008" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">伪代码，梯度下降(为了简洁省略了一些细节)</p><pre class="ks kt ku kv gt kw kx ky kz aw la bi"><span id="0d5f" class="lb lc iq kx b gy ld le l lf lg">While (k &lt; max_iterations):<br/>  select batch size n<br/>  For each instance in n:<br/>    Calculate network output<br/>    For each weight dimension m:<br/>      # arbitrarily chosen update algorithm<br/>      delta_wi = delta_wi + eta * (desired - output) * x(n)i<br/>  For each weight dimension m:<br/>      wi(k+1) = wi(k) + delta_wi<br/>  k += 1</span></pre><p id="bb97" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">该算法以 O(knm)运行，(丢弃计算网络输出所花费的时间，因为这完全依赖于网络架构，如果你在做典型的矩阵*向量运算，W^T*X).，则在每个节点可以是 O(m)</p><p id="148f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> O(knm): </strong>循环的 k 次迭代，批量大小的 n 个实例，权重的 m 维。</p><p id="58ee" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">然而，神经网络算法的最佳特性之一是它们对并行化的可扩展性。我们能够将批量更新中的<strong class="jw ir"> n 个实例</strong>和<strong class="jw ir"> m 个维度</strong>拆分到我们 GPU 上的<strong class="jw ir"> c 个不同内核中。</strong></p><p id="ce6f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">NVIDIA GPU 通常有&gt; 1，000 个内核，</strong>相比之下，CPU 速度快得惊人，但只有大约 4 到 8 个内核。这对我们的神经网络训练意味着什么？</p><p id="6ba3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这些内核允许我们将我们的批处理(n/c)分开，并在每个内核上并行运行训练过程。由于梯度下降更新网络中权重参数的方式，这是可能的。GD 不是在每个训练步骤后立即更新权重，而是聚集该批的误差，然后在计算完该批中所有实例的更新后更新权重。此外，权重的更新可以分开，因为这些操作也彼此独立地运行。</p><p id="d03f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">感谢阅读，我希望这能帮助你获得一些关于 GPU 和梯度下降算法的直觉！如果你有兴趣尝试 GPU，我强烈推荐<a class="ae lh" href="https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d" rel="noopener"> Google Colab 运行时。</a></p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="li lj l"/></div></figure></div><div class="ab cl lk ll hu lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="ij ik il im in"><h1 id="ffc4" class="lr lc iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated"><a class="ae lh" href="https://medium.com/@connorshorten300" rel="noopener"> CShorten </a></h1><p id="fc78" class="pw-post-body-paragraph ju jv iq jw b jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr ij bi translated">Connor Shorten 是一名计算机科学学生。对软件经济学、深度学习和软件工程感兴趣。</p></div></div>    
</body>
</html>