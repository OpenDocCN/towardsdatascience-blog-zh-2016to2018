<html>
<head>
<title>Why models with significant variables can be useless predictors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么有重要变量的模型可能是无用的预测器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-models-with-significant-variables-can-be-useless-predictors-3354722a4c05?source=collection_archive---------13-----------------------#2018-12-12">https://towardsdatascience.com/why-models-with-significant-variables-can-be-useless-predictors-3354722a4c05?source=collection_archive---------13-----------------------#2018-12-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="36ec" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">理解你的回归</h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/0958b4af84f124a002757dca05f04292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VxhayhZ3Rh15tcxX.jpg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk"><em class="ko">Photo by Rakicevic Nenad from Pexels</em></figcaption></figure><h2 id="cf46" class="kp kq it bd kr ks kt dn ku kv kw dp kx ky kz la lb lc ld le lf lg lh li lj iz bi translated">统计模型中的重要变量不能保证预测性能</h2><p id="1a5b" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu ky lv lw lx lc ly lz ma lg mb mc md me im bi translated">在数据科学或实验科学课上，你首先要学习(或应该学习)的一件事是解释模型和预测模型之间的区别。解释模型通常测试关于变量之间关系的假设，例如它们是否显著协变，或者组之间是否存在显著的数值差异。另一方面，预测模型关注的是经过训练的模型能够在多大程度上估计出未在训练中使用的结果。不幸的是，在解释模型中有多个重要变量并不能保证预测模型的高准确性。</p><p id="82f8" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">这两种模型类型经常被混为一谈，例如将回归模型中的自变量描述为“预测”因变量。专注于统计建模的研究人员也可能假设这种高显著性(例如 p <.001 in="" their="" variables="" may="" be="" an="" indicator="" of="" prediction="" ability="" the="" model.="" this="" confusion="" permeates="" not="" only="" class="ae mk" href="http://science.sciencemag.org/content/355/6324/486" rel="noopener ugc nofollow" target="_blank">社会科学以及其他数据丰富的科学学科，如<a class="ae mk" href="https://www.pnas.org/content/112/45/13892" rel="noopener ugc nofollow" target="_blank">遗传学</a>和<a class="ae mk" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5761318/" rel="noopener ugc nofollow" target="_blank">神经科学</a>)。</.001></p><p id="d081" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">如此多的研究人员仍然将这两者混为一谈的原因之一可能是他们经常通过文本示例<a class="ae mk" href="https://www.psychologytoday.com/us/blog/perception-and-performance/201801/hypotheses-versus-predictions" rel="noopener ugc nofollow" target="_blank">而不是数字示例</a>来了解二者的区别。为了说明模型中的显著性如何不能保证模型预测，这里有一个快速模拟，它创建了一个合成数据集来显示假设检验模型中的显著变量如何不能用于预测实际结果。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="3db5" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">首先，让我们加载所需的库并生成一些合成数据。我们可以使用 sci-kit learn 的<code class="fe ms mt mu mv b"><a class="ae mk" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html" rel="noopener ugc nofollow" target="_blank">make_classification</a></code>功能轻松做到这一点。我们有意通过翻转 80%的 y 标签来使分类变得困难。将有 1000 个样本和 3 个特征，其中 3 个特征将在它们与 y 变量(0 和 1)的关系中提供信息。我们还将人工数据分为训练集(80%)和测试集(20%)。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="6361" class="kp kq it mv b gy ne nf l ng nh">%matplotlib inline<br/>%config InlineBackend.figure_format = 'retina'<br/>import numpy as np, pandas as pd, matplotlib.pyplot as plt<br/>from sklearn.datasets import make_classification<br/>from sklearn.model_selection import train_test_split</span><span id="4a3d" class="kp kq it mv b gy ni nf l ng nh"># Generate synthetic data<br/>X,y = make_classification(n_samples=1000, n_features=3, n_informative=3, n_redundant=0, n_classes=2, n_clusters_per_class=1, flip_y=.8, random_state=2018)<br/>colnames = ['X'+str(col) for col in range(X.shape[1])]<br/>X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2, random_state=1)</span><span id="3afe" class="kp kq it mv b gy ni nf l ng nh">df_train = pd.DataFrame(np.concatenate([X_train,np.expand_dims(y_train,axis=1)],axis=1), columns = np.concatenate([colnames,['y']]))<br/>df_test = pd.DataFrame(np.concatenate([X_test,np.expand_dims(y_test,axis=1)],axis=1), columns = np.concatenate([colnames,['y']]))</span></pre><p id="9521" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">现在让我们以散点图的形式可视化数据，其中轴代表变量。每个点代表一个样本，颜色表示 y 标签，0 或 1。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="61b3" class="kp kq it mv b gy ne nf l ng nh">from mpl_toolkits.mplot3d import Axes3D</span><span id="dc5e" class="kp kq it mv b gy ni nf l ng nh">fig = plt.figure()<br/>ax = fig.add_subplot(111, projection='3d')<br/>ax.view_init(elev=45., azim=45)<br/>for _y in [0,1]:<br/>    ax.scatter(X.T[0,y==_y],X.T[1,y==_y],X.T[2,y==_y],alpha=.3,s=10,label=f'y{_y}')<br/>plt.legend()<br/>plt.tight_layout();</span></pre><figure class="mw mx my mz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nj"><img src="../Images/2a9efac25ded58ed2927b672ba3c6cb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*I8UumlcC1tt1fDWi.png"/></div></div></figure><p id="2456" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">散点图看起来很乱，但正如预期的那样，因为我们使分类变得困难。为了查看变量中是否有任何显著的组差异，让我们分别绘制每个变量的分布。我们还可以进行简单的假设 t 检验，以确定两组在这些变量上的差异。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="b321" class="kp kq it mv b gy ne nf l ng nh">from scipy.stats import stats<br/>kwargs = dict(histtype='stepfilled', alpha=0.3, density=True, bins=60)<br/>f,axes = plt.subplots(1,len(colnames),figsize=(20,4))<br/>for ix, x_label in enumerate(colnames):<br/>    y0 = df_train.query('y==0')[x_label]<br/>    y1 = df_train.query('y==1')[x_label]<br/>    ax = axes[ix]<br/>    ax.hist(y0, **kwargs,label='y0')<br/>    ax.hist(y1, **kwargs,label='y1')<br/>    ax.axvline(np.mean(y0),color='b',alpha = .5)<br/>    ax.axvline(np.mean(y1),color='r',alpha = .5)<br/>    # Run t-test between groups <br/>    t, p = stats.ttest_ind(y0,y1, equal_var=False)<br/>    title = f"{x_label} group difference \n t: {t:.2f}, p: {p:.2e}"<br/>    ax.set(xlabel=x_label,ylabel='counts',title=title)<br/>plt.legend()<br/>plt.show();</span></pre><figure class="mw mx my mz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/cc8ef8fb7a9a2dde0ae437e57761f3ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hJ--JTZi4PMdt5ya.png"/></div></div></figure><p id="2653" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">从显示组均值和 t 检验结果差异的直方图中，我们可以看到，两个变量 X0 和 X2 显示了两组之间的统计学显著差异。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="8409" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">作为一个更稳健的测试，我们通过用 X 变量拟合关于 y 的逻辑回归建立一个解释模型，假设 X0 和 X2 在估计 y 的值时显著相关</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="a2da" class="kp kq it mv b gy ne nf l ng nh">import statsmodels.formula.api as smf<br/>formula = 'y~'+'+'.join(colnames)<br/>mod = smf.logit(formula=formula,data=df_train)<br/>res = mod.fit()<br/>print(res.summary())</span></pre><figure class="mw mx my mz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nk"><img src="../Images/830c547cc9dd416510274600db5100f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oxmeQvX7hZuapIrn.png"/></div></div></figure><p id="0491" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">咻，我们的假设被证实了，X0 和 X2 显示出与 y 标签的显著关系。因为我们有两个重要的预测变量，一个是 p <.05 and="" the="" other="" p="" one="" might="" assume="" that="" we="" can="" do="" really="" well="" in="" class="nl">预测 y 标签。</.05></p><p id="7325" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">我们来看看是不是这样。</p><p id="f79f" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">为了测试我们的 Xs 是否能够很好地预测 y 标签，我们在训练集上拟合了一个逻辑回归模型，但在测试集上评估了它的预测准确性。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="005f" class="kp kq it mv b gy ne nf l ng nh">from sklearn.linear_model import LogisticRegression</span><span id="5597" class="kp kq it mv b gy ni nf l ng nh">clf = LogisticRegression(solver='lbfgs')<br/>clf.fit(y=df_train['y'], X= df_train[colnames])<br/>print("Training Accuracy:", clf.score(df_train[colnames],df_train['y']))<br/>mean_score=clf.score(df_test[colnames],df_test['y'])<br/>print("Test Accuracy:", clf.score(df_test[colnames],df_test['y']))</span><span id="e59c" class="kp kq it mv b gy ni nf l ng nh">&gt;&gt; Training Accuracy: 0.58625 <br/>&gt;&gt; Test Accuracy: 0.55</span></pre><p id="0165" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">我们的结果显示准确性(55%)略高于偶然性(50%)，但没有当你看到 p <.01./></p><p id="d0de" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">Let’s look at what these performance look like graphically in a <a class="ae mk" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py" rel="noopener ugc nofollow" target="_blank">混淆矩阵</a>和<a class="ae mk" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py" rel="noopener ugc nofollow" target="_blank"> ROC(接收器操作特性)曲线</a>时肾上腺素激增那么令人印象深刻。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="3691" class="kp kq it mv b gy ne nf l ng nh">from sklearn.metrics import confusion_matrix, roc_curve, auc</span><span id="d62a" class="kp kq it mv b gy ni nf l ng nh">def plot_confusion_matrix(cm, classes,<br/>                          normalize=False,<br/>                          title='Confusion matrix',<br/>                          cmap=plt.cm.Blues):<br/>    """<br/>    This function prints and plots the confusion matrix.<br/>    Normalization can be applied by setting `normalize=True`.<br/>    """<br/>    import itertools</span><span id="5263" class="kp kq it mv b gy ni nf l ng nh">if normalize:<br/>        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]<br/>        print("Normalized confusion matrix")<br/>    else:<br/>        print('Confusion matrix, without normalization')<br/>    plt.imshow(cm, interpolation='nearest', cmap=cmap,vmin=0,vmax=1)<br/>    plt.title(title)<br/>    plt.colorbar()<br/>    tick_marks = np.arange(len(classes))<br/>    plt.xticks(tick_marks, classes, rotation=45)<br/>    plt.yticks(tick_marks, classes)</span><span id="3505" class="kp kq it mv b gy ni nf l ng nh">fmt = '.2f' if normalize else 'd'<br/>    thresh = cm.max() / 2.<br/>    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):<br/>        plt.text(j, i, format(cm[i, j], fmt),<br/>                 horizontalalignment="center",<br/>                 color="white" if cm[i, j] &gt; thresh else "black")</span><span id="6a43" class="kp kq it mv b gy ni nf l ng nh">plt.ylabel('True label')<br/>    plt.xlabel('Predicted label')<br/>    plt.tight_layout()<br/>    <br/>cm = confusion_matrix(y_true = df_test['y'], y_pred = clf.predict(df_test[colnames]))<br/>plot_confusion_matrix(cm, classes=clf.classes_,normalize=True)</span></pre><figure class="mw mx my mz gt kd gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/786b5c3f34427723d197fa1a1eaf869f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/0*KOu0Zd8ouwQv_H0m.png"/></div></figure><p id="020a" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">如果这是一个非常好的模型，我们会在混淆矩阵的对角线上看到强烈的深色细胞。单元格之间缺乏区分说明了我们的模型在区分标签方面表现不佳。它在 54%的时间里准确地预测了 0 标签，而对标签 1 的预测只有 43%。</p><p id="aae5" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">绘制 ROC 曲线是说明模型的敏感性和特异性的一种方式，其中一个好的模型将由偏离对角线最远的<a class="ae mk" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" rel="noopener ugc nofollow" target="_blank">曲线来说明。</a></p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="a2b0" class="kp kq it mv b gy ne nf l ng nh">y_score = clf.decision_function(X_test) fpr, tpr, thresholds =roc_curve(y_test, y_score) roc_auc = auc(fpr,tpr) plt.plot(fpr, tpr, lw=1, alpha=0.3,label='ROC test (AUC = %0.2f)' % (roc_auc)) plt.legend() plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8); plt.axis('square');</span></pre><figure class="mw mx my mz gt kd gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/7c6df763ec6ce0d864c718b2cc4a5c66.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/0*2h3WgOq40j8KigyU.png"/></div></figure><p id="eec3" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">我们的 ROC 线几乎没有与对角线分开，再次显示了它的糟糕表现。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="093f" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">最后，由于<a class="ae mk" rel="noopener" target="_blank" href="/chance-is-not-enough-evaluating-model-significance-with-permutations-e3b17de6ba04">机会本身并不是模型显著性的良好指标</a>，让我们测试一下随机模型与其他随机可能模型相比表现如何。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="0772" class="kp kq it mv b gy ne nf l ng nh">from sklearn.model_selection import KFold, RepeatedKFold<br/>from sklearn.linear_model import LogisticRegressionCV</span><span id="1e44" class="kp kq it mv b gy ni nf l ng nh">np.random.seed(1)<br/>kfold = RepeatedKFold(n_splits=5, n_repeats = 5)<br/>n_permute = 200<br/>permuted_scores = []<br/>count = 0 <br/>for train_ix, test_ix in kfold.split(X):<br/>    X_train, X_test = X[train_ix], X[test_ix]<br/>    y_train, y_test = y[train_ix], y[test_ix]<br/>    train_df = pd.DataFrame({'y':y_train})<br/>    for permute_ix in range(n_permute): <br/>        count+=1<br/>        if count%100==0:<br/>            print(count,end=',')<br/>        y_train_shuffled = train_df[['y']].transform(np.random.permutation)<br/>        clf = LogisticRegressionCV(cv=5)<br/>        clf.fit(X_train,y_train_shuffled.values.ravel())<br/>        score = clf.score(X_test,y_test)<br/>        permuted_scores.append(score)<br/>train_shuffled_score = np.mean(permuted_scores)<br/>score_percentile = np.mean(mean_score &lt; permuted_scores)<br/>title = f'True score percentile : {score_percentile}'<br/>f, ax = plt.subplots() <br/>plt.hist(permuted_scores,bins=15)<br/>ax.axvline(train_shuffled_score,color='k',linestyle='--',label='Chance')<br/>ax.axvline(mean_score,color='r',linestyle='--',label='True classification score')<br/>ax.set(xlim=[0.1,.9],xlabel='Accuracy (%)',ylabel='Count',title = title)<br/>plt.legend();</span></pre><figure class="mw mx my mz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi no"><img src="../Images/e0c66e5bba02304b9118c7e288517b2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*c_RRx7K5GKGNBoSn.png"/></div></div></figure><p id="03dd" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">我们 55%的比机会更好的准确性证明并不比其他随机模型好多少(p=.0812)。这最终表明，解释性模型中有重要的预测因素并不能保证预测模型的性能。</p><h1 id="6331" class="np kq it bd kr nq nr ns ku nt nu nv kx nw nx ny lb nz oa ob lf oc od oe lj of bi translated">我们学到了什么</h1><p id="3b5e" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu ky lv lw lx lc ly lz ma lg mb mc md me im bi translated">上面的例子说明了解释或假设检验模型的重要变量可能无法保证预测的重要性能。当然，会有重要模型确实提供良好预测的情况，但重要的是要记住这种模拟并不总是如此。</p><h1 id="f526" class="np kq it bd kr nq nr ns ku nt nu nv kx nw nx ny lb nz oa ob lf oc od oe lj of bi translated">结论</h1><p id="7dda" class="pw-post-body-paragraph lk ll it lm b ln lo lp lq lr ls lt lu ky lv lw lx lc ly lz ma lg mb mc md me im bi translated">下次你的假设检验朋友告诉你，他们不需要运行预测模型，因为他们的变量非常重要，确保你用模拟数据的例子向他们说明，他们在模型中的重要变量根本不能保证预测的准确性。</p><p id="de4a" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated">标签:<code class="fe ms mt mu mv b"><a class="ae mk" href="http://jinhyuncheong.com/tag/machine-learning" rel="noopener ugc nofollow" target="_blank">machine-learning</a></code> <code class="fe ms mt mu mv b"><a class="ae mk" href="http://jinhyuncheong.com/tag/significance" rel="noopener ugc nofollow" target="_blank">significance</a></code> <code class="fe ms mt mu mv b"><a class="ae mk" href="http://jinhyuncheong.com/tag/permutations" rel="noopener ugc nofollow" target="_blank">permutations</a></code> <code class="fe ms mt mu mv b"><a class="ae mk" href="http://jinhyuncheong.com/tag/analysis" rel="noopener ugc nofollow" target="_blank">analysis</a></code> <code class="fe ms mt mu mv b"><a class="ae mk" href="http://jinhyuncheong.com/tag/data" rel="noopener ugc nofollow" target="_blank">data</a></code> <code class="fe ms mt mu mv b"><a class="ae mk" href="http://jinhyuncheong.com/tag/prediction" rel="noopener ugc nofollow" target="_blank">prediction</a></code> <code class="fe ms mt mu mv b"><a class="ae mk" href="http://jinhyuncheong.com/tag/simulation" rel="noopener ugc nofollow" target="_blank">simulation</a></code> <code class="fe ms mt mu mv b"><a class="ae mk" href="http://jinhyuncheong.com/tag/python" rel="noopener ugc nofollow" target="_blank">python</a></code></p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="e776" class="pw-post-body-paragraph lk ll it lm b ln mf lp lq lr mg lt lu ky mh lw lx lc mi lz ma lg mj mc md me im bi translated"><em class="nl">原载于 2018 年 12 月 12 日</em><a class="ae mk" href="http://jinhyuncheong.com/jekyll/update/2018/12/12/Significant_predictors_that_cant_predict.html" rel="noopener ugc nofollow" target="_blank"><em class="nl">jinhyuncheong.com</em></a><em class="nl">。</em></p></div></div>    
</body>
</html>