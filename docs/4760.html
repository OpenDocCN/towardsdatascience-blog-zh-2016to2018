<html>
<head>
<title>Scrape the web for top rated movies on TV</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在网上搜寻收视率最高的电视电影</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scrape-the-web-for-top-rated-movies-on-tv-ac66df0bd502?source=collection_archive---------8-----------------------#2018-09-05">https://towardsdatascience.com/scrape-the-web-for-top-rated-movies-on-tv-ac66df0bd502?source=collection_archive---------8-----------------------#2018-09-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/2dad6315dfdec02144ba69f4925ab93b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D52CsZmqCYvifA3M"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@frankokay?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Frank Okay</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1a29" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我将展示如何使用<a class="ae kc" href="http://scrapy.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> <em class="lb"> Scrapy 框架</em> </strong> </a>在互联网上搜索顶级电影。这个网页抓取器的<strong class="kf ir"> <em class="lb">目标</em> </strong>是找到在<a class="ae kc" href="https://www.themoviedb.org/" rel="noopener ugc nofollow" target="_blank">电影数据库</a>上具有高用户评级的电影。这些影片的列表将存储在一个<strong class="kf ir"> <em class="lb"> SQLite 数据库</em> </strong>和<strong class="kf ir"> <em class="lb">中，并通过电子邮件发送给</em> </strong>。这样你就知道你再也不会错过电视上的大片了。</p><h1 id="f4a2" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">寻找一个好的网页来抓取</h1><p id="17b8" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">我从在线电视指南开始寻找比利时电视频道上的电影。但是您可以轻松地修改我的代码，将其用于任何其他网站。为了让你刮电影的时候更轻松，确定你要刮的网站</p><ul class=""><li id="4019" class="mf mg iq kf b kg kh kk kl ko mh ks mi kw mj la mk ml mm mn bi translated">具有带<strong class="kf ir"> <em class="lb">的可理解类或 id </em> </strong>的 HTML 标签</li><li id="8d0f" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">以一种<strong class="kf ir"> <em class="lb">一致</em> </strong>的方式使用类和 id</li><li id="164f" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">有<strong class="kf ir"> <em class="lb">结构良好的网址</em> </strong></li><li id="03f6" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">在一个页面上包含所有相关的<strong class="kf ir"> <em class="lb">电视频道</em> </strong></li><li id="089e" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">每个工作日都有一个<strong class="kf ir"> <em class="lb">单独的页面</em> </strong></li><li id="5139" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated"><strong class="kf ir"> <em class="lb">只列出电影</em> </strong>，没有其他节目类型，如现场表演、新闻、报道等。除非你能轻易地将电影与其他节目类型区分开来。</li></ul><p id="8c75" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了发现的结果，我们将从电影数据库<a class="ae kc" href="https://www.themoviedb.org/" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"><em class="lb"/></strong></a>【TMDB】中获取电影评级和其他一些信息。</p><h1 id="bcf4" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">决定存储什么信息</h1><p id="a860" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">我将收集以下关于电影的信息:</p><ul class=""><li id="96bc" class="mf mg iq kf b kg kh kk kl ko mh ks mi kw mj la mk ml mm mn bi translated">电影名称</li><li id="11a7" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">电视频道</li><li id="4e14" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">电影开始的时间</li><li id="275e" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">这部电影在电视上播出的日期</li><li id="5984" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">类型</li><li id="11e2" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">情节</li><li id="b5d5" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">出厂日期</li><li id="c0f2" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">链接到 TMDB 的详细页面</li><li id="4a40" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated">TMDB 评级</li></ul><p id="7e2f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以用所有演员、导演、有趣的电影事实等等来补充这个列表。所有你想知道更多的信息。</p><p id="ea75" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 Scrapy 中，该信息将存储在<strong class="kf ir"> <em class="lb">项</em> </strong>的字段中。</p><h1 id="277d" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">创建 Scrapy 项目</h1><p id="50ca" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">我假设你已经安装了 Scrapy。如果没有，可以按照 Scrapy 优秀的<a class="ae kc" href="http://doc.scrapy.org/en/latest/intro/install.html" rel="noopener ugc nofollow" target="_blank">安装手册。</a></p><p id="3db5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">安装 Scrapy 后，打开命令行并转到您想要存储 Scrapy 项目的目录。然后运行:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="107f" class="nc ld iq my b gy nd ne l nf ng">scrapy startproject topfilms</span></pre><p id="7066" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这将为 top films 项目创建一个文件夹结构，如下所示。现在可以忽略 topfilms.db 文件。这是我们将在下一篇关于管道的博客文章中创建的 SQLite 数据库。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/f97103a9f0d137c3d9c98172229a930b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/0*dZ6phochXc8Dq1L6"/></div></figure><h1 id="5d1d" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">定义废料项目</h1><p id="10e4" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">在这个故事中，我们将使用文件<strong class="kf ir"> <em class="lb"> items.py </em>。</strong>创建你的 Scrapy 项目时默认创建 Items.py。</p><p id="6fd7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个<code class="fe ni nj nk my b">scrapy.Item</code>是一个容器，将在网页抓取过程中被填充。它将保存我们想要从网页中提取的所有字段。该项目的内容可以用与<strong class="kf ir"> <em class="lb"> Python dict </em> </strong>相同的方式访问。</p><p id="77cf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">打开 items.py 并添加一个包含以下字段的<code class="fe ni nj nk my b">Scrapy.Item class</code>:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="0878" class="nc ld iq my b gy nd ne l nf ng">import scrapy</span><span id="5b24" class="nc ld iq my b gy nl ne l nf ng">class TVGuideItem(scrapy.Item):<br/>    title = scrapy.Field()<br/>    channel = scrapy.Field()<br/>    start_ts = scrapy.Field()<br/>    film_date_long = scrapy.Field()<br/>    film_date_short = scrapy.Field()<br/>    genre = scrapy.Field()<br/>    plot = scrapy.Field()<br/>    rating = scrapy.Field()<br/>    tmdb_link = scrapy.Field()<br/>    release_date = scrapy.Field()<br/>    nb_votes = scrapy.Field()</span></pre><h1 id="f846" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">用管道处理项目</h1><p id="8268" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">在开始一个新的 Scrapy 项目后，您将拥有一个名为<strong class="kf ir"> pipelines.py </strong>的文件。打开该文件，复制粘贴如下所示的代码。之后，我将一步一步地向您展示代码的每一部分是做什么的。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="568f" class="nc ld iq my b gy nd ne l nf ng">import sqlite3 as lite<br/>con = None  # db connection<br/>class StoreInDBPipeline(object):<br/>    def __init__(self):<br/>        self.setupDBCon()<br/>        self.dropTopFilmsTable()<br/>        self.createTopFilmsTable()</span><span id="0fa1" class="nc ld iq my b gy nl ne l nf ng">def process_item(self, item, spider):<br/>        self.storeInDb(item)<br/>        return item</span><span id="1360" class="nc ld iq my b gy nl ne l nf ng">def storeInDb(self, item):<br/>        self.cur.execute("INSERT INTO topfilms(\<br/>        title, \<br/>        channel, \<br/>        start_ts, \<br/>        film_date_long, \<br/>        film_date_short, \<br/>        rating, \<br/>        genre, \<br/>        plot, \<br/>        tmdb_link, \<br/>        release_date, \<br/>        nb_votes \<br/>        ) \<br/>        VALUES( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? )",<br/>        (<br/>        item['title'],<br/>        item['channel'],<br/>        item['start_ts'],<br/>        item['film_date_long'],<br/>        item['film_date_short'],<br/>        float(item['rating']),<br/>        item['genre'],<br/>        item['plot'],<br/>        item['tmdb_link'],<br/>        item['release_date'],<br/>        item['nb_votes']<br/>        ))<br/>        self.con.commit()</span><span id="be20" class="nc ld iq my b gy nl ne l nf ng">def setupDBCon(self):<br/>        self.con = lite.connect('topfilms.db')<br/>        self.cur = self.con.cursor()</span><span id="c6a3" class="nc ld iq my b gy nl ne l nf ng">def __del__(self):<br/>        self.closeDB()</span><span id="8a62" class="nc ld iq my b gy nl ne l nf ng">def createTopFilmsTable(self):<br/>        self.cur.execute("CREATE TABLE IF NOT EXISTS topfilms(id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \<br/>        title TEXT, \<br/>        channel TEXT, \<br/>        start_ts TEXT, \<br/>        film_date_long TEXT, \<br/>        film_date_short TEXT, \<br/>        rating TEXT, \<br/>        genre TEXT, \<br/>        plot TEXT, \<br/>        tmdb_link TEXT, \<br/>        release_date TEXT, \<br/>        nb_votes \<br/>        )")</span><span id="646d" class="nc ld iq my b gy nl ne l nf ng">def dropTopFilmsTable(self):<br/>        self.cur.execute("DROP TABLE IF EXISTS topfilms")<br/>        <br/>    def closeDB(self):<br/>        self.con.close()</span></pre><p id="e492" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们从导入<a class="ae kc" href="https://docs.python.org/2/library/sqlite3.html" rel="noopener ugc nofollow" target="_blank"> SQLite 包</a>开始，并给它起别名<code class="fe ni nj nk my b">lite</code>。我们还初始化了一个用于数据库连接的变量<code class="fe ni nj nk my b">con</code>。</p><h2 id="9507" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">创建一个类来存储数据库中的项</h2><p id="1875" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">接下来，您创建一个具有逻辑名称的<a class="ae kc" href="https://docs.python.org/2/tutorial/classes.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> <em class="lb">类</em> </strong> </a>。在设置文件中启用管道后(稍后将详细介绍)，这个类将被调用。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="4e29" class="nc ld iq my b gy nd ne l nf ng">class StoreInDBPipeline(object):</span></pre><h2 id="fe78" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">定义构造函数方法</h2><p id="e79a" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">构造函数方法是名为<code class="fe ni nj nk my b">__init__</code>的方法。当创建一个<code class="fe ni nj nk my b">StoreInDBPipeline</code>类的实例时，这个方法会自动运行。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="2759" class="nc ld iq my b gy nd ne l nf ng">def __init__(self):<br/>        self.setupDBCon()<br/>        self.dropTopFilmsTable()<br/>        self.createTopFilmsTable()</span></pre><p id="a724" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在构造函数方法中，我们启动了在构造函数方法下面定义的另外三个方法。</p><h2 id="ebbb" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">SetupDBCon 方法</h2><p id="59e0" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">通过方法<code class="fe ni nj nk my b">setupDBCon</code>，我们创建了 topfilms 数据库(如果它还不存在的话)并用<code class="fe ni nj nk my b">connect</code>函数连接到它。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="f85c" class="nc ld iq my b gy nd ne l nf ng">def setupDBCon(self):<br/>        self.con = lite.connect('topfilms.db')<br/>	self.cur = self.con.cursor()</span></pre><p id="26ac" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们将 alias lite 用于 SQLite 包。其次，我们用<code class="fe ni nj nk my b">cursor</code>函数创建一个光标对象。使用这个游标对象，我们可以在数据库中执行 SQL 语句。</p><h2 id="1693" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">DropTopFilmsTable 方法</h2><p id="832f" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">构造函数中调用的第二个方法是<code class="fe ni nj nk my b">dropTopFilmsTable</code>。顾名思义，它删除 SQLite 数据库中的表。</p><p id="a26c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每次运行 web scraper 时，数据库都会被完全删除。如果你也想这样做，那就看你自己了。如果你想对电影数据进行一些查询或分析，你可以保存每次运行的抓取结果。</p><p id="87e1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我只想看接下来几天的顶级电影，仅此而已。因此，我决定在每次运行时删除数据库。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="bb9f" class="nc ld iq my b gy nd ne l nf ng">def dropTopFilmsTable(self):<br/>        self.cur.execute("DROP TABLE IF EXISTS topfilms")</span></pre><p id="9a2a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用光标对象<code class="fe ni nj nk my b">cur</code>我们执行<code class="fe ni nj nk my b">DROP</code>语句。</p><h2 id="217f" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">CreateTopFilmsTable 方法</h2><p id="63b6" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">放下 top films 表后，我们需要创建它。这是通过构造函数方法中的最后一个方法调用来完成的。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="ccd6" class="nc ld iq my b gy nd ne l nf ng">def createTopFilmsTable(self):<br/>        self.cur.execute("CREATE TABLE IF NOT EXISTS topfilms(id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \<br/>        title TEXT, \<br/>        channel TEXT, \<br/>        start_ts TEXT, \<br/>        film_date_long TEXT, \<br/>        film_date_short TEXT, \<br/>        rating TEXT, \<br/>        genre TEXT, \<br/>        plot TEXT, \<br/>        tmdb_link TEXT, \<br/>        release_date TEXT, \<br/>        nb_votes \<br/>        )")</span></pre><p id="ce8a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们再次使用游标对象 cur 来执行<code class="fe ni nj nk my b">CREATE TABLE</code>语句。添加到桌面电影中的字段与我们之前创建的 Scrapy 项目中的字段相同。为了简单起见，我在 SQLite 表中使用了与 Item 中完全相同的名称。只有<code class="fe ni nj nk my b">id</code>字段是额外的。</p><blockquote class="nx ny nz"><p id="31c4" class="kd ke lb kf b kg kh ki kj kk kl km kn oa kp kq kr ob kt ku kv oc kx ky kz la ij bi translated"><strong class="kf ir">旁注</strong>:查看 SQLite 数据库的一个很好的应用是 Firefox 中的<a class="ae kc" href="https://addons.mozilla.org/nl/firefox/addon/sqlite-manager/" rel="noopener ugc nofollow" target="_blank"> SQLite 管理器插件。你可以在 Youtube </a>上观看这个<a class="ae kc" href="https://youtu.be/y-yA7YT-7gw" rel="noopener ugc nofollow" target="_blank"> SQLite 管理器教程来学习如何使用这个插件。</a></p></blockquote><h2 id="7798" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">流程项目方法</h2><p id="f824" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">此方法必须在 Pipeline 类中实现，并且必须返回 dict、Item 或 DropItem 异常。在我们的网络刮刀，我们将返回项目。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="cbd5" class="nc ld iq my b gy nd ne l nf ng">def process_item(self, item, spider):<br/>        self.storeInDb(item)<br/>	return item</span></pre><p id="4f5f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与解释的其他方法相比，它有两个额外的参数。被刮的<code class="fe ni nj nk my b">item</code>和刮物品的<code class="fe ni nj nk my b">spider</code>。从这个方法中，我们启动<code class="fe ni nj nk my b">storeInDb</code>方法，然后返回项目。</p><h2 id="3b37" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">StoreInDb 方法</h2><p id="cd1b" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">这个方法执行一个<code class="fe ni nj nk my b">INSERT</code>语句将抓取的条目插入 SQLite 数据库。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="71c7" class="nc ld iq my b gy nd ne l nf ng">def storeInDb(self, item):<br/>        self.cur.execute("INSERT INTO topfilms(\<br/>        title, \<br/>        channel, \<br/>        start_ts, \<br/>        film_date_long, \<br/>        film_date_short, \<br/>        rating, \<br/>        genre, \<br/>        plot, \<br/>        tmdb_link, \<br/>        release_date, \<br/>        nb_votes \<br/>        ) \<br/>        VALUES( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? )",<br/>        (<br/>        item['title'],<br/>        item['channel'],<br/>        item['start_ts'],<br/>        item['film_date_long'],<br/>        item['film_date_short'],<br/>        float(item['rating']),<br/>        item['genre'],<br/>        item['plot'],<br/>        item['tmdb_link'],<br/>        item['release_date'],<br/>        item['nb_votes']<br/>        ))<br/>        self.con.commit()</span></pre><p id="d662" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">表字段的值来自 item，这是此方法的一个参数。这些值被简单地称为一个字典值(记住一个条目只不过是一个字典？).</p><h2 id="d324" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">每个构造函数都有一个…析构函数</h2><p id="972f" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">与构造函数方法相对应的是名为<code class="fe ni nj nk my b">__del__</code>的析构函数方法。在 pipelines 类的析构函数方法中，我们关闭了与数据库的连接。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="8402" class="nc ld iq my b gy nd ne l nf ng">def __del__(self):<br/>	self.closeDB()</span></pre><h2 id="ce2b" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">CloseDB 方法</h2><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="5fca" class="nc ld iq my b gy nd ne l nf ng">def closeDB(self):<br/>	self.con.close()</span></pre><p id="bfd5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在最后一个方法中，我们用<code class="fe ni nj nk my b">close</code>函数关闭数据库连接。所以现在我们已经写了一个全功能的管道。还剩下最后一步来启用管道。</p><h2 id="ca6d" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">在 settings.py 中启用管道</h2><p id="5a80" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">打开<strong class="kf ir"> <em class="lb"> settings.py </em> </strong>文件，添加以下代码:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="6f82" class="nc ld iq my b gy nd ne l nf ng">ITEM_PIPELINES = {<br/>    'topfilms.pipelines.StoreInDBPipeline':1<br/>}</span></pre><p id="a7c2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lb">整数值</em> </strong>表示流水线运行的顺序。因为我们只有一个管道，所以我们给它赋值 1。</p><h1 id="58bb" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">在 Scrapy 中创建一个蜘蛛</h1><p id="0e56" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">现在我们就来看看刺儿头的核心，<strong class="kf ir"> <em class="lb">蜘蛛</em> </strong>。这是你的刮网器将完成的地方。我将一步一步地向您展示如何创建一个。</p><h2 id="c869" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">导入必要的包</h2><p id="f4fc" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">首先，我们将导入必要的包和模块。我们使用<code class="fe ni nj nk my b">CrawlSpider</code>模块跟踪在线电视指南中的链接。</p><p id="6a25" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe ni nj nk my b">Rule</code>和<code class="fe ni nj nk my b">LinkExtractor</code>用于确定我们想要关注的链接。</p><p id="7891" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe ni nj nk my b">config</code>模块包含一些在蜘蛛中使用的常量，如<code class="fe ni nj nk my b">DOM_1, DOM_2</code>和<code class="fe ni nj nk my b">START_URL</code>。配置模块位于当前目录的上一个目录。这就是为什么你在配置模块前看到两个点。</p><p id="01e2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们导入了<code class="fe ni nj nk my b">TVGuideItem</code>。该 TVGuideItem 将用于包含抓取过程中的信息。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="7720" class="nc ld iq my b gy nd ne l nf ng">import scrapy<br/>from scrapy.spiders import CrawlSpider, Rule<br/>from scrapy.linkextractors import LinkExtractor<br/>from fuzzywuzzy import fuzz<br/>from ..config import *<br/>from topfilms.items import TVGuideItem</span></pre><h2 id="2dc1" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">告诉蜘蛛去哪里</h2><p id="30d3" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">其次，我们创建了 CrawlSpider 类的子类。这是通过插入 CrawlSpider 作为<code class="fe ni nj nk my b">TVGuideSpider</code>类的参数来实现的。</p><p id="6956" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们给蜘蛛一个<code class="fe ni nj nk my b">name</code>，提供<code class="fe ni nj nk my b">allowed_domains</code>(例如 themoviedb.org)和<code class="fe ni nj nk my b">start_urls</code>。在我的例子中，start_urls 是电视指南的网页，所以您应该通过自己的首选网站来更改它。</p><p id="ca0c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过<code class="fe ni nj nk my b">rules</code>和<code class="fe ni nj nk my b">deny </code>参数，我们告诉爬行器在起始 URL 上跟随(不跟随)哪些 URL。不跟随的 URL 是用正则表达式指定的。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/b1fd927e75e48c9863dded4e84e59293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/0*r0r11AyaIBEC7ODH"/></div></figure><p id="638b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我对昨天放映的电影不感兴趣，所以我拒绝蜘蛛跟踪以“<em class="lb"> gisteren </em>”结尾的 URL。</p><p id="d1c4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好吧，但是蜘蛛应该跟踪哪些 URL 呢？为此，我使用了<code class="fe ni nj nk my b">restrict_xpaths</code>参数。它说要关注所有带有 class = " button button–beta "的 URL。这些事实上是未来一周每天的电影链接。</p><p id="fd0e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，通过<code class="fe ni nj nk my b">callback</code>参数，我们让蜘蛛知道当它跟踪一个 URL 时该做什么。它将执行功能<code class="fe ni nj nk my b">parse_by_day</code>。我将在下一部分解释这一点。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="2cb1" class="nc ld iq my b gy nd ne l nf ng">class TVGuideSpider(CrawlSpider):<br/>    name = "tvguide"<br/>    allowed_domains = [DOM_1, DOM_2]<br/>    start_urls = [START_URL]</span><span id="538b" class="nc ld iq my b gy nl ne l nf ng"># Extract the links from the navigation per day<br/>    # We will not crawl the films for yesterday<br/>    rules = (<br/>        Rule(LinkExtractor(allow=(), deny=(r'\/gisteren'), restrict_xpaths=('//a[@class="button button--beta"]',)), callback="parse_by_day", follow= True),<br/>    )</span></pre><h2 id="6800" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">解析跟踪的 URL</h2><p id="74ac" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">TVGuideScraper 的一部分功能<code class="fe ni nj nk my b">parse_by_day</code>每天从网页上抓取每个频道所有电影的概览。<code class="fe ni nj nk my b">response</code>参数来自运行网络抓取程序时启动的<code class="fe ni nj nk my b">Request</code>。</p><p id="ddeb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在被抓取的网页上，你需要找到用来显示我们感兴趣的信息的 HTML 元素。两个很好的工具是 Chrome 开发者工具和 Firefox 中的 Firebug 插件。</p><p id="37da" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们想要存储的一个东西是我们正在抓取电影的<code class="fe ni nj nk my b">date</code>。这个日期可以在带有<code class="fe ni nj nk my b">class="grid__col__inner"</code>的 div 中的段落(p)中找到。显然，这是您应该为自己的网页修改的内容。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi od"><img src="../Images/76a4711ede8f359759f7be1c3bc255cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/0*-EML6UFd2TbqVCmY"/></div></figure><p id="0abb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">利用响应对象的<code class="fe ni nj nk my b">xpath method</code>，我们提取段落中的文本。在这篇关于如何使用 xpath 函数的教程中，我学到了很多东西。</p><p id="5448" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过使用<code class="fe ni nj nk my b">extract_first</code>，我们确保不会将这个日期存储为一个列表。否则，在 SQLite 数据库中存储日期时会出现问题。</p><p id="ba45" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">之后，我对 film_date_long 执行了一些数据清理，并创建了格式为 YYYYMMDD 的<code class="fe ni nj nk my b">film_date_short</code>。我创建了这种 YYYYMMDD 格式，以便稍后按时间顺序对电影进行排序。</p><p id="065b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，刮电视频道。如果是在<code class="fe ni nj nk my b">ALLOWED_CHANNELS</code>(在 config 模块中定义)的列表中，我们继续刮标题和开始时间。该信息存储在由<code class="fe ni nj nk my b">TVGuideItem()</code>启动的项目中。</p><p id="77c4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这之后，我们想继续刮电影数据库。我们将使用 URL<a class="ae kc" href="https://www.themoviedb.org/search?query=" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">https://www.themoviedb.org/search?query=</strong></a>来显示被抓取的电影的搜索结果。对于这个 URL，我们要添加电影标题(代码中的<code class="fe ni nj nk my b">url_part</code>)。我们只是重复使用在电视指南网页上的链接中找到的 URL 部分。</p><p id="1fbe" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了这个 URL，我们创建了一个新的请求，并继续 TMDB。使用<code class="fe ni nj nk my b">request.meta['item'] = item</code>，我们将已经抓取的数据添加到请求中。这样我们可以继续填充我们当前的 TVGuideItem。</p><p id="3993" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe ni nj nk my b">Yield request</code>实际发起请求。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="035d" class="nc ld iq my b gy nd ne l nf ng">def parse_by_day(self, response):</span><span id="698a" class="nc ld iq my b gy nl ne l nf ng">film_date_long = response.xpath('//div[@class="grid__col__inner"]/p/text()').extract_first()<br/>        film_date_long = film_date_long.rsplit(',',1)[-1].strip()  # Remove day name and white spaces</span><span id="9c70" class="nc ld iq my b gy nl ne l nf ng"># Create a film date with a short format like YYYYMMDD to sort the results chronologically<br/>        film_day_parts = film_date_long.split()</span><span id="99e3" class="nc ld iq my b gy nl ne l nf ng">months_list = ['januari', 'februari', 'maart',<br/>                  'april', 'mei', 'juni', 'juli',<br/>                  'augustus', 'september', 'oktober',<br/>                  'november', 'december' ]</span><span id="fe46" class="nc ld iq my b gy nl ne l nf ng">year = str(film_day_parts[2])<br/>        month = str(months_list.index(film_day_parts[1]) + 1).zfill(2)<br/>        day = str(film_day_parts[0]).zfill(2)</span><span id="2831" class="nc ld iq my b gy nl ne l nf ng">film_date_short = year + month + day</span><span id="7671" class="nc ld iq my b gy nl ne l nf ng">for col_inner in response.xpath('//div[@class="grid__col__inner"]'):<br/>            chnl = col_inner.xpath('.//div[@class="tv-guide__channel"]/h6/a/text()').extract_first()</span><span id="1020" class="nc ld iq my b gy nl ne l nf ng">if chnl in ALLOWED_CHANNELS:<br/>                for program in col_inner.xpath('.//div[@class="program"]'):<br/>                    item = TVGuideItem()<br/>                    item['channel'] = chnl<br/>                    item['title'] = program.xpath('.//div[@class="title"]/a/text()').extract_first()<br/>                    item['start_ts'] = program.xpath('.//div[@class="time"]/text()').extract_first()<br/>                    item['film_date_long'] = film_date_long<br/>                    item['film_date_short'] = film_date_short</span><span id="ccfb" class="nc ld iq my b gy nl ne l nf ng">detail_link = program.xpath('.//div[@class="title"]/a/@href').extract_first()<br/>                    url_part = detail_link.rsplit('/',1)[-1]</span><span id="4df6" class="nc ld iq my b gy nl ne l nf ng"># Extract information from the Movie Database <a class="ae kc" href="http://www.themoviedb.org" rel="noopener ugc nofollow" target="_blank">www.themoviedb.org</a><br/>                    request = scrapy.Request("https://www.themoviedb.org/search?query="+url_part,callback=self.parse_tmdb)<br/>                    request.meta['item'] = item  # Pass the item with the request to the detail page</span><span id="3d75" class="nc ld iq my b gy nl ne l nf ng">yield request</span></pre><h2 id="fa6b" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">抓取电影数据库中的附加信息</h2><p id="e64f" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">正如您注意到的，在函数<code class="fe ni nj nk my b">parse_by_day</code>中创建的请求中，我们使用了回调函数<code class="fe ni nj nk my b">parse_tmdb</code>。这个函数在请求抓取 TMDB 网站时使用。</p><p id="0b2b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在第一步中，我们获取 parse_by_day 函数传递的商品信息。</p><p id="0f0e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">带有 TMDB 搜索结果的页面可能会列出同一个电影名称的多个搜索结果(查询中传递了 url_part)。我们也用<code class="fe ni nj nk my b">if tmddb_titles</code>检查是否有结果。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/631c1c8a03f22e812c30b98ece1ee27a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/0*ncBMqbk9fzZ-Szi0"/></div></figure><p id="92a7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用<a class="ae kc" href="https://pypi.python.org/pypi/fuzzywuzzy" rel="noopener ugc nofollow" target="_blank"> fuzzywuzzy </a>包对电影片名进行模糊匹配。为了使用 fuzzywuzzy 包，我们需要将<code class="fe ni nj nk my b">import</code>语句与前面的导入语句一起添加。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="c130" class="nc ld iq my b gy nd ne l nf ng">from fuzzywuzzy import fuzz</span></pre><p id="bb41" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们找到 90%匹配，我们使用搜索结果来做剩下的搜集工作。我们不再看其他搜索结果。为此，我们使用了<code class="fe ni nj nk my b">break</code>语句。</p><p id="1c04" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们从搜索结果页面中收集<code class="fe ni nj nk my b">genre</code>、<code class="fe ni nj nk my b">rating</code>和<code class="fe ni nj nk my b">release_date</code>，收集方式与之前使用 xpath 函数的方式类似。为了获得发布日期的 YYYYMMDD 格式，我们使用<code class="fe ni nj nk my b">split</code>和<code class="fe ni nj nk my b">join</code>函数执行一些数据处理。</p><p id="76b3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们想再次对 TMDB 的详细信息页面发起一个新的请求。这个请求将调用<code class="fe ni nj nk my b">parse_tmdb_detail</code>函数来提取电影情节和 TMDB 的票数。这将在下一节中解释。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="0d74" class="nc ld iq my b gy nd ne l nf ng">def parse_tmdb(self, response):<br/>        item = response.meta['item']  # Use the passed item</span><span id="e741" class="nc ld iq my b gy nl ne l nf ng">tmdb_titles = response.xpath('//a[@class="title result"]/text()').extract()</span><span id="390c" class="nc ld iq my b gy nl ne l nf ng">if tmdb_titles:  # Check if there are results on TMDB<br/>            for tmdb_title in tmdb_titles:<br/>                match_ratio = fuzz.ratio(item['title'], tmdb_title)<br/>                if match_ratio &gt; 90:<br/>                    item['genre'] = response.xpath('.//span[@class="genres"]/text()').extract_first()<br/>                    item['rating'] = response.xpath('//span[@class="vote_average"]/text()').extract_first()<br/>                    release_date = response.xpath('.//span[@class="release_date"]/text()').extract_first()<br/>                    release_date_parts = release_date.split('/')<br/>                    item['release_date'] = "/".join([release_date_parts[1].strip(), release_date_parts[0].strip(), release_date_parts[2].strip()])<br/>                    tmdb_link = "https://www.themoviedb.org" + response.xpath('//a[@class="title result"]/@href').extract_first()<br/>                    item['tmdb_link'] = tmdb_link</span><span id="b6c4" class="nc ld iq my b gy nl ne l nf ng"># Extract more info from the detail page<br/>                    request = scrapy.Request(tmdb_link,callback=self.parse_tmdb_detail)<br/>                    request.meta['item'] = item  # Pass the item with the request to the detail page</span><span id="d750" class="nc ld iq my b gy nl ne l nf ng">yield request<br/>                    break  # We only consider the first match<br/>                else:<br/>                    return</span></pre><h2 id="17be" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">从详细信息页面抓取电影情节</h2><p id="2e67" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">我们要讨论的最后一个函数是一个短函数。像以前一样，我们获取 parse_tmdb 函数传递的项目，并抓取<code class="fe ni nj nk my b">plot</code>和<code class="fe ni nj nk my b">number of votes</code>的详细信息页面。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/c3b5f356f52071050ef06ce1123bcc9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/0*C-Tj8dZ8yxfx_3gV"/></div></figure><p id="29dd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个阶段，我们已经完成了为这部电影搜集信息的工作。换句话说，这部电影的项目已经满员了。Scrapy 然后将使用管道中编写的代码来处理这些数据，并将其放入数据库。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="ecb4" class="nc ld iq my b gy nd ne l nf ng">def parse_tmdb_detail(self, response):<br/>        item = response.meta['item']  # Use the passed item</span><span id="485a" class="nc ld iq my b gy nl ne l nf ng">        item['nb_votes'] = response.xpath('//span[@itemprop="ratingCount"]/text()').extract_first()<br/>        item['plot'] = response.xpath('.//p[@id="overview"]/text()').extract_first()</span><span id="a79e" class="nc ld iq my b gy nl ne l nf ng">        yield item</span></pre><h1 id="3f8a" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">在 Scrapy 中使用扩展</h1><p id="608e" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">在关于管道的部分，我们已经看到了如何将抓取结果存储在 SQLite 数据库中。现在我将向您展示如何通过电子邮件<strong class="kf ir"> <em class="lb">发送刮擦结果。通过这种方式，你可以在邮箱里看到下周收视率最高的电影。</em></strong></p><h2 id="72a3" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">导入必要的包</h2><p id="dd4f" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">我们将使用文件<strong class="kf ir"> <em class="lb">扩展名. py </em> </strong>。当您创建 Scrapy 项目时，这个文件会自动创建在根目录中。我们首先导入我们将在本文件后面使用的包。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="5ff3" class="nc ld iq my b gy nd ne l nf ng">import logging<br/>from scrapy import signals<br/>from scrapy.exceptions import NotConfigured<br/>import smtplib<br/>import sqlite3 as lite<br/>from config import *</span></pre><p id="3bba" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并不真正需要<code class="fe ni nj nk my b">logging</code>包。但是这个包可以用来调试你的程序。或者只是向日志中写入一些信息。<br/><code class="fe ni nj nk my b">signals</code>模块将帮助我们知道蜘蛛何时被打开和关闭。我们将在蜘蛛完成工作后发送带有电影的电子邮件。</p><p id="34a9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们从<code class="fe ni nj nk my b">scrapy.exceptions</code>模块导入方法<code class="fe ni nj nk my b">NotConfigured</code>。当在<strong class="kf ir"> <em class="lb"> settings.py </em> </strong>文件中没有配置扩展名时，会出现这个问题。具体来说，参数<code class="fe ni nj nk my b">MYEXT_ENABLED</code>必须设置为<code class="fe ni nj nk my b">True</code>。我们将在后面的代码中看到这一点。</p><p id="0401" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">导入<code class="fe ni nj nk my b">smtplib</code>包以便能够发送电子邮件。我使用我的 Gmail 地址发送电子邮件，但是你可以修改 config.py 中的代码来使用另一个电子邮件服务。</p><p id="478c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们导入<code class="fe ni nj nk my b">sqlite3</code>包来从数据库中提取收视率最高的电影，并导入<code class="fe ni nj nk my b">config</code>来获取我们的常量。</p><h2 id="a951" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">在扩展中创建 SendEmail 类</h2><p id="9d96" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">首先，我们定义了<code class="fe ni nj nk my b">logger</code>对象。通过这个对象，我们可以在特定事件时将消息写入日志。然后我们用构造函数方法创建了<code class="fe ni nj nk my b">SendEmail</code>类。在构造函数中，我们将<code class="fe ni nj nk my b">FROMADDR</code>和<code class="fe ni nj nk my b">TOADDR</code>分配给类的相应属性。这些常量设置在<strong class="kf ir"> <em class="lb"> config.py </em> </strong>文件中。这两个属性我都用了我的 Gmail 地址。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="9075" class="nc ld iq my b gy nd ne l nf ng">logger = logging.getLogger(__name__)</span><span id="96f1" class="nc ld iq my b gy nl ne l nf ng">class SendEmail(object):<br/>    def __init__(self):<br/>        self.fromaddr = FROMADDR<br/>        self.toaddr  = TOADDR</span></pre><h2 id="6c13" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">实例化扩展对象</h2><p id="14bf" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated"><code class="fe ni nj nk my b">SendEmail</code>对象的第一个方法是<code class="fe ni nj nk my b">from_crawler</code>。我们做的第一项检查是在 settings.py 文件中是否启用了<code class="fe ni nj nk my b">MYEXT_ENABLED</code>。如果不是这种情况，我们抛出一个<code class="fe ni nj nk my b">NotConfigured</code>异常。发生这种情况时，扩展中的其余代码不会被执行。</p><p id="e168" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<strong class="kf ir"> <em class="lb"> settings.py </em> </strong>文件中我们需要添加以下代码来启用这个扩展。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="839a" class="nc ld iq my b gy nd ne l nf ng">MYEXT_ENABLED = True<br/>EXTENSIONS = {<br/>    'topfilms.extensions.SendEmail': 500,<br/>    'scrapy.telnet.TelnetConsole': None<br/>}</span></pre><p id="4b98" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以我们将布尔标志<code class="fe ni nj nk my b">MYEXT_ENABLED</code>设置为<code class="fe ni nj nk my b">True</code>。然后我们将自己的扩展<code class="fe ni nj nk my b">SendEmail</code>添加到<code class="fe ni nj nk my b">EXTENSIONS</code>字典中。整数值 500 指定了扩展必须执行的顺序。我还不得不禁用了<code class="fe ni nj nk my b">TelnetConsole</code>。否则发送电子邮件不起作用。通过放置<code class="fe ni nj nk my b">None</code>而不是整数顺序值来禁用该扩展。</p><p id="ff4b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们用<code class="fe ni nj nk my b">cls()</code>函数实例化扩展对象。我们将一些<code class="fe ni nj nk my b">signals</code>连接到这个扩展对象。我们对<code class="fe ni nj nk my b">spider_opened</code>和<code class="fe ni nj nk my b">spider_closed</code>信号感兴趣。最后我们返回<code class="fe ni nj nk my b">ext</code>对象。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="9afe" class="nc ld iq my b gy nd ne l nf ng">@classmethod<br/>    def from_crawler(cls, crawler):<br/>        # first check if the extension should be enabled and raise<br/>        # NotConfigured otherwise<br/>        if not crawler.settings.getbool('MYEXT_ENABLED'):<br/>            raise NotConfigured</span><span id="ee88" class="nc ld iq my b gy nl ne l nf ng"># instantiate the extension object<br/>        ext = cls()</span><span id="7489" class="nc ld iq my b gy nl ne l nf ng"># connect the extension object to signals<br/>        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)<br/>        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)</span><span id="5ca4" class="nc ld iq my b gy nl ne l nf ng"># return the extension object<br/>        return ext</span></pre><h2 id="6d86" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">定义 spider_opened 事件中的操作</h2><p id="15c7" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">当蜘蛛被打开时，我们只是想把它写到日志中。因此，我们使用在代码顶部创建的<code class="fe ni nj nk my b">logger</code>对象。使用<code class="fe ni nj nk my b">info</code>方法，我们向日志中写入一条消息。<code class="fe ni nj nk my b">Spider.name</code>替换为我们在 TVGuideSpider.py 文件中定义的名称。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="36bb" class="nc ld iq my b gy nd ne l nf ng">def spider_opened(self, spider):<br/>        logger.info("opened spider %s", spider.name)</span></pre><h2 id="f416" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">在 spider_closed 事件后发送电子邮件</h2><p id="c6dd" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">在<code class="fe ni nj nk my b">SendEmail</code>类的最后一个方法中，我们发送包含顶级电影概述的电子邮件。</p><p id="3b61" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们再次向日志发送通知，告知蜘蛛已经关闭。其次，我们创建一个到 SQLite 数据库的连接，该数据库包含<strong class="kf ir"> <em class="lb"> ALLOWED_CHANNELS 下周的所有电影。</em> </strong>我们选择带有<code class="fe ni nj nk my b">rating &gt;= 6.5</code>的影片。您可以根据需要将评级更改为更高或更低的阈值。然后，生成的影片按格式为 YYYYMMDD 的<code class="fe ni nj nk my b">film_date_short</code>和开始时间<code class="fe ni nj nk my b">start_ts</code>排序。</p><p id="5f40" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们获取游标<code class="fe ni nj nk my b">cur</code>中的所有行，并检查是否有一些使用<code class="fe ni nj nk my b">len</code>函数的结果。例如，当您将阈值等级设置得太高时，可能会没有结果。</p><p id="be33" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用<code class="fe ni nj nk my b">for row in data</code>我们浏览每一个结果电影。我们从<code class="fe ni nj nk my b">row</code>中提取所有有趣的信息。对于一些数据，我们用<code class="fe ni nj nk my b">encode('ascii','ignore')</code>进行编码。这是为了忽略一些特殊字符，如é、à、è等。否则，我们会在发送电子邮件时出错。</p><p id="65d9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当收集了关于电影的所有数据后，我们编写了一个字符串变量<code class="fe ni nj nk my b">topfilm</code>。每个<code class="fe ni nj nk my b">topfilm</code>然后连接到变量<code class="fe ni nj nk my b">topfilms_overview</code>，这将是我们发送的电子邮件的信息。如果我们的查询结果中没有电影，我们会在短消息中提到这一点。</p><p id="79b7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，由于有了<code class="fe ni nj nk my b">smtplib</code>包，我们用 Gmail 地址发送邮件。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="110e" class="nc ld iq my b gy nd ne l nf ng">def spider_closed(self, spider):<br/>        logger.info("closed spider %s", spider.name)</span><span id="efb1" class="nc ld iq my b gy nl ne l nf ng"># Getting films with a rating above a threshold<br/>        topfilms_overview = ""<br/>        con = lite.connect('topfilms.db')<br/>        cur = con.execute("SELECT title, channel, start_ts, film_date_long, plot, genre, release_date, rating, tmdb_link, nb_votes "<br/>                          "FROM topfilms "<br/>                          "WHERE rating &gt;= 6.5 "<br/>                          "ORDER BY film_date_short, start_ts")</span><span id="2527" class="nc ld iq my b gy nl ne l nf ng">data=cur.fetchall()</span><span id="9cec" class="nc ld iq my b gy nl ne l nf ng">if len(data) &gt; 0:  # Check if we have records in the query result<br/>            for row in data:<br/>                title = row[0].encode('ascii', 'ignore')<br/>                channel = row[1]<br/>                start_ts = row[2]<br/>                film_date_long = row[3]<br/>                plot = row[4].encode('ascii', 'ignore')<br/>                genre = row[5]<br/>                release_date = row[6].rstrip()<br/>                rating = row[7]<br/>                tmdb_link = row[8]<br/>                nb_votes = row[9]<br/>                topfilm = ' - '.join([title, channel, film_date_long, start_ts])<br/>                topfilm = topfilm + "\r\n" + "Release date: " + release_date<br/>                topfilm = topfilm + "\r\n" + "Genre: " + str(genre)<br/>                topfilm = topfilm + "\r\n" + "TMDB rating: " + rating + " from " + nb_votes + " votes"<br/>                topfilm = topfilm + "\r\n" + plot<br/>                topfilm = topfilm + "\r\n" + "More info on: " + tmdb_link<br/>                topfilms_overview = "\r\n\r\n".join([topfilms_overview, topfilm])</span><span id="3ecd" class="nc ld iq my b gy nl ne l nf ng">con.close()</span><span id="afd6" class="nc ld iq my b gy nl ne l nf ng">if len(topfilms_overview) &gt; 0:<br/>            message = topfilms_overview<br/>        else:<br/>            message = "There are no top rated films for the coming week."</span><span id="fb24" class="nc ld iq my b gy nl ne l nf ng">msg = "\r\n".join([<br/>          "From: " + self.fromaddr,<br/>          "To: " + self.toaddr,<br/>          "Subject: Top Films Overview",<br/>          message<br/>          ])<br/>        username = UNAME<br/>        password = PW<br/>        server = smtplib.SMTP(GMAIL)<br/>        server.ehlo()<br/>        server.starttls()<br/>        server.login(username,password)<br/>        server.sendmail(self.fromaddr, self.toaddr, msg)<br/>        server.quit()</span></pre><h2 id="e5f6" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">通过分机发送电子邮件的结果</h2><p id="46dc" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">这段代码的最终结果是在你的邮箱里有一个顶级电影的概览。太好了！现在，您不必再在在线电视指南上查找这些内容了。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/aee6ff1ea7962f9a8d66f72eef1da191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/0*SuRZuKi2RIkRJD3y"/></div></figure><h1 id="1bfb" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">规避知识产权禁令的技巧</h1><p id="3135" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">当你在短时间内发出许多请求时，你就有被服务器禁止的风险。在这最后一部分，我将向你展示一些避免 IP 封禁的技巧。</p><h2 id="0e29" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">推迟你的请求</h2><p id="8cb5" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">避免 IP 禁止的一个简单方法是在每个请求 之间<strong class="kf ir"> <em class="lb">暂停。在 Scrapy 中，只需在<strong class="kf ir"> <em class="lb"> settings.py </em> </strong>文件中设置一个参数即可。您可能已经注意到，settings.py 文件中有很多参数被注释掉了。</em></strong></p><p id="344e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">搜索参数<code class="fe ni nj nk my b">DOWNLOAD_DELAY</code>并取消注释。我将<strong class="kf ir"> <em class="lb">暂停长度设置为 2 秒</em> </strong>。根据您必须发出的请求数量，您可以更改这一点。但我会把它设置为至少 1 秒。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="c146" class="nc ld iq my b gy nd ne l nf ng">DOWNLOAD_DELAY=2</span></pre><h2 id="f159" class="nc ld iq bd le nm nn dn li no np dp lm ko nq nr lq ks ns nt lu kw nu nv ly nw bi translated">避免 IP 封禁的更高级方法</h2><p id="7e80" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">默认情况下，每次你做一个请求，你用<strong class="kf ir"> <em class="lb">同一个用户代理</em> </strong>来做。由于有了包<code class="fe ni nj nk my b">fake_useragent</code>，我们可以很容易地为每个请求改变用户代理。</p><p id="4af4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这段代码的所有功劳归于 Alecxe，他写了一个很好的 python 脚本来使用 fake_useragent 包。</p><p id="25eb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们在 web scraper 项目的根目录下创建一个文件夹<strong class="kf ir"><em class="lb">scrapy _ fake _ user agent</em></strong>。在这个文件夹中，我们添加了两个文件:</p><ul class=""><li id="7763" class="mf mg iq kf b kg kh kk kl ko mh ks mi kw mj la mk ml mm mn bi translated"><strong class="kf ir"> <em class="lb"> __init__。py </em> </strong>是一个空文件</li><li id="9457" class="mf mg iq kf b kg mo kk mp ko mq ks mr kw ms la mk ml mm mn bi translated"><strong class="kf ir">T33】middleware . pyT35】</strong></li></ul><p id="e235" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要使用这个<a class="ae kc" href="http://doc.scrapy.org/en/latest/topics/spider-middleware.html" rel="noopener ugc nofollow" target="_blank">中间件</a>，我们需要在<strong class="kf ir"> <em class="lb"> settings.py </em> </strong>文件中启用它。这是通过代码完成的:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="cd95" class="nc ld iq my b gy nd ne l nf ng">DOWNLOADER_MIDDLEWARES = {<br/>    'scrapy.downloadermiddleware.useragent.UserAgentMiddleware': None,<br/>    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,<br/>}</span></pre><p id="f6ec" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们通过指定<em class="lb"> None </em>而不是一个整数值来禁用 Scrapy 的默认值<code class="fe ni nj nk my b">UserAgentMiddleware</code>。然后我们启用自己的中间件<code class="fe ni nj nk my b">RandomUserAgentMiddleware</code>。直观地说，中间件是在请求 期间<strong class="kf ir"> <em class="lb">执行的一段代码。</em></strong></p><p id="5898" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在文件<strong class="kf ir"><em class="lb">middleware . py</em></strong>中，我们为每个请求添加代码到<strong class="kf ir"> <em class="lb">随机化用户代理</em> </strong> <em class="lb"> </em>。确保您安装了 fake_useragent 包。从<a class="ae kc" href="https://pypi.python.org/pypi/fake-useragent" rel="noopener ugc nofollow" target="_blank">fake _ user agent 包</a>中，我们导入了<code class="fe ni nj nk my b">UserAgent</code>模块。这包含了<strong class="kf ir"> <em class="lb">不同用户代理</em> </strong>的列表。在 RandomUserAgentMiddleware 类的构造函数中，我们实例化了 UserAgent 对象。在方法<strong class="kf ir"><em class="lb">process _ request</em></strong>中，我们将用户代理设置为来自<code class="fe ni nj nk my b">ua</code>对象的随机用户代理。在请求的报头中。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="b2fb" class="nc ld iq my b gy nd ne l nf ng">from fake_useragent import UserAgent</span><span id="c3c8" class="nc ld iq my b gy nl ne l nf ng">class RandomUserAgentMiddleware(object):<br/>    def __init__(self):<br/>        super(RandomUserAgentMiddleware, self).__init__()</span><span id="6bc4" class="nc ld iq my b gy nl ne l nf ng">self.ua = UserAgent()</span><span id="e5d8" class="nc ld iq my b gy nl ne l nf ng">def process_request(self, request, spider):<br/>        request.headers.setdefault('User-Agent', self.ua.random)</span></pre><h1 id="8e54" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">结论</h1><p id="499b" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">就是这样！我希望你现在对如何在你的网页抓取项目中使用 Scrapy 有一个清晰的认识。</p></div></div>    
</body>
</html>