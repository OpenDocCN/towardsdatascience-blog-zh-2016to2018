<html>
<head>
<title>Overview of Text Similarity Metrics in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中文本相似性度量概述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50?source=collection_archive---------1-----------------------#2018-05-15">https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50?source=collection_archive---------1-----------------------#2018-05-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1aa5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Jaccard 指数和余弦相似度——应该在哪里使用，各有利弊。</h2></div><p id="8d8c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在研究搜索引擎的自然语言模型时，我经常会问这样的问题“这两个词有多相似？”、“这两句话有多像？”，“这两个文档有多相似？”。我已经在之前的帖子中谈到过<a class="ae le" rel="noopener" target="_blank" href="/how-to-train-custom-word-embeddings-using-gpu-on-aws-f62727a1e3f6">自定义单词嵌入，其中单词的意思被考虑到单词的相似性。在这篇博文中，我们将更多地研究句子或文档相似性的技术。</a></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/a08c81bfbf2312c45138525131a7b989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6v8wsFYnk2clsbsv9jeW3A.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">How do we make sense of all this text around us?</figcaption></figure><p id="60f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有一些文本相似性度量，但我们将查看最常见的 Jaccard 相似性和余弦相似性。</p><h1 id="ba92" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak"> Jaccard 相似度:</strong></h1><p id="cf10" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Jaccard_index" rel="noopener ugc nofollow" target="_blank">雅克卡相似度</a>或并集上的交集被定义为<strong class="kk iu">交集的大小除以两个集合的并集的大小。</strong>让我们以两个句子为例:</p><p id="5d0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">句子 1: </strong> AI 是我们的朋友，它一直很友好<br/> <strong class="kk iu">句子 2: </strong> AI 和人类一直很友好</p><p id="ff14" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使用 Jaccard 相似度计算相似度，我们将首先执行<strong class="kk iu">词条化</strong>以将单词减少到相同的根单词。在我们的例子中，“朋友”和“友好的”都将成为“朋友”，“有”和“有”都将成为“有”。画出我们得到的两个句子的维恩图:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/5feddb824eefce7b2e4029b345e75d18.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*u2ZZPh5er5YbmOg7k-s0-A.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Venn Diagram of the two sentences for Jaccard similarity</figcaption></figure><p id="589f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于上面两个句子，我们得到 Jaccard 相似度为<strong class="kk iu"> 5/(5+3+2) = 0.5 </strong>，这是集合的交集的大小除以集合的总大小。<br/>Python 中 Jaccard 相似度的代码为:</p><pre class="lg lh li lj gt mt mu mv mw aw mx bi"><span id="5dba" class="my lw it mu b gy mz na l nb nc">def get_jaccard_sim(str1, str2): <br/>    a = set(str1.split()) <br/>    b = set(str2.split())<br/>    c = a.intersection(b)<br/>    return float(len(c)) / (len(a) + len(b) - len(c))</span></pre><p id="ff61" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里要注意的一点是，由于我们使用集合，“朋友”在句子 1 中出现了两次，但这并不影响我们的计算——这将随着余弦相似度而改变。</p><h1 id="fb31" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">余弦相似度:</h1><p id="7a3e" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">余弦相似度</a>通过测量<strong class="kk iu">两个矢量</strong>之间角度的余弦来计算相似度。计算方法如下:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/84447fc68d0048c6faf3df79fcda395b.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*hub04IikybZIBkSEcEOtGA.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Cosine Similarity calculation for two vectors A and B [<a class="ae le" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">source</a>]</figcaption></figure><p id="30f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">用余弦相似度，我们需要把句子转换成向量</strong>。一种方法是使用带有 TF (术语频率)<strong class="kk iu">或 TF-IDF </strong>(术语频率-逆文档频率)的<strong class="kk iu">单词包。TF 或<a class="ae le" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>的选择取决于应用，与余弦相似性的实际执行方式无关，余弦相似性只需要向量。<em class="ne"> TF 一般对文本相似性比较好，但 TF-IDF 对搜索查询相关性比较好。</em></strong></p><p id="e18b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一种方法是使用<a class="ae le" href="https://machinelearningmastery.com/develop-word-embeddings-python-gensim/" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>或我们自己定制的单词嵌入将单词转换成向量。在之前的<a class="ae le" rel="noopener" target="_blank" href="/how-to-train-custom-word-embeddings-using-gpu-on-aws-f62727a1e3f6">帖子</a>中，我谈到了训练我们自己的自定义单词嵌入。</p><p id="10b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">具有单词包和单词嵌入 tf/ tf-idf 之间有两个主要区别:<br/> 1 .tf / tf-idf 为每个单词创建一个数字，单词嵌入通常为每个单词创建一个向量。<br/> 2。tf / tf-idf 对于分类文档整体来说是好的，但是单词嵌入对于识别上下文内容是好的。</p><p id="5477" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们计算这两个句子的余弦相似度:</p><p id="1ce3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">句子 1: </strong>人工智能是我们的朋友，它一直很友好<br/> <strong class="kk iu">句子 2: </strong>人工智能和人类一直很友好</p><p id="2cb5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第 1 步</strong>，我们将使用单词袋来计算词频:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nf"><img src="../Images/fc57f752e93e1c391d9e8ed3d52109c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gW4O7JslixizXwVlIFW1uA.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Term Frequency after lemmatization of the two sentences</figcaption></figure><p id="9b36" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第二步，</strong>上面显示的词频计数的主要问题是，它倾向于较长的文档或句子。解决这个问题的一个方法是<strong class="kk iu">用各自的量值或<a class="ae le" href="https://en.wikipedia.org/wiki/Norm_(mathematics)" rel="noopener ugc nofollow" target="_blank"> L2 规范</a>来归一化</strong>术语频率。对每个频率的平方求和并取平方根，句子 1 的 L2 范数是 3.3166，句子 2 是 2.6458。将上述频率除以这些标准，我们得到:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ng"><img src="../Images/57b539f87e2a6e0da06d1ea213f58cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*isbRAoH1WrNzI78M9gcOFA.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Normalization of term frequencies using L2 Norms</figcaption></figure><p id="d8c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">步骤 3，</strong>由于我们已经将两个向量归一化为长度为 1，所以可以用点积计算余弦相似度:<br/>余弦相似度=(0.302 * 0.378)+(0.603 * 0.378)+(0.302 * 0.378)+(0.302 * 0.378)+(0.302 * 0.378)= 0.684</p><p id="e500" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，两个句子的余弦相似度是<strong class="kk iu"> 0.684 </strong>，这不同于完全相同的两个句子的 Jaccard 相似度是<strong class="kk iu"> 0.5 </strong>(上面计算的)</p><p id="f02a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Python 中字符串成对余弦相似性的代码是:</p><pre class="lg lh li lj gt mt mu mv mw aw mx bi"><span id="d977" class="my lw it mu b gy mz na l nb nc">from collections import Counter<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.metrics.pairwise import cosine_similarity<br/>def get_cosine_sim(*strs): <br/>    vectors = [t for t in get_vectors(*strs)]<br/>    return cosine_similarity(vectors)<br/>    <br/>def get_vectors(*strs):<br/>    text = [t for t in strs]<br/>    vectorizer = CountVectorizer(text)<br/>    vectorizer.fit(text)<br/>    return vectorizer.transform(text).toarray()</span></pre><h2 id="a1e9" class="my lw it bd lx nh ni dn mb nj nk dp mf kr nl nm mh kv nn no mj kz np nq ml nr bi translated"><strong class="ak">雅克卡相似度和余弦相似度的区别:</strong></h2><ol class=""><li id="06a1" class="ns nt it kk b kl mn ko mo kr nu kv nv kz nw ld nx ny nz oa bi translated">Jaccard 相似度对于每个句子/文档只取<strong class="kk iu">唯一的一组单词</strong>，而余弦相似度取<strong class="kk iu">向量的总长度</strong>。(这些向量可以由词项频率包或 tf-idf 构成)</li><li id="30bf" class="ns nt it kk b kl ob ko oc kr od kv oe kz of ld nx ny nz oa bi translated">这意味着，如果你在句子 1 中多次重复单词“朋友”，余弦相似度<strong class="kk iu">会改变</strong>，但 Jaccard 相似度不会改变。对于 ex，如果单词“朋友”在第一句中重复 50 次，余弦相似性下降到 0.4，但是 Jaccard 相似性保持在 0.5。</li><li id="3e56" class="ns nt it kk b kl ob ko oc kr od kv oe kz of ld nx ny nz oa bi translated">Jaccard 相似性适用于重复不重要的情况，余弦相似性适用于在分析文本相似性时重复很重要的情况。对于两个产品描述，使用 Jaccard 相似度会更好，因为一个词的重复不会降低它们的相似度。</li></ol><p id="c37e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你知道每一个的更多应用，请在下面的评论中提及，因为这将帮助其他人。我的关于文本相似性度量概述的博客到此结束。祝你在自己的文本探索中好运！</p><p id="1a35" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我发现的关于信息检索主题的最好的书之一是<a class="ae le" href="https://www.amazon.com/dp/0521865719/?tag=omnilence-20" rel="noopener ugc nofollow" target="_blank">信息检索简介</a>，这是一本涵盖了许多关于 NLP、信息检索和搜索概念的极好的书。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/1d402cc4c8ffa19a92d59a91190761e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*OhmC-7bT6QeA1aOZ3vN2aQ.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk"><a class="ae le" href="https://www.amazon.com/dp/0521865719/?tag=omnilence-20" rel="noopener ugc nofollow" target="_blank">One of the best books on this topic: Intro To Information Retrieval</a></figcaption></figure><p id="5b90" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另外，看看我的播客吧！我有一个播客叫做“<a class="ae le" href="https://podcasts.apple.com/us/podcast/the-data-life-podcast/id1453716761" rel="noopener ugc nofollow" target="_blank">数据生活播客</a>”。你可以在任何地方听到你的播客。在这一集<a class="ae le" href="https://open.spotify.com/episode/6ju3WreKQUtvcKvL9j76l9" rel="noopener ugc nofollow" target="_blank">中</a>你将听到与 Paul Azunre(曼宁著作<a class="ae le" href="https://www.manning.com/books/transfer-learning-for-natural-language-processing?a_aid=Omnilence&amp;a_bid=d53fed17" rel="noopener ugc nofollow" target="_blank">NLP 中的迁移学习</a>的作者)关于 BERT、Elmo、单词嵌入等趋势的有趣对话。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oh"><img src="../Images/9994ca28523af53df668a8c22309a141.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cYohSuAFupyrtFL2sRKnRw.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">My podcast <a class="ae le" href="https://open.spotify.com/episode/6ju3WreKQUtvcKvL9j76l9" rel="noopener ugc nofollow" target="_blank">episode</a> with Paul Azunre on transfer learning and NLP</figcaption></figure><p id="505d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您有任何问题，请在我的<a class="ae le" href="https://www.linkedin.com/in/sanketgupta107/" rel="noopener ugc nofollow" target="_blank"> LinkedIn 个人资料</a>中给我留言。感谢阅读！</p></div></div>    
</body>
</html>