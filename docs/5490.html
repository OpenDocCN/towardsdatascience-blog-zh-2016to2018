<html>
<head>
<title>A gentle introduction to OCR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OCR 简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa?source=collection_archive---------2-----------------------#2018-10-22">https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa?source=collection_archive---------2-----------------------#2018-10-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/3e154a250ccf387fa88c494a9161a89e.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*77Hn7O_KROhi37Gt7Aq67Q.png"/></div></figure><blockquote class="ju"><p id="4457" class="jv jw iq bd jx jy jz ka kb kc kd ke dk translated">想了解更多？参观<a class="ae kf" href="https://www.shibumi-ai.com/" rel="noopener ugc nofollow" target="_blank">www.Shibumi-ai.com</a></p><p id="8b13" class="jv jw iq bd jx jy kg kh ki kj kk ke dk translated">在这里阅读<a class="ae kf" rel="noopener" target="_blank" href="/ocr-101-all-you-need-to-know-e6a5c5d5875b">这个帖子的</a><strong class="ak">重访</strong>版本</p></blockquote><h1 id="5937" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">介绍</h1><p id="c413" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">光学字符识别(OCR)是最早解决的计算机视觉任务之一，因为在某些方面它不需要深度学习。因此，在 2012 年深度学习热潮之前就有不同的 OCR 实现，有些甚至可以追溯到 1914 年(！).</p><p id="d0fa" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">这让很多人以为 OCR 挑战是<strong class="ll ir">“解决了</strong>”，就不再有挑战性了。另一个来自类似来源的信念是，OCR 不需要<strong class="ll ir">深度学习</strong>，或者换句话说，使用深度学习进行 OCR 是一种矫枉过正。</p><p id="d1fc" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">任何实践计算机视觉或一般意义上的机器学习的人都知道，没有所谓的已解决的任务，这个案例也不例外。相反，OCR 仅在非常具体的用例上产生非常好的结果，但是一般来说，它仍然被认为是具有挑战性的。</p><p id="70df" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">此外，对于某些不需要深度学习的 OCR 任务，确实有很好的解决方案。然而，要真正迈向更好、更通用的解决方案，深度学习将是强制性的。</p><h2 id="f0e8" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">我为什么要写 OCR？</h2><p id="5700" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">像我的许多作品/文章一样，这也是作为客户的项目开始的。我被要求解决一个特定的 OCR 任务。</p><p id="2874" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">在从事这项工作期间和之后，我得出了一些值得分享的结论和见解。此外，在集中精力完成一项任务后，很难停下来扔掉它，所以我继续我的研究，并希望获得一个更好、更通用的解决方案。</p><h2 id="be65" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">你会在这里找到什么</h2><p id="5c98" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">在这篇文章中，我将探索一些用于解决不同 OCR 任务的<strong class="ll ir">策略</strong>、<strong class="ll ir">方法</strong>和<strong class="ll ir">逻辑</strong>，并分享一些有用的方法。在最后一部分，我们将用代码解决一个现实世界的问题。这不应该被认为是一个详尽的评论(不幸的是),因为这种方法的深度、历史和广度对于这种博客文章来说太广了。</p><p id="df3c" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">然而，和往常一样，我不会放过你参考文章、数据集、知识库和其他相关的博客文章。</p><h1 id="d197" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw mx ky kz la my lc ld le mz lg lh li bi translated">OCR 的类型</h1><p id="b507" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">如前所述，OCR 有不止一种含义。从最普遍的意义上来说，它指的是从每一个可能的图像中提取文本，无论是一本书的标准印刷页，还是一张随机的带有涂鸦的图像(<strong class="ll ir"> in the wild </strong>)。在这之间，你可能会发现许多其他任务，比如阅读<strong class="ll ir">车牌</strong>，无人机器人<strong class="ll ir">验证码</strong>，<strong class="ll ir">街道标志</strong>等。</p><p id="a216" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">尽管这些选项中的每一个都有自己的困难，但显然“在野外”的任务是最难的。</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/cabeb630ed5f9112f71a6bb39179cc0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*n6BoysCl4xSmMHcDwqK_ZA.png"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">Left: Printed text. Right: text in the wild</figcaption></figure><p id="de5f" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">从这些例子中，我们可以得出 OCR 任务的一些<strong class="ll ir">属性</strong>:</p><ul class=""><li id="a33e" class="nj nk iq ll b lm mg lq mh lu nl ly nm mc nn ke no np nq nr bi translated"><strong class="ll ir">文字密度</strong>:在打印/书写的页面上，文字密度大。然而，给定一个只有一个街道标志的街道图像，文本是稀疏的。</li><li id="bae1" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke no np nq nr bi translated"><strong class="ll ir">文本的结构</strong>:页面上的文本是结构化的，大部分是严格的行，而野生的文本可能会以不同的旋转方式散落在各处。</li><li id="8327" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke no np nq nr bi translated">字体:印刷字体更容易，因为它们比嘈杂的手写字符更有结构。</li><li id="85ba" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke no np nq nr bi translated"><strong class="ll ir">字符类型:</strong>文本可能以不同的语言出现，这些语言可能彼此非常不同。此外，文本的结构可能不同于数字，例如门牌号等。</li><li id="1908" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke no np nq nr bi translated"><strong class="ll ir">伪像</strong>:很明显，室外照片比舒适的扫描仪要嘈杂得多。</li><li id="77fd" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke no np nq nr bi translated"><strong class="ll ir">位置</strong>:一些任务包括裁剪/居中的文本，而在其他情况下，文本可能位于图像中的随机位置。</li></ul><h1 id="aa2c" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw mx ky kz la my lc ld le mz lg lh li bi translated">数据集/任务</h1><h2 id="7332" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">SVHN</h2><p id="8bb0" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">一个很好的起点是街景门牌号数据集。顾名思义，这是从谷歌街景中提取的一组门牌号数据。任务难度中等。数字以各种形状和书写风格出现，然而，每个门牌号位于图像的中间，因此不需要检测。这些图像的分辨率不是很高，它们的排列可能有点奇特。</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi nx"><img src="../Images/ad556fd191e2521a97c2a4b8a21e8e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wa6uLBFTeg9rcSoC"/></div></div></figure><h2 id="88a3" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">车牌</h2><p id="bed6" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">另一个常见的挑战是车牌识别，这在实践中不是很难，也不是很有用。这个任务和大多数 OCR 任务一样，需要检测车牌，然后识别它的<strong class="ll ir">字符</strong>。由于车牌的形状相对恒定，一些方法在实际识别数字之前使用简单的整形方法。以下是一些来自网络的例子:</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi oc"><img src="../Images/06d24c9280c91515a6f8d629eef1b26e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3cGk3lVSAiaRAnPCqDCEEw.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">OpenALPR example. with car type a s abonus</figcaption></figure><ol class=""><li id="6a75" class="nj nk iq ll b lm mg lq mh lu nl ly nm mc nn ke od np nq nr bi translated">OpenALPR 是一个非常强大的工具，不需要深度学习，可以识别不同国家的车牌</li><li id="fb5f" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke od np nq nr bi translated">这个<a class="ae kf" href="https://github.com/qjadud1994/CRNN-Keras" rel="noopener ugc nofollow" target="_blank"> repo </a>提供了 CRNN 模型的实现(将进一步讨论)来识别韩国车牌。</li><li id="9488" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke od np nq nr bi translated">监督. ly，一家数据应用公司，<a class="ae kf" rel="noopener" target="_blank" href="/number-plate-detection-with-supervisely-and-tensorflow-part-1-e84c74d4382c">写了</a>关于使用他们的工具生成的人工数据来训练一个车牌识别器(人工数据也将被进一步讨论)</li></ol><h2 id="3195" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">验证码</h2><p id="7009" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">由于互联网上到处都是机器人，区分它们和真正人类的常见做法是视觉任务，特别是文本阅读，又名验证码。这些文本中的许多是随机的和扭曲的，这应该使计算机更难阅读。我不确定开发验证码的人是否预测到了计算机视觉的进步，但是今天大多数文本验证码并不难解决，尤其是如果我们不试图一次解决所有的验证码。</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/25084799112065559ac530d44580cf15.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*sRzszQ-SBtcGW3KyQwJtEQ.png"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">Facebook knows how to make challenging CAPTCHAs</figcaption></figure><p id="25a7" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">亚当·盖特基提供了一个<a class="ae kf" href="https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710" rel="noopener">不错的教程</a>，用深度学习解决一些验证码，其中包括再次合成人工数据。</p><h2 id="1181" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">PDF OCR</h2><p id="a370" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">OCR 最常见的情况是印刷/pdf OCR。打印文档的结构化特性使得解析它们变得更加容易。大多数 OCR 工具(例如<a class="ae kf" href="https://github.com/tesseract-ocr/" rel="noopener ugc nofollow" target="_blank"> Tesseract </a>)主要是为了解决这个任务，并获得良好的结果。因此，我不会在这篇文章中对这个任务做过多的阐述。</p><h2 id="eb27" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">野外的 OCR</h2><p id="2fc0" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">这是最具挑战性的 OCR 任务，因为它将所有一般的计算机视觉挑战(如噪声、光照和伪像)引入 OCR。此任务的一些相关数据集是<a class="ae kf" href="https://vision.cornell.edu/se3/coco-text-2/" rel="noopener ugc nofollow" target="_blank"> coco-text </a>和<a class="ae kf" href="http://tc11.cvc.uab.es/datasets/SVT_1" rel="noopener ugc nofollow" target="_blank"> SVT </a>数据集，它们再次使用街景图像来提取文本。</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi of"><img src="../Images/d0fbb34eb90018f01bf026cc55d7ebf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*h1CDmAxHf1Ee0illDor2HA.png"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">COCO text example</figcaption></figure><h2 id="06b0" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">合成文本</h2><p id="97de" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">SynthText 不是一个数据集，甚至可能不是一个任务，但提高训练效率的一个好主意是人工数据生成。由于文本的扁平性质，在图像上随意添加字符或单词看起来比其他任何物体都要自然。</p><p id="9a53" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">我们之前已经看到了一些为简单任务生成的数据，比如验证码和车牌。在野外生成文本稍微复杂一点。该任务包括考虑图像的深度信息。幸运的是，SynthText 是一个很好的作品，它接收带有上述注释的图像，并智能地散布文字(来自新闻组数据集)。</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi og"><img src="../Images/3bb2fb0c7dbf000f3bb5ab05a84ce2d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*GLFRyZ99LJHmCewUqhKLdA.png"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">SynthText process illustration: top right is the segmentation of an image, bottom right is the depth data. Bottom left is a surface analyses of the image, which according to text is sprinkled on the image.</figcaption></figure><p id="8156" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">为了使“分散”的文本看起来真实而有用，SynthText 库为每幅图像设置了两个遮罩，一个是深度遮罩，另一个是分段遮罩。如果您喜欢使用自己的图像，也应该添加这些数据</p><ul class=""><li id="b99b" class="nj nk iq ll b lm mg lq mh lu nl ly nm mc nn ke no np nq nr bi translated">建议查看<a class="ae kf" href="https://github.com/ankush-me/SynthText" rel="noopener ugc nofollow" target="_blank"> repo </a>，自己生成一些图像。你应该注意，回购使用了 opencv 和 maptlotlib 的一些过时版本，所以一些修改可能是必要的。</li></ul><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/0d06b53f181a9f602de73f962328eef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/0*xs7V0gNwOdgfJkP6"/></div></figure><h2 id="3abd" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">Mnist</h2><p id="7be3" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">虽然这并不是一个真正的 OCR 任务，但是要写 OCR 而不包括 Mnist 例子是不可能的。最广为人知的计算机视觉挑战实际上并不是一项经过深思熟虑的 OCR 任务，因为它一次只包含一个字符(数字)，而且只有 10 个数字。然而，它可能暗示了为什么 OCR 被认为是容易的。此外，在一些方法中，每个字母将被单独检测，然后 Mnist like(分类)模型变成 relevantץ</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/28d906d9df33254b9e2538eb2a04adcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/0*JdoUIidwCrrUZPPc"/></div></figure><h1 id="4dd0" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw mx ky kz la my lc ld le mz lg lh li bi translated">战略</h1><p id="dc14" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">正如我们已经看到和暗示的，文本识别主要是一个两步任务。首先，您希望<strong class="ll ir">检测</strong>图像中的文本外观，可能是密集的(如在打印的文档中)或稀疏的(如在野外的文本)。</p><p id="e06b" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">在检测行/字级别之后，我们可以再次从一大组解决方案中进行选择，这些解决方案通常来自三种主要方法:</p><ol class=""><li id="ce04" class="nj nk iq ll b lm mg lq mh lu nl ly nm mc nn ke od np nq nr bi translated">经典的计算机视觉技术。</li><li id="229a" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke od np nq nr bi translated">专业化的深度学习。</li><li id="f73f" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke od np nq nr bi translated">标准深度学习方法(检测)。</li></ol><p id="94e1" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">让我们逐一检查一下:</p><h2 id="4016" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">1.经典的计算机视觉技术</h2><p id="e187" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">如前所述，计算机视觉解决各种文字识别问题由来已久。你可以在网上找到很多例子:</p><ul class=""><li id="e1ae" class="nj nk iq ll b lm mg lq mh lu nl ly nm mc nn ke no np nq nr bi translated">伟大的<strong class="ll ir">阿德里安·罗斯布鲁克</strong>在他的网站上有大量的教程，像这个<a class="ae kf" href="https://www.pyimagesearch.com/2017/07/17/credit-card-ocr-with-opencv-and-python/" rel="noopener ugc nofollow" target="_blank">一个</a>，这个<a class="ae kf" href="https://www.pyimagesearch.com/2017/07/24/bank-check-ocr-with-opencv-and-python-part-i/" rel="noopener ugc nofollow" target="_blank">一个</a>和<a class="ae kf" href="https://www.pyimagesearch.com/category/optical-character-recognition-ocr/" rel="noopener ugc nofollow" target="_blank">更多的</a>。</li><li id="5bf5" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke no np nq nr bi translated"><strong class="ll ir">堆栈溢出</strong>也有一些像<a class="ae kf" href="https://stackoverflow.com/questions/9413216/simple-digit-recognition-ocr-in-opencv-python" rel="noopener ugc nofollow" target="_blank">这个</a>一样的宝石。</li></ul><p id="8c06" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">经典 CV 方法通常声称:</p><ol class=""><li id="1d33" class="nj nk iq ll b lm mg lq mh lu nl ly nm mc nn ke od np nq nr bi translated">应用<strong class="ll ir">滤镜</strong>让人物从背景中凸显出来。</li><li id="0ff0" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke od np nq nr bi translated">应用<strong class="ll ir">轮廓检测</strong>逐个识别字符。</li><li id="bbda" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke od np nq nr bi translated">应用<strong class="ll ir">图像分类</strong>识别字符</li></ol><p id="2b5b" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">显然，如果第二部分做得好，第三部分很容易，无论是模式匹配还是机器学习(例如 Mnist)。</p><p id="e4ce" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">然而，轮廓检测对于概括来说是相当具有挑战性的。它需要大量的手动微调，因此在大多数问题中变得不可行。例如，让我们将一个简单的计算机视觉脚本从<a class="ae kf" href="http://scikit-image.org/docs/dev/auto_examples/segmentation/plot_label.html#sphx-glr-download-auto-examples-segmentation-plot-label-py" rel="noopener ugc nofollow" target="_blank">应用到来自 SVHN 数据集的一些图像上。第一次尝试我们可能会取得很好的结果:</a></p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/cefc152e87b7b4d6acd76c017110e1c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/0*R6r8eGNyoylNVj4G"/></div></figure><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/49321efd08810d889f14abb2283f99ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/0*PMmcCq_CtcwIYg0x"/></div></figure><p id="7dca" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">但是当角色之间的距离越来越近时，事情就开始变得不可收拾了:</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3f0bf07018cf9edc43723679d2aecf39.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/0*HwWJowf1UobFjbNR"/></div></figure><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/56ca860a83a132b9555a1598db04f81b.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/0*lVny6F7mu7wzhFAG"/></div></figure><p id="0db6" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">我艰难地发现，当你开始摆弄参数时，你可能会减少这样的错误，但不幸的是会引起其他错误。换句话说，如果你的任务不简单，这些方法就不合适。</p><h2 id="940f" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">2.专门的深度学习方法</h2><p id="2cba" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">大多数成功的深度学习方法都在通用性方面表现出色。然而，考虑到上述属性，专用网络可能非常有用。</p><p id="1189" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">我将在这里检查一些著名方法的非详尽示例，并对介绍它们的文章做一个快速总结。与往常一样，每篇文章都以“任务 X(文本识别)最近获得关注”为开头，并继续详细描述他们的方法。仔细阅读这些文章会发现，这些方法是从以前的深度学习/文本识别工作中收集的。</p><p id="32dd" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">结果也描述得很透彻，但是由于设计上的许多差异(包括数据集的微小差异)，实际比较是不可能的。真正了解这些方法在您的任务中的性能的唯一方法，是获取它们的代码(最好到最坏:找到<strong class="ll ir">官方的</strong>回购，找到<strong class="ll ir">非官方但评价很高的</strong>回购，<strong class="ll ir">自己实现</strong>)并在您的数据上尝试。</p><p id="42ba" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">因此，我们总是更喜欢附有好的回购的文章，如果可能的话，甚至是演示。</p><p id="b9cd" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated"><strong class="ll ir">东</strong></p><p id="583c" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated"><a class="ae kf" href="https://arxiv.org/pdf/1704.03155.pdf" rel="noopener ugc nofollow" target="_blank"> EAST </a>(高效准确的场景文本检测器)是一种简单而强大的<strong class="ll ir">文本检测方法</strong>。使用专门的网络。</p><p id="5de3" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">与我们将要讨论的其他方法不同，只限于文本检测(不是实际的识别)，但是它的健壮性值得一提。<br/>另一个优势是它也被添加到<strong class="ll ir"> open-CV </strong>库(从版本 4 开始)所以你可以很容易地使用它(见教程<a class="ae kf" href="https://www.pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/" rel="noopener ugc nofollow" target="_blank">这里</a>)。这个网络实际上是众所周知的 U 形网的一个版本，它可以很好地检测大小不同的特征。这个网络的底层前馈“stem”(如文中杜撰，见下图)可能非常— <strong class="ll ir">文中使用 PVANet </strong>，然而 opencv 实现使用<strong class="ll ir"> Resnet </strong>。显然，它也可以被预先训练(例如使用 imagenet)。与 U-Net 一样，特征是从网络的不同层次提取的。</p></div><div class="ab cl ol om hu on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="ij ik il im in"><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi os"><img src="../Images/8c3219d218fedc159402886060d18187.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*Kd5tAQp83XY2udt8BE1d0A.png"/></div></figure><p id="749d" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">最后，网络允许两种类型的输出旋转边界框:带有旋转角度(2X2+1 个参数)的标准边界框或“四边形”,它只是带有所有顶点坐标的旋转边界框。</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/87e0ee7de7522e353a91ceacf5919634.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*BVae8WaaVsznWP9cVHJotw.png"/></div></figure><p id="8a03" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">如果现实生活的结果就像上面的图片一样，那么识别这些文字并不需要太多的努力。然而，现实生活的结果并不完美。</p><p id="d370" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated"><strong class="ll ir"> CRNN </strong></p><p id="9bfc" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">卷积回归神经网络是 2015 年的一篇文章，其中提出了一种混合(或三混合？)端到端架构，旨在以三步方法捕获单词。</p><p id="c34f" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">其思想如下:第一层是标准的全卷积网络。网络的最后一层被定义为特征层，并划分为“特征列”。在下图中可以看到，每个这样的特征列是如何表示文本中的某一部分的。</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/9a07846c4a73528cb091ad4aab66df0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*5QhwZ7bChMaTsI8ZRvCY_w.png"/></div></figure><p id="234d" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">然后，特征列被输入到深度双向 LSTM 中，后者输出一个序列，用于查找字符之间的关系。</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/fd5da0717f0245b610c5bc212ba43345.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/0*nGWtig3Cd0Jma2nX"/></div></figure><p id="ef48" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">最后，第三部分是转录层。它的目标是获取杂乱的字符序列，其中一些字符是冗余的，而另一些是空白的，并使用概率方法来统一和解释它。</p><p id="e6d8" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">这种方法叫做<strong class="ll ir"> CTC 丢失</strong>，在这里可以读到<a class="ae kf" href="https://gab41.lab41.org/speech-recognition-you-down-with-ctc-8d3b558943f0" rel="noopener ugc nofollow" target="_blank">。该层可以与/或不与预定义的词典一起使用，这可以促进单词的预测。</a></p><p id="8cd4" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">本文在固定文本词典的情况下达到了高准确率(&gt; 95%)，在没有固定文本词典的情况下达到了不同的成功率。</p><h2 id="7ccd" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">STN-net/参见</h2><p id="66ff" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated"><a class="ae kf" href="https://arxiv.org/pdf/1712.05404.pdf" rel="noopener ugc nofollow" target="_blank">参见</a> —半监督的端到端场景文本识别，是 Christian Bartzi 的作品。他和他的同事应用真正的端到端策略来检测和识别文本。他们使用非常弱的监督(他们称之为半监督，与通常意义不同)。因为他们训练网络只使用<strong class="ll ir">文本注释</strong>(没有边界框)。这允许他们使用更多的数据，但使他们的训练过程非常具有挑战性，他们讨论了不同的技巧来使其工作，例如，不要在超过两行文本的图像上训练(至少在训练的第一阶段)。</p><p id="2a32" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">这份文件有一个更早的版本，叫做<a class="ae kf" href="https://arxiv.org/abs/1707.08831" rel="noopener ugc nofollow" target="_blank"> STN OCR </a>。在最后的论文中，研究人员改进了他们的方法和陈述，此外，由于结果的高质量，他们更加强调他们方法的通用性。</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi oc"><img src="../Images/fccbb7f5ddb1af5910f1c151760bdcb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XAUtH9C1iPLa9clk9RA-8Q.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">SEE strategy</figcaption></figure><p id="bb07" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">STN-OCR 的名字暗示了使用空间转换器的策略。</p><p id="7178" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">他们在<strong class="ll ir"> </strong>中训练<strong class="ll ir">两个级联网络</strong>，其中第一个网络，即变换器，学习对图像的变换，以输出更容易解释的子图像。</p><p id="7b4c" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">然后，另一个 LSTM 在前的前馈网络(嗯…好像我们以前见过)识别文本。</p><p id="745b" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">研究人员在这里强调了使用 resnet 的重要性(他们使用了两次),因为它提供了到早期层的“强”传播。然而，这种做法如今已被广泛接受。</p><p id="5c90" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">不管怎样，这都是一个有趣的尝试。</p><h2 id="ccf6" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">3.标准深度学习方法</h2><p id="1500" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">正如标题所暗示的，在检测到“单词”后，我们可以应用标准的深度学习检测方法，如 SSD、YOLO 和 Mask RCNN。因为网上有太多的信息，所以我不打算详细说明这些方法。</p><p id="8647" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">我必须说这是我目前最喜欢的方法，因为我喜欢深度学习的“端到端”哲学，在这种哲学中，你可以应用一个强大的模型，通过一些调整可以解决几乎所有问题。在这篇文章的下一部分，我们将看到它实际上是如何工作的。</p><p id="c122" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">然而，当涉及到密集、相似的类时，SSD 和其他检测模型会受到挑战，如这里的<a class="ae kf" href="https://arxiv.org/pdf/1611.10012.pdf" rel="noopener ugc nofollow" target="_blank">所述</a>。我觉得这有点讽刺，因为事实上，深度学习模型发现识别数字和字母比识别更具挑战性和复杂的物体(如狗、猫或人)要困难得多。它们往往达不到预期的精度，因此，专门的方法蓬勃发展。</p><h1 id="0411" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw mx ky kz la my lc ld le mz lg lh li bi translated">实际例子</h1><p id="8670" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">所以说了这么多之后，是时候动手了，试着自己做些模特。我们将尝试解决<a class="ae kf" href="http://ufldl.stanford.edu/housenumbers/" rel="noopener ugc nofollow" target="_blank"> SVHN </a>任务。SVHN 数据包含三个不同的数据集:<em class="ow">训练</em>、<em class="ow">测试</em>和<em class="ow">额外</em>。差异不是 100%清楚，但是最大的<em class="ow">额外的</em>数据集(约 50 万样本)包括更容易识别的图像。因此，为了这次拍摄，我们将使用它。</p><p id="d127" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">要准备该任务，请执行以下操作:</p><ul class=""><li id="2a8f" class="nj nk iq ll b lm mg lq mh lu nl ly nm mc nn ke no np nq nr bi translated">你需要一台 Tensorflow≥1.4、Keras≥2 的基本 GPU 机器</li><li id="6cc3" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke no np nq nr bi translated">从<a class="ae kf" href="https://github.com/pierluigiferrari/ssd_keras" rel="noopener ugc nofollow" target="_blank">这里</a>克隆 SSD_Keras 项目。</li><li id="a494" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke no np nq nr bi translated">从<a class="ae kf" href="https://drive.google.com/open?id=1vmEF7FUsWfHquXyCqO17UaXOPpRbwsdj" rel="noopener ugc nofollow" target="_blank">这里</a>下载 coco 数据集上预先训练好的 SSD300 模型。</li><li id="785b" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke no np nq nr bi translated">在此从<a class="ae kf" href="https://github.com/shgidi/OCR" rel="noopener ugc nofollow" target="_blank">克隆<em class="ow">该</em>项目的回购..</a></li><li id="d9cc" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke no np nq nr bi translated">下载<a class="ae kf" href="http://ufldl.stanford.edu/housenumbers/extra.tar.gz" rel="noopener ugc nofollow" target="_blank">extra.tar.gz</a>文件，其中包含 SVHN 数据集的额外图像。</li><li id="9d2b" class="nj nk iq ll b lm ns lq nt lu nu ly nv mc nw ke no np nq nr bi translated">更新此项目报告中 json_config.json 中的所有相关路径。</li></ul><p id="a7d3" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">为了有效地遵循流程，您应该阅读以下说明，并从项目的 repo 中运行<strong class="ll ir"> ssd_OCR.ipynb </strong>笔记本。</p><p id="a7a6" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">而且…你已经准备好开始了！</p><h2 id="31c1" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">步骤 1:解析数据</h2><p id="a1a7" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">不管你喜不喜欢，但是在探测任务中没有“黄金”格式的数据表示。一些众所周知的格式有:coco、via、pascal、xml。还有更多。例如，SVHN 数据集用晦涩的<em class="ow">进行了注释。mat </em>格式。幸运的是，这个<a class="ae kf" href="https://gist.github.com/veeresht/7bf499ee6d81938f8bbdb3c6ef1855bf" rel="noopener ugc nofollow" target="_blank">要点</a>提供了一个巧妙的<em class="ow"> read_process_h5 </em>脚本来转换。mat 文件转换为标准 json，您应该更进一步，将其进一步转换为 pascal 格式，如下所示:</p><pre class="nb nc nd ne gt ox oy oz pa aw pb bi"><span id="9d2c" class="ml km iq oy b gy pc pd l pe pf">def json_to_pascal(json, filename): #filename is the .mat file<br/>    # convert json to pascal and save as csv<br/>    pascal_list = []<br/>    for i in json:<br/>        for j in range(len(i['labels'])):<br/>            pascal_list.append({'fname': i['filename'] <br/>            ,'xmin': int(i['left'][j]), 'xmax': int(i['left'][j]+i['width'][j])<br/>            ,'ymin': int(i['top'][j]),  'ymax': int(i['top'][j]+i['height'][j])<br/>            ,'class_id': int(i['labels'][j])})<br/>    df_pascal = pd.DataFrame(pascal_list,dtype='str')<br/>    df_pascal.to_csv(filename,index=False)</span><span id="3c81" class="ml km iq oy b gy pg pd l pe pf"><em class="ow">p = read_process_h5(file_path)</em></span><span id="e507" class="ml km iq oy b gy pg pd l pe pf">json_to_pascal(p, data_folder+'pascal.csv')</span></pre><p id="03b3" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">现在我们应该有一个更加标准的<em class="ow"> pascal.csv </em>文件，它将允许我们继续前进。如果转换太慢，你应该注意到我们不需要所有的数据样本。~10K 就够了。</p><h2 id="00eb" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">第二步:看数据</h2><p id="60a3" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">在开始建模过程之前，您最好对数据进行一些探索。我只为健全性测试提供了一个快速功能，但是我建议您做一些进一步的分析:</p><pre class="nb nc nd ne gt ox oy oz pa aw pb bi"><span id="5a11" class="ml km iq oy b gy pc pd l pe pf">def viz_random_image(df):<br/>    file = np.random.choice(df.fname)<br/>    im = skimage.io.imread(data_folder+file)<br/>    annots =  df[df.fname==file].iterrows()</span><span id="c064" class="ml km iq oy b gy pg pd l pe pf">    plt.figure(figsize=(6,6))<br/>    plt.imshow(im)</span><span id="add4" class="ml km iq oy b gy pg pd l pe pf">    current_axis = plt.gca()</span><span id="6e5b" class="ml km iq oy b gy pg pd l pe pf">    for box in annots:<br/>        label = box[1]['class_id']<br/>        current_axis.add_patch(plt.Rectangle(<br/>            (box[1]['xmin'], box[1]['ymin']), box[1]['xmax']-box[1]['xmin'],<br/>            box[1]['ymax']-box[1]['ymin'], color='blue', fill=False, linewidth=2))  <br/>        current_axis.text(box[1]['xmin'], box[1]['ymin'], label, size='x-large', color='white', bbox={'facecolor':'blue', 'alpha':1.0})<br/>        plt.show()<br/></span><span id="9ba3" class="ml km iq oy b gy pg pd l pe pf">viz_random_image(df)</span></pre><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/2ebfc0cba06a9fe7ee435df915ba25ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*zLCfj1HVgil2JRywTQgjew.png"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">A representative sample form SVHN dataset</figcaption></figure><p id="f0aa" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">对于接下来的步骤，我在 repo 中提供了一个<em class="ow"> utils_ssd.py </em>，以方便训练、负重等。部分代码取自 SSD_Keras repo，也是广泛使用的。</p><h2 id="c228" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">第三步:选择策略</h2><p id="29cd" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">如前所述，对于这个问题，我们有许多可能的方法。在本教程中，我将采用标准的深度学习检测方法，并将使用 SSD 检测模型。我们将从这里的<a class="ae kf" href="https://github.com/pierluigiferrari/ssd_keras" rel="noopener ugc nofollow" target="_blank">使用<strong class="ll ir"> SSD keras </strong>实现。这是 PierreLuigi 很好的实现。虽然它的 GitHub 明星比 rykov8 的实现少，但它看起来更新了，也更容易集成。当你选择使用哪个项目时，这是一件非常重要的事情。其他好的选择将是 YOLO 模型和掩模 RCNN。</a></p><h2 id="bff4" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated"><strong class="ak">步骤 4:加载并训练 SSD 模型</strong></h2><p id="5f0e" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated"><strong class="ll ir">一些定义</strong></p><p id="f701" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">要使用 repo，您需要验证您有 SSD_keras repo，并填写 json_config.json 文件中的路径，以允许笔记本找到这些路径。</p><p id="c4d0" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">从导入开始:</p><pre class="nb nc nd ne gt ox oy oz pa aw pb bi"><span id="a09a" class="ml km iq oy b gy pc pd l pe pf"><strong class="oy ir">import</strong> <strong class="oy ir">os</strong><br/><strong class="oy ir">import</strong> <strong class="oy ir">sys</strong><br/><strong class="oy ir">import</strong> <strong class="oy ir">skimage.io</strong><br/><strong class="oy ir">import</strong> <strong class="oy ir">scipy</strong><br/><strong class="oy ir">import</strong> <strong class="oy ir">json</strong></span><span id="e787" class="ml km iq oy b gy pg pd l pe pf"><strong class="oy ir">with</strong> open('json_config.json') <strong class="oy ir">as</strong> f:     json_conf = json.load(f)</span><span id="6388" class="ml km iq oy b gy pg pd l pe pf">ROOT_DIR = os.path.abspath(json_conf['ssd_folder']) <em class="ow"># add here mask RCNN path</em><br/>sys.path.append(ROOT_DIR)<br/><br/><strong class="oy ir">import</strong> <strong class="oy ir">cv2</strong><br/><strong class="oy ir">from</strong> <strong class="oy ir">utils_ssd</strong> <strong class="oy ir">import</strong> *<br/><strong class="oy ir">import</strong> <strong class="oy ir">pandas</strong> <strong class="oy ir">as</strong> <strong class="oy ir">pd</strong><br/><strong class="oy ir">from</strong> <strong class="oy ir">PIL</strong> <strong class="oy ir">import</strong> Image<br/><br/><strong class="oy ir">from</strong> <strong class="oy ir">matplotlib</strong> <strong class="oy ir">import</strong> pyplot <strong class="oy ir">as</strong> plt<br/><br/>%matplotlib inline<br/>%load_ext autoreload<br/>% autoreload 2</span></pre><p id="adab" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">以及更多的定义:</p><pre class="nb nc nd ne gt ox oy oz pa aw pb bi"><span id="b1f5" class="ml km iq oy b gy pc pd l pe pf">task = 'svhn'</span><span id="7a8d" class="ml km iq oy b gy pg pd l pe pf">labels_path = f'{data_folder}pascal.csv'</span><span id="b145" class="ml km iq oy b gy pg pd l pe pf">input_format = ['class_id','image_name','xmax','xmin','ymax','ymin' ]<br/>    <br/>df = pd.read_csv(labels_path)</span></pre><p id="e458" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated"><strong class="ll ir">车型配置</strong>:</p><pre class="nb nc nd ne gt ox oy oz pa aw pb bi"><span id="dd10" class="ml km iq oy b gy pc pd l pe pf"><strong class="oy ir">class</strong> <strong class="oy ir">SVHN_Config</strong>(Config):<br/>    batch_size = 8<br/>    <br/>    dataset_folder = data_folder<br/>    task = task<br/>    <br/>    labels_path = labels_path<br/><br/>    input_format = input_format<br/><br/>conf=SVHN_Config()<br/><br/>resize = Resize(height=conf.img_height, width=conf.img_width)<br/>trans = [resize]</span></pre><p id="053f" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated"><strong class="ll ir">定义型号，装载重量</strong></p><p id="bbd9" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">与大多数深度学习案例一样，我们不会从头开始训练，但我们会加载预训练的权重。在这种情况下，我们将加载 SSD 模型的<a class="ae kf" href="https://drive.google.com/open?id=1vmEF7FUsWfHquXyCqO17UaXOPpRbwsdj" rel="noopener ugc nofollow" target="_blank">权重</a>，在 COCO 数据集上训练，该数据集有 80 个类。很明显，我们的任务只有 10 个类，因此在加载权重后，我们将重建顶层以获得正确数量的输出。我们在 init_weights 函数中完成。补充说明:在这种情况下，正确的输出数量是 44:每个类 4 个(边界框坐标),另外 4 个用于背景/无类。</p><pre class="nb nc nd ne gt ox oy oz pa aw pb bi"><span id="d011" class="ml km iq oy b gy pc pd l pe pf">learner = SSD_finetune(conf)<br/>learner.get_data(create_subset=<strong class="oy ir">True</strong>)<br/><br/>weights_destination_path=learner.init_weights()<br/><br/>learner.get_model(mode='training', weights_path = weights_destination_path)<br/>model = learner.model<br/>learner.get_input_encoder()<br/>ssd_input_encoder = learner.ssd_input_encoder<br/><br/><em class="ow"># Training schedule definitions</em><br/>adam = Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) <br/>ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)<br/>model.compile(optimizer=adam, loss=ssd_loss.compute_loss)</span></pre><p id="44a9" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated"><strong class="ll ir">定义数据加载器</strong></p><pre class="nb nc nd ne gt ox oy oz pa aw pb bi"><span id="4956" class="ml km iq oy b gy pc pd l pe pf">train_annotation_file=f'<strong class="oy ir">{conf.dataset_folder}</strong>train_pascal.csv'<br/>val_annotation_file=f'<strong class="oy ir">{conf.dataset_folder}</strong>val_pascal.csv'<br/>subset_annotation_file=f'<strong class="oy ir">{conf.dataset_folder}</strong>small_pascal.csv'</span><span id="3ab2" class="ml km iq oy b gy pg pd l pe pf">batch_size=4<br/>ret_5_elements={'original_images','processed_images','processed_labels','filenames','inverse_transform'}</span><span id="adf7" class="ml km iq oy b gy pg pd l pe pf">train_generator = learner.get_generator(batch_size, trans=trans, anot_file=train_annotation_file,<br/>                  encoder=ssd_input_encoder)</span><span id="14f2" class="ml km iq oy b gy pg pd l pe pf">val_generator = learner.get_generator(batch_size,trans=trans, anot_file=val_annotation_file,<br/>                 returns={'processed_images','encoded_labels'}, encoder=ssd_input_encoder,val=True)</span></pre><h2 id="5f42" class="ml km iq bd kn mm mn dn kr mo mp dp kv lu mq mr kz ly ms mt ld mc mu mv lh mw bi translated">5.训练模型</h2><p id="774e" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated"><strong class="ll ir">现在</strong>模型准备好了，我们将设置一些与上次训练相关的定义，并开始训练</p><pre class="nb nc nd ne gt ox oy oz pa aw pb bi"><span id="babb" class="ml km iq oy b gy pc pd l pe pf">learner.init_training()</span><span id="76e0" class="ml km iq oy b gy pg pd l pe pf">history = learner.train(train_generator, val_generator, steps=100,epochs=80)</span></pre><p id="fd5a" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">作为奖励，我在训练脚本中包含了<strong class="ll ir"> <em class="ow"> training_plot </em> </strong>回调，以在每个时期后可视化一个随机图像。例如，下面是第六个纪元<strong class="ll ir">后的预测快照:</strong></p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/cefe89847c1b3d8c6f0b253198d94783.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*24S3wR8AoFoes6Ddmn3rLw.png"/></div></figure><p id="5f7c" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">SSD_Keras repo 在几乎每个时期后处理保存模型，因此您可以稍后加载模型，只需更改<em class="ow"> weights_destination_path </em>行使其等于路径</p><pre class="nb nc nd ne gt ox oy oz pa aw pb bi"><span id="3c8b" class="ml km iq oy b gy pc pd l pe pf">weights_destination_path = &lt;path&gt;</span></pre><p id="6a7c" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">如果你按照我的指示做，你应该能训练出这个模型。ssd_keras 提供了更多的特性，例如数据扩充、不同的加载器和评估器。经过短暂的训练，我已经达到了&gt; 80 地图。</p><p id="c821" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">你达到了多高？</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi pj"><img src="../Images/4c21f24e6c2027d304bac24ed44c5b98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*y84JlUC2sOzql3um"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">Training for 4X100X60 samples, from tensorboard</figcaption></figure><h1 id="30b2" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw mx ky kz la my lc ld le mz lg lh li bi translated">摘要</h1><p id="5945" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ke ij bi translated">在这篇文章中，我们讨论了 OCR 领域的不同挑战和方法。正如深度学习/计算机视觉中的许多问题一样，它比乍看起来要复杂得多。我们已经看到了它的许多子任务，以及解决它的一些不同的方法，目前没有一个是银弹。另一方面，我们已经看到，没有太多的争论，达成初步结果并不困难。</p><p id="214e" class="pw-post-body-paragraph lj lk iq ll b lm mg lo lp lq mh ls lt lu mi lw lx ly mj ma mb mc mk me mf ke ij bi translated">希望你喜欢！</p></div></div>    
</body>
</html>