<html>
<head>
<title>Pytorch: how and when to use Module, Sequential, ModuleList and ModuleDict</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pytorch:如何以及何时使用模块、顺序、模块列表和模块指令</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-how-and-when-to-use-module-sequential-modulelist-and-moduledict-7a54597b5f17?source=collection_archive---------3-----------------------#2018-09-24">https://towardsdatascience.com/pytorch-how-and-when-to-use-module-sequential-modulelist-and-moduledict-7a54597b5f17?source=collection_archive---------3-----------------------#2018-09-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/f4cb6bdd9cde347a31e6e675fd3b4b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G02W6-MM2CSCsa9DiA5NFg.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/photos/C0koz3G1I4I?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a> on <a class="ae jg" href="https://unsplash.com/search/photos/lego?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="9e53" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">嘿，我在</em><a class="ae jg" href="https://www.linkedin.com/in/francesco-saverio-zuppichini-94659a150/" rel="noopener ugc nofollow" target="_blank"><em class="le">LinkedIn</em></a><em class="le">过来打个招呼👋</em></p><p id="016d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">在 Pytorch 1.7 更新</em></p><p id="ac5c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在这里找到代码<a class="ae jg" href="https://github.com/FrancescoSaverioZuppichini/Pytorch-how-and-when-to-use-Module-Sequential-ModuleList-and-ModuleDict" rel="noopener ugc nofollow" target="_blank"/></p><p id="0092" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae jg" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>是一个开源的深度学习框架，提供了一种创建 ML 模型的智能方式。即使文档做得很好，我仍然发现大多数人仍然能够写出糟糕的、没有条理的 PyTorch 代码。</p><p id="611b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">今天我们来看看 PyTorch 的三个主要构建模块是如何使用的:<code class="fe lf lg lh li b">Module, Sequential and ModuleList</code>。我们将从一个例子开始，并不断改进。</p><p id="03df" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这四个类都包含在<code class="fe lf lg lh li b">torch.nn</code>中</p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><h1 id="bb35" class="lp lq jj bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">模块:主要构建模块</h1><p id="6593" class="pw-post-body-paragraph kg kh jj ki b kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated"><a class="ae jg" href="https://pytorch.org/docs/stable/nn.html?highlight=module" rel="noopener ugc nofollow" target="_blank">模块</a>是主要的构建模块，它定义了所有神经网络的基类，您必须对其进行子类化。</p><p id="5631" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们创建一个经典的 CNN 分类器作为示例:</p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><pre class="lj lk ll lm gt ms li mt mu aw mv bi"><span id="37d8" class="mw lq jj li b gy mx my l mz na">MyCNNClassifier( (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (fc1): Linear(in_features=25088, out_features=1024, bias=True) (fc2): Linear(in_features=1024, out_features=10, bias=True) )</span></pre><p id="edf5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一个非常简单的分类器，编码部分使用两层 3x3 convs + batchnorm + relu，解码部分使用两个线性层。如果您不熟悉 PyTorch，您可能以前见过这种类型的编码，但是有两个问题。</p><p id="0891" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们想增加一层，我们必须再次在<code class="fe lf lg lh li b">__init__</code>和<code class="fe lf lg lh li b">forward</code>函数中写很多代码。此外，如果我们有一些想要在另一个模型中使用的公共块，例如 3x3 conv + batchnorm + relu，我们必须重新编写它。</p><h1 id="cd5e" class="lp lq jj bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">连续:堆叠和合并层</h1><p id="1a24" class="pw-post-body-paragraph kg kh jj ki b kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated"><a class="ae jg" href="https://pytorch.org/docs/stable/nn.html?highlight=sequential#torch.nn.Sequential" rel="noopener ugc nofollow" target="_blank">顺序</a>是模块的容器，可以堆叠在一起，同时运行。</p><p id="0061" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以注意到我们已经把所有东西都储存到<code class="fe lf lg lh li b">self</code>里了。我们可以使用<code class="fe lf lg lh li b">Sequential</code>来改进我们的代码。</p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><pre class="lj lk ll lm gt ms li mt mu aw mv bi"><span id="bec2" class="mw lq jj li b gy mx my l mz na">MyCNNClassifier( (conv_block1): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (conv_block2): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) ) )</span></pre><p id="c5f1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好多了 uhu？</p><p id="5789" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你有没有注意到<code class="fe lf lg lh li b">conv_block1</code>和<code class="fe lf lg lh li b">conv_block2</code>看起来几乎一样？我们可以创建一个重述<code class="fe lf lg lh li b">nn.Sequential</code>的函数来简化代码！</p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><p id="bc3b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们可以在模块中调用这个函数</p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><pre class="lj lk ll lm gt ms li mt mu aw mv bi"><span id="e764" class="mw lq jj li b gy mx my l mz na">MyCNNClassifier( (conv_block1): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (conv_block2): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) ) )</span></pre><p id="ff3b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更干净！还是<code class="fe lf lg lh li b">conv_block1</code>和<code class="fe lf lg lh li b">conv_block2</code>差不多！我们可以用<code class="fe lf lg lh li b">nn.Sequential</code>合并它们</p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><pre class="lj lk ll lm gt ms li mt mu aw mv bi"><span id="433c" class="mw lq jj li b gy mx my l mz na">MyCNNClassifier( (encoder): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) ) )</span></pre><p id="9a0a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe lf lg lh li b">self.encoder</code>现占据展位<code class="fe lf lg lh li b">conv_block</code>。我们已经为我们的模型解耦了逻辑，使它更容易阅读和重用。我们的<code class="fe lf lg lh li b">conv_block</code>功能可以导入到另一个模型中使用。</p><h1 id="aa95" class="lp lq jj bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">动态连续:一次创建多个层</h1><p id="532d" class="pw-post-body-paragraph kg kh jj ki b kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">如果我们可以在<code class="fe lf lg lh li b">self.encoder</code>中添加一个新的层，硬编码会不方便:</p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><p id="a88f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们可以将大小定义为一个数组并自动创建所有的层，而不需要写入每一层，这是不是很好？幸运的是，我们可以创建一个数组并将其传递给<code class="fe lf lg lh li b">Sequential</code></p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><pre class="lj lk ll lm gt ms li mt mu aw mv bi"><span id="a8c1" class="mw lq jj li b gy mx my l mz na">MyCNNClassifier( (encoder): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) ) )</span></pre><p id="be04" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来分解一下。我们创建了一个数组<code class="fe lf lg lh li b">self.enc_sizes</code>来保存编码器的大小。然后我们通过迭代大小创建一个数组<code class="fe lf lg lh li b">conv_blocks</code>。因为我们必须给 booth 一个尺寸，并给每一层一个特大号，所以我们通过将数组移动一位来实现数组本身。</p><p id="c13c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了清楚起见，看一下下面的例子:</p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><pre class="lj lk ll lm gt ms li mt mu aw mv bi"><span id="e888" class="mw lq jj li b gy mx my l mz na">1 32 32 64</span></pre><p id="afcf" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，由于<code class="fe lf lg lh li b">Sequential</code>不接受列表，我们通过使用<code class="fe lf lg lh li b">*</code>操作符来分解它。</p><p id="e091" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Tada！现在，如果我们只想添加一个尺寸，我们可以很容易地在列表中添加一个新的数字。将大小作为参数是一种常见的做法。</p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><pre class="lj lk ll lm gt ms li mt mu aw mv bi"><span id="740c" class="mw lq jj li b gy mx my l mz na">MyCNNClassifier( (encoder): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): Sequential( (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) ) )</span></pre><p id="95ac" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以对解码器部分做同样的事情</p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><pre class="lj lk ll lm gt ms li mt mu aw mv bi"><span id="f92e" class="mw lq jj li b gy mx my l mz na">MyCNNClassifier( (encoder): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) (decoder): Sequential( (0): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() ) (1): Sequential( (0): Linear(in_features=1024, out_features=512, bias=True) (1): Sigmoid() ) ) (last): Linear(in_features=512, out_features=10, bias=True) )</span></pre><p id="4c32" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们遵循相同的模式，我们为解码部分创建一个新的块，linear + sigmoid，并传递一个包含大小的数组。我们必须添加一个<code class="fe lf lg lh li b">self.last</code>,因为我们不想激活输出</p><p id="d5cb" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们甚至可以将我们的模型一分为二！编码器+解码器</p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><pre class="lj lk ll lm gt ms li mt mu aw mv bi"><span id="49df" class="mw lq jj li b gy mx my l mz na">MyCNNClassifier( (encoder): MyEncoder( (conv_blokcs): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) ) (decoder): MyDecoder( (dec_blocks): Sequential( (0): Sequential( (0): Linear(in_features=1024, out_features=512, bias=True) (1): Sigmoid() ) ) (last): Linear(in_features=512, out_features=10, bias=True) ) )</span></pre><p id="ae4a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意<code class="fe lf lg lh li b">MyEncoder</code>和<code class="fe lf lg lh li b">MyDecoder</code>也可以是返回<code class="fe lf lg lh li b">nn.Sequential</code>的函数。我更喜欢对模型使用第一种模式，对构建模块使用第二种模式。</p><p id="c20f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过将我们的模块分成子模块，共享代码、<strong class="ki jk">调试代码、T23 测试代码变得更加容易。</strong></p><h1 id="3b40" class="lp lq jj bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">ModuleList:当我们需要迭代时</h1><p id="d817" class="pw-post-body-paragraph kg kh jj ki b kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated"><code class="fe lf lg lh li b">ModuleList</code>允许您将<code class="fe lf lg lh li b">Module</code>存储为列表。当你需要遍历层并存储/使用一些信息时，比如在 U-net 中，它会很有用。</p><p id="5456" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe lf lg lh li b">Sequential</code>之间的主要区别是<code class="fe lf lg lh li b">ModuleList</code>没有<code class="fe lf lg lh li b">forward</code>方法，所以内层没有连接。假设我们需要解码器中每个层的每个输出，我们可以通过以下方式存储它:</p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><pre class="lj lk ll lm gt ms li mt mu aw mv bi"><span id="7467" class="mw lq jj li b gy mx my l mz na">torch.Size([4, 16]) torch.Size([4, 32]) [None, None]</span></pre><h1 id="aa14" class="lp lq jj bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">ModuleDict:当我们需要选择时</h1><p id="c5ac" class="pw-post-body-paragraph kg kh jj ki b kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">如果我们想在我们的<code class="fe lf lg lh li b">conv_block</code>中切换到<code class="fe lf lg lh li b">LearkyRelu</code>呢？我们可以使用<code class="fe lf lg lh li b">ModuleDict</code>创建一个<code class="fe lf lg lh li b">Module</code>的字典，并在需要时动态切换<code class="fe lf lg lh li b">Module</code></p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><pre class="lj lk ll lm gt ms li mt mu aw mv bi"><span id="e390" class="mw lq jj li b gy mx my l mz na">Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): LeakyReLU(negative_slope=0.01) ) Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() )</span></pre><h1 id="bb6f" class="lp lq jj bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">最终实施</h1><p id="4830" class="pw-post-body-paragraph kg kh jj ki b kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">让我们把一切都结束吧！</p><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><figure class="lj lk ll lm gt iv"><div class="bz fp l di"><div class="ln lo l"/></div></figure><pre class="lj lk ll lm gt ms li mt mu aw mv bi"><span id="6fed" class="mw lq jj li b gy mx my l mz na">MyCNNClassifier( (encoder): MyEncoder( (conv_blokcs): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): LeakyReLU(negative_slope=0.01) ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): LeakyReLU(negative_slope=0.01) ) ) ) (decoder): MyDecoder( (dec_blocks): Sequential( (0): Sequential( (0): Linear(in_features=1024, out_features=512, bias=True) (1): Sigmoid() ) ) (last): Linear(in_features=512, out_features=10, bias=True) ) )</span></pre><h1 id="4e45" class="lp lq jj bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">结论</h1><p id="4064" class="pw-post-body-paragraph kg kh jj ki b kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">你可以在这里找到代码<a class="ae jg" href="https://github.com/FrancescoSaverioZuppichini/Pytorch-how-and-when-to-use-Module-Sequential-ModuleList-and-ModuleDict" rel="noopener ugc nofollow" target="_blank"/></p><p id="f804" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">综上所述。</p><ul class=""><li id="09e0" class="nb nc jj ki b kj kk kn ko kr nd kv ne kz nf ld ng nh ni nj bi translated">当你有一个由多个小块组成大块时，使用<code class="fe lf lg lh li b">Module</code></li><li id="0d0d" class="nb nc jj ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated">当你想从层中创建一个小块时，使用<code class="fe lf lg lh li b">Sequential</code></li><li id="dd03" class="nb nc jj ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated">当你需要迭代一些层或构建块并做一些事情时，使用<code class="fe lf lg lh li b">ModuleList</code></li><li id="2ddb" class="nb nc jj ki b kj nk kn nl kr nm kv nn kz no ld ng nh ni nj bi translated">当需要参数化模型的某些模块时，例如激活功能，使用<code class="fe lf lg lh li b">ModuleDict</code></li></ul><p id="3907" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那都是乡亲们！</p><p id="0311" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您的阅读</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><p id="c757" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">原载于</em><a class="ae jg" href="https://gist.github.com/7b228760eefe8e77fb0a37b5783a379c" rel="noopener ugc nofollow" target="_blank"><em class="le">gist.github.com</em></a><em class="le">。</em></p></div></div>    
</body>
</html>