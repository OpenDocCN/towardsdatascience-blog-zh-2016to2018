<html>
<head>
<title>Andrew Ng’s Machine Learning Course in Python (Linear Regression)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">吴恩达的 Python(线性回归)机器学习教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-linear-regression-dd04fba8e137?source=collection_archive---------4-----------------------#2018-12-06">https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-linear-regression-dd04fba8e137?source=collection_archive---------4-----------------------#2018-12-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/58c7c44e30031932fee9209b222e7a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4u5JYQsltAMmS-4geuTNPA.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Machine Learning — Andrew Ng</figcaption></figure><p id="ea53" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我是一名药学本科生，一直想做的远不止是一名临床药师。我曾试图在我对它的热爱和我拥有的医疗保健知识之间找到某种融合，但在当今这个时代，人们真的会感到迷失在丰富的信息中。</p><p id="2f3c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">6 个月前，我偶然发现了数据科学的概念及其在医疗保健行业的应用。鉴于数据和计算能力的进步，利用计算机来识别、诊断和治疗疾病不再是梦想。在更高级的层面上，计算机视觉可以帮助使用射线照相图像识别疾病，而在更简单的层面上，算法可以检测改变生活的潜在药物相互作用。</p><p id="c457" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">带着进入医疗 IT 行业的目标，我为那些没有技术背景的人设计了一个数据科学课程，我在这里<a class="ae ld" href="https://link.medium.com/AEFl6oLRES" rel="noopener">展示了它</a>。</p><p id="b095" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">斯坦福大学在 Coursera(<a class="ae ld" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">https://www.coursera.org/learn/machine-learning</a>)开设的吴恩达机器学习是数据科学社区强烈推荐的课程之一。经过 6 个月的基础数学和 python 培训，我开始了这门课程，以步入机器学习的世界。你们很多人都知道，这门课是在 Octave 或 Matlab 中进行的。虽然学习一些 Octave 编程并完成编程任务是很好的，但我想测试我的 python 知识，并尝试从头开始完成 python 的任务。</p><p id="752e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">本文将是我撰写的系列文章的一部分，记录我在课程中编程作业的 python 实现。这绝不是其他人的指南，因为我也在不断学习，但可以作为那些希望做同样事情的人的起点。也就是说，我很高兴从你们那里收到一些建设性的反馈。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><p id="f8fd" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">首先将使用数据集</strong> <code class="fe ll lm ln lo b"><strong class="kh iu">ex1data1.txt</strong></code>进行一元线性回归</p><p id="c642" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">首先，我将导入所有相关的库并将数据集加载到 jupyter 笔记本中</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="be7d" class="lx ly it lo b gy lz ma l mb mc">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span><span id="8355" class="lx ly it lo b gy md ma l mb mc">data=pd.read_csv("Uni_linear.txt", header=None)</span></pre><p id="7ccf" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了养成良好的习惯，我会经常看数据，并对数据有良好的感觉</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="8057" class="lx ly it lo b gy lz ma l mb mc">data.head()<br/>data.describe()</span></pre><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi me"><img src="../Images/4b67e065498becffbc6fb3cfde33d853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*95f-rFetyo4hcohACaAiPA.png"/></div></div></figure><p id="55c2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">绘制数据，以显示因变量(y)和自变量(X)之间的关系</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="922f" class="lx ly it lo b gy lz ma l mb mc">plt.scatter(data[0],data[1])<br/>plt.xticks(np.arange(5,30,step=5))<br/>plt.yticks(np.arange(-5,30,step=5))<br/>plt.xlabel("Population of City (10,000s)")<br/>plt.ylabel("Profit ($10,000")<br/>plt.title("Profit Vs Population")</span></pre><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi me"><img src="../Images/ec34bd4c294ad692adf59bfe79bcd784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7aNj-TsbNJOP--sLIjX6YQ.png"/></div></div></figure><p id="a3d2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我已经习惯了这种绘制图形的方式，但是我意识到使用 matplotlib 有一种面向对象的方式，我将在本作业的其他一些图形中使用这种方式</p><p id="29f3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来计算成本函数 J(θ)</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="4112" class="lx ly it lo b gy lz ma l mb mc">def computeCost(X,y,theta):<br/>    """<br/>    Take in a numpy array X,y, theta and generate the cost function     of using theta as parameter in a linear regression model<br/>    """<br/>    m=len(y)<br/>    predictions=X.dot(theta)<br/>    square_err=(predictions - y)**2<br/>    <br/>    return 1/(2*m) * np.sum(square_err)</span></pre><p id="860e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">初始化 X，y 并计算使用θ=(0，0)的成本</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="8957" class="lx ly it lo b gy lz ma l mb mc">data_n=data.values<br/>m=len(data_n[:,-1])<br/>X=np.append(np.ones((m,1)),data_n[:,0].reshape(m,1),axis=1)<br/>y=data_n[:,1].reshape(m,1)<br/>theta=np.zeros((2,1))</span><span id="58b7" class="lx ly it lo b gy md ma l mb mc">computeCost(X,y,theta)</span></pre><p id="d002" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这可能不是最好的方法，但这是我找到的唯一一个为₀.增加一列 1 的解决方案这里的<code class="fe ll lm ln lo b">computeCost</code>函数将给出<code class="fe ll lm ln lo b">32.072733877455676</code></p><p id="70fd" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在通过最小化成本函数 J(θ)来实现梯度下降以优化θ</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="fd8c" class="lx ly it lo b gy lz ma l mb mc">def gradientDescent(X,y,theta,alpha,num_iters):<br/>    """<br/>    Take in numpy array X, y and theta and update theta by taking   num_iters gradient steps<br/>    with learning rate of alpha<br/>    <br/>    return theta and the list of the cost of theta during each  iteration<br/>    """<br/>    <br/>    m=len(y)<br/>    J_history=[]<br/>    <br/>    for i in range(num_iters):<br/>        predictions = X.dot(theta)<br/>        error = np.dot(X.transpose(),(predictions -y))<br/>        descent=alpha * 1/m * error<br/>        theta-=descent<br/>        J_history.append(computeCost(X,y,theta))<br/>    <br/>    return theta, J_history</span><span id="7a1f" class="lx ly it lo b gy md ma l mb mc">theta,J_history = gradientDescent(X,y,theta,0.01,1500)</span><span id="559b" class="lx ly it lo b gy md ma l mb mc">print("h(x) ="+str(round(theta[0,0],2))+" + "+str(round(theta[1,0],2))+"x1")</span></pre><p id="db1f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">print 语句将打印出假设:<code class="fe ll lm ln lo b"><em class="mf">h</em>(<em class="mf">x</em>) = -3.63 + 1.17x₁</code>，显示四舍五入到小数点后两位的优化θ值</p><p id="ae24" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了使任务更加完整，我还尝试将标准单变量情况下的成本函数可视化</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="544e" class="lx ly it lo b gy lz ma l mb mc">from mpl_toolkits.mplot3d import Axes3D</span><span id="504f" class="lx ly it lo b gy md ma l mb mc">#Generating values for theta0, theta1 and the resulting cost value</span><span id="2a85" class="lx ly it lo b gy md ma l mb mc">theta0_vals=np.linspace(-10,10,100)<br/>theta1_vals=np.linspace(-1,4,100)<br/>J_vals=np.zeros((len(theta0_vals),len(theta1_vals)))</span><span id="de09" class="lx ly it lo b gy md ma l mb mc">for i in range(len(theta0_vals)):<br/>    for j in range(len(theta1_vals)):<br/>        t=np.array([theta0_vals[i],theta1_vals[j]])<br/>        J_vals[i,j]=computeCost(X,y,t)</span><span id="2a86" class="lx ly it lo b gy md ma l mb mc">#Generating the surface plot<br/>fig = plt.figure()<br/>ax = fig.add_subplot(111, projection='3d')<br/>surf=ax.plot_surface(theta0_vals,theta1_vals,J_vals,cmap="coolwarm")<br/>fig.colorbar(surf, shrink=0.5, aspect=5)<br/>ax.set_xlabel("$\Theta_0$")<br/>ax.set_ylabel("$\Theta_1$")<br/>ax.set_zlabel("$J(\Theta)$")</span><span id="12fe" class="lx ly it lo b gy md ma l mb mc">#rotate for better angle<br/>ax.view_init(30,120)</span></pre><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi me"><img src="../Images/03a393495dedf6ed0c869b093997082c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cQrow2sdjUWdFUoD-zv7zA.png"/></div></div></figure><p id="fef3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面的代码块生成如图所示的 3d 表面图。正如在讲座中提到的，成本函数是一个凸函数，只有一个全局最小值，因此，梯度下降将总是导致找到全局最小值</p><p id="02cb" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">顺便说一下，我使用了 mplot3d 教程来帮助我进行 3d 绘图。(<a class="ae ld" href="https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html" rel="noopener ugc nofollow" target="_blank">https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html</a>)</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="1930" class="lx ly it lo b gy lz ma l mb mc">plt.plot(J_history)<br/>plt.xlabel("Iteration")<br/>plt.ylabel("$J(\Theta)$")<br/>plt.title("Cost function using Gradient Descent")</span></pre><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi me"><img src="../Images/47a2e42185196d7652d356ef9c45f28e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F9oMOOjy3k64FTm4_Tzlag.png"/></div></div></figure><p id="da12" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">相对于迭代次数绘制成本函数给出了良好的下降趋势，表明梯度下降实现在降低成本函数方面起作用</p><p id="4c7a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，有了优化的θ值，我将把预测值(最佳拟合线)一起绘制成图</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="e0d6" class="lx ly it lo b gy lz ma l mb mc">plt.scatter(data[0],data[1])<br/>x_value=[x for x in range(25)]<br/>y_value=[y*theta[1]+theta[0] for y in x_value]<br/>plt.plot(x_value,y_value,color="r")<br/>plt.xticks(np.arange(5,30,step=5))<br/>plt.yticks(np.arange(-5,30,step=5))<br/>plt.xlabel("Population of City (10,000s)")<br/>plt.ylabel("Profit ($10,000")<br/>plt.title("Profit vs Population")</span></pre><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi me"><img src="../Images/999b0f4835b7bd3d797e87768af26ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OcjouzANSJ6W-jB7mbV6gQ.png"/></div></div></figure><p id="c9dd" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">同样，这可能不是基于θ生成直线的最佳方式，如果有更好的方式，请告诉我</p><p id="51e5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">作业的最后一部分包括根据你的模型做出预测</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="e9a5" class="lx ly it lo b gy lz ma l mb mc">def predict(x,theta):<br/>    """<br/>    Takes in numpy array of x and theta and return the predicted value of y based on theta<br/>    """<br/>    <br/>    predictions= np.dot(theta.transpose(),x)<br/>    <br/>    return predictions[0]</span><span id="d580" class="lx ly it lo b gy md ma l mb mc">predict1=predict(np.array([1,3.5]),theta)*10000</span><span id="56fd" class="lx ly it lo b gy md ma l mb mc">print("For population = 35,000, we predict a profit of $"+str(round(predict1,0)))</span></pre><p id="1556" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">打印报表打印:<code class="fe ll lm ln lo b">For population = 35,000, we predict a profit of $4520.0</code></p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="641a" class="lx ly it lo b gy lz ma l mb mc">predict2=predict(np.array([1,7]),theta)*10000<br/>print("For population = 70,000, we predict a profit of $"+str(round(predict2,0)))</span></pre><p id="c088" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">打印报表打印:<code class="fe ll lm ln lo b">For population = 70,000, we predict a profit of $45342.0</code></p><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mg"><img src="../Images/2afec5192b49683cba44945e9b72d516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d5-kjdjx__ZCaRajMYuQig.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Machine Learning – Andrew Ng</figcaption></figure><p id="b302" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">现在使用数据集</strong> <code class="fe ll lm ln lo b"><strong class="kh iu">ex1data2.txt</strong></code>进行多元线性回归</p><p id="973a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">与所有数据集一样，我首先加载数据并查看数据</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="95cf" class="lx ly it lo b gy lz ma l mb mc">data2=pd.read_csv("Multi_linear.txt", header=None)<br/>data2.head()<br/>data2.describe()</span></pre><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi me"><img src="../Images/ce652616c582aaaebe4da9cdefa3c715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1VfAVCBaTo_-SGX-nC7ZQQ.png"/></div></div></figure><p id="0e75" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如你所见，现在 X 有两个特征，使它成为一个多元问题</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="b1a1" class="lx ly it lo b gy lz ma l mb mc"># Create 2 subplot, 1 for each variable<br/>fig, axes = plt.subplots(figsize=(12,4),nrows=1,ncols=2)</span><span id="7f79" class="lx ly it lo b gy md ma l mb mc">axes[0].scatter(data2[0],data2[2],color="b")<br/>axes[0].set_xlabel("Size (Square Feet)")<br/>axes[0].set_ylabel("Prices")<br/>axes[0].set_title("House prices against size of house")<br/>axes[1].scatter(data2[1],data2[2],color="r")<br/>axes[1].set_xlabel("Number of bedroom")<br/>axes[1].set_ylabel("Prices")<br/>axes[1].set_xticks(np.arange(1,6,step=1))<br/>axes[1].set_title("House prices against number of bedroom")</span><span id="c25b" class="lx ly it lo b gy md ma l mb mc"># Enhance layout<br/>plt.tight_layout()</span></pre><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/eb621a95ca098339cb26cba437d312bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*32drbv6jkm5ErsXtAWdQVg.png"/></div></div></figure><p id="d5cd" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">针对每个功能绘制价格图显示了它们之间的关系。仅仅通过看这个图，我们就应该期望因变量和自变量之间有某种程度的正相关。</p><p id="485e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于使用梯度下降的多变量问题优化，需要特征归一化来加速优化过程。</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="4444" class="lx ly it lo b gy lz ma l mb mc">def featureNormalization(X):<br/>    """<br/>    Take in numpy array of X values and return normalize X values,<br/>    the mean and standard deviation of each feature<br/>    """<br/>    mean=np.mean(X,axis=0)<br/>    std=np.std(X,axis=0)<br/>    <br/>    X_norm = (X - mean)/std<br/>    <br/>    return X_norm , mean , std</span><span id="4f43" class="lx ly it lo b gy md ma l mb mc">data_n2=data2.values<br/>m2=len(data_n2[:,-1])<br/>X2=data_n2[:,0:2].reshape(m2,2)<br/>X2, mean_X2, std_X2 = featureNormalization(X2)<br/>X2 = np.append(np.ones((m2,1)),X2,axis=1)<br/>y2=data_n2[:,-1].reshape(m2,1)<br/>theta2=np.zeros((3,1))</span></pre><p id="f46e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来是测试我们之前的函数，<code class="fe ll lm ln lo b">computeCost(X, y, theta)</code>和<code class="fe ll lm ln lo b">gradientDescent(X, y, theta, alpha, num_iters)</code>是否适用于多特征输入</p><p id="43ed" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">使用<code class="fe ll lm ln lo b">computeCost(X2,y2,theta2)</code>给出<code class="fe ll lm ln lo b">65591548106.45744</code>，这是使用θ(0，0，0)作为参数的成本</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="07c4" class="lx ly it lo b gy lz ma l mb mc">theta2, J_history2 = gradientDescent(X2,y2,theta2,0.01,400)<br/>print("h(x) ="+str(round(theta2[0,0],2))+" + "+str(round(theta2[1,0],2))+"x1 + "+str(round(theta2[2,0],2))+"x2")</span></pre><p id="51d9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">打印语句 print: <code class="fe ll lm ln lo b">h(x) =334302.06 + 99411.45x1 + 3267.01x2</code>，它是优化后的θ值，四舍五入到小数点后两位</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="bb2c" class="lx ly it lo b gy lz ma l mb mc">plt.plot(J_history2)<br/>plt.xlabel("Iteration")<br/>plt.ylabel("$J(\Theta)$")<br/>plt.title("Cost function using Gradient Descent")</span></pre><figure class="lp lq lr ls gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi me"><img src="../Images/c2959fe2f37698d2228cb9b5fa01e384.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bkd5Jt0bijJoNxfN9bkVpg.png"/></div></div></figure><p id="ec3e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">绘制<em class="mf">J</em>(θ)<em class="mf"/>与迭代次数的关系图给出了一个下降趋势，证明我们的<code class="fe ll lm ln lo b">gradientDescent </code>函数也适用于多元情况</p><p id="9209" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">最后，使用优化后的θ值对一栋 1650 平方英尺、有 3 间卧室的房子进行预测。</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="d427" class="lx ly it lo b gy lz ma l mb mc">#feature normalisation of x values<br/>x_sample = featureNormalization(np.array([1650,3]))[0]<br/>x_sample=np.append(np.ones(1),x_sample)<br/>predict3=predict(x_sample,theta2)<br/>print("For size of house = 1650, Number of bedroom = 3, we predict a house value of $"+str(round(predict3,0)))</span></pre><p id="2578" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">此打印声明打印:<code class="fe ll lm ln lo b">For size of house = 1650, Number of bedroom = 3, we predict a house value of $430447.0</code></p><p id="487a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">第一个练习到此为止。希望你能像我写它一样喜欢阅读它。请随时给我留下一些意见，告诉我如何改进。如果你想访问 Jupyter 笔记本来完成这个任务，我已经上传了 Github 中的代码(<a class="ae ld" href="https://github.com/Benlau93/Machine-Learning-by-Andrew-Ng-in-Python" rel="noopener ugc nofollow" target="_blank">https://Github . com/Ben lau 93/Machine-Learning-by-Andrew-Ng-in-Python</a>)。</p><p id="9eae" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于本系列中的其他 python 实现，</p><ul class=""><li id="4ab9" class="mi mj it kh b ki kj km kn kq mk ku ml ky mm lc mn mo mp mq bi translated"><a class="ae ld" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-logistic-regression-c0ae25509feb" rel="noopener">逻辑回归</a></li><li id="a12a" class="mi mj it kh b ki mr km ms kq mt ku mu ky mv lc mn mo mp mq bi translated"><a class="ae ld" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-regularized-logistic-regression-lasso-regression-721f311130fb" rel="noopener">正则化逻辑回归</a></li><li id="2a4d" class="mi mj it kh b ki mr km ms kq mt ku mu ky mv lc mn mo mp mq bi translated"><a class="ae ld" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-neural-networks-e526b41fdcd9" rel="noopener">神经网络</a></li><li id="1703" class="mi mj it kh b ki mr km ms kq mt ku mu ky mv lc mn mo mp mq bi translated"><a class="ae ld" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-support-vector-machines-435fc34b7bf9" rel="noopener">支持向量机</a></li><li id="eda6" class="mi mj it kh b ki mr km ms kq mt ku mu ky mv lc mn mo mp mq bi translated"><a class="ae ld" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-kmeans-clustering-pca-b7ba6fafa74" rel="noopener">无监督学习</a></li><li id="98e9" class="mi mj it kh b ki mr km ms kq mt ku mu ky mv lc mn mo mp mq bi translated"><a class="ae ld" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-anomaly-detection-1233d23dba95" rel="noopener">异常检测</a></li></ul><p id="ff36" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">感谢阅读。</p></div></div>    
</body>
</html>