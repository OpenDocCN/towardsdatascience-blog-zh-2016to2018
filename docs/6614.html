<html>
<head>
<title>Review: SharpMask — 1st Runner Up in COCO Segmentation 2015 (Instance Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">点评:sharp mask—2015 年 COCO 细分(实例细分)亚军</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-sharpmask-instance-segmentation-6509f7401a61?source=collection_archive---------12-----------------------#2018-12-21">https://towardsdatascience.com/review-sharpmask-instance-segmentation-6509f7401a61?source=collection_archive---------12-----------------------#2018-12-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7423" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">一个细化模块，一个编码器解码器架构</strong>由<strong class="ak">脸书 AI Research (FAIR) </strong></h2></div><p id="ac5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">在</span>这个故事里，<strong class="kh ir"> SharpMask </strong>，由<strong class="kh ir">脸书 AI Research (FAIR) </strong>进行点评。<strong class="kh ir">编码器解码器架构</strong>从 2016 年开始普及。通过将自顶向下传递的特征映射连接到自底向上传递的特征映射，可以进一步提高性能。</p><ul class=""><li id="9164" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated"><strong class="kh ir">对象检测</strong>:识别对象类别，并使用图像中每个已知对象的边界框定位位置。</li><li id="a8af" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">语义分割</strong>:为图像中每个已知的物体识别每个像素的物体类别。<strong class="kh ir">标签是类感知的。</strong></li><li id="1e86" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">实例分割</strong>:识别图像中每个已知对象的每个像素的每个对象实例。标签是实例感知的。</li></ul><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/b302305040fb4b0b1019b4fb4c3eb38c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h5-4hqG0Ij3-8jLv27Pp8w.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Object Detection (Left), Semantic Segmentation (Middle), Instance Segmentation</strong></figcaption></figure><p id="9e6e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SharpMask 在 MS COCO 分段挑战中获得<strong class="kh ir">第二名，在 MS COCO 检测挑战</strong>中获得<strong class="kh ir">第二名。已在<strong class="kh ir"> 2016 ECCV </strong>发表，引用<strong class="kh ir"> 200 余次</strong>。(<a class="mp mq ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----6509f7401a61--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</strong></p><ul class=""><li id="c67c" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">MS COCO 的平均召回率提高了 10–20%。</li><li id="db9b" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">通过优化架构，速度比<a class="ae mr" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339"> DeepMask </a>提升 50%。</li><li id="c6b0" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">通过使用额外的图像比例，小物体召回率提高了约 2 倍。</li><li id="35f1" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">通过将 SharpMask 应用到<a class="ae mr" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快速 R-CNN </a>上，对象检测结果也得到改善。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="8326" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">涵盖哪些内容</h1><ol class=""><li id="148e" class="lk ll iq kh b ki nr kl ns ko nt ks nu kw nv la nw lq lr ls bi translated"><strong class="kh ir">编码器解码器架构</strong></li><li id="8201" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la nw lq lr ls bi translated"><strong class="kh ir">一些细节</strong></li><li id="b568" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la nw lq lr ls bi translated"><strong class="kh ir">架构优化</strong></li><li id="d5d4" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la nw lq lr ls bi translated"><strong class="kh ir">结果</strong></li></ol></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="1bf6" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated"><strong class="ak"> 1。编码器解码器架构</strong></h1><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nx"><img src="../Images/0cbcc4c0e188d66914cfd98f2c60106b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HeHpfq7ke9knVUbyrdaceA.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Architectures for Instance Segmentation</strong></figcaption></figure><h2 id="7e24" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">(一)传统的前馈网络</h2><ul class=""><li id="c32e" class="lk ll iq kh b ki nr kl ns ko nt ks nu kw nv la lp lq lr ls bi translated">该网络包含<strong class="kh ir">一系列卷积层</strong>与<strong class="kh ir">汇集阶段交错，减少了特征图的空间维度</strong>，随后是一个全连接层以生成对象遮罩。因此，每个像素预测都基于对象的完整视图，然而，由于多个池阶段，其<strong class="kh ir">输入特征分辨率较低。</strong></li><li id="1566" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">这种网络架构类似于<a class="ae mr" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度屏蔽</a>方法。</li></ul><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ok"><img src="../Images/52fa4ca85f8da7d1de71c0a4346de207.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eBA2myIB-FBwwhzi0SOIXA.png"/></div></div></figure><ul class=""><li id="166e" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated"><a class="ae mr" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度蒙版</a>仅粗略对齐对象边界。</li><li id="7bc1" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">锐化掩模产生更清晰、像素精确的对象掩模。</strong></li></ul><h2 id="caac" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">(b)多尺度网络</h2><ul class=""><li id="14a7" class="lk ll iq kh b ki nr kl ns ko nt ks nu kw nv la lp lq lr ls bi translated">这种架构相当于从每个网络层进行独立预测，并对结果进行上采样和平均。</li><li id="f07b" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">这种网络架构类似于<a class="ae mr" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>和<a class="ae mr" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">累积视觉 1 </a>方法(注意:它们不是用于实例分割)。</li></ul><h2 id="73e4" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">(c)编码器解码器网络和(d)细化模块</h2><ul class=""><li id="7a1d" class="lk ll iq kh b ki nr kl ns ko nt ks nu kw nv la lp lq lr ls bi translated">在自底向上通过(网络左侧)的一系列<strong class="kh ir">卷积之后，特征图非常小。</strong></li><li id="d4cf" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">这些特征图在自上而下的通道(网络的右侧)<strong class="kh ir">使用 2 倍双线性插值进行<strong class="kh ir"> 3×3 卷积和逐渐上采样</strong> <strong class="kh ir">。</strong></strong></li><li id="30b6" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">除此之外，在上采样之前，在自下而上的过程中的对应的相同大小的特征图<strong class="kh ir"> <em class="ol"> F </em> </strong>被<strong class="kh ir">连接</strong>到自上而下的过程中的掩模编码特征图<strong class="kh ir"> <em class="ol"> M </em> </strong>。</li><li id="6918" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">在每次拼接之前，还要对<em class="ol"> F </em>进行 3×3 卷积</strong>，以<strong class="kh ir">减少特征图</strong>的数量，因为直接拼接计算量很大。</li><li id="935c" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">串联也被用于许多深度学习方法，如著名的<a class="ae mr" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760"> U-Net </a>。</li><li id="41db" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">并且作者重构了精化模块，这导致了更有效的实现，如下所示:</li></ul><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi om"><img src="../Images/50a6b1a6b6f1b9e3fe4bda07a574e565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yX5DIstitqBlzhf8z8ps-A.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">(a) Original (b) Refactored but equivalent model that leads to a more effcient implementation</strong></figcaption></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="5fa3" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated"><strong class="ak"> 2。一些细节</strong></h1><p id="ee0c" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko on kq kr ks oo ku kv kw op ky kz la ij bi translated">使用 ImageNet-预训练的 50 层<a class="ae mr" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>。</p><h2 id="caff" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">两阶段训练</h2><p id="ffeb" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko on kq kr ks oo ku kv kw op ky kz la ij bi translated">首先，模型被训练以使用前馈路径联合推断粗略的逐像素分割掩模和对象分数。第二，前馈路径被“冻结”并且精化模块被训练。</p><ul class=""><li id="6114" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">可以获得更快的收敛。</li><li id="3891" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">我们可以仅使用前向路径得到粗略的掩码，或者使用自下而上和自上而下的路径得到清晰的掩码。</li><li id="6ec4" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">一旦前向分支已经收敛，整个网络的微调增益是最小的。</li></ul><h2 id="cb0a" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">在全图推理过程中</h2><ul class=""><li id="c8c8" class="lk ll iq kh b ki nr kl ns ko nt ks nu kw nv la lp lq lr ls bi translated">只有最有希望的位置被细化。优化了前 N 个评分建议窗口。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="bd86" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">3.<strong class="ak">架构优化</strong></h1><p id="bbac" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko on kq kr ks oo ku kv kw op ky kz la ij bi translated">要求降低网络的复杂性。并且发现<a class="ae mr" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">deep mask</a>40%的时间用于特征提取，40%用于掩膜预测，20%用于评分预测。</p><h2 id="de57" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">3.1.主干架构</h2><ul class=""><li id="6784" class="lk ll iq kh b ki nr kl ns ko nt ks nu kw nv la lp lq lr ls bi translated"><strong class="kh ir">输入尺寸<em class="ol"> W </em> </strong>:减小 W 会降低步幅密度 S，从而进一步损害准确性。</li><li id="d524" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">汇集层<em class="ol"> P </em> </strong>:更多的汇集层 P 导致更快的计算，但也导致特征分辨率的损失。</li><li id="05fc" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">步幅密度<em class="ol"> S </em> </strong>:在保持 W 不变的情况下使步幅加倍会大大降低性能</li><li id="f4d6" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">深度<em class="ol"> D </em></strong></li><li id="3f14" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">特征通道<em class="ol"> F </em></strong></li></ul><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi oq"><img src="../Images/e2520dfd507cedd9816cc4db3b3900f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A2ygrljAUJzMGnMXeQtV-Q.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Results for Various W, P, D, S, F</strong></figcaption></figure><ul class=""><li id="a783" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated"><strong class="kh ir"> W160-P4-D39-F128: </strong>获得速度和精度的权衡。</li><li id="182b" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">最上面一行和最后一行分别是使用不包括时间分数预测时间的多尺度推理的<a class="ae mr" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a>和夏普掩码(即 W160-P4-D39-F128)的定时。</li><li id="bbd7" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><a class="ae mr" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度蒙版</a>和锐度蒙版的总时间分别为每张图像 1.59 秒和 0.76 秒。这意味着<a class="ae mr" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度遮罩</a>和锐度遮罩的 FPS 分别约为 0.63 FPS 和 1.32 FPS。</li></ul><h2 id="c90e" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">3.2.头部结构</h2><p id="47fc" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko on kq kr ks oo ku kv kw op ky kz la ij bi translated">头部架构也消耗了模型的一定复杂度。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi or"><img src="../Images/b7b6b18b53490239eeca2c28aaf309f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U-Gf9k_HmcEDo5A6gR2fHA.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Various Head Architecture</strong></figcaption></figure><ul class=""><li id="41b1" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">(a):原始<a class="ae mr" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度遮罩</a>头部架构获取遮罩并评分。</li><li id="f5e2" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">(b)到(d):各种公共共享 conv 和全连接层，以获得遮罩和分数。</li></ul><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi os"><img src="../Images/ba1164fda374eb333a1b85e0a334e964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zP66czUmWpgXmVveZC2NdQ.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Results for Various Head ARchitectures</strong></figcaption></figure><ul class=""><li id="22e0" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">选择磁头 C 是因为其简单性和时间性。</li></ul><h2 id="b422" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">3.3.不同 Conv 的特征地图数量</h2><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ot"><img src="../Images/3f43c86f2e047a156e4540c2d835e21c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bmfafcKqt3X9aF2OldP37Q.png"/></div></div></figure><ul class=""><li id="1add" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">(a)对于所有卷积，特征图的数量是相同的。</li><li id="5a35" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">(b)特征图的数量沿自下而上的路径减少，沿自上而下的路径增加。</li><li id="b499" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">和(b)具有更短的推断时间和相似的 AUC(在 10，100，1000 个建议时 ar 的平均值)。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="0084" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">4.结果</h1><h2 id="cfb5" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">4.1.COCO 女士细分</h2><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi om"><img src="../Images/2fcc3f631ce72f24116697424c83b43e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T0mWgHoS3IE_Nji1YelcCg.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Results on MS COCO Segmentation</strong></figcaption></figure><ul class=""><li id="e467" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated"><strong class="kh ir">深面具-我们的</strong> : <a class="ae mr" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深面具</a>躯干和头部优化，比<a class="ae mr" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深面具</a>好。</li><li id="4a23" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">比以前最先进的方法更好</li><li id="8f49" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">SharpMaskZoom&amp;SharpMaskZoom</strong>:带有一个或两个额外的更小的缩放比例，并实现了对小对象的 AR 的大幅提升。</li></ul><h2 id="ad08" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">4.2.2015 年可可小姐挑战赛中的物体检测和结果</h2><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ou"><img src="../Images/0847e2caac95938b08ad838b4e38811f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HrCvdhQGCcpQ7MpAl4RHuQ.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Results on MS COCO</strong></figcaption></figure><h2 id="32fa" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">顶端</h2><ul class=""><li id="3b35" class="lk ll iq kh b ki nr kl ns ko nt ks nu kw nv la lp lq lr ls bi translated">通过对以<a class="ae mr" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a>为特征提取主干的<a class="ae mr" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快速 R-CNN </a>应用 SharpMask，即第三行 SharpMask+VGG，比选择性搜索(即原来的 <a class="ae mr" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener"> <strong class="kh ir">快速 R-CNN </strong> </a> <strong class="kh ir">)和 RPN(区域建议网络，即</strong> <a class="ae mr" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202"> <strong class="kh ir">更快 R-CNN </strong> </a></li></ul><h2 id="56bd" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">中间</h2><ul class=""><li id="d2b5" class="lk ll iq kh b ki nr kl ns ko nt ks nu kw nv la lp lq lr ls bi translated">SharpMask+MPN(另一个主干称为 MultiPathNet)，它在 COCO 女士分割挑战中获得<strong class="kh ir">第二名。</strong></li></ul><h2 id="fce7" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">底部</h2><ul class=""><li id="3e2c" class="lk ll iq kh b ki nr kl ns ko nt ks nu kw nv la lp lq lr ls bi translated">夏普面具+MPN，在可可小姐探测挑战中获得<strong class="kh ir">第二名，比<a class="ae mr" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a>强。</strong></li></ul><p id="df95" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但那一刻 SharpMask 只用<a class="ae mr" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a>做骨干。因此，结果是低劣的。</p><h2 id="1393" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">4.3.定性结果</h2><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ov"><img src="../Images/99ab266cfd40a67e09396f4869125eb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VWGc8XfotLUMo7HGE9OoAw.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">SharpMask proposals with highest IoU to the ground truth on selected COCO images. Missed objects (no matching proposals with IoU &gt; 0:5) are marked in red. The last row shows a number of failure cases.</strong></figcaption></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><p id="0a29" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过逐渐上采样，早期特征图与后期特征图连接，SharpMask 优于<a class="ae mr" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339"> DeepMask </a>。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h2 id="7ed4" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">参考</h2><p id="ffe9" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko on kq kr ks oo ku kv kw op ky kz la ij bi translated">【2016 ECCV】【sharp mask】<br/><a class="ae mr" href="https://arxiv.org/abs/1603.08695" rel="noopener ugc nofollow" target="_blank">学习提炼对象片段</a></p><h2 id="c840" class="ny na iq bd nb nz oa dn nf ob oc dp nj ko od oe nl ks of og nn kw oh oi np oj bi translated">我的相关评论</h2><p id="49a1" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko on kq kr ks oo ku kv kw op ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae mr" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae mr" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae mr" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae mr" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae mr" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae mr" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae mr" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">yolo v1</a><a class="ae mr" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a><a class="ae mr" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">yolo v2/yolo 9000</a></p><p id="e227" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/>T23】[<a class="ae mr" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a>][<a class="ae mr" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a>][<a class="ae mr" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a>][<a class="ae mr" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>][<a class="ae mr" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a>][<a class="ae mr" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a>]</strong></p><p id="22bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">生物医学图像分割<br/></strong><a class="ae mr" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a><a class="ae mr" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a><a class="ae mr" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a><a class="ae mr" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a></p><p id="2719" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">实例分割<br/></strong><a class="ae mr" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度遮罩</a></p></div></div>    
</body>
</html>