<html>
<head>
<title>Review: Stochastic Depth (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:随机深度(图像分类)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-stochastic-depth-image-classification-a4e225807f4a?source=collection_archive---------12-----------------------#2018-11-27">https://towardsdatascience.com/review-stochastic-depth-image-classification-a4e225807f4a?source=collection_archive---------12-----------------------#2018-11-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="a126" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>这个故事里，<strong class="jp ir">随机深度</strong>是简短回顾。随机深度，一个训练短网络的训练程序，在测试时使用深度网络。这是由困境所激发的:</p><blockquote class="ku kv kw"><p id="9c96" class="jn jo kx jp b jq jr js jt ju jv jw jx ky jz ka kb kz kd ke kf la kh ki kj kk ij bi translated"><strong class="jp ir">使用深度模型</strong>:我们可以得到更好的预测精度，但是由于梯度消失问题，很难训练。</p><p id="08af" class="jn jo kx jp b jq jr js jt ju jv jw jx ky jz ka kb kz kd ke kf la kh ki kj kk ij bi translated"><strong class="jp ir">使用更浅的模型</strong>:我们可以更容易地训练网络，但预测精度有时不够好。</p></blockquote><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/87ff847d7bbc8ac32d8004da4ab39883.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fKbtkmy-MvkRjuAWysEqnA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Can we?</strong></figcaption></figure><p id="2a56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过使用随机深度，网络在训练过程中被缩短，即，层的子集被随机丢弃，并通过身份函数绕过它们。并且在测试/推断期间使用完整的网络。通过这种方式:</p><ul class=""><li id="6ba5" class="ls lt iq jp b jq jr ju jv jy lu kc lv kg lw kk lx ly lz ma bi translated"><strong class="jp ir">培训时间大幅减少</strong></li><li id="2b85" class="ls lt iq jp b jq mb ju mc jy md kc me kg mf kk lx ly lz ma bi translated"><strong class="jp ir">测试误差也显著改善</strong></li></ul><p id="c2cc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是我写故事的时候在<strong class="jp ir"> 2016 ECCV </strong>上的一篇论文，有大约<strong class="jp ir"> 400 次引用</strong>。(<a class="mg mh ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----a4e225807f4a--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl mi mj hu mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="ij ik il im in"><h1 id="b594" class="mp mq iq bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">涵盖哪些内容</h1><ol class=""><li id="79c2" class="ls lt iq jp b jq nn ju no jy np kc nq kg nr kk ns ly lz ma bi translated"><strong class="jp ir">原</strong> <a class="ae nt" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="jp ir"> ResNet </strong> </a>的简要修改</li><li id="c4f0" class="ls lt iq jp b jq mb ju mc jy md kc me kg mf kk ns ly lz ma bi translated"><a class="ae nt" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="jp ir"> ResNet </strong> </a> <strong class="jp ir">带随机深度</strong></li><li id="4825" class="ls lt iq jp b jq mb ju mc jy md kc me kg mf kk ns ly lz ma bi translated"><strong class="jp ir">一些结果和分析</strong></li></ol></div><div class="ab cl mi mj hu mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="ij ik il im in"><h1 id="12bd" class="mp mq iq bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">1.原始<a class="ae nt" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>的简要修订</h1><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/896a018e6ba48a63cf6c7822429c52c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*1MAjQzn9vo20ntlzGNJwFw.png"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nv"><img src="../Images/13b8f9c5bb59c5a38b0511cba2bcb124.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gGTDsQ_ALD65vucaFVsXUQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">One l-th Residual Block (ResBlock) in Original ResNet</strong></figcaption></figure><p id="c63b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<strong class="jp ir">原</strong> <a class="ae nt" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="jp ir"> ResNet </strong> </a>中，假设<em class="kx"> H </em> _ <em class="kx"> l-1 </em>是上述 ResBlock 的输入，<em class="kx"> H </em> _ <em class="kx"> l-1 </em>会经过两条路径。</p><ul class=""><li id="8c2d" class="ls lt iq jp b jq jr ju jv jy lu kc lv kg lw kk lx ly lz ma bi translated"><strong class="jp ir">上路径<em class="kx">f _ l</em>(<em class="kx">H</em>_<em class="kx">l-1</em>)</strong>:<strong class="jp ir">conv&gt;BN&gt;ReLU&gt;conv&gt;BN</strong></li><li id="8431" class="ls lt iq jp b jq mb ju mc jy md kc me kg mf kk lx ly lz ma bi translated"><strong class="jp ir">下部路径 id(<em class="kx">H</em>_<em class="kx">l-1</em>)</strong>:<strong class="jp ir">标识路径</strong>不修改输入</li></ul><p id="bd54" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后这两路的输出<strong class="jp ir">加在一起，然后 ReLU，变成<em class="kx"> H </em> _ <em class="kx"> l </em>。</strong></p><p id="8844" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过使用相同路径，即跳过连接或快捷连接，我们可以保持输入信号，并试图避免梯度消失问题。最后，我们可以获得一个非常深的模型。</p><p id="987d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，这种深度模型的<strong class="jp ir">训练时间</strong>是<strong class="jp ir">长</strong>。</p><p id="15c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，可能会有<strong class="jp ir">过度拟合问题</strong>。</p></div><div class="ab cl mi mj hu mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="ij ik il im in"><h1 id="8e6f" class="mp mq iq bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">2.具有随机深度的 ResNet</h1><h2 id="1e35" class="nw mq iq bd mr nx ny dn mv nz oa dp mz jy ob oc nd kc od oe nh kg of og nl oh bi translated">2.1.培养</h2><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oi"><img src="../Images/cb5e7dc10237ffce2eeaf993c729284e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5uuze5xIB42hsT02p5wNA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Some ResBlocks are Dropped Randomly Based on Bernoulli Random Variable</strong></figcaption></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oj"><img src="../Images/abde7a0ebc28c553051278387f29c8d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZuCIYiOuX6E5S6fdLBvHQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Linear Decay Rule</strong></figcaption></figure><p id="7d79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过使用随机深度，在训练期间，<strong class="jp ir">对于每个小批量，每个 ResBlock 将具有“存活”概率<em class="kx"> p </em> _ <em class="kx"> l. </em> </strong></p><p id="6868" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果幸存下来，它会被保存下来。否则如上所示跳过。</p><p id="abd3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">用随机深度训练的网络可以被解释为不同深度的网络的隐含集合。</strong></p><p id="35ae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">决定<em class="kx"> p </em> _ <em class="kx"> l </em>，</p><ul class=""><li id="bdb9" class="ls lt iq jp b jq jr ju jv jy lu kc lv kg lw kk lx ly lz ma bi translated">一种是沿整个模型有<em class="kx"> p </em> _ <em class="kx"> l </em>的<strong class="jp ir">定值</strong>。</li><li id="3f34" class="ls lt iq jp b jq mb ju mc jy md kc me kg mf kk lx ly lz ma bi translated">一个是沿着整个模型对<em class="kx"> p </em> _ <em class="kx"> l </em>有<strong class="jp ir">线性衰减规则</strong>。</li></ul><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ok"><img src="../Images/e9660a039a959a3e957effd3f806b790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WsBrkierLwccNFLNZiT1Fg.png"/></div></div></figure><p id="7820" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果使用线性衰减规则，较早(较深)的层有更大的机会幸存(跳过)，如上例所示。</p><p id="f54b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">训练时，预期网络深度比整个网络深度短。</strong>若全网深度<em class="kx"> L </em> =110，<em class="kx"> p_L </em> =0.5，则期望网络深度 E(L’)= 40。<strong class="jp ir">因此，训练时间要短得多。</strong></p><h2 id="3a3c" class="nw mq iq bd mr nx ny dn mv nz oa dp mz jy ob oc nd kc od oe nh kg of og nl oh bi translated"><strong class="ak"> 2.2。测试</strong></h2><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ol"><img src="../Images/8da6ad2d329031ee117231c48fbca501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cbh6iDItxyiRALVpaYMAow.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Weighted by p_l during testing</strong></figcaption></figure><p id="cd62" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">测试期间的随机深度需要对网络进行小的修改</strong>。因为在训练期间，函数<em class="kx"> f </em> _ <em class="kx"> l </em>仅在所有更新的一小部分<em class="kx"> p </em> _ <em class="kx"> l </em>内有效，并且下一层的相应权重被校准用于该生存概率。因此，我们需要<strong class="jp ir">根据任何给定函数<em class="kx"> f </em> _ <em class="kx"> l </em>参与训练的预期次数<em class="kx"> p </em> _ <em class="kx"> l </em>来重新校准其输出。</strong></p></div><div class="ab cl mi mj hu mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="ij ik il im in"><h1 id="077e" class="mp mq iq bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">3.<strong class="ak">一些结果和分析</strong></h1><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi om"><img src="../Images/15b65fc561a2379f1a26e6dd2f73f603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fCbWyyQmNCzMBDMBS1yAtw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">CIFAR10, CIFAR100, ImageNet Results</strong></figcaption></figure><h2 id="4d06" class="nw mq iq bd mr nx ny dn mv nz oa dp mz jy ob oc nd kc od oe nh kg of og nl oh bi translated">3.1.西法尔-10，西法尔-100，SHVN</h2><p id="c41a" class="pw-post-body-paragraph jn jo iq jp b jq nn js jt ju no jw jx jy on ka kb kc oo ke kf kg op ki kj kk ij bi translated">例如在图的左上方:</p><ul class=""><li id="a45f" class="ls lt iq jp b jq jr ju jv jy lu kc lv kg lw kk lx ly lz ma bi translated">恒定深度的训练损失&lt; Training loss of Stochastic Depth</li><li id="e060" class="ls lt iq jp b jq mb ju mc jy md kc me kg mf kk lx ly lz ma bi translated">Test loss of constant depth (6.41%) &gt;随机深度的测试损失(5.25%)</li><li id="1fcf" class="ls lt iq jp b jq mb ju mc jy md kc me kg mf kk lx ly lz ma bi translated">这意味着随机深度减少了过度拟合。</li></ul><p id="b352" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用 110 或 1202 层模型的 CIFAR-100 和 SHVN 的趋势相似。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oq"><img src="../Images/3cca5d3c13f86556cc31dfa4ebe48e62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*--jnrC3zaCW1vPrXCvS3QA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Training Time</strong></figcaption></figure><ul class=""><li id="aa06" class="ls lt iq jp b jq jr ju jv jy lu kc lv kg lw kk lx ly lz ma bi translated">训练时间也缩短了很多。</li></ul><h2 id="8102" class="nw mq iq bd mr nx ny dn mv nz oa dp mz jy ob oc nd kc od oe nh kg of og nl oh bi translated">3.2.ImageNet</h2><p id="5150" class="pw-post-body-paragraph jn jo iq jp b jq nn js jt ju no jw jx jy on ka kb kc oo ke kf kg op ki kj kk ij bi translated">对于图右下方的 ImageNet:</p><ul class=""><li id="8172" class="ls lt iq jp b jq jr ju jv jy lu kc lv kg lw kk lx ly lz ma bi translated">虽然<strong class="jp ir">验证误差非常接近</strong>(恒定深度为 23.06%，随机深度为 23.38%)，但是<strong class="jp ir">训练时间缩短了 25%。</strong></li><li id="6431" class="ls lt iq jp b jq mb ju mc jy md kc me kg mf kk lx ly lz ma bi translated"><strong class="jp ir">如果训练时间相等，最终误差为 21.98%。</strong></li></ul><p id="b284" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是每个数据集的详细结果:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi or"><img src="../Images/6e6444dde08cb631aadc249b07c12397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6EFo5E2C9oBQ0DFmOcv3fg.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Detailed Results for Each Dataset</figcaption></figure><h2 id="7076" class="nw mq iq bd mr nx ny dn mv nz oa dp mz jy ob oc nd kc od oe nh kg of og nl oh bi translated">3.3.分析</h2><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi os"><img src="../Images/328bdea891fab6d9a8ae3320cf6ac153.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l5kpjdVGrVNU2eYmJcJkCA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Mean Gradient Magnitude</strong></figcaption></figure><p id="099a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过查看每个时期的平均梯度幅度，<strong class="jp ir">随机深度始终比恒定深度具有更大的权重。</strong>这意味着，<strong class="jp ir">梯度消失问题在随机深度中不太严重。</strong></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ot"><img src="../Images/4894bcd512627fe3595f308926b4e7cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jhWzFy_5cN4_7ec4eJiCQg.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Test Error vs Survival Probability (Left), and Test Error Heatmap with P_L vs Network Depth (Right)</strong></figcaption></figure><p id="082b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于左侧的测试误差与生存概率:</p><ul class=""><li id="c3a4" class="ls lt iq jp b jq jr ju jv jy lu kc lv kg lw kk lx ly lz ma bi translated">两种分配规则(线性衰减和均匀分配)都比恒定深度产生更好的结果。</li><li id="39c6" class="ls lt iq jp b jq mb ju mc jy md kc me kg mf kk lx ly lz ma bi translated">线性衰减规则始终优于均匀规则。</li><li id="ad0b" class="ls lt iq jp b jq mb ju mc jy md kc me kg mf kk lx ly lz ma bi translated">当<em class="kx"> p </em> _ <em class="kx"> L </em>在 0.4 到 0.8 范围内时，线性衰减法则获得竞争结果。</li><li id="5f6f" class="ls lt iq jp b jq mb ju mc jy md kc me kg mf kk lx ly lz ma bi translated">在<strong class="jp ir"><em class="kx">p</em>_<em class="kx">L</em>= 0.2</strong>的情况下，线性衰减的随机深度仍然表现良好，同时给出<strong class="jp ir">训练时间减少 40%</strong>。</li></ul><p id="fdec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于右侧的热图:</p><ul class=""><li id="0bba" class="ls lt iq jp b jq jr ju jv jy lu kc lv kg lw kk lx ly lz ma bi translated"><strong class="jp ir">p_<em class="kx">L</em>= 0.5 的较深网络更好。</strong></li><li id="f06c" class="ls lt iq jp b jq mb ju mc jy md kc me kg mf kk lx ly lz ma bi translated">一个足够深的模型是随机深度显著优于基线所必需的。</li></ul></div><div class="ab cl mi mj hu mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="ij ik il im in"><p id="8495" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除了丢弃之外，我们还可以<strong class="jp ir">通过使用随机深度来减少网络深度，从而减少过拟合，从而丢弃一些模块。</strong></p></div><div class="ab cl mi mj hu mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="ij ik il im in"><h1 id="8930" class="mp mq iq bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">参考</h1><p id="cddf" class="pw-post-body-paragraph jn jo iq jp b jq nn js jt ju no jw jx jy on ka kb kc oo ke kf kg op ki kj kk ij bi translated">【2016 ECCV】【随机深度】<br/> <a class="ae nt" href="https://arxiv.org/abs/1603.09382" rel="noopener ugc nofollow" target="_blank">深度随机的深度网络</a></p><h1 id="6104" class="mp mq iq bd mr ms ou mu mv mw ov my mz na ow nc nd ne ox ng nh ni oy nk nl nm bi translated">我的相关评论</h1><p id="58db" class="pw-post-body-paragraph jn jo iq jp b jq nn js jt ju no jw jx jy on ka kb kc oo ke kf kg op ki kj kk ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(想)(到)(这)(些)(人)(了)(,)(我)(们)(还)(没)(想)(要)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(要)(到)(这)(些)(人)(,)(我)(们)(就)(不)(能)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。</p></div></div>    
</body>
</html>