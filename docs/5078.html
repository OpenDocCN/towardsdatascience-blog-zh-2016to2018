<html>
<head>
<title>Ridge and Lasso Regression: L1 and L2 Regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">岭和套索回归:L1 和 L2 正则化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b?source=collection_archive---------0-----------------------#2018-09-26">https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b?source=collection_archive---------0-----------------------#2018-09-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="55d0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Scikit 的完整指南-学习</h2></div><p id="232f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从我上周讨论的非常重要的非监督学习技术<a class="ae le" rel="noopener" target="_blank" href="/dive-into-pca-principal-component-analysis-with-python-43ded13ead21">，</a>继续，今天我们将通过线性回归深入探讨监督学习，特别是两个特殊的线性回归模型——Lasso 和 Ridge regression。</p><p id="9aae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于我使用了术语“线性”,首先让我们澄清一下，线性模型是使用输入要素的线性函数来预测输出的最简单方法之一。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/c543df28a93a2a899809c296ae4e145b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0W935wbGEcLZoqXIxkuhTQ.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Linear model with n features for output prediction</figcaption></figure><p id="8074" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的等式(1.1)中，我们已经示出了基于 n 个特征的线性模型。只考虑单一特征，因为你可能已经知道<em class="lv"> w[0] </em>将是斜率，而<em class="lv"> b </em>将代表截距。线性回归寻找优化<em class="lv"> w </em>和<em class="lv"> b </em>，使得它最小化成本函数。成本函数可以写成</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lw"><img src="../Images/7531ff7fcc6af933d5bf7a6ea26873a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6hJC8qMP3AFBVuJuBzE9Hw.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Cost function for simple linear model</figcaption></figure><p id="4667" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的等式中，我假设数据集有 M 个实例和 p 个特征。一旦我们对分成训练集和测试集的数据集使用线性回归，计算训练集和测试集的分数可以给我们一个粗略的概念，即模型是过拟合还是欠拟合。如果你足够幸运的话，选择的线性模型也可以是恰到好处的！如果数据集上的特征很少，训练集和测试集的得分都很低，那么这就是拟合不足的问题。另一方面，如果我们有大量的特征，并且测试分数比训练分数相对较差，那么这就是过度概括或过度拟合的问题。<strong class="kk iu">岭和套索回归是降低模型复杂性和防止简单线性回归可能导致的过度拟合的一些简单技术</strong>。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="a931" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv">岭回归:</em> </strong> <em class="lv"> </em>在岭回归中，通过增加一个等价于系数大小平方的惩罚来改变成本函数。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi me"><img src="../Images/3e38181cb522af5863b01be29de94ef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hAGhQehrqAmT1pvz3q4t8Q.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Cost function for ridge regression</figcaption></figure><p id="3f76" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这相当于在如下条件下最小化等式 1.2 中的成本函数</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mf"><img src="../Images/424748b17fc96b5894721dd734a0440d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sC4KLMHU0j_1gR3VmlgGtg.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Supplement 1: Constrain on Ridge regression coefficients</figcaption></figure><p id="cb59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此岭回归对系数<em class="lv"> (w)施加了约束。</em>罚项(λ)正则化系数，使得如果系数取大值，优化函数被罚。因此<strong class="kk iu">，岭回归缩小了系数，有助于降低模型复杂性和多重共线性。</strong>回归情商。1.3 可以看出，当λ → 0 时，成本函数变得类似于线性回归成本函数(等式。1.2).因此<em class="lv">降低对特征的约束(低λ),模型将类似于线性回归模型。</em>让我们看一个使用波士顿数据的例子，下面是我用来描述线性回归作为岭回归的极限情况的代码</p><pre class="lg lh li lj gt mg mh mi mj aw mk bi"><span id="8dd0" class="ml mm it mh b gy mn mo l mp mq">import matplotlib.pyplot as plt<br/>import numpy as np <br/>import pandas as pd<br/>import matplotlib<br/>matplotlib.rcParams.update({'font.size': 12})</span><span id="b8f7" class="ml mm it mh b gy mr mo l mp mq">from sklearn.datasets import load_boston<br/>from sklearn.cross_validation import train_test_split<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.linear_model import Ridge</span><span id="bf45" class="ml mm it mh b gy mr mo l mp mq">boston=load_boston()<br/>boston_df=pd.DataFrame(boston.data,columns=boston.feature_names)<br/>#print boston_df.info()</span><span id="dd13" class="ml mm it mh b gy mr mo l mp mq"># add another column that contains the house prices which in scikit learn datasets are considered as target<br/>boston_df['Price']=boston.target<br/>#print boston_df.head(3)</span><span id="16f1" class="ml mm it mh b gy mr mo l mp mq">newX=boston_df.drop('Price',axis=1)<br/>print newX[0:3] # check <br/>newY=boston_df['Price']</span><span id="d5a8" class="ml mm it mh b gy mr mo l mp mq">#print type(newY)# pandas core frame</span><span id="8d94" class="ml mm it mh b gy mr mo l mp mq">X_train,X_test,y_train,y_test=train_test_split(newX,newY,test_size=0.3,random_state=3)<br/>print len(X_test), len(y_test)</span><span id="1193" class="ml mm it mh b gy mr mo l mp mq">lr = LinearRegression()<br/>lr.fit(X_train, y_train)</span><span id="87ce" class="ml mm it mh b gy mr mo l mp mq">rr = Ridge(alpha=0.01) </span><span id="5cca" class="ml mm it mh b gy mr mo l mp mq"># higher the alpha value, more restriction on the coefficients; low alpha &gt; more generalization,<br/># in this case linear and ridge regression resembles</span><span id="359a" class="ml mm it mh b gy mr mo l mp mq">rr.fit(X_train, y_train)</span><span id="b310" class="ml mm it mh b gy mr mo l mp mq">rr100 = Ridge(alpha=100) #  comparison with alpha value<br/>rr100.fit(X_train, y_train)</span><span id="e63c" class="ml mm it mh b gy mr mo l mp mq">train_score=lr.score(X_train, y_train)<br/>test_score=lr.score(X_test, y_test)</span><span id="3e11" class="ml mm it mh b gy mr mo l mp mq">Ridge_train_score = rr.score(X_train,y_train)<br/>Ridge_test_score = rr.score(X_test, y_test)</span><span id="460b" class="ml mm it mh b gy mr mo l mp mq">Ridge_train_score100 = rr100.score(X_train,y_train)<br/>Ridge_test_score100 = rr100.score(X_test, y_test)</span><span id="8970" class="ml mm it mh b gy mr mo l mp mq">plt.plot(rr.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Ridge; $\alpha = 0.01$',zorder=7) </span><span id="3341" class="ml mm it mh b gy mr mo l mp mq">plt.plot(rr100.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Ridge; $\alpha = 100$') </span><span id="c71e" class="ml mm it mh b gy mr mo l mp mq">plt.plot(lr.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Linear Regression')</span><span id="f583" class="ml mm it mh b gy mr mo l mp mq">plt.xlabel('Coefficient Index',fontsize=16)<br/>plt.ylabel('Coefficient Magnitude',fontsize=16)<br/>plt.legend(fontsize=13,loc=4)<br/>plt.show()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ms"><img src="../Images/40859994cc76ec3c9cc4dfcc93901022.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S1i1O3HZMD4FsHTFN5_vGQ.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Figure 1: Ridge regression for different values of alpha is plotted to show linear regression as limiting case of ridge regression. Source: Author.</figcaption></figure><p id="7a7d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来理解上图。在 X 轴上，我们绘制了系数索引，对于波士顿数据，有 13 个特征(对于 Python，第 0 个索引是指第 1 个特征)。对于较低的α值(0.01)，当系数限制较少时，系数的大小几乎与线性回归相同。对于较高的α (100)值，我们看到，与线性回归情况相比，系数指数 3、4、5 的幅度要小得多。这是一个使用岭回归<em class="lv">缩小</em>系数大小的例子。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="7e56" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> Lasso 回归:</strong>Lasso(最小绝对收缩和选择算子)回归的成本函数可以写成</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mt"><img src="../Images/13fe5226e34f5af23bfb684872cf278d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P5Lq5mAi4WAch7oIeiS3WA.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Cost function for Lasso regression</figcaption></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mu"><img src="../Images/a1b8ecc03f6dc6d73961170817d2eaee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JH9eAS2I9mwOpuFLg-gD6g.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Supplement 2: Lasso regression coefficients; subject to similar constrain as Ridge, shown before.</figcaption></figure><p id="4a33" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就像岭回归成本函数一样，对于λ= 0，上面的等式简化为等式 1.2。<em class="lv">唯一的区别是，不考虑系数的平方，而是考虑幅度。</em>这种类型的正则化(L1)可导致零系数，即某些特征在输出评估中被完全忽略。<strong class="kk iu">因此，Lasso 回归不仅有助于减少过度拟合，还能帮助我们进行特征选择。</strong>正如岭回归一样，可以控制正则化参数(λ),我们将使用<code class="fe mv mw mx mh b">sklearn</code>中的癌症数据集看到以下效果。我使用癌症数据而不是我以前使用的波士顿房屋数据的原因是，癌症数据集有 30 个特征，而波士顿房屋数据只有 13 个特征。因此，通过改变正则化参数可以很好地描述 Lasso 回归的特征选择。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi my"><img src="../Images/d10bd1049f75be2942567af4446fb179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9gPxjrEAkqWV5tzPEgSkZw.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Figure 2: Lasso regression and feature selection dependence on the regularization parameter value. Source: Author.</figcaption></figure><p id="ec6e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我用来绘制这些图的代码如下</p><pre class="lg lh li lj gt mg mh mi mj aw mk bi"><span id="020e" class="ml mm it mh b gy mn mo l mp mq">import math <br/>import matplotlib.pyplot as plt <br/>import pandas as pd<br/>import numpy as np</span><span id="c6e4" class="ml mm it mh b gy mr mo l mp mq"># difference of lasso and ridge regression is that some of the coefficients can be zero i.e. some of the features are <br/># completely neglected</span><span id="0069" class="ml mm it mh b gy mr mo l mp mq">from sklearn.linear_model import Lasso<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.cross_validation import train_test_split</span><span id="e235" class="ml mm it mh b gy mr mo l mp mq">cancer = load_breast_cancer()<br/>#print cancer.keys()</span><span id="28f5" class="ml mm it mh b gy mr mo l mp mq">cancer_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)</span><span id="a0d7" class="ml mm it mh b gy mr mo l mp mq">#print cancer_df.head(3)</span><span id="1383" class="ml mm it mh b gy mr mo l mp mq">X = cancer.data<br/>Y = cancer.target</span><span id="333a" class="ml mm it mh b gy mr mo l mp mq">X_train,X_test,y_train,y_test=train_test_split(X,Y, test_size=0.3, random_state=31)</span><span id="96ed" class="ml mm it mh b gy mr mo l mp mq">lasso = Lasso()<br/>lasso.fit(X_train,y_train)<br/>train_score=lasso.score(X_train,y_train)<br/>test_score=lasso.score(X_test,y_test)<br/>coeff_used = np.sum(lasso.coef_!=0)</span><span id="d4f2" class="ml mm it mh b gy mr mo l mp mq">print "training score:", train_score <br/>print "test score: ", test_score<br/>print "number of features used: ", coeff_used</span><span id="9094" class="ml mm it mh b gy mr mo l mp mq">lasso001 = Lasso(alpha=0.01, max_iter=10e5)<br/>lasso001.fit(X_train,y_train)</span><span id="9aff" class="ml mm it mh b gy mr mo l mp mq">train_score001=lasso001.score(X_train,y_train)<br/>test_score001=lasso001.score(X_test,y_test)<br/>coeff_used001 = np.sum(lasso001.coef_!=0)</span><span id="c3a2" class="ml mm it mh b gy mr mo l mp mq">print "training score for alpha=0.01:", train_score001 <br/>print "test score for alpha =0.01: ", test_score001<br/>print "number of features used: for alpha =0.01:", coeff_used001</span><span id="805d" class="ml mm it mh b gy mr mo l mp mq">lasso00001 = Lasso(alpha=0.0001, max_iter=10e5)<br/>lasso00001.fit(X_train,y_train)</span><span id="56d2" class="ml mm it mh b gy mr mo l mp mq">train_score00001=lasso00001.score(X_train,y_train)<br/>test_score00001=lasso00001.score(X_test,y_test)<br/>coeff_used00001 = np.sum(lasso00001.coef_!=0)</span><span id="6e0e" class="ml mm it mh b gy mr mo l mp mq">print "training score for alpha=0.0001:", train_score00001 <br/>print "test score for alpha =0.0001: ", test_score00001<br/>print "number of features used: for alpha =0.0001:", coeff_used00001</span><span id="dd11" class="ml mm it mh b gy mr mo l mp mq">lr = LinearRegression()<br/>lr.fit(X_train,y_train)<br/>lr_train_score=lr.score(X_train,y_train)<br/>lr_test_score=lr.score(X_test,y_test)</span><span id="e5f9" class="ml mm it mh b gy mr mo l mp mq">print "LR training score:", lr_train_score <br/>print "LR test score: ", lr_test_score</span><span id="eeb3" class="ml mm it mh b gy mr mo l mp mq">plt.subplot(1,2,1)<br/>plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\alpha = 1$',zorder=7) # alpha here is for transparency<br/>plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\alpha = 0.01$') # alpha here is for transparency<br/><br/>plt.xlabel('Coefficient Index',fontsize=16)<br/>plt.ylabel('Coefficient Magnitude',fontsize=16)<br/>plt.legend(fontsize=13,loc=4)</span><span id="9185" class="ml mm it mh b gy mr mo l mp mq">plt.subplot(1,2,2)</span><span id="6cbe" class="ml mm it mh b gy mr mo l mp mq">plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\alpha = 1$',zorder=7) # alpha here is for transparency<br/>plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\alpha = 0.01$') # alpha here is for transparency<br/>plt.plot(lasso00001.coef_,alpha=0.8,linestyle='none',marker='v',markersize=6,color='black',label=r'Lasso; $\alpha = 0.00001$') # alpha here is for transparency<br/>plt.plot(lr.coef_,alpha=0.7,linestyle='none',marker='o',markersize=5,color='green',label='Linear Regression',zorder=2)</span><span id="2922" class="ml mm it mh b gy mr mo l mp mq">plt.xlabel('Coefficient Index',fontsize=16)<br/>plt.ylabel('Coefficient Magnitude',fontsize=16)<br/>plt.legend(fontsize=13,loc=4)<br/>plt.tight_layout()<br/>plt.show()</span><span id="2147" class="ml mm it mh b gy mr mo l mp mq"><br/>#output </span><span id="c5ad" class="ml mm it mh b gy mr mo l mp mq">training score: 0.5600974529893081<br/>test score:  0.5832244618818156<br/>number of features used:  4</span><span id="58dd" class="ml mm it mh b gy mr mo l mp mq">training score for alpha=0.01: 0.7037865778498829<br/>test score for alpha =0.01:  0.664183157772623<br/>number of features used: for alpha =0.01: 10</span><span id="da75" class="ml mm it mh b gy mr mo l mp mq">training score for alpha=0.0001: 0.7754092006936697<br/>test score for alpha =0.0001:  0.7318608210757904<br/>number of features used: for alpha =0.0001: 22</span><span id="32cf" class="ml mm it mh b gy mr mo l mp mq">LR training score: 0.7842206194055068<br/>LR test score:  0.7329325010888681</span></pre><p id="5c8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们用简短的总结来理解剧情和代码。</p><ul class=""><li id="255e" class="mz na it kk b kl km ko kp kr nb kv nc kz nd ld ne nf ng nh bi translated">Lasso 回归中正则化参数的默认值(由α给出)为 1。</li><li id="436a" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">由此，在癌症数据集中的 30 个特征中，仅使用了 4 个特征(系数的非零值)。</li><li id="dd5a" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">训练和测试分数(只有 4 个特征)都很低；得出结论，该模型不符合癌症数据集。</li><li id="9b20" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">通过减少 alpha 和增加迭代次数来减少这种欠拟合。现在α = 0.01，非零特征=10，训练和测试分数增加。</li><li id="6e94" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">图 2 的左图显示了两个不同α值的系数大小的比较。对于α= 1，我们可以看到大部分系数为零或接近零，而α= 0.01 则不是这样。</li><li id="c891" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">进一步减少α =0.0001，非零特征= 22。训练和测试分数类似于基本线性回归情况。</li><li id="ae72" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">在右图中，当α = 0.0001 时，套索回归和线性回归的系数非常相似。</li></ul></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h2 id="b8cf" class="ml mm it bd nn no np dn nq nr ns dp nt kr nu nv nw kv nx ny nz kz oa ob oc od bi translated">套索正则化如何导致特征选择？</h2><p id="a2da" class="pw-post-body-paragraph ki kj it kk b kl oe ju kn ko of jx kq kr og kt ku kv oh kx ky kz oi lb lc ld im bi translated">到目前为止，我们已经学习了岭和套索回归的基础知识，并看到了一些理解应用的例子。现在，我将尝试解释为什么套索回归可以导致特征选择，而岭回归只减少接近零的系数，而不是零。下面的插图将帮助我们更好地理解，我们将假设一个只有两个特征的假设数据集。使用脊和套索回归系数的约束(如以上补充 1 和 2 所示)，我们可以绘制下图</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oj"><img src="../Images/9d6d9e7999915b97105239cf7efd44c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jd03Hyt2bpEv1r7UijLlpg.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Figure 3: Why LASSO can reduce dimension of feature space? Example on 2D feature space. Modified from the plot used in ‘The Elements of Statistical Learning’ by Author.</figcaption></figure><p id="594a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于二维特征空间，用青色和绿色绘制套索和脊回归的约束区域(见附录 1 和 2)。椭圆轮廓是线性回归的成本函数(等式)。1.2).现在，如果我们放宽了系数的条件，那么约束区域会变得更大，最终它们会到达椭圆的中心。当岭和套索回归类似于线性回归结果时就是这种情况。否则，<strong class="kk iu">两种方法都通过找到椭圆轮廓碰到约束区域的第一个点来确定系数。菱形(套索)在轴上有角，不像圆盘，每当椭圆区域碰到这样的点，其中一个特征完全消失！</strong>对于更高维度的特征空间，使用 Lasso 回归可以在轴上有许多解决方案，因此我们只选择重要的特征。</p><p id="a9d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，为了结束这次冥想，让我们总结一下到目前为止我们学到了什么</p><ol class=""><li id="2e02" class="mz na it kk b kl km ko kp kr nb kv nc kz nd ld ok nf ng nh bi translated">脊套回归的代价函数和正则项的重要性。</li><li id="f261" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ok nf ng nh bi translated">通过一些使用简单数据集的例子来理解线性回归作为套索和岭回归的极限情况。</li><li id="dd40" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ok nf ng nh bi translated">理解了为什么 Lasso 回归可以导致特征选择，而 Ridge 只能将系数缩小到接近于零。</li></ol><p id="6433" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于进一步的阅读，我建议“统计学习的元素”；J. Friedman 等人，Springer，第 79-91 页，2008 年。这里展示的使用 L1 和 L2 来演示正则化的例子受到了 Andreas Muller 的《使用 Python 进行机器学习》一书的影响。</p><p id="7393" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望你喜欢这篇文章，并保持快乐！干杯！</p><p id="4194" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">页（page 的缩写）请参阅阿坎沙·拉瓦特的评论，他对在应用岭回归算法之前标准化变量提出了批评意见。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="d79b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv">如果你对更深入的基础机器学习概念感兴趣，可以考虑加盟 Medium 使用</em> </strong> <a class="ae le" href="https://saptashwa.medium.com/membership" rel="noopener"> <strong class="kk iu"> <em class="lv">我的链接</em> </strong> </a> <strong class="kk iu"> <em class="lv">。你不用额外付钱，但我会得到一点佣金。感谢大家！！</em>T15】</strong></p><div class="ol om gp gr on oo"><a href="https://medium.com/@saptashwa/membership?source=publishing_settings-------------------------------------" rel="noopener follow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">通过我的推荐链接加入媒体</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">更多来自 Saptashwa(以及媒体上的所有其他作者)。你的会员费直接支持 Saptashwa 和其他作家…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">medium.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc lp oo"/></div></div></a></div></div></div>    
</body>
</html>