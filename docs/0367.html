<html>
<head>
<title>How to Do Linear Regression using Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用梯度下降进行线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-do-linear-regression-using-gradient-descent-79a2ff4ace05?source=collection_archive---------3-----------------------#2017-04-23">https://towardsdatascience.com/how-to-do-linear-regression-using-gradient-descent-79a2ff4ace05?source=collection_archive---------3-----------------------#2017-04-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/1d438433cca58ac77a1c1b39850a343d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YVFXttuvC7ip9vlwlfI5TA.png"/></div></div></figure><p id="b00b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">梯度下降是我从 Siraj Raval 的<a class="ae kw" href="https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101" rel="noopener ugc nofollow" target="_blank">深度学习基础纳米学位</a>中学到的第一个有趣的话题。这个练习的回购可以在<a class="ae kw" href="https://github.com/llSourcell/linear_regression_live" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><p id="a458" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个练习的目的是研究学生的考试成绩和学习时间之间的关系。为了实现这一目标，我们使用称为线性回归的策略来模拟因变量(学生的考试成绩)和解释变量(学习时间)之间的关系。我们还使用梯度下降来进一步优化我们的模型。梯度下降可能是机器学习和深度学习中最流行的方法。</p><p id="4114" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的<a class="ae kw" href="https://raw.githubusercontent.com/llSourcell/linear_regression_live/master/data.csv" rel="noopener ugc nofollow" target="_blank">数据集</a>包含 x-y 平面中的 x 和 y 数据点的集合，其中 x 是学生的考试分数，y 是学生学习的小时数。</p><p id="66e0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们使用<em class="kx"> Python </em>以编程方式计算最佳拟合线，该线描述了学生学习的小时数和学生在测试中获得的分数之间的线性关系。我们使用<em class="kx"> Python </em>是因为它被认为是最流行和最具语言性的机器学习框架。</p><p id="b941" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将从一个非常标准的起始代码开始。</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="7ff1" class="lh li iq ld b gy lj lk l ll lm">if __name__ = '__main__':<br/>    run()</span></pre><p id="4bf8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，我们使用<em class="kx"> numpy </em>在内存中提取并解析我们的数据集，以在其上运行算法。我们使用分隔符'，'，意思是用来在。csv 文件</p><p id="d3d5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">学习率是超级参数(即我们用作模型的调谐旋钮，或者模型学习的速度)</p><p id="a8f4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果学习率太低，我们的模型将太慢而无法收敛，如果学习率太高，它将永远无法收敛。我们想要达到一个平衡，一个最佳的学习速度。在机器学习中，我们并不总是知道最佳学习速率会是多少，所以猜测和检查是获得该值的最佳方式</p><p id="6f26" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们从初始 m 和 b 值= 0 <em class="kx">(方程 y = mx+b) </em>开始。我们从 0 开始，因为我们将随着时间的推移学习这些值。迭代次数= 1000(即我们希望从每个训练步骤中学习多少次迭代)。我们选择 1000 步，因为我们数据集很小。随着数据集变大，考虑到 CPU 等因素，迭代次数应该是 10K 或 100K</p><p id="082d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们通过使用<em class="kx">gradient _ descent _ runner</em>函数计算 b 和 m，函数的输入为“点”——x，y 点数组，初始 b 值为起始 b 值，初始 m 值为初始 m 值，学习速率和上面定义的迭代次数</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="b46b" class="lh li iq ld b gy lj lk l ll lm">from numpy import *</span><span id="dbda" class="lh li iq ld b gy ln lk l ll lm">def run():</span><span id="b691" class="lh li iq ld b gy ln lk l ll lm">    points = genfromtext('data.csv', delimiter=',')<br/>    learning_rate = 0.0001</span><span id="ee62" class="lh li iq ld b gy ln lk l ll lm">    #y = mx + b (slope formula)</span><span id="22b2" class="lh li iq ld b gy ln lk l ll lm">    initial_b = 0<br/>    initial_m = 0 # ideal slope, will start with 0</span><span id="fcaa" class="lh li iq ld b gy ln lk l ll lm">    num_iterations = 1000<br/>    <br/>    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)<br/>    <br/>    print "After {0} iterations b = {1}, m = {2}, error = {3}".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points))</span></pre><p id="ea9d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kx"> gradient_descent_runner </em>函数定义如下:</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="a568" class="lh li iq ld b gy lj lk l ll lm">def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):<br/> <br/>    b = starting_b<br/>    m = starting_m</span><span id="9920" class="lh li iq ld b gy ln lk l ll lm">    for i in range(num_iterations):<br/>        b, m = step_gradient(b,m, array(points), learning_rate</span><span id="02ba" class="lh li iq ld b gy ln lk l ll lm"> return [b, m]</span></pre><p id="1c2f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里，我们将 starting_b 和 starting_m 值赋给 b 和 m，对于每次迭代，我们将通过将 b 和 m 的先前值输入 step_gradient 函数来计算 b 和 m，该函数接受 b、m 的值、点 x、y 的数组和学习速率</p><p id="cf38" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后我们返回这个最佳对 b 和 m。</p><p id="1fed" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我们编写“step_gradient”函数之前，我们将编写另一个函数，该函数计算给定一组点的 b 和 m 的线性模型的误差平方和的平均值:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lo"><img src="../Images/2839903060b85a6bd9e6fba8b0e22902.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A11vh24lKdV7y0gXb8OOfg.png"/></div></div></figure><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="b5ce" class="lh li iq ld b gy lj lk l ll lm">def compute_error_for_line_given_points(b, m, points):<br/>    totalError = 0<br/>    for i in range(0, len(points)):<br/>    x = points[i, 0]<br/>    y = points[i, 1]</span><span id="c8ab" class="lh li iq ld b gy ln lk l ll lm">    totalError += (y - (m * x + b)) ** 2</span><span id="be7f" class="lh li iq ld b gy ln lk l ll lm">    return totalError / float(len(points))</span></pre><p id="e73b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了确定如何用给定的点集来最好地拟合我们的模型，我们希望最小化这些点到我们的线性模型之间的距离。计算总误差有助于我们确定我们的模型有多差，这样我们就可以每一步都更新它。“y”是真实数据，mx+b 是我们的模型用来预测“y”的数据，取这些值之间的差值，就得到我们模型的正(或负)误差值。我们迭代地对这些平方差求和，并除以点数，以便获得我们的线性模型的误差平方和。3D 下图显示了所有可能的 x 轴截距、y 轴截距和误差值。我们想找到误差最小的点，即曲线的底部(或局部最小值)</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/03da653e898759e014f929098ad2b444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_BCwJrbfK-a9nNQikPM_Vg.png"/></div></figure><p id="9afa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，我们继续为我们的模型编写阶跃梯度函数。梯度可以最好地理解为斜率、切线或移动方向(向上或向下),以便最小化误差。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lq"><img src="../Images/e2ec3014470af58aa5bc762fa2097b6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xihrqM-cOGFWlLBHv_xOLA.png"/></div></div></figure><p id="d361" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了计算梯度，我们计算 b 和 m 的偏导数。然后，我们通过从学习率和每个值的梯度值的乘积中减去 b 和 m 的当前值来计算新的 b 和 m 值</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lr"><img src="../Images/78c1afe704e9ee613686a637e7bfd004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*GqEcPRYV2Ppug7uB5tJVGQ.png"/></div></div></figure><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="d77f" class="lh li iq ld b gy lj lk l ll lm">def step_gradient(b_current, m_current, points, learning_rate):<br/> <br/>    b_gradient = 0<br/>    m_gradient = 0<br/>    N = float(len(points))</span><span id="d511" class="lh li iq ld b gy ln lk l ll lm">    for i in range(0, len(points)):<br/>        x = points[i, 0]<br/>        y = points[i, 1]</span><span id="c3b9" class="lh li iq ld b gy ln lk l ll lm">        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))<br/>        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))</span><span id="74a4" class="lh li iq ld b gy ln lk l ll lm">    new_b = b_current - (learning_rate * b_gradient)<br/>    new_m = m_current - (learning_rate * m_gradient)</span><span id="a586" class="lh li iq ld b gy ln lk l ll lm">    return [new_b, new_m]</span></pre><p id="a055" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里的学习率允许我们的模型学习并收敛于 b 和 m 的新的优化值，然后用于根据给定的学习小时数预测测试分数</p><p id="5c4a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">完整代码如下:</p><pre class="ky kz la lb gt lc ld le lf aw lg bi"><span id="41c8" class="lh li iq ld b gy lj lk l ll lm">from numpy import *</span><span id="2200" class="lh li iq ld b gy ln lk l ll lm">def compute_error_for_line_given_points(b, m, points):<br/>    totalError = 0<br/>    for i in range(0, len(points)):<br/>        x = points[i, 0]<br/>        y = points[i, 1]</span><span id="7065" class="lh li iq ld b gy ln lk l ll lm">        totalError += (y - (m * x + b)) ** 2</span><span id="2d78" class="lh li iq ld b gy ln lk l ll lm">    return totalError / float(len(points))</span><span id="e70c" class="lh li iq ld b gy ln lk l ll lm">def step_gradient(b_current, m_current, points, learning_rate):<br/> <br/>    b_gradient = 0<br/>    m_gradient = 0<br/>    N = float(len(points))</span><span id="76ce" class="lh li iq ld b gy ln lk l ll lm">    for i in range(0, len(points)):<br/>        x = points[i, 0]<br/>        y = points[i, 1]</span><span id="1ee7" class="lh li iq ld b gy ln lk l ll lm">        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))<br/>        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))</span><span id="5737" class="lh li iq ld b gy ln lk l ll lm">    new_b = b_current - (learning_rate * b_gradient)<br/>    new_m = m_current - (learning_rate * m_gradient)</span><span id="1bd6" class="lh li iq ld b gy ln lk l ll lm">    return [new_b, new_m]</span><span id="b5a6" class="lh li iq ld b gy ln lk l ll lm">def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):<br/> <br/>    b = starting_b<br/>    m = starting_m</span><span id="e264" class="lh li iq ld b gy ln lk l ll lm">    for i in range(num_iterations):<br/>        b, m = step_gradient(b,m, array(points), learning_rate</span><span id="6866" class="lh li iq ld b gy ln lk l ll lm">    return [b, m]</span><span id="9c14" class="lh li iq ld b gy ln lk l ll lm">def run():</span><span id="fb69" class="lh li iq ld b gy ln lk l ll lm">    points = genfromtext('data.csv', delimiter=',')<br/>    learning_rate = 0.0001</span><span id="958d" class="lh li iq ld b gy ln lk l ll lm">    #y = mx + b (slope formula)</span><span id="2044" class="lh li iq ld b gy ln lk l ll lm">    initial_b = 0<br/>    initial_m = 0 # ideal slope, will start with 0</span><span id="443c" class="lh li iq ld b gy ln lk l ll lm">    num_iterations = 1000<br/>    <br/>    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)<br/>    <br/>    print "After {0} iterations b = {1}, m = {2}, error = {3}".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points))</span><span id="3b8f" class="lh li iq ld b gy ln lk l ll lm">if __name__ = '__main__':<br/>    run()</span></pre><p id="b11b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">来源:</p><p id="19e7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">1/ Siraj Raval 的《如何用梯度下降法做线性回归:【https://www.youtube.com/watch?v=XdM6ER7zTLk<a class="ae kw" href="https://www.youtube.com/watch?v=XdM6ER7zTLk" rel="noopener ugc nofollow" target="_blank"/></p><p id="c82f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2/深度学习纳米度:</p><p id="081a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101" rel="noopener ugc nofollow" target="_blank">https://www . uda city . com/course/deep-learning-nano degree-foundation-nd 101</a></p></div></div>    
</body>
</html>