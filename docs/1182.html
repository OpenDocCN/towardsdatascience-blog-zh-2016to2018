<html>
<head>
<title>Multi-state LSTMs for categorical features</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类特征的多态 LSTMs</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-state-lstms-for-categorical-features-66cc974df1dc?source=collection_archive---------2-----------------------#2017-08-07">https://towardsdatascience.com/multi-state-lstms-for-categorical-features-66cc974df1dc?source=collection_archive---------2-----------------------#2017-08-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="9efd" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">语境</h1><p id="8edd" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ir">神经网络</strong>现在在很多方面都有广泛的应用。从图像标题生成到乳房<strong class="kn ir"> </strong>癌症<strong class="kn ir"> </strong>预测，这种巨大的应用多样性是各种重要的<strong class="kn ir">神经架构</strong> ( <strong class="kn ir">前馈</strong>神经网络、<strong class="kn ir">卷积</strong>神经网络等)的自然结果。在所有这些架构中，<strong class="kn ir">长短期记忆</strong>(<strong class="kn ir">LSTM</strong>)——递归神经网络的一个特例——已经被证明在诸如<a class="ae lj" href="https://www.tensorflow.org/tutorials/seq2seq" rel="noopener ugc nofollow" target="_blank">机器翻译</a>、<a class="ae lj" href="http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/" rel="noopener ugc nofollow" target="_blank">时间序列预测</a>或任何数据为<strong class="kn ir">顺序</strong>的任务上非常成功。这主要是由于他们能够记忆相对的<strong class="kn ir">长期相关性</strong>，这是通过考虑先前的信息以进行进一步预测来实现的。</p><blockquote class="lk"><p id="32bf" class="ll lm iq bd ln lo lp lq lr ls lt li dk translated">但是仅靠 LSTMs 是不够的。有时我们需要调整这些层，使它们适应手头的任务。</p></blockquote><p id="38b7" class="pw-post-body-paragraph kl km iq kn b ko lu kq kr ks lv ku kv kw lw ky kz la lx lc ld le ly lg lh li ij bi translated">在<a class="ae lj" href="https://www.kwyk.fr/" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> Kwyk </strong> </a>，我们提供<strong class="kn ir">在线数学练习</strong>。每次回答一个练习，我们都会收集一些数据，然后使用网站为每个学生量身定制作业。为了确定哪些练习最有可能让学生进步，我们需要知道他/她在任何给定的时间点上成功完成每个练习的可能性有多大。通过正确预测这些<strong class="kn ir">成功概率</strong>，我们可以选择<strong class="kn ir">最大化整体进度</strong>的作业。</p><p id="57a2" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">这篇文章旨在介绍我们改进的 LSTM 单元“<strong class="kn ir">多态 LSTM </strong>”，我们的模型就是基于它来尝试解决这个问题的。</p><h1 id="9117" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">快速数据概述</h1><p id="bf09" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">当一个练习被回答时，我们保存关于学生(或“用户”)和练习的信息，以及一个额外的<strong class="kn ir">分数</strong>值<strong class="kn ir"> </strong>，该值是(0)还是(1)取决于用户的成功。我们收集的信息以<strong class="kn ir">分类特征</strong>的形式告诉我们:“<strong class="kn ir">是哪个练习？</strong>“，”<strong class="kn ir">是哪一章？</strong>、<strong class="kn ir">、</strong>、<strong class="kn ir">是什么学生？</strong>、<strong class="kn ir">、</strong>、<strong class="kn ir">他/她是哪个年级的？</strong>“…这会产生具有几十到几千种形态的特征，我们用它们来预测“得分”变量并获得<strong class="kn ir">成功概率</strong>。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi me"><img src="../Images/6271d7aae99bf7510463c80a2485e473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0A4Qbm1KAeZiRdaBjeW-A.png"/></div></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">A sample of our data</figcaption></figure><h1 id="5204" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">基于 LSTM 的模型</h1><p id="ca2f" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">选择 LSTM 模式背后有两个主要动机。</p><p id="4e88" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">第一个原因是我们使用的所有特征都是绝对的。这就是我们想要学习的真正信息来源。通过手动烹饪特征，如之前得分的移动平均值，我们会因自己的主观选择而引入<strong class="kn ir">偏差</strong>。因此，我们选择依靠<strong class="kn ir">神经网络，</strong>直接反馈我们的数据，让特征自动形成<strong class="kn ir"/><strong class="kn ir">客观形成</strong>。</p><p id="8f33" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">第二个动机是在所有可用的架构中选择 LSTM。这仅仅是因为我们相信，对于每个学生来说，每次练习成功的概率<strong class="kn ir">取决于所有之前的时间结果</strong>，因此<strong class="kn ir">是连续的</strong>。</p><blockquote class="lk"><p id="af69" class="ll lm iq bd ln lo lp lq lr ls lt li dk translated">考虑到这一点，我们想到了一种架构，其中对于每个分类特征，共享的 LSTM 将保存每个模态的先前结果的历史。</p></blockquote><p id="3d91" class="pw-post-body-paragraph kl km iq kn b ko lu kq kr ks lv ku kv kw lw ky kz la lx lc ld le ly lg lh li ij bi translated">让我们以“<strong class="kn ir"> user_id </strong>”特性为例。在这种情况下，我们需要改编一个 LSTM 细胞来即时记忆每个学生过去如何成功的历史。这是通过在基本的 LSTM 单元中添加我们称之为“T42”的多状态“T43”来实现的:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi mv"><img src="../Images/a308569b5240ab1d39d3294492bc4c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ad9H0gBVw12dIZ2nEB6Lxw.png"/></div></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk"><strong class="bd mw">Left</strong> : Basic LSTM cell / <strong class="bd mw">Right</strong> : Multi-state LSTM cell. Note : If you are not familiar with LSTMs you can refer to <a class="ae lj" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">this great post by Christopher Olah</a>.</figcaption></figure><p id="a082" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">每次一个学生的标签被馈送到多状态 LSTM 单元，它就开始查找相应的<strong class="kn ir">状态</strong>和<strong class="kn ir">以前的分数</strong>。然后，学生的状态被发送到<strong class="kn ir">更新</strong>一个<strong class="kn ir">共享 LSTM 单元</strong>。同时，先前的分数被馈送到这个 LSTM 单元，该单元产生一个<strong class="kn ir">输出</strong>。接下来，<strong class="kn ir"> </strong>产生的<strong class="kn ir">状态</strong>返回到<strong class="kn ir">更新</strong>多状态中的学生数据。最后，一旦我们观察到实际的<strong class="kn ir">分数</strong>，这个值又被发送到<strong class="kn ir">更新</strong>学生在多状态中的先前分数。</p><p id="a415" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">这个基本 LSTM 单元的修改版本工作方式类似，但有一个额外的优势:它现在可以直接接受一系列学生标签作为输入:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi mx"><img src="../Images/4e7bb349a155ba63e10da8610c6eb95f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oeCFtXIquaO4KlDoFWr_zg.png"/></div></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">An unrolled Multi-state LSTM.</figcaption></figure><p id="574c" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">在前面的例子中，我们考虑了“user_id”分类特性，但这实际上可以应用于我们所有的分类特性。事实上，举例来说，在“<strong class="kn ir"> exercise_id </strong>”功能上使用多状态 LSTM 将导致它学习每个练习的<strong class="kn ir">历史成功率</strong>。从这里开始，我们使用的完整网络非常简单:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi mx"><img src="../Images/51258eccfc4b97b882f6e99d17688670.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r9I7vzqh7CFsImARNxg71Q.png"/></div></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">The complete graph we used to solve our problem.</figcaption></figure><p id="e5da" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">总而言之，这种架构试图学习每个分类特征的每个模态的单独历史，然后使用 LSTM 的输出来预测给定学生在给定练习中的成功概率。</p><h1 id="4920" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">结果</strong></h1><h2 id="4df0" class="my jo iq bd jp mz na dn jt nb nc dp jx kw nd ne kb la nf ng kf le nh ni kj nj bi translated">基线</h2><p id="e764" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了评估我们基于 LSTM 的模型，我们需要一个基准。在尝试任何深度学习方法之前，我们曾经有一个 boosting 算法(<a class="ae lj" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir">梯度 Boosting </strong> </a>)使用了一些手工特征。从所有的分类特征中，我们为每个特征的每个模态编造了一些先前得分的<strong class="kn ir">快</strong> <a class="ae lj" href="https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir">移动平均值</strong> </a> <strong class="kn ir"> </strong>。然后，我们将这些特征输入到梯度推进分类器中，让奇迹发生。</p><h2 id="f50e" class="my jo iq bd jp mz na dn jt nb nc dp jx kw nd ne kb la nf ng kf le nh ni kj nj bi translated">比较</h2><p id="ba79" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">两个分类器都有相同的方法。事实上，基线模型使用手工移动平均线作为特征，基于 LSTM 的方法自动学习这些历史来预测分数。果不其然，两种算法得到了相近的准确率分数(基线:74.61%，LSTM 基网络:<strong class="kn ir"> 75.34% </strong>)。<strong class="kn ir"> </strong>但预测的分布却大不相同:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi nk"><img src="../Images/70ae12759efb5acaf119dcb2a4a7fbbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fw4WfcywFabOsx-pJE-ymw.png"/></div></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">We can see that the LSTM-based model is much better at segregating students.</figcaption></figure><p id="cdf2" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">事实上，基于 LSTM 的模型似乎将<em class="mu">分成了 3 到 4 个不同的人群</em>:</p><ul class=""><li id="acbf" class="nl nm iq kn b ko lz ks ma kw nn la no le np li nq nr ns nt bi translated">一个给不应该能答对的学生(概率&lt;10%)</li><li id="462b" class="nl nm iq kn b ko nu ks nv kw nw la nx le ny li nq nr ns nt bi translated">One for those who have a moderate to good chance to succeed (50<probability these="" can="" be="" students="" for="" whom="" we="" don="" have="" enough="" data="" or="" simply="" good-average="" students.=""/></li><li id="3a91" class="nl nm iq kn b ko nu ks nv kw nw la nx le ny li nq nr ns nt bi translated">One for those who have a very important chance of succeeding (probability&gt; 90%)。</li></ul><p id="d7b3" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">相反，基线只预测了总体平均成功率周围的偏斜正态分布，没有特别的歧视。</p><p id="5909" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">但是，为了进一步研究这两种模型之间的差异，我们需要更详细地了解 MSE。事实上，单从 MSE 来看，这两种模型具有相似的性能:</p><ul class=""><li id="3345" class="nl nm iq kn b ko lz ks ma kw nn la no le np li nq nr ns nt bi translated">基线:0.17</li><li id="95ee" class="nl nm iq kn b ko nu ks nv kw nw la nx le ny li nq nr ns nt bi translated">LSTM 网络:<strong class="kn ir"> 0.16 </strong></li></ul><p id="2160" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">但是正如我们在<a class="ae lj" href="https://medium.com/towards-data-science/metrics-random-processes-in-classification-fd5bafa79505" rel="noopener">之前的帖子</a>中所讨论的，当真正的随机性与目标相关联时(这里，预测人类行为)，我们可以通过查看其<strong class="kn ir">可靠性度量(REL) </strong>来更好地评估分类器的性能。</p><p id="0038" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">通过将数据分类到相似预测概率的箱中来计算可靠性。然后，我们针对箱中的平均目标计算每个箱的 MSE。最后，我们取一个总平均值来得到我们的 REL 值。</p><p id="0927" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">为每个模型获得的指标是:</p><ul class=""><li id="2cf0" class="nl nm iq kn b ko lz ks ma kw nn la no le np li nq nr ns nt bi translated">基线:3.86 e-3</li><li id="aa2c" class="nl nm iq kn b ko nu ks nv kw nw la nx le ny li nq nr ns nt bi translated">LSTM 网络:<strong class="kn ir"> 2.86 e-4 </strong></li></ul><p id="3035" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">事实上，我们看到基于 LSTM 的网络比梯度推进基线精确 10 倍以上，这可以解释我们在比较两种分布时观察到的更好的分离。</p><h1 id="ae1b" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">结论</h1><p id="dda2" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">总而言之，我们已经在一个真实的例子上看到了<strong class="kn ir"> LSTM </strong> s 对于<strong class="kn ir">序列数据</strong>分类是如何有效的。但是我们也看到了我们的<strong class="kn ir">LSTM 的修改版本</strong>—<strong class="kn ir">多态 LSTM</strong>—<em class="mu">如何能够在没有任何预处理或特征工程的情况下达到更好的性能</em>。事实上，这种新的 LSTM 细胞可以直接接受一系列标签作为输入，这意味着它<strong class="kn ir">只能用于分类特征</strong>并且仍然产生良好的结果。</p><p id="af35" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated">为了进一步改进多状态 LSTM 的 T1，下一步将会考虑多个标签之间的 T2 相关性。事实上，当预测一个学生在给定的一对相似练习中的表现时，预测的概率应该非常相似。尝试确保这种行为的一种方法是将一个嵌入到网络中的的<a class="ae lj" href="https://medium.com/towards-data-science/a-non-nlp-application-of-word2vec-c637e35d3668" rel="noopener">练习集成起来，让它<strong class="kn ir">学习依赖关系</strong>。</a></p><p id="63f5" class="pw-post-body-paragraph kl km iq kn b ko lz kq kr ks ma ku kv kw mb ky kz la mc lc ld le md lg lh li ij bi translated"><em class="mu">作者:</em><a class="ae lj" href="https://www.linkedin.com/in/hichamelboukkouri" rel="noopener ugc nofollow" target="_blank"><em class="mu"/></a></p></div></div>    
</body>
</html>