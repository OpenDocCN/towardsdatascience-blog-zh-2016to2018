<html>
<head>
<title>Is ReLU after Sigmoid bad?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">乙状结肠后的ReLU不好吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/is-relu-after-sigmoid-bad-661fda45f7a2?source=collection_archive---------4-----------------------#2018-03-11">https://towardsdatascience.com/is-relu-after-sigmoid-bad-661fda45f7a2?source=collection_archive---------4-----------------------#2018-03-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3ddf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最近有一篇关于深度学习的心理模型的博客文章，从光学角度进行了类比。我们对一些模型都有直觉，但很难用语言表达出来，我相信有必要为这个心智模型共同努力。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/9c8d1f0e90dfa79a3500d8c31a9713da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*A9EE2TTJDvVzo2wK.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Sigmoid graph from wikipedia</figcaption></figure><p id="5591" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最近我和Rajasekhar(为一个<a class="ae kl" href="https://kwoc.kossiitkgp.in/" rel="noopener ugc nofollow" target="_blank"> KWoC </a>项目)正在分析不同的激活函数之间如何相互作用，我们发现<strong class="jp ir">在最后两层的sigmoid之后使用relu会恶化模型</strong>的性能。我们使用MNIST数据集和一个四层全连接网络，第一层是784维的输入层，然后第二层是500维的隐藏层，之后是另一个具有256维的隐藏层，最后是10维的输出层。除了输入层，我们对每一层的输出使用非线性。由于我们将研究限制在四个激活函数(<em class="ky"> ReLU，Sigmoid，Tanh，卢瑟</em>)，我们可以通过激活函数的不同组合构建64个不同的模型。我们在所有模型中使用随机梯度下降，学习率为0.01，动量为0.5。我们在所有实验中使用交叉熵损失和32的批量大小。我们对每个模型进行了9次实验，精确度的平均值和标准偏差显示在[ <a class="ae kl" href="https://github.com/nishnik/sherlocked/blob/master/inspect_activations/Activations_Results_multiple_iterations.md" rel="noopener ugc nofollow" target="_blank"> nishnik/sherlocked </a>的表格中。我在这里简单总结一下:</p><ol class=""><li id="3814" class="kz la iq jp b jq jr ju jv jy lb kc lc kg ld kk le lf lg lh bi translated">如果第一层有<strong class="jp ir"> relu </strong>激活，第二层和第三层有除(sigmoid，relu)之外的(relu，tanh，sigmoid，relu)的任意组合，则平均测试精度大于<strong class="jp ir"> 85% </strong>。对于(<strong class="jp ir"> relu，sigmoid，relu </strong>)，我们得到的平均测试精度为<strong class="jp ir"> 34.91% </strong></li><li id="35b7" class="kz la iq jp b jq li ju lj jy lk kc ll kg lm kk le lf lg lh bi translated">如果第一层有<strong class="jp ir"> tanh </strong>激活，第二层和第三层有除(sigmoid，relu)之外的(relu，tanh，sigmoid，relu)的任意组合，则平均测试精度大于<strong class="jp ir"> 86% </strong>。对于(<strong class="jp ir"> tanh，sigmoid，relu </strong>)，我们得到的平均测试精度为<strong class="jp ir"> 51.57% </strong></li><li id="5f84" class="kz la iq jp b jq li ju lj jy lk kc ll kg lm kk le lf lg lh bi translated">如果第一层有<strong class="jp ir"> sigmoid </strong>激活，第二层和第三层有除(sigmoid，relu)之外的(relu，tanh，sigmoid，relu)的任意组合，则平均测试精度大于<strong class="jp ir"> 76% </strong>。对于(<strong class="jp ir"> sigmoid，sigmoid，relu </strong>)，我们得到的平均测试精度为<strong class="jp ir"> 16.03% </strong></li><li id="80c4" class="kz la iq jp b jq li ju lj jy lk kc ll kg lm kk le lf lg lh bi translated">如果第一层有<strong class="jp ir">卢瑟</strong>激活，第二层和第三层有除(sigmoid，relu)之外的(relu，tanh，sigmoid，relu)的任意组合，那么平均测试精度大于<strong class="jp ir"> 91% </strong>。对于(<strong class="jp ir">卢瑟，sigmoid，relu </strong>)我们得到的平均测试准确率为<strong class="jp ir"> 75.16% </strong></li><li id="2598" class="kz la iq jp b jq li ju lj jy lk kc ll kg lm kk le lf lg lh bi translated">如果最后两层具有(<strong class="jp ir"> sigmoid，relu </strong>)的话，精确度的变化也很大</li></ol><p id="cb98" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们也在CIFAR-10上进行了实验，结果是相似的[ <a class="ae kl" href="https://github.com/rajasekharmekala/sherlocked/blob/c99a8e8df7059a91220f439c762382e4f5ffc973/inspect_activations/CIFAR/Results.md" rel="noopener ugc nofollow" target="_blank">链接</a> ](抱歉格式错误)。在最后两次激活为(<strong class="jp ir"> sigmoid，relu </strong>)的每种情况下，准确度为<strong class="jp ir"> 10% </strong>，否则准确度≥ <strong class="jp ir"> 50%。</strong></p><p id="a47d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我们在每一层中使用批规范进行实验。和其他组合一样，精确度也有了很大的提高。[<a class="ae kl" href="https://github.com/nishnik/sherlocked/blob/master/inspect_activations/batchNorm/BatchNormResults.md" rel="noopener ugc nofollow" target="_blank">MNIST的结果</a> ]。同样，在最后一层使用batchnorm就像charm一样让模型学习。</p><p id="e07f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，对于最后两层中的(<strong class="jp ir"> sigmoid，relu </strong>)来说，模型不能学习，即梯度不能很好地反向传播。(sigmoid(output_2)* weigh _ 3+bias _ 3)&lt;大多数情况下为0，或者sigmoid(output _ 2)正在达到极值(消失梯度)。这两个我还在做实验。在<a class="ae kl" href="https://twitter.com/nishantiam" rel="noopener ugc nofollow" target="_blank">twitter.com/nishantiam</a>给我一些建议，或者在[<a class="ae kl" href="https://github.com/nishnik/sherlocked" rel="noopener ugc nofollow" target="_blank">nish Nik/Sherlock</a>上制造一个问题。</p></div></div>    
</body>
</html>