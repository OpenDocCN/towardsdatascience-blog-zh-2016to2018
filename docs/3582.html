<html>
<head>
<title>[ Paper Summary ] A Theoretical Explanation for Perplexing Behaviours of Back-propagation based Visualizations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[论文摘要]对基于反向传播的可视化的复杂行为的理论解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-summary-a-theoretical-explanation-for-perplexing-behaviours-of-back-propagation-based-220083d7dddd?source=collection_archive---------5-----------------------#2018-05-27">https://towardsdatascience.com/paper-summary-a-theoretical-explanation-for-perplexing-behaviours-of-back-propagation-based-220083d7dddd?source=collection_archive---------5-----------------------#2018-05-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/70570ed6931db521a4503b3abee4ef7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*OlIvZg1nvH6skqlZbQpSRA.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/lauren-pelc-mcarthur-l0HlKlRMBvdzAFYYM" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="0780" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我实际上也对这个想法很好奇，反向传播张量是什么样子的？关于这些张量我们能了解到什么？如果我们研究这些，我们有可能最终理解神经网络是如何学习的吗？这篇论文是我第一次读到这些话题。</p><blockquote class="kx ky kz"><p id="fc77" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated">请注意，这篇文章是为了让未来的我回顾并记住这篇文章的内容，而不是阅读整篇文章。</p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="c491" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">摘要</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/465b8269d5b0660236745d333dc4fefb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CqXYlcgkiel7LuCfsfa7qw.png"/></div></div></figure><p id="dea1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">为了可视化和理解卷积神经网络(CNN)是如何学习的，引入了引导反向传播(<a class="ae jy" href="https://github.com/Lasagne/Recipes/blob/master/examples/Saliency%20Maps%20and%20Guided%20Backpropagation.ipynb" rel="noopener ugc nofollow" target="_blank"> GBP </a>)和去卷积网络(<a class="ae jy" href="https://papers.nips.cc/paper/5485-deep-convolutional-neural-network-for-image-deconvolution.pdf" rel="noopener ugc nofollow" target="_blank"> DeconvNet </a>),但是在证明它们的行为时，一些理论丢失了。作者声称 GBP 和 DeconvNet 正在部分地进行图像恢复，因此与网络决策过程无关。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="def8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">简介</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/6007621a2a89dd5bf1e1926b6925def4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fwBlptAvWtuUf3tkdZi6Qw.png"/></div></div></figure><p id="2752" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">由于深度学习在自动驾驶汽车等危险情况下的使用越来越多，理解模型的内部工作变得至关重要。如上所述，已经引入了一些可视化反向传播的方法，但是这些方法真的回答了网络内部正在学习什么的问题吗？</p><p id="7975" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">GBP 和 DeconvNet 给出了比显著性映射技术更好的图像分辨率，然而，尽管类别不同，由它们的方法产生的可视化通常保持相同。(因此，在分辨率质量和指出图像中哪个位置被认为对网络重要之间可能会有一个折衷。)</p><blockquote class="kx ky kz"><p id="de95" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated"><strong class="kb ir">本文提供的实验表明，GBP 和 DeconvNet 本质上是在进行(部分)图像恢复，而不是突出显示与类别相关的像素或可视化学习到的权重，这意味着原则上它们与神经网络的决策无关</strong></p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="d55f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">基于反向传播的可视化</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lx"><img src="../Images/299fb3df219a409e6bc2fd292b5ce875.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JHwHTVpTTBNFHo-WHbbsNw.png"/></div></div></figure><p id="4a2f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这里，本文讨论了不同的方法之间的差异可视化的梯度反向传播和所有这些可以总结在一个图像。(我理解为显著图→正常背道具，Deconv →反向激活，GBP →两者结合)</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ly"><img src="../Images/23275a1d6063aa392b7e0c7b2c04836e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EOSj4HE8iJceyCJN_DyJAw.png"/></div></div></figure><p id="575d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">作者还为每种方法提供了良好的可视化效果。(通过观察它们，我们可以观察到来自 GBP 的结果非常清晰。)</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lz"><img src="../Images/4fc2c760373ca9e38b6fa9b6de653886.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FmYqEr3BPzWQB1j_hY4yIw.png"/></div></div></figure><p id="b69a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">的确，为了成为一个好的可视化，人类的可解释性是非常重要的。然而，作者指出，GBP 和 DeconvNet 产生如此清晰的图像的原因是因为它们修改了真实的梯度，因此当模型试图执行分类时，阻碍了对哪些像素是重要的了解。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="ab3f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">理论解释</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ma"><img src="../Images/ac37f7fb085e52d3cb91fe55f7da1eba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dJHv9RzfqhEe481SXEP3gg.png"/></div></div></figure><p id="afe5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">论文的这一部分涉及大量的数学知识，因为它不仅解释了前馈运算，还解释了非常简洁的方程中的反向传播。所以现在就下结论，我们可以注意到一些事情。</p><ol class=""><li id="090d" class="mb mc iq kb b kc kd kg kh kk md ko me ks mf kw mg mh mi mj bi translated">显著图和去卷积可视化都应该产生随机噪声，传达关于输入图像和类逻辑的很少信息。</li><li id="de1c" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">然而，对于更复杂的模型，DeconvNet 的行为不像显著性映射。</li><li id="3a02" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">在一个简单的模型(如 3 层 cnn)中结合 backward ReLu() GBP 实际上可以近似地恢复原始图像。当我们希望了解网络的内部运作时，这是不可取的。旁注:当我们希望可视化清晰时，本地连接是非常重要的属性。</li></ol></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="c086" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">理论解释:更真实的模型/预训练模型</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mp"><img src="../Images/324dec5c0ceee2a3018b180fa862567e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ghDcRZAgys9HOAi-iJdJiA.png"/></div></div></figure><p id="b20f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这里，作者引入了最大池操作，并修改了原始方程，以适应更现实的 CNN 架构/预训练模型，并发现了三个重要的发现。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mq"><img src="../Images/8c9806808c0263e379ea29e0cc34cf30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O_LmZnMptv-kMfR9fyNjag.png"/></div></div></figure><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mr"><img src="../Images/5bc3f0384faca823b4532f107a1261df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IIZjuVjK7FeWVkCkL1VBJA.png"/></div></div></figure><ol class=""><li id="143f" class="mb mc iq kb b kc kd kg kh kk md ko me ks mf kw mg mh mi mj bi translated">在更真实的设置行为中取消配置，如英镑</li><li id="059f" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">增加深度不会改变 GBP 和 DeconvNet 的行为。</li><li id="9663" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">即使在预先训练的模型中，GBP 和 DeconvNet 可视化也不区分类别。</li></ol></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="87ef" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">实验</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ms"><img src="../Images/a63402324316c24c03d0cb307c0ab8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B9ioKpZ4F11TYQaafYM92w.png"/></div></div></figure><p id="b1c4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">作者提供了用三种不同类型的网络进行的实验的结果:a)三个 CNN 层网络，b)三个全连接网络，c) VGG-16 网络。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mt"><img src="../Images/0677af161e50c1dc53c5639d099300e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zVK1Bl1SyJbj5-Scc7-dBw.png"/></div></div></figure><p id="7554" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，GBP 产生的图像非常清晰，任何人都可以分辨出这是 cat。(美国有线电视新闻网和 FCN)。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mu"><img src="../Images/6439cdc0b6d14253494da72eb0f99cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ohAua4OpzL81UCIqO-OOQ.png"/></div></div></figure><p id="64e9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">此外，在简单的 CNN 网络以及 VGG-16 网络中添加 max-pooling 层之后，我们可以观察到 DeonveNet 开始产生更易于人类理解的图像，这证实了理论分析。作者还对网络深度的变化进行了不同的实验，发现基于反向传播的方法的行为没有改变。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mv"><img src="../Images/985f2cd895cb0e31662f4a2036e86930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wd79ygDmixhOrLooWbhVMg.png"/></div></div></figure><p id="5ddc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，作者给了网络一个对抗性的输入，看看它如何影响可视化。理论上，它应该显著地影响显著性映射，因为中间层的预测类别标签和触发状态都已经改变。然而，没有太多的图像恢复任务。如上所述，顶行是原始输入，底行是对抗输入。我们可以观察到，所得到的可视化仅针对显著性映射而改变。因此，作者得出以下结论。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mw"><img src="../Images/4fbabdf1648fe1c4bb544cf54415283f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V-jzcRpzwYJ4qKkE85um4A.png"/></div></div></figure><p id="063a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">请注意，作者已经用“部分训练权重的 VGG”和“平均 l2 距离统计”进行了实验，并获得了类似的结论。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="ed6e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结论</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mx"><img src="../Images/75aa2042fbb245ebe757ee3d43950b51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uwZMYOH59k-INWTdEkLbcw.png"/></div></div></figure><p id="72e4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">本文的作者提供了大量的实验，表明 GBP/DeconvNet 实际上正在执行图像恢复任务。通过反向 Relu()、本地连接和最大池的组合。因此证明需要开发更好的方法来可视化 CNN 的内部工作。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="30fc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="c6a5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当我们想要了解一个人时，我们需要和他有相同的看法。理解他们的观点，也许这是我们想了解 CNN 时需要做的，GBP/DeconvNet 对我们有好处，但只对我们…</p><p id="a788" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你希望看到我所有写作的列表，请<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">在这里查看我的网站</a>。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的 twitter <a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>关注我，并访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我也实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文 pos </a> t。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="ead3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="174b" class="mb mc iq kb b kc kd kg kh kk md ko me ks mf kw mg mh mi mj bi translated">聂，张，杨，&amp;帕特尔，A. (2018)。基于反向传播的可视化令人困惑的行为的理论解释。Arxiv.org。检索于 2018 年 5 月 26 日，来自<a class="ae jy" href="https://arxiv.org/abs/1805.07039" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1805.07039</a></li><li id="5eeb" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">千层面/食谱。(2018).GitHub。检索于 2018 年 5 月 26 日，来自<a class="ae jy" href="https://github.com/Lasagne/Recipes/blob/master/examples/Saliency%20Maps%20and%20Guided%20Backpropagation.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Lasagne/Recipes/blob/master/examples/studentity % 20 maps % 20 和% 20 guided % 20 back propagation . ipynb</a></li><li id="5deb" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">(2018).Papers.nips.cc .于 2018 年 5 月 26 日检索，来自<a class="ae jy" href="https://papers.nips.cc/paper/5485-deep-convolutional-neural-network-for-image-deconvolution.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/5485-deep-convolutionary-neural-network-for-image-convolution . pdf</a></li></ol></div></div>    
</body>
</html>