<html>
<head>
<title>Neural Networks: All You Need to Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络:你需要知道的一切</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nns-aynk-c34efe37f15a?source=collection_archive---------1-----------------------#2018-04-07">https://towardsdatascience.com/nns-aynk-c34efe37f15a?source=collection_archive---------1-----------------------#2018-04-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ab05" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">任何大型 ML 项目的主干都始于一个网络……一个神经网络，这是你需要了解的所有信息。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/01e9962da7a15048c26217ba15809ad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w4wR9mLS46s3gsX_7pW_ww.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">A Neural Network performing a prediction</figcaption></figure><p id="9af6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如副标题所述，<a class="ae lr" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">神经网络</a> (NNs)正在<em class="ls">几乎任何需要启发式方法来解决问题的地方</em>被使用。这篇文章将教你所有你<em class="ls">需要</em>了解的 NN。读完这篇文章后，你应该对神经网络有一个大致的了解，它们是如何工作的，以及如何自己制作一个。</p><p id="2046" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下是我要复习的内容:</p><ul class=""><li id="a992" class="lt lu iq kx b ky kz lb lc le lv li lw lm lx lq ly lz ma mb bi translated"><strong class="kx ir">神经网络的历史</strong></li><li id="8723" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><strong class="kx ir">什么是真正的神经网络？</strong></li><li id="b4a7" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><strong class="kx ir">单位/神经元</strong></li><li id="4adf" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><strong class="kx ir">重量/参数/连接</strong></li><li id="c499" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><strong class="kx ir">偏差</strong></li><li id="4d51" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><strong class="kx ir">超参数</strong></li><li id="8b84" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><strong class="kx ir">激活功能</strong></li><li id="6503" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><strong class="kx ir">层层</strong></li><li id="127f" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><strong class="kx ir">神经网络学习时会发生什么？</strong></li><li id="00a2" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><strong class="kx ir">实施细节(如何管理项目中的一切)</strong></li><li id="9fd4" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><strong class="kx ir">关于神经网络的更多信息(更多资源链接)</strong></li></ul><p id="7a32" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我偶尔会写这些有些简短但内容丰富的文章，以帮助您<strong class="kx ir">了解更多关于 AI 和机器学习的知识</strong>。您可以在 twitter 上关注我的<a class="ae lr" href="https://twitter.com/SuryanshTweets" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir"/></a>或在这里关注我的<a class="ae lr" href="https://medium.com/@SuryanshWrites" rel="noopener"><strong class="kx ir"/></a>来了解我的最新文章。你也可以在 twitter 上问我问题，或者在这里发表评论。</p><h2 id="88bb" class="mh mi iq bd mj mk ml dn mm mn mo dp mp le mq mr ms li mt mu mv lm mw mx my mz bi translated">我花了一个多星期才写完这篇文章的所有内容，如果你觉得有帮助就鼓掌吧。我希望这篇文章教你一些新的东西，让我们开始吧！</h2></div><div class="ab cl na nb hu nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ij ik il im in"><h1 id="d025" class="nh mi iq bd mj ni nj nk mm nl nm nn mp jw no jx ms jz np ka mv kc nq kd my nr bi translated">神经网络的历史</h1><p id="9f78" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi nx translated">因为，我不想用很多关于 NNs 的历史来烦你，我只会简单回顾一下他们的历史，T42。如果你想更深入地了解他们的历史，这里有一篇关于这个主题的维基文章。这一部分主要基于维基文章。</p><p id="a17a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这一切都始于 1943 年沃伦·麦卡洛克 和<a class="ae lr" href="https://en.wikipedia.org/wiki/Walter_Pitts" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">沃尔特·皮茨</strong> </a>创造了第一个神经网络模型。他们的模型纯粹基于数学和算法，由于缺乏计算资源而无法测试。</p><p id="d26a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">后来，在 1958 年，<a class="ae lr" href="https://en.wikipedia.org/wiki/Frank_Rosenblatt" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">弗兰克·罗森布拉特</strong> </a>创造了第一个可以进行模式识别的模型。这将改变一切。<a class="ae lr" href="https://en.wikipedia.org/wiki/Perceptron" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">感知器</strong> </a>。然而，他只给出了符号和模型。实际模型仍然无法测试。在此之前有相对较少的研究。</p><p id="086f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第一个可以测试的多层神经网络由阿列克谢·伊瓦赫年科、T21<strong class="kx ir">和帕拉</strong>于 1965 年发表。</p><p id="6ad2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">此后，由于机器学习模型的高可行性，神经网络的研究停滞不前。这是 1969 年由<a class="ae lr" href="https://en.wikipedia.org/wiki/Marvin_Minsky" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">马文·明斯基</strong> </a>和<a class="ae lr" href="https://en.wikipedia.org/wiki/Seymour_Papert" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">西蒙·派珀特</strong> </a>完成的。</p><p id="5405" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，这种停滞是相对短暂的，因为 6 年后的 1975 年<a class="ae lr" href="https://en.wikipedia.org/wiki/Paul_Werbos" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir"> Paul Werbos </strong> </a>提出了<a class="ae lr" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">反向传播</strong> </a>，它解决了<a class="ae lr" href="https://en.wikipedia.org/wiki/Exclusive_or" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir">【XOR】</strong></a><strong class="kx ir">问题</strong>，并且总体上使神经网络学习更加有效。</p><p id="fdf2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae lr" href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir"> Max-pooling </strong> </a>后来在 1992 年引入，它有助于 3D 对象识别，因为它有助于最小平移不变性和对变形的容忍。</p><p id="4111" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在 2009 年至 2012 年间，尤尔根·施密德胡伯的 研究小组创造的<a class="ae lr" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">递归神经网络</strong> </a>和深度前馈神经网络在<a class="ae lr" href="https://en.wikipedia.org/wiki/Pattern_recognition" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">模式识别</strong> </a>和<a class="ae lr" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">机器学习</strong> </a> <strong class="kx ir">中赢得了 8 项国际比赛。</strong></p><p id="6630" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2011 年，Deep NNs 开始将卷积层与最大池层相结合，最大池层的输出随后被传递到几个完全连接的层，这些层之后是输出层。这些被称为<a class="ae lr" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">卷积神经网络</strong> </a> <strong class="kx ir">。</strong></p><p id="a89a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这之后还有一些研究，但是这些是你应该知道的主要话题。</p><h1 id="6647" class="nh mi iq bd mj ni og nk mm nl oh nn mp jw oi jx ms jz oj ka mv kc ok kd my nr bi translated">什么是真正的神经网络？</h1><p id="123a" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">一个想到 NN 的好方法是作为一个<a class="ae lr" href="https://en.wikipedia.org/wiki/Function_composition" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">复合函数</strong> </a>。你给它一些输入，它给你一些输出。</p><p id="1d30" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">基本神经网络的体系结构由三部分组成。这些是:</p><ul class=""><li id="be99" class="lt lu iq kx b ky kz lb lc le lv li lw lm lx lq ly lz ma mb bi translated"><strong class="kx ir">单位/神经元。</strong></li><li id="2b5e" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><strong class="kx ir">连接/重量/参数。</strong></li><li id="b81b" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><strong class="kx ir">偏见。</strong></li></ul><p id="a57c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上面提到的所有东西都是你构建一个神经网络的基本架构所需要的。</p><p id="f796" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可以把这些想象成一个建筑的<strong class="kx ir">积木/砖块</strong>。根据你对建筑功能的要求，你可以安排砖块，反之亦然。水泥可以被认为是<strong class="kx ir">砝码。不管你的砝码有多重，如果你手头没有足够数量的砖块来解决问题，这座建筑将会倒塌。然而，你可以让建筑以最小的精度运行(用最少的砖)，然后逐步地在这个架构上解决问题。</strong></p><p id="f2ea" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我将在后面的部分中更多地讨论权重、偏差和单位。这些部分可能很短，但是这些部分强调了它们的重要性。</p><h1 id="19cd" class="nh mi iq bd mj ni og nk mm nl oh nn mp jw oi jx ms jz oj ka mv kc ok kd my nr bi translated">单位/神经元</h1><p id="7bf0" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">作为 NNs 架构的三个部分中最不重要的部分，这些函数包含权重和偏差，并等待数据到达它们。数据到达后，它们执行一些计算，然后使用一个激活函数将数据限制在一个范围内(大多数情况下)。</p><p id="babb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">把这些单位想象成一个包含权重和偏差的盒子。盒子从两端打开。一端接收数据，另一端输出修改后的数据。然后数据开始进入盒子，盒子将权重与数据相乘，然后将偏差加到相乘后的数据上。这是一个单一的单位，也可以认为是一个功能。此函数与此类似，是直线的函数模板:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/0c151d830113eea2f7386f38f9755f40.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/0*GicY5-zfBlZqTthN."/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">y = mx + b</figcaption></figure><p id="810c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">想象一下有多个这样的东西。从现在开始，您将为同一个数据点(输入)计算多个输出。这些输出然后被发送到另一个单元，该单元然后计算神经网络的最终输出。</p><p id="b90e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果这一切都在你眼前一闪而过，那么继续读下去，你应该能理解更多。</p><h1 id="cdf3" class="nh mi iq bd mj ni og nk mm nl oh nn mp jw oi jx ms jz oj ka mv kc ok kd my nr bi translated">重量/参数/连接</h1><p id="5418" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">作为神经网络最重要的部分，这些(和偏差)是神经网络为了推广到一个问题必须学习的数字。这就是此时你需要知道的全部。</p><h1 id="3aba" class="nh mi iq bd mj ni og nk mm nl oh nn mp jw oi jx ms jz oj ka mv kc ok kd my nr bi translated">偏见</h1><p id="54b6" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">这些数字代表神经网络“认为”在将权重与数据相乘后应该添加的内容。当然，这些<em class="ls">总是</em>错误的，但是神经网络也会学习最优偏差。</p><h1 id="4279" class="nh mi iq bd mj ni og nk mm nl oh nn mp jw oi jx ms jz oj ka mv kc ok kd my nr bi translated">超参数</h1><p id="d9a2" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">这些是您必须手动设置的值。如果你把神经网络想象成一台机器，改变机器行为的节点就是神经网络的超参数。</p><p id="9f66" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可以在这里阅读我的另一篇文章<a class="ae lr" rel="noopener" target="_blank" href="/gas-and-nns-6a41f1e8146d">(遗传算法+神经网络=两全其美)</a>找出如何让你的计算机学习神经网络的“最佳”超参数。</p><h1 id="3037" class="nh mi iq bd mj ni og nk mm nl oh nn mp jw oi jx ms jz oj ka mv kc ok kd my nr bi translated">激活功能</h1><p id="05fa" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">这些也称为映射函数。它们在 x 轴上接受一些输入，并输出一个限定范围内的值(大多数情况下)。大多数情况下，它们用于将单元的大输出转换为较小的值，并提高神经网络的非线性度。你对激活函数的选择可以极大地提高或阻碍你的神经网络的性能。如果您愿意，您可以为不同的单元选择不同的激活功能。</p><p id="251d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下是一些常见的激活功能:</p><ul class=""><li id="b8d4" class="lt lu iq kx b ky kz lb lc le lv li lw lm lx lq ly lz ma mb bi translated"><a class="ae lr" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">乙状结肠</a>。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b1b91dbea2e67c3b9bce04a479dc6a99.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*MPyJ8zJ6ktJ_qlAx.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The Sigmoid function</figcaption></figure><ul class=""><li id="e1be" class="lt lu iq kx b ky kz lb lc le lv li lw lm lx lq ly lz ma mb bi translated"><a class="ae lr" href="https://reference.wolfram.com/language/ref/Tanh.html" rel="noopener ugc nofollow" target="_blank"> Tanh </a>。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/4ab7383f2680aceee6ba9034ec72271e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/0*EBq--fyq8ACqmdGa."/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The tanh function</figcaption></figure><ul class=""><li id="8e19" class="lt lu iq kx b ky kz lb lc le lv li lw lm lx lq ly lz ma mb bi translated"><a class="ae lr" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU </a>:整流线性单元。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/92e513e82cf97f22cfe1a1245319256b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SurwKOHw8LxXuc13."/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The ReLU function</figcaption></figure><ul class=""><li id="02f6" class="lt lu iq kx b ky kz lb lc le lv li lw lm lx lq ly lz ma mb bi translated"><a class="ae lr" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank">泄漏的 ReLU </a>。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/011d157e57e32261c3b7e7c8013677a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/0*9dxjbDoB5ZNhdtP0."/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The Leaky ReLU function</figcaption></figure><h1 id="2aa1" class="nh mi iq bd mj ni og nk mm nl oh nn mp jw oi jx ms jz oj ka mv kc ok kd my nr bi translated">层</h1><p id="25b5" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">这就是帮助神经网络在任何问题中获得复杂性的原因。增加层数(单位)会增加神经网络输出的非线性。</p><p id="02ad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每一层都包含一定数量的单元。大多数情况下的数量完全取决于创作者。然而，对于一个简单的任务有太多的层会不必要地增加它的复杂性，并且在大多数情况下会降低它的准确性。反之亦然。</p><p id="7da1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每个神经网络都有两层。这些是输入和输出层。这两者之间的任何层都称为隐藏层。下图中的神经网络包含一个输入层(8 个单元)、一个输出层(4 个单元)和 3 个隐藏层，每个隐藏层包含 9 个单元。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/3dccd4c3a8f1369cdea619788b6271b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/0*sBqjpSeS8P1Y-_dG."/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">A Deep Neural Net</figcaption></figure><p id="3420" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">具有两个或更多隐藏层的 NN，每层包含大量单元，称为深度神经网络，它催生了一个新的学习领域，称为<a class="ae lr" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>。图中所示的神经网络就是这样一个例子。</p><h1 id="5a14" class="nh mi iq bd mj ni og nk mm nl oh nn mp jw oi jx ms jz oj ka mv kc ok kd my nr bi translated">神经网络学习时会发生什么？</h1><p id="7d90" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">教神经网络概括问题的最常见方法是使用<a class="ae lr" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>。因为我已经写了一篇关于这个主题的详细文章，你可以阅读来充分理解 GD(梯度下降)，我将不在这篇文章中解释 GD。下面是 GD 文章:<a class="ae lr" href="https://hackernoon.com/gradient-descent-aynk-7cbe95a778da" rel="noopener ugc nofollow" target="_blank">渐变下降:你需要知道的一切</a>。</p><p id="9e93" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">再加上 GD，另一种教授 NN 的常用方法是使用<a class="ae lr" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">反向传播</a>。利用这一点，使用微积分中的<a class="ae lr" href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener ugc nofollow" target="_blank">链规则</a>将神经网络输出层的误差向后传播。对于一个没有很好掌握微积分的初学者来说，理解这个很有挑战性，所以不要被它淹没。<a class="ae lr" href="http://neuralnetworksanddeeplearning.com/chap2.html" rel="noopener ugc nofollow" target="_blank">点击此处</a>查看一篇文章，当我在与反向传播作斗争时，这篇文章确实帮了我大忙。我花了一天半的时间才弄明白错误反向传播时发生了什么。</p><p id="f5c3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在训练神经网络时有许多不同的注意事项。然而，在一篇面向初学者的文章中浏览它们会非常乏味，而且对初学者来说是不必要的。</p><h1 id="7fc9" class="nh mi iq bd mj ni og nk mm nl oh nn mp jw oi jx ms jz oj ka mv kc ok kd my nr bi translated">实施细节(如何管理项目中的一切)</h1><p id="84be" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">为了解释项目中的一切是如何管理的，我创建了一个<a class="ae lr" href="https://jupyter.org/" rel="noopener ugc nofollow" target="_blank"> JupyterNotebook </a>包含一个学习<a class="ae lr" href="https://en.wikipedia.org/wiki/Exclusive_or" rel="noopener ugc nofollow" target="_blank"> XOR 逻辑门</a>的小 NN。<a class="ae lr" href="https://github.com/Frixoe/xor-neural-network/blob/master/XOR-Net-Notebook.ipynb" rel="noopener ugc nofollow" target="_blank">点击这里</a>查看笔记本。</p><p id="4af9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在查看并理解笔记本中发生的事情后，您应该对基本神经网络是如何构造的有一个大致的概念。</p><p id="960b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在笔记本中创建的 NN 中的训练数据以矩阵形式排列。这是数据在中的一般排列方式。不同项目中显示的矩阵的维度可能会有所不同。</p><p id="8551" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通常对于大量的数据，数据被分成两类:训练数据(60%)和测试数据(40%)。然后，神经网络根据训练数据进行训练，然后根据测试数据测试其准确性。</p><h1 id="91a8" class="nh mi iq bd mj ni og nk mm nl oh nn mp jw oi jx ms jz oj ka mv kc ok kd my nr bi translated">关于神经网络的更多内容(更多资源的链接)</h1><p id="ddb3" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">如果您仍然不明白发生了什么，我建议您查看下面提供的资源链接。</p><p id="a12a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">YouTube:</p><ul class=""><li id="c111" class="lt lu iq kx b ky kz lb lc le lv li lw lm lx lq ly lz ma mb bi translated">Siraj Raval </li><li id="61dd" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><a class="ae lr" href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw" rel="noopener ugc nofollow" target="_blank">3 蓝色 1 棕色</a></li><li id="321d" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><a class="ae lr" href="https://www.youtube.com/playlist?list=PLRqwX-V7Uu6aCibgK1PTWWu9by6XFdCfh" rel="noopener ugc nofollow" target="_blank">编码序列</a></li><li id="afd2" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">布兰登·罗尔</li><li id="e7b3" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><a class="ae lr" href="https://www.youtube.com/channel/UCrBzGHKmGDcwLFnQGHJ3XYg" rel="noopener ugc nofollow" target="_blank">巨型神经网络</a></li><li id="7309" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">雨果·拉罗彻尔</li><li id="7c14" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><a class="ae lr" href="https://www.youtube.com/channel/UCQALLeQPoZdZC4JNUboVEUg" rel="noopener ugc nofollow" target="_blank">贾布里勒</a></li><li id="6cee" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">路易斯·塞拉诺</li></ul><p id="5727" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Coursera:</p><ul class=""><li id="2486" class="lt lu iq kx b ky kz lb lc le lv li lw lm lx lq ly lz ma mb bi translated"><a class="ae lr" href="https://www.coursera.org/learn/neural-networks" rel="noopener ugc nofollow" target="_blank">多伦多大学的用于机器学习的神经网络</a></li><li id="4c86" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><a class="ae lr" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习专业化</a>由吴恩达</li><li id="4a61" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated"><a class="ae lr" href="https://www.coursera.org/learn/intro-to-deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习简介</a>国立研究型大学高等经济学院</li></ul></div><div class="ab cl na nb hu nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ij ik il im in"><h2 id="209d" class="mh mi iq bd mj mk ml dn mm mn mo dp mp le mq mr ms li mt mu mv lm mw mx my mz bi translated">就这样，希望你学到了新的东西！</h2><h2 id="b8f8" class="mh mi iq bd mj mk ml dn mm mn mo dp mp le mq mr ms li mt mu mv lm mw mx my mz bi translated">如果你觉得这篇文章有帮助，请鼓掌。<a class="ae lr" href="https://twitter.com/SuryanshTweets" rel="noopener ugc nofollow" target="_blank">在 Twitter 上关注我</a>和<a class="ae lr" href="https://medium.com/@SuryanshWrites" rel="noopener">在这里</a>关注我的最新帖子。如果你有任何问题，你可以发微博给我，或者通过评论让我知道。我很乐意回答这些问题。</h2></div></div>    
</body>
</html>