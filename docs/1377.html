<html>
<head>
<title>Yet another text generation project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">另一个文本生成项目</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/yet-another-text-generation-project-5cfb59b26255?source=collection_archive---------1-----------------------#2017-08-29">https://towardsdatascience.com/yet-another-text-generation-project-5cfb59b26255?source=collection_archive---------1-----------------------#2017-08-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn jo jp"><p id="9c30" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated">我对深度学习比较陌生。我正努力尽快赶上进度，但我担心在这个过程中我可能会挖出一些旧东西。从我初学者的角度来看，文本生成还是很刺激的！我希望我能通过这篇文章和附带的知识库帮助一些人…</p></blockquote><p id="0d4d" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">和很多人一样，我是在看了 Andrej Karpathy 的著名博文:<a class="ae ks" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">《递归神经网络的不合理有效性》</a>后爱上递归神经网络(RNN)的。这篇文章令人大开眼界，它证明了 RNN 在学习序列数据方面有多好，以及一个训练有素的网络如何可以用来生成新的和类似的数据。</p><p id="0fc1" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">在他的文章中，要学习的序列数据是文本。文本被认为是一串相继出现的字符。一个基于 RNN 的神经网络被训练来从前面的字符序列中预测下一个字符。这种网络通常被称为“char-rnn”。</p><p id="7e64" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">为了更好地工作，网络从字符序列中学习高级概念。它逐渐了解数据中越来越多有趣的方面。它可以学习单词和如何拼写单词，如何尊重良好的语法，最终，它可能能够生成或多或少有意义的带标点的句子。对于最后一部分，为了生成从头到尾都有意义的长句(甚至是更大的任务，如生成整个故事)，网络必须学会保存和管理大量的上下文(这很难)。</p><p id="0457" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">多年来，相关代码已经被移植到更新的深度学习库中，如谷歌的<a class="ae ks" href="https://github.com/tensorflow/tensorflow" rel="noopener ugc nofollow" target="_blank"> tensorflow </a>，以及高级 API <a class="ae ks" href="https://github.com/fchollet/keras" rel="noopener ugc nofollow" target="_blank"> keras </a>。RNN 的生成性用法现在非常流行。然而，让我开始这个主题的是一个应用程序的想法(我以为我首先想到的，我太天真了)。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><h1 id="5f43" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">特朗普机器人军队</h1><p id="5407" class="pw-post-body-paragraph jq jr iq jt b ju ly jw jx jy lz ka kb kp ma ke kf kq mb ki kj kr mc km kn ko ij bi translated">由于唐纳德·特朗普的言论非常有趣，我想，作为一个个人项目，我可以训练一个神经网络来产生更多的言论，也许我可以通过阅读结果来获得一两个笑声。很快就发现我不是第一个有这个想法的人。</p><p id="e192" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">我发现的一些项目可以追溯到 2015 年，像<a class="ae ks" href="https://github.com/ppramesi/RoboTrumpDNN" rel="noopener ugc nofollow" target="_blank">这个叫做 RoboTrump </a>的项目，网络在单词级别工作(下一个单词预测)。</p><p id="2288" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">有些是特朗普在总统竞选中还是局外人时创造的，他们几乎不知道:</p><blockquote class="jn jo jp"><p id="52e7" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated">“我写这篇文章的时候，美国正在进行总统选举。这次我们有一些不寻常的候选人，我想看看 RNN 能从他们的演讲中学到多少有趣的东西。”deeplearningathome.com<a class="ae ks" href="http://deeplearningathome.com/" rel="noopener ugc nofollow" target="_blank"/></p></blockquote><p id="5e72" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">最成功的似乎是<a class="ae ks" href="http://twitter.com/DeepDrumpf" rel="noopener ugc nofollow" target="_blank"> @DeepDrumpf </a>(有自己的推特账号)。它甚至引起了我们最喜欢的 char-rnn 专家的注意:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi md"><img src="../Images/b827cb69fb6db4be0befc3df69598476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*akpihD45eyDhZE8qf17BaA.png"/></div></figure><p id="520d" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">足够的考古学，回到我(非革命的)对这个主题的看法。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><h1 id="9695" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">我对特朗普的看法</h1><p id="0b1f" class="pw-post-body-paragraph jq jr iq jt b ju ly jw jx jy lz ka kb kp ma ke kf kq mb ki kj kr mc km kn ko ij bi translated">我用 Keras 2.0.6 创建了一个 char-rnn(用 tensorflow 后端)。在这个地址的<a class="ae ks" href="https://github.com/jctestud/char-rnn" rel="noopener ugc nofollow" target="_blank">什么都有。它采用两个 python 笔记本的形式，一个用于训练，一个用于测试。您可以使用数据集，从头开始训练模型，或者跳过这一部分，使用提供的权重来玩文本生成(玩得开心！).</a></p><blockquote class="jn jo jp"><p id="9bf6" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated">注意:训练代码是“多 GPU 兼容”。根据我的经验，如果输入网络的序列很长(例如 120 个)，可以显著减少训练时间。对于双 GPU，我每个时期的时间减少了 56%。</p></blockquote><h2 id="c2a2" class="ml lb iq bd lc mm mn dn lg mo mp dp lk kp mq mr lo kq ms mt ls kr mu mv lw mw bi translated">收集和清理数据</h2><p id="fce2" class="pw-post-body-paragraph jq jr iq jt b ju ly jw jx jy lz ka kb kp ma ke kf kq mb ki kj kr mc km kn ko ij bi translated">我基于特朗普推特档案建立了一个文本数据集，这是一个所有总统推特的最新档案(从 2009 年开始约有 32000 条推特)。</p><p id="fa97" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">这表示 3.3MB 的文本数据，大约是 350 万个字符。据我所知，对于我们要做的事情来说，它并不是很大。<br/>字符“词汇”按顺序固定为 98 个可打印的 ASCII 字符(在稳定的词汇上工作有助于改变模型的用途)</p><h2 id="c1e8" class="ml lb iq bd lc mm mn dn lg mo mp dp lk kp mq mr lo kq ms mt ls kr mu mv lw mw bi translated">数据准备</h2><p id="e3d5" class="pw-post-body-paragraph jq jr iq jt b ju ly jw jx jy lz ka kb kp ma ke kf kq mb ki kj kr mc km kn ko ij bi translated">训练基于向网络输入 60 个字符的固定大小的序列。使用我的 GPU，我能够处理包含 512 个样本的大批量数据。</p><p id="eaf7" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">一批是 512 个 60 个字符的序列的列表。每个字符只能是词汇表中 98 个可用条目中的一个。对于每个 60 个字符的序列，预期的输出是第 61 个字符。预测只是一个步骤(长度为 1 的序列)。</p><p id="a86b" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">准备这些批次会占用大量内存。因此，批量创建是在训练时实时完成的。</p><h2 id="8690" class="ml lb iq bd lc mm mn dn lg mo mp dp lk kp mq mr lo kq ms mt ls kr mu mv lw mw bi translated">模型架构和培训</h2><p id="9ba3" class="pw-post-body-paragraph jq jr iq jt b ju ly jw jx jy lz ka kb kp ma ke kf kq mb ki kj kr mc km kn ko ij bi translated">该建筑的灵感来自弗朗索瓦·乔莱(Franç ois Chollet)的作品，该作品可在 Keras repository 中找到(它本身源自卡帕西的作品)。</p><p id="bd17" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">神经网络有 4 个堆叠的 512 单元 LSTM 层，每一层后面都有 20%随机神经元<a class="ae ks" href="http://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/" rel="noopener ugc nofollow" target="_blank">丢失</a>用于正则化。<br/>最后一个递归层被设置为只返回其最终状态(一步)。<br/>一个全连接的常规层获取大小为 512 的“隐藏”向量，并将其带回词汇表的大小(98)。<br/>最后一个处理是将 softmax 函数应用于该向量，使其成为字符概率分布。<br/>最后，使用<a class="ae ks" href="http://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank"> Adam 优化器</a>训练网络，以最小化交叉熵损失函数(或多或少分布之间的距离)。</p><p id="740a" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">经过十几代的训练，输出开始听起来连贯。</p><p id="47ac" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">注意:对于 Keras 鉴赏家，我使用“无状态”模式进行训练，这意味着在两个(甚至连续的)60 个字符的序列之间的网络中没有保存任何东西。但是，在生成文本时，网络以 1 对 1 字符的“有状态”模式使用(RNN 单元状态从不重置)</p><h2 id="6260" class="ml lb iq bd lc mm mn dn lg mo mp dp lk kp mq mr lo kq ms mt ls kr mu mv lw mw bi translated">模型检验</h2><p id="02c5" class="pw-post-body-paragraph jq jr iq jt b ju ly jw jx jy lz ka kb kp ma ke kf kq mb ki kj kr mc km kn ko ij bi translated">这个模型现在可以用来生成类似 Trump 的文本。可以给网络一个种子(一些文本)来预热，并在生成过程中 100%自主之前获得一些上下文。</p><p id="b443" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">在生成的同时也加入了一些即兴创作。在每一步中，所选择的字符并不总是网络输出中概率最高的字符。这个额外的步骤防止了循环，并给出了更多的人工结果。</p><p id="9a60" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">下面是一个低多样性的例子:</p><pre class="me mf mg mh gt mx my mz na aw nb bi"><span id="ff6c" class="ml lb iq my b gy nc nd l ne nf">[I, Donald J. Trump, president of the United ]States of The Apprentice and the world in the world that we have to be a great trade deal and the people of the World Trade Center in the world that we have to be a great person to be a great problem.</span></pre><p id="202a" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">同样的种子和更高的多样性:</p><pre class="me mf mg mh gt mx my mz na aw nb bi"><span id="5792" class="ml lb iq my b gy nc nd l ne nf">[I, Donald J. Trump, president of the United ]States. Were going to have you to say what happens to try to make America great again. We're going to tell you that we have to win. We owe $100.</span></pre><p id="bc4b" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">这个模型本身很有趣，但它很少连贯。它无法长时间保持上下文。很容易看到网络忘记了不久前开始的事情。</p><p id="cb15" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">60 个字符的序列长度相对较短可能是一个原因。该长度决定了训练时梯度可以向后传播多远(随时间截断的反向传播)。更大的“回顾”可以使学习更长的依赖性成为可能。例如，一个经过理想训练的 char-rnn 应该能够跟踪引号和括号，以便确保将它们关闭，记住当前句子的主题等。</p><p id="4170" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">糟糕的结果也可能来自数据本身。可能是尺寸太有限了。此外，推特可能不是学习语言的最佳方式。Tweets 很短并且独立，它们包含奇怪的文本，比如 URL 和用户句柄等等。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><h2 id="642a" class="ml lb iq bd lc mm mn dn lg mo mp dp lk kp mq mr lo kq ms mt ls kr mu mv lw mw bi translated">最后的测试…</h2><p id="6fcc" class="pw-post-body-paragraph jq jr iq jt b ju ly jw jx jy lz ka kb kp ma ke kf kq mb ki kj kr mc km kn ko ij bi translated">基于该网络了解到的一切，它能回答一个真正的问题吗:covfefe 推文的真正结局是什么？</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ng"><img src="../Images/d296c41ac78d2399424fca0f14f9f257.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z1GRYecAgimpJc9VdsAt_g.png"/></div></div></figure><p id="c7a0" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">以下是以推文开头为种子的一些尝试:</p><pre class="me mf mg mh gt mx my mz na aw nb bi"><span id="c525" class="ml lb iq my b gy nc nd l ne nf">[Despite the constant negative press cov]erage. I was going to be great. But if they’re going to stop the good guy who has a big night.</span><span id="e8cd" class="ml lb iq my b gy nl nd l ne nf">[Despite the constant negative press cov]erage. Looking forward to it. The fundraisers are totally promised by the Central Park South...</span><span id="cc90" class="ml lb iq my b gy nl nd l ne nf">[Despite the constant negative press cov]erage lines to Trump Tower. That’s not the relationships. It doesn’t have to be a disaster to Iraq.</span><span id="4bbb" class="ml lb iq my b gy nl nd l ne nf">[Despite the constant negative press cov]erage car millions of people and spot. We are going to come back. We have to go to golf.</span></pre><p id="6e33" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">看起来他真的是指报道…强大的 Covfefe 被错误的召唤了。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><p id="f10e" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated"><em class="js">这个故事到此结束。如果你喜欢，可以关注我的</em> <a class="ae ks" href="https://medium.com/@jctestud" rel="noopener"> <em class="js">中</em> </a> <em class="js">或</em> <a class="ae ks" href="https://twitter.com/jctestud" rel="noopener ugc nofollow" target="_blank"> <em class="js">推特</em> </a> <em class="js">获取最新消息。如果你喜欢美食，你可能会喜欢阅读</em> <a class="ae ks" href="https://medium.com/@jctestud/food-ingredient-reverse-engineering-through-gradient-descent-2a8d3880dd81" rel="noopener"> <em class="js">这篇</em> </a> <em class="js">。如果你更喜欢《权力的游戏》，</em> <a class="ae ks" href="https://medium.com/jctestud/game-of-thrones-word-embeddings-does-r-l-j-part-1-8ca70a8f1fad" rel="noopener"> <em class="js">去那里</em> </a> <em class="js">。</em></p><p id="8de9" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">另外，如果你发现有什么不对的地方，或者有任何问题，欢迎在下面评论。</p></div></div>    
</body>
</html>