<html>
<head>
<title>Outperforming Tensorflow’s Default Auto Differentiation Optimizers, with Interactive Code [Manual Back Prop with TF]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优于Tensorflow的默认自动微分优化器，具有交互式代码[带TF的手动回推]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/outperforming-tensorflows-default-auto-differentiation-optimizers-with-interactive-code-manual-e587a82d340e?source=collection_archive---------4-----------------------#2018-03-03">https://towardsdatascience.com/outperforming-tensorflows-default-auto-differentiation-optimizers-with-interactive-code-manual-e587a82d340e?source=collection_archive---------4-----------------------#2018-03-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/32c8646928a93c6b52b7cdfc7f1d1249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kBGYOhViXzgWubL0YWj_Gw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Image from <a class="ae kc" href="https://pixabay.com/en/winter-wintry-moon-human-2945906/" rel="noopener ugc nofollow" target="_blank">pixel bay</a></figcaption></figure><p id="2b29" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以我对这个想法思考了很久，有没有一种不同的(甚至更好的)方法来训练一个神经网络？Tensroflow、Keras、pyTorch等框架都很神奇，非常好用。这不仅要感谢它们为我们执行自动微分的能力，还要感谢它们为我们提供了广泛的优化选择。但这并不意味着我们只能依靠他们的自动微分。</p><p id="d37d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，让我们做一些不同的事情，我将尝试超越Tensorflows自动微分在他们的默认实现中，总共有10个优化器。我们将用来超越自动差异化的两项技术是…</p><p id="8c6c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">a.<a class="ae kc" href="https://becominghuman.ai/only-numpy-implementing-adding-gradient-noise-improves-learning-for-very-deep-networks-with-adf23067f9f1" rel="noopener ugc nofollow" target="_blank">谷歌大脑的梯度噪声</a> <br/> b. <a class="ae kc" href="https://hackernoon.com/only-numpy-dilated-back-propagation-and-google-brains-gradient-noise-with-interactive-code-3a527fc8003c" rel="noopener ugc nofollow" target="_blank">每层用ADAM优化器放大反向传播</a></p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="a3a1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">网络架构/实验任务</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi li"><img src="../Images/01891c91fd77c9a4cb182b39ddc1ba9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qYQPhfGB4dLQlHiyVuvvbg.png"/></div></div></figure><p id="a3d9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的实验非常简单，我们将使用全连接神经网络(有5层)，对<a class="ae kc" href="https://www.tensorflow.org/versions/r1.1/get_started/mnist/beginners#the_mnist_data" rel="noopener ugc nofollow" target="_blank"> Tensorflow的MNIST数据集</a>进行分类。以上是每一层是如何构建的。由于我们将使用不同的优化方法，各层应该有不同的方法来执行反向传播，因此有三种方法。标准反向传播、Google Brain的附加噪声和ADAM反向传播。另外，请注意两个细节。</p><blockquote class="ln lo lp"><p id="4f23" class="kd ke lq kf b kg kh ki kj kk kl km kn lr kp kq kr ls kt ku kv lt kx ky kz la ij bi translated">1.我们将使用Tensorflow <a class="ae kc" href="https://www.tensorflow.org/versions/r1.1/get_started/mnist/beginners#the_mnist_data" rel="noopener ugc nofollow" target="_blank"> MNIST </a>数据集提供的每一个数据。</p><p id="e3bc" class="kd ke lq kf b kg kh ki kj kk kl km kn lr kp kq kr ls kt ku kv lt kx ky kz la ij bi translated">2.我们将使用矢量图像。</p></blockquote></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="5aef" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">tensor flow优化人员列表</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lu"><img src="../Images/23b90d26c3c3099a14d10942bd7e6fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oEzj7QP-d3p0Hww3zEAzyQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Screen Shot from <a class="ae kc" href="https://www.tensorflow.org/api_guides/python/train#Optimizers" rel="noopener ugc nofollow" target="_blank">Tensor Flow</a></figcaption></figure><p id="2652" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以上是我们将与谷歌大脑的噪声和扩张反向传播进行比较的优化器的完整列表。现在，为了更容易地看出哪些是哪些，让我们为每个优化器分配颜色。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="799a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">对比案例列表</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lv"><img src="../Images/5beafbdf99463cad092f60515aa9f636.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wA1SlIwJ_uucYgGiHpx4hA.png"/></div></div></figure><p id="edef" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上所示，我们总共有17个案例，每个案例都有自己的颜色。请查看下面我使用的确切颜色。</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lw"><img src="../Images/c8d02d04522b9d2f269d971c335a2e46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Y0A_MMSBLIB3O8Bdp37ZA.png"/></div></div></figure><p id="cf9d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您希望了解更多关于matplotlib颜色的信息，请访问此页面获取更多信息<a class="ae kc" href="https://matplotlib.org/users/colors.html" rel="noopener ugc nofollow" target="_blank"/>。现在让我们给每种情况分配不同的优化方法。</p><pre class="lj lk ll lm gt lx ly lz ma aw mb bi"><span id="b5c5" class="mc md iq ly b gy me mf l mg mh">Case 0  → Google Brain's Added Gradient Noise + Standard Back Prop</span><span id="825e" class="mc md iq ly b gy mi mf l mg mh">Case 1  → Dilated ADAM Back Propagation Sparse Connection by Multiplication </span><span id="4809" class="mc md iq ly b gy mi mf l mg mh">Case 2  → Dilated ADAM Back Propagation Sparse Connection by Multiplication + Google Brain's Added Gradient Noise</span><span id="904f" class="mc md iq ly b gy mi mf l mg mh">Case 3  → Dilated ADAM Back Propagation Dense Connection by Addition</span><span id="2d6d" class="mc md iq ly b gy mi mf l mg mh">Case 4  → Dilated ADAM Back Propagation Dense Connection by Multiplication</span><span id="cacc" class="mc md iq ly b gy mi mf l mg mh">Case 5  → Dilated ADAM Back Propagation Dense Connection by Addition (Different Decay / Proportion Rate)</span><span id="9685" class="mc md iq ly b gy mi mf l mg mh">Case 6  → Dilated ADAM Back Propagation Dense Connection by Addition (Different Decay / Proportion Rate)</span><span id="340a" class="mc md iq ly b gy mi mf l mg mh">Case 7  → <a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer" rel="noopener ugc nofollow" target="_blank">tf.train.GradientDescentOptimizer</a><br/>Case 8  → <a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/AdadeltaOptimizer" rel="noopener ugc nofollow" target="_blank">tf.train.AdadeltaOptimizer</a><br/>Case 9  → <a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer" rel="noopener ugc nofollow" target="_blank">tf.train.AdagradOptimizer</a> <br/>Case 10 → <a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/AdagradDAOptimizer" rel="noopener ugc nofollow" target="_blank">tf.train.AdagradDAOptimizer</a><br/>Case 11 → <a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer" rel="noopener ugc nofollow" target="_blank">tf.train.MomentumOptimizer</a><br/>Case 12 → <a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer" rel="noopener ugc nofollow" target="_blank">tf.train.AdamOptimizer</a><br/>Case 13 → <a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer" rel="noopener ugc nofollow" target="_blank">tf.train.FtrlOptimizer</a><br/>Case 14 → <a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/ProximalGradientDescentOptimizer" rel="noopener ugc nofollow" target="_blank">tf.train.ProximalGradientDescentOptimizer</a><br/>Case 15 → <a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/ProximalAdagradOptimizer" rel="noopener ugc nofollow" target="_blank">tf.train.ProximalAdagradOptimizer</a><br/>Case 16 → <a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer" rel="noopener ugc nofollow" target="_blank">tf.train.RMSPropOptimizer</a></span></pre><blockquote class="mj"><p id="4c1f" class="mk ml iq bd mm mn mo mp mq mr ms la dk translated"><strong class="ak"> <em class="mt">本质上情况0 ~ 6是手动反向传播，情况7 ~ 16是自动微分。</em>T3】</strong></p></blockquote></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="bb77" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">不同试验列表/基础优势/ <br/>随机初始化</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mu"><img src="../Images/cb1823f577ee2e0e10c66008c13e2413.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Crj7Pt2kJZosRTxTpWOX2Q.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Experiment Trials</figcaption></figure><p id="1887" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果其中一个优化方法<strong class="kf ir"> <em class="lq">从根本上优于另一个</em> </strong>，那么每次我们运行实验时，那个方法都会胜过其余的。为了增加我们捕捉这一基本特征的概率，让我们进行3次不同的试验，每次试验我们将进行10次试验。(<em class="lq">**注意* *每次试验的超参数设置互不相同。</em>)</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mv"><img src="../Images/6b3eec9cb4e04bd4c25556643ad29d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Meyq9NL9VofF3AnLQTCFMw.png"/></div></div></figure><p id="1150" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另外为了保证方法的优越性，让随机种子值来初始化权重。</p><blockquote class="ln lo lp"><p id="769d" class="kd ke lq kf b kg kh ki kj kk kl km kn lr kp kq kr ls kt ku kv lt kx ky kz la ij bi translated"><strong class="kf ir">每次实验后，我们都会比较所有案例，以了解哪一个案例有… </strong></p><p id="6c18" class="kd ke lq kf b kg kh ki kj kk kl km kn lr kp kq kr ls kt ku kv lt kx ky kz la ij bi translated"><strong class="kf ir"> a .最低成本率(或错误率)<br/> b .训练图像的最高精度<br/> c .测试图像的最高精度</strong></p><p id="4945" class="kd ke lq kf b kg kh ki kj kk kl km kn lr kp kq kr ls kt ku kv lt kx ky kz la ij bi translated"><strong class="kf ir">当所有实验完成后，我们将会看到频率条形图，其中哪个案例在每个标准下表现最佳。</strong></p></blockquote></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="6e6e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">试验一结果</strong></p><div class="lj lk ll lm gt ab cb"><figure class="mw jr mx my mz na nb paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/6e50acef9cff51ec20fee2eec705edc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*EtyaRJ0Wg6cqXuWGY6DoUg.png"/></div></figure><figure class="mw jr mx my mz na nb paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/92e5e84e888a763bbfe3222c43d0c7ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*mRLMF97TxkfofbS04G9RvQ.png"/></div></figure><figure class="mw jr nc my mz na nb paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/80861d34badd7e994a17e924d9901026.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*_yp_GOSYGZFASsPFd_dqjQ.png"/></div></figure></div><p id="7f49" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">左侧图</strong>→10次实验的频率柱状图<em class="lq">最低成本率</em> <br/> <strong class="kf ir">中间图</strong>→10次实验的频率柱状图<em class="lq">训练图像的最高准确率</em> <br/> <strong class="kf ir">右侧图</strong>→10次实验的频率柱状图<em class="lq">测试图像的最高准确率</em></p><p id="b622" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当学习率设置为0.001时，神经元更宽，每层有1024个神经元。似乎扩张反向传播易于过度拟合，因为它们在训练图像上的最低成本率和最高精度上最频繁，但是在测试图像上的最高精度上没有显示出来。</p><p id="fbc5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我看到了一种潜力，通过适当的调整，他们可以超越每一个案例。</p><pre class="lj lk ll lm gt lx ly lz ma aw mb bi"><span id="486d" class="mc md iq ly b gy me mf l mg mh"><strong class="ly ir">Percentage of Best Performing at <em class="lq">Lowest Cost Rate</em></strong></span><span id="e3cf" class="mc md iq ly b gy mi mf l mg mh">1. <strong class="ly ir">Case 5: 60%</strong><br/>('Dilated ADAM Back Propagation Dense Connection by Addition (Different Decay / Proportion Rate)')</span><span id="80d9" class="mc md iq ly b gy mi mf l mg mh">2. <strong class="ly ir">Case 3: 30%</strong><br/>('Dilated ADAM Back Propagation Dense Connection by Addition')</span><span id="cc28" class="mc md iq ly b gy mi mf l mg mh">3. <strong class="ly ir">Case 0: 20%</strong><br/>('Google Brain's Added Gradient Noise + Standard Back Prop')</span><span id="04c7" class="mc md iq ly b gy mi mf l mg mh">4. <strong class="ly ir">Case 1: 10%</strong><br/>('Dilated ADAM Back Propagation Sparse Connection by Multiplication')<br/>-------------------------------------------------------------<br/><strong class="ly ir">Percentage of Best Performing at <em class="lq">Highest Accuracy on Training Images</em></strong></span><span id="e52a" class="mc md iq ly b gy mi mf l mg mh">1. <strong class="ly ir">Case 3: 40%</strong><br/>('Dilated ADAM Back Propagation Dense Connection by Addition')</span><span id="7a81" class="mc md iq ly b gy mi mf l mg mh">2. <strong class="ly ir">Case 5: 30%</strong><br/>('Dilated ADAM Back Propagation Dense Connection by Addition (Different Decay / Proportion Rate)')</span><span id="ece8" class="mc md iq ly b gy mi mf l mg mh">3. <strong class="ly ir">Case 0: 20%</strong><br/>('Google Brain's Added Gradient Noise + Standard Back Prop')</span><span id="1c37" class="mc md iq ly b gy mi mf l mg mh">4. <strong class="ly ir">Case 1: 10%</strong><br/>('Dilated ADAM Back Propagation Sparse Connection by Multiplication')<br/>-------------------------------------------------------------<br/><strong class="ly ir">Percentage of Best Performing at <em class="lq">Highest Accuracy on Testing Images</em></strong></span><span id="4eed" class="mc md iq ly b gy mi mf l mg mh">1. <strong class="ly ir">Case 0: 60%</strong><br/>('Google Brain's Added Gradient Noise + Standard Back Prop')</span><span id="e80c" class="mc md iq ly b gy mi mf l mg mh">2. <strong class="ly ir">Case 16: 40%</strong><br/>('<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer" rel="noopener ugc nofollow" target="_blank">tf.train.RMSPropOptimizer</a>')</span></pre><blockquote class="mj"><p id="bffd" class="mk ml iq bd mm mn mo mp mq mr ms la dk translated">实际上，情况0 ~ 6是人工反向传播，而情况7 ~ 16是自动微分。</p></blockquote></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="2de8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">试验2结果</strong></p><div class="lj lk ll lm gt ab cb"><figure class="mw jr nd my mz na nb paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/cf8e98bb111fd4d722d2aa073d619be2.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*rMFybuOtXX8zzO46NQFI6g.png"/></div></figure><figure class="mw jr nd my mz na nb paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/bf6d4e12c18c5128d507d8bdb09f0b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*YFNgDyrCRopW1KSFgp1Oag.png"/></div></figure><figure class="mw jr nd my mz na nb paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/48be40ed74df6e656e5d9933036b19b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*HSZuECgn51R1KFHBBUMVBg.png"/></div></figure></div><p id="5e27" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">左侧图</strong> →最低成本率 <br/> <strong class="kf ir">上10次实验的频率柱状图</strong> →训练图像上10次实验的频率柱状图<em class="lq">→最高准确率</em> <br/> <strong class="kf ir">右侧图</strong> →测试图像上10次实验的频率柱状图<em class="lq">最高准确率</em></p><p id="200f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当学习率设置得稍高(0.0025)且神经元较窄(824)时，情况5(大部分)优于所有其他情况。</p><pre class="lj lk ll lm gt lx ly lz ma aw mb bi"><span id="f8ff" class="mc md iq ly b gy me mf l mg mh"><strong class="ly ir">Percentage of Best Performing at <em class="lq">Lowest Cost Rate</em></strong></span><span id="53e3" class="mc md iq ly b gy mi mf l mg mh">1. <strong class="ly ir">Case 3: 60%</strong><br/>('<em class="lq">Dilated ADAM Back Propagation Dense Connection by Addition</em>')</span><span id="d6e9" class="mc md iq ly b gy mi mf l mg mh">2. <strong class="ly ir">Case 5: 40%</strong><br/>('Dilated ADAM Back Propagation Dense Connection by Addition (Different Decay / Proportion Rate)')<br/>-------------------------------------------------------------<br/><strong class="ly ir">Percentage of Best Performing at <em class="lq">Highest Accuracy on Training Images</em></strong></span><span id="a410" class="mc md iq ly b gy mi mf l mg mh">1. <strong class="ly ir">Case 3: 50%</strong><br/>('<em class="lq">Dilated ADAM Back Propagation Dense Connection by Addition</em>')</span><span id="322f" class="mc md iq ly b gy mi mf l mg mh">2. <strong class="ly ir">Case 5: 50%</strong><br/>('Dilated ADAM Back Propagation Dense Connection by Addition (Different Decay / Proportion Rate)')<br/>-------------------------------------------------------------<br/><strong class="ly ir">Percentage of Best Performing at <em class="lq">Highest Accuracy on Testing Images</em></strong></span><span id="a153" class="mc md iq ly b gy mi mf l mg mh">1. <strong class="ly ir">Case 5: 70%</strong><br/>('Dilated ADAM Back Propagation Dense Connection by Addition (Different Decay / Proportion Rate)')</span><span id="1ae4" class="mc md iq ly b gy mi mf l mg mh">2. <strong class="ly ir">Case 3: 20%</strong><br/>('<em class="lq">Dilated ADAM Back Propagation Dense Connection by Addition</em>')</span><span id="725d" class="mc md iq ly b gy mi mf l mg mh">3. <strong class="ly ir">Case 1: 10%</strong><br/>('Dilated ADAM Back Propagation Sparse Connection by Multiplication')</span></pre><blockquote class="mj"><p id="f124" class="mk ml iq bd mm mn mo mp mq mr ms la dk translated">实际上，情况0 ~ 6是人工反向传播，而情况7 ~ 16是自动微分。</p></blockquote></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="f217" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">试验三结果</strong></p><div class="lj lk ll lm gt ab cb"><figure class="mw jr nd my mz na nb paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/8ee9059ba5412840ab14d924cb806f77.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*xWm5rn-9MOrUe334eOcMDg.png"/></div></figure><figure class="mw jr nd my mz na nb paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/2fff14231f21968ccaafe54fc7fadcbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*h_rv-aUEkjWkBPrcR9p2kg.png"/></div></figure><figure class="mw jr nd my mz na nb paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/17b2b4e2e5763afbd8d38165dab978e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*PF0Ll_BT_B2OZ5FzFH_7pw.png"/></div></figure></div><p id="996c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">左图</strong> →最低成本率 <br/> <strong class="kf ir">上10次实验的频率柱状图</strong> →训练图像上10次实验的频率柱状图<em class="lq"/><br/><strong class="kf ir">右图</strong> →测试图像上10次实验的频率柱状图<em class="lq"/></p><p id="2308" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当学习率设置为0.001，神经元更窄(824)时，谷歌自己的方法(大部分)优于所有其他情况。</p><pre class="lj lk ll lm gt lx ly lz ma aw mb bi"><span id="d945" class="mc md iq ly b gy me mf l mg mh"><strong class="ly ir">Percentage of Best Performing at <em class="lq">Lowest Cost Rate</em></strong></span><span id="64c6" class="mc md iq ly b gy mi mf l mg mh">1. <strong class="ly ir">Case 0: 90%</strong><br/>('Google Brain's Added Gradient Noise + Standard Back Prop')</span><span id="98d8" class="mc md iq ly b gy mi mf l mg mh">2. <strong class="ly ir">Case 3: 10%</strong><br/>('<em class="lq">Dilated ADAM Back Propagation Dense Connection by Addition</em>')<br/>-------------------------------------------------------------<br/><strong class="ly ir">Percentage of Best Performing at <em class="lq">Highest Accuracy on Training Images</em></strong></span><span id="cd70" class="mc md iq ly b gy mi mf l mg mh">1. <strong class="ly ir">Case 0: 90%</strong><br/>('Google Brain's Added Gradient Noise + Standard Back Prop')</span><span id="81f9" class="mc md iq ly b gy mi mf l mg mh">2. <strong class="ly ir">Case 3: 10%</strong><br/>('<em class="lq">Dilated ADAM Back Propagation Dense Connection by Addition</em>')<br/>-------------------------------------------------------------<br/><strong class="ly ir">Percentage of Best Performing at <em class="lq">Highest Accuracy on Testing Images</em></strong></span><span id="f7f6" class="mc md iq ly b gy mi mf l mg mh">1. <strong class="ly ir">Case 0: 70%</strong><br/>('Google Brain's Added Gradient Noise + Standard Back Prop')</span><span id="4521" class="mc md iq ly b gy mi mf l mg mh">2. <strong class="ly ir">Case 7: 10%</strong><br/>('<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer" rel="noopener ugc nofollow" target="_blank">tf.train.GradientDescentOptimizer</a>')</span><span id="4374" class="mc md iq ly b gy mi mf l mg mh">3. <strong class="ly ir">Case 14: 10%</strong><br/>('<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/ProximalGradientDescentOptimizer" rel="noopener ugc nofollow" target="_blank">tf.train.ProximalGradientDescentOptimizer</a>')</span><span id="aff6" class="mc md iq ly b gy mi mf l mg mh">4. <strong class="ly ir">Case 16: 10%</strong><br/>('<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer" rel="noopener ugc nofollow" target="_blank">tf.train.RMSPropOptimizer</a>')</span></pre><blockquote class="mj"><p id="1947" class="mk ml iq bd mm mn mo mp mq mr ms la dk translated">实际上，情况0 ~ 6是人工反向传播，而情况7 ~ 16是自动微分。</p></blockquote></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="9e14" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">(3月6日更新)试验4结果</strong></p><div class="lj lk ll lm gt ab cb"><figure class="mw jr nd my mz na nb paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/4eac515456fe5ced91c0d8b45733c18f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*3UktGLaezwChqp4FYl7MiA.png"/></div></figure><figure class="mw jr nd my mz na nb paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/c1e17dfdd6c1d12e2a069bb87a725bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*e0gickrYguXP_FRKNDKggg.png"/></div></figure><figure class="mw jr nd my mz na nb paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/b6d38560b5bc55bea940004d8e804d5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*EnYdtcm0v727tgznow-5EQ.png"/></div></figure></div><p id="c7ab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">左侧图</strong>→10次实验的频率柱状图<em class="lq">最低成本率</em> <br/> <strong class="kf ir">中间图</strong>→10次实验的频率柱状图<em class="lq">训练图像的最高准确率</em> <br/> <strong class="kf ir">右侧图</strong>→10次实验的频率柱状图<em class="lq">测试图像的最高准确率</em></p><p id="6a52" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当学习率被设置为0.0025且具有更宽的神经元(1024)时，情况2表现良好。</p><pre class="lj lk ll lm gt lx ly lz ma aw mb bi"><span id="0ecd" class="mc md iq ly b gy me mf l mg mh"><strong class="ly ir">Percentage of Best Performing at <em class="lq">Lowest Cost Rate</em></strong></span><span id="e40a" class="mc md iq ly b gy mi mf l mg mh">1. <strong class="ly ir">Case 5: 60%</strong><br/>('Dilated ADAM Back Propagation Dense Connection by Addition (Different Decay / Proportion Rate)')</span><span id="1eb0" class="mc md iq ly b gy mi mf l mg mh">2. <strong class="ly ir">Case 3: 40%</strong><br/>('Dilated ADAM Back Propagation Dense Connection by Addition')<br/>-------------------------------------------------------------<br/><strong class="ly ir">Percentage of Best Performing at <em class="lq">Highest Accuracy on Training Images</em></strong></span><span id="7513" class="mc md iq ly b gy mi mf l mg mh">1. <strong class="ly ir">Case 5: 60%</strong><br/>('Dilated ADAM Back Propagation Dense Connection by Addition (Different Decay / Proportion Rate)')</span><span id="e18a" class="mc md iq ly b gy mi mf l mg mh">2. <strong class="ly ir">Case 3: 40%</strong><br/>('Dilated ADAM Back Propagation Dense Connection by Addition')<br/>-------------------------------------------------------------<br/><strong class="ly ir">Percentage of Best Performing at <em class="lq">Highest Accuracy on Testing Images</em></strong></span><span id="32bd" class="mc md iq ly b gy mi mf l mg mh">1. <strong class="ly ir">Case 3: 50%</strong><br/>('Dilated ADAM Back Propagation Dense Connection by Addition')</span><span id="cf74" class="mc md iq ly b gy mi mf l mg mh">2. <strong class="ly ir">Case 1: 30%</strong><br/>('Dilated ADAM Back Propagation Sparse Connection by Multiplication')</span><span id="fdbb" class="mc md iq ly b gy mi mf l mg mh">2. <strong class="ly ir">Case 5: 20%</strong><br/>('Dilated ADAM Back Propagation Dense Connection by Addition (Different Decay / Proportion Rate)')</span></pre></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="3c14" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">存档的培训结果</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/5caee87100cd9ed8bd0d6295578d6d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mVTC_u5RLxoLIVdutuYpKw.png"/></div></div></figure><p id="877c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了增加这个实验的透明度，我写了另一篇博文，其中包含随着时间推移的成本图、随着时间推移的训练图像的准确性图和随着时间推移的测试图像的准确性图。<a class="ae kc" href="https://medium.com/@SeoJaeDuk/archived-training-results-out-performing-tensorflows-default-auto-differentiation-optimizers-28c5a75e9fc0" rel="noopener">要访问它，请点击此处</a>。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="9eed" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">缺点</strong></p><p id="e40f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我还没有时间来优化每个汽车差异化的所有超参数。然而，我在每种情况下都保持了完全相同的学习率，但是有可能我设置的学习率对于张量流的自动微分来说不是最优的。如果你正在进行这个实验(如果你愿意，你可以使用我在交互代码部分提供的代码。)并为每个设置找到一个好的hyper参数，请通过评论让我知道，我很想看看它们是否也能被超越。</p><p id="685f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也就是说，我相信Tensorflow已经超级优化了它的算法，这使得自动微分执行得更快，但在每个网络上都获得了最高的性能。总的来说，这可能是一个公平的比较。</p><p id="4cc0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我还注意到扩张反向传播的两点。<br/> <em class="lq"> 1。它在浅网络或神经元数量较少的情况下表现不佳<br/> 2。它长期表现良好。</em></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/7af657743661098652e5b623c21ee09f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*WC0rSwu0D5dONUPvxhk3gw.png"/></div></figure><p id="96d3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">红框→ </strong>案例15的错误率低于任何其他案例<br/> <strong class="kf ir">蓝框→ </strong>在第100个时期后，案例5开始超出执行</p><p id="e6b4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上所述，大多数时候，在第一个100个时期，自动微分方法具有较低的误差率。然而，在一定量的时期之后，例如100或150，扩张的反向传播开始胜过。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="e80a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">交互代码</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/2e23f5101d5c76caf4a47a43a7b3457b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*koGnjlVMCBvyA3nWlUTr9Q.png"/></div></div></figure><p id="db91" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lq">我搬到了谷歌Colab寻找交互代码！所以你需要一个谷歌帐户来查看代码，你也不能在谷歌实验室运行只读脚本，所以在你的操场上做一个副本。最后，我永远不会请求允许访问你在Google Drive上的文件，仅供参考。编码快乐！</em></p><p id="bb0d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要访问<a class="ae kc" href="https://colab.research.google.com/drive/1JZgghElrUi_PExj04mLiIWYPYjvb0Md7" rel="noopener ugc nofollow" target="_blank">试验1的代码，请点击此处</a>。<br/>要访问<a class="ae kc" href="https://colab.research.google.com/drive/1BzDIPr386Nq9Pnqdgt7xKAVteyYS_IO5" rel="noopener ugc nofollow" target="_blank">试验2的代码，请点击此处</a>。<br/>要访问<a class="ae kc" href="https://colab.research.google.com/drive/1fZYVGloay9AUVBWKidKFLiRVpcz9NefO" rel="noopener ugc nofollow" target="_blank">试验3的代码，请点击此处</a>。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="468a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">引用<br/>(如果您希望使用该实现或任何信息，请引用这篇博文)</strong></p><p id="93f5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lq"> APA </em></p><pre class="lj lk ll lm gt lx ly lz ma aw mb bi"><span id="f009" class="mc md iq ly b gy me mf l mg mh">Outperforming Tensorflow’s Default Auto Differentiation Optimizers, with Interactive Code [Manual…. (2018). Medium. Retrieved 3 March 2018, from <a class="ae kc" href="https://medium.com/@SeoJaeDuk/outperforming-tensorflows-default-auto-differentiation-optimizers-with-interactive-code-manual-e587a82d340e" rel="noopener">https://medium.com/@SeoJaeDuk/outperforming-tensorflows-default-auto-differentiation-optimizers-with-interactive-code-manual-e587a82d340e</a></span></pre><p id="14e2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lq">司法协助</em></p><pre class="lj lk ll lm gt lx ly lz ma aw mb bi"><span id="df1b" class="mc md iq ly b gy me mf l mg mh">"Outperforming Tensorflow’S Default Auto Differentiation Optimizers, With Interactive Code [Manual…." Medium. N. p., 2018. Web. 3 Mar. 2018.</span></pre><p id="e8ab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lq">哈佛</em></p><pre class="lj lk ll lm gt lx ly lz ma aw mb bi"><span id="179a" class="mc md iq ly b gy me mf l mg mh">Medium. (2018). Outperforming Tensorflow’s Default Auto Differentiation Optimizers, with Interactive Code [Manual…. [online] Available at: <a class="ae kc" href="https://medium.com/@SeoJaeDuk/outperforming-tensorflows-default-auto-differentiation-optimizers-with-interactive-code-manual-e587a82d340e" rel="noopener">https://medium.com/@SeoJaeDuk/outperforming-tensorflows-default-auto-differentiation-optimizers-with-interactive-code-manual-e587a82d340e</a> [Accessed 3 Mar. 2018].</span></pre></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="bbd1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">最后的话</strong></p><p id="86ee" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我想用我最喜欢的两句话来结束这篇文章。</p><p id="f940" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://en.wikipedia.org/wiki/Jeff_Bezos" rel="noopener ugc nofollow" target="_blank">杰夫·贝索斯</a>(亚马逊CEO):<a class="ae kc" href="https://medium.com/parsa-vc/what-i-learned-from-jeff-bezos-after-reading-every-amazon-shareholder-letter-172d92f38a41" rel="noopener"><strong class="kf ir"><em class="lq">一切都是为了长远……</em></strong></a><br/><a class="ae kc" href="https://en.wikipedia.org/wiki/Ginni_Rometty" rel="noopener ugc nofollow" target="_blank">金尼·罗梅蒂</a>(IBM CEO):<a class="ae kc" href="http://www.businessinsider.com/ibm-ceo-growth-and-comfort-dont-co-exist-2014-10" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"><em class="lq">成长与安逸不能共存</em> </strong> </a></p><p id="d72e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果发现任何错误，请发电子邮件到jae.duk.seo@gmail.com给我，如果你希望看到我所有写作的列表，请在这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同时，在我的twitter上关注我<a class="ae kc" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae kc" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae kc" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>了解更多内容。如果你感兴趣的话，我还做了解耦神经网络<a class="ae kc" href="https://becominghuman.ai/only-numpy-implementing-and-comparing-combination-of-google-brains-decoupled-neural-interfaces-6712e758c1af" rel="noopener ugc nofollow" target="_blank">的比较。</a></p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="a1f4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">参考</strong></p><ol class=""><li id="469b" class="nh ni iq kf b kg kh kk kl ko nj ks nk kw nl la nm nn no np bi translated">训练| TensorFlow。(2018).张量流。检索于2018年3月3日，来自<a class="ae kc" href="https://www.tensorflow.org/api_guides/python/train#Optimizers" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ guides/python/train # optimizer</a></li><li id="ed5b" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">Seo，J. D. (2018年02月05日)。带张量流的手动回推:解耦递归神经网络，从Google修改NN…2018年2月21日检索，来自<a class="ae kc" rel="noopener" target="_blank" href="/manual-back-prop-with-tensorflow-decoupled-recurrent-neural-network-modified-nn-from-google-f9c085fe8fae">https://towards data science . com/manual-Back-Prop-with-tensor flow-Decoupled-Recurrent-Neural-Network-modified-NN-from-Google-f9c 085 Fe 8 FAE</a></li><li id="93cd" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">j . bort(2014年10月07日)。IBM首席执行官Ginni Rometty:增长和舒适不能共存。检索于2018年2月21日，来自<a class="ae kc" href="http://www.businessinsider.com/ibm-ceo-growth-and-comfort-dont-co-exist-2014-10" rel="noopener ugc nofollow" target="_blank">http://www . business insider . com/IBM-CEO-growth-and-comfort-don-co-exist-2014-10</a></li><li id="dbe2" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">Saljoughian，P. (2017年11月20日)。读完每一封亚马逊股东信后，我从杰夫·贝索斯身上学到了什么。检索于2018年2月21日，来自<a class="ae kc" href="https://medium.com/parsa-vc/what-i-learned-from-jeff-bezos-after-reading-every-amazon-shareholder-letter-172d92f38a41" rel="noopener">https://medium . com/parsa-VC/what-I-learn-from-Jeff-be zos-after-reading-every-Amazon-shareholder-letter-172d 92 f 38 a 41</a></li><li id="ad6c" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">页（page的缩写）(未注明)。Pinae/TensorFlow-MNIST示例。检索于2018年2月22日，来自<a class="ae kc" href="https://github.com/pinae/TensorFlow-MNIST-example/blob/master/fully-connected.py" rel="noopener ugc nofollow" target="_blank">https://github . com/pinae/tensor flow-MNIST-example/blob/master/full-connected . py</a></li><li id="9e1a" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">如何打印一个前面有一定数量空格的整数？(未注明)。检索于2018年2月22日，来自<a class="ae kc" href="https://stackoverflow.com/questions/45521183/how-do-i-print-an-integer-with-a-set-number-of-spaces-before-it" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/45521183/how-do-I-print-an-integer-with-set-number-of-spaces-before-it</a></li><li id="9b79" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">如何在没有科学记数法和给定精度的情况下漂亮地打印一个numpy.array？(未注明)。检索于2018年2月22日，来自<a class="ae kc" href="https://stackoverflow.com/questions/2891790/how-to-pretty-printing-a-numpy-array-without-scientific-notation-and-with-given" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/2891790/how-to-pretty-printing-a-numpy-array-with-with-given</a></li><li id="d843" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">将浮点数限制在小数点后两位。(未注明)。检索于2018年2月22日，来自<a class="ae kc" href="https://stackoverflow.com/questions/455612/limiting-floats-to-two-decimal-points" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/455612/limiting-floats-to-two-decimal-points</a></li><li id="e661" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">如何防止tensorflow分配整个GPU内存？(未注明)。检索于2018年2月23日，来自<a class="ae kc" href="https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/34199233/how-to-prevent-tensor flow-from-allocation-the-total-of-a-GPU-memory</a></li><li id="1b9a" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">关闭tensorflow中的会话不会重置图形。(未注明)。检索于2018年2月23日，来自<a class="ae kc" href="https://stackoverflow.com/questions/42706761/closing-session-in-tensorflow-doesnt-reset-graph" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/42706761/closing-session-in-tensor flow-sints-reset-graph</a></li><li id="4c3b" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">[1]“TF . reset _ default _ graph | tensor flow”，<em class="lq"> TensorFlow </em>，2018。【在线】。可用:<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/reset_default_graph." rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/reset _ default _ graph。</a>【访问时间:2018年2月23日】。</li><li id="8981" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">在tensorflow中创建float64变量。(未注明)。检索于2018年2月23日，来自<a class="ae kc" href="https://stackoverflow.com/questions/35884045/creating-a-float64-variable-in-tensorflow" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/35884045/creating-a-float 64-variable-in-tensor flow</a></li><li id="fe78" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">Tensorflow中的global_step是什么意思？(未注明)。检索于2018年2月23日，来自<a class="ae kc" href="https://stackoverflow.com/questions/41166681/what-does-global-step-mean-in-tensorflow" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/41166681/what-does-global-step-mean-in-tensor flow</a></li><li id="c5dc" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">python中的零列表。(未注明)。检索于2018年2月23日，来自<a class="ae kc" href="https://stackoverflow.com/questions/8528178/list-of-zeros-in-python" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/8528178/list-of-zeros-in-python</a></li><li id="c02d" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">使用列表中的max()/min()获取返回的max或min项的索引。(未注明)。检索于2018年2月23日，来自<a class="ae kc" href="https://stackoverflow.com/questions/2474015/getting-the-index-of-the-returned-max-or-min-item-using-max-min-on-a-list" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/2474015/getting-the-index-of-returned-max-or-min-item-using-max-min-on-a-list</a></li><li id="25c5" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">如何防止tensorflow分配整个GPU内存？(未注明)。2018年2月23日检索，来自<a class="ae kc" href="https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/34199233/how-to-prevent-tensor flow-from-allocation-the-total-of-a-GPU-memory</a></li><li id="5c2f" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">如何用TensorFlow得到稳定的结果，设置随机种子？(未注明)。2018年2月23日检索，来自<a class="ae kc" href="https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/36288235/how-to-get-stable-results-with-tensor flow-setting-random-seed</a></li><li id="93b7" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">指定颜色。(未注明)。检索于2018年2月23日，来自<a class="ae kc" href="https://matplotlib.org/users/colors.html" rel="noopener ugc nofollow" target="_blank">https://matplotlib.org/users/colors.html</a></li><li id="3fb2" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">艾森，硕士(未注明)。使用matplotlib设置绘图的条形颜色。检索于2018年2月24日，来自<a class="ae kc" href="http://matthiaseisen.com/pp/patterns/p0178/" rel="noopener ugc nofollow" target="_blank">http://matthiaseisen.com/pp/patterns/p0178/</a></li><li id="860a" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">关闭事件。(未注明)。检索于2018年2月24日，来自<a class="ae kc" href="https://matplotlib.org/gallery/event_handling/close_event.html#sphx-glr-gallery-event-handling-close-event-py" rel="noopener ugc nofollow" target="_blank">https://matplotlib . org/gallery/event _ handling/close _ event . html # sphx-glr-gallery-event-handling-close-event-py</a></li><li id="a248" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">模块:tf.contrib.opt | TensorFlow。(2018).张量流。检索于2018年3月3日，来自<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/opt" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/contrib/opt</a></li><li id="4ba6" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">吉尼·罗梅蒂。(2018).En.wikipedia.org。检索于2018年3月3日，来自<a class="ae kc" href="https://en.wikipedia.org/wiki/Ginni_Rometty" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Ginni_Rometty</a></li><li id="445a" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">杰夫·贝索斯。(2018).En.wikipedia.org。检索于2018年3月3日，来自<a class="ae kc" href="https://en.wikipedia.org/wiki/Jeff_Bezos" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Jeff_Bezos</a></li><li id="ae55" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">Only Numpy:实现“添加梯度噪声改善非常深度网络的学习”来自…(2018).成为人类:人工智能杂志。检索于2018年3月3日，来自<a class="ae kc" href="https://becominghuman.ai/only-numpy-implementing-adding-gradient-noise-improves-learning-for-very-deep-networks-with-adf23067f9f1" rel="noopener ugc nofollow" target="_blank">https://becoming human . ai/only-numpy-implementing-adding-gradient-noise-improves-learning-for-very-deep-networks-with-ADF 23067 F9 f1</a></li></ol></div></div>    
</body>
</html>