<html>
<head>
<title>Effect of Feature Standardization on Linear Support Vector Machines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征标准化对线性支持向量机的影响</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/effect-of-feature-standardization-on-linear-support-vector-machines-13213765b812?source=collection_archive---------4-----------------------#2017-07-26">https://towardsdatascience.com/effect-of-feature-standardization-on-linear-support-vector-machines-13213765b812?source=collection_archive---------4-----------------------#2017-07-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="2c40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为支持向量机(SVM)优化是通过最小化决策向量<em class="kl"> w，</em>来实现的，所以最优超平面受输入特征规模的影响，因此建议在 SVM 模型训练之前对数据进行标准化(均值为 0，方差为 1)。在这篇文章中，我展示了标准化对双特征线性 SVM 的影响，以及一些(希望)对结果的实际解释。</p><p id="1251" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我在这里使用的数据来自<a class="ae km" href="https://collegescorecard.ed.gov/" rel="noopener ugc nofollow" target="_blank">大学记分卡</a>，这是一个免费的美国教育部数据来源，包含美国大学的各种统计数据。我使用了由 Kaggle 托管的<a class="ae km" href="https://www.kaggle.com/kaggle/college-scorecard" rel="noopener ugc nofollow" target="_blank">版本的大学记分卡数据。</a></p><p id="702c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用此数据源，我将训练一个 SVM 来根据(1)注册学生的平均 SAT 分数和(2)注册学生的平均家庭收入预测大学的州内学费是大于还是小于每年 2 万美元。这个模型有点做作和简单——我的主要目标不是成功地预测目标类本身，而是展示特性标准化对最终模型的影响。</p><p id="bbf4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在该数据集中，有 4166 所大学具有非空平均 SAT 分数和家庭收入数据。有相当多的大学在这些特征上具有空值，我在分析中排除了这些特征:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi kn"><img src="../Images/f909fee0811875a15b3ad8c757a90f4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*b3kJlvPQSSQJc8GDifw7zA.png"/></div></figure><p id="4bbe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每年 2 万美元的州内学费介于该变量的第 50 和第 75 百分位之间——因此在这个目标阶层的两边都有相当数量的大学。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi kv"><img src="../Images/d04abd96544987cfdd2b00dd7ca80350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*_XOvK_QSMdxSLeLHoeQTog.png"/></div></figure><p id="6b88" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面，我根据平均 SAT 分数和家庭收入绘制了大学学费。应该立即明确的是，这些数据不是线性可分的。在线性 SVM 中，没有超平面可以按照学费类别清晰地分割数据点——在模型选择的任何决策超平面中，总是会有大学位于“错误的一边”。sklearn SVM 实现将根据用户指定的超参数“C ”,搜索最佳超平面(和+1/-1 边缘超平面),该超参数确定边缘超平面“错误侧”上的数据点的惩罚程度。</p><p id="3c76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于每组特征(非标准化，然后标准化)，我运行了 11 个线性 SVM 模型，C 系数为 5**-3，5**-2，…5**3。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="kw kx l"/></div></figure><p id="4bfa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">很明显，这两种模型在运行时间上有很大的不同:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi ky"><img src="../Images/e70e724f0762560e2a5a838aa849b2bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*9tvhL-4pdMRsPEUlzPOIIQ.png"/></div></figure><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi kz"><img src="../Images/a656147dadf6054e02eb84c771864c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*WNGFMUWy8Py6Fm3QZB1jcw.png"/></div></figure><p id="c361" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下两个表格记录了每个拟合 SVM 的分类预测精度(字段“测量”)。顶部的结果集代表 60%的训练数据子集，而底部的结果集代表 40%的测试或维持数据子集。我还包括了每次运行的拟合决策向量<em class="kl"> w </em>和标量<em class="kl"> b </em>。所有模型的准确率都相似，但是<em class="kl"> w </em>和<em class="kl"> b </em>的拟合值却大相径庭。</p><div class="ko kp kq kr gt ab cb"><figure class="la ks lb lc ld le lf paragraph-image"><img src="../Images/2361e54f2a95d03aa16e78792ee8c989.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*HIdtCZBOqntvIO52cyoqWQ.png"/></figure><figure class="la ks lg lc ld le lf paragraph-image"><img src="../Images/110505f0e0a802dccf27ca61e5bbec49.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*EKsFjnrqa-AIWGhUjLd5fQ.png"/></figure></div><p id="bdd0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面的 6 个图显示了各种罚系数 c 值的拟合 SVM 超平面和(+1，-1)余量。上面的三个图显示了非标准化特征的拟合超平面，而下面的三个图显示了标准化特征的拟合超平面。</p><p id="3430" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不难看出，非标准化数据会产生对系数 c 高度敏感的决策超平面。前三个支持向量机中的每一个都会产生肉眼明显不同的决策超平面。标准化数据跨超参数产生更加一致的 SVM 超平面。底部图之间的差异在于，C 值越低，(+1，-1)边缘超平面的范围越宽。直观地，由于边缘超平面的“错误侧”上的数据点在低 C SVM 模型中受到的惩罚越少，因此模型逻辑具有更大的自由度来拟合远离决策超平面的边缘边界。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lh"><img src="../Images/632464b543fb16f6e37da865339d32bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yaiw1o17Iymb_9mFY_j5Yw.png"/></div></div></figure><div class="lm ln gp gr lo lp"><a href="https://github.com/dasotelo/Python_Projects/blob/master/SVM_College_Tuition.py" rel="noopener  ugc nofollow" target="_blank"><div class="lq ab fo"><div class="lr ab ls cl cj lt"><h2 class="bd ir gy z fp lu fr fs lv fu fw ip bi translated">达索特罗/SVM _ 学院 _ 学费. py</h2><div class="lw l"><p class="bd b dl z fp lu fr fs lv fu fw dk translated">github.com</p></div></div><div class="lx l"><div class="ly l lz ma mb lx mc kt lp"/></div></div></a></div></div></div>    
</body>
</html>