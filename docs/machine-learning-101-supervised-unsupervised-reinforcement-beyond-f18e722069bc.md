# 机器学习 101 |监督、非监督、强化和超越

> 原文：<https://towardsdatascience.com/machine-learning-101-supervised-unsupervised-reinforcement-beyond-f18e722069bc?source=collection_archive---------1----------------------->

![](img/b4a9fcd2fd299414eb7ff13d7489beb9.png)

机器学习包含数据科学家和其他专业人员使用的大量想法、工具和技术。

让我们来看看其中的一些概念，以及如何用它们来解决问题。

# 监督机器学习的问题和解决方案

最简单的任务属于**监督学习**的范畴。在监督学习中，我们可以访问正确的输入输出对的示例，并在训练阶段展示给机器。手写识别的常见示例通常被视为监督学习任务。我们向计算机显示一些手写数字的图像以及这些数字的正确标签，计算机学习将图像与标签相关联的模式。

通过明确的例子，学习如何以这种方式执行任务，相对容易理解，也容易实现，但有一项至关重要的任务:只有当我们能够访问正确的输入输出对的数据集时，我们才能完成这项任务。在手写的例子中，这意味着在某些时候我们需要派人去对训练集中的图像进行分类。这是一项费力的工作，通常不可行，但在数据确实存在的地方，监督学习算法在广泛的任务中非常有效。

监督机器学习任务可以大致分为两个子组:**回归**和**分类**。回归是估计或预测连续量的问题。一个月后，S & P 500 的价值会是多少？孩子成年后会有多高？今年我们有多少客户会转向竞争对手？这些是归入回归范畴的问题的例子。为了在有监督的机器学习框架中解决这些问题，我们将收集过去处理相同问题的“正确答案”输入/输出对的例子。对于输入，我们将确定我们认为能够预测我们希望预测的结果的**特征**。

对于第一个问题，我们可以尝试收集给定日期标准普尔 500 下股票的历史价格以及一个月后标准普尔 500 的价值作为特性。这将形成我们的训练集，机器将尝试从中确定特征和最终标准普尔 500 值之间的某种函数关系。

**分类**处理将观察值分配到离散的类别，而不是估计连续的数量。在最简单的情况下，有两种可能的类别；这种情况被称为**二元分类**。许多重要的问题都可以用二元分类法来描述。某个客户会离开我们去找竞争对手吗？给定的患者是否患有癌症？给定的图像包含热狗吗？用于执行二进制分类的算法特别重要，因为许多用于执行更一般类型的分类(其中存在任意标签)的算法仅仅是一组一起工作的二进制分类器。例如，手写识别问题的简单解决方案是简单地训练一组二进制分类器:0-检测器、1-检测器、2-检测器等等，它们输出图像是它们各自数字的确定性。分类器仅输出其分类器具有最高确定性的数字。

# 无监督机器学习

另一方面，有一类完全不同的任务被称为**无监督学习**。监督学习任务找到我们有一个“正确答案”数据集可以学习的模式。无监督的学习任务可以找到我们找不到的模式。这可能是因为“正确的答案”是不可观察的，或不可行的，或者对于一个给定的问题，甚至没有一个“正确的答案”本身。

无监督任务的一大子类就是**聚类**的问题。聚类指的是将观察结果分组在一起，使得同一组的成员彼此相似，而与其他组的成员不同。这里的一个常见应用是在市场营销中，我们希望识别具有相似偏好或购买习惯的客户或潜在客户的细分市场。集群的一个主要挑战是，通常很难或者不可能知道应该存在多少个集群，或者集群应该是什么样子。

![](img/f33139343ebedf424ed497ab98e812ca.png)

*Source:* [*https://arxiv.org/abs/1511.06434*](https://arxiv.org/abs/1511.06434)

一类非常有趣的无监督任务是**生成模型**。生成模型是模拟生成训练数据的过程的模型。一个好的生成模型将能够生成在某种意义上类似于训练数据的新数据。这种类型的学习是无监督的，因为生成数据的*过程*是不可直接观察的——只有数据本身是可观察的。

这一领域的最新发展导致了图像生成方面惊人的、有时甚至是可怕的进步。这里的图像是通过训练一种称为深度卷积广义对抗网络模型的无监督学习模型来生成面部图像，并向其请求微笑男子的图像而创建的。

# 强化学习、混合和超越

一种新型的学习问题最近引起了广泛关注，它被称为**强化学习**。在强化学习中，我们不为机器提供正确的输入输出对的例子，但我们确实以*奖励信号*的形式为机器提供了量化其性能的方法。强化学习方法类似于人类和动物的学习方式:机器尝试一系列不同的事情，当它做得好的时候就会得到奖励。

强化学习在解空间巨大或无限的情况下是有用的，并且通常应用于机器可以被认为是与其环境交互的代理的情况。这种模型的第一个大成功故事是由一个小团队完成的，[训练了一个强化学习模型，只使用游戏的像素输出作为输入来玩雅达利视频游戏](https://arxiv.org/abs/1312.5602)。该模型最终能够在三场游戏中超越人类玩家，而创建该模型的公司[不久后被谷歌以超过 5 亿美元](https://techcrunch.com/2014/01/26/google-deepmind/)收购。

为了对玩 Atari 视频游戏的问题实施监督学习，我们需要一个数据集，该数据集包含数百万或数十亿个由真人玩的示例游戏，以供机器学习。相比之下，强化学习的工作原理是根据机器在任务中的表现给予奖励。简单的视频游戏非常适合这种类型的任务，因为分数可以作为一种奖励。机器继续通过模拟来学习哪种模式能最大化它的回报。

通常，一些或所有这些不同领域之间的混合方法会产生好的结果。例如，某些领域的一项重要任务是**异常检测**的任务。异常检测算法监控一些信号，并指示何时发生了奇怪的事情。欺诈检测就是一个很好的例子。我们需要一种算法来监控信用卡交易流，并标记出奇怪的交易。但是怪异是什么意思呢？这个问题适合于一种监督/无监督的混合方法。当然，我们希望算法能够检测到一些已知的模式，我们可以通过向监督学习模型显示已知欺诈模式的示例来训练它。但我们也希望能够检测出以前未知的潜在欺诈或其他异常活动的例子，这可能通过无监督学习的方法来实现。

# 你可以用机器学习的基础知识走很长的路

许多最先进的工具需要大量复杂的知识，包括高等数学、统计学和软件工程。对于一个想要开始的初学者来说，这可能看起来势不可挡，特别是如果你想与一些令人兴奋的新型模型合作，这些模型会产生令人毛骨悚然的微笑男子图像或驾驶自动驾驶汽车。

好消息是，你可以用基础的东西做很多事情，这些东西很容易得到。在 R 和 Python 中实现了各种监督和非监督学习模型，这些模型可以免费获得，并且可以在您自己的计算机上直接设置，甚至像线性或逻辑回归这样的简单模型也可以用于执行有趣和重要的机器学习任务。

***有兴趣了解更多机器学习背后的科学吗？*** *了解更多关于我们的* [*数据科学*](http://www.brainstation.io/course/data-science) *课程。*

这篇文章最初出现在 [BrainStation 博客](https://blog.brainstation.io/machine-learning-supervised-unsupervised-reinforcement/)上。