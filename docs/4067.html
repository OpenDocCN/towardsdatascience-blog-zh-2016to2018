<html>
<head>
<title>3 silver bullets of word embeddings in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中单词嵌入的三大法宝</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a?source=collection_archive---------1-----------------------#2018-07-15">https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a?source=collection_archive---------1-----------------------#2018-07-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/7207e662a23de1546533a32a6a9c6c99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7D26_3Oc5hwZpKei.jpg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Credit: <a class="ae kf" href="https://pixabay.com/en/books-stack-book-store-1163695/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/en/books-stack-book-store-1163695/</a></figcaption></figure><p id="fb48" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单词嵌入是解决许多自然语言处理问题的银弹。现代的 NLP 架构大多采用了单词嵌入和放弃<a class="ae kf" rel="noopener" target="_blank" href="/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016">单词包(BoW) </a>、<a class="ae kf" rel="noopener" target="_blank" href="/combing-lda-and-word-embeddings-for-topic-modeling-fe4a1315a5b4">潜在狄利克雷分配(LDA)、潜在语义分析(LSA) </a>等。</p><p id="f780" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看完这篇文章，你会知道:</p><ul class=""><li id="e5ce" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">单词嵌入的历史</li><li id="35e6" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">文字嵌入设计</li><li id="ac86" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">应用现成的单词嵌入模型</li><li id="41c8" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">嵌入可视化</li><li id="ce57" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">拿走</li></ul><h1 id="3e88" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">单词嵌入的历史</h1><p id="3fc8" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">传统上，我们使用词袋来表示特征(例如 TF-IDF 或计数矢量化)。除了 BoW，我们可以在单词特征上应用 LDA 或 LSA。然而，它们也有一些局限性，如高维向量、稀疏特征等。单词嵌入是低维向量中的一个稠密特征。在大多数自然语言处理问题中，单词嵌入提供了更好的向量特征。</p><p id="0849" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2013 年，Mikolov 等人使单词嵌入流行起来。最终，单词嵌入是 NLP 中最先进的技术。他发布了 word2vec 工具包，并允许我们享受美妙的预训练模型。后来，gensim 提供了一个惊人的包装器，以便我们可以采用不同的预训练的单词嵌入模型，包括 Word2Vec(由谷歌)，GloVe(由斯坦福)，fastText(由脸书)。</p><p id="d9f8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 Tomas 等人推出 Word2Vec 的 12 年前，Bengio 等人发表了一篇论文[1]来处理语言建模，这是单词嵌入的最初想法。当时，他们把这个过程命名为“学习单词的分布式表示”。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/8a384b0a83eebfb2d85a65a1be78bc50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*FZVMHwCLO3fFo7FvMyA94Q.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Capture from A Neural Probabilistic Language Model [2] (Benigo et al, 2003)</figcaption></figure><p id="f758" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2008 年，Ronan 和 Jason [3]引入了预训练嵌入的概念，并表明这是一种解决 NLP 问题的惊人方法。直到 2013 年，Tomas 发布了预训练模型(word 2 vec ), word embedding 才开始出名。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/22a53a037243907fd3b15f9a62f3107b.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*D6A44ZN5_zwTyuCAODM0fA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Capture from A Unified Architecture for Natural Language Processing [3] (Collobert &amp; Weston, 2008)</figcaption></figure><p id="5813" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">时间线:</p><ul class=""><li id="ee62" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">2001 年:Bengio 等人提出了单词嵌入的概念</li><li id="7824" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">2008 年:罗南和杰森引入了预训练模型的概念</li><li id="3bba" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">2013 年:Mikolov 等人发布了预训练模型 Word2Vec</li></ul><h1 id="7583" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">文字嵌入设计</h1><p id="632c" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated"><strong class="ki iu"> <em class="nb">低维</em> </strong></p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/1a2188736a67173cf36bb9dac6b3cf04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/0*Yx5aM1S_h6WnpQE8.jpeg"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo Credit: <a class="ae kf" href="https://www.foodnetwork.com/recipes/food-network-kitchen/four-layer-birthday-cake-3363221" rel="noopener ugc nofollow" target="_blank">https://www.foodnetwork.com/recipes/food-network-kitchen/four-layer-birthday-cake-3363221</a></figcaption></figure><p id="a4b7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了解决高维问题，单词嵌入使用预定义的向量空间如 300 来表示每个单词。出于演示的目的，我使用三维来表示以下单词:</p><ul class=""><li id="bca5" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">苹果:[1.11，2.24，7.88]</li><li id="0ee6" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">橙色:[1.01，2.04，7.22]</li><li id="0908" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">汽车:[8.41，2.34，-1.28]</li><li id="fa68" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">表格:[-1.41，7.34，3.01]</li></ul><p id="4489" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如预定义的向量空间(即，在上面的演示中为 3)，维度(或特征)的数量是固定的，无论语料库有多大。与 BoW 相比，当唯一字增加时，维数会增加。假设我们的文档中有 10k 个唯一的单词，BoW 中的特征数量是 10k(没有过滤高/低频单词)，而在我们的演示中维度可以保持为 3。</p><p id="021d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nb">语义关系</em> </strong></p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="ab gu cl nd"><img src="../Images/3861b61f9a0f6a3d175c7e77b55f6cf1.png" data-original-src="https://miro.medium.com/v2/format:webp/1*oF1QyMamN5jXCXfffSRrqA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo Credit: <a class="ae kf" href="https://gointothestory.blcklst.com/similar-but-different-c722f39d923d" rel="noopener ugc nofollow" target="_blank">https://gointothestory.blcklst.com/similar-but-different-c722f39d923d</a></figcaption></figure><p id="7192" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常，单词向量编码单词之间的语义关系。这是单词嵌入的一个非常重要的概念，因为它有利于处理自然语言处理问题。词向量如果有相似的意思就会闭合。比如买和买会更近。与 BoW 不同，它只代表 0 或 1(表示有一个词或没有接近)，它不能代表两个词是否有相似的意思。</p><p id="d12b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的例子中，你可能会注意到苹果的向量和橘子的向量比其他的更接近，而苹果的向量相对远离汽车的向量。</p><p id="bbec" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nb">【连续词袋】&amp;</em></strong></p><p id="ee5b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Mikolov 等人提出了两种新的架构[4],这两种架构降低了计算复杂度并包含了额外的上下文。</p><p id="a2f5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">CBOW 就是在目标词(<strong class="ki iu"> w </strong>)前后同时使用<strong class="ki iu"> n </strong>个词。例如，“词向量编码词之间的语义关系”。如果窗口(n)是 3，这里是预测列表的子集:</p><ul class=""><li id="d6a8" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">情况 1，单词前:{Empty}，单词后:(单词，向量，编码)，预测单词:“the”</li><li id="6c24" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">案例 2，词前:(the)，词后:(向量，编码语义)，预测词:“词”</li></ul><p id="7bb7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Skip-gram 使用相反的方法，使用目标单词来预测目标单词前后的 n 个单词。例如，“词向量编码词之间的语义关系”。如果窗口(n)是 3，这里是预测列表的子集:</p><ul class=""><li id="a9de" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">案例 1，预测单词:“the”，单词:(单词，向量，编码)</li><li id="075e" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">案例 2，预测单词:“单词”，单词:(向量，编码，语义)</li></ul><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ne"><img src="../Images/c820d977791c02ff5df75c397b10c86b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QwiTOcVmwesADjQ3zMvSjA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Capture from Efficient Estimation of Word Representations in Vector Space (Tomas et al., 2013)</figcaption></figure><p id="f5f7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nb">阴性采样</em> </strong></p><p id="1143" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">而不是利用所有其他单词作为负面标签训练记录。Mikolov 等人提出使用合适的少量负训练记录来训练模型。从而使整个操作变得更快。</p><p id="6fee" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你不熟悉负抽样，你可以查看这篇文章了解更多信息。</p><h1 id="deb0" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">应用现成的单词嵌入模型</h1><p id="7c7e" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">介绍了历史和模型结构，我们如何使用单词嵌入来解决自然语言处理问题？</p><p id="d7ad" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有两种处理单词嵌入的方法:</p><ul class=""><li id="7649" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">利用现成的模型</li><li id="7cf2" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">构建特定领域的模型。</li></ul><p id="3c70" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文将采用第一种方法。选择 3 个知名的预训练模型，并利用 gensim 加载这些模型。著名的 NLP 库 Gensim 已经实现了处理这 3 种模型的接口。为了简单起见，你可以用我的包装器来加载这三个模型。</p><p id="adf4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nb"> Word2Vec </em> </strong></p><p id="ff8c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个<a class="ae kf" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank">单词向量</a>是在谷歌新闻上训练的，由谷歌提供。基于来自谷歌新闻数据的 1000 亿个单词，他们训练了 300 个维度的模型。</p><p id="6553" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Mikolov 等人使用 skip-gram 和负采样建立了这个模型，该模型于 2013 年发布。</p><pre class="mw mx my mz gt nf ng nh ni aw nj bi"><span id="4949" class="nk lt it ng b gy nl nm l nn no">word_embedding = WordEmbedding()<br/>word_embedding.load(source='word2vec', file_path=word2vec_file_path)</span><span id="fbb1" class="nk lt it ng b gy np nm l nn no">print(word_embedding.get_vector(source='word2vec', word='apple'))</span></pre><p id="71d7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出</p><pre class="mw mx my mz gt nf ng nh ni aw nj bi"><span id="e44e" class="nk lt it ng b gy nl nm l nn no">[-0.06445312 -0.16015625 -0.01208496  0.13476562 -0.22949219  0.16210938<br/>  0.3046875  -0.1796875  -0.12109375  0.25390625 -0.01428223 -0.06396484<br/>...]</span></pre><p id="18a2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nb">手套</em> </strong></p><p id="cc1c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">斯坦福大学 NLP 团队提供了单词表示的全局向量(<a class="ae kf" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe </a>)。斯坦福大学基于 2，6，42，840 亿个令牌提供了从 25，50，100，200 到 300 个维度的各种模型。</p><p id="c21a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">斯坦福大学自然语言处理小组应用词-词共现概率来构建嵌入。换句话说，如果两个词同时出现很多次，这两个词可能有相似的意思，所以矩阵会更接近。</p><pre class="mw mx my mz gt nf ng nh ni aw nj bi"><span id="a32a" class="nk lt it ng b gy nl nm l nn no">word_embedding = WordEmbedding()<br/>word_embedding.load(source='glove', file_path=glove_file_path)</span><span id="b2a6" class="nk lt it ng b gy np nm l nn no">print(word_embedding.get_vector(source='glove', word='apple'))</span></pre><p id="91c9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出</p><pre class="mw mx my mz gt nf ng nh ni aw nj bi"><span id="0372" class="nk lt it ng b gy nl nm l nn no">[ 0.52042001 -0.83139998  0.49961001  1.28929996  0.1151      0.057521<br/> -1.37530005 -0.97312999  0.18346     0.47672001 -0.15112001  0.35532001<br/>...]</span></pre><p id="2dc3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">快速文本</p><p id="7e2f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://fasttext.cc/" rel="noopener ugc nofollow" target="_blank"> fastText </a>由脸书发布，提供 3 款 300 尺寸。用子词训练预训练模型中的一个。比如“差”，它会被“di”、“dif”、“diff”等等训练。</p><pre class="mw mx my mz gt nf ng nh ni aw nj bi"><span id="43bf" class="nk lt it ng b gy nl nm l nn no">word_embedding = WordEmbedding()<br/>word_embedding.load(source='fasttext', file_path=fasttext_file_path)</span><span id="68e0" class="nk lt it ng b gy np nm l nn no">print(word_embedding.get_vector(source='fasttext', word='apple'))</span></pre><p id="826b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出</p><pre class="mw mx my mz gt nf ng nh ni aw nj bi"><span id="8591" class="nk lt it ng b gy nl nm l nn no">[ 0.26407328  0.30484504  0.04900438 -0.44377801  0.16519009 -0.09473443<br/> -0.01351437 -0.17237368  0.0374852   0.34962645  0.14334701 -0.11134619<br/>...]</span></pre><h1 id="1caa" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">嵌入可视化</h1><p id="dae4" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">最先进的自然语言处理技术之一是单词嵌入，它实际上是什么？这是一个矩阵，最简单的方法是 x 和 y 坐标，但我们有 300 维，而不是 2 维。</p><p id="1a79" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过使用主成分分析(PCA)或 T-分布随机邻居嵌入(t-SNE)来可视化它。通过利用 TensorBoard，可以轻松呈现可视化。</p><pre class="mw mx my mz gt nf ng nh ni aw nj bi"><span id="3298" class="nk lt it ng b gy nl nm l nn no">word_embedding.build_visual_metadata(embedding=embedding, words=words, file_dir='./word_embedding')</span></pre><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nq"><img src="../Images/cf5c116c8dddcc8aced3ee3272c80400.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*glwOAs3oK5IOOegT9ru7sw.png"/></div></div></figure><h1 id="c2e6" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">拿走</h1><p id="3e0a" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">要访问所有代码，可以访问我的 github repo。</p><ul class=""><li id="4ac8" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">应该使用哪种现成模型？根据您的数据，<strong class="ki iu">有可能所有这些数据对您的领域特定数据</strong>都没有用。</li><li id="f861" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">我们应该根据你的数据训练单词嵌入层吗？根据我的经验，如果您处理<strong class="ki iu">领域特定的文本，并且您的大部分单词不能从现成的模型</strong>中找到，您可以考虑构建定制的单词嵌入层。</li><li id="3b40" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">由于浏览器资源问题，Tensorboard 选择了前 100000 个向量。推荐给<strong class="ki iu">自己挑一小部分矢量</strong>。</li><li id="6e3b" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">GloVe、Word2Vec 和 fasttext 的最大型号大小分别为~5.5GB、~3.5GB 和~8.2GB。GloVe、Word2Vec、fasttext 分别需要 9、1、9 分钟左右。在资源有限的情况下，It <strong class="ki iu">可能不容易部署到生产</strong>。</li></ul><h1 id="215e" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">参考</h1><p id="9ca0" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">[1] Yoshua Bengio，Ducharme Rejean 和 Vincent Pascal。一种神经概率语言模型。2001.<a class="ae kf" href="https://papers.nips.cc/paper/1839-a-neural-probabilistic-language-model.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/1839-a-neural-probability-language-model . pdf</a></p><p id="50b0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2] Yoshua Bengio，Ducharme Rejean，Vincent Pascal 和 Janvin Christian。一种神经概率语言模型。2003 年 3 月。http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf<a class="ae kf" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="088a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]科洛伯特·罗南和韦斯顿·杰森。自然语言处理的统一架构:具有多任务学习的深度神经网络。2008.【https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf T4】</p><p id="0a72" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[4]托马斯·米科洛夫、格雷戈·科拉多、程凯和杰弗里·迪恩。向量空间中单词表示的有效估计。2013 年 9 月。<a class="ae kf" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1301.3781.pdf</a></p><h1 id="09c0" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">关于我</h1><p id="dce4" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以通过<a class="ae kf" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae kf" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae kf" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p></div></div>    
</body>
</html>