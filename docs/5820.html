<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://towardsdatascience.com/a-new-hyperbolic-tangent-based-activation-function-for-neural-networks-2aaa20087d7c?source=collection_archive---------21-----------------------#2018-11-10">https://towardsdatascience.com/a-new-hyperbolic-tangent-based-activation-function-for-neural-networks-2aaa20087d7c?source=collection_archive---------21-----------------------#2018-11-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><p id="4bc7" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">一种新的基于双曲正切的神经网络激活函数</p></div><div class="ab cl jo jp hu jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="ij ik il im in"><p id="7330" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">摘要:</strong></p><p id="89b2" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">在这篇文章中，我为神经网络引入了一种新的基于双曲正切的激活函数，<em class="jv">正切线性单元(TaLU)，</em>。使用 CIFAR-10 和 CIFAR-100 数据库评估了该函数的性能。所提出的激活函数的性能相当于或优于其他激活函数，例如:<em class="jv">【标准整流线性单元(ReLU)】</em><em class="jv">【泄漏整流线性单元(Leaky ReLU)】</em>和指数线性单元(eLU)。</p><p id="213a" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">简介:</strong></p><p id="f862" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">校正线性单元(ReLU)(Nair V .和 Hinton，G. E .,“校正线性单元改善受限玻尔兹曼机器”，<em class="jv"> ICML </em>，2010，第 807–810 页)是最流行的非饱和激活函数之一，用于神经网络。然而，ReLU 遇到了<em class="jv">死亡 ReLU </em>的问题，其中一些神经元开始输出 0。有时，一半的神经元死亡，特别是如果它们以大的学习速率使用(Géron，a .，“用 Scikit-Learn &amp; TensorFlow 进行动手机器学习”，第 1 版。，<em class="jv">奥莱利媒体公司</em>，2017 年，第 281 页)。为了避免这些问题，徐等人(徐，b .，王，n .，陈，t .，李，m .，“卷积网络中整流激活的经验评估”，<em class="jv">aexiv 预印本 arXiv:1505.00853v2 </em>，2015)评估了 ReLU 的变体，例如<em class="jv">泄漏整流线性单元</em> (leaky ReLU)、<em class="jv">参数整流线性单元</em> (PReLU)和<em class="jv">随机整流线性单元</em> (RReLU)。据观察，泄漏 ReLU 的表现大多优于标准 ReLU。Clevert 等人(Clevert，d .，Unterthiner，t .，Hochreiter，s .，“通过指数线性单元进行快速准确的深度网络学习”，<em class="jv"> aerXiv 预印本 arXiv:1511.07289 </em>，2015)提出了指数线性单元(ELU)，据观察，该单元优于 ReLU 的所有变体(Géron，a .，“使用 Scikit-Learn &amp; TensorFlow 进行机器学习”，第 1 版。，<em class="jv">奥莱利媒体公司</em>，2017 年，第 282 页)。</p><p id="0348" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">我在双曲正切函数的基础上提出了一种新的激活函数<em class="jv">【TaLU】</em>。下面讨论的是 TaLU 的定义，随后是它的优化参数的确定以及它与其他激活函数如 ReLU、leaky ReLU 和 eLU 的比较。</p><p id="e367" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">切线线性单位(TaLU): </strong></p><p id="fb00" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">切线线性单位(TaLu)可以在<strong class="ir jn">图 1 </strong>中说明，可以用下面的<strong class="ir jn">等式描述。</strong></p><figure class="jx jy jz ka gt kb gh gi paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="gh gi jw"><img src="../Images/c6722e640b3445e81d90b3d4a219330e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YmmSd40fgluD3dkppmQAew.png"/></div></div><figcaption class="ki kj gj gh gi kk kl bd b be z dk">(Image by author)</figcaption></figure><p id="ed26" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">图 1: </strong>切线线性单元(TaLU)</p><figure class="jx jy jz ka gt kb gh gi paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="gh gi km"><img src="../Images/38873e86f091fa6fb8817adaa1dcf90d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*190fkpW3fiwy2vgWB7_szA.png"/></div></div></figure><p id="7bbd" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">其中α是固定参数，其值为&lt; 0. α was tested from -0.50 to -0.01 in this article.</p><p id="92a9" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">实验设置:</strong></p><p id="3af8" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">在 CIFAR-10 和 CIFAR-100 数据集上测试了所提出的激活函数，并与 ReLU、leaky ReLu (α = 0.01)和 ELU (α = 1)进行了性能比较。本研究中使用了基于 python 的库:Tensorflow、Numpy、Pandas 和 Keras 库。神经网络架构在<strong class="ir jn">图 2 </strong>中描述。从为 CIFAR-10 和 CIFAR-100 提供的训练数据集将数据以 9:1 的比例分为训练和验证，并在为 CIFAR-10 和 CIFAR-100 提供的测试数据集上测试该模型。</p><figure class="jx jy jz ka gt kb gh gi paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="gh gi kn"><img src="../Images/5787b62f16e8cff27740025ad44953e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rJQhR-vQvcnpuYvtwEep2g.png"/></div></div></figure><p id="e096" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">图 2: </strong>本次研究中使用的神经网络架构(注:基于实验，第 1、2、5、7、11 层的激活函数(af)为 Talu、ReLU、eLU 和 leaky ReLU)。</p><p id="ec76" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">代码可以在 https://github.com/mjain72/TaLuActivationFunction<a class="ae ko" href="https://github.com/mjain72/TaLuActivationFunction" rel="noopener ugc nofollow" target="_blank">找到</a></p><p id="d548" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">【TaLU 的参数研究:</p><p id="d218" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">最初，进行参数研究以确定α的最佳值。<strong class="ir jn">表 1 </strong>显示了 CIFAR-10 数据集的训练、验证和测试的损失和准确度值。在训练期间，每个时期的数量是 25。α的最佳值为-0.05。在 CIFAR-100 数据集上观察到类似的α最佳值(<strong class="ir jn">表 2 </strong>)。</p><p id="aef7" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">表 1: </strong>使用 CIFAR-10 数据集对 TaLU 进行的参数研究</p><figure class="jx jy jz ka gt kb gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/255cd7dd46427da7271ea84e0635cd08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*8DfE4U6ZEvR5aOzWQP1R8Q.png"/></div></figure><p id="5697" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">表 2: </strong>使用 CIFAR-100 数据集对 TaLU 进行参数研究</p><figure class="jx jy jz ka gt kb gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/8d0bc79910a01abf8f3d2fc9a33ecaea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*JH8QA4F8P1Pm6Gnd44RN4A.png"/></div></figure><p id="e131" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">基于这些研究，在与 ReLU、leaky ReLU 和 eLU 激活函数的比较研究中，决定对 TaLU 使用α = -0.05。</p><p id="c1dc" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">不同激活功能的 TaLU 对比研究:</strong></p><p id="0915" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">一旦确定α的最佳值为-0.05，在 TaLU 的情况下，我们使用该值与其他激活函数进行比较研究。<strong class="ir jn">表 3 </strong>显示了在 CIFAR-100 数据集的情况下，各种函数的性能。据观察，就准确性而言，TaLU 的性能优于其他激活功能。</p><p id="39e9" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">表 3:</strong>CIFAR-100 数据集各种激活函数的性能</p><figure class="jx jy jz ka gt kb gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/8ed515cdae0893c241930432e4acff23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*e9PxHHwLVghUlveQXor5Uw.png"/></div></figure><p id="a12d" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">表 4 </strong>显示了在 CIFAR-10 数据集的情况下，各种激活函数的性能。同样在这种情况下，TaLU 的性能与其他激活功能相当或更好。</p><p id="39b7" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">表 4:</strong>CIFAR-10 数据集各种激活函数的性能</p><figure class="jx jy jz ka gt kb gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/7aab08611617daa6069a9412ca1caaa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*Cq8pYJC_KPr0LrOSnSXOTw.png"/></div></figure><p id="f227" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">结论:</strong></p><p id="4529" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">基于上述研究，可以得出结论，所提出的激活函数 TaLU 提供了比当前使用的激活函数(如 ReLU、leaky ReLU 和 eLU)更好或相似的性能，并且应该在未来的研究中进行评估。</p><p id="98ea" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">收敛曲线:</strong></p><figure class="jx jy jz ka gt kb gh gi paragraph-image"><div class="gh gi kq"><img src="../Images/383b3bc9847b356181a49b0f6bc750fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*6nU56t5bdIzjD8bSQRZ1Ow.png"/></div><figcaption class="ki kj gj gh gi kk kl bd b be z dk">(Image by author)</figcaption></figure><p id="7ced" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">图 3: </strong>不同α值下 TaLU 的收敛曲线，显示在括号中，使用 CIFAR-10 数据集。</p><figure class="jx jy jz ka gt kb gh gi paragraph-image"><div class="gh gi kq"><img src="../Images/2e98228bd6a672159ff28110f2e7eaba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*HBcGj8CBGhXB1YvtB-Ajzw.png"/></div><figcaption class="ki kj gj gh gi kk kl bd b be z dk">(Image by author)</figcaption></figure><p id="79ca" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">图 4: </strong>不同α值下 TaLU 的收敛曲线，显示在括号中，使用 CIFAR-100 数据集。</p><figure class="jx jy jz ka gt kb gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/acd9c1d7c90a131dc54b8e2eab27209c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*vBJPd5t5C8uwOmlzt8inRQ.png"/></div><figcaption class="ki kj gj gh gi kk kl bd b be z dk">(Image by author)</figcaption></figure><p id="3093" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">图 5: </strong>不同激活函数的收敛曲线，使用 CIFAR-10 数据集。</p><figure class="jx jy jz ka gt kb gh gi paragraph-image"><div class="gh gi ks"><img src="../Images/fcedd230c625afcec168db349abb7f4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*8-G8xDQ_a4NyK3gU3zYTFg.png"/></div><figcaption class="ki kj gj gh gi kk kl bd b be z dk">(Image by author)</figcaption></figure><p id="445e" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">图 6:</strong>CIFAR-100 数据集不同激活函数的收敛曲线。</p></div></div>    
</body>
</html>