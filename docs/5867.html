<html>
<head>
<title>SVM and Kernel SVM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SVM 和内核 SVM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/svm-and-kernel-svm-fed02bef1200?source=collection_archive---------3-----------------------#2018-11-13">https://towardsdatascience.com/svm-and-kernel-svm-fed02bef1200?source=collection_archive---------3-----------------------#2018-11-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/7ada5b4e446846ee23eca1f50562cfa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X0jUKCL--l2jfYvOo4TKQg.jpeg"/></div></div></figure><h1 id="3d01" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">摘要</h1><p id="8f5b" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在本文中，您将了解到<strong class="lb iu"> SVM </strong>或<strong class="lb iu">支持向量机</strong>，这是最流行的人工智能算法之一(它是十大人工智能算法之一)，以及关于<strong class="lb iu">内核技巧</strong>，它处理<strong class="lb iu">非线性</strong>和<strong class="lb iu">更高维度</strong>。我们将触及像<strong class="lb iu">超平面、拉格朗日乘数</strong>这样的主题，我们将有<strong class="lb iu">视觉示例</strong>和<strong class="lb iu">代码示例</strong>(类似于在<a class="ae lx" href="http://dummyprogramming.com/stupid-simple-ai-series/" rel="noopener ugc nofollow" target="_blank"> KNN 章节</a>中使用的代码示例)来更好地理解这个非常重要的算法。</p><h1 id="655c" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">SVM 解释道</h1><p id="3b0c" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="lb iu">支持向量机</strong>是一种<strong class="lb iu">监督学习算法</strong>，主要用于<strong class="lb iu">分类</strong>，但也可用于<strong class="lb iu">回归</strong>。主要思想是基于标记的数据(训练数据),该算法试图找到可用于分类新数据点的<strong class="lb iu">最佳超平面</strong>。在二维中，超平面是一条简单的直线。</p><p id="43ba" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated"><strong class="lb iu">通常</strong>学习算法试图学习一个类的<strong class="lb iu">最常见的特征(一个类与另一个类的区别)</strong>，并且分类是基于那些学习到的代表性特征(因此分类是基于类之间的差异)。SVM 的工作方式正好相反。它<strong class="lb iu">找到</strong>和<strong class="lb iu">类之间最相似的例子</strong>。这些将是<strong class="lb iu">支持向量</strong>。</p><p id="0a12" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">作为一个例子，让我们考虑两类，苹果和柠檬。</p><p id="e21c" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">其他算法将学习苹果和柠檬最明显、最有代表性的特征，比如苹果是绿色的、圆形的，而柠檬是黄色的、椭圆形的。</p><p id="ad9d" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">相比之下，SVM 将寻找与柠檬非常相似的苹果，例如黄色椭圆形的苹果。这将是一个支持向量。另一个支持向量将是一个类似于苹果的柠檬(绿色和圆形)。因此<strong class="lb iu">其他算法</strong>学习<strong class="lb iu">差异</strong>而<strong class="lb iu"> SVM </strong>学习<strong class="lb iu">相似性</strong>。</p><p id="3d8e" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">如果我们想象一下上面 2D 的例子，我们会得到这样的结果:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/88145989f584ca8056ceff4031264a8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eH4sQUkJc8YT76jL.png"/></div></div></figure><p id="982b" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">当我们从左到右，所有的例子将被归类为苹果，直到我们到达黄色的苹果。从这一点来看，新示例是苹果的置信度下降，而柠檬类的置信度增加。当柠檬类置信度变得大于苹果类置信度时，新示例将被分类为柠檬(介于黄色苹果和绿色柠檬之间)。</p><p id="a243" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">基于这些支持向量，该算法试图找到分隔类别的最佳超平面<strong class="lb iu">。在 2D，超平面是一条线，所以它看起来像这样:</strong></p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/3c65135ed79e25a410d1d73049b672a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6f4wnyOzFom45hh9.png"/></div></div></figure><p id="83a6" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">好的，但是<strong class="lb iu">为什么我把蓝色的边界画成上面图片</strong> <strong class="lb iu">的样子？</strong>我也可以画出这样的界限:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/d78bf6f44477afbd0c3152237f5714f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jfUOIfBTU6oRyaRv.png"/></div></div></figure><p id="70aa" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">如你所见，我们有<strong class="lb iu">无限的可能性来画出决策边界</strong>。那么如何才能找到最优的呢？</p><h1 id="ee05" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">寻找最优超平面</h1><p id="7050" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">直觉上，最佳线<strong class="lb iu">是远离苹果和柠檬示例</strong>的线<strong class="lb iu">(具有最大的余量)。为了获得最优解，我们必须<strong class="lb iu">以两种方式最大化利润</strong>(如果我们有多个类别，那么我们必须考虑每个类别来最大化利润)。</strong></p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/c98a49755e75fef0e2c0f5dfc8584498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VpDK7t9et977TSwG.png"/></div></div></figure><p id="f5d8" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">因此，如果我们将上面的图片与下面的图片进行比较，我们可以很容易地观察到，第一个是最优超平面(线)，第二个是次优解，因为边缘要短得多。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/4d25221e5898a582309e1c0a6df9786d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6jEF9zPYt7v650GQ.png"/></div></div></figure><p id="c7d1" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">因为我们想要最大化考虑到所有类<strong class="lb iu"/>的边距，而不是为每个类使用一个边距<strong class="lb iu">，我们使用考虑到所有类</strong>的“全局”边距、<strong class="lb iu">。该边距看起来像下图中的紫色线:</strong></p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/3aa0c10c7904d7a523193d2de0b0a637.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SgZeIjb1kKpKyk7I.png"/></div></div></figure><p id="fc3e" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">这个边距与边界的<strong class="lb iu">正交</strong>，与支持向量的<strong class="lb iu">等距</strong>。</p><p id="5397" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">那么向量在哪里呢？每个计算(计算距离和最佳超平面)都在<strong class="lb iu">矢量空间</strong>中进行，因此每个数据点都被视为一个矢量。空间<strong class="lb iu">的<strong class="lb iu">尺寸</strong>由示例的属性</strong>数量定义。为了理解背后的数学，请阅读这个简单的向量、超平面和优化的数学描述:<a class="ae lx" href="https://www.svm-tutorial.com/" rel="noopener ugc nofollow" target="_blank"> SVM 简洁地</a>。</p><p id="eb61" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">总之，<strong class="lb iu">支持向量</strong>是<strong class="lb iu">定义超平面</strong>的位置和边缘的数据点。我们称它们为<strong class="lb iu">“支持”向量</strong>，因为这些是类的代表性数据点，<strong class="lb iu">如果我们移动它们中的一个，位置和/或边距将改变</strong>。移动其他数据点不会影响超平面的边缘或位置。</p><p id="52fd" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">为了进行分类，我们不需要所有的训练数据点(像在 KNN 的情况下)，我们只需要保存支持向量。在最坏的情况下，所有的点都将是支持向量，但这是非常罕见的，如果发生这种情况，那么你应该检查你的模型的错误或缺陷。</p><p id="726c" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">所以基本上<strong class="lb iu">学习等同于寻找具有最佳余量的超平面</strong>，所以这是一个简单的<strong class="lb iu">优化问题</strong>。</p><h1 id="2ede" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">基本步骤</h1><p id="02f5" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">SVM 的基本步骤是:</p><ol class=""><li id="0af0" class="mi mj it lb b lc ly lg lz lk mk lo ml ls mm lw mn mo mp mq bi translated">选择<strong class="lb iu">两个超平面</strong>(在 2D)，这两个超平面将数据<strong class="lb iu">分开，它们之间没有点</strong>(红线)</li><li id="fe8b" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated"><strong class="lb iu">最大化他们的距离</strong>(边距)</li><li id="36fb" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated"><strong class="lb iu">平均线</strong>(这里是两条红线中间的线)将是<strong class="lb iu">决策边界</strong></li></ol><p id="5c1b" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">这很好也很容易，但是找到最佳余量，优化问题不是微不足道的(在 2D 很容易，当我们只有两个属性时，但是如果我们有 N 个维度，N 是一个非常大的数字呢)</p><p id="59c2" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">为了解决优化问题，我们使用<strong class="lb iu">拉格朗日乘数</strong>。为了理解这个技巧，你可以阅读下面两篇文章:<a class="ae lx" href="https://www.svm-tutorial.com/2016/09/duality-lagrange-multipliers/" rel="noopener ugc nofollow" target="_blank">二元 Langrange 乘数</a>和<a class="ae lx" href="https://medium.com/@andrew.chamberlain/a-simple-explanation-of-why-lagrange-multipliers-works-253e2cdcbf74" rel="noopener">一个关于 Langrange 乘数为什么工作的简单解释</a>。</p><p id="9d4a" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">到目前为止，我们有线性可分的数据，所以我们可以使用一条线作为类边界。但是如果我们不得不处理非线性数据集呢？</p><h1 id="71ab" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">非线性数据集的 SVM</h1><p id="2107" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">非线性数据的一个例子是:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/e85abfb95383305090dea9645bc890e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3jWNwLMhrhazDmdg.png"/></div></div></figure><p id="e518" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">在这种情况下，我们找不到一条直线来区分苹果和柠檬。那么如何才能解决这个问题呢。我们将使用<strong class="lb iu">内核技巧！</strong></p><p id="2c0c" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">基本思想是，当一个数据集在当前维度上不可分时，<strong class="lb iu">添加另一个维度</strong>，也许这样数据将是可分的。想一想，上面的例子是在 2D，它是不可分的，但也许在 3D 中苹果和柠檬之间有一个间隙，也许有一个等级差异，所以柠檬在第一层，苹果在第二层。在这种情况下，我们可以很容易地在 level 1 和 level 2 之间绘制一个分离超平面(在 3D 中，超平面是一个平面)。</p><h1 id="a639" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">映射到更高维度</h1><p id="d287" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">为了解决这个问题，我们<strong class="lb iu">不应该只是盲目地增加另一个维度</strong>，我们应该转换空间，以便我们有意识地产生这种层次差异。</p><h1 id="b705" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">从 2D 到 3D 的映射</h1><p id="cf8c" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">让我们假设我们增加了另一个叫做<strong class="lb iu"> X3 </strong>的维度。另一个重要的转变是，在新的维度中，使用公式<strong class="lb iu"> x1 + x2 </strong>来组织点。</p><p id="61a7" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">如果我们画出由<strong class="lb iu"> x + y </strong>公式定义的平面，我们会得到这样的结果:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/c620371146a70df7f8e0e6785ad2c71c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/0*4tGRdSHgOoKZoQAT.png"/></div></figure><p id="ab5b" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">现在我们必须将苹果和柠檬(只是简单的点)映射到这个新空间。仔细想想，我们做了什么？我们刚刚使用了一个变换，其中<strong class="lb iu">我们基于距离</strong>添加了级别。如果你在原点，那么这些点将在最低层。当我们远离原点时，这意味着我们正在<strong class="lb iu">爬山</strong>(从平面的中心向边缘移动)，因此点的高度会更高。现在如果我们考虑原点是中心的柠檬，我们会得到这样的东西:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/5e490938763fcf0e82c126241c0a4617.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GKpdwmJ-YoZVWqeS.png"/></div></div></figure><p id="77ae" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">现在我们可以很容易地将这两个类分开。这些转换被称为<strong class="lb iu">内核</strong>。常见的核有:<strong class="lb iu">多项式核、高斯核、径向基函数(RBF)、拉普拉斯 RBF 核、Sigmoid 核、Anove RBF 核</strong>等(参见<a class="ae lx" href="https://data-flair.training/blogs/svm-kernel-functions/" rel="noopener ugc nofollow" target="_blank">核函数</a>或更详细的描述<a class="ae lx" href="https://mlkernels.readthedocs.io/en/latest/kernels.html" rel="noopener ugc nofollow" target="_blank">机器学习核</a>)。</p><h1 id="f4aa" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">从 1D 到 2D 的测绘</h1><p id="0c2f" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">另一个在 2D 更容易的例子是:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/563b38b0697621019565e9e5998c29e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fMVjaxA1buUoH62L.png"/></div></div></figure><p id="4c41" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">在使用了内核和所有的转换之后，我们将得到:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/0e9a03aa8584b3c8bbeb852666cd0ac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gpNYO_ZXmrt_Hjxg.png"/></div></div></figure><p id="47d7" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">所以在转换之后，我们可以很容易地用一行代码来划分这两个类。</p><p id="37f3" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">在现实生活中，我们不会有一条简单的直线，但我们会有很多曲线和高维数。在某些情况下，我们不会有两个超平面来分隔数据，它们之间没有点，所以<strong class="lb iu">我们需要一些权衡，对异常值的容忍</strong>。幸运的是，SVM 算法有一个所谓的<strong class="lb iu">正则化参数</strong>来配置权衡并容忍异常值。</p><h1 id="246f" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">调谐参数</h1><p id="7246" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">正如我们在上一节<strong class="lb iu">中看到的，选择正确的内核至关重要</strong>，因为如果转换不正确，那么模型的结果会非常糟糕。根据经验，<strong class="lb iu">总是检查你是否有线性数据</strong>，在这种情况下<strong class="lb iu">总是使用线性 SVM </strong>(线性内核)。<strong class="lb iu">线性 SVM 是一个参数模型</strong>，但<strong class="lb iu"> RBF 核 SVM 不是</strong>，因此后者的复杂性随着训练集的大小而增长。不仅<strong class="lb iu">训练一个 RBF 核 SVM </strong>更昂贵，而且你还必须<strong class="lb iu">保持核矩阵在</strong>周围，并且<strong class="lb iu">投影</strong> <strong class="lb iu">到</strong>这个“无限的”<strong class="lb iu">高维空间</strong>中，在预测过程中，数据变成线性可分的<strong class="lb iu">也更昂贵</strong>。此外，您有<strong class="lb iu">更多的超参数来调整</strong>，因此模型选择也更加昂贵！最后，对复杂的模型进行过度拟合要容易得多！</p><h1 id="ed87" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">正规化</h1><p id="d879" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="lb iu">正则化参数</strong>(python 中的<strong class="lb iu">称之为</strong> <strong class="lb iu"> C </strong>)告诉 SVM 优化<strong class="lb iu">你想要避免多少漏分类</strong>每个训练示例。</p><p id="5ba8" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">如果<strong class="lb iu"> C 比</strong> <strong class="lb iu">高</strong>，那么优化会选择<strong class="lb iu">更小的</strong>超平面，所以训练数据<strong class="lb iu">的漏检率会更低</strong>。</p><p id="fcf8" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">另一方面，如果<strong class="lb iu"> C 比</strong> <strong class="lb iu">低</strong>，那么<strong class="lb iu">的裕量将会大</strong>，即使有<strong class="lb iu">将会漏分类</strong>训练数据的例子。这显示在以下两个图表中:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/9f65b519fa598661889bedd64fcbab58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rvt2H-wO55hKjJ5Y.png"/></div></div></figure><p id="f42d" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">正如你在图中看到的，当 C 较低时，即使两个苹果被归类为柠檬，边距也较高(因此我们没有太多曲线，直线也没有严格遵循数据点)。当 C 高时，边界充满曲线，并且所有训练数据被正确分类。<strong class="lb iu">不要忘记</strong>，即使所有的训练数据都被正确分类，这并不意味着增加 C 将总是增加精度(因为过拟合)。</p><h1 id="20c6" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">微克</h1><p id="ce37" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">下一个重要参数是<strong class="lb iu">伽马</strong>。gamma 参数定义了<strong class="lb iu">单个训练示例的影响达到</strong>的程度。这意味着<strong class="lb iu">高伽玛</strong>将只考虑靠近可信超平面的点<strong class="lb iu">，而<strong class="lb iu">低伽玛</strong> <strong class="lb iu">将考虑距离</strong>较远的点<strong class="lb iu">。</strong></strong></p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/d29106936e5e1fae203cf63e0614a641.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*P5cqyr_n84SQDuAN.png"/></div></div></figure><p id="41ee" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">正如您所看到的，减少 Gamma 将导致寻找正确的超平面将考虑距离更远的点，因此将使用越来越多的点(绿线表示在寻找最佳超平面时考虑了哪些点)。</p><h1 id="1755" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">边缘</h1><p id="6cc7" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">最后一个参数是<strong class="lb iu">余量</strong>。我们已经讨论过利润率，<strong class="lb iu">更高的利润率产生更好的模型</strong>，因此更好的分类(或预测)。页边距应始终最大化<strong class="lb iu"/>。</p><h1 id="66b0" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">使用 Python 的 SVM 示例</h1><p id="c9b0" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在本例中，我们将使用 Social_Networks_Ads.csv 文件，该文件与我们在上一篇文章中使用的文件相同，参见<a class="ae lx" href="http://dummyprogramming.com/stupid-simple-ai-series-knn/#knn-code-example" rel="noopener ugc nofollow" target="_blank"> KNN 使用 Python 的示例</a>。</p><p id="4660" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">在这个例子中，我将只写下 SVM 和 KNN 之间的差异，因为我不想在每篇文章中重复我自己！如果你想要<strong class="lb iu">完整的解释</strong>关于我们如何读取数据集，我们如何解析和分割我们的数据，或者我们如何评估或绘制决策边界，那么请<strong class="lb iu">阅读上一章的代码示例</strong> ( <a class="ae lx" href="http://dummyprogramming.com/stupid-simple-ai-series-knn/#knn-code-example" rel="noopener ugc nofollow" target="_blank"> KNN </a>)！</p><p id="a7b0" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">因为<strong class="lb iu"> sklearn </strong>库是一个写得非常好、非常有用的 Python 库，所以我们没有太多代码需要修改。唯一的区别是我们必须从<strong class="lb iu"> sklearn.svm </strong>中导入<strong class="lb iu"> SVC </strong>类(在 sklearn 中 SVC = SVM ),而不是从 sklearn.neighbors 中导入 KNeighborsClassifier 类</p><pre class="me mf mg mh gt mx my mz na aw nb bi"><span id="5b10" class="nc kc it my b gy nd ne l nf ng"># Fitting SVM to the Training set<br/>from sklearn.svm import SVC<br/>classifier = SVC(kernel = 'rbf', C = 0.1, gamma = 0.1)<br/>classifier.fit(X_train, y_train)</span></pre><p id="e701" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">导入 SVC 后，我们可以使用预定义的构造函数创建新模型。这个构造函数有很多参数，但我将只描述最重要的参数，大多数时候你不会用到其他参数。</p><p id="3765" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">最重要的参数是:</p><ol class=""><li id="9907" class="mi mj it lb b lc ly lg lz lk mk lo ml ls mm lw mn mo mp mq bi translated"><strong class="lb iu">内核:</strong>要使用的内核类型。最常见的内核是<strong class="lb iu"> rbf </strong>(这是默认值)、<strong class="lb iu"> poly </strong>或者<strong class="lb iu"> sigmoid </strong>，但是你也可以创建自己的内核。</li><li id="4a91" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated"><strong class="lb iu"> C: </strong>这是在<a class="ae lx" href="#tuning" rel="noopener ugc nofollow">调整参数</a>部分描述的<strong class="lb iu">正则化参数</strong></li><li id="90ac" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated"><strong class="lb iu">伽玛:</strong>这也在<a class="ae lx" href="#tuning" rel="noopener ugc nofollow">调谐参数</a>一节中描述</li><li id="514b" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated"><strong class="lb iu">度数:</strong>仅当选择的内核是 poly 时使用<strong class="lb iu">并设置 polinom 的度数</strong></li><li id="4594" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated"><strong class="lb iu">概率:</strong>这是一个布尔参数，如果为真，那么模型将为每个预测返回属于响应变量的每个类别的概率向量。所以基本上它会给你每个预测的置信度。</li><li id="ccf7" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated"><strong class="lb iu">收缩:</strong>这表示您是否想要一个<strong class="lb iu">收缩启发式算法</strong>用于您的 SVM 优化，该算法用于<a class="ae lx" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf" rel="noopener ugc nofollow" target="_blank">顺序最小优化</a>。它的默认值是真的，一个<strong class="lb iu">如果你没有很好的理由，请不要把这个值改成假的</strong>，因为缩小会大大<strong class="lb iu"/><strong class="lb iu">提高你的性能</strong>，对于非常<strong class="lb iu">小的损失</strong>而言<strong class="lb iu">精度</strong>在大多数情况下。</li></ol><p id="cf65" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">现在让我们看看运行这段代码的输出。训练集的决策界限如下所示:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/b9c0162f3f42069071116ae93948b813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/0*AiUdfzW8iJmkAGAi.png"/></div></figure><p id="412d" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">正如我们在<a class="ae lx" href="#tuning" rel="noopener ugc nofollow">调谐参数</a>部分所见，因为 C 值很小(0.1)，所以判定边界是平滑的。</p><p id="7294" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">现在，如果我们将 C 从 0.1 增加到 100，决策边界中将有更多曲线:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ef596b1c84a82272a451dfaa79d7ba43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/0*pl5Eu6AZI0s3SEym.png"/></div></figure><p id="c71c" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">如果我们使用 C=0.1，但现在我们将 Gamma 从 0.1 增加到 10，会发生什么？让我们看看！</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/0c98ea9eadba65e735a98e541a918483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/0*GOxuLqcSpXyhNNEZ.png"/></div></figure><p id="b2d7" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">这里发生了什么？为什么我们会有这么差的模型？正如你在<a class="ae lx" href="#tuning" rel="noopener ugc nofollow">调整参数</a>部分看到的，<strong class="lb iu">高伽玛</strong>意味着当计算似是而非的超平面时，我们只考虑<strong class="lb iu">靠近</strong>的点。现在，因为绿色点<strong class="lb iu">的<strong class="lb iu">密度</strong>仅在所选绿色区域</strong>中较高，在该区域中，这些点足够接近似是而非的超平面，所以选择了这些超平面。小心 gamma 参数，因为如果将其设置为非常高的值(什么是“非常高的值”取决于数据点的密度)，这可能会对模型的结果产生非常坏的影响。</p><p id="dc7d" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">对于此示例，C 和 Gamma 的最佳值分别为 1.0 和 1.0。现在，如果我们在测试集上运行我们的模型，我们将得到下图:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/5612484c35b4cf2ec4055cd7b78f6629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/0*lJXOBji5rAlv7s1J.png"/></div></figure><p id="8f8f" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">而<strong class="lb iu">混淆矩阵</strong>看起来是这样的:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/9cc4d1c0761f5f84d8165199ef92addd.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/0*5uVGfqbMbt9f-1VY.png"/></div></figure><p id="d4e6" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">如你所见，我们只有<strong class="lb iu"> 3 个假阳性</strong>和<strong class="lb iu"> 4 个假阴性</strong>。该模型的<strong class="lb iu">准确率</strong>为<strong class="lb iu"> 93% </strong>这是一个非常好的结果，我们获得了比使用<a class="ae lx" href="http://dummyprogramming.com/stupid-simple-ai-series-knn/" rel="noopener ugc nofollow" target="_blank"> KNN </a>(准确率为 80%)更好的分数。</p><p id="4b4e" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated"><strong class="lb iu">注意:</strong>精确度不是 ML 中使用的唯一指标，也不是评估模型的最佳指标，因为<a class="ae lx" rel="noopener" target="_blank" href="/accuracy-paradox-897a69e2dd9b">精确度悖论</a>。为了简单起见，我们使用这个指标，但是稍后，在章节<strong class="lb iu">评估人工智能算法的指标</strong>中，我们将讨论<strong class="lb iu">准确性悖论</strong>，并且我将展示该领域中使用的其他非常流行的指标。</p><h1 id="1ea9" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">结论</h1><p id="8093" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在这篇文章中，我们看到了一个非常流行和强大的监督学习算法，<strong class="lb iu">支持向量机</strong>。我们已经学习了<strong class="lb iu">的基本思想</strong>，什么是<strong class="lb iu">超平面</strong>，什么是<strong class="lb iu">支持向量</strong>以及它们为什么如此重要。我们也看到了许多<strong class="lb iu">视觉表现</strong>，这有助于我们更好地理解所有的概念。</p><p id="e8f8" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">我们接触的另一个重要话题是<strong class="lb iu">内核技巧</strong>，它帮助我们<strong class="lb iu">解决非线性问题</strong>。</p><p id="43e2" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">为了有一个更好的模型，我们看到了调整算法的技术。在文章的最后，我们有一个用 Python 编写的<strong class="lb iu">代码示例，它向我们展示了如何使用 KNN 算法。</strong></p><figure class="me mf mg mh gt ju gh gi paragraph-image"><a href="https://ko-fi.com/zozoczako"><div class="gh gi nm"><img src="../Images/b1bd188a08576c80bde7d92888d2e27c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tYE4n1ydG0AvXMn_-ADaXw@2x.png"/></div></a></figure><p id="9497" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">我真的很喜欢咖啡，因为它让我有精力写更多的文章。</p><p id="5aee" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated"><strong class="lb iu">如果你喜欢这篇文章，那么你可以请我喝杯咖啡来表达你的欣赏和支持！</strong></p><p id="47ab" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">作为最后的想法，我想给出一些<strong class="lb iu">的优点&amp;缺点</strong>和一些流行的<strong class="lb iu">用例</strong>。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="27cf" class="pw-post-body-paragraph kz la it lb b lc ly le lf lg lz li lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated"><strong class="lb iu">成为媒介上的作家:</strong><a class="ae lx" href="https://czakozoltan08.medium.com/membership" rel="noopener">https://czakozoltan08.medium.com/membership</a></p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="ac11" class="kb kc it bd kd ke nu kg kh ki nv kk kl km nw ko kp kq nx ks kt ku ny kw kx ky bi translated">赞成的意见</h1><ol class=""><li id="4b33" class="mi mj it lb b lc ld lg lh lk nz lo oa ls ob lw mn mo mp mq bi translated">SVN 可能非常有效，因为它只使用了训练数据的子集，只使用了支持向量</li><li id="c54c" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated">在<strong class="lb iu">较小的数据集、非线性数据集</strong>和<strong class="lb iu">高维空间</strong>上运行良好</li><li id="63a0" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated">在<strong class="lb iu">维数大于样本数</strong>的情况下<strong class="lb iu">非常有效</strong></li><li id="a0e5" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated">它可以有<strong class="lb iu">高精度</strong>，有时甚至可以比神经网络表现得更好</li><li id="7150" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated"><strong class="lb iu">不</strong>对过度配合非常<strong class="lb iu">敏感</strong></li></ol><h1 id="d983" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">骗局</h1><ol class=""><li id="2f73" class="mi mj it lb b lc ld lg lh lk nz lo oa ls ob lw mn mo mp mq bi translated">当我们有大量数据集时，训练时间很长</li><li id="d57b" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated">当数据集有更多的<strong class="lb iu">噪声</strong>(即目标类别重叠)<strong class="lb iu">时，SVM 表现不佳</strong></li></ol><h1 id="f637" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">流行的使用案例</h1><ol class=""><li id="ed5f" class="mi mj it lb b lc ld lg lh lk nz lo oa ls ob lw mn mo mp mq bi translated">文本分类</li><li id="7203" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated">检测垃圾邮件</li><li id="25c2" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated">情感分析</li><li id="5bcc" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated">基于方面的识别</li><li id="0057" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated">基于方面的识别</li><li id="47e8" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated">手写数字识别</li></ol><h1 id="c977" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">参考</h1><ol class=""><li id="2a61" class="mi mj it lb b lc ld lg lh lk nz lo oa ls ob lw mn mo mp mq bi translated"><a class="ae lx" href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/" rel="noopener ugc nofollow" target="_blank">了解 SVM </a></li><li id="25f5" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated">SVM:一个简单的解释</li><li id="8b43" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated">SVM:理解数学</li><li id="80ac" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated"><a class="ae lx" href="https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72" rel="noopener"> SVM 理论</a></li><li id="414b" class="mi mj it lb b lc mr lg ms lk mt lo mu ls mv lw mn mo mp mq bi translated"><a class="ae lx" href="https://www.udemy.com/machinelearning/learn/v4/t/lecture/5714406?start=0" rel="noopener ugc nofollow" target="_blank"> Udemy:机器学习 A-Z </a></li></ol></div></div>    
</body>
</html>