<html>
<head>
<title>Word to Vectors — Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">词到向量—自然语言处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817?source=collection_archive---------0-----------------------#2017-05-18">https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817?source=collection_archive---------0-----------------------#2017-05-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="a6b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">自然语言处理为什么很难？</strong></p><p id="0203" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">计算机用编程语言与人类交流，这种语言清晰、精确，而且通常是结构化的。但是，自然(人类)语言有很多歧义。有多个意思相同的词(同义词)，有多个意思的词(一词多义)，其中一些在性质上完全相反(自动反义词)，还有当用作名词和动词时表现不同的词。这些词在自然语言的上下文中是有意义的，人类可以很容易地理解和区分，但机器不能。这就是NLP成为人工智能中最困难和最有趣的任务之一的原因。</p><p id="6fc4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">使用自然语言处理可以完成什么？</strong></p><p id="7a2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让计算机“理解”人类语言可以完成几项任务。一个活生生的例子就是我在这篇文章中用来检查拼写和语法的软件。以下是目前使用NLP完成的一些任务:</p><ol class=""><li id="a6b0" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">拼写和语法检查</li><li id="f12c" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">寻找同义词和反义词</li><li id="cade" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">从文档、网站解析信息</li><li id="5103" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">理解句子、文档和查询的含义</li><li id="f315" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">机器翻译(例如，将文本从英语翻译成德语)</li><li id="9ece" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">回答问题和执行任务(例如安排约会等。)</li></ol><p id="0037" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">如何表示文字？</strong></p><p id="d303" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我们需要能够将单词表示为机器学习模型的输入。表示单词的一种数学方法是向量。英语中估计有1300万个单词。但其中许多是相关的。从配偶到伴侣，从酒店到汽车旅馆。那么，我们是否希望1300万个单词都有独立的向量呢？</p><p id="b8a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不。我们必须寻找一个N维向量空间(其中N &lt;&lt; 13 million) that is sufficient to encode all semantics in our language. We need to have a sense of similarity and difference between words. We can exploit concept of vectors and distances between them (Cosine, Euclidean etc. ) to find similarities and differences between words.</p><p id="f018" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">我们如何表示单词的意思？</strong></p><p id="8cfa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们对英语词汇中的1300万个单词(或者更多)使用不同的向量，我们将面临几个问题。首先，我们有一个大的向量，里面有很多“0”和一个“1”(在不同的位置代表不同的单词)。这也被称为<strong class="jp ir">一键编码。</strong>其次，当我们在谷歌中搜索诸如“新泽西州的酒店”等短语时，我们希望返回与新泽西州的“汽车旅馆”、“住宿”和“住宿”相关的结果。如果我们使用一键编码，这些单词没有自然的相似性。理想情况下，我们希望同义词/相似词的点积(因为我们处理的是向量)接近1。</p><p id="cb48" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">基于分布相似性的表示:</strong></p><blockquote class="kz la lb"><p id="3f71" class="jn jo lc jp b jq jr js jt ju jv jw jx ld jz ka kb le kd ke kf lf kh ki kj kk ij bi translated">从一个人交的朋友你就可以知道他说了什么——j·r·弗斯</p></blockquote><p id="9231" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用非常简单的外行话来说，就拿一个字‘银行’来说吧。这个词有许多含义，其中一个是金融机构，另一个是水边的土地。如果在一个句子中，银行与邻近的词如货币、政府、国库、利率等一起出现。我们可以理解为前者的意思。相反，如果相邻的词是水、海岸、河流、土地等。情况是后者。我们可以利用这个概念来处理多义词和同义词，并使我们的模型能够学习。</p><p id="dadf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Word2Vec </strong></p><p id="5d09" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们如何构建简单、可扩展、快速的训练模型，能够运行数十亿个单词，产生非常好的单词表示？让我们研究一下Word2Vec模型来寻找这个问题的答案。</p><p id="f9ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Word2Vec是一组帮助导出单词与其上下文单词之间关系的模型。让我们看看Word2Vec中的两个重要模型:Skip-grams和CBOW</p><p id="7758" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">跳棋</strong></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/8dbce9a4692ad87b0ef2a048c2f1e3ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yiH5sZI-IBxDSQMKhvbcHw.png"/></div></div></figure><p id="577e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Skip-gram模型中，我们取一个<strong class="jp ir">中心</strong>单词和一个<strong class="jp ir">上下文(邻居)</strong>单词的窗口，并且我们尝试为每个中心单词预测出一些窗口大小的上下文单词。因此，我们的模型将定义一个概率分布，即给定一个中心词，一个词在上下文中出现的概率，我们将选择我们的向量表示来最大化该概率。</p><p id="a4d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">连续单词包模型(CBOW) </strong></p><p id="43d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从抽象的角度来说，这与跳格是相反的。在CBOW中，我们试图通过对周围单词的向量求和来预测中心单词。</p><p id="6a86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是关于把单词转换成向量。但是“学习”发生在哪里呢？本质上，我们从单词向量的小的随机初始化开始。我们的预测模型通过最小化损失函数来学习向量。在Word2vec中，这是通过前馈神经网络和优化技术(如随机梯度下降)实现的。还存在基于计数的模型，其在我们的语料库中形成单词的共现计数矩阵；我们有一个大矩阵，每行代表“单词”,每列代表“上下文”。“上下文”的数量当然很大，因为它在大小上基本上是组合的。为了克服这个尺寸问题，我们将<a class="ae ls" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank"> SVD </a>应用于矩阵。这减少了矩阵的维数，保留了最大限度的信息。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lt"><img src="../Images/df7a6c227c74943962074960143676f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*svLRt3OwVyqZiyDammWqiA.png"/></div></div></figure><p id="9219" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总之，将单词转换为深度学习算法可以摄取和处理的向量，有助于更好地理解自然语言。小说家埃尔·多克托罗在他的《比利·巴斯盖特》一书中颇有诗意地表达了这一观点。</p><blockquote class="kz la lb"><p id="662d" class="jn jo lc jp b jq jr js jt ju jv jw jx ld jz ka kb le kd ke kf lf kh ki kj kk ij bi translated">就像数字是语言一样，语言中的所有字母都变成了数字，所以每个人都以同样的方式理解它。你失去了字母的声音，不管它们是咔嚓声、砰砰声还是碰上颚的声音，或者发出“哦”或“啊”的声音，以及任何可能被误读或被它的音乐或它放在你脑海中的图片欺骗的声音，所有这些都消失了，连同口音，你有了一种完全新的理解，一种数字语言，一切对每个人来说都变得像墙上的文字一样清晰。所以正如我所说的，阅读数字的时间到了。</p></blockquote></div></div>    
</body>
</html>