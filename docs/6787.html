<html>
<head>
<title>Predicting Reddit Comment Upvotes with Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用机器学习预测 Reddit 评论投票数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-reddit-comment-karma-a8f570b544fc?source=collection_archive---------19-----------------------#2018-12-31">https://towardsdatascience.com/predicting-reddit-comment-karma-a8f570b544fc?source=collection_archive---------19-----------------------#2018-12-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div class="gh gi io"><img src="../Images/5870b8835726e24eaa3951de0db2a359.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*5am_MxESuWqHL03BhpU4aw.png"/></div></figure><figure class="iw ix iy iz gt is gh gi paragraph-image"><div class="gh gi iv"><img src="../Images/8415b193b4895242041327534cb97f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/format:webp/1*t_Fs2xZ24rw1GHP6oRL0qg.png"/></div></figure><div class=""/><p id="8650" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在本文中，我们将使用 Python 和 scikit-learn 包来预测 Reddit 上一条评论的投票数。我们拟合了各种回归模型，并使用以下指标比较了它们的性能:</p><ul class=""><li id="02af" class="kx ky jc kb b kc kd kg kh kk kz ko la ks lb kw lc ld le lf bi translated">r 来衡量拟合优度</li><li id="a815" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated">在<em class="ll">测试集</em>上测量精确度的平均绝对误差(MAE)和均方根误差(RMSE)。</li></ul><p id="e3df" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这篇文章基于来自<a class="ae lm" href="https://github.com/areevesman/reddit-upvote-modeling" rel="noopener ugc nofollow" target="_blank">这个</a> Github 库的工作。代码可以在<a class="ae lm" href="https://github.com/areevesman/reddit-upvote-modeling/blob/master/model_fitting/associated_nb_for_medium.ipynb" rel="noopener ugc nofollow" target="_blank">这个</a>笔记本里找到。</p><figure class="iw ix iy iz gt is gh gi paragraph-image"><div class="gh gi iv"><img src="../Images/8415b193b4895242041327534cb97f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/format:webp/1*t_Fs2xZ24rw1GHP6oRL0qg.png"/></div></figure><h1 id="9eeb" class="ln lo jc bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">背景</h1><p id="2cf1" class="pw-post-body-paragraph jz ka jc kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw ij bi translated">Reddit 是一个流行的社交媒体网站。在这个网站上，用户在各种<em class="ll">子主题</em>中发布<em class="ll">主题</em>，如下图所示。</p><figure class="iw ix iy iz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mq"><img src="../Images/e95df5cd9e895e156e52c5cac6116bdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vPeZSebZ0SUEV1Xgft8j8Q.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">A thread in the “AskReddit” subreddit</figcaption></figure><p id="d7bd" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">用户可以对主题或其他评论进行评论。他们还可以给其他线程和评论投<em class="ll">赞成票</em>或<em class="ll">反对票</em>。</p><figure class="iw ix iy iz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mz"><img src="../Images/624efb991d1c5cd1f6573f32837faeb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HKeDRrbaOQtGSOrLGEFTfg.png"/></div></div></figure><p id="dc0c" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们的目标是预测评论将获得的支持票数。</p><figure class="iw ix iy iz gt is gh gi paragraph-image"><div class="gh gi iv"><img src="../Images/8415b193b4895242041327534cb97f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/format:webp/1*t_Fs2xZ24rw1GHP6oRL0qg.png"/></div></figure><h1 id="053f" class="ln lo jc bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">数据</h1><p id="6e61" class="pw-post-body-paragraph jz ka jc kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw ij bi translated">该数据是一个 pickle 文件，包含发生在 2015 年 5 月的 1，205，039 行(评论)，存放在 google drive 上，可以使用此<a class="ae lm" href="https://drive.google.com/file/d/1pKh6vu-NmWFJVq0BmdSy9kq_cSVWFV-J/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">链接</a>下载。</p><p id="94be" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">下面列出了将用于建模的目标变量和相关特征。它们可以分为几类。</p><h2 id="b6d8" class="na lo jc bd lp nb nc dn lt nd ne dp lx kk nf ng mb ko nh ni mf ks nj nk mj nl bi translated">目标变量</h2><ul class=""><li id="edf1" class="kx ky jc kb b kc ml kg mm kk nm ko nn ks no kw lc ld le lf bi translated"><strong class="kb jd">得分</strong>:评论的投票数</li></ul><h2 id="c0ce" class="na lo jc bd lp nb nc dn lt nd ne dp lx kk nf ng mb ko nh ni mf ks nj nk mj nl bi translated">评论级别功能</h2><ul class=""><li id="be09" class="kx ky jc kb b kc ml kg mm kk nm ko nn ks no kw lc ld le lf bi translated"><strong class="kb jd">镀金</strong>:评论上镀金标签(高级赞)的数量</li><li id="b641" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd">区分</strong>:页面上的用户类型。“版主”、“管理员”或“用户”</li><li id="0ec0" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd">争议性</strong>:一个布尔值，表示(1)或(0)评论是否有争议(热门评论的支持票数与反对票数接近)</li><li id="4c21" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> over_18 </strong>:线是否被标记为 NSFW</li><li id="f58c" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> time_lapse </strong>:评论和线程上第一条评论之间的时间，以秒为单位</li><li id="1464" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd">小时评论</strong>:发布了一天中的小时评论</li><li id="e58b" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd">工作日</strong>:星期几发表评论</li><li id="2c3a" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> is_flair </strong>:评论是否有 flair 文本(<a class="ae lm" href="https://www.reddit.com/r/help/comments/3tbuml/whats_a_flair/" rel="noopener ugc nofollow" target="_blank">https://www . Reddit . com/r/help/comments/3 tbuml/whats _ a _ flair/</a>)</li><li id="a8fe" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> is_flair_css </strong>:是否有注释 flair 的 css 类</li><li id="1141" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd">深度</strong>:线程中注释的深度(注释拥有的父注释数)</li><li id="57fb" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> no_of_linked_sr: </strong>注释中提到的子条目数</li><li id="c50a" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd">链接网址数量:</strong>评论中链接的网址数量</li><li id="3808" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd">主观性</strong>:“我”的实例数</li><li id="b164" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> is_edited </strong> : <strong class="kb jd"> </strong>评论是否被编辑</li><li id="5916" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd">被引用</strong>:评论是否引用另一个评论</li><li id="c3c7" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> no_quoted </strong>:评论中引用的次数</li><li id="77c1" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> senti_neg </strong>:负面情绪得分</li><li id="228c" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> senti_neu </strong>:中性情绪得分</li><li id="4e72" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> senti_pos </strong>:正面情绪得分</li><li id="ef79" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> senti_comp </strong>:复合情绪得分</li><li id="7e0f" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd">字数</strong>:评论的字数</li></ul><h2 id="a329" class="na lo jc bd lp nb nc dn lt nd ne dp lx kk nf ng mb ko nh ni mf ks nj nk mj nl bi translated">父级功能</h2><ul class=""><li id="1da6" class="kx ky jc kb b kc ml kg mm kk nm ko nn ks no kw lc ld le lf bi translated"><strong class="kb jd"> time_since_parent </strong>:注释和父注释之间的时间，以秒为单位</li><li id="8e2d" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> parent_score: </strong>父评论的分数(如果评论没有父，则为 NaN)</li><li id="1d91" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> parent_cos_angle </strong>:注释与其父注释的嵌入之间的余弦相似度(<a class="ae lm" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a>)</li></ul><h2 id="a67a" class="na lo jc bd lp nb nc dn lt nd ne dp lx kk nf ng mb ko nh ni mf ks nj nk mj nl bi translated">注释树根功能</h2><ul class=""><li id="7274" class="kx ky jc kb b kc ml kg mm kk nm ko nn ks no kw lc ld le lf bi translated"><strong class="kb jd"> is_root </strong>:评论是否为根</li><li id="4a5a" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd">time _ since _ comment _ tree _ root</strong>:评论和评论树根之间的时间(秒)</li><li id="5795" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd">评论树根得分</strong>:评论树根得分</li></ul><h2 id="220f" class="na lo jc bd lp nb nc dn lt nd ne dp lx kk nf ng mb ko nh ni mf ks nj nk mj nl bi translated">线程级功能</h2><ul class=""><li id="3542" class="kx ky jc kb b kc ml kg mm kk nm ko nn ks no kw lc ld le lf bi translated"><strong class="kb jd"> link_score </strong>:在线评论的 upvotes 开启</li><li id="931e" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> upvote_ratio </strong>:帖子评论上的所有投票中，upvotes 的百分比为 on</li><li id="54c8" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> link_ups </strong>:线程上的投票数</li><li id="7679" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> time_since_link </strong>:线程创建以来的时间(秒)</li><li id="563f" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> no_past_comments </strong>:发表评论前帖子上的评论数</li><li id="84d9" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> score_till_now </strong>:该评论发布时的帖子得分</li><li id="6adc" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> title_cos_angle </strong>:注释与其线程标题嵌入之间的余弦相似度</li><li id="2056" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> is_selftext </strong>:线程是否有 selftext</li></ul><figure class="iw ix iy iz gt is gh gi paragraph-image"><div class="gh gi iv"><img src="../Images/8415b193b4895242041327534cb97f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/format:webp/1*t_Fs2xZ24rw1GHP6oRL0qg.png"/></div></figure><h1 id="3bb1" class="ln lo jc bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">设置</h1><p id="6ee9" class="pw-post-body-paragraph jz ka jc kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw ij bi translated">让我们加载我们需要的所有库。</p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="ac49" class="na lo jc nq b gy nu nv l nw nx"><strong class="nq jd">import</strong> <strong class="nq jd">pandas</strong> <strong class="nq jd">as</strong> <strong class="nq jd">pd<br/>import</strong> <strong class="nq jd">numpy</strong> <strong class="nq jd">as</strong> <strong class="nq jd">np<br/>import</strong> <strong class="nq jd">matplotlib.pyplot</strong> <strong class="nq jd">as</strong> <strong class="nq jd">plt<br/>import</strong> <strong class="nq jd">seaborn</strong> <strong class="nq jd">as</strong> <strong class="nq jd">sns</strong></span><span id="542c" class="na lo jc nq b gy ny nv l nw nx"><strong class="nq jd">from</strong> <strong class="nq jd">sklearn.metrics</strong> <strong class="nq jd">import</strong> mean_squared_error, r2_score, mean_absolute_error<br/><strong class="nq jd">from</strong> <strong class="nq jd">sklearn.model_selection</strong> <strong class="nq jd">import</strong> train_test_split<br/><strong class="nq jd">from</strong> <strong class="nq jd">sklearn.preprocessing</strong> <strong class="nq jd">import</strong> LabelBinarizer</span><span id="0fbb" class="na lo jc nq b gy ny nv l nw nx"><strong class="nq jd">from</strong> <strong class="nq jd">sklearn.dummy</strong> <strong class="nq jd">import</strong> DummyRegressor<br/><strong class="nq jd">from</strong> <strong class="nq jd">sklearn.linear_model</strong> <strong class="nq jd">import</strong> LinearRegression<br/><strong class="nq jd">from</strong> <strong class="nq jd">sklearn.linear_model</strong> <strong class="nq jd">import</strong> LassoCV<br/><strong class="nq jd">from</strong> <strong class="nq jd">sklearn.linear_model</strong> <strong class="nq jd">import</strong> RidgeCV<br/><strong class="nq jd">from</strong> <strong class="nq jd">sklearn.linear_model</strong> <strong class="nq jd">import</strong> ElasticNetCV<br/><strong class="nq jd">from</strong> <strong class="nq jd">sklearn.neighbors</strong> <strong class="nq jd">import</strong> KNeighborsRegressor<br/><strong class="nq jd">from</strong> <strong class="nq jd">sklearn.tree</strong> <strong class="nq jd">import</strong> DecisionTreeRegressor<br/><strong class="nq jd">from</strong> <strong class="nq jd">sklearn.ensemble</strong> <strong class="nq jd">import</strong> RandomForestRegressor<br/><strong class="nq jd">from</strong> <strong class="nq jd">sklearn.ensemble</strong> <strong class="nq jd">import</strong> GradientBoostingRegressor</span><span id="8d05" class="na lo jc nq b gy ny nv l nw nx"><strong class="nq jd">import</strong> <strong class="nq jd">warnings<br/></strong>warnings.filterwarnings('ignore')</span></pre><p id="b536" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们还定义了一些与模型交互的函数。</p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="b1a3" class="na lo jc nq b gy nu nv l nw nx"><strong class="nq jd">def</strong> model_diagnostics(model, pr=True):<br/>    """<br/>    Returns and prints the <!-- -->R-squared<!-- -->, <!-- -->RMSE<!-- --> and the <!-- -->MAE<!-- --> for a trained model<br/>    """<br/>    y_predicted = model.predict(X_test)<br/>    r2 = r2_score(y_test, y_predicted)<br/>    mse = mean_squared_error(y_test, y_predicted)<br/>    mae = mean_absolute_error(y_test, y_predicted)<br/>    if pr:<br/>        print(f"R-Sq: <strong class="nq jd">{r2:.4}</strong>")<br/>        print(f"RMSE: {np.sqrt(mse)}")<br/>        print(f"MAE: <strong class="nq jd">{mae}</strong>")<br/>    <br/>    <strong class="nq jd">return</strong> [r2,np.sqrt(mse),mae]</span><span id="7037" class="na lo jc nq b gy ny nv l nw nx"><strong class="nq jd">def</strong> plot_residuals(y_test, y_predicted):<br/>    """"<br/>    Plots the distribution for actual and predicted values of the target variable. Also plots the distribution for the residuals<br/>    """<br/>    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, sharey=<strong class="nq jd">True</strong>)<br/>    sns.distplot(y_test, ax=ax0, kde = <strong class="nq jd">False</strong>)<br/>    ax0.set(xlabel='Test scores')<br/>    sns.distplot(y_predicted, ax=ax1, kde = <strong class="nq jd">False</strong>)<br/>    ax1.set(xlabel="Predicted scores")<br/>    plt.show()<br/>    fig, ax2 = plt.subplots()<br/>    sns.distplot((y_test-y_predicted), ax = ax2,kde = <strong class="nq jd">False</strong>)<br/>    ax2.set(xlabel="Residuals")<br/>    plt.show()</span><span id="9d58" class="na lo jc nq b gy ny nv l nw nx"><strong class="nq jd">def</strong> y_test_vs_y_predicted(y_test,y_predicted):<br/>    """<br/>    Produces a scatter plot for the actual and predicted values of the target variable<br/>    """<br/>    fig, ax = plt.subplots()<br/>    ax.scatter(y_test, y_predicted)<br/>    ax.set_xlabel("Test Scores")<br/>    ax.set_ylim([-75, 1400])<br/>    ax.set_ylabel("Predicted Scores")<br/>    plt.show()</span><span id="0860" class="na lo jc nq b gy ny nv l nw nx"><strong class="nq jd">def</strong> get_feature_importance(model):<br/>    """<br/>    For fitted tree based models, get_feature_importance can be used to get the feature importance as a tidy output<br/>    """<br/>    X_non_text = pd.get_dummies(df[cat_cols])<br/>    features = numeric_cols + bool_cols + list(X_non_text.columns)<br/>    feature_importance = dict(zip(features, model.feature_importances_))<br/>    <strong class="nq jd">for</strong> name, importance <strong class="nq jd">in</strong> sorted(feature_importance.items(), key=<strong class="nq jd">lambda</strong> x: x[1], reverse=<strong class="nq jd">True</strong>):<br/>        print(f"<strong class="nq jd">{name:&lt;30}</strong>: <strong class="nq jd">{importance:&gt;6.2%}</strong>")<br/>        print(f"<strong class="nq jd">\n</strong>Total importance: {sum(feature_importance.values()):.2%}")<br/>    <strong class="nq jd">return</strong> feature_importance</span></pre><p id="953c" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb jd">读入数据</strong></p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="4967" class="na lo jc nq b gy nu nv l nw nx">df = pd.read_pickle('reddit_comments.pkl')</span></pre><p id="bbaa" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb jd">处理缺失值</strong></p><p id="4662" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">数据有一些缺失值，通过插补或删除观察值来处理。以下各列中出现缺失值，原因如下:</p><ul class=""><li id="af89" class="kx ky jc kb b kc kd kg kh kk kz ko la ks lb kw lc ld le lf bi translated"><strong class="kb jd"> parent_score </strong>:一些评论没有父评论(估算)</li><li id="079c" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> comment_tree_root_score </strong>和<strong class="kb jd">time _ since _ comment _ tree _ root</strong>:一些评论是评论树的根(估算)</li><li id="b67a" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd"> parent_cosine，parent_euc，title_cosine，title_euc </strong>:部分评论缺少有手套单词嵌入的单词(dropped)。此外，一些评论没有父项(parent_cosine，parent_title imputed)</li></ul><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="04e6" class="na lo jc nq b gy nu nv l nw nx">df = df[~df.title_cosine.isna()] # drop where parent/title_cosine is NaN</span><span id="ba6a" class="na lo jc nq b gy ny nv l nw nx">parent_scrore_impute = df.parent_score.mode()[0] # impute with mode of parent_score column<br/>comment_tree_root_score_impute = df.comment_tree_root_score.mode()[0] # impute with mode of comment_tree_root_score column<br/>time_since_comment_tree_root_impute = df.time_since_comment_tree_root.mode()[0] # impute with mode of time_since_comment_tree_root column<br/>parent_cosine_impute = 0<br/>parent_euc_impute = 0<br/>df.loc[df.parent_score.isna(), 'parent_score'] = parent_scrore_impute<br/>df.loc[df.comment_tree_root_score.isna(), 'comment_tree_root_score'] = comment_tree_root_score_impute<br/>df.loc[df.time_since_comment_tree_root.isna(), 'time_since_comment_tree_root'] = time_since_comment_tree_root_impute<br/>df.loc[df.parent_cosine.isna(), 'parent_cosine'] = parent_cosine_impute<br/>df.loc[df.parent_euc.isna(), 'parent_euc'] = parent_euc_impute</span></pre><p id="2816" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb jd">选择变量</strong></p><p id="d32d" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在下一步中，我们定义在训练模型时使用哪些变量。我们为布尔变量、多类别变量和数字变量制作了一个列表。</p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="dfe8" class="na lo jc nq b gy nu nv l nw nx">bool_cols = ['over_18', 'is_edited', 'is_quoted', 'is_selftext']</span><span id="c19e" class="na lo jc nq b gy ny nv l nw nx">cat_cols = ['subreddit', 'distinguished', 'is_flair', 'is_flair_css','hour_of_comment', 'weekday']<br/><br/>numeric_cols = ['gilded', 'controversiality', 'upvote_ratio','time_since_link',<br/>                'depth', 'no_of_linked_sr', 'no_of_linked_urls', 'parent_score',<br/>                'comment_tree_root_score', 'time_since_comment_tree_root',<br/>                'subjectivity', 'senti_neg', 'senti_pos', 'senti_neu',<br/>                'senti_comp', 'no_quoted', 'time_since_parent', 'word_counts',<br/>                'no_of_past_comments', 'parent_cosine','parent_euc',<br/>                'title_cosine', 'title_euc', 'no_quoted','link_score']</span></pre><p id="662c" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">使用我们的变量列表，我们可以为建模准备数据。下面的步骤使用 scikit-learn 的<code class="fe nz oa ob nq b">LabelBinarizer</code>从分类列中生成虚拟变量，然后组合所有变量。</p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="a9e7" class="na lo jc nq b gy nu nv l nw nx">lb = LabelBinarizer()<br/>cat = [lb.fit_transform(df[col]) <strong class="nq jd">for</strong> col <strong class="nq jd">in</strong> cat_cols]<br/>bol = [df[col].astype('int') <strong class="nq jd">for</strong> col <strong class="nq jd">in</strong> bool_cols]<br/>t = df.loc[:, numeric_cols].values<br/>final = [t] + bol + cat<br/>y = df.score.values<br/>x = np.column_stack(tuple(final))</span></pre><p id="9980" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们使用 80–20 的比例将数据分成训练集和测试集。</p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="855c" class="na lo jc nq b gy nu nv l nw nx">X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10)</span></pre><figure class="iw ix iy iz gt is gh gi paragraph-image"><div class="gh gi iv"><img src="../Images/8415b193b4895242041327534cb97f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/format:webp/1*t_Fs2xZ24rw1GHP6oRL0qg.png"/></div></figure><h1 id="7377" class="ln lo jc bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">建模</h1><p id="e977" class="pw-post-body-paragraph jz ka jc kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw ij bi translated">在本节中，我们使用 scikit-learn 来拟合 Reddit 数据上的模型。我们从基线模型开始，然后尝试用套索、岭和弹性网回归来改进结果。此外，我们尝试了 K 近邻、决策树、随机森林和梯度推进回归。</p><p id="a2cd" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">首先，让我们定义一个字典来存储模型诊断的结果。</p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="d6f3" class="na lo jc nq b gy nu nv l nw nx">model_performance_dict = dict()</span></pre><h2 id="163d" class="na lo jc bd lp nb nc dn lt nd ne dp lx kk nf ng mb ko nh ni mf ks nj nk mj nl bi translated">线性回归模型</h2><p id="51ba" class="pw-post-body-paragraph jz ka jc kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw ij bi translated"><strong class="kb jd">基线模型</strong></p><p id="16d2" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们用一个简单的模型来建立一个基线。这个模型总是预测平均票数。</p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="60fa" class="na lo jc nq b gy nu nv l nw nx">baseline = DummyRegressor(strategy='mean')<br/>baseline.fit(X_train,y_train)<br/>model_performance_dict["Baseline"] = model_diagnostics(baseline)</span></pre><p id="3370" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb jd">线性回归</strong></p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="0ceb" class="na lo jc nq b gy nu nv l nw nx">linear = LinearRegression()<br/>linear.fit(X_train,y_train)<br/>model_performance_dict["Linear Regression"] = model_diagnostics(linear)</span></pre><p id="f616" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb jd">套索回归</strong></p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="6da9" class="na lo jc nq b gy nu nv l nw nx">lasso = LassoCV(cv=30).fit(X_train, y_train)<br/>model_performance_dict["Lasso Regression"] = model_diagnostics(lasso)</span></pre><p id="5ac8" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb jd">岭回归</strong></p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="b776" class="na lo jc nq b gy nu nv l nw nx">ridge = RidgeCV(cv=10).fit(X_train, y_train)<br/>model_performance_dict["Ridge Regression"] = model_diagnostics(ridge)</span></pre><p id="e81d" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb jd">弹性网回归</strong></p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="08d5" class="na lo jc nq b gy nu nv l nw nx">elastic_net = ElasticNetCV(cv = 30).fit(X_train, y_train)<br/>model_performance_dict["Elastic Net Regression"] = model_diagnostics(elastic_net)</span></pre><h2 id="5c5a" class="na lo jc bd lp nb nc dn lt nd ne dp lx kk nf ng mb ko nh ni mf ks nj nk mj nl bi translated">非线性回归模型</h2><p id="d970" class="pw-post-body-paragraph jz ka jc kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw ij bi translated"><strong class="kb jd"> K 近邻回归</strong></p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="bb06" class="na lo jc nq b gy nu nv l nw nx">knr = KNeighborsRegressor()<br/>knr.fit(X_train, y_train)<br/>model_performance_dict["KNN Regression"] = model_diagnostics(knr)</span></pre><p id="4e8e" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb jd">决策树回归</strong></p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="313d" class="na lo jc nq b gy nu nv l nw nx">dt = DecisionTreeRegressor(min_samples_split=45, min_samples_leaf=45, random_state = 10)<br/>dt.fit(X_train, y_train)<br/>model_performance_dict["Decision Tree"] = model_diagnostics(dt)</span></pre><p id="c0e8" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb jd">随机森林回归</strong></p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="6592" class="na lo jc nq b gy nu nv l nw nx">rf = RandomForestRegressor(n_jobs=-1, n_estimators=70, min_samples_leaf=10, random_state = 10)<br/>rf.fit(X_train, y_train)<br/>model_performance_dict["Random Forest"] = model_diagnostics(rf)</span></pre><p id="cd2a" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb jd">梯度推进回归</strong></p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="bb39" class="na lo jc nq b gy nu nv l nw nx">gbr = GradientBoostingRegressor(n_estimators=70, max_depth=5)<br/>gbr.fit(X_train, y_train)<br/>model_performance_dict["Gradient Boosting Regression"] = model_diagnostics(gbr)</span></pre><figure class="iw ix iy iz gt is gh gi paragraph-image"><div class="gh gi iv"><img src="../Images/8415b193b4895242041327534cb97f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/format:webp/1*t_Fs2xZ24rw1GHP6oRL0qg.png"/></div></figure><h1 id="b4be" class="ln lo jc bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">模型比较</h1><p id="988d" class="pw-post-body-paragraph jz ka jc kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw ij bi translated">我们基于三个指标来比较这些模型:R、MAE 和 RMSE。为此，我们定义了下面的函数。</p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="6297" class="na lo jc nq b gy nu nv l nw nx"><strong class="nq jd">def</strong> model_comparison(model_performance_dict, sort_by = 'RMSE', metric = 'RMSE'):<br/><br/>    Rsq_list = []<br/>    RMSE_list = []<br/>    MAE_list = []<br/>    <strong class="nq jd">for</strong> key <strong class="nq jd">in</strong> model_performance_dict.keys():<br/>        Rsq_list.append(model_performance_dict[key][0])<br/>        RMSE_list.append(model_performance_dict[key][1])<br/>        MAE_list.append(model_performance_dict[key][2])<br/><br/>    props = pd.DataFrame([])<br/><br/>    props["R-squared"] = Rsq_list<br/>    props["RMSE"] = RMSE_list<br/>    props["MAE"] = MAE_list<br/>    props.index = model_performance_dict.keys()<br/>    props = props.sort_values(by = sort_by)<br/><br/>    fig, ax = plt.subplots(figsize = (12,6))<br/><br/>    ax.bar(props.index, props[metric], color="blue")<br/>    plt.title(metric)<br/>    plt.xlabel('Model')<br/>    plt.xticks(rotation = 45)<br/>    plt.ylabel(metric)</span></pre><p id="1a7a" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">让我们使用这个函数来比较基于每个指标的模型。</p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="3ce8" class="na lo jc nq b gy nu nv l nw nx">model_comparison(model_performance_dict, sort_by = 'R-squared', metric = 'R-squared')</span></pre><figure class="iw ix iy iz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi oc"><img src="../Images/68b933953aa346748a8ce0a65dafbde5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aD-0HZdZUWfrwIBPzZOUGg.png"/></div></div></figure><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="2ae2" class="na lo jc nq b gy nu nv l nw nx">model_comparison(model_performance_dict, sort_by = 'R-squared', metric = 'MAE')</span></pre><figure class="iw ix iy iz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi od"><img src="../Images/9c8dc08b810b31c9c7017b1ef849e8a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t4Im0q8MpQVpBZl2-obESA.png"/></div></div></figure><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="ce3a" class="na lo jc nq b gy nu nv l nw nx">model_comparison(model_performance_dict, sort_by = 'R-squared', metric = 'RMSE')</span></pre><figure class="iw ix iy iz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi oe"><img src="../Images/b2e2e317544a5a4e54f479cc36834314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mhgt0CyFx8DvXiOMvo90BA.png"/></div></div></figure><figure class="iw ix iy iz gt is gh gi paragraph-image"><div class="gh gi iv"><img src="../Images/8415b193b4895242041327534cb97f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/format:webp/1*t_Fs2xZ24rw1GHP6oRL0qg.png"/></div></figure><h1 id="7e1b" class="ln lo jc bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">解释结果</h1><p id="2f0a" class="pw-post-body-paragraph jz ka jc kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw ij bi translated">考虑到性能和训练时间，随机森林模型是一个合理的选择。平均绝对误差约为 9.7，这意味着平均而言，模型估计误差约为 9.7。让我们看一些图，了解更多关于模型性能的信息。</p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="7b24" class="na lo jc nq b gy nu nv l nw nx">y_predicted = rf.predict(X_test)<br/>plot_residuals(y_test,y_predicted)</span></pre><figure class="iw ix iy iz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi of"><img src="../Images/3c55681d18aae5dc81b8bb41ffd9a610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*srlpC1zqBDqY_sU69pwl1Q.png"/></div></div></figure><p id="096a" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">比较测试分数和预测分数的直方图，我们注意到，当目标变量较小时，模型往往会高估目标变量。此外，模型从不预测目标变量会远大于 2000。在目标变量较大的少数情况下，结果似乎有偏差。大多数意见只有少量的支持票，但 model 预计这些意见会得到更多的支持票。然而，当评论有极端数量的支持票时，模型会低估它。</p><p id="ca91" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">残差的这种分布表明，合乎逻辑的下一步是探索堆叠模型的结果。堆叠是一种集合技术(如随机森林、梯度推进等。)这通常可以提高性能。我们将首先拟合一个分类器来预测向上投票的数量(具有像“少”、“一些”、“许多”这样的类),并且结果将被用作回归模型中的附加预测器。这种方法有可能减少错误并提高拟合度，因为除了我们的原始信息之外，回归模型还会有关于评论数量的提示，以帮助它进行预测。</p><p id="ae4c" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">基于树的模型也允许我们量化他们使用的特性的重要性。</p><pre class="iw ix iy iz gt np nq nr ns aw nt bi"><span id="df8c" class="na lo jc nq b gy nu nv l nw nx">rf_importances = get_feature_importance(rf)</span></pre><figure class="iw ix iy iz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi og"><img src="../Images/ecd1751e92c74ba2eb2f688b02ed35ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1aVRTJjIgGCQx0LGq4tW3A.png"/></div></div></figure><p id="db69" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最不重要的特征是不同子数据的指示变量。由于该数据仅包括五个最受欢迎且相当普通的子主题(食品、世界新闻、电影、科学和游戏)的评论，我们不认为这些功能非常重要。此外，还有许多不太重要或不重要的评论。这些特征可以被移除。这有助于避免过度拟合，并减少训练模型所需的时间。</p><p id="fd19" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">五个最重要的特性是描述评论所在线程或评论父线程的特性。我们可能会预料到这一点，因为受欢迎和有趋势的内容会显示给更多的用户，所以接近有很多支持票的内容的评论也更有可能获得很多支持票。</p><p id="ac26" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">还需要注意的是，许多非常重要的特性都缺少值。因此，对缺失值的处理方式进行更深入的分析可以提高模型的性能(例如，当我们删除评论树根时，parent score 是最重要的特性，大约占 25%)。使用平均值、中值的插值或使用简单线性回归的预测也值得测试。</p><figure class="iw ix iy iz gt is gh gi paragraph-image"><div class="gh gi iv"><img src="../Images/8415b193b4895242041327534cb97f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/format:webp/1*t_Fs2xZ24rw1GHP6oRL0qg.png"/></div></figure><h1 id="dad9" class="ln lo jc bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">结论</h1><p id="3d1a" class="pw-post-body-paragraph jz ka jc kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw ij bi translated">在本文中，我们概述了一个使用 scikit-learn python 库预测 Reddit 评论投票的机器学习工作流。我们比较了线性和非线性回归模型的性能，发现随机森林回归是最佳选择。</p><p id="c33d" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在快速检查了这个模型的残差之后，我们看到了许多改进的空间。该项目的下一步可能包括:</p><ul class=""><li id="9982" class="kx ky jc kb b kc kd kg kh kk kz ko la ks lb kw lc ld le lf bi translated">使用较少的特征拟合模型，并将它们的性能与原始模型进行比较</li><li id="9b57" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated">分析缺失值及其对模型性能的影响</li><li id="5b86" class="kx ky jc kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated">堆叠模型以提高性能</li></ul><p id="d9df" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">本文基于一个项目，该项目最初由 Adam Reevesman、Gokul Krishna Guruswamy、乐海、Maximillian Alfaro 和 Prakhar Agrawal 在旧金山大学数据科学硕士的<em class="ll">机器学习入门</em>课程中完成。相关的工作可以在<a class="ae lm" href="https://github.com/areevesman/reddit-upvote-modeling" rel="noopener ugc nofollow" target="_blank">这个</a> Github 仓库中找到，本文的代码可以在<a class="ae lm" href="https://github.com/areevesman/reddit-upvote-modeling/blob/master/model_fitting/associated_nb_for_medium.ipynb" rel="noopener ugc nofollow" target="_blank">这个</a>笔记本中找到。</p><p id="e4fd" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我将很高兴收到以上任何反馈。在 areevesman@gmail.com，你可以通过 LinkedIn 或者电子邮件找到我。</p><figure class="iw ix iy iz gt is gh gi paragraph-image"><div class="gh gi iv"><img src="../Images/8415b193b4895242041327534cb97f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/format:webp/1*t_Fs2xZ24rw1GHP6oRL0qg.png"/></div></figure></div></div>    
</body>
</html>