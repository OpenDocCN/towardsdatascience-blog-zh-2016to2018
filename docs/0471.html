<html>
<head>
<title>Simple and Multiple Linear Regression in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的简单和多元线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9?source=collection_archive---------0-----------------------#2017-05-08">https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9?source=collection_archive---------0-----------------------#2017-05-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="9341" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">Python中线性回归的快速介绍</strong></p><p id="2f8b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">大家好！在<a class="ae ko" href="https://medium.com/@adi.bronshtein/a-quick-introduction-to-the-pandas-python-library-f1b678f34673" rel="noopener">简单介绍了“熊猫”库</a>和<a class="ae ko" href="https://medium.com/@adi.bronshtein/a-quick-introduction-to-the-numpy-library-6f61b7dee4db" rel="noopener"> NumPy库</a>之后，我想提供一个用Python构建模型的快速介绍，还有什么比最基本的模型之一线性回归更好的起点呢？这将是关于机器学习的第一篇文章，我计划在未来写一些更复杂的模型。敬请期待！但是现在，让我们关注线性回归。</p><p id="60f6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇博文中，我想重点介绍线性回归的概念，以及它在Python中的实现。<a class="ae ko" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank">线性回归</a>是一种统计模型，用于检验两个(简单线性回归)或多个(多元线性回归)变量(因变量和自变量)之间的线性关系。线性关系基本上意味着当一个(或多个)自变量增加(或减少)时，因变量也增加(或减少):</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/1dbf80cb25b95cf5a2158f15706d1fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*N2usf10aKCq1JBIqxj-YFQ.png"/></div></figure><p id="dd27" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如你所见，线性关系可以是正的(自变量上升，因变量上升)，也可以是负的(自变量上升，因变量下降)。正如我所说的，我将重点关注回归模型在Python中的实现，所以我不想过多地钻研回归背后的数学，但我会写一点关于它的内容。如果你想写一篇关于这方面的博文，请不要犹豫给我回信！</p></div><div class="ab cl kx ky hx kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="im in io ip iq"><h2 id="e069" class="le lf it bd lg lh li dn lj lk ll dp lm kb ln lo lp kf lq lr ls kj lt lu lv lw bi translated">稍微了解一下数学</h2><p id="a61e" class="pw-post-body-paragraph jq jr it js b jt lx jv jw jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn im bi translated">变量Y和X之间的关系由以下等式表示:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="cf4c" class="le lf it md b gy mh mi l mj mk"><strong class="md iu">Y`i = mX + b</strong></span></pre><p id="406a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个等式中，<em class="ml"> Y </em>是因变量——或者是我们试图预测或估计的变量；x是独立变量——我们用来做预测的变量；m是回归线的斜率——它代表了<em class="ml"> X </em>对<em class="ml"> Y </em>的影响。换句话说，如果<em class="ml"> X </em>增加1个单位，<em class="ml"> Y </em>将增加正好<em class="ml"> m </em>个单位。(<strong class="js iu">“全公开”</strong>:只有当我们知道<em class="ml"> X </em>和<em class="ml"> Y </em>有线性关系时才成立。在几乎所有的线性回归情况下，这都不会是真的！)<em class="ml"> b </em>是常数，也称为Y截距。如果X等于<em class="ml"> 0，Y </em>将等于<em class="ml"> b </em> ( <strong class="js iu">警告</strong>):参见之前的完整披露！).这在现实生活中不一定适用——我们不会总是知道<em class="ml"> X </em>和<em class="ml"> Y </em>之间的确切关系或者有一个确切的线性关系。</p><p id="04b4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些警告将我们引向一个简单的线性回归。在SLR模型中，我们基于数据构建模型——斜率和Y轴截距来自数据；此外，我们不需要<em class="ml"> X </em>和<em class="ml"> Y </em>之间的关系是完全线性的。SLR模型还包括数据中的误差(也称为残差)。我现在不会过多地讨论它，也许在以后的文章中，但残差基本上是Y的真实值和Y的预测/估计值之间的差异。需要注意的是，在线性回归中，我们试图预测一个连续变量。在回归模型中，我们试图通过找到“最佳拟合线”来最小化这些误差——误差的回归线将是最小的。我们试图将黑线的长度(或者更准确地说，是蓝点与红线的距离)最小化，尽可能接近零。它与最小化<a class="ae ko" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">均方误差(MSE) </a>或<a class="ae ko" href="https://en.wikipedia.org/wiki/Residual_sum_of_squares" rel="noopener ugc nofollow" target="_blank">误差平方和(SSE) </a>相关(或等效)，也称为“残差平方和”。(RSS)但是这可能超出了这篇博客的范围:-)</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/4a86d502a174353ee0cb767f000702fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A71zTD6_QqUzLhMKj1Rgiw.png"/></div></div></figure><p id="71dc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在大多数情况下，我们会有不止一个自变量——我们会有多个变量；它可以小到两个独立变量，大到数百个(理论上甚至数千个)变量。在这些情况下，我们将使用多元线性回归模型(MLR)。回归方程与简单回归方程非常相似，只是变量更多:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="8e56" class="le lf it md b gy mh mi l mj mk"><strong class="md iu">Y’i = b0 + b1X1i + b2X2i</strong></span></pre><p id="adf8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇文章的数学部分到此结束:)准备好用Python实现它了吗？</p></div><div class="ab cl kx ky hx kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="im in io ip iq"><h1 id="4502" class="mr lf it bd lg ms mt mu lj mv mw mx lm my mz na lp nb nc nd ls ne nf ng lv nh bi translated">Python中的线性回归</h1><p id="df3a" class="pw-post-body-paragraph jq jr it js b jt lx jv jw jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn im bi translated">在Python中有两种主要的方法来执行线性回归——使用<a class="ae ko" href="http://www.statsmodels.org/stable/regression.html" rel="noopener ugc nofollow" target="_blank"> Statsmodels </a>和<a class="ae ko" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>。使用<a class="ae ko" href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.linregress.html" rel="noopener ugc nofollow" target="_blank"> Scipy库</a>也是可能的，但是我觉得这不如我提到的另外两个库那么常见。让我们来研究一下在这两种情况下进行线性回归:</p><h2 id="75ad" class="le lf it bd lg lh li dn lj lk ll dp lm kb ln lo lp kf lq lr ls kj lt lu lv lw bi translated">统计模型中的线性回归</h2><p id="8dbf" class="pw-post-body-paragraph jq jr it js b jt lx jv jw jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn im bi translated"><a class="ae ko" href="http://www.statsmodels.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Statsmodels </a>是“一个Python模块，它为许多不同的统计模型的估计，以及进行统计测试和统计数据探索提供了类和函数。”(来自文档)</p><p id="0171" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与<a class="ae ko" href="https://medium.com/@adi.bronshtein/a-quick-introduction-to-the-pandas-python-library-f1b678f34673" rel="noopener"> Pandas </a>和<a class="ae ko" href="https://medium.com/@adi.bronshtein/a-quick-introduction-to-the-numpy-library-6f61b7dee4db" rel="noopener"> NumPy </a>一样，获取或安装Statsmodels的最简单方法是通过<a class="ae ko" href="https://www.continuum.io/downloads" rel="noopener ugc nofollow" target="_blank"> Anaconda包</a>。如果出于某种原因，你有兴趣以另一种方式安装，请查看<a class="ae ko" href="http://www.statsmodels.org/stable/install.html" rel="noopener ugc nofollow" target="_blank">此链接</a>。安装后，每次需要使用它时，您都需要导入它:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="b3b0" class="le lf it md b gy mh mi l mj mk"><strong class="md iu">import statsmodels.api as sm</strong></span></pre><p id="2318" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看看如何实际使用Statsmodels进行线性回归。我将使用我在DC 大会<a class="ae ko" href="https://generalassemb.ly/locations/washington-dc/downtown-dc" rel="noopener ugc nofollow" target="_blank">上的</a><a class="ae ko" href="https://generalassemb.ly/education/data-science-immersive" rel="noopener ugc nofollow" target="_blank">数据科学课</a>中的一个例子:</p><p id="391f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，我们从sklearn (我提到的另一个库)导入一个<a class="ae ko" href="http://scikit-learn.org/stable/datasets/" rel="noopener ugc nofollow" target="_blank">数据集:</a></p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="6127" class="le lf it md b gy mh mi l mj mk"><strong class="md iu">from</strong> <strong class="md iu">sklearn</strong> <strong class="md iu">import</strong> datasets <em class="ml">## imports datasets from scikit-learn</em><br/>data = datasets.load_boston() <em class="ml">## loads Boston dataset from datasets library </em></span></pre><p id="0dd8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是波士顿房价的数据集(链接到描述)。因为它是为测试和学习机器学习工具而指定的数据集，所以它附带了数据集的描述，我们可以通过使用命令<strong class="js iu">打印</strong>数据来查看它。DESCR(这只适用于sklearn数据集，而不是每个数据集！虽然会很酷…)。为了更好地理解变量，我添加了描述的开头:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="5b3e" class="le lf it md b gy mh mi l mj mk">Boston House Prices dataset<br/>===========================<br/><br/>Notes<br/>------<br/>Data Set Characteristics:  <br/><br/>    :Number of Instances: 506 <br/><br/>    :Number of Attributes: 13 numeric/categorical predictive<br/>    <br/>    :Median Value (attribute 14) is usually the target<br/><br/>    :Attribute Information (in order):<br/>        - CRIM     per capita crime rate by town<br/>        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.<br/>        - INDUS    proportion of non-retail business acres per town<br/>        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)<br/>        - NOX      nitric oxides concentration (parts per 10 million)<br/>        - RM       average number of rooms per dwelling<br/>        - AGE      proportion of owner-occupied units built prior to 1940<br/>        - DIS      weighted distances to five Boston employment centres<br/>        - RAD      index of accessibility to radial highways<br/>        - TAX      full-value property-tax rate per $10,000<br/>        - PTRATIO  pupil-teacher ratio by town<br/>        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town<br/>        - LSTAT    % lower status of the population<br/>        - MEDV     Median value of owner-occupied homes in $1000's<br/><br/>    :Missing Attribute Values: None<br/><br/>    :Creator: Harrison, D. and Rubinfeld, D.L.<br/><br/>This is a copy of UCI ML housing dataset.<br/>http://archive.ics.uci.edu/ml/datasets/Housing<br/><br/><br/>This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.</span></pre><p id="2b88" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">运行<em class="ml"> data.feature_names </em>和<em class="ml"> data.target </em>将分别打印自变量和因变量的列名。也就是说，Scikit-learn已经将房价数据设置为目标变量，并将13个其他变量设置为预测变量。让我们看看如何对这个数据集进行线性回归。</p><p id="6de5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，我们应该将数据加载为pandas数据框以便于分析，并将房屋价值中值设置为我们的目标变量:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="31a3" class="le lf it md b gy mh mi l mj mk"><strong class="md iu">import</strong> <strong class="md iu">numpy</strong> <strong class="md iu">as</strong> <strong class="md iu">np</strong><br/><strong class="md iu">import</strong> <strong class="md iu">pandas</strong> <strong class="md iu">as</strong> <strong class="md iu">pd</strong></span><span id="139c" class="le lf it md b gy ni mi l mj mk"># define the data/predictors as the pre-set feature names  <br/>df = pd.DataFrame(data.data, columns=data.feature_names)<br/><br/><em class="ml"># Put the target (housing value -- MEDV) in another DataFrame</em><br/>target = pd.DataFrame(data.target, columns=["MEDV"])</span></pre><p id="040a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们在这里所做的是获取数据集并将其作为熊猫数据框加载；之后，我们设置预测值(如df)，即数据集中预设的独立变量。我们也在设定目标——因变量，或者我们试图预测/估计的变量。</p><p id="9f6f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们要拟合一个线性回归模型。我们需要选择我们认为能很好预测因变量的变量——这可以通过检查变量之间的相关性，通过绘制数据并直观地搜索关系，通过对哪些变量能很好预测y等进行初步研究来完成。对于第一个例子，让我们以RM——房间的平均数量和LSTAT——较低地位人口的百分比为例。需要注意的是，Statsmodels默认情况下不添加常量。让我们先看看回归模型中没有常数的情况:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="630d" class="le lf it md b gy mh mi l mj mk"><em class="ml">## Without a constant</em><br/><br/><strong class="md iu">import</strong> <strong class="md iu">statsmodels.api</strong> <strong class="md iu">as</strong> <strong class="md iu">sm</strong><br/><br/>X = df["RM"]<br/>y = target["MEDV"]<br/><br/><em class="ml"># Note the difference in argument order</em><br/>model = sm.OLS(y, X).fit()<br/>predictions = model.predict(X) # make the predictions by the model<br/><br/><em class="ml"># Print out the statistics</em><br/>model.summary()</span></pre><p id="9d78" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nj"><img src="../Images/6d38af6fdda5fa8d8c383673c9527b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XTFRhbi48lb3Up1SGCo_ug.png"/></div></div></figure><p id="e41d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">解读桌子这是一张很长的桌子，不是吗？首先我们有因变量，模型和方法。<strong class="js iu"> OLS </strong>代表<a class="ae ko" href="https://en.wikipedia.org/wiki/Ordinary_least_squares" rel="noopener ugc nofollow" target="_blank">普通最小二乘法</a>，方法“最小二乘法”意味着我们试图拟合一条回归线，这将最小化到回归线的距离的平方(见本文的前一部分)。日期和时间是不言自明的:)观察次数也是如此。残差和模型的Df与<a class="ae ko" href="https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)" rel="noopener ugc nofollow" target="_blank">自由度</a>相关——“统计数据最终计算中可自由变化的值的数量。”</p><p id="8af5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">系数3.6534意味着<em class="ml"> RM </em>变量增加1，<em class="ml"> MDEV </em>的预测值增加<em class="ml"> 3.6534 </em>。其他几个重要的值是R平方——我们的模型解释的方差的百分比；标准误差(是统计抽样分布的标准偏差，通常是平均值)；假设检验的t分数和p值——<em class="ml">RM</em>具有统计上显著的p值；<em class="ml"> RM有95%的置信区间(</em>意味着我们以95%的置信度预测<em class="ml"> RM </em>的值在<em class="ml"> 3.548 </em>到<em class="ml"> 3.759 </em>之间)。</p><p id="149f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们想在模型中添加一个常量，我们必须使用命令<code class="fe nk nl nm md b">X = sm.add_constant(X)</code>来设置它，其中X是包含输入(独立)变量的数据框的名称。</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="818a" class="le lf it md b gy mh mi l mj mk"><strong class="md iu">import</strong> <strong class="md iu">statsmodels.api</strong> <strong class="md iu">as</strong> <strong class="md iu">sm # </strong>import statsmodels<strong class="md iu"> </strong><br/><br/>X = df["RM"] <em class="ml">## X usually means our input variables (or independent variables)</em><br/>y = target["MEDV"] <em class="ml">## Y usually means our output/dependent variable</em><br/>X = sm.add_constant(X) <em class="ml">## let's add an intercept (beta_0) to our model</em><br/><br/><em class="ml"># Note the difference in argument order</em><br/>model = sm.OLS(y, X).fit() <em class="ml">## sm.OLS(output, input)</em><br/>predictions = model.predict(X)<br/><br/><em class="ml"># Print out the statistics</em><br/>model.summary()</span></pre><p id="a57e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nn"><img src="../Images/0fe4f734472af93892ac9afb7e1b1d7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X6IV-_exaTqm7y0p5FC2Zg.png"/></div></div></figure><p id="065b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">解读表格</strong> —常数项的系数不同。如果没有常数，我们将强制我们的模型通过原点，但是现在我们有一个y截距在<em class="ml"> -34.67 </em>。我们还将<em class="ml"> RM </em>预测器的斜率从<em class="ml"> 3.634 </em>更改为<em class="ml"> 9.1021 </em>。</p><p id="7d13" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们尝试拟合一个包含多个变量的回归模型——我们将使用我之前提到的RM和LSTAT。模型拟合是相同的:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="dc91" class="le lf it md b gy mh mi l mj mk">X = df[[“RM”, “LSTAT”]]<br/>y = target[“MEDV”]</span><span id="fe13" class="le lf it md b gy ni mi l mj mk">model = sm.OLS(y, X).fit()<br/>predictions = model.predict(X)</span><span id="173c" class="le lf it md b gy ni mi l mj mk">model.summary()</span></pre><p id="a954" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出是:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi no"><img src="../Images/6cd0f54a70e327a02ace9962972db41b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PtOVDktQ-QoClML4RZybaA.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Note: this table looks different because I’ve updated my Jupyter Notebook</figcaption></figure><p id="fa79" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">解释输出</strong> —我们可以看到，该模型的R平方值更高，为0.948，这意味着该模型解释了因变量中94.8%的方差。每当我们向回归模型中添加变量时，R会更高，但这是一个相当高的R。我们可以看到<em class="ml"> RM </em>和<em class="ml"> LSTAT </em>在预测(或估计)中值房屋价值方面具有统计显著性；毫不奇怪，我们看到当<em class="ml"> RM </em>增加<em class="ml"> 1 </em>，<em class="ml"> MEDV </em>将增加4.9069，当<em class="ml"> LSTAT </em>增加<em class="ml"> 1 </em>，<em class="ml"> MEDV </em>将<strong class="js iu">减少</strong>-0.6557。你可能还记得，LSTAT是较低地位人口的百分比，不幸的是，我们可以预计它会降低房屋的中值。同样的逻辑，房子里的房间越多，通常它的价值就会越高。</p><p id="5d7a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是统计模型中一元和多元线性回归的例子。我们可以在回归模型中使用尽可能少或尽可能多的变量——最多13个！接下来，我将演示如何在SKLearn中运行线性回归模型。</p><h2 id="b184" class="le lf it bd lg lh li dn lj lk ll dp lm kb ln lo lp kf lq lr ls kj lt lu lv lw bi translated">SKLearn中的线性回归</h2><p id="ce36" class="pw-post-body-paragraph jq jr it js b jt lx jv jw jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn im bi translated">当谈到Python中的机器学习时，SKLearn几乎是黄金标准。它有许多学习算法，用于回归、分类、聚类和降维。查看<a class="ae ko" href="https://medium.com/@adi.bronshtein/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7" rel="noopener">我关于KNN算法的帖子</a>获得不同算法的地图和更多SKLearn的链接。为了使用线性回归，我们需要导入它:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="caf5" class="le lf it md b gy mh mi l mj mk"><strong class="md iu">from</strong> <strong class="md iu">sklearn</strong> <strong class="md iu">import</strong> linear_model</span></pre><p id="f23c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们使用之前用过的数据集，波士顿房价。该过程在开始时是相同的——从SKLearn导入数据集并加载到波士顿数据集中:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="df84" class="le lf it md b gy mh mi l mj mk"><strong class="md iu">from</strong> <strong class="md iu">sklearn</strong> <strong class="md iu">import</strong> datasets <em class="ml">## imports datasets from scikit-learn</em><br/>data = datasets.load_boston() <em class="ml">## loads Boston dataset from datasets library</em></span></pre><p id="031d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们将数据加载到Pandas(与之前相同):</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="eca8" class="le lf it md b gy mh mi l mj mk"># define the data/predictors as the pre-set feature names  <br/>df = pd.DataFrame(data.data, columns=data.feature_names)<br/><br/><em class="ml"># Put the target (housing value -- MEDV) in another DataFrame</em><br/>target = pd.DataFrame(data.target, columns=["MEDV"])</span></pre><p id="59cb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，和以前一样，我们有包含自变量的数据框(标记为“df”)和包含因变量的数据框(标记为“target”)。让我们用SKLearn拟合一个回归模型。首先，我们将定义X和y，这一次我将使用数据框中的所有变量来预测房价:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="7163" class="le lf it md b gy mh mi l mj mk">X = df<br/>y = target[“MEDV”]</span></pre><p id="4b76" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后我会拟合一个模型:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="0de5" class="le lf it md b gy mh mi l mj mk">lm = linear_model.LinearRegression()<br/>model = lm.fit(X,y)</span></pre><p id="8149" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">lm.fit()函数适合线性模型。我们希望使用模型进行预测(这就是我们在这里的目的！)，所以我们将使用lm.predict():</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="3bd5" class="le lf it md b gy mh mi l mj mk">predictions = lm.predict(X)<br/>print(predictions)[0:5]</span></pre><p id="9673" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">print函数将打印y的前5个预测(为了“节省空间”，我没有打印整个列表。移除[0:5]将打印整个列表):</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="8aac" class="le lf it md b gy mh mi l mj mk">[ 30.00821269  25.0298606   30.5702317   28.60814055  27.94288232]</span></pre><p id="7f4e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">记住，lm.predict()使用我们拟合的线性模型来预测y(因变量)。你一定注意到了，当我们用SKLearn运行线性回归时，我们不会像在Statsmodels中那样得到一个漂亮的表(好吧，它不是那么漂亮…但它相当有用)。我们能做的是使用内置函数返回分数、系数和估计截距。让我们看看它是如何工作的:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="767f" class="le lf it md b gy mh mi l mj mk">lm.score(X,y)</span></pre><p id="a3c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">会给出这样的输出:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="d01b" class="le lf it md b gy mh mi l mj mk">0.7406077428649428</span></pre><p id="097e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是我们模型的R分。你们可能还记得，这是解释预测方差的百分比。如果你有兴趣，<a class="ae ko" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score" rel="noopener ugc nofollow" target="_blank">在这里阅读更多</a>。接下来，让我们检查预测值的系数:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="69df" class="le lf it md b gy mh mi l mj mk">lm.coef_</span></pre><p id="49ad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将给出以下输出:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="7217" class="le lf it md b gy mh mi l mj mk">array([ -1.07170557e-01,   4.63952195e-02,   2.08602395e-02,<br/>         2.68856140e+00,  -1.77957587e+01,   3.80475246e+00,<br/>         7.51061703e-04,  -1.47575880e+00,   3.05655038e-01,<br/>        -1.23293463e-02,  -9.53463555e-01,   9.39251272e-03,<br/>        -5.25466633e-01])</span></pre><p id="baa3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">截距:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="d97e" class="le lf it md b gy mh mi l mj mk">lm.intercept_</span></pre><p id="ca46" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这将给出以下输出:</p><pre class="kq kr ks kt gt mc md me mf aw mg bi"><span id="5970" class="le lf it md b gy mh mi l mj mk">36.491103280363134</span></pre><p id="bd38" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些都是我之前提到的多元回归方程的(估计/预测)部分。查看文档，了解关于coef_和intercept_的更多信息。</p></div><div class="ab cl kx ky hx kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="im in io ip iq"><p id="5a65" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以，这是一个快速(但相当长！)关于如何用Python进行线性回归的介绍。实际上，您不会使用整个数据集，但是您会将数据拆分为用于训练模型的训练数据和用于测试模型/预测的测试数据。如果你想了解它，请看看我的下一篇博文。与此同时，我希望你喜欢这篇文章，我会在下一篇文章中“看到”你。</p><p id="a662" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢您的阅读！</p></div></div>    
</body>
</html>