<html>
<head>
<title>CNN Model Comparison in Udacity’s Driving Simulator</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Udacity驾驶模拟器中的CNN模型比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cnn-model-comparison-in-udacitys-driving-simulator-9261de09b45?source=collection_archive---------0-----------------------#2017-01-25">https://towardsdatascience.com/cnn-model-comparison-in-udacitys-driving-simulator-9261de09b45?source=collection_archive---------0-----------------------#2017-01-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/9b402075500042ebc1104d527f47a294.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c1qUMtzt-M8haEjS8xWujA.png"/></div></div></figure><p id="49cf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于Udacity的自动驾驶汽车Nanodegree项目3，他们创建了一个驾驶模拟器，可用于训练和测试自动驾驶模型。我使用模拟器中车辆的前向图像和转向角度，训练并比较了两个架构差异很大的卷积神经网络(CNN)模型。结果表明，对于这种类型的任务，CNN架构的选择不如所使用的数据和增强技术重要。在结果中可以看到模拟器中模型的并排视频。</p><h1 id="500b" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">方法</h1><p id="4b31" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">我对这个项目的一般方法是使用Udacity提供的训练数据和尽可能多的必要的数据增强技术来产生可以在模拟器中成功绕过不同轨道的模型。我尝试了两种不同的CNN模型:第一种是他们论文中公认的NVIDIA模型:</p><p id="7114" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae lz" href="https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf" rel="noopener ugc nofollow" target="_blank">https://images . NVIDIA . com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px . pdf</a></p><p id="6ced" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第二个是我为Udacity的开源挑战#2“使用深度学习预测转向角度”开发的类似模型:</p><p id="a9e5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae lz" href="https://github.com/chrisgundling/self-driving-car/tree/master/steering-models/community-models/cg23" rel="noopener ugc nofollow" target="_blank">https://github . com/Chris gundling/自驾车/树/主/转向模型/社区模型/cg23 </a></p><p id="c9d7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这些模型非常不同，因为NVIDIA只使用了大约25万个参数，而我尝试的原始VGG风格模型有近3400万个参数。</p><p id="c0e7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我试验了各种数据预处理技术、7种不同的数据扩充方法，并改变了我测试的两个模型中每一个的压差。最后我发现，虽然VGG风格的模型驾驶稍微平稳，但需要更多的超参数调整。NVIDIA的架构在推广到测试轨道(Track2)方面做得更好，付出的努力更少。最终的NVIDIA模型没有使用dropout，并且能够在轨道1和2上行驶。我在CPU上的模拟器中测试时，遇到了较大的VGG风格模型的转向延迟问题。我最终将这个模型的图像尺寸从128X128X3缩小到64X64X3，并将全连接层的尺寸减半。这使得VGG风格的模型可以在赛道1和2上行驶。</p><h1 id="66a2" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">承认</h1><p id="d05d" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">我要感谢Vivek Yadav在这个项目上的精彩帖子。实现的一些数据增强技术是他在这篇文章中描述的想法:</p><p id="34d1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae lz" href="https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9#.yget03t5g" rel="noopener ugc nofollow" target="_blank">https://chatbotslife . com/using-augmentation-to-mimic-human-driving-496 b 569760 a9 # . yget 03 t5g</a></p><p id="4bd7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我还要感谢Udacity的Yousuf Fauzan对旋转透视变换的实现。我在“线外”活动中遇到了优素福，他在Udacity的挑战#2中运用了这一技术。该脚本使用旋转矩阵对图像进行不同程度的透视变换。</p><h1 id="2c6f" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">数据探索</h1><p id="4967" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">Udacity提供的数据由8036个中心、左侧和右侧组成。jpg图片，总数据量为24109个例子(只有323 MB的数据！).这些训练图像都来自轨道1。这些图像宽320×高160。下面的示例显示了从汽车上以一个时间步长拍摄的左/中/右图像。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ma"><img src="../Images/37d583f16cb104a67fec91e3b79d1c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iWH1dNPEBaBSFTRm_rypqw.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Left, Center and Right Camera Images Respectively from the Simulator Vehicle on Track 1</figcaption></figure><p id="fe99" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Udacity数据集驾驶日志中提供的转向角绘制如下。转向角以1/25的因子进行预缩放，因此它们在-1和1之间(模拟器产生的最大/最小转向角为+/- 25度)。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/9fa7425068ecaddf4834ee15da081fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8rvKHMFCNRRnAG0Zwv9DyQ.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Udacity Provided Training Data</figcaption></figure><p id="9da8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我知道模型训练需要一个评估指标，所以我分离出480个中心摄像机图像和它们对应的转向角度，用于验证集。这是一个比我通常使用的验证集更小的验证集，但是我不想放弃太多的训练数据。我考虑实现k-fold交叉验证来开发一个更好的验证误差度量，但是我发现单个验证集与实际驾驶能力相当一致。我没有使用专门的“测试集”来计算均方根误差，而是将第2轨的性能作为我的测试集。</p><h1 id="60a6" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated"><strong class="ak">数据预处理</strong></h1><p id="fbd5" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">我密切关注了关于收集培训数据的Slack的讨论。似乎很多人很难只用他们的电脑键盘来收集“平稳”的驾驶数据。Udacity的数据在一定程度上解决了这个问题，但基于使用Udacity挑战#2中真实人类转向数据的经验，所提供的模拟器转向数据仍然看起来更像是阶跃。</p><p id="9618" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">指数平滑</strong></p><p id="a1d9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我通过对驾驶训练数据应用指数平滑来解决这个问题。指数平滑(布朗的方法)有助于产生更平滑的转向过渡，但也会截断和移动数据。我应用了一个缩放比例来恢复一些转向幅度，并将数据移动了几个时间步长，这样转向就不会被延迟。对于前1200个训练示例，可以在下面看到指数平滑的结果。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mk"><img src="../Images/62dc2e01909bc55804bf837c3debf7e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RqHYWYIzi3gbfCyiGAXBNA.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Exponential Smoothing of Udacity Steering Data</figcaption></figure><p id="a27a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">正常化</strong></p><p id="5207" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我使用等式(x-128)/128将所有图像像素值标准化。这会将值规范化为介于-1和1之间。我这样做是为了训练数据、验证数据，并在模拟器驾驶脚本中实现它来测试模型。</p><p id="a655" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">裁剪</strong></p><p id="8c64" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在增强后，我从每张图像中剪掉了底部的20个像素和顶部的40个像素。这从图像中移除了汽车的前部和地平线以上的大部分天空。</p><p id="3cfd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">调整规模</strong></p><p id="5420" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于NVIDIA型号，我与他们的方法保持一致，使用高度为66像素、宽度为200像素的图像。对于VGG风格的模型，我使用了64高64宽的图片来缩小来自Udacity挑战#2的原始模型。</p><h1 id="265a" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated"><strong class="ak">数据增强</strong></h1><p id="b269" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">使用了七种不同的增强技术来增加模型在训练期间可以看到的图像数量。这大大降低了模型过度拟合数据的趋势。</p><p id="a63d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">增量的另一个主要好处是它们可以模拟恢复。视点变换、图像移位和使用左/右图像都添加了模拟汽车恢复到中心的训练数据，而实际上不必收集任何恢复数据。使用定制的生成器/产量方法在运行中执行扩增。实施的数据扩充技术如下:</p><ol class=""><li id="6787" class="ml mm iq ka b kb kc kf kg kj mn kn mo kr mp kv mq mr ms mt bi translated"><strong class="ka ir"> <em class="mu">透视/视点变换</em> </strong> —与NVIDIA论文中描述的类似，旋转透视变换应用于图像。我必须对透视变换的转向角度调整做一些调整，因为我发现一对一的透视变换角度对转向角度调整太大了。我决定在-80到80度之间均匀地旋转图像透视。然后，我将视角除以200，以调整转向角度。这提供了+/- 0.4单位或10度的最大/最小转向角调节。</li><li id="5af3" class="ml mm iq ka b kb mv kf mw kj mx kn my kr mz kv mq mr ms mt bi translated"><strong class="ka ir"> <em class="mu">图像翻转</em> </strong> —由于训练数据中的左转弯和右转弯不均匀，因此图像翻转对于模型泛化到轨迹2非常重要。当图像翻转时，我也翻转了转向角的符号。</li><li id="04fd" class="ml mm iq ka b kb mv kf mw kj mx kn my kr mz kv mq mr ms mt bi translated"><strong class="ka ir"> <em class="mu">左/右摄像机图像</em> </strong> —我使用了来自汽车的左/右摄像机图像，这立即使训练数据的大小增加了三倍。在仔细检查左/右图像并寻找中心图像的共同特征后，我估计左/右图像从中心摄像机水平偏移了大约60个像素。基于这些信息，我为这些左/右图像选择了+/- 0.25单位或+/- 6.25度的转向角校正。</li><li id="6697" class="ml mm iq ka b kb mv kf mw kj mx kn my kr mz kv mq mr ms mt bi translated"><strong class="ka ir"><em class="mu"/></strong><em class="mu"/>—我对图像进行了水平和垂直移动。我的最大/最小水平和垂直位移是每个方向40像素。我在模型训练时调整了这个值。考虑到我估计左/右图像会偏移60个像素，我对最大水平偏移应用了稍微小一点的转向角校正。垂直移动没有转向角校正。</li><li id="216a" class="ml mm iq ka b kb mv kf mw kj mx kn my kr mz kv mq mr ms mt bi translated"><strong class="ka ir"> <em class="mu">图像亮度</em> </strong> —我通过转换到HSV色彩空间和将V像素值从0.5缩放到1.1来调整图像的亮度。这主要是为了帮助推广到轨迹2，那里的图像通常更暗。</li><li id="7cba" class="ml mm iq ka b kb mv kf mw kj mx kn my kr mz kv mq mr ms mt bi translated"><strong class="ka ir"> <em class="mu">图像模糊</em> </strong> —我不确定这种技术对模拟器有多大用处，但这种技术应该有助于模型在使用更多“真实世界”类型的数据时进行归纳，这些数据有时会出现模糊。我使用了可变高斯平滑来模糊图像。</li><li id="04a9" class="ml mm iq ka b kb mv kf mw kj mx kn my kr mz kv mq mr ms mt bi translated"><strong class="ka ir"> <em class="mu">图像旋转</em></strong><em class="mu">——</em>与透视变换不同，我对图像进行了小旋转，以模拟相机的抖动。同样，不确定这对模拟器有多大用处，但对自动驾驶汽车上的真实摄像头会有用。</li></ol><p id="8371" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于视点变换、图像偏移和左/右图像，我还尝试实现了基于速度的转向角校正。我的直觉是，在更高的速度下，转向校正应该更小或更平缓。我惊讶地发现，我不能让这个工作，以及有一个恒定的转向角校正。随着进一步的调整和对左/右相机图像位置的更好的了解，我认为这个方法会起作用。</p><p id="aab1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">基于速度的转向调整是通过定义汽车返回中心的2秒响应时间来实现的。随着车速的增加，在2秒钟内返回中心所需的转向角减小。下图显示了转向角校正的计算方法:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/8b755cc96e4f90d03e4335c49986f7eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*E1WgDbex5tnIKHUwPxTjnw.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Speed Based Steering Adjustment</figcaption></figure><h1 id="717d" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">数据生成程序</h1><p id="b3b8" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">所实现的数据生成器在左/中/右图像之间随机选择，并且还随机选择要应用的增强技术。我发现仅提供增强的训练数据不如用非增强的原始图像和增强的图像的组合来训练模型有效。</p><p id="80e5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先用较大的转弯训练模型，然后让较小转弯的数据慢慢漏入训练中。这个想法直接归功于其他几个在Medium上发布这个想法的学生。如果该模型最初是用低转向角训练的，它将偏向于更直的驾驶，我发现它在最尖锐的弯道中表现不佳。使用数据生成器，下图显示了数据生成器在训练运行期间为前30个训练示例生成的图像和增强选项。这些图像已经被裁剪和调整大小。图像标题如下所示:</p><ul class=""><li id="f321" class="ml mm iq ka b kb kc kf kg kj mn kn mo kr mp kv nb mr ms mt bi translated">ang:图像的转向角度标签</li><li id="2a7c" class="ml mm iq ka b kb mv kf mw kj mx kn my kr mz kv nb mr ms mt bi translated">凸轮:相机选择(左/中/右)</li><li id="6274" class="ml mm iq ka b kb mv kf mw kj mx kn my kr mz kv nb mr ms mt bi translated">8月:1是不增加，2是增加</li><li id="aced" class="ml mm iq ka b kb mv kf mw kj mx kn my kr mz kv nb mr ms mt bi translated">opt:数据扩充选项为:<em class="mu"> 1。翻转、抖动、模糊、亮度</em>、<em class="mu"> 2。移动图像和3。旋转视点变换</em></li></ul><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/e78409ed4e6421e8f7f669343898a264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-hEnuzYGhWn26F735XOdgQ.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Image Augmentation Examples</figcaption></figure><h1 id="8750" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">模型设置和超级参数</h1><p id="5578" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">我的目标是训练这两个模型中的每一个。NVIDIA type和2。具有尽可能多的相似超参数的VGG类型。我使用以下参数来训练这两个模型。</p><ul class=""><li id="9925" class="ml mm iq ka b kb kc kf kg kj mn kn mo kr mp kv nb mr ms mt bi translated">最大历元数— 8 (5或6个历元的训练通常为NVIDIA提供最佳模型，而VGG风格只有1-3个历元)</li><li id="e2ce" class="ml mm iq ka b kb mv kf mw kj mx kn my kr mz kv nb mr ms mt bi translated">每个时期的样本数— 23040</li><li id="83d1" class="ml mm iq ka b kb mv kf mw kj mx kn my kr mz kv nb mr ms mt bi translated">批量大小— 64</li><li id="3933" class="ml mm iq ka b kb mv kf mw kj mx kn my kr mz kv nb mr ms mt bi translated">优化器—学习率为1e-4的Adam</li><li id="b429" class="ml mm iq ka b kb mv kf mw kj mx kn my kr mz kv nb mr ms mt bi translated">激活—VGG风格的Relu和NVIDIA型号的elu</li></ul><p id="62a1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在Keras中实现的NVIDIA模型如下所示:</p><pre class="mb mc md me gt nd ne nf ng aw nh bi"><span id="7ea1" class="ni kx iq ne b gy nj nk l nl nm"># Layer 1</span><span id="295d" class="ni kx iq ne b gy nn nk l nl nm">x = Convolution2D(24, 5, 5, activation=’elu’, subsample=(2, 2), border_mode=’valid’, init=’he_normal’)(img_input)</span><span id="42b5" class="ni kx iq ne b gy nn nk l nl nm"># Layer 2</span><span id="4851" class="ni kx iq ne b gy nn nk l nl nm">x = Convolution2D(36, 5, 5, activation=’elu’, subsample=(2, 2), border_mode=’valid’, init=’he_normal’)(x)</span><span id="a471" class="ni kx iq ne b gy nn nk l nl nm"># Layer 3</span><span id="5cca" class="ni kx iq ne b gy nn nk l nl nm">x = Convolution2D(48, 5, 5, activation=’elu’, subsample=(2, 2), border_mode=’valid’, init=’he_normal’)(x)</span><span id="37e5" class="ni kx iq ne b gy nn nk l nl nm"># Layer 4</span><span id="c8a0" class="ni kx iq ne b gy nn nk l nl nm">x = Convolution2D(64, 3, 3, activation=’elu’, subsample=(1, 1), border_mode=’valid’, init=’he_normal’)(x)</span><span id="65b2" class="ni kx iq ne b gy nn nk l nl nm"># Layer 5</span><span id="29e6" class="ni kx iq ne b gy nn nk l nl nm">x = Convolution2D(64, 3, 3, activation=’elu’, subsample=(1, 1), border_mode=’valid’, init=’he_normal’)(x)</span><span id="1d26" class="ni kx iq ne b gy nn nk l nl nm"># Flatten</span><span id="f41f" class="ni kx iq ne b gy nn nk l nl nm">y = Flatten()(x)</span><span id="8af8" class="ni kx iq ne b gy nn nk l nl nm"># FC 1</span><span id="3f19" class="ni kx iq ne b gy nn nk l nl nm">y = Dense(100, activation=’elu’, init=’he_normal’)(y)</span><span id="9061" class="ni kx iq ne b gy nn nk l nl nm"># FC 2</span><span id="626b" class="ni kx iq ne b gy nn nk l nl nm">y = Dense(50, activation=’elu’, init=’he_normal’)(y)</span><span id="e54a" class="ni kx iq ne b gy nn nk l nl nm"># FC 3</span><span id="8c40" class="ni kx iq ne b gy nn nk l nl nm">y = Dense(10, activation=’elu’, init=’he_normal’)(y)</span><span id="4cae" class="ni kx iq ne b gy nn nk l nl nm"># Output Layer</span><span id="25cc" class="ni kx iq ne b gy nn nk l nl nm">y = Dense(1, init=’he_normal’)(y)</span><span id="7df1" class="ni kx iq ne b gy nn nk l nl nm">model = Model(input=img_input, output=y)</span><span id="7942" class="ni kx iq ne b gy nn nk l nl nm">model.compile(optimizer=Adam(lr=1e-4), loss = ‘mse’)</span></pre><p id="6f52" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">模型建筑</strong></p><p id="03d2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这两个模型的模型架构如下所示。如简介中所述，这些模型的主要调整参数是压差。对于NVIDIA型号，令人有些惊讶的是，该型号在轨道1和2上都表现最佳，没有掉线。任何掉线都会导致赛车在弯道中转向不够有力。对于VGG型模型，最后一个conv层和全连接层中的一些丢失提高了性能。</p><p id="0a17" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="mu"> NVIDIA型号结构和参数</em></p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/68fb1c1c1cd44a7cfbc5c23cfad9f47c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KIDk9E0-hQ61Wes3fMCZyw.png"/></div></div></figure><p id="b901" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这使得每个图像向前传递0.6 MB，向后传递1.2 MB。使用64的批处理大小，在向后传递期间最大内存使用量将是75 MB。</p><p id="81b8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="mu"> VGG型号结构及参数</em></p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/63560a032dd506479f0f530461c2933e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ugf5sd4VJSR9buJFEKT03A.png"/></div></div></figure><p id="a478" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这使得每个图像向前传递1.2 MB(约0.3MB * 4字节)，向后传递2.4 MB。使用64的批处理大小，在向后传递期间最大内存使用量将是150 MB。将结构和参数与NVIDIA的模型进行比较，在近430万个参数中，该模型的参数明显多于NVIDIA的。</p><h1 id="b010" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">结果</h1><p id="86b4" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">以下视频显示了路线1和路线2的两种型号的并排比较。虽然VGG风格的车型似乎驾驶稍微顺畅，但英伟达的车型在赛道2上表现更好。我在使用VGG风格模型过度拟合轨道1时遇到了麻烦，驾驶行为高度依赖于模型训练的下降和时代数。</p><p id="b6c2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">以0.2节气门开度在轨道1上行驶</strong></p><figure class="mb mc md me gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Track 1 Driving — <em class="ns">NVIDIA Model on Left and VGG Style Model on Right</em></figcaption></figure><p id="73a7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">在0.3节气门开度的轨道2上行驶</strong></p><figure class="mb mc md me gt jr"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Track 2 Driving — <em class="ns">NVIDIA Model on Left and VGG Style Model on Right</em></figcaption></figure><h1 id="32fe" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">结论</h1><p id="1da1" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">自从Udacity首次发布Nanodegree Term 1项目以来，我最期待的就是参与这个项目。由于在道路上收集高质量和高数量的真实世界数据存在困难，因此能够在模拟器中用看似无限的数据训练这些模型可能会在自动驾驶方面取得重大突破。正如在这个项目中所看到的，即使使用相对少量的数据和相当简单的CNN模型，车辆也能够成功地通过这两个过程。</p><p id="298e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我期待着继续这项工作，并一直致力于实施RNN/LSTM和强化学习方法来控制转向，油门和刹车。我也很好奇使用真实世界和模拟器数据的组合来训练这些模型，以了解如何在模拟器中训练的模型可以推广到真实世界，反之亦然。</p><h1 id="8ea2" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">硬件/测试设置</h1><p id="3df1" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">我使用Udacity的CarND AWS AMI的修改版和g 2.2x大GPU来训练这些模型，并在我带CPU的Macbook Pro笔记本电脑上测试这些模型。模拟器被设置为“最快”的设置，屏幕分辨率为640 X 480。</p><h1 id="637a" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">密码</h1><p id="789e" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">这个项目的所有代码都是用Python实现的，可以在我的github档案中找到:</p><p id="965b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">【https://github.com/chrisgundling T4】</p></div></div>    
</body>
</html>