<html>
<head>
<title>Building a Translation System In Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在几分钟内建立一个翻译系统</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-translation-system-in-minutes-d82a154f603e?source=collection_archive---------1-----------------------#2017-10-12">https://towardsdatascience.com/building-a-translation-system-in-minutes-d82a154f603e?source=collection_archive---------1-----------------------#2017-10-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2bdf" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用OpenNMT-py创建基线NMT模型</h2></div><p id="8f66" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">序列到序列(seq2seq)[1]是一种通用的结构，能够做许多事情(语言翻译、文本摘要[2]、视频字幕[3]等)。).关于seq2seq的简短介绍，下面是一些不错的帖子:<a class="ae lb" href="https://medium.com/towards-data-science/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d" rel="noopener">【4】</a><a class="ae lb" href="https://medium.com/@devnag/seq2seq-the-clown-car-of-deep-learning-f88e1204dac3" rel="noopener">【5】</a>。</p><p id="4102" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">肖恩·罗伯逊的<a class="ae lb" href="https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb" rel="noopener ugc nofollow" target="_blank">教程笔记本【6】</a>和杰瑞米·霍华德的讲座<a class="ae lb" href="http://course.fast.ai/lessons/lesson12.html" rel="noopener ugc nofollow" target="_blank">【6】</a><a class="ae lb" href="http://course.fast.ai/lessons/lesson13.html" rel="noopener ugc nofollow" target="_blank">【7】</a>是牢牢掌握seq2seq技术细节的绝佳起点。然而，在处理现实世界的问题时，我会尽量避免自己实现所有这些细节。重新发明轮子通常不是一个好主意，尤其是当你对这个领域非常陌生的时候。我发现OpenNMT项目非常活跃，有很好的文档，并且可以开箱即用:</p><div class="lc ld gp gr le lf"><a href="http://opennmt.net/" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">OpenNMT -开源神经机器翻译</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">OpenNMT是一个工业级的开源(MIT)神经机器翻译系统，利用Torch/ PyTorch…</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">opennmt.net</p></div></div><div class="lo l"><div class="lp l lq lr ls lo lt lu lf"/></div></div></a></div><p id="5a74" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还有一些更通用的框架<a class="ae lb" href="https://github.com/google/seq2seq" rel="noopener ugc nofollow" target="_blank">(例如，[8] </a>)，但是可能需要一些定制来使它适用于您的特定问题。</p><p id="4acf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">OpenNMT有两个官方版本:</p><blockquote class="lv lw lx"><p id="436e" class="kf kg ly kh b ki kj jr kk kl km ju kn lz kp kq kr ma kt ku kv mb kx ky kz la ij bi translated"><a class="ae lb" href="https://github.com/OpenNMT/OpenNMT" rel="noopener ugc nofollow" target="_blank"> OpenNMT-Lua </a>(又名OpenNMT):用<a class="ae lb" href="http://torch.ch" rel="noopener ugc nofollow" target="_blank"> LuaTorch </a>开发的主项目。<br/>优化稳定的代码，适合生产和大规模实验。</p><p id="fc11" class="kf kg ly kh b ki kj jr kk kl km ju kn lz kp kq kr ma kt ku kv mb kx ky kz la ij bi translated"><a class="ae lb" href="https://github.com/OpenNMT/OpenNMT-py" rel="noopener ugc nofollow" target="_blank"> OpenNMT-py </a>:使用<a class="ae lb" href="http://pytorch.org" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>的OpenNMT的轻量版。<br/>最初由脸书人工智能研究团队创建，作为PyTorch的一个样本项目，这个版本更容易扩展，适合于研究目的，但不包括所有功能。</p></blockquote><p id="9e6a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在接下来的部分中，我们将使用PyTorch版本。我们将带您了解使用中等规模的数据集创建一个非常基本的翻译系统所需的步骤。</p><h2 id="aa8d" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">步骤1:获取OpenNMT-py</h2><p id="b015" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">将Github上的OpenNMT-py git存储库克隆到本地文件夹中:</p><div class="lc ld gp gr le lf"><a href="https://github.com/OpenNMT/OpenNMT-py" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">OpenNMT/OpenNMT-py</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">http://opennmt.net/ py torch中的OpenNMT-py -开源神经机器翻译</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">github.com</p></div></div><div class="lo l"><div class="na l lq lr ls lo lt lu lf"/></div></div></a></div><p id="9c5b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您打算以后定制或扩展它，那么您可能希望在Github上派生存储库。自述文件中还建议:</p><blockquote class="lv lw lx"><p id="582e" class="kf kg ly kh b ki kj jr kk kl km ju kn lz kp kq kr ma kt ku kv mb kx ky kz la ij bi translated">Codebase接近稳定的0.1版本。如果你想要稳定的代码，我们目前推荐分叉。</p></blockquote><h2 id="4c8b" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">步骤2:下载数据集</h2><p id="ffc4" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">这里我们将使用来自<a class="ae lb" href="https://challenger.ai/competition/translation/subject?lan=en" rel="noopener ugc nofollow" target="_blank">AI Challenger——英汉机器翻译竞赛</a>的数据集。这是一个拥有1000万对英汉句子的数据集。英语字幕是从英语学习网站和电影字幕中提取的会话英语。从我的了解来看，大部分的翻译都是爱好者提交的，不一定是专业人士。翻译的中文句子由人工注释者检查。</p><div class="lc ld gp gr le lf"><a href="https://challenger.ai/competition/translation/subject?lan=en" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">英汉机器翻译- AI挑战者</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">英汉机器翻译-奖金:30万-提高英汉机器翻译的性能…</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">challenger.ai</p></div></div><div class="lo l"><div class="nb l lq lr ls lo lt lu lf"/></div></div></a></div><p id="5b1c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下载数据集需要注册帐户，可能还需要进行身份验证(不记得后者是否是强制性的)。如果这对你来说是个问题，你可以试试来自WMT17 的<a class="ae lb" href="http://www.statmt.org/wmt17/translation-task.html#download" rel="noopener ugc nofollow" target="_blank">数据集。</a></p><p id="d4e0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">AI Challenger数据集存在一些问题:1 .翻译的质量不一致。2.因为许多句子来自电影字幕，所以翻译通常依赖于上下文(与上一句或下一句相关)。但是，数据集中没有可用的上下文信息。</p><p id="9c17" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看现成的模型在这个数据集上的表现。由于内存限制，我将数据集下采样到<strong class="kh ir">100万个</strong>句子。</p><p id="e854" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(我们假设您将数据集放入OpenNMT根目录下的<em class="ly"> </em>文件夹<strong class="kh ir"> <em class="ly">挑战者</em> </strong> <em class="ly"> </em>。)</p><h2 id="6b43" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">步骤3:将数据集转换为纯文本</h2><p id="352c" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">验证和测试数据集采用XML格式。我们需要将其转换为纯文本文件，其中一行由一个句子组成。一个简单的方法就是使用BeautifulSoup。下面是一段示例代码:</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="67fd" class="mc md iq nh b gy nl nm l nn no">with open(input_file, "r") as f:<br/>    soup = BeautifulSoup(f.read(), "lxml")<br/>    lines = [<br/>      (int(x["id"]), x.text.strip()) for x in soup.findAll("seg")]<br/>    # Ensure the same order<br/>    lines = sorted(lines, key=lambda x: x[0])</span></pre><h2 id="541e" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">第四步:标记英语和汉语句子</h2><p id="965f" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">输入的句子必须用空格分隔的标记来标记。</p><p id="6bfe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于英语来说，有一些标记符可供选择。一个例子是<code class="fe np nq nr nh b">nltk.tokenize.word_tokenize</code>:</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="fbd7" class="mc md iq nh b gy nl nm l nn no">with open(output_file, "w") as f:<br/>    f.write(<br/>        "\n".join([<br/>             " ".join(word_tokenize(l[1]))<br/>             for l in lines<br/>         ])<br/>    )</span></pre><p id="bbcf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它变成“这是一个整洁的一-二。沃克给伯顿。”变成“这是一个整洁的一-二。沃克呼叫伯顿。".</p><p id="b12d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于中文，我们使用最简单的字符级记号化，也就是把每个字符当作一个记号:</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="99e2" class="mc md iq nh b gy nl nm l nn no">with open(output_file, "w") as f:<br/>    f.write(<br/>        "\n".join([<br/>            " ".join([c if c != " " else "&lt;s&gt;" for c in l[1]])<br/>            for l in lines<br/>        ])<br/>    )</span></pre><p id="9389" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">It turns “我就一天24小时都得在她眼皮子底下。” into “我 就 一 天 2 4 小 时 都 得 在 她 眼 皮 子 底 下 。”. (Note because the token are space-separated, we need a special token “&lt;s&gt;” to represent the space characters.)</p><p id="9ccf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(我没有提供第3步和第4步的完整代码，因为这真的是初级Python编程。你应该能够独立完成这些任务。)</p><h2 id="3da2" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">步骤5:预处理数据集</h2><p id="6995" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">只需在根目录下运行以下命令:</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="857a" class="mc md iq nh b gy nl nm l nn no">python preprocess.py -train_src challenger/train.en.sample \<br/>      -train_tg challenger/train.zh.sample \<br/>      -valid_src challenger/valid.en \<br/>      -valid_tgt challenger/valid.zh  \<br/>      -save_data challenger/opennmt -report_every 10000</span></pre><p id="18b5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">预处理脚本将遍历数据集，跟踪标记频率，并构建一个词汇表。我在这里遇到了内存问题，不得不将训练数据集向下采样到一百万行，但是我认为原始数据集经过一些优化后应该适合16GB的内存。</p><h2 id="9447" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">第六步:训练模型</h2><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="a1aa" class="mc md iq nh b gy nl nm l nn no">python train.py -data challenger/opennmt \<br/>    -save_model models/baseline -gpuid 0 \<br/>    -learning_rate 0.001 -opt adam -epochs 20</span></pre><p id="0a0e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它会用你的第一个GPU来训练一个模型。默认的模型结构是:</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="35ac" class="mc md iq nh b gy nl nm l nn no">NMTModel (<br/>  (encoder): RNNEncoder (<br/>    (embeddings): Embeddings (<br/>      (make_embedding): Sequential (<br/>        (emb_luts): Elementwise (<br/>          (0): Embedding(50002, 500, padding_idx=1)<br/>        )<br/>      )<br/>    )<br/>    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)<br/>  )<br/>  (decoder): InputFeedRNNDecoder (<br/>    (embeddings): Embeddings (<br/>      (make_embedding): Sequential (<br/>        (emb_luts): Elementwise (<br/>          (0): Embedding(6370, 500, padding_idx=1)<br/>        )<br/>      )<br/>    )<br/>    (dropout): Dropout (p = 0.3)<br/>    (rnn): StackedLSTM (<br/>      (dropout): Dropout (p = 0.3)<br/>      (layers): ModuleList (<br/>        (0): LSTMCell(1000, 500)<br/>        (1): LSTMCell(500, 500)<br/>      )<br/>    )<br/>    (attn): GlobalAttention (<br/>      (linear_in): Linear (500 -&gt; 500)<br/>      (linear_out): Linear (1000 -&gt; 500)<br/>      (sm): Softmax ()<br/>      (tanh): Tanh ()<br/>    )<br/>  )<br/>  (generator): Sequential (<br/>    (0): Linear (500 -&gt; 6370)<br/>    (1): LogSoftmax ()<br/>  )<br/>)</span></pre><p id="d83d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">源语料库和目标语料库的词汇量分别为50，002和6，370。源词汇明显被截断到5万。目标词汇量相对较小，因为没有那么多常用汉字。</p><h2 id="fa62" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">步骤7:翻译测试/验证句子</h2><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="1ea3" class="mc md iq nh b gy nl nm l nn no">python translate.py \<br/>    -model models/baseline_acc_58.79_ppl_7.51_e14  \<br/>    -src challenger/valid.en -tgt challenger/valid.zh \<br/>    -output challenger/valid_pred.58.79 -gpu 0 -replace_unk</span></pre><p id="7197" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">把<code class="fe np nq nr nh b">models/baseline_acc_58.79_ppl_7.51_e14</code>换成自己的型号。模型命名要明显:<em class="ly">这是一个经过14个历元训练的模型，在验证集</em>上的准确率为58.79，困惑度为7.51。</p><p id="391a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您也可以使用以下公式计算BLEU分数:</p><pre class="nc nd ne nf gt ng nh ni nj aw nk bi"><span id="a749" class="mc md iq nh b gy nl nm l nn no">wget <a class="ae lb" href="https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/generic/multi-bleu.perl" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/generic/multi-bleu.perl</a></span><span id="0bf1" class="mc md iq nh b gy ns nm l nn no">perl multi-bleu.perl challenger/valid.zh \<br/>     &lt; challenger/valid_pred.58.79</span></pre><p id="4af2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在你有一个工作的翻译系统！</p><h2 id="d7ae" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">步骤8:(可选)对输出进行去爆震和转换</h2><p id="b58d" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">如果你想把翻译提交给AI Challenger，你需要把步骤4反过来，然后是步骤3。同样，它们应该很容易实现。</p><h2 id="4787" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">一些例子</h2><p id="8181" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi">English: You knew it in your heart you haven’t washed your hair<br/>Chinese(pred): 你心里清楚你没洗头发<br/>Chinese(gold): 你心里知道你压根就没洗过头</p><p id="10ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">English: I never dreamed that one of my own would be going off to a University, but here I stand,<br/>Chinese(pred): 我从来没梦到过我的一个人会去大学，但是我站在这里，<br/>Chinese(gold): 我从没想过我的孩子会上大学，但我站在这，</p><p id="c9d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">English: We just don’t have time to waste on the wrong man.<br/>Chinese(pred): 我们只是没时间浪费人。<br/>Chinese(gold): 如果找错了人我们可玩不起。</p><p id="38b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以上三个例子，从上到下分别是<em class="ly">语义正确</em>、<em class="ly">部分正确</em>、<em class="ly">完全无法理解</em>。在检查了几个例子后，我发现大多数机器翻译的句子是部分正确的，而且语义正确的句子数量惊人。考虑到我们到目前为止付出的努力如此之少，这个结果还不错。</p><h2 id="c8f9" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">下一步</h2><p id="a3b5" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">如果你提交结果，你应该绕过<strong class="kh ir"> <em class="ly"> .22 </em> </strong> BLEU。目前最高BLEU分数为<strong class="kh ir"> <em class="ly"> .33 </em> </strong>，有很大的提升空间。您可以查看根文件夹中的<code class="fe np nq nr nh b">opts.py</code>,了解更多内置模型参数。或者深入代码库，弄清楚事情是如何工作的，哪里可以改进。</p><p id="6cf0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其他途径包括对中文句子进行分词、添加命名实体识别、使用发音词典<a class="ae lb" href="http://www.speech.cs.cmu.edu/cgi-bin/cmudict" rel="noopener ugc nofollow" target="_blank">【10】</a>对未见过的英文名称进行猜测翻译等。</p><p id="fc16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(2017/10/14更新:如果你使用<a class="ae lb" href="https://github.com/fxsjy/jieba" rel="noopener ugc nofollow" target="_blank"> jieba </a>和jieba.cut默认设置对中文句子进行分词，你会绕过<strong class="kh ir"> <em class="ly"> .20 </em> </strong> BLEU上的公共leaderboad。得分下降的一个可能原因是它的中文词汇量大得多。从输出中的<em class="ly"> &lt; unk &gt; </em>的数量就可以看出来。)</p><h2 id="7e5f" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">参考资料:</h2><ol class=""><li id="1f5c" class="nt nu iq kh b ki mv kl mw ko nv ks nw kw nx la ny nz oa ob bi translated"><a class="ae lb" href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> Sutskever，I .、Vinyals，o .、&amp; Le，Q. V. (2014年)。用神经网络进行序列对序列学习</a>。</li><li id="5965" class="nt nu iq kh b ki oc kl od ko oe ks of kw og la ny nz oa ob bi translated">【纳尔拉帕提，r .】，周，b .【多斯桑托斯，c .】向，B. (2016)。使用序列到序列的RNNs和超越序列的抽象文本摘要。</li><li id="bd3b" class="nt nu iq kh b ki oc kl od ko oe ks of kw og la ny nz oa ob bi translated"><a class="ae lb" href="http://arxiv.org/abs/1505.00487" rel="noopener ugc nofollow" target="_blank"> Venugopalan，s .、Rohrbach，m .、Donahue，j .、Mooney，r .、Darrell，t .、&amp; Saenko，K. (2015)。序列到序列</a></li><li id="d842" class="nt nu iq kh b ki oc kl od ko oe ks of kw og la ny nz oa ob bi translated"><a class="ae lb" href="https://medium.com/towards-data-science/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d" rel="noopener">序列对序列模型:介绍和概念</a></li><li id="3fa8" class="nt nu iq kh b ki oc kl od ko oe ks of kw og la ny nz oa ob bi translated"><a class="ae lb" href="https://medium.com/@devnag/seq2seq-the-clown-car-of-deep-learning-f88e1204dac3" rel="noopener"> seq2seq:深度学习的小丑车</a></li><li id="a739" class="nt nu iq kh b ki oc kl od ko oe ks of kw og la ny nz oa ob bi translated"><a class="ae lb" href="https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb" rel="noopener ugc nofollow" target="_blank">实用PyTorch:用一个序列翻译成序列网络及注意事项</a></li><li id="87c0" class="nt nu iq kh b ki oc kl od ko oe ks of kw og la ny nz oa ob bi translated"><a class="ae lb" href="http://course.fast.ai/lessons/lesson12.html" rel="noopener ugc nofollow" target="_blank">程序员的前沿深度学习，第2部分，第12讲——注意力模型</a></li><li id="de7f" class="nt nu iq kh b ki oc kl od ko oe ks of kw og la ny nz oa ob bi translated"><a class="ae lb" href="http://course.fast.ai/lessons/lesson13.html" rel="noopener ugc nofollow" target="_blank">面向编码人员的前沿深度学习，第2部分，第13讲——神经翻译</a></li><li id="4a9e" class="nt nu iq kh b ki oc kl od ko oe ks of kw og la ny nz oa ob bi translated"><a class="ae lb" href="https://github.com/google/seq2seq" rel="noopener ugc nofollow" target="_blank">Google/seq 2 seq:tensor flow的通用编解码框架</a></li><li id="b062" class="nt nu iq kh b ki oc kl od ko oe ks of kw og la ny nz oa ob bi translated"><a class="ae lb" href="http://www.speech.cs.cmu.edu/cgi-bin/cmudict" rel="noopener ugc nofollow" target="_blank">CMU发音词典</a></li></ol></div></div>    
</body>
</html>