<html>
<head>
<title>Intuition Behind Principal Components</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分背后的直觉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intuition-behind-principal-components-849d3ef9d616?source=collection_archive---------9-----------------------#2018-12-30">https://towardsdatascience.com/intuition-behind-principal-components-849d3ef9d616?source=collection_archive---------9-----------------------#2018-12-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/51719c929d39f13ea17e0f382b7cbbaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kgk7SDrSKeCvdrA4.jpg"/></div></div></figure><blockquote class="jy"><p id="279c" class="jz ka iq bd kb kc kd ke kf kg kh ki dk translated">PCA 一直困扰着我。这是那些我已经记了很多遍，但在我的潜意识中却无法正确理解的概念之一。这是在我完全理解线性代数的细微差别之前。</p></blockquote><p id="d9c4" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ki ij bi translated">如果你理解矩阵的特征值、特征向量和协方差，那么你最终会在这篇博客中找到 PCA 的平静。如果这些概念有点生疏，不要担心，你可以在继续之前快速修改它们<a class="ae lg" href="http://math.mit.edu/linearalgebra/ila0601.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lh">这里</em> </a>和<a class="ae lg" href="http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm" rel="noopener ugc nofollow" target="_blank"> <em class="lh">这里</em> </a> <em class="lh"> </em>。</p><p id="c876" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">假设我们为 5 个不同的个体测量了 9 个不同的变量/属性，并将数据放入一个 9 乘 5 的矩阵中。</p><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/558f1e2c874c4e2c4779a396bb1c3539.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*FwM3mK69K5_sHILcQZSd4w.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Matrix A : 9x5dimensional</figcaption></figure><p id="d838" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">同样假设，我们也对这个数据应用了我们可爱的标准化，因此每一行的平均值为零。所以在上面的矩阵中，均值(行 1)=均值(行 2)..平均值(第 9 行)= 0</p><p id="f961" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">让我们称这个矩阵为 A，我在行中有变量/属性，在列中有观察值。</p><p id="af64" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">现在在我跳到主成分及其解释之前，首先让我们计算 a 的协方差矩阵。</p><p id="4d2d" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">A 的协方差矩阵将是 S = AAt/(n-1 ),其中 At =矩阵 A 的转置，n 是观测值的数量= 5</p><p id="756e" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">现在，为了确保我们对矩阵 S 的维数是正确的，AAt 的维数应该是(9x5)乘以(5x9) = (9x9)</p><p id="e2be" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">因此，基本上，由于我们已经将变量(m=9)放在行中，所以协方差矩阵的维数将是 9x9。</p><p id="e284" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">这是我已经计算过的矩阵 S——</p><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/5bf6895d29d72e8c9e36b11c341730bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*AzO5KzLcsXP0T4gKIm74ZA.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Matrix A : 9x5</figcaption></figure><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lx"><img src="../Images/e6f19c5749668882ef09656626fba01c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5gLV3bH4-Ih1W9Nv01Wc5w.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Matrix B = At (A transpose) : 5x9</figcaption></figure><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ly"><img src="../Images/215c81bf0009fccd45009395ceca206e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6BlXcX-V75J-m2IDlX4cuw.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Covariance Matrix S : AAt/(n-1) : 9x9</figcaption></figure><p id="b961" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">现在，是时候让事情真正热起来，并计算上述矩阵的一些性质</p><p id="c72a" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">S 的特征值个数:</p><p id="492e" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">首先，看起来 9×9 维矩阵 S 的特征值总数等于 9，然而这不是真的。矩阵 AAt 和 AtA 的特征值个数必须相同(证明<a class="ae lg" href="https://math.stackexchange.com/questions/1087064/non-zero-eigenvalues-of-aat-and-ata" rel="noopener ugc nofollow" target="_blank"> <em class="lh">此处</em> </a>)</p><p id="9efd" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">因此，AtA 矩阵将具有 5x5 的维度(检查！)并因此将具有最多 5 个非零特征值，矩阵 AAt 也是如此。</p><p id="263f" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">然而，矩阵 A 的所有列都不是独立的(记得我们将所有行居中，所以 A 的所有列的总和将为零)，因此，矩阵 AAt 的非零特征值将仅为 4。</p><p id="24f1" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">即 S 的特征值的数量:4</p><p id="f76c" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">我们把 A 的 4 个奇异值(奇异值分解)称为:</p><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/28d6cb352d430f9d14c3d661fc283673.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*pO52hOIdcHExOWzAdJPNAw.png"/></div></figure><p id="8ec6" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">因此，S 的特征值= A 的信号值的平方=</p><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/86bfafaa1ee3f8674b07fe8c9fc45b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*D1mDs6_UYN1LIrk6UxtAkg.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Eigenvalues of S</figcaption></figure><p id="daf7" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">上述特征值是直接从矩阵 S 的值通过编程计算出来的。</p><p id="0093" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">注意，S 的特征值之和也等于 S ~1756.7 的对角元素之和</p><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/da8e7e5613cdad77ffdba09df6ddbe02.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*vzbJT5fkIc91S2_R9RH3PQ.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Sum of Eigenvalues of S = 1756.7</figcaption></figure><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mc"><img src="../Images/4d7247c71a9e0e88ad0a4f618f0b45be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ACB27Dz2ncIkDhExa4NAA.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Sum of diagonals elements of S = 1752.91</figcaption></figure><p id="edce" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">现在，让我们也计算矩阵 a 的每一行(属性)的方差。(因为我们对每一行的均值是零，方差只是每个值的平方和除以 4)</p><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi md"><img src="../Images/08a6f31d3a5d9ab517856a5760d93e71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ke00wfyx1FXdUjbolIpbYw.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Last Column is variance of each row of matrix A</figcaption></figure><p id="87b8" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">一旦个体方差在这里，我们可以计算:</p><p id="131f" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">矩阵 A 的属性总方差=所有属性方差之和= <strong class="kl ir"> 1753.39 </strong></p><blockquote class="jy"><p id="726e" class="jz ka iq bd kb kc me mf mg mh mi ki dk translated">请注意，协方差矩阵 S 的特征值之和几乎与原始数据矩阵 a 的总方差之和相同。这是一个重要的性质。</p></blockquote><p id="9afd" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ki ij bi translated">因此，我们的 4 个特征值涵盖了数据中的所有方差，注意第一个特征值是最大的，然后它继续下降。</p><p id="d2a0" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">矩阵 S 的每个特征值与其特征向量相关联。在我们的例子中，我们有 4 个特征向量</p><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/93e2c728bf8388ebbac6b6f250b041d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*kDd6Qa23adJALS4YYXLZsA.png"/></div></figure><p id="a700" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">其中每个特征向量相互垂直，并根据它们各自的特征值计算</p><p id="68bf" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">最大特征值具有特征向量(u1 ),我们称之为第一主分量，因为该主分量能够解释其方向上的最大方差(1753.39 中的 1323.9)</p><p id="40d2" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">第二大特征值具有特征向量(u2 ),该特征向量也称为第二主分量，并解释了(在其垂直于第一主分量方向上的总共 1753.39 个方差中的 397.2 个方差)</p><p id="7076" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">类似地，第三和第四特征向量解释了总方差相对较小的部分，但这是我们停止的地方。如果我们对前两个特征值求和，我们将得到 98%的总方差，仅由两个特征向量解释，这基本上意味着 u1 和 u2 包含关于矩阵 A 中数据的最大信息，因此我们只需要两个向量，而不是 A 的所有列。这也是我们所说的 PCA 中的降维。</p><p id="8467" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated"><em class="lh">上面的例子是有意将 5 维减少到 2 维，然而，很难想象这样的例子</em></p><h2 id="61d5" class="mk ml iq bd mm mn mo dn mp mq mr dp ms ku mt mu mv ky mw mx my lc mz na nb nc bi translated">现在让我们举一个更简单的例子，当我们对这个方法有些熟悉的时候，让它永远在我们的脑海中具体化</h2><p id="9b2e" class="pw-post-body-paragraph kj kk iq kl b km nd ko kp kq ne ks kt ku nf kw kx ky ng la lb lc nh le lf ki ij bi translated">我们有两个属性年龄和身高为 6 人，如下所示矩阵格式[2x6]，年龄和身高行以零为中心。</p><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/ba5494da8e4df7084d2edbb3a851d15a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ortN-XZdVgmNQvS9VHwVRQ.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Matrix A: 2x6 with Mean(row1) = Mean(row2) = 0</figcaption></figure><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/48de7290c89e1d1d68925d1c0d7d4a08.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*bIu7PWdLvfvI1WosqQFtQg.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Plotted points in 2-D</figcaption></figure><p id="5ac8" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">二维数据也可以如上所示绘制，X 为年龄，y 为身高</p><p id="6e01" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">有了矩阵 A 中的数据，我们来计算它的协方差矩阵 S = AAt/(n-1)</p><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/a8f00faeef4fe6c6a8b6aad317c36921.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pbGqFqYYXCjddy8mkLzokw.png"/></div></div></figure><p id="e5a5" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">回想一下，我们的协方差矩阵是 2x2</p><p id="ba6c" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">现在，我们计算 S 的特征值，可以找到 57 和 3(回想一下，S 的特征值之和是 diagnol = 60 上的元素之和，这意味着到目前为止我们是好的。</p><p id="302f" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">现在，我们找到矩阵 A 属性的总方差</p><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nl"><img src="../Images/83655a4baa7099114cabb6e39ec19224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cuk9d14Sz_WkX_oOl_rt-A.png"/></div></div></figure><p id="7a4f" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">所以总方差= 20 + 40 = 60</p><p id="c2b9" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">回想一下矩阵 A 中方差=协方差矩阵 S 的特征值之和= 60</p><p id="17df" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">所以，第一个特征值 57 解释了我们数据中 60 个变量中的 57 个。如果我们找到它的特征向量，它就是向量</p><p id="2766" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">u1 = [0.6 0.8]</p><p id="1505" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">这在理论上可以解释最大方差。</p><p id="6ca8" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">当我们在二维空间中绘制这个向量时，我们会得到红线，它基本上只是二维空间中的点在一维空间中的表示。这里，特征向量 u1 是第一主分量，包含关于矩阵 a 的最大信息。</p><figure class="lo lp lq lr gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/aabe9d951fb2a42201bebef1411e56dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*_ONTP4tIP71kCG18OTfb5A.png"/></div></figure><p id="78fa" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">我希望现在有意义的是如何将一个矩阵简化为一组正交的特征向量，这些特征向量通过它们的特征值来解释总方差的最大分数。并且如果我们有总共 q 个特征向量(或主成分)，那么我们只选择最上面的 r 个特征向量(r<n/></p><p id="39f0" class="pw-post-body-paragraph kj kk iq kl b km li ko kp kq lj ks kt ku lk kw kx ky ll la lb lc lm le lf ki ij bi translated">干杯！</p></div></div>    
</body>
</html>