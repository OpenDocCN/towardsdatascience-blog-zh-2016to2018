<html>
<head>
<title>Another Twitter sentiment analysis with Python-Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">另一个使用 Python 的 Twitter 情感分析——第 2 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-2-333514854913?source=collection_archive---------1-----------------------#2017-12-18">https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-2-333514854913?source=collection_archive---------1-----------------------#2017-12-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/58774c6728b52530a1a257ff9d96c170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kfJsMywM3RBnE2cHSAayig.png"/></div></div></figure><p id="ff3c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这篇博文是我目前在伦敦大会上为我的顶点项目所做的 Twitter 情绪分析项目的第二部分。你可以在这里找到第一部<a class="ae kw" rel="noopener" target="_blank" href="/another-twitter-sentiment-analysis-bb5b01ebad90">。</a></p><h1 id="6421" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">重新清理数据</h1><p id="2c9e" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">在我继续学习 EDA 和数据可视化之前，我对数据清理部分做了一些修改，因为我在上一篇文章中定义的数据清理函数存在错误。</p><p id="af4c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我意识到的第一个问题是，在清理过程中，否定词被分成两部分，当我过滤长度超过一个音节的标记时，撇号后面的“t”消失了。这使得像“不可以”这样的词最终和“可以”一样。对于情感分析来说，这似乎不是一件小事。</p><p id="6ed3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我意识到的第二个问题是，有些 url 链接不是以“http”开头的，有时人们会以“www.websitename.com”的形式粘贴链接。当我将 url 地址正则表达式模式定义为“https？://[A-Za-z0–9。/]+'.这种正则表达式模式的另一个问题是它只检测字母、数字、句点和斜杠。这意味着如果它包含任何其他特殊字符，如“=”、“_”、“~”等，它将无法捕获 url 的一部分。</p><p id="505e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第三个问题是 twitter ID 的 regex 模式。在之前的清理函数中，我将其定义为' @[A-Za-z0–9]+'，但是稍微用<a class="ae kw" href="http://kagan.mactane.org/blog/2009/09/22/what-characters-are-allowed-in-twitter-usernames/comment-page-1/" rel="noopener ugc nofollow" target="_blank">谷歌了一下</a>，我发现 twitter ID 也允许下划线符号作为可以与 ID 一起使用的字符。除下划线符号外，只允许字母和数字字符。</p><p id="b806" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面是更新的数据清理功能。清洗的顺序是</p><ol class=""><li id="3760" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">增加马力</li><li id="a6f4" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">物料清单删除</li><li id="04f1" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">url 地址(' http:'模式)，twitter ID 删除</li><li id="c363" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">url 地址(' www。图案)移除</li><li id="5437" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">小写字母的</li><li id="38ed" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">否定处理</li><li id="b9b5" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">删除数字和特殊字符</li><li id="7755" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">符号化和连接</li></ol><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="bdfd" class="mx ky iq mt b gy my mz l na nb"><strong class="mt ir">import</strong> <strong class="mt ir">pandas</strong> <strong class="mt ir">as</strong> <strong class="mt ir">pd</strong>  <br/><strong class="mt ir">import</strong> <strong class="mt ir">numpy</strong> <strong class="mt ir">as</strong> <strong class="mt ir">np</strong><br/><strong class="mt ir">import</strong> <strong class="mt ir">matplotlib.pyplot</strong> <strong class="mt ir">as</strong> <strong class="mt ir">plt</strong><br/>plt.style.use('fivethirtyeight')<br/><br/>%matplotlib inline<br/>%config InlineBackend.figure_format = 'retina'</span><span id="3ed2" class="mx ky iq mt b gy nc mz l na nb"><strong class="mt ir">import</strong> <strong class="mt ir">re</strong><br/><strong class="mt ir">from</strong> <strong class="mt ir">bs4</strong> <strong class="mt ir">import</strong> BeautifulSoup<br/><strong class="mt ir">from</strong> <strong class="mt ir">nltk.tokenize</strong> <strong class="mt ir">import</strong> WordPunctTokenizer<br/>tok = WordPunctTokenizer()<br/><br/>pat1 = r'@[A-Za-z0-9_]+'<br/>pat2 = r'https?://[^ ]+'<br/>combined_pat = r'|'.join((pat1, pat2))<br/>www_pat = r'www.[^ ]+'<br/>negations_dic = {"isn't":"is not", "aren't":"are not", "wasn't":"was not", "weren't":"were not",<br/>                "haven't":"have not","hasn't":"has not","hadn't":"had not","won't":"will not",<br/>                "wouldn't":"would not", "don't":"do not", "doesn't":"does not","didn't":"did not",<br/>                "can't":"can not","couldn't":"could not","shouldn't":"should not","mightn't":"might not",<br/>                "mustn't":"must not"}<br/>neg_pattern = re.compile(r'\b(' + '|'.join(negations_dic.keys()) + r')\b')<br/><br/><strong class="mt ir">def</strong> tweet_cleaner_updated(text):<br/>    soup = BeautifulSoup(text, 'lxml')<br/>    souped = soup.get_text()<br/>    <strong class="mt ir">try</strong>:<br/>        bom_removed = souped.decode("utf-8-sig").replace(u"<strong class="mt ir">\ufffd</strong>", "?")<br/>    <strong class="mt ir">except</strong>:<br/>        bom_removed = souped<br/>    stripped = re.sub(combined_pat, '', bom_removed)<br/>    stripped = re.sub(www_pat, '', stripped)<br/>    lower_case = stripped.lower()<br/>    neg_handled = neg_pattern.sub(<strong class="mt ir">lambda</strong> x: negations_dic[x.group()], lower_case)<br/>    letters_only = re.sub("[^a-zA-Z]", " ", neg_handled)<br/>    <em class="nd"># During the letters_only process two lines above, it has created unnecessay white spaces,</em><br/>    <em class="nd"># I will tokenize and join together to remove unneccessary white spaces</em><br/>    words = [x <strong class="mt ir">for</strong> x  <strong class="mt ir">in</strong> tok.tokenize(letters_only) <strong class="mt ir">if</strong> len(x) &gt; 1]<br/>    <strong class="mt ir">return</strong> (" ".join(words)).strip()</span></pre><p id="29d3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">更新清理函数后，我重新清理了数据集中的全部 160 万个条目。你可以从我的<a class="ae kw" rel="noopener" target="_blank" href="/another-twitter-sentiment-analysis-bb5b01ebad90">上一篇</a>中找到清洁过程的细节。</p><p id="6aae" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">清理之后，我将清理后的数据导出为 csv 格式，然后加载为数据框，如下图所示。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="8e0c" class="mx ky iq mt b gy my mz l na nb">csv = 'clean_tweet.csv'<br/>my_df = pd.read_csv(csv,index_col=0)<br/>my_df.head()</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/c3cb19d848afebbac6749a00a350b78f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sKj5PWVh6mu9GaAl8Xk51A.png"/></div></div></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="82ac" class="mx ky iq mt b gy my mz l na nb">my_df.info()</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nf"><img src="../Images/351bba87f3f08ed45c7cfdf0a6374a96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zk-q6bwgw5sWSaf8jxkgzQ.png"/></div></div></figure><p id="f88a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">数据中似乎有一些空条目，让我们进一步调查。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="c971" class="mx ky iq mt b gy my mz l na nb">my_df[my_df.isnull().any(axis=1)].head()</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/831bf7981d5c9929ea55fe37fd0fb6fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a9j34W1RpZR00e0XeoM9cw.png"/></div></div></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="b26d" class="mx ky iq mt b gy my mz l na nb">np.sum(my_df.isnull().any(axis=1))</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/c94d46fcf25d6aa5668db7d90d4c6b45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lryJYDOzeMTN7VH9AM34Yg.png"/></div></div></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="d628" class="mx ky iq mt b gy my mz l na nb">my_df.isnull().any(axis=0)</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/98da97280e09e9a1f5eeae3d41bba382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PdGQ6STl-cHATO7ClSJ59w.png"/></div></div></figure><p id="4dd8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">似乎有 3981 个条目的文本列为空。这很奇怪，因为原始数据集没有空条目，因此如果清理后的数据集中有任何空条目，那一定是在清理过程中发生的。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="a897" class="mx ky iq mt b gy my mz l na nb">df = pd.read_csv("./trainingandtestdata/training.1600000.processed.noemoticon.csv",header=None)<br/>df.iloc[my_df[my_df.isnull().any(axis=1)].index,:].head()</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/e96d8ebb74b4990c533c4475e3b3a3ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YjRhu2ARhLMjWSp54RMvjA.png"/></div></div></figure><p id="25ef" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过查看原始数据中的这些条目，似乎只有文本信息，要么是 twitter ID，要么是 url 地址。无论如何，这些是我决定为情感分析丢弃的信息，所以我将丢弃这些空行，并更新数据框。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="32cf" class="mx ky iq mt b gy my mz l na nb">my_df.dropna(inplace=True)<br/>my_df.reset_index(drop=True,inplace=True)<br/>my_df.info()</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/2618651ccd1ad482b9008b31c51a7856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tvHNmPoXSU9dJjp3nOSsjw.png"/></div></div></figure><h1 id="27c4" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">词云</h1><p id="6871" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我选择的第一个文本视觉化是有争议的单词 cloud。词云通过根据单个词的频率成比例地调整其大小，然后以随机排列的方式呈现它们，来表示词在文档中的用法。围绕 word cloud 有很多争论，我有点同意那些反对使用 word cloud 作为数据分析的人的观点。对 word cloud 的一些担忧是，它仅支持最粗略的文本分析，并且它通常应用于文本分析不合适的情况，并且它让查看者自己找出数据的上下文，而不提供叙述。</p><p id="5545" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是在推文的情况下，文本分析是最重要的分析，它以一种快速和肮脏的方式提供了关于什么样的词在语料库中频繁出现的一般想法。所以，我将试一试，并找出还有哪些方法可以用于文本可视化。</p><p id="230b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于 wordcloud，我使用了 python 库 word cloud。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="4237" class="mx ky iq mt b gy my mz l na nb">neg_tweets = my_df[my_df.target == 0]<br/>neg_string = []<br/><strong class="mt ir">for</strong> t <strong class="mt ir">in</strong> neg_tweets.text:<br/>    neg_string.append(t)<br/>neg_string = pd.Series(neg_string).str.cat(sep=' ')</span><span id="8ce2" class="mx ky iq mt b gy nc mz l na nb"><strong class="mt ir">from</strong> <strong class="mt ir">wordcloud</strong> <strong class="mt ir">import</strong> WordCloud<br/><br/>wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_string)<br/>plt.figure(figsize=(12,10))<br/>plt.imshow(wordcloud, interpolation="bilinear")<br/>plt.axis("off")<br/>plt.show()</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nl"><img src="../Images/52498f8d909da802d6372a176c89dc18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bpuq-vmpcEyqwEQU6fYZHw.png"/></div></div></figure><p id="1e72" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有些大词可以被解释成中性词，如“今天”、“现在”等。我可以看到一些较小的单词在负面推文中是有意义的，比如“该死”、“啊”、“小姐”、“坏”等等。但是有“爱”在相当大的尺寸，所以我想看看发生了什么。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="9372" class="mx ky iq mt b gy my mz l na nb"><strong class="mt ir">for</strong> t <strong class="mt ir">in</strong> neg_tweets.text[:200]:<br/>    <strong class="mt ir">if</strong> 'love' <strong class="mt ir">in</strong> t:<br/>        <strong class="mt ir">print</strong> t</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/2c1bbdc4b44dfd4f75ec30d426ae090c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z25n2mBAE5RjcsLcWSC7rQ.png"/></div></div></figure><p id="1f76" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好吧，即使推文包含“爱”这个词，在这些情况下，它是负面情绪，因为推文混合了像“爱”但“想念”这样的情绪。或者有时以讽刺的方式使用。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="a3ce" class="mx ky iq mt b gy my mz l na nb">pos_tweets = my_df[my_df.target == 1]<br/>pos_string = []<br/><strong class="mt ir">for</strong> t <strong class="mt ir">in</strong> pos_tweets.text:<br/>    pos_string.append(t)<br/>pos_string = pd.Series(pos_string).str.cat(sep=' ')</span><span id="16b5" class="mx ky iq mt b gy nc mz l na nb">wordcloud = WordCloud(width=1600, height=800,max_font_size=200,colormap='magma').generate(pos_string) plt.figure(figsize=(12,10)) plt.imshow(wordcloud, interpolation="bilinear") plt.axis("off") plt.show()</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/3bf3e5f6450546874c89a9669075fbea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FFUM6Piayz5J3bz7afJivg.png"/></div></div></figure><p id="8474" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我又一次看到一些自然的大字体单词，“今天”、“现在”，但是像“哈哈”、“爱”、“棒极了”这样的单词也很突出。</p><p id="51a0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有趣的是，“工作”这个词在负面词云中相当大，但在正面词云中也相当大。这可能意味着许多人表达了对工作的负面情绪，但也有许多人对工作持积极态度。</p><h1 id="a931" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">为更多数据可视化做准备</h1><p id="48d6" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">为了让我在下一步实现一些数据可视化，我需要词频数据。推文中使用了什么样的词，以及在整个语料库中使用了多少次。我使用计数矢量器来计算术语频率，尽管计数矢量器也用于拟合、训练和预测，但在这个阶段，我将只提取术语频率进行可视化。</p><p id="f2bc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">计数矢量器有一些可用的参数选项，例如移除停用词、限制最大项数。然而，为了首先获得数据集的全貌，我实现了包含停用词，并且不限制最大术语数。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="e30c" class="mx ky iq mt b gy my mz l na nb"><strong class="mt ir">from</strong> <strong class="mt ir">sklearn.feature_extraction.text</strong> <strong class="mt ir">import</strong> CountVectorizer<br/>cvec = CountVectorizer()<br/>cvec.fit(my_df.text)</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/5e39ba0a8c7e28978d4f2fef88f29953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HpREDj_z8kqtdOWdf6o7GQ.png"/></div></div></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="0b5b" class="mx ky iq mt b gy my mz l na nb">len(cvec.get_feature_names())</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/22aa7dfeef7ec1235a0696c0924fb1b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i48UQ6gGolCz9czd4ll8lA.png"/></div></div></figure><p id="d074" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好了，看起来计数矢量器已经从语料库中提取了 264，936 个单词。</p><p id="9bba" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">！重要更新(10/01/2018) </strong>:我刚刚意识到，我不必经历我下面做的所有批处理。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/7814d867b1012022b1cbcb27d088e0c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*EsboLesPq-MAqxupzGJhNg.jpeg"/></div></figure><p id="0efb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">是啊！我下面做的所有批处理和计算都可以用少得多的代码行来完成(虽然不完全是一行)。一旦使用拟合的计数矢量器转换数据，就可以直接从稀疏矩阵中获得术语频率。而这一切都可以在不到一秒的时间内完成！如果你看了下面，你会发现我花了将近 40 分钟才得到术语频率表。我想我用最艰难的方式学到了这一课。直接从稀疏矩阵中获得频率项的代码如下。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="b1ed" class="mx ky iq mt b gy my mz l na nb">neg_doc_matrix = cvec.transform(my_df[my_df.target == 0].text)<br/>pos_doc_matrix = cvec.transform(my_df[my_df.target == 1].text)<br/>neg_tf = np.sum(neg_doc_matrix,axis=0)<br/>pos_tf = np.sum(pos_doc_matrix,axis=0)<br/>neg = np.squeeze(np.asarray(neg_tf))<br/>pos = np.squeeze(np.asarray(pos_tf))<br/>term_freq_df = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()</span></pre><p id="badc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我将在下面留下我最初写的，作为对我自己愚蠢的提醒。这个故事的寓意是，如果你可以直接用稀疏矩阵工作，你绝对应该不用转换成密集矩阵！</p></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><p id="b060" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于下面的部分，我不得不经历许多试验和错误，因为内存使用超载。如果代码需要时间来实现，但仍然在运行，这是可以的，但这不是块运行多长时间的问题，我的 mac book pro 只是简单地放弃了，要么杀死内核，要么冻结。经过无数次尝试，我终于成功地批量处理了数据。</p><p id="2273" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我一直犯的错误是，当我分割文档矩阵时，我试图在' document _ matrix . to array()[start _ index，end_index]'中分割它，我最终意识到，因为我试图首先将整个 documnet_matrix 转换为数组，然后从那里分割，无论我如何更改批处理大小，我可怜的 macbook pro 都无法处理请求。在我将切片更改为' document_matrix[start_index，end_index]之后。toarray()'，我的 mac book pro 为我做了一件非常棒的工作。</p><p id="ed9d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，在处理以下任务的过程中，您的 mac book 可以在寒冷的冬天成为一个出色的取暖设备。在亚洲，我们称之为“一石二鸟”。不客气</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/f05cac95465df3efc46d460da99cca80.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*-sxBFTEG1qu8MYOA2UZDwQ.jpeg"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Cats are the best at finding warm spots in winter!</figcaption></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="5703" class="mx ky iq mt b gy my mz l na nb">document_matrix = cvec.transform(my_df.text)<br/>my_df[my_df.target == 0].tail()</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/9ba71af5024dc2d74d1b961dd911f0ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8H0cXvF29Q0YCB0Sd8dfQ.png"/></div></div></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="263d" class="mx ky iq mt b gy my mz l na nb">%%time<br/>neg_batches = np.linspace(0,798179,100).astype(int)<br/>i=0<br/>neg_tf = []<br/><strong class="mt ir">while</strong> i &lt; len(neg_batches)-1:<br/>    batch_result = np.sum(document_matrix[neg_batches[i]:neg_batches[i+1]].toarray(),axis=0)<br/>    neg_tf.append(batch_result)<br/>    <strong class="mt ir">if</strong> (i % 10 == 0) | (i == len(neg_batches)-2):<br/>        <strong class="mt ir">print</strong> neg_batches[i+1],"entries' term freuquency calculated"<br/>    i += 1</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/b5a4324ec6bcb122b0bb8d68ed114cf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CKGvM3rOCSteqHUN4b8OVg.png"/></div></div></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="a578" class="mx ky iq mt b gy my mz l na nb">my_df.tail()</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nz"><img src="../Images/92f2f408b09998a6dd5d828b6408b88b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NhfrJyRIPKiVwkSGN3f-qw.png"/></div></div></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="8327" class="mx ky iq mt b gy my mz l na nb">%%time<br/>pos_batches = np.linspace(798179,1596019,100).astype(int)<br/>i=0<br/>pos_tf = []<br/><strong class="mt ir">while</strong> i &lt; len(pos_batches)-1:<br/>    batch_result = np.sum(document_matrix[pos_batches[i]:pos_batches[i+1]].toarray(),axis=0)<br/>    pos_tf.append(batch_result)<br/>    <strong class="mt ir">if</strong> (i % 10 == 0) | (i == len(pos_batches)-2):<br/>        <strong class="mt ir">print</strong> pos_batches[i+1],"entries' term freuquency calculated"<br/>    i += 1</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oa"><img src="../Images/185ed4dbf8a394e160f257e07773c868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ambXHUHugsoGqSh2sj2GcA.png"/></div></div></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="07fc" class="mx ky iq mt b gy my mz l na nb">neg = np.sum(neg_tf,axis=0)<br/>pos = np.sum(pos_tf,axis=0)<br/>term_freq_df = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()<br/>term_freq_df.head()</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ob"><img src="../Images/e40a2f357cd3b5e028ac32052d4d5cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tfwpbw51XU2oUrqBzypCoQ.png"/></div></div></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="7917" class="mx ky iq mt b gy my mz l na nb">term_freq_df.columns = ['negative', 'positive']<br/>term_freq_df['total'] = term_freq_df['negative'] + term_freq_df['positive']<br/>term_freq_df.sort_values(by='total', ascending=False).iloc[:10]</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/990c29b05c48f3fed5ae9e2acc51e0a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_HX7A-W5VsBdllmaHzGrRw.png"/></div></div></figure><p id="aa3d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好了，术语频率数据框已经创建好了！正如你所看到的，最常见的单词都是停用词，如“to”、“the”等。你可能想知道为什么要经历所有繁重的处理，而不删除停用词，也不限制最大词数，以获得停用词占主导地位的词频率？因为我真的很想用上面得到的结果来检验<a class="ae kw" href="https://en.wikipedia.org/wiki/Zipf%27s_law" rel="noopener ugc nofollow" target="_blank">齐夫定律</a>。我需要停止字！你会明白我的意思。</p><p id="2ec1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我会在下一篇文章中告诉你更多关于齐夫定律的内容，我保证会有更多的可视化。你会发现上面的频率计算并不是毫无意义的。</p><p id="ae1b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">和往常一样，你可以从下面的链接找到 Jupyter 笔记本。</p><p id="c530" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://github.com/tthustla/twitter_sentiment_analysis_part2/blob/master/Capstone_part3-Copy1.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/tthustle sa/Twitter _ 情操 _ 分析 _ part 2/blob/master/Capstone _ part 3-copy 1 . ipynb</a></p></div></div>    
</body>
</html>