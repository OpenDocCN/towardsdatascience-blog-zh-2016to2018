<html>
<head>
<title>Algorithms for hyperparameter optimisation in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中超参数优化算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/algorithms-for-hyperparameter-optimisation-in-python-edda4bdb167?source=collection_archive---------5-----------------------#2018-11-03">https://towardsdatascience.com/algorithms-for-hyperparameter-optimisation-in-python-edda4bdb167?source=collection_archive---------5-----------------------#2018-11-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/a5a81713a4aeae5a6d26cadd09d046b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WAvEN_WTn1bLYwgjuzXF1Q.jpeg"/></div></div></figure><p id="e5fa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">超参数通常对机器学习算法的成功有重大影响。一个配置不良的 ML 模型可能不会比一个配置良好的 ML 模型表现得更好，而一个配置良好的 ML 模型可以达到最先进的结果。</p><p id="acee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">寻找最佳超参数的过程可能非常乏味，与其说是科学，不如说是艺术。这个微调模型参数的过程被称为超参数优化。在模型调整的过程中，我们经常会发现自己处于以下状态:</p><ol class=""><li id="baf1" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">厌倦了一次又一次地手动重新训练模型(没有任何性能增益)。</li><li id="4ba3" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">等待彻底的网格搜索返回结果。(总感觉像是永恒)</li><li id="d60b" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">运行固定迭代次数的随机搜索，只希望它返回一些性能增益。</li><li id="4831" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">你与原力(YODA)融为一体，并完善了微调模型的艺术。(如果你属于这一类，请与我们普通人分享你的神秘知识)</li></ol><p id="2045" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你经常发现自己处于状态 1-3，那么这篇文章将帮助你变得更好。</p><p id="7b03" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">手动调谐并不都是不好的，因为它在选择下一个参数集时考虑了先前运行的结果(<strong class="ka ir"> <em class="lk">)通知搜索</em> </strong>)。这为我们节省了一些迭代，因为我们不会盲目地运行所有的排列。与此相反，网格/随机搜索独立运行每个排列，但不需要人工干预。为了两全其美，我们需要一个基于历史运行识别有希望的样本空间的自动化过程。<strong class="ka ir"> <br/> <br/> </strong>实现这一点的有效方法之一是贝叶斯优化。这种技术有效地权衡了参数空间的探索和利用，以返回最佳优化评估标准的配置。一个额外的优势是，即使被优化的基础函数<em class="lk"> f </em>是随机的、非凸的或者甚至是非连续的，它们仍然有效。</p><p id="8e73" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">基于序列模型的优化(SMBO)是贝叶斯优化的简洁形式。SMBO 是一种通用的函数优化技术，号称是最有效的调用技术之一。</p><blockquote class="ll"><p id="f055" class="lm ln iq bd lo lp lq lr ls lt lu kv dk translated"><em class="lv"> SMBO 通过识别可能已经绘制的超参数赋值来工作，并且根据损失函数值在其他点看起来是有希望的。</em></p></blockquote><p id="6d2f" class="pw-post-body-paragraph jy jz iq ka b kb lw kd ke kf lx kh ki kj ly kl km kn lz kp kq kr ma kt ku kv ij bi translated">在这篇博客中，我们将介绍超参数优化的理论，然后使用 hyperopt 进行实际演示。Hyperopt 是 python 中的一个超参数优化库，它使用 TPE(SMBO 的一种风格)进行优化。在深入 SMBO 之前，让我们回顾一下超参数、网格搜索和随机搜索的基础知识。如果您已经熟悉第(1–3)部分，您可以跳过它们。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="3e75" class="mi mj iq bd mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bi translated"><strong class="ak">目录</strong></h1><ol class=""><li id="f49c" class="kw kx iq ka b kb ng kf nh kj ni kn nj kr nk kv lb lc ld le bi translated">基本定义</li><li id="4e2d" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">基于序列模型的优化(SMBO)</li><li id="127b" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">SMBO 的类型:基于高斯的和基于 TPE 的</li><li id="5c47" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">使用 Hyperopt 的实际操作示例</li></ol><h2 id="7e65" class="nl mj iq bd mk nm nn dn mo no np dp ms kj nq nr mw kn ns nt na kr nu nv ne nw bi translated"><strong class="ak">基本定义</strong></h2><p id="3cf5" class="pw-post-body-paragraph jy jz iq ka b kb ng kd ke kf nh kh ki kj nx kl km kn ny kp kq kr nz kt ku kv ij bi translated">1.<strong class="ka ir"> <em class="lk">超参数</em> </strong>是模型内置的配置变量。这些变量需要微调以产生性能更好的模型。这些参数取决于型号，并且因型号而异。例如，随机森林模型将具有以下超参数</p><figure class="oa ob oc od gt jr"><div class="bz fp l di"><div class="oe of l"/></div></figure><figure class="oa ob oc od gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi og"><img src="../Images/529b08e1d51e4649ae1a09825a0bbabf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZhSF6eGttzkWCqWmoDKDIQ.png"/></div></div><figcaption class="oh oi gj gh gi oj ok bd b be z dk">Internal features of a random forest model</figcaption></figure><p id="44f3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以上所有特征都是模型的固有特征。对于基于树的集成方法，如随机森林或梯度推进 max_depth、min_sample_leaf 和 n_estimators(集成中的树的数量)是最重要的。</p><p id="c520" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2.<strong class="ka ir">网格搜索</strong>是超参数优化最基本的算法。这就像在内置特性的所有可能值上运行嵌套循环。以下示例中的 rf_params 包含需要微调的模型特征。</p><figure class="oa ob oc od gt jr"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="9921" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在上述情况下，模型将被重新训练 300 次。<br/>2(n _ estimator)* 3(max _ features)* 10(max _ depth)* 5(cv runs)= 300</p><p id="7bc2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是一个非常狭窄的搜索空间，因为我们只搜索 3 个特征。如果我们进行彻底的搜索，组合的数量很容易超过 10k。</p><p id="ccf4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">3.<strong class="ka ir">随机搜索</strong>是网格搜索，随机选择下一个特征集，总运行次数有上限。</p><figure class="oa ob oc od gt jr"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="7493" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">随机搜索的代码与 GridSearch 相同，唯一的区别是我们添加了 n_iter=100，它固定了允许运行次数的上限。网格/随机搜索是<strong class="ka ir"> <em class="lk">无信息搜索</em> </strong>的例子，意味着下一个特征集独立于上一次运行的输出。这两种方法都需要在每次迭代中进行重新训练，这导致了巨大的成本。</p><h2 id="a379" class="nl mj iq bd mk nm nn dn mo no np dp ms kj nq nr mw kn ns nt na kr nu nv ne nw bi translated"><strong class="ak">基于序列模型的优化(SMBO) </strong></h2><p id="d1eb" class="pw-post-body-paragraph jy jz iq ka b kb ng kd ke kf nh kh ki kj nx kl km kn ny kp kq kr nz kt ku kv ij bi translated">SMBO 通过顺序选择不同的超参数集来最小化验证损失，其中下一个集是通过贝叶斯推理选择的(取决于之前的运行)。直观地说，SMBO 会回顾上次运行的结果，以将未来的搜索集中在看起来更有希望的领域。</p><p id="d5c4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">只要评估适应度函数(<em class="lk"> f : X - &gt; R </em>)的成本很高，就使用 SMBO。在这种情况下，计算一个近似的<em class="lk"> f </em> ( <strong class="ka ir">代理<em class="lk">M</em>T24】)。这款<strong class="ka ir"> <em class="lk"> M </em> </strong>算起来比较便宜。通常，SMBO 中的内部循环是该代理的数值优化或该代理的某种转换(下面代码中的<em class="lk">第 3 行)。最大化替代值的点<em class="lk"> x* </em>成为应该评估真实损失函数<em class="lk"> f </em>的建议(<em class="lk">行 4 </em>)。</em></strong></p><figure class="oa ob oc od gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ol"><img src="../Images/75bac4b56ddadf99d6dbea52bb243202.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9qBREj5zPnn3kBIETbRW5g.png"/></div></div><figcaption class="oh oi gj gh gi oj ok bd b be z dk">Figure 1 : The pseudo-code of generic Sequential Model-Based Optimisation</figcaption></figure><p id="da97" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="lk"> H:观察历史<br/> T:试验次数<br/> f:真函数<br/> M:逼近 f 的替代函数<br/> S:计算下一个超参数赋值<br/>x *:M 最小化的样本实例</em></p><p id="44c8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">伪代码解释:<br/> <em class="lk">第 1 行:</em>初始化一个空的<em class="lk"> H. <br/>第 2 行:初始化固定次数的循环尝试。<br/>第三行:<br/> 1。L </em>为<em class="lk"> f </em>获得一个名为<em class="lk"> M. <br/> 2 的代理函数。定义一个需要最小化的评估标准。<br/> 3。对 x 的多个实例运行 S(x，M)</em>以找到<em class="lk"> x* </em>哪个<em class="lk"> </em>最小化 S. <br/> <em class="lk">行 4 : </em> <em class="lk"> f </em>被评估为最佳 x. <br/> <em class="lk">行 5 : H </em>用当前值<em class="lk"> x，f(x)更新。<br/>第 6 行:M 在每次迭代后被</em>更新，以成为<em class="lk"> f 的更好近似。</em></p><p id="a5cb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">采集函数</strong>定义了在目标空间中探索新区域和利用已知具有有利价值的区域之间的平衡。不同风格的 SMBO 使用不同的算法来优化预期的改进(EI，采集函数的流行选择)。例如，Hyperopt 通过 TPE (Tree Parzen Estimators)优化 EI，而 Spearmint 使用高斯过程优化 EI。</p><figure class="oa ob oc od gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi om"><img src="../Images/6945340dd525063d96280cf3c0eb80e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2-tmwtHLdzlS5kycoICEA.png"/></div></div><figcaption class="oh oi gj gh gi oj ok bd b be z dk">SMBO optimises the EI ( Expected Improvement )</figcaption></figure><ol class=""><li id="845c" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">Pm:后 GP 知道<em class="lk"> H </em></li><li id="e1f4" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">y*:新候选人的替代值</li><li id="43ca" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">y:上一个候选人的替代值</li></ol><p id="b97b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">直观地说，它定义了在给定位置 x 的最佳先前观察客观值上的非负 EI。</p><h2 id="3379" class="nl mj iq bd mk nm nn dn mo no np dp ms kj nq nr mw kn ns nt na kr nu nv ne nw bi translated"><strong class="ak"> <em class="lv">基于高斯过程的 EI 优化</em> </strong></h2><p id="2ed3" class="pw-post-body-paragraph jy jz iq ka b kb ng kd ke kf nh kh ki kj nx kl km kn ny kp kq kr nz kt ku kv ij bi translated"><strong class="ka ir">高斯过程(GP) </strong>是一个<a class="ae on" href="https://en.wikipedia.org/wiki/Stochastic_process" rel="noopener ugc nofollow" target="_blank">随机过程</a>(由时间或空间索引的随机变量的集合)，使得这些随机变量的每个有限集合都有一个<a class="ae on" href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution" rel="noopener ugc nofollow" target="_blank">多元正态分布</a>，即它们的每个有限<a class="ae on" href="https://en.wikipedia.org/wiki/Linear_combination" rel="noopener ugc nofollow" target="_blank">线性组合</a>都是正态分布的。</p><p id="22a3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">GP 是贝叶斯优化中建模目标函数(<em class="lk"> f </em>)的标准代理。在这种设置中，假设<em class="lk"> f </em>是具有均值<em class="lk"> u </em>和协方差核<em class="lk">K</em>的 GP，核<em class="lk"> K </em>的选择可以对替代重建有重大影响。在这种方法中，模型<em class="lk"> M(图 1 的第 6 行)</em>定义了预测分布<em class="lk"> p(y|x，D)。</em></p><h2 id="1f77" class="nl mj iq bd mk nm nn dn mo no np dp ms kj nq nr mw kn ns nt na kr nu nv ne nw bi translated"><strong class="ak">树形结构 Parzen 估计器方法(TPE ) </strong></h2><p id="98f7" class="pw-post-body-paragraph jy jz iq ka b kb ng kd ke kf nh kh ki kj nx kl km kn ny kp kq kr nz kt ku kv ij bi translated"><a class="ae on" href="https://stats.stackexchange.com/questions/244012/can-you-explain-parzen-window-kernel-density-estimation-in-laymans-terms" rel="noopener ugc nofollow" target="_blank"> TPE </a>是以树形结构组织的核估计器，以保持条件相关性。与直接对<em class="lk"> p(y|x) </em>建模的基于 GP 的方法相反，TPE 通过<em class="lk"> p(x|y) </em>和<em class="lk"> p(y) </em>对其进行近似。</p><p id="f3e2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">TPE 对 GP 优化预期改进的方式进行了两项更改。</p><ol class=""><li id="9392" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">p(y|x)替换为 p(x|y) * p(y)/p(x)</li><li id="0be8" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">情况 1 : p(x|y) = l(x) if y <y/>情况 2 : p(x|y) = g(x) if y≥y*</li></ol><ul class=""><li id="7fb2" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv oo lc ld le bi translated"><em class="lk"> l(x) </em>是使用观测值{ <em class="lk"> x </em> }形成的密度，使得相应的损失<em class="lk"> f(x) </em>小于<em class="lk">y∫</em></li><li id="0839" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv oo lc ld le bi translated"><em class="lk"> g(x) </em>包含剩余的观测值</li></ul><p id="4e59" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">利用这两种分布，可以优化与预期改善成比例的封闭形式项。</p><figure class="oa ob oc od gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi op"><img src="../Images/3758fe2181ab49e512485a106e4bfb69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JuZCviqlhYfcUerg3-Q03Q.png"/></div></div></figure><p id="9a2a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这导致了下面的结论</p><figure class="oa ob oc od gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oq"><img src="../Images/ab36a885fec13eb15c0afa888d7025e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SjzOqRoj82XpAU_0jd2BmQ.png"/></div></div><figcaption class="oh oi gj gh gi oj ok bd b be z dk">EI after TPE specific optimisations</figcaption></figure><p id="68a3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后一个表达式表明，为了最大化改进，我们希望点 x 在<em class="lk"> l(x) </em>下具有高概率，而在<em class="lk"> g(x) </em>下具有低概率。这仅仅意味着选择<em class="lk"> x 的</em>,其具有小于先前报告值的 EI 的概率更高。</p><h2 id="fd8c" class="nl mj iq bd mk nm nn dn mo no np dp ms kj nq nr mw kn ns nt na kr nu nv ne nw bi translated">使用 Hyperopt 进行参数优化</h2><p id="7185" class="pw-post-body-paragraph jy jz iq ka b kb ng kd ke kf nh kh ki kj nx kl km kn ny kp kq kr nz kt ku kv ij bi translated">Hyperopt 是用于通过 SMBO 执行自动化模型调优的 Python 库。应用 hyperopt 进行超参数优化是一个 3 步流程:</p><ol class=""><li id="e492" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">定义目标函数。</li><li id="48c5" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">定义搜索空间(xgb_space)。</li><li id="8070" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">定义一个试验数据库来保存每次迭代的结果。</li></ol><p id="8bca" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以下代码块是如何对 xgboost(梯度增强库)执行超参数优化的示例。</p><figure class="oa ob oc od gt jr"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="5035" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了对其他模型使用相同的脚本，只需对模型进行少量修改。您需要对 xgb_space 变量进行修改，以适应您选择的模型的搜索空间。在目标函数中，您需要更改交叉验证的运行方式(如我们使用的 xgb 特定方法),以及您希望最小化的指标。例如，对于 LightGBM(转到<a class="ae on" href="https://medium.com/@abhisheksharma_57055/what-makes-lightgbm-lightning-fast-a27cf0d9785e" rel="noopener"> LightGBM </a>了解这个令人敬畏的库)，您可以通过做一些小的修改来使用代码。</p><p id="c970" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">将来我会在 hyperopt 库上发布一个更详细的 hands on blog。</p><p id="5acc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个博客是对我们目前可用的超参数优化技术的一个简要概述。我们从不知情的搜索开始，如网格/随机搜索，然后转移到不同的贝叶斯方法，如基于 GP 的 SMBO 和基于 TPE 的 SMBO，最后是 hyperopt 的工作示例。</p><h2 id="471a" class="nl mj iq bd mk nm nn dn mo no np dp ms kj nq nr mw kn ns nt na kr nu nv ne nw bi translated">参考</h2><p id="e609" class="pw-post-body-paragraph jy jz iq ka b kb ng kd ke kf nh kh ki kj nx kl km kn ny kp kq kr nz kt ku kv ij bi translated">如果你想深入了解，请浏览以下材料</p><ol class=""><li id="4e7a" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated"><a class="ae on" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank"> Hyperopt </a> github 存储库</li><li id="607e" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated"><a class="ae on" href="http://iopscience.iop.org/article/10.1088/1749-4699/8/1/014008/meta" rel="noopener ugc nofollow" target="_blank">远视纸</a></li><li id="76be" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated"><a class="ae on" href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf" rel="noopener ugc nofollow" target="_blank">超参数优化算法</a></li></ol><p id="352d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请在评论区分享你的想法和主意。</p></div></div>    
</body>
</html>