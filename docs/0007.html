<html>
<head>
<title>Dimensionality Reduction — Does PCA really improve classification outcome?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维——主成分分析真的能改善分类结果吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dimensionality-reduction-does-pca-really-improve-classification-outcome-6e9ba21f0a32?source=collection_archive---------0-----------------------#2017-01-09">https://towardsdatascience.com/dimensionality-reduction-does-pca-really-improve-classification-outcome-6e9ba21f0a32?source=collection_archive---------0-----------------------#2017-01-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="d6e0" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">介绍</h1><p id="33fc" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我遇到了一些关于降维技术的资源。这个主题无疑是最有趣的主题之一，想到有算法能够通过选择仍然代表整个数据集的最重要的特征来减少特征的数量，真是太好了。作者指出这些算法的优点之一是可以改善分类任务的结果。</p><p id="6f95" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">在这篇文章中，我将使用主成分分析(PCA)来验证这一说法，以尝试提高神经网络对数据集的分类性能。PCA真的能改善分类结果吗？我们去看看。</p><h1 id="046b" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">降维算法</h1><p id="c604" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在开始编码之前，我们先来谈谈降维算法。有两种主要的降维算法:线性判别分析(LDA)和主成分分析(PCA)。这两者之间的基本区别是LDA使用类别信息来发现新特征，以便最大化其可分性，而PCA使用每个特征的方差来做同样的事情。在这种情况下，LDA可以被认为是监督算法，而PCA是无监督算法。</p><h1 id="edf8" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">谈论PCA</h1><p id="c3ab" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">PCA背后的思想是简单地找到一组概括数据的低维轴。为什么我们需要汇总数据呢？让我们考虑一下这个例子:我们有一个由汽车的一组属性组成的数据集。这些属性通过大小、颜色、圆形度、紧凑度、半径、座位数、车门数、行李箱大小等来描述每辆车。然而，这些特征中的许多将测量相关的属性，因此是多余的。所以要去掉这些冗余，用较少的属性来描述每辆车。这正是PCA的目标。例如，将车轮数量视为轿车和公共汽车的一个特征，这两个类别中的几乎每个示例都有四个车轮，因此我们可以看出该特征具有较低的方差(在一些罕见的公共汽车中，从四个车轮到六个车轮或更多)，因此该特征将使公共汽车和轿车看起来相同，但它们实际上彼此非常不同。现在，考虑高度作为一个特征，汽车和公共汽车有不同的值，方差从最低的汽车到最高的公共汽车有很大的范围。显然，这些车辆的高度是区分它们的良好属性。回想一下，PCA不考虑类的信息，它只查看每个特征的方差，因为合理的假设是呈现高方差的特征更可能在类之间具有良好的分割。</p><p id="1006" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">通常，人们最终会错误地认为PCA从数据集中选择了一些特征，而丢弃了其他特征。该算法实际上是基于旧属性的组合来构建新的属性集。从数学上来说，PCA执行线性变换，将原始特征集移动到由主分量组成的新空间。这些新特征对我们来说没有任何实际意义，只有代数意义，因此不要认为线性地组合特征，你会发现你从未想过它可能存在的新特征。许多人仍然相信机器学习算法是神奇的，他们将成千上万的输入直接输入到算法中，并希望为他们的业务找到所有的见解和解决方案。别被骗了。数据科学家的工作是使用机器学习算法作为一套工具，而不是魔术棒，通过对数据进行良好的探索性分析来定位对业务的见解。牢记在心。</p><h1 id="7ed7" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">主成分空间是什么样子的？</h1><p id="0225" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在新的特征空间中，我们正在寻找一些在不同类别之间有很大差异的属性。正如我在前面的例子中所展示的，一些呈现低方差的属性是没有用的，它会使例子看起来一样。另一方面，主成分分析寻找尽可能跨类显示更多变化的属性来构建主成分空间。该算法使用方差矩阵、协方差矩阵、特征向量和特征值对的概念来执行PCA，结果提供一组特征向量及其各自的特征值。PCA的表现如何是下一篇文章的素材。</p><p id="7bb3" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">那么，特征值和特征向量应该怎么处理呢？很简单，特征向量代表主分量空间的新的一组轴，特征值携带每个特征向量的方差的信息。因此，为了降低数据集的维数，我们将选择那些方差较大的特征向量，丢弃那些方差较小的特征向量。通过下面的例子，我们会越来越清楚它到底是如何工作的。</p><h1 id="5e9d" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">让我们最终看到一些代码。</h1><p id="46ab" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在，我们到了这篇文章有趣的部分。让我们看看主成分分析是否真的改善了分类任务的结果。</p><p id="1c4f" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">为了解决这个问题，我的策略是在数据集上应用神经网络，并观察它的初始结果。然后，我将在分类之前执行PCA，并对新数据集应用相同的神经网络，最后比较两个结果。</p><p id="3c54" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">该数据集来源于UCI机器学习知识库，称为“Statlog(车辆轮廓)数据集”。该数据集存储了四种车辆轮廓的一些测量值，用于分类。它由946个示例和18个测量(属性)所有数值组成，您可以在此链接查看更多详细信息:<a class="ae lo" href="https://archive.ics.uci.edu/ml/datasets/Statlog+(Vehicle+Silhouettes)" rel="noopener ugc nofollow" target="_blank">https://archive . ics . UCI . edu/ml/datasets/Statlog+(Vehicle+Silhouettes)</a>。神经网络将是一个多层感知器，具有四个隐藏节点和一个输出节点，所有节点都使用sigmoid函数作为激活函数，PCA函数来自R包。</p><h1 id="f8c5" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">准备数据集</h1><p id="911a" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">首先，我要为二元分类准备数据集。</p><p id="3435" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">我将只从两个类中选择例子，以便组成一个二元分类。这些例子将来自“公共汽车”和“北京汽车股份有限公司”类。“北京汽车股份有限公司”将被0级取代，“公共汽车”将被1级取代。下一步是将数据集分为训练数据集和测试数据集，分别占总类示例的60%和40%。</p><p id="6cb2" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">在之前的数据集准备之后，让我们一次使用所有特征来建模神经网络，然后应用测试数据集。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="30d2" class="ly jo iq lu b gy lz ma l mb mc"><em class="md"># Load library</em><br/><strong class="lu ir">library</strong>( dplyr )<br/><br/><em class="md"># Load dataset</em><br/>data = read.csv( "../dataset/vehicle.csv", stringsAsFactor = FALSE )<br/><br/><em class="md"># Transform dataset</em><br/>dataset = data %&gt;% <br/>            filter( class == "bus" | class == "saab" ) %&gt;%<br/>            transform( class = ifelse( class == "saab", 0, 1 ) )<br/>dataset = as.data.frame( sapply( dataset, as.numeric ) )<br/><br/><em class="md"># Spliting training and testing dataset</em><br/>index = sample( 1:nrow( dataset ), nrow( dataset ) * 0.6, replace = FALSE ) <br/><br/>trainset = dataset[ index, ]<br/>test = dataset[ -index, ]<br/>testset = test %&gt;% select( -class )<br/><br/><em class="md"># Building a neural network (NN)</em><br/><strong class="lu ir">library</strong>( neuralnet )<br/>n = names( trainset )<br/>f = as.formula( paste( "class ~", paste( n[!n %in% "class"], collapse = "+" ) ) )<br/>nn = neuralnet( f, trainset, hidden = 4, linear.output = FALSE, threshold = 0.01 )<br/><br/>plot( nn, rep = "best" )</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi me"><img src="../Images/01f2ca1eee97fd0959402435c6641e60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gRLUN9xtBco7ZfI8GUru_A.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Figure 1. Neural Network MultiLayer-Perceptron</figcaption></figure><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="49a7" class="ly jo iq lu b gy lz ma l mb mc"><em class="md"># Testing the result output</em><br/>nn.results = compute( nn, testset )<br/><br/>results = data.frame( actual = test$class, prediction = round( nn.results$net.result ) )<br/><br/><em class="md"># Confusion matrix</em><br/><strong class="lu ir">library</strong>( caret )<br/>t = table( results )<br/>print( confusionMatrix( t ) )</span><span id="fa86" class="ly jo iq lu b gy mq ma l mb mc">## Confusion Matrix and Statistics<br/>## <br/>##       prediction<br/>## actual  0  1<br/>##      0 79  0<br/>##      1 79 16<br/>##                                                 <br/>##                Accuracy : 0.545977              <br/>##                  95% CI : (0.4688867, 0.6214742)<br/>##     No Information Rate : 0.908046              <br/>##     P-Value [Acc &gt; NIR] : 1                     <br/>##                                                 <br/>##                   Kappa : 0.1553398             <br/>##  Mcnemar's Test P-Value : &lt;0.0000000000000002   <br/>##                                                 <br/>##             Sensitivity : 0.5000000             <br/>##             Specificity : 1.0000000             <br/>##          Pos Pred Value : 1.0000000             <br/>##          Neg Pred Value : 0.1684211             <br/>##              Prevalence : 0.9080460             <br/>##          Detection Rate : 0.4540230             <br/>##    Detection Prevalence : 0.4540230             <br/>##       Balanced Accuracy : 0.7500000             <br/>##                                                 <br/>##        'Positive' Class : 0                     <br/>##</span></pre><h1 id="22b2" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">不含PCA的结果</h1><p id="b67c" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">似乎我们得到了一些结果。首先，看看混淆矩阵。基本上，混淆矩阵表示有多少例子被分类。主对角线显示正确分类的实例，次对角线显示错误分类。在第一个结果中，分类器显示自己非常困惑，因为它正确地分类了几乎所有来自“saab”类的示例，但它也将大多数“bus”类的示例分类为“saab”类。加强这一结果，我们可以看到，准确性的价值约为50%，这是一个非常糟糕的分类任务的结果。该分类器基本上有50%的概率将新实例分类为“汽车”类，50%的概率分类为“公共汽车”类。类似地，神经网络为每一个新的例子抛硬币来选择它应该分类到哪个类别。</p><h1 id="277a" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">让我们看看PCA是否能帮助我们</h1><p id="e94e" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在，让我们对数据集执行主成分分析，并获得特征值和特征向量。实际上，你会看到来自R包的PCA函数提供了一组已经按降序排序的特征值，这意味着第一个分量是方差最高的那个，第二个分量是方差第二高的特征向量，以此类推。下面的代码显示了如何根据特征值选择特征向量。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="ce2a" class="ly jo iq lu b gy lz ma l mb mc"><em class="md"># PCA</em><br/>pca_trainset = trainset %&gt;% select( -class )<br/>pca_testset = testset<br/>pca = prcomp( pca_trainset, scale = T )<br/><br/><em class="md"># variance</em><br/>pr_var = ( pca$sdev )^2 <br/><br/><em class="md"># % of variance</em><br/>prop_varex = pr_var / sum( pr_var )<br/><br/><em class="md"># Plot</em><br/>plot( prop_varex, xlab = "Principal Component", <br/>                  ylab = "Proportion of Variance Explained", type = "b" )</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi me"><img src="../Images/a5816df9446778511dd8c5f3418f4b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5md3qy6oFFZpEz_gpi5J5g.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Figura 02. Percentage of Variance from each Principal Component</figcaption></figure><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="d992" class="ly jo iq lu b gy lz ma l mb mc"><em class="md"># Scree Plot</em><br/>plot( cumsum( prop_varex ), xlab = "Principal Component", <br/>                            ylab = "Cumulative Proportion of Variance Explained", type = "b" )</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi me"><img src="../Images/1184290bfee0584b05179efd82cdea55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wdhbr1RA4TCU2EjVPr3dtA.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Figure 03. Cumulative sum of Variance</figcaption></figure><p id="6cc0" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">stats默认包中的本地R函数“prcomp”执行PCA，它返回所需的所有特征值和特征向量。第一个图显示了每个特征的方差百分比。可以看到，第一个分量的方差最大，约为50%，而第八个分量的方差约为0%。因此，这表明我们应该选择前八个组件。第二张图显示了方差的另一个角度，尽管所有方差的累积和，您可以看到前八个特征值对应于所有方差的大约98%。事实上，这是一个相当不错的数字，这意味着只有2%的信息丢失。最大的好处是，我们从一个有18个特征的空间转移到另一个只有8个特征的空间，只损失2%的信息。毫无疑问，这就是降维的力量。</p><p id="43e8" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">既然我们已经知道了将构成新空间的要素的数量，让我们创建新的数据集，然后再次对神经网络建模，并检查我们是否获得了新的更好的结果。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="35be" class="ly jo iq lu b gy lz ma l mb mc"><em class="md"># Creating a new dataset</em><br/>train = data.frame( class = trainset$class, pca$x )<br/>t = as.data.frame( predict( pca, newdata = pca_testset ) )<br/><br/>new_trainset = train[, 1:9]<br/>new_testset =  t[, 1:8]<br/><br/><em class="md"># Build the neural network (NN)</em><br/><strong class="lu ir">library</strong>( neuralnet )<br/>n = names( new_trainset )<br/>f = as.formula( paste( "class ~", paste( n[!n %in% "class" ], collapse = "+" ) ) )<br/>nn = neuralnet( f, new_trainset, hidden = 4, linear.output = FALSE, threshold=0.01 )<br/><br/><em class="md"># Plot the NN</em><br/>plot( nn, rep = "best" )</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi me"><img src="../Images/ee8dac2215fd4cd6642f400aa3cfef0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dExiQmYMh7lRQauE2Q0-KA.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Figure 04. Neural Network with new dataset</figcaption></figure><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="d7dd" class="ly jo iq lu b gy lz ma l mb mc"><em class="md"># Test the resulting output</em><br/>nn.results = compute( nn, new_testset )<br/><br/><em class="md"># Results</em><br/>results = data.frame( actual = test$class, <br/>                      prediction = round( nn.results$net.result ) )<br/><br/><em class="md"># Confusion Matrix</em><br/><strong class="lu ir">library</strong>( caret )<br/>t = table( results ) <br/>print( confusionMatrix( t ) )</span><span id="6a3a" class="ly jo iq lu b gy mq ma l mb mc">## Confusion Matrix and Statistics<br/>## <br/>##       prediction<br/>## actual  0  1<br/>##      0 76  3<br/>##      1  1 94<br/>##                                                 <br/>##                Accuracy : 0.9770115             <br/>##                  95% CI : (0.9421888, 0.9937017)<br/>##     No Information Rate : 0.5574713             <br/>##     P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022 <br/>##                                                 <br/>##                   Kappa : 0.9535318             <br/>##  Mcnemar's Test P-Value : 0.6170751             <br/>##                                                 <br/>##             Sensitivity : 0.9870130             <br/>##             Specificity : 0.9690722             <br/>##          Pos Pred Value : 0.9620253             <br/>##          Neg Pred Value : 0.9894737             <br/>##              Prevalence : 0.4425287             <br/>##          Detection Rate : 0.4367816             <br/>##    Detection Prevalence : 0.4540230             <br/>##       Balanced Accuracy : 0.9780426             <br/>##                                                 <br/>##        'Positive' Class : 0                     <br/>##</span></pre><p id="f3d9" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">嗯，我想我们现在有更好的结果了。让我们仔细检查一下。</p><p id="31fe" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">混淆矩阵这次显示了非常好的结果，神经网络在两个类中都犯了较少的错误分类，这可以通过主对角线的值以及准确度值约为95%看出。这意味着分类器有95%的机会正确地分类一个新的看不见的例子。对于分类问题来说，这是一个完全不差的结果。</p><h1 id="f524" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">结论</h1><p id="19f5" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">降维在机器学习中起着非常重要的作用，尤其是当你处理成千上万的特征时。主成分分析是顶级的降维算法之一，在实际工程中不难理解和使用。正如我们在这篇文章中看到的，这种技术，除了使特征操作的工作变得更容易之外，还有助于改善分类器的结果。</p><p id="39cc" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">最后，第一个问题的答案是肯定的，事实上主成分分析有助于改善分类器的结果。</p><h1 id="8b4b" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">下一步是什么？</h1><p id="bc87" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">正如我之前提到的，还有其他可用的降维技术，如线性判别分析、因子分析、Isomap及其变体。ideia正在探索每种方法的优点和缺点，并检查其单独和组合的结果。LDA结合PCA会改善分类器的结果吗？好吧，让我们在接下来的帖子里研究一下。</p><p id="30e7" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">完整的代码可以在我的git hub仓库和数据集上找到。(<a class="ae lo" href="https://github.com/Meigarom/machine_learning" rel="noopener ugc nofollow" target="_blank">https://github.com/Meigarom/machine_learning</a></p><p id="2efb" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">感谢你花时间阅读这篇文章。我真的很感激，随时欢迎反馈。</p><p id="0fee" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">一会儿见。</p></div></div>    
</body>
</html>