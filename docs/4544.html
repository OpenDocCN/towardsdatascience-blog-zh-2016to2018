<html>
<head>
<title>Perceptron Learning Algorithm: A Graphical Explanation Of Why It Works</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">感知器学习算法:其工作原理的图形解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975?source=collection_archive---------0-----------------------#2018-08-22">https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975?source=collection_archive---------0-----------------------#2018-08-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="4f9f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章将讨论著名的<em class="kl">感知机学习算法，</em>最初由<a class="ae km" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank"> Frank Rosenblatt </a>于 1943 年提出，后来由<a class="ae km" href="http://science.sciencemag.org/content/165/3895/780" rel="noopener ugc nofollow" target="_blank"> Minsky 和 Papert </a>于 1969 年完善并仔细分析。这是我之前关于<a class="ae km" rel="noopener" target="_blank" href="/mcculloch-pitts-model-5fdf65ac5dd1">麦卡洛克-皮茨神经元</a>模型和<a class="ae km" rel="noopener" target="_blank" href="/4d8c70d5cc8d">感知器</a>模型的帖子的后续。</p><p id="1ba6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">引用注:本文的概念、内容和结构完全基于 IIT 马德拉斯大学教授的</em><a class="ae km" href="https://www.cse.iitm.ac.in/~miteshk/" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir"><em class="kl">Mitesh Khapra</em></strong></a><em class="kl"/><strong class="jp ir"><em class="kl"/></strong><em class="kl">讲座幻灯片和视频</em> <a class="ae km" href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="noopener ugc nofollow" target="_blank"> <em class="kl"> CS7015:深度学习</em> </a> <em class="kl">。</em></p><h1 id="6c8a" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">感知器</h1><p id="2e3c" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">你可以浏览我之前关于感知器模型的帖子(上面有链接),但我假设你不会。因此，感知器不是我们今天在人工神经网络或任何深度学习网络中使用的 Sigmoid 神经元。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/6ffe2c19f204f83cb767347c8c6ac730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fyapb-JRFJ-VtnLYLLXCwg.png"/></div></div></figure><p id="0b87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感知器模型是比麦卡洛克-皮茨神经元更通用的计算模型。它接受一个输入，对其进行聚合(加权和)，只有当聚合和大于某个阈值时才返回 1，否则返回 0。如上所示重写阈值，并使其成为具有可变权重的常量输入，我们最终会得到如下结果:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/79d9dd18774e77dee8646185ffa01ed7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gKFs7YU44vJFiS2rF3-bpg.png"/></div></div></figure><p id="ece1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">单个感知器只能用于实现<strong class="jp ir">线性可分的</strong>功能。它接受实数和布尔输入，并将一组<strong class="jp ir">权重</strong>与它们相关联，同时还有一个<strong class="jp ir">偏差</strong>(我上面提到的阈值)。我们学习权重，我们得到函数。让我们用一个感知器来学习一个 OR 函数。</p><h2 id="4b59" class="mc ko iq bd kp md me dn kt mf mg dp kx jy mh mi lb kc mj mk lf kg ml mm lj mn bi translated">或使用感知器的功能</h2><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mo"><img src="../Images/f978282f56c44bb5457e434570b16a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5LeL8JDfoGbkUg0cu1M-w.png"/></div></div></figure><p id="6a04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面所发生的是，我们基于不同输入集的 or 函数输出定义了几个条件(当输出为 1 时，加权和必须大于或等于 0)，我们基于这些条件求解权重，并且我们得到了一条完美地将正输入与负输入分开的线。</p><p id="5ff5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">没有任何意义吗？也许现在是你浏览我所说的那篇文章的时候了。Minsky 和 Papert 还提出了一种使用一组示例(数据)学习这些权重的更具原则性的方法。请注意，这不是一个乙状结肠神经元，我们不会做任何梯度下降。</p><h1 id="9d84" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated"><strong class="ak">热身——线性代数基础</strong></h1><h2 id="64b8" class="mc ko iq bd kp md me dn kt mf mg dp kx jy mh mi lb kc mj mk lf kg ml mm lj mn bi translated">矢量</h2><p id="79d9" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">向量可以用多种方式定义。对物理学家来说，矢量是位于空间任何地方的任何东西，有大小和方向。对于一个 CS 爱好者来说，向量只是一个用来存储一些数据的数据结构——整数、字符串等等。在本教程中，我希望你用数学家的方式想象一个矢量，其中矢量是一个箭头，它的尾部在原点，在空间中延伸。这不是描述向量的最好的数学方法，但是只要你有直觉，你就可以做得很好。</p><p id="f28d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">注:以下截图是我从</em><a class="ae km" href="http://www.3blue1brown.com/" rel="noopener ugc nofollow" target="_blank"><em class="kl">3 blue 1 brown</em></a><em class="kl">的视频上借来的</em><a class="ae km" href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="noopener ugc nofollow" target="_blank"><em class="kl">Vectors</em></a><em class="kl">。如果你还不知道他，请查看他的系列文章</em><a class="ae km" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="noopener ugc nofollow" target="_blank"><em class="kl"/></a><em class="kl"/><a class="ae km" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr" rel="noopener ugc nofollow" target="_blank"><em class="kl">微积分</em> </a> <em class="kl">。当谈到数学可视化时，他简直不可思议。</em></p><h2 id="af83" class="mc ko iq bd kp md me dn kt mf mg dp kx jy mh mi lb kc mj mk lf kg ml mm lj mn bi translated"><strong class="ak">矢量表示法</strong></h2><p id="0121" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">二维向量可以在 2D 平面上表示如下:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/63b34b6ad9c98f6b923a9b38c0206023.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*WfqYmJ_bWDIdGvTQOjFZ2A.png"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk"><em class="mu">Source: </em><a class="ae km" href="http://www.3blue1brown.com/" rel="noopener ugc nofollow" target="_blank"><em class="mu">3Blue1Brown</em></a><em class="mu">’s video on </em><a class="ae km" href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="noopener ugc nofollow" target="_blank"><em class="mu">Vectors</em></a></figcaption></figure><p id="b860" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将这一思想推进到 3 维，我们在 3 维空间中得到如下箭头:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/3ac561a5fd2092026c4bbf85782d80ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*KFSKU4q6InY1PLprWfy1ig.png"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk"><em class="mu">Source: </em><a class="ae km" href="http://www.3blue1brown.com/" rel="noopener ugc nofollow" target="_blank"><em class="mu">3Blue1Brown</em></a><em class="mu">’s video on </em><a class="ae km" href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="noopener ugc nofollow" target="_blank"><em class="mu">Vectors</em></a></figcaption></figure><h2 id="e49f" class="mc ko iq bd kp md me dn kt mf mg dp kx jy mh mi lb kc mj mk lf kg ml mm lj mn bi translated">两个向量的点积</h2><p id="12ee" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">以使本教程更加枯燥为代价，让我们看看什么是点积。假设你有两个向量 oh size <strong class="jp ir"> n+1 </strong>、<strong class="jp ir"> w </strong>和<strong class="jp ir"> x </strong>，这些向量的点积(<strong class="jp ir"> <em class="kl"> w.x </em> </strong>)可以计算如下:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/e72e42cb10fb4394a459895afb17a897.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0zI1zKIOakgNuPMIg__UJg.png"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">The transpose is just to write it in a matrix multiplication form.</figcaption></figure><p id="6c81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，<strong class="jp ir"> w </strong>和<strong class="jp ir"> x </strong>只是一个<strong class="jp ir"> n+1 维</strong>空间中的两个孤独的箭头(直观地说，它们的点积量化了一个向量向另一个向量的方向移动了多少)。所以从技术上讲，感知器只是在计算一个蹩脚的点积(在检查它是大于还是小于 0 之前)。感知器给出的区分正例与反例的判定边界线实际上就是<strong class="jp ir"> w . x </strong> = <strong class="jp ir"> </strong> 0。</p><h2 id="f5a9" class="mc ko iq bd kp md me dn kt mf mg dp kx jy mh mi lb kc mj mk lf kg ml mm lj mn bi translated">两个向量之间的角度</h2><p id="7725" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">现在，如果你知道向量之间的角度和它们各自的大小，同样的点积可以用不同的方法计算。方法如下:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/fb70af13912bde75ab4dacd81ddc1cfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*-VwGx_ttwoyzYFKAIdX0SQ.png"/></div></figure><p id="9ed1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">反过来，你可以得到两个向量之间的角度，只要你知道向量，只要你知道如何计算向量的大小和它们的点积。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi my"><img src="../Images/f00f2945643855a7a0ae9e2be87b31a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*Gltd0rXq2ne62BscH6Vf2A.png"/></div></figure><p id="d5b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我说<strong class="jp ir"> w </strong>和<strong class="jp ir"> x </strong>的夹角余弦为 0 时，你看到了什么？我看到箭头<strong class="jp ir"> w </strong>垂直于箭头<strong class="jp ir"> x </strong>在<strong class="jp ir"> </strong>一个 n+1 维空间(说实话在 2 维空间)。所以基本上，当两个向量的点积为 0 时，它们是互相垂直的。</p><h2 id="3168" class="mc ko iq bd kp md me dn kt mf mg dp kx jy mh mi lb kc mj mk lf kg ml mm lj mn bi translated">设置问题</h2><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mz"><img src="../Images/593ea02665e21a1df5e3292d622de0fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NZ6814J6M0xB-P896pylrA.png"/></div></div></figure><p id="c149" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用一个感知器来估计我是否会根据上述输入的历史数据来观看电影。数据有正反两个例子，正的是我看的电影即 1。基于这些数据，我们将使用感知器学习算法来学习权重。为了视觉上的简单，我们将只假设二维输入。</p><h1 id="2b1c" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">感知机学习算法</h1><p id="1cf5" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">我们的目标是找到能够完美分类我们数据中的正输入和负输入的<strong class="jp ir"> w </strong>向量。我将直接进入算法。这是:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/eca031ec6a4d6af40eeb1dfeb12e87e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*PbJBdf-WxR0Dd0xHvEoh4A.png"/></div></figure><p id="e0fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们用一些随机向量初始化<strong class="jp ir"> w </strong>。然后我们迭代数据中的所有例子，(<em class="kl"> P </em> U <em class="kl"> N </em>)包括正面和负面的例子。现在，如果一个输入<strong class="jp ir"> x </strong>属于<em class="kl"> P </em>，理想情况下，点积<strong class="jp ir"> w.x </strong>应该是多少？我会说大于或等于 0，因为这是我们的感知机在一天结束时唯一想要的，所以让我们给它。而如果<strong class="jp ir"> x </strong>属于<em class="kl"> N </em>，那么点积必须小于 0。因此，如果您查看 while 循环中的 if 条件:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/14d27ae44135e602d32fafd0fe4daaa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*iXjNpsP40-UYvwxkfQwUhw.png"/></div></figure><p id="0c8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">情况 1: </strong>当<strong class="jp ir"> x </strong>属于<em class="kl"> P </em>及其点积<strong class="jp ir">w . x</strong>T79】0<br/><strong class="jp ir">情况 2: </strong>当<strong class="jp ir"> x </strong>属于<em class="kl"> N </em>及其点积<strong class="jp ir"> w.x </strong> ≥ 0</p><p id="857d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">只有在这些情况下，我们才更新随机初始化的<strong class="jp ir"> w </strong>。否则，我们根本不接触<strong class="jp ir"> w </strong>，因为案例 1 和案例 2 违反了感知器的规则。所以我们在例 1 中把<strong class="jp ir"> x </strong>加到<strong class="jp ir"> w </strong>(咳咳矢量相加咳咳)，在例 2 中把<strong class="jp ir"> w </strong>减去<strong class="jp ir"> x </strong>。</p><h2 id="028d" class="mc ko iq bd kp md me dn kt mf mg dp kx jy mh mi lb kc mj mk lf kg ml mm lj mn bi translated">为什么指定的更新规则会起作用？</h2><p id="e01e" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">但是为什么会这样呢？如果你已经明白了为什么会这样，你就明白了我这篇文章的全部主旨，现在你可以继续你的生活了，谢谢你的阅读，再见。但是如果你不确定为什么这些看似任意的 x<strong class="jp ir">x</strong>和<strong class="jp ir"> w </strong>的运算会帮助你学习到完美的<strong class="jp ir"> w </strong>可以完美的分类<em class="kl"> P </em>和<em class="kl"> N </em>，请继续使用我的方法。</p><p id="5968" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们已经建立了当<strong class="jp ir"> x </strong>属于<em class="kl"> P </em>时，我们要<strong class="jp ir">w . x</strong>T80】0，基本感知器法则。我们这样说也是指当<strong class="jp ir"> x </strong>属于<em class="kl"> P </em>时，<strong class="jp ir"> w </strong>与<strong class="jp ir"> x </strong>之间的角度应大于 90 度。填空。</p><p id="5659" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">答:<strong class="jp ir"> w </strong>和<strong class="jp ir"> x </strong>之间的角度应该小于 90°，因为角度的余弦与点积成正比。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/ca6a3e4abee41a9c61107d7de4517d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*AjFrPxeZEot-apPCAC3mSQ.png"/></div></figure><p id="960a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以不管<strong class="jp ir"> w </strong>向量可能是什么，只要它与正例数据向量(<strong class="jp ir"> x </strong> E <em class="kl"> P </em>)的角度小于 90 度，与负例数据向量(<strong class="jp ir"> x </strong> E <em class="kl"> N </em>)的角度大于 90 度，我们就没事了。理想情况下，它应该是这样的:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/dfe92039feffaeac4b7c9da0d427e229.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*D09EzbR-sGbX-qv2jcEPhw.png"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">x_0 is always 1 so we ignore it for now.</figcaption></figure><p id="17e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以我们现在强烈认为当<strong class="jp ir"> x </strong>属于<em class="kl"> P </em>类时<strong class="jp ir"> w </strong>和<strong class="jp ir"> x </strong>之间的角度应该小于 90°，当<strong class="jp ir"> x </strong>属于<em class="kl"> N </em>类时它们之间的角度应该大于 90°。停下来，说服你自己，上面的陈述是真实的，你确实相信它们。这就是为什么这一更新有效的原因:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ne"><img src="../Images/7fc1691a20bfdcc8a88734c2a95fb757.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ny1n6xH8g2JR2XVhcGRVvA.png"/></div></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">Now this is slightly inaccurate but it is okay to get the intuition.</figcaption></figure><p id="5827" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，当我们将<strong class="jp ir"> x </strong>与<strong class="jp ir"> w </strong>相加时，当 x 属于 P 并且<strong class="jp ir">w . x</strong>T36】0(情况 1)时，我们实际上是<strong class="jp ir">增加<em class="kl"> cos(alpha) </em> </strong>值，这意味着<strong class="jp ir">减少<em class="kl"> alpha </em>值</strong>，即<strong class="jp ir"> w </strong>与<strong class="jp ir"> x </strong>、<strong class="jp ir">之间的角度类似的直觉也适用于<strong class="jp ir"> x </strong>属于<em class="kl"> N </em>且<strong class="jp ir"> w.x </strong> ≥ 0 的情况(情况 2)。</strong></p><p id="666a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里有一个玩具模拟，模拟了我们最终可能如何学习正面例子中小于 90 度的角度和负面例子中大于 90 度的角度。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/9a027f385d63e850813aae4d1bd98ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/1*Nnb6IQW1qAkXi0LG6CTEGQ.gif"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">We start with a random vector <strong class="bd ng">w</strong>.</figcaption></figure><h1 id="461f" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">收敛性的证明</h1><p id="49d0" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">现在，你没有理由相信这一定会收敛于所有类型的数据集。看起来可能会有这样的情况，w 继续四处移动，永远不会收敛。但是人们已经证明了这个算法是收敛的。我附上哥伦比亚大学的迈克尔·柯林斯教授的证明— <a class="ae km" href="http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf" rel="noopener ugc nofollow" target="_blank">在这里找到论文</a>。</p><h1 id="a4e5" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">结论</h1><p id="531c" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">在这篇文章中，我们快速地看了一下什么是感知机。然后我们用线性代数的一些基础知识热身。然后我们看了一下<em class="kl">感知器学习算法</em>，然后继续想象它为什么工作，即如何学习适当的权重。</p><p id="9a87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢你阅读这篇文章。自己活也让别人活！<br/>答</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi nh"><img src="../Images/ad1f120e49e2513fae15ed2098a215a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x2gUD6BhNjkF7ac4UEwg5Q.jpeg"/></div></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">Photo by <a class="ae km" href="https://unsplash.com/photos/5mZ_M06Fc9g?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Roman Mager</a> on <a class="ae km" href="https://unsplash.com/search/photos/math?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div>    
</body>
</html>