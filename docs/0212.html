<html>
<head>
<title>Markov Chains— A take on Life</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">马尔可夫链——对生活的一种理解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/markov-chains-a-take-on-life-35614859c99c?source=collection_archive---------0-----------------------#2017-03-31">https://towardsdatascience.com/markov-chains-a-take-on-life-35614859c99c?source=collection_archive---------0-----------------------#2017-03-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="fc39" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">给定现在，未来独立于过去</p><p id="fbe6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在我最近上的另一堂课的第24张幻灯片中，有一句看似无关紧要的话，却完全揭示了我们是如何思考和行动的(至少我是这样思考和行动的)。在我开始之前，让我给你简单介绍一下我是如何偶然发现这句话的。我还将冒昧地提供一个小窥视模型的工作原理，其标语可能价值数百万美元。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="ae2e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">最近我开始看<a class="ae kt" href="https://www.youtube.com/playlist?list=PLHOg3HfW_teiYiq8yndRVwQ95LLPVUDJe" rel="noopener ugc nofollow" target="_blank">这些</a>关于<a class="ae kt" href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel="noopener ugc nofollow" target="_blank">强化学习</a>的讲座，由<a class="ae kt" href="https://deepmind.com/" rel="noopener ugc nofollow" target="_blank">谷歌deep mind</a>alpha go团队的首席程序员大卫·西尔弗主讲。对于外行人来说，<em class="ks">强化学习是机器学习的一个子领域，它处理基于代理人收到的奖励做出决策的过程</em>。请允许我更清楚地说明这一点:代理人可以实现的任何目标都可以用期望累积报酬的最大化来描述。通俗地说，如果一个代理想要完成它的目标，那么与它为达到目标所采取的行动相关的回报将会最大化。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ku"><img src="../Images/b9e015c92a83970923033ef260863e82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qhZwsXiweQcHYtvvb39xdg.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">An example of RL for the robot agent. Image from <a class="ae kt" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch01.html" rel="noopener ugc nofollow" target="_blank">Safaribooksonline</a></figcaption></figure><p id="95fd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">举个例子，考虑让一个人形机器人学会走路的过程；正奖励可能由机器人到达目的地(或在目的地的方向上迈出每一步)构成，而负奖励可能与机器人摔倒的动作相关联，或采取使其远离目标的动作相关联(这是一个天真的例子，因为如果机器人正在探索更好的全局路径以达到目标，则它可能实际上在远离目标时获得正奖励)。正是通过这些积极和消极奖励的结合，机器人最终学会了如何到达它的潜在目的地。</p><p id="e9cc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这种过程可以很容易地通过代理人处于某个状态，并通过考虑所有立即可获得的未来状态的回报并朝着回报最大的状态移动而采取行动来可视化(是的，这听起来有点不真实，但相信我这不是)。不难分析，一个贪婪的代理人会采取一种行动，这种行动对应于从当前状态可能获得的最高回报。</p><p id="20c2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">马尔可夫链是强化学习的支柱，因为它们以一种非常简单的方式帮助建立决策的概念；代理已经处于的状态的整个序列可以归结为它的当前状态，即，下一个可到达的状态可以由代理的当前状态来预测，而不管代理已经处于的状态的历史序列。下图简洁地说明了这个想法:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/14da3e607c1eaf0c6b357be68529419d.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*N2udQkkUFYGD8hb_WiosGg.png"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">A simple Markov Model</figcaption></figure><p id="2397" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">边上的标签表示从当前状态移动到下一个状态的概率。例如，如果今天下雨，明天下雪的概率是0.02，下雨的概率是0.8等等。</p><p id="60f5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">另一个解释马尔可夫链的简单例子是:假设你正在访问一个群岛，有桥连接着这些岛屿。这些桥代表了从一个岛移动到另一个岛的可能性。你明天要去的岛是由你今天所在的岛决定的，你以前的职位与这个决定无关。</p><p id="4567" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于面向数学的，马尔可夫模型的公式可以描述为:</p><p id="076e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">一个状态<em class="ks"> S_t+1 </em>是<strong class="jw ir">马尔可夫</strong>当且仅当(原谅没有下标) :</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ll"><img src="../Images/33a25780e5ce0b3fa796bd078cb3e693.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*WqhNxSqZZAi0rn4P4fg99Q.png"/></div></div></figure><p id="e62c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">解释:在给定代理处于[RHS]的整个状态序列的情况下，移动到下一个状态的概率等于在给定当前状态[LHS]的情况下，移动到下一个状态的概率。换句话说，由于过去状态的所有信息已经浓缩在代理的当前状态中，我们可以假设从当前状态到下一个状态的转移概率完全取决于当前状态。</p><p id="fde8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这也可以表述为:</p><p id="a7e8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><em class="ks">鉴于现在，未来独立于过去。</em></p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="2bac" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">正是第一堂课幻灯片中的这一行，让我意识到这句话不仅适用于马尔可夫模型，也适用于我们所有人的生活。塑造我们未来的决定完全取决于我们在当前心态下做出的选择；我们过去的所有经历都隐含在我们现在的状态中，因为它们引导我们走到了今天。</p><p id="5358" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对过去的选择感到遗憾不会改变宇宙中事情将如何发展的更大计划，哀叹只会让我们付出代价，并模糊我们未来的决策过程。所以，让我们试着通过充分利用我们此刻所处的状态来放下过去。</p><p id="e77d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们通过分析给定我们当前情况下所有可能行动的回报，来模拟一个理性主体做出最优决策。还有什么比人类一直做出理性决策更好的AI，我说的对吗！；-)</p></div></div>    
</body>
</html>