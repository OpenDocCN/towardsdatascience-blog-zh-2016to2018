<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://towardsdatascience.com/interpretability-of-deep-learning-models-9f52e54d72ab?source=collection_archive---------4-----------------------#2018-11-17">https://towardsdatascience.com/interpretability-of-deep-learning-models-9f52e54d72ab?source=collection_archive---------4-----------------------#2018-11-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><p id="f548" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">深度神经网络(DNN)的模型可解释性一直是用例的一个限制因素，这些用例需要解释建模中涉及的功能，许多行业(如金融服务)都是如此。无论是通过监管还是通过选择，金融机构都更喜欢易于人类解读的结构模型，这就是这些行业中深度学习模型采用缓慢的原因。一个关键用例的例子是风险模型，其中银行通常更喜欢经典的统计方法，如广义线性模型、贝叶斯模型和传统的机器学习模型，如基于树的模型，这些模型很容易用人类的直觉来解释和说明。</p><p id="c830" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">可解释性从一开始就是一个重要的研究领域，因为深度学习模型可以实现高精度，但以高抽象性为代价(即<em class="jn">精度 vs 可解释性问题</em>)。这一点也很重要，因为<em class="jn">信任</em>，因为一个不被信任的模型是不会被使用的模型(即尝试向高层管理人员销售一个黑盒模型)。</p><p id="ef17" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">要理解这个问题，只需想象一个简单的多层 DNN，它试图使用许多特征变量来预测一个项目的价格，并问自己:</p><ul class=""><li id="a733" class="jo jp iq ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">就解释结果而言，每个连接的权重意味着什么？</li><li id="f671" class="jo jp iq ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">哪一组权重在最终预测中起最重要的作用？</li><li id="03e9" class="jo jp iq ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">知道权重的大小能告诉我输入变量的重要性吗？</li></ul><p id="490d" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">首先，神经网络中的权重是每个神经元之间每个连接有多强的度量。因此，观察 DNN 的第一个致密层，你可以知道第一层神经元和输入之间的连接有多强。第二，在第一层之后，你失去了<em class="jn">一对多</em>的关系，它变成了一个巨大的<em class="jn">多对多</em>的纠缠混乱。这意味着一层中的神经元可能与远处的某个其他神经元相关(即，神经元由于反向传播而经历非局部性效应)。最后，权重确实讲述了关于输入的故事，但是在应用激活函数(即非线性)之后，它们所具有的信息在神经元中被压缩，使得很难解码。难怪深度学习模型被称为黑盒。</p><p id="7718" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">用于解释神经网络的方法通常分为两大类:( 1)显著性方法，和(2)特征属性(FA)</p><p id="373f" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">显著性方法擅长可视化网络内部正在发生的事情，并回答如下问题:(1)给定一些输入，哪些权重被激活？或者(2)图像的哪些区域被特定的卷积层检测到？。在我们的例子中，这不是我们所追求的，因为它并没有告诉我们任何关于哪个特征在描述我们的最终预测时是“最好”的。</p><p id="4171" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">FA 方法是一种试图将结构模型拟合到数据子集上的方法，其方式是找出每个变量在输出变量中的解释力。S. Lundberg，S. Lee 在 NIPS 2017 年的一篇论文(<a class="ae kc" href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf" rel="noopener ugc nofollow" target="_blank">参见论文</a>)中表明，所有模型，如<em class="jn"> LIME、DeepLIFT 和逐层相关性传播</em>都是一个更大的方法家族的一部分，该家族被称为附加特征属性(AFA)方法，其定义如下:</p><p id="3786" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir kd">加性特征归因方法</strong> <em class="jn">有一个解释模型，就是二元变量的线性函数:</em></p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi ke"><img src="../Images/8aadafefaa0e23b2e2a9216e347c1855.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*986PLqRsnh5KNj6NmOdn7A.png"/></div></figure><p id="03c9" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><em class="jn">其中 z’是{0,1}^M 的子集，m 是简化特征的数量，Phi_i 是权重(即贡献或效果)。</em></p><p id="a376" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><em class="jn"> LIME、DeepLIFT 和逐层相关性传播</em>都试图最小化目标函数以逼近 g(z’)。所以它们都是 AFA 方法家族的一部分。</p><p id="f0db" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">在同一篇论文中，作者提出了一种基于博弈论的估计这个线性模型的贡献(<em class="jn"> Phi_i </em>)的方法，称为 Shapley 值。为什么要引入博弈论呢？</p><p id="8018" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">博弈论中有一个分支研究合作游戏，其目标是预测所有玩家之间最公平的财富分配(即支出)，这些玩家共同努力实现共同的结果。这恰好是作者提出的:<strong class="ir kd">使用 Shapley 值作为特征变量对模型输出预测贡献的度量。</strong></p><p id="ee31" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">作者表明，Shapley 值是特定类型的合作博弈的最优解，AFA 方法也是其中的一部分(<a class="ae kc" href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf" rel="noopener ugc nofollow" target="_blank">参见论文</a>)。最后，他们提出了<strong class="ir kd"><em class="jn">【SHAP】(沙普利加法解释)</em> </strong>值作为特征重要性的统一度量，以及一些有效生成它们的方法，如:</p><ul class=""><li id="b14b" class="jo jp iq ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">内核 SHAP(线性石灰+沙普利值)</li><li id="c27d" class="jo jp iq ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">深度 SHAP(深度提升+沙普利值)</li><li id="7d0d" class="jo jp iq ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">树 SHAP(树解释器+ Shapley 值)</li></ul><p id="661b" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">在本文中，我将向您展示如何将 DeepSHAP 应用于神经网络，但在开始之前，您必须安装以下 python 库:</p><ul class=""><li id="faf3" class="jo jp iq ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">形状:<a class="ae kc" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank">https://github.com/slundberg/shap</a>(&gt;pip 安装形状)</li><li id="2497" class="jo jp iq ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">海生的</li><li id="4e82" class="jo jp iq ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">熊猫</li><li id="d9a3" class="jo jp iq ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">keras +张量流</li><li id="a7ae" class="jo jp iq ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">Jupyter notebook 是必需的(因为 shap 使用了一些有用的 javascript 情节)</li></ul><h1 id="0a96" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">DeepSHAP 示例:</h1><p id="a4a7" class="pw-post-body-paragraph io ip iq ir b is lk iu iv iw ll iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ij bi translated">我们将使用 seaborn 附带的钻石数据集。这是一个回归问题，我们试图根据钻石的质量来估计钻石的价格。</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="lp lq l"/></div></figure><p id="9653" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">我们的数据看起来像</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/cd88590c65beeb48d8adc6872bacff6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*rZUcAf0gNRmxozaRf7W8dw.png"/></div></figure><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ls"><img src="../Images/7eae14b2f869470af17d7c19ad10ffa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4nGDUAuBUjToH9J2DKgxNg.png"/></div></div></figure><p id="6d25" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">让我们获取目标和特征变量，并对分类变量进行编码:</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="lp lq l"/></div></figure><p id="9d43" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">这为我们以后的使用提供了映射</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/0c16f6151f2553e0b1760b0ecd70c0fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*53jrL4Oy3viOAsFOpsGkGQ.png"/></div></figure><p id="40e7" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">现在让我们开始训练和测试设置:</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="lp lq l"/></div></figure><p id="2cfa" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">现在让我们建立一个神经网络模型，使用一个简单的前馈全连接网络(用<a class="ae kc" href="https://en.wikipedia.org/wiki/Early_stopping" rel="noopener ugc nofollow" target="_blank">提前停止</a>)</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="lp lq l"/></div></figure><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/47e868c92f6f5dfb26491ac92d10f7b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*PN6Eq4yiXlzyubt6Y6w7TA.png"/></div></figure><p id="5624" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">我的设置有一个 NVDIA GPU P4000，所以它运行非常快，如果你没有 GPU，只需根据你的内存限制增加你的批处理大小。因此，15 秒后，我们得到:</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi lz"><img src="../Images/aeb7a23f868333275d1c3d075ac8c337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Wc5p3nrSw5GeGUumc-lXQ.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">The final mape loss is around 15% which is good enough for our purposes. The bumps are probably the large batch sizes.</figcaption></figure><h2 id="f724" class="me kn iq bd ko mf mg dn ks mh mi dp kw ja mj mk la je ml mm le ji mn mo li mp bi translated">现在 DeepSHAP:</h2><p id="add3" class="pw-post-body-paragraph io ip iq ir b is lk iu iv iw ll iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ij bi translated">由于 shap python 库，使用 DeepSHAP 相当容易。只需按如下方式进行设置:</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">(Beware: You should use a jupyter notebook to view a specific type of visualization that is very powerful communicating the explanatory power of a variable called force plots)</figcaption></figure><p id="3585" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">方法<em class="jn"> shap_values(X) </em>使用 shapley 采样执行基于深度提升的 shap 算法的拟合。你选择的数据越多，花费的时间就越长，所以要小心。</p><p id="b9f9" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">现在，让我们来看看模型推断出的一些个人属性:</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="lp lq l"/></div></figure><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mq"><img src="../Images/183300166287fd58d2b4bc329b99a31b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KubSdxduPSWE8Z_aOsLBvg.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Figure 1. Force plot for one record (i.e. item) in the dataset</figcaption></figure><p id="977d" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">为了理解这些力图，你需要回到作者的论文:</p><blockquote class="mr ms mt"><p id="f2be" class="io ip jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ij bi translated">…SHAP (SHapley 附加解释)值将预期模型预测的变化归因于每个特征。它们解释了如何从基值 E[f(z)]中获得，如果我们不知道当前输出 f(x)的任何特征，则可以预测该基值。此图显示了单一订购。然而，当模型是非线性的或者输入特征不是独立的时，将特征添加到期望值的顺序很重要，并且 SHAP 值是通过在所有可能的排序中平均φi 值而产生的。</p></blockquote><p id="b8eb" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">请注意，这里有一个<strong class="ir kd"> <em class="jn">基值</em> </strong>，它是由 DeepSHAP 计算出的预期值(即 E[f(z)]),如果您不知道任何特性，它就是预测值。还有这个<strong class="ir kd"> <em class="jn">输出值(即所有特征贡献和基值之和)</em> </strong>等于实际模型的预测。那么，SHAP 值只是告诉你从<strong class="ir kd"> <em class="jn">基值</em> </strong>到<strong class="ir kd"> <em class="jn">输出值</em> </strong>每个特征增加了多少贡献。</p><p id="d0b6" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">对于图 1，可以这样理解:</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="lp lq l"/></div></figure><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/c63852b628fc40e8003cb10e40fec276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*39OBbv5-G6hb8R1RLfKDWg.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">figure 2. SHAP values from shap_df</figcaption></figure><p id="47ee" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">这些是从基础值到记录输出值(即数据集中的单个项目)的所有单个要素贡献，在图 1 中显示为力图。可以看到有不同量级的正负贡献。在这种情况下，<strong class="ir kd"> <em class="jn"> x、y、z、克拉、净度、切割</em>T3 和<strong class="ir kd">T5】深度 </strong>都有助于将值从基础值增加到模型输出预测的值，而<strong class="ir kd">颜色</strong>和<strong class="ir kd">表格</strong>有助于将值从基础值减少到预测值。另外，请注意<strong class="ir kd"><em class="jn">SHAP 值的单位与实际目标值(价格)</em> </strong>相同。</strong></p><p id="6958" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">下面是获得输出值的方法:</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi my"><img src="../Images/b02a62ccdc7ae63d1d9da9daa0aa4665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*WmFZ97bod3zfUaCB2uYuUg.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Figure 2. How to get the base value and output value</figcaption></figure><p id="dd18" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">尝试将它们相加，您会看到输出值等于实际模型预测值。这是一个非常强大和有见地的解释方法，帮助您理解目标变量单位方面的贡献。</p><p id="c299" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">现在，如果您想查看每个特征变量的总体贡献，您只需:</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="lp lq l"/></div></figure><p id="e1af" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">绘制贡献的平均值会得到图 3，图 3 显示了每个特征变量对整体平均模型输出的平均贡献。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/f4f8b11a636d16f136e9c25303a43614.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*izkp0l8lq53LOMFU8mvNAw.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Figure 3. Average impact of each feature en overall prediction magnitude</figcaption></figure><h1 id="9c97" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">额外奖励:石灰和子模块选择(SP)-石灰</h1><p id="3c83" class="pw-post-body-paragraph io ip iq ir b is lk iu iv iw ll iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ij bi translated">只是为了检查一下，让我们尝试一种更古老但也更有趣的方法。</p><p id="b551" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><a class="ae kc" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank"> <strong class="ir kd"> LIME </strong> </a>是一篇论文介绍的算法<em class="jn">“我为什么要相信你？”:解释马尔科·图利奥·里贝罗等人在 2016 年对任何分类器</em>的预测。</p><p id="a0bc" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir kd"> LIME </strong>是一种智能算法，通过使用可解释模型(即，在本文中，它是线性模型等)围绕单个预测执行局部近似，实现任何黑盒分类器或回归器的可解释性。).也就是说，它回答了问题<em class="jn">:我是否足够信任一个人的预测，并据此采取行动？因此，检查或调试单个预测是一个很好的算法。</em></p><p id="4c53" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir kd"> SP-LIME </strong>是一个试图回答问题<em class="jn">我信任这个模型吗？它通过将问题框架化为子模块优化问题来实现。也就是说，它选取模型的一系列实例及其相应的预测，以代表整个模型的性能。执行这些选择的方式是，解释更多不同实例的输入要素具有更高的重要性分数。</em></p><h2 id="2a40" class="me kn iq bd ko mf mg dn ks mh mi dp kw ja mj mk la je ml mm le ji mn mo li mp bi translated">石灰产生的结果:</h2><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="lp lq l"/></div></figure><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi na"><img src="../Images/4ee8237cea4979d867c6683bae78a61b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dBx3jyqndg0_H_MXVxajYw.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Figure 4. Result of LIME for the same individual record as before.</figcaption></figure><p id="12af" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">我们可以看到，特征 x、y、z、克拉、净度、切割和深度都对石灰模型有积极的贡献，尽管与 SHAP 的顺序不同(但它们几乎非常相似)。此外，要素表和颜色对石灰模型也有负面影响，从而确认 SHAP 值结果。</p><p id="fa3b" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir kd">SP-LIME 的结果:</strong></p><p id="8319" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">为了计算总的贡献，我们必须使用石灰。这有点繁琐，因为我们必须手动计算解释矩阵并获得每个特征的平均值。我使用的样本大小为 20，实验次数为 5，因为 SP 优化算法会占用大量内存。</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="lp lq l"/></div></figure><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nb"><img src="../Images/74575e6270905a88fdd52b818b0a2075.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x2BfnRx4TpL86WIr2YFUhA.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Figure 5. Result for SP-LIME</figcaption></figure><p id="081c" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">SP-LIME 的整体属性与 SHAP 的有一点点不同(虽然真的没有那么大)，但是请记住，我没有优化 SP-LIME 的参数。如果我使用更大的样本量并增加 SP 算法运行的实验次数，这将最有可能符合 SHAP，因为 LIME 被证明是一种 AFA 型算法，并且是 Shapley 值的近似值。</p><h1 id="7734" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">结论:</h1><p id="1886" class="pw-post-body-paragraph io ip iq ir b is lk iu iv iw ll iy iz ja lm jc jd je ln jg jh ji lo jk jl jm ij bi translated">我希望你已经学到了一些新的东西，并鼓励你用不同的方法在你自己的数据集中尝试它(例如 DNN，CNN，catboost 等)。).它还适用于分类和不同类型的数据，如图像数据和文本数据。如有任何问题，请发邮件至 eduardo.denadai[@]gmail.com</p></div></div>    
</body>
</html>