<html>
<head>
<title>The intuition behind Shannon’s Entropy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">香农熵背后的直觉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-intuition-behind-shannons-entropy-e74820fe9800?source=collection_archive---------3-----------------------#2018-09-29">https://towardsdatascience.com/the-intuition-behind-shannons-entropy-e74820fe9800?source=collection_archive---------3-----------------------#2018-09-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8966" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">【警告:太容易了！]</h2></div><p id="e9ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">伊恩·古德菲勒的《深度学习》一书<em class="lf">的<a class="ae le" href="https://www.deeplearningbook.org/contents/prob.html" rel="noopener ugc nofollow" target="_blank">第 3.13 章信息论</a>中写道:</em></p><blockquote class="lg lh li"><p id="6f18" class="ki kj lf kk b kl km ju kn ko kp jx kq lj ks kt ku lk kw kx ky ll la lb lc ld im bi translated">我们将事件 X = x 的<strong class="kk iu">自我信息定义为</strong></p><p id="207c" class="ki kj lf kk b kl km ju kn ko kp jx kq lj ks kt ku lk kw kx ky ll la lb lc ld im bi translated"><strong class="kk iu">I(x)=-log P(x)</strong></p><p id="cff2" class="ki kj lf kk b kl km ju kn ko kp jx kq lj ks kt ku lk kw kx ky ll la lb lc ld im bi translated">因此，我们对 I(x)的定义是以纳特为单位的。一个 nat 是通过观察概率为 1/e 的事件获得的信息量。</p><p id="c509" class="ki kj lf kk b kl km ju kn ko kp jx kq lj ks kt ku lk kw kx ky ll la lb lc ld im bi">…</p><p id="6de7" class="ki kj lf kk b kl km ju kn ko kp jx kq lj ks kt ku lk kw kx ky ll la lb lc ld im bi translated">我们可以使用<strong class="kk iu">香农熵来量化整个概率分布中的不确定性。</strong></p></blockquote><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/63aa89f3bbb911b55c270ae51b109caa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*csdD5q5tDJhphQZEvymhwA.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">The definition of Entropy for a probability distribution (from The Deep Learning Book)</figcaption></figure><h1 id="a328" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">但是这个公式是什么意思呢？</h1><p id="a55d" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">对于任何想要精通机器学习的人来说，理解香农熵是至关重要的。Shannon 熵产生了一个函数，它是 ML 从业者的面包和黄油——<strong class="kk iu">交叉熵</strong>在分类中大量用作损失函数，还有<strong class="kk iu">KL 散度</strong>广泛用于变分推理。</p><h2 id="ef17" class="mz md it bd me na nb dn mi nc nd dp mm kr ne nf mo kv ng nh mq kz ni nj ms nk bi translated">为了理解熵，我们需要从“比特”的角度开始思考。</h2><p id="b4b2" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">位为 0 或 1。</p><p id="1cfe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，用 1 比特，我们可以表示 2 个不同的事实(又名<strong class="kk iu">信息</strong>)，要么是 1，要么是 0(或真或假)。假设你是 1945 年二战的一名指挥官。你的电报员告诉你，如果纳粹投降，他会给你一个“1”，如果他们不投降，他会给你一个“0”。</p><p id="6d57" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2018 年，你可以在智能手机上输入完全相同的信息</p><p id="ae50" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“战争结束了”(我们用 8 位* 15 个字符= <strong class="kk iu"> 120 位</strong>，而不是 1 位)</p><p id="fd15" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">《战争还没有结束》(8 比特* 19 字符= <strong class="kk iu"> 152 比特</strong>)</p><p id="a433" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们使用 100 多位来发送一条信息<strong class="kk iu">,这条信息可以减少到一位。</strong></p><p id="e11e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设明天有四种可能的战争结果，而不是两种。1)德日双双投降。2)德国投降，日本不投降。3)日本投降，德国不投降。4)双方都不投降。现在你的电报员需要 2 位(00，01，10，11)来编码这个信息。同样，即使有 256 种不同的情况，他也只需要 8 位。</p><p id="60e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再正式一点，<strong class="kk iu">一个变量的熵</strong>就是这个变量所包含的“信息量”。你可以把变量想象成来自电报员的<strong class="kk iu">消息。新闻可以是任何东西。不一定是 4 个州，256 个州等等。在现实生活中，新闻可以是数百万个不同的事实。</strong></p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><p id="1c3e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，回到我们的公式 3.49:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/63aa89f3bbb911b55c270ae51b109caa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*csdD5q5tDJhphQZEvymhwA.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">The definition of Entropy for a probability distribution (from The Deep Learning Book)</figcaption></figure><p id="3bbd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> I(x) </strong>是<strong class="kk iu">X</strong>的信息内容。</p><p id="4e09" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">I(x)本身是一个随机变量。在我们的例子中，战争的可能结果。这样，<strong class="kk iu"> H(x) </strong>就是<strong class="kk iu">每一个可能信息的期望值。</strong></p><p id="7b1e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<a class="ae le" href="https://en.wikipedia.org/wiki/Expected_value" rel="noopener ugc nofollow" target="_blank">期望值</a>的定义，上述等式可以改写为</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ns"><img src="../Images/3e11034a4eb4be9a7f2d9c40298382ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*15VgXhmDZcKVam7IHpTP-g.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Because -log P(x) = log (1/P(x))</figcaption></figure><blockquote class="nt"><p id="b2d8" class="nu nv it bd nw nx ny nz oa ob oc ld dk translated">等等…为什么我们要取概率的倒数？</p></blockquote><p id="58cf" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">H(X)是一个<strong class="kk iu">整体</strong>概率分布中的<strong class="kk iu">总信息量</strong>。<strong class="kk iu"> </strong>这个意思是<strong class="kk iu"> 1/p(x) </strong>应该是<strong class="kk iu">每个案例的信息</strong>(打赢战争，输掉战争等)。</p><h2 id="42c0" class="mz md it bd me na nb dn mi nc nd dp mm kr ne nf mo kv ng nh mq kz ni nj ms nk bi translated"><strong class="ak">那么问题就是……</strong></h2><h1 id="fb0b" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">为什么 1/p(x)是信息量？</h1><p id="541a" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">假设纳粹有 50%的机会投降(p = 1/2)。然后，如果你的电报员告诉你他们确实投降了，你就可以消除 total 2 事件(投降和不投降)的不确定性，这是 p 的倒数(=1/2)。</p><p id="2666" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当你的所有事件发生的可能性都相等，并且你知道一个事件刚刚发生，你可以排除所有其他事件(总共 1/p 个事件)发生的可能性。例如，假设有 4 个事件，它们发生的可能性都相等(p = 1/4)。当一个事件发生时，它表示其他三个事件没有发生。因此，我们知道总共 4 个事件发生了什么。</p><h2 id="124c" class="mz md it bd me na nb dn mi nc nd dp mm kr ne nf mo kv ng nh mq kz ni nj ms nk bi translated">那些不太可能发生的事件呢？</h2><p id="c8f5" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">假设纳粹有 75%的几率投降，有 25%的几率不投降。</p><blockquote class="lg lh li"><p id="75e9" class="ki kj lf kk b kl km ju kn ko kp jx kq lj ks kt ku lk kw kx ky ll la lb lc ld im bi translated">‘投降’这个事件有多少信息？</p></blockquote><p id="f4a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">log(1/0.75)= log(1.333)= 0.41</strong>(向前省略以 2 为基数的对数)</p><blockquote class="lg lh li"><p id="1968" class="ki kj lf kk b kl km ju kn ko kp jx kq lj ks kt ku lk kw kx ky ll la lb lc ld im bi translated">'不投降'事件有多少信息？</p></blockquote><p id="0192" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> log (1/0.25) = log(4) = 2 </strong></p><p id="7ef9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如你所见，不太可能的事件有更高的熵。</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><p id="91de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是为什么信息是概率的倒数的直觉。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oi"><img src="../Images/fd3f74ad756815a3ed8581d8ca4a551a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mEIWwyolHOdY3TmBus7HtQ.png"/></div></div></figure><p id="5171" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">黑点就是新闻。</p><p id="3ba2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过了解黑点，我们可以同时消除另外 3 个白点。</p><p id="d3f4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">共 4 点(总信息)突发</strong>。</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><p id="cd67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，知道了<strong class="kk iu">一个</strong>黑点，我们总共能爆多少个<strong class="kk iu">点</strong>？</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oj"><img src="../Images/7a60765b3bb27b411d684277292c06ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zCWZhHcihzZlSHR8dy2aWQ.png"/></div></div></figure></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ok"><img src="../Images/e0d1dd3d04a710cab00987df90d2a99e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qJDURcpSpSxI9C_Z-D6gdQ.png"/></div></div></figure><p id="c1a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以消去总共 1 又 1/3 = 1.333 个点，也就是<strong class="kk iu">3/4 的倒数。</strong></p><blockquote class="nt"><p id="8cd0" class="nu nv it bd nw nx ol om on oo op ld dk translated"><strong class="ak">你能爆的总点数=每条新闻的信息量。</strong></p></blockquote><p id="b39e" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">于是，<strong class="kk iu">每一个可能新闻</strong>中的信息是 0.25 * log(4)+0.75 * log(1.333)= 0.81(香农熵公式。)</p><p id="6e8a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们知道 1/p 来自哪里了。但为什么是日志？香农认为任何事物的信息量都可以用比特来衡量。要用比特写一个数<em class="lf"> N </em>，我们需要取<em class="lf"> N </em>的以 2 为底的对数。</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h2 id="644c" class="mz md it bd me na nb dn mi nc nd dp mm kr ne nf mo kv ng nh mq kz ni nj ms nk bi translated">外卖食品</h2><p id="c04f" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">如果我们有 P(win) =1，熵就是 0。它没有一点不确定性。(-log1 = 0)</p><p id="7e2c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，热力学“熵”和信息论中的“熵”都捕捉到了不断增加的随机性。</p><p id="5a37" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，在我们的示例中，对于“同等可能性”的消息，熵(2 位)高于“不同等可能性”的消息(0.81 位)。这是因为在“不太可能”的消息中存在较少的不确定性。一个事件比另一个事件更有可能发生。这降低了不确定性。</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><p id="6ba3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于实施爱好者，这里是 Python 代码。</p><p id="0258" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看看字符数越多，不确定性(熵)越大。</p><pre class="ln lo lp lq gt oq or os ot aw ou bi"><span id="51bc" class="mz md it or b gy ov ow l ox oy">import math <br/>import random</span><span id="0460" class="mz md it or b gy oz ow l ox oy">def H(sentence): <br/>    """<br/>    Equation 3.49 (Shannon's Entropy) is implemented.<br/>    """<br/>    entropy = 0 <br/>    # There are 256 possible ASCII characters<br/>    for character_i in range(256): <br/>        Px = sentence.count(chr(character_i))/len(sentence) <br/>        if Px &gt; 0: <br/>            entropy += - Px * math.log(Px, 2) <br/>    return entropy</span><span id="8d5d" class="mz md it or b gy oz ow l ox oy"># The telegrapher creates the "encoded message" with length 10000.<br/># When he uses only 32 chars <br/>simple_message ="".join([chr(random.randint(0,32)) for i in range(10000)])</span><span id="f38b" class="mz md it or b gy oz ow l ox oy"># When he uses all 255 chars<br/>complex_message ="".join([chr(random.randint(0,255)) for i in range(10000)])</span><span id="385f" class="mz md it or b gy oz ow l ox oy"><strong class="or iu"># Seeing is believing.</strong></span><span id="2ae4" class="mz md it or b gy oz ow l ox oy"><strong class="or iu">In [20]: H(simple_message)<br/>Out[20]: 5.0426649536728 the </strong></span><span id="24a6" class="mz md it or b gy oz ow l ox oy"><strong class="or iu">In [21]: H(complex_message)<br/>Out[21]: 7.980385887737537</strong></span><span id="5df7" class="mz md it or b gy oz ow l ox oy"><strong class="or iu"># The entropy increases as the uncertainty of which character will be sent increases.</strong></span></pre></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><p id="505b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下一篇文章中，我将解释我们如何将香农熵扩展到交叉熵和 KL 散度。</p></div></div>    
</body>
</html>