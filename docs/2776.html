<html>
<head>
<title>Data Analytics with Python by Web scraping: Illustration with CIA World Fact-book</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过网络抓取使用 Python 进行数据分析:CIA World Fact-book 示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-analytics-with-python-by-web-scraping-illustration-with-cia-world-factbook-abbdaa687a84?source=collection_archive---------1-----------------------#2018-03-04">https://towardsdatascience.com/data-analytics-with-python-by-web-scraping-illustration-with-cia-world-factbook-abbdaa687a84?source=collection_archive---------1-----------------------#2018-03-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b1a7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在本文中，我们将展示如何使用 Python 库和 HTML 解析从网站中提取有用的信息，并回答一些重要的分析问题。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2283b7b8bb192555680512e9d5d15d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X2QkNgg-vR3NRnGDquRm9w.png"/></div></div></figure><p id="5e39" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi lq translated"><span class="l lr ls lt bm lu lv lw lx ly di">在</span>一个数据科学项目中，几乎总是最耗时和最混乱的部分是数据收集和清理。每个人都喜欢建立一两个很酷的深度神经网络(或 XGboost)模型，并用很酷的 3D 交互情节来展示自己的技能。但是这些模型需要原始数据作为开始，而且这些数据来的不容易也不干净。</p><blockquote class="lz ma mb"><p id="b0b9" class="ku kv mc kw b kx ky ju kz la lb jx lc md le lf lg me li lj lk mf lm ln lo lp im bi translated"><strong class="kw iu">毕竟，生活不是一个充满数据的 zip 文件等着你去解压和建模的游戏:-) </strong></p></blockquote><p id="c410" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">但是为什么要收集数据或建立模型呢</strong>？根本动机是回答一个商业或科学或社会问题。<em class="mc">有没有趋势</em>？<em class="mc">这个东西和那个</em>有关系吗？<em class="mc">对该实体的测量能否预测该现象的结果</em>？这是因为回答这个问题将验证你作为该领域科学家/从业者的假设。你只是在用数据(而不是像化学家那样用试管或像物理学家那样用磁铁)来测试你的假设，并从科学上证明/反驳它。<strong class="kw iu">这是数据科学的“科学”部分。不多不少……</strong></p><p id="91ad" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">相信我，想出一个高质量的问题并不难，这需要一点数据科学技术的应用来回答。每一个这样的问题都变成了你的一个小项目，你可以编写代码，在 Github 这样的开源平台上展示给你的朋友。即使你的职业不是数据科学家，也没有人能阻止你编写很酷的程序来回答一个好的数据问题。这表明你是一个善于处理数据的人，一个能用数据讲述故事的人。</p><p id="fde3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">今天让我们来解决这样一个问题…</p><blockquote class="lz ma mb"><p id="fe52" class="ku kv mc kw b kx ky ju kz la lb jx lc md le lf lg me li lj lk mf lm ln lo lp im bi translated"><strong class="kw iu">一个国家的 GDP(按购买力平价计算)和其互联网用户的百分比有什么关系吗？低收入/中等收入/高收入国家也有类似的趋势吗？</strong></p></blockquote><p id="45d3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，你能想到的收集数据来回答这个问题的来源有很多。我发现美国中央情报局(CIA)的一个网站是收集数据的好地方，这个网站上有世界各国的基本事实信息。</p><p id="a781" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，我们将使用以下 Python 模块来构建我们数据库和可视化，</p><ul class=""><li id="fbe0" class="mg mh it kw b kx ky la lb ld mi lh mj ll mk lp ml mm mn mo bi translated"><strong class="kw iu">熊猫</strong>，<strong class="kw iu"> Numpy，matplotlib/seaborn </strong></li><li id="2b25" class="mg mh it kw b kx mp la mq ld mr lh ms ll mt lp ml mm mn mo bi translated">Python <strong class="kw iu"> urllib </strong>(用于发送 HTTP 请求)</li><li id="1328" class="mg mh it kw b kx mp la mq ld mr lh ms ll mt lp ml mm mn mo bi translated"><strong class="kw iu"> BeautifulSoup </strong>(用于 HTML 解析)</li><li id="4d07" class="mg mh it kw b kx mp la mq ld mr lh ms ll mt lp ml mm mn mo bi translated"><strong class="kw iu">正则表达式模块</strong>(用于查找要搜索的精确匹配文本)</li></ul><p id="91ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">先说程序结构来回答这个数据科学的问题。在我的<a class="ae mu" href="https://github.com/tirthajyoti/Web-Database-Analytics-Python" rel="noopener ugc nofollow" target="_blank"> Github 库</a>中的这里有<a class="ae mu" href="https://github.com/tirthajyoti/Web-Database-Analytics-Python/blob/master/CIA-Factbook-Analytics2.ipynb" rel="noopener ugc nofollow" target="_blank">整个锅炉板代码。喜欢的话请随意叉星。</a></p><h2 id="2639" class="mv mw it bd mx my mz dn na nb nc dp nd ld ne nf ng lh nh ni nj ll nk nl nm nn bi translated">阅读前面的 HTML 页面并传递给 BeautifulSoup</h2><p id="f9c6" class="pw-post-body-paragraph ku kv it kw b kx no ju kz la np jx lc ld nq lf lg lh nr lj lk ll ns ln lo lp im bi translated">这是中情局世界概况首页的样子，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/3fe61f8068a72ee811017788545fd777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CjEOFPmEDpz5z-Wc_YOfNg.png"/></div></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Fig: CIA World Factbook front page</figcaption></figure><p id="f546" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们使用一个简单的带有 SSL 错误忽略上下文的 urllib 请求来检索这个页面，然后将它传递给神奇的 BeautifulSoup，后者为我们解析 HTML 并生成一个漂亮的文本转储。对于那些不熟悉 BeautifulSoup 库的人来说，他们可以观看下面的视频或阅读这篇关于 Medium 的<a class="ae mu" href="https://medium.freecodecamp.org/how-to-scrape-websites-with-python-and-beautifulsoup-5946935d93fe" rel="noopener ugc nofollow" target="_blank">大信息量文章。</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="b12f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是阅读首页 HTML 的代码片段，</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="db69" class="mv mw it ob b gy of og l oh oi">ctx = ssl.create_default_context()<br/>ctx.check_hostname = False<br/>ctx.verify_mode = ssl.CERT_NONE</span><span id="042c" class="mv mw it ob b gy oj og l oh oi"># Read the HTML from the URL and pass on to BeautifulSoup<br/>url = '<a class="ae mu" href="https://www.cia.gov/library/publications/the-world-factbook/'" rel="noopener ugc nofollow" target="_blank">https://www.cia.gov/library/publications/the-world-factbook/'</a><br/>print("Opening the file connection...")<br/>uh= urllib.request.urlopen(url, context=ctx)<br/>print("HTTP status",uh.getcode())<br/>html =uh.read().decode()<br/>print(f"Reading done. Total {len(html)} characters read.")</span></pre><p id="3dc3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面是我们如何将它传递给 BeautifulSoup，并使用<code class="fe ok ol om ob b">find_all</code>方法来查找 HTML 中嵌入的所有国家名称和代码。基本上，这个想法是<strong class="kw iu">找到名为‘option’</strong>的 HTML 标签。标签中文本是国家名称，标签值的第 5 和第 6 个字符代表 2 个字符的国家代码。</p><p id="8bfc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，你可能会问，你怎么知道你只需要提取第五和第六个字符呢？简单的回答是<strong class="kw iu">你必须检查 soup 文本，也就是你自己解析的 HTML 文本，并确定那些索引</strong>。没有通用的方法来确定这一点。每个 HTML 页面和底层结构都是独一无二的。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="9e08" class="mv mw it ob b gy of og l oh oi">soup = BeautifulSoup(html, 'html.parser')<br/>country_codes=[]<br/>country_names=[]</span><span id="720b" class="mv mw it ob b gy oj og l oh oi">for <strong class="ob iu">tag</strong> in soup.find_all('option'):<br/>    country_codes.append(<strong class="ob iu">tag.get('value')</strong>[5:7])<br/>    country_names.append(<strong class="ob iu">tag.text</strong>)</span><span id="db44" class="mv mw it ob b gy oj og l oh oi">temp=country_codes.pop(0) # <em class="mc">To remove the first entry 'World'</em><br/>temp=country_names.pop(0) # <em class="mc">To remove the first entry 'World'</em></span></pre><h2 id="8ec7" class="mv mw it bd mx my mz dn na nb nc dp nd ld ne nf ng lh nh ni nj ll nk nl nm nn bi translated">抓取:通过单独抓取每个页面，将所有国家的所有文本数据下载到字典中</h2><p id="2810" class="pw-post-body-paragraph ku kv it kw b kx no ju kz la np jx lc ld nq lf lg lh nr lj lk ll ns ln lo lp im bi translated">这一步是他们所说的必不可少的刮或爬。要做到这一点，<strong class="kw iu">要识别的关键是每个国家信息页面的 URL 是如何构建的</strong>。现在，在一般情况下，这可能很难得到。在这种特殊情况下，快速检查显示了一个非常简单和有规律的结构。这是澳大利亚的截图，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/cfa72d05927e2d5f433c132aedb6d2d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vYfbPogbxVdPhX9hoSUc6g.png"/></div></div></figure><p id="370a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这意味着有一个固定的网址，你必须附加 2 个字符的国家代码，你得到该国家的网页的网址。因此，我们可以遍历国家代码列表，使用 BeautifulSoup 提取所有文本并存储在本地字典中。下面是代码片段，</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="aa2c" class="mv mw it ob b gy of og l oh oi"># Base URL<br/>urlbase = '<a class="ae mu" href="https://www.cia.gov/library/publications/the-world-factbook/geos/'" rel="noopener ugc nofollow" target="_blank">https://www.cia.gov/library/publications/the-world-factbook/geos/'</a><br/># Empty data dictionary<br/>text_data=dict()</span><span id="b468" class="mv mw it ob b gy oj og l oh oi"># Iterate over every country<br/>for i in range(1,len(country_names)-1):<br/>    country_html=country_codes[i]+'.html'<br/>    <strong class="ob iu">url_to_get=urlbase+country_html</strong><br/>    # Read the HTML from the URL and pass on to BeautifulSoup<br/>    html = urllib.request.urlopen(url_to_get, context=ctx).read()<br/>    soup = BeautifulSoup(html, 'html.parser')<br/>    <strong class="ob iu">txt=soup.get_text()</strong><br/>    <strong class="ob iu">text_data[country_names[i]]=txt</strong><br/>    print(f"Finished loading data for {country_names[i]}")<br/>    <br/>print ("\n**Finished downloading all text data!**")</span></pre><h2 id="c11f" class="mv mw it bd mx my mz dn na nb nc dp nd ld ne nf ng lh nh ni nj ll nk nl nm nn bi translated">如果你喜欢，可以存放在泡菜堆里</h2><p id="c3d3" class="pw-post-body-paragraph ku kv it kw b kx no ju kz la np jx lc ld nq lf lg lh nr lj lk ll ns ln lo lp im bi translated">为了更好地衡量，我更喜欢将这些数据序列化并<strong class="kw iu">存储在一个</strong> <a class="ae mu" href="https://pythontips.com/2013/08/02/what-is-pickle-in-python/" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> Python pickle 对象</strong> </a> <strong class="kw iu"> </strong>中。这样，下次打开 Jupyter 笔记本时，我就可以直接读取数据，而无需重复网页抓取步骤。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="28be" class="mv mw it ob b gy of og l oh oi">import pickle<br/>pickle.dump(text_data,open("text_data_CIA_Factobook.p", "wb"))</span><span id="4cff" class="mv mw it ob b gy oj og l oh oi"># Unpickle and read the data from local storage next time<br/>text_data = pickle.load(open("text_data_CIA_Factobook.p", "rb"))</span></pre><h2 id="6ac7" class="mv mw it bd mx my mz dn na nb nc dp nd ld ne nf ng lh nh ni nj ll nk nl nm nn bi translated">使用正则表达式从文本转储中提取 GDP/人均数据</h2><p id="7f23" class="pw-post-body-paragraph ku kv it kw b kx no ju kz la np jx lc ld nq lf lg lh nr lj lk ll ns ln lo lp im bi translated">这是程序的核心文本分析部分，在这里我们借助<a class="ae mu" href="https://docs.python.org/3/howto/regex.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> <em class="mc">正则表达式</em> </strong>模块</a>在庞大的文本串中找到我们要找的东西并提取相关的数值数据。现在，正则表达式是 Python(或几乎所有高级编程语言)中的丰富资源。它允许在大型文本语料库中搜索/匹配特定的字符串模式。这里，我们使用非常简单的正则表达式方法来匹配精确的单词，如“<em class="mc"> GDP —人均(PPP): </em>”，然后读取其后的几个字符，提取某些符号(如$和括号)的位置，最终提取人均 GDP 的数值。这里用一个图来说明这个想法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/72c3a4dbf8c131f1e018882f69db1ac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1FgkmYUwds5pKIZC4HvkTw.png"/></div></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Fig: Illustration of the text analytics</figcaption></figure><p id="d6f5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本书中还使用了其他正则表达式技巧，例如，正确提取 GDP 总量，而不管该数字是以十亿还是万亿为单位。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="838a" class="mv mw it ob b gy of og l oh oi"># <strong class="ob iu">'b' to catch 'billions', 't' to catch 'trillions'</strong><br/>start = re.search('\$',string)<br/>end = <strong class="ob iu">re.search('[b,t]',string)</strong><br/>if (start!=None and end!=None):<br/>    start=start.start()<br/>    end=end.start()<br/>    a=string[start+1:start+end-1]<br/>    a = convert_float(a)<br/>    if (string[end]=='t'):<br/>    # <strong class="ob iu">If the GDP was in trillions, multiply it by 1000</strong><br/>        a=1000*a</span></pre><p id="f6d1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面是示例代码片段。<strong class="kw iu">注意代码</strong>中的多个错误处理检查。这是必要的，因为 HTML 页面极其不可预测的性质。不是所有的国家都有 GDP 数据，不是所有的页面都有完全相同的数据用词，不是所有的数字看起来都一样，不是所有的字符串都有类似的$和()。很多事情都可能出错。</p><blockquote class="lz ma mb"><p id="b8a1" class="ku kv mc kw b kx ky ju kz la lb jx lc md le lf lg me li lj lk mf lm ln lo lp im bi translated">为所有场景规划和编写代码几乎是不可能的，但至少您必须有代码来处理出现的异常，以便您的程序不会停止，并可以优雅地转到下一页进行处理。</p></blockquote><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="e9fa" class="mv mw it ob b gy of og l oh oi"># Initialize dictionary for holding the data<br/>GDP_PPP = {}<br/># Iterate over every country<br/>for i in range(1,len(country_names)-1):<br/>    country= country_names[i]<br/>    txt=text_data[country]       <br/>    pos = txt.find('GDP - per capita (PPP):')<br/>    <strong class="ob iu">if pos!=-1: </strong>#If the wording/phrase is not present<br/>        pos= pos+len('GDP - per capita (PPP):')<br/>        string = txt[pos+1:pos+11]<br/>        start = re.search('\$',string)<br/>        end = re.search('\S',string)<br/>        <strong class="ob iu">if (start!=None and end!=None): </strong>#If search fails somehow<br/>            start=start.start()<br/>            end=end.start()<br/>            a=string[start+1:start+end-1]<br/>            #print(a)<br/>            a = convert_float(a)<br/>            <strong class="ob iu">if (a!=-1.0): </strong>#If the float conversion fails somehow<br/>                print(f"GDP/capita (PPP) of {country}: {a} dollars")<br/>                # Insert the data in the dictionary<br/>                GDP_PPP[country]=a<br/>            else:<br/>                print("**Could not find GDP/capita data!**")<br/>        else:<br/>            print("**Could not find GDP/capita data!**")<br/>    else:<br/>        print("**Could not find GDP/capita data!**")</span><span id="ba35" class="mv mw it ob b gy oj og l oh oi">print ("\nFinished finding all GDP/capita data")</span></pre><h2 id="6fd4" class="mv mw it bd mx my mz dn na nb nc dp nd ld ne nf ng lh nh ni nj ll nk nl nm nn bi translated">不要忘记使用熊猫内/左连接方法</h2><p id="6d64" class="pw-post-body-paragraph ku kv it kw b kx no ju kz la np jx lc ld nq lf lg lh nr lj lk ll ns ln lo lp im bi translated">需要记住的一点是，所有这些文本分析都会产生国家略有不同的数据框架，因为不同国家可能没有不同类型的数据。人们可以使用一个<a class="ae mu" href="https://pandas.pydata.org/pandas-docs/stable/merging.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">熊猫左连接</strong> </a>来创建一个包含所有共同国家交集的数据框架，该数据框架的所有数据都是可用的/可以提取的。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="6275" class="mv mw it ob b gy of og l oh oi">df_combined = df_demo.join(df_GDP, how='left')<br/>df_combined.dropna(inplace=True)</span></pre><h2 id="6ca6" class="mv mw it bd mx my mz dn na nb nc dp nd ld ne nf ng lh nh ni nj ll nk nl nm nn bi translated">啊，现在很酷的东西，模特……但是等等！先做过滤吧！</h2><p id="399e" class="pw-post-body-paragraph ku kv it kw b kx no ju kz la np jx lc ld nq lf lg lh nr lj lk ll ns ln lo lp im bi translated">在 HTML 解析、页面抓取和文本挖掘的所有艰苦工作之后，现在您已经准备好收获好处了——渴望运行回归算法和酷的可视化脚本！但是，等等，在生成这些图之前，通常你需要清理你的数据(特别是对于这种社会经济问题)。基本上，您需要过滤掉异常值，例如非常小的国家(如岛国)，这些国家可能具有您想要绘制的参数的极端扭曲值，但不符合您想要调查的主要潜在动态。几行代码对这些过滤器来说很有用。可能有更多的方式来实现它们，但是我试图保持它非常简单和容易理解。例如，下面的代码创建了过滤器来排除总 GDP 为 500 亿美元、低收入和高收入界限分别为 5，000 美元和 25，000 美元(人均 GDP)的小国。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="184f" class="mv mw it ob b gy of og l oh oi"># Create a filtered data frame and x and y arrays<br/>filter_gdp = df_combined['Total GDP (PPP)'] &gt; 50<br/>filter_low_income=df_combined['GDP (PPP)']&gt;5000<br/>filter_high_income=df_combined['GDP (PPP)']&lt;25000</span><span id="d555" class="mv mw it ob b gy oj og l oh oi">df_filtered = df_combined[filter_gdp][filter_low_income][filter_high_income]</span></pre><h2 id="2d8e" class="mv mw it bd mx my mz dn na nb nc dp nd ld ne nf ng lh nh ni nj ll nk nl nm nn bi translated">最后，可视化</h2><p id="67e4" class="pw-post-body-paragraph ku kv it kw b kx no ju kz la np jx lc ld nq lf lg lh nr lj lk ll ns ln lo lp im bi translated">我们使用<a class="ae mu" href="https://seaborn.pydata.org/generated/seaborn.regplot.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> seaborn regplot </strong>函数</a>来创建散点图(互联网用户%与人均国内生产总值之比),其中显示了线性回归拟合和 95%的置信区间。他们看起来像是在跟踪。人们可以将结果解释为</p><blockquote class="lz ma mb"><p id="cc86" class="ku kv mc kw b kx ky ju kz la lb jx lc md le lf lg me li lj lk mf lm ln lo lp im bi translated">一个国家的互联网用户%和人均 GDP 之间有很强的正相关关系。此外，低收入/低 GDP 国家的相关性强度明显高于高 GDP 的发达国家。这可能意味着，与发达国家相比，互联网接入有助于低收入国家更快地发展，并改善其公民的平均状况。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/c9fb92d5f29b71ed5fc5b51469840ea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UAMZrO5oXN_vKvwu-Zhaxg.png"/></div></div></figure><h2 id="543c" class="mv mw it bd mx my mz dn na nb nc dp nd ld ne nf ng lh nh ni nj ll nk nl nm nn bi translated">摘要</h2><p id="a9d0" class="pw-post-body-paragraph ku kv it kw b kx no ju kz la np jx lc ld nq lf lg lh nr lj lk ll ns ln lo lp im bi translated">本文介绍了一个 Python 演示笔记本，演示了如何使用 BeautifulSoup 通过 HTML 解析抓取网页以下载原始信息。之后，还说明了如何使用正则表达式模块来搜索和提取用户需要的重要信息。</p><blockquote class="lz ma mb"><p id="bccb" class="ku kv mc kw b kx ky ju kz la lb jx lc md le lf lg me li lj lk mf lm ln lo lp im bi translated">最重要的是，它演示了在挖掘杂乱的 HTML 解析文本时，如何或为什么不可能有简单、通用的规则或程序结构。我们必须检查文本结构，并进行适当的错误处理检查，以优雅地处理所有情况，从而维护程序的流程(而不是崩溃),即使它无法提取所有这些场景的数据。</p></blockquote><p id="20c8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我希望读者可以从提供的笔记本文件中受益，并根据自己的要求和想象来构建它。更多网络数据分析笔记本，<a class="ae mu" href="https://github.com/tirthajyoti/Web-Database-Analytics-Python" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">请看我的知识库。</strong>T11】</a></p></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><p id="7a3d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi lq translated"><span class="l lr ls lt bm lu lv lw lx ly di">如果</span>您有任何问题或想法要分享，请联系作者在<a class="ae mu" href="mailto:tirthajyoti@gmail.com" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu">tirthajyoti【AT】Gmail . com</strong></a>。你也可以查看作者的<a class="ae mu" href="https://github.com/tirthajyoti?tab=repositories" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> GitHub 资源库</strong> </a>中其他有趣的 Python、R 或 MATLAB 代码片段和机器学习资源。如果你像我一样对机器学习/数据科学充满热情，请随时<a class="ae mu" href="https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/" rel="noopener ugc nofollow" target="_blank">在 LinkedIn 上添加我</a>或<a class="ae mu" href="https://twitter.com/tirthajyotiS" rel="noopener ugc nofollow" target="_blank">在 Twitter 上关注我。</a></p></div></div>    
</body>
</html>