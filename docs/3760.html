<html>
<head>
<title>[ Google / ICLR 2017 / Paper Summary ] Gradients of Counterfactuals</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">【谷歌/ ICLR 2017 /论文摘要】反事实的梯度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/google-iclr-2017-paper-summary-gradients-of-counterfactuals-6306510935f2?source=collection_archive---------6-----------------------#2018-06-15">https://towardsdatascience.com/google-iclr-2017-paper-summary-gradients-of-counterfactuals-6306510935f2?source=collection_archive---------6-----------------------#2018-06-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/94b4c43cfceff1c7d0546ec42343aa78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*hhs1OorueEU9-pI5AdK0lQ.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/education-neuroscience-ojmB7lOn3VUU8" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="0d3a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><a class="ae jy" href="https://arxiv.org/search?searchtype=author&amp;query=Sundararajan%2C+M" rel="noopener ugc nofollow" target="_blank"> Mukund Sundararajan </a>，是论文“<a class="ae jy" href="https://arxiv.org/abs/1805.12233" rel="noopener ugc nofollow" target="_blank">神经元</a>有多重要？”的作者之一(我也做了论文总结<a class="ae jy" rel="noopener" target="_blank" href="/nips2018-google-paper-summary-how-important-is-a-neuron-3de4b085eb03">这里</a>)。但是今天我想看看他之前的论文“反事实的梯度”。</p><blockquote class="kx ky kz"><p id="7791" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated"><strong class="kb ir">请注意，这篇帖子是为了我未来的自己复习这篇论文上的材料，而不是从头再看一遍。</strong></p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="https://arxiv.org/abs/1611.02639" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="25df" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">摘要</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/a87c8a18320f9fd207ef0c8b08659309.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5KlDXbRsJ3ogVJroz14oLQ.png"/></div></div></figure><p id="2233" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">执行分类时，梯度可用于识别哪些要素对网络很重要。然而，在深度神经网络中，不仅神经元，而且整个网络都可能遭受饱和。且导致给出小梯度值，即使对于非常重要的特征也是如此。</p><p id="b99e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这项工作中，作者提出了内部梯度，这是反事实输入的梯度。(缩小原始输入)。该方法不仅易于实现，而且能更好地捕捉特征的重要性。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="c8c3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">简介</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/4b1bdf00b56b2eb17fd2bc953432e9de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GGis8DMDlw8_cN-qgkHOJg.png"/></div></div></figure><p id="5efc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同样，梯度可以用来确定哪些特征是重要的。然而，由于网络内的饱和，重要特征可能具有非常小的梯度。以前有过克服这个问题的工作，但是它们需要开发者做大量的工作。相比之下，内部渐变非常容易实现，只需要最小的变化。(仅输入值)</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="2761" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">我们(作者)的手法</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lx"><img src="../Images/55b53566009bfd7b2dddacd811f2dedb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SRPU9vh-i8-PrscK-JwKvA.png"/></div></div></figure><p id="bb00" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">作者首先从调查梯度如何用于测量特征重要性开始，他们决定使用 GoogleNet。如上所述，当我们直接可视化与原始图像重叠的归一化梯度时，我们不能准确地说出为什么网络将该图像分类为相机。</p><p id="d66f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">凭直觉，我们应该期望镜头区域和相机整体的渐变比左上区域更亮。因此，作者从原始图像中裁剪掉了左边部分，然而，即使这样做了，我们也可以观察到图像中最奇怪的部分梯度最强。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="6eb5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">饱和度</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ly"><img src="../Images/592c1b81ac1947d967b32b393ec16b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-N21Rtk-_bkdB3CZ-2jalg.png"/></div></div></figure><p id="739f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从上面的实验中，我们已经知道，由于网络中的饱和，梯度有可能表现出奇怪的行为。为了捕捉网络中饱和度的分布范围，作者提出了反事实输入。并且这些输入可以通过将α值乘以原始图像来获得。</p><p id="775d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">通过大量的实验，作者发现饱和度在谷歌网络中广泛分布。(更具体地说，甚至在中间层以及最终软最大层和预软最大层中。)此外，作者还注意到，当输入值乘以低 alpha 值时，网络中会有更多的活动。(如 0.02)。</p><p id="7844" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，众所周知的事实是，梯度的饱和会阻止模型收敛到高质量的最小值，如果发生这种情况，网络的性能会受到严重影响。然而，当网络中仍然存在饱和时，GoogleNet 具有很好的性能测量，因此作者做出了以下假设。</p><blockquote class="kx ky kz"><p id="4e96" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated">我们的假设是，重要特征的梯度在训练过程的早期没有饱和。梯度仅在特征已经被充分学习之后饱和，即，输入远离决策边界。</p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="04e3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">内部梯度</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lz"><img src="../Images/9fa31852ccf137f8084202a989a513f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HBm2dfiJlaLDIP2p7kEAIg.png"/></div></div></figure><p id="5afd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在本节中，作者描述了如何创建反事实输入，并指出这样一个事实，即我们可以通过在颜色维度上对它们求和来聚合所有的最终渐变。当我们在每个比例因子α下可视化这些梯度时，我们得到如下结果。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ma"><img src="../Images/5e1897356a9d59a8a673cc0038b0c11a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mT-36t6twM_w6y3LnjIZvw.png"/></div></div></figure><p id="8dba" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们可以观察到，随着比例因子的降低，网络将该图像归类为相机的原因变得更有意义。当我们绘制最终梯度的绝对幅度时，我们得到如下图。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/d57cbfa6d4ed54fdc6d4377f59ede8b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*_gC58ymkxpZk4oalYjAriQ.png"/></div></figure><p id="e082" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这表明随着α值的增加，最终梯度的值由于饱和而减小。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="e5aa" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">累积内部渐变</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mc"><img src="../Images/c0d8c0a9c6fcee6416c5d9d6c325c0ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-zaiNBtuAF81ubSsDPrWnA.png"/></div></div></figure><p id="67dd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在本节中，作者通过将不同 alpha 值的所有梯度值相加来扩展内部梯度。他们用一种非常聪明的方式做到了这一点，他们不是有多个α值，比如说 100，而是取α从 0 到 1 的所有可能值的积分。最后，他们用黎曼和来逼近积分梯度。</p><div class="ll lm ln lo gt ab cb"><figure class="md jr me mf mg mh mi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/753185ce4eca2d74904bf2e356f455db.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*E6AijxwAkd-Of9CWwnLuNg.png"/></div></figure><figure class="md jr mj mf mg mh mi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/f5485d14f16064b6048cdfc06bbc0bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*ijbYBWZWYKhK8Wug8LF_Aw.png"/></div></figure></div><p id="5975" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →积分梯度的原始方程<br/> <strong class="kb ir">右图</strong> →用黎曼和近似积分</p><p id="c501" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这个方法最好的部分就是简单地说，我们只是一遍又一遍地计算梯度。但是使用了缩小版本的输入，所以实现起来非常简单和自然。当我们将得到的积分梯度可视化时，我们会得到如下结果。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/ef02717dd23cccbb0d442b4f35093978.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*FoO9ntu0Ty8tH9XQHNdKmQ.png"/></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="e90e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">评估/调试网络/讨论</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/b02f1cf29e0c08511503d7ae346dddf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*TB6O17C424WiLb1Il1ny5Q.png"/></div></figure><p id="423c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">总之，在本节中，作者进行了额外的实验来评估积分梯度。(例如像素烧蚀或比较最高有效梯度的边界框)并且当与纯梯度相比时，积分梯度给出了更好的结果。(如果您希望查看更多示例，请<a class="ae jy" href="https://github.com/ankurtaly/Integrated-Gradients" rel="noopener ugc nofollow" target="_blank">点击此处</a>。)</p><p id="c681" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在精度要求较高的环境中，例如医疗诊断，了解网络中的情况非常重要。并且更准确地知道哪些特征贡献给哪些类，积分梯度可以用作获得更多洞察的工具。</p><p id="36b3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，作者讨论了这种方法的局限性。</p><p id="1340" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> a .无法捕获特征交互</strong> →模型可以执行一些操作，将某些特征组合在一起。重要的分数没有办法代表这些组合。</p><p id="dc5e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> b .特征相关性→ </strong>如果相似的特征出现多次，模型可以为其中任何一个分配权重。(或者两者都有)。但是这些重量可能不是人类能够理解的。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="9054" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">相关工作</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mm"><img src="../Images/77c68cfb5e61229d77a60b05b00d55ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uQSSTjdUrRxoWPYN5VfPLQ.png"/></div></div></figure><p id="acc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在本节中，作者讨论了不同研究人员提出的其他方法，希望揭开神经网络内部工作的神秘面纱。</p><p id="be6c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">基于梯度的方法</strong> →使用纯梯度或内部/整体梯度</p><p id="d0f4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">基于分数反向传播的方法</strong> → <a class="ae jy" href="https://arxiv.org/abs/1704.02685" rel="noopener ugc nofollow" target="_blank">深度提升</a>、<a class="ae jy" href="https://arxiv.org/abs/1604.00825" rel="noopener ugc nofollow" target="_blank">逐层相关传播</a>、<a class="ae jy" href="https://arxiv.org/abs/1311.2901" rel="noopener ugc nofollow" target="_blank">反卷积网络</a>、导向反向传播<a class="ae jy" href="https://arxiv.org/abs/1412.6806" rel="noopener ugc nofollow" target="_blank"/></p><p id="1374" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">基于模型近似的方法 → <a class="ae jy" href="http://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf" rel="noopener ugc nofollow" target="_blank">我为什么要相信你？</a></p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="6065" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">应用于其他网络</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mn"><img src="../Images/4c028bca9ebf00dc7db682e15f0c99d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*17jn3KBpF9TBpJa3oolmeg.png"/></div></div></figure><p id="f7fc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我不会深入每个实验的细节，但简而言之作者已经进行了两次实验。(一个与药物有关，另一个与语言建模有关。)通过使用积分梯度，他们能够确定哪些特征对模型做出预测起着最重要的作用。让他们更深入地了解模型在做什么。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="8c5a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结论</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mo"><img src="../Images/f91ecf977ae19544c42981d42691a047.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HDwCAULujmh-nBAdVLnyXw.png"/></div></div></figure><p id="c381" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">总之，作者提供了一种新的和简单的方法来衡量特征的重要性。并提供了广泛的实验，表明内部/积分梯度如何优于传统的梯度方法。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="c5a8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">遗言</strong></p><p id="04ca" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我真的相信这篇论文打开了研究神经网络的大门。总的来说，这是一篇相当长的论文，所以我没有包括很多内容和细节，如果你有时间，我强烈推荐你阅读这篇论文。</p><p id="bfff" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你希望看到我所有写作的列表，请<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">在这里查看我的网站</a>。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的推特<a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>关注我，并访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我也实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文 pos </a> t。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="20d9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="d581" class="mp mq iq kb b kc kd kg kh kk mr ko ms ks mt kw mu mv mw mx bi translated">Sundararajan，m .，Taly，a .，和 Yan，Q. (2016 年)。反事实的梯度。Arxiv.org。检索于 2018 年 6 月 15 日，来自<a class="ae jy" href="https://arxiv.org/abs/1611.02639" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1611.02639</a></li><li id="0813" class="mp mq iq kb b kc my kg mz kk na ko nb ks nc kw mu mv mw mx bi translated">Dhamdhere，m . Sundararajan 和 q . Yan(2018 年)。一个神经元有多重要？。Arxiv.org。检索于 2018 年 6 月 15 日，来自<a class="ae jy" href="https://arxiv.org/abs/1805.12233" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1805.12233</a></li><li id="4f6c" class="mp mq iq kb b kc my kg mz kk na ko nb ks nc kw mu mv mw mx bi translated">【NIPS2018/ Google /论文摘要】一个神经元有多重要？。(2018).走向数据科学。检索于 2018 年 6 月 15 日，来自<a class="ae jy" rel="noopener" target="_blank" href="/nips2018-google-paper-summary-how-important-is-a-neuron-3de4b085eb03">https://towards data science . com/nips 2018-Google-paper-summary-how-importance-is-a-neuron-3d E4 b 085 EB 03</a></li><li id="8af9" class="mp mq iq kb b kc my kg mz kk na ko nb ks nc kw mu mv mw mx bi translated">ankurtaly/集成渐变。(2018).GitHub。检索于 2018 年 6 月 15 日，来自<a class="ae jy" href="https://github.com/ankurtaly/Integrated-Gradients" rel="noopener ugc nofollow" target="_blank">https://github.com/ankurtaly/Integrated-Gradients</a></li><li id="126e" class="mp mq iq kb b kc my kg mz kk na ko nb ks nc kw mu mv mw mx bi translated">斯普林根贝格，j .，多索维茨基，a .，布罗克斯，t .，&amp;里德米勒，M. (2014)。追求简单:全卷积网。Arxiv.org。检索于 2018 年 6 月 15 日，来自<a class="ae jy" href="https://arxiv.org/abs/1412.6806" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1412.6806</a></li><li id="8b6b" class="mp mq iq kb b kc my kg mz kk na ko nb ks nc kw mu mv mw mx bi translated">m .泽勒和 r .弗格斯(2013 年)。可视化和理解卷积网络。Arxiv.org。检索于 2018 年 6 月 15 日，来自<a class="ae jy" href="https://arxiv.org/abs/1311.2901" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1311.2901</a></li><li id="91e7" class="mp mq iq kb b kc my kg mz kk na ko nb ks nc kw mu mv mw mx bi translated">宾德，a .，蒙塔冯，g .，巴赫，s .，M：ller，k .，&amp; Samek，W. (2016 年)。具有局部重正化层的神经网络的逐层相关性传播。Arxiv.org。检索于 2018 年 6 月 15 日，来自 https://arxiv.org/abs/1604.00825<a class="ae jy" href="https://arxiv.org/abs/1604.00825" rel="noopener ugc nofollow" target="_blank"/></li><li id="c534" class="mp mq iq kb b kc my kg mz kk na ko nb ks nc kw mu mv mw mx bi translated">Shrikumar，a .，Greenside，p .，和 Kundaje，A. (2017 年)。通过传播激活差异学习重要特征。Arxiv.org。检索于 2018 年 6 月 15 日，来自 https://arxiv.org/abs/1704.02685<a class="ae jy" href="https://arxiv.org/abs/1704.02685" rel="noopener ugc nofollow" target="_blank"/></li><li id="fd48" class="mp mq iq kb b kc my kg mz kk na ko nb ks nc kw mu mv mw mx bi translated">(2018).Kdd.org。检索于 2018 年 6 月 15 日，来自<a class="ae jy" href="http://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf" rel="noopener ugc nofollow" target="_blank">http://www . KDD . org/KDD 2016/papers/files/RFP 0573-ribeiroa . pdf</a></li></ol><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nd"><img src="../Images/5bbd8b62efecad02f8f91ff247ec1f4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VYBGYSv2ofBug1PkhRkNcg.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Photo by <a class="ae jy" href="https://unsplash.com/photos/yqNRxWNrC04?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Dollar Gill</a> on <a class="ae jy" href="https://unsplash.com/search/photos/photography?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div>    
</body>
</html>