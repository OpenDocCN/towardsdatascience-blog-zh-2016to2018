<html>
<head>
<title>Seq2Seq model in TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流中的 Seq2Seq 模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/seq2seq-model-in-tensorflow-ec0c557e560f?source=collection_archive---------1-----------------------#2018-05-02">https://towardsdatascience.com/seq2seq-model-in-tensorflow-ec0c557e560f?source=collection_archive---------1-----------------------#2018-05-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e772279d91d64be150d6613477845ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0xrL1Zb2uLnQWqej."/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@marcusdepaula?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Marcus dePaula</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="dfa8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个项目中，我将在 TensorFlow 中建立一个名为<code class="fe lb lc ld le b"><strong class="kf ir">seq2seq model or encoder-decoder model</strong></code>的语言翻译模型。该模型的目标是将英语句子翻译成法语句子。我将展示详细的步骤，他们将回答像<code class="fe lb lc ld le b">how to define encoder model</code>、<code class="fe lb lc ld le b">how to define decoder model</code>、<code class="fe lb lc ld le b">how to build the entire seq2seq model</code>、<code class="fe lb lc ld le b">how to calculate the loss and clip gradients</code>这样的问题。</p><p id="4116" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请访问<a class="ae kc" href="https://github.com/deep-diver/EN-FR-MLT-tensorflow" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> Github repo </strong> </a>了解 Jupyter 笔记本中更详细的信息和实际代码。它将涵盖更多的主题，如<code class="fe lb lc ld le b">how to preprocess the dataset</code>、<code class="fe lb lc ld le b">how to define inputs</code>和<code class="fe lb lc ld le b">how to train and get prediction</code>。</p><p id="0fca" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是 Udacity 深度学习纳米学位的一部分。一些代码/功能(保存、加载、测量精度等)由 Udacity 提供。然而，大部分是我自己实现的，每一节都有更丰富的解释和参考。此外，基础数字(关于模型)借用自<a class="ae kc" href="https://github.com/lmthang/thesis" rel="noopener ugc nofollow" target="_blank"> Luong (2016) </a>。</p><h1 id="3ecf" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">构建 Seq2Seq 模型的步骤</h1><p id="29ab" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">您可以将整个模型分成两个小的子模型。第一个子模型称为<strong class="kf ir">【E】</strong><strong class="kf ir"><em class="mi">编码器</em> </strong>，第二个子模型称为<strong class="kf ir">【D】</strong><strong class="kf ir"><em class="mi">解码器</em> </strong>。就像任何其他 RNN 架构一样，接受原始输入文本数据。最后，<strong class="kf ir">【E】</strong>输出一个神经表示。这是一个非常典型的工作，但是你需要注意这个输出到底是什么。<strong class="kf ir">【E】</strong>的输出将成为<strong class="kf ir">【D】</strong>的输入数据。</p><p id="bfcf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是为什么我们称<strong class="kf ir">【E】</strong>为<em class="mi">编码器</em>，称<strong class="kf ir">【D】</strong>为<em class="mi">解码器</em>。<strong class="kf ir">【E】</strong>做出一个以神经表征形式编码的输出，我们不知道它到底是什么。它被加密了。<strong class="kf ir">【D】</strong>具有查看<strong class="kf ir">【E】</strong>输出内部的能力，它将创建一个完全不同的输出数据(在本例中翻译成法语)。</p><p id="2678" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了建立这样一个模型，总共有 6 个步骤。我注意到要实现的功能与每个步骤相关。</p><p id="de55" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> (1)定义编码器模型的输入参数</strong></p><ul class=""><li id="03f5" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated"><code class="fe lb lc ld le b">enc_dec_model_inputs</code></li></ul><p id="5a4a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> (2)建立编码器模型</strong></p><ul class=""><li id="1519" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated"><code class="fe lb lc ld le b">encoding_layer</code></li></ul><p id="3cdc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> (3)定义解码器模型的输入参数</strong></p><ul class=""><li id="593f" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated"><code class="fe lb lc ld le b">enc_dec_model_inputs</code>、<code class="fe lb lc ld le b">process_decoder_input</code>、<code class="fe lb lc ld le b">decoding_layer</code></li></ul><p id="f65c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> (4)为训练建立解码器模型</strong></p><ul class=""><li id="ac38" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated"><code class="fe lb lc ld le b">decoding_layer_train</code></li></ul><p id="7a04" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> (5)建立用于推理的解码器模型</strong></p><ul class=""><li id="5577" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated"><code class="fe lb lc ld le b">decoding_layer_infer</code></li></ul><p id="a483" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> (6)将(4)和(5)放在一起</strong></p><ul class=""><li id="a1dc" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated"><code class="fe lb lc ld le b">decoding_layer</code></li></ul><p id="87e6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> (7)连接编码器和解码器型号</strong></p><ul class=""><li id="fab5" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated"><code class="fe lb lc ld le b">seq2seq_model</code></li></ul><p id="2790" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> (8)定义损失函数、优化器，并应用梯度剪切</strong></p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ms"><img src="../Images/7aa42941e3a738fde1efdf47e780cf7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_rSHLjFShknAu3jt3rbcNQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd mx">Fig 1. Neural Machine Translation / Training Phase</strong></figcaption></figure><h1 id="b34b" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">编码器输入(1)、(3)</h1><p id="cdb4" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated"><code class="fe lb lc ld le b">enc_dec_model_inputs</code>函数创建并返回与建筑模型相关的参数(TF 占位符)。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="a2fa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输入占位符将输入英文句子数据，其形状为<code class="fe lb lc ld le b">[None, None]</code>。第一个<code class="fe lb lc ld le b">None</code>表示批量大小，批量大小未知，因为用户可以设置。第二个<code class="fe lb lc ld le b">None</code>表示句子的长度。setence 的最大长度因批次而异，因此无法用确切的数字进行设置。</p><ul class=""><li id="93f3" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated">一种选择是将每批中所有句子的长度设置为最大长度。无论您选择哪种方法，您都需要在空位置添加特殊字符<code class="fe lb lc ld le b">&lt;PAD&gt;</code>。然而，对于后一种选择，可能会有不必要的更多<code class="fe lb lc ld le b">&lt;PAD&gt;</code>字符。</li></ul><p id="9e6a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">targets 占位符类似于 inputs 占位符，只是它将输入法语句子数据。</p><p id="22df" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">target_sequence_length 占位符表示每个句子的长度，所以形状是<code class="fe lb lc ld le b">None</code>，一个列张量，与批量大小相同。这个特定的值需要作为 TrainerHelper 的参数，以建立用于训练的解码器模型。我们将在(4)中看到。</p><p id="cbaa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">max_target_len 获取所有目标句子(序列)长度的最大值。如你所知，我们在 target_sequence_length 参数中有所有句子的长度。从中获取最大值的方法是使用<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/reduce_max" rel="noopener ugc nofollow" target="_blank"> tf.reduce_max </a>。</p><h1 id="f396" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">过程解码器输入(3)</h1><p id="cb17" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">在解码器端，我们需要两种不同的输入，分别用于训练和推理。在训练阶段，输入作为目标标签被提供，但是它们仍然需要被嵌入。然而，在推理阶段，每个时间步的输出将是下一个时间步的输入。它们也需要被嵌入，并且嵌入向量应该在两个不同的相位之间共享。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi na"><img src="../Images/1c77523d6fa6bfb4ef9fbdbf35a62017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-4RY5LwoedeAZM7qO1A0cQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd mx">Fig 2. </strong><code class="fe lb lc ld le b"><strong class="bd mx">&lt;GO&gt;</strong></code><strong class="bd mx"> insertion</strong></figcaption></figure><p id="e926" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本节中，我将为训练阶段预处理目标标签数据。这不是什么特别的任务。你所需要做的就是在所有目标数据前面加上<code class="fe lb lc ld le b">&lt;GO&gt;</code>特殊标记。<code class="fe lb lc ld le b">&lt;GO&gt;</code> token 是一种引导性的 token，比如说“这是翻译的开始”。对于这个过程，你需要知道 TensorFlow 的三个库。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="f72d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/strided_slice" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">TF _ slide</strong></a></p><ul class=""><li id="45c8" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated">提取张量的步长切片(广义 python 数组索引)。</li><li id="f460" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated">可以认为是分裂成多个张量与跨度窗口大小从开始到结束</li><li id="bb72" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated">参数:TF 张量，开始，结束，步幅</li></ul><p id="9e03" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/concat" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> TF 填充</strong> </a></p><ul class=""><li id="c07f" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated">创建一个用标量值填充的张量。</li><li id="845b" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated">参数:TF 张量(必须是 int32/int64)，要填充的值</li></ul><p id="e562" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/fill" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> TF concat </strong> </a></p><ul class=""><li id="5817" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated">沿一维连接张量。</li><li id="880b" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated">参数:一个 TF 张量列表(本例中为 tf.fill 和 after_slice)，axis=1</li></ul><p id="79c7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对目标标签数据进行预处理后，我们会在后面实现 decoding_layer 函数时嵌入。</p><h1 id="9554" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">编码(2)</h1><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/469196499dc622f8a67996e5fd66b458.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SSlwwoeJsvKcJ2qdbSYurQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd mx">Fig 3. Encoding model highlighted — Embedding/RNN layers</strong></figcaption></figure><p id="9def" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图 3 所示，编码模型由两个不同的部分组成。第一部分是嵌入层。一个句子中的每个单词将由指定为<code class="fe lb lc ld le b">encoding_embedding_size</code>的特征数量来表示。这一层赋予了单词<a class="ae kc" href="https://stackoverflow.com/questions/40784656/tf-contrib-layers-embed-sequence-is-for-what/44280918#44280918" rel="noopener ugc nofollow" target="_blank">有用解释</a>更丰富的代表力。第二部分是 RNN 层。你可以利用任何种类的 RNN 相关技术或算法。例如，在这个项目中，多个 LSTM 单元在漏失技术应用后堆叠在一起。你可以使用不同种类的 RNN 细胞，如 GRU</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="751b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">嵌入层</strong></p><ul class=""><li id="57df" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence" rel="noopener ugc nofollow" target="_blank">TF contrib . layers . embed _ sequence</a></li></ul><p id="6e90" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> RNN 层层</strong></p><ul class=""><li id="4bba" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell" rel="noopener ugc nofollow" target="_blank">TF contrib . rnn . lstmcell</a><br/>:简单指定它有多少个内部单元</li><li id="9538" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper" rel="noopener ugc nofollow" target="_blank">TF contrib . rnn . drop out wrapper</a><br/>:用 keep 概率值包装单元格</li><li id="60c6" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell" rel="noopener ugc nofollow" target="_blank">TF contrib . rnn . multirnncell</a><br/>:堆叠多个 rnn(类型)单元格<br/> : <a class="ae kc" href="https://github.com/tensorflow/tensorflow/blob/6947f65a374ebf29e74bb71e36fd82760056d82c/tensorflow/docs_src/tutorials/recurrent.md#stacking-multiple-lstms" rel="noopener ugc nofollow" target="_blank">这个 API 是如何使用的？</a></li></ul><p id="852e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">编码模式</strong></p><ul class=""><li id="1871" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn" rel="noopener ugc nofollow" target="_blank"> TF nn.dynamic_rnn </a> <br/>:将嵌入图层和 rnn 图层放在一起</li></ul><h1 id="38b9" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">解码—训练过程(4)</h1><p id="98ab" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">解码模型可以被认为是两个独立的过程，训练和推理。这不是因为它们具有不同的架构，而是因为它们共享相同的架构及其参数。这是因为他们有不同的策略来支持共享模型。对于这个(训练)和下一个(推断)部分，图 4 清楚地显示了它们是什么。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/5c109044a4625e96f9081dc57ef54a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RNIF_HsG9BBl3Ka_G0KUew.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd mx">Fig 4. Decoder shifted inputs</strong></figcaption></figure><p id="d19e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当编码器使用<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence" rel="noopener ugc nofollow" target="_blank">TF contrib . layers . embed _ sequence</a>时，它不适用于解码器，即使它可能要求其输入嵌入。这是因为相同的嵌入向量应该通过训练和推断阶段共享。<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence" rel="noopener ugc nofollow" target="_blank">TF contrib . layers . embed _ sequence</a>运行前只能嵌入准备好的数据集。推理过程需要的是动态嵌入能力。在运行模型之前，不可能嵌入推理过程的输出，因为当前时间步的输出将是下一个时间步的输入。</p><p id="4f35" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们如何嵌入？我们很快就会看到。然而，现在，你需要记住的是训练和推理过程共享相同的嵌入参数。对于训练部分，应该交付嵌入的输入。在推理部分，应该只传送训练部分中使用的嵌入参数。</p><p id="f0eb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">先看训练部分。</p><ul class=""><li id="dcde" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated"><code class="fe lb lc ld le b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/TrainingHelper" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.contrib.seq2seq.TrainingHelper</strong></a></code><strong class="kf ir"><br/></strong>:training helper 是我们传递嵌入输入的地方。顾名思义，这只是一个助手实例。这个实例应该交付给 BasicDecoder，这是构建解码器模型的实际过程。</li><li id="334b" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated"><code class="fe lb lc ld le b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.contrib.seq2seq.BasicDecoder</strong></a></code><strong class="kf ir"><br/></strong>:basic decoder 构建解码器模型。这意味着它连接解码器端的 RNN 层和由 TrainingHelper 准备的输入。</li><li id="9a8e" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated"><code class="fe lb lc ld le b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.contrib.seq2seq.dynamic_decode</strong></a></code><strong class="kf ir"><br/></strong>:dynamic _ decode 展开解码器模型，以便 BasicDecoder 可以检索每个时间步长的实际预测。</li></ul><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="71af" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">解码——推理过程(5)</h1><ul class=""><li id="7243" class="mj mk iq kf b kg md kk me ko ni ks nj kw nk la mo mp mq mr bi translated"><code class="fe lb lc ld le b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/GreedyEmbeddingHelper" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.contrib.seq2seq.GreedyEmbeddingHelper</strong></a></code><strong class="kf ir"><br/></strong>:greedyembedinghelper 动态获取当前步骤的输出，并将其交给下一个时间步骤的输入。为了动态地嵌入每个输入结果，应该提供嵌入参数(只是一串权重值)。与此同时，GreedyEmbeddingHelper 要求提供与批量和<code class="fe lb lc ld le b">end_of_sequence_id</code>相同数量的<code class="fe lb lc ld le b">start_of_sequence_id</code>。</li><li id="929c" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated"><code class="fe lb lc ld le b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.contrib.seq2seq.BasicDecoder</strong></a></code> <strong class="kf ir"> <br/> </strong>:同培训流程一节所述</li><li id="da43" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated"><code class="fe lb lc ld le b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.contrib.seq2seq.dynamic_decode</strong></a></code> <strong class="kf ir"> <br/> : </strong>同培训流程一节所述</li></ul><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="bb59" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">构建解码层(3)、(6)</h1><p id="1b41" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated"><strong class="kf ir">嵌入目标序列</strong></p><ul class=""><li id="2c45" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence" rel="noopener ugc nofollow" target="_blank">TF contrib . layers . embed _ sequence</a>创建了嵌入参数的内部表示，因此我们无法查看或检索它。相反，你需要通过<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/Variable" rel="noopener ugc nofollow" target="_blank"> TF 变量</a>手动创建一个嵌入参数。</li><li id="f4b4" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated">手动创建的嵌入参数用于训练阶段，以在训练运行前通过<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup" rel="noopener ugc nofollow" target="_blank"> TF nn.embedding_lookup </a>转换提供的目标数据(句子序列)。<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup" rel="noopener ugc nofollow" target="_blank">手动创建嵌入参数的 TF nn.embedding_lookup </a>返回与<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence" rel="noopener ugc nofollow" target="_blank">TF contrib . layers . embed _ sequence</a>类似的结果。对于推理过程，每当解码器计算当前时间步的输出时，它将被共享的嵌入参数嵌入，并成为下一个时间步的输入。你只需要提供嵌入参数给 GreedyEmbeddingHelper，那么它会帮助这个过程。</li><li id="ae32" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated"><a class="ae kc" href="https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do" rel="noopener ugc nofollow" target="_blank">embedding _ lookup 如何工作？</a> <br/>:简而言之，选择指定的行</li><li id="2384" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated">注意:请小心设置变量范围。如前所述，参数/变量在训练和推理过程之间共享。共享可以通过<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/variable_scope" rel="noopener ugc nofollow" target="_blank"> tf.variable_scope </a>指定。</li></ul><p id="1bff" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">构建解码器 RNN 层</strong></p><ul class=""><li id="3400" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated">如图 3 和图 4 所示，解码器模型中的 RNN 层数必须等于编码器模型中的 RNN 层数。</li></ul><p id="1ec2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">创建一个输出层，将解码器的输出映射到我们的词汇表中的元素</p><ul class=""><li id="70b2" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated">这只是一个完全连接的层，以获得每个单词在最后出现的概率。</li></ul><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="3814" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">构建 Seq2Seq 模型(7)</h1><p id="b1de" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">在本节中，将之前定义的函数、<code class="fe lb lc ld le b">encoding_layer</code>、<code class="fe lb lc ld le b">process_decoder_input</code>和<code class="fe lb lc ld le b">decoding_layer</code>放在一起，构建一个完整的序列到序列模型。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="cc65" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">构建图形+定义损失，优化器，带梯度剪裁</h1><p id="cc0d" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated"><code class="fe lb lc ld le b">seq2seq_model</code>功能创建模型。它定义了前馈和反向传播应该如何流动。该模型可训练的最后一步是决定和应用使用什么优化算法。在本节中，<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss" rel="noopener ugc nofollow" target="_blank">TF contrib . seq 2 seq . sequence _ loss</a>用于计算损失，然后<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer" rel="noopener ugc nofollow" target="_blank"> TF train。AdamOptimizer </a>用于计算损耗的梯度下降。让我们在下面的代码单元中检查每个步骤。</p><p id="41f4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">从检查点加载数据</strong></p><ul class=""><li id="615b" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated">(source_int_text，target_int_text)是输入数据，而(source_vocab_to_int，target_vocab_to_int)是用于查找每个单词的索引号的词典。</li><li id="c06d" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated">max_target_sentence_length 是源输入数据中最长句子的长度。在解码器模式下构建推理过程时，这将用于 GreedyEmbeddingHelper。</li></ul><p id="33fb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">创建输入</strong></p><ul class=""><li id="d145" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated">enc_dec_model_inputs 函数的输入(输入数据、目标、目标序列长度、最大目标序列长度)</li><li id="dd72" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated">超参数输入函数的输入(lr，keep_prob)</li></ul><p id="6eca" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">构建 seq2seq 模型</strong></p><ul class=""><li id="2f3b" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated">通过 seq2seq_model 函数建立模型。它将返回 train _ logits(计算损失的 logit)和 inference _ logits(来自预测的 logit)。</li></ul><p id="c042" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">成本函数</strong></p><ul class=""><li id="2a50" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss" rel="noopener ugc nofollow" target="_blank">使用 TF contrib . seq 2 seq . sequence _ loss</a>。这个损失函数只是一个加权的 softmax 交叉熵损失函数，但它是特别设计用于时间序列模型(RNN)。权重应该作为一个参数明确提供，它可以由<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/sequence_mask" rel="noopener ugc nofollow" target="_blank"> TF sequence_mask </a>创建。在这个项目中，<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/sequence_mask" rel="noopener ugc nofollow" target="_blank"> TF sequence_mask </a>创建[batch_size，max_target_sequence_length]大小的变量，然后马克斯只将第一个 target_sequence_length 元素的个数设为 1。这意味着零件的重量会比其他零件轻。</li></ul><p id="a0f8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">优化器</strong></p><ul class=""><li id="7ea2" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer" rel="noopener ugc nofollow" target="_blank"> TF 列车。使用 AdamOptimizer </a>，这是应该指定学习率的地方。你也可以选择其他算法，这只是一个选择。</li></ul><p id="e234" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">渐变裁剪</strong></p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nl"><img src="../Images/bf7beced220dd16e5b5ba0ada478562a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i-DQRotsPEmS5MCAcPo5lA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd mx">Fig 5. Gradient Clipping</strong></figcaption></figure><ul class=""><li id="9578" class="mj mk iq kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated">由于递归神经网络是臭名昭著的消失/爆炸梯度，梯度裁剪技术被认为是改善这一问题。</li><li id="2527" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated">这个概念真的很简单。您决定阈值以保持梯度在某个边界内。在这个项目中，阈值的范围在-1 和 1 之间。</li><li id="af36" class="mj mk iq kf b kg nb kk nc ko nd ks ne kw nf la mo mp mq mr bi translated">现在，您需要将这些概念性知识应用到张量流代码中。幸运的是，有这个<a class="ae kc" href="https://www.tensorflow.org/api_guides/python/train#Gradient_Clipping" rel="noopener ugc nofollow" target="_blank"> TF 渐变裁剪的官方指南 How？</a>。在 breif 中，您通过调用<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#compute_gradients" rel="noopener ugc nofollow" target="_blank"> compute_gradients </a>从优化器手动获取梯度值，然后使用<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/clip_by_value" rel="noopener ugc nofollow" target="_blank"> clip_by_value </a>操作梯度值。最后，您需要通过调用<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#apply_gradients" rel="noopener ugc nofollow" target="_blank"> apply_gradients </a>将修改后的梯度放回到优化器中</li></ul><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="28ce" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">关于我自己</h1><p id="19bd" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">我的深度学习背景是<a class="ae kc" href="https://www.udacity.com/course/deep-learning-nanodegree--nd101" rel="noopener ugc nofollow" target="_blank">uda city { Deep Learning N</a>D&amp;<a class="ae kc" href="https://www.udacity.com/course/ai-artificial-intelligence-nanodegree--nd898" rel="noopener ugc nofollow" target="_blank">AI-nd</a>with contentrations(<a class="ae kc" href="https://www.udacity.com/course/computer-vision-nanodegree--nd891" rel="noopener ugc nofollow" target="_blank">CV</a>，<a class="ae kc" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank"> NLP </a>，VUI)}，<a class="ae kc" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank">Coursera Deep Learning . AI Specialization</a>(AI-ND 被拆分成 4 个不同的部分，我是和之前版本的 ND 一起完成的)。还有，我目前正在服用<a class="ae kc" href="https://www.udacity.com/course/data-analyst-nanodegree--nd002" rel="noopener ugc nofollow" target="_blank"> Udacity 数据分析师 ND </a>，目前已经完成 80%。</p></div></div>    
</body>
</html>