<html>
<head>
<title>Review: PSPNet — Winner in ILSVRC 2016 (Semantic Segmentation / Scene Parsing)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">点评:PSP net——ils vrc 2016(语义分割/场景解析)获奖者</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d?source=collection_archive---------3-----------------------#2018-12-14">https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d?source=collection_archive---------3-----------------------#2018-12-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7c58" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">金字塔场景解析网络:金字塔池模块</h2></div><p id="14fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di"> T </span>他的时代，<strong class="kh ir"> PSPNet(金字塔场景解析网)</strong>，由<strong class="kh ir"> CUHK </strong>和<strong class="kh ir"> SenseTime </strong>点评。</p><ul class=""><li id="83c4" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated"><strong class="kh ir">语义分割</strong>是为了<strong class="kh ir">只知道已知物体的每个像素的类别标签</strong>。</li><li id="9675" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">基于语义分割的场景解析</strong>，就是<strong class="kh ir">知道图像</strong>内所有像素的类别标签。</li></ul><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/d7106cda5c739d1c3fb16ed109a678c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*440pTS6AiYrHPAIe_GxjeA.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Scene Parsing</strong></figcaption></figure><p id="4c37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过使用金字塔池模块，结合基于不同区域的上下文聚合，PSPNet 超越了最先进的方法，如<a class="ae mp" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>、<a class="ae mp" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLab </a>和<a class="ae mp" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5"> DilatedNet </a>。最后，PSPNet:</p><ul class=""><li id="a6bc" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated"><strong class="kh ir">获得 2016 年 ImageNet 场景解析挑战赛冠军</strong></li><li id="49f2" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">当时在 PASCAL VOC 2012 &amp;城市景观数据集</strong>上获得第一名</li></ul><p id="e590" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并发表在<strong class="kh ir"> 2017 CVPR </strong>上，引用<strong class="kh ir"> 600 余次</strong>。(<a class="mq mr ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----e089e5df177d--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="6c78" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">涵盖哪些内容</h1><ol class=""><li id="f785" class="lk ll iq kh b ki nr kl ns ko nt ks nu kw nv la nw lq lr ls bi translated"><strong class="kh ir">全球信息的需求</strong></li><li id="72ad" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la nw lq lr ls bi translated"><strong class="kh ir">金字塔池模块</strong></li><li id="f9aa" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la nw lq lr ls bi translated"><strong class="kh ir">一些细节</strong></li><li id="8fd8" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la nw lq lr ls bi translated"><strong class="kh ir">消融研究</strong></li><li id="52c4" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la nw lq lr ls bi translated"><strong class="kh ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="3067" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">1.<strong class="ak">全球信息的需要</strong></h1><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nx"><img src="../Images/ba91b19b591eb234af43df61c6347f6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PdN6i8d5TVQS31nnAzciQg.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">(c) Original FCN without Context Aggregation, (d) PSPNet with Context Aggregation</strong></figcaption></figure><ul class=""><li id="f2bd" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated"><strong class="kh ir">不匹配关系</strong> : <a class="ae mp" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>根据外观预测黄色盒子里的船是“汽车”。但常识是，车很少过江。</li><li id="7e5d" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">混淆类别</strong> : <a class="ae mp" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>预测盒子里的物体是摩天大楼的一部分，也是建筑物的一部分。应该排除这些结果，这样整个对象要么是摩天大楼，要么是建筑物，但不能两者都是。</li><li id="bbc9" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">不显眼类</strong>:枕头与床单外观相似。忽略全局场景类别可能无法解析枕头。</li></ul><p id="4c07" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们需要图像的一些全局信息。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="bf3d" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">2.金字塔池模块</h1><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ny"><img src="../Images/46d6b605e4be5ce9d77fbb8fd95ea592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IxUlWP8RBtxNS1N6hyBAxA.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Pyramid Pooling Module After Feature Extraction (Colors are important in this figure!!!)</strong></figcaption></figure><h2 id="5a45" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">(a)和(b)</h2><p id="d294" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko ol kq kr ks om ku kv kw on ky kz la ij bi translated">在(a)处，我们有一个输入图像。在(b)处，<a class="ae mp" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>使用扩张网络策略(<a class="ae mp" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deep lab</a>/<a class="ae mp" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a>)进行特征提取。扩张的卷积跟随<a class="ae mp" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLab </a>。这里的特征图大小是输入图像的 1/8。</p><h2 id="276f" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">(c).1。<strong class="ak">分地区平均统筹</strong></h2><p id="b8d6" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko ol kq kr ks om ku kv kw on ky kz la ij bi translated">在(c)，<strong class="kh ir">对每个特征图执行子区域平均汇集。</strong></p><ul class=""><li id="0d59" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated"><strong class="kh ir">红色</strong>:这是对每个特征图执行<strong class="kh ir">全局平均汇集</strong>的最粗级别，以生成单个面元输出。</li><li id="cedb" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">橙色</strong>:这是第二层，将特征图分成<strong class="kh ir"> 2×2 </strong>子区域，然后对每个子区域进行平均池化。</li><li id="b16e" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">蓝色</strong>:这是第三层，将特征图分成<strong class="kh ir"> 3×3 </strong>个子区域，然后对每个子区域进行平均合并。</li><li id="04f5" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">绿色</strong>:这是将特征图划分为<strong class="kh ir"> 6×6 </strong>子区域，然后对每个子区域进行合并的最细级别。</li></ul><h2 id="b600" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">(c).2。<strong class="ak"> 1×1 卷积降维</strong></h2><p id="532c" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko ol kq kr ks om ku kv kw on ky kz la ij bi translated">然后对每个汇集的特征图执行<strong class="kh ir"> 1×1 卷积</strong>以<strong class="kh ir">将上下文表示减少到原始</strong>的 1/ <em class="oo"> N </em>(黑色)如果金字塔的级别大小是<em class="oo"> N </em>。</p><ul class=""><li id="7dc5" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">在本例中，<em class="oo"> N </em> =4，因为总共有 4 个级别(红、橙、蓝、绿)。</li><li id="92a5" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">如果输入特征图的数量是 2048，那么输出特征图将是(1/4)×2048 = 512，即 512 个输出特征图。</li></ul><h2 id="cf69" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">(c).3。用于上采样的双线性插值</h2><p id="f1fa" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko ol kq kr ks om ku kv kw on ky kz la ij bi translated">执行双线性插值以对每个低维特征图进行上采样，使其具有与原始特征图(黑色)相同的大小。</p><h2 id="0852" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">(c).4。用于上下文聚合的串联</h2><p id="d483" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko ol kq kr ks om ku kv kw on ky kz la ij bi translated">所有不同级别的上采样特征图都与原始特征图(黑色)连接在一起。这些特征图被融合为全局先验。这是(c)处的金字塔池模块的结尾。</p><h2 id="9dfc" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">(四)</h2><p id="228f" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko ol kq kr ks om ku kv kw on ky kz la ij bi translated">最后，随后是卷积层，以在(d)生成最终预测图。</p><p id="1732" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">子区域平均池的思想实际上非常类似于<a class="ae mp" href="https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679" rel="noopener"> SPPNet </a>中的空间金字塔池。1×1 卷积然后串联与<a class="ae mp" rel="noopener" target="_blank" href="/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568">exception</a>或<a class="ae mp" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>使用的深度方向可分离卷积中的深度方向卷积非常相似，除了双线性插值用于使所有特征图的大小相等。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="bc8b" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated"><strong class="ak"> 3。关于训练的一些细节</strong></h1><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi op"><img src="../Images/3d05bbb42c8d55a78371b676d11e2aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*z3iUe2FjLFJ6Z5lsJ5eSPQ.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Auxiliary Loss at the Middle</strong></figcaption></figure><ul class=""><li id="08f0" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">辅助损耗在训练时使用。将权重α0.4 添加到辅助损失中，以平衡最终损失和辅助损失。在测试过程中，辅助损耗被放弃。这是一种用于训练非常深的网络的深度监督训练策略。这个思路类似于<a class="ae mp" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener"> GoogLeNet / Inception-v1 </a>中的辅助分类器。</li><li id="4b55" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">使用“Poly”学习代替基本学习。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="7de4" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">4.消融研究</h1><p id="80eb" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko ol kq kr ks om ku kv kw on ky kz la ij bi translated"><strong class="kh ir"> ADE2K </strong>数据集是<strong class="kh ir"> ImageNet 场景解析挑战赛 2016 </strong>中的数据集。这是一个更具挑战性的数据集，有多达 150 个类，1，038 个图像级标签。并且有 20K/2K/3K 图像用于训练/验证/测试。</p><p id="c0f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">验证集用于消融研究。</p><h2 id="cfac" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">4.1.最大(Max)与平均(AVE)池化和降维(DR)</h2><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/944b0422c30a4255af8eb0167699c82a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*7EfzTI_a7SIhWxztSM3LAA.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Different Approaches on ADE2K Validation Set Results</strong></figcaption></figure><ul class=""><li id="e982" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated"><strong class="kh ir"> ResNet50-Baseline </strong>:基于 ResNet50 的 FCN，网络扩大。</li><li id="a95f" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">B1</strong>’和’<strong class="kh ir">b 1236</strong>’:分别为面元尺寸{1×1}和{1×1，2×2，3×3，6×6}的汇集特征图。</li><li id="017c" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">最大</strong>、<strong class="kh ir">平均</strong>:分别为最大汇集和平均汇集操作。</li><li id="78d0" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated"><strong class="kh ir">DR</strong>’:降维。</li></ul><p id="70ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">平均池一直有更好的结果。而且用 DR 比不用 DR 好。</p><h2 id="17dd" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">4.2.辅助损耗</h2><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi or"><img src="../Images/8507b3dd3b12411887905f4ee68adec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*R6w1h8xrAIfE7xP7oXqJaQ.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Different Weights for Auxiliary Loss on ADE2K Validation Set Results</strong></figcaption></figure><p id="601f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">α=0.4 时，性能最佳。因此，使用α=0.4。</p><h2 id="dc86" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">4.3.不同深度数和多尺度(MS)测试</h2><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi os"><img src="../Images/9f8411db1b6e70be43df2d60e2ed9775.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*Yny5Fe7XOZCb2gg9yQHgoA.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Different Depth Numbers and Multi-Scale Testing on ADE2K Validation Set Results</strong></figcaption></figure><p id="97d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们所知，更深的模型有更好的结果。多尺度测试有助于改善结果。</p><h2 id="1d14" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">4.4.数据扩充(DA)及其与其他技术的比较</h2><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/3ea7322536b6f09812a94fc24540609e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*a0uPJCykRiYo-4uz15h-Yw.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Comparison with State-of-the-art Approaches on ADE2K Validation Set Results (All are Single Scale Except the Last Row.)</strong></figcaption></figure><ul class=""><li id="4916" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">ResNet269+DA+AL+PSP :对于单一尺度的测试，把所有的东西结合在一起，它比最先进的方法要好得多。</li><li id="bd62" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">ResNet269+DA+AL+PSP+MS :同样通过多尺度测试，获得更好的效果。</li><li id="dbec" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">下面是一些例子:</li></ul><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/26c99aa29d20a7abb83546238b332f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*WNE7C8-MHU5_Xqdk916t6g.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">ADE2K Examples</strong></figcaption></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="f7fe" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated"><strong class="ak"> 5。与最先进方法的比较</strong></h1><h2 id="c488" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">5.1.ADE2K—2016 年 ImageNet 场景解析挑战赛</h2><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/3011c700cb9b708a93f7bb39ada43694.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*MLagEJgIorecTA0fCljLcA.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">ADE2K Test Set</strong></figcaption></figure><ul class=""><li id="0436" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">PSPNet 赢得了 2016 年 ImageNet 场景解析挑战赛。</li></ul><h2 id="cee3" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">5.2.帕斯卡 VOC 2012</h2><p id="e89a" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko ol kq kr ks om ku kv kw on ky kz la ij bi translated">对于 DA，有 10582/1449/1456 个图像用于训练/验证/测试。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ca"><img src="../Images/0b1e51933f26e8f5bb501a7664fe8706.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0G22BuBgMEnvCjvrwdktnA.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">PASCAL VOC 2012 Test Set</strong></figcaption></figure><ul class=""><li id="0528" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">+'表示也由 MS COCO 数据集预先训练。</li><li id="5fd0" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">PSPNet 再次胜过所有最先进的方法，如 FCN、DeconvNet、DeepLab 和 Dilation8。</li><li id="07ae" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">下面是一些例子:</li></ul><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/452b6a6a2b4472ed66fcea27b5fbd540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*jpuqIqSxOZJh-p-62oRWmg.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">PASCAL VOC 2012 Examples</strong></figcaption></figure><h2 id="f607" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">5.3.城市景观</h2><p id="4510" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko ol kq kr ks om ku kv kw on ky kz la ij bi translated">它包含从 50 个城市不同季节收集的 5000 幅高质量像素级精细注释图像。有 2975/500/1525 用于培训/验证/测试。它定义了包含东西和对象的 19 个类别。此外，在比较中，为两种设置提供 20000 个粗略注释的图像，即，仅用精细数据或用精细和粗略数据两者进行训练。两者的训练都用“++”标记。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/e238754dbb0fe7544372590c881c9974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*gUa5GDpHOTqoHkbKjqOAig.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Cityscapes Test Set</strong></figcaption></figure><ul class=""><li id="d56d" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">仅用精细数据训练，或者同时用精细和粗糙数据训练，PSPNet 也得到了最好的结果。</li><li id="3d70" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">下面是一些例子:</li></ul><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/3881d1276185d28e1ceceb45df8ee6b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*XcNlnENUb44YJ6YXLxmCWg.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Cityscapes Examples</strong></figcaption></figure><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/488314863bc225cdd2865f2d93d0bede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*J33mxWAtCSEV1GsWV3vKLQ.gif"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Cityscapes Examples</strong></figcaption></figure><ul class=""><li id="de7c" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">作者还上传了 Cityscapes 数据集的视频，令人印象深刻:</li></ul><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="oz pa l"/></div></figure><ul class=""><li id="b763" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">另外两个视频例子:</li></ul><p id="b811" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">https://www.youtube.com/watch?v=gdAVqJn_J2M<a class="ae mp" href="https://www.youtube.com/watch?v=gdAVqJn_J2M" rel="noopener ugc nofollow" target="_blank"/></p><p id="3677" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">【https://www.youtube.com/watch?v=HYghTzmbv6Q T4】</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><p id="f558" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">利用金字塔池模块，获取图像的全局信息，改善结果。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h2 id="0857" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">参考</h2><p id="5e77" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko ol kq kr ks om ku kv kw on ky kz la ij bi translated">【2017 CVPR】【PSPNet】<br/><a class="ae mp" href="https://arxiv.org/abs/1612.01105" rel="noopener ugc nofollow" target="_blank">金字塔场景解析网</a></p><h2 id="a917" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">我对图像分类的相关综述</h2><p id="ce12" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko ol kq kr ks om ku kv kw on ky kz la ij bi translated">)(我)(们)(都)(不)(想)(要)(让)(这)(些)(人)(都)(有)(这)(些)(的)(情)(况)(,)(我)(们)(都)(不)(想)(会)(有)(什)(么)(情)(况)(,)(我)(们)(都)(不)(想)(会)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(感)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(都)(是)(很)(好)(的)(,)(我)(们)(都)(是)(很)(好)(的)(。</p><h2 id="fb51" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">我对语义分割的相关评论</h2><p id="b748" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko ol kq kr ks om ku kv kw on ky kz la ij bi translated">[<a class="ae mp" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a>][<a class="ae mp" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a>][<a class="ae mp" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deep lab v1&amp;deep lab v2</a>][<a class="ae mp" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>][<a class="ae mp" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a>]</p><h2 id="b7f8" class="nz na iq bd nb oa ob dn nf oc od dp nj ko oe of nl ks og oh nn kw oi oj np ok bi translated">我对生物医学图像分割的相关综述</h2><p id="5dfa" class="pw-post-body-paragraph kf kg iq kh b ki nr jr kk kl ns ju kn ko ol kq kr ks om ku kv kw on ky kz la ij bi translated">[ <a class="ae mp" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">累计视频 1 </a> ] [ <a class="ae mp" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">累计视频 2 / DCAN </a> ] [ <a class="ae mp" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">优网</a> ] [ <a class="ae mp" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener"> CFS-FCN </a> ]</p></div></div>    
</body>
</html>