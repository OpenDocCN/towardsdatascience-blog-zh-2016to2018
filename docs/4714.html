<html>
<head>
<title>Common Loss functions in machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中常见的损失函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23?source=collection_archive---------0-----------------------#2018-09-02">https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23?source=collection_archive---------0-----------------------#2018-09-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e2ebca239c142c5d28c278e2afdd79d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0P1Qh6ArbD6LFEQOpq9xHw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Loss functions and optimizations</figcaption></figure><p id="6e0c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">机器通过损失函数来学习。这是一种评估特定算法对给定数据建模程度的方法。如果预测与实际结果相差太多，损失函数就会产生一个非常大的数字。逐渐地，在一些优化函数的帮助下，损失函数学习减少预测中的误差。在本文中，我们将介绍几种损失函数及其在机器/深度学习领域的应用。</p><p id="e45f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">机器学习中的算法没有放之四海而皆准的损失函数。在为特定问题选择损失函数时涉及各种因素，例如所选择的机器学习算法的类型、计算导数的容易程度以及某种程度上数据集中离群值的百分比。</p><p id="89e5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">大体上，损失函数可以根据我们正在处理的学习任务的类型分为两大类——回归损失和 T2 分类损失。在分类中，我们试图预测有限分类值集的输出，即给定手写数字图像的大数据集，将它们分类为 0-9 个数字中的一个。另一方面，回归处理预测连续值，例如给定的楼层面积、房间数量、房间大小，预测房间的价格。</p><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="448f" class="lj lk iq lf b gy ll lm l ln lo"><strong class="lf ir">NOTE </strong><br/>        n        - Number of training examples.<br/>        i        - ith training example in a data set.<br/>        y(i)     - Ground truth label for ith training example.<br/>        y_hat(i) - Prediction for ith training example.</span></pre><h2 id="c2a8" class="lj lk iq bd lp lq lr dn ls lt lu dp lv kn lw lx ly kr lz ma mb kv mc md me mf bi translated">回归损失</h2><p id="e1f8" class="pw-post-body-paragraph kc kd iq ke b kf mg kh ki kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ij bi translated"><strong class="ke ir">均方误差/二次损失/L2 损失</strong></p><p id="ab1a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="ml">数学公式</em> </strong> :-</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/e3977d7b3159495958cd0e4662c35233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*SGhoeJ_BgcfqU06CmX41rw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Mean Squared Error</figcaption></figure><p id="614d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">顾名思义，<em class="ml">均方误差</em>是以预测值和实际观测值的平方差的平均值来衡量的。它只关心误差的平均大小，而不考虑它们的方向。然而，由于平方，与偏差较小的预测相比，远离实际值的预测会受到严重的惩罚。加上 MSE 有很好的数学属性，这使得计算梯度更容易。</p><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="ba04" class="lj lk iq lf b gy ll lm l ln lo">import numpy as np</span><span id="3429" class="lj lk iq lf b gy mn lm l ln lo">y_hat = np.array([0.000, 0.166, 0.333])<br/>y_true = np.array([0.000, 0.254, 0.998])</span><span id="1b76" class="lj lk iq lf b gy mn lm l ln lo">def rmse(predictions, targets):<br/>    differences = predictions - targets<br/>    differences_squared = differences ** 2<br/>    mean_of_differences_squared = differences_squared.mean()<br/>    rmse_val = np.sqrt(mean_of_differences_squared)<br/>    return rmse_val</span><span id="0847" class="lj lk iq lf b gy mn lm l ln lo">print("d is: " + str(["%.8f" % elem for elem in y_hat]))<br/>print("p is: " + str(["%.8f" % elem for elem in y_true]))</span><span id="6da8" class="lj lk iq lf b gy mn lm l ln lo">rmse_val = rmse(y_hat, y_true)<br/>print("rms error is: " + str(rmse_val))</span></pre><p id="478a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">平均绝对误差/L1 损失</strong></p><p id="1ebf" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="ml">数学公式</em> </strong> :-</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/3930622a011c35cb4326283899144691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*piCo0iDgPmESnQkHSwAK6A.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Mean absolute error</figcaption></figure><p id="179d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="ml">另一方面，平均绝对误差</em>是预测值和实际观测值之间的绝对差值的平均值。像 MSE 一样，这也是在不考虑误差方向的情况下测量误差的大小。与 MSE 不同，MAE 需要更复杂的工具，如线性编程来计算梯度。此外，MAE 对异常值更健壮，因为它不使用 square。</p><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="a2c3" class="lj lk iq lf b gy ll lm l ln lo">import numpy as np</span><span id="149e" class="lj lk iq lf b gy mn lm l ln lo">y_hat = np.array([0.000, 0.166, 0.333])<br/>y_true = np.array([0.000, 0.254, 0.998])<br/><br/>print("d is: " + str(["%.8f" % elem for elem in y_hat]))<br/>print("p is: " + str(["%.8f" % elem for elem in y_true]))<br/><br/>def mae(predictions, targets):<br/>    differences = predictions - targets<br/>    absolute_differences = np.absolute(differences)<br/>    mean_absolute_differences = absolute_differences.mean()<br/>    return mean_absolute_differences</span><span id="58e6" class="lj lk iq lf b gy mn lm l ln lo">mae_val = mae(y_hat, y_true)<br/>print ("mae error is: " + str(mae_val))</span></pre><p id="5c6f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">平均偏差误差</strong></p><p id="2b01" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">与机器学习领域相比，这在机器学习领域更不常见。这和 MSE 一样，唯一的区别是我们不取绝对值。显然需要谨慎，因为正负误差会相互抵消。虽然在实践中不太准确，但它可以确定模型是有正偏差还是负偏差。</p><p id="78c7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="ml">数学公式</em> </strong> :-</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/188efe7f4f98c16573a437ec0c3cc1a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*BpYT_vpYizQpeY3bGuvTbw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Mean bias error</figcaption></figure><h2 id="afc4" class="lj lk iq bd lp lq lr dn ls lt lu dp lv kn lw lx ly kr lz ma mb kv mc md me mf bi translated">分类损失</h2><p id="a9de" class="pw-post-body-paragraph kc kd iq ke b kf mg kh ki kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ij bi translated"><strong class="ke ir">铰链损耗/多级 SVM 损耗</strong></p><p id="1eb8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">简单地说，正确类别的得分应该比所有错误类别的得分之和大一些安全余量(通常是 1)。因此，铰链损失用于<a class="ae mq" href="https://link.springer.com/chapter/10.1007/978-0-387-69942-4_10" rel="noopener ugc nofollow" target="_blank">最大间隔</a>分类，最显著的是用于<a class="ae mq" href="https://en.wikipedia.org/wiki/Support_vector_machine" rel="noopener ugc nofollow" target="_blank">支持向量机</a>。虽然不是<a class="ae mq" href="https://ipfs.io/ipfs/QmXoypizjW3WknFiJnKLwHCnL72vedxjQkDDP1mXWo6uco/wiki/Differentiable_function.html" rel="noopener ugc nofollow" target="_blank">可微的</a>，但它是一个凸函数，这使得它很容易与机器学习领域中常用的凸优化器一起工作。</p><p id="c80c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="ml">数学公式</em> </strong> :-</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mr"><img src="../Images/c9a4500a105ac26a5d125dce4be7a2e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ekz6PLfuwA0I_w-xmMqBqg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">SVM Loss or Hinge Loss</figcaption></figure><p id="b53e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">考虑一个例子，其中我们有三个训练样本和三个要预测的类别——狗、猫和马。低于我们的算法对每一类的预测值:-</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ms"><img src="../Images/09fd209ee5b22fa5b27c926e194f5544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNIxTeFzMbPjMgreM2G_ww.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Hinge loss/ Multi class SVM loss</figcaption></figure><p id="44a0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">计算所有 3 个训练示例的铰链损耗:-</p><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="9945" class="lj lk iq lf b gy ll lm l ln lo"><strong class="lf ir">## 1st training example</strong><br/>max(0, (1.49) - (-0.39) + 1) + max(0, (4.21) - (-0.39) + 1)<br/>max(0, 2.88) + max(0, 5.6)<br/>2.88 + 5.6<br/><em class="ml">8.48 (High loss as very wrong prediction)</em></span><span id="839b" class="lj lk iq lf b gy mn lm l ln lo"><strong class="lf ir">## 2nd training example<br/></strong>max(0, (-4.61) - (3.28)+ 1) + max(0, (1.46) - (3.28)+ 1)<br/>max(0, -6.89) + max(0, -0.82)<br/>0 + 0<br/><em class="ml">0 (Zero loss as correct prediction)</em></span><span id="b254" class="lj lk iq lf b gy mn lm l ln lo"><strong class="lf ir">## 3rd training example<br/></strong>max(0, (1.03) - (-2.27)+ 1) + max(0, (-2.37) - (-2.27)+ 1)<br/>max(0, 4.3) + max(0, 0.9)<br/>4.3 + 0.9<br/><em class="ml">5.2 (High loss as very wrong prediction)</em></span></pre><p id="e5ef" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">交叉熵损失/负对数似然</strong></p><p id="fec3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是分类问题最常见的设置。交叉熵损失随着预测概率偏离实际标签而增加。</p><p id="89ce" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="ml">数学公式</em> </strong> :-</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/cee671476c138a354cc2cbcf82d8e453.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zi1wKAAGGt1Bn6mqo2MSFw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Cross entropy loss</figcaption></figure><p id="685b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">请注意，当实际标签为 1 (y(i) = 1)时，函数的后半部分消失，而当实际标签为 0 (y(i) = 0)时，函数的前半部分消失。简而言之，我们只是将实际预测概率的对数乘以地面真实类。一个重要的方面是交叉熵损失严重惩罚了那些有信心但错误的预测。</p><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="d0cb" class="lj lk iq lf b gy ll lm l ln lo">import numpy as np</span><span id="1f97" class="lj lk iq lf b gy mn lm l ln lo">predictions = np.array([[0.25,0.25,0.25,0.25],<br/>                        [0.01,0.01,0.01,0.96]])<br/>targets = np.array([[0,0,0,1],<br/>                   [0,0,0,1]])</span><span id="44d0" class="lj lk iq lf b gy mn lm l ln lo">def cross_entropy(predictions, targets, epsilon=1e-10):<br/>    predictions = np.clip(predictions, epsilon, 1. - epsilon)<br/>    N = predictions.shape[0]<br/>    ce_loss = -np.sum(np.sum(targets * np.log(predictions + 1e-5)))/N<br/>    return ce_loss</span><span id="10d7" class="lj lk iq lf b gy mn lm l ln lo">cross_entropy_loss = cross_entropy(predictions, targets)<br/>print ("Cross entropy loss is: " + str(cross_entropy_loss))</span></pre></div></div>    
</body>
</html>