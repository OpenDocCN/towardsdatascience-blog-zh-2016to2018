<html>
<head>
<title>CIFAR-10 Image Classification in TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流中的CIFAR-10影像分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c?source=collection_archive---------1-----------------------#2018-04-17">https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c?source=collection_archive---------1-----------------------#2018-04-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/1fc16131853f9a73f625aa171ee77ae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZCCqwSV6pWLBAhkN"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@rawpixel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">rawpixel</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="fb51" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个故事中，我将对来自<a class="ae kc" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10数据集</a>的图像进行分类。这个故事包括预处理图像和训练/预测卷积神经网络模型。</p><p id="9f7b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个笔记本的一些代码和描述是借用了<a class="ae kc" href="https://github.com/udacity/deep-learning/tree/master/image-classification" rel="noopener ugc nofollow" target="_blank">这个回购</a>由<a class="ae kc" href="http://www.udacity.com" rel="noopener ugc nofollow" target="_blank"> Udacity </a>提供，但是这个故事提供了更丰富的描述。代码和jupyter笔记本可以在我的github repo里找到，<a class="ae kc" href="https://github.com/deep-diver/CIFAR10-img-classification-tensorflow" rel="noopener ugc nofollow" target="_blank">https://github . com/deep-diver/cifar 10-img-classification-tensor flow</a>。</p><p id="baf7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我的深度学习背景是<a class="ae kc" href="https://www.udacity.com/course/deep-learning-nanodegree--nd101" rel="noopener ugc nofollow" target="_blank">uda city { Deep Learning N</a>D&amp;<a class="ae kc" href="https://www.udacity.com/course/ai-artificial-intelligence-nanodegree--nd898" rel="noopener ugc nofollow" target="_blank">AI-nd</a>with contentrations(<a class="ae kc" href="https://www.udacity.com/course/computer-vision-nanodegree--nd891" rel="noopener ugc nofollow" target="_blank">CV</a>，<a class="ae kc" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank"> NLP </a>，VUI)}，<a class="ae kc" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank">Coursera Deep Learning . AI Specialization</a>(AI-ND被拆分成4个不同的部分，我是和之前版本的ND一起完成的)。还有，我目前正在服用<a class="ae kc" href="https://www.udacity.com/course/data-analyst-nanodegree--nd002" rel="noopener ugc nofollow" target="_blank"> Udacity数据分析师ND </a>，已经80%完成。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h2 id="2ac7" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">在跳进去之前…</h2><ul class=""><li id="4fac" class="mb mc iq kf b kg md kk me ko mf ks mg kw mh la mi mj mk ml bi translated">理解原始数据和原始标签</li></ul><h2 id="857d" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">实施预处理功能</h2><ul class=""><li id="cc03" class="mb mc iq kf b kg md kk me ko mf ks mg kw mh la mi mj mk ml bi translated">规格化，一个热编码</li><li id="31ea" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">培训/开发/测试集中的数据分割</li></ul><h2 id="f195" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">建立网络</h2><ul class=""><li id="367a" class="mb mc iq kf b kg md kk me ko mf ks mg kw mh la mi mj mk ml bi translated">CNN模型及其代价函数和优化器</li></ul><h2 id="81fd" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">训练神经网络</h2><ul class=""><li id="0361" class="mb mc iq kf b kg md kk me ko mf ks mg kw mh la mi mj mk ml bi translated">超参数</li><li id="28b6" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">训练模型</li></ul><h2 id="7aef" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">测试模型(预测)</h2></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="c1ee" class="mr lj iq bd lk ms mt mu ln mv mw mx lq my mz na lt nb nc nd lw ne nf ng lz nh bi translated">在跳进去之前…</h1><h2 id="50a5" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">文件列表</h2><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/1a934a7033ebc2d9970635d16e19f404.png" data-original-src="https://miro.medium.com/v2/resize:fit:202/format:webp/1*AyvPEj4ewfyU1TMFWWfj1Q.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd nn">Fig 1. list of files of batch</strong></figcaption></figure><p id="4e2f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图1所示，数据集被分成几批以<strong class="kf ir">防止</strong>你的机器运行<strong class="kf ir">出内存</strong>。CIFAR-10数据集由5个批次组成，命名为<code class="fe no np nq nr b">data_batch_1</code>、<code class="fe no np nq nr b">data_batch_2</code>等。如<a class="ae kc" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">官方网站</a>所述，<strong class="kf ir">每个文件</strong>使用python中的<strong class="kf ir"> pickle </strong>模块打包数据。</p><h2 id="956b" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">了解原始影像数据集</h2><p id="2f09" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">原来的一个<strong class="kf ir">批量数据</strong>是用numpy数组表示的<strong class="kf ir"> (10000 x 3072)矩阵</strong>。<strong class="kf ir">列数</strong>，(10000)，表示<strong class="kf ir">样本数据的数量</strong>。如<a class="ae kc" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10/CIFAR-100数据集</a>中所述，<strong class="kf ir">行向量</strong>，(3072)代表一幅32×32像素的<strong class="kf ir">彩色图像</strong>。因为这个项目将使用CNN进行分类任务，所以原始的行向量是不合适的。为了将图像数据馈入CNN模型，输入张量的维数应该是<strong class="kf ir">(宽度x高度x数量_通道)</strong>或<strong class="kf ir">(数量_通道x宽度x高度)</strong>。这取决于你的选择(查看<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" rel="noopener ugc nofollow" target="_blank"> tensorflow conv2d </a>)。我要用第一种选择，因为tensorflow的CNN操作中默认选择是这样的。</p><h2 id="d8d2" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">如何重塑成这样的形态？</h2><p id="8494" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">如果您计算32*32*3 == 3072，则图像的行向量具有完全相同的元素数量。为了将行向量重新整形为(宽度x高度x数量_通道)形式，需要两个步骤。<strong class="kf ir">第一步</strong>是使用<code class="fe no np nq nr b"><a class="ae kc" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">reshape</strong></a></code>功能，<strong class="kf ir">第二步</strong>是使用numpy中的<code class="fe no np nq nr b"><a class="ae kc" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">transpose</strong></a></code>功能。</p><p id="9d93" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据numpy官方网站的定义，<code class="fe no np nq nr b"><a class="ae kc" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">reshape</strong></a></code>将数组转换成新的形状，而不改变其数据。在这里，短语<strong class="kf ir">不改变其数据</strong>是一个重要的部分，因为你不想伤害数据。<code class="fe no np nq nr b"><a class="ae kc" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">reshape</strong></a></code>运营应分三个更详细的步骤进行。下面的方向是用逻辑概念描述的。</p><ol class=""><li id="d0bf" class="mb mc iq kf b kg kh kk kl ko nv ks nw kw nx la ny mj mk ml bi translated"><strong class="kf ir">将</strong>行向量分成<strong class="kf ir"> 3块</strong>，每块代表一个颜色通道。<br/> -得到的数组有(3×1024)个矩阵，总共就有<strong class="kf ir">(10000×3×1024)</strong>个张量。</li><li id="9e65" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la ny mj mk ml bi translated"><strong class="kf ir">用32 </strong>将中的每3块进一步分割<strong class="kf ir">。32是图像的宽度和高度。<br/> -这导致(3×32×32)，这使得<strong class="kf ir">(10000×3×32×32)</strong>张量总计</strong></li></ol><p id="77c5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了实现numpy中的逻辑概念，<code class="fe no np nq nr b"><a class="ae kc" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">reshape</strong></a></code>应该用下面的自变量来调用，(10000，3，32，32)。正如您所注意到的，当提供第三个值(32，width)时，reshape函数不会自动进一步分割。您需要明确指定最后一个值(32，height)</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nz"><img src="../Images/542275fe36ddc5d00d16d308b1f749a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8mlH1a4MEOxr6wDjCZE8Ug.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd nn">Fig 2. reshape and transpose</strong></figcaption></figure><p id="77ba" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">故事还没有结束。现在，一个图像数据被表示为(num_channel，width，height)形式。然而，<strong class="kf ir">这并不是tensorflow和matplotlib期待的形状</strong>。他们期待不同的形状(宽度，高度，通道数)。你需要交换每个轴的顺序，这就是<code class="fe no np nq nr b"><a class="ae kc" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">transpose</strong></a></code>的作用。</p><p id="8c8e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe no np nq nr b"><a class="ae kc" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">transpose</strong></a></code>可以接受一个轴列表，每个值指定它想要移动的维度的索引。例如，在(num_channel，width，height)的numpy数组中调用transpose with argument (1，2，0)将返回(width，height，num_channel)的新numpy数组。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="ak">Code 1. reshape and transpose after loading</strong></figcaption></figure><h2 id="11ee" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">了解原始标签</h2><p id="38c2" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">标签数据只是一个包含从<strong class="kf ir"> 0到9 </strong>的10000个数字的<strong class="kf ir">列表，对应CIFAR-10 </strong>中的<strong class="kf ir"> 10个类中的每一个。</strong></p><ul class=""><li id="f842" class="mb mc iq kf b kg kh kk kl ko nv ks nw kw nx la mi mj mk ml bi translated">飞机:0</li><li id="316d" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">汽车:1</li><li id="395f" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">小鸟:2</li><li id="223b" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">猫:3</li><li id="7a4e" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">鹿:4</li><li id="f47e" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">狗:5</li><li id="f8d2" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">青蛙:6</li><li id="37e1" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">马:7</li><li id="928a" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">船舶:8艘</li><li id="41c6" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">卡车:9辆</li></ul><p id="bf31" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">代码1 </strong>定义一个函数，返回一个方便的图像类别列表。该功能<strong class="kf ir">将用于预测阶段</strong>。因为预测的输出是一个数字，所以它应该被转换成字符串，以便人们能够阅读。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="ak">Code 2. label names</strong></figcaption></figure><h2 id="e6d0" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">探索数据</h2><p id="a72f" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">你可以在<a class="ae kc" href="https://github.com/deep-diver/CIFAR10-img-classification-tensorflow" rel="noopener ugc nofollow" target="_blank"> my github </a>通过改变<code class="fe no np nq nr b">batch_id</code>和<code class="fe no np nq nr b">sample_id</code>来玩笔记本中的代码单元。<code class="fe no np nq nr b">batch_id</code>是一个批次(1-5)的id。<code class="fe no np nq nr b">sample_id</code>是批次中图像和标签对的id。</p><p id="dc28" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面定义的display_stats回答了一些问题，比如在给定的一批数据中..</p><ul class=""><li id="14ef" class="mb mc iq kf b kg kh kk kl ko nv ks nw kw nx la mi mj mk ml bi translated">“所有可能的标签是什么？”</li><li id="e65a" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">"图像数据的取值范围是什么？"</li><li id="9117" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">"标签是有序的还是随机的？"</li></ul><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="ak">Code 3. showing sample image in batch #3</strong></figcaption></figure><p id="865a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我已经尝试了第三批和第7000张图片。如图3中的结果所示，每个类别的图像数据的数量大致相同。</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/0c7e99054646a2fbf6a03ecf32fb62ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*fTMYyJvhV6E8ktgDaqcfDA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd nn">Fig 3. showing sample image in batch #3</strong></figcaption></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="30d7" class="mr lj iq bd lk ms mt mu ln mv mw mx lq my mz na lt nb nc nd lw ne nf ng lz nh bi translated">实现预处理功能</h1><p id="799c" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">您可能会注意到，一些框架/库，如TensorFlow、Numpy或Scikit-learn，提供了与我将要构建的类似的功能。这反映了我不太依赖框架或库的目的。我相信我可以把自己的模型做得更好，或者复制/试验论文中介绍的最先进的模型。对于这个故事，我将实现<code class="fe no np nq nr b"><strong class="kf ir">normalize</strong></code>和<code class="fe no np nq nr b"><strong class="kf ir">one-hot-encode</strong></code>函数。</p><h2 id="bded" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">使标准化</h2><p id="f898" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated"><code class="fe no np nq nr b"><strong class="kf ir">normalize</strong></code>函数获取数据<code class="fe no np nq nr b">x</code>，并将其作为规范化的Numpy数组返回。<code class="fe no np nq nr b">x</code>可以是任何东西，也可以是N维数组。在这个故事中，它将是一个图像的三维数组。<a class="ae kc" href="https://www.quora.com/What-is-the-meaning-of-min-max-normalization" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">最小-最大归一化</strong> </a> <strong class="kf ir"> ( </strong> <code class="fe no np nq nr b"><strong class="kf ir">y = (x-min) / (max-min)</strong></code> <strong class="kf ir"> ) </strong>技术被使用，但是也有其他选项。通过应用最小-最大归一化，<strong class="kf ir">原始图像数据</strong>将在0到1 的范围内进行<strong class="kf ir">变换。为什么应该执行规范化的一个简单答案与激活函数有些关系。</strong></p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="ak">Code 4. min-max normalize function</strong></figcaption></figure><p id="0d0a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，<a class="ae kc" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> sigmoid </strong> </a>激活函数接受一个输入值，<strong class="kf ir">输出</strong>一个新值，范围为<strong class="kf ir">从0到1 </strong>。当输入值较大时，输出值很容易达到最大值1。同样，当输入值稍小时，输出值很容易达到最大值0。</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi od"><img src="../Images/304a4ee5d9723fe363946c35233b792d.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*bE8H0RW4hQlEK9D-_gJ_ww.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd nn">Fig 4. sigmoid function</strong></figcaption></figure><p id="567c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">再比如，<a class="ae kc" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> ReLU </strong> </a>激活函数取一个输入值，输出一个范围从<strong class="kf ir"> 0到无穷大</strong>的新值。当输入值稍大时，输出值线性增加。但是，当输入值稍小时，输出值很容易达到最大值0。</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/b2037cfb0a814d5cea5408c7bdb2af7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*6VTl6drfVxubLF6NMAqlEA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd nn">Fig 5. ReLU function</strong></figcaption></figure><p id="88ab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，当您考虑图像数据时，所有值最初的范围是从0到255。这听起来像是当它被传递给<a class="ae kc" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid </a>函数时，输出几乎总是1，而当它被传递给<a class="ae kc" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU </a>函数时，输出可能非常巨大。当执行反向传播过程以优化网络时，这可能导致<a class="ae kc" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">爆炸/消失梯度问题</a>。为了避免这个问题，最好让所有的值都在0和1之间。</p><h2 id="012b" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">独热编码</h2><p id="8c8b" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">稍后，我将解释模型。目前，你需要知道的是模型的输出。它是基于模型预测结果的每类图像的一组概率。为了在代码中表示这些概率，需要一个具有与图像类别数量相同的元素数量的向量。例如，CIFAR-10提供了10种不同的图像类别，因此您还需要一个大小为10的矢量。</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/cfb1390ffadf38382c249c4135e08e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hUPfMQm-RlSIVI5mtOFGHg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd nn">Fig 6. one-hot-encoding process</strong></figcaption></figure><p id="ca3d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们的模型应该能够比较预测与地面真相标签。这意味着标签数据的形状也应该转换成大小为10的向量。相反，因为label是基本事实，所以将值1设置为相应的元素。</p><p id="23c2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe no np nq nr b"><strong class="kf ir">one_hot_encode</strong></code>函数接受输入<code class="fe no np nq nr b"><strong class="kf ir">x</strong></code>，这是一个标签列表(基本事实)。列表中元素的总数是一个批次中样本的总数。<code class="fe no np nq nr b"><strong class="kf ir">one_hot_encode</strong></code>函数返回一个二维张量，其中行数是批量的大小，列数是图像类的数量。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="ak">Code 5. one hot encoding function</strong></figcaption></figure><h2 id="2d98" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">预处理所有数据并保存</h2><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi og"><img src="../Images/34b6f4e26039ae202c7daf50bec6b004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_MNUsSTnLdE3k2W3XKEf_Q.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd nn">Fig 7. train/valid/test set</strong></figcaption></figure><p id="a0fb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面的代码6使用之前实现的函数<code class="fe no np nq nr b"><strong class="kf ir">normalize</strong></code>和<code class="fe no np nq nr b"><strong class="kf ir">one-hot-encode</strong></code>来预处理给定的数据集。如图7所示，来自每个批次的10%的数据将被合并以形成验证数据集。其余90%的数据用作训练数据集。最后，有测试数据集已经提供。下面的代码单元将预处理所有的CIFAR-10数据，并将其保存到外部文件中。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="ak">Code 6. preprocessing</strong></figcaption></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="0bbc" class="mr lj iq bd lk ms mt mu ln mv mw mx lq my mz na lt nb nc nd lw ne nf ng lz nh bi translated">张量流基础</h1><p id="efbb" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">在开始构建网络和训练过程之前，提醒自己TensorFlow是如何工作的以及有哪些包是很好的。</p><h2 id="a671" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">TensorFlow软件包</h2><p id="fa64" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">TensorFlow附带一堆包。您甚至可以找到具有类似功能的模块。例如，tf.nn.conv2d和tf.layers.conv2d都是二维卷积运算。以下是每个包的类别的用途。</p><p id="15b2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe no np nq nr b"><strong class="kf ir">tf.nn</strong></code> <strong class="kf ir">:神经网络的低层APIs】</strong></p><ul class=""><li id="7315" class="mb mc iq kf b kg kh kk kl ko nv ks nw kw nx la mi mj mk ml bi translated">这个包中的每个API都有其唯一的用途</li><li id="8443" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">例如，为了在conv2d之后应用激活函数，您需要两个单独的API调用</li><li id="82e3" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">你可能需要自己手动设置很多设置</li></ul><p id="9e71" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe no np nq nr b"><strong class="kf ir">tf.layers</strong></code> <strong class="kf ir">:神经网络的高级API</strong></p><ul class=""><li id="b4c4" class="mb mc iq kf b kg kh kk kl ko nv ks nw kw nx la mi mj mk ml bi translated">这个包下的每个API可能都有简化的过程</li><li id="5e8a" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">例如，为了在conv2d之后应用激活函数，您不需要两次单独的API调用。此程序包下的Intead，conv2d API具有激活参数</li><li id="7a89" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">这个包中的每个API在参数中都有很多默认设置</li></ul><p id="1e2c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe no np nq nr b"><strong class="kf ir">tf.contrib</strong></code> <strong class="kf ir">:包含易变的或者实验性的API</strong></p><ul class=""><li id="c1db" class="mb mc iq kf b kg kh kk kl ko nv ks nw kw nx la mi mj mk ml bi translated">就像文档解释的那样，这个包提供了实验代码</li><li id="17f8" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">您可能会找到更方便的API</li><li id="3cac" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">当你在主包下找不到功能的时候，你可以去看看这个包</li><li id="0d92" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">它意味着包含最终应该合并到核心TensorFlow中的特性和贡献，但是您可以将它们想象为正在构建中</li></ul><p id="f4a6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我将在每个不同的包下使用API，这样我可以熟悉不同的API用法。</p><h2 id="0cd8" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">张量流工作流</h2><p id="9549" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">根据官方文档，<strong class="kf ir"> TensorFlow </strong>使用一个<strong class="kf ir">数据流图</strong>来根据各个操作之间的依赖关系来表示您的计算。这导致了一个低级编程模型，其中您首先<strong class="kf ir">定义数据流图</strong>，然后创建一个TensorFlow <strong class="kf ir">会话来跨一组本地和远程设备运行图</strong>的一部分。</p><p id="71ef" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">数据流</strong>是<strong class="kf ir">并行计算</strong>的通用编程模型。在数据流图中，节点代表计算单元，边代表计算消耗或产生的数据。例如，在TensorFlow图中，tf.matmul操作将对应于具有两条传入边(要相乘的矩阵)和一条传出边(相乘的结果)的单个节点。</p><p id="59ec" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">大多数<strong class="kf ir"> TensorFlow </strong>程序<strong class="kf ir">从数据流图构建阶段</strong>开始。在这个阶段，您调用TensorFlow API函数来构造新的tf。操作(节点)和tf。张量(边缘)对象，并将它们添加到一个<code class="fe no np nq nr b"><strong class="kf ir">tf.Graph</strong></code>实例中。<strong class="kf ir"> TensorFlow提供了一个默认图</strong>，它是同一上下文中所有API函数的一个<strong class="kf ir">隐式参数。</strong></p><p id="1922" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe no np nq nr b"><strong class="kf ir">tf.Session.run</strong></code> <strong class="kf ir">方法</strong>是<strong class="kf ir">运行</strong> <code class="fe no np nq nr b"><strong class="kf ir">tf.Operation</strong></code> <strong class="kf ir">或评估</strong> <code class="fe no np nq nr b"><strong class="kf ir">tf.Tensor</strong></code>的主要机制。您可以将一个或多个<code class="fe no np nq nr b"><strong class="kf ir">tf.Operation</strong></code>或<code class="fe no np nq nr b"><strong class="kf ir">tf.Tensor</strong></code>对象传递给<code class="fe no np nq nr b"><strong class="kf ir">tf.Session.run</strong></code>，TensorFlow将执行计算结果所需的操作。</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/8586c75cee74ca886caa7d2d9c099dc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/1*SmfhKWHXHVEMg8KqNaj-uw.gif"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd nn">Fig 9. TensorFlow dataflow graph</strong></figcaption></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="fe40" class="mr lj iq bd lk ms mt mu ln mv mw mx lq my mz na lt nb nc nd lw ne nf ng lz nh bi translated">建立网络</h1><p id="d6ab" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">下面的图8简单地显示了模型将会是什么样子。</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oi"><img src="../Images/88e093e17e03e6e0373641b5034520c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xlOZHo8svfDWDyxFFnMurQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd nn">Fig 8. classification model</strong></figcaption></figure><h2 id="6aaf" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">为模型准备输入</h2><p id="46e7" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">为了训练模型，至少应该提供两种数据。应该将图像数据输入到模型中，以便模型可以学习并输出其预测。应该在模型的末尾提供标签数据，以便与预测输出进行比较。要提供的值有很多，但我只打算再包括一个。<code class="fe no np nq nr b"><strong class="kf ir">keep_prob</strong></code>是单个数字在什么概率下每层应该保留多少个单元。这就是所谓的<a class="ae kc" href="https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5" rel="noopener">辍学技术</a>。</p><p id="7e66" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">无论<strong class="kf ir">进给数据</strong>应该放在模式的前面、中间还是最后，这些<strong class="kf ir">进给数据</strong>都称为<strong class="kf ir">输入</strong>。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="ak">Code 7. input tensors</strong></figcaption></figure><p id="5320" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TensorFlow中的<code class="fe no np nq nr b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/placeholder" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.placeholer</strong></a></code>创建一个<strong class="kf ir">输入。</strong>每个<strong class="kf ir">输入</strong>需要指定期望的数据类型及其维度形状。形状中无表示长度未定义，可以是任何长度。</p><h2 id="1bb1" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">创建模型</h2><p id="d1cf" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">整个模型总共由14层组成。除了下面列出的层什么技术适用于建立模型。</p><ol class=""><li id="93df" class="mb mc iq kf b kg kh kk kl ko nv ks nw kw nx la ny mj mk ml bi translated"><strong class="kf ir">与64个大小为(3x3)的不同滤波器卷积</strong></li><li id="aa9a" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la ny mj mk ml bi translated"><strong class="kf ir">最大2池</strong> <br/> - ReLU激活功能<br/> -批量正常化</li><li id="9596" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la ny mj mk ml bi translated"><strong class="kf ir">与128个大小为(3×3)的不同滤波器卷积</strong></li><li id="bbe9" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la ny mj mk ml bi translated"><strong class="kf ir">最大池乘2 </strong> <br/> - ReLU激活功能<br/> -批量正常化</li><li id="c81b" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la ny mj mk ml bi translated"><strong class="kf ir">与256个大小为(3×3)的不同滤波器卷积</strong></li><li id="1103" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la ny mj mk ml bi translated"><strong class="kf ir">最大2池</strong> <br/> - ReLU激活功能<br/> -批量正常化</li><li id="32ec" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la ny mj mk ml bi translated"><strong class="kf ir">与大小为(3x3)的512个不同滤波器卷积</strong></li><li id="716c" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la ny mj mk ml bi translated"><strong class="kf ir">最大2池</strong> <br/> - ReLU激活功能<br/> -批量正常化</li><li id="3d2a" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la ny mj mk ml bi translated"><strong class="kf ir">展平最后一次卷积操作的三维输出。</strong></li><li id="c02d" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la ny mj mk ml bi translated"><strong class="kf ir">全连通128层</strong> <br/> -脱扣<br/> -批量正常化</li><li id="c012" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la ny mj mk ml bi translated"><strong class="kf ir">256单元全连接层</strong> <br/> -脱扣<br/> -批量正常化</li><li id="59f4" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la ny mj mk ml bi translated"><strong class="kf ir">全连接512层</strong> <br/> -漏接<br/> -批量归一化</li><li id="c28d" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la ny mj mk ml bi translated"><strong class="kf ir">1024单元全连通层</strong> <br/> -漏接<br/> -批量归一化</li><li id="d1a1" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la ny mj mk ml bi translated"><strong class="kf ir">具有10个单元(图像类别数)的全连接层</strong></li></ol><p id="904d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图9描述了当您使用<strong class="kf ir">【通道x宽度x高度】</strong>张量格式时，概念卷积操作与TensorFlow实现的不同之处。</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oj"><img src="../Images/327ea7e9a0a9152a4bb23bd9cc0fa9ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ikG1RNlKsNccw7vnlfaBHA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd nn">Fig 9. convolving operation in tensorflow with [NCWH] form of input</strong></figcaption></figure><p id="3f75" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面的代码8展示了如何在TensorFlow中构建模型。我们先来看看卷积层。可以使用<code class="fe no np nq nr b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.nn.conv2d</strong></a></code>或<code class="fe no np nq nr b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.layers.conv2d</strong></a></code>创建卷积层。后一种更方便，因为它有更多可选参数。前一种选择创建了最基本的卷积层，您可能需要在<code class="fe no np nq nr b"><strong class="kf ir">tf.nn.conv2d</strong></code>之前或之后添加更多卷积层。比如激活函数可以在<code class="fe no np nq nr b"><strong class="kf ir">tf.layers.conv2d</strong></code>中直接指定为参数，但是在使用<code class="fe no np nq nr b"><strong class="kf ir">tf.nn.conv2d</strong></code>的时候要手动添加。</p><p id="6490" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">构建卷积层时，有三点需要考虑。这就是步幅、填充和过滤器。步长决定了滤波器的窗口在每个卷积步骤中应该移动多少，它是一个长度为4的一维张量。但是，从技术上来说，官方文件说<em class="ok">'必须有步幅[0] =步幅[3] = 1' </em>。所以你只能控制步幅[1]和步幅[2]的值，但是把它们设置成相等的值是不是很常见。[1，1，1，1]和[1，2，2，1]是最常见的用例。我将使用[1，1，1，1],因为我想逐个像素地进行卷积。</p><p id="a5bd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当一个完整的卷积操作完成后，图像的输出尺寸变得小于输入尺寸。但是，您可以通过在图像周围应用额外的0值像素来强制保持不变。当填充设置为“相同”时，图像的输出大小将保持与输入图像相同。另一方面，当填充被设置为“有效”时，它会更小。我将使用“相同的”填充样式，因为它更容易管理每个卷积层中图像的大小。</p><p id="7ad0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可以用<code class="fe no np nq nr b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/Variable" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.Variable</strong></a></code>定义过滤器，因为它只是一堆权重值，并且随着时间的推移在训练网络时会发生变化。过滤器应该是形状为[过滤器_高度，过滤器_宽度，入口_通道，出口_通道]的4-D张量。in_channels表示当前卷积运算所应用的通道数，out_channels表示当前卷积运算将要产生的通道数。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="ak">Code 8. CNN model</strong></figcaption></figure><p id="265b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如前所述，<code class="fe no np nq nr b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.nn.conv2d</strong></a></code>没有将激活函数作为参数的选项(而<code class="fe no np nq nr b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.layers.conv2d</strong></a></code>有)，<code class="fe no np nq nr b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/relu" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.nn.relu</strong></a></code>被明确地添加在<code class="fe no np nq nr b"><strong class="kf ir">tf.nn.conv2d</strong></code>操作之后。然后通过使用<code class="fe no np nq nr b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/max_pool" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.nn.max_pool</strong></a></code>功能应用最大池化。max pooling操作可以被视为一种特殊的conv2d操作，只是它没有权重。目的是通过保留最强的值来缩小图像。ksize=[1，2，2，1]和stamps =[1，2，2，1]表示将图像缩小一半。</p><p id="5ff1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe no np nq nr b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.contrib.layers.flatten</strong></a></code>、<code class="fe no np nq nr b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.contrib.layers.fully_connected</strong></a></code>、<code class="fe no np nq nr b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/dropout" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.nn.dropout</strong></a></code>功能直观易懂，使用非常方便。只有一件重要的事情要记住，你不要在完全连接的层列表的末尾指定激活函数。这将在稍后定义成本函数时指定。</p><h2 id="d562" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">超参数</h2><p id="9263" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">超参数是通过十几次实验选择的。需要注意的一点是，必须在定义优化器之前定义learning_rate，因为这是您需要将learning rate作为构造函数参数的地方。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="ak">Code 9. hyper parameters</strong></figcaption></figure><h2 id="1818" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">成本函数和优化器</h2><p id="f649" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">最后，您将定义成本、优化器和准确性。<code class="fe no np nq nr b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/reduce_mean" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.reduce_mean</strong></a></code>取一个输入张量进行约简，输入张量是预测结果和地面事实之间某些损失函数的结果。因为CIFAR-10必须测量10个等级的损耗，所以使用了<code class="fe no np nq nr b"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">tf.nn.softmax_cross_entropy_with_logis</strong></a></code>函数。在训练网络时，您想要的是通过应用您选择的算法来最小化成本。可能是<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer" rel="noopener ugc nofollow" target="_blank"> SGD </a>，<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer" rel="noopener ugc nofollow" target="_blank"> AdamOptimizer </a>，<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer" rel="noopener ugc nofollow" target="_blank">adagradmoptimizer</a>之类的。你必须研究每种算法是如何工作的，以选择使用什么，但是AdamOptimizer通常适用于大多数情况。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="ak">Code 10. cost function &amp; optimizer &amp; accuracy</strong></figcaption></figure><h1 id="d711" class="mr lj iq bd lk ms ol mu ln mv om mx lq my on na lt nb oo nd lw ne op ng lz nh bi translated">训练神经网络</h1><p id="be7d" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated">您已经定义了成本、优化器和准确性，它们实际上是什么..</p><ul class=""><li id="8358" class="mb mc iq kf b kg kh kk kl ko nv ks nw kw nx la mi mj mk ml bi translated"><strong class="kf ir">成本<br/> - </strong> reduce_mean = &gt;约化后的<strong class="kf ir">张量</strong></li><li id="6745" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><strong class="kf ir">优化器<br/> - </strong> AdamOptimizer = &gt;应用指定渐变的<strong class="kf ir">操作</strong>。</li><li id="2a80" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><strong class="kf ir">精度<br/> - </strong>化简_均值= &gt;化简<strong class="kf ir">张量</strong></li></ul><p id="a5a4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">tf。官方文档中的Session.run方法解释了它运行TensorFlow计算的一个“步骤”,通过运行必要的图形片段来执行每个操作，并在获取中评估每个张量，用<code class="fe no np nq nr b"><strong class="kf ir">feed_dict</strong></code>中的值替换相应的输入值。获取参数可以是单个图形元素，或者任意嵌套的列表、元组等。</p><p id="270b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里真正的图元是<code class="fe no np nq nr b"><strong class="kf ir">tf.Tensor</strong></code>或<code class="fe no np nq nr b"><strong class="kf ir">tf.Operation</strong></code>。成本、优化和准确性就是其中之一。这意味着它们可以被指定为提取参数的一部分。然后，你可以在这个过程中加入一些变量。这是张量流的一个便利特性。一旦构建了图表，您需要做的就是将数据输入图表并指定要检索的结果。</p><h2 id="48c6" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">单一优化</h2><p id="9c38" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated"><code class="fe no np nq nr b"><strong class="kf ir">train_neural_network</strong></code>函数对给定的一批数据运行优化任务。它将在一个循环中使用，并在以后的多个时期和批次中使用。简单来说，<code class="fe no np nq nr b"><strong class="kf ir">session.run</strong></code>照顾工作。它将第一个参数作为要运行的内容，将第二个参数作为数据列表提供给网络，以便从第一个参数中检索结果。如前所述，您希望通过运行optimizer来最小化成本，因此这必须是第一个参数。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="ak">Code 11. single optimization task</strong></figcaption></figure><h2 id="30e1" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">显示统计数据</h2><p id="6a6f" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated"><code class="fe no np nq nr b"><strong class="kf ir">print_stats</strong></code>显示当前训练步骤的成本和准确性。与<code class="fe no np nq nr b"><strong class="kf ir">train_neural_network</strong></code>功能类似的过程也适用于此。不是将优化器交付给<code class="fe no np nq nr b"><strong class="kf ir">session.run</strong></code>函数，而是给出成本和精度。请注意<code class="fe no np nq nr b"><strong class="kf ir">keep_prob</strong></code>被设置为1。辍学率必须适用于培训阶段，否则必须根据论文设置为1。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="ak">Code 12. showing stats</strong></figcaption></figure><h2 id="446d" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">分批训练模型</h2><p id="d1b9" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko ns kq kr ks nt ku kv kw nu ky kz la ij bi translated"><code class="fe no np nq nr b"><strong class="kf ir">train_neural_network</strong></code>函数在给定批次上运行优化任务。因为CIFAR-10数据集有5个单独的批次，并且每个批次包含不同的图像数据，所以应该对每个批次运行train_neural_network。这可以用简单的代码来完成，如代码13所示。代码13为每个批次运行10个时期的训练，图10显示了训练结果。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="ak">Code 13. train over batches</strong></figcaption></figure><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/1e07b0b96f40d0a13bd5a8ed3d9999e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*hK1Xk9aX5TiEko46hL5vDg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd nn">Fig 10. loss and accuracy</strong></figcaption></figure><h1 id="60be" class="mr lj iq bd lk ms ol mu ln mv om mx lq my on na lt nb oo nd lw ne op ng lz nh bi translated">测试模型</h1><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi or"><img src="../Images/fed01581adfadd729718088c62a5c354.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*AW0QoQqQb1SjEHLci-AY9w.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd nn">Fig 11. prediction result</strong></figcaption></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="b9f0" class="mr lj iq bd lk ms mt mu ln mv mw mx lq my mz na lt nb nc nd lw ne nf ng lz nh bi translated">参考</h1><ul class=""><li id="88f1" class="mb mc iq kf b kg md kk me ko mf ks mg kw mh la mi mj mk ml bi translated">CIFAR-10/CIFAR-100数据</li><li id="b343" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://docs.python.org/3/library/pickle.html" rel="noopener ugc nofollow" target="_blank">蟒蛇泡菜</a></li><li id="e665" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html" rel="noopener ugc nofollow" target="_blank">数字整形</a></li><li id="e871" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html" rel="noopener ugc nofollow" target="_blank"> numpy转置</a></li><li id="69fe" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://stackoverflow.com/questions/32034237/how-does-numpys-transpose-method-permute-the-axes-of-an-array" rel="noopener ugc nofollow" target="_blank"> numpy转置轴说明列表</a></li><li id="1bf2" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" rel="noopener ugc nofollow" target="_blank">张量流conv2d </a></li><li id="570c" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://en.wikipedia.org/wiki/Row-_and_column-major_order" rel="noopener ugc nofollow" target="_blank">行主要订单解释</a></li><li id="4e4c" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.quora.com/What-is-the-meaning-of-min-max-normalization" rel="noopener ugc nofollow" target="_blank">最小-最大归一化</a></li><li id="3fcb" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.youtube.com/watch?v=FDCfw-YqWTE" rel="noopener ugc nofollow" target="_blank">观看“为什么要标准化输入”/ deeplearning.ai —吴恩达。</a></li><li id="6317" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.youtube.com/watch?v=qhXZsFVxGKo" rel="noopener ugc nofollow" target="_blank">爆炸，虚化梯度下降/深度学习. ai——吴恩达。</a></li><li id="0078" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science" rel="noopener ugc nofollow" target="_blank">一个热编码</a></li><li id="d3de" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/extend/architecture" rel="noopener ugc nofollow" target="_blank">张量流架构</a></li><li id="251b" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/versions/r1.3/programmers_guide/graphs" rel="noopener ugc nofollow" target="_blank">张量流图和时段</a></li><li id="6ccf" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/DType" rel="noopener ugc nofollow" target="_blank">张量流数据类型</a></li><li id="22ea" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/placeholder" rel="noopener ugc nofollow" target="_blank"> Tensorflow占位符</a></li><li id="4d79" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/Variable" rel="noopener ugc nofollow" target="_blank">张量流变量</a></li><li id="5c55" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">【tf.nn下的Tensorflow Conv2D】</li><li id="8ad0" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">【tf.nn下的tensor flow ReLU</li><li id="1e8a" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">【tf.nn下的tensor flow Max Pooling</li><li id="288b" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">【tf.nn下的张量流丢失</li><li id="9c48" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">【tf.layers下的张量流批量归一化</li><li id="c4b5" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten" rel="noopener ugc nofollow" target="_blank">张量流在tf.contrib下展平</a></li><li id="094c" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected" rel="noopener ugc nofollow" target="_blank">张量流在tf.contrib下全连接</a></li><li id="a74c" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">批量归一化(原纸)</a></li><li id="fe6d" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.youtube.com/watch?v=NbGUU6ZYtus" rel="noopener ugc nofollow" target="_blank">批量定额为什么行得通？/ deeplearning.ai —吴恩达。</a></li><li id="0e38" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" rel="noopener ugc nofollow" target="_blank">退学(原论文)</a></li><li id="737a" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.youtube.com/watch?v=ARq74QuavAo" rel="noopener ugc nofollow" target="_blank">了解辍学/deep learning . ai——吴恩达。</a></li><li id="41bb" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5" rel="noopener">(深度)机器学习中的辍学</a></li><li id="d094" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.quora.com/What-is-the-meaning-of-flattening-step-in-a-convolutional-neural-network" rel="noopener ugc nofollow" target="_blank">卷积神经网络中的展平步骤是什么意思？</a></li><li id="ad40" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="http://cs231n.github.io/convolutional-networks/" rel="noopener ugc nofollow" target="_blank">卷积神经网络(CNN/conv nets)—cs 231n</a></li><li id="6d42" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" rel="noopener ugc nofollow" target="_blank">可视化和理解卷积网络</a></li><li id="a34b" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://github.com/ducha-aiki/caffenet-benchmark" rel="noopener ugc nofollow" target="_blank">评估CNN design choices在ImageNet-2012上的表现</a></li><li id="9bfb" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2" rel="noopener ugc nofollow" target="_blank"> Tensorflow Softmax与Logits的交叉熵</a></li><li id="3027" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/reduce_mean" rel="noopener ugc nofollow" target="_blank">张量流减少平均</a></li><li id="9e3a" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_guides/python/train" rel="noopener ugc nofollow" target="_blank">张量流优化器</a></li><li id="8ff4" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/equal" rel="noopener ugc nofollow" target="_blank">张量流等于</a></li><li id="4cfb" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/cast" rel="noopener ugc nofollow" target="_blank">张量流投射</a></li><li id="2dd5" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="http://ruder.io/optimizing-gradient-descent/" rel="noopener ugc nofollow" target="_blank">梯度下降优化算法概述</a></li><li id="2d14" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="http://www.deeplearningbook.org/contents/optimization.html" rel="noopener ugc nofollow" target="_blank">深度模型训练优化</a></li><li id="3d80" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/Session#run" rel="noopener ugc nofollow" target="_blank"> Tensorflow会话运行功能</a></li><li id="e9bf" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/reduce_mean" rel="noopener ugc nofollow" target="_blank">张量流tf.reduce_mean </a></li><li id="9934" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer" rel="noopener ugc nofollow" target="_blank">tensor flow TF . train . adamotimizer</a></li><li id="70bd" class="mb mc iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated"><a class="ae kc" href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130" rel="noopener ugc nofollow" target="_blank">分类数据集结果—远高于70% </a></li></ul></div></div>    
</body>
</html>