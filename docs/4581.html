<html>
<head>
<title>Natural Language Processing: Count Vectorization with scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理:用 scikit-learn 计算矢量化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e?source=collection_archive---------1-----------------------#2018-08-24">https://towardsdatascience.com/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e?source=collection_archive---------1-----------------------#2018-08-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a6da" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">这是一个关于如何使用 scikit-learn 对真实文本数据进行计数矢量化的演示。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5a1a1ab2d0f281d53e5b62b2bd1d980c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UOjWvDziH86T2MmiDpp98Q.png"/></div></div></figure></div><div class="ab cl kr ks hu kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ij ik il im in"><h1 id="ee65" class="ky kz iq bd la lb lc ld le lf lg lh li jw lj jx lk jz ll ka lm kc ln kd lo lp bi translated">计数矢量化(也称为一键编码)</h1><p id="1adc" class="pw-post-body-paragraph lq lr iq ls b lt lu jr lv lw lx ju ly lz ma mb mc md me mf mg mh mi mj mk ml ij bi translated">如果你还没有，看看我之前关于单词嵌入的博文:<a class="ae mm" rel="noopener" target="_blank" href="/introduction-to-word-embeddings-4cf857b12edc">单词嵌入介绍</a></p><p id="49c2" class="pw-post-body-paragraph lq lr iq ls b lt mn jr lv lw mo ju ly lz mp mb mc md mq mf mg mh mr mj mk ml ij bi translated">在那篇博客文章中，我们讨论了很多不同的方法来表示机器学习中使用的单词。这是一个高层次的概述，我们将在这里展开，并检查我们如何在一些真实的文本数据上实际使用计数矢量化。</p><h1 id="e95b" class="ky kz iq bd la lb ms ld le lf mt lh li jw mu jx lk jz mv ka lm kc mw kd lo lp bi translated">计数向量化概述</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl mx"><img src="../Images/289df55030a7c20b6981d8e9997eefc2.png" data-original-src="https://miro.medium.com/v2/format:webp/1*YEJf9BQQh0ma1ECs6x_7yQ.png"/></div></figure><p id="5970" class="pw-post-body-paragraph lq lr iq ls b lt mn jr lv lw mo ju ly lz mp mb mc md mq mf mg mh mr mj mk ml ij bi translated">今天，我们将探讨用数字表示文本数据的最基本方法之一:一键编码(或计数矢量化)。想法很简单。</p><p id="85b4" class="pw-post-body-paragraph lq lr iq ls b lt mn jr lv lw mo ju ly lz mp mb mc md mq mf mg mh mr mj mk ml ij bi translated">我们将创建向量，其维数等于我们的词汇表的大小，如果文本数据以该词汇为特征，我们将在该维数上放置一个 1。每当我们再次遇到这个词，我们将增加计数，留下 0 到处我们没有找到这个词甚至一次。</p><p id="fe2b" class="pw-post-body-paragraph lq lr iq ls b lt mn jr lv lw mo ju ly lz mp mb mc md mq mf mg mh mr mj mk ml ij bi translated">这样做的结果将是非常大的向量，但是，如果我们在真实的文本数据上使用它们，我们将得到我们的文本数据的单词内容的非常精确的计数。不幸的是，这不能提供任何语义或关系信息，但这没关系，因为这不是使用这种技术的目的。</p><p id="0c13" class="pw-post-body-paragraph lq lr iq ls b lt mn jr lv lw mo ju ly lz mp mb mc md mq mf mg mh mr mj mk ml ij bi translated">今天，我们将使用来自<a class="ae mm" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>的软件包。</p><h1 id="e185" class="ky kz iq bd la lb ms ld le lf mt lh li jw mu jx lk jz mv ka lm kc mw kd lo lp bi translated">一个基本例子</h1><p id="65d6" class="pw-post-body-paragraph lq lr iq ls b lt lu jr lv lw lx ju ly lz ma mb mc md me mf mg mh mi mj mk ml ij bi translated">以下是使用计数矢量化获取矢量的基本示例:</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="55a6" class="nd kz iq mz b gy ne nf l ng nh">from sklearn.feature_extraction.text import CountVectorizer<br/><br/><br/># To create a Count Vectorizer, we simply need to instantiate one.<br/># There are special parameters we can set here when making the vectorizer, but<br/># for the most basic example, it is not needed.<br/>vectorizer = CountVectorizer()<br/><br/># For our text, we are going to take some text from our previous blog post<br/># about count vectorization<br/>sample_text = ["One of the most basic ways we can numerically represent words "<br/>               "is through the one-hot encoding method (also sometimes called "<br/>               "count vectorizing)."]<br/><br/># To actually create the vectorizer, we simply need to call fit on the text<br/># data that we wish to fix<br/>vectorizer.fit(sample_text)<br/><br/># Now, we can inspect how our vectorizer vectorized the text<br/># This will print out a list of words used, and their index in the vectors<br/>print('Vocabulary: ')<br/>print(vectorizer.vocabulary_)<br/><br/># If we would like to actually create a vector, we can do so by passing the<br/># text into the vectorizer to get back counts<br/>vector = vectorizer.transform(sample_text)<br/><br/># Our final vector:<br/>print('Full vector: ')<br/>print(vector.toarray())<br/><br/># Or if we wanted to get the vector for one word:<br/>print('Hot vector: ')<br/>print(vectorizer.transform(['hot']).toarray())<br/><br/># Or if we wanted to get multiple vectors at once to build matrices<br/>print('Hot and one: ')<br/>print(vectorizer.transform(['hot', 'one']).toarray())<br/><br/># We could also do the whole thing at once with the fit_transform method:<br/>print('One swoop:')<br/>new_text = ['Today is the day that I do the thing today, today']<br/>new_vectorizer = CountVectorizer()<br/>print(new_vectorizer.fit_transform(new_text).toarray())</span></pre><p id="cf56" class="pw-post-body-paragraph lq lr iq ls b lt mn jr lv lw mo ju ly lz mp mb mc md mq mf mg mh mr mj mk ml ij bi translated">我们的产出:</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="09cb" class="nd kz iq mz b gy ne nf l ng nh">Vocabulary:<br/>{'one': 12, 'of': 11, 'the': 15, 'most': 9, 'basic': 1, 'ways': 18, 'we': 19,<br/>  'can': 3, 'numerically': 10, 'represent': 13, 'words': 20, 'is': 7,<br/>  'through': 16, 'hot': 6, 'encoding': 5, 'method': 8, 'also': 0,<br/>  'sometimes': 14, 'called': 2, 'count': 4, 'vectorizing': 17}<br/>Full vector:<br/>[[1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1]]<br/>Hot vector:<br/>[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]<br/>Hot and one:<br/>[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]<br/> [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]]<br/>One swoop:<br/>[[1 1 1 1 2 1 3]]</span></pre><h1 id="3ff0" class="ky kz iq bd la lb ms ld le lf mt lh li jw mu jx lk jz mv ka lm kc mw kd lo lp bi translated">在真实数据上使用它:</h1><p id="a07b" class="pw-post-body-paragraph lq lr iq ls b lt lu jr lv lw lx ju ly lz ma mb mc md me mf mg mh mi mj mk ml ij bi translated">所以还是用在一些真实数据上吧！我们将查看 scikit-learn 附带的<a class="ae mm" href="http://scikit-learn.org/stable/datasets/twenty_newsgroups.html" rel="noopener ugc nofollow" target="_blank"> 20 个新闻组数据集</a>。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="3cd8" class="nd kz iq mz b gy ne nf l ng nh">from sklearn.datasets import fetch_20newsgroups<br/>from sklearn.feature_extraction.text import CountVectorizer<br/><br/>import numpy as np<br/><br/># Create our vectorizer<br/>vectorizer = CountVectorizer()<br/><br/># Let's fetch all the possible text data<br/>newsgroups_data = fetch_20newsgroups()<br/><br/># Why not inspect a sample of the text data?<br/>print('Sample 0: ')<br/>print(newsgroups_data.data[0])<br/>print()<br/><br/># Create the vectorizer<br/>vectorizer.fit(newsgroups_data.data)<br/><br/># Let's look at the vocabulary:<br/>print('Vocabulary: ')<br/>print(vectorizer.vocabulary_)<br/>print()<br/><br/># Converting our first sample into a vector<br/>v0 = vectorizer.transform([newsgroups_data.data[0]]).toarray()[0]<br/>print('Sample 0 (vectorized): ')<br/>print(v0)<br/>print()<br/><br/># It's too big to even see...<br/># What's the length?<br/>print('Sample 0 (vectorized) length: ')<br/>print(len(v0))<br/>print()<br/><br/># How many words does it have?<br/>print('Sample 0 (vectorized) sum: ')<br/>print(np.sum(v0))<br/>print()<br/><br/># What if we wanted to go back to the source?<br/>print('To the source:')<br/>print(vectorizer.inverse_transform(v0))<br/>print()<br/><br/># So all this data has a lot of extra garbage... Why not strip it away?<br/>newsgroups_data = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))<br/><br/># Why not inspect a sample of the text data?<br/>print('Sample 0: ')<br/>print(newsgroups_data.data[0])<br/>print()<br/><br/># Create the vectorizer<br/>vectorizer.fit(newsgroups_data.data)<br/><br/># Let's look at the vocabulary:<br/>print('Vocabulary: ')<br/>print(vectorizer.vocabulary_)<br/>print()<br/><br/># Converting our first sample into a vector<br/>v0 = vectorizer.transform([newsgroups_data.data[0]]).toarray()[0]<br/>print('Sample 0 (vectorized): ')<br/>print(v0)<br/>print()<br/><br/># It's too big to even see...<br/># What's the length?<br/>print('Sample 0 (vectorized) length: ')<br/>print(len(v0))<br/>print()<br/><br/># How many words does it have?<br/>print('Sample 0 (vectorized) sum: ')<br/>print(np.sum(v0))<br/>print()<br/><br/># What if we wanted to go back to the source?<br/>print('To the source:')<br/>print(vectorizer.inverse_transform(v0))<br/>print()</span></pre><p id="e079" class="pw-post-body-paragraph lq lr iq ls b lt mn jr lv lw mo ju ly lz mp mb mc md mq mf mg mh mr mj mk ml ij bi translated">我们的产出:</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="eb92" class="nd kz iq mz b gy ne nf l ng nh">Sample 0:<br/>From: lerxst@wam.umd.edu (where's my thing)<br/>Subject: WHAT car is this!?<br/>Nntp-Posting-Host: rac3.wam.umd.edu<br/>Organization: University of Maryland, College Park<br/>Lines: 15<br/><br/> I was wondering if anyone out there could enlighten me on this car I saw<br/>the other day. It was a 2-door sports car, looked to be from the late 60s/<br/>early 70s. It was called a Bricklin. The doors were really small. In addition,<br/>the front bumper was separate from the rest of the body. This is<br/>all I know. If anyone can tellme a model name, engine specs, years<br/>of production, where this car is made, history, or whatever info you<br/>have on this funky looking car, please e-mail.<br/><br/>Thanks,<br/>- IL<br/>   ---- brought to you by your neighborhood Lerxst ----<br/><br/><br/><br/><br/><br/><br/>Vocabulary:<br/>{'from': 56979, 'lerxst': 75358, 'wam': 123162, 'umd': 118280, 'edu': 50527,<br/>  'where': 124031, 'my': 85354, 'thing': 114688, 'subject': 111322,<br/>  'what': 123984, 'car': 37780, 'is': 68532, 'this': 114731, 'nntp': 87620,<br/>  'posting': 95162, 'host': 64095, 'rac3': 98949, 'organization': 90379,<br/>  'university': 118983, 'of': 89362, 'maryland': 79666,<br/>  'college': 40998, ... } (Abbreviated...)<br/><br/>Sample 0 (vectorized):<br/>[0 0 0 ... 0 0 0]<br/><br/>Sample 0 (vectorized) length:<br/>130107<br/><br/>Sample 0 (vectorized) sum:<br/>122<br/><br/>To the source:<br/>[array(['15', '60s', '70s', 'addition', 'all', 'anyone', 'be', 'body',<br/>       'bricklin', 'brought', 'bumper', 'by', 'called', 'can', 'car',<br/>       'college', 'could', 'day', 'door', 'doors', 'early', 'edu',<br/>       'engine', 'enlighten', 'from', 'front', 'funky', 'have', 'history',<br/>       'host', 'if', 'il', 'in', 'info', 'is', 'it', 'know', 'late',<br/>       'lerxst', 'lines', 'looked', 'looking', 'made', 'mail', 'maryland',<br/>       'me', 'model', 'my', 'name', 'neighborhood', 'nntp', 'of', 'on',<br/>       'or', 'organization', 'other', 'out', 'park', 'please', 'posting',<br/>       'production', 'rac3', 'really', 'rest', 'saw', 'separate', 'small',<br/>       'specs', 'sports', 'subject', 'tellme', 'thanks', 'the', 'there',<br/>       'thing', 'this', 'to', 'umd', 'university', 'wam', 'was', 'were',<br/>       'what', 'whatever', 'where', 'wondering', 'years', 'you', 'your'],<br/>      dtype='&lt;U180')]<br/><br/>Sample 0:<br/>I was wondering if anyone out there could enlighten me on this car I saw<br/>the other day. It was a 2-door sports car, looked to be from the late 60s/<br/>early 70s. It was called a Bricklin. The doors were really small. In addition,<br/>the front bumper was separate from the rest of the body. This is<br/>all I know. If anyone can tellme a model name, engine specs, years<br/>of production, where this car is made, history, or whatever info you<br/>have on this funky looking car, please e-mail.<br/><br/>Vocabulary:<br/>{'was': 95844, 'wondering': 97181, 'if': 48754, 'anyone': 18915, 'out': 68847,<br/>  'there': 88638, 'could': 30074, 'enlighten': 37335, 'me': 60560, 'on': 68080,<br/>  'this': 88767, 'car': 25775, 'saw': 80623, 'the': 88532, 'other': 68781,<br/>  'day': 31990, 'it': 51326, 'door': 34809, 'sports': 84538, 'looked': 57390,<br/>  'to': 89360, 'be': 21987, 'from': 41715, 'late': 55746, '60s': 9843,<br/>  'early': 35974, '70s': 11174, 'called': 25492, 'bricklin': 24160, 'doors': 34810,<br/>  'were': 96247, 'really': 76471, ... } (Abbreviated...)<br/><br/>Sample 0 (vectorized):<br/>[0 0 0 ... 0 0 0]<br/><br/>Sample 0 (vectorized) length:<br/>101631<br/><br/>Sample 0 (vectorized) sum:<br/>85<br/><br/>To the source:<br/>[array(['60s', '70s', 'addition', 'all', 'anyone', 'be', 'body',<br/>       'bricklin', 'bumper', 'called', 'can', 'car', 'could', 'day',<br/>       'door', 'doors', 'early', 'engine', 'enlighten', 'from', 'front',<br/>       'funky', 'have', 'history', 'if', 'in', 'info', 'is', 'it', 'know',<br/>       'late', 'looked', 'looking', 'made', 'mail', 'me', 'model', 'name',<br/>       'of', 'on', 'or', 'other', 'out', 'please', 'production', 'really',<br/>       'rest', 'saw', 'separate', 'small', 'specs', 'sports', 'tellme',<br/>       'the', 'there', 'this', 'to', 'was', 'were', 'whatever', 'where',<br/>       'wondering', 'years', 'you'], dtype='&lt;U81')]</span></pre><h1 id="4a59" class="ky kz iq bd la lb ms ld le lf mt lh li jw mu jx lk jz mv ka lm kc mw kd lo lp bi translated">现在怎么办？</h1><p id="08cc" class="pw-post-body-paragraph lq lr iq ls b lt lu jr lv lw lx ju ly lz ma mb mc md me mf mg mh mi mj mk ml ij bi translated">所以，你现在可能想知道什么？我们知道如何基于计数对这些东西进行矢量化，但是我们实际上能利用这些信息做什么呢？</p><p id="8ad7" class="pw-post-body-paragraph lq lr iq ls b lt mn jr lv lw mo ju ly lz mp mb mc md mq mf mg mh mr mj mk ml ij bi translated">首先，我们可以做一系列的分析。我们可以查看词频，我们可以删除停用词，我们可以可视化事物，我们可以尝试聚类。现在我们有了这些文本数据的数字表示，我们可以做很多以前做不到的事情！</p><p id="811b" class="pw-post-body-paragraph lq lr iq ls b lt mn jr lv lw mo ju ly lz mp mb mc md mq mf mg mh mr mj mk ml ij bi translated">但是让我们更具体一点。我们一直在使用来自 20 个新闻组数据集中的文本数据。为什么不把它用在任务上呢？</p><p id="c3d0" class="pw-post-body-paragraph lq lr iq ls b lt mn jr lv lw mo ju ly lz mp mb mc md mq mf mg mh mr mj mk ml ij bi translated">20 个新闻组数据集是一个论坛帖子的数据集，分为 20 个不同的类别。为什么不使用我们的矢量化工具来尝试对这些数据进行分类呢？</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="58fc" class="nd kz iq mz b gy ne nf l ng nh">from sklearn.datasets import fetch_20newsgroups<br/>from sklearn.feature_extraction.text import CountVectorizer<br/><br/>from sklearn.naive_bayes import MultinomialNB<br/>from sklearn import metrics<br/><br/># Create our vectorizer<br/>vectorizer = CountVectorizer()<br/><br/># All data<br/>newsgroups_train = fetch_20newsgroups(subset='train',<br/>                                      remove=('headers', 'footers', 'quotes'))<br/>newsgroups_test = fetch_20newsgroups(subset='test',<br/>                                     remove=('headers', 'footers', 'quotes'))<br/><br/># Get the training vectors<br/>vectors = vectorizer.fit_transform(newsgroups_train.data)<br/><br/># Build the classifier<br/>clf = MultinomialNB(alpha=.01)<br/><br/>#  Train the classifier<br/>clf.fit(vectors, newsgroups_train.target)<br/><br/># Get the test vectors<br/>vectors_test = vectorizer.transform(newsgroups_test.data)<br/><br/># Predict and score the vectors<br/>pred = clf.predict(vectors_test)<br/>acc_score = metrics.accuracy_score(newsgroups_test.target, pred)<br/>f1_score = metrics.f1_score(newsgroups_test.target, pred, average='macro')<br/><br/>print('Total accuracy classification score: {}'.format(acc_score))<br/>print('Total F1 classification score: {}'.format(f1_score))</span></pre><p id="8b68" class="pw-post-body-paragraph lq lr iq ls b lt mn jr lv lw mo ju ly lz mp mb mc md mq mf mg mh mr mj mk ml ij bi translated">我们的产出:</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="1335" class="nd kz iq mz b gy ne nf l ng nh">Total accuracy classification score: 0.6460435475305364<br/>Total F1 classification score: 0.6203806145034193</span></pre><p id="73ea" class="pw-post-body-paragraph lq lr iq ls b lt mn jr lv lw mo ju ly lz mp mb mc md mq mf mg mh mr mj mk ml ij bi translated">嗯……所以不是超级神奇，但我们只是使用计数向量！一个更丰富的表现将为我们的分数创造奇迹！</p><h1 id="5462" class="ky kz iq bd la lb ms ld le lf mt lh li jw mu jx lk jz mv ka lm kc mw kd lo lp bi translated">包扎</h1><p id="cf5d" class="pw-post-body-paragraph lq lr iq ls b lt lu jr lv lw lx ju ly lz ma mb mc md me mf mg mh mi mj mk ml ij bi translated">希望您感觉已经学到了很多关于计数矢量化的知识，如何使用它，以及它的一些潜在应用！</p><p id="1cd2" class="pw-post-body-paragraph lq lr iq ls b lt mn jr lv lw mo ju ly lz mp mb mc md mq mf mg mh mr mj mk ml ij bi translated">如果你喜欢读这篇文章，给我留言或者给我的 GoFundMe 捐款来帮助我继续我的 ML 研究！</p><p id="edc0" class="pw-post-body-paragraph lq lr iq ls b lt mn jr lv lw mo ju ly lz mp mb mc md mq mf mg mh mr mj mk ml ij bi translated">敬请关注即将推出的更多单词嵌入内容！</p></div></div>    
</body>
</html>