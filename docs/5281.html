<html>
<head>
<title>On Superintelligence of Machines: Trepidation and Therapy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器的超智能:恐惧与治疗</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/on-superintelligence-of-machines-trepidation-and-therapy-61486e8d3dad?source=collection_archive---------12-----------------------#2018-10-08">https://towardsdatascience.com/on-superintelligence-of-machines-trepidation-and-therapy-61486e8d3dad?source=collection_archive---------12-----------------------#2018-10-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="ce18" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是我为一门名为“人工智能:法律、伦理和政策”的课程写的一篇文章。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/c82f3189ab7042b2ae24a5b5d25ee618.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JpSmkJ8Yj-vtM1uUJwKypw.jpeg"/></div></div></figure><p id="d474" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尼克·博斯特罗姆在他的书中将超智能定义为“在几乎所有感兴趣的领域大大超过人类认知表现的任何智力。”超智能人工智能(SAI)的概念让我们焦虑，我们担心人工智能会失控，威胁人类。这个被认为是“<strong class="jp ir">奇点</strong>”的事件引发了存在主义辩论、阴谋论，并引发了伦理问题。但是这种奇点是可以实现的吗？如果是这样，我们该怎么办？</p><h1 id="8469" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">我们会实现奇点吗？</h1><p id="0b86" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">未来研究所(Institute for the Future)的创始人罗伊·阿马拉(Roy Amara)创造了一句现在被称为“阿马拉定律”的格言:“我们往往会高估一项技术在短期内的效果，而低估其长期效果。”我们经常会读到这样的标题:“无人驾驶汽车将在未来 10 年出现。”自 1970 年以来，同样的标题一直在<a class="ae mb" href="https://orfe.princeton.edu/~alaink/SmartDrivingCars/PDFs/Nov2013MORGAN-STANLEY-BLUE-PAPER-AUTONOMOUS-CARS%EF%BC%9A-SELF-DRIVING-THE-NEW-AUTO-INDUSTRY-PARADIGM.pdf" rel="noopener ugc nofollow" target="_blank">出现。跑道 AI 也是如此。看到我们生活中许多其他方面的进步，我们开始认为对人工智能来说也是如此。</a></p><p id="5280" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">AI 的进步速度是不对称的。人工智能可能擅长一些任务，而人类在许多其他方面更胜一筹。我们错误地将一个分支领域的进步与总体进步并列。那些指出深度学习一夜成名的人忘记了，它花了 50 年才达到今天的水平。这些人还忽略了间断平衡的基本理论，该理论认为稳定的系统倾向于停留在静止状态。技术方面的一个例子是飞机，自从 60 年前出现喷气式飞机以来，飞机一直没有重大发展。</p><p id="07eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，没有一条通向普遍智能的清晰道路，也没有任何提议的方法将所有已知的人工智能进展归结为一个单一的 SAI。罗德尼·布鲁克斯可能是世界上最有成就的机器人专家，他说我们对人工智能进步甚至接管世界的速度不合理地乐观。在<a class="ae mb" href="https://www.technologyreview.com/s/609048/the-seven-deadly-sins-of-ai-predictions/" rel="noopener ugc nofollow" target="_blank">人工智能预测的七宗罪中，</a>布鲁克斯讨论了预测人工智能增长的陷阱，并向我们保证，即使机器接管，我们至少在几百年内是安全的。</p><h1 id="f2ac" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">有什么大不了的？</h1><p id="8369" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">博斯特罗姆担心"<strong class="jp ir">协调问题</strong>"<strong class="jp ir">:</strong>SAI 的目标与我们的目标不一致。他假设一个矛盾的“<strong class="jp ir">回形针最大化器</strong>”，它在寻求创造回形针的过程中，将利用宇宙的所有能量，在这个过程中消灭人类。正如 2010 年一项名为“<strong class="jp ir"> Roko 的蛇怪</strong>”的“少错”实验所描述的那样，一些人害怕更多的报复结果根据 Roko 的说法，SAI 可能会合理化，然后追溯性地惩罚所有对其起源没有帮助的人类。著名的麻省理工学院宇宙学家 Max Tegmark 想象了 12 个人工智能的后果场景，从人类和 SAI 生物共存的自由主义乌托邦，到动物园管理员把人类作为动物园动物关在笼子里，就像库尔特·冯内古特的 5 号屠宰场一样。然而，正如文档杂志 2018 年 4 月 9 日的一篇文章<a class="ae mb" href="http://www.documentjournal.com/2018/04/the-existential-paranoia-fueling-elon-musks-fear-of-ai/" rel="noopener ugc nofollow" target="_blank">所言，“害怕人工智能叛乱的特权人士总是以反映他们自己意识形态的剥削性术语来想象它。”值得注意的是，这些未来学家和末日论者大多是投机者，而不是人工智能研究人员，例如，博斯特罗姆本人、埃隆·马斯克和</a><a class="ae mb" href="https://futureoflife.org/2015/10/12/elon-musk-donates-10m-to-keep-ai-beneficial/" rel="noopener ugc nofollow" target="_blank">斯蒂芬·霍金</a>。</p><h1 id="e823" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">如果 AI 变得超智能，我们能做什么？</h1><p id="9b5d" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">没什么！</p><p id="8c65" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">既然人工智能已经超越了人类的认知能力，那么在生存国际象棋中，我们肯定无法战胜它。博斯特罗姆认为，我们必须机智地协商我们的生存条件。有些人甚至愿意崇拜人工智能霸主，并已经在硅谷建立了人工智能教堂。他们相信这个人工智能将会是真正“倾听”的干涉主义之神(尼克·凯夫<a class="ae mb" href="https://www.youtube.com/watch?v=gxAOL_w2Ujo" rel="noopener ugc nofollow" target="_blank">唱</a>的那个)。“宠物比家畜好”，他们打趣道。因此，创建支票、安装自动防故障装置和切断开关，可能会让我们免于度过一个存在危机的不眠之夜，但在 SAI 的黎明，我们将受到冷漠的上帝的摆布</p><h1 id="572d" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">我们现在能做什么？</h1><p id="6e55" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">AI 还在青春期前。我们仍然握着插头。为什么不屈服于博斯特罗姆的恐惧，为了更大的利益而把它拔掉呢？当然，埃隆·马斯克<a class="ae mb" href="https://futureoflife.org/2015/10/12/elon-musk-donates-10m-to-keep-ai-beneficial/" rel="noopener ugc nofollow" target="_blank">捐赠了</a>数千万美元来减缓恶意人工智能的增长，但他也将花费数十亿美元让汽车变得更智能。用《智人》的历史学家和作家尤瓦尔·诺亚·哈拉里的<a class="ae mb" href="https://www.theguardian.com/books/2017/sep/22/life-30-max-tegmark-review" rel="noopener ugc nofollow" target="_blank">话说，“我们可能会基于短视的短期考虑做出最深刻的决定。地球上生命的未来将由散布恐怖主义威胁恐惧的三流政客、担心季度收入的股东以及试图最大化客户体验的营销专家决定。”人工智能是如此有利可图的工具，以至于<em class="kl">的经济人</em>不会放过它。出于好的和坏的目的，将会有持续的研究。如果我们现在强加规则，肯定会有一个支持人工智能的游说团体，他们会用任何可用的法律武器激烈地对抗博斯特罗姆人。如果规定过于极端，研究可能会转入地下，甚至被浪漫化为反权威运动。此外，我们失去了潜在的好处，如改善医疗保健。</a></p><p id="cefe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">人工智能还在许多方面造福了我们的生活，并且有潜力做得更多。它已经在社会结构中根深蒂固，很难实施极端的监管。合乎逻辑的做法是 Grady Booch，一位多产的软件工程师和 UML 的开发者<a class="ae mb" href="https://ieet.org/index.php/IEET2/more/Booch20170225" rel="noopener ugc nofollow" target="_blank">所说的</a>:给人工智能注入人类的价值观，这样我们就可以学会共存。超人类主义的支持者看到了一个未来，在这个未来中，人工智能和人类将不再是分离的存在，而是随着生物技术的进步，他们将成为一体。我们不知道人工智能的未来会是什么样子，我们也无法猜测它的意图，所以最好不要陷入危言耸听，认为 SAI 是邪恶的。等到人工智能变得超级聪明的时候，无论是好是坏，世界都已经发生了根本性的变化。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="3374" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我非常感谢 Maura Grossman 博士和 Alex Williams 启发我进行批判性思考并提供详细的反馈。</p></div></div>    
</body>
</html>