<html>
<head>
<title>Deep learning and machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习和机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-and-machine-learning-c1101debe0c?source=collection_archive---------3-----------------------#2017-08-15">https://towardsdatascience.com/deep-learning-and-machine-learning-c1101debe0c?source=collection_archive---------3-----------------------#2017-08-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="aa01" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2015年7月14日</p><p id="629e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇博客文章是关于我目前正在从事的一项正式工作的非正式讨论——我的博士背景文献综述。因此，它的链接非常丰富——我在这里记录的旅程本质上是由其他人的工作组成的。这篇文章也可能被解释为试图说服我的上司 <a class="ae km" href="http://www.cs.cf.ac.uk/contactsandpeople/staffpage.php?emailname=o.f.rana" rel="noopener ugc nofollow" target="_blank"> <em class="kl">奥马尔·拉纳教授</em> </a> <em class="kl">和</em> <a class="ae km" href="https://www.maynoothuniversity.ie/faculty-science-engineering/our-people/ronan-reilly" rel="noopener ugc nofollow" target="_blank"> <em class="kl">罗南·雷利教授</em> </a> <em class="kl">我实际上是在做事情，而不是阅读论文和调试</em><a class="ae km" href="https://github.com/torch/torch7/wiki/Cheatsheet" rel="noopener ugc nofollow" target="_blank"><em class="kl">torch 7</em></a><em class="kl">/</em><a class="ae km" href="http://deeplearning.net/software/theano/" rel="noopener ugc nofollow" target="_blank"><em class="kl">Theano</em></a><em class="kl">代码。但事实并非如此:)</em></p><h1 id="68c3" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">机器学习-背景</h1><p id="d88c" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">机器学习领域目前非常活跃。一周接一周，一个月接一个月，以前的记录在<a class="ae km" href="http://www.technologyreview.com/news/537436/baidus-artificial-intelligence-supercomputer-beats-google-at-image-recognition/" rel="noopener ugc nofollow" target="_blank">翻滚</a>然后又翻滚(尽管<a class="ae km" href="http://www.nytimes.com/2015/06/04/technology/computer-scientists-are-astir-after-baidu-team-is-barred-from-ai-competition.html?_r=1" rel="noopener ugc nofollow" target="_blank">规则在一路上被打破</a>)。库使用Nvidia的<a class="ae km" href="http://www.nvidia.co.uk/object/cuda-parallel-computing-uk.html" rel="noopener ugc nofollow" target="_blank"> CUDA </a>实现针对GPU硬件优化的算法，研究人员可以从Nvidia 购买<a class="ae km" href="http://maxwell.nvidia.com/titan-x" rel="noopener ugc nofollow" target="_blank">硬件，或者在云中租赁机器</a><a class="ae km" href="https://aws.amazon.com/blogs/aws/new-g2-instance-type-with-4x-more-gpu-power/" rel="noopener ugc nofollow" target="_blank">，每次一小时</a>。家庭/小公司的程序员现在可以在非常大的数据集上训练非常大的模型，这在3-4年前只有大得多的公司才能考虑做<a class="ae km" href="http://www.cs.stanford.edu/~acoates/papers/CoatesHuvalWangWuNgCatanzaro_icml2013.pdf" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="c1f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么这很重要？嗯，机器学习最简单的定义是<em class="kl">软件，当针对特定任务</em>进行测量时，它会随着时间的推移提高自己的性能。这句话描述了一种与我们目前所习惯的软件非常不同的软件——操作系统、浏览器和大多数应用程序不会根据“经验”有意义地修改它们的行为——它们一次又一次地重复做完全相同的事情。拥有可训练的——可以学习的——软件是一种非常有价值的能力，我们将在后面看到。</p><p id="c148" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我看来，机器学习代表了我们这个时代的重大挑战之一，与理解现实的<a class="ae km" href="http://en.wikipedia.org/wiki/The_Fabric_of_Reality" rel="noopener ugc nofollow" target="_blank">结构</a>或什么是<a class="ae km" href="http://www.newscientist.com/article/dn25471-spark-of-life-metabolism-appears-in-lab-without-cells.html#.VW9NHlxVhBd" rel="noopener ugc nofollow" target="_blank">生命本身</a>不相上下。</p><h1 id="1a5b" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">机器学习！=深度学习(只不过现在有点)</h1><p id="4a08" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">机器学习不是深度学习(它是深度学习的超集)，但最近(尤其是在主流媒体上)，这两个术语已经成为同义词。深度学习是机器学习的一个<em class="kl">分支</em>，它源于<a class="ae km" href="http://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>——在我看来是所有人工智能分支中最有趣的，因为它具有生物合理性(映射到人类大脑中类似的功能或特征)以及它为研究人员提供的路线图/思想宝库。自20世纪60年代以来，人工神经网络一直非常受欢迎，也非常不受欢迎，因为第一次炒作超过了现实，然后从业者又赶上了(明斯基和帕普特在1969年的书<a class="ae km" href="http://en.wikipedia.org/wiki/Perceptrons_%28book%29" rel="noopener ugc nofollow" target="_blank">感知器</a>中对人工神经网络的贬低至今仍在该领域回响，尽管它遭到了反驳)。</p><p id="e2ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里引用明斯基是恰当的——直到最近谈论在<a class="ae km" href="http://www.swi-prolog.org/" rel="noopener ugc nofollow" target="_blank"> Prolog </a>和<a class="ae km" href="https://common-lisp.net/" rel="noopener ugc nofollow" target="_blank"> Lisp </a>中实现的符号逻辑和神经网络一样可以接受(甚至更可以接受)。但是像神经网络一样，Prolog和LISP已经过时了。然而，从软件工程的角度来看，它们是诱人的。例如，Prolog处理如下易于理解的事实和规则:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="23fb" class="lz ko iq lv b gy ma mb l mc md">likes(joe, fish).<br/>likes(joe, mary).<br/>likes(mary, book).<br/>likes(john, book).</span></pre><p id="5cdb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们带着问题“咨询”系统时，Prolog使用归结反驳来回答，于是我们得到:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="af10" class="lz ko iq lv b gy ma mb l mc md">?- likes(mary,fish).<br/>no<br/><em class="kl">"no" means nothing matches the question, no does not mean false.</em></span></pre><p id="5f57" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是Prolog很慢，规则/事实数据库变得难以管理。Prolog / Lisp在日本遭遇了自己的人工智能冬天，因为非专业硬件超过了优雅的专用硬件(它们的表现基本上符合摩尔定律)。最近，<a class="ae km" href="http://arxiv.org/pdf/1502.05698v5.pdf" rel="noopener ugc nofollow" target="_blank">谷歌的研究人员建立了</a>深度学习系统，专注于相同的领域，显示出有希望的结果，但它们的内部工作方式不像(尽管简单)Prolog等效程序那样明显/清晰。这既不是一件好事也不是一件坏事——简而言之，这是对古老的<a class="ae km" href="https://web.media.mit.edu/~minsky/papers/SymbolicVs.Connectionist.html" rel="noopener ugc nofollow" target="_blank">象征主义与联结主义</a>辩论的重述。在实践中，硬结果很重要，现在神经网络正在赢得这场辩论..</p><p id="28d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面的示意图显示了人工智能的同心分组/子集，以提供深度学习和机器学习之间关系的更真实的画面。</p><figure class="lq lr ls lt gt mf gh gi paragraph-image"><div class="gh gi me"><img src="../Images/a94036cf1e4f89d8e6102b16b74c1007.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/0*Pd1Zvdngagm16niU.png"/></div></figure><p id="658b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图一。描述人工智能中不同子领域之间关系的示意图，来自<a class="ae km" href="http://www-labs.iro.umontreal.ca/~bengioy/DLbook/intro.html" rel="noopener ugc nofollow" target="_blank">本吉奥的书</a>。</p><p id="805c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经网络有它们的问题——其中之一是不透明的问题。虽然可视化工具存在，但大部分神经网络看起来像，而且确实是，浮点数的大格子。不经意的观察者从它们那里得到的信息不会比通过观察人类大脑更有意义，当试图将神经网络集成到工作软件系统中时，这不是很好。</p><h1 id="2f76" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">那么什么是神经网络呢？</h1><p id="b8bc" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">Haykin的<a class="ae km" href="http://www.amazon.co.uk/Neural-Networks-Learning-Machines-International/dp/0131293761/ref=sr_1_1?ie=UTF8&amp;qid=1433363604&amp;sr=8-1&amp;keywords=simon+haykin" rel="noopener ugc nofollow" target="_blank">规范文本</a>将神经网络定义如下:</p><p id="f0b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“由简单处理单元组成的大规模并行分布式处理器，具有存储经验知识并使其可供使用的自然倾向”。</p><p id="a8a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些简单的处理单元就是神经元。神经元的标准模型是由麦卡洛克和皮茨在1943年提出的，从那以后就没怎么改变过。神经元接受输入X的向量，这些输入被加权，并且可选的偏置b(图中未示出)基于应用于权重的某个函数(根据期望的特性使用许多不同的函数)生成输出或激活。下图描绘了这种风格化的神经元。</p><figure class="lq lr ls lt gt mf gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/07630453197f2a99ca0fc94d3eae3ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*stzFSutf9G3E0Nmq.png"/></div></figure><p id="1ebd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图二。单个神经元的标准(麦卡洛克-皮茨)模型。</p><p id="3d1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现实世界的神经网络有成千上万个这样的神经元，它们一起提供了经验知识的存储，最有用的是，在先前已经看到的数据的背景下理解新的、看不见的数据的能力。GPU非常适合训练神经网络，因为它们也是大规模并行的，如下图所示。</p><figure class="lq lr ls lt gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi mj"><img src="../Images/a520a6a435e669bb46226e1cac158bb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IG0lnHB3_S-NHt7D.png"/></div></div></figure><p id="685a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图3。Nvidia的Titan X GPU，拥有3，072个CUDA内核。</p><h1 id="dcff" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">神经网络的个人历史</h1><p id="179c" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">我第一次遇到神经网络是在1997年——我最后一年的项目结合了<a class="ae km" href="http://www.ra.cs.uni-tuebingen.de/SNNS/" rel="noopener ugc nofollow" target="_blank">SNNS</a>(torch 7/the ano的祖先)和<a class="ae km" href="http://www.sciencedirect.com/science/article/pii/0925231294900639" rel="noopener ugc nofollow" target="_blank">西姆德雷拉</a>创造了一个“通过观察学习”的6自由度机械臂。我们教网络嵌套杯子——就像一个孩子会做的那样。事实上，随着网络的训练，它显示了嵌套策略的复杂性进展(线性- &gt;预堆叠),正如<a class="ae km" href="http://www.ncbi.nlm.nih.gov/pubmed/24777522" rel="noopener ugc nofollow" target="_blank">皮亚杰为真实儿童记录的</a>。这个项目也旨在成为进一步工作的垫脚石，该工作受到人类大脑中<a class="ae km" href="http://en.wikipedia.org/wiki/Broca%27s_area" rel="noopener ugc nofollow" target="_blank">布罗卡区</a>发展的启发，如果我们预先训练或调节简单任务的神经网络，它在学习更复杂的任务时会更成功。预训练/调节是训练深网时要考虑的一个重要启发(见下文)。事后看来，我怀疑我为了我的项目结束演示过度训练了这个网络，以确保在我使用的简单的20x20视网膜上找到杯子的准确性！在1997年(从主观上来说)，从计算机科学的角度来看，神经网络被认为是优雅的，但可疑地接近心理学和软件工程，可以从诸如基于案例的推理、推理、决策树等技术中获得更好的(和可理解的)结果。</p><p id="f78f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2003年，我的理学硕士再次关注神经网络，但几乎是作为主要活动(分布式计算)的副业。这项工作的主要目的是展示一个异构计算节点集群如何有效地复制一个非常著名的基准测试<a class="ae km" href="http://www.ncbi.nlm.nih.gov/pubmed/3172241" rel="noopener ugc nofollow" target="_blank">，该基准测试使用钱和Sejnowski，1998 </a>的神经网络模型预测球状蛋白质的二级结构，使用基于的Linda 和David Gelernter 的基于元组的架构进行近似线性加速。此外，这项工作只使用了前馈神经网络——递归网络，当然LSTM会产生更好的结果，因为它们能够保留氨基酸输入序列的信息。</p><p id="bf30" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2003年，关于投资银行使用神经网络来预测股票市场和监控投资组合风险的流言四起。如果这是真的，那么我向读者提出，2008年的事件表明，这些神经网络可能已经找到了一系列不幸的局部极小值..</p><p id="c188" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2006年，世界变了。<a class="ae km" href="http://www.cs.toronto.edu/~hinton/" rel="noopener ugc nofollow" target="_blank"> Geoffrey Hinton </a>、<a class="ae km" href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html" rel="noopener ugc nofollow" target="_blank"> Yoshua Bengio </a>和<a class="ae km" href="http://yann.lecun.com/" rel="noopener ugc nofollow" target="_blank"> Yann LeCun </a>开始将神经网络分层堆叠——每一层都从其输入中提取重要/相关的特征，并将这些特征传递给其上一层。此外，焦点从简单的分类器转移到了生成模型——实际上，网络的主要任务变成了在不断增加的抽象级别(通过堆栈)上生成输入数据，以便提取有用的特征。这些都是重大突破，时机也很偶然——在硬件方面，Nvidia(和AMD，但OpenCL对深度学习来说几乎是死的)正在让GPU卡成为通过<a class="ae km" href="http://www.nvidia.co.uk/object/cuda-parallel-computing-uk.html" rel="noopener ugc nofollow" target="_blank"> CUDA </a>访问的计算资源。即使在今天，我们仍然处于一个黄金时代，因为这三位一体的融合:</p><ol class=""><li id="a296" class="mo mp iq jp b jq jr ju jv jy mq kc mr kg ms kk mt mu mv mw bi translated">带有训练算法的核心思想(神经网络)<a class="ae km" href="http://psych.stanford.edu/~jlm/papers/PDP/Volume%201/Chap8_PDP86.pdf" rel="noopener ugc nofollow" target="_blank">反向传播:Rumelhart，Hinton，Williams </a>本质上服从并行化。</li><li id="9225" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated">内在并行硬件(GPU)来训练这些神经网络。</li><li id="fd3b" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated">越来越大的数据集(标记的和未标记的)作为输入提供给神经网络。</li></ol><p id="60d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2013年3月，<a class="ae km" href="http://www.wired.com/2013/03/google_hinton/" rel="noopener ugc nofollow" target="_blank">辛顿加入谷歌</a>，2013年12月，<a class="ae km" href="https://www.facebook.com/yann.lecun/posts/10151728212367143" rel="noopener ugc nofollow" target="_blank">乐存加入脸书</a>。这两家公司、百度以及更多公司都在深度学习方面投入巨资，以自动分类、理解和翻译网络上的丰富内容——文本、图像、视频、语音..<em class="kl">你为什么会这么做是显而易见的——社交平台变得更加相关，手机变得更加强大和有用，广告变得更有针对性，电子商务网站提供更好的搜索结果，推荐是真正有用的——可能性是无限的。仅仅是让谷歌和脸书在这一领域开展工作，就要为当前许多(准确和不准确的)主流媒体对人工智能的报道负责。</em></p><p id="eda8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这让我们很好地了解了神经网络的典型用途。</p><h1 id="681f" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">实际应用</h1><p id="74cb" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">简而言之，神经网络从它们接受训练的数据中提取特征和模式——通常是人类看不到的特征和模式。然后，一个经过训练的神经网络可以得到<em class="kl">新的、看不见的</em>数据，并做出预测。这个一般化步骤非常有用，特别是对于<a class="ae km" href="http://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络</a>，其可以随时间吸收数据<em class="kl">，即编码时间信息或序列。神经网络变得非常擅长:</em></p><ul class=""><li id="1d89" class="mo mp iq jp b jq jr ju jv jy mq kc mr kg ms kk nc mu mv mw bi translated">图像解析(边缘检测、对象检测和分类)</li><li id="a26b" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk nc mu mv mw bi translated">场景理解</li><li id="5632" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk nc mu mv mw bi translated">语音识别</li><li id="b4af" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk nc mu mv mw bi translated">自然语言处理</li><li id="ced9" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk nc mu mv mw bi translated">语言翻译</li><li id="1624" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk nc mu mv mw bi translated">玩简单的电子游戏</li><li id="2f55" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk nc mu mv mw bi translated">多标签系统的分类和聚类</li><li id="d741" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk nc mu mv mw bi translated">时间序列预测。</li></ul><p id="8477" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于特定的用例，人工神经网络有时能够并且将会表现更好，通常是通过更简单/更老的技术。构建一个允许使用多种ML技术的问题总是明智的，这样可以进行对等比较。然而，特别是在计算机视觉和序列翻译领域，深度学习方法主导了各种排行榜。</p><p id="0b85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更一般地，如果神经网络N可以在系统S和从时间步长0到当前时间步长t在该系统中发生的事件E上被训练，例如E = {e0，..然后可以对e(t+1)，e(t+2)等中将要发生的事情提供合理到良好的预测。，那么这就有广泛的适用性——在电子商务、个性化医疗、库存管理中——只要是你能想到的，都需要这种预测。这是神经网络的基本吸引力(与理解图像及其元素相比，这一点经常被忽视/低估)。</p><h1 id="1be1" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">神经网络的弱点</h1><p id="6244" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">毫无疑问，神经网络是令人沮丧的工作。神经网络是:</p><ol class=""><li id="5eba" class="mo mp iq jp b jq jr ju jv jy mq kc mr kg ms kk mt mu mv mw bi translated">难以培训(时间过长，最终表现平平)。</li><li id="6cc7" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated">对输入的呈现方式非常敏感，因此可以提取正确的特征(ICLR是一个致力于表征学习的会议)——<a class="ae km" href="http://arxiv.org/pdf/1206.5538.pdf" rel="noopener ugc nofollow" target="_blank">本吉奥、库维尔和文森特</a>对这些复杂性进行了很好的综述。</li><li id="c558" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated">权重的初始状态会对最终绩效产生巨大的(正面和负面)影响。</li><li id="6070" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated">服从“经验法则”，这些法则散布在文献中，被发现/重新发现/重新发明:例如，课程学习，颠倒输入顺序，调整学习速度。无论如何，这不是一个完整的列表..</li></ol><p id="e586" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尽管如此，各种形式的神经网络已经在许多不同的领域展示了一流的成果。这就是它们有趣的原因，也是我认为研究人员和公司坚持使用它们的原因。它们复杂、深奥的本质远远超过了它们产生的结果。</p><h1 id="c07a" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">深度学习的“国家状况”——2015年ICLR</h1><p id="edf0" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">看看某个研究领域最近的会议上提出的主题，了解该领域当前的“国家状况”是有益的。机器学习也不例外，我参加的最后一次会议是几周前在加州圣地亚哥举行的<a class="ae km" href="http://www.iclr.cc/doku.php?id=iclr2015:main" rel="noopener ugc nofollow" target="_blank"> ICLR 2015 </a>。正如在<a class="ae km" href="http://www.machinedlearnings.com/2015/05/iclr-2015-review.html?utm_source=twitterfeed&amp;utm_medium=twitter" rel="noopener ugc nofollow" target="_blank">对ICLR的其他评论</a>中已经提到的，所有提交的论文都可以在<a class="ae km" href="http://arxiv.org/" rel="noopener ugc nofollow" target="_blank"> Arxiv </a>上获得，这太棒了。很高兴看到这些论文的作者展示他们的作品，或者在海报会议上直接谈论他们。还有很多其他优秀的会议——<a class="ae km" href="https://nips.cc/Conferences/current" rel="noopener ugc nofollow" target="_blank">NIPS</a>、<a class="ae km" href="http://www.kdd.org/kdd2015/" rel="noopener ugc nofollow" target="_blank"> KDD </a>、<a class="ae km" href="http://ijcai-15.org/" rel="noopener ugc nofollow" target="_blank">IJCAI</a>——我选择ICLR，因为它在我心目中是最新鲜的！</p><p id="d108" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，至今使用的<a class="ae km" href="http://deeplearning.net/datasets/" rel="noopener ugc nofollow" target="_blank">数据集</a>开始<a class="ae km" href="http://culurciello.github.io/tech/2015/05/17/image-rec.html" rel="noopener ugc nofollow" target="_blank">挣扎</a>。</p><p id="d1fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二，研究人员正在寻找共同的参考点，以此来对正在开发的最新系统进行评级。玩具任务是一个很好的例子，它与<a class="ae km" href="http://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf" rel="noopener ugc nofollow" target="_blank">课程学习</a>有着清晰的渊源。ICLR 2015的一篇论文正在接受审查，但我猜没有被接受的是<a class="ae km" href="http://arxiv.org/pdf/1409.2329v4.pdf" rel="noopener ugc nofollow" target="_blank">递归神经网络正则化</a>。关于这篇论文，我最喜欢的是Github 上的<a class="ae km" href="https://github.com/wojzaremba/lstm" rel="noopener ugc nofollow" target="_blank">代码——一种将论文中的数据与可再现的输出相匹配的好方法。</a></p><p id="cf79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae km" href="http://arxiv.org/abs/1412.7580" rel="noopener ugc nofollow" target="_blank">带fbfft的快速卷积网络:一个GPU性能评估</a>演示了在训练深度网络时如何更努力地驱动GPU，并且已经有了该库的另一个版本(提示:使用16位浮点而不是32位！)不难想象，如果确定了进一步的不同优化，Nvidia将在未来为深度学习社区生产专用硬件..</p><p id="55f8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，从直觉的角度来看，我个人最喜欢的是记忆网络。它具有较低的生物学似然性(或者当然该论文中没有探讨这方面),但是具有较高的实际应用性。我们可以很容易地想象出这种架构的变体，例如，它学会了访问SQL / NoSQL数据库中保存的业务事实，或者访问类似Prolog的事实和规则。</p><h1 id="aef5" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">机器学习的商品化</h1><p id="2522" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">机器学习已经渡过了某种难关，现在被视为商业应用的必备工具。语音和计算机视觉的传统应用将继续，但ML也将在POBAs(普通的旧商业应用)中变得无处不在。ML的商品化已经开始了:</p><ol class=""><li id="b386" class="mo mp iq jp b jq jr ju jv jy mq kc mr kg ms kk mt mu mv mw bi translated"><a class="ae km" href="http://azure.microsoft.com/en-gb/services/machine-learning/" rel="noopener ugc nofollow" target="_blank">微软(今年)</a></li><li id="4442" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated"><a class="ae km" href="https://cloud.google.com/prediction/docs" rel="noopener ugc nofollow" target="_blank">谷歌(2011年，很少被采用)</a></li><li id="857e" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated"><a class="ae km" href="https://aws.amazon.com/blogs/aws/amazon-machine-learning-make-data-driven-decisions-at-scale/" rel="noopener ugc nofollow" target="_blank">亚马逊(今年)</a></li></ol><p id="c339" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还存在许多较小的服务，随着时间的推移，随着赢家的出现，我们可以期待看到整合。对于那些希望在本地使用ML的工程师，我们提供以下服务:</p><p id="79a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ML从业者的祸根是超参数选择(也称为上帝参数——使你美丽的神经网络出色工作或比随机猜测好不了多少的配置标志)。事实上，<a class="ae km" href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank"> Bergstra和Bengio证明了</a>随机猜测超参数通常是一种“足够好”的策略，因为算法通常只对一两个超参数敏感。最近，Snoek等人<a class="ae km" href="http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf" rel="noopener ugc nofollow" target="_blank">应用贝叶斯先验</a>和高斯过程来进一步完善这种方法(我猜我会称之为智能随机性加学习)。<a class="ae km" href="https://github.com/HIPS/Spearmint" rel="noopener ugc nofollow" target="_blank">Spearmint</a>——构建的实现已经被剥离成一个具有漂亮的<a class="ae km" href="https://www.whetlab.com/docs/getting-started" rel="noopener ugc nofollow" target="_blank"> API </a>的<a class="ae km" href="https://www.whetlab.com/" rel="noopener ugc nofollow" target="_blank">启动</a>。考虑到一些深度网络需要大约两周的时间来训练，<em class="kl">任何更快的到达最优超参数的路径都是有利的。</em></p><h1 id="0747" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">该领域的未来方向</h1><p id="2907" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">很明显，与理论相比，围绕机器学习的软件工具相当不成熟。研究人员撰写并发表论文，而不是生产强度代码。因此，将机器学习与计算机科学的其他分支(操作系统、网络、数据存储、搜索)进行比较表明，在这一领域需要做很多工作。</p><p id="c5c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">冒着过度对冲的风险，下一个突破要么来自现有的方法(见2014 / 2015年，施密德胡伯和霍克雷特的《长短期记忆》又名<a class="ae km" href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" rel="noopener ugc nofollow" target="_blank"> LSTM </a>变得多么受欢迎),经过15年的探索之后),要么来自新的思维，可能是受生物构造的启发。Geoffrey Hinton目前关于胶囊/流形学习的工作就是一个很好的例子。对于研究人员来说，看到文献中存在哪些由于计算困难而被放弃的想法/技术，现在可能更容易处理，这肯定是有希望的！</p><p id="02c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有可能我们所要做的就是将网络堆叠得更高(900+层深？)根据Srivastava、Greff和Schmidhuber的<a class="ae km" href="http://arxiv.org/pdf/1505.00387.pdf" rel="noopener ugc nofollow" target="_blank">高速公路网论文继续前进，但我的感觉是“深度”方法只剩下这么多距离了。无论如何，有趣的是，目前六层似乎是最佳的(但是</a><a class="ae km" href="http://arxiv.org/abs/1409.4842" rel="noopener ugc nofollow" target="_blank">谷歌网</a>使用22或27层，这取决于你如何计算它们)。</p><p id="1d04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有点奇怪的是，反向传播结合随机梯度下降(SGD)仍然是使用的最好/规范的学习算法。自1986年以来，它确实经受住了时间的考验，并且非常适合GPU。目前的研究似乎集中在网络<em class="kl">架构</em>上，但学习算法的复兴似乎也将不可避免。</p><p id="2dcc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大多数神经网络都是由老师训练的——对于好的输出，我们奖励网络低误差，对于差的输出，我们惩罚它。这种方法工作得很好，但是需要很好标记的数据，这通常是一种奢侈或者根本不可能实现的。谷歌Deepmind正在倡导强化学习，将其作为开发能够很好地跨问题转移的系统的一种方式(在他们迄今为止的<a class="ae km" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" rel="noopener ugc nofollow" target="_blank">开创性论文</a>中，一个单一的神经网络已经学习/泛化到足以玩<em class="kl">所有的</em>雅达利游戏)。</p><p id="131a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，有理由期待神经网络复杂性的降低，如果不是理论上的，那么肯定是实践和使用上的。使用<a class="ae km" href="http://www.cs.toronto.edu/~fritz/absps/cvq.pdf" rel="noopener ugc nofollow" target="_blank">最小描述长度</a>或<a class="ae km" href="http://en.wikipedia.org/wiki/VC_dimension" rel="noopener ugc nofollow" target="_blank">VAP Nik–Chervonenkis维度</a>作为度量，我们感兴趣的是构建最简单、最紧凑的神经网络(具体来说，使用最少的参数)来解决给定的问题或任务。更简单的网络也会训练得更快——这在实践中是一个非常好的好处。</p><h1 id="3318" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">回顾/总结</h1><p id="68b0" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">这篇博文比我预想的要长！一部分是历史之旅，一部分是对机器学习的概述，带有明显的深度学习倾向，这是一篇比我目前正在为我的博士论文撰写的文章更非正式的文献综述。</p><p id="7aba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这一领域的创新步伐很快——每年有四五个大型会议，每个会议都会带来新的公告和突破。目前，软件工程、快速、可扩展的硬件和良好的理论真正融合在一起。</p><p id="c0b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">深度学习/神经网络可能不是将所有人工智能结合在一起的单一统一理论(有太多的未知和以前的失败)，但我怀疑它将对人工智能的发展产生深远的影响，至少在未来十年内。</p><h1 id="5fbc" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">进一步阅读</h1><p id="90a8" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">如果你仍然对机器学习感兴趣(你为什么会不感兴趣？！)我认为公平地说，在这个领域，你只是不能做足够的阅读——基础已经建立，但前沿每天都在被推进和挑战。这篇博文包含了许多有趣的链接，除此之外，如果你想了解更多，我强烈推荐这个领域的主要思想领袖/实践者的Reddit ML AMAs。它们是(没有优先顺序——它们都有助于优秀的、有洞察力的阅读):</p><ol class=""><li id="b203" class="mo mp iq jp b jq jr ju jv jy mq kc mr kg ms kk mt mu mv mw bi translated"><a class="ae km" href="http://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/" rel="noopener ugc nofollow" target="_blank">吴恩达和亚当·科茨(2015年4月15日)</a></li><li id="2841" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated"><a class="ae km" href="http://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/" rel="noopener ugc nofollow" target="_blank">于尔根·施密德于贝尔(2015年3月4日)</a></li><li id="60aa" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated"><a class="ae km" href="http://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/" rel="noopener ugc nofollow" target="_blank">杰弗里·辛顿(2014年11月10日)</a></li><li id="18bd" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated"><a class="ae km" href="http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/" rel="noopener ugc nofollow" target="_blank">迈克尔·乔丹(2014年9月10日)</a></li><li id="bc03" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated"><a class="ae km" href="http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/" rel="noopener ugc nofollow" target="_blank">扬·勒村(2014年5月15日)</a></li><li id="988c" class="mo mp iq jp b jq mx ju my jy mz kc na kg nb kk mt mu mv mw bi translated"><a class="ae km" href="http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio/" rel="noopener ugc nofollow" target="_blank">约舒阿·本吉奥(2014年2月27日)</a></li></ol></div></div>    
</body>
</html>