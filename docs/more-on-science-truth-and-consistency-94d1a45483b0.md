# 更多关于科学、真理和一致性的内容

> 原文：<https://towardsdatascience.com/more-on-science-truth-and-consistency-94d1a45483b0?source=collection_archive---------3----------------------->

根据“科学方法”的说法，“科学”分为四个步骤:

1.  建立一个模型
2.  捕捉相关数据
3.  寻找模型和数据之间的偏差。
4.  回到第一步，修改模型，看下一轮偏差是否减小。

人们可能会对此稍加修改，提出“工程方法”，其中重点不是模型和数据之间的一般偏差，而是偏差的特定子集:您希望建立一个更好的*老鼠*陷阱，即使它在捕捉老鼠方面更差，因此相对于老鼠的偏差会进一步减少，即使代价是增加相对于老鼠的偏差，或者类似的事情。

第三步与一致性有关:一致性越强意味着偏差越少、越小。*如果第 2 步执行得当*，也就是说(训练)数据是现实的无偏(且足够大)样本，则模型更接近现实，即“更真实”*其他条件不变的情况下*，更一致的模型是更好的模型。

但是“平等对待一切”是一个含糊的词。每当模型根据不同的(训练)数据进行训练时，根据定义，所有的东西都不相等。此外，统计学家知道，原则上，任何样本都有*误差最小化最优:样本本身，最终过度拟合。这就是为什么教科书统计学不鼓励在模型中添加更多的部件。这是各种验证技术背后的基本原理。*

*选择(即使是无意的)训练集、测试集和实际应用集(据我所知，我意识到没有“应用集”这样的通用术语，至少没有与训练集和测试集相提并论。我觉得应该有。)的模型，但是，迫使我们回到步骤 2。如果发现一个模型对于某个数据子集是稳健的，以一组条件为特征，那么它就和我们所拥有的“真理”一样接近，但只是以这些条件为条件。如果某个森林中的骑士说“你”，并且他们一直这样做，那么“那个森林中的骑士说‘你’”的模式是一个很好的模式——只要你不试图将它应用于所有的*骑士。除了没有任何逻辑基础的推断，我们不知道一个普通骑士无条件地说了什么。**

*换句话说，这个模型的实际应用主要取决于第二步和第三步之间的相互作用。如果我们天真地根据某个森林中的骑士的数据来训练模型，但试图根据其他森林的数据来测试它，那么我们会错误地拒绝它。我们应该小心比较训练集和测试集:如果测试集似乎使您在训练集中发现的内容无效，问题可能是测试集实际上不同于训练集。对于更像训练集的测试集子集，模型可能仍然成立(同样，对于更像测试集的训练集子集，它以前不成立)。训练集和测试集相同的(隐含)假设是我们需要参与的“故意忽视”的一部分(我喜欢 Weisberg 的措辞转变！)为了让事情简单化。但我们需要记住，这可能不是真的。当然，就算模型好，也只是对有问题的森林骑士好。去另一个森林，那里的骑士不会知道“你”是什么意思。不在天魔森林的时候，就不要去烦天魔模型了。*

*因此，如果来自训练集和测试集的结果不匹配，我们可能有两种可能性:*

1.  *样本足够接近→模型不好。(按照步骤 3)*
2.  *样本不同→模型是好的(也许)，但只是有条件的(按照步骤 2。)*

*这两种可能性都需要进一步调查。在这个阶段，实验可能是必要的，也可能是不必要的。在发现训练集和测试集之间的差异以及该模型仅适用于特定森林中的骑士的可能性之后，您真正需要的是获得更多关于该森林的条件观察(因为您已经有大量来自骑士而非该森林的观察。)你面前的问题就是简单回答是否:P(Ni|forest X) >>>> P(Ni|！森林 X)。也许您已经有足够的数据来回答这个问题，至少暂时有，或者没有(也就是说，您没有足够的数据来“测试”它，因为您已经用完了旧的测试集)。但是，如果您可以从 forest X 获得新的数据，就不需要进行全面的实验。*

*那么，对“真理”的统计识别需要是一个微妙的过程:我们不仅要识别无条件地说“一致”的观察，还要识别“有条件一致”的观察。大部分的努力都需要投入到“有条件”的成分中。可能有一些普遍一致的真理是可以观察到的，没有任何附加条件，但如果是这样，它们将很容易被发现，而不是花哨的“科学！”需要去发现它们。但是，当我们寻找条件真理时，抽样就成了关键:条件真理(更多地)适用于现实的特定子集，而不是其他子集。当我们找到合适的样本时，我们会知道的。如果我们不这样做，我们就会搞砸，看不到存在的东西——只是因为它们不在任何地方。*

*PS。从一个非常普遍的经历中出现的一个相关点(当然对我来说——大概对其他人来说)是，模型的用户是否有可能记录他们的模型在何时何地有多错误。作为事后分析的手段，对于统计学的更传统的用途来说，这很容易:我们有数据，除了“过去”事件的记录之外，我们不会假装这些数据有用，在我们拟合模型之后，我们可以立即说出偏差在哪里。对于产生定量预测与记录保存相结合的数据，即使在创建模型时“应用数据”尚不存在，也可以追溯研究不匹配的程度。对于其他各种应用程序，尚不清楚是否有可能对模型进行回顾性审查。我经常收到错误的定向广告、搜索结果或推荐，我想我们都遇到过这样的事情。如果我在另一端，我想得到一些关于算法有多错误的信息，对于哪些观众，在什么条件下。这些信息似乎根本不可能以有意义的方式获得。诚然，我们偶尔会有机会报告调查结果/结果/广告/建议是否恰当，但我不认为所有用户和环境的回复率是随机的。*

*有时，错误是显而易见的:我不懂越南语，所以越南语广告显然是错误的。但我会说西班牙语，能读一点德语和匈牙利语(只是举例，不一定是现实的反映)。因此，即使匈牙利语广告可能是“错误的”,因为我不能很好地阅读它，我也没有寻找信息，它可能会意外地产生一些我可以利用的相关信息，即使我没有积极地寻找它。通常，用户自己不一定知道他们在寻找什么。我们不知道普希金诗歌的“正确”翻译是什么(通常，这才是重点)。“正确的答案”通常不存在于精确定义的形式中，除非我们错误地假设它们存在于不存在的条件下(同样，凯恩斯的选美比赛游戏在这里是恰当的:参与者认为美的共同标准是什么的共同期望，而不是他们真正认为什么是“美”，往往会驱动“市场”。更重要的是，这种“共同但错误的信念”比事实更加一致。然而，问他们什么是“美”是没有意义的，因为它没有一个明确的、普遍定义的答案。)即使我们知道用户和算法输出之间的匹配，量化“错误”可能有点棘手。但大多数定向广告、搜索结果和推荐甚至无法与用户真正想要的东西匹配。*

*这似乎特别令人不安:如果你不知道你的模型与现实有多大的误差，也不能估计误差在不同条件下是如何变化的，你怎么能做科学呢？*