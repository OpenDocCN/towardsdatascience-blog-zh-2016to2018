# 介绍 PCFGs，最左边的推导，乔姆斯基 NF，CKY 算法和许多其他可怕的单词变得简单

> 原文：<https://towardsdatascience.com/intro-to-pcfgs-left-most-derivations-chomsky-nf-cky-algorithm-and-many-other-scary-words-80114e896369?source=collection_archive---------3----------------------->

![](img/adbb4c4cfe673cae5dcd9213e4ace11c.png)

写这篇文章的想法是非常自发的，只要我现在工作的主要领域是命名实体识别和从原始多语言文本中提取高级数据；概率上下文无关文法绝对不是我感兴趣的类型(二叉树结构一点也不吸引我)，我也不是用 PCFGs 开发一些应用程序的专家。然而，了解这样一个概念对每个自然语言处理科学家来说都是一个很好的实践，所以我为什么不应该花一些时间来研究 PCFG 的基本思想和细节呢？

请不要期望这篇文章是对上下文无关语法世界的“深度探索”,并解释先进的方法论；这只是对想知道它如何工作以及我们在哪里使用它的人的介绍。

## 上下文无关文法的定义

基本上，我们可以将 CFG 视为元组，由 4 个元素组成: *G = (N，E，R，S)* ，其中

*   *N 是所有非终端符号的集合(在我们的例子中是 POS 标签)；*
*   *E 是所有终端符号(单词)的集合；*
*   *R 是以下形式的规则集:*

![](img/0c2206fce1510ca16d45d1513465846e.png)

*   *S 是卓越的开始符号。*

如果规则由 2 个元素组成(非终结来自 *N* 和终结或非终结来自 *N* 和 *E* 的并集)，那么它是一元规则。如果规则由左边的非终结符和任意长度的终结符和非终结符的序列组成，那么它是一个混合规则。

## 解析树的定义(最左边的派生)

如果给我们一个上下文无关的语法，那么最左边的派生是来自 *E** (由来自 *E* 的字符串组成的所有可能序列的集合)的一个字符串序列，它形成一个句子。我们正在为索引为 *i* 的每个 *s* 检索该字符串，方法是获取索引为 *i-1* 的 *s* ，并挑选 *N* 中最左边的非终结符 *X* ，并用一些 *β* 更新它，其中我们在 *R* 中有一个规则 X → β。

解析树不是一个复杂的概念，但是它们有不同的重要术语，值得解释和讨论。

*   每个最左边的派生总是以从 E*开始的索引为 n 的字符串 s 结束；
*   *s 用指数 n 表示导数的产率；*
*   *如果至少有一个 yield 为 s，则来自 E*的字符串 s 在语言中。*

![](img/83b219617028d3e203ab6d9eb214f99a.png)

## 概率解析树的定义(概率上下文无关文法)

这个概念解释起来也很简单，但是它背后有很多符号背景。让我们假设，我们的上下文无关文法是不明确的:给定一个上下文无关文法，我们可以有许多产出。对于给定的语法，我们如何考虑什么产量是最好的？正确的答案总是一个——概率。

*   索引为 G 的 T 是文法 G 下所有可能的最左导子的集合；
*   *对于从 T 到索引为 G 的任何导子 T，我们写 yield(t)来表示来自 E*的字符串 s，这是 T(T 中的单词序列)的 yield；*
*   *对于来自 E*的给定句子 s，我们用 T(s)来指代集合:*

![](img/9ea8353d99d90972741c74fbd0301833.png)

*   如果句子 s 至少有一棵解析树，我们说它是语法的。

我们的主要想法是扩展我们的定义，给出可能导数的概率分布。

求导的概率大于零的条件要满足(我们不能有求导是很符合逻辑的，这是完全不可能的):

![](img/19260d1c6df9f60da8666807a0dedf13.png)

而且全概率的条件也要满足(概率的和法则；不同的派生是不同的事件，它们一起形成一个结果):

![](img/8bf1c8958f45092aca766719c003a939.png)

现在，有了这 2 个公式，我们可以将概率上下文无关文法定义为 *G = (N，E，R，S) + set(q (α→β))。*

这个集合中的每一个元素都是在一个最左边的推导中选择规则α→β的条件概率 q (α→β)，假设被展开的非终结点是 *α。*

现在，我们可以用新的符号以这种方式重新制定全概率规则(现在，我们将 *α* 视为我们可以拥有的任何最左侧推导的根):

![](img/807849ca81c09bf6f6bf7a05681e832e.png)

关于导数大于零的概率的想法也保持不变，只是替换了相同的概念:

![](img/10bc1ef03517656f419ccb6a6764991c.png)

给定规则集 q (α→β)的整个解析树的概率由联合概率规则定义:

![](img/3827f94264f145167a613a9f5c6a621a.png)

**从语料库中导出 PCFG** 为了从预定义的语料库中生成先前的概率上下文无关语法，我们需要规则 q (α→β)上的一些概率分布。为了得到它们，我们可以使用最大似然估计，公式如下:

![](img/b7451eb0e71ff9f2ffc08e09724b8115.png)

这基本上是条件概率估计的简单定义。我们可以使用拉普拉斯平滑和其他技术来更好地计算我们的概率，这取决于我们正在处理的任务和语料库。

**乔姆斯基范式
这种范式背后的思想是，我们的 PCFGs 中使用的每个规则都应该采用以下形式之一:**

![](img/4114e469471e207bc11381e9c8db47f1.png)![](img/b6563529434f5cef190182f09763fc58.png)

它只意味着这样一个事实:如果一个节点都不是来自 *N* 的终端，那么它可以有两片叶子；如果它是来自 *E* 的终端(单词)，那么它可以有一片叶子。

## CKY 算法

我认为这个概念是光子晶体光纤光栅理论中最复杂的部分；然而，它的输入是解析树，它显示了我们试图从给定语法的句子中找到的句法依赖。

CKY 算法的目标是找到给定句子和 PCFG 的最可能的分析树；它通过逐步选择最可能的树结构来解决这个问题。

![](img/4b98e76b22b54af5b614d41918e594b5.png)

我觉得前面的伪代码不是很好理解，所以我尽量在上面做些小说明。

该算法可以非正式地分为两步:初始化和优化。

初始化部分意味着从语法上给我们正在处理的句子中的每个非终结符赋值；如果有一个以上的终端导致当前的非终端，那么我们要记住他们两个。因此，想象中的“三角形表”的底部填充了起始终端正上方的非终端。

优化部分是 3 个嵌套 for 循环的结构:对于我们正在处理的每个跨度，对于句子的每个开始和结束(结束被定义为开始+跨度)我们正在寻找非结束，这是最可能的树的根，它支配从开始到结束的范围内的子元素(在前面的循环中计算)。使用从+ 1 开始到-1 结束范围的分割点递归地找到最大非终结符(以找到所有可能的单词组合)。

CKY 有不同的变体，可能定义选择非终结点是选择拆分点之前的前一步，以及相反的情况；作者可能会声称我们只是在寻找一个解析树的结构，但是使用 CKY，我们也可以找到最可能的 PCFG 的概率。

## 结论

我希望这篇文章对不熟悉创建解析树的读者有用，以便理解它的语法结构并能够估计它的概率；我不认为这篇文章对有经验的数据科学家和 NLP 专家有帮助。

说到我的个人经验(我没有很多年的经验)，我可以说我从来没有见过真正的解析树在真实数据上运行；人们应该始终明白，这个概念作为理论基础是重要的，但在实践中很少使用。

我见过的解析树的最佳实现来自 NLTK 它的所有工作都做得很好(老实说，这是一个科学软件包应该做的)。

感谢任何形式的反馈，为新的帖子和像这样的小笔记提供了很多灵感。