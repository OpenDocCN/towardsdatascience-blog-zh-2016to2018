<html>
<head>
<title>How I Tackled My First Kaggle Challenge Using Deep Learning — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我如何使用深度学习解决我的第一个Kaggle挑战——第1部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-i-tackled-my-first-kaggle-challenge-using-deep-learning-part-1-b0da29e1351b?source=collection_archive---------4-----------------------#2017-06-10">https://towardsdatascience.com/how-i-tackled-my-first-kaggle-challenge-using-deep-learning-part-1-b0da29e1351b?source=collection_archive---------4-----------------------#2017-06-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/2828cb9ae50557fc15cee535c34cedf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KUS5ibNoTKn6i5CRYDjV8Q.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Artificial Intelligence</figcaption></figure><div class=""/><p id="0b43" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在过去的几周里，我一直在参加免费且优秀的<a class="ae la" href="http://course.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai </a>在线课程，该课程从实用的角度教授深度学习。作为一名编程出身的人，我发现这是正确的方法。然而，我一直在用各种材料补充我的理论理解(我强烈推荐那些来自<a class="ae la" href="http://cs231n.github.io/" rel="noopener ugc nofollow" target="_blank"> CS231n斯坦福课程</a>的笔记)。</p><h2 id="6790" class="lb lc jf bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">进入Kaggle</h2><p id="4a24" class="pw-post-body-paragraph kc kd jf ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated"><a class="ae la" href="https://www.kaggle.com" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>是应用深度学习挑战的战场和训练场，我被其中一个特别吸引:州立农场分心司机检测挑战。在这项挑战中，我们得到了一组大约20K的驾驶员照片，这些驾驶员要么处于专注状态，要么处于分心状态(例如，拿着电话，化妆等)。).测试集由大约80K幅图像组成。目标是建立一个模型，该模型可以在一组10个类别中准确地对给定的驾驶员照片进行分类，同时最小化<a class="ae la" href="https://www.quora.com/What-is-an-intuitive-explanation-for-the-log-loss-function" rel="noopener ugc nofollow" target="_blank">对数损失</a>(即，每次分类器得到错误的预测时，惩罚分数都会按照对数顺序上升)。</p><p id="5b56" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我对State Farm challenge感兴趣，因为我目前正在开发一款由人工智能驱动的dash cam手机应用程序，它将使驾驶变得更加安全和丰富。KamCar将做的一件事是检测司机的注意力分散/困倦，并提醒司机避免灾难。根据<a class="ae la" href="https://www.cdc.gov/motorvehiclesafety/distracted_driving/" rel="noopener ugc nofollow" target="_blank">疾病预防控制中心</a>的数据，司机分心是20%车祸的原因，我相信，随着当前深度学习的进步和智能手机功能的增强，我们可以采取更多措施来解决这一问题。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/4a151a86efd4ab630a6c169ba3702b40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*67Mz9ho1Bx13hbfKkh75pw.gif"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Driver in different distracted states</figcaption></figure><p id="c987" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在严格遵循杰瑞米·霍华德(fast.ai联合创始人)的方法之前，我尝试了自己的愚蠢方法，但收效甚微。</p><h2 id="b461" class="lb lc jf bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">第1步—获得正确的验证集</h2><p id="d8b8" class="pw-post-body-paragraph kc kd jf ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">据我所知，对于有多少训练集应该放在验证集中没有明确的规则，所以我设计了一个在训练集和验证集之间80/20的划分。</p><p id="6b9a" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">当我开始应对州立农场挑战时，我只是在所有10个类中随机移动了20%的图像，从训练集到验证集。但是当我对数据运行一个简单的线性模型时，训练集的损失是巨大的(超过14)，而验证集的准确性未能超过17%。</p><p id="1160" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我回到州立农场挑战赛页面，再次阅读其详细信息，并注意到以下内容:</p><blockquote class="me"><p id="c170" class="mf mg jf bd mh mi mj mk ml mm mn kz dk translated"><em class="mo">训练和测试数据被分割在驱动程序上，</em> <strong class="ak"> <em class="mo">使得一个驱动程序只能出现在训练或测试集上。</em> </strong></p></blockquote><p id="82ec" class="pw-post-body-paragraph kc kd jf ke b kf mp kh ki kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz ij bi translated"><em class="mu">有趣… </em>我想…既然我们根据验证集来验证(而不是训练)我们的模型，它应该表现出与测试集相似的属性，对吗？因此，解决方案是<strong class="ke jg">分割训练集和验证集，使得验证集中一定比例的驾驶员不在训练集中。</strong> State Farm方便地提供了一个csv文件，该文件将给定的驱动程序id映射到一个文件名，因此分割非常简单:</p><pre class="ma mb mc md gt mv mw mx my aw mz bi"><span id="3ff2" class="lb lc jf mw b gy na nb l nc nd">import pandas as pd<br/>import random</span><span id="0a09" class="lb lc jf mw b gy ne nb l nc nd">df = pd.read_csv(path + ‘/driver_imgs_list.csv’)<br/>by_drivers = df.groupby(‘subject’)<br/>unique_drivers = by_drivers.groups.keys()</span><span id="36c2" class="lb lc jf mw b gy ne nb l nc nd"># Set validation set percentage with regards to training set<br/>val_pct = 0.2<br/>random.shuffle(unique_drivers)</span><span id="3ea5" class="lb lc jf mw b gy ne nb l nc nd"># These are the drivers we will be entirely moving to the validation set<br/>to_val_drivers = unique_drivers[:int(len(unique_drivers) * val_pct)]</span></pre><h2 id="1249" class="lb lc jf bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">步骤2 —从样品组开始</h2><p id="487a" class="pw-post-body-paragraph kc kd jf ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">由于训练和验证集总计20K个图像，当您只是检查一些设置是否有效时，在这么大的数据量上训练模型仍然需要一点时间。</p><p id="e964" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">样本集只是训练集和验证集的子集:您的模型将根据样本训练集进行训练，以快速评估哪些有效，哪些无效。这样做让我节省了很多时间！我选择我的样本集是大约20%的数据，其中图像被随机复制到样本集。</p><h2 id="1d16" class="lb lc jf bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">第三步——尝试准系统模型</h2><p id="317c" class="pw-post-body-paragraph kc kd jf ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">下面的模型非常简单，事实上它根本没有卷积层:</p><pre class="ma mb mc md gt mv mw mx my aw mz bi"><span id="f0d0" class="lb lc jf mw b gy na nb l nc nd">def linear_model():<br/>    model = Sequential()</span><span id="0c77" class="lb lc jf mw b gy ne nb l nc nd">   # image size is 3 (RGB) x 224x224 (WidthxHeight)<br/>   model.add(BatchNormalization(axis=1, input_shape=(3, img_size_1D, img_size_1D)))<br/>   <br/>    model.add(Flatten())<br/>    # here we have 10 classes        <br/>    model.add(Dense(num_classes, activation='softmax'))<br/>    <br/>    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>    return model</span></pre><p id="dd94" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">有趣的是，在没有卷积和正则化的情况下，它是如何在验证集上达到接近40%的准确率的！</p><pre class="ma mb mc md gt mv mw mx my aw mz bi"><span id="f286" class="lb lc jf mw b gy na nb l nc nd">model = linear_model()<br/>model.optimizer.lr = 10e-5</span><span id="b87c" class="lb lc jf mw b gy ne nb l nc nd">model.fit_generator(sample_train_batches, samples_per_epoch=sample_train_batches.nb_sample, nb_epoch=3, <br/>                   validation_data=sample_val_batches, nb_val_samples=sample_val_batches.nb_sample, verbose=1)</span><span id="3020" class="lb lc jf mw b gy ne nb l nc nd">Epoch 1/3<br/>1803/1803 [==============================] - 29s - loss: 5.7358 - acc: 0.2806 - val_loss: 10.9750 - val_acc: 0.1741<br/>Epoch 2/3<br/>1803/1803 [==============================] - 24s - loss: 1.6279 - acc: 0.6339 - val_loss: 4.6160 - val_acc: 0.3304<br/>Epoch 3/3<br/>1803/1803 [==============================] - 24s - loss: 0.5111 - acc: 0.8358 - val_loss: 3.1399 - val_acc: 0.3951</span></pre><p id="1fbc" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">显然，你可以看出它大大超出了训练集，因为在仅仅3次完整运行中，它就达到了超过80%的准确率，而验证集的准确率是两倍。问题是我们的简单模型已经学会记忆大多数图像的正确权重，这使得它无法很好地概括以前从未遇到过的驾驶员图像(猜猜看，他们都在验证集中！).</p><h2 id="fc4c" class="lb lc jf bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">第四步——在你的生活中增加一些回旋的余地</h2><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/a82f291a6ee1d16848a735facc639291.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z7hd8FZeI_eodazwIapvAw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Example of a convolutional neural network — from Adit Deshpande’s blog <a class="ae la" href="https://adeshpande3.github.io/" rel="noopener ugc nofollow" target="_blank">https://adeshpande3.github.io/</a></figcaption></figure><p id="8081" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">好了，这才是真正有趣的地方……我创建了一个模型，用一些卷积来测试这样的架构是否会提高准确性:</p><pre class="ma mb mc md gt mv mw mx my aw mz bi"><span id="c348" class="lb lc jf mw b gy na nb l nc nd">def simple_convnet():<br/>    model = Sequential([<br/>        BatchNormalization(axis=1, input_shape=(3,224,224)),<br/>        Convolution2D(32,3,3, activation='relu'),<br/>        BatchNormalization(axis=1),<br/>        MaxPooling2D((3,3)),<br/>        Convolution2D(64,3,3, activation='relu'),<br/>        BatchNormalization(axis=1),<br/>        MaxPooling2D((3,3)),<br/>        Flatten(),<br/>        Dense(200, activation='relu'),<br/>        BatchNormalization(),<br/>        Dense(10, activation='softmax')<br/>    ])</span><span id="3207" class="lb lc jf mw b gy ne nb l nc nd">model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])<br/>    return model</span></pre><p id="88cf" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">令人惊讶的是(至少对我来说)，该模型在验证集上表现很差，但在训练集上很快达到了100%的准确率:</p><pre class="ma mb mc md gt mv mw mx my aw mz bi"><span id="fa5e" class="lb lc jf mw b gy na nb l nc nd">Epoch 1/3<br/>1803/1803 [==============================] - 34s - loss: 1.2825 - acc: 0.6184 - val_loss: 2.0999 - val_acc: 0.2612<br/>Epoch 2/3<br/>1803/1803 [==============================] - 25s - loss: 0.2360 - acc: 0.9590 - val_loss: 2.2691 - val_acc: 0.2098<br/>Epoch 3/3<br/>1803/1803 [==============================] - 26s - loss: 0.0809 - acc: 0.9939 - val_loss: 2.4817 - val_acc: 0.1808<br/>Epoch 1/3<br/>1803/1803 [==============================] - 30s - loss: 0.0289 - acc: 0.9994 - val_loss: 2.6927 - val_acc: 0.1585<br/>Epoch 2/3<br/>1803/1803 [==============================] - 29s - loss: 0.0160 - acc: 1.0000 - val_loss: 2.7905 - val_acc: 0.1540<br/>Epoch 3/3<br/>1803/1803 [==============================] - 26s - loss: 0.0128 - acc: 1.0000 - val_loss: 2.7741 - val_acc: 0.1562</span></pre><h2 id="9156" class="lb lc jf bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">第五步——有意愿，就有增加</h2><p id="60d2" class="pw-post-body-paragraph kc kd jf ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">数据扩充使我们能够对图像进行随机修改，以降低模型记忆特定图像权重的能力。一些类型的增强包括旋转、宽度/高度移动、<a class="ae la" href="https://en.wikipedia.org/wiki/Shear_mapping" rel="noopener ugc nofollow" target="_blank">剪切</a>和RGB通道移动。我尝试了一系列参数，并确定了以下最佳结果:</p><pre class="ma mb mc md gt mv mw mx my aw mz bi"><span id="33b6" class="lb lc jf mw b gy na nb l nc nd">gen_all = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, shear_range=0.15, channel_shift_range=10, width_shift_range=0.1)</span></pre><p id="93e9" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">经过多次运行，我设法在验证集上实现了60%的准确率，这对于接下来的事情来说是非常令人鼓舞的:</p><pre class="ma mb mc md gt mv mw mx my aw mz bi"><span id="a4ff" class="lb lc jf mw b gy na nb l nc nd">Epoch 13/15<br/>1803/1803 [==============================] - 26s - loss: 0.4834 - acc: 0.8697 - val_loss: 1.4806 - val_acc: 0.5625<br/>Epoch 14/15<br/>1803/1803 [==============================] - 26s - loss: 0.4944 - acc: 0.8658 - val_loss: 1.4361 - val_acc: 0.5759<br/>Epoch 15/15<br/>1803/1803 [==============================] - 27s - loss: 0.4959 - acc: 0.8597 - val_loss: 1.3884 - val_acc: 0.6004</span></pre><p id="865a" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于这样一个简单的模型来说还不错，你同意吗？</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/0a47edca4e09a2067157ccf4163fbf54.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/1*dLCrhVXmbguOkYYF68xs-w.gif"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Not bad at all, according to Michelle Obama</figcaption></figure><p id="6e34" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我正在进行这个系列的第2部分，在那里我们将使用完整的数据集，并通过利用已经预先训练好的<a class="ae la" href="https://arxiv.org/pdf/1409.1556" rel="noopener ugc nofollow" target="_blank"> VGG网</a>模型来执行<a class="ae la" href="https://en.wikipedia.org/wiki/Transfer_learning" rel="noopener ugc nofollow" target="_blank">迁移学习</a>。代码也将很快出现在我的github上。</p><p id="76fb" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">敬请期待，喜欢就分享，不要犹豫留下评论:)。</p></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><p id="69be" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="mu">我是建筑</em><a class="ae la" href="http://gokamcar.com/" rel="noopener ugc nofollow" target="_blank"><em class="mu">KamCa</em></a><em class="mu">r，AI驱动的dash cam app，让驾驶更安全、更丰富的体验。如果你是一名移动开发者，想要开发一些令人兴奋的技术和产品，或者只是想提供一些建议，请在</em><a class="ae la" href="https://twitter.com/Ed_Forson" rel="noopener ugc nofollow" target="_blank"><em class="mu">Twitter</em></a><em class="mu">或这里联系我:)</em></p></div></div>    
</body>
</html>