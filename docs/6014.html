<html>
<head>
<title>Understanding binary cross-entropy / log loss: a visual explanation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解二元交叉熵/对数损失:一个直观的解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a?source=collection_archive---------0-----------------------#2018-11-21">https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a?source=collection_archive---------0-----------------------#2018-11-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ab54cc54ec95874eafbd043b59046fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BJZqDD5ieVE6XxdD"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@freegraphictoday?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">G. Crescoli</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="2ae1" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">介绍</h1><p id="d91a" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如果你正在训练一个<strong class="ld ir">二进制分类器</strong>，很可能你正在使用<strong class="ld ir">二进制交叉熵</strong> / <strong class="ld ir">对数损失</strong>作为你的损失函数。</p><p id="e4fb" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">有没有想过<strong class="ld ir">用这个损失函数</strong>到底是什么意思？问题是，考虑到今天的库和框架的易用性，很容易忽略所使用的损失函数的真正含义。</p><h1 id="a8c9" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">动机</h1><p id="1f18" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我在寻找一篇博客文章，以一种视觉上清晰简洁的方式解释<strong class="ld ir">二元交叉熵</strong> / <strong class="ld ir">日志损失</strong>背后的概念，这样我就可以在<a class="ae kc" href="https://www.datascienceretreat.com/" rel="noopener ugc nofollow" target="_blank">数据科学务虚会</a>上向我的学生展示它。因为我找不到任何适合我的目的的，所以我自己承担了写它的任务:-)</p><h1 id="f115" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">一个简单的分类问题</h1><p id="8fc8" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们从 10 个随机点开始:</p><p id="3a70" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><code class="fe me mf mg mh b">x = [-2.2, -1.4, -0.8, 0.2, 0.4, 0.8, 1.2, 2.2, 2.9, 4.6]</code></p><p id="17c1" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">这是我们唯一的<strong class="ld ir">特色</strong> : <strong class="ld ir"> <em class="mi"> x </em> </strong>。</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/fcfa3480a9fcf083b866fbb0706f05be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*233u_eZT4P8Q7uC9ES9jFw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 0: the feature</figcaption></figure><p id="334a" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">现在，让我们给我们的点分配一些<strong class="ld ir">颜色</strong>:<strong class="ld ir">红色</strong>和<strong class="ld ir">绿色</strong>。这些是我们的<strong class="ld ir">标签</strong>。</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/b4be5c7eb431f5c258de6a744b5f7919.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*zdmnljReyuOb6_h6luMaNw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1: the data</figcaption></figure><p id="1674" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">所以，我们的分类问题非常简单:给定我们的<strong class="ld ir">特征<em class="mi">x</em>T36】，我们需要预测它的<strong class="ld ir">标签</strong> : <strong class="ld ir">红色</strong>或者<strong class="ld ir">绿色</strong>。</strong></p><p id="d65c" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">既然这是一个<strong class="ld ir">二元分类</strong>，我们也可以把这个问题提出来:“点<strong class="ld ir">是绿色的</strong>”或者更好一点，“点<strong class="ld ir">是绿色的</strong>的概率是多少”？理想情况下，<strong class="ld ir">绿点</strong>的概率为<strong class="ld ir"> 1.0 </strong>(绿色)，而<strong class="ld ir">红点</strong>的概率为<strong class="ld ir"> 0.0 </strong>(绿色)。</p><p id="c6bc" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">在此设置中，<strong class="ld ir">绿点</strong>属于<strong class="ld ir">正类</strong> ( <strong class="ld ir">是</strong>，为绿色)，而<strong class="ld ir">红点</strong>属于<strong class="ld ir">负类</strong> ( <strong class="ld ir">否</strong>，为非绿色)。</p><p id="e83d" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">如果我们<strong class="ld ir">拟合一个模型</strong>来执行这个分类，它将<strong class="ld ir">预测我们每一个点都是绿色的概率</strong>。给定我们所知道的点的颜色，我们如何<strong class="ld ir">评估</strong>预测的概率有多好(或多差)？这就是<strong class="ld ir">损失函数</strong>的全部目的！对于<strong class="ld ir">不良预测</strong>，它应该返回<strong class="ld ir">高值</strong>，对于<strong class="ld ir">良好预测</strong>，它应该返回<strong class="ld ir">低值</strong>。</p><p id="b740" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">对于像我们的例子一样的<strong class="ld ir">二元分类</strong>，典型的<strong class="ld ir">损失函数</strong>是<strong class="ld ir">二元交叉熵</strong> / <strong class="ld ir">对数损失</strong>。</p><h1 id="e041" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">损失函数:二元交叉熵/对数损失</h1><p id="901c" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如果你查找这个<strong class="ld ir">损失函数</strong>，你会发现:</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/7022e55b09bc4b4050d3387fbb99bc01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*rdBw0E-My8Gu3f_BOB6GMA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Binary Cross-Entropy / Log Loss</figcaption></figure><p id="be33" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">其中<strong class="ld ir"> y </strong>为的<strong class="ld ir">标号(对于</strong> <strong class="ld ir">绿色</strong>点为<strong class="ld ir"> 1 </strong> <strong class="ld ir">，对于</strong> <strong class="ld ir">红色</strong>点为<strong class="ld ir">0</strong><strong class="ld ir">)<strong class="ld ir">p(y)</strong>为所有<strong class="ld ir"> N </strong>点为绿色</strong>的预测<strong class="ld ir">概率。</strong></p><p id="87c2" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">阅读这个公式，它告诉你，对于每一个<strong class="ld ir">绿色</strong>点(<em class="mi"> y=1 </em>)，它将<em class="mi"> log(p(y)) </em>加到损失上，即它是绿色的的<strong class="ld ir"> log 概率。反之，它为每个<strong class="ld ir">红色</strong>点(<em class="mi"> y=0 </em>)加上<em class="mi"> log(1-p(y)) </em>，即它为红色</strong>的<strong class="ld ir"> log 概率。当然，不一定很难，但也没有那么直观…</strong></p><p id="58df" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">况且<strong class="ld ir">熵</strong>跟这一切有什么关系？为什么我们首先采用<strong class="ld ir">概率对数</strong>？这些都是有效的问题，我希望在下面的“<em class="mi">展示数学</em>”部分回答它们。</p><p id="3688" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">但是，在进入更多公式之前，让我给你看一个上面公式的<strong class="ld ir">可视化表示</strong>…</p><h1 id="d9ee" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">计算损失——可视化方法</h1><p id="7594" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">首先，让我们<strong class="ld ir">将</strong>分门别类，<strong class="ld ir">正</strong>或<strong class="ld ir">负</strong>，如下图:</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/a55493207283a8e73d9dc0efe2e1a1c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*Fa-WkLN9vg_2uhOg-47XZw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2: splitting the data!</figcaption></figure><p id="89d8" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">现在，让我们训练一个<strong class="ld ir">逻辑回归</strong>来对我们的点进行分类。拟合回归是一条<em class="mi">s 形曲线</em>，代表对于任何给定的<em class="mi">x</em>T85】点为绿色的<strong class="ld ir">概率。看起来是这样的:</strong></p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/58d8c6da4deb58e135d258434bde2c97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*5T7AYwKH_9InxK9gc4n5hA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3: fitting a Logistic Regression</figcaption></figure><p id="3646" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">那么，对于所有属于<strong class="ld ir">正类</strong> ( <strong class="ld ir">绿</strong>)的点，我们的分类器给出的预测<strong class="ld ir">概率</strong>是多少？这些是下的<strong class="ld ir">绿色条</strong><strong class="ld ir"><em class="mi">s 形曲线</em>，在<strong class="ld ir"> <em class="mi"> x </em> </strong>坐标处对应的点。</strong></p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/df53363cc63cd56c72ea419e2a5f739b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*MgSLK7Dz14devDqfg8g-7Q.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4: probabilities of classifying points in the POSITIVE class correctly</figcaption></figure><p id="0bd9" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">好的，到目前为止，一切顺利！<strong class="ld ir">负类</strong>中的点呢？记住， <em class="mi">下的<strong class="ld ir">绿色条</strong><em class="mi"/><strong class="ld ir">s 形曲线</strong></em>代表给定点为<strong class="ld ir">绿色</strong>的概率。那么，给定点<strong class="ld ir">红</strong>的概率是多少？上方的<strong class="ld ir">红色条<em class="mi">s 形曲线</em>，当然:-)</strong></p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/979a7f942fa58354a217dd68f457fe5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*j8cBH1Pcr0CHmXPgzgKCPQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 5: probabilities of classifying points in the NEGATIVE class correctly</figcaption></figure><p id="61af" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">将所有这些放在一起，我们最终会得到这样的结果:</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/c52783e54163615d643b44079566591b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*JLdsBjbAz2zwgQ9m977Bsw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 6: all probabilities put together!</figcaption></figure><p id="8c0c" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">条形代表与每个点的相应<strong class="ld ir">真实类别</strong>相关的<strong class="ld ir">预测概率</strong>！</p><p id="84d6" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">好了，我们有了预测的概率…是时候<strong class="ld ir">通过计算<strong class="ld ir">二元交叉熵</strong> / <strong class="ld ir">日志损失</strong>来评估</strong>了！</p><p id="783d" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">这些<strong class="ld ir">概率就是我们所需要的全部</strong>，所以，让我们<strong class="ld ir">去掉<em class="mi"> x </em>轴</strong>并使棒条彼此相邻:</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/9c1ca4b7a0ce712868276cbb762c33bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*Xe_a5G1QliSaXYbVf06v2w.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 7: probabilities of all points</figcaption></figure><p id="4b81" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">嗯，<em class="mi">挂杆</em>已经没有多大意义了，所以让我们<strong class="ld ir">重新定位它们</strong>:</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/8b4c1fbef132d69c4e2c9624001e7fe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*72oORljVj0UjFFHLy1rJyA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 8: probabilities of all points — much better :-)</figcaption></figure><p id="ab44" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">因为我们试图计算一个<strong class="ld ir">损失</strong>，我们需要惩罚坏的预测，对吗？如果<strong class="ld ir">真类</strong>关联的<strong class="ld ir">概率</strong>为<strong class="ld ir"> 1.0 </strong>，我们需要它的<strong class="ld ir">损失</strong>为<strong class="ld ir">零</strong>。反之，如果那个<strong class="ld ir">概率是</strong> <strong class="ld ir">低</strong>，比如说<strong class="ld ir"> 0.01 </strong>，我们需要它的<strong class="ld ir">损失</strong>是<strong class="ld ir">巨大</strong>！</p><p id="7983" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">事实证明，采用概率的<strong class="ld ir">(负)对数非常适合我们的目的(<em class="mi">由于 0.0 和 1.0 之间的值的对数是负的，我们采用负对数来获得损失的正值</em>)。</strong></p><blockquote class="mw mx my"><p id="0cb9" class="lb lc mi ld b le lz lg lh li ma lk ll mz mb lo lp na mc ls lt nb md lw lx ly ij bi translated">实际上，我们使用<strong class="ld ir"> log </strong>的原因来自于<strong class="ld ir">交叉熵</strong>的定义，请查看下面的“<strong class="ld ir">展示数学</strong>”部分了解更多细节。</p></blockquote><p id="247c" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">下面的图给了我们一个清晰的画面——随着<strong class="ld ir">真实等级</strong>的<strong class="ld ir">预测概率</strong>变得<strong class="ld ir">更接近零</strong>，损失<strong class="ld ir">呈指数增长</strong>:</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/2516aba0b052a458aa8f81aa4af42b85.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*T8KWtAn8FkAcsg8RsjiZ6Q.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 9: Log Loss for different probabilities</figcaption></figure><p id="cad1" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">很公平！让我们<strong class="ld ir">取概率</strong>的(负)对数——这些是每个点对应的<strong class="ld ir">损失</strong>。</p><p id="8279" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">最后，我们计算所有这些损失的<strong class="ld ir">平均值。</strong></p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/6bc8ec3d83eb783d4fe9df12c1a32623.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*i9EPio8R8j1Dd5kdaLy6pQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 10: finally, the loss!</figcaption></figure><p id="b5ff" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">瞧！我们已经成功计算出这个玩具例子的<strong class="ld ir">二元交叉熵</strong> / <strong class="ld ir">对数损失</strong>。<strong class="ld ir">是 0.3329！</strong></p><h1 id="23b9" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">给我看看代码</h1><p id="d9bd" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如果您想<strong class="ld ir">再次检查我们找到的值</strong>，只需<strong class="ld ir">运行下面的代码</strong>并自己查看:-)</p><figure class="mk ml mm mn gt jr"><div class="bz fp l di"><div class="ne nf l"/></div></figure><h1 id="3e51" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">给我看看数学(真的？！)</h1><p id="831e" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">玩笑归玩笑，这篇文章<strong class="ld ir">不是</strong>旨在非常数学化…但是对于那些想要理解<strong class="ld ir">熵</strong>、<strong class="ld ir">对数</strong>在所有这些中的作用的读者，我们开始吧:-)</p><blockquote class="mw mx my"><p id="39f6" class="lb lc mi ld b le lz lg lh li ma lk ll mz mb lo lp na mc ls lt nb md lw lx ly ij bi translated">如果你想更深入地了解<strong class="ld ir">信息论</strong>，包括所有这些概念——熵、交叉熵以及更多更多——请查看<strong class="ld ir">克里斯·奥拉的</strong> <a class="ae kc" href="http://colah.github.io/posts/2015-09-Visual-Information/" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">帖子</strong> </a>查看，它非常详细！</p></blockquote><h2 id="8e13" class="ng ke iq bd kf nh ni dn kj nj nk dp kn lm nl nm kr lq nn no kv lu np nq kz nr bi translated"><strong class="ak">分配</strong></h2><p id="7066" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">先说我们的积分分布。由于<strong class="ld ir"> y </strong>代表我们点的<strong class="ld ir">类</strong>(我们有<strong class="ld ir"> 3 个红点</strong>和<strong class="ld ir"> 7 个绿点</strong>，这就是它的分布，姑且称之为<strong class="ld ir"> q(y) </strong>，看起来像是:</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/3642f70183b40e5885914763935e6c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*QTufu69NX4W9tZQD1AduQQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 11: q(y), the distribution of our points</figcaption></figure><h2 id="6b8c" class="ng ke iq bd kf nh ni dn kj nj nk dp kn lm nl nm kr lq nn no kv lu np nq kz nr bi translated">熵</h2><p id="acca" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="ld ir">熵</strong>是与给定分布<strong class="ld ir"> q(y) </strong>相关的不确定性的<strong class="ld ir">度量。</strong></p><p id="92f5" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">如果<strong class="ld ir">我们所有的点</strong>都是<strong class="ld ir">绿色</strong>会怎么样？<strong class="ld ir">的<strong class="ld ir">不确定性</strong>会是什么样的</strong>分布？<strong class="ld ir">零</strong>对吧？毕竟，一个点的颜色是毫无疑问的:它<strong class="ld ir">总是</strong>绿色！所以，<strong class="ld ir">熵为零</strong>！</p><p id="4b38" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">另一方面，如果我们确切地知道<strong class="ld ir">一半的点</strong>是<strong class="ld ir">绿色</strong>而<strong class="ld ir">的另一半是</strong>、<strong class="ld ir">红色</strong>会怎么样？这是最糟糕的情况，对吗？我们在猜测一个点的颜色上绝对没有优势:这完全是随机的！在这种情况下，熵由下面的公式给出(<em class="mi">我们有两类(颜色)——红色或绿色——因此，</em> <strong class="ld ir"> <em class="mi"> 2 </em> </strong>):</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/4a9172a6155e1d2da65842a47b50f949.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*1R1M9mDGxcrN3tGy8M9atw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Entropy for a half-half distribution</figcaption></figure><p id="ccc1" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">对于在之间的<strong class="ld ir">每隔一种情况，我们可以计算分布</strong>的<strong class="ld ir">熵，就像我们的<strong class="ld ir"> q(y) </strong>一样，使用下面的公式，其中<em class="mi"> C </em>是类的数量:</strong></p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/bdc357d790040ffba5f3ca901b86909e.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*Y0OJOAehqME6ePORQzzliQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Entropy</figcaption></figure><p id="593f" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">所以，如果我们<em class="mi">知道</em>一个随机变量的<strong class="ld ir">真实分布</strong>，我们就可以计算出它的<strong class="ld ir">熵</strong>。但是，如果是这样的话，<em class="mi">为什么首先要费心训练一个分类器</em>？毕竟，我们<strong class="ld ir">知道</strong>真实的分布…</p><p id="879d" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">但是，如果我们<strong class="ld ir">不</strong>呢？我们能否尝试用一些<strong class="ld ir">其他分布</strong>来近似真实分布，比如说<strong class="ld ir"> p(y) </strong>？我们当然可以！:-)</p><h2 id="073c" class="ng ke iq bd kf nh ni dn kj nj nk dp kn lm nl nm kr lq nn no kv lu np nq kz nr bi translated">交叉熵</h2><p id="45d9" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们假设我们的<strong class="ld ir">分</strong> <strong class="ld ir">跟随</strong>这个<strong class="ld ir">其他</strong>分布<strong class="ld ir"> p(y) </strong>。但是我们知道他们是从<strong class="ld ir">真</strong> ( <em class="mi">未知</em>)分布<strong class="ld ir"> q(y) </strong>实际上来的对吧？</p><p id="9aad" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">如果我们像这样计算<strong class="ld ir">熵</strong>，我们实际上是在计算两个分布之间的<strong class="ld ir">交叉熵</strong>:</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/dd898d2ce13d3498b29ac4ff81aec285.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*loucyTXzGHuHi6D4PxjDlA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Cross-Entropy</figcaption></figure><p id="8fb1" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">如果我们奇迹般地将<em class="mi">与</em> <strong class="ld ir"> p(y) </strong> <em class="mi">到</em> <strong class="ld ir"> q(y) </strong> <em class="mi">完美匹配</em>，那么<strong class="ld ir">交叉熵</strong> <em class="mi">和</em> <strong class="ld ir">熵</strong> <em class="mi">的计算值也将与</em>匹配。</p><p id="d03f" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">因为这很可能永远不会发生，<strong class="ld ir">交叉熵的值将大于对真实分布计算的熵</strong>。</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/3c8c052f16a7cb1933c99588f61f2638.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*gddnak1nw5kl5azS_fQxVg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Cross-Entropy minus Entropy</figcaption></figure><p id="78e6" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">原来，<strong class="ld ir">交叉熵</strong>和<strong class="ld ir">熵</strong>之间的差异有一个名字…</p><h2 id="b80e" class="ng ke iq bd kf nh ni dn kj nj nk dp kn lm nl nm kr lq nn no kv lu np nq kz nr bi translated">库尔贝克-莱布勒散度</h2><p id="1acd" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="ld ir"> Kullback-Leibler 散度</strong>，或简称为“<strong class="ld ir"> <em class="mi"> KL 散度</em> </strong>，是两个分布之间<strong class="ld ir">相异度</strong>的度量:</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/101953c32a090bf65707eba2037e3549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*WGUAYepfkWSKj9RZpQE2Jw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">KL Divergence</figcaption></figure><p id="c80f" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">这意味着，p(y)越接近 q(y) ，<strong class="ld ir">越低</strong>，<strong class="ld ir">散度</strong>，因此，<strong class="ld ir">交叉熵</strong>，将是。</p><p id="f5b5" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">所以，我们需要找一个好的<strong class="ld ir"> p(y) </strong>来用……但是，这是我们<strong class="ld ir">分类器</strong>应该做的，不是吗？！<strong class="ld ir">的确如此</strong>！它寻找<strong class="ld ir">最可能的</strong> <strong class="ld ir"> p(y) </strong>，也就是<strong class="ld ir">最小化交叉熵</strong>的那个。</p><h2 id="8d08" class="ng ke iq bd kf nh ni dn kj nj nk dp kn lm nl nm kr lq nn no kv lu np nq kz nr bi translated">损失函数</h2><p id="da00" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在其训练期间，<strong class="ld ir">分类器</strong>使用其训练集中的<strong class="ld ir"> N 个点</strong>中的每一个来计算<strong class="ld ir">交叉熵</strong>损失，有效地<strong class="ld ir">拟合分布 p(y) </strong>！由于每个点的概率是 1/N，交叉熵由下式给出:</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/6792b8fb351f28680f31e28916677a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*zT0aUONCe8_8WciINwDLIQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Cross-Entropy —point by point</figcaption></figure><p id="7601" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">还记得上面的图 6 到 10 吗？我们需要在与每个点的真实类别相关联的<em class="mi">概率之上计算<strong class="ld ir">交叉熵</strong>。这意味着使用<strong class="ld ir">绿色条</strong>用于<strong class="ld ir">正级</strong> ( <em class="mi"> y=1 </em>)中的点，使用<strong class="ld ir">红色<em class="mi">悬挂</em>条</strong>用于<strong class="ld ir">负级</strong> <strong class="ld ir">级</strong> ( <em class="mi"> y=0 </em>)中的点，或者从数学上来说:</em></p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/6e9773dd893a3f67769eb15f76e82c77.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*2NqGP3bk6DOD8nq51rLdWg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Mathematical expression corresponding to Figure 10 :-)</figcaption></figure><p id="cb92" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">最后一步是计算两个类别中所有点的<strong class="ld ir">平均值</strong>，<strong class="ld ir">正值</strong>和<strong class="ld ir">负值</strong>:</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/1f9c94c797808589a99ea62270f755f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*FC71vSkV7G0Sj3nOnFwJQw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Binary Cross-Entropy — computed over positive and negative classes</figcaption></figure><p id="f20c" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">最后，通过一点点操作，我们可以从正类或负类<strong class="ld ir">中取任意一点，在相同的公式下:</strong></p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/7022e55b09bc4b4050d3387fbb99bc01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*rdBw0E-My8Gu3f_BOB6GMA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Binary Cross-Entropy — the usual formula</figcaption></figure><p id="6c56" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir">瞧吧</strong>！我们回到了<strong class="ld ir">原始公式</strong>用于<strong class="ld ir">二元交叉熵/对数损失</strong> :-)</p><h1 id="7bb8" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">最后的想法</h1><p id="bf19" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我真心希望这篇文章能够<strong class="ld ir">在一个经常被认为是理所当然的概念上</strong>闪耀一些新的光芒，这个概念就是<strong class="ld ir">二元交叉熵</strong>作为<strong class="ld ir">损失函数</strong>。此外，我也希望它能向你展示一点点<strong class="ld ir">机器学习</strong>和<strong class="ld ir">信息论</strong>是如何联系在一起的。</p><blockquote class="mw mx my"><p id="0a39" class="lb lc mi ld b le lz lg lh li ma lk ll mz mb lo lp na mc ls lt nb md lw lx ly ij bi translated">2022 年 7 月 10 日更新:我刚刚在 YouTube 上发布了一个视频，里面有这篇文章的动画版本——来看看吧！</p></blockquote><figure class="mk ml mm mn gt jr"><div class="bz fp l di"><div class="ob nf l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Understanding Binary Cross-Entropy / Log Loss in 5 Minutes!</figcaption></figure><p id="4ffd" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><em class="mi">如果你有什么想法、意见或问题，欢迎在下方留言评论或联系我</em> <a class="ae kc" href="https://twitter.com/dvgodoy" rel="noopener ugc nofollow" target="_blank"> <em class="mi">推特</em> </a> <em class="mi">。</em></p></div></div>    
</body>
</html>