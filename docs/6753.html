<html>
<head>
<title>Modality tests and kernel density estimations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">模态测试和核密度估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/modality-tests-and-kernel-density-estimations-3f349bb9e595?source=collection_archive---------6-----------------------#2018-12-30">https://towardsdatascience.com/modality-tests-and-kernel-density-estimations-3f349bb9e595?source=collection_archive---------6-----------------------#2018-12-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="29bc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">python 中的多模态测试</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/585bda70d6c1256e3a3b1f1d6b07ed44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i0cmoyvC0tWgCQ66y6LPMw.jpeg"/></div></div></figure><p id="5631" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当处理大量可能具有不同数据分布的数据集时，我们面临以下考虑因素:</p><ul class=""><li id="98cc" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">数据分布是单峰的吗？如果是，哪种模型最接近它(均匀分布、T 分布、卡方分布、柯西分布等)？</li><li id="374b" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">如果数据分布是多模态的，我们能否自动识别模态的数量并提供更细粒度的描述性统计？</li><li id="4968" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">如何估计一个新数据集的概率密度函数？</li></ul><p id="ca82" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">本<a class="ae mb" href="https://github.com/ciortanmadalina/modality_tests/blob/master/kernel_density.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>处理以下主题:</p><ul class=""><li id="023e" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">直方图与概率密度函数近似</li><li id="a95a" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">核密度估计</li><li id="2ca1" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">最佳带宽的选择:Silverman/ Scott/ Grid 搜索交叉验证</li><li id="1097" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">单峰分布的统计检验</li><li id="2d2d" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">单峰性倾斜试验</li><li id="6a40" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">基于核密度估计的数据分布模式数识别</li></ul><h1 id="0455" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">直方图和 pdf</h1><p id="1df8" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">正如这篇博文<a class="ae mb" href="https://mglerner.github.io/posts/histograms-and-kernel-density-estimation-kde-2.html" rel="noopener ugc nofollow" target="_blank">中所解释的，https://mgl Lerner . github . io/posts/histograms-and-kernel-density-estimation-kde-2 . html</a>直方图的缺点是在不合适大小的容器中隐藏了实际数据分布的一些细节。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="dded" class="ne md iq na b gy nf ng l nh ni">def plotHistogramAndPdf(data, x, pdf):<br/>    ax = plt.gca()<br/>    plt.hist(data, bins = 4, alpha = 0.4, label = 'histogram of input values');<br/>    plt.ylabel('Frequency')<br/>    plt.xlabel('x values')<br/>    ax2 = ax.twinx()<br/>    plt.plot(x, pdf, c = 'red', label = 'probability density function');<br/>    plt.ylabel('PDF')<br/>    [tl.set_color('r') for tl in ax2.get_yticklabels()]<br/>    ax.legend(bbox_to_anchor=(0.4, 1.15))<br/>    ax2.legend(bbox_to_anchor=(1.15,1.15))<br/>    plt.savefig('figures/hist.jpg', bbox_inches='tight')<br/>    <br/>plotHistogramAndPdf(data, x, true_pdf)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/8857c2b660c9996eb751fe1a80e4f183.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*dw3aH7IbgqGzdpxCsfQPow.jpeg"/></div></figure><h1 id="953f" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">核密度估计</h1><p id="cefa" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">核密度估计依赖于任意带宽，该带宽决定了返回的近似的平滑程度。以下示例说明了各种带宽值的影响:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="eae8" class="ne md iq na b gy nf ng l nh ni">def getKernelDensityEstimation(values, x, bandwidth = 0.2, kernel = 'gaussian'):<br/>    model = KernelDensity(kernel = kernel, bandwidth=bandwidth)<br/>    model.fit(values[:, np.newaxis])<br/>    log_density = model.score_samples(x[:, np.newaxis])<br/>    return np.exp(log_density)</span><span id="1a16" class="ne md iq na b gy nk ng l nh ni">for bandwidth in np.linspace(0.2, 3, 3):<br/>    kde = getKernelDensityEstimation(data, x, bandwidth=bandwidth)<br/>    plt.plot(x, kde, alpha = 0.8, label = f'bandwidth = {round(bandwidth, 2)}')<br/>plt.plot(x, true_pdf, label = 'True PDF')<br/>plt.legend()<br/>plt.title('Effect of various bandwidth values \nThe larger the bandwidth, the smoother the approximation becomes');<br/>plt.savefig('figures/bw.jpg', bbox_inches='tight')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/12ea3ef22278b236c48d8892e1490211.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*s3rT4rfbQ2A4zs5ApEe_Cg.jpeg"/></div></figure><h1 id="5e40" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">核密度估计最佳带宽的选择方法</h1><p id="08f7" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">为了确定最佳带宽，有几种方法:</p><ul class=""><li id="0e9c" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">Silverman 的经验法则:假设未知密度为高斯分布。它不是最佳带宽选择器，但可以用作非常快速、相当好的估计器，或者用作多级带宽选择器中的第一估计器。更精确的求解方程插件规则使用积分平方密度导数泛函的估计来估计最佳带宽。它们需要大量计算来使用迭代方法求解非线性方程。他们用腐烂作为第一估计</li><li id="ef00" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">Scott 的经验法则:对于正态分布数据的随机样本是最佳的，在某种意义上，它最小化了密度估计的积分均方误差。</li></ul><p id="ac71" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这两种方法具有计算速度快的优点，但是它们通常给出太少的面元，并且很可能对底层数据分布进行欠拟合。这两种方法都已经在 statsmodels 包中实现，如下图所示。</p><ul class=""><li id="c2e7" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">基于交叉验证的方法:statsmodels 带有一个 cv 带宽参数。或者，我们可以实现网格搜索交叉验证。与前两种方法不同，执行网格搜索可能需要更多的计算，尤其是对于较大的数据集</li></ul><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="8d06" class="ne md iq na b gy nf ng l nh ni">from statsmodels.nonparametric.bandwidths import bw_silverman, bw_scott, select_bandwidth</span><span id="9c47" class="ne md iq na b gy nk ng l nh ni">silverman_bandwidth = bw_silverman(data)</span><span id="b95e" class="ne md iq na b gy nk ng l nh ni"># select bandwidth allows to set a different kernel<br/>silverman_bandwidth_gauss = select_bandwidth(data, bw = 'silverman', kernel = 'gauss')</span><span id="3a18" class="ne md iq na b gy nk ng l nh ni">scott_bandwidth = bw_scott(data)</span><span id="dd71" class="ne md iq na b gy nk ng l nh ni">def bestBandwidth(data, minBandwidth = 0.1, maxBandwidth = 2, nb_bandwidths = 30, cv = 30):<br/>    """<br/>    Run a cross validation grid search to identify the optimal bandwidth for the kernel density<br/>    estimation.<br/>    """<br/>    from sklearn.model_selection import GridSearchCV<br/>    model = GridSearchCV(KernelDensity(),<br/>                        {'bandwidth': np.linspace(minBandwidth, maxBandwidth, nb_bandwidths)}, cv=cv) <br/>    model.fit(data[:, None])<br/>    return model.best_params_['bandwidth']</span><span id="45d7" class="ne md iq na b gy nk ng l nh ni">cv_bandwidth = bestBandwidth(data)</span><span id="5384" class="ne md iq na b gy nk ng l nh ni">print(f"Silverman bandwidth = {silverman_bandwidth}")<br/>print(f"Scott bandwidth = {scott_bandwidth}")<br/>print(f"CV bandwidth = {cv_bandwidth}")</span></pre><p id="310d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如预期的那样，第一个 Silverman 和 Scott 返回了更大的带宽值，这导致了更大的箱，从而丢失了关于数据分布的信息。</p><p id="c8c8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">Statsmodels 允许基于交叉验证和最大似然运算符自动搜索最佳带宽:</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="5d8e" class="ne md iq na b gy nf ng l nh ni">from statsmodels.nonparametric.kernel_density import KDEMultivariate<br/>stats_models_cv = KDEMultivariate(data, 'c', bw = 'cv_ml').pdf(x)</span></pre><h1 id="b2f9" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">画出不同的近似值</h1><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="d379" class="ne md iq na b gy nf ng l nh ni">plt.figure(figsize= (14, 6))<br/>plt.plot(x, true_pdf, label = 'True PDF')</span><span id="11cb" class="ne md iq na b gy nk ng l nh ni">kde = getKernelDensityEstimation(data, x, bandwidth=silverman_bandwidth)<br/>plt.plot(x, kde, alpha = 0.8, label = f'Silverman bandwidth')</span><span id="9ef6" class="ne md iq na b gy nk ng l nh ni">kde = getKernelDensityEstimation(data, x, bandwidth=scott_bandwidth)<br/>plt.plot(x, kde, alpha = 0.8, label = f'Scott bandwidth')</span><span id="db2e" class="ne md iq na b gy nk ng l nh ni">kde = getKernelDensityEstimation(data, x, bandwidth=cv_bandwidth)<br/>plt.plot(x, kde, alpha = 0.8, label = f'CV bandwidth')</span><span id="b502" class="ne md iq na b gy nk ng l nh ni">plt.plot(x, stats_models_cv, alpha = 0.8, label = f'Statsmodels CV maximum likelihood')</span><span id="93c9" class="ne md iq na b gy nk ng l nh ni">plt.legend()<br/>plt.title('Comparative of various bandwidth estimations for KDE');<br/>plt.savefig('figures/comp_bw.jpg', bbox_inches='tight')<br/></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/632115e14acb7b7f40f5a87a60b6f47b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rM5YFxd-tvw1RYWlodpiAg.jpeg"/></div></div></figure><h1 id="a143" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">单峰分布的统计检验</h1><p id="e361" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">有许多统计测试可以解决数据形态问题:</p><ul class=""><li id="3852" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">倾斜试验</li><li id="880f" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">过量质量测试</li><li id="f459" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">地图测试</li><li id="41cc" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">模式存在测试</li><li id="c589" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">矮小测试</li><li id="9d95" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">跨度测试</li><li id="8eab" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">马鞍试验</li></ul><p id="01ce" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">不幸的是，在 python 开源库中实现的并不多。</p><h1 id="7d5d" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">倾斜试验</h1><p id="2dab" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">以下 python 包<a class="ae mb" href="https://github.com/BenjaminDoran/unidip" rel="noopener ugc nofollow" target="_blank">https://github.com/BenjaminDoran/unidip</a>提供了倾角测试的实现，以及利用单峰性的哈迪根倾角测试在数据中广泛提取密度峰值的功能。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="92ca" class="ne md iq na b gy nf ng l nh ni">from unidip import UniDip<br/>import unidip.dip as dip</span><span id="7047" class="ne md iq na b gy nk ng l nh ni">data = np.msort(data)<br/>print(dip.diptst(data))<br/>intervals = UniDip(data).run()<br/>print(intervals)</span></pre><h1 id="8b55" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">确定并绘制 KDE 的局部最大值</h1><p id="94f5" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">一旦我们有了核密度函数的估计，我们就可以确定该分布是否是多峰的，并识别对应于这些模式的最大值或峰值。<br/>这可以通过识别一阶导数改变符号的点来实现。默认情况下，getinflexinpoints 方法可以返回所有拐点(最小值+最大值)，或者只返回一个选择(typeOfInflexion = 'max'/ 'min ')。<br/>下图描绘了可能对应于多种数据分布模式的最大值。可以通过基于峰的高度设置阈值来继续分析，以便过滤掉一些不太重要的值。</p><pre class="kg kh ki kj gt mz na nb nc aw nd bi"><span id="761c" class="ne md iq na b gy nf ng l nh ni">def getExtremePoints(data, typeOfExtreme = None, maxPoints = None):<br/>    """<br/>    This method returns the indeces where there is a change in the trend of the input series.<br/>    typeOfExtreme = None returns all extreme points, max only maximum values and min<br/>    only min,<br/>    """<br/>    a = np.diff(data)<br/>    asign = np.sign(a)<br/>    signchange = ((np.roll(asign, 1) - asign) != 0).astype(int)<br/>    idx = np.where(signchange ==1)[0]</span><span id="21a7" class="ne md iq na b gy nk ng l nh ni">if typeOfInflexion == 'max' and data[idx[0]] &lt; data[idx[1]]:<br/>        idx = idx[1:][::2]<br/>        <br/>    elif typeOfInflexion == 'min' and data[idx[0]] &gt; data[idx[1]]:<br/>        idx = idx[1:][::2]<br/>    elif typeOfInflexion is not None:<br/>        idx = idx[::2]<br/>    <br/>    # sort ids by min value<br/>    if 0 in idx:<br/>        idx = np.delete(idx, 0)<br/>    if (len(data)-1) in idx:<br/>        idx = np.delete(idx, len(data)-1)<br/>    idx = idx[np.argsort(data[idx])]<br/>    # If we have maxpoints we want to make sure the timeseries has a cutpoint<br/>    # in each segment, not all on a small interval<br/>    if maxPoints is not None:<br/>        idx= idx[:maxPoints]<br/>        if len(idx) &lt; maxPoints:<br/>            return (np.arange(maxPoints) + 1) * (len(data)//(maxPoints + 1))<br/>    <br/>    return idx</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/68628a898aee7ab3c26b41e4fb748504.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*wGgA3P46hR98puvem45Q6w.jpeg"/></div></figure><p id="8ab1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们注意到获得的值对应于生成的分布的初始锚。</p><h1 id="ed5e" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">资源</h1><ul class=""><li id="672e" class="ln lo iq kt b ku mu kx mv la no le np li nq lm ls lt lu lv bi translated"><a class="ae mb" href="https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/" rel="noopener ugc nofollow" target="_blank">https://jakevdp . github . io/blog/2013/12/01/kernel-density-estimation/</a></li><li id="f7ea" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><a class="ae mb" href="https://mglerner.github.io/posts/histograms-and-kernel-density-estimation-kde-2.html" rel="noopener ugc nofollow" target="_blank">https://mglerner . github . io/posts/histograms-and-kernel-density-estimation-kde-2 . html</a></li><li id="a12c" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><a class="ae mb" href="https://en.wikipedia.org/wiki/Multimodal_distribution" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Multimodal_distribution</a></li></ul></div></div>    
</body>
</html>