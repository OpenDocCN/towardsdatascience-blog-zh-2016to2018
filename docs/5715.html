<html>
<head>
<title>Installing Hadoop 3.1.0 multi-node cluster on Ubuntu 16.04 Step by Step</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Ubuntu 16.04 上逐步安装 Hadoop 3.1.0 多节点集群</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/installing-hadoop-3-1-0-multi-node-cluster-on-ubuntu-16-04-step-by-step-8d1954b31505?source=collection_archive---------2-----------------------#2018-11-05">https://towardsdatascience.com/installing-hadoop-3-1-0-multi-node-cluster-on-ubuntu-16-04-step-by-step-8d1954b31505?source=collection_archive---------2-----------------------#2018-11-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/0ae99f030bdb6eadbf044ca1bb3bb4ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jkRHs8CMv7KOIYINaktmgA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Image Source: www.<a class="ae kc" href="https://mapr.com/products/apache-hadoop/" rel="noopener ugc nofollow" target="_blank">mapr.com/products/apache-hadoop/</a></figcaption></figure><p id="3de7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">网上有很多关于安装 Hadoop 3 的链接。他们中的许多人工作不好或需要改进。这篇文章摘自官方文件和其他文章以及 Stackoverflow.com 的许多回答</p><h1 id="103a" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">1.先决条件</h1><p id="cace" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir"> <em class="me">注意:所有先决条件必须应用于名称节点和数据节点</em> </strong></p><h1 id="8880" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">1.1.安装 JAVA、SSH 和其他软件实用程序</h1><p id="3824" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">首先，我们需要为 Java 8 安装 SSH 和一些软件安装实用程序:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="4dda" class="mo lc iq mk b gy mp mq l mr ms">sudo apt install \</span><span id="3b15" class="mo lc iq mk b gy mt mq l mr ms">openssh-server \</span><span id="fff1" class="mo lc iq mk b gy mt mq l mr ms">software-properties-common \</span><span id="a623" class="mo lc iq mk b gy mt mq l mr ms">python-software-properties</span></pre><p id="d1c3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们需要安装 Oracle 的 Java 8 发行版，并更新当前的 OS。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="8cc5" class="mo lc iq mk b gy mp mq l mr ms">sudo add-apt-repository ppa:webupd8team/java</span><span id="26b1" class="mo lc iq mk b gy mt mq l mr ms">sudo apt update</span><span id="f1d4" class="mo lc iq mk b gy mt mq l mr ms">sudo apt install oracle-java8-installer</span></pre><p id="9643" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要验证 java 版本，您可以使用以下命令:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="9d73" class="mo lc iq mk b gy mp mq l mr ms">java -version</span></pre><h1 id="7a83" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">1.2.为 Hadoop 创建专门的用户和组</h1><p id="7c3b" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们将使用专用的 Hadoop 用户帐户来运行 Hadoop 应用程序。虽然这不是必需的，但建议这样做，因为它有助于将 Hadoop 安装与同一台机器上运行的其他软件应用程序和用户帐户(安全、权限、备份等)分开。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="40bc" class="mo lc iq mk b gy mp mq l mr ms">sudo addgroup hadoopgroup</span><span id="797b" class="mo lc iq mk b gy mt mq l mr ms">sudo adduser --ingroup hadoopgroup hadoopuser</span><span id="7d11" class="mo lc iq mk b gy mt mq l mr ms">sudo adduser hadoopuser sudo</span></pre><p id="1ffe" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以使用以下命令检查组和用户:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="14e8" class="mo lc iq mk b gy mp mq l mr ms">compgen -g</span><span id="7bdc" class="mo lc iq mk b gy mt mq l mr ms">compgen -u</span></pre><h1 id="81a6" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">1.3.SSH 配置</h1><p id="d821" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">Hadoop 需要 SSH 访问来管理其不同的节点，即远程机器加上您的本地机器。</p><p id="144e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，您需要以 Hadoopuser 的身份登录</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="a224" class="mo lc iq mk b gy mp mq l mr ms">sudo su -- hadoopuser</span></pre><p id="8713" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下命令用于使用 SSH 生成一个键值对</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="a45f" class="mo lc iq mk b gy mp mq l mr ms">ssh-keygen -t rsa -P “” -f ~/.ssh/id_rsa</span></pre><p id="5b37" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将 id_rsa.pub 中的公钥复制到 authorized_keys 中。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="3ceb" class="mo lc iq mk b gy mp mq l mr ms">cat ~/.ssh/id_rsa.pub&gt;&gt; ~/.ssh/authorized_keys</span><span id="b7d2" class="mo lc iq mk b gy mt mq l mr ms">chmod 0600 ~/.ssh/authorized_keys</span></pre><p id="b2c7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">确保 hadoopuser 可以 ssh 到自己的帐户，无需密码。从 hadoopuser 帐户 ssh 到 localhost，以确保它工作正常。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="6a71" class="mo lc iq mk b gy mp mq l mr ms">ssh localhost</span></pre><p id="ffa0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="me">注意:如果你得到错误:ssh:连接到主机本地主机端口 22:连接被拒绝，那么，请尝试使用下面的命令安装 ssh-server。</em></p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="202b" class="mo lc iq mk b gy mp mq l mr ms">sudo apt-get install ssh</span></pre><h1 id="5f2f" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">2.下载并配置 Hadoop</h1><p id="7ac0" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">在本文中，我们将在三台机器上安装 Hadoop:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/ac8aa689832a3f2f07f5671de2eb41df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*j9zDlz7YQo8zHNfhHgbDhA.png"/></div></figure><p id="fea1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一台机器将充当名称节点(主节点)和数据节点(从节点)，其他机器是数据节点(从节点)</p><p id="0d2d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每台机器上，我们必须使用以下命令编辑/etc/hosts 文件</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="aa6a" class="mo lc iq mk b gy mp mq l mr ms">sudo gedit /etc/hosts</span></pre><p id="05e1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个文件必须包含这些行:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="80e7" class="mo lc iq mk b gy mp mq l mr ms">127.0.0.1 localhost</span><span id="212e" class="mo lc iq mk b gy mt mq l mr ms">10.0.1.1 hadoop-namenode</span><span id="192d" class="mo lc iq mk b gy mt mq l mr ms">10.0.1.2 hadoop-datanode-2</span><span id="3087" class="mo lc iq mk b gy mt mq l mr ms">10.0.1.3 hadoop-datadnode-3</span></pre><p id="7a0a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="me">注意:如果/etc/hosts 文件包含下面一行</em></p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="8253" class="mo lc iq mk b gy mp mq l mr ms">127.0.1.1 &lt;Hostname&gt;</span></pre><p id="5036" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后你要删除这一行。</p><h1 id="441a" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">2.1.下载 Hadoop</h1><p id="9e39" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们将把所有软件安装在/opt 目录下，并将 HDFS 的底层数据也存储在那里。下面我们将用一个命令创建文件夹。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="e77f" class="mo lc iq mk b gy mp mq l mr ms">sudo mkdir -p /opt/{hadoop/{logs},hdfs/{datanode,namenode},yarn/{logs}</span></pre><p id="7b32" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">文件夹的布局将如下所示</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="429d" class="mo lc iq mk b gy mp mq l mr ms">/opt/</span><span id="7306" class="mo lc iq mk b gy mt mq l mr ms">├── hadoop</span><span id="5ac1" class="mo lc iq mk b gy mt mq l mr ms">│ ├── logs</span><span id="5c54" class="mo lc iq mk b gy mt mq l mr ms">├── hdfs</span><span id="04f9" class="mo lc iq mk b gy mt mq l mr ms">│ ├── datanode</span><span id="f3cb" class="mo lc iq mk b gy mt mq l mr ms">│ └── namenode</span><span id="6eed" class="mo lc iq mk b gy mt mq l mr ms">├── yarn</span><span id="4798" class="mo lc iq mk b gy mt mq l mr ms">│ ├── logs</span></pre><p id="bb4e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以使用以下命令下载 hadoop-3.1.0.tar.gz:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="a10a" class="mo lc iq mk b gy mp mq l mr ms">wget -c -O hadoop.tar.gz <a class="ae kc" href="http://www-eu.apache.org/dist/hadoop/common/hadoop-3.1.0/hadoop-3.1.0.tar.gz" rel="noopener ugc nofollow" target="_blank">http://www-eu.apache.org/dist/hadoop/common/hadoop-3.1.0/hadoop-3.1.0.tar.gz</a></span></pre><p id="0b7b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要解压缩 Hadoop 包，您可以使用以下命令:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="0f64" class="mo lc iq mk b gy mp mq l mr ms">sudo tar -xvf hadoop.tar.gz \</span><span id="6472" class="mo lc iq mk b gy mt mq l mr ms"> --directory=/opt/hadoop \</span><span id="6b1a" class="mo lc iq mk b gy mt mq l mr ms">--strip 1</span></pre><p id="0648" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Hadoop 3 的二进制版本压缩了 293 MB。它的解压缩大小为 733 MB，其中有 400 MB 的小文档文件，解压缩可能需要很长时间。您可以通过在上面的命令中添加以下行来跳过这些文件:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="2615" class="mo lc iq mk b gy mp mq l mr ms">--exclude=hadoop-3.1.0/share/doc</span></pre><h1 id="c66a" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">2.2.Hadoop 常见配置</h1><p id="6a81" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir"> <em class="me">注意:这些步骤必须在名称节点和数据节点上完成。</em>T11】</strong></p><p id="d52e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有一些环境设置将被 Hadoop、Hive 和 Spark 使用，并由 root 和普通用户帐户共享。为了集中这些设置，我将它们存储在/etc/profile 中，并从/root/创建了一个符号链接。bashrc 到这个文件。这样，所有用户都将拥有集中管理的设置。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="5d98" class="mo lc iq mk b gy mp mq l mr ms">sudo gedit /etc/profile</span></pre><p id="acda" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">/etc/profile 必须类似于:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="a1ac" class="mo lc iq mk b gy mp mq l mr ms">if [ “$PS1” ]; then</span><span id="0387" class="mo lc iq mk b gy mt mq l mr ms">if [ “$BASH” ] &amp;&amp; [ “$BASH” != “/bin/sh” ]; then</span><span id="4ca3" class="mo lc iq mk b gy mt mq l mr ms"># The file bash.bashrc already sets the default PS1.</span><span id="002c" class="mo lc iq mk b gy mt mq l mr ms"># PS1=’\h:\w\$ ‘</span><span id="c1fa" class="mo lc iq mk b gy mt mq l mr ms">if [ -f /etc/bash.bashrc ]; then</span><span id="b188" class="mo lc iq mk b gy mt mq l mr ms">. /etc/bash.bashrc</span><span id="89e9" class="mo lc iq mk b gy mt mq l mr ms">fi</span><span id="a1f1" class="mo lc iq mk b gy mt mq l mr ms">else</span><span id="40c7" class="mo lc iq mk b gy mt mq l mr ms">if [ “`id -u`” -eq 0 ]; then</span><span id="4607" class="mo lc iq mk b gy mt mq l mr ms">PS1=’# ‘</span><span id="f240" class="mo lc iq mk b gy mt mq l mr ms">else</span><span id="ef7c" class="mo lc iq mk b gy mt mq l mr ms">PS1=’$ ‘</span><span id="a3e0" class="mo lc iq mk b gy mt mq l mr ms">fi</span><span id="049f" class="mo lc iq mk b gy mt mq l mr ms">fi</span><span id="c607" class="mo lc iq mk b gy mt mq l mr ms">fi</span><span id="5e88" class="mo lc iq mk b gy mt mq l mr ms">if [ -d /etc/profile.d ]; then</span><span id="c2a7" class="mo lc iq mk b gy mt mq l mr ms">for i in /etc/profile.d/*.sh; do</span><span id="3f99" class="mo lc iq mk b gy mt mq l mr ms">if [ -r $i ]; then</span><span id="8be9" class="mo lc iq mk b gy mt mq l mr ms">. $i</span><span id="482e" class="mo lc iq mk b gy mt mq l mr ms">fi</span><span id="cbc2" class="mo lc iq mk b gy mt mq l mr ms">done</span><span id="8503" class="mo lc iq mk b gy mt mq l mr ms">unset i</span><span id="7d70" class="mo lc iq mk b gy mt mq l mr ms">fi</span><span id="eba3" class="mo lc iq mk b gy mt mq l mr ms">export HADOOP_HOME=/opt/hadoop</span><span id="1c5a" class="mo lc iq mk b gy mt mq l mr ms">export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><span id="e4d5" class="mo lc iq mk b gy mt mq l mr ms">export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop</span><span id="2f9d" class="mo lc iq mk b gy mt mq l mr ms">export HDFS_NAMENODE_USER=root</span><span id="407c" class="mo lc iq mk b gy mt mq l mr ms">export HDFS_DATANODE_USER=root</span><span id="1b44" class="mo lc iq mk b gy mt mq l mr ms">export HDFS_SECONDARYNAMENODE_USER=root</span><span id="7657" class="mo lc iq mk b gy mt mq l mr ms">export JAVA_HOME=/usr/lib/jvm/java-8-oracle</span><span id="a633" class="mo lc iq mk b gy mt mq l mr ms">export HADOOP_MAPRED_HOME=/opt/hadoop</span><span id="8548" class="mo lc iq mk b gy mt mq l mr ms">export HADOOP_COMMON_HOME=/opt/hadoop</span><span id="e6ca" class="mo lc iq mk b gy mt mq l mr ms">export HADOOP_HDFS_HOME=/opt/hadoop</span><span id="7a48" class="mo lc iq mk b gy mt mq l mr ms">export YARN_HOME=/opt/hadoop</span></pre><p id="b01f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下命令将在~/之间创建一个符号链接。bashrc 和/etc/profile，并应用对/etc/profile 所做的更改</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="0ed1" class="mo lc iq mk b gy mp mq l mr ms">sudo ln -sf /etc/profile /root/.bashrc</span><span id="aaec" class="mo lc iq mk b gy mt mq l mr ms">source /etc/profile</span></pre><p id="3846" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更新/opt/HADOOP/etc/HADOOP/HADOOP-env . sh 文件并设置 JAVA_HOME 变量和 HADOOP_HOME、HADOOP _ CONF _ 目录和 HADOOP _ LOG _ 目录变量</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="172b" class="mo lc iq mk b gy mp mq l mr ms">export JAVA_HOME=/usr/lib/jvm/java-8-oracle</span><span id="8eac" class="mo lc iq mk b gy mt mq l mr ms">export HADOOP_HOME=/opt/hadoop</span><span id="32e2" class="mo lc iq mk b gy mt mq l mr ms">export HADOOP_CONF_DIR=/opt/hadoop /etc/hadoop</span><span id="254d" class="mo lc iq mk b gy mt mq l mr ms">export HADOOP_LOG_DIR=/opt/hadoop/logs</span></pre><p id="56a9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注销并重新登录到您的 hadoopuser 帐户，并使用以下命令检查 Hadoop 安装。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="305d" class="mo lc iq mk b gy mp mq l mr ms">hadoop -version</span></pre><h1 id="de10" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">2.3.主节点配置</h1><p id="4c74" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">首先，我们必须更新位于/opt/Hadoop/etc/Hadoop/的 hdfs-site.xml 文件，以定义这台机器上的名称节点和数据节点，并定义复制因子和其他设置:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="d009" class="mo lc iq mk b gy mp mq l mr ms">sudo gedit /opt/hadoop/etc/hadoop/hdfs-site.xml</span></pre><p id="0885" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该文件必须类似于:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="7d12" class="mo lc iq mk b gy mp mq l mr ms">&lt;configuration&gt;</span><span id="e861" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="b51c" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><span id="661a" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;file:///opt/hdfs/namenode&lt;/value&gt;</span><span id="f52d" class="mo lc iq mk b gy mt mq l mr ms">&lt;description&gt;NameNode directory for namespace and transaction logs storage.&lt;/description&gt;</span><span id="bb9b" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="97f6" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="1e39" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><span id="900f" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;file:///opt/hdfs/datanode&lt;/value&gt;</span><span id="3886" class="mo lc iq mk b gy mt mq l mr ms">&lt;description&gt;DataNode directory&lt;/description&gt;</span><span id="0d63" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="d74f" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="4683" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;dfs.replication&lt;/name&gt;</span><span id="b1e4" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;3&lt;/value&gt;</span><span id="bbfc" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="8043" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="799f" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;dfs.permissions&lt;/name&gt;</span><span id="ec7c" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;false&lt;/value&gt;</span><span id="1df8" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="9a90" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="7369" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;dfs.datanode.use.datanode.hostname&lt;/name&gt;</span><span id="7c3e" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;false&lt;/value&gt;</span><span id="d1d4" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="dda2" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="124b" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;dfs.namenode.datanode.registration.ip-hostname-check&lt;/name&gt;</span><span id="8ed1" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;false&lt;/value&gt;</span><span id="bd90" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="b8a7" class="mo lc iq mk b gy mt mq l mr ms">&lt;/configuration&gt;</span></pre><p id="409d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们必须更新位于/opt/hadoop/etc/hadoop 的 core-site.xml 文件，并让 hadoop 发行版知道名称节点的位置:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="eddd" class="mo lc iq mk b gy mp mq l mr ms">sudo gedit /opt/hadoop/etc/hadoop/core-site.xml</span></pre><p id="db8c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该文件必须类似于:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="14cd" class="mo lc iq mk b gy mp mq l mr ms">&lt;configuration&gt;</span><span id="d3b1" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="f4bc" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><span id="e0de" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;hdfs://hadoop-namenode:9820/&lt;/value&gt;</span><span id="8f71" class="mo lc iq mk b gy mt mq l mr ms">&lt;description&gt;NameNode URI&lt;/description&gt;</span><span id="940f" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="7c77" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="e6b8" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;io.file.buffer.size&lt;/name&gt;</span><span id="61f1" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;131072&lt;/value&gt;</span><span id="bee8" class="mo lc iq mk b gy mt mq l mr ms">&lt;description&gt;Buffer size&lt;/description&gt;</span><span id="7581" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="74ca" class="mo lc iq mk b gy mt mq l mr ms">&lt;/configuration&gt;</span></pre><p id="3055" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们必须更新位于/opt/hadoop/etc/hadoop/的 yarn-site.xml 文件</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="e50d" class="mo lc iq mk b gy mp mq l mr ms">sudo gedit /opt/hadoop/etc/hadoop/yarn-site.xml</span></pre><p id="3a25" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该文件必须类似于:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="a471" class="mo lc iq mk b gy mp mq l mr ms">&lt;configuration&gt;</span><span id="7e09" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="3746" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><span id="8353" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><span id="8f98" class="mo lc iq mk b gy mt mq l mr ms">&lt;description&gt;Yarn Node Manager Aux Service&lt;/description&gt;</span><span id="be85" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="13af" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="f525" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;</span><span id="bdd0" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><span id="9647" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="c59e" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="ae36" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;</span><span id="7611" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;file:///opt/yarn/local&lt;/value&gt;</span><span id="f0eb" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="2eed" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="a49e" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;</span><span id="9e1d" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;file:///opt/yarn/logs&lt;/value&gt;</span><span id="d3c9" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="6609" class="mo lc iq mk b gy mt mq l mr ms">&lt;/configuration&gt;</span></pre><p id="303c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们必须更新位于/opt/hadoop/etc/hadoop/的 mapred-site.xml 文件</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="f6b0" class="mo lc iq mk b gy mp mq l mr ms">&lt;configuration&gt;</span><span id="52f7" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="77b7" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><span id="1070" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;yarn&lt;/value&gt;</span><span id="d487" class="mo lc iq mk b gy mt mq l mr ms">&lt;description&gt;MapReduce framework name&lt;/description&gt;</span><span id="6f93" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="e9ff" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="a9ba" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><span id="afa5" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;hadoop-namenode:10020&lt;/value&gt;</span><span id="1d54" class="mo lc iq mk b gy mt mq l mr ms">&lt;description&gt;Default port is 10020.&lt;/description&gt;</span><span id="5590" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="3bc1" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="c3be" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><span id="1f8f" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt; hadoop-namenode:19888&lt;/value&gt;</span><span id="f2b4" class="mo lc iq mk b gy mt mq l mr ms">&lt;description&gt;Default port is 19888.&lt;/description&gt;</span><span id="b24a" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="bf6c" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="fba3" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt;</span><span id="b632" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;/mr-history/tmp&lt;/value&gt;</span><span id="b7c4" class="mo lc iq mk b gy mt mq l mr ms">&lt;description&gt;Directory where history files are written by MapReduce jobs.&lt;/description&gt;</span><span id="32b8" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="09df" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="1469" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt;</span><span id="08b2" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;/mr-history/done&lt;/value&gt;</span><span id="db4e" class="mo lc iq mk b gy mt mq l mr ms">&lt;description&gt;Directory where history files are managed by the MR JobHistory Server.&lt;/description&gt;</span><span id="be8c" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="bec3" class="mo lc iq mk b gy mt mq l mr ms">&lt;/configuration&gt;</span></pre><p id="da04" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们必须格式化名称节点</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="f420" class="mo lc iq mk b gy mp mq l mr ms">hdfs namenode –format</span></pre><p id="ba23" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们必须将您的数据节点(Slaves)添加到位于/opt/hadoop/etc/hadoop 的 workers 文件中</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="f499" class="mo lc iq mk b gy mp mq l mr ms">10.0.1.1</span><span id="4939" class="mo lc iq mk b gy mt mq l mr ms">10.0.1.2</span><span id="0b2e" class="mo lc iq mk b gy mt mq l mr ms">10.0.1.3</span></pre><p id="4717" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">配置数据节点后，您必须确保名称节点可以无密码访问它们:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="5389" class="mo lc iq mk b gy mp mq l mr ms">ssh-copy-id -i /home/hadoopuser/.ssh/id_rsa.pub hadoopuser@10.0.1.2</span><span id="a953" class="mo lc iq mk b gy mt mq l mr ms">ssh-copy-id -i /home/hadoopuser/.ssh/id_rsa.pub hadoopuser@10.0.1.3</span></pre><h1 id="39d7" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">2.4.配置数据节点</h1><p id="d3f7" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir"> <em class="me">注意:您可以将 Hadoop.tar.gz 文件从名称节点复制到数据节点并提取，而不是下载 Hadoop。你可以使用下面的命令:</em> </strong></p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="88d2" class="mo lc iq mk b gy mp mq l mr ms">scp hadoop.tar.gz hadoop-datanode-2:/home/hadoopuser</span><span id="2775" class="mo lc iq mk b gy mt mq l mr ms">scp hadoop.tar.gz hadoop-datanode-3:/home/hadoopuser</span></pre><p id="0fa2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每个数据节点上，您必须执行以下步骤:</p><p id="483b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们必须更新位于/opt/hadoop/etc/hadoop 目录下的 hdfs-site.xml、core-site.xml、yarn-site.xml 和 mapred-site.xml，如下所示:</p><p id="4002" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> hdfs-site.xml </strong></p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="4157" class="mo lc iq mk b gy mp mq l mr ms">&lt;configuration&gt;</span><span id="3e67" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="2a33" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><span id="22cc" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;file:///opt/hdfs/datanode&lt;/value&gt;</span><span id="a49d" class="mo lc iq mk b gy mt mq l mr ms">&lt;description&gt;DataNode directory&lt;/description&gt;</span><span id="8269" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="6ad8" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="f16b" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;dfs.replication&lt;/name&gt;</span><span id="63f4" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;3&lt;/value&gt;</span><span id="0a9d" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="ad05" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="f2c9" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;dfs.permissions&lt;/name&gt;</span><span id="dd72" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;false&lt;/value&gt;</span><span id="b264" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="e5f1" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="a764" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;dfs.datanode.use.datanode.hostname&lt;/name&gt;</span><span id="b1a2" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;false&lt;/value&gt;</span><span id="1bf2" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="9add" class="mo lc iq mk b gy mt mq l mr ms">&lt;/configuration&gt;</span></pre><p id="0fe8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> core-site.xml </strong></p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="583a" class="mo lc iq mk b gy mp mq l mr ms">&lt;configuration&gt;</span><span id="30b8" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="7d25" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><span id="615c" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;hdfs://hadoop-namenode:9820/&lt;/value&gt;</span><span id="588f" class="mo lc iq mk b gy mt mq l mr ms">&lt;description&gt;NameNode URI&lt;/description&gt;</span><span id="0d85" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="c126" class="mo lc iq mk b gy mt mq l mr ms">&lt;/configuration&gt;</span></pre><p id="a33b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> yarn-site.xml </strong></p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="21b7" class="mo lc iq mk b gy mp mq l mr ms">&lt;configuration&gt;</span><span id="867e" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="7666" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><span id="67e9" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><span id="7fd7" class="mo lc iq mk b gy mt mq l mr ms">&lt;description&gt;Yarn Node Manager Aux Service&lt;/description&gt;</span><span id="f808" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="097e" class="mo lc iq mk b gy mt mq l mr ms">&lt;/configuration&gt;</span></pre><p id="e7dc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> mapred-site.xml </strong></p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="1adc" class="mo lc iq mk b gy mp mq l mr ms">&lt;configuration&gt;</span><span id="9214" class="mo lc iq mk b gy mt mq l mr ms">&lt;property&gt;</span><span id="a573" class="mo lc iq mk b gy mt mq l mr ms">&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><span id="0ee5" class="mo lc iq mk b gy mt mq l mr ms">&lt;value&gt;yarn&lt;/value&gt;</span><span id="95b8" class="mo lc iq mk b gy mt mq l mr ms">&lt;description&gt;MapReduce framework name&lt;/description&gt;</span><span id="c3fe" class="mo lc iq mk b gy mt mq l mr ms">&lt;/property&gt;</span><span id="9b36" class="mo lc iq mk b gy mt mq l mr ms">&lt;/configuration&gt;</span></pre><h1 id="a323" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">2.5.启动 Hadoop</h1><p id="5ad9" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">完成上述步骤后，我们必须从名称节点执行以下命令来启动名称节点、数据节点和辅助名称节点:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="aa50" class="mo lc iq mk b gy mp mq l mr ms">start-dfs.sh</span></pre><p id="2d1b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它将给出以下输出:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="55a5" class="mo lc iq mk b gy mp mq l mr ms">Starting namenodes on [hadoop-namenode]</span><span id="8daa" class="mo lc iq mk b gy mt mq l mr ms">Starting datanodes</span><span id="2ac8" class="mo lc iq mk b gy mt mq l mr ms">Starting secondary namenodes [hadoop-namenode]</span></pre><p id="4cfe" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，要启动资源管理器和节点管理器，我们必须执行以下命令:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="e95f" class="mo lc iq mk b gy mp mq l mr ms">start-yarn.sh</span></pre><p id="90b9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它将给出以下输出:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="f4c8" class="mo lc iq mk b gy mp mq l mr ms">Starting resourcemanager</span><span id="d7f4" class="mo lc iq mk b gy mt mq l mr ms">Starting nodemanagers</span></pre><p id="ab5a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">之后，为了确保 Hadoop 成功启动，我们必须在 name node 上运行 jps 命令，data nodes 必须给出以下输出:</p><p id="8e60" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">在名称节点上(忽略进程 id):</strong></p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="68e5" class="mo lc iq mk b gy mp mq l mr ms">16488 NameNode</span><span id="a896" class="mo lc iq mk b gy mt mq l mr ms">16622 DataNode</span><span id="5e18" class="mo lc iq mk b gy mt mq l mr ms">17215 NodeManager</span><span id="4d54" class="mo lc iq mk b gy mt mq l mr ms">17087 ResourceManager</span><span id="6846" class="mo lc iq mk b gy mt mq l mr ms">17530 Jps</span><span id="16bf" class="mo lc iq mk b gy mt mq l mr ms">16829 SecondaryNameNode</span></pre><p id="9e31" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">在数据节点上(忽略进程 id):</strong></p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="ca6c" class="mo lc iq mk b gy mp mq l mr ms">2306 DataNode</span><span id="4904" class="mo lc iq mk b gy mt mq l mr ms">2479 NodeManager</span><span id="5f2d" class="mo lc iq mk b gy mt mq l mr ms">2581 Jps</span></pre><p id="9944" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您得到类似的输出，那么所有的 Hadoop 守护进程都会成功启动。</p><p id="8c0f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意:您可以通过运行 hdfs dfsadmin -report 命令<em class="me">(它必须返回 Live datanodes (3)) </em>，在/opt/hadoop/logs 下检查日志并检查是否一切正常。</p><h1 id="66dc" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">2.6.在浏览器上访问 Hadoop</h1><p id="0d50" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir"> Namenode </strong></p><p id="65bc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">访问以下网址:<a class="ae kc" href="https://hadoop-namenode:9870/" rel="noopener ugc nofollow" target="_blank">https://Hadoop-NameNode:9870/</a></p><p id="b78a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">资源经理</strong></p><p id="9f7a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">访问以下 URL:<a class="ae kc" href="https://hadoop-namenode:8088/" rel="noopener ugc nofollow" target="_blank">https://Hadoop-NameNode:8088/</a></p><h1 id="5de9" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">3.参考</h1><ul class=""><li id="b6eb" class="mv mw iq kf b kg lz kk ma ko mx ks my kw mz la na nb nc nd bi translated">[1] F. Houbart，“如何安装和设置 hadoop 集群”，Linode，2017 年 10 月 16 日。【在线】。可用:<a class="ae kc" href="https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster." rel="noopener ugc nofollow" target="_blank">https://www . Li node . com/docs/databases/Hadoop/how-to-install-and-set-up-Hadoop-cluster。</a>【2018 年 7 月 20 日获取】。</li><li id="81f7" class="mv mw iq kf b kg ne kk nf ko ng ks nh kw ni la na nb nc nd bi translated">[2]“栈溢出问答”，栈溢出，[在线]。可用:<a class="ae kc" href="https://www.Stackoverflow.com." rel="noopener ugc nofollow" target="_blank">https://www.Stackoverflow.com。</a></li><li id="f7ca" class="mv mw iq kf b kg ne kk nf ko ng ks nh kw ni la na nb nc nd bi translated">[3]“Apache Hadoop 文档”，Apache，[在线]。可用:<a class="ae kc" href="https://www.hadoop.apache.org." rel="noopener ugc nofollow" target="_blank">https://www.hadoop.apache.org。</a>【访问时间 2018 年 01 月 07 日】。</li><li id="0e12" class="mv mw iq kf b kg ne kk nf ko ng ks nh kw ni la na nb nc nd bi translated">[4] G. Bansal，“在 Ubuntu 上安装 Hadoop 3.0.0 多节点集群”，2017 年 8 月 31 日。【在线】。可用:<a class="ae kc" href="http://www.gaurav3ansal.blogspot.com/2017/08/installing-hadoop-300-alpha-4-multi.html." rel="noopener ugc nofollow" target="_blank">http://www . gaur av 3 ansal . blogspot . com/2017/08/installing-Hadoop-300-alpha-4-multi . html</a>【访问时间 16 07 2018】。</li><li id="208a" class="mv mw iq kf b kg ne kk nf ko ng ks nh kw ni la na nb nc nd bi translated">[5] M. Litwintschik，“Hadoop 3 单节点安装指南”，2018 年 3 月 19 日。【在线】。可用:<a class="ae kc" href="http://www.tech.marksblogg.com/hadoop-3-single-node-install-guide.html." rel="noopener ugc nofollow" target="_blank">http://www . tech . marksblogg . com/Hadoop-3-single-node-install-guide . html</a>【2018 年 10 月 06 日访问】。</li></ul></div></div>    
</body>
</html>