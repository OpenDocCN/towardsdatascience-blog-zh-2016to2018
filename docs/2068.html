<html>
<head>
<title>Ensembling ConvNets using Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Keras 组装 ConvNets</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ensembling-convnets-using-keras-237d429157eb?source=collection_archive---------0-----------------------#2017-12-13">https://towardsdatascience.com/ensembling-convnets-using-keras-237d429157eb?source=collection_archive---------0-----------------------#2017-12-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/366a2c510672d162f36aced24c2c3e12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bP1aXDzET4VWXVEqC9EnwA.jpeg"/></div></div></figure><blockquote class="jy jz ka"><p id="f9c4" class="kb kc kd ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">编辑:2019 年 2 月</strong></p><p id="0093" class="kb kc kd ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">微小的代码更改。改进的<a class="ae la" href="https://github.com/LawnboyMax/keras_ensemblng" rel="noopener ugc nofollow" target="_blank">体验篇</a>Jupyter 笔记本版。</p></blockquote><h1 id="02aa" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">介绍</h1><blockquote class="jy jz ka"><p id="57a2" class="kb kc kd ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在统计学和机器学习中，集成方法使用多种学习算法来获得比单独从任何组成学习算法获得的性能更好的预测性能。与统计力学中的统计集成(通常是无限的)不同，机器学习集成仅由一组具体的有限备选模型组成，但通常允许在这些备选模型中存在更加灵活的结构。<a class="ae la" href="https://en.wikipedia.org/wiki/Ensemble_learning" rel="noopener ugc nofollow" target="_blank">【1】</a></p></blockquote><p id="c315" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">使用集成的主要动机是找到一个假设，该假设不一定包含在构建它的模型的假设空间内。从经验上看，当模型之间存在显著差异时，集成往往会产生更好的结果。<a class="ae la" href="http://jair.org/papers/paper614.html" rel="noopener ugc nofollow" target="_blank">【2】</a></p><h2 id="4800" class="mc lc iq bd ld md me dn lh mf mg dp ll lz mh mi lp ma mj mk lt mb ml mm lx mn bi translated">动机</h2><p id="417c" class="pw-post-body-paragraph kb kc iq ke b kf mo kh ki kj mp kl km lz mq kp kq ma mr kt ku mb ms kx ky kz ij bi translated">如果你看看一场大型机器学习比赛的结果，你很可能会发现，最好的结果是由一群模型而不是单个模型实现的。例如，ILSVRC2015 上得分最高的单一模型架构排在第 13 位。第 1-12 名由不同的组合占据。</p><p id="67c4" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">我还没有看到关于如何在集成中使用多个神经网络的教程或文档，所以我决定就这个主题制作一个实用指南。</p><p id="cb65" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">我将使用<a class="ae la" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>，特别是它的<a class="ae la" href="https://keras.io/models/model/" rel="noopener ugc nofollow" target="_blank">功能 API </a>，重新创建三个小型 CNN(与 ResNet50、Inception 等相比)。)来自相对知名的论文。我将在<a class="ae la" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10 </a>训练数据集上分别训练每个模型。<a class="ae la" href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" rel="noopener ugc nofollow" target="_blank">【3】</a>然后，将使用测试集对每个模型进行评估。之后，我会把这三个模型放在一个合奏中进行评估。预计集成在测试集上将比集成中单独的任何单个模型表现得更好。</p><p id="87ff" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">有许多不同类型的合奏；堆叠就是其中之一。它是更一般的类型之一，理论上可以代表任何其他的系综技术。堆叠包括训练一个学习算法来组合其他几个学习算法的预测。<a class="ae la" href="https://en.wikipedia.org/wiki/Ensemble_learning#Stacking" rel="noopener ugc nofollow" target="_blank">【1】</a>为了这个例子，我将使用一种最简单的叠加形式，它包括取集合中模型输出的平均值。由于平均不需要任何参数，所以不需要训练这个集合(只需要它的模型)。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/4cc4edca7f4cabb06add5a882d2ec47e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fy-6esoTWsTutld4fdSyCQ.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">This post’s ensemble in a nutshell</figcaption></figure><h2 id="9f19" class="mc lc iq bd ld md me dn lh mf mg dp ll lz mh mi lp ma mj mk lt mb ml mm lx mn bi translated">准备数据</h2><p id="51ec" class="pw-post-body-paragraph kb kc iq ke b kf mo kh ki kj mp kl km lz mq kp kq ma mr kt ku mb ms kx ky kz ij bi translated">首先，导入依赖项。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="13be" class="mc lc iq nd b gy nh ni l nj nk">from keras.callbacks import History<br/>from keras.callbacks import ModelCheckpoint, TensorBoard<br/>from keras.datasets import cifar10<br/>from keras.engine import training<br/>from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Activation, Average<br/>from keras.losses import categorical_crossentropy<br/>from keras.models import Model, Input<br/>from keras.optimizers import Adam<br/>from keras.utils import to_categorical<br/>from tensorflow.python.framework.ops import Tensor<br/>from typing import Tuple, List<br/>import glob<br/>import numpy as np<br/>import os</span></pre><p id="7723" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">我使用 CIFAR-10，因为相对容易找到描述在这个数据集上运行良好的架构的论文。使用一个流行的数据集也使得这个例子很容易重现。</p><p id="d7d0" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">在这里，数据集被导入。训练和测试图像数据都被归一化。训练标签向量被转换成独热矩阵。不需要转换测试标签向量，因为它不会在训练中使用。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="1832" class="mc lc iq nd b gy nh ni l nj nk">def load_data() -&gt; Tuple [np.ndarray, np.ndarray, <br/>                          np.ndarray, np.ndarray]:<br/>    (x_train, y_train), (x_test, y_test) = cifar10.load_data()<br/>    x_train = x_train / 255.<br/>    x_test = x_test / 255.<br/>    y_train = to_categorical(y_train, num_classes=10)<br/>    return x_train, x_test, y_train, y_test</span><span id="be93" class="mc lc iq nd b gy nl ni l nj nk">x_train, x_test, y_train, y_test = load_data()</span></pre><p id="c92b" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">该数据集由来自 10 个类别的 60000 幅 32x32 RGB 图像组成。50000 幅图像用于训练/验证，另外 10000 幅用于测试。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="6b78" class="mc lc iq nd b gy nh ni l nj nk">print('x_train shape: {} | y_train shape: {}\nx_test shape : {} | y_test shape : {}'.format(x_train.shape, y_train.shape,                                                                                      x_test.shape, y_test.shape))</span></pre><p id="558f" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated"><em class="kd">&gt;&gt;&gt;x _ train shape:(50000，32，32，3) | y_train shape: (50000，10) </em></p><p id="41ee" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated"><em class="kd">&gt;&gt;&gt;x _ 测试形状:(10000，32，32，3)| y _ 测试形状:(10000，1) </em></p><p id="ef15" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">由于所有三个模型都使用相同形状的数据，因此定义一个供每个模型使用的输入图层是有意义的。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="4640" class="mc lc iq nd b gy nh ni l nj nk">input_shape = x_train[0,:,:,:].shape<br/>model_input = Input(shape=input_shape)</span></pre><h2 id="965d" class="mc lc iq bd ld md me dn lh mf mg dp ll lz mh mi lp ma mj mk lt mb ml mm lx mn bi translated">第一个模型:ConvPool-CNN-C</h2><p id="9bec" class="pw-post-body-paragraph kb kc iq ke b kf mo kh ki kj mp kl km lz mq kp kq ma mr kt ku mb ms kx ky kz ij bi translated">我要训练的第一个模型是 ConvPool-CNN-C。</p><p id="b645" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">这个模型非常简单。它有一个共同的模式，即几个卷积层之后是一个池层。对于这个模型，有些人唯一不熟悉的是它的最后几层。不是使用几个完全连接的层，而是使用一个全局平均池层。</p><p id="4253" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">这里有一个全局池层如何工作的简要概述。最后一个卷积层<code class="fe nm nn no nd b">Conv2D(10, (1, 1))</code>输出对应十个输出类的 10 个特征图。然后，<code class="fe nm nn no nd b">GlobalAveragePooling2D()</code>层计算这 10 个特征地图的空间平均值，这意味着它的输出只是一个长度为 10 的矢量。之后，对该向量应用 softmax 激活。如您所见，这种方法在某种程度上类似于在模型顶部使用 FC 层。你可以在 Network paper 中阅读更多关于全球池层及其在 Network 中的优势。<a class="ae la" href="https://arxiv.org/abs/1312.4400" rel="noopener ugc nofollow" target="_blank">【5】</a></p><p id="e5f7" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">需要注意的重要一点是:没有激活函数应用于最后一个<code class="fe nm nn no nd b">Conv2D(10, (1, 1))</code>层的输出，因为这个层的输出必须首先通过<code class="fe nm nn no nd b">GlobalAveragePooling2D()</code>。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="4e5a" class="mc lc iq nd b gy nh ni l nj nk">def conv_pool_cnn(model_input: Tensor) -&gt; training.Model:<br/>    <br/>    x = Conv2D(96, kernel_size=(3, 3), activation='relu', padding = 'same')(model_input)<br/>    x = Conv2D(96, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(96, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = MaxPooling2D(pool_size=(3, 3), strides = 2)(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = MaxPooling2D(pool_size=(3, 3), strides = 2)(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(192, (1, 1), activation='relu')(x)<br/>    x = Conv2D(10, (1, 1))(x)<br/>    x = GlobalAveragePooling2D()(x)<br/>    x = Activation(activation='softmax')(x)<br/>    <br/>    model = Model(model_input, x, name='conv_pool_cnn')<br/>    <br/>    return model</span></pre><p id="0a12" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">实例化模型。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="069d" class="mc lc iq nd b gy nh ni l nj nk">conv_pool_cnn_model = conv_pool_cnn(model_input)</span></pre><p id="58df" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">为了简单起见，每个模型都使用相同的参数进行编译和训练。使用 20 个时段，批次大小为 32(每个时段 1250 步)，对于三个模型中的任何一个来说，似乎足以达到一些局部最小值。随机选择 20%的训练数据集用于验证。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="d034" class="mc lc iq nd b gy nh ni l nj nk">NUM_EPOCHS = 20</span><span id="0cb4" class="mc lc iq nd b gy nl ni l nj nk">def compile_and_train(model: training.Model, num_epochs: int) -&gt; Tuple [History, str]: <br/>    <br/>    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['acc']) <br/>    filepath = 'weights/' + model.name + '.{epoch:02d}-{loss:.2f}.hdf5'<br/>    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_weights_only=True,<br/>                                                 save_best_only=True, mode='auto', period=1)<br/>    tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=32)<br/>    history = model.fit(x=x_train, y=y_train, batch_size=32, <br/>                     epochs=num_epochs, verbose=1, callbacks=[checkpoint, tensor_board], validation_split=0.2)<br/>    weight_files = glob.glob(os.path.join(os.getcwd(), 'weights/*'))<br/>    weight_file = max(weight_files, key=os.path.getctime) # most recent file</span><span id="52f2" class="mc lc iq nd b gy nl ni l nj nk">    return history, weight_file</span></pre><p id="e505" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">使用单个 Tesla K80 GPU 在一个时期内训练这个模型和下一个模型大约需要 1 分钟。如果您使用 CPU，训练可能需要一段时间。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="09ab" class="mc lc iq nd b gy nh ni l nj nk">_, conv_pool_cnn_weight_file = compile_and_train(conv_pool_cnn_model, NUM_EPOCHS)</span></pre><p id="8ac1" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">该模型达到了约 79%的验证准确率。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/b8775a3b01138df72f8970fc30a2da71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HXW-Hsk5hhWnBKOjAHu31Q.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">ConvPool-CNN-C validation accuracy and loss</figcaption></figure><p id="39ed" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">通过计算测试集的错误率来评估模型。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="5d4b" class="mc lc iq nd b gy nh ni l nj nk">def evaluate_error(model: training.Model) -&gt; np.float64:</span><span id="e087" class="mc lc iq nd b gy nl ni l nj nk">    pred = model.predict(x_test, batch_size = 32)<br/>    pred = np.argmax(pred, axis=1)<br/>    pred = np.expand_dims(pred, axis=1) # make same shape as y_test<br/>    error = np.sum(np.not_equal(pred, y_test)) / y_test.shape[0]   <br/> <br/>    return error</span><span id="01d3" class="mc lc iq nd b gy nl ni l nj nk">evaluate_error(conv_pool_cnn_model)</span></pre><p id="4295" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated"><em class="kd"> &gt; &gt; &gt; 0.2414 </em></p><h2 id="05b8" class="mc lc iq bd ld md me dn lh mf mg dp ll lz mh mi lp ma mj mk lt mb ml mm lx mn bi translated">第二种模式:全 CNN-C</h2><p id="03bc" class="pw-post-body-paragraph kb kc iq ke b kf mo kh ki kj mp kl km lz mq kp kq ma mr kt ku mb ms kx ky kz ij bi translated">下一个 CNN，ALL-CNN-C，出自同一篇论文。<a class="ae la" href="https://arxiv.org/abs/1412.6806v3" rel="noopener ugc nofollow" target="_blank">【4】</a>这款和上一款很像。实际上，唯一的区别是使用跨距为 2 的卷积层来代替最大池层。再次注意，在<code class="fe nm nn no nd b">Conv2D(10, (1, 1))</code>层之后没有立即使用激活功能。如果在该层之后立即使用 ReLU 激活，模型将无法训练。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="840b" class="mc lc iq nd b gy nh ni l nj nk">def all_cnn(model_input: Tensor) -&gt; training.Model:<br/>    <br/>    x = Conv2D(96, kernel_size=(3, 3), activation='relu', padding = 'same')(model_input)<br/>    x = Conv2D(96, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(96, (3, 3), activation='relu', padding = 'same', strides = 2)(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same', strides = 2)(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(192, (1, 1), activation='relu')(x)<br/>    x = Conv2D(10, (1, 1))(x)<br/>    x = GlobalAveragePooling2D()(x)<br/>    x = Activation(activation='softmax')(x)<br/>        <br/>    model = Model(model_input, x, name='all_cnn')<br/>    <br/>    return model</span><span id="9101" class="mc lc iq nd b gy nl ni l nj nk">all_cnn_model = all_cnn(model_input)<br/>_, all_cnn_weight_file = compile_and_train(all_cnn_model, NUM_EPOCHS)</span></pre><p id="5f35" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">该模型收敛到约 75%的验证准确性。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/42d1a8c39734bdfcf534db5e1332a6b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zKu98qvSPYmK-T4wg3z9yw.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">ALL-CNN-C validation accuracy and loss</figcaption></figure><p id="44f9" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">由于两个模型非常相似，错误率不会相差太多。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="3a2d" class="mc lc iq nd b gy nh ni l nj nk">evaluate_error(all_cnn_model)</span></pre><p id="1453" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated"><em class="kd">&gt;&gt;&gt;0.260900000000002</em></p><h2 id="5f65" class="mc lc iq bd ld md me dn lh mf mg dp ll lz mh mi lp ma mj mk lt mb ml mm lx mn bi translated">第三种模式:网络中的网络 CNN</h2><p id="a8f0" class="pw-post-body-paragraph kb kc iq ke b kf mo kh ki kj mp kl km lz mq kp kq ma mr kt ku mb ms kx ky kz ij bi translated">第三个 CNN 是网络中的网络。<a class="ae la" href="https://arxiv.org/abs/1312.4400" rel="noopener ugc nofollow" target="_blank">【5】</a>这是一篇 CNN 的文章，介绍了全局池层。它比前两个型号小，因此训练起来更快。最后卷积层后没有<code class="fe nm nn no nd b">relu</code>！</p><p id="4dad" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">我没有在 MLP 卷积层中使用多层感知器，而是使用了 1x1 内核的卷积层。这样，需要优化的参数更少，训练速度更快，我可以获得更好的结果(当使用 FC 层时，无法获得高于 50%的验证精度)。该论文指出，mlpconv 层所应用的功能相当于普通卷积层上的级联跨通道参数池，而普通卷积层又相当于具有 1×1 卷积核的卷积层。如果我对架构的解释不正确，请纠正我。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="47cc" class="mc lc iq nd b gy nh ni l nj nk">def nin_cnn(model_input: Tensor) -&gt; training.Model:<br/>    <br/>    #mlpconv block 1<br/>    x = Conv2D(32, (5, 5), activation='relu',padding='valid')(model_input)<br/>    x = Conv2D(32, (1, 1), activation='relu')(x)<br/>    x = Conv2D(32, (1, 1), activation='relu')(x)<br/>    x = MaxPooling2D((2,2))(x)<br/>    x = Dropout(0.5)(x)<br/>    <br/>    #mlpconv block2<br/>    x = Conv2D(64, (3, 3), activation='relu',padding='valid')(x)<br/>    x = Conv2D(64, (1, 1), activation='relu')(x)<br/>    x = Conv2D(64, (1, 1), activation='relu')(x)<br/>    x = MaxPooling2D((2,2))(x)<br/>    x = Dropout(0.5)(x)<br/>    <br/>    #mlpconv block3<br/>    x = Conv2D(128, (3, 3), activation='relu',padding='valid')(x)<br/>    x = Conv2D(32, (1, 1), activation='relu')(x)<br/>    x = Conv2D(10, (1, 1))(x)<br/>    <br/>    x = GlobalAveragePooling2D()(x)<br/>    x = Activation(activation='softmax')(x)<br/>    <br/>    model = Model(model_input, x, name='nin_cnn')<br/>    <br/>    return model</span><span id="49f8" class="mc lc iq nd b gy nl ni l nj nk">nin_cnn_model = nin_cnn(model_input)</span></pre><p id="7998" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">这个模型的训练速度要快得多——在我的机器上，每个时期 15 秒。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="7083" class="mc lc iq nd b gy nh ni l nj nk">_, nin_cnn_weight_file = compile_and_train(nin_cnn_model, NUM_EPOCHS)</span></pre><p id="77e0" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">该模型实现了约 65%的验证准确率。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/287f9a622fee2a79ea7f8133a5a17192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nq_DwqQjL2ZZVU4wQXkSQQ.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">NIN-CNN validation accuracy and loss</figcaption></figure><p id="e3e1" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">这个模型比其他两个更简单，所以错误率有点高。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="84d2" class="mc lc iq nd b gy nh ni l nj nk">evaluate_error(nin_cnn_model)</span></pre><p id="a7c3" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated"><em class="kd"> &gt; &gt; &gt; 0。0.3164000000000001</em></p><h2 id="71c1" class="mc lc iq bd ld md me dn lh mf mg dp ll lz mh mi lp ma mj mk lt mb ml mm lx mn bi translated">三模型集成</h2><p id="b518" class="pw-post-body-paragraph kb kc iq ke b kf mo kh ki kj mp kl km lz mq kp kq ma mr kt ku mb ms kx ky kz ij bi translated">现在这三个模型将被组合成一个整体。</p><p id="ec0d" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">这里，所有三个模型都被重新实例化，并且加载了最佳保存的权重。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="98ed" class="mc lc iq nd b gy nh ni l nj nk">CONV_POOL_CNN_WEIGHT_FILE = os.path.join(os.getcwd(), 'weights', 'conv_pool_cnn_pretrained_weights.hdf5')<br/>ALL_CNN_WEIGHT_FILE = os.path.join(os.getcwd(), 'weights', 'all_cnn_pretrained_weights.hdf5')<br/>NIN_CNN_WEIGHT_FILE = os.path.join(os.getcwd(), 'weights', 'nin_cnn_pretrained_weights.hdf5')</span><span id="c5c5" class="mc lc iq nd b gy nl ni l nj nk"><br/>conv_pool_cnn_model = conv_pool_cnn(model_input)<br/>all_cnn_model = all_cnn(model_input)<br/>nin_cnn_model = nin_cnn(model_input)<br/><br/>conv_pool_cnn_model.load_weights(CONV_POOL_CNN_WEIGHT_FILE)<br/>all_cnn_model.load_weights(ALL_CNN_WEIGHT_FILE)<br/>nin_cnn_model.load_weights(NIN_CNN_WEIGHT_FILE)<br/><br/>models = [conv_pool_cnn_model, all_cnn_model, nin_cnn_model]</span></pre><p id="d537" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">集合模型定义非常简单。它使用所有先前模型之间共享的相同输入层。在顶层，集成通过使用<code class="fe nm nn no nd b">Average()</code>合并层计算三个模型输出的平均值。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="31d1" class="mc lc iq nd b gy nh ni l nj nk">def ensemble(models: List [training.Model], model_input: Tensor) -&gt; training.Model:<br/>    <br/>    outputs = [model.outputs[0] for model in models]<br/>    y = Average()(outputs)<br/>    <br/>    model = Model(model_input, y, name='ensemble')<br/>    <br/>    return model</span></pre><p id="bee0" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">正如所料，集合的错误率比任何单一模型都低。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="5cfd" class="mc lc iq nd b gy nh ni l nj nk">evaluate_error(ensemble_model)</span></pre><p id="1150" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated"><em class="kd"> &gt; &gt; &gt; 0.2049 </em></p><h2 id="3c4b" class="mc lc iq bd ld md me dn lh mf mg dp ll lz mh mi lp ma mj mk lt mb ml mm lx mn bi translated">其他可能的合奏</h2><p id="a554" class="pw-post-body-paragraph kb kc iq ke b kf mo kh ki kj mp kl km lz mq kp kq ma mr kt ku mb ms kx ky kz ij bi translated">为了完整起见，我们可以检查由两个模型组合组成的集合的性能。其中两个模型的错误率低于单一模型。</p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="5977" class="mc lc iq nd b gy nh ni l nj nk">pair_A = [conv_pool_cnn_model, all_cnn_model]<br/>pair_B = [conv_pool_cnn_model, nin_cnn_model]<br/>pair_C = [all_cnn_model, nin_cnn_model]</span><span id="4f00" class="mc lc iq nd b gy nl ni l nj nk">pair_A_ensemble_model = ensemble(pair_A, model_input)<br/>evaluate_error(pair_A_ensemble_model)</span></pre><p id="d76c" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated"><em class="kd">&gt;&gt;&gt;0.211999999999999</em></p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="6aaf" class="mc lc iq nd b gy nh ni l nj nk">pair_B_ensemble_model = ensemble(pair_B, model_input)<br/>evaluate_error(pair_B_ensemble_model)</span></pre><p id="5e84" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated"><em class="kd">&gt;&gt;&gt;0.228199999999999</em></p><pre class="mu mv mw mx gt nc nd ne nf aw ng bi"><span id="3070" class="mc lc iq nd b gy nh ni l nj nk">pair_C_ensemble_model = ensemble(pair_C, model_input)<br/>evaluate_error(pair_C_ensemble_model)</span></pre><p id="a2b7" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated"><em class="kd"> &gt; &gt; &gt; 0.2447 </em></p><h1 id="32b0" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结论</h1><p id="79c7" class="pw-post-body-paragraph kb kc iq ke b kf mo kh ki kj mp kl km lz mq kp kq ma mr kt ku mb ms kx ky kz ij bi translated">重申一下引言中说过的话:每个模型都有自己的弱点。使用集成背后的原因是，通过堆叠表示关于数据的不同假设的不同模型，我们可以找到不在构建集成的模型的假设空间中的更好的假设。</p><p id="befb" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">通过使用非常基本的集合，与在大多数情况下使用单一模型相比，实现了更低的错误率。这证明了组合的有效性。</p><p id="63cc" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">当然，当使用系综来完成机器学习任务时，有一些实际的考虑要记住。由于集成意味着将多个模型堆叠在一起，这也意味着每个模型的输入数据都需要向前传播。这增加了需要执行的计算量，从而增加了评估(预测)时间。如果你在研究或比赛中使用系综，增加评估时间并不重要。然而，在设计商业产品时，这是一个非常关键的因素。另一个考虑因素是最终模型尺寸的增加，这也可能是在商业产品中整体使用的限制因素。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="e541" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km lz ko kp kq ma ks kt ku mb kw kx ky kz ij bi translated">你可以从我的<a class="ae la" href="https://github.com/LawnboyMax/keras_ensemblng" rel="noopener ugc nofollow" target="_blank"> GitHub </a>获得 Jupyter 笔记本源代码。</p><h2 id="aae1" class="mc lc iq bd ld md me dn lh mf mg dp ll lz mh mi lp ma mj mk lt mb ml mm lx mn bi translated">参考</h2><ol class=""><li id="aca6" class="nz oa iq ke b kf mo kj mp lz ob ma oc mb od kz oe of og oh bi translated"><strong class="ke ir">集成学习</strong>。(未注明)。在<em class="kd">维基百科</em>里。检索于 2017 年 12 月 12 日，来自<a class="ae la" href="https://en.wikipedia.org/wiki/Ensemble_learning" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Ensemble_learning</a></li><li id="ecd5" class="nz oa iq ke b kf oi kj oj lz ok ma ol mb om kz oe of og oh bi translated">D.Opitz 和 R. Maclin (1999 年)<strong class="ke ir">流行的集合方法:实证研究</strong>，第 11 卷，第 169-198 页(可在<a class="ae la" href="http://jair.org/papers/paper614.html" rel="noopener ugc nofollow" target="_blank">http://jair.org/papers/paper614.html</a>获得)</li><li id="892e" class="nz oa iq ke b kf oi kj oj lz ok ma ol mb om kz oe of og oh bi translated"><a class="ae la" href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ke ir">从微小图像中学习多层特征</strong> </a>，Alex Krizhevsky，2009。</li><li id="b131" class="nz oa iq ke b kf oi kj oj lz ok ma ol mb om kz oe of og oh bi translated"><a class="ae la" href="https://arxiv.org/abs/1412.6806v3" rel="noopener ugc nofollow" target="_blank">arXiv:1412.6806 v3</a>【cs。LG]</li><li id="4437" class="nz oa iq ke b kf oi kj oj lz ok ma ol mb om kz oe of og oh bi translated"><a class="ae la" href="https://arxiv.org/abs/1312.4400v3" rel="noopener ugc nofollow" target="_blank">arXiv:1312.4400 v3</a>【cs。NE]</li></ol></div></div>    
</body>
</html>