<html>
<head>
<title>An implementation guide to Word2Vec using NumPy and Google Sheets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 NumPy 和 Google Sheets 的 Word2Vec 实现指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281?source=collection_archive---------1-----------------------#2018-12-06">https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281?source=collection_archive---------1-----------------------#2018-12-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e561" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解 Word2Vec 的内部工作原理</h2></div><p id="dd6f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文是使用 NumPy 和 Google Sheets 实现 Word2Vec 的指南。如果你读这篇文章有困难，可以考虑在这里订阅中级会员！</p><p id="d559" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Word2Vec 被誉为自然语言处理(NLP)领域最大、最新的突破之一。这个概念简单、优雅且(相对)容易掌握。快速的谷歌搜索会返回多个关于如何使用标准库的结果，比如<a class="ae le" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"> Gensim </a>和<a class="ae le" href="https://www.tensorflow.org/tutorials/representation/word2vec" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>。此外，出于好奇，请查看 Tomas Mikolov 使用 C 的原始实现。原文可以在<a class="ae le" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="87b1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文的重点是详细介绍 Word2Vec。为此，我使用 NumPy 在 Python 上实现了 Word2Vec(在其他教程的帮助下),还准备了一个 Google 工作表来展示计算。以下是<a class="ae le" href="https://github.com/DerekChia/word2vec_numpy" rel="noopener ugc nofollow" target="_blank">代码</a>和<a class="ae le" href="https://docs.google.com/spreadsheets/u/3/d/1mgf82Ue7MmQixMm2ZqnT1oWUucj6pEcd2wDs_JgHmco/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">谷歌表单</a>的链接。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/d04b4e72229e7b9ff28d739353eb7c2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*usSa3eBzkP0F8IrAe9g1VA.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Fig. 1 — Step-by-step introduction to Word2Vec. Presented in code and Google Sheets</figcaption></figure><h1 id="cf66" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">直觉</h1><p id="37eb" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">Word2Vec 的目标是为进一步的 NLP 任务生成带有语义的单词的向量表示。每个单词向量通常有几百个维度，并且语料库中的每个唯一单词被分配一个空间向量。例如，单词“happy”可以表示为 4 维向量[0.24，0.45，0.11，0.49],“sad”具有向量[0.88，0.78，0.45，0.91]。</p><p id="fc2c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单词到向量的转换也称为<a class="ae le" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank"> <em class="ms">单词嵌入</em> </a> <em class="ms">。</em>之所以要进行这样的转换，是为了让机器学习算法可以对数字(在向量中)而不是单词进行线性代数运算。</p><p id="58ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要实现 Word2Vec，有两种风格可供选择— <strong class="kk iu">连续词包(CBOW) </strong>或<strong class="kk iu">连续跳格(SG) </strong>。简而言之，CBOW 试图从其相邻单词(上下文单词)中猜测输出(目标单词)，而连续跳格从目标单词中猜测上下文单词。实际上，Word2Vec 是基于<a class="ae le" href="https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_hypothesis" rel="noopener ugc nofollow" target="_blank">分布假设</a>的，其中每个单词的上下文都在它附近的单词中。因此，通过查看它的相邻单词，我们可以尝试预测目标单词。</p><p id="d900" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据 Mikolov 的说法(在这篇文章的<a class="ae le" href="https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures" rel="noopener ugc nofollow" target="_blank">中引用)，Skip-gram 和 CBOW 的区别如下:</a></p><blockquote class="mt mu mv"><p id="0d59" class="ki kj ms kk b kl km ju kn ko kp jx kq mw ks kt ku mx kw kx ky my la lb lc ld im bi translated"><strong class="kk iu"><em class="it">Skip-gram:</em></strong><em class="it"/><strong class="kk iu"><em class="it">好用的</em> </strong> <em class="it">用</em> <strong class="kk iu"> <em class="it">少量的训练数据</em> </strong> <em class="it">，甚至代表好用的</em> <strong class="kk iu"> <em class="it">生僻字</em> </strong> <em class="it">或词组</em></p><p id="8185" class="ki kj ms kk b kl km ju kn ko kp jx kq mw ks kt ku mx kw kx ky my la lb lc ld im bi translated"><strong class="kk iu"> <em class="it"> CBOW: </em> </strong> <em class="it">数倍于</em><strong class="kk iu"><em class="it"/></strong><em class="it">比【skip-gram】</em><strong class="kk iu"><em class="it">精度稍好</em> </strong> <em class="it"> </em> <strong class="kk iu"> <em class="it">为</em></strong><em class="it"/><strong class="kk iu"><em class="it">频繁</em> </strong> <em class="it">单词</em></p></blockquote><p id="01db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更详细地说，由于<strong class="kk iu"> Skip-gram </strong>学习从给定单词预测上下文单词，在两个单词(一个不经常出现，另一个更频繁出现)并排放置的情况下，当涉及到最小化损失时，两者将具有相同的处理，因为每个单词将被视为目标单词和上下文单词。与<strong class="kk iu"> CBOW </strong>相比，不常用的单词将只是用于预测目标单词的上下文单词集合的一部分。因此，该模型将为不常用的单词分配低概率。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mz"><img src="../Images/ae21c4221a0d5b25792305be78b9363d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*o2FCVrLKtdcxPQqc.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Fig. 2 — Word2Vec — CBOW and skip-gram model architectures. Credit: <a class="ae le" href="http://idli.group/Natural-Language-Processing-using-Vectoriziation.html" rel="noopener ugc nofollow" target="_blank">IDIL</a></figcaption></figure><h1 id="5a85" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">实现进程</h1><p id="dd11" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在本文中，我们将实现<strong class="kk iu"> Skip-gram </strong>架构。为了便于阅读，内容分为以下几个部分:</p><ol class=""><li id="27d3" class="na nb it kk b kl km ko kp kr nc kv nd kz ne ld nf ng nh ni bi translated"><strong class="kk iu">数据准备</strong> —定义语料库，清理、规范化和分词</li><li id="5d44" class="na nb it kk b kl nj ko nk kr nl kv nm kz nn ld nf ng nh ni bi translated"><strong class="kk iu">超参数</strong> —学习率、时期、窗口大小、嵌入大小</li><li id="a934" class="na nb it kk b kl nj ko nk kr nl kv nm kz nn ld nf ng nh ni bi translated"><strong class="kk iu">生成训练数据</strong> —构建词汇，对单词进行一次性编码，构建将 id 映射到单词的字典，反之亦然</li><li id="1fc4" class="na nb it kk b kl nj ko nk kr nl kv nm kz nn ld nf ng nh ni bi translated"><strong class="kk iu">模型训练</strong> —通过正向传递传递编码的字，计算错误率，使用反向传播调整权重并计算损失</li><li id="16d5" class="na nb it kk b kl nj ko nk kr nl kv nm kz nn ld nf ng nh ni bi translated"><strong class="kk iu">推理</strong> —获取词向量，寻找相似词</li><li id="8655" class="na nb it kk b kl nj ko nk kr nl kv nm kz nn ld nf ng nh ni bi translated"><strong class="kk iu">进一步改进</strong> —通过跳过 gram 负采样(SGNS)和分层 Softmax 加快训练时间</li></ol><h1 id="f9b2" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">1.数据准备</h1><p id="276b" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">首先，我们从以下语料库开始:</p><blockquote class="mt mu mv"><p id="f4e7" class="ki kj ms kk b kl km ju kn ko kp jx kq mw ks kt ku mx kw kx ky my la lb lc ld im bi translated">自然语言处理和机器学习既有趣又令人兴奋</p></blockquote><p id="a219" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了简单起见，我们选择了一个没有标点符号和大写字母的句子。此外，我们没有删除停用词“和”和“是”。</p><p id="2cd0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在现实中，文本数据是非结构化的，可能是“脏的”。清理它们将涉及诸如删除停用词、标点符号、将文本转换为小写(实际上取决于您的使用情况)、替换数字等步骤。KDnuggets 有一篇关于这个过程的优秀文章。或者，Gensim 还提供了一个使用<code class="fe no np nq nr b"><a class="ae le" href="https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess" rel="noopener ugc nofollow" target="_blank">gensim.utils.simple_preprocess</a></code>执行简单文本预处理的函数，它将文档转换成一系列小写标记，忽略太短或太长的标记。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="86e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在预处理之后，我们继续对语料库进行标记。这里，我们在空白上标记我们的语料库，结果是一个单词列表:</p><blockquote class="mt mu mv"><p id="b054" class="ki kj ms kk b kl km ju kn ko kp jx kq mw ks kt ku mx kw kx ky my la lb lc ld im bi translated">【“自然”、“语言”、“处理”、“和”、“机器”、“学习”、“是”、“有趣”、“和”、“令人兴奋”】</p></blockquote><h1 id="ec38" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">2.超参数</h1><p id="509b" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在我们进入实际实现之前，让我们定义一些我们稍后需要的超参数。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="9635" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe no np nq nr b">[window_size]:</code>如上所述，上下文单词是与目标单词相邻的单词。但是这些词应该有多远或多近才能被认为是邻居呢？这就是我们将<code class="fe no np nq nr b">window_size</code>定义为 2 的地方，这意味着在目标单词左右 2 的单词被认为是上下文单词。参考下面的图 3，注意，当窗口滑动时，语料库中的每个单词都将是目标单词。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nu"><img src="../Images/4acaecbfafd919a5e4a643e34e922afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tD7P83Bl7dB91iNwYHEmEg.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">FIg. 3 — With a window_size of 2, the target word is highlighted in orange and context words in green</figcaption></figure><p id="cf87" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe no np nq nr b">[n]:</code>这是单词嵌入的维度，通常在 100 到 300 之间，取决于你的词汇量。尺寸大小超过 300 往往会使<a class="ae le" href="http://www.aclweb.org/anthology/D14-1162" rel="noopener ugc nofollow" target="_blank">收益递减</a>(参见第 1538 页图 2 (a))。请注意，尺寸也是隐藏层的大小。</p><p id="b57c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe no np nq nr b">[epochs]:</code>这是训练纪元的数目。在每个时期，我们循环所有的训练样本。</p><p id="2967" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe no np nq nr b">[learning_rate]:</code>学习率控制相对于损失梯度的权重调整量。</p><h1 id="a6be" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">3.生成培训数据</h1><p id="f1b2" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在这一节中，我们的主要目标是将我们的语料库转换成用于训练 Word2Vec 模型的一次性编码表示。从我们的语料库中，图 4 放大了 10 个窗口(#1 到#10)，如下所示。每个窗口由目标单词及其上下文单词组成，分别以橙色和绿色突出显示。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nv"><img src="../Images/089193242c2b56cfa41383f2dea023a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vunPUSipHyot3vvwLcND_w.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Fig. 4 — One-hot encoding for each target word and its context words</figcaption></figure><p id="430b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一个和最后一个训练窗口中的第一个和最后一个元素的示例如下所示:</p><blockquote class="mt mu mv"><p id="00d0" class="ki kj ms kk b kl km ju kn ko kp jx kq mw ks kt ku mx kw kx ky my la lb lc ld im bi translated"><em class="it"> # 1【目标(</em> <strong class="kk iu"> <em class="it">自然</em> </strong> <em class="it">)】、【上下文(</em>语言<em class="it">、</em>处理<em class="it">)】</em><br/><em class="it">【列表(</em><strong class="kk iu"><em class="it">)【1，0，0，0，0，0，0，0】</em></strong><br/>列表(【0，1，0</p><p id="1cc1" class="ki kj ms kk b kl km ju kn ko kp jx kq mw ks kt ku mx kw kx ky my la lb lc ld im bi translated"><em class="it">* * * * * * # 2 至#9 删除**** </em></p><p id="bda3" class="ki kj ms kk b kl km ju kn ko kp jx kq mw ks kt ku mx kw kx ky my la lb lc ld im bi translated"><em class="it"> #10【目标(</em> <strong class="kk iu"> <em class="it">精彩</em> </strong> <em class="it">)】、【上下文(</em>趣味<em class="it">、</em>、<em class="it">)【列表(</em><strong class="kk iu"><em class="it">)【0，0，0，0，0，0，1】</em></strong><em class="it">)<br/>列表(</em></p></blockquote><p id="f9ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了生成一次性训练数据，我们首先初始化<code class="fe no np nq nr b">word2vec()</code>对象，然后通过传递<code class="fe no np nq nr b">settings</code>和<code class="fe no np nq nr b">corpus</code>作为参数，使用对象<code class="fe no np nq nr b">w2v</code>调用函数<code class="fe no np nq nr b">generate_training_data</code>。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="6956" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在函数<code class="fe no np nq nr b">generate_training_data</code>中，我们执行了以下操作:</p><ol class=""><li id="2b75" class="na nb it kk b kl km ko kp kr nc kv nd kz ne ld nf ng nh ni bi translated"><code class="fe no np nq nr b">self.v_count</code> —词汇长度(注意，词汇是指语料库中唯一词的数量)</li><li id="d95c" class="na nb it kk b kl nj ko nk kr nl kv nm kz nn ld nf ng nh ni bi translated"><code class="fe no np nq nr b">self.words_list</code> —词汇表中的单词列表</li><li id="e0bf" class="na nb it kk b kl nj ko nk kr nl kv nm kz nn ld nf ng nh ni bi translated"><code class="fe no np nq nr b">self.word_index</code> —以词汇中的每个关键字为词，以值为索引的字典</li><li id="da2d" class="na nb it kk b kl nj ko nk kr nl kv nm kz nn ld nf ng nh ni bi translated"><code class="fe no np nq nr b">self.index_word</code> —字典，每个关键字作为索引，值作为词汇中的单词</li><li id="8640" class="na nb it kk b kl nj ko nk kr nl kv nm kz nn ld nf ng nh ni bi translated"><code class="fe no np nq nr b">for</code>循环使用<code class="fe no np nq nr b">word2onehot</code>函数将每个目标及其上下文单词的一键表示追加到<code class="fe no np nq nr b">training_data</code>。</li></ol><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h1 id="5fd5" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">4.模特培训</h1><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nw"><img src="../Images/735086a81994b3d088ac0da97bfc4c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uuVrJhSF89KGJPltvJ4xhg.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Fig. 5 — Word2Vec — skip-gram network architecture</figcaption></figure><p id="9c45" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用我们的<code class="fe no np nq nr b">training_data</code>，我们现在准备训练我们的模型。训练从<code class="fe no np nq nr b">w2v.train(training_data)</code>开始，我们传入训练数据并调用函数<code class="fe no np nq nr b">train</code>。</p><p id="4572" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Word2Vec 模型由两个权重矩阵(<code class="fe no np nq nr b">w1</code>和<code class="fe no np nq nr b">w2</code>)组成，出于演示目的，我们已经将值分别初始化为(9x10)和(10x9)的形状。这有助于计算反向传播误差，这将在本文后面讨论。在实际训练中，您应该随机初始化权重(例如使用<code class="fe no np nq nr b">np.random.uniform()</code>)。为此，注释第 9 行和第 10 行，取消第 11 行和第 12 行的注释。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h2 id="58d0" class="nx lw it bd lx ny nz dn mb oa ob dp mf kr oc od mh kv oe of mj kz og oh ml oi bi translated">训练—向前传球</h2><p id="1b0f" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">接下来，我们开始使用第一个训练示例来训练我们的第一个纪元，方法是将代表目标词的一键向量的<code class="fe no np nq nr b">w_t</code>传递给<code class="fe no np nq nr b">forward_pass</code>函数。在<code class="fe no np nq nr b">forward_pass</code>函数中，我们在<code class="fe no np nq nr b">w1</code>和<code class="fe no np nq nr b">w_t</code>之间执行点积以产生<code class="fe no np nq nr b">h</code>(第 24 行)。然后，我们使用<code class="fe no np nq nr b">w2</code>和<code class="fe no np nq nr b">h</code>执行另一个点积来产生输出层<code class="fe no np nq nr b">u</code>(第 26 行)。最后，我们运行<code class="fe no np nq nr b">u</code>到<a class="ae le" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> softmax </a>来强制每个元素在 0 和 1 的范围内，从而在返回预测矢量<code class="fe no np nq nr b">y_pred</code>、隐藏层<code class="fe no np nq nr b">h</code>和输出层<code class="fe no np nq nr b">u</code>之前给出我们预测的概率(第 28 行)。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="ceb2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我附上了一些截图，以显示第一个窗口(#1)中第一个训练样本的计算，其中目标词是“自然”，上下文词是“语言”和“处理”。请随意查看谷歌表单<a class="ae le" href="https://docs.google.com/spreadsheets/d/1mgf82Ue7MmQixMm2ZqnT1oWUucj6pEcd2wDs_JgHmco/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">中的公式。</a></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oj"><img src="../Images/cad175aee60017464f70a1b4828e05ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JHqzFok6Vz60HqoDf0OogQ.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Fig. 6— Calculate hidden layer, output later and softmax</figcaption></figure><h2 id="de97" class="nx lw it bd lx ny nz dn mb oa ob dp mf kr oc od mh kv oe of mj kz og oh ml oi bi translated">训练——错误、反向传播和损失</h2><p id="41ee" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated"><strong class="kk iu">误差— </strong>利用<code class="fe no np nq nr b">y_pred</code>、<code class="fe no np nq nr b">h</code>和<code class="fe no np nq nr b">u</code>，我们继续计算这组特定目标和上下文单词的误差。这是通过总结<code class="fe no np nq nr b">y_pred</code>和<code class="fe no np nq nr b">w_c</code>中每个上下文单词之间的差异来完成的。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ok"><img src="../Images/6b9b41fbaaf73ce07dbfd5fdf8df709b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pp5kV6uF7S0exTujskhbZw.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Fig. 7 — Calculating Error — context words are ‘language’ and ‘processing’</figcaption></figure><p id="c609" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">反向传播</strong> —接下来，我们使用反向传播函数<code class="fe no np nq nr b">backprop</code>，通过传入误差<code class="fe no np nq nr b">EI</code>、隐藏层<code class="fe no np nq nr b">h</code>和目标词<code class="fe no np nq nr b">w_t</code>的向量，计算我们需要使用函数<code class="fe no np nq nr b">backprop</code>改变权重的调整量。</p><p id="cfcd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了更新权重，我们将待调整的权重(<code class="fe no np nq nr b">dl_dw1</code>和<code class="fe no np nq nr b">dl_dw2</code>)乘以学习率，然后从当前权重(<code class="fe no np nq nr b">w1</code>和<code class="fe no np nq nr b">w2</code>)中减去它。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ol"><img src="../Images/f07757b1219ae94c67a1b7a823394ec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZWoH_NpUFGCPuHmtXUW5AA.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Fig. 8 — Backpropagation — Calculating delta for W1 and W2</figcaption></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi om"><img src="../Images/b44758d67d661ef6747f95309d4e2344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L5hsW2devGu2SfNtnir5Kg.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Fig. 9 — Backpropagation — Adjusting weights to get updated W1 and W2</figcaption></figure><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="a434" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">损失</strong> —最后，我们根据损失函数计算每个训练样本完成后的总损失。注意损失函数由两部分组成。第一部分是输出层中所有元素总和的负值(在 softmax 之前)。第二部分获取上下文单词的数量，并乘以输出层中所有元素的和的对数(在指数之后)。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/7134ef7a5f8a7e8f71cb75a4711052c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*XPhzBnf1xEb0u67qazx9nA.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Fig. 10 — Loss function for Word2Vec skip-gram. Credit: <a class="ae le" href="https://arxiv.org/pdf/1411.2738.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1411.2738.pdf</a></figcaption></figure><h1 id="d5da" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">5.推理</h1><p id="a626" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">现在我们已经完成了 50 个纪元的训练，两个权重(<code class="fe no np nq nr b">w1</code>和<code class="fe no np nq nr b">w2</code>)现在都准备好执行推理。</p><h2 id="f618" class="nx lw it bd lx ny nz dn mb oa ob dp mf kr oc od mh kv oe of mj kz og oh ml oi bi translated">获取单词的向量</h2><p id="c89c" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">有了一组训练好的权重，我们可以做的第一件事就是查看词汇表中某个单词的单词向量。我们可以简单地通过对照训练的权重来查找单词的索引(<code class="fe no np nq nr b">w1</code>)来做到这一点。在下面的例子中，我们查找单词“machine”的向量。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><pre class="lg lh li lj gt oo nr op oq aw or bi"><span id="dfe1" class="nx lw it nr b gy os ot l ou ov">&gt; print(w2v.word_vec("machine"))</span><span id="49a3" class="nx lw it nr b gy ow ot l ou ov">[ 0.76702922 -0.95673743  0.49207258  0.16240808 -0.4538815  -0.74678226  0.42072706 -0.04147312  0.08947326 -0.24245257]</span></pre><h2 id="6484" class="nx lw it bd lx ny nz dn mb oa ob dp mf kr oc od mh kv oe of mj kz og oh ml oi bi translated">查找相似的单词</h2><p id="e6de" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">我们可以做的另一件事是找到相似的单词。即使我们的词汇量很小，我们仍然可以通过计算单词之间的<a class="ae le" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">余弦相似度</a>来实现函数<code class="fe no np nq nr b">vec_sim</code>。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><pre class="lg lh li lj gt oo nr op oq aw or bi"><span id="d560" class="nx lw it nr b gy os ot l ou ov">&gt; w2v.vec_sim("machine", 3)</span><span id="eda8" class="nx lw it nr b gy ow ot l ou ov">machine 1.0<br/>fun 0.6223490454018772<br/>and 0.5190154215400249</span></pre><h1 id="2cf1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak"> 6。进一步改进</strong></h1><p id="d9e7" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">如果你还在读这篇文章，干得好，谢谢你！但这并不是结束。您可能已经注意到，在上面的反向传播步骤中，我们需要调整训练样本中未涉及的所有其他单词的权重。如果你的词汇量很大(例如几万个)，这个过程会花费很长时间。</p><p id="a27d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了解决这个问题，下面是 Word2Vec 中的两个特性，您可以实现它们来加快速度:</p><ul class=""><li id="022e" class="na nb it kk b kl km ko kp kr nc kv nd kz ne ld ox ng nh ni bi translated"><a class="ae le" href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" rel="noopener ugc nofollow" target="_blank">Skip-gram Negative Sampling(SGNS)</a>有助于加快训练时间，提高生成的单词向量的质量。这是通过训练网络仅修改一小部分权重而不是所有权重来实现的。回想一下上面的例子，我们每隔一个单词更新一次权重，如果 vocab 的大小很大，这将花费很长的<em class="ms">时间。使用 SGNS，我们只需要更新目标词和少量(例如 5 到 20 个)随机“负面”词的权重。</em></li><li id="a6ed" class="na nb it kk b kl nj ko nk kr nl kv nm kz nn ld ox ng nh ni bi translated"><a class="ae le" href="https://becominghuman.ai/hierarchical-softmax-as-output-activation-function-in-neural-network-1d19089c4f49" rel="noopener ugc nofollow" target="_blank">分级 Softmax </a>也是另一个加快训练时间的技巧，取代了原来的 Softmax。主要思想是，不需要评估所有的输出节点来获得概率分布，我们只需要评估它的大约 log(以 2 为基数)。它使用二叉树(<a class="ae le" href="https://en.wikipedia.org/wiki/Huffman_coding" rel="noopener ugc nofollow" target="_blank">霍夫曼编码树</a>)表示，其中输出层中的节点表示为树叶，其节点表示为与其子节点的相对概率。</li></ul><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="ab gu cl oy"><img src="../Images/709573703bf9d1a3bf02ca559faeb60b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*a4idodtq60y2U5HqpB_MTQ.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Fig. 11 — Hierarchical Binary Tree — Path from root to W2 is highlighted</figcaption></figure><p id="d4fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除此之外，为什么不尝试调整代码来实现连续词袋(CBOW)架构呢？😃</p><h1 id="de7c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="1656" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">本文是对 Word2Vec 和 Word 嵌入世界的介绍。同样值得注意的是，有预先训练好的嵌入可用，如<a class="ae le" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe </a>、<a class="ae le" href="https://fasttext.cc" rel="noopener ugc nofollow" target="_blank"> fastText </a>和<a class="ae le" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank"> ELMo </a>可以直接下载使用。还有 Word2Vec 的扩展比如<a class="ae le" href="https://radimrehurek.com/gensim/models/doc2vec.html" rel="noopener ugc nofollow" target="_blank"> Doc2Vec </a>和最近的<a class="ae le" href="https://code2vec.org/" rel="noopener ugc nofollow" target="_blank"> Code2Vec </a>其中文档和代码被转化为向量。😉</p><p id="fd68" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我要感谢<a class="oz pa ep" href="https://medium.com/u/fdf264797c2a?source=post_page-----13445eebd281--------------------------------" rel="noopener" target="_blank">任杰谭</a>、<a class="ae le" href="https://twitter.com/remykarem" rel="noopener ugc nofollow" target="_blank">莱米</a>和<a class="ae le" href="http://seowyuxin.com" rel="noopener ugc nofollow" target="_blank">庾信</a>花时间评论和阅读本文的草稿。💪</p><p id="0b79" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="ms">注:本文首发于我的博客</em><a class="ae le" href="https://derekchia.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets/" rel="noopener ugc nofollow" target="_blank"><em class="ms">https://derekchia . com/an-implementation-guide-to-word 2 vec-using-numpy-and-Google-sheets/</em></a></p><h1 id="7f87" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><div class="pb pc gp gr pd pe"><a href="https://github.com/nathanrooy/word2vec-from-scratch-with-python/blob/master/word2vec.py" rel="noopener  ugc nofollow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">nathan rooy/word 2 vec-用 python 从头开始</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">用 Python 从头开始实现了一个非常简单、简单、低效的 skip-gram word2vec</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">github.com</p></div></div><div class="pn l"><div class="po l pp pq pr pn ps lp pe"/></div></div></a></div><div class="pb pc gp gr pd pe"><a href="https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/" rel="noopener  ugc nofollow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">用 Python 和 NumPy 从头开始 Word2vec</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">TL；DR - word2vec 很牛逼，也真的很简单。了解它是如何工作的，并实现您自己的版本。自从加入…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">nathanrooy.github.io</p></div></div><div class="pn l"><div class="pt l pp pq pr pn ps lp pe"/></div></div></a></div><div class="pb pc gp gr pd pe"><a href="https://stats.stackexchange.com/questions/325053/why-word2vec-maximizes-the-cosine-similarity-between-semantically-similar-words" rel="noopener  ugc nofollow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">word2vec 为什么最大化语义相似词之间的余弦相似度</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">感谢您为交叉验证提供答案！你过去的一些回答不太受欢迎，你…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">stats.stackexchange.com</p></div></div><div class="pn l"><div class="pu l pp pq pr pn ps lp pe"/></div></div></a></div><div class="pb pc gp gr pd pe"><a rel="noopener follow" target="_blank" href="/hierarchical-softmax-and-negative-sampling-short-notes-worth-telling-2672010dbe08"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">分层软最大值和负采样:值得讲述的简短笔记</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">感谢观众对我上一篇(也是唯一一篇)帖子的意外和愉快的关注，这篇帖子是献给…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">towardsdatascience.com</p></div></div><div class="pn l"><div class="pv l pp pq pr pn ps lp pe"/></div></div></a></div></div></div>    
</body>
</html>