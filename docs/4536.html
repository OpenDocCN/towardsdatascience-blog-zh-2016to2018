<html>
<head>
<title>Silver, Gold &amp; Electrum: 3 Data Techniques for Multi-Task Deep Learning.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多任务深度学习的 3 种数据技术。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/silver-gold-electrum-3-data-techniques-for-multi-task-deep-learning-2655004970a2?source=collection_archive---------20-----------------------#2018-08-21">https://towardsdatascience.com/silver-gold-electrum-3-data-techniques-for-multi-task-deep-learning-2655004970a2?source=collection_archive---------20-----------------------#2018-08-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/6aae7a46c134a7ca20debec3f4e994b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LlS5j8Jr6U6WQMup.jpg"/></div></div></figure><div class=""/><p id="3e6e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">TLDR；</strong>这篇文章对多任务学习技术进行了高度的介绍，然后概述了三种实用的数据准备方法，我称之为银、金和银金矿。</p><h1 id="f2d7" class="kw kx jb bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">什么是多任务学习？</h1><div class="lu lv lw lx gt ab cb"><figure class="ly is lz ma mb mc md paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/96997e4da94fa724adc05e3be4af3575.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*7qvuzga33zahxgNO"/></div></figure><figure class="ly is lz ma mb mc md paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/56ff77add28c5fa000c9f5bd72373907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*a6PoRTBBQcjW2h-H"/></div></figure></div><p id="e9fd" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">多任务学习</strong> (MTL)是学习<strong class="ka jc">的<strong class="ka jc">共享表征</strong>互补</strong>任务<strong class="ka jc">任务</strong>以提高给定目标任务结果的过程。</p><p id="1bc8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">数据科学领域之外的 MTL 的一个很好的例子是健身房的组合练习，例如俯卧撑和引体向上，它们相互补充，以最大限度地增加全身的肌肉。</p><p id="dd21" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">单独俯卧撑主要锻炼胸部，其次是背部。而引体向上主要锻炼背部，其次是上胸部。然而，当一起做时，引体向上和俯卧撑相辅相成，为胸部和背部提供比单独做时更好的增益。</p><p id="5db4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了更好地理解这个概念，让我们后退一步，回顾一下机器学习和深度学习的基础知识。</p><p id="508d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">机器学习</strong> ML 的目标是找到<strong class="ka jc">特性</strong>来训练<strong class="ka jc">模型</strong>，该模型将输入数据(如图片、时间序列或音频)转换为给定的输出(如字幕、价格值、转录)。在传统的数据科学中，这些特征通常是手工选择的。</p><figure class="lu lv lw lx gt is gh gi paragraph-image"><div class="gh gi me"><img src="../Images/43569db71fbefed2f611e310ae6d0a07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*FJkeIzPkQ1RAos-Z"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">A canonical example of a feed forward deep neural network.</figcaption></figure><p id="11ba" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在<strong class="ka jc">深度学习(DL) </strong>中，我们通过将我们的输入表示为向量，并通过一系列巧妙的线性代数运算将其转换为给定的输出，来学习提取这些特征。</p><p id="09a9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后我们使用一个叫做<strong class="ka jc">损失函数</strong>的等式来评估我们的输出是否是我们所期望的。该过程的目标是使用来自每个训练输入的损失函数的结果来指导我们的模型提取将在下一次传递中导致较低损失值的特征。</p><blockquote class="mj mk ml"><p id="bf51" class="jy jz mm ka b kb kc kd ke kf kg kh ki mn kk kl km mo ko kp kq mp ks kt ku kv ij bi translated"><strong class="ka jc">“多任务学习</strong> (MTL)是通过组合<strong class="ka jc"/><strong class="ka jc">任务</strong>的<strong class="ka jc">损失函数</strong>来学习<strong class="ka jc">共享表征</strong>以提高给定目标任务的结果的过程。”</p></blockquote><p id="3907" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过在相关任务之间共享表示，我们可以使模型更好地概括每个原始任务。</p><p id="319d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">例如，让我们看看两个 NLP 任务<a class="ae mq" href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" rel="noopener ugc nofollow" target="_blank">词性标注</a>和<a class="ae mq" href="https://en.wikipedia.org/wiki/Phrase_chunking" rel="noopener ugc nofollow" target="_blank">句法短语分块</a>。如果我们使用相同的表示层，并通过结合我们的位置损失和分块预测来更新我们的权重，我们可以将我们的结果提高 0.7 的<a class="ae mq" href="http://anthology.aclweb.org/P16-2038" rel="noopener ugc nofollow" target="_blank"> F1 分数。当 MTL 模型共享所有相同的参数，但是通过在输出层直接组合两个或更多不同的损耗来更新时，这被称为<strong class="ka jc">硬参数共享</strong>。</a></p><p id="c24c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">另一方面，<strong class="ka jc">选择性参数共享，</strong>利用下游任务的分级性质，并且仅在相关层更新任务损失。想法是上游任务的更好的表示将导致更好的整体结果。在我们的分块示例中，使用软参数共享导致<a class="ae mq" href="http://anthology.aclweb.org/P16-2038" rel="noopener ugc nofollow" target="_blank"> 1.2 F1 分数比我们的独立结果</a>高。</p><figure class="lu lv lw lx gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mr"><img src="../Images/e4a0c9e2b417314ba81ab4b5c2bbaafe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wOb_0PyxlpOEupXfU51C3g.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">From left to right. F1 results on standalone chunking, MTL (Hard Parameter sharing), MTL (Soft Parameter Sharing)</figcaption></figure><p id="a29e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一个多任务学习的聪明例子是通过将预测<a class="ae mq" href="https://arxiv.org/abs/1604.03357" rel="noopener ugc nofollow" target="_blank">人阅读时的凝视位置的任务与压缩句子结合起来进行的。</a></p><figure class="lu lv lw lx gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ms"><img src="../Images/ca70f7961ae71691553f577b0141c28e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ECgkaBXMq1A0LL-x.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">An example of gaze tracking.</figcaption></figure><p id="336b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在对象检测模型中，诸如<a class="ae mq" href="https://github.com/fizyr/keras-retinanet" rel="noopener ugc nofollow" target="_blank"> RetinaNet </a>和<a class="ae mq" href="https://github.com/yhenon/keras-frcnn" rel="noopener ugc nofollow" target="_blank">faster CNN</a>之类的最先进的区域提议网络使用多任务学习来组合来自区域提议和分类任务的损失。</p><figure class="lu lv lw lx gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mt"><img src="../Images/d3bb3d2b505d7b7a068027ceeaf9f864.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jJMvYtkXVT4lkB2f.PNG"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">RetinaNet MultiTask Architecture Image <a class="ae mq" href="http://jleehome.blogspot.com/2018/02/notes-on-focal-loss-for-dense-object.html" rel="noopener ugc nofollow" target="_blank">Src</a></figcaption></figure><h2 id="d4e4" class="mu kx jb bd ky mv mw dn lc mx my dp lg kj mz na lk kn nb nc lo kr nd ne ls nf bi translated">MTL 为什么工作</h2><p id="960f" class="pw-post-body-paragraph jy jz jb ka b kb ng kd ke kf nh kh ki kj ni kl km kn nj kp kq kr nk kt ku kv ij bi translated">多任务学习似乎直觉上可行，原因如下:</p><ul class=""><li id="4f1f" class="nl nm jb ka b kb kc kf kg kj nn kn no kr np kv nq nr ns nt bi translated"><strong class="ka jc">隐性数据扩充</strong>——通过一起学习两个相关的任务，我们可以在每个任务之间转移隐性理解。</li><li id="7a92" class="nl nm jb ka b kb nu kf nv kj nw kn nx kr ny kv nq nr ns nt bi translated"><strong class="ka jc">聚焦表征——</strong>MTL 将习得的特征表征聚焦在正在学习的任务之间的交集上。</li><li id="8be8" class="nl nm jb ka b kb nu kf nv kj nw kn nx kr ny kv nq nr ns nt bi translated"><strong class="ka jc">注意</strong> <strong class="ka jc">规则化</strong>——MTL 提供监督<em class="mm">防止模特过度配合主任务</em> <strong class="ka jc"> <em class="mm">。</em> </strong>在这种情况下任务应该是<em class="mm">并行学习的。</em></li></ul><h2 id="86d2" class="mu kx jb bd ky mv mw dn lc mx my dp lg kj mz na lk kn nb nc lo kr nd ne ls nf bi translated">警告</h2><p id="ee41" class="pw-post-body-paragraph jy jz jb ka b kb ng kd ke kf nh kh ki kj ni kl km kn nj kp kq kr nk kt ku kv ij bi translated">然而，在执行多任务学习时会面临一些挑战。</p><ul class=""><li id="3a52" class="nl nm jb ka b kb kc kf kg kj nn kn no kr np kv nq nr ns nt bi translated">让它发挥作用并不容易。</li><li id="b4eb" class="nl nm jb ka b kb nu kf nv kj nw kn nx kr ny kv nq nr ns nt bi translated">很难找到好的任务对，大多数任务对在某些有害的性能上没有任何改进。</li><li id="fc8d" class="nl nm jb ka b kb nu kf nv kj nw kn nx kr ny kv nq nr ns nt bi translated">当两个或更多的任务重叠太多，MTL 往往提供微不足道的收益。</li><li id="2c1b" class="nl nm jb ka b kb nu kf nv kj nw kn nx kr ny kv nq nr ns nt bi translated">如果一个网络不够宽，MTL 伤害所有的任务。</li></ul><p id="95b4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">根据经验，更多的任务内数据会比简单地增加更多的任务带来更好的结果。然而，在处理不平衡数据集时，它被证明是有效的。我希望在未来我们可以建立一个框架来尽量减少这些警告。这些挫折通常是为 MTL 准备数据的结果。下面，我将概述三种适用于 MTL 的最佳实践数据技术。</p><p id="b7ab" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">关于多任务学习的更深入的阅读，我强烈推荐 Sebastian Ruder 的<a class="ae mq" href="http://ruder.io/multi-task/" rel="noopener ugc nofollow" target="_blank">帖子</a>和来自 BIU CS 89–687 课程的<a class="ae mq" href="http://u.cs.biu.ac.il/~89-687/lec9.pdf" rel="noopener ugc nofollow" target="_blank"> Yoav Goldberg 的幻灯片</a>，以及来自<a class="ae mq" href="https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies-ebook/dp/B071FGKZMH/ref=sr_1_1?ie=UTF8&amp;qid=1534841076&amp;sr=8-1&amp;keywords=yoav+goldberg+neural+network+methods+for+natural+language+processing" rel="noopener ugc nofollow" target="_blank"> Yoav 的书</a>的第 20 章。</p><h1 id="d3f4" class="kw kx jb bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">MTL 数据技术</h1><figure class="lu lv lw lx gt is gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/52a64bda130a394a92990f99ccbd104c.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/0*ZwWq7jbjk0unFUlg.jpg"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Assorted metal coins. Image <a class="ae mq" href="https://mysite.du.edu/~jcalvert/phys/copper.htm" rel="noopener ugc nofollow" target="_blank">Src</a></figcaption></figure><p id="0845" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如前所述，多任务学习的最大驱动力之一是数据。多任务学习面临的一个挑战通常是给定数据集中所有任务缺乏统一的预先标记或<strong class="ka jc">黄金</strong>标记的数据。</p><p id="7c08" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">例如，如果一个人想要在带有词性任务的<a class="ae mq" href="https://nlp.stanford.edu/projects/snli/" rel="noopener ugc nofollow" target="_blank"> SNLI 数据集</a>上用纯金色标签执行多任务学习，他们需要用 POS 标签手动注释 570，000 个人类书写的英语句子对中的每一个。</p><p id="621e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这种注释是昂贵的，并且通常是禁止的，下面我提出了三种不同的技术，可以应用于处理给定数据集的直接金标记的缺乏，我称之为银、金和银金矿。</p><h2 id="6802" class="mu kx jb bd ky mv mw dn lc mx my dp lg kj mz na lk kn nb nc lo kr nd ne ls nf bi translated"><strong class="ak">白银:</strong></h2><p id="532d" class="pw-post-body-paragraph jy jz jb ka b kb ng kd ke kf nh kh ki kj ni kl km kn nj kp kq kr nk kt ku kv ij bi translated">因为我们没有 POS 值，所以我们可以使用一个预先训练好的标记器，比如 Spacy 或 AllenNLP，来为我们的 SNLI 任务生成 POS 标记，并根据它们计算我们的损失。虽然不能保证标签是正确的，但是我们可以希望它们足够接近来指导我们的模型学习我们的主要 SNLI 任务所需的 POS 的正确特征。这种方法的优点是我们的主要 SNLI 任务的 POS 覆盖将是完整的，缺点是我们在我们的模型中引入了噪声。</p><h2 id="2f30" class="mu kx jb bd ky mv mw dn lc mx my dp lg kj mz na lk kn nb nc lo kr nd ne ls nf bi translated"><strong class="ak">金色:</strong></h2><p id="84b6" class="pw-post-body-paragraph jy jz jb ka b kb ng kd ke kf nh kh ki kj ni kl km kn nj kp kq kr nk kt ku kv ij bi translated">虽然我们可能没有给定任务(如 SNLI)的黄金 POS 数据，但我们仍然可以利用来自单独数据集(如 TreeBank)的黄金数据，然后我们可以根据 treebank 计算 POS 任务的损失，并根据 SNLI 数据集计算 SNLI 任务的损失。这种方法的优点是我们保证树库值没有噪声。缺点是，在树库和 SLNI 数据集之间的 POS 标签和句法结构之间可能存在不均匀的覆盖。</p><h2 id="3ac2" class="mu kx jb bd ky mv mw dn lc mx my dp lg kj mz na lk kn nb nc lo kr nd ne ls nf bi translated"><strong class="ak">银金矿:</strong></h2><p id="f131" class="pw-post-body-paragraph jy jz jb ka b kb ng kd ke kf nh kh ki kj ni kl km kn nj kp kq kr nk kt ku kv ij bi translated">Electrum 结合了上述黄金和白银技术，交替或组合加权损失。希望来自 treebank 数据集的黄金值将有助于调节白银数据中的噪声，但白银数据中的正确值将为我们的最终模型提供更好的覆盖范围。这种方法的缺点是需要计算额外的损失和两倍的数据。</p><h1 id="5fdd" class="kw kx jb bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated"><strong class="ak">行动号召</strong></h1><p id="3c7b" class="pw-post-body-paragraph jy jz jb ka b kb ng kd ke kf nh kh ki kj ni kl km kn nj kp kq kr nk kt ku kv ij bi translated">在我自己的实验中，金、银和银金矿的数据准备方法对我很有效。在未来的帖子中，我希望通过一个基本的 MTL 任务为这种方法提供更好的基准，并可能提供一个简单的 API 来管理 MTL 的数据集和损失。</p><p id="c9f7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果这是你会在下面发现有趣的评论或者发推特给我<a class="ae mq" href="https://twitter.com/pythiccoder" rel="noopener ugc nofollow" target="_blank">@ python coder</a>。希望有合作者</p><h2 id="1a1e" class="mu kx jb bd ky mv mw dn lc mx my dp lg kj mz na lk kn nb nc lo kr nd ne ls nf bi translated">资源</h2><ul class=""><li id="694d" class="nl nm jb ka b kb ng kf nh kj oa kn ob kr oc kv nq nr ns nt bi translated"><a class="ae mq" href="https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies-ebook/dp/B071FGKZMH/ref=sr_1_1?ie=UTF8&amp;qid=1534841076&amp;sr=8-1&amp;keywords=yoav+goldberg+neural+network+methods+for+natural+language+processing" rel="noopener ugc nofollow" target="_blank">自然语言处理中的神经网络方法 2017 章</a> 20</li><li id="e8f3" class="nl nm jb ka b kb nu kf nv kj nw kn nx kr ny kv nq nr ns nt bi translated"><a class="ae mq" href="http://ruder.io/multi-task/" rel="noopener ugc nofollow" target="_blank">深度神经网络中多任务学习概述</a></li><li id="2f71" class="nl nm jb ka b kb nu kf nv kj nw kn nx kr ny kv nq nr ns nt bi translated"><a class="ae mq" href="http://u.cs.biu.ac.il/~89-687/lec9.pdf" rel="noopener ugc nofollow" target="_blank">面向 NLP 课程的 Bar Ilan 深度学习 89–687</a></li><li id="7b05" class="nl nm jb ka b kb nu kf nv kj nw kn nx kr ny kv nq nr ns nt bi translated"><a class="ae mq" href="https://www.coursera.org/lecture/machine-learning-projects/multi-task-learning-l9zia" rel="noopener ugc nofollow" target="_blank"> Coursera MTL 讲座</a></li></ul></div></div>    
</body>
</html>