<html>
<head>
<title>Machine Learning 101: An Intuitive Introduction to Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习101:梯度下降的直观介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-101-an-intuitive-introduction-to-gradient-descent-366b77b52645?source=collection_archive---------2-----------------------#2018-03-06">https://towardsdatascience.com/machine-learning-101-an-intuitive-introduction-to-gradient-descent-366b77b52645?source=collection_archive---------2-----------------------#2018-03-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/47f45921423865bb3b91df9faed4795c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8cFeuAM4kj7A8-Z9rELNYg.jpeg"/></div></div></figure><p id="c962" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">毫无疑问，梯度下降是大多数机器学习(ML)算法的核心和灵魂。我绝对相信你应该花时间去理解它。因为一旦你这样做了，首先，你会更好地理解大多数最大似然算法是如何工作的。此外，理解基本概念是发展对更复杂主题的直觉的关键。</p><p id="bd05" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了理解梯度下降的核心，让我们有一个运行的例子。这个任务是这个领域中的一个老任务——使用一些历史数据作为先验知识来预测房价。</p><p id="2247" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但我们的目标是讨论梯度下降。为了做到这一点，让我们的例子足够简单，这样我们就可以集中在好的部分。</p><p id="12f3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是，在我们继续之前，你可以在这里得到代码<a class="ae kw" href="https://github.com/sthalles/blog-resources" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="ab52" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">基本概念</h1><p id="4927" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">假设你想爬一座很高的山。你的目标是最快到达山顶。你环顾四周，意识到你有不止一条路可以开始。因为你在底部，所有这些选项似乎都让你更接近顶峰。</p><p id="7b69" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是你想以最快的方式到达顶峰。那么，你如何做到这一点呢？你如何迈出一步让你尽可能接近顶峰？</p><p id="49fc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">到目前为止，还不清楚如何迈出这一步。这就是梯度可以帮助你的地方。</p><p id="90cd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">正如汗学院<a class="ae kw" href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/modal/v/gradient" rel="noopener ugc nofollow" target="_blank">的视频</a>中所说，梯度捕捉了一个多变量函数的所有偏导数。</p><p id="dbf8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们一步一步来，看看效果如何。</p><p id="7cd8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">简单来说，导数是函数在给定点的变化率或斜率。</p><p id="cdec" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以<em class="ma"> f(x) = x </em>函数为例。<em class="ma"> f(x) </em>的导数是另一个函数<em class="ma">f’(x)</em>，它计算<em class="ma"> f(x) </em>在给定点<em class="ma"> x </em>的斜率。这种情况下，对于<em class="ma"> x = 2 </em>，<em class="ma"> f(x) = x </em>的斜率为<em class="ma"> 2x </em>或<em class="ma"> 2*2 = 4 </em>。</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/82b761762ac594b694945e766771d97f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*J-TbI94E1nXNHIubJc7VeQ.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">The slope of f(x)=x² at different points.</figcaption></figure><p id="a45f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">简单来说，导数指向<strong class="ka ir">最陡上坡</strong>的方向。好的一面是，梯度是完全一样的。除了一个例外，梯度是存储偏导数的向量值函数。换句话说，梯度是一个向量，它的每个分量都是一个特定变量的偏导数。</p><p id="06d4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">再以函数，<em class="ma"> f(x，y) = 2x + y </em>为例。</p><p id="2f19" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里，<em class="ma"> f(x，y) </em>是一个多元函数。它的梯度是一个向量，包含<em class="ma"> f(x，y) </em>的偏导数。第一个相对于x，第二个相对于y。</p><p id="78f9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果我们计算<em class="ma"> f(x，y) </em>的偏导数，我们得到。</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/f06951dd7702b557599491181c6d7a55.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*5lwyAtotE9QplnyWCkN1xw.png"/></div></figure><p id="4971" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以梯度是下面的向量:</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/507b030f8de45461c058e4f8b991f203.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*kpRcEaYu9dEI1wc0gLdPxA.png"/></div></figure><p id="7230" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请注意，每个分量表示每个函数变量的最陡上升方向。换个说法，梯度指向函数增加最多的方向。</p><p id="1556" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">回到爬山的例子，坡度把你指向最快到达山顶的方向。换句话说，梯度指向表面的较高高度。</p><p id="fd28" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">同样，如果我们得到一个有4个变量的函数，我们将得到一个有4个偏导数的梯度向量。一般来说，<em class="ma"> n变量</em>函数会产生一个<em class="ma"> n维</em>梯度向量。</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/ee9072b46f21a34860258664ef1fdcca.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*cR6W8zCEhKSz8XnAgll2_g.png"/></div></figure><p id="ad1b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然而，对于梯度下降，我们不想尽可能快地最大化<em class="ma"> f </em>，我们想要<strong class="ka ir">最小化</strong>f。</p><p id="6cf0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是，让我们先定义我们的任务，事情会看起来干净得多。</p><h1 id="1659" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">预测房价</h1><p id="d86d" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们要解决根据历史数据预测房价的问题。为了建立一个机器学习模型，我们通常至少需要3样东西。一个问题<em class="ma"> T </em>，一个性能度量<em class="ma"> P </em>，以及一个经验<em class="ma"> E </em>，我们的模型将从这些地方学习模式。</p><p id="2cb8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了解决任务<em class="ma"> T </em>，我们将使用一个简单的线性回归模型。这个模型将从经验中学习，经过训练后，它将能够将其知识推广到看不见的数据中。</p><p id="64ae" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">线性模型是一个很好的学习模型。它是许多其他ML算法的基础，如神经网络和支持向量机。</p><p id="2073" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于这个例子，体验<em class="ma"> E </em>，是房屋数据集。房屋数据集包含圣路易斯奥比斯波县及其周边地区最近的房地产列表。</p><p id="37d5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该集合包含781条数据记录，可在此处以CSV格式下载。在8个可用特性中，为了简单起见，我们将只关注其中的两个:尺寸和价格。对于781条记录中的每一条，以平方英尺为单位的大小将是我们的输入特征，价格将是我们的目标值。</p><p id="83c5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，为了检查我们的模型是否恰当地从经验中学习，我们需要一种机制来衡量它的性能。为此，我们采用均方误差(MSE)作为性能指标。</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mn"><img src="../Images/debf876326cd19b43bd7fee748720312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*GQ6vjZ9j0K5V7BReHywWAA.png"/></div></div></figure><p id="6d87" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">多年来，MSE一直是线性回归的标准。但是从理论上讲，任何其他的误差度量，比如绝对误差，都是可行的。MSE的一些好处是它对较大误差的惩罚大于绝对误差。</p><p id="cbf2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">既然我们已经形式化了我们的学习算法，让我们深入代码。</p><p id="c906" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，我们使用Pandas加载python中的数据，并分离大小和价格特性。之后，我们将数据标准化，以防止一些特征的价值超过其他一些特征。此外，梯度下降与归一化数据相比收敛速度更快。</p><figure class="mc md me mf gt jr"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="58e3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下图，你可以通过它的面积以平方米为单位看到房价的分布。</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/51d095f9830a78953ae1fa994861f039.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*jnKw9n73EDnZPeez62V4yQ.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Distribution of house prices by Size. The data is normalized to the [0,1] interval.</figcaption></figure><p id="2b3f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">线性回归模型的工作原理是在数据上画一条线。因此，我们的模型由一个简单的直线方程表示。</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/f0a747c5b3c1a58c318e74a784f70d1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*SuNmDkv0qNJbKGZoEoGBmw.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Line equation. m and b are the slope and the y-intercept respectively. The x variable is the placeholder for the input values.</figcaption></figure><p id="db84" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于线性模型，两个自由参数是斜率<em class="ma"> m </em>和y截距<em class="ma"> y </em>。这两个变量是我们要改变的旋钮，为了找到最好的线方程。</p><p id="c9ba" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">迭代地，我们将对它们进行细微的改变，这样它就可以沿着误差面上最陡下降的方向。在每次迭代之后，这些权重变化将改进我们的模型，以便它能够代表数据集的趋势。</p><p id="cf67" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在继续之前，请记住，对于梯度下降，我们希望采取与梯度相反的方向。</p><p id="2280" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你可以把梯度下降想象成一个球滚下山谷。我们希望它坐落在大山的最深处，然而，很容易看出事情可能会出错。</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ms"><img src="../Images/6d86ecb1f19c2e02eb88171be0aa738d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QEME_QUKOjntJpBBUdgTNA.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">In analogy, we can think of Gradient Descent as being a ball rolling down on a valley. The deepest valley is the optimal global minimum and that is the place we aim for.</figcaption></figure><p id="7146" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">根据球开始滚动的位置，它可能会停在谷底。但不是在最低的一个。这被称为局部最小值，在我们的模型中，谷是误差面。</p><p id="3f02" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">注意，在类比中，并不是所有的局部最小值都是不好的。有些其实和最低(全局)的差不多低(好)。事实上，对于高维误差曲面，最常见的是采用这些局部最小值中的一个(不算太坏)。</p><p id="d913" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">类似地，我们初始化模型权重的方式可能导致它停留在局部最小值。为了避免这种情况，我们用来自具有零均值和低方差的随机正态分布的值来初始化两个权重向量。</p><p id="6777" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在每次迭代中，我们将随机选取数据集的一个子集，并将其与权重进行线性组合。这个子集被称为<em class="ma">小批量</em>。线性组合后，我们将结果向量输入MSE函数，以计算当前误差。</p><p id="c135" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有了这个误差信号，我们就可以计算误差的偏导数，得到梯度。</p><p id="fc6d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，我们得到关于<em class="ma"> W0 </em>的偏导数。</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/f6869fcc960fdeb1632043698132cb31.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*YEmG06l_rVPr_VxHpqwoow.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Partial with respect to <em class="mu">W0</em></figcaption></figure><p id="decf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第二，我们做同样的事情，但是把<em class="ma"> W1 </em>作为演员。</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/29b943701e37dbd1d297aa8964f563a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*33cCzVSSvn7abeM8FD3YyA.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Partial with respect to <em class="mu">W1</em></figcaption></figure><p id="3c49" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有了这两个分音，我们就有了梯度向量:</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/949ffde91419c7f5c63b64208f77750d.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*xYJv4gFidsB7DxzRJ-HB0A.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">The Gradient</figcaption></figure><p id="a177" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<em class="ma"> Err </em>是MSE误差函数。</p><p id="d53f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这样，我们的下一步是使用梯度更新权重向量<em class="ma"> W0 </em>和<em class="ma"> W1 </em>，以最小化误差。</p><p id="f7aa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们希望更新权重，以便它们可以在下一次迭代中降低误差。我们需要使它们遵循每个梯度信号的相反方向。为了做到这一点，我们要在那个方向上迈出η大小的小步。</p><p id="54a7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">步长η是学习速率，它控制学习速率。根据经验，一个好的起点是0.1。最后，更新步骤规则被设置为:</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/031f91f24d4f9c92322024b4f4d257d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*IrVapDSwToF6nHVuS35MzQ.png"/></div></figure><p id="192c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在代码中，完整的模型如下所示。看两个梯度<em class="ma"> DW0 </em>和<em class="ma"> DW1 </em>前面的<strong class="ka ir">负号</strong>。这保证了我们将采取与梯度方向相反的步骤。</p><figure class="mc md me mf gt jr"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="c97f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">更新重量后，我们用另一个随机小批量重复该过程。就是这样。</p><p id="7dff" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一步一步地，每一次权重更新都会导致线条向其最佳表现方向发生微小的移动。最后，当误差方差足够小时，我们可以停止学习。</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/909863dbccb35778fe583dd0c04d2e1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*1mwtcdCPw0eQbpUkus8vwA.gif"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Linear model conversion over time. The first weight updates cause the line to rapidly reach an ideal representation.</figcaption></figure><p id="b314" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个版本的梯度下降被称为小批量随机梯度下降。在这个版本中，我们使用训练数据的一个小子集来计算梯度。每个小批量梯度提供了最佳方向的近似值。即使梯度没有指向准确的方向，但实际上它收敛到非常好的解。</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/077a59b80af5698150cbcc0e35307f97.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*v8cfIyKL5t0rj2Arjw60UA.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Error signal by epoch. Note that after decreasing the error signal very fast the model slows down and converges.</figcaption></figure><p id="2307" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你仔细观察错误/事件图表，你会发现在开始时，学习的速度更快。</p><p id="6c74" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然而，经过一段时间后，它开始减速并趋于平稳。发生这种情况是因为，在开始时，指向最陡下降的梯度向量的幅度很大。结果，两个权重变量<em class="ma"> W0 </em>和<em class="ma"> W1 </em>遭受更剧烈的变化。</p><p id="1377" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然而，随着它们越来越接近误差表面的顶点，梯度慢慢变得越来越小，这导致权重的变化非常小。</p><p id="ed02" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，学习曲线稳定下来，这个过程就完成了。</p><p id="bb34" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">尽情享受吧！</p><figure class="mc md me mf gt jr gh gi paragraph-image"><div class="ab gu cl na"><img src="../Images/54a0a5fa3fed5f48f4692e244b5529b6.png" data-original-src="https://miro.medium.com/v2/1*RzOHqzrJEIrB5ZY0nJAcvg.gif"/></div></figure></div></div>    
</body>
</html>