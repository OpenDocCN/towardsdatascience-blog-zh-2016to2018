<html>
<head>
<title>Object Detection using Google AI Open Images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用谷歌人工智能开放图像的对象检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/object-detection-using-google-ai-open-images-541ea601cfa5?source=collection_archive---------6-----------------------#2018-12-14">https://towardsdatascience.com/object-detection-using-google-ai-open-images-541ea601cfa5?source=collection_archive---------6-----------------------#2018-12-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="96f4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">学会打造自己的自动驾驶汽车！！！….开玩笑</h2></div><p id="107d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Atindra Bandi、Alyson Brown、Sagar Chadha、Amy Dang 和 Jason Su 的项目</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/63aa7c2c6188a509c7817696d14b9038.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RP6Q-267Axv09G6p"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Photo by <a class="ae ls" href="https://unsplash.com/@introspectivedsgn?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Erik Mclean</a> on <a class="ae ls" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="77c8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你上一次只用脸登录手机是什么时候？或者点击了一张与一些朋友的自拍，并使用了 Snapchat 滤镜，在你的脸上放置了一些花哨的狗耳朵？你知道吗，这些很酷的功能是由一个奇特的神经网络实现的，它不仅可以识别照片中有一张脸，还可以检测耳朵应该去哪里。从某种意义上说，你的手机可以“看到”你，它甚至知道你长什么样！</p><p id="4af1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">帮助计算机“看”的技术被称为“<strong class="kh ir">计算机视觉”</strong>。近年来，由于计算能力的爆炸使得深度学习模型更快更可行，计算机视觉应用变得越来越普遍。许多公司，如亚马逊、谷歌、特斯拉、脸书和微软，都在大力投资这项技术及其应用。</p><h1 id="3a24" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">计算机视觉任务</strong></h1><p id="2c95" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">我们关注两个主要的计算机视觉任务——图像分类和目标检测。</p><ol class=""><li id="ee95" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated"><strong class="kh ir">图像分类</strong>专注于将图像分组到预定义的类别中。为了实现这一点，我们需要有我们感兴趣的类的多个图像，并训练计算机将像素数字转换为符号。这只是说电脑看到猫的照片，说里面有猫。</li><li id="ad85" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated"><strong class="kh ir">对象检测</strong>利用图像分类器来找出图像中存在的内容和位置。通过使用卷积神经网络(CNN ),这些任务变得更加容易，这使得在图像的一次通过中检测多个类别成为可能。</li></ol><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ne"><img src="../Images/5f6ca4e5f67f2a1876c683e2a7f01de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ly6JczJzzpIxQl1S"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">For more details on the difference in such tasks, please reference the following <a class="ae ls" rel="noopener" target="_blank" href="/evolution-of-object-detection-and-localization-algorithms-e241021d8bad">article</a>.</figcaption></figure><h1 id="9c83" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">计算机视觉很酷！</h1><p id="d9bc" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">认识到未来许多有趣的数据科学应用将涉及图像工作，我和我的初露头角的数据科学家团队决定在 Kaggle 上举办的<a class="ae ls" href="https://www.kaggle.com/c/google-ai-open-images-object-detection-track" rel="noopener ugc nofollow" target="_blank">谷歌人工智能开放图像挑战赛</a>上一试身手。我们认为这是接触神经网络和卷积的绝佳机会，有可能给我们的教授和同学留下深刻印象。这个挑战为我们提供了 170 万张<strong class="kh ir"> </strong>图片<strong class="kh ir"> </strong>和 1200 万张<strong class="kh ir"/><strong class="kh ir"/>包围盒标注(它们相对于图片的 X 和 Y 坐标)的 500 个<strong class="kh ir"> </strong>对象类。你可以在这里找到数据<a class="ae ls" href="https://www.figure-eight.com/dataset/open-images-annotated-with-bounding-boxes/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="75ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们强烈推荐任何想了解 CNN 的人去读吴恩达的关于卷积神经网络的课程。</p><h1 id="c28e" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">弄脏我们的手！</h1><p id="ac46" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><strong class="kh ir">探索性数据分析</strong> —和所有的数据分析一样，我们开始探索我们拥有的图像和我们需要探测的物体类型。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nf"><img src="../Images/0ee1499cdfe15ab9501cc1c05dd56e72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eH16__PHvmrH_UR1"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Frequency of Classes in the Training Dataset</figcaption></figure><p id="effe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">快速浏览训练图像可以发现，就出现的次数而言，某些物体比其他物体更具存在感。上图显示了前 43 名班级的分布情况。很明显，这是一个巨大的差距，需要以某种方式解决。为了节省时间和金钱(GPU 成本很高:( )我们选择了前面提到的 43 个对象类和包含这些对象的大约 300K 图像的子集。对于训练数据中的每个对象类，我们有大约 400 张图像。</p><h2 id="99ca" class="ng lu iq bd lv nh ni dn lz nj nk dp md ko nl nm mf ks nn no mh kw np nq mj nr bi translated">选择对象检测算法</h2><p id="4f5f" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">我们考虑了各种对象检测算法，包括 VGG、盗梦空间和 YOLO，但最终<strong class="kh ir">选择了 YOLO 算法</strong>，因为它的速度、计算能力和大量在线文章可以指导我们完成这个过程。面对计算和时间的限制，我们做了两个关键的决定-</p><ol class=""><li id="ac60" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">使用被训练来识别某些物体的 YOLO v2 模型。</li><li id="6b51" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated">利用迁移学习来训练最后一个卷积层，以识别以前未见过的对象，如吉他、房子、男人/女人、鸟等。</li></ol><h2 id="e8b2" class="ng lu iq bd lv nh ni dn lz nj nk dp md ko nl nm mf ks nn no mh kw np nq mj nr bi translated">对 YOLO 的投入</h2><p id="c00c" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">YOLO 算法需要一些特定的输入-</p><ol class=""><li id="af0e" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated"><strong class="kh ir">输入图像尺寸</strong> — YOLO 网络设计用于特定的输入图像尺寸。我们发送了尺寸为 608 * 608 的图像。</li><li id="7206" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated"><strong class="kh ir">班数</strong> — 43。这是定义 YOLO 输出尺寸所必需的。</li><li id="baaf" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated"><strong class="kh ir">锚箱— </strong>要使用的锚箱的数量和尺寸。</li><li id="e26e" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated"><strong class="kh ir">置信度和 IoU 阈值</strong> —定义选择哪些锚框以及如何在锚框之间挑选的阈值。</li><li id="4e55" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated"><strong class="kh ir">带边框信息的图像名称</strong> —对于每张图像，我们需要以如下所示的特定格式向 YOLO 提供图像中的内容</li></ol><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ns"><img src="../Images/b31282ccdf130517893eb0dc7faa4336.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4LCkDjRARFBtYmalX5bq8w.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Sample input for YOLO</figcaption></figure><p id="d605" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">下面是 YOLO 输入的代码片段</em></p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nt nu l"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Inputs into YOLO</figcaption></figure><h2 id="9bb8" class="ng lu iq bd lv nh ni dn lz nj nk dp md ko nl nm mf ks nn no mh kw np nq mj nr bi translated">YOLO v2 架构</h2><p id="ef5a" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">该架构如下所示——它有 23 个卷积层，每个卷积层都有自己的批量归一化、漏 RELU 激活和最大池。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nv"><img src="../Images/239a5811b1840b3f4fc861b0f5690ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*c07LAfYZrIvYa1Bw"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Representative of the actual YOLO v2 architecture.</figcaption></figure><p id="0e9a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些层试图从图像中提取多个重要特征，以便可以检测到各种类别。为了对象检测的目的，YOLO 算法将输入图像分成 19*19 的网格，每个网格具有 5 个不同的锚框。然后，它尝试检测每个网格单元中的类，并将一个对象分配给每个网格单元的 5 个锚定框之一。锚定框的形状不同，旨在为每个网格单元捕捉不同形状的对象。</p><p id="8904" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">YOLO 算法为每个定义的锚定框输出一个矩阵(如下所示)</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/8754e0bf3d8a9ac23b37ac0cc299a325.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/0*IEhgzOTyuvilAirO"/></div></figure><p id="60d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们必须为 43 个类训练算法，我们得到的输出维数为:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nx"><img src="../Images/2d1de97af21e226669edd25dcf426c4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y1E8dZjHJWnEmLSeDVejaQ.png"/></div></div></figure><p id="941b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些矩阵给我们观察每个锚盒中的对象的概率，以及该对象是什么类的概率。为了过滤掉不具有任何类别或者具有与一些其他框相同的对象的锚框，我们使用两个阈值— <strong class="kh ir"> IoU 阈值</strong>来过滤掉捕获相同对象的锚框，以及<strong class="kh ir">置信度阈值</strong>来过滤掉不包含任何具有高置信度的类别的框。</p><p id="253d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">下面是 YOLO v2 架构最后几层的插图:</em></p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nt nu l"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Last few layers of YOLO v2 architecture (Only for illustration purposes)</figcaption></figure><h2 id="54db" class="ng lu iq bd lv nh ni dn lz nj nk dp md ko nl nm mf ks nn no mh kw np nq mj nr bi translated">迁移学习</h2><p id="f4d6" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">转移学习是指获得一个已经训练好的神经网络来分类图像，并将其用于我们的特定目的。这节省了我们的计算时间，因为我们不需要训练大量的权重——例如，我们使用的 YOLO v2 模型有大约 5000 万个权重——在我们使用的谷歌云实例上，训练可能需要 4-5 天。</p><p id="1596" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了成功实施迁移学习，我们必须对我们的模型进行一些更新:</p><ul class=""><li id="9cf9" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la ny mw mx my bi translated"><strong class="kh ir">输入图像尺寸— </strong>我们下载的型号使用的输入图像尺寸为 416*416。由于我们训练的一些对象非常小——鸟、鞋——我们不想把输入图像挤压得太厉害。为此，我们使用大小为 608*608 的输入图像。</li><li id="97ce" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la ny mw mx my bi translated"><strong class="kh ir">网格大小— </strong>我们更改了网格大小的尺寸，以便它将图像划分为 19*19 个网格单元，而不是我们下载的模型的默认 13*13。</li><li id="e026" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la ny mw mx my bi translated"><strong class="kh ir">输出层— </strong>由于我们训练的是不同数量的类 43，而不是原始模型训练的 80，因此输出层被更改为输出矩阵维度，如上所述。</li></ul><p id="fa24" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们重新初始化了 YOLO 的最后一个卷积层的权重，以在我们的数据集上训练它，最终帮助我们识别独特的类。下面是相同的代码片段-</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nt nu l"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Re-initializing the last convolution layer of YOLO</figcaption></figure><h1 id="35eb" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">价值函数</h1><p id="4ba1" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">在任何对象检测问题中，我们都希望在图像中以<strong class="kh ir">高置信度</strong>识别在<strong class="kh ir">正确位置</strong>的<strong class="kh ir">正确对象</strong>。<a class="ae ls" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">成本函数</a>有 3 个主要组成部分:</p><ol class=""><li id="45d3" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated"><strong class="kh ir">分类损失:</strong>是类别条件概率的平方误差，如果检测到物体。因此，损失函数仅在对象存在于网格单元中时惩罚分类错误。</li><li id="465a" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated"><strong class="kh ir">定位损失:</strong>它是预测的边界框位置和尺寸与地面真实框的平方误差，如果这些框负责检测物体的话。为了补偿边界框坐标预测的损失，我们使用一个正则化参数(ƛcoord).此外，为了确保较大框中的小偏差没有较小框中的小偏差重要，该算法使用边界框宽度和高度的平方根。</li><li id="6e14" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated"><strong class="kh ir">置信度损失:</strong>是包围盒置信度得分的平方误差。大多数盒子不负责检测物体，因此等式被分成两部分，一部分用于检测物体的盒子，另一部分用于其余的盒子。正则项λnoobj(默认值:0.5)应用于后一部分，以降低未检测到对象的框的权重。</li></ol><p id="057c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请随意参考原始 YOlO <a class="ae ls" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>以获得成本函数的详细信息。</p><p id="4bdb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">YOLO 的妙处在于，它使用的误差易于使用优化函数进行优化，如随机梯度下降(SGD)、带动量的 SGD 或 Adam 等。下面的代码片段显示了我们用于优化成本函数的参数。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nt nu l"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Training algorithm for YOLO (Adam optimizer)</figcaption></figure><h1 id="ea0d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">输出精度—平均精度(地图得分):</strong></h1><p id="7886" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">在对象检测中有许多指标来评估模型，对于我们的项目，我们决定使用 mAP 得分，这是所有 IoU 阈值上不同召回值的最大精度的平均值。为了理解 mAP，我们将快速回顾一下 precision、recall 和 union 上的交集)。</p><h2 id="7ce8" class="ng lu iq bd lv nh ni dn lz nj nk dp md ko nl nm mf ks nn no mh kw np nq mj nr bi translated"><em class="nz">精度&amp;召回</em></h2><p id="f29d" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">精度衡量正确的正面预测的百分比。回忆是所有可能结果中真正肯定的比例。这两个值是反向相关的，并且还依赖于您为模型设置的模型得分阈值(在我们的例子中，它是置信度得分)。它们的数学定义如下:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oa"><img src="../Images/3c2859482a88f2adfd5aa878d543d28e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c7d8kk5P0qXkmNGwMxHfIA.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk"><a class="ae ls" href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173" rel="noopener">Source</a></figcaption></figure><h2 id="3e5a" class="ng lu iq bd lv nh ni dn lz nj nk dp md ko nl nm mf ks nn no mh kw np nq mj nr bi translated"><em class="nz">交集超过联合(借据)</em></h2><p id="fab5" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">IoU 衡量两个区域之间有多少重叠，等于重叠面积与并集面积之比。这测量你的预测(从你的物体探测器)与地面事实(真实物体边界)相比有多好。总而言之，mAP 得分是所有 IoU 阈值的平均 AP。</p><h1 id="6ee9" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结果</h1><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/40604f4087b2033a60d655d0d97ec8ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/0*OrWP3clyl1VlGDvg"/></div></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/a94fc90651693186353daf8321e086a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/0*2GgkbXusZbKPAeDL"/></div></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/85c883bd3cb19fd4fb380cfbda288dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/0*xFrZ-9PNypo4qCUW"/></div></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/7f9cb30a352222ffd4cb8f1ed837e07d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/0*aTuTeAl2S9MpVTCe"/></div></figure><h1 id="27f9" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结论</h1><p id="8efc" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">物体检测不同于其他计算机视觉任务。您可以使用预先训练的模型，并根据需要进行编辑以满足您的需求。你可能需要 GCP 或另一个允许更高计算能力的平台。数学很难，看别人文章，不及格快。</p><h2 id="c543" class="ng lu iq bd lv nh ni dn lz nj nk dp md ko nl nm mf ks nn no mh kw np nq mj nr bi translated">经验教训</h2><p id="6481" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">一开始，我们发现该模型无法预测许多类别，因为许多类别只有少量训练图像，这导致了不平衡的训练数据集。因此，我们决定只使用最流行的 43 个类，这不是一个完美的方法，但每个类至少有 500 张图片。然而，我们预测的可信度仍然很低。为了解决这个问题，我们选择了包含目标类的图像。</p><p id="c381" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">物体检测是一个非常具有挑战性的话题，但是不要害怕，尽量从各种在线开放资源中学习，比如 Coursera、YouTube 教学视频、GitHub 和 Medium。所有这些免费的智慧可以帮助你在这个令人惊叹的领域取得成功！</p><h2 id="cf16" class="ng lu iq bd lv nh ni dn lz nj nk dp md ko nl nm mf ks nn no mh kw np nq mj nr bi translated">未来工作——继续或改进</h2><ol class=""><li id="1f09" class="mq mr iq kh b ki ml kl mm ko od ks oe kw of la mv mw mx my bi translated">在更多类别上训练模型，以检测更多种类的对象。为了达到这个目标，我们需要首先解决不平衡数据的问题。一个潜在的解决方案是，我们可以用这些更稀有的类收集更多的图像。</li></ol><p id="28e0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> a .数据扩充</strong> —稍微改变现有图像以创建新图像</p><p id="b1c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> b .图像复制</strong> —我们可以多次使用相同的图像来训练特定稀有类上的算法</p><p id="d885" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">c . Ensemble</strong>——在流行类上训练一个模型，在稀有类上训练另一个模型，并使用两者的预测。</p><p id="d2d9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.此外，我们可以尝试不同型号的合奏，如 MobileNet、VGG 等。这是也用于对象检测的卷积神经网络算法。</p><p id="35a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你想详细了解我们团队的代码，这里有 GitHub 的链接。请随时提供任何反馈或意见！</p><div class="og oh gp gr oi oj"><a href="https://github.com/bandiatindra/Object-Detection-Project" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd ir gy z fp oo fr fs op fu fw ip bi translated">bandiatindra/物体探测项目</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">通过在 GitHub 上创建一个帐户，为 bandiatindra/Object-Detection 项目开发做出贡献。</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">github.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox lm oj"/></div></div></a></div></div></div>    
</body>
</html>