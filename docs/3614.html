<html>
<head>
<title>AI for artists : Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">艺术家的人工智能:第一部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ai-for-artists-part-1-8d74502725d0?source=collection_archive---------2-----------------------#2018-05-31">https://towardsdatascience.com/ai-for-artists-part-1-8d74502725d0?source=collection_archive---------2-----------------------#2018-05-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ede7a92ef18bfc2bcdcf65ad8029572a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I_aGK7klT1bq41l5cN3Ucg.jpeg"/></div></div></figure><blockquote class="jy jz ka"><p id="0eea" class="kb kc kd ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">艺术不仅仅是对自然现实的模仿，事实上，它是对自然现实的一种形而上学的补充，为了征服自然而与自然并列。</p><p id="4050" class="kb kc kd ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">弗里德里希·尼采</p></blockquote><p id="08dd" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="kd">注意，本文是艺术家 AI 系列的一部分。</em> </strong> <a class="ae ld" rel="noopener" target="_blank" href="/ai-for-artists-part-1-8d74502725d0"> <strong class="ke ir"> <em class="kd">第一部分</em></strong></a><strong class="ke ir"><em class="kd"/></strong><a class="ae ld" rel="noopener" target="_blank" href="/ai-for-artists-part-2-c3e41653747a"><strong class="ke ir"><em class="kd">第二部分</em> </strong> </a></p><p id="b7db" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">艺术和技术的历史总是交织在一起的。历史上发生的艺术革命是由制作作品的工具促成的。燧石刀的精确性使得人类能够用猛犸象象牙雕刻出第一批形象艺术作品。在当代，艺术家使用从 3D 打印到虚拟现实的工具，拓展了自我表达的可能性。</p><p id="9ef9" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">我们正在进入一个时代，人工智能越来越多地出现在几乎每个领域。埃隆·马斯克认为，到 2030 年，艺术将在所有方面超过人类，但艺术一直被视为人类的万神殿，是人工智能永远无法复制的人类事物。在这一系列文章中，我们将在机器学习的帮助下创作出令人惊叹的艺术作品。</p><h1 id="6e16" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">项目 1:神经风格转移</h1><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mc"><img src="../Images/8b6e4d9014ae7e902d2e31f8671a1d4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IZHZsYi4O7au_qxIYcNEag.jpeg"/></div></div></figure><p id="12e8" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">什么是神经风格转移？</p><p id="8d80" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">它仅仅是以另一种方式重新想象一幅图像的过程。这是使用卷积神经网络进行图像处理的最酷的应用之一。想象一下，你可以让任何一位著名的艺术家(例如米开朗基罗)在几毫秒内为你画出你想要的任何东西。在这篇文章中，我将尝试给出一个关于实现细节的简要描述。欲了解更多信息，您可以参考<a class="ae ld" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank"> Gatys 等人的论文，2015 </a>。本文实现了我们试图做的优化问题</p><p id="700b" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">在我们开始之前，我们将介绍一些基础知识，这可以帮助你更好地理解概念，或者如果你只对代码感兴趣，你可以直接进入下面的链接<a class="ae ld" href="https://github.com/hnarayanan/artistic-style-transfer" rel="noopener ugc nofollow" target="_blank">https://github.com/hnarayanan/artistic-style-transfer</a>或<a class="ae ld" href="https://github.com/sav132/neural-style-transfer" rel="noopener ugc nofollow" target="_blank">https://github.com/sav132/neural-style-transfer</a>。绝对推荐<a class="ae ld" href="https://www.coursera.org/learn/convolutional-neural-networks" rel="noopener ugc nofollow" target="_blank">卷积神经网络(CNN) </a>吴恩达课程，以便更深层次地理解概念。</p><h1 id="6d87" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">基本原则</h1><p id="1e01" class="pw-post-body-paragraph kb kc iq ke b kf mh kh ki kj mi kl km la mj kp kq lb mk kt ku lc ml kx ky kz ij bi translated">让我们想象一下，我们正在尝试建立一个图像分类器，它可以预测图像是什么。我们使用监督学习来解决这个问题。给定一个由 D = W×H×3(色深= 3)组成的彩色图像(RGB 图像)被存储为一个数组。我们假设有“n”个类别可以分类。任务是提出一个将我们图像分类为“n”个图像之一的函数。</p><p id="ba46" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">为了建立这一点，我们从一组先前分类标记的“训练数据”开始。分数函数我们可以用一个简单的线性激活函数[F(x，W，b) = Wx +b]。W —大小为 n X D 的矩阵称为权重，大小为 n X 1 的向量 b 称为偏差。为了预测每个类别的概率，我们将这个输出通过一个叫做<a class="ae ld" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> <em class="kd"> softmax </em> </a> <em class="kd">的函数</em><em class="kd"/>将分数压缩为一组 0 到 1 之间的数字，加起来等于 1。假设我们的训练数据是一组<em class="kd"> N </em>预先分类的示例<strong class="ke ir"> xi </strong> ∈ℝ <em class="kd"> D </em>，每个示例都有正确的类别<em class="kd">易</em> ∈1，…，<em class="kd"> K。确定所有这些示例的总损失是</em> <a class="ae ld" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank"> <em class="kd">交叉熵</em> </a> <em class="kd">损失:</em></p><blockquote class="mm"><p id="1034" class="mn mo iq bd mp mq mr ms mt mu mv kz dk translated"><em class="mw">L(</em><strong class="ak"><em class="mw">s</em></strong><em class="mw">)=∑I log(syi)</em></p></blockquote><p id="e993" class="pw-post-body-paragraph kb kc iq ke b kf mx kh ki kj my kl km la mz kp kq lb na kt ku lc nb kx ky kz ij bi translated">对于优化部分，我们使用梯度下降。我们必须找到最小化这种损失的权重和偏差。</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/acd50eb664109329dc6b1daf20287081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tO_DcOvcg--rESqMB0dy2Q.png"/></div></div></figure><p id="e547" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">我们的目标是找到曲线底部的全局损失最小值。我们还使用一个称为学习率(α)的参数，这是一个衡量我们修改权重速度的指标。</p><p id="7e80" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">综上所述，最初我们给出了一些图像作为一个原始的数字数组，我们有一个参数化的得分函数(线性变换后是一个 softmax 函数),带我们去分类得分。我们有一种评估其性能的方法(交叉熵损失函数)。然后，我们改进分类器的参数(使用梯度下降优化)。但是这里的精度较低，因此我们使用卷积神经网络来提高精度。</p><h2 id="87a1" class="nd lf iq bd lg ne nf dn lk ng nh dp lo la ni nj ls lb nk nl lw lc nm nn ma no bi translated">卷积神经网络(CNN)基础</h2><figure class="md me mf mg gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/f98818f2d8a2d15c8c979feeda632299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*kTeFAopKBLhrJ-vYwAvnTg.png"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Diagram of a simple network from Wikipedia</figcaption></figure><p id="4948" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">以前我们使用线性得分函数，但这里我们将使用非线性得分函数。为此，我们使用<em class="kd">神经元</em>，它们是<em class="kd">函数</em>，它首先将其每个输入乘以一个权重，并将这些加权的输入相加为一个数字，并添加一个偏差。然后，它将这个数字通过一个称为<em class="kd">激活</em>的非线性函数，并产生一个输出。</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/bc2905831ecb7285735228f80f6710ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xt78mMGQESnrgiFK_KWVNA.png"/></div></div></figure><p id="5cb5" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">通常，为了提高我们的分类器的准确性，我们可能会认为通过向我们的得分函数添加更多的层来做到这一点是很容易的。但是有一些问题-</p><p id="22e3" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">1.通常，神经网络完全忽略图像的 2D 结构。例如，如果我们将输入图像作为一个 30×30 的矩阵，他们将输入图像作为一个 900 的数组。你可以想象，共享邻近像素的一些有用信息正在丢失。</p><p id="130f" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">2.当我们添加更多的层时，我们需要学习的参数数量增长非常快。</p><p id="db33" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">为了解决这些问题，我们使用卷积神经网络。</p><p id="9476" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">普通网络和 CNN 之间的区别在于，它不是使用线性阵列的输入数据，而是使用具有宽度、高度和深度的输入数据，并输出 3D 数量。人们想象的 2D 输入图像(<em class="kd"> W </em> × <em class="kd"> H </em>)通过引入颜色深度作为第三维(<em class="kd"> W </em> × <em class="kd"> H </em> × <em class="kd"> d </em>)被转换成 3D。(灰度为 1，RGB 为 3。)类似地，人们可能想象的长度为<em class="kd"> C </em>的线性输出实际上表示为 1×1× <em class="kd"> C </em>。我们使用两种类型的图层</p><p id="87d7" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">1.卷积层</p><p id="686d" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">第一层是卷积(Conv) <em class="kd"> </em>层<em class="kd">。</em>这里我们有<em class="kd"> </em>一组过滤器。让我们假设我们有 K 个 T21 这样的过滤器。每个滤波器都很小，其范围由<em class="kd"> F </em>表示，并具有其输入的深度值。例如，典型的滤波器可能是 3×3×3 (3 个像素宽和 3 个像素高，3 个像素来自输入 3 通道彩色图像的深度)。</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/6e8db7a310822d1e4d94a2e3aa6dbd7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*qtinjiZct2w7Dr4XoFixnA.gif"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Convolutional layer with K = 2 filters, each with a spatial extent F = 3 , moving at a stride S = 2, and input padding P = 1. (Reproduced from CS231n notes)</figcaption></figure><p id="5f0b" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">我们以表示我们移动速度的步幅<em class="kd"> S </em>在输入音量上滑动滤波器组。根据控制输出空间尺寸的需要，该输入可以用零进行空间填充(<em class="kd"> P </em>)。当我们滑动时，每个过滤器计算输入的点积以产生 2D 输出，当我们将这些过滤器堆叠在我们集合中的所有过滤器上时，我们得到 3D 输出体积。</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nw"><img src="../Images/380ddacae4027933ae792e48f881afb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FEkbpZpnn3nJu0IhzRagwg.png"/></div></div></figure><p id="ed98" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">2.汇集层</p><p id="9f96" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">它的功能是逐渐减小表示的空间大小，以减少网络中的参数和计算量。它没有任何要学习的参数。</p><p id="72d3" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">例如，空间范围<em class="kd"> F </em> =2 且跨度<em class="kd"> S </em> =2 的最大池图层将输入尺寸从 4×4 减半至 2×2，深度保持不变。它通过选取每组 2×2 数中的最大值，并仅将这些值传递给输出来实现这一点。</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nx"><img src="../Images/7c9a5ed0bb59c0a716f0d53a15853335.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JPqRD5iG-JEvF8cr-2mB4A.png"/></div></div></figure><p id="20e8" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">这就结束了基本原理，我希望你已经了解了基本的工作原理。</p><h1 id="6f46" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">我们开始吧！</h1><p id="4eef" class="pw-post-body-paragraph kb kc iq ke b kf mh kh ki kj mi kl km la mj kp kq lb mk kt ku lc ml kx ky kz ij bi translated"><strong class="ke ir">内容图像和样式图像</strong></p><p id="6948" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">内容图像(c)是您想要重新创建的图像。它为新的输出图像提供主要内容。它可以是一只狗的任何图像，一张自拍照或者几乎任何你想以新的风格画出来的东西。另一方面，风格图像提供图像的艺术特征，例如图案、笔触、颜色、曲线和形状。让我们称风格转移输出图像为<strong class="ke ir"> x </strong>。</p><p id="6ffa" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated"><strong class="ke ir">损失函数</strong></p><p id="65be" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated"><em class="kd"> Lcontent </em> ( <strong class="ke ir"> c </strong>，<strong class="ke ir"> x </strong>):这里我们的目标是最小化内容图像和输出图像之间的损失，这意味着当两个输入图像(<strong class="ke ir"> c </strong>和<strong class="ke ir"> x </strong>)在内容上非常接近时，我们有一个趋向于 0 的函数，并且随着它们的内容偏离而增长。我们称这个函数为<em class="kd">内容损失</em>。</p><p id="b12d" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated"><em class="kd"> Lstyle </em> ( <strong class="ke ir"> s </strong>，<strong class="ke ir"> x </strong>):这个函数显示<em class="kd">两幅图像在风格上</em>有多接近。同样，该函数随着其两个输入图像(<strong class="ke ir"> s </strong>和<strong class="ke ir"> x </strong>)在风格上趋于偏离而增长。我们称这个函数为<em class="kd">风格损失</em>。</p><p id="e55e" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">现在我们需要找到一个图片<strong class="ke ir"> x </strong>，这样它与内容图片和风格图片就没有什么不同。</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/cbc7954267452531b9198f2ebd76fb00.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*AL5qxywtyJw0zOoFaBx-Yg.jpeg"/></div></figure><p id="92d9" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">α和β用于平衡结果图像中的内容和样式。</p><p id="0ec0" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">在这里，我们将使用<a class="ae ld" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank"> VGGNet </a>，这是一个基于 CNN 的图像分类器，它已经学会对感知(例如，笔画大小、空间样式控制和颜色控制)和语义信息进行编码，我们需要这些信息来测量这些语义差异术语。</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/4093c0b03d35787d8b14d92aea2caf28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*CeFN-zdpx81x000V5Ul76A.jpeg"/></div></figure><p id="0fdd" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">VGGNet 通过将相同的较小卷积滤波器配置重复 16 次，大大简化了 ConvNet 的设计:VGGNet 中的所有滤波器都限制为 3×3，跨距和填充为 1，以及跨距为 2 的 2×2 最大池滤波器。</p><p id="5416" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">我们将首先复制用绿色标记的 16 层变体，用于分类，在下一个笔记本中，我们将看到它如何重新用于样式转换问题。</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oa"><img src="../Images/17734975aaa69209701659b2078f4485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3KDrFeqYRtUHlxOn9BGUcA.png"/></div></div></figure><p id="285c" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">普通的 VGG 获取图像并返回类别分数，但是这里我们获取中间层的输出并构建<em class="kd"> Lcontent </em>和<em class="kd"> Lstyle </em>。这里我们不包括任何完全连接的层。</p><p id="3011" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">让我们开始编码，</p><p id="59a1" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">导入必要的包。</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="d673" class="nd lf iq oc b gy og oh l oi oj">from keras.applications.vgg16 import preprocess_input, decode_predictions</span><span id="c799" class="nd lf iq oc b gy ok oh l oi oj">import time<br/>from PIL import Image<br/>import numpy as np</span><span id="3a6b" class="nd lf iq oc b gy ok oh l oi oj">from keras import backend<br/>from keras.models import Model<br/>from keras.applications.vgg16 import VGG16</span><span id="d306" class="nd lf iq oc b gy ok oh l oi oj">from scipy.optimize import fmin_l_bfgs_b<br/>from scipy.misc import imsave</span></pre><p id="59a5" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">加载并预处理内容和样式图像</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="dcf0" class="nd lf iq oc b gy og oh l oi oj">height = 450<br/>width = 450</span><span id="07d3" class="nd lf iq oc b gy ok oh l oi oj">content_image_path = 'images/styles/SSSA.JPG'<br/>content_image = Image.open(content_image_path)<br/>content_image = content_image.resize((width, height))</span><span id="ad5b" class="nd lf iq oc b gy ok oh l oi oj">style_image_path = 'images/styles/The_Scream.jpg'<br/>style_image = Image.open(style_image_path)<br/>style_image = style_image.resize((width, height))<br/></span></pre><p id="c526" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">现在我们将这些图像转换成适合数字处理的形式。特别是，我们添加了另一个维度(超出高度 x 宽度 x 3 个维度),以便稍后我们可以将这两个图像的表示连接到一个公共数据结构中。</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="d2d3" class="nd lf iq oc b gy og oh l oi oj">content_array = np.asarray(content_image, dtype='float32')<br/>content_array = np.expand_dims(content_array, axis=0)<br/>style_array = np.asarray(style_image, dtype='float32')<br/>style_array = np.expand_dims(style_array, axis=0)<br/></span></pre><p id="f2b8" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">现在，我们需要压缩这些输入数据，以匹配“<a class="ae ld" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank">用于大规模图像识别的甚深卷积网络</a>”中所做的工作，该论文介绍了<em class="kd"> VGG 网络</em>。</p><p id="e161" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">为此，我们需要执行两个转换:</p><p id="27bb" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">1.从每个像素中减去平均 RGB 值(之前在 ImageNet 训练集上计算的，可以从 Google 搜索中获得)。</p><p id="f849" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">2.将数组的排序从<em class="kd"> RGB </em>更改为<em class="kd"> BGR。</em></p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="eb68" class="nd lf iq oc b gy og oh l oi oj">content_array[:, :, :, 0] -= 103.939<br/>content_array[:, :, :, 1] -= 116.779<br/>content_array[:, :, :, 2] -= 123.68<br/>content_array = content_array[:, :, :, ::-1]<br/><br/>style_array[:, :, :, 0] -= 103.939<br/>style_array[:, :, :, 1] -= 116.779<br/>style_array[:, :, :, 2] -= 123.68<br/>style_array = style_array[:, :, :, ::-1]</span></pre><p id="70ad" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">现在我们准备使用这些数组来定义 Keras 后端中的变量。我们还引入了一个占位符变量来存储组合图像，该组合图像保留了内容图像的内容，同时合并了样式图像的样式。</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="1c7f" class="nd lf iq oc b gy og oh l oi oj">content_image = backend.variable(content_array)<br/>style_image = backend.variable(style_array)<br/>combination_image = backend.placeholder((1, height, width, 3))</span></pre><p id="edc0" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">最后，我们将所有这些图像数据连接成适于由 Keras VGG16 模型处理的单个张量。</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="5739" class="nd lf iq oc b gy og oh l oi oj">input_tensor = backend.concatenate([content_image,<br/>                                    style_image,<br/>                                    combination_image], axis=0)</span></pre><p id="2d2f" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">原始论文使用来自<a class="ae ld" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank"> Simonyan 和 Zisserman (2015) </a>的 19 层 VGG 网络模型，但我们将改为遵循<a class="ae ld" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank"> Johnson et al. (2016) </a>并使用 16 层模型(VGG16)。由于我们对图像分类不感兴趣，我们可以设置<code class="fe ol om on oc b">include_top=False</code>，这样我们就不包括任何完全连接的层。</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="c74a" class="nd lf iq oc b gy og oh l oi oj">model = VGG16(input_tensor=input_tensor, weights='imagenet',<br/>              include_top=False)</span></pre><p id="3cc6" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">我们想要最小化的损失函数可以分解为<em class="kd">内容损失</em>、<em class="kd">风格损失</em>和<em class="kd">总变化损失</em>。</p><p id="7b69" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">这些术语的相对重要性由一组标量权重决定。这些价值观的选择取决于你，但是下面的对我来说更好</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="5ab5" class="nd lf iq oc b gy og oh l oi oj">content_weight = 0.050<br/>style_weight = 4.0<br/>total_variation_weight = 1.0</span></pre><p id="0077" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">对于内容损失，我们从<code class="fe ol om on oc b">block2_conv2.</code>中提取内容特征。内容损失是内容和组合图像之间的平方欧几里德距离。</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="c4f7" class="nd lf iq oc b gy og oh l oi oj">def content_loss(content, combination):<br/>    return backend.sum(backend.square(combination - content))</span><span id="ce60" class="nd lf iq oc b gy ok oh l oi oj">layer_features = layers['block2_conv2']<br/>content_image_features = layer_features[0, :, :, :]<br/>combination_features = layer_features[2, :, :, :]</span><span id="791f" class="nd lf iq oc b gy ok oh l oi oj">loss += content_weight * content_loss(content_image_features,<br/>                                      combination_features)</span></pre><p id="c8d8" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">对于风格损失，我们首先定义一个叫做<em class="kd">克矩阵</em>的东西。<a class="ae ld" href="https://inst.eecs.berkeley.edu/~ee127a/book/login/def_Gram_matrix.html" rel="noopener ugc nofollow" target="_blank">一组图像的 Gram 矩阵</a>，表示两幅图像之间的相似性或差异性。如果你有一个(m x n)的图像，把它整形为(m*n x 1)的向量。类似地，将所有图像转换为矢量形式并形成一个矩阵，比如说 A. <br/>那么这组图像的克矩阵 G 将为</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="fb0a" class="nd lf iq oc b gy og oh l oi oj">G = A.transpose() * A;</span></pre><p id="0eeb" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">每个元素 G(i，j)将表示图像 I 和 j 之间的相似性度量</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oo"><img src="../Images/0d7f7b1e7dd81377a51fd54475a3e6cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X6upHYkV8Z3ugK36fQc_sg.png"/></div></div></figure><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="edc5" class="nd lf iq oc b gy og oh l oi oj">def gram_matrix(x):<br/>    features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))<br/>    gram = backend.dot(features, backend.transpose(features))<br/>    return gram</span></pre><p id="3ffa" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">我们通过计算风格和组合图像的 Gram 矩阵之间的差的 Frobenius 范数(它是定义为其元素的绝对平方和的平方根的矩阵的矩阵<strong class="ke ir">范数</strong>)来获得风格损失。</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="43df" class="nd lf iq oc b gy og oh l oi oj">def style_loss(style, combination):<br/>    S = gram_matrix(style)<br/>    C = gram_matrix(combination)<br/>    channels = 3<br/>    size = height * width<br/>    return backend.sum(backend.square(S - C)) / (4. * (channels ** 2) * (size ** 2))<br/><br/>feature_layers = ['block1_conv2', 'block2_conv2',<br/>                  'block3_conv3', 'block4_conv3',<br/>                  'block5_conv3']<br/>for layer_name in feature_layers:<br/>    layer_features = layers[layer_name]<br/>    style_features = layer_features[1, :, :, :]<br/>    combination_features = layer_features[2, :, :, :]<br/>    sl = style_loss(style_features, combination_features)<br/>    loss += (style_weight / len(feature_layers)) * sl</span></pre><p id="e7c0" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">现在我们计算总变异损失，</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="0a8d" class="nd lf iq oc b gy og oh l oi oj">def total_variation_loss(x):<br/>    a = backend.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])<br/>    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])<br/>    return backend.sum(backend.pow(a + b, 1.25))<br/><br/>loss += total_variation_weight * total_variation_loss(combination_image)</span></pre><p id="1195" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">现在我们有我们的总损失，它的时间来优化结果图像。我们从定义梯度开始，</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="56de" class="nd lf iq oc b gy og oh l oi oj">grads = backend.gradients(loss, combination_image)</span></pre><p id="b627" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">然后，我们引入一个<code class="fe ol om on oc b">Evaluator</code>类，它在一次通过中计算损失和梯度，同时使用<code class="fe ol om on oc b">loss</code>和<code class="fe ol om on oc b">grads </code>函数检索它们。</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="8fdb" class="nd lf iq oc b gy og oh l oi oj">outputs = [loss]<br/>outputs += grads<br/>f_outputs = backend.function([combination_image], outputs)<br/><br/>def eval_loss_and_grads(x):<br/>    x = x.reshape((1, height, width, 3))<br/>    outs = f_outputs([x])<br/>    loss_value = outs[0]<br/>    grad_values = outs[1].flatten().astype('float64')<br/>    return loss_value, grad_values<br/><br/>class Evaluator(object):<br/><br/>    def __init__(self):<br/>        self.loss_value = None<br/>        self.grads_values = None<br/><br/>    def loss(self, x):<br/>        assert self.loss_value is None<br/>        loss_value, grad_values = eval_loss_and_grads(x)<br/>        self.loss_value = loss_value<br/>        self.grad_values = grad_values<br/>        return self.loss_value<br/><br/>    def grads(self, x):<br/>        assert self.loss_value is not None<br/>        grad_values = np.copy(self.grad_values)<br/>        self.loss_value = None<br/>        self.grad_values = None<br/>        return grad_values<br/><br/>evaluator = Evaluator()</span></pre><p id="70c7" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">这个合成图像最初是像素的随机集合，我们使用<code class="fe ol om on oc b">fmin_l_bfgs_b() </code>函数(<a class="ae ld" href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" rel="noopener ugc nofollow" target="_blank"> <strong class="ke ir">有限内存 BFGS </strong> ( <strong class="ke ir"> L-BFGS </strong>或<strong class="ke ir"> LM-BFGS </strong> ) </a>是一个<a class="ae ld" href="https://en.wikipedia.org/wiki/Optimization_(mathematics)" rel="noopener ugc nofollow" target="_blank">优化算法</a>)对其进行迭代改进。</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="01d4" class="nd lf iq oc b gy og oh l oi oj">x = np.random.uniform(0, 255, (1, height, width, 3)) - 128.<br/><br/>iterations = 10<br/><br/>for i in range(iterations):<br/>    print('Start of iteration', i)<br/>    start_time = time.time()<br/>    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),<br/>                                     fprime=evaluator.grads, maxfun=20)<br/>    print('Current loss value:', min_val)<br/>    end_time = time.time()<br/>    print('Iteration %d completed in %ds' % (i, end_time - start_time))</span></pre><p id="dd23" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">要恢复输出图像，请执行以下操作</p><pre class="md me mf mg gt ob oc od oe aw of bi"><span id="0f4c" class="nd lf iq oc b gy og oh l oi oj">x = x.reshape((height, width, 3))<br/>x = x[:, :, ::-1]<br/>x[:, :, 0] += 103.939<br/>x[:, :, 1] += 116.779<br/>x[:, :, 2] += 123.68<br/>x = np.clip(x, 0, 255).astype('uint8')<br/><br/>image_final <!-- -->= Image.fromarray(x)</span></pre><p id="3f26" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">生成的图像在 image_final 中可用。</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mc"><img src="../Images/cd5781d091e67b04848f561a1b6a5d1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-cL7cOpp57Uw4qeCdMp8VQ.jpeg"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Raja Ravi Varma painting in the style of Edvard Munch</figcaption></figure><p id="3af1" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated"><em class="kd">如果你是初学者，想入门深度学习领域，可以访问我的博客</em><a class="ae ld" href="https://aiforart.wordpress.com/" rel="noopener ugc nofollow" target="_blank"><em class="kd">AI for art</em></a><em class="kd">。</em></p><h1 id="c7d3" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">结论</h1><p id="4ffc" class="pw-post-body-paragraph kb kc iq ke b kf mh kh ki kj mi kl km la mj kp kq lb mk kt ku lc ml kx ky kz ij bi translated">这个项目将让你对 CNN 的工作有一个大致的了解，并澄清许多基本的疑问。在这一系列文章中，我们将探索深度学习用于创造性目的的各种方式。</p><p id="1bfa" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">感谢您的宝贵时间！</p><p id="81e5" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated"><strong class="ke ir">参考:</strong></p><div class="op oq gp gr or os"><a href="https://medium.com/data-science-group-iitr/artistic-style-transfer-with-convolutional-neural-network-7ce2476039fd" rel="noopener follow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd ir gy z fp ox fr fs oy fu fw ip bi translated">基于卷积神经网络的艺术风格转换</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">我们都使用过像 Prisma 和 Lucid 这样的应用程序，但有没有想过这些东西是如何工作的？就像我们从我们的…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">medium.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg jw os"/></div></div></a></div><div class="op oq gp gr or os"><a href="https://www.coursera.org/learn/convolutional-neural-networks" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd ir gy z fp ox fr fs oy fu fw ip bi translated">卷积神经网络| Coursera</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">本课程将教你如何建立卷积神经网络…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">www.coursera.org</p></div></div><div class="pb l"><div class="ph l pd pe pf pb pg jw os"/></div></div></a></div><div class="op oq gp gr or os"><a href="https://github.com/hnarayanan/artistic-style-transfer" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd ir gy z fp ox fr fs oy fu fw ip bi translated">hnarayanan/艺术风格转移</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">艺术风格转移——用于艺术风格转移的卷积神经网络。</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">github.com</p></div></div><div class="pb l"><div class="pi l pd pe pf pb pg jw os"/></div></div></a></div><div class="op oq gp gr or os"><a href="https://arxiv.org/abs/1409.1556" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd ir gy z fp ox fr fs oy fu fw ip bi translated">[1409.1556]用于大规模图像识别的超深度卷积网络</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">摘要:在这项工作中，我们研究了卷积网络的深度对其在大规模数据传输中的准确性的影响</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">arxiv.org</p></div></div></div></a></div><ul class=""><li id="9148" class="pj pk iq ke b kf kg kj kk la pl lb pm lc pn kz po pp pq pr bi translated"><a class="ae ld" href="http://cs231n.stanford.edu/" rel="noopener ugc nofollow" target="_blank">http://cs231n.stanford.edu/</a></li><li id="ca70" class="pj pk iq ke b kf ps kj pt la pu lb pv lc pw kz po pp pq pr bi translated">(神经类型转移:综述)<a class="ae ld" href="https://arxiv.org/pdf/1705.04058.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1705.04058.pdf</a></li><li id="9de3" class="pj pk iq ke b kf ps kj pt la pu lb pv lc pw kz po pp pq pr bi translated"><a class="ae ld" href="http://cs231n.github.io/" rel="noopener ugc nofollow" target="_blank">http://cs231n.github.io/</a></li><li id="7735" class="pj pk iq ke b kf ps kj pt la pu lb pv lc pw kz po pp pq pr bi translated">(一种艺术风格的神经算法)<a class="ae ld" href="https://arxiv.org/pdf/1508.06576.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1508.06576.pdf</a></li><li id="9aac" class="pj pk iq ke b kf ps kj pt la pu lb pv lc pw kz po pp pq pr bi translated"><a class="ae ld" href="http://bangqu.com/0905b5.html" rel="noopener ugc nofollow" target="_blank">http://bangqu.com/0905b5.html</a></li></ul></div></div>    
</body>
</html>