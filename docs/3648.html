<html>
<head>
<title>Taming LSTMs: Variable-sized mini-batches and why PyTorch is good for your health</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">驯服 LSTMs:可变大小的小批量和为什么 PyTorch 对你的健康有益</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e?source=collection_archive---------0-----------------------#2018-06-04">https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e?source=collection_archive---------0-----------------------#2018-06-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/4dc62a7a8922b40bf4cb0ba87d84981f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*_odqFvts1GBGFgddcmcGvg.gif"/></div></figure><p id="dd14" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">如果你使用过 PyTorch，你可能会感到兴奋，精力增加，甚至想在阳光下走一会儿。你的生活感觉又完整了。也就是说，直到您尝试使用 RNNs 进行可变规模的小批量生产。</p><p id="cc53" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">并非所有的希望都破灭了。读完这篇文章后，你会回到你和 PyTorch 私奔到日落的幻想中，而你的循环网络达到了你只在 Arxiv 上读到过的新精度。</p><p id="d939" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu">我们将开发的忍者技能:</strong></p><ol class=""><li id="06a3" class="kv kw it jz b ka kb ke kf ki kx km ky kq kz ku la lb lc ld bi translated">如何在 PyTorch 中实现每个小批量中序列大小可变的 LSTM。</li><li id="aa09" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">PyTorch 中<a class="ae lj" href="https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence" rel="noopener ugc nofollow" target="_blank">填充序列</a>和<a class="ae lj" href="https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pad_packed_sequence" rel="noopener ugc nofollow" target="_blank">填充序列</a>的作用。</li><li id="adb4" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">通过时间屏蔽<a class="ae lj" href="https://en.wikipedia.org/wiki/Backpropagation_through_time" rel="noopener ugc nofollow" target="_blank">反向传播的填充令牌。</a></li></ol><blockquote class="lk ll lm"><p id="0a6a" class="jx jy ln jz b ka kb kc kd ke kf kg kh lo kj kk kl lp kn ko kp lq kr ks kt ku im bi translated"><strong class="jz iu">TL；DR 版本</strong>:填充句子，使所有句子长度相同，<a class="ae lj" href="https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence" rel="noopener ugc nofollow" target="_blank"> pack_padded_sequence </a>，遍历 LSTM，使用<a class="ae lj" href="https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pad_packed_sequence" rel="noopener ugc nofollow" target="_blank"> pad_packed_sequence </a>，展平所有输出并标记，屏蔽填充输出，计算交叉熵。</p></blockquote></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="cd77" class="ly lz it bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">为什么这很难，为什么我在乎？</h1><p id="f165" class="pw-post-body-paragraph jx jy it jz b ka mw kc kd ke mx kg kh ki my kk kl km mz ko kp kq na ks kt ku im bi translated">速度和性能。</p><p id="a049" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">将可变长度的元素一次输入到一个 LSTM 中是一个巨大的技术挑战，PyTorch 等框架已经很大程度上解决了这个问题(Tensorflow 也有一个很好的抽象，但它非常非常复杂)。</p><p id="b701" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">此外，文档不清楚，例子太旧。通过对来自多个例子而不是一个例子的梯度有一个更好的估计器，正确地做这将加速训练和增加梯度下降的准确性。</p><p id="39b9" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">虽然 rnn 很难并行化，因为每一步都依赖于前一步，但我们可以通过使用迷你批处理获得巨大的提升。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="e142" class="ly lz it bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated"><strong class="ak">序列标签</strong></h1><p id="d638" class="pw-post-body-paragraph jx jy it jz b ka mw kc kd ke mx kg kh ki my kk kl km mz ko kp kq na ks kt ku im bi translated">虽然我不能帮你解决你对贾斯汀比伯的困扰(我不会说)，但我可以帮你对你最喜欢的 JB 歌曲<a class="ae lj" href="https://www.youtube.com/watch?v=fRh_vgS2dFE" rel="noopener ugc nofollow" target="_blank"><em class="ln"/></a>进行词性标注。</p><p id="15f6" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这里有一个歌曲句子的例子:“现在说对不起还来得及吗？”(删除了'<strong class="jz iu">'至'</strong>'和'<strong class="jz iu">？'</strong>)。</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nb"><img src="../Images/6405557a55d9615daec88a9d3d65d50d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wf9iOTO853P5ewjPX079RQ.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">LSTM/GRU model we’re building</figcaption></figure></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="f3c2" class="ly lz it bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated"><strong class="ak">数据格式化</strong></h1><p id="df85" class="pw-post-body-paragraph jx jy it jz b ka mw kc kd ke mx kg kh ki my kk kl km mz ko kp kq na ks kt ku im bi translated">虽然您可以进行大量的格式化，但我们不会...为了简单起见，让我们用不同大小的序列制作这一批人为的数据。</p><pre class="nc nd ne nf gt no np nq nr aw ns bi"><span id="0977" class="nt lz it np b gy nu nv l nw nx">sent_1_x = ['is', 'it', 'too', 'late', 'now', 'say', 'sorry']<br/>sent_1_y = ['VB', 'PRP', 'RB', 'RB', 'RB', 'VB', 'JJ']</span><span id="dc02" class="nt lz it np b gy ny nv l nw nx">sent_2_x = ['ooh', 'ooh']<br/>sent_2_y = ['NNP', 'NNP']</span><span id="c716" class="nt lz it np b gy ny nv l nw nx">sent_3_x = ['sorry', 'yeah']<br/>sent_3_y = ['JJ', 'NNP']</span><span id="284e" class="nt lz it np b gy ny nv l nw nx">X = [sent_1_x, sent_2_x, sent_3_x]<br/>Y = [sent_1_y, sent_2_y, sent_3_y]</span></pre><p id="c620" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">当我们将每个句子输入到嵌入层时，每个单词都会映射到一个索引，所以我们需要将它们转换成整数列表。</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nz"><img src="../Images/295bd23be770b7890c723f63ff039a71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y29f-F-HVDUSJMGzU3LdRg.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">Indexing an embedding matrix</figcaption></figure><p id="2bba" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这里我们将这些句子映射到它们对应的词汇索引</p><pre class="nc nd ne nf gt no np nq nr aw ns bi"><span id="41ba" class="nt lz it np b gy nu nv l nw nx"># map sentences to vocab<br/>vocab = {'&lt;PAD&gt;': 0, 'is': 1, 'it': 2, 'too': 3, 'late': 4, 'now': 5, 'say': 6, 'sorry': 7, 'ooh': 8, 'yeah': 9} </span><span id="a6be" class="nt lz it np b gy ny nv l nw nx"># fancy nested list comprehension<br/>X =  [[vocab[word] for word in sentence] for sentence in X]</span><span id="f54e" class="nt lz it np b gy ny nv l nw nx"># X now looks like:  <br/># [[1, 2, 3, 4, 5, 6, 7], [8, 8], [7, 9]]</span></pre><p id="4088" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">分类标签也是如此(在我们的例子中是 POS 标签)。这些不会被嵌入。</p><pre class="nc nd ne nf gt no np nq nr aw ns bi"><span id="b5e5" class="nt lz it np b gy nu nv l nw nx">tags = {'&lt;PAD&gt;': 0, 'VB': 1, 'PRP': 2, 'RB': 3, 'JJ': 4, 'NNP': 5}</span><span id="a077" class="nt lz it np b gy ny nv l nw nx"># fancy nested list comprehension<br/>Y =  [[tags[tag] for tag in sentence] for sentence in Y]</span><span id="1e43" class="nt lz it np b gy ny nv l nw nx"># Y now looks like:<br/># [[1, 2, 3, 3, 3, 1, 4], [5, 5], [4, 5]]</span></pre></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="d48e" class="ly lz it bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated"><strong class="ak">招数 1:通过填充使 mini-batch 中的所有序列具有相同的长度。</strong></h1><p id="7a35" class="pw-post-body-paragraph jx jy it jz b ka mw kc kd ke mx kg kh ki my kk kl km mz ko kp kq na ks kt ku im bi translated">盒子里有什么东西，有各种不同的长度？不是我们的小批量！</p><p id="91bb" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">为了让 PyTorch 完成它的工作，我们需要在填充之前保存每个序列的长度。我们将使用这些信息来掩盖损失函数。</p><pre class="nc nd ne nf gt no np nq nr aw ns bi"><span id="a77b" class="nt lz it np b gy nu nv l nw nx">import numpy as np</span><span id="426e" class="nt lz it np b gy ny nv l nw nx">X = [[0, 1, 2, 3, 4, 5, 6], <br/>    [7, 7], <br/>    [6, 8]]</span><span id="a9be" class="nt lz it np b gy ny nv l nw nx"># get the length of each sentence<br/>X_lengths = [len(sentence) for sentence in X]</span><span id="bc17" class="nt lz it np b gy ny nv l nw nx"># create an empty matrix with padding tokens<br/>pad_token = vocab['&lt;PAD&gt;']<br/>longest_sent = max(X_lengths)<br/>batch_size = len(X)<br/>padded_X = np.ones((batch_size, longest_sent)) * pad_token</span><span id="debf" class="nt lz it np b gy ny nv l nw nx"># copy over the actual sequences<br/>for i, x_len in enumerate(X_lengths):<br/>  sequence = X[i]<br/>  padded_X[i, 0:x_len] = sequence[:x_len]</span><span id="c3db" class="nt lz it np b gy ny nv l nw nx"># padded_X looks like:<br/>array([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],<br/>       [ 8.,  8.,  0.,  0.,  0.,  0.,  0.],<br/>       [ 7.,  9.,  0.,  0.,  0.,  0.,  0.]])</span></pre><p id="d9db" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们对标签做同样的事情:</p><pre class="nc nd ne nf gt no np nq nr aw ns bi"><span id="6f14" class="nt lz it np b gy nu nv l nw nx">import numpy as np</span><span id="aad0" class="nt lz it np b gy ny nv l nw nx">Y = [[1, 2, 3, 3, 3, 1, 4], <br/>    [5, 5], <br/>    [4, 5]]</span><span id="084a" class="nt lz it np b gy ny nv l nw nx"># get the length of each sentence<br/>Y_lengths = [len(sentence) for sentence in Y]</span><span id="b98a" class="nt lz it np b gy ny nv l nw nx"># create an empty matrix with padding tokens<br/>pad_token = tags['&lt;PAD&gt;']<br/>longest_sent = max(Y_lengths)<br/>batch_size = len(Y)<br/>padded_Y = np.ones((batch_size, longest_sent)) * pad_token</span><span id="e002" class="nt lz it np b gy ny nv l nw nx"># copy over the actual sequences<br/>for i, y_len in enumerate(Y_lengths):<br/>  sequence = Y[i]<br/>  padded_Y[i, 0:y_len] = sequence[:y_len]</span><span id="9622" class="nt lz it np b gy ny nv l nw nx"># padded_Y looks like:<br/>array([[ 1.,  2.,  3.,  3.,  3.,  1.,  4.],<br/>       [ 5.,  5.,  0.,  0.,  0.,  0.,  0.],<br/>       [ 4.,  5.,  0.,  0.,  0.,  0.,  0.]])</span></pre><p id="f091" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu">数据处理汇总:</strong></p><p id="bfab" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们将单词转换成索引序列，并用零填充每个序列，这样一批单词的大小就都一样了。我们的数据现在看起来像:</p><pre class="nc nd ne nf gt no np nq nr aw ns bi"><span id="5640" class="nt lz it np b gy nu nv l nw nx"># X <br/>array([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],<br/>       [ 8.,  8.,  0.,  0.,  0.,  0.,  0.],<br/>       [ 7.,  9.,  0.,  0.,  0.,  0.,  0.]])</span><span id="1735" class="nt lz it np b gy ny nv l nw nx"># Y <br/>array([[ 1.,  2.,  3.,  3.,  3.,  1.,  4.],<br/>       [ 5.,  5.,  0.,  0.,  0.,  0.,  0.],<br/>       [ 4.,  5.,  0.,  0.,  0.,  0.,  0.]])</span></pre></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="14ab" class="ly lz it bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">模型</h1><p id="e47b" class="pw-post-body-paragraph jx jy it jz b ka mw kc kd ke mx kg kh ki my kk kl km mz ko kp kq na ks kt ku im bi translated">我们将使用 PyTorch 创建一个非常简单的 LSTM 网络。这些层将是:</p><ol class=""><li id="cc7e" class="kv kw it jz b ka kb ke kf ki kx km ky kq kz ku la lb lc ld bi translated">把...嵌入</li><li id="acbc" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">LSTM</li><li id="65a3" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">线性的</li><li id="9c12" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">Softmax</li></ol><figure class="nc nd ne nf gt ju"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h1 id="7fab" class="ly lz it bd ma mb oc md me mf od mh mi mj oe ml mm mn of mp mq mr og mt mu mv bi translated">招数二:如何使用 PyTorch pack_padded_sequence 和 pad_packed_sequence</h1><p id="d19c" class="pw-post-body-paragraph jx jy it jz b ka mw kc kd ke mx kg kh ki my kk kl km mz ko kp kq na ks kt ku im bi translated">概括地说，我们现在正在给一个已经填充了每个元素的批次供料。在向前传球中，我们将:</p><ol class=""><li id="e910" class="kv kw it jz b ka kb ke kf ki kx km ky kq kz ku la lb lc ld bi translated">嵌入序列</li><li id="791e" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">使用 pack_padded_sequence 确保 LSTM 看不到填充的项目</li><li id="df73" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">将打包的批处理运行到 LSTM 中</li><li id="ff3a" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">使用 pad_packed_sequence 撤消打包</li><li id="4081" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">转换 lstm 输出，这样我们就可以馈送到线性层</li><li id="c139" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">通过 log_softmax 运行</li><li id="1560" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">将形状转换回来，以便我们以(batch_size，seq_len，nb_tags)结束</li></ol><figure class="nc nd ne nf gt ju"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h1 id="dfb7" class="ly lz it bd ma mb oc md me mf od mh mi mj oe ml mm mn of mp mq mr og mt mu mv bi translated">技巧 3:屏蔽掉我们不想在损失函数中考虑的网络输出</h1><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/57d528be01587679530637704e95a743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*3U8qtLOIee4TV7mtv-nJvA.jpeg"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">Mask out those padded activations</figcaption></figure><p id="ded9" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">最后，我们准备计算损失函数。这里的要点是，我们不想考虑填充元素的网络输出。</p><p id="1860" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu">直觉预警:</strong>考虑这样做的最佳方式是扁平化所有网络输出和标签。然后计算这一序列的损失。</p><figure class="nc nd ne nf gt ju"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="218b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">哇，就这么简单。现在，您可以使用小批量更快地训练您的模型，并继续痴迷于 JB(仍然不会告诉，不要担心)。</p><p id="abae" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我知道你现在的感受…</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7fccd61ed6564bac95ff46ffb41c426a.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*SmFYrU8wxx9P7PIkzsEusQ.gif"/></div></figure><p id="aec2" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这当然是一个非常简单的 LSTM。你可以做些什么来美化你的模型(不全面):</p><ol class=""><li id="f7b6" class="kv kw it jz b ka kb ke kf ki kx km ky kq kz ku la lb lc ld bi translated">用手套嵌入初始化。</li><li id="7342" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">使用 GRU 细胞。</li><li id="cddc" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">使用双向机制(别忘了修改 init_hidden)。</li><li id="43cc" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">通过用卷积网络创建编码向量并附加到单词向量，使用字符级特征。</li><li id="3ea2" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">添加辍学。</li><li id="d038" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">增加层数</li><li id="d19a" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">…太多了</li><li id="43bd" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">当然，使用 Python 的最佳 hyperparemeter 优化库进行非常彻底的超参数搜索:<a class="ae lj" href="https://github.com/williamFalcon/test_tube" rel="noopener ugc nofollow" target="_blank"> <strong class="jz iu">试管</strong> </a> <strong class="jz iu"> </strong>(声明:我写了试管)。</li></ol></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="6305" class="ly lz it bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">总而言之:</h1><p id="e0d4" class="pw-post-body-paragraph jx jy it jz b ka mw kc kd ke mx kg kh ki my kk kl km mz ko kp kq na ks kt ku im bi translated">这就是在 PyTorch 中通过向 LSTM 提供可变长度的批量输入来恢复理智的方法</p><ol class=""><li id="5a77" class="kv kw it jz b ka kb ke kf ki kx km ky kq kz ku la lb lc ld bi translated">首先按最大序列对输入进行排序</li><li id="5ef2" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">通过填充到批中最大的序列，使所有长度相同</li><li id="c134" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">使用 pack_padded_sequence 确保 LSTM 看不到填充的项目(脸书团队，你真的应该重命名这个 API)。</li><li id="3a35" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">使用 pad_packed_sequence 撤消步骤 3。</li><li id="4047" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">将输出和标签展平为一个长向量。</li><li id="aaec" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">屏蔽掉你不想要的输出</li><li id="3c30" class="kv kw it jz b ka le ke lf ki lg km lh kq li ku la lb lc ld bi translated">计算交叉熵。</li></ol><h1 id="2d01" class="ly lz it bd ma mb oc md me mf od mh mi mj oe ml mm mn of mp mq mr og mt mu mv bi translated">完整代码:</h1><figure class="nc nd ne nf gt ju"><div class="bz fp l di"><div class="oa ob l"/></div></figure></div></div>    
</body>
</html>