<html>
<head>
<title>PyTorch: First program and walk through</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch:第一个程序并遍历</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-first-program-and-walk-through-ceb739134ab9?source=collection_archive---------2-----------------------#2017-09-13">https://towardsdatascience.com/pytorch-first-program-and-walk-through-ceb739134ab9?source=collection_archive---------2-----------------------#2017-09-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3909" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我看到Fast.ai正在PyTorch上转移，我看到PyTorch最适合研究原型。因此，我决定在PyTorch中实现一些研究论文。我已经在平行点研究过C-DSSM模型。但是我的实现是在喀拉斯。我将强调黑客的观点，将代码从Keras移植到PyTorch，而不是博客中的研究观点。<br/>我的实现在<a class="ae kl" href="https://github.com/nishnik/Deep-Semantic-Similarity-Model-PyTorch" rel="noopener ugc nofollow" target="_blank">nish Nik/Deep-Semantic-Similarity-Model-py torch</a>，我也记录了代码。<br/>更多关于C-DSSM车型<a class="ae kl" href="http://research.microsoft.com/pubs/226585/cikm2014_cdssm_final.pdf" rel="noopener ugc nofollow" target="_blank">在这里</a>。</p><ol class=""><li id="f7e9" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">C-DSSM模型接受多个输入。一个是查询，正面单据和负面单据。在Keras中，您可以传递一个列表:</li></ol><pre class="kv kw kx ky gt kz la lb lc aw ld bi"><span id="af4e" class="le lf iq la b gy lg lh l li lj">Model(inputs = [query, pos_doc] + neg_docs, outputs = prob)<br/># where query and pos_doc is the numpy array and neg_docs is a list of numpy array</span></pre><p id="0b15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在PyTorch中，您可以:</p><pre class="kv kw kx ky gt kz la lb lc aw ld bi"><span id="1f5f" class="le lf iq la b gy lg lh l li lj">def forward(self, q, pos, negs):<br/># this is in the class definition itself, you can easily access negs[0] for the 0th element of list. I was surprised to find that it works with list<br/># Another way would have been:<br/># def forward(self, x):<br/>#     q = x[0]<br/>#     pos = x[1]<br/>#     negs = x[2:]</span></pre><p id="bb6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.C-DSSM模型中的第一层是Conv1d层。所以，我对比了Keras和PyTorch的conv1d文档。在喀拉斯:</p><pre class="kv kw kx ky gt kz la lb lc aw ld bi"><span id="037c" class="le lf iq la b gy lg lh l li lj">keras.layers.convolutional.Conv1D(filters, kernel_size, strides=1, padding='valid', dilation_rate=1, activation=<strong class="la ir">None</strong>, use_bias=<strong class="la ir">True</strong>, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=<strong class="la ir">None</strong>, bias_regularizer=<strong class="la ir">None</strong>, activity_regularizer=<strong class="la ir">None</strong>, kernel_constraint=<strong class="la ir">None</strong>, bias_constraint=<strong class="la ir">None</strong>)</span></pre><p id="cae8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在PyTorch:</p><pre class="kv kw kx ky gt kz la lb lc aw ld bi"><span id="e79e" class="le lf iq la b gy lg lh l li lj"><em class="lk">class </em>torch.nn.Conv1d<!-- -->(<em class="lk">in_channels</em>, <em class="lk">out_channels</em>, <em class="lk">kernel_size</em>, <em class="lk">stride=1</em>, <em class="lk">padding=0</em>, <em class="lk">dilation=1</em>, <em class="lk">groups=1</em>, <em class="lk">bias=True</em>)</span></pre><p id="fedb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我必须说明:</p><ol class=""><li id="6994" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated"><em class="lk">内核大小</em>:与Keras <em class="lk">内核大小</em>相同</li><li id="e9b3" class="km kn iq jp b jq ll ju lm jy ln kc lo kg lp kk kr ks kt ku bi translated"><em class="lk"> out_channels </em>:同Keras <em class="lk">滤镜</em></li><li id="9d32" class="km kn iq jp b jq ll ju lm jy ln kc lo kg lp kk kr ks kt ku bi translated"><em class="lk"> in_channels </em>:输入的通道数</li></ol><blockquote class="lq lr ls"><p id="95be" class="jn jo lk jp b jq jr js jt ju jv jw jx lt jz ka kb lu kd ke kf lv kh ki kj kk ij bi translated">我对in_channels感到困惑，我曾设想一个5*90000的输入，其中内核将排成一行并给出5*300的输出。但是我错了</p></blockquote><p id="4aef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PyTorch拥有动态图形，这也是为什么它对程序员来说很棒的原因，你可以看到图形创建时发生了什么。</p><p id="ac5a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以我输入了:</p><pre class="kv kw kx ky gt kz la lb lc aw ld bi"><span id="9ee9" class="le lf iq la b gy lg lh l li lj">lq<br/>( 0 ,.,.) = <br/> 1.8577e-01 8.3356e-01 8.5541e-01 … 1.1579e-01 6.4221e-01 6.4712e-01<br/> 8.3658e-01 1.4647e-01 2.0220e-01 … 2.2165e-01 2.1841e-01 3.0833e-01<br/> 7.1619e-01 1.8811e-01 1.6903e-01 … 9.2867e-01 5.6902e-01 9.1074e-01<br/> 9.4578e-01 9.4889e-01 8.4584e-01 … 3.7839e-01 3.6997e-01 2.7487e-01<br/> 6.4675e-01 2.5806e-01 3.1640e-01 … 9.8110e-01 8.6193e-01 1.0357e-02<br/>[torch.FloatTensor of size 3x5x90000]</span></pre><p id="3d07" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我对它进行了卷积核运算，权重为:</p><pre class="kv kw kx ky gt kz la lb lc aw ld bi"><span id="c2fc" class="le lf iq la b gy lg lh l li lj">( 0 ,.,.) = <br/>3.0515e-01<br/>-3.3296e-01<br/>-4.1675e-01<br/>1.3134e-01<br/>4.1769e-01</span></pre><p id="8a14" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">和偏差:</p><pre class="kv kw kx ky gt kz la lb lc aw ld bi"><span id="e133" class="le lf iq la b gy lg lh l li lj">0.1725</span></pre><p id="e3d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">得到的结果是:</p><pre class="kv kw kx ky gt kz la lb lc aw ld bi"><span id="862f" class="le lf iq la b gy lg lh l li lj">qc<br/> 4.5800e-02 5.3139e-01 5.3828e-01 … 2.0578e-01 4.6649e-01 -7.2540e-02<br/> -7.0118e-02 -5.8567e-01 -5.5335e-01 … 3.0976e-01 -3.3483e-02 -1.3638e-01<br/> -8.8448e-02 -4.1210e-02 -8.5710e-02 … -6.0755e-01 -5.5320e-01 -4.0962e-01<br/> … ⋱ … <br/> -2.8095e-01 1.1081e-01 7.9184e-02 … -3.7869e-01 -1.7801e-01 -4.1227e-01<br/> -8.7498e-01 -8.1998e-01 -8.2176e-01 … -7.7750e-01 -8.6171e-01 -5.1366e-01<br/> -5.7234e-01 -2.5415e-01 -2.5126e-01 … -5.8857e-01 -4.8405e-01 -2.4303e-01<br/>[torch.FloatTensor of size 3x300x90000]</span></pre><p id="ecb4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们手动计算:(内核权重*输入)+偏差</p><pre class="kv kw kx ky gt kz la lb lc aw ld bi"><span id="cb71" class="le lf iq la b gy lg lh l li lj">0.30515 * 0.18577 + -0.33296 * 0.83658 + -0.41675 * 0.71619 + 0.13134 * 0.94578 + 0.41769 * 0.645 + 0.1725</span></pre><p id="05b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">结果是:</p><pre class="kv kw kx ky gt kz la lb lc aw ld bi"><span id="fad0" class="le lf iq la b gy lg lh l li lj">0.045796651399999944</span></pre><p id="cb2d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">哪个接近qc[0][0][0] <br/>注意:我刚刚展示了第一维度的第0个元素。(仅与数学相关的内容)<br/>因此，这个动态图让我直观地了解了Conv1d在PyTorch中是如何按列操作的。<br/>上面的代码很简单:</p><pre class="kv kw kx ky gt kz la lb lc aw ld bi"><span id="eea0" class="le lf iq la b gy lg lh l li lj">query_len = 5<br/>lq = np.random.rand(3, query_len, WORD_DEPTH)<br/>lq = Variable(torch.from_numpy(lq).float())<br/>query_conv = nn.Conv1d(WORD_DEPTH, K, FILTER_LENGTH)<br/># print(lq)<br/># print (query_conv.weight)<br/># print (query_conv.bias)<br/>qc = query_conv(lq)<br/># print (qc)</span></pre><p id="09d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.在获得查询、pos和否定文档的所有向量之后。我们必须计算余弦相似度，在Keras中你必须为它创建一个新层。在PyTorch:</p><pre class="kv kw kx ky gt kz la lb lc aw ld bi"><span id="1551" class="le lf iq la b gy lg lh l li lj">dots = [q_s.dot(pos_s)]<br/>dots = dots + [q_s.dot(neg_s) for neg_s in neg_ss]</span></pre><p id="c874" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">变得如此简单:)</p><p id="039e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4.现在<code class="fe lw lx ly la b">dots</code>是一个列表，要把它转换成PyTorch变量，我们只需做:</p><pre class="kv kw kx ky gt kz la lb lc aw ld bi"><span id="408b" class="le lf iq la b gy lg lh l li lj">dots = torch.stack(dots)</span></pre><p id="721d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PS:这是我在PyTorch中的第一个实现，如果代码中有任何问题或者你不明白的地方，请联系我。<br/>我的twitter句柄是<a class="ae kl" href="https://twitter.com/nishantiam" rel="noopener ugc nofollow" target="_blank">nishan tiam</a>github句柄是<a class="ae kl" href="https://github.com/nishnik" rel="noopener ugc nofollow" target="_blank"> nishnik </a>。</p></div></div>    
</body>
</html>