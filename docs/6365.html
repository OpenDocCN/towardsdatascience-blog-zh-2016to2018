<html>
<head>
<title>Object detection and tracking in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 中的目标检测和跟踪</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/object-detection-and-tracking-in-pytorch-b3cf1a696a98?source=collection_archive---------2-----------------------#2018-12-10">https://towardsdatascience.com/object-detection-and-tracking-in-pytorch-b3cf1a696a98?source=collection_archive---------2-----------------------#2018-12-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="635e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">检测图像中的多个对象并在视频中跟踪它们</h2></div><p id="2c66" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我之前的故事中，我讲述了如何用你自己的图像在 PyTorch 中训练一个<a class="ae lb" rel="noopener" target="_blank" href="/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5">图像分类器，然后用它进行图像识别。现在，我将向您展示如何使用预训练的分类器来检测图像中的多个对象，然后在视频中跟踪它们。</a></p><p id="0704" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图像分类(识别)和物体检测有什么区别？在分类中，你识别图像中的主要物体，整个图像被一个类分类。在检测中，多个对象在图像中被识别、分类，并且位置也被确定(作为边界框)。</p><h1 id="a924" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">图像中的目标检测</h1><p id="6041" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">有几种用于对象检测的算法，其中最流行的是 YOLO 和 SSD。对于这个故事，我将使用<strong class="kh ir"> YOLOv3 </strong>。我不会进入 YOLO(你只看一次)如何工作的技术细节——你可以在这里<a class="ae lb" href="https://pjreddie.com/yolo/" rel="noopener ugc nofollow" target="_blank">阅读</a>——但重点是如何在你自己的应用程序中使用它。</p><p id="6aee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以让我们直接进入代码吧！这里的 Yolo 检测代码是基于<a class="ae lb" href="https://github.com/eriklindernoren/PyTorch-YOLOv3" rel="noopener ugc nofollow" target="_blank">埃里克·林德诺伦</a>对<a class="ae lb" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" rel="noopener ugc nofollow" target="_blank">约瑟夫·雷德蒙和阿里·法尔哈迪论文</a>的实现。下面的代码片段来自一个 Jupyter 笔记本，你可以在我的<a class="ae lb" href="https://github.com/cfotache/pytorch_objectdetecttrack" rel="noopener ugc nofollow" target="_blank"> Github repo </a>中找到。在运行之前，您需要运行<code class="fe lz ma mb mc b">config</code>文件夹中的<code class="fe lz ma mb mc b">download_weights.sh</code>脚本来下载 Yolo weights 文件。我们从导入所需的模块开始:</p><pre class="md me mf mg gt mh mc mi mj aw mk bi"><span id="a949" class="ml ld iq mc b gy mm mn l mo mp">from models import *<br/>from utils import *</span><span id="cee5" class="ml ld iq mc b gy mq mn l mo mp">import os, sys, time, datetime, random<br/>import torch<br/>from torch.utils.data import DataLoader<br/>from torchvision import datasets, transforms<br/>from torch.autograd import Variable</span><span id="88c5" class="ml ld iq mc b gy mq mn l mo mp">import matplotlib.pyplot as plt<br/>import matplotlib.patches as patches<br/>from PIL import Image</span></pre><p id="ddfa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们加载预训练的配置和权重，以及在其上训练了<a class="ae lb" href="https://github.com/pjreddie/darknet" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> Darknet </strong> </a>模型的 COCO 数据集的类名。在 PyTorch 中，不要忘记在加载后将模型设置为<code class="fe lz ma mb mc b">eval</code>模式。</p><pre class="md me mf mg gt mh mc mi mj aw mk bi"><span id="3581" class="ml ld iq mc b gy mm mn l mo mp">config_path='config/yolov3.cfg'<br/>weights_path='config/yolov3.weights'<br/>class_path='config/coco.names'<br/>img_size=416<br/>conf_thres=0.8<br/>nms_thres=0.4</span><span id="09ae" class="ml ld iq mc b gy mq mn l mo mp"># Load model and weights<br/>model = Darknet(config_path, img_size=img_size)<br/>model.load_weights(weights_path)<br/>model.cuda()<br/>model.eval()<br/>classes = utils.load_classes(class_path)<br/>Tensor = torch.cuda.FloatTensor</span></pre><p id="3cbf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面也有一些预定义的值:图像大小(416px 正方形)、置信度阈值和非最大抑制阈值。</p><p id="372c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是返回指定图像检测结果的基本函数。注意，它需要一个枕头图像作为输入。大部分代码处理的是将图像的大小调整为 416 像素的正方形，同时保持其纵横比并填充溢出。实际检测在最后 4 行。</p><pre class="md me mf mg gt mh mc mi mj aw mk bi"><span id="f839" class="ml ld iq mc b gy mm mn l mo mp">def detect_image(img):<br/>    # scale and pad image<br/>    ratio = min(img_size/img.size[0], img_size/img.size[1])<br/>    imw = round(img.size[0] * ratio)<br/>    imh = round(img.size[1] * ratio)<br/>    img_transforms=transforms.Compose([transforms.Resize((imh,imw)),<br/>         transforms.Pad((max(int((imh-imw)/2),0), <br/>              max(int((imw-imh)/2),0), max(int((imh-imw)/2),0),<br/>              max(int((imw-imh)/2),0)), (128,128,128)),<br/>         transforms.ToTensor(),<br/>         ])<br/>    # convert image to Tensor<br/>    image_tensor = img_transforms(img).float()<br/>    image_tensor = image_tensor.unsqueeze_(0)<br/>    input_img = Variable(image_tensor.type(Tensor))<br/>    # run inference on the model and get detections<br/>    with torch.no_grad():<br/>        detections = model(input_img)<br/>        detections = utils.non_max_suppression(detections, 80, <br/>                        conf_thres, nms_thres)<br/>    return detections[0]</span></pre><p id="e06f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，让我们通过加载一个图像，获得检测，然后用检测到的对象周围的边界框来显示它，将它放在一起。同样，这里的大部分代码处理图像的缩放和填充，以及为每个检测到的类获取不同的颜色。</p><pre class="md me mf mg gt mh mc mi mj aw mk bi"><span id="7600" class="ml ld iq mc b gy mm mn l mo mp"># load image and get detections<br/>img_path = "images/blueangels.jpg"<br/>prev_time = time.time()<br/>img = Image.open(img_path)<br/>detections = detect_image(img)<br/>inference_time = datetime.timedelta(seconds=time.time() - prev_time)<br/>print ('Inference Time: %s' % (inference_time))</span><span id="5761" class="ml ld iq mc b gy mq mn l mo mp"># Get bounding-box colors<br/>cmap = plt.get_cmap('tab20b')<br/>colors = [cmap(i) for i in np.linspace(0, 1, 20)]</span><span id="07cd" class="ml ld iq mc b gy mq mn l mo mp">img = np.array(img)<br/>plt.figure()<br/>fig, ax = plt.subplots(1, figsize=(12,9))<br/>ax.imshow(img)</span><span id="9234" class="ml ld iq mc b gy mq mn l mo mp">pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))<br/>pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))<br/>unpad_h = img_size - pad_y<br/>unpad_w = img_size - pad_x</span><span id="26ae" class="ml ld iq mc b gy mq mn l mo mp">if detections is not None:<br/>    unique_labels = detections[:, -1].cpu().unique()<br/>    n_cls_preds = len(unique_labels)<br/>    bbox_colors = random.sample(colors, n_cls_preds)<br/>    # browse detections and draw bounding boxes<br/>    for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:<br/>        box_h = ((y2 - y1) / unpad_h) * img.shape[0]<br/>        box_w = ((x2 - x1) / unpad_w) * img.shape[1]<br/>        y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]<br/>        x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]<br/>        color = bbox_colors[int(np.where(<br/>             unique_labels == int(cls_pred))[0])]<br/>        bbox = patches.Rectangle((x1, y1), box_w, box_h,<br/>             linewidth=2, edgecolor=color, facecolor='none')<br/>        ax.add_patch(bbox)<br/>        plt.text(x1, y1, s=classes[int(cls_pred)], <br/>                color='white', verticalalignment='top',<br/>                bbox={'color': color, 'pad': 0})<br/>plt.axis('off')<br/># save image<br/>plt.savefig(img_path.replace(".jpg", "-det.jpg"),        <br/>                  bbox_inches='tight', pad_inches=0.0)<br/>plt.show()</span></pre><p id="17ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以把这些代码片段放在一起运行代码，或者<a class="ae lb" href="https://github.com/cfotache/pytorch_objectdetecttrack" rel="noopener ugc nofollow" target="_blank">从我的 Github 下载笔记本</a>。以下是图像中物体检测的几个例子:</p><div class="md me mf mg gt ab cb"><figure class="mr ms mt mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><img src="../Images/990ec72007ac9da8135ee7fdce72193f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*KCW5w7q0Wk0g-j22PXsahA.jpeg"/></div></figure><figure class="mr ms ne mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><img src="../Images/75ac9ba977b361a7ab0f03ca11c61dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*pcmDj9tmHiLMeTjs6i-8wA.jpeg"/></div></figure></div><div class="ab cb"><figure class="mr ms nf mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><img src="../Images/20fe32b8dba129be73e43f9ee7ca8bcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*4dN1r-YgBstJdquuigjtPA.jpeg"/></div></figure><figure class="mr ms ng mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><img src="../Images/089874e09642167558d10468bc2a7fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*Ipio8dDLsWd5l7UJXfWfWQ.jpeg"/></div></figure></div><h1 id="4eac" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">视频中的目标跟踪</h1><p id="5018" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">现在你知道如何检测图像中的不同物体了。当你在视频中一帧一帧地做的时候，你会看到那些跟踪框在移动，这种可视化可能会很酷。但是，如果那些视频帧中有多个对象，您如何知道一帧中的对象是否与前一帧中的对象相同？这被称为对象跟踪，并使用多次检测来识别特定对象。</p><p id="81e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有几个算法可以做到这一点，我决定使用<strong class="kh ir">排序</strong>，这是非常容易使用和相当快。<a class="ae lb" href="https://arxiv.org/pdf/1602.00763.pdf" rel="noopener ugc nofollow" target="_blank"> SORT(简单的在线和实时跟踪)</a>是亚历克斯·比雷、葛宗元、莱昂内尔·奥特、法比奥·拉莫斯、本·乌普克罗夫特在 2017 年发表的一篇论文，该论文提出使用<strong class="kh ir">卡尔曼滤波器</strong>来预测先前识别的物体的轨迹，并将它们与新的检测结果进行匹配。作者亚历克斯·比雷也写了一个多功能的<a class="ae lb" href="https://github.com/abewley/sort" rel="noopener ugc nofollow" target="_blank"> Python 实现</a>，我将在本文中使用它。请确保您从我的 Github repo 中下载了 Sort 版本，因为我必须做一些小的更改才能将其集成到我的项目中。</p><p id="284c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在来看代码，前 3 个代码段将与单个图像检测中的相同，因为它们处理在单个帧上获得 YOLO 检测。不同之处在于最后一部分，对于每个检测，我们调用 Sort 对象的 Update 函数来获取对图像中对象的引用。因此，与前一个示例中的常规检测(包括边界框的坐标和类别预测)不同，我们将获得被跟踪的对象，除了上述参数之外，还包括对象 ID。然后，我们以几乎相同的方式显示，但添加了 ID 并使用不同的颜色，以便您可以轻松地看到视频帧中的对象。</p><p id="4c95" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我还使用了<strong class="kh ir"> OpenCV </strong>来读取视频并显示视频帧。请注意，Jupyter 笔记本在处理视频时非常慢。您可以使用它进行测试和简单的可视化，但是我还提供了一个独立的 Python 脚本，它将读取源视频，并输出一个包含被跟踪对象的副本。在笔记本上播放 OpenCV 视频并不容易，所以可以保留这段代码用于其他实验。</p><pre class="md me mf mg gt mh mc mi mj aw mk bi"><span id="a0cb" class="ml ld iq mc b gy mm mn l mo mp">videopath = 'video/intersection.mp4'</span><span id="9e4d" class="ml ld iq mc b gy mq mn l mo mp">%pylab inline <br/>import cv2<br/>from IPython.display import clear_output</span><span id="3425" class="ml ld iq mc b gy mq mn l mo mp">cmap = plt.get_cmap('tab20b')<br/>colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]</span><span id="81e4" class="ml ld iq mc b gy mq mn l mo mp"># initialize Sort object and video capture<br/>from sort import *<br/>vid = cv2.VideoCapture(videopath)<br/>mot_tracker = Sort()</span><span id="f47f" class="ml ld iq mc b gy mq mn l mo mp">#while(True):<br/>for ii in range(40):<br/>    ret, frame = vid.read()<br/>    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)<br/>    pilimg = Image.fromarray(frame)<br/>    detections = detect_image(pilimg)</span><span id="f9a0" class="ml ld iq mc b gy mq mn l mo mp">    img = np.array(pilimg)<br/>    pad_x = max(img.shape[0] - img.shape[1], 0) * <br/>            (img_size / max(img.shape))<br/>    pad_y = max(img.shape[1] - img.shape[0], 0) * <br/>            (img_size / max(img.shape))<br/>    unpad_h = img_size - pad_y<br/>    unpad_w = img_size - pad_x<br/>    if detections is not None:<br/>        tracked_objects = mot_tracker.update(detections.cpu())</span><span id="44f5" class="ml ld iq mc b gy mq mn l mo mp">        unique_labels = detections[:, -1].cpu().unique()<br/>        n_cls_preds = len(unique_labels)<br/>        for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:<br/>            box_h = int(((y2 - y1) / unpad_h) * img.shape[0])<br/>            box_w = int(((x2 - x1) / unpad_w) * img.shape[1])<br/>            y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])<br/>            x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])</span><span id="6ac3" class="ml ld iq mc b gy mq mn l mo mp">            color = colors[int(obj_id) % len(colors)]<br/>            color = [i * 255 for i in color]<br/>            cls = classes[int(cls_pred)]<br/>            cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h),<br/>                         color, 4)<br/>            cv2.rectangle(frame, (x1, y1-35), (x1+len(cls)*19+60,<br/>                         y1), color, -1)<br/>            cv2.putText(frame, cls + "-" + str(int(obj_id)), <br/>                        (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, <br/>                        1, (255,255,255), 3)</span><span id="77ee" class="ml ld iq mc b gy mq mn l mo mp">    fig=figure(figsize=(12, 8))<br/>    title("Video Stream")<br/>    imshow(frame)<br/>    show()<br/>    clear_output(wait=True)</span></pre><p id="b1b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在您使用笔记本之后，您可以使用常规 Python 脚本进行实时处理(您可以从摄像机获取输入)和保存视频。这是我用这个程序制作的视频样本。</p><figure class="md me mf mg gt ms"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="e091" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就这样，你现在可以自己尝试检测图像中的多个对象，并在视频帧中跟踪这些对象。</p><p id="d62c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你想在自定义图像数据集上检测和跟踪你自己的物体，你可以阅读我的下一个故事，关于<a class="ae lb" rel="noopener" target="_blank" href="/training-yolo-for-object-detection-in-pytorch-with-your-custom-dataset-the-simple-way-1aa6f56cf7d9"> <strong class="kh ir">在自定义数据集</strong> </a>上训练 Yolo 进行物体检测。</p><p id="b5c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Chris Fotache 是一名人工智能研究员，在新泽西州工作。他涵盖了与我们生活中的人工智能、Python 编程、机器学习、计算机视觉、自然语言处理等相关的主题。</p></div></div>    
</body>
</html>