<html>
<head>
<title>Naive Bayes Classifier: Part 2. Characterization and Evaluation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朴素贝叶斯分类器:第 2 部分。表征和评估</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/naive-bayes-classifier-part-2-characterization-and-evaluation-96b37f781c7c?source=collection_archive---------15-----------------------#2018-08-25">https://towardsdatascience.com/naive-bayes-classifier-part-2-characterization-and-evaluation-96b37f781c7c?source=collection_archive---------15-----------------------#2018-08-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="a32c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">封闭形式的解决方案是甜蜜的。不需要绞手/挥手来表达观点。给定假设，模型预测是精确的，因此我们可以很容易地评估假设的影响。并且，我们获得了评估应用于这些具有精确解的相同极限情况的替代(例如，数值)方法的方法。我们当然是在讨论之前的<a class="ae kl" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/" rel="noopener ugc nofollow" target="_blank">文章</a>，其中我们获得了朴素贝叶斯预测决策边界的闭合形式解，而真正的决策边界是已知的线性或非线性形式。由于已经有了很好的基础，我们将认真地开始我们在前一篇文章中概述的“下一步”。如果需要的话，请回顾一下，因为我的文章往往有 2000 多字，几乎没有重复的空间。这个帖子实际上有点长，但主要是因为图表和模拟结果，所以应该不会太差。</p><h1 id="1ab7" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">1.混乱矩阵</h1><p id="3ed6" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">对于我们的 2 类(<em class="lp">C1</em>和“<em class="lp">而不是 C1</em>或<em class="lp">C2</em>)和 2 特征([ <em class="lp"> x，y】【T9])情况，一个很好的表征任何分类器性能的方法是计算下面图 1 所示的<a class="ae kl" href="https://en.wikipedia.org/wiki/Confusion_matrix" rel="noopener ugc nofollow" target="_blank">混淆矩阵</a>。真实边界和预测边界相交，并将特征空间分为四个区域。这些区域中的数据点已经被正确地(即<em class="lp">真</em>)或不正确地(即<em class="lp">假</em>)识别为属于(即<em class="lp">正</em>)或不属于<em class="lp">C1</em>(即<em class="lp">负</em>)。例如，右上角区域被标记为<em class="lp">假阴性</em>，因为预测<em class="lp">假阳性</em>将其归类为'<em class="lp">而非</em><em class="lp">C1 '</em>。其他区域的命名也是如此。</em></p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/90086945d8ff1b8f4fd6d81f129fefd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/0*k8XC6a096d9i5l36.jpg"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Figure 1. Evaluating the confusion matrix in the assignment of a class <em class="mc">C_1</em> to a data point. The areas of intersection <em class="mc">FP</em> and <em class="mc">FN</em> are the key variables determining the quality of the classifier</figcaption></figure><p id="2f41" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相交面积<em class="lp"> FP </em>和<em class="lp"> FN </em>以及由此计算的任何度量产生对分类器有效性的重要洞察。例如，用于<em class="lp">C1</em>的高度<em class="lp">特定的</em>分类器永远不会将真正的“<em class="lp">not C1”</em>数据点放置在“<em class="lp">C1”</em>区域中。同样，如果分类器对<em class="lp">C1</em>高度敏感，它将永远不会将真正的<em class="lp">C1</em>放置在“<em class="lp">not C1”</em>区域中。分类器的整体准确度当然是正确预测的分数。所有这些导致了通常使用的<a class="ae kl" href="https://en.wikipedia.org/wiki/Confusion_matrix" rel="noopener ugc nofollow" target="_blank">以下定义</a>。</p><p id="d3e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">(1)</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi md"><img src="../Images/d04553c797812d1098979e9eb946c8c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/0*cG7PPaveGmtFS0NR.png"/></div></figure><p id="5cb6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每个分类练习都会为这些指标中的每一个产生一个值。运行一系列这样的练习并从每次运行中收集这些度量，我们可以表征分类器对于该特征空间中的数据有多好。高灵敏度(小<em class="lp"> FN </em>区)&amp;高特异性(小<em class="lp"> FP </em>区)自然是分类器的理想特征。</p><h1 id="b5b5" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">2.相交的区域</h1><p id="7abf" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">随着初步问题的解决，我们准备量化朴素贝叶斯分类器对于我们在<a class="ae kl" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/" rel="noopener ugc nofollow" target="_blank">上一篇文章</a>中考虑的线性和非线性边界的表现。如下图 2 所示，<em class="lp"> k </em>的每个值在<em class="lp"> A_1 </em>和<em class="lp"> A_2 </em>之间分割特征空间，而<em class="lp"> A_1 + A_2 </em>保持不变。预测边界自然是这个参数<em class="lp"> k </em>的函数，并且每个这样的预测边界产生一个值用于<em class="lp"> FN </em>和<em class="lp"> FP </em>我们感兴趣的相交区域。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi me"><img src="../Images/0036bc632c5f5b47904fd727f059f22a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/0*UQz7nkNjmtDAtfmQ.jpg"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Figure 2. As the point P moves, the feature space gets split between the classes. <em class="mc">FP</em> and <em class="mc">FN</em> are the areas of interest that meet at the intersection point of the true and predicted boundaries. (A) Point P moves vertically from 0 through <em class="mc">b</em>. The true and predicted boundaries always intersect at (<em class="mc">a/2, k/2</em>) (B) Point P moves horizontally from 0 through <em class="mc">a</em> . The intersection point <em class="mc">x*</em> for the true and predicted boundaries is a root of the quadratic given in Equation 6.</figcaption></figure><p id="ffaf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在非线性抛物线边界的情况下，我们用<em class="lp"> y = kx^2 </em>而不是<em class="lp"> y = x^2 </em>来工作，就像我们在之前的文章中那样。导出更新的决策边界是前面详述的相同几何过程的直接应用。为了便于参考，我们简单地写下线性和抛物线情况下的结果。<em class="lp"> y_p </em>代表下面的预测边界，我们用<em class="lp"> y_t </em>代表真实边界。</p><p id="2414" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">(2)</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/3168f6cd4ed7fac75df220132bf0b944.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/0*bNyiKytltHgO4r8g.png"/></div></figure><p id="d0b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">(3)</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/fbc90a1180f25b3aef44ab5309fb1f57.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/0*QTep8tDP8BMLYzVF.png"/></div></figure><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mh"><img src="../Images/4cdcb570a29152e93ee38e7b59f67ff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7snvwVMGO_0gSy30.jpg"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Figure 3. Varying <em class="mc">k</em> is a means to change the relative class sizes while holding the total size constant. In all the cases above <em class="mc">a</em> = <em class="mc">b</em> = 1.0, so <em class="mc">A_1 + A_2 = 1.0</em>. When one of the class sizes goes to zero i.e. <em class="mc">A_1/A_2 → ∞</em> for the linear case, and <em class="mc">A_1/A_2 → 0</em> for the parabolic case we expect better predictions in either case as indicated by vanishing <em class="mc">FP</em> and <em class="mc">FN</em>. (A) For linear case, naive bayes prediction is exact, i.e <em class="mc">FP</em> = <em class="mc">FN</em> = 0 for <em class="mc">A_1/A_2 = 1</em> (B) For the nonlinear case has <em class="mc">FN</em> = 0, but <em class="mc">FP</em> is at maximum when <em class="mc">A_1/A_2</em> = 2.</figcaption></figure><p id="7101" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图 3 说明了<em class="lp"> FP </em>和<em class="lp"> FN </em>之间的动态相互作用和权衡，因为<em class="lp"> k </em>在任一情况下都是变化的。交点<em class="lp"> FP </em>和<em class="lp"> FN </em>的面积就是我们所追求的，因为我们有<em class="lp"> y_p </em>和<em class="lp"> y_t </em>的显式方程，所以可以直接积分。</p><h1 id="7ee9" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">2.1 线性情况下的 FP 和 FN</h1><p id="8a31" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在线性情况下，我们知道真实边界和预测边界相交于点<em class="lp"> (a/2，k/2) </em>。</p><p id="90de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">(4)</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/26b9c307132eb45b0dac7e0f56a7ceab.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/0*vW1C_j5orsMajYzH.png"/></div></figure><p id="48bf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从方程<a class="ae kl" href="http://xplordat.com/2018/08/25/naive-bayes-classifier-part-2-characterization-and-evaluation/#id745239542" rel="noopener ugc nofollow" target="_blank"> 2 </a>中代入<em class="lp"> y_t = kx/a </em>和<em class="lp"> y_p </em>，积分简化得到:</p><p id="9ea3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">(5)</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/0d89b32612ac4a5b3ab2efa702757abe.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/0*euQXDywZPFQ9a9nF.png"/></div></figure><h1 id="0418" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">抛物线情况下的 2.2 FP 和 FN</h1><p id="0189" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">这里的交点是方程<em class="lp"> y_p=k x^2 </em>的解，其中<em class="lp"> y_p </em>由方程<a class="ae kl" href="http://xplordat.com/2018/08/25/naive-bayes-classifier-part-2-characterization-and-evaluation/#id2342212039" rel="noopener ugc nofollow" target="_blank"> 3 </a>给出。不幸的是，没有显式的解决方案，但整个事情简化为一个二次，需要解决一个有意义的根。</p><p id="d05e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">(6)</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/d708ea9e4e257034def545f97b21d8f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/0*vHbzrmiL4CEowE-n.png"/></div></figure><p id="cb75" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参照图 2B，面积<em class="lp"> FP </em>和<em class="lp"> FN </em>的计算公式如下</p><p id="9a56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">(7)</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/fd0b560942913c34c09bb88f4355f06b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/0*JCyKamFvsQOa2NjS.png"/></div></figure><p id="74a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">被积函数和极限是确切已知的，因此可以使用如下代码片段对积分进行精确的数值计算:</p><pre class="lr ls lt lu gt mq mr ms mt aw mu bi"><span id="f9bb" class="mv kn iq mr b gy mw mx l my mz">import numpy as np from scipy.integrate<br/>import quad</span><span id="2de4" class="mv kn iq mr b gy na mx l my mz">def integrand (x, k):<br/>   yt = min (k * x * x, b)<br/>   temp = 1.0/k**0.5 + (1.0/k**0.5) * (3*a/(2*(b/k)**0.5) — 1.0) *   (b/(k*x*x) — 1.0)<br/>   yp = min ((a/temp)**2, b)<br/>   return abs(yt — yp)</span><span id="eb52" class="mv kn iq mr b gy na mx l my mz">def findIntersectionX (k):<br/>   c2 = 4 * (b*k)**0.5 - 3*a*k<br/>   c1 = -2 * a * (b*k)**0.5<br/>   c0 = b * (3 * a - 2 * (b/k)**0.5)</span><span id="c82d" class="mv kn iq mr b gy na mx l my mz">   x1 = (-c1 + (c1**2 - 4 * c2 * c0)**0.5) / (2*c2)<br/>   x2 = (-c1 - (c1**2 - 4 * c2 * c0)**0.5) / (2*c2)</span><span id="f70d" class="mv kn iq mr b gy na mx l my mz">   if ( (x1 &gt;= xmin) and (x1 &lt;= xmax) ):<br/>      return x1<br/>   elif ( (x2 &gt;= xmin) and (x2 &lt;= xmax) ):<br/>      return x2<br/>   else:<br/>      print ("ERROR!") # should not happen!</span><span id="2d5d" class="mv kn iq mr b gy na mx l my mz">a = 1.0<br/>b = 1.0<br/>k = 10.0<br/>xmin = 0.0<br/>xmax = (b/k)**0.5</span><span id="00de" class="mv kn iq mr b gy na mx l my mz">xIntersection = findIntersectionX (k)<br/>fp_area = quad(integrand, xmin, xIntersection, args=(k),epsabs=1.0e-8)<br/>fn_area = quad(integrand, xIntersection, xmax, args=(k), epsabs=1.0e-8)</span></pre><h1 id="fdf8" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">3.特征</h1><p id="c0c0" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">有了获取任何<em class="lp"> k </em>的<em class="lp"> FP </em>和<em class="lp"> FN </em>的方法，我们就可以对分类器的灵敏度、特异性等进行评估了……我们在第 1 节中讨论过了。<em class="lp"> TP </em>和<em class="lp"> TN </em>直接从几何图形中得出。</p><p id="4b38" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">(8)</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/56ecc3866d401703baaf34628506e530.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/0*fPL3THjwOVjeqlZj.png"/></div></figure><p id="032e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图 4 示出了作为<em class="lp"> A_1/A_2 </em>的函数的相交面积及其获得的度量。<em class="lp"> FP </em>和<em class="lp"> FN </em>面积是整体面积的一小部分，精度极好。毫无疑问，这两种情况下的度量都很好，即使线性情况似乎有优势。由于导出解的设计，这些结果中的<em class="lp">a1/a2</em>的范围在两种情况下都有所限制(抛物线情况下<em class="lp"> 0 → 2 </em>，线性情况下<em class="lp"> 1 → ∞ </em>)。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nc"><img src="../Images/7203b6a8550763c3d1773c1e50fc1bb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QC2lQ8-2BB76fmVk.jpg"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Figure 4. In all the cases above <em class="mc">a = b = 1.0</em>, so <em class="mc">A_1 + A_2 = 1.0</em>. (A) Both the intersection areas <em class="mc">FP, FN</em> go to zero for the linear case when <em class="mc">A_1/A_2 = 1</em> and when <em class="mc">A_1/A_2 → ∞</em> as expected. Only <em class="mc">FN</em> is zero in the parabolic case when <em class="mc">A_1/A_2 = 2</em> confirming our assessment from Figure 3. (B) The naive bayes classifier is more sensitive than specific in either case.</figcaption></figure><p id="7bf0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在图 4B 中观察到的敏感性、特异性和准确性通过一个点并不是偶然的。当灵敏度和特异性相等时，</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/81e2e1b1e75107018ce8912a888fb131.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/0*zm2Sg1rP7Y9dX9VY.png"/></div></figure><p id="93a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中最后一个表达式是精度的定义。</p><h1 id="8bf8" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">4.用 SciKit 进行数值模拟</h1><p id="3069" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">SciKit 拥有优秀的算法实现，包括用于分类的朴素贝叶斯。<a class="ae kl" href="http://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes" rel="noopener ugc nofollow" target="_blank">高斯朴素贝叶斯</a>从训练数据中逼近高斯分布，以估计新数据点<em class="lp"> x，y </em>的<em class="lp"> P(x|C_1) </em>和<em class="lp"> P(y|C_1) </em>。当这个新的数据点<em class="lp"> x，y </em>远离决策边界时，这是好的，但是否则显然会有一些误差。这种误差对分类的影响是什么，这是我们试图通过一些数值模拟来探索的问题。此外，我们应用了两种竞争分类技术，即<a class="ae kl" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>和<a class="ae kl" href="http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" rel="noopener ugc nofollow" target="_blank">MLP 分类器</a>，同样在 SciKit 中实现。对这些技术中的假设进行分析是另一篇文章的主题。在这里我们简单地使用它们——sci kit 使尝试变得非常容易。</p><p id="b6e6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">生成训练和测试数据轻而易举，因为我们知道确切的决策界限。</p><pre class="lr ls lt lu gt mq mr ms mt aw mu bi"><span id="5668" class="mv kn iq mr b gy mw mx l my mz">from sklearn.naive_bayes import GaussianNB<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.neural_network import MLPClassifier<br/>from sklearn.metrics import *<br/>import numpy as np</span><span id="b398" class="mv kn iq mr b gy na mx l my mz">train = np.array(np.loadtxt(open('./train.csv', "rb"),              delimiter=",",skiprows=1))<br/>trainX = train[:,0:2] # x, y locations<br/>trainY = train[:,2] # Class label 1 (i.e. C1) or 0 (i.e. Not C1)<br/>test = np.array(np.loadtxt(open('./test.csv', "rb"), delimiter=",",skiprows=1))<br/>testX = test[:,0:2]<br/>testY = test[:,2]</span><span id="f590" class="mv kn iq mr b gy na mx l my mz">gnb = GaussianNB()<br/>lr = LogisticRegression()<br/>mlp = MLPClassifier(hidden_layer_sizes=(4,))</span><span id="46b5" class="mv kn iq mr b gy na mx l my mz">stats = []<br/>for id, clf, name in [(0,lr, 'lr'),(1,gnb, 'gnb'),(2,mlp, 'mlp')]:<br/>    clf.fit(trainX, trainY)<br/>    predictions = clf.predict (testX)</span><span id="fade" class="mv kn iq mr b gy na mx l my mz">    sk_tn, sk_fp, sk_fn, sk_tp = confusion_matrix (testY, predictions).ravel()<br/>    sk_accuracy = accuracy_score(testY, predictions)<br/>    sk_precision = precision_score(testY, predictions)<br/>    sk_sensitivity = recall_score(testY, predictions)<br/>    sk_specificity = sk_tn / (sk_tn + sk_fp)</span></pre><p id="376f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们对三个分类器的大部分参数使用默认值。默认情况下，MLPClassifier 使用 100 个神经元和一个隐藏层。一个隐藏层很好，但是考虑到我们只有 2 个输入神经元(一个用于<em class="lp"> x </em>，另一个用于<em class="lp"> y </em>),使用 100 个隐藏神经元是大材小用。我们选了 4 个，我们可能会做得更少。在训练模型之后，我们得到混淆矩阵。当然，所有其他的指标都可以通过计算得出。</p><h1 id="64cb" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">4.1 sci kit 中高斯近似的误差</h1><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ne"><img src="../Images/0a464b19b36f80d4e9ac7ccff2b7c5ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Qxw3Ea82IVnoWB3Z.jpg"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Figure 5. In all the cases above <em class="mc">a = b = 1.0</em> so <em class="mc">A_1 + A_2 = 1.0</em>. MLP uses a single hidden layer with 4 neurons. (A) &amp; ( C ) The mispredictions from gaussian naive bayes overlap and extend beyond the analytic results (B) Both logistic regression and MLP have essentially no mispredictions in the linear case (D) Logistic regression (in red) does worse than MLP for the nonlinear case.</figcaption></figure><p id="3a21" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图 5A 和 5C 显示了 SciKit 的高斯朴素贝叶斯模拟对于线性情况的结果，其中<em class="lp"> k = 0.6 </em>(即<em class="lp"> A_1/A_2 = 2.33 </em>)。红色的预测失误区域比用分析朴素贝叶斯预测获得的预测失误区域更大。众所周知，逻辑回归是一种线性分类器，因此预计图 5B 中的预测接近完美。在所有情况下，MLP 当然是最好的，即使在一个隐藏层中只有 4 个神经元。但是我们将在后面看到，这种快乐的情景不适用于具有更严重非线性的边界。</p><h1 id="fad0" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">4.2 分类指标</h1><p id="d60a" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">图 6 比较了三个分类器在多个仿真中的性能，其中比率<em class="lp"> A_1/A_2 </em>在保持<em class="lp"> A_1 </em> + <em class="lp"> A_2 </em>为 1 的同时发生变化。这非常类似于图 4，其中我们比较了分析性朴素贝叶斯预测和事实。一些简单的观察如下。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nf"><img src="../Images/0cda20147f949cb21a42cdfc5f0278de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FgLfESZsa7vphIQp.jpg"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Figure 6. In all the cases above <em class="mc">a = b = 1.0</em>, so <em class="mc">A_1 + A_2 = 1.0</em>. All three classifiers can be considered quite good even while MLP does beat out the rest in general.</figcaption></figure><ul class=""><li id="9ba0" class="ng nh iq jp b jq jr ju jv jy ni kc nj kg nk kk nl nm nn no bi translated">朴素贝叶斯具有最大的误差区域(<em class="lp"> FP </em>和<em class="lp"> FN </em>)。即使在线性情况下，逻辑回归的 FP 也随着<em class="lp">a1/a2</em>增加。鉴于逻辑回归是一个线性分类器，这将需要一些进一步的分析，为什么。</li><li id="9b83" class="ng nh iq jp b jq np ju nq jy nr kc ns kg nt kk nl nm nn no bi translated">就错误表征的总面积(即<em class="lp"> FP + FN </em>)而言，我们看到 MLP、逻辑回归和朴素贝叶斯的优秀程度依次递减。</li><li id="a998" class="ng nh iq jp b jq np ju nq jy nr kc ns kg nt kk nl nm nn no bi translated">根据需求，人们可以选择在该标准方面表现优异的分类器。例如，MLP 在这一点上非常突出。</li></ul><h1 id="6a39" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">5.神经网络更擅长分类吗？</h1><p id="4e9c" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">从第 3 节和第 4 节的结果来看，神经网络似乎更善于对具有线性或非线性分离边界的数据进行分类。但是这样的概括可能是错误的。当我们知道真相时，就像这里的情况一样，我们可以对隐藏层的数量、内部神经元的数量、激活函数等进行调整，以获得与真相更好的匹配。但这一结果可能不具有超出这一具体实践范围的普遍有效性。为了在实践中看到这一点，让我们考虑正弦波作为分离边界的情况，并尝试用同样的三种方案进行分类。</p><p id="0dfe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">(9)</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/0d59a36391e702f768c34c0363efaf5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/0*aPCIgUNzmSh6ql7C.png"/></div></figure><p id="8246" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着<em class="lp"> k </em>的增加，特征空间被分割成越来越多的不连续区域，分类变得更加困难。使用 SciKit 获得分类结果的机制与第 4 节中的相同，因此我们直接查看结果。</p><p id="69f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图 7A 绘制了当不连续区域的数量通过改变<em class="lp"> k </em>而变化时，相交区域的大小。很明显，当我们使用单一隐藏层时，MLP 对任何 k 都没有好处。MLP。<em class="lp"> FP </em>和<em class="lp"> FP </em>加起来大约占总面积的 20-30 %,所以有很多数据会被错误分类。这反映在图 7B 中较小的特异性值中。对于 6 个隐藏层，MLP 对于所有的<em class="lp"> k </em>值具有小得多的误差区域，从而具有更好的特异性，如图 7B 所示。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mh"><img src="../Images/94aabbd3d2bd1ed1a0dd3921b43f0afa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JCTxBLXJ-rQwmWCP.jpg"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Figure 7. The upshot from the results here is that MLP can do better but careful experimentation is required before generalizing the applicability of the model. When <em class="mc">k = 4</em>, in Equation 9, there would be 4 non-contiguous zones for the two classes. (A) MLP with a single hidden layer yields no improvement over naive bayes or logistic regression for any k. Six hidden layers with ten neurons each yields better classification but there could be other combinations that can do equally well or better. (B) Much better specificity with 6 hidden layers for all values of k. (C — E): Increasing the number of layers is not guaranteed to improve performance for MLP. It starts to get better as we add more hidden layers but worsens after 6 layers.</figcaption></figure><p id="0ce3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图 7C-7E 显示了当<em class="lp"> k = 4 </em>时所有情况下获得的误差区域。误差区域随着隐藏层的增加而减少，产生 6 层的最佳分类。但不幸的是，7 层的效果更差，有点出乎意料…所谓的炼金术在起作用。显然，这里还有更多的事情需要更深入的分析。</p><h1 id="a94f" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">6.后续步骤</h1><p id="932e" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">至此，我们结束了这篇有点长但简单明了的帖子。对于线性和抛物线分离边界，我们已经表明，这里测试的所有三个分类器都很好，MLP 挤掉了逻辑回归和朴素贝叶斯。MLP 的出色表现很有趣，但目前还不清楚它究竟如何做得更好。对于更严重的非线性，MLP 可能工作得很好，但是可能需要仔细的实验来概括所获得的结果。尽管如此，这里的结果是足够令人鼓舞的，我们希望在未来的帖子中尝试使用神经网络来解决文本分类问题。</p></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="23e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lp">原载于 2018 年 8 月 25 日</em><a class="ae kl" href="http://xplordat.com/2018/08/25/naive-bayes-classifier-part-2-characterization-and-evaluation/" rel="noopener ugc nofollow" target="_blank"><em class="lp">【xplordat.com】</em></a><em class="lp">。</em></p></div></div>    
</body>
</html>