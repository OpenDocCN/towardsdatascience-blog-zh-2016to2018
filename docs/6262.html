<html>
<head>
<title>Linear Algebra explained in the context of deep learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习背景下的线性代数解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-algebra-explained-in-the-context-of-deep-learning-8fcb8fca1494?source=collection_archive---------6-----------------------#2018-12-04">https://towardsdatascience.com/linear-algebra-explained-in-the-context-of-deep-learning-8fcb8fca1494?source=collection_archive---------6-----------------------#2018-12-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/2cb23c81bb5de620c2dbd1831963b636.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RVWh4RPCnEP17EML"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@charlesdeluvio?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Charles Deluvio 🇵🇭🇨🇦</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="85ce" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我使用了自顶向下的方式来解释深度学习的线性代数。首先提供应用程序和用途，然后深入提供概念。</p><p id="d7d8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">维基百科中线性代数的定义:</p><blockquote class="lb lc ld"><p id="73b6" class="kd ke le kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated"><strong class="kf ir">线性代数</strong>是关于<strong class="kf ir">线性</strong>方程和<strong class="kf ir">线性</strong>函数及其通过矩阵和向量空间表示的数学分支。</p></blockquote><h1 id="6176" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">目录:</h1><ul class=""><li id="d6e0" class="mg mh iq kf b kg mi kk mj ko mk ks ml kw mm la mn mo mp mq bi translated">引言。</li><li id="275a" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated">向量和矩阵的数学观点。</li><li id="f7f9" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated">矩阵的类型。</li><li id="097e" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated">矩阵分解。</li><li id="3305" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated">规范。</li><li id="1356" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated">矢量化。</li><li id="e5c6" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated">广播。</li><li id="0169" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated">外部资源。</li></ul><figure class="mx my mz na gt jr gh gi paragraph-image"><a href="https://www.buymeacoffee.com/laxmanvijay"><div class="gh gi mw"><img src="../Images/6d60b235fcc46a4bd696b90e886419ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*Dpw8-hNGI2fDmosV4E8DVQ.png"/></div></a><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="https://www.buymeacoffee.com/laxmanvijay" rel="noopener ugc nofollow" target="_blank">Laxman Vijay (buymeacoffee.com)</a></figcaption></figure><p id="ee68" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">简介:</strong></p><p id="1a8f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你开始学习深度学习，你首先会接触到的是前馈神经网络，这是深度学习中最简单也是最有用的网络。在引擎盖下，前馈神经网络只是一个复合函数，将一些矩阵和向量相乘。</p><p id="fb11" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这并不是说向量和矩阵是进行这些运算的唯一方法，但是如果你这样做，它们会变得非常高效。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/85fc627a7050abc6d013df1864957db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*pI5c92ACX6Burmzet8uQYQ.png"/></div></figure><p id="85a2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上图显示了一个简单的向前传播信息的前馈神经网络。</p><p id="7ee5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该图像是神经网络的漂亮表示，但是计算机如何理解这一点。在计算机中，神经网络的各层用向量表示。将输入层视为 X，将隐藏层视为 h。现在不考虑输出层。(这里不涉及前馈神经网络的计算过程。)</p><p id="6c6f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，它可以用向量和矩阵来表示，</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/ccdb41f911d6a24bb29b13135517ac70.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*xmWCqN_VbKQXJg8veEpXug.png"/></div></figure><p id="7ea8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上图显示了计算上述神经网络的第一个也是唯一一个隐藏层的输出所需的操作(未显示输出层的计算)。我们来分解一下。</p><p id="d4f6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">网络的每一列都是向量。向量是数据(或特征)集合的动态数组。在当前的神经网络中，向量'<strong class="kf ir"> x </strong>'保存输入。将输入表示为向量并不是强制性的，但是如果你这样做，它们会变得越来越便于并行执行操作。</p><p id="81f6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度学习，具体来说，神经网络的计算成本很高，所以它们需要这个好技巧来加快计算速度。</p><p id="1067" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">这叫矢量化。它们使得计算速度极快。这就是为什么深度学习需要 GPU 的主要原因之一</strong><a class="ae kc" href="https://www.analyticsvidhya.com/blog/2017/05/gpus-necessary-for-deep-learning/" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"><em class="le"/></strong></a><strong class="kf ir">，因为它们擅长矩阵乘法之类的矢量化运算。(我们将在最后深入了解这一点)。</strong></p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><p id="a3d8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">隐藏层 H 的输出通过执行<strong class="kf ir"> H = f( <em class="le"> W </em>来计算。x + b)。</strong></p><p id="f8fe" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里 W 称为权重矩阵，b 称为偏差，f 是激活函数。(本文不解释关于前馈神经网络，如果你需要一本关于 FFNN 概念的初级读本，<a class="ae kc" rel="noopener" target="_blank" href="/deep-learning-feedforward-neural-network-26a6705dbdc7">看这里</a>。)</p><p id="be15" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们分解这个等式，</p><p id="59e4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一个组件是<strong class="kf ir"> <em class="le"> W </em> </strong>。<strong class="kf ir">T11】xT13】；这是一个<strong class="kf ir">矩阵-向量乘积，</strong>因为 W 是矩阵，<strong class="kf ir"> <em class="le"> x </em> </strong>是向量。在开始乘法运算之前，让我们先了解一下符号:<em class="le">通常向量用小粗斜体字母表示(如</em> <strong class="kf ir"> <em class="le"> x </em> </strong> <em class="le">)，矩阵用大写粗斜体字母表示(如</em> <strong class="kf ir"> <em class="le"> X </em> </strong> <em class="le">)。如果字母是大写加粗但不是斜体，那么它是一个张量(如</em> <strong class="kf ir"> X </strong> <em class="le">)。</em></strong></p><p id="e90d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从计算机科学的角度来看</p><p id="37d8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">标量:单个数字。</p><p id="0656" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Vector:值的列表。(秩 1 张量)</p><p id="d545" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">矩阵:二维值列表。(秩 2 张量)</p><p id="09be" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">张量:秩为 n 的多维矩阵。</p><p id="7868" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">向下钻取:</strong></p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/54b5f0bd130b1071079b14cb76443b8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*h44UirtotiOItVLZGBI6Nw.png"/></div></figure><p id="fa44" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">从数学的角度:</strong></p><p id="b6a3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">矢量:</strong></p><p id="c609" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">矢量是既有大小又有方向的量。它是一个存在于空间的实体，如果它是一个存在于真实空间的二维向量，它的存在用<strong class="kf ir"> <em class="le"> x∈ ℝ </em> </strong> <em class="le">表示。(每个元素表示沿不同轴的坐标。)</em></p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/8370121a156925212190bf32296347fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*OGCWkaenQbXJ7vgWOLgAJw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">red and blue color vectors are the basis vectors.</figcaption></figure><p id="bb4b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2D 空间中的所有向量都可以通过称为<strong class="kf ir">基向量</strong>的两个向量的线性组合来获得。(用 I 和 j 表示)(一般来说，N 维的向量可以用 N 个基向量来表示。)它们是<strong class="kf ir">单位法向量，因为它们的大小是 1，并且它们彼此垂直</strong>。这两个向量中的一个不能用另一个向量来表示。所以它们被称为<strong class="kf ir">线性无关矢量</strong>。(如果任何一个向量不能由一组向量的线性组合得到，那么这个向量与那个集合线性无关)。可以通过这两个向量的线性组合获得的 2D 空间中的所有点集被称为这些向量的<strong class="kf ir">跨度</strong>。如果一个向量由一组其他向量的线性组合(加法、乘法)来表示，那么它<strong class="kf ir">线性依赖于该组向量</strong>。(将这个新向量添加到现有集合中是没有用的。)</p><p id="74f9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">任何两个矢量都可以相加。它们可以相乘。它们的乘法有两种类型，点积和叉积。<a class="ae kc" href="https://ltcconline.net/greenl/courses/107/vectors/dotcros.htm" rel="noopener ugc nofollow" target="_blank">参考此处。</a></p><p id="0711" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">矩阵:</strong></p><p id="1120" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">矩阵是数字的 2D 阵列。它们代表<strong class="kf ir">转换</strong>。2 * 2 矩阵的每一列表示在 2D 空间被应用该变换之后的 2 个基本向量中的每一个。它们的空间表示是<strong class="kf ir"> <em class="le"> W ∈ ℝ *有 3 行 2 列。</em>T19】</strong></p><p id="55d3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">一个矩阵的向量积叫做那个向量的变换，而一个矩阵的矩阵积叫做变换的合成。</strong></p><p id="9e1c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">只有一个矩阵不对向量做任何变换。就是单位矩阵(<strong class="kf ir"> <em class="le"> I </em> </strong>)。<strong class="kf ir"><em class="le"/></strong><strong class="kf ir"><em class="le">I 列代表基矢。</em> </strong></p><p id="ee63" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">矩阵的行列式</strong><strong class="kf ir"><em class="le"/></strong>用 det(<strong class="kf ir"><em class="le"/></strong>)表示矩阵所描述的线性变换的比例因子。</p><p id="fa8f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">为什么数学视角对深度学习研究者很重要？因为它们帮助我们理解基本对象的基本设计概念。他们也帮助设计深度学习问题的创造性解决方案。但是不用担心，有很多语言和软件包为我们做这些实现。但是知道它们的实现也很好。</strong></p><p id="7c84" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">python 编程语言的 numpy 就是这样一个库。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/f09d8c86fa4e376dd887e2b7544d883d.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*zE4NFdMVbMCA0CGujquaeA.png"/></div></figure><p id="5a5c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有很多学习数字的资源。(这对学习深度学习很重要，如果用 python 的话。)<a class="ae kc" href="https://docs.scipy.org/doc/numpy-1.15.0/user/basics.html" rel="noopener ugc nofollow" target="_blank">看这里</a>。</p><p id="6993" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="le">在这里，np.array 创建了一个 numpy 数组。</em></p><p id="b1be" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="le"> np.random 是一个包含随机数生成方法的包。</em></p><p id="2fa3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="le">点法是计算矩阵间乘积的方法。</em></p><p id="5113" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="le">我们可以改变 numpy 数组的形状并检查它。</em></p><p id="543f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="le">这里可以看到，W.x 的乘积是一个矢量，加上 b，b 是一个标量。这会自动将 b 扩展为转置([1，1])。</em> <strong class="kf ir"> <em class="le">这种 b 到几个位置的隐式复制称为广播。(一会儿我们会深入了解。)</em> </strong></p><p id="f427" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="le">你注意到转置这个词了吗:</em> <strong class="kf ir"> <em class="le">一个矩阵的转置是矩阵的镜像跨过对角线(从矩阵的左上到右下。</em>)</strong></p><pre class="mx my mz na gt nm nn no np aw nq bi"><span id="b56c" class="nr lj iq nn b gy ns nt l nu nv">## numpy code for transpose<br/>import numpy as np<br/>A = np.array([[1,2],<br/>              [3,4],<br/>              [5,6]])<br/>B = np.transpose(A) <br/>##or<br/>B = A.T</span></pre><p id="ddba" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">矩阵类型:</strong></p><p id="4062" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">对角矩阵</strong>:除主对角元素外，所有元素为零。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/7feb405935371b3fc6751c013b2014b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*niNtr3lEec8Ng56E74NAXw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">diagonal matrix</figcaption></figure><p id="bf50" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">单位矩阵</strong>:对角值为 1 的对角矩阵。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/fe04a540af6bbf63245f641650ebb69a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*h2En9F9dKvpd7PW0xTxMTA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">identity matrix</figcaption></figure><pre class="mx my mz na gt nm nn no np aw nq bi"><span id="3f3b" class="nr lj iq nn b gy ns nt l nu nv">## numpy code to create identity matrix<br/>import numpy as np<br/>a = np.eye(4)</span></pre><p id="3f49" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对称矩阵:与其转置矩阵相等的矩阵。A =转置(A)</p><p id="e88d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">奇异矩阵</strong>:行列式为零，列线性相关的矩阵。它们的秩小于矩阵的行数或列数。</p><p id="e5a9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">矩阵分解:</strong></p><p id="69eb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">矩阵分解</strong>或<strong class="kf ir">矩阵分解</strong>是将矩阵分解成矩阵的乘积。有许多不同的矩阵分解；每一种都在一类特定的问题中找到用途。最广泛使用的一种矩阵分解叫做<strong class="kf ir">特征分解</strong>，我们将矩阵分解成一组特征向量和特征值。</p><p id="ad14" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">方阵的本征向量<strong class="kf ir"><em class="le"/></strong>是非零向量<strong class="kf ir"><em class="le"/></strong>使得乘以<strong class="kf ir"><em class="le"/></strong>仅改变<strong class="kf ir"><em class="le"/></strong>的比例</p><p id="0151" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"><em class="le">A . v</em></strong>=λ。<strong class="kf ir"> <em class="le"> v </em> </strong></p><p id="f5ec" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里<strong class="kf ir"> <em class="le"> v </em> </strong>是本征向量，λ是本征值。</p><pre class="mx my mz na gt nm nn no np aw nq bi"><span id="8db0" class="nr lj iq nn b gy ns nt l nu nv">## numpy program to find eigen vectors.<br/><br/>from numpy import array<br/>from numpy.linalg import eig<br/># define matrix<br/>A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])<br/>print(A)<br/># calculate eigendecomposition<br/>values, vectors = eig(A)<br/>print(values)<br/>print(vectors)</span></pre><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/8a4be0b11c070686194125d053c776c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z_yGwdmC993Zs2iUjMcAQA.png"/></div></div></figure><p id="eccd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">特征分解在机器学习中非常有用。这对于降维这样的概念特别有用。</p><p id="56ec" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有关特征分解的更多信息，请参考深度学习书籍<a class="ae kc" href="http://www.deeplearningbook.org/contents/linear_algebra.html" rel="noopener ugc nofollow" target="_blank">第 2 章</a></p><p id="95e2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度学习中还会用到其他几种矩阵分解技术。<a class="ae kc" href="https://heartbeat.fritz.ai/applications-of-matrix-decompositions-for-machine-learning-f1986d03571a" rel="noopener ugc nofollow" target="_blank">看这里。</a></p><p id="3975" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">规范:</strong></p><p id="2e6e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">过拟合和欠拟合:</strong></p><p id="1d8c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当你参加深度学习讲座时，你经常会听到过度适应和欠适应这个术语。这些术语描述了深度学习模型的准确性状态。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nz"><img src="../Images/a509007d016bf2b89701f01e43748e90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_7OPgojau8hkiPUiHoGK_w.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">This image is the best explanation of overfitting and underfitting.</figcaption></figure><p id="7a7c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">过拟合是指模型对训练数据学习得太好。它真的窃取了训练数据。在过拟合模型中，训练精度非常高，验证精度非常低。</p><p id="82af" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">欠拟合模型无法学习训练数据。在欠拟合模型中，训练和验证精度都非常低。</p><p id="d75a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">过度拟合和欠拟合都会导致模型性能不佳。但到目前为止，应用机器学习中最常见的问题是过度拟合。</p><p id="d0ed" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了减少过度拟合，我们必须使用一种叫做<strong class="kf ir">正则化的技术。<em class="le">防止对训练数据的强记，从而避免过拟合的风险。</em>T13】</strong></p><p id="c8d8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度学习工程师的<strong class="kf ir">最重要的职责</strong>是创建一个通常适合输入的模型。正则化有几种方法。最著名的是<strong class="kf ir"> L1 正则化(Lasso)和 L2 正则化(Ridge)。</strong></p><p id="2bdc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">没有提供这些的细节，但是要理解这些，你必须知道什么是规范。</p><p id="f5a5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">定额:</strong></p><p id="6138" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">范数是向量的大小。向量<strong class="kf ir"> <em class="le"> x </em> </strong>的范数的一般公式为:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/c23fe251b2846b038f0b8122bc0b0b93.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*51aoPMb2DjeeuLCBQ0qT2Q.png"/></div></figure><p id="c422" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">p=2 的<strong class="kf ir"> L 范数</strong>称为欧几里德范数，因为它是原点和<strong class="kf ir"> <em class="le"> x. </em> </strong>之间的欧几里德距离</p><p id="d40b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> L 范数</strong>就是向量所有元素的和。当系统需要更高的精度时，它用于机器学习。清楚地区分零元素和非零元素。L 范数也被称为<strong class="kf ir">曼哈顿范数</strong>。</p><p id="ea95" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还有<strong class="kf ir">最大范数，</strong>是量值最大的元素的绝对值。</p><p id="9325" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">矩阵的 L 范数等价于<strong class="kf ir"> frobenius 范数。</strong></p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/5272777b1af13263b1990d8bb7c3a081.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*OTD-K5jjCklutYc-c-dkQQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">frobenius norm.</figcaption></figure><p id="c969" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不仅在正则化，规范也用于优化程序。</p><p id="88c2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好了，现在在所有这些概念和理论之后，我们开始涵盖深度学习所需的最重要的部分。它们是向量化和广播。</p><h1 id="a25e" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated"><strong class="ak">矢量化:</strong></h1><p id="b4c2" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">这是减少循环执行的技巧，通过提供向量形式的数据使进程并行执行。</p><p id="e333" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">许多 CPU 都有“向量”或“SIMD”(单指令多数据)指令集，这些指令集同时对两个、四个或更多个数据应用相同的操作。20 世纪 90 年代初，SIMD 在通用 CPU 上开始流行。</p><p id="7ca2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更多详细信息，请查看<a class="ae kc" href="https://en.wikipedia.org/wiki/Flynn%27s_taxonomy" rel="noopener ugc nofollow" target="_blank">弗林的分类。</a></p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/41bb793b231152bb6e8457dc76cdb557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*23cQOr3db-kngEFo8l9z1w.png"/></div></div></figure><p id="ef34" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">向量化是重写一个循环的过程，它不是处理数组中的一个元素 N 次，而是同时处理(比如说)数组中的 4 个元素 N/4 次。</p><p id="0b46" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Numpy 在他们的算法中大量实现了矢量化。这是 numpy 的官方声明。</p><blockquote class="lb lc ld"><p id="5ea3" class="kd ke le kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">矢量化描述了没有任何显式循环、索引等。，在代码中——当然，这些事情只是在优化、预编译 C 代码的“幕后”发生。矢量化代码有许多优点，其中包括:</p><p id="28fe" class="kd ke le kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">矢量化代码更加简洁，也更容易阅读</p><p id="ea62" class="kd ke le kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">更少的代码行通常意味着更少的错误</p><p id="fb08" class="kd ke le kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">代码更类似于标准的数学符号(通常更容易正确地编写数学结构)</p><p id="8173" class="kd ke le kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">矢量化会产生更多“Pythonic 式”代码。如果没有向量化，我们的代码将充斥着低效和难以阅读的循环。</p></blockquote><p id="0404" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">代码示例:</strong></p><pre class="mx my mz na gt nm nn no np aw nq bi"><span id="3ef5" class="nr lj iq nn b gy ns nt l nu nv">## to add two arrays together.</span><span id="d3dc" class="nr lj iq nn b gy og nt l nu nv">## consider two basic python lists.<br/>a = [1,2,3,4,5]<br/>b = [2,3,4,5,6]<br/>c = []</span><span id="8ab2" class="nr lj iq nn b gy og nt l nu nv">## without vectorization.</span><span id="4a94" class="nr lj iq nn b gy og nt l nu nv">for i in range(len(a)): <br/>    c.append(a[i]+b[i])</span><span id="4f1f" class="nr lj iq nn b gy og nt l nu nv">## using vectorization.</span><span id="694c" class="nr lj iq nn b gy og nt l nu nv">a = np.array([1,2,3,4,5])<br/>b = np.array([2,3,4,5,6])<br/>c = a+b</span></pre><p id="f470" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面的代码示例是一个过于简化的矢量化示例。而<strong class="kf ir">当输入数据变大时，矢量化才真正发挥作用。</strong></p><p id="edd1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于矢量化的更多细节，请看这里。</p><h1 id="1840" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">广播:</h1><p id="3944" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko oc kq kr ks od ku kv kw oe ky kz la ij bi translated">下一个重要的概念是广播。杰瑞米·霍华德爵士在他的一次机器学习讲座中说，广播可能是机器学习程序员最重要的工具和技能。</p><p id="0c25" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来自<a class="ae kc" href="https://docs.scipy.org/doc/numpy-1.10.0/user/basics.broadcasting.html" rel="noopener ugc nofollow" target="_blank"> Numpy 文档</a>:</p><pre class="mx my mz na gt nm nn no np aw nq bi"><span id="55ae" class="nr lj iq nn b gy ns nt l nu nv">The term broadcasting describes how numpy treats arrays with <br/>different shapes during arithmetic operations. Subject to certain <br/>constraints, the smaller array is “broadcast” across the larger <br/>array so that they have compatible shapes. Broadcasting provides a <br/>means of vectorizing array operations so that looping occurs in C<br/>instead of Python. It does this without making needless copies of <br/>data and usually leads to efficient algorithm implementations.</span></pre><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/150c1e8f434b2991ed0bbdabcbdcb7e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*AbfEjGON0C52GcwQ18bvTw.jpeg"/></div></figure><p id="c7e3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代码示例:</p><pre class="mx my mz na gt nm nn no np aw nq bi"><span id="7403" class="nr lj iq nn b gy ns nt l nu nv"><strong class="nn ir"> </strong>a = np.array([1.0, 2.0, 3.0])<br/><strong class="nn ir"> </strong>b = 2.0<br/> a * b<br/> array([ 2.,  4.,  6.])</span><span id="b66e" class="nr lj iq nn b gy og nt l nu nv">this is similar to</span><span id="d00c" class="nr lj iq nn b gy og nt l nu nv">a = np.array([1.0, 2.0, 3.0])<br/>b = np.array([2.0, 2.0, 2.0])<br/>a * b<br/>array([ 2.,  4.,  6.])</span></pre><p id="a919" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数组 b 被扩展，从而可以应用算术运算。</p><p id="6442" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">广播不是一个新概念。这是一个相对古老的工具，可以追溯到 50 年代。在他的论文<a class="ae kc" href="http://www.eecg.toronto.edu/~jzhu/csc326/readings/iverson.pdf" rel="noopener ugc nofollow" target="_blank"> " <strong class="kf ir">符号作为思维工具</strong> " </a>，<a class="ae kc" href="https://en.wikipedia.org/wiki/Kenneth_E._Iverson" rel="noopener ugc nofollow" target="_blank">中，Kenneth Iverson </a>描述了几种数学使用工具，这些工具允许我们以新的视角思考。他第一次提到广播不是作为一种计算机算法，而是作为一种数学过程。他在一个名为<a class="ae kc" href="https://en.wikipedia.org/wiki/APL_(programming_language)" rel="noopener ugc nofollow" target="_blank"> APL </a>的软件中实现了许多这样的工具。</p><p id="f711" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">他的儿子后来扩展了他的想法，继续创造了另一个软件，叫做 J 软件。这一姿态意味着，通过软件，我们得到的是超过 50 年的深入研究，利用这些，我们可以在一小段代码中实现非常复杂的数学函数。</p><p id="7b65" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样非常方便的是，这些研究也在我们今天使用的语言中找到了自己的方式，比如 python 和 numpy。</p><p id="0966" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以请记住，这些不是一夜之间产生的简单想法。这些就像是思考数学及其软件实现的基本方法。(以上内容摘自 fast.ai 机器学习课程。)</p><p id="fa4f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于 numpy 版本广播的更多细节，<a class="ae kc" href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html" rel="noopener ugc nofollow" target="_blank">看这里</a>。</p><p id="7ffe" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好了，这就够了，这篇文章向初学者介绍了许多新单词和术语。但是我也跳过了几个向量代数的深层概念。这可能是压倒性的，但我仍然使概念尽可能实用，(欢迎反馈！).</p><p id="3ec6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于我刚刚开始深度学习，我决定帮助其他已经开始的人，为他们提供关于深度学习术语和东西的直观文章。所以如果你觉得这些文章有什么错误，请在评论中发表。</p><p id="11fa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">以下是一些有用的资源:</strong></p><p id="9a92" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深入直观的线性代数视频:<a class="ae kc" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="noopener ugc nofollow" target="_blank"> 3blue1brown </a></p><p id="3271" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">电子书:<a class="ae kc" href="https://www.math.ucdavis.edu/~linear/linear-guest.pdf" rel="noopener ugc nofollow" target="_blank">https://www.math.ucdavis.edu/~linear/linear-guest.pdf</a></p><p id="1e5a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">学习深度学习的最佳站点:<a class="ae kc" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai </a></p><p id="6141" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一本完整的学习书:<a class="ae kc" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">深度学习书，作者 Ian Goodfellow。</a></p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/6d60b235fcc46a4bd696b90e886419ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*Dpw8-hNGI2fDmosV4E8DVQ.png"/></div></figure></div></div>    
</body>
</html>