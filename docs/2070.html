<html>
<head>
<title>Automatic Speaker Recognition using Transfer Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于迁移学习的自动说话人识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/automatic-speaker-recognition-using-transfer-learning-6fab63e34e74?source=collection_archive---------2-----------------------#2017-12-13">https://towardsdatascience.com/automatic-speaker-recognition-using-transfer-learning-6fab63e34e74?source=collection_archive---------2-----------------------#2017-12-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/71ad1f620d4bc6e8e27d1420949ea15d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4bDmlXpxMj7HfD37B2D3Fw.jpeg"/></div></div></figure><div class=""/><p id="d7b7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由Christopher Gill、Hamza Ghani、Yousef Abdelrazzaq、Minkoo Park设计的项目</p><blockquote class="kx ky kz"><p id="35dd" class="jy jz kw ka b kb kc kd ke kf kg kh ki la kk kl km lb ko kp kq lc ks kt ku kv ij bi translated">当面临创建一个动态的<strong class="ka jc">声音识别器</strong>的挑战时，我们团队自然会选择一个<strong class="ka jc">图像分类器</strong></p></blockquote><p id="e7eb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">即使今天语音交互设备(想想Siri和Alexa)的技术突破频繁，也很少有公司尝试过支持多用户配置文件。Google Home在这一领域最为雄心勃勃，允许多达六个用户配置文件。这项技术最近的蓬勃发展使得这个项目的潜力让我们的团队非常兴奋。我们还想从事一个在深度学习研究中仍然是热门话题的项目，创建有趣的工具，了解更多关于神经网络架构的知识，并在可能的情况下做出原创贡献。</p><p id="d068" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们试图创建一个系统，能够快速添加用户资料，并准确地识别他们的声音，只有很少的训练数据，最多几个句子！这种从一个样本到仅有几个样本的学习被称为<a class="ae ld" href="https://en.wikipedia.org/wiki/One-shot_learning" rel="noopener ugc nofollow" target="_blank"> <em class="kw">一次学习</em> </a>。本文将详细概述我们项目的各个阶段。</p><h1 id="0246" class="le lf jb bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">一.项目概述</h1><p id="67f7" class="pw-post-body-paragraph jy jz jb ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated"><strong class="ka jc">目标:</strong>用最少的训练对说话者进行分类，这样只需要几个单词或句子就可以达到很高的准确率。</p><p id="7158" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">数据:</strong>训练数据是从开放域有声读物的来源Librivox上刮下来的。测试数据要么是从YouTube上刮下来的，要么是现场收集的。</p><p id="41c6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">方法:</strong>总之，我们将所有的音频数据转换为声谱图形式。然后，我们在许多说话人身上训练了一个来自Cifar-10的CNN作为特征提取器，输入到SVM中进行最终分类。这种方法被称为<em class="kw"/><a class="ae ld" href="http://ruder.io/transfer-learning/" rel="noopener ugc nofollow" target="_blank"><em class="kw"/></a>转移学习。这种方法使我们能够获得SVM的小样本高性能和CNN的特征学习。</p><p id="d776" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们提出的系统有很多潜在的应用。它们的范围从家庭助理需求(想想Alexa和Google Home)到生物安全、营销工具，甚至是间谍活动(识别高调目标)。它也可以用作语音数据收集中<a class="ae ld" href="https://en.wikipedia.org/wiki/Speaker_diarisation" rel="noopener ugc nofollow" target="_blank">说话人日记化</a>的工具。给定先前对所包含的语音的一些小的暴露，具有多个扬声器的音频文件可以被准确地分离。这为更多潜在的“干净数据”打开了大门，这些数据可用于创建更复杂的特定于语音的模型。</p><p id="eaa7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">业绩:</strong>结果基本上是积极的。通过20-35秒的训练音频，我们的模型能够在我们的测试中以63-95%的准确率区分三个扬声器。然而，在5个以上的发言者或统一的性别测试组中，表现会严重下降。</p><p id="2111" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc"> Github链接:</strong>【https://github.com/hamzag95/voice-classification】T4</p><h1 id="2a6e" class="le lf jb bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak">二。数据收集</strong></h1><h2 id="d702" class="mh lf jb bd lg mi mj dn lk mk ml dp lo kj mm mn ls kn mo mp lw kr mq mr ma ms bi translated"><strong class="ak">背景</strong></h2><p id="a133" class="pw-post-body-paragraph jy jz jb ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">说话人和语音识别领域的最大挑战之一是缺乏开源数据。大多数语音数据要么是专有的、难以访问的、未被充分标记的、每个说话者的数量不足的，要么是嘈杂的。在许多相关研究论文中，数据不足被引用作为不追求更进一步、更复杂的模型和应用的理由。</p><p id="00db" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们看到这是一个为这一领域的研究做出新贡献的机会。几个小时的谷歌搜索后，我们的团队得出结论，我们项目的最佳潜在音频来源是<a class="ae ld" href="https://librivox.org/search?primary_key=0&amp;search_category=author&amp;search_page=1&amp;search_form=get_results" rel="noopener ugc nofollow" target="_blank"> LibriVox </a>，一个开放领域有声读物的广泛来源，以及<a class="ae ld" href="https://www.youtube.com/" rel="noopener ugc nofollow" target="_blank"> YouTube </a>。这些来源是根据最符合我们的标准(如下所示)而选择的。</p><h2 id="67ab" class="mh lf jb bd lg mi mj dn lk mk ml dp lo kj mm mn ls kn mo mp lw kr mq mr ma ms bi translated">音频源标准</h2><ul class=""><li id="b8e7" class="mt mu jb ka b kb mc kf md kj mv kn mw kr mx kv my mz na nb bi translated">一个模型有足够多的独特说话者来学习语音中的普遍差异</li><li id="fd83" class="mt mu jb ka b kb nc kf nd kj ne kn nf kr ng kv my mz na nb bi translated">男性和女性发言人，最好使用多种语言</li><li id="1137" class="mt mu jb ka b kb nc kf nd kj ne kn nf kr ng kv my mz na nb bi translated">每个扬声器至少有1小时的音频可用</li><li id="bca2" class="mt mu jb ka b kb nc kf nd kj ne kn nf kr ng kv my mz na nb bi translated">音频可以自动标记元数据</li><li id="7204" class="mt mu jb ka b kb nc kf nd kj ne kn nf kr ng kv my mz na nb bi translated">音频噪音极小(几乎没有背景噪音/音乐，质量不错，几乎没有外来声音)</li><li id="ca15" class="mt mu jb ka b kb nc kf nd kj ne kn nf kr ng kv my mz na nb bi translated">用于合法使用和数据集使用的开放域或知识共享许可</li></ul><h2 id="ede7" class="mh lf jb bd lg mi mj dn lk mk ml dp lo kj mm mn ls kn mo mp lw kr mq mr ma ms bi translated"><strong class="ak"> <em class="nh">从Librivox中抓取音频</em> </strong></h2><p id="b731" class="pw-post-body-paragraph jy jz jb ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">我们使用BeautifulSoup和Selenium编写了脚本来解析网站LibriVox并下载我们想要的有声书。BeautifulSoup本身是不够的，因为部分网站需要时间(1-2秒)来加载。因此，我们使用selenium来等待，直到网页上的某些元素出现并变得可废弃。</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/5f26921d4fd0944c3a16239057ba1411.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D4rtqgxgWDfgp_Pz."/></div></div></figure><p id="9261" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的第一次尝试使用了一个脚本，该脚本从LibriVox的默认主页开始移动，并下载页面中特定文件大小范围内的所有音频。我们后来意识到这是有缺陷的，因为许多有声读物实际上是多个叙述者的合作，很难自动分离。因此，我们必须找到一种方法来获得带有独特扬声器的有声读物。不幸的是，LibriVox API没有包含一个根据项目类型(单独或协作)或讲述人姓名进行过滤的字段。</p><p id="f15c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">相反，我们使用高级搜索，只包括“独唱”叙述者书籍。很快我们意识到假设每本书都有一个独特的演讲者是有问题的，因为LibriVox有许多重复的叙述者。为了解决这个问题，我们必须阅读每本书的元数据，以维护一个勉强的叙述者列表，以确保我们的数据被正确标记。最终，我们拥有6000多个独特的扬声器和24000多个小时的音频链接。然而，由于时间限制，我们对162个不同的说话人进行了采样，以进行声谱图转换。下载链接的完整列表可以在我们的项目GitHub <a class="ae ld" href="https://github.com/hamzag95/voice-classification/blob/master/data_collection/download_links.txt" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="7d4f" class="mh lf jb bd lg mi mj dn lk mk ml dp lo kj mm mn ls kn mo mp lw kr mq mr ma ms bi translated"><strong class="ak"> <em class="nh">从Youtube上抓取音频</em> </strong></h2><p id="aad9" class="pw-post-body-paragraph jy jz jb ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">从Youtube上，我们搜集了7个Youtube明星的视频链接和他们的教程/信息视频。我们发现教程往往最符合我们的标准，因为它们大多包含干净的语言。Selenium需要自动化这个过程，因为抓取YouTube需要滚动。这个过程可以在下面的视频中实时看到。</p><figure class="nj nk nl nm gt is"><div class="bz fp l di"><div class="nn no l"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Scraping videos on Youtube using Selenium</figcaption></figure><p id="2102" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们没有收集更多的个人资料，因为根据视频中包括的嘉宾、音乐等来手动过滤和验证视频的效率很低。尽管就语言清洁度而言，教程频道通常符合要求，但它们在性别上严重偏向男性。视频也会有不同质量的音频和背景噪音。我们决定不使用我们收集的数据来训练神经网络，但认为它们对测试是有用的。YouTube仍然是一个音频来源，具有很大的数据收集潜力，但在日期验证和清理方面要求非常高。</p><h1 id="d812" class="le lf jb bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">三。数据处理</h1><h2 id="d4a4" class="mh lf jb bd lg mi mj dn lk mk ml dp lo kj mm mn ls kn mo mp lw kr mq mr ma ms bi translated">背景</h2><p id="acef" class="pw-post-body-paragraph jy jz jb ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">最终，所有收集的音频都必须转换为503x800(x3)的声谱图图像，这些图像可以捕捉5秒钟的音频。由于下载格式不同，转换从Librivox和YouTube收集的数据的步骤略有不同。</p><p id="b915" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于我们的各种处理需求，我们非常幸运地拥有诸如<a class="ae ld" href="https://www.ffmpeg.org/" rel="noopener ugc nofollow" target="_blank"> ffmpeg </a>、<a class="ae ld" href="https://github.com/chirlu/sox" rel="noopener ugc nofollow" target="_blank"> sox </a>和<a class="ae ld" href="http://mp3splt.sourceforge.net/mp3splt_page/home.php" rel="noopener ugc nofollow" target="_blank"> mp3splt </a>等工具，它们加快了处理速度，同时最大限度地降低了音频质量损失。</p><p id="e1e8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc"> YoutTube音频处理</strong></p><p id="0de0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦收集了YouTube视频链接，我们很幸运地找到了<a class="ae ld" href="https://github.com/rg3/youtube-dl" rel="noopener ugc nofollow" target="_blank"> YouTube-DL </a>库，它允许我们轻松地下载我们想要的WAV格式的视频。当试图将这些数据转换成频谱图时，我们发现每个文件都产生了两个频谱图，因为它是立体声音频。这是我们遇到的与LibriVox音频处理的主要区别。</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/5b804e7f077d83b4bc0f9af1579d80ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDObUEEEp1qAby07XUAhiQ.png"/></div></div></figure><p id="bfa6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，该过程可以总结为以下几点:</p><ol class=""><li id="05f9" class="mt mu jb ka b kb kc kf kg kj nu kn nv kr nw kv nx mz na nb bi translated">手动检查抓取的YouTube链接以验证可用性</li><li id="3b9c" class="mt mu jb ka b kb nc kf nd kj ne kn nf kr ng kv nx mz na nb bi translated">以WAV格式下载所有经过验证的链接，并自动标记/分类音频</li><li id="251f" class="mt mu jb ka b kb nc kf nd kj ne kn nf kr ng kv nx mz na nb bi translated">将单声道WAV文件分割成5秒钟的片段</li><li id="58c3" class="mt mu jb ka b kb nc kf nd kj ne kn nf kr ng kv nx mz na nb bi translated">将所有立体声WAV文件转换为单声道WAV</li><li id="67de" class="mt mu jb ka b kb nc kf nd kj ne kn nf kr ng kv nx mz na nb bi translated">将所有音频片段转换为频谱图</li></ol><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ny"><img src="../Images/39106d60b47fd17a0e3716d6ef049f51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-geWmoHwcI7KIr58."/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">YouTube data processing phases.</figcaption></figure><p id="efe9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">YouTube音频处理的视频可以在下面看到:</p><figure class="nj nk nl nm gt is"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="f5c8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc"> LibriVox音频处理</strong></p><p id="ba2c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们使用单个<a class="ae ld" href="https://github.com/hamzag95/voice-classification/blob/master/data_collection/AudioBook_DataProcessing.ipynb" rel="noopener ugc nofollow" target="_blank">脚本</a>处理LibriVox音频，该脚本将不同处理级别的数据放入不同的目录中，以便潜在的未来用户可以按照他们的意愿更改片段长度或转换类型。</p><p id="f17b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这一过程可以概括为以下几点:</p><ol class=""><li id="b14b" class="mt mu jb ka b kb kc kf kg kj nu kn nv kr nw kv nx mz na nb bi translated">为单个扬声器组合所有下载的章节</li><li id="860f" class="mt mu jb ka b kb nc kf nd kj ne kn nf kr ng kv nx mz na nb bi translated">将组合音频修剪到所需长度</li><li id="db7b" class="mt mu jb ka b kb nc kf nd kj ne kn nf kr ng kv nx mz na nb bi translated">使用ffmpeg将微调音频转换为16位16khz单声道WAV</li><li id="c302" class="mt mu jb ka b kb nc kf nd kj ne kn nf kr ng kv nx mz na nb bi translated">移除超过0.5秒的静默</li><li id="4291" class="mt mu jb ka b kb nc kf nd kj ne kn nf kr ng kv nx mz na nb bi translated">将WAV文件分割成5秒钟的片段</li><li id="1d5e" class="mt mu jb ka b kb nc kf nd kj ne kn nf kr ng kv nx mz na nb bi translated">将每个片段转换成声谱图</li></ol><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/0c9eddf67c2122c62beba794b8db0528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4BR80DOVO-0IXvU2."/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">LibriVox data processing stages</figcaption></figure><h1 id="3389" class="le lf jb bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">四。学问</h1><h2 id="8bdc" class="mh lf jb bd lg mi mj dn lk mk ml dp lo kj mm mn ls kn mo mp lw kr mq mr ma ms bi translated">我们的模型</h2><p id="c2f5" class="pw-post-body-paragraph jy jz jb ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">我们通过修改现有的<a class="ae ld" href="https://github.com/hamzag95/keras/blob/master/examples/cifar10_cnn.py" rel="noopener ugc nofollow" target="_blank"> Cifar-10架构</a>来创建CNN，并在来自57个不同扬声器的频谱图上对其进行训练。使用这个经过训练的神经网络，我们通过移除最后一个完全连接的层来提取特征，并将展平层的输出输入到SVM中，这一过程称为迁移学习。没有公开可用的预先训练好的声音分类模型，所以我们创建并训练自己的神经网络。</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/b0c32a517ea949c8c0066d16812ee0ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oOq2KYyw-_EKqng1."/></div></div></figure><h2 id="329e" class="mh lf jb bd lg mi mj dn lk mk ml dp lo kj mm mn ls kn mo mp lw kr mq mr ma ms bi translated">CNN架构</h2><p id="47fc" class="pw-post-body-paragraph jy jz jb ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">我们架构中的绿色层是卷积层，而蓝色层是最大池化层。对于所有卷积层，我们使用3×3内核。对于最大池，我们使用2x2的池大小。我们在每层之间使用relu激活函数，在最后一层使用softmax激活函数。我们的损失函数是分类交叉熵。</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/1bbb03d7d56ecb48d6fed635870ee3bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LMUlLnSmswtSys2k."/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Modified Cifar-10 Architecture</figcaption></figure><h2 id="444c" class="mh lf jb bd lg mi mj dn lk mk ml dp lo kj mm mn ls kn mo mp lw kr mq mr ma ms bi translated">CNN培训</h2><p id="6ce8" class="pw-post-body-paragraph jy jz jb ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">当对6个不同的人进行训练时，神经网络的准确率为97%。这6个人中的每一个都有大约一个小时的CNN训练音频。在我们对162个不同说话人的更大数据集进行数据采集和处理之后，由于AWS上训练和存储空间的时间限制，我们在57个不同说话人上训练了我们的神经网络。我们用45分钟的音频(大约2700秒)训练了57个扬声器中的每一个。一个时代后，我们的CNN是97%准确。CNN花了大约一个半小时来训练大约24000幅声谱图。</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oc"><img src="../Images/9726b0a1b1023cf02cee555f766a9864.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*imEYd10uzzI8JNU8."/></div></div></figure><h2 id="3036" class="mh lf jb bd lg mi mj dn lk mk ml dp lo kj mm mn ls kn mo mp lw kr mq mr ma ms bi translated">SVM与迁移学习</h2><p id="68d2" class="pw-post-body-paragraph jy jz jb ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">我们现在已经有了一个不错的神经网络来识别57个不同的人。我们切断了最后一层，这是一个密集层分类57人，并使用展平层馈入一个SVM。支持向量机应该在数据量较小(与神经网络相比)和维数较高的情况下表现良好。使用我们的CNN作为特征提取器，我们有大约400，000个维度的数据。我们使用径向基函数作为SVM的核。</p><p id="8e21" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用35秒的音频在3个不同的扬声器上进行训练，并在35秒上进行测试，结果准确率为95%。输入SVM，我们看到，通过对3个不同说话者中的每一个进行15秒的训练，并对每个说话者进行15秒的测试，我们的SVM得到了83%的准确率。我们看到，我们现在能够在15-20秒内学会某人的声音，而不是45分钟的音频。</p><h2 id="b5ae" class="mh lf jb bd lg mi mj dn lk mk ml dp lo kj mm mn ls kn mo mp lw kr mq mr ma ms bi translated">更多示例和结果</h2><p id="ad5a" class="pw-post-body-paragraph jy jz jb ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">我们所有的例子都将试图区分三种新的声音。</p><p id="520b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当我们第一次测试SVM时，我们在三个YouTubers上测试了它，每个训练集和测试集有7个样本。我们有95%的准确率。</p><p id="e807" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这些指数对应于特定的人。本例中的阵列是这样设置的:</p><p id="61bb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[ <a class="ae ld" href="https://www.youtube.com/user/christendominique" rel="noopener ugc nofollow" target="_blank">克里斯腾·多米尼克</a>，<a class="ae ld" href="https://www.youtube.com/user/tusharroy2525" rel="noopener ugc nofollow" target="_blank">图沙尔，</a> <a class="ae ld" href="https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A" rel="noopener ugc nofollow" target="_blank">斯里拉杰·拉瓦尔</a> ]</p><p id="5be7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">输出0是Christen(女性)，1是Tushar(男性)，2是Sriraj(男性)。</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/85b9cd62518a2aea5aaf7e0b5f8207d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uxp8pZoEZgYUuZmg."/></div></div></figure><p id="7a5f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们在这里看到，该模型从未错误分类Christen，但对于一个样本，将Sriraj分类为Tushar。对于未来的测试，我们试图将用于训练的样本数量减少到大约5个。人名在数组中出现的次数就是测试集的样本数。</p><p id="9ec2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了测试我们的程序，我们制作了一个界面，在这个界面上，我们记录发言者，并创建一个测试和训练集。我们使用这个程序来运行现场演示，并在真人身上进行测试，而不仅仅是收集数据。</p><p id="a4fd" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，我们的分类器是语言不可知的；它可以独立于语言识别你的声音。一个名字在数组中出现的顺序是分类器在这个人说话时预测的索引。看到的第一个数组是预测，第二个数组是真实的说话者。</p><p id="df0b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面是我们的模型的一些现场测试的结果。</p><p id="1443" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我们班做现场演示的时候，学习三个人的声音，两男一女，达到了63%的准确率。这是基于5个用于训练的样本和3个用于测试的样本。下面是我们做的课堂演示的一个例子。(0 →卡拉马尼斯，1 →迪马基斯，2 →莫尼卡)</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/697ef6e114ca650d40d861d8de5cd874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*QxbFOwOz3w0NKdXU."/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">In class live demo with our professors and teaching assistant with some mixed language</figcaption></figure><p id="d935" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">另一个例子包括两个男性和一个女性声音，所有声音都在英语和各自不同的外语(西班牙语、阿拉伯语和乌尔都语)之间切换。我们的模型有90%的准确性。</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oe"><img src="../Images/1388ee218a9296bec582bdb3455b7638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*ayjHtA9bCssyhFz0."/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Demo among friends mixing english and their native languages</figcaption></figure><p id="6503" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是另一个准确率为86%的例子。</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div class="gh gi of"><img src="../Images/a313ede2073c975326b1b233882cc7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/0*QCqg3O9pjyKAa70t."/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Demo among friends in purely english</figcaption></figure><p id="0f25" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">结果普遍是积极的！对不同性别的3人小组进行测试，通常会产生60-90%的准确率。然而，我们的模型确实有局限性。当在完全相同性别的群体中进行测试时，或者随着群体规模的增加，表现会下降。对同性群体的测试通常会产生40-60%的准确率。当群体规模超过6时，准确性接近随机猜测。</p><h1 id="7392" class="le lf jb bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak">五、结论</strong></h1><h2 id="313b" class="mh lf jb bd lg mi mj dn lk mk ml dp lo kj mm mn ls kn mo mp lw kr mq mr ma ms bi translated">摘要</h2><p id="2108" class="pw-post-body-paragraph jy jz jb ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">我们想创建一个模型，仅用几个句子的训练数据来识别说话者。我们选择通过使用现有的图像分类架构来实现这一点，使用频谱图来表示音频。这包括一个重要的数据收集组件，使我们创建了一个包含162个扬声器的数据集，包括分段音频文件和分段频谱图。我们选择使用take transfer learning方法，训练Cifar-10 CNN的衍生物，并提取特征以馈送给SVM来分类新的说话者。我们将我们的训练数据限制在每人20-35秒(4-7个样本)。这种方法为不同性别的三人组带来了惊人的准确率(60-90%)。性别一致的小组的结果不那么令人印象深刻，但始终比随机猜测好得多。</p><h2 id="3827" class="mh lf jb bd lg mi mj dn lk mk ml dp lo kj mm mn ls kn mo mp lw kr mq mr ma ms bi translated"><strong class="ak">投稿</strong></h2><p id="4bb5" class="pw-post-body-paragraph jy jz jb ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">按照我们在开始这个项目时设定的目标，我们的团队成功地对这个研究领域做出了原创性的贡献。在一个缺乏开源数据是研究项目常见障碍的领域，我们设法为独特的演讲者创建了一个非常大的音频下载链接集。同样，这个链接列表可以在<a class="ae ld" href="https://github.com/hamzag95/voice-classification/blob/master/data_collection/download_links.txt" rel="noopener ugc nofollow" target="_blank">这里</a>找到。此外，我们对音频数据使用图像识别结合SVM的迁移学习的方法还没有被深入研究。我们希望我们的架构和方法对未来的研究有用。</p><p id="2f8f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">注:链接到完整的数据集，包括音频和频谱图即将推出…</p><h2 id="eedd" class="mh lf jb bd lg mi mj dn lk mk ml dp lo kj mm mn ls kn mo mp lw kr mq mr ma ms bi translated"><strong class="ak">未来的变化/改进</strong></h2><p id="f927" class="pw-post-body-paragraph jy jz jb ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">这个项目的许多障碍包括时间和计算机资源，如存储和计算能力。以下是我们的团队或其他希望改进我们工作的人未来可能采取的步骤</p><p id="29e4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先是访问更多的存储空间，这样我们就可以用每人4小时的音频来训练我们的神经网络，并使用我们收集的所有162个扬声器。我们相信这将会成为一个更好的特征提取器，用于SVM。</p><p id="5669" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其次，在将特征输入SVM之前，进行一些特征选择。尽管具有非线性核的支持向量机能够抵抗过拟合，但是用如此少的样本获得如此多的特征可能会导致过拟合，这可以解释准确度分数的高变化。如果有更多的时间，我们会对神经网络的架构进行更多的实验，特别是在分类层之前添加另一个密集层。这将把SVM的输入从大约400，000个要素减少到我们在新的密集图层中设置的任意结点数。另一个可以探索的架构是基于VGG19的模型。</p><p id="a37f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第三，争取更多的发言人。我们能够从LibriVox中抓取6000多个独特的扬声器，尽管这需要时间来下载和预处理数据，以及惊人的存储量。我们将尝试刮约500-1000，使用约30-45分钟。来看看这是如何改进我们的特征提取器的。这需要很多时间来训练。供参考:57位发言人，45分钟。花了一个半小时来训练。</p></div></div>    
</body>
</html>