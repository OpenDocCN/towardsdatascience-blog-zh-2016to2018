<html>
<head>
<title>Seeing the random forest from the decision trees: An explanation of Random Forest</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从决策树看随机森林——对随机森林的一种解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/seeing-the-random-forest-from-the-decision-trees-an-intuitive-explanation-of-random-forest-beaa2d6a0d80?source=collection_archive---------1-----------------------#2017-04-16">https://towardsdatascience.com/seeing-the-random-forest-from-the-decision-trees-an-intuitive-explanation-of-random-forest-beaa2d6a0d80?source=collection_archive---------1-----------------------#2017-04-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/3112888003ae78d40bb073870e2c9ed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dLt8zvqEmnSi40dBxPl_KA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">So many bagged decision trees, so little processing resources.</figcaption></figure></div><div class="ab cl kc kd hu ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="ij ik il im in"><p id="566c" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">在过去的这个周末，我有点无聊，决定温习一下我的简历。作为一名在<a class="ae lh" href="http://insighthealthdata.com" rel="noopener ugc nofollow" target="_blank"> Insight </a>的研究员，我几乎完全是用 Python 编程的，但我实际上没有做过太多的 R 语言预测分析，除了非常普通的线性回归。我想要一个有点干净的数据源，这样我就可以在 r 中进行建模了。因此，一个好的干净数据源是好的 ole' <a class="ae lh" href="http://kaggle.com" rel="noopener ugc nofollow" target="_blank"> kaggle </a>。我决定研究一个<a class="ae lh" href="https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings" rel="noopener ugc nofollow" target="_blank">视频游戏销售数据集</a>。</p></div><div class="ab cl kc kd hu ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="ij ik il im in"><h1 id="af3a" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">决定，决定…</h1><p id="51c2" class="pw-post-body-paragraph kj kk iq kl b km mg ko kp kq mh ks kt ku mi kw kx ky mj la lb lc mk le lf lg ij bi translated">决策树，以及它们的表亲如袋装决策树、随机森林、梯度推进决策树等。，通常被称为<a class="ae lh" href="https://en.wikipedia.org/wiki/Ensemble_learning" rel="noopener ugc nofollow" target="_blank">系综方法。</a></p><p id="afc4" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">为了理解更复杂的集成方法，我认为理解这些方法中最常见的决策树和随机森林是很好的。让我们举一个最简单的例子:使用决策树的回归。对于一个给定的 n 维数据集，你可以生成一个有 n 个分支和 n 个叶子的决策树。决策树的目标是确定最大程度减少残差平方和的分支，并尽可能提供最具预测性的叶子。也许一个数字会有所帮助…</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/d5d5b7b3644378b341dc087cbf787a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*efE4_JWSPccA9h7y3sI-jA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Taken from the book <strong class="bd mq"><em class="mr">Introduction to Statistical Learning</em></strong></figcaption></figure><p id="57b1" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">上图显示了一个棒球相关的数据集，我们想要确定一个球员的日志工资。在左图中，如果一个球员的经验少于 4.5 年，他们预计可以赚 511 万美元。如果一个玩家有超过 4.5 年的经验，但少于 117.5 次点击，他们预计会赚 6 千美元(同样基于日志)。在右边的数据中，预测值分别代表子空间 R1、R2 和 R3。</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/4c348ccc4d444215d62255f0604cf2f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*dbwwOor1HvSxsKJshcQYPw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">For those who like the see the math: decision trees essentially grow branches that reduce the sum of the errors for Rj sub-spaces in the data.</figcaption></figure><p id="2766" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">上面的例子使用了连续的数据，但是我们可以将其扩展到分类。在分类设置中，我们本质上是在增加分支以减少分类错误，尽管这并不那么简单。在分类设置中，我们采用一种类似熵的度量，并尝试减少每个分支的熵值，以提供最佳的分支分裂。<a class="ae lh" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" rel="noopener ugc nofollow" target="_blank">基尼指数</a>是一个常用的指标。</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/f26298d8143e80bf12625c94b8202b38.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*qXaZxT6wcbTB2mfB7fnluQ.png"/></div></figure><p id="281b" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">p-hat mk 表示来自第 k 类的第 m 个区域中的观测值的比例。从本质上说，基尼指数是一种方差的度量。方差越大，错误分类就越多。因此，基尼系数越低，分类越好。</p><h1 id="e169" class="li lj iq bd lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb my md me mf bi translated">将这些预测打包…</h1><p id="b2a3" class="pw-post-body-paragraph kj kk iq kl b km mg ko kp kq mh ks kt ku mi kw kx ky mj la lb lc mk le lf lg ij bi translated">决策树通常被称为“贪婪的”。这只是算法如何试图确定减少误差的最佳方式的函数。不幸的是，这导致了模型过度拟合和模型过度一般化。</p><p id="9d5d" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">一种用于应对这种情况的方法被称为<a class="ae lh" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" rel="noopener ugc nofollow" target="_blank">自举聚合或简称为‘bagging’</a>。如果你理解统计学中的 bootstrapping 的概念(在估计未知人口的方差和误差方面)，当涉及决策树时，bagging 是类似的。</p><p id="fac7" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">在打包过程中，我们决定要从数据集中抽取多少个重复的引导，将它们放入同一个决策树中，然后将它们聚合在一起。这给了我们一个更健壮的结果，并且不容易过度拟合。</p><p id="25c8" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">此外，通常每棵装袋的树会留下三分之一的样本。然后，我们可以将装袋的树与该样本进行拟合，并获得<a class="ae lh" href="https://en.wikipedia.org/wiki/Out-of-bag_error" rel="noopener ugc nofollow" target="_blank">袋外误差率</a>。这实质上是交叉验证的决策树版本，尽管您可以在超出袋外错误率的基础上执行交叉验证！</p><h1 id="2e4c" class="li lj iq bd lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb my md me mf bi translated">进入随机森林</h1><p id="8005" class="pw-post-body-paragraph kj kk iq kl b km mg ko kp kq mh ks kt ku mi kw kx ky mj la lb lc mk le lf lg ij bi translated">现在我们对决策树和装袋有了大致的了解，随机森林的概念就相对简单了。香草随机森林是一个袋装决策树，其中一个附加算法在每次分裂时随机抽取<em class="mz"> m </em>个预测值。这有助于对随机森林中使用的树进行去相关处理，并有助于自动应对<a class="ae lh" href="https://en.wikipedia.org/wiki/Multicollinearity" rel="noopener ugc nofollow" target="_blank">多重共线性。</a></p><p id="8ee7" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">在分类中，所有的树都聚集在一起。从这种聚合中，该模型实质上采取了投票/投票来将数据分配给一个类别。</p><p id="cf14" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">对于给定的观察，我们可以通过观察每个袋装树为该观察输出什么类来预测类。然后我们查看所有的树，看看这个观察被预测了多少次。如果观察结果是根据大多数袋装树预测的，则为该观察结果指定一个类别。</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/1bb6f062ef229ca099a9433e88b71339.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*rXd6aK4-_uuUaXCVZ_FX0g.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">A random forest takes a random subset of features from the data, and creates n random trees from each subset. Trees are aggregated together at end.</figcaption></figure><h1 id="d255" class="li lj iq bd lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb my md me mf bi translated">一个在 R 中应用随机森林的危险的简短例子，使用视频游戏 Sales kaggle 数据集</h1></div><div class="ab cl kc kd hu ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="ij ik il im in"><p id="23ed" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated"><a class="ae lh" href="https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings" rel="noopener ugc nofollow" target="_blank">可在此处找到数据集的概述。</a></p><p id="9e58" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">我所有乱七八糟的代码都可以在我的 github 上找到。</p><p id="0f3f" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated"><strong class="kl ir">本例的目标是看看销售数字和游戏所在的主机是否可以预测其类型(例如，体育、动作、RPG、策略等)。).</strong></p><p id="82b4" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">在这个例子中，我使用了<code class="fe nb nc nd ne b">caret</code>和<code class="fe nb nc nd ne b">ggplot2</code>。我使用包<code class="fe nb nc nd ne b">dummies</code>为分类预测器生成虚拟变量。</p><p id="71f3" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">我想得到一些使用<code class="fe nb nc nd ne b">caret</code>的练习，它本质上是 R 版的<code class="fe nb nc nd ne b">scikit-learn</code>。但是首先，和任何数据集一样，有必要对其进行一点探索。我的一般方法是首先在数据中寻找古怪之处，探索潜在的相关性，然后再深入一点，看看数据中是否有任何其他值得注意的趋势。理想情况下，您会希望在建模之前从各个方面检查数据。为了简洁起见，我跳过了一些数据探索，直接进行了一些建模。</p><p id="1a68" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">首先，我检查了数据中缺失的值。有一大堆的<code class="fe nb nc nd ne b">NaNs</code>，所以我继续使用<code class="fe nb nc nd ne b">DMwR</code>包做了 K-最近邻插补。</p><p id="8544" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">接下来，我想检查一下销售数据，看看是否有异常值。曾经有。而且分布是高度倾斜的。</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/7e6b94ff9bb249408cf8c8815030ee26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*N9-BmYxgLL-yTYYrf_hxDw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Most sales were far less than $20 million total.</figcaption></figure><p id="3375" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">我继续使用对数转换将它们标准化。</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/9d2f584bc68fd1e76822ab87a014858a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*b6WM19dxXQeVFm4oKguYsg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Normalized-ish, but still sorta skewed.</figcaption></figure><p id="3ea2" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">在这里，我为每个游戏运行的不同主机生成虚拟变量，然后检查相关性。毫不奇怪，全球销售额与所有其他销售额相关联。评论家分数和计数不是。这里没有显示控制台的相关性。考虑到控制台虚拟数据的稀疏性，这里没有什么值得注意的。如果多重共线性是一个大问题，可以简单地删除全局销售变量，而不是保留所有其他销售变量。</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/0b882b6a752ba5ee601d9a669d85ee9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*_10IK6R4NU34BuZ_pHNfIA.png"/></div></figure><p id="5df1" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">在<code class="fe nb nc nd ne b">caret</code>中，我做了 80%-20%的训练测试分割，作为进行建模的惯例。我将所有的流派重新标为数字，它们如下:</p><ol class=""><li id="3df7" class="ng nh iq kl b km kn kq kr ku ni ky nj lc nk lg nl nm nn no bi translated">运动</li><li id="cd90" class="ng nh iq kl b km np kq nq ku nr ky ns lc nt lg nl nm nn no bi translated">平台游戏</li><li id="7f0e" class="ng nh iq kl b km np kq nq ku nr ky ns lc nt lg nl nm nn no bi translated">竞赛</li><li id="a3c1" class="ng nh iq kl b km np kq nq ku nr ky ns lc nt lg nl nm nn no bi translated">角色扮演游戏</li><li id="0cee" class="ng nh iq kl b km np kq nq ku nr ky ns lc nt lg nl nm nn no bi translated">难题</li><li id="049c" class="ng nh iq kl b km np kq nq ku nr ky ns lc nt lg nl nm nn no bi translated">多方面的</li><li id="431b" class="ng nh iq kl b km np kq nq ku nr ky ns lc nt lg nl nm nn no bi translated">射手</li><li id="da3a" class="ng nh iq kl b km np kq nq ku nr ky ns lc nt lg nl nm nn no bi translated">模拟</li><li id="db18" class="ng nh iq kl b km np kq nq ku nr ky ns lc nt lg nl nm nn no bi translated">行动</li><li id="1f6d" class="ng nh iq kl b km np kq nq ku nr ky ns lc nt lg nl nm nn no bi translated">战斗的</li><li id="8ac9" class="ng nh iq kl b km np kq nq ku nr ky ns lc nt lg nl nm nn no bi translated">冒险</li><li id="f454" class="ng nh iq kl b km np kq nq ku nr ky ns lc nt lg nl nm nn no bi translated">战略</li></ol><p id="518b" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">我做了一些网格搜索在每个树分裂可用的功能数量。回想一下，当随机森林为树中的每个节点创建一个分割时，它不会获取所有可用的功能。这是模型中可操作的<em class="mz">超参数</em>。</p><pre class="mm mn mo mp gt nu ne nv nw aw nx bi"><span id="e754" class="ny lj iq ne b gy nz oa l ob oc">mtry &lt;- sqrt(ncol(vg))<br/>tunegrid &lt;- expand.grid(.mtry = mtry)</span></pre><p id="9250" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">在上面的代码片段中，我将列数的平方根作为可用的初始数字特性。在此基础上进行网格搜索，使得<code class="fe nb nc nd ne b">caret</code>将迭代初始起始变量，然后在下一次拟合迭代中进行另一次 sqrt(ncol(vg))附加特征，然后再次评估模型。</p><pre class="mm mn mo mp gt nu ne nv nw aw nx bi"><span id="bda0" class="ny lj iq ne b gy nz oa l ob oc">metric &lt;- 'Accuracy'<br/>control &lt;- trainControl(method = 'repeatedcv', number = 10, repeats = 2, search = 'random', savePredictions = TRUE)</span></pre><p id="ec92" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">接下来，我将我的度量设置为准确性，因为这是一个分类过程。我做交叉验证来评估我的训练数据是否不可靠。典型的折叠次数为 5-10 次(表示为<code class="fe nb nc nd ne b">number</code>参数)。我做了一个随机搜索，因为它更快，计算量更小。</p><p id="3f4f" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">使用 caret，我训练了两个模型。一个有 15 棵袋装树。另一个有 500 袋树。500 树模型需要一些时间来运行(可能大约 30 分钟？).人们可以很容易地将袋装树木的数量纳入网格搜索中。为了简洁(和时间)，我只比较了两个模型。</p><p id="6773" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">注:我允许模型使用 Box Cox 来确定如何适当地规范化数据(它对数据进行了对数转换)。</p><pre class="mm mn mo mp gt nu ne nv nw aw nx bi"><span id="f112" class="ny lj iq ne b gy nz oa l ob oc">model_train1 &lt;- train(Genre ~ ., data = vg_train, method = 'rf', trControl = control, tunegrid = tunegrid, metric = metric, ntree = 15, preProcess = c('BoxCox'))</span><span id="deee" class="ny lj iq ne b gy od oa l ob oc">model_train2 &lt;- train(Genre ~ ., data = vg_train, method = 'rf', trControl = control, tunegrid = tunegrid, metric = metric, ntree = 500, preProcess = c('BoxCox'))</span></pre><p id="c76c" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">我的交叉验证结果显示，500 树模型做得好一点点…但只是一点点。给定交叉验证结果，每个分割 21 个特征似乎是合适的。</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/30c4cb746f6c922c5b5294eb399c83d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*FpqjlixKegJ4c-AktWweDQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Model 1 with 15 bagged trees.</figcaption></figure><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/aa7f529f8b28c05009261152fdc0a7e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*RkCHWErmuqQTnllVEOG5zA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Model 2 with 500 bagged trees</figcaption></figure><p id="e631" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">我的准确度是<em class="mz"> </em> <strong class="kl ir"> <em class="mz">然而全然可怕的</em> </strong>。我在 Model 2 <strong class="kl ir">的整体准确率只有 34.4%。</strong></p><p id="c5b4" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">随机森林允许我们查看<em class="mz">特征重要性</em>，这是一个特征的基尼指数在每次分裂时降低了多少。某个特征的基尼指数下降得越多，它就越重要。下图从 0 到 100 分对这些特性进行评分，100 分是最重要的。</p><p id="eb3f" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">看来<strong class="kl ir">用户数</strong>和<strong class="kl ir">评论家数</strong>尤为重要。然而，鉴于模型拟合度如此之差，我不确定解释这些变量有多大用处。我已经包含了变量重要性代码的一个片段，以防您想要复制它。</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/1e4eb56073f250e16c3b8e870955a9e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3fA9ZovuqNhTmtz_OuB57g.png"/></div></div></figure><pre class="mm mn mo mp gt nu ne nv nw aw nx bi"><span id="e25c" class="ny lj iq ne b gy nz oa l ob oc"># Save the variable importance values from our model object generated from caret.<br/>x&lt;-varImp(model_train2, scale = TRUE)</span><span id="7569" class="ny lj iq ne b gy od oa l ob oc"># Get the row names of the variable importance data<br/>rownames(x$importance)</span><span id="3b2b" class="ny lj iq ne b gy od oa l ob oc"># Convert the variable importance data into a dataframe<br/>importance &lt;- data.frame(rownames(x$importance), x$importance$Overall)</span><span id="92bc" class="ny lj iq ne b gy od oa l ob oc"># Relabel the data<br/>names(importance)&lt;-c('Platform', 'Importance')</span><span id="86e6" class="ny lj iq ne b gy od oa l ob oc"># Order the data from greatest importance to least important<br/>importance &lt;- transform(importance, Platform = reorder(Platform, Importance))</span><span id="e83e" class="ny lj iq ne b gy od oa l ob oc"># Plot the data with ggplot.<br/>ggplot(data=importance, aes(x=Platform, y=Importance)) +<br/>  geom_bar(stat = 'identity',colour = "blue", fill = "white") + coord_flip())</span></pre></div><div class="ab cl kc kd hu ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="ij ik il im in"><p id="0ef0" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">我们可以查看混淆矩阵，看看有多少准确的分类和错误的分类。对角线表示正确的分类百分比。斜线表示模型错误分类一个流派的次数百分比。</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/d2439d95045091725cd36ee73f843e96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*kv1KPxmYVVr1ZkNz2sk9og.png"/></div></figure><p id="cbaf" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">太可怕了。射击游戏有 68%的机会被正确分类……但是也有很大一部分时间被错误分类为策略游戏。</p><h1 id="fa5a" class="li lj iq bd lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb my md me mf bi translated">外卖食品</h1><p id="4fef" class="pw-post-body-paragraph kj kk iq kl b km mg ko kp kq mh ks kt ku mi kw kx ky mj la lb lc mk le lf lg ij bi translated">为什么我们的模型做得这么差？有几个原因。该模型往往不符合数据。这可能意味着 random forest 不够复杂，不足以捕捉数据中的趋势，我们可能必须使用另一个模型来使用更复杂的方法。然而，更有可能的是，这些特征根本不能预测视频游戏的类型。</p><p id="83bb" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">如果我们的功能有点糟糕，我们可以做两件事之一:我们可以为给定的数据集设计一些额外的功能。例如，我们可能能够创建一个变量，表示数据中每种游戏类型的平均评论家分数，作为一个预测器(但这可能是无趣的)。</p><p id="142d" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">我们可能要做的是从视频游戏存储库中获取一些额外的信息，该存储库中可能有每种类型的视频游戏的额外历史销售数据。第二个简单的方法是简单地计算每个流派的总销售额，然后将其应用于整个数据集。这么多选择！</p><p id="6588" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">或者答案更简单。可能是数据在类别方面不平衡。如果是这种情况(如果你进一步检查数据的话，确实如此)，你可能想要修剪或合并流派来纠正这种情况。</p><h1 id="7291" class="li lj iq bd lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb my md me mf bi translated">结论</h1><p id="6062" class="pw-post-body-paragraph kj kk iq kl b km mg ko kp kq mh ks kt ku mi kw kx ky mj la lb lc mk le lf lg ij bi translated">随机森林是机器学习中常用的模型，通常被称为经常使用的现成模型。在许多情况下，它的性能优于它的许多等效参数，并且启动时计算量较小。当然，对于任何模型，请确保您知道为什么要选择模型，例如随机森林(提示，可能您不知道您的数据的分布，可能您的数据非常高维，可能您有很多共线性，可能您想要一个易于解释的模型)。不要像我在这里一样漫不经心地选择一个模型。:-)</p><h1 id="f11a" class="li lj iq bd lk ll mu ln lo lp mv lr ls lt mw lv lw lx mx lz ma mb my md me mf bi translated">资源</h1><p id="b885" class="pw-post-body-paragraph kj kk iq kl b km mg ko kp kq mh ks kt ku mi kw kx ky mj la lb lc mk le lf lg ij bi translated">我并不真正深入研究随机森林的机制。如果你想深入了解，我强烈推荐两本书:</p><ul class=""><li id="8204" class="ng nh iq kl b km kn kq kr ku ni ky nj lc nk lg of nm nn no bi translated"><a class="ae lh" href="http://www-bcf.usc.edu/~gareth/ISL/" rel="noopener ugc nofollow" target="_blank">统计学习简介:R 中的应用</a></li><li id="e906" class="ng nh iq kl b km np kq nq ku nr ky ns lc nt lg of nm nn no bi translated"><a class="ae lh" href="http://statweb.stanford.edu/~tibs/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank">统计学习的要素</a></li></ul><p id="c604" class="pw-post-body-paragraph kj kk iq kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg ij bi translated">后者被一些人认为是机器学习的圣经！</p></div></div>    
</body>
</html>