<html>
<head>
<title>The Mathematics Behind Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析背后的数学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643?source=collection_archive---------0-----------------------#2018-12-20">https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643?source=collection_archive---------0-----------------------#2018-12-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="20dd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从原始数据到主成分</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1bde12f87efa4ac3fdbd928858760187.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iravOG2sZBxjZFGX"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@mangofantasy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tim Johnson</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="d762" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="4bdb" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">主成分分析(PCA)的中心思想是降低由大量相关变量组成的数据集的维数，同时尽可能多地保留数据集中存在的变异。这是通过转换成一组新的变量来实现的，即<em class="mk">主成分</em><em class="mk">【PCs】</em>，它们是不相关的，并且是有序的，因此前几个保留了所有原始变量中存在的大部分变化。</p><h1 id="11f4" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">PCA 背后的数学</h1><p id="b9a6" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">PCA 可以被认为是一个无监督的学习问题。从原始数据集获取主成分的整个过程可以简化为六个部分:</p><ul class=""><li id="443a" class="ml mm iq lq b lr mn lu mo lx mp mb mq mf mr mj ms mt mu mv bi translated">取由<em class="mk"> d+1 维</em>组成的整个数据集，忽略标签，这样我们的新数据集就变成了<em class="mk"> d 维。</em></li><li id="a513" class="ml mm iq lq b lr mw lu mx lx my mb mz mf na mj ms mt mu mv bi translated">计算整个数据集的每个维度的平均值。</li><li id="1962" class="ml mm iq lq b lr mw lu mx lx my mb mz mf na mj ms mt mu mv bi translated">计算整个数据集的<em class="mk">协方差矩阵</em></li><li id="ca6e" class="ml mm iq lq b lr mw lu mx lx my mb mz mf na mj ms mt mu mv bi translated">计算<em class="mk">特征向量</em>和相应的<em class="mk">特征值</em>。</li><li id="742e" class="ml mm iq lq b lr mw lu mx lx my mb mz mf na mj ms mt mu mv bi translated">将特征向量按特征值递减排序，选择 k 个特征值最大的特征向量，形成一个<em class="mk"> d × k 维</em>矩阵<strong class="lq ir"> W. </strong></li><li id="2ffd" class="ml mm iq lq b lr mw lu mx lx my mb mz mf na mj ms mt mu mv bi translated">使用这个<em class="mk"> d × k 特征向量矩阵</em>将样本变换到新的子空间上。</li></ul><p id="5c5d" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">所以，让我们一个一个地展示这背后的数学原理。</p><ol class=""><li id="9854" class="ml mm iq lq b lr mn lu mo lx mp mb mq mf mr mj ne mt mu mv bi translated"><strong class="lq ir">取由<em class="mk"> d+1 维</em>组成的整个数据集，忽略标签，这样我们的新数据集就变成了<em class="mk"> d 维。</em> </strong></li></ol><p id="5f0e" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">假设我们有一个数据集，它是 d+1 维的。其中，在现代机器学习范例中，d 可以被认为是<em class="mk"> X_train </em>，1 可以被认为是 y_train <em class="mk">(标签)</em>。因此，<em class="mk"> X_train + y_train </em>组成了我们完整的训练数据集。</p><p id="8a90" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">因此，在我们去掉标签后，我们剩下的是<em class="mk"> d 维</em>数据集，这将是我们用来寻找主成分的数据集。此外，让我们假设在忽略标签(即 d = 3)后，我们剩下一个三维数据集。</p><p id="9306" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">我们将假设样本来自两个不同的类，其中数据集的一半样本被标记为类 1，另一半被标记为类 2。</p><p id="d0c1" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">让我们的数据矩阵<strong class="lq ir"> X </strong>是三个学生的分数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/7309b99b81e0aef9399e06afda957ba0.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*1HD7YIaVhfUjQ2ARKi7gNA.png"/></div></figure><p id="b6c1" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated"><strong class="lq ir"> 2。计算整个数据集每个维度的平均值。</strong></p><p id="ec27" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">上表中的数据可以用矩阵<strong class="lq ir"> A，</strong>来表示，其中矩阵中的每一列显示测试分数，每一行显示学生的分数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/8f9b7eb59d047acc7f2408981cf2dbf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*a0-0h6YJsVtH9lG9A1_-VQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Matrix A</figcaption></figure><p id="7217" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">因此，矩阵<strong class="lq ir"> A </strong>的平均值为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/4db5256610bcebe799b8d0bb877df898.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/format:webp/1*r1zlnStJxBq8buNZLhyT7A.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Mean of Matrix A</figcaption></figure><p id="b1e7" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated"><strong class="lq ir"> 3。计算整个数据集的<em class="mk">协方差矩阵</em>(有时也称为方差-协方差矩阵)</strong></p><p id="1fad" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">因此，我们可以使用以下公式计算两个变量<strong class="lq ir"> X </strong>和<strong class="lq ir"> Y </strong>的协方差:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/4698c958be48cd9a029c4546e8194465.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*xNSxC6LtyrOwFAe8dfTWgA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="https://getcalc.com/statistics-covariance-calculator.htm" rel="noopener ugc nofollow" target="_blank">https://getcalc.com/statistics-covariance-calculator.htm</a></figcaption></figure><p id="ce85" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">使用上述公式，我们可以找到<strong class="lq ir"> A. </strong>的协方差矩阵。同样，结果将是 d ×d 维的<em class="mk">方阵。</em></p><p id="6c4b" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">让我们像这样重写我们的原始矩阵</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e632a9284568aed38ee8decd21f55769.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*LB9qyXaROHAKqlaqk_LWdQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Matrix A</figcaption></figure><p id="f7df" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">它的 c <em class="mk">卵巢矩阵</em>会是</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/8c7435d65e181cca803f239058cdfc35.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*E0ImSfZ3Rea3Y-tWnX1cEQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Covariance Matrix of A</figcaption></figure><p id="df6d" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">这里值得注意的几点是:</p><ul class=""><li id="354c" class="ml mm iq lq b lr mn lu mo lx mp mb mq mf mr mj ms mt mu mv bi translated">如对角线上的蓝色<em class="mk">所示，我们可以看到每次测试分数的变化。艺考方差最大(720)；还有英语考试，最小的(360)。所以我们可以说艺考成绩比英语考试成绩有更多的可变性。</em></li><li id="36d8" class="ml mm iq lq b lr mw lu mx lx my mb mz mf na mj ms mt mu mv bi translated">协方差在矩阵<strong class="lq ir"> A </strong>的非对角线元素中显示为黑色</li></ul><p id="d84f" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated"><strong class="lq ir"> a) </strong>数学与英语的协方差为正(360)，数学与美术的协方差为正(180)。这意味着分数趋向于以积极的方式协变。随着数学分数的上升，艺术和英语的分数也趋于上升；反之亦然。</p><p id="0b62" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">T21 然而，英语和艺术之间的协方差是零。这意味着英语和艺术成绩之间没有可预测的关系。</p><p id="930d" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated"><strong class="lq ir"> 4。计算特征向量和相应的特征值</strong></p><blockquote class="nl nm nn"><p id="b14c" class="lo lp mk lq b lr mn jr lt lu mo ju lw no nb lz ma np nc md me nq nd mh mi mj ij bi translated">直观上，特征向量是一个向量，当对其应用线性变换时，其方向保持不变。</p></blockquote><p id="3126" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">现在，我们可以很容易地从上面的协方差矩阵计算特征值和特征向量。</p><blockquote class="nl nm nn"><p id="ce3b" class="lo lp mk lq b lr mn jr lt lu mo ju lw no nb lz ma np nc md me nq nd mh mi mj ij bi translated">设<strong class="lq ir"> <em class="iq"> A </em> </strong>为方阵，<strong class="lq ir"> ν </strong>为向量，<strong class="lq ir"> λ </strong>为标量，满足<strong class="lq ir"><em class="iq">A</em></strong>ν<strong class="lq ir">=λ</strong>ν，则<strong class="lq ir"> λ </strong>称为与特征向量<strong class="lq ir"/><strong class="lq ir">ν<em class="iq">A</em></strong>关联的特征值。</p></blockquote><p id="d526" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated"><strong class="lq ir"> <em class="mk"> A </em> </strong>的特征值是特征方程的根</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/967c9b48bc5fdeb40bfcefdf3f39551f.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*4NSJKK38x5Db3DlB2TVQcg.png"/></div></figure><p id="a6b0" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">计算<em class="mk"> det(A-λI) </em>首先，<em class="mk"> I </em>是一个单位矩阵:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/9caf159496dbe1fc7e42ab7e6377a51c.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*DogIPZUWEUpvdT3YZSVwCA.png"/></div></figure><p id="ffbc" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">先简化矩阵，我们可以以后再计算行列式，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/a02bc46dc2eb9f5ce6e64f950e3030c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*GrDal75rL0YcMdCNIlijuQ.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/2961d5f73df334597e4a68ac1800bfc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*4L8Ip319TwuAdWoLwofxNA.png"/></div></figure><p id="0ca7" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">既然我们已经有了简化的矩阵，我们就可以求出它的行列式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/d2386f47c8af5addf0a7f34c75eb4560.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*BzktUJKs11-rFIH6_GAI4w.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/c445ce40f6b3452f16cc0bf913f97ae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*pclbXVQlph0muCKfuMg4kA.png"/></div></figure><p id="2b88" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">我们现在有了方程，我们需要求解<em class="mk"> λ，</em>从而得到矩阵的<em class="mk">特征值。因此，将上面的等式等于零:</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/79094ffad6cf038ab93c2e6af5e4ee02.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*Gps62pfonm5ZeC5GwD2X6g.png"/></div></figure><p id="5107" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">求解此方程得到<strong class="lq ir"> <em class="mk"> λ、</em> </strong>的值后，我们得到以下值</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/4f8b13232db108605c2cdc39b594a02a.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*6KgzE41U5GTPcFG187zt0g.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Eigenvalues</figcaption></figure><p id="c052" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">现在，我们可以计算上述特征值对应的特征向量。我不会在这里展示如何计算特征向量，访问这个<a class="ae kv" href="https://study.com/academy/lesson/eigenvalues-eigenvectors-definition-equation-examples.html" rel="noopener ugc nofollow" target="_blank">链接</a>来了解如何计算特征向量。</p><p id="6e81" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">因此，在求解了<em class="mk">特征向量</em>之后，我们将得到相应的<em class="mk">特征向量</em>的如下解</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c659436eb567b37b54c64ed755e0985e.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*cfDBspXxFBGJ3yIhm5tXGA.png"/></div></figure><p id="952a" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated"><strong class="lq ir"> 5。将特征向量按特征值递减排序，选择 k 个特征值最大的特征向量，形成一个<em class="mk"> d × k 维</em>矩阵 W. </strong></p><p id="910c" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">我们从降低我们的特征空间的维度的目标开始，即，通过 PCA 将特征空间投影到更小的子空间上，其中特征向量将形成这个新的特征子空间的轴。然而，特征向量仅定义新轴的方向，因为它们都具有相同的单位长度 1。</p><p id="5ddd" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">因此，为了决定对于低维子空间，我们要去掉哪个(些)特征向量，我们必须看一下特征向量的相应特征值。粗略地说，具有最低特征值的特征向量承载关于数据分布的最少信息，并且那些是我们想要丢弃的。</p><p id="0da2" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">常见的方法是将特征向量从最高到最低的对应特征值进行排序，选择顶部的<em class="mk"> k </em>个特征向量。</p><p id="5847" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">因此，按降序排列特征值后，我们有</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/314f70f3f3ef7dceb3af4be30d826f77.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*ZJOOlx0T7JtqIiyD0cgVgw.png"/></div></figure><p id="ff9c" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">对于我们的简单示例，我们将 3 维特征空间缩减为 2 维特征子空间，我们将两个具有最高特征值的特征向量进行组合，以构建我们的<em class="mk"> d×k </em>维特征向量矩阵<strong class="lq ir"> W. </strong></p><p id="22ef" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">所以，<em class="mk">特征向量</em>对应的两个最大特征值是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/8e93061dda6b9b802a1a194d372f6bc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*LdwD0hzCfPuvrbpuZMcB4A.png"/></div></figure><p id="4dfb" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated"><strong class="lq ir"> 6。将样本转换到新的子空间上</strong></p><p id="9c4b" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">在最后一步中，我们使用刚刚计算的 3x2 维矩阵<strong class="lq ir"> <em class="mk"> W </em> </strong>通过等式<strong class="lq ir"><em class="mk">y = W′×x</em></strong>将样本变换到新的子空间上，其中<strong class="lq ir"><em class="mk">W′</em></strong>是矩阵<strong class="lq ir"><em class="mk"/></strong>的<em class="mk">转置</em></p><p id="e6b0" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated"><em class="mk">最后，我们计算了两个主成分，并将数据点投影到新的子空间上。</em></p><p id="5ccd" class="pw-post-body-paragraph lo lp iq lq b lr mn jr lt lu mo ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated"><strong class="lq ir">学分和来源:</strong></p><ol class=""><li id="e901" class="ml mm iq lq b lr mn lu mo lx mp mb mq mf mr mj ne mt mu mv bi translated"><a class="ae kv" href="http://sebastianraschka.com/Articles/2014_pca_step_by_step.html" rel="noopener ugc nofollow" target="_blank">塞巴斯蒂安·拉什卡博客</a></li><li id="9d43" class="ml mm iq lq b lr mw lu mx lx my mb mz mf na mj ne mt mu mv bi translated"><a class="ae kv" href="https://stattrek.com/matrix-algebra/covariance-matrix.aspx" rel="noopener ugc nofollow" target="_blank"> Stattrek 矩阵代数</a></li></ol></div></div>    
</body>
</html>