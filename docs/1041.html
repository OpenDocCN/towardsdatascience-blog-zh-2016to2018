<html>
<head>
<title>Neural Networks: a Mixture of Experts with Attention</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络:专家与注意力的混合体</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-a-mixture-of-experts-with-attention-30e196657065?source=collection_archive---------2-----------------------#2017-07-23">https://towardsdatascience.com/neural-networks-a-mixture-of-experts-with-attention-30e196657065?source=collection_archive---------2-----------------------#2017-07-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="6380" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个<em class="kl">混合专家</em> (MoE)是一种特殊类型的神经网络:神经元连接成许多小簇，每个簇只在特殊情况下活跃。网络的较低层提取特征，并召集专家来评估这些特征——对于每种情况，只召集一些专家。混合的专家有明显的优势:他们可以用<em class="kl">更大的专业化</em>来应对特定的环境，允许网络展示<em class="kl">更多样的行为</em>；专家可以接收混合刺激，整合来自不同传感器的数据。而当网络在运行时，<strong class="jp ir">只有少数专家在活动</strong>——即使是一个庞大的网络也只需要少量的处理能力。随着神经网络变得越来越复杂，集成了许多数据流，并提供了更多样的响应，专家模型的混合将占主导地位。因此，它有助于理解专家的混合体是如何进化的。</p><p id="b1cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">基于特征的注意力</strong></p><p id="143f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">专家的混合已经被用于翻译任务。每个小的专家群学习处理一个单独的词类或特殊的语法规则。然而，专家翻译的混合体目前没有使用注意力模型。</p><p id="ce88" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意力只是一个过滤器，一次只允许部分输入进入网络。通过不断转移注意力，网络可以处理任何规模的输入。一个图像识别网络可以接收许多不同大小的图像，并努力一幅一幅地解析这些图像。一个翻译网络可以在句子之间跳跃，形成被许多行分隔的单词之间的关系。以这种方式移动输入的专家组合将能够提取单词之间更丰富的关系，并且能够更好地准确翻译。当某些特征被识别时，每个专家被调用；这些特性可能出现在输入的不同区域，需要对输入有广泛的理解。专家会知道<em class="kl">哪里需要注意</em>。</p><p id="4a3d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">注意力记忆</strong></p><p id="387c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">专家的混合也是更复杂记忆形式的理想模型。解析翻译通常需要网络关注文本中的几个不同区域。多处出现的语句必须<em class="kl">组合成一个连贯的整体</em>，这样翻译才有意义。一群专家必须将注意力集中在一个领域，同时记住来自另一个领域的信息。这是通过<em class="kl">将专家集群连接到网络的过去状态</em>来实现的，类似于LSTM的连接。</p><p id="5ac2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">LSTMs wire <strong class="jp ir">每个神经元</strong>到<strong class="jp ir">都有自己的过去</strong>、<em class="kl">而不考虑其邻居的过去状态</em>。然而，专家的混合体会被连接到触发专家的特征检测器<em class="kl">的过去状态。这是一种“更高级”的记忆。一个特性可能出现，而<strong class="jp ir">不会</strong>触发一个专家集群。然而，当<em class="kl">与相关输入</em>结合时，该特征的记忆可能会在稍后</em>触发其专家<em class="kl">！专家将对当前输入和过去特征的组合做出响应。这种行为目前在LSTM或其他循环网络中是不可能的。而且，这一切都很重要。</em></p><p id="ee12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">评审流程</strong></p><p id="1122" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果一个专家的混合体有一个注意力系统，那么它可以在文本中跳来跳去，<em class="kl">寻找信息，直到找到触发专家的东西</em>。这类似于我们复习所读内容的方式；如果我们感到困惑，我们会跳回到不同的地方，<strong class="jp ir">直到</strong>我们找到澄清文本的信息。一只LSTM犬无法四处跳跃，直到它变得清晰。如果注意力系统与特征层次的记忆相连，一群专家将审查它的输入<em class="kl">，直到它确定无疑</em>。它四处寻找支持结论的信息，而不是脱口而出答案。</p><p id="cd1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个审查过程可以被捕获和可视化。当我们看到<strong class="jp ir">什么</strong>信息触发了一个专家，我们知道网络已经基于那个信息做出了决定。而且，因为每个专家处理一个非常具体的子任务，我们知道每个专家正在解决什么样的问题。<em class="kl">这是‘窥视内部’神经网络黑盒的最佳方式</em>。“我们得到了这个答案，<strong class="jp ir">因为</strong>这个专家在看到<strong class="jp ir">这个信息时开火了</strong>……”这是我们可能接近的告诉我们它在想什么的网络。</p><p id="75ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">成长</strong></p><p id="a17e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">专家的组合也是解决复杂多变的问题的理想选择。当必须学习新信息时，大多数网络需要<em class="kl">完全重新训练</em>，并且它们<em class="kl">丢失了它们在</em>之前学习的课程。混合的专家可以保留其旧知识，并简单地<strong class="jp ir">插入新的专家群</strong>，接受新信息的培训。当使用反向传播训练这种“扩充”的专家混合物时，所有旧的神经连接都被“冻结”——<em class="kl">只有新的集群被允许学习</em>。这些“菜鸟”很快成为新任务的专家，而不会在网络的其他部分失去原有的专业知识。</p><p id="f79a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总之，这些修改提供了一个专家的混合体，他们有能力适应、消化任何规模的输入，并回忆起影响结果的复杂特征。我期望这些品质对于“下一波”神经网络任务来说是至关重要的:<strong class="jp ir">连贯反应</strong>。目前，被训练生成文本的神经网络仅从文本中学习<em class="kl">。被训练来识别图像的网络仅从图像中学习。这种“与世隔绝”的训练将会停止。生成真实文本的神经网络需要对真实世界有足够的了解，以使其陈述在逻辑上和物理上连贯，而不仅仅是语法上正确。一个图像识别网络将需要尝试在3D中实例化它的图像，看看它是否有意义。谷歌已经开始在这一领域开展工作，用“<a class="ae km" href="https://arxiv.org/pdf/1706.05137.pdf" rel="noopener ugc nofollow" target="_blank">一个模型来学习所有这些</a>”，它从文本、图像和声音的稀疏配对中学习。他们还有很长的路要走。而且，他们还没有将注意力、记忆力和成长能力的专家结合起来使用。我们很快就会需要它。</em></p></div></div>    
</body>
</html>