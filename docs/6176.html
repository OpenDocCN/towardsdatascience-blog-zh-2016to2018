<html>
<head>
<title>Neural Networks II: First Contact</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络 II:第一次接触</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-ii-first-contact-7ad5094db1d2?source=collection_archive---------12-----------------------#2018-11-29">https://towardsdatascience.com/neural-networks-ii-first-contact-7ad5094db1d2?source=collection_archive---------12-----------------------#2018-11-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="ec9b" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/neural-network-notes/latest" rel="noopener">神经网络简介</a></h2><div class=""/><p id="0d29" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这个关于神经网络的<a class="ae ku" href="https://medium.com/@pabloruizruiz/neural-networks-notes-fa42ab388bb8" rel="noopener">系列帖子</a>是脸书 PyTorch 挑战赛期间笔记收集的一部分，在 Udacity 的<a class="ae ku" href="https://eu.udacity.com/course/deep-learning-nanodegree--nd101" rel="noopener ugc nofollow" target="_blank">深度学习纳米学位项目之前。</a></p><h1 id="6dcd" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">内容</h1><ol class=""><li id="f4d4" class="lt lu iq jy b jz lv kd lw kh lx kl ly kp lz kt ma mb mc md bi translated"><strong class="jy ja">简介</strong></li><li id="1ac1" class="lt lu iq jy b jz me kd mf kh mg kl mh kp mi kt ma mb mc md bi translated"><strong class="jy ja">向前传球</strong></li><li id="1a35" class="lt lu iq jy b jz me kd mf kh mg kl mh kp mi kt ma mb mc md bi translated"><strong class="jy ja">反向传播</strong></li><li id="9b0e" class="lt lu iq jy b jz me kd mf kh mg kl mh kp mi kt ma mb mc md bi translated"><strong class="jy ja">学习</strong></li><li id="f90f" class="lt lu iq jy b jz me kd mf kh mg kl mh kp mi kt ma mb mc md bi translated"><strong class="jy ja">测试</strong></li><li id="9597" class="lt lu iq jy b jz me kd mf kh mg kl mh kp mi kt ma mb mc md bi translated"><strong class="jy ja">结论</strong></li></ol><h1 id="d30f" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">1.介绍</h1><p id="a558" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mj kj kk kl mk kn ko kp ml kr ks kt ij bi translated">在下图中，显示了一个人工神经网络。我们可以看到如何将我们的输入数据(<em class="mm"> X </em>)转发到神经网络中，并获得对我们输出的预测<em class="mm"> Y_hat </em>。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mn"><img src="../Images/f2824797910495814586e4a1f1c187ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JOvnkqDOkJxJHwM0M7F_ZQ.png"/></div></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Figure 1. Schematic Feed Forward Neural Network</figcaption></figure><p id="e7e0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因为我们有了预测应该有的实际值，所以我们可以使用所谓的“成本函数”来计算误差。通过这些，我们可以看到每个神经元对该误差的责任有多大，并更新它们的值，以便随着时间(实际上，随着输入向量的迭代次数)减少该误差。</p><p id="0369" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在写下方程之前，为了更容易理解，让我们把神经网络转换成矩阵符号:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nd"><img src="../Images/861f82465b32fd29f2b9bfd0e392d0c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aO4BQG5qqHQDJQCr--tRLA.png"/></div></div></figure><p id="41e6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">请注意，我们使用 n 和 m 来表示它可以是任何数字，也可以是更多的隐藏层。为了简单起见，我们将检查图 1 中所示的 NN，它意味着 1 个隐藏层，<em class="mm"> n=2 </em>，<em class="mm"> m=3 </em>。</p><p id="a069" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在下一个表格中，我们还可以看到一个表格，其中包含我们在本文档中使用的符号:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ne"><img src="../Images/d29a6483afd57d3f90a94d9962daa018.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TbI1edu_SBfrlhHzxSpVuA.png"/></div></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Table 1. Notation used</figcaption></figure><h1 id="1952" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">2.前进传球</h1><p id="394d" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mj kj kk kl mk kn ko kp ml kr ks kt ij bi translated">现在我们有了所有这些信息，我们可以在神经网络上发展数学方法。就在它之前，也是为了方便可视化，让我们看看神经元内部发生了什么，例如我们的第一个隐藏神经元。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nd"><img src="../Images/ebdb5cf02695af20dbaf85fb3ca3e09c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xE2Sdg45rO-_zD4UxdRuIQ.png"/></div></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Figure 2. Single Neuron Scheme</figcaption></figure><p id="3760" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这应该仍然是本系列第 1 部分中介绍的收集器和分配器的概念。</p><p id="5dbd" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">根据图示，神经元将乘以相应权重的所有输入相加，然后应用一个激活函数将输出传递给下一步。</p><p id="862d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因此，描述 NN 的整个方程组是:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ne"><img src="../Images/cd13d3fe874a588c7f74944d864dd1cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WjQIuUx4LAf7cl_f4g0Hpg.png"/></div></div></figure><p id="0473" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">有了这些等式，我们就完成了将在我们的 Neural_Network 类中定义的 forward 方法(我们将开始引入 Python 符号，以便在查看代码时更加熟悉)。</p><p id="859b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">然后，将使用 NN 的输出和真实值来计算成本函数(注意这是监督学习)。根据你的问题，有几种方法可以计算这个函数。</p><p id="bbd9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这一次，我们将尝试最小化 RMSE(均方根误差)，这对应于下一个公式。RMSE 是用于回归问题的典型误差函数，正如交叉熵用于分类问题一样。然而，在这两种情况下，我们都可以使用各种不同的函数</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nf"><img src="../Images/58e36ba73b11913a23c0885028f4d849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sXK7CxcTrFcGSJ4PQsY2sA.png"/></div></div></figure><p id="d5b8" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">更有趣的是，我们可以根据网络的参数来表达相同的等式:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ng"><img src="../Images/251afc31f547ed7ea094d8354fa3b135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OJ077cN5HgEyKrZWuFeN4g.png"/></div></div></figure><h1 id="8f29" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">3.反向传播</h1><p id="5b3e" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mj kj kk kl mk kn ko kp ml kr ks kt ij bi translated">这是告知神经网络关于该值应该具有的误差，并将其反向传播给<em class="mm">‘告诉’</em>权重的时刻，它们对该误差负有多大责任，因此它们可以更新它们自己的值，以便持续减小该误差。</p><p id="c117" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因此，我们想知道<em class="mm"> dJ/dW1 </em>和<em class="mm"> dJ/dW2 的值是多少。</em></p><p id="412b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如果我们首先开发对应于隐藏层权重的梯度:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nh"><img src="../Images/0b2c9ff202fe01752d655f8d1408214d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7M5U-Eyxtx5H_j6Ul8Ov4A.png"/></div></div></figure><p id="845b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在，如果我们暂时忘记常数项，将链式法则应用于剩余的导数项:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ni"><img src="../Images/66d2938bb4099c36a8292347e7a9aa5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KcmZJ3gwRVBrDdGYhBedrg.png"/></div></div></figure><blockquote class="nj nk nl"><p id="a5bd" class="jw jx mm jy b jz ka kb kc kd ke kf kg nm ki kj kk nn km kn ko no kq kr ks kt ij bi translated">1–预测 Y_hat 是 z3 的函数，因此可以直接导出。</p><p id="b527" class="jw jx mm jy b jz ka kb kc kd ke kf kg nm ki kj kk nn km kn ko no kq kr ks kt ij bi translated">2–预测 Y_hat 可以相对于 W2 绘制成斜率为 a2 的直线。数学上，这被表示为该向量的转置。此外，我们已经将剩余的项分组到 delta3 中，这被称为“反向传播误差”。</p><p id="fde3" class="jw jx mm jy b jz ka kb kc kd ke kf kg nm ki kj kk nn km kn ko no kq kr ks kt ij bi translated">上述陈述也可以在<a class="ae ku" href="https://www.youtube.com/watch?v=GlcnxUlrtek&amp;t=301s" rel="noopener ugc nofollow" target="_blank">这个伟大的 youtube 解释</a>中重温。</p></blockquote><p id="2618" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在，再次将链式法则应用于下一个方程，我们可以计算剩余的梯度:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi np"><img src="../Images/c9965f372f94efb127ea5fb569b3e0fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Yc_YBgCybYvI8ybzG-fJw.png"/></div></div></figure><blockquote class="nj nk nl"><p id="081c" class="jw jx mm jy b jz ka kb kc kd ke kf kg nm ki kj kk nn km kn ko no kq kr ks kt ij bi translated">1–输入<em class="iq"> z3 </em>可以相对于 a2 绘制成斜率为 w2 → w2 的直线。T</p><p id="109b" class="jw jx mm jy b jz ka kb kc kd ke kf kg nm ki kj kk nn km kn ko no kq kr ks kt ij bi translated">2–输入 a2 是 z2 的函数，因此可以直接导出</p><p id="2fb8" class="jw jx mm jy b jz ka kb kc kd ke kf kg nm ki kj kk nn km kn ko no kq kr ks kt ij bi translated">3–输入 z3 可以相对于 a2 绘制成斜率为 X → X.T 的直线</p></blockquote><h1 id="437a" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">4.学习—权重更新</h1><p id="ed3f" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mj kj kk kl mk kn ko kp ml kr ks kt ij bi translated">到目前为止，我们已经解释了如何进行前向步骤来获得预测，然后如何使用真实值来计算成本函数值，以及如何使用反向传播(偏导数+链规则)来告诉每个神经元它们对该错误负有多大责任。但是现在，我们的 NN 是怎么学习的呢？嗯，和我们做的一样:尝试，失败，学习。这些步骤就是我们所说的:训练。</p><p id="6c3f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因此，我们已经讨论了尝试(转发)和失败(开销)步骤。现在，剩下的步骤 learn 是通过更新权重值来完成的。每个节点(或神经元)将按照下一个等式更新其最后一个值:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ne"><img src="../Images/8324d513d1c7ca2618c0b9417436f6ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YwyXSnZDMzI3HG5IXYh4KQ.png"/></div></div></figure><p id="428c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">该参数<em class="mm">λ</em>被称为“<strong class="jy ja">学习率</strong>”。因此，作为<em class="mm"> dJ/dWi </em>的错误<em class="mm"> J </em>是由<em class="mm"> Wi 的责任造成的——(注意 Wi 是一整层的权重)。鉴于所犯的错误(由于他们的错误)乘以这个学习率，我们确切地告诉我们的神经元纠正他们自己的值。</em></p><p id="c982" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如果学习率的值太高(通常从 0.01 或 0.001 开始)，学习的过程就不能收敛，如果值太低，学习就可能永远进行下去。就像现实生活中一样！我们想尽可能快地学习，但是我们知道在开始的时候我们需要慢慢来吸收所有我们不知道的信息，直到我们有信心继续尝试新的信息</p><p id="bd61" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">(*)情商。9 是学习的最简单的实现。通常，我们说<strong class="jy ja">优化器</strong>负责对权重进行更新。优化器有许多不同的实现。这种情况下，最基本的和其余的支柱被称为<a class="ae ku" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank"><strong class="jy ja"/></a><strong class="jy ja">t</strong>。这里有一篇关于不同优化器以及如何实现它们的<a class="ae ku" href="http://ruder.io/optimizing-gradient-descent/" rel="noopener ugc nofollow" target="_blank">好文章</a>。</p><h1 id="679e" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">5.测试——我们学会了吗？</h1><p id="75b5" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mj kj kk kl mk kn ko kp ml kr ks kt ij bi translated">学习的过程一遍又一遍地重复，直到我们认为我们已经学会了。但是,“已经学会”是什么意思呢？就像为了考试而学习一样，我们通常会做所有书籍中的问题，直到我们确保能够解决所有问题。但是后来考试来了。在考试中，有(通常)你没有处理过的数据，你不知道你会发现什么！</p><p id="7230" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">但是，如果你已经做了足够的研究，你很确定如果你对考试数据应用<em class="mm"> forward( ) </em>函数，你将得到一个 A+的输出(你的答案)。NN 的行为完全相同。它进行训练，直到有信心转发新数据。(小心<a class="ae ku" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank">过拟合</a>，训练过度)。</p><h1 id="9752" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">5.结论——我们学会了吗？</h1><p id="500b" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mj kj kk kl mk kn ko kp ml kr ks kt ij bi translated">我希望这很有趣！这只是神经网络世界的开始！现在你可以去<a class="ae ku" href="https://github.com/PabloRR100/Artificial-Neural-Networks" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>看看如何从头开始编写这个代码，或者如何使用 PyTorch 这样的深度学习框架来实现。</p><p id="896f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我强烈建议您从头开始编程，并确保您了解所有内容。然后你将准备好节省使用这些库的时间，但是要确信你理解在引擎盖下发生了什么。</p><p id="d666" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">你需要什么，就留下评论，如果你喜欢，请推荐它！下次再见，享受神经网络！！</p></div></div>    
</body>
</html>