<html>
<head>
<title>Math of Q-Learning — Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Q 学习的数学——Python</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/math-of-q-learning-python-code-5dcbdc49b6f6?source=collection_archive---------5-----------------------#2018-12-06">https://towardsdatascience.com/math-of-q-learning-python-code-5dcbdc49b6f6?source=collection_archive---------5-----------------------#2018-12-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f823" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">理解贝尔曼方程的由来</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9fbd32e43cbb2af4459fbf10ae84d11c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aOdvtfjd2JClq20o"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@brett_jordan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Brett Jordan</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="1339" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">q 学习</h1><p id="0664" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><em class="mk"> Q-Learning </em>是一种<strong class="lq ir"> <em class="mk">强化学习</em> </strong>是一种<em class="mk">机器学习</em>。强化学习最近(通常)被用来教一个 AI 玩游戏(谷歌 DeepMind Atari 等)。我们的目标是理解称为 Q-Learning 的强化学习的简单版本，并编写一个将学习如何玩简单“游戏”的程序。让我们开始吧！</p><h1 id="9902" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">马尔可夫链</h1><p id="75ba" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">马尔可夫链是一种数学模型，它经历具有概率规则的状态转换。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/7e7e756c3d0da2ae6265125d2796c5cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*xfK_RdBZOkE4buKDPDZdKg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Markov chain — Wikipedia</figcaption></figure><p id="900e" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">这里我们有两个状态<strong class="lq ir"> E </strong>和<strong class="lq ir"> A </strong>，以及从一个状态到另一个状态的概率(例如，从状态 E 开始，有 70%的机会到达状态 A)。</p><h1 id="95dd" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">马尔可夫决策过程</h1><p id="a371" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">马尔可夫决策过程(MDP)是马尔可夫链的扩展，用于模拟更复杂的环境。在这个扩展中，我们增加了在每个状态下做出选择的可能性，这被称为<strong class="lq ir">动作</strong>。我们还添加了一个<strong class="lq ir">奖励</strong>，这是从一个状态到另一个状态采取一个行动的环境反馈。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/ef82d0c2750eab214142108200e3fe3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eevLRhyeokWPDFu1e7iuMQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="3526" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">在上图中，我们处于初始状态<strong class="lq ir"> <em class="mk">不懂</em> </strong> <em class="mk">，</em>在这里我们有两种可能的行动，<strong class="lq ir"> <em class="mk">学习</em> </strong>和<strong class="lq ir"> <em class="mk">不学习</em> </strong>。对于<em class="mk">研究</em>动作，根据一个概率规则，我们可能以不同的状态结束。这就是我们所说的<strong class="lq ir">随机</strong>环境(random)，也就是说，对于在同一状态下采取的同一动作，我们可能会有不同的结果(<strong class="lq ir"> <em class="mk">理解</em> </strong>而<strong class="lq ir"> <em class="mk">不理解</em> </strong>)。</p><p id="44db" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated"><em class="mk">在强化学习中，这是我们如何模拟一个游戏或环境，我们的目标将是最大化我们从那个环境中获得的</em> <strong class="lq ir"> <em class="mk">奖励</em> </strong> <em class="mk">。</em></p><h1 id="e390" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">报酬</h1><p id="77f2" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">回报是来自环境的反馈，它告诉我们做得有多好。例如，它可以是你在游戏中获得的硬币数量。我们的目标是最大化总回报。因此，我们需要把它写下来。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/46885d7339aa58d8c4c7709e14d580cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/0*lXszd8hM87XmeQLg.png"/></div></figure><p id="cf1b" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">这是我们在某个时间点<strong class="lq ir"><em class="mk"/></strong>开始可以得到的总奖励。</p><p id="146c" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">例如，如果我们使用上面提到的 MDP。我们最初处于<strong class="lq ir"> <em class="mk">不了解</em></strong><em class="mk"/>的状态，我们采取了<em class="mk"> </em> <strong class="lq ir"> <em class="mk">学习</em> </strong> <em class="mk"> </em>的行动，从而把我们随机带到了<em class="mk"/><strong class="lq ir"><em class="mk"/></strong>不了解的状态。因此我们经历了报酬 r(t+1)=-1。现在我们可以决定采取另一个行动，这个行动将给出 r(t+2)等等。总回报是我们在环境中采取行动所获得的所有直接回报的总和。</p><p id="8c44" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">以这种方式定义奖励会导致两个主要问题:</p><ul class=""><li id="836f" class="mt mu iq lq b lr mm lu mn lx mv mb mw mf mx mj my mz na nb bi translated">这个和有可能达到无穷大，这没有意义，因为我们想最大化它。</li><li id="3117" class="mt mu iq lq b lr nc lu nd lx ne mb nf mf ng mj my mz na nb bi translated">我们对未来回报的考虑和对眼前回报的考虑一样多。</li></ul><p id="302b" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">解决这些问题的一个方法是对未来的奖励使用递减因子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/5efffdc708100a43c2ec9779c8074045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/0*mflzCazg40-MvyTu.png"/></div></figure><p id="6f29" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">设置<strong class="lq ir"> γ=1 </strong>让我们回到第一个表达式，这里每个奖励都是同等重要的。设置<strong class="lq ir"> γ=0 </strong>导致只寻找直接的回报(总是为最佳的下一步行动)。将<strong class="lq ir"> γ </strong>设置在<strong class="lq ir"> 0 </strong>和<strong class="lq ir"> 1 </strong>之间是一种折衷，更多地寻求眼前的回报，但仍然考虑未来的回报。</p><p id="5611" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">我们可以用递归的方式重写这个表达式，这在以后会很方便。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/c949cb02945be89bdfa34c2633b6cb05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/0*SLvOh8O40AgFlUFf.png"/></div></figure><h1 id="b68d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">政策</h1><p id="8851" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">策略是一种功能，它告诉在特定状态下采取什么操作。该函数通常表示为<strong class="lq ir"> π(s，a) </strong>，并产生在状态<strong class="lq ir"> s </strong>下采取行动<strong class="lq ir"> a </strong>的概率。我们要<strong class="lq ir">找到最大化奖励</strong>函数的政策。</p><p id="96d1" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">如果我们回到以前的 MDP 为例，政策可以告诉你采取行动的概率<strong class="lq ir"><em class="mk"/></strong><em class="mk"/>当你处于<em class="mk"> </em> <strong class="lq ir"> <em class="mk">不懂</em> </strong> <em class="mk">的状态时。</em></p><p id="915a" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">此外，因为这是一个概率分布，所有可能行动的总和必须等于 1。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/0138621cfba28e46e7cda97f4f4f309b.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/0*g4HPsjrUtYNqHTiK.png"/></div></figure><h1 id="8be8" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">记号</h1><p id="b993" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们将开始摆弄一些方程，为此我们需要引入新的符号。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/e193525088a55c919dcb6f3a5ad961ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_dLhsfjo8I_4Xb1X.png"/></div></div></figure><p id="4e43" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">这是通过动作<strong class="lq ir"> a </strong>从状态<strong class="lq ir"> s </strong>到状态<strong class="lq ir">s’</strong>的预期<strong class="lq ir">即时回报</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/afc2454cf80b8cc8ec5720c2b0c6c7f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/0*bpCGFbMgE2LYD_-W.png"/></div></figure><p id="d629" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">这是通过动作<strong class="lq ir"> a </strong>从状态<strong class="lq ir"> s </strong>到状态<strong class="lq ir">s’</strong>的<strong class="lq ir">转移概率</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/ef82d0c2750eab214142108200e3fe3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eevLRhyeokWPDFu1e7iuMQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="52b0" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">在本例中:</p><ul class=""><li id="85a4" class="mt mu iq lq b lr mm lu mn lx mv mb mw mf mx mj my mz na nb bi translated">从状态<em class="mk"> </em> <strong class="lq ir"> <em class="mk">不懂</em> </strong>到状态<strong class="lq ir"> <em class="mk">不懂</em> </strong>通过行动<strong class="lq ir"> <em class="mk">不学习</em> </strong>等于<strong class="lq ir"> 0 </strong>。</li><li id="ec28" class="mt mu iq lq b lr nc lu nd lx ne mb nf mf ng mj my mz na nb bi translated">从状态<strong class="lq ir"> <em class="mk">不懂</em> </strong>到状态<strong class="lq ir"> <em class="mk">懂</em> </strong>通过行动<strong class="lq ir"> <em class="mk">学习</em> </strong>的概率等于<strong class="lq ir"> 80% </strong>。</li></ul><h1 id="fb36" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">价值函数</strong></h1><p id="e762" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">存在两种所谓的“价值函数”。<strong class="lq ir">状态值</strong>功能，以及<strong class="lq ir">动作值</strong>功能。这些功能是分别测量“值”或<em class="mk">某个状态有多好</em>或<em class="mk">某个动作有多好</em>的一种方式。</p><h2 id="d807" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">状态值</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/036cecead2884649b20a0dfdb50b876d.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/0*sjMCQm0KLpuJbDxd.png"/></div></figure><p id="bdc8" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">一个状态的<em class="mk">值</em>是我们从那个状态开始可以得到的预期总报酬。这取决于告诉我们如何做决定的政策。</p><h2 id="850c" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">价值函数</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/e5e568e07b7fa7646b3bdc6b9296ea5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/0*vhfCtTbOJxElBWKd.png"/></div></figure><p id="59b2" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">在某个状态下采取行动的<em class="mk">值</em>是我们从那个状态开始并采取行动所能得到的预期总回报。也要看政策。</p><h1 id="152b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">Q 学习的贝尔曼方程</h1><p id="d7fd" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">既然我们已经解决了符号问题，我们终于可以开始玩数学了！在计算过程中查看下图可以帮助您理解。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/c991452aa2908ad148eb5c38e0e24857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*Jy8LqMijoDoVADWexjLi0g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="2283" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">我们将从扩展状态值函数开始。<strong class="lq ir"> <em class="mk">预期</em> </strong>操作符是<strong class="lq ir"> <em class="mk">线性</em> </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/7c6edb6bb8964b444a5893fd71dfa911.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cK3VBvGUKU4Q4upU.png"/></div></div></figure><p id="a0e4" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">接下来，我们可以扩展动作值函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/2baeb74c895fd292bc5bc87ad6bd799b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nQj2YTrt1B-1VgtV.png"/></div></div></figure><p id="a72e" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">这种形式的 Q 值非常普遍。它处理随机环境，但是我们可以用确定性的<strong class="lq ir">来写。也就是说，无论何时你采取行动，你总是在<strong class="lq ir">相同的下一个状态</strong>结束，并获得<strong class="lq ir">相同的奖励</strong>。在这种情况下，我们不需要对概率进行加权求和，等式变为:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/aa1c4740e9fcf3530225f84f59d611d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/0*XMde4eknDv11n547.png"/></div></figure><p id="4e14" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">其中<strong class="lq ir">s’</strong>是您在状态<strong class="lq ir"> s </strong>中采取行动<strong class="lq ir"> a </strong>的最终状态。更正式地说，这是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/18e4704263102ee6005e6c21438800f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/0*FZt3-7XbfIjNYQJx.png"/></div></figure><h2 id="c684" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">贪婪的政策</h2><p id="bac0" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">你可能已经在网上读到过<em class="mk">贪婪政策</em>。贪婪策略是指你总是选择<strong class="lq ir">最优下一步</strong>的策略。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/002cb9d7e175635a3fa2dc23e5f65149.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*rQ7hXKOPSxcR271w.gif"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Greedy Algorithm — Wikipedia</figcaption></figure><p id="db54" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">在<strong class="lq ir">贪婪策略上下文</strong>中，我们可以写出状态值和动作值函数之间的关系。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/263d798c23efea887d94defd7c7318a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/0*1uaMoqlk7RTkWKba.png"/></div></figure><p id="12ad" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">因此，将这个代入前面的等式，我们得到在确定性环境中遵循贪婪策略的(状态，动作)对的 Q 值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/eb36ad7f1fc0cb64593bf03082a48356.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Oy_aMmqo0ghQRA56.png"/></div></div></figure><p id="5aa8" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">或者简单地说，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/ca2e760b90f8d4384fda1417b90132ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/0*Chz7J204az5VkjBi.png"/></div></figure><p id="36ed" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">而这就是 Q-Learning 上下文中的<strong class="lq ir">贝尔曼方程</strong>！它告诉我们，在某个状态<strong class="lq ir"> s </strong>下，一个动作<strong class="lq ir"> a </strong>的值是你采取那个动作得到的<strong class="lq ir">即时回报</strong>，加上你在下一个状态下可以得到的<strong class="lq ir">最大预期回报</strong>。</p><p id="513b" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">当你想到它的时候，它实际上是有意义的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/6899c6c7d165110bbd0d0e9e48b1ba4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*aWIfTBbwF5DGGrc2Qfdd5A.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">left or right ? — Image by Author</figcaption></figure><p id="a9e0" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">在这里，如果你只看<strong class="lq ir">即时奖励</strong>，你肯定会选择向左。不幸的是，游戏结束后，你不能得到更多的积分。</p><p id="930c" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">如果你加上<strong class="lq ir">下一个状态</strong>的<strong class="lq ir">最大期望报酬</strong>，那么你很可能会向右走，因为<strong class="lq ir"><em class="mk"/></strong>的最大期望报酬等于零，而<strong class="lq ir"><em class="mk"/></strong>的最大期望报酬很可能高于 10–5 = 5<strong class="lq ir"><em class="mk">。</em>T29】</strong></p><p id="9c96" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">你也可以调整<strong class="lq ir"> γ </strong>来指定<strong class="lq ir"> </strong> <em class="mk">对下一个奖励有多重要。</em></p><h1 id="19dd" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">Python 代码</h1><p id="d9e3" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这是一个简单的环境，由一个 5 乘 5 的网格组成。一个宝藏(T)放在格子的右下角。代理(O)从网格的左上角开始。</p><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="5291" class="nm kx iq ol b gy op oq l or os">O....<br/>.....<br/>.....<br/>.....<br/>....T</span></pre><p id="0de6" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">代理需要使用 4 个可用的动作到达宝藏:<strong class="lq ir"><em class="mk"/></strong><strong class="lq ir"><em class="mk">右</em></strong><strong class="lq ir"><em class="mk">上</em></strong><strong class="lq ir"><em class="mk">下</em> </strong>。</p><p id="cde3" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">如果代理采取了一个直接引导他到 T 的行动，他就得到<strong class="lq ir"> 1 </strong>的奖励，否则得到<strong class="lq ir"> 0 </strong>的奖励。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="2162" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">代码被很好地注释了，这就是我们刚刚讨论的内容。现在有趣的部分，Q 学习算法！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="d334" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">我几乎注释了这段代码的每一行，所以希望它容易理解！</p><h2 id="5a26" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">运行代码</h2><p id="032d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">将上述两个文件放在同一个目录中，并运行:</p><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="322c" class="nm kx iq ol b gy op oq l or os">python3 medium_qlearning_rl.py</span></pre><p id="a38c" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx mo lz ma mb mp md me mf mq mh mi mj ij bi translated">在纪元编号<strong class="lq ir"> <em class="mk"> 40 </em> </strong>左右，代理应该已经学会使用最短路径之一(8 步)到达宝藏。</p></div><div class="ab cl ov ow hu ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="ij ik il im in"><h1 id="52fe" class="kw kx iq bd ky kz pc lb lc ld pd lf lg jw pe jx li jz pf ka lk kc pg kd lm ln bi translated">结论</h1><p id="5681" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们已经看到了如何推导统计公式来找到贝尔曼方程，并使用它来教人工智能如何玩一个简单的游戏。请注意，在这个游戏中，可能状态的数量是有限的，这就是为什么构建 Q 表(离散值接近 Q 函数真实值的表)仍然是可管理的。图形游戏怎么样，比如 Flappy Bird，Mario Bros，或者 Call Of Duty？游戏显示的每一帧都可以认为是不同的状态。在这种情况下，不可能建立 Q 表，我们所做的是使用神经网络，其目标是学习 Q 函数。该神经网络通常将游戏的当前状态作为输入，并输出在该状态下可能采取的最佳行动。这被称为<strong class="lq ir">深度 Q 学习</strong>，也正是深蓝或阿尔法围棋这样的人工智能成功击败国际象棋或围棋世界冠军的原因。</p><h1 id="ebd4" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">我希望你喜欢这篇文章！多待一会儿吧！😎</h1></div></div>    
</body>
</html>