<html>
<head>
<title>Twitter Advertising</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Twitter广告</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/twitter-advertising-1d497d066fef?source=collection_archive---------4-----------------------#2017-06-08">https://towardsdatascience.com/twitter-advertising-1d497d066fef?source=collection_archive---------4-----------------------#2017-06-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ff2a05af185c0656cf4fade504673fd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lBCLv_WO_yiZ2-JH."/></div></div></figure><h1 id="7c21" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">介绍</h1><p id="5a50" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这是我在大会数据科学沉浸式项目中的顶点项目的非技术性报告。顶点计划是指参加该计划时所学一切的高潮。我选择对Twitter数据执行自然语言处理(NLP ),以便协助广告活动。这个项目更多地面向广告商、市场营销和任何想要扩展他们的客户关系平台以与他们的追随者交流的公司。</p><h1 id="eba0" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">目标</h1><p id="4941" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">通过推文的NLP，公司可以衡量人们对他们产品的感受。我的目标之一是创建一个模型，对一条信息是否具有积极或消极的情感价值进行分类。这种分类的一部分是过滤掉那些正面/负面的推文，看看问题源自哪里。一个公司可以直接联系那些对他们的产品有问题的顾客，同时奖励那些忠诚于这个品牌的顾客。</p><h1 id="b077" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">数据</h1><p id="7d90" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">出于介绍的目的，所有的工作都是用python编写的，使用了几个库。这些库包括Numpy、Pandas、Matplotlib、Scikit Learn和Keras。通过使用Twitter API，tweets被挖掘和收集。推文以压缩的JSON格式返回。如下例所示，信息是嵌套的，通常难以阅读。</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d860eaa117631662bf1816a1503d788d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0Mn2PgQaonyvh61w."/></div></div></figure><p id="49df" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">通过一个自动化的过程，某些特征被选中，数据被保存到一个熊猫数据框架中。</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/bbf5bbf9028f71779e7b99d4c86051d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sawG45ZlxCErF9Hy."/></div></div></figure><p id="4640" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">本项目中使用了以下功能:</p><ul class=""><li id="09b9" class="md me iq ky b kz ly ld lz lh mf ll mg lp mh lt mi mj mk ml bi translated">创建时间:创建推文的时间戳。</li><li id="b18d" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">收藏次数:这条推文被“收藏”的总次数</li><li id="8cc9" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">标签:与tweet相关联的标签。</li><li id="0310" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">位置:用户声明的位置。</li><li id="0518" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">挖掘时间:tweet被挖掘的时间戳。</li><li id="13b0" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">转发次数:这条推文被“转发”的总次数。</li><li id="1664" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">屏幕名称:用户的屏幕名称。</li><li id="a495" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">来源:推文的来源(即:设备)。</li><li id="f319" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">文本:推文的文本。</li><li id="2ef3" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">字数:一条推文中的总字数。</li><li id="22d9" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">tweet Length:tweet的字符长度。</li></ul><p id="3016" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">返回的数据不干净。我说它不干净是因为为了让我处理数据，我需要一种特定的格式。有许多函数是为处理这个过程而编写的。我清理数据的过程如下:</p><ul class=""><li id="6a2f" class="md me iq ky b kz ly ld lz lh mf ll mg lp mh lt mi mj mk ml bi translated">使用位置变量提取tweet的坐标。如果没有，我将它设置为加州旧金山。为什么是SF？因为Twitter总部位于旧金山。</li><li id="b5e8" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">created_at和mined_at变量被转换为datetime对象以执行与时间相关的函数。</li><li id="8bf4" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">hashtags变量在开头接收' # '，空格被删除。这是为了让所有的标签保持相同的格式。</li><li id="e743" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">源变量删除了html代码。</li><li id="856d" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">文本变量移除了超链接，移除了“@”提及，单词被标记化。示例:</li></ul><p id="ea90" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated"><a class="ae mr" href="https://twitter.com/hashtag/TattoosForShawn?src=hash" rel="noopener ugc nofollow" target="_blank"> <em class="ms"> # </em> <strong class="ky ir"> <em class="ms">纹身sForShawn </em> </strong> </a> <em class="ms">我是</em> <strong class="ky ir"> <em class="ms">思维</em> </strong> <em class="ms">点歌</em> <strong class="ky ir"> <em class="ms">耳机</em> </strong> <em class="ms">，然后那根线就是歌迷们歌唱生活的声波聚会</em><a class="ae mr" href="https://twitter.com/ShawnMendes" rel="noopener ugc nofollow" target="_blank"><em class="ms">@ Shawn Mendes</em></a></p><p id="f802" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">变成了:‘纹身sforshawn’，‘思考’，‘耳机’，‘电线’，‘声音’，‘海浪’，‘粉丝’，‘唱歌’，‘生活’，‘派对’</p><p id="72fd" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">下图显示了数据清理后数据帧的外观:</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/60b1ebfe4d6a37a9bed8da67b746610f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ODcLax1C3HwU0mqy."/></div></div></figure><h1 id="94d7" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">特征工程</h1><p id="881c" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">除了从twitter中挖掘出来的特性，还有其他一些特性是为了更好地理解数据而创建的。VADER(用于情感推理的效价感知词典)库包括情感强度分析器，该分析器是确定给定推文的情感值的基础。这是一个基于规则的情绪分析工具，专门针对社交媒体中表达的情绪。给定一条消息，分析器返回可测量的值:</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/368fd9405ac9b5d28e05f93b63828da3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/0*899_mFpt_h0VjD3k."/></div></figure><ul class=""><li id="9754" class="md me iq ky b kz ly ld lz lh mf ll mg lp mh lt mi mj mk ml bi translated">消极情绪</li><li id="1bff" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">neu:中性情绪</li><li id="5eb7" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">积极情绪</li><li id="c365" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">复合:复合(即综合得分)</li></ul><p id="d6a3" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">Nodebox英语语言学图书馆允许通过回归意象词典进行心理内容分析。RID对文本中主要、次要和情感过程思想进行评分。这是一个有趣的包，因为它开辟了一种查看信息的新方式。</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/4d22aa9c69472a965a855ee733e5c945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/0*fdCXpm9e3sGT6NV6."/></div></figure><ul class=""><li id="156f" class="md me iq ky b kz ly ld lz lh mf ll mg lp mh lt mi mj mk ml bi translated">初级:涉及梦和幻想的自由形式联想思维</li><li id="58c9" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">二级:逻辑性强，以现实为基础，注重解决问题</li><li id="46ca" class="md me iq ky b kz mm ld mn lh mo ll mp lp mq lt mi mj mk ml bi translated">情绪:恐惧、悲伤、憎恨、喜爱等的表达。</li></ul><h1 id="5de7" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">探索数据</h1><p id="118b" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">探索性数据分析(EDA)是一种分析数据集以总结其主要特征的方法。我选择从两个不同的角度来看这些数据。第一种是对收集到的所有公共信息进行更全面的概述。第二个通过更具体的观点，好像我是一个对我的信息感兴趣的公司。我们先来看一个大概的看法。</p><p id="637a" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">我选择查看每个特性，并查看数据的整体视图。time_created变量显示，大多数推文是在下午发送的，然后是在午夜之后的几个小时。请注意，这些时间是以太平洋标准时间为基准的。标签聚集了挖掘过程中讨论的主要话题。</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/4d2efcc5e24e167bcd1a841e747f789d.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/0*Qnx8ZgPiKU9H57nJ."/></div></figure><p id="0dc2" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">收集的推文主要集中在美国，因为情感库是在英语语言上训练的。这也是看外国字符时的编码问题。如果没有命名城市，加利福尼亚州旧金山是默认城市，因此在其他位置中很突出。许多用户在他们的个人资料中没有位置，在他们的推文中也没有。Twitter的总部在旧金山市中心，所以推文是从那里发出的。</p><p id="46af" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">sources变量显示twitter的移动使用占据了首位。前三名依次是iPhone、Android和Twitter网络客户端。web客户端指的是通过web浏览器在桌面上使用twitter，以及在移动设备上使用web浏览器。</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/9354198d0d037e8c394e1972394d1594.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/0*93qv57SRsoaMPlMp."/></div></figure><p id="386d" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">在将推文分成各自的情绪值后，我决定看看字数分布。负面和正面推文的字数分布峰值都是每条推文15个字。中性的推文大多在10个字及以下。</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/d34d3713abd106ff51f5c00d8b094798.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/0*KzMsVQBQr6h9aFBU."/></div></figure><p id="8056" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">对于VADER情绪分析，我将着眼于复合值。下图中的复合金额是VADER情绪分析的聚合值。复合值的范围从-1到1；消极情绪，到中性情绪，到积极情绪。大多数推文被评为中性。我认为这是因为VADER情绪分析器没有对2017年的新词进行训练，即:fleek。</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/255d11926cf709feddab5a11eac503fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/0*2zcAAhWGZzoZI52S."/></div></figure><h1 id="95f5" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">#万豪</h1><p id="3150" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在对数据进行总体概述后，我想看看与万豪品牌相关的具体推文。获取数据的过程与之前相同，只是我将标签“#Marriott”传递给一个搜索参数，因此只返回与该标签相关的信息。</p><p id="920d" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">我想看的第一件事是#万豪在世界上的什么地方被谈论。使用坐标变量，我可以在地图上画出推文的来源。</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/933c6892bd0e9feaaa0a0957c684e119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*44TjE6JqLsNMSreS."/></div></div></figure><p id="5013" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">通过观察一段时间内的情绪值，我能够衡量与万豪相关的正面或负面推文的数量。大多数推文都是对万豪的正面评价，只有少数是负面评价。</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ec70d07606e5d93d8c776cabce680591.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*L1eQBw3pOFanohfp."/></div></div></figure><p id="f905" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">一条复合得分高达0.951的推文示例:</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/9cc7510319492a043af0464d9b8578ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/0*Pi_vMTyeRX4_ya46."/></div></figure><p id="6678" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">一条复合得分为-0.6551的推文示例:</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/d17cb302a64a6c9d41f895fe674d39b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/0*i0al0oQ3QyQT08iH."/></div></figure><p id="8a17" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">作为万豪的员工，我能够联系有问题的客户，并提供某种解决方案。同样的道理也适用于奖励评论最好的酒店。</p><h1 id="d61d" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">数据建模</h1><p id="c86b" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">VADER情感分析器是对推文情感价值进行分类的基础。我想看看我是否能利用课堂上学到的模型对推文进行分类。在继续之前，我需要将文本数据转换成python可以理解的格式。我用word2vec从单词中创建向量。在word2vec中，使用了单词的分布式表示。取一个有几百维(比如300维)的向量。每个单词都由这些元素的权重分布来表示。因此，向量中的元素和单词之间不是一对一的映射，而是单词的表示分布在向量中的所有元素上，向量中的每个元素都有助于许多单词的定义。单词“dog”的300维向量示例:</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/4d42150cb84a6437228b8f6753949188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/0*Jo5QUOcUNF0dfeua."/></div></figure><pre class="lu lv lw lx gt nb nc nd ne aw nf bi"><span id="c153" class="ng jz iq nc b gy nh ni l nj nk"># creates a vector given a sentence/tweet<br/># takes a list of words, and array size of w2v vector<br/># tokens == a list of words (the tokenized tweet)<br/># size == the dimensional size of the vector</span><span id="d989" class="ng jz iq nc b gy nl ni l nj nk">size = 300</span><span id="9cb0" class="ng jz iq nc b gy nl ni l nj nk">def buildWordVector(tokens, size):<br/>    vec = np.zeros(size).reshape((1, size))<br/>    count = 0.<br/>    for word in tokens:<br/>        try:<br/>            vec+= w2v[word].reshape((1,size))<br/>            count += 1.<br/>        except BaseException: # handling the case where the token is not<br/>            try:<br/>                continue<br/> <br/>    if count != 0:<br/>        # get centroid of sentence<br/>        vec /= count<br/>    return vec</span></pre><p id="847e" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">一旦每个单词都被矢量化，下一步就是创建推文本身的矢量。tweet中的每个单词向量相加在一起，除以tweet中的单词总数，这为tweet创建了一个向量，它是相加向量的质心。这是我遇到的强大的东西。通过创建这些向量，您可以执行矩阵数学运算并找到新的模式。可以对这些向量执行余弦相似性，这使你能够看到相似的推文/词以及最相反的推文/词。一旦用户的推文被矢量化，就可以用同样的步骤为用户创建向量。</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/187b5fe692518ee6df5bdac0e7b7cd1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*I6dUErCR68AqXoz_."/></div></div></figure><p id="2887" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">作为参考，有三类被分类:消极，中立和积极。数据被分成85/15的比例，用于训练和测试数据。我的基本估计是多项式逻辑回归。我正在查看更具体的回忆分数，因为回忆是“结果有多完整”。高召回率意味着算法返回了大部分相关结果。如你所见，中性分类高于其他分类。</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/b6b1455710ef39ced31670b1b875f377.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/0*NHMa5aHqIvH7nUW6."/></div></figure><p id="8ea8" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">额外树分类器是尝试的第二个模型。“额外树”分类器，也称为“极度随机化树”分类器，是随机森林的变体。与随机森林不同，在每一步都使用整个样本，并随机选取决策边界，而不是最佳边界。还是那句话，中性分类高于其余。负面分类的表现很糟糕。</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/8dc9b49aca6fe00c4190157b2b787b8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/0*Grj9Wn58JCq-JXhA."/></div></figure><p id="2877" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">我最后一次分类的尝试是用神经网络来完成。Keras是用Python编写的开源神经网络库。我对Keras的第一次尝试是创建一个又深又宽的神经网络。它有5个致密层，分别含有500、800、1000、300和3个神经元。以及设置为0.1、0.5、0.2、0.2和0.2的每个密集层之前的下降层。前四层的激活是“relu”，最后一层是“softmax”。</p><pre class="lu lv lw lx gt nb nc nd ne aw nf bi"><span id="62ef" class="ng jz iq nc b gy nh ni l nj nk"># in order for keras to accurately multi-classify, must convert from # 1 column to 3 columns<br/>y_train = keras.utils.np_utils.to_categorical(y_train,       num_classes=3)<br/>y_test = keras.utils.np_utils.to_categorical(y_test, num_classes=3)</span><span id="1034" class="ng jz iq nc b gy nl ni l nj nk"># MULTI-CLASSIFICATION<br/># Evaluating with a Keras NN</span><span id="62c6" class="ng jz iq nc b gy nl ni l nj nk"># fix random seed for reproducibility<br/>seed = 7<br/>np.random.seed(seed)</span><span id="5abe" class="ng jz iq nc b gy nl ni l nj nk">from keras.constraints import maxnorm</span><span id="e3b3" class="ng jz iq nc b gy nl ni l nj nk"># Keras neural network deep and wide.<br/>model_keras = Sequential()<br/>model_keras.add(Dropout(0.1, input_shape=(train_vecs_w2v.shape[1],)))<br/>model_keras.add(Dense(500, activation=’relu’, kernel_constraint=maxnorm(1)))<br/>model_keras.add(Dropout(0.5))<br/>model_keras.add(Dense(800, activation=’relu’, kernel_constraint=maxnorm(1)))<br/>model_keras.add(Dropout(0.2))<br/>model_keras.add(Dense(1000, activation=’relu’, kernel_constraint=maxnorm(1)))<br/>model_keras.add(Dropout(0.2))<br/>model_keras.add(Dense(300, activation=’relu’, kernel_constraint=maxnorm(1)))<br/>model_keras.add(Dropout(0.2))<br/>model_keras.add(Dense(3, activation=’softmax’))</span><span id="3550" class="ng jz iq nc b gy nl ni l nj nk">epochs = 20</span><span id="698d" class="ng jz iq nc b gy nl ni l nj nk">model_keras.compile(loss=’categorical_crossentropy’,<br/> optimizer=’adam’,<br/> metrics=[‘accuracy’])</span><span id="daec" class="ng jz iq nc b gy nl ni l nj nk">history = model_keras.fit(train_vecs_w2v, y_train, epochs=epochs, validation_data=(test_vecs_w2v, y_test), batch_size=32, verbose=2)</span><span id="ef63" class="ng jz iq nc b gy nl ni l nj nk">score = model_keras.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=0)</span><span id="0ef0" class="ng jz iq nc b gy nl ni l nj nk">print ‘Evaluated score on test data:’, score[1]</span><span id="1f6a" class="ng jz iq nc b gy nl ni l nj nk"># summarize history for accuracy<br/>plt.figure(num=None, figsize=(8, 6), dpi=100, facecolor=’w’, edgecolor=’k’)<br/>plt.plot(history.history[‘acc’])<br/>plt.plot(history.history[‘val_acc’], c=’r’, ls=’dashed’)<br/>plt.title(‘model accuracy’)<br/>plt.ylabel(‘accuracy’)<br/>plt.xlabel(‘epoch’)<br/>plt.legend([‘train’, ‘test’], loc=’upper left’)<br/>plt.hlines(y=score[1], xmin=0, xmax=epochs)<br/>plt.show()</span><span id="1a7c" class="ng jz iq nc b gy nl ni l nj nk"># summarize history for loss<br/>plt.figure(num=None, figsize=(8, 6), dpi=100, facecolor=’w’, edgecolor=’k’)<br/>plt.plot(history.history[‘loss’])<br/>plt.plot(history.history[‘val_loss’], c=’r’, ls=’dashed’)<br/>plt.title(‘model loss’)<br/>plt.ylabel(‘loss’)<br/>plt.xlabel(‘epoch’)<br/>plt.legend([‘train’, ‘test’], loc=’upper left’)<br/>plt.show()</span></pre><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/bc9fee15754d8bde37c709271f64027f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/0*q1TXUNplEDFe9p6H."/></div></figure><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/0a18238c486e7488f637a2624ea9fb9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/0*ySnVPOOdVfz2Yoig."/></div></figure><p id="30c2" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">所有的分类模型似乎都有一个模式。负面分类总是比其余的低。再看数据，负面情绪值的推文大概占整个数据集的20%。中性的占44%，积极的占33%。下一步是将所有推文限制在相同的数据量。我决定从每个班级随机抽取4000个数据点。该数据然后被放入70/30的训练测试分割中；70%的数据用于训练，30%用于测试。我的下一步是尝试短而浅的神经网络设置。三个致密层，分别为32、16和3个神经元。每个密集层之前的三个下降层设置为0.1、0.3和0.2。纪元已更改为50，批次大小已更改为2048。</p><pre class="lu lv lw lx gt nb nc nd ne aw nf bi"><span id="1da1" class="ng jz iq nc b gy nh ni l nj nk"># df is the pandas dataframe object which contains all the clean data</span><span id="af7e" class="ng jz iq nc b gy nl ni l nj nk"># Function to create targets based on compound score<br/>def get_targets(row):<br/>    if row &lt; 0:<br/>        return 0<br/>    elif row == 0:<br/>        return 1<br/>    else:<br/>        return 2</span><span id="1954" class="ng jz iq nc b gy nl ni l nj nk"># Creating the targets based of the compound value<br/>df['target'] = df['compound'].map(lambda x: get_targets(x))<br/>neg = df[df.target == 0]<br/>neu = df[df.target == 1]<br/>pos = df[df.target == 2]</span><span id="bbb5" class="ng jz iq nc b gy nl ni l nj nk"># Randomly sampling tweets to a limit of 4000 each<br/>neg_sampled = neg.sample(4000)<br/>neu_sampled = neu.sample(4000)<br/>pos_sampled = pos.sample(4000)</span><span id="7358" class="ng jz iq nc b gy nl ni l nj nk"># concating all sampled datasets together<br/>df_test = pd.concat([neg_sampled, neu_sampled, pos_sampled])<br/>df_test.shape</span><span id="30ca" class="ng jz iq nc b gy nl ni l nj nk"># Creating training/testing data, 70/30 split<br/>X_train, X_test, y_train, y_test = train_test_split(df_test.token_text, df_test.target, train_size=0.7)</span><span id="9f7a" class="ng jz iq nc b gy nl ni l nj nk"># For every tweet, creating a word vector array for tweet<br/>X_train = np.concatenate([buildWordVector(z, num_features) for z in X_train])<br/>X_train = scale(X_train)</span><span id="0f98" class="ng jz iq nc b gy nl ni l nj nk">X_test = np.concatenate([buildWordVector(z, num_features) for z in X_test])<br/>X_test = scale(X_test)</span><span id="575e" class="ng jz iq nc b gy nl ni l nj nk"># in order for keras to accurately multi-classify, must convert from 1 column to 3 columns<br/>y_train = keras.utils.np_utils.to_categorical(y_train, num_classes=3)<br/>y_test = keras.utils.np_utils.to_categorical(y_test, num_classes=3)</span><span id="9ee4" class="ng jz iq nc b gy nl ni l nj nk"># MULTI-CLASSIFICATION<br/># Evaluating with a Keras NN</span><span id="5e2e" class="ng jz iq nc b gy nl ni l nj nk"># fix random seed for reproducibility<br/>seed = 7<br/>np.random.seed(seed)</span><span id="f511" class="ng jz iq nc b gy nl ni l nj nk">leng = X_train.shape[1]<br/>X_train = X_train.reshape(X_train.shape[0], 1, leng)<br/>X_test = X_test.reshape(X_test.shape[0], 1, leng)</span><span id="3785" class="ng jz iq nc b gy nl ni l nj nk"># Keras neural network short and shallow<br/>model_keras = Sequential()<br/>model_keras.add(Dropout(0.1, input_shape=(X_train.shape[1],)))<br/>model_keras.add(Dense(32, activation=’relu’, kernel_constraint=maxnorm(1)))<br/>model_keras.add(Dropout(0.3))<br/>model_keras.add(Dense(16, activation=’relu’, kernel_constraint=maxnorm(1)))<br/>model_keras.add(Dropout(0.2))<br/>model_keras.add(Dense(3, activation=’softmax’))</span><span id="96dc" class="ng jz iq nc b gy nl ni l nj nk"># SGD Parameters<br/>learning_rate = 0.01<br/>decay_rate = learning_rate / epochs<br/>momentum = .95<br/>sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=True)</span><span id="cd1c" class="ng jz iq nc b gy nl ni l nj nk">model_keras.compile(loss=’categorical_crossentropy’,<br/> optimizer=’adam’,<br/> metrics=[‘accuracy’])</span><span id="939d" class="ng jz iq nc b gy nl ni l nj nk"># Model parameters<br/>epochs = 50<br/>batch = 2048</span><span id="e741" class="ng jz iq nc b gy nl ni l nj nk">history = model_keras.fit(X_train, y_train, <br/> validation_data=(X_test, y_test), <br/> epochs=epochs, <br/> batch_size=batch, <br/> verbose=0)</span><span id="eae2" class="ng jz iq nc b gy nl ni l nj nk">score = model_keras.evaluate(X_test, y_test, <br/> batch_size=batch, verbose=0)</span><span id="8ad8" class="ng jz iq nc b gy nl ni l nj nk">print ‘Evaluated score on test data:’, score[1]</span><span id="2c00" class="ng jz iq nc b gy nl ni l nj nk"># summarize history for accuracy<br/>plt.figure(num=None, figsize=(8, 6), dpi=100, facecolor=’w’, edgecolor=’k’)<br/>plt.plot(history.history[‘acc’])<br/>plt.plot(history.history[‘val_acc’], c=’r’, ls=’dashed’)<br/>plt.title(‘model accuracy’)<br/>plt.ylabel(‘accuracy’)<br/>plt.xlabel(‘epoch’)<br/>plt.legend([‘train’, ‘test’], loc=’upper left’)<br/>plt.hlines(y=score[1], xmin=0, xmax=epochs)<br/>plt.show()</span><span id="f722" class="ng jz iq nc b gy nl ni l nj nk"># summarize history for loss<br/>plt.figure(num=None, figsize=(8, 6), dpi=100, facecolor=’w’, edgecolor=’k’)<br/>plt.plot(history.history[‘loss’])<br/>plt.plot(history.history[‘val_loss’], c=’r’, ls=’dashed’)<br/>plt.title(‘model loss’)<br/>plt.ylabel(‘loss’)<br/>plt.xlabel(‘epoch’)<br/>plt.legend([‘train’, ‘test’], loc=’upper left’)<br/>plt.show()</span></pre><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/b0fccba22977521d6ae4b3899f897447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/0*ymVDQ0WFOCzxSWKO."/></div></figure><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/33f4b2cf0ada4b36138fe163e7a86262.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/0*2C-wyq1Ud6UlN-PZ."/></div></figure><p id="0975" class="pw-post-body-paragraph kw kx iq ky b kz ly lb lc ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt ij bi translated">使用Keras分类器的试验比以前的模型表现得更好。从回忆分数来看，负数看起来没有以前那么糟糕了。即使每个情感值都有4000个数据点，也可能是中性类有更多独特的词。进一步的测试将尝试添加一个LSTM层，以便添加一个记忆层，以及一个卷积层，以便找到隐藏的模式。</p><h1 id="3d30" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">结论</h1><p id="1e60" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">获得更好的分类器分数所需的数据必须是特定的。它不可能是随机的推文。从字面上看，最负面的词和最正面的词都要被搜索和收集。这样做之后，我相信这个分类器会像VADER情感分析一样出色。</p></div></div>    
</body>
</html>