<html>
<head>
<title>Use torchtext to Load NLP Datasets — Part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用torchtext加载NLP数据集—第一部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-i-5da6f1c89d84?source=collection_archive---------5-----------------------#2018-01-24">https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-i-5da6f1c89d84?source=collection_archive---------5-----------------------#2018-01-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3d1e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">PyTorch Tensors管道的简单CSV文件</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f52a80e0c9a0552466d74ed70a9f1835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kw2TcCGoU1-7Ex8RbyjB0A.jpeg"/></div></div></figure><p id="1600" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你是PyTorch用户，你可能已经熟悉了<em class="ln"> torchvision </em>库，因为<em class="ln"> torchvision </em>已经变得相对稳定和强大，而<a class="ae lo" href="http://pytorch.org/docs/master/torchvision/index.html" rel="noopener ugc nofollow" target="_blank">已经成为PyTorch的官方文档</a>。鲜为人知的<em class="ln"> torchtext </em>库试图实现与<em class="ln"> torchvision </em>相同的东西，但使用NLP数据集。它仍在积极开发中，并且有些问题可能需要您自己解决<a class="ae lo" href="https://github.com/pytorch/text/issues/140" rel="noopener ugc nofollow" target="_blank">【1】</a><a class="ae lo" href="https://github.com/pytorch/text/issues/177" rel="noopener ugc nofollow" target="_blank">【2】</a>。尽管如此，我发现它已经相当有用了。最好使用<em class="ln"> torchtext </em>并在需要时定制或扩展它(如果你的用例是通用的，也许还可以创建一个PR)。)而不是自己构建整个预处理管道。</p><p id="6e2d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这篇文章中，我将使用<a class="ae lo" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge" rel="noopener ugc nofollow" target="_blank">有毒评论分类数据集</a>作为一个例子，并尝试使用<em class="ln"> torchtext </em>演示一个加载该数据集的工作管道。我以前在一次不成功的尝试中使用过这个数据集，我们将在这里使用相同的标记化方案:</p><div class="lp lq gp gr lr ls"><a href="https://medium.com/@ceshine/learning-note-starspace-for-multi-label-text-classification-81de0e8fca53" rel="noopener follow" target="_blank"><div class="lt ab fo"><div class="lu ab lv cl cj lw"><h2 class="bd ir gy z fp lx fr fs ly fu fw ip bi translated">[学习笔记]用于多标签文本分类的StarSpace</h2><div class="lz l"><h3 class="bd b gy z fp lx fr fs ly fu fw dk translated">StarSpace是一个雄心勃勃的模型，试图解决广泛的实体嵌入问题。它已经被创建并且…</h3></div><div class="ma l"><p class="bd b dl z fp lx fr fs ly fu fw dk translated">medium.com</p></div></div></div></a></div></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="a623" class="mi mj iq bd mk ml mm dn mn mo mp dp mq la mr ms mt le mu mv mw li mx my mz na bi translated">证明文件</h2><p id="647b" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">首先需要解决的是文档。在<a class="ae lo" href="https://github.com/pytorch/text/blob/master/README.rst" rel="noopener ugc nofollow" target="_blank">项目自述</a>之外基本没有简洁的代码示例。下一个最好的事情是在<em class="ln"> test </em>文件夹中的单元测试。你必须弄清楚东西在哪里，然后自己把它们放在一起。</p><p id="6e58" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">您可以使用sphinx从docstrings构建文档(在本文中我们使用了<em class="ln"> torchtext </em> 0.2.1):</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="b8b6" class="mi mj iq nh b gy nl nm l nn no">pip install sphinx sphinx_rtd_theme<br/>git clone --branch v0.2.1 <a class="ae lo" href="https://github.com/pytorch/text.git" rel="noopener ugc nofollow" target="_blank">https://github.com/pytorch/text.git</a><br/>cd text/docs<br/>make html</span></pre><p id="df34" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">HTML页面将位于<em class="ln"> text/docs/build/html </em>文件夹中。</p><p id="cfdc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我个人觉得直接看源代码比较容易。</p><h2 id="0fb9" class="mi mj iq bd mk ml mm dn mn mo mp dp mq la mr ms mt le mu mv mw li mx my mz na bi translated">预处理CSV文件</h2><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="9742" class="mi mj iq nh b gy nl nm l nn no">import pandas as pd<br/>import numpy as np</span><span id="5ea2" class="mi mj iq nh b gy np nm l nn no">VAL_RATIO = 0.2</span><span id="8ab2" class="mi mj iq nh b gy np nm l nn no">def prepare_csv(seed=999):<br/>    df_train = pd.read_csv("data/train.csv")<br/>    df_train["comment_text"] = \<br/>        df_train.comment_text.str.replace("\n", " ")<br/>    idx = np.arange(df_train.shape[0])<br/>    np.random.seed(seed)<br/>    np.random.shuffle(idx)<br/>    val_size = int(len(idx) * VAL_RATIO)<br/>    df_train.iloc[idx[val_size:], :].to_csv(<br/>        "cache/dataset_train.csv", index=False)<br/>    df_train.iloc[idx[:val_size], :].to_csv(<br/>        "cache/dataset_val.csv", index=False)<br/>    df_test = pd.read_csv("data/test.csv")<br/>    df_test["comment_text"] = \<br/>        df_test.comment_text.str.replace("\n", " ")<br/>    df_test.to_csv("cache/dataset_test.csv", index=False)</span></pre><p id="3225" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">和上一篇文章一样，我把原始数据放在<em class="ln">数据</em>文件夹中，所有从它导出的数据都放在<em class="ln">缓存</em>文件夹中。这个<em class="ln"> prepare_csv </em>功能的存在主要有两个原因:</p><ol class=""><li id="4896" class="nq nr iq kt b ku kv kx ky la ns le nt li nu lm nv nw nx ny bi translated">您必须自己将训练数据集分为训练数据集和验证数据集。torchtext 不会为你这样做。</li><li id="9302" class="nq nr iq kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated">需要删除换行符。否则<em class="ln"> torchtext </em>无法正确读取csv文件。</li></ol><p id="08db" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">是为了确保我们每次都有相同的分成。</p><h2 id="b7c6" class="mi mj iq bd mk ml mm dn mn mo mp dp mq la mr ms mt le mu mv mw li mx my mz na bi translated">标记化</h2><p id="0421" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">如前所述，标记化方案与前一篇文章中的相同:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="d890" class="mi mj iq nh b gy nl nm l nn no">import re</span><span id="12d3" class="mi mj iq nh b gy np nm l nn no">import spacy<br/>NLP = spacy.load('en')<br/>MAX_CHARS = 20000</span><span id="953e" class="mi mj iq nh b gy np nm l nn no">def tokenizer(comment):<br/>    comment = re.sub(<br/>        r"[\*\"“”\n\\…\+\-\/\=\(\)‘•:\[\]\|’\!;]", " ", <br/>        str(comment))<br/>    comment = re.sub(r"[ ]+", " ", comment)<br/>    comment = re.sub(r"\!+", "!", comment)<br/>    comment = re.sub(r"\,+", ",", comment)<br/>    comment = re.sub(r"\?+", "?", comment)<br/>    if (len(comment) &gt; MAX_CHARS):<br/>        comment = comment[:MAX_CHARS]<br/>    return [<br/>        x.text for x in NLP.tokenizer(comment) if x.text != " "]</span></pre><p id="bf25" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这个函数返回一个注释的标记列表。很长的注释被修剪为<em class="ln"> MAX_CHARS </em>个字符，否则NLP.tokenizer可能需要很长时间才能返回。</p><h2 id="9b42" class="mi mj iq bd mk ml mm dn mn mo mp dp mq la mr ms mt le mu mv mw li mx my mz na bi translated">加载数据集</h2><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="d24c" class="mi mj iq nh b gy nl nm l nn no">import logging</span><span id="89c4" class="mi mj iq nh b gy np nm l nn no">import torch<br/>from torchtext import data</span><span id="1b14" class="mi mj iq nh b gy np nm l nn no">LOGGER = logging.getLogger("toxic_dataset")</span><span id="8b0b" class="mi mj iq nh b gy np nm l nn no">def get_dataset(fix_length=100, lower=False, vectors=None):<br/>    if vectors is not None:<br/>        # pretrain vectors only supports all lower cases<br/>        lower = True<br/>    LOGGER.debug("Preparing CSV files...")<br/>    prepare_csv()<br/>    comment = data.Field(<br/>        sequential=True,<br/>        fix_length=fix_length,<br/>        tokenize=tokenizer,<br/>        pad_first=True,<br/>        tensor_type=torch.cuda.LongTensor,<br/>        lower=lower<br/>    )<br/>    LOGGER.debug("Reading train csv file...")<br/>    train, val = data.TabularDataset.splits(<br/>        path='cache/', format='csv', skip_header=True,<br/>        train='dataset_train.csv', validation='dataset_val.csv',<br/>        fields=[<br/>            ('id', None),<br/>            ('comment_text', comment),<br/>            ('toxic', data.Field(<br/>                use_vocab=False, sequential=False,<br/>                tensor_type=torch.cuda.ByteTensor)),<br/>            ('severe_toxic', data.Field(<br/>                use_vocab=False, sequential=False, <br/>                tensor_type=torch.cuda.ByteTensor)),<br/>            ('obscene', data.Field(<br/>                use_vocab=False, sequential=False, <br/>                tensor_type=torch.cuda.ByteTensor)),<br/>            ('threat', data.Field(<br/>                use_vocab=False, sequential=False, <br/>                tensor_type=torch.cuda.ByteTensor)),<br/>            ('insult', data.Field(<br/>                use_vocab=False, sequential=False, <br/>                tensor_type=torch.cuda.ByteTensor)),<br/>            ('identity_hate', data.Field(<br/>                use_vocab=False, sequential=False, <br/>                tensor_type=torch.cuda.ByteTensor)),<br/>        ])<br/>    LOGGER.debug("Reading test csv file...")<br/>    test = data.TabularDataset(<br/>        path='cache/dataset_test.csv', format='csv', <br/>        skip_header=True,<br/>        fields=[<br/>            ('id', None),<br/>            ('comment_text', comment)<br/>        ])<br/>    LOGGER.debug("Building vocabulary...")<br/>    comment.build_vocab(<br/>        train, val, test,<br/>        max_size=20000,<br/>        min_freq=50,<br/>        vectors=vectors<br/>    )<br/>    LOGGER.debug("Done preparing the datasets")<br/>    return train, val, test</span></pre><p id="d7ed" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该功能有两个主要组件:<code class="fe oe of og nh b">data.Field</code>和<code class="fe oe of og nh b">data.TabularDataset</code>。<code class="fe oe of og nh b">comment</code>变量指定了<code class="fe oe of og nh b">comment_text</code>列的预处理管道，并在<code class="fe oe of og nh b">train</code>、<code class="fe oe of og nh b">validation</code>、<code class="fe oe of og nh b">test</code>数据集之间共享，因此它们使用相同的词汇。一些细节:</p><ol class=""><li id="0304" class="nq nr iq kt b ku kv kx ky la ns le nt li nu lm nv nw nx ny bi translated"><code class="fe oe of og nh b">sequential=True</code>指定该列保存序列。</li><li id="cf26" class="nq nr iq kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated"><code class="fe oe of og nh b">tokenizer=tokenizer</code>指定记号赋予器。如果输入列足够干净，可以使用内置的标记器，例如<code class="fe oe of og nh b">tokenizer="spacy"</code>。</li><li id="e68f" class="nq nr iq kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated"><code class="fe oe of og nh b">fix_length</code>将所有序列填充或修剪到固定长度。如果未设置，长度将是每批中最长序列的长度。</li><li id="98f6" class="nq nr iq kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated"><code class="fe oe of og nh b">pad_first=True</code>从左侧填充序列。例如，如果目标长度为5，<code class="fe oe of og nh b">A text sequence</code>将被填充为<code class="fe oe of og nh b">&lt;pad&gt; &lt;pad&gt; A text sequence</code>。</li><li id="d151" class="nq nr iq kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated"><code class="fe oe of og nh b">tensor_type</code>指定返回的张量类型。由于在大多数情况下我们使用GPU来训练模型，将其设置为<code class="fe oe of og nh b">torch.cuda.LongTensor</code>将省去我们稍后将其移动到GPU内存的麻烦。</li><li id="379b" class="nq nr iq kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated"><code class="fe oe of og nh b">lower</code>指定我们是否将所有英文字符设置为小写。</li></ol><p id="e6f8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">另一个非常方便的特性是<code class="fe oe of og nh b">.build_vocab</code>，它构建了词汇表，因此我们可以在以后将记号/单词转换成整数，并且可以选择性地为您加载预训练的单词向量(<code class="fe oe of og nh b">comment.vocab.vectors</code>将是与当前词汇表对齐的加载向量)。在处检查可用的预训练矢量<a class="ae lo" href="https://github.com/pytorch/text/blob/v0.2.1/torchtext/vocab.py#L379" rel="noopener ugc nofollow" target="_blank">。<code class="fe oe of og nh b">max_size</code>设置最大词汇量，<code class="fe oe of og nh b">min_freq</code>设置一个单词在语料库中出现的最少次数。</a></p><p id="33ef" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">字段<code class="fe oe of og nh b">toxic</code>、<code class="fe oe of og nh b">severe_toxic</code>、<code class="fe oe of og nh b">obscene</code>、<code class="fe oe of og nh b">threat</code>、<code class="fe oe of og nh b">insult</code>、<code class="fe oe of og nh b">identity_hate</code>为二元变量。稍后我们需要将它们组合在一起作为模型的目标。</p><p id="20c2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><code class="fe oe of og nh b">data.TabularDataset.splits</code>做的基本和<code class="fe oe of og nh b">data.TabularDataset.__init__</code>一样，但是同时读取多个文件。测试数据集在单独的调用中加载，因为它没有目标列。我们必须在csv文件中以精确的顺序指定字段，并且不能跳过任何列。因此，我们必须明确指定<code class="fe oe of og nh b">("id", None)</code>来跳过第一列。</p><h2 id="e515" class="mi mj iq bd mk ml mm dn mn mo mp dp mq la mr ms mt le mu mv mw li mx my mz na bi translated">创建批处理并遍历数据集</h2><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="669d" class="mi mj iq nh b gy nl nm l nn no">def get_iterator(dataset, batch_size, train=True, <br/>    shuffle=True, repeat=False):<br/>    dataset_iter = data.Iterator(<br/>        dataset, batch_size=batch_size, device=0,<br/>        train=train, shuffle=shuffle, repeat=repeat,<br/>        sort=False<br/>    )<br/>    return dataset_iter</span></pre><p id="ad2c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这应该很简单。它会返回一些我们可以迭代的东西，当整个数据集都被读取后就会停止。我们还没有使用高级的<code class="fe oe of og nh b">sort</code>特性，因为我们使用的是固定长度，我们可能不需要这样做。</p><p id="3e9f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面是一个简单的用法示例(针对一个时期):</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="6764" class="mi mj iq nh b gy nl nm l nn no">for examples in get_iterator(<br/>            self.train_dataset, batch_size, train=True,<br/>            shuffle=True, repeat=False<br/>        ):<br/>    x = examples.comment_text # (fix_length, batch_size) Tensor<br/>    y = torch.stack([<br/>        examples.toxic, examples.severe_toxic, <br/>        examples.obscene,<br/>        examples.threat, examples.insult, <br/>        examples.identity_hate<br/>    ], dim=1)</span></pre><p id="1da6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">并使用加载的预训练向量(假设您的单词嵌入位于<code class="fe oe of og nh b">model.word_em</code>，训练数据集作为<code class="fe oe of og nh b">train_dataset</code>加载):</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="bbcb" class="mi mj iq nh b gy nl nm l nn no">model.word_em.weight.data = \<br/>    train_dataset.fields["comment_text"].vocab.vectors</span></pre><p id="a23e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">就是这样！我们差不多已经有了开始构建和训练模型所需的东西。</p><p id="1c9d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> 20180207更新:</strong>我注意到<strong class="kt ir"> </strong> <code class="fe oe of og nh b">get_iterator</code>既没有<code class="fe oe of og nh b">random_state</code>也没有<code class="fe oe of og nh b">seed</code>参数。实际上是靠<code class="fe oe of og nh b">random</code>内置模块来管理随机性。因此，您需要执行以下操作之一来获得不同时期之间的不同批次:</p><ol class=""><li id="be60" class="nq nr iq kt b ku kv kx ky la ns le nt li nu lm nv nw nx ny bi translated">在历元之间用不同的种子运行<code class="fe oe of og nh b">random.seed(seed)</code>(在调用<code class="fe oe of og nh b">get_iterator</code>之前)</li><li id="f4c7" class="nq nr iq kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated">在<code class="fe oe of og nh b">get_iterator</code>中使用<code class="fe oe of og nh b">repeat=True</code>。您必须自己在循环内部定义一个停止标准，否则for循环将永远继续下去。</li></ol><h2 id="e265" class="mi mj iq bd mk ml mm dn mn mo mp dp mq la mr ms mt le mu mv mw li mx my mz na bi translated">未完待续…</h2><p id="e19b" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">上述方法存在一个重大问题。真的很慢。在我的电脑上，整个数据集加载过程大约需要7分钟，而实际的模型训练大约需要10分钟。我们应该序列化标记化的序列，也许还应该序列化词汇表，以使它更快。一旦我想通了，我打算在下一篇文章中写下如何去做。</p><p id="0eba" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">除了序列化，许多事情也可以改进。例如，一个更灵活的训练/验证分割方案会有很大帮助。高级排序机制和压缩序列可能也值得探索。</p><h2 id="dad1" class="mi mj iq bd mk ml mm dn mn mo mp dp mq la mr ms mt le mu mv mw li mx my mz na bi translated">附加链接</h2><p id="a23f" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">更详细的教程:</p><div class="lp lq gp gr lr ls"><a href="http://anie.me/On-Torchtext/" rel="noopener  ugc nofollow" target="_blank"><div class="lt ab fo"><div class="lu ab lv cl cj lw"><h2 class="bd ir gy z fp lx fr fs ly fu fw ip bi translated">Torchtext教程</h2><div class="lz l"><h3 class="bd b gy z fp lx fr fs ly fu fw dk translated">大约2-3个月前，我遇到了这个库:Torchtext。我漫不经心地浏览了自述文件，意识到…</h3></div><div class="ma l"><p class="bd b dl z fp lx fr fs ly fu fw dk translated">阿尼.我</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om kp ls"/></div></div></a></div><p id="bd30" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">第二部分出版:</p><div class="lp lq gp gr lr ls"><a href="https://medium.com/@ceshine/use-torchtext-to-load-nlp-datasets-part-ii-f146c8b9a496" rel="noopener follow" target="_blank"><div class="lt ab fo"><div class="lu ab lv cl cj lw"><h2 class="bd ir gy z fp lx fr fs ly fu fw ip bi translated">使用torchtext加载NLP数据集—第二部分</h2><div class="lz l"><h3 class="bd b gy z fp lx fr fs ly fu fw dk translated">序列化和更容易的交叉验证</h3></div><div class="ma l"><p class="bd b dl z fp lx fr fs ly fu fw dk translated">medium.com</p></div></div><div class="oh l"><div class="on l oj ok ol oh om kp ls"/></div></div></a></div><h2 id="7391" class="mi mj iq bd mk ml mm dn mn mo mp dp mq la mr ms mt le mu mv mw li mx my mz na bi translated">Python环境</h2><ul class=""><li id="a49b" class="nq nr iq kt b ku nb kx nc la oo le op li oq lm or nw nx ny bi translated">Python 3.6</li><li id="5d7c" class="nq nr iq kt b ku nz kx oa la ob le oc li od lm or nw nx ny bi translated">数字版本1.14.0</li><li id="4349" class="nq nr iq kt b ku nz kx oa la ob le oc li od lm or nw nx ny bi translated">熊猫</li><li id="f406" class="nq nr iq kt b ku nz kx oa la ob le oc li od lm or nw nx ny bi translated">PyTorch 0.4.0a0+f83ca63(应该很接近0.3.0)</li><li id="2f24" class="nq nr iq kt b ku nz kx oa la ob le oc li od lm or nw nx ny bi translated">火炬文本0.2.1</li><li id="cfc4" class="nq nr iq kt b ku nz kx oa la ob le oc li od lm or nw nx ny bi translated">空间2.0.5</li></ul><h2 id="c27b" class="mi mj iq bd mk ml mm dn mn mo mp dp mq la mr ms mt le mu mv mw li mx my mz na bi translated">这篇文章中使用的所有代码</h2><p id="21e4" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">(不包括示例用法。)</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="os ot l"/></div></figure></div></div>    
</body>
</html>