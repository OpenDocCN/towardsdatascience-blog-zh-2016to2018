<html>
<head>
<title>An Introductory Example of Bayesian Optimization in Python with Hyperopt</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Hyperopt 实现 Python 中贝叶斯优化的介绍性示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0?source=collection_archive---------1-----------------------#2018-06-28">https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0?source=collection_archive---------1-----------------------#2018-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div class="gh gi io"><img src="../Images/07dcda6f037f68b95bddc8cf885613c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*Qfk1dEE_Ipfx8v9MLsdmfA.jpeg"/></div></figure><div class=""/><div class=""><h2 id="0c3f" class="pw-subtitle-paragraph ju iw ix bd b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dk translated">学习强大优化框架基础的实践示例</h2></div><p id="4d6d" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">虽然寻找一个函数的最小值可能看起来很平常，但这是一个延伸到许多领域的关键问题。例如，优化机器学习模型的超参数只是一个最小化问题:这意味着搜索验证损失最低的超参数。</p><p id="0154" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated"><a class="ae li" href="https://sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf" rel="noopener ugc nofollow" target="_blank">贝叶斯优化</a>是一种基于概率模型的方法，用于找到任何返回实值度量的函数的最小值。该函数可以简单到 f(x) = x，也可以复杂到深度神经网络关于数百个模型架构和超参数选择的验证误差。</p><p id="0d54" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated"><a class="ae li" href="http://proceedings.mlr.press/v28/bergstra13.pdf" rel="noopener ugc nofollow" target="_blank">最近的结果</a>表明机器学习模型的<a class="ae li" href="https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf" rel="noopener ugc nofollow" target="_blank">贝叶斯超参数优化</a>比手动、随机或网格搜索更有效，具有:</p><ul class=""><li id="bdf8" class="lj lk ix ko b kp kq ks kt kv ll kz lm ld ln lh lo lp lq lr bi translated"><strong class="ko iy">测试集上更好的整体性能</strong></li><li id="0c13" class="lj lk ix ko b kp ls ks lt kv lu kz lv ld lw lh lo lp lq lr bi translated"><strong class="ko iy">优化所需的时间更少</strong></li></ul><p id="f88e" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">很明显，这么强大的方法一定很难使用，对吗？幸运的是，有许多 Python 库，比如<a class="ae li" href="https://hyperopt.github.io/hyperopt/" rel="noopener ugc nofollow" target="_blank"> Hyperopt </a>，允许简单应用贝叶斯优化。其实一行就可以做基本的贝叶斯优化！</p><figure class="lx ly lz ma gt is"><div class="bz fp l di"><div class="mb mc l"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Bayesian Optimization of a 1-D polynomial</figcaption></figure><p id="9e68" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">如果你能理解上面代码中的一切，那么你大概可以停止阅读，开始使用这种方法。如果你想要更多的解释，在这篇文章中，我们将通过一个 Hyperopt 程序的基本结构，以便稍后我们可以将这个框架扩展到更复杂的问题，如机器学习超参数优化。这篇文章的代码可以在 GitHub 上的<a class="ae li" href="https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Introduction%20to%20Bayesian%20Optimization%20with%20Hyperopt.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本中找到。</a></p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h2 id="0693" class="mo mp ix bd mq mr ms dn mt mu mv dp mw kv mx my mz kz na nb nc ld nd ne nf ng bi translated">贝叶斯优化入门</h2><p id="dade" class="pw-post-body-paragraph km kn ix ko b kp nh jy kr ks ni kb ku kv nj kx ky kz nk lb lc ld nl lf lg lh ij bi translated">优化是找到产生最低输出值的<a class="ae li" href="https://www.courses.psu.edu/for/for466w_mem14/Ch11/HTML/Sec1/ch11sec1_ObjFn.htm" rel="noopener ugc nofollow" target="_blank">目标函数</a>的输入值或一组值，称为“损失”。目标函数 f(x) = x 具有单个输入，并且是一维优化问题。<a class="ae li" href="https://arxiv.org/abs/1502.02127" rel="noopener ugc nofollow" target="_blank">通常，在机器学习</a>中，我们的目标函数是多维的，因为它接受一组模型超参数。对于低维中的简单函数，我们可以通过尝试许多输入值并查看哪一个产生最低损失来找到最小损失。我们可以创建一个输入值的网格，并尝试所有的值—网格搜索—或者随机选取一些值—随机搜索。只要目标函数的评估(“evals”)是廉价的，这些不知情的方法可能是足够的。然而，对于复杂的目标函数，如神经网络的 5 倍交叉验证误差，目标函数的每次评估意味着训练网络 5 次！</p><p id="f54e" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">对于需要几天训练的模型，我们想要一种方法来<em class="nm">限制对评估函数</em>的调用。<a class="ae li" href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank">对于高维问题，随机搜索实际上比网格搜索更有效</a>，但仍然是一种<em class="nm">统一方法</em>，其中搜索不使用先前的结果来挑选下一个输入值进行尝试。让我们看看你是否比随机搜索更聪明。假设我们从为回归任务训练随机森林中得到以下结果:</p><figure class="lx ly lz ma gt is gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/0afccd75b539c55ffc25b67ceb0f0057.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*2qDZxQkRoP28CidZtoT-gQ.png"/></div></figure><p id="871f" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">如果你选择下一批要评估的树，你会集中在哪里？很明显，最好的选择是 100 棵树左右，因为数量越少，损失越小。你基本上已经在你的头脑中完成了贝叶斯优化:使用先前的结果，你形成了目标函数的<a class="ae li" href="http://www.statisticshowto.com/probabilistic/" rel="noopener ugc nofollow" target="_blank">概率模型</a>,该模型表示更少数量的树可能导致更低的误差。</p><p id="2343" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated"><a class="ae li" href="https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf" rel="noopener ugc nofollow" target="_blank">贝叶斯优化</a>，也称为<a class="ae li" href="https://arimo.com/data-science/2016/bayesian-optimization-hyperparameter-tuning/" rel="noopener ugc nofollow" target="_blank">基于序列模型的优化(SMBO) </a>，通过建立目标函数的概率模型来实现这一思想，该模型将输入值映射到损失概率:p(损失|输入值)。概率模型，也称为<a class="ae li" href="https://en.wikipedia.org/wiki/Surrogate_model" rel="noopener ugc nofollow" target="_blank">代理或响应面</a>，比实际的目标函数更容易优化。贝叶斯方法通过将一个标准(通常是<a class="ae li" href="https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf" rel="noopener ugc nofollow" target="_blank">预期改进</a>)应用于代理来选择下一个要评估的值。<strong class="ko iy">其概念是通过花更多时间选择下一个要尝试的值来限制目标函数的评估。</strong></p><p id="2444" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated"><a class="ae li" href="https://en.wikipedia.org/wiki/Bayesian_inference" rel="noopener ugc nofollow" target="_blank">贝叶斯推理</a>意味着基于新的证据更新模型，并且，随着每次评估，替代物被重新计算以结合最新的信息。算法运行的时间越长，代理函数就越接近实际的目标函数。<a class="ae li" href="https://static.sigopt.com/b/20a144d208ef255d3b981ce419667ec25d8412e2/pdf/SigOpt_Bayesian_Optimization_Primer.pdf" rel="noopener ugc nofollow" target="_blank">贝叶斯优化方法</a>在构造替代函数的方式上有所不同:常见的选择包括高斯过程、随机森林回归，以及在 Hyperopt 中的选择<a class="ae li" href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf" rel="noopener ugc nofollow" target="_blank"> Tree Parzen 估计器(TPE) </a>。</p><p id="9918" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">这些方法的细节可能有点难以理解(我在这里写了一个高层次的<a class="ae li" rel="noopener" target="_blank" href="/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f">概述)，并且也很难找出哪个工作得最好:如果你阅读算法设计者的文章，每个人都声称他们的方法是优越的！然而，特定的算法并不像从随机/网格搜索升级到贝叶斯优化那样重要。使用任何库(</a><a class="ae li" href="https://github.com/HIPS/Spearmint" rel="noopener ugc nofollow" target="_blank">留兰香</a>，远视，<a class="ae li" href="https://www.cs.ubc.ca/labs/beta/Projects/SMAC/" rel="noopener ugc nofollow" target="_blank"> SMAC </a>)都可以上手！记住这一点，让我们看看如何将贝叶斯优化付诸实践。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h2 id="862e" class="mo mp ix bd mq mr ms dn mt mu mv dp mw kv mx my mz kz na nb nc ld nd ne nf ng bi translated">Hyperopt 中的优化示例</h2><p id="bd1a" class="pw-post-body-paragraph km kn ix ko b kp nh jy kr ks ni kb ku kv nj kx ky kz nk lb lc ld nl lf lg lh ij bi translated">在 hyperpt 中公式化一个优化问题<a class="ae li" href="https://hyperopt.github.io/hyperopt/" rel="noopener ugc nofollow" target="_blank">需要四个部分:</a></p><ol class=""><li id="5ca5" class="lj lk ix ko b kp kq ks kt kv ll kz lm ld ln lh no lp lq lr bi translated"><strong class="ko iy">目标函数:</strong>接受一个输入并返回一个损失以最小化</li><li id="da54" class="lj lk ix ko b kp ls ks lt kv lu kz lv ld lw lh no lp lq lr bi translated"><strong class="ko iy">域空间:</strong>要评估的输入值的范围</li><li id="d62b" class="lj lk ix ko b kp ls ks lt kv lu kz lv ld lw lh no lp lq lr bi translated"><strong class="ko iy">优化算法:</strong>用于构造代理函数和选择下一个要评估的值的方法</li><li id="c964" class="lj lk ix ko b kp ls ks lt kv lu kz lv ld lw lh no lp lq lr bi translated"><strong class="ko iy">结果:</strong>算法用来建立模型的分数、值对</li></ol><p id="2deb" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">一旦我们知道如何指定这四个部分，它们就可以应用于任何优化问题。现在，我们将讨论一个基本问题。</p><h2 id="fd1a" class="mo mp ix bd mq mr ms dn mt mu mv dp mw kv mx my mz kz na nb nc ld nd ne nf ng bi translated">目标函数</h2><p id="c3fb" class="pw-post-body-paragraph km kn ix ko b kp nh jy kr ks ni kb ku kv nj kx ky kz nk lb lc ld nl lf lg lh ij bi translated">目标函数可以是任何返回我们想要最小化的真实值的函数。(如果我们有一个想要最大化的值，比如精度，那么我们只需要让函数返回这个度量的负值。)</p><p id="979a" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">这里我们将使用多项式函数，代码和图形如下所示:</p><figure class="lx ly lz ma gt is"><div class="bz fp l di"><div class="mb mc l"/></div></figure><figure class="lx ly lz ma gt is gh gi paragraph-image"><div class="gh gi np"><img src="../Images/08773a2b0cc6c26a18263ac607f9ec0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*iNiCVxg59vMryDRouqUzoA.png"/></div></figure><p id="f982" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">这个问题是一维的，因为我们正在优化单个值 x。在 Hyperopt 中，目标函数可以接受任意数量的输入，但必须返回单个损失以最小化。</p><h2 id="865b" class="mo mp ix bd mq mr ms dn mt mu mv dp mw kv mx my mz kz na nb nc ld nd ne nf ng bi translated">领域空间</h2><p id="28ca" class="pw-post-body-paragraph km kn ix ko b kp nh jy kr ks ni kb ku kv nj kx ky kz nk lb lc ld nl lf lg lh ij bi translated">域空间是我们想要搜索的输入值。作为第一次尝试，我们可以在函数定义的范围内使用均匀分布:</p><pre class="lx ly lz ma gt nq nr ns nt aw nu bi"><span id="e84d" class="mo mp ix nr b gy nv nw l nx ny">from hyperopt import hp</span><span id="e17f" class="mo mp ix nr b gy nz nw l nx ny"># Create the domain space<br/>space = hp.uniform('x', -5, 6)</span></pre><p id="8b0b" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">为了可视化该域，我们可以从空间中抽取样本并绘制直方图:</p><figure class="lx ly lz ma gt is gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/77fdad74c17870211ffef554400838f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*xfcy8-0DU1LBUuzTVnpqGA.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Uniform domain space</figcaption></figure><p id="31b0" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">如果我们知道最佳值在哪里，那么我们可以创建一个更智能的域，将更多的概率放在得分更高的区域。(关于在这个问题上使用正态分布的例子，见笔记本<a class="ae li" href="https://github.com/WillKoehrsen/hyperparameter-optimization" rel="noopener ugc nofollow" target="_blank">。)</a></p><h2 id="06c6" class="mo mp ix bd mq mr ms dn mt mu mv dp mw kv mx my mz kz na nb nc ld nd ne nf ng bi translated">最优化算法</h2><p id="42cd" class="pw-post-body-paragraph km kn ix ko b kp nh jy kr ks ni kb ku kv nj kx ky kz nk lb lc ld nl lf lg lh ij bi translated">虽然这在技术上是最困难的概念，但在 Hyperopt 中创建一个优化算法只需要一行代码。我们正在使用<a class="ae li" href="https://github.com/hyperopt/hyperopt/blob/master/hyperopt/tpe.py" rel="noopener ugc nofollow" target="_blank">树形结构的 Parzen 评估模型</a>，我们可以让 Hyperopt 使用<code class="fe ob oc od nr b">suggest</code>方法为我们配置它。</p><pre class="lx ly lz ma gt nq nr ns nt aw nu bi"><span id="3134" class="mo mp ix nr b gy nv nw l nx ny">from hyperopt import tpe</span><span id="7e11" class="mo mp ix nr b gy nz nw l nx ny"># Create the algorithm<br/>tpe_algo = tpe.suggest</span></pre><p id="2f62" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">幕后有很多我们不必担心的理论！在笔记本中，我们也使用随机搜索算法进行比较。</p><h2 id="febb" class="mo mp ix bd mq mr ms dn mt mu mv dp mw kv mx my mz kz na nb nc ld nd ne nf ng bi translated">结果(试验)</h2><p id="a02a" class="pw-post-body-paragraph km kn ix ko b kp nh jy kr ks ni kb ku kv nj kx ky kz nk lb lc ld nl lf lg lh ij bi translated">这并不是绝对必要的，因为 Hyperopt 会在内部跟踪算法的结果。然而，如果我们想要检查算法的进展，我们需要创建一个<code class="fe ob oc od nr b">Trials</code>对象来记录值和分数:</p><pre class="lx ly lz ma gt nq nr ns nt aw nu bi"><span id="225f" class="mo mp ix nr b gy nv nw l nx ny">from hyperopt import Trials</span><span id="e975" class="mo mp ix nr b gy nz nw l nx ny"># Create a trials object<br/>tpe_trials = Trials()</span></pre><h2 id="db65" class="mo mp ix bd mq mr ms dn mt mu mv dp mw kv mx my mz kz na nb nc ld nd ne nf ng bi translated">最佳化</h2><p id="d832" class="pw-post-body-paragraph km kn ix ko b kp nh jy kr ks ni kb ku kv nj kx ky kz nk lb lc ld nl lf lg lh ij bi translated">现在问题定义好了，就可以最小化我们的目标函数了！为此，我们使用<code class="fe ob oc od nr b">fmin</code>函数，该函数接受上述四个部分，以及最大数量的试验:</p><figure class="lx ly lz ma gt is"><div class="bz fp l di"><div class="mb mc l"/></div></figure><pre class="lx ly lz ma gt nq nr ns nt aw nu bi"><span id="96a0" class="mo mp ix nr b gy nv nw l nx ny"><strong class="nr iy">{'x': 4.878208088771056}</strong></span></pre><p id="07da" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">对于这次运行，该算法在不到 1000 次试验中找到了 x 的最佳值(使损失最小化的值)。最佳对象只返回使函数最小化的输入值。虽然这是我们正在寻找的，但它并没有给我们太多的方法。要获得更多细节，我们可以从 trials 对象获得结果:</p><figure class="lx ly lz ma gt is"><div class="bz fp l di"><div class="mb mc l"/></div></figure><figure class="lx ly lz ma gt is gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/cef5c4cde7fd394d8e4136680e87ad18.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*cF4PRvXaz0oeDn9r1aANOg.png"/></div></figure><p id="a447" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">可视化对于直观理解正在发生的事情很有用。例如，让我们按顺序绘制 x 的值:</p><figure class="lx ly lz ma gt is gh gi paragraph-image"><div class="gh gi of"><img src="../Images/0b9858ce8c13dc4074735031a9fb9a3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*rNcXXToCHfFvN6zTEgX5mg.png"/></div></figure><p id="00a4" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">随着时间的推移，输入值聚集在红线所示的最佳值周围。这是一个简单的问题，因此算法在寻找 x 的最佳值时没有太大的困难。</p><p id="c22e" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">为了与简单的搜索形成对比，如果我们用随机搜索运行同样的问题，我们会得到下图:</p><figure class="lx ly lz ma gt is gh gi paragraph-image"><div class="gh gi of"><img src="../Images/8f9d45d053c4b3a2dce9cd6ec470eca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*FPBWcAEGPUAK81KWyqpNAA.png"/></div></figure><p id="032a" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">随机搜索基本上是随机尝试值！当我们查看 TPE 算法和随机搜索的 x 值的直方图时，这些值之间的差异变得更加明显:</p><div class="lx ly lz ma gt ab cb"><figure class="og is oh oi oj ok ol paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><img src="../Images/97b9df1c3455fa93ddca5fdd81a02142.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*fqs6C_hylKgFV1SOtDlBpg.png"/></div></figure><figure class="og is oq oi oj ok ol paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><img src="../Images/7966669651f5e969a89a0d5743d2ff49.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*VtPt7tt6Ya4QsYnxSewb_Q.png"/></div></figure></div><p id="e730" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">在这里，我们看到了基于贝叶斯模型的优化的主要好处:更加关注有希望的输入值。当我们搜索几十个参数，并且每个评估需要几个小时或几天的时间时，减少评估的次数是至关重要的。贝叶斯优化通过基于以前的结果推理将来应该尝试什么输入值来最小化评估的次数。</p><p id="531d" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">(在这种情况下，由于基本的一维目标函数和评估次数，随机搜索实际上找到了非常接近最优的 x 值。)</p><h2 id="f5dc" class="mo mp ix bd mq mr ms dn mt mu mv dp mw kv mx my mz kz na nb nc ld nd ne nf ng bi translated">后续步骤</h2><p id="7f14" class="pw-post-body-paragraph km kn ix ko b kp nh jy kr ks ni kb ku kv nj kx ky kz nk lb lc ld nl lf lg lh ij bi translated">一旦我们掌握了如何最小化一个简单的函数，我们就可以把它扩展到任何需要优化一个返回实值的函数的问题。例如，调整机器学习模型的超参数只需要对基本框架进行一些调整:目标函数必须接受模型超参数并返回验证损失，域空间需要特定于模型。</p><p id="ce0a" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">为了了解这看起来像什么，我写了一个笔记本，在那里我调整了梯度推进机器的超参数，这将是下一篇文章！</p><h1 id="000f" class="or mp ix bd mq os ot ou mt ov ow ox mw kd oy ke mz kg oz kh nc kj pa kk nf pb bi translated">结论</h1><p id="4895" class="pw-post-body-paragraph km kn ix ko b kp nh jy kr ks ni kb ku kv nj kx ky kz nk lb lc ld nl lf lg lh ij bi translated">基于贝叶斯模型的优化是直观的:根据过去的结果选择下一个输入值进行评估，以集中搜索更有希望的值。最终结果是，与无信息随机或网格搜索方法相比，减少了搜索迭代的总次数。虽然这只是一个简单的例子，但是我们可以将这里的概念用于各种有用的情况。</p><p id="0970" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">这篇文章的要点是:</p><ol class=""><li id="b4fd" class="lj lk ix ko b kp kq ks kt kv ll kz lm ld ln lh no lp lq lr bi translated">贝叶斯优化是一种通过构建目标函数的概率(替代)模型来寻找函数最小值的有效方法</li><li id="d27c" class="lj lk ix ko b kp ls ks lt kv lu kz lv ld lw lh no lp lq lr bi translated">代理由过去的搜索结果通知，并且通过从该模型中选择下一个值，搜索集中在有希望的值上</li><li id="6b53" class="lj lk ix ko b kp ls ks lt kv lu kz lv ld lw lh no lp lq lr bi translated">这些方法的总体结果是减少搜索时间和更好的价值</li><li id="be65" class="lj lk ix ko b kp ls ks lt kv lu kz lv ld lw lh no lp lq lr bi translated">这些强大的技术可以在 Hyperopt 这样的 Python 库中轻松实现</li><li id="a07d" class="lj lk ix ko b kp ls ks lt kv lu kz lv ld lw lh no lp lq lr bi translated">贝叶斯优化框架可以扩展到复杂的问题，包括机器学习模型的超参数调整</li></ol><p id="8bb0" class="pw-post-body-paragraph km kn ix ko b kp kq jy kr ks kt kb ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">一如既往，我欢迎反馈和建设性的批评。可以通过推特<a class="ae li" href="http://twitter.com/@koehrsen_will" rel="noopener ugc nofollow" target="_blank"> @koehrsen_will </a>联系到我。</p></div></div>    
</body>
</html>