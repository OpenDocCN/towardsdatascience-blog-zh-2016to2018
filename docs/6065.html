<html>
<head>
<title>Planning by Dynamic Programming: Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">动态规划规划:强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/planning-by-dynamic-programming-reinforcement-learning-ed4924bbaa4c?source=collection_archive---------4-----------------------#2018-11-24">https://towardsdatascience.com/planning-by-dynamic-programming-reinforcement-learning-ed4924bbaa4c?source=collection_archive---------4-----------------------#2018-11-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cb74" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">第三部分:解释动态规划、策略评估、策略迭代和价值迭代的概念</h2></div><p id="411d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇博文中，我将解释如何使用动态编程来评估和寻找最优策略。这一系列的博客文章包含了 David Silver 在<a class="ae lb" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" rel="noopener ugc nofollow" target="_blank">强化学习简介</a>中解释的概念总结。</p><p id="f10e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">零件:<a class="ae lb" rel="noopener" target="_blank" href="/reinforcement-learning-an-introduction-to-the-concepts-applications-and-code-ced6fbfd882d">1</a><a class="ae lb" rel="noopener" target="_blank" href="/getting-started-with-markov-decision-processes-reinforcement-learning-ada7b4572ffb">2</a>3<a class="ae lb" rel="noopener" target="_blank" href="/model-free-prediction-reinforcement-learning-507297e8e2ad">4</a>…</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/66897864fa59a4bc1206cca0de233cd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LVbN-5hPzwWodxhN"/></div></div></figure><p id="2afb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在将使用前面部分中讨论的 MDPs 和 Bellman 方程等概念来确定给定策略有多好，以及如何在马尔可夫决策过程中找到最优策略。</p><h1 id="fd67" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">动态规划</h1><p id="0968" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated"><em class="ml">动态规划</em>是一种通过将复杂问题分解为子问题来解决复杂问题的方法。子问题的解决方案被组合以解决整体问题。</p><p id="32c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">动态编程的两个必需属性是:</p><ul class=""><li id="17ee" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated"><strong class="kh ir"> <em class="ml">最优子结构</em> </strong>:子问题的最优解可以用来解决整体问题。</li><li id="3500" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><strong class="kh ir"> <em class="ml">重叠子问题</em> </strong>:子问题多次重复出现。子问题的解决方案可以被缓存和重用</li></ul><p id="4316" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">马尔可夫决策过程满足这两个性质:</p><ul class=""><li id="b4fd" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated"><em class="ml">贝尔曼方程给出了递归分解</em>，它告诉我们如何将最优值函数分解成两部分。即一个步骤的最佳行为，然后是剩余步骤的最佳值。</li><li id="8fcb" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><em class="ml">价值函数存储并重用解决方案</em>。缓存所有 MDP 的好信息，告诉你从那个状态开始你能得到的最佳回报。</li></ul><h1 id="a604" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">动态规划规划</h1><p id="657c" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">当有人告诉我们 MDP 的结构(即当我们知道转移结构、奖励结构等)时，动态规划可以用来解决强化学习问题。).因此用动态规划对<strong class="kh ir"> <em class="ml">规划</em> </strong>中的一个 MDP 进行求解:</p><ul class=""><li id="ea5e" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated"><strong class="kh ir">预测问题</strong> <em class="ml">(政策评估)</em>:</li></ul><p id="f3da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给定一个 MDP<em class="ml">S，一个 P，一个 R，一个γ&gt;T31】和一个策略<em class="ml"> π </em>。求价值函数<em class="ml"> v_π </em>(它告诉你在每种状态下你将要得到多少奖励)。也就是说，目标是找出一个政策<em class="ml"> π </em>有多好<em class="ml">。</em></em></p><ul class=""><li id="6696" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated"><strong class="kh ir">控制问题</strong>(找到在 MDP 最好做的事情):</li></ul><p id="2f3f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给定 MDP <em class="ml"> &lt; S，A，P，R，γ &gt;。</em>求最优值函数<em class="ml"> v_π和</em>最优策略<em class="ml"> π*。</em>也就是说，我们的目标是找到能给你带来最大回报的政策，并选择最佳行动。</p><h2 id="0f06" class="na lp iq bd lq nb nc dn lu nd ne dp ly ko nf ng ma ks nh ni mc kw nj nk me nl bi translated">政策评价</h2><p id="9f0e" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated"><strong class="kh ir"> <em class="ml">问题</em> </strong> <em class="ml"> : </em>评估一个给定的策略<em class="ml"> π </em>和 MDP。(找出一个政策<em class="ml"> π </em>有多好)<br/> <strong class="kh ir"> <em class="ml">解</em> </strong>:贝尔曼期望备份的迭代应用。</p><p id="dbd2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="ml">方法:<br/> </em> </strong>从初始值函数<em class="ml">v₁</em>(MDP 中所有状态的一个值)开始。例如以值 0 开始。因此没有奖励。然后用<em class="ml">贝尔曼期望方程</em>计算<em class="ml"> v₂ </em>，重复多次，最终收敛到<em class="ml"> v_π。</em></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/6bd8be007914e4c82762b3339f5812cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*ooZp_GfMNZGgz2KFBLF1hw.png"/></div></figure><p id="6a08" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实现这种收敛的一种方法是使用<em class="ml">同步备份</em>，其中我们在每一步都考虑所有状态。</p><ol class=""><li id="a478" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la nn ms mt mu bi translated">在每次迭代 k+1 时，对于所有状态 s ∈ S</li><li id="d09f" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la nn ms mt mu bi translated">更新<em class="ml"> vₖ₊₁(s) </em>从<em class="ml">vₖ(s')</em>，其中<em class="ml">s’</em>是<em class="ml"> s </em>的继承状态</li></ol><p id="fcd4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ml"> vₖ(s') </em>使用第 2 部分中讨论的贝尔曼期望方程进行更新。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c6ed9a2b6244253930e55dea68418ccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*Y0572CPmQnHxlkepbrEJSA.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Bellman Expectation Equation</figcaption></figure><p id="36f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="ml">举例:<br/> </em> </strong>假设我们有一个有<em class="ml"> 14 个状态</em>的网格世界，其中每个方块代表一个<strong class="kh ir">状态</strong>。我们还有 2 个<strong class="kh ir">终端状态</strong>，一个在网格世界的右下角，另一个在左上角。我们可以采取将<em class="ml">向上</em>、<em class="ml">向下</em>、<em class="ml">向左</em>和<em class="ml">向右</em>的<strong class="kh ir">动作</strong>，我们采取的每一个过渡都会给我们一个<em class="ml"> -1 </em>的<strong class="kh ir">即时奖励</strong>，直到达到一个终止状态。由于<strong class="kh ir">折扣因子</strong>为<em class="ml"> γ=1 </em>，我们给定的代理策略<em class="ml"> π </em>遵循统一的随机策略:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/008d6a9d11ec8eb55e60177e13294bed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*YmR071Pdj6nmhnwy6cmYhQ.png"/></div></figure><p id="8d95" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们需要计算处于每个状态的值，以确定我们定义的策略<em class="ml"> π </em>有多好。</p><p id="65fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1.按照我们上面描述的方法，我们可以从初始值函数 v₀开始，所有值都是 0。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/26eb59463828511150cd5fd466fbb841.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*dEC_2qJ88A2ZZYUe10awDQ.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">v₀</figcaption></figure><p id="ce59" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.接下来，我们通过应用一步前瞻，使用贝尔曼期望方程来计算新的价值函数 v₁。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/d671342c0d8e33c5e52b99819c48b4bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*pVbecCJI4F98ljJFoMAgJg.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">v₁: Blue block is calculated using the Bellman Expectation Equation such that 0.25(-1+1(0))+0.25(-1+1(0))+0.25(-1+1(0))+0.25(-1+1(0)) = -1</figcaption></figure><p id="d198" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.重复使用 v₁计算 v₂的过程</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/135146e2f905353e2244c5a608815e54.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*X3Mtl115td0lired1eRScg.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">v₂: Blue block is calculated using the Bellman Expectation Equation such that 0.25(-1+1(-1))+0.25(-1+1(-1))+0.25(-1+1(-1))+0.25(-1+1(0))=-1.75</figcaption></figure><p id="568a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">4.用<em class="ml"> k=∞ </em>重复这个过程，最终将我们的价值函数收敛到<em class="ml"> v_π。</em></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/3e26d012e247a03525bedfe4714ab0cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*l543EnTfa-rMu5uHBXFlrA.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">vπ</figcaption></figure><p id="c49c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，对于我们的策略<em class="ml"> π </em>，我们计算了 MDP 中每个州的相关值。</p><p id="0423" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">如何完善一项政策？<br/> </strong>在上面的方法中，我们已经评估了一个给定的策略，但是没有在我们的环境中找到最佳的策略(要采取的行动)。为了改进给定的策略，我们可以<em class="ml">评估</em>给定的策略<em class="ml"> π </em>并通过<em class="ml">相对于<em class="ml"> v_π贪婪地</em>行动来改进策略。</em>这可以使用<strong class="kh ir"> <em class="ml">策略迭代</em> </strong>来完成。</p><h2 id="6398" class="na lp iq bd lq nb nc dn lu nd ne dp ly ko nf ng ma ks nh ni mc kw nj nk me nl bi translated">策略迭代</h2><p id="699e" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated"><strong class="kh ir"> <em class="ml">问题</em> </strong> <em class="ml"> : </em>为给定的 MDP 寻找最佳策略<em class="ml">【π*】</em>。<br/> <strong class="kh ir"> <em class="ml">解法</em> </strong>:贝尔曼期望方程政策迭代与行事贪婪。</p><p id="c17a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="ml">方法:<br/> </em> </strong>以给定策略开始<em class="ml"> π </em></p><ol class=""><li id="e3a6" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la nn ms mt mu bi translated">使用策略评估来评估策略<em class="ml"> π </em>(如上所述)</li><li id="6ef9" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la nn ms mt mu bi translated">通过<strong class="kh ir"> <em class="ml">对<em class="ml"> v_π </em>的贪婪</em> </strong>行为来改进<em class="ml"> π </em>的策略，得到新的策略<em class="ml">π’</em>。</li><li id="1080" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la nn ms mt mu bi translated">重复直到新策略<em class="ml">π’</em>收敛到最优策略<em class="ml"> π* </em>。</li></ol><p id="ac9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了贪婪地行动，我们使用一步前瞻来确定给予我们最大<strong class="kh ir"> <em class="ml">行动值函数</em> </strong>(在第 2 部分中描述)的行动:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/649a84279f67a378fc4b7c547785c51a.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*nJeLoyC8l6tlNATVhLMdxQ.png"/></div></figure><p id="d636" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回想动作值函数有以下等式:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/fa4476585802ee0a072114bd8ed37244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*HAoT2rL868dHGQWlFyIe_A.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">action-value function</figcaption></figure><h2 id="202b" class="na lp iq bd lq nb nc dn lu nd ne dp ly ko nf ng ma ks nh ni mc kw nj nk me nl bi translated"><strong class="ak"> <em class="nz">值迭代</em> </strong></h2><p id="b181" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">控制问题的另一种方法是使用<em class="ml">贝尔曼最优方程</em>进行数值迭代。首先，我们需要定义如何使用<strong class="kh ir"> <em class="ml">最优性原则将一个最优策略分成几个部分。</em> </strong></p><p id="8fcf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">最优性原理<br/> </strong>任何<strong class="kh ir"> <em class="ml">最优策略</em> </strong>都可以细分为使整体行为最优的两个组成部分:<br/> -最优的第一个动作<em class="ml">A∫</em><br/>-随后是后续状态的最优策略<em class="ml"> S </em></p><blockquote class="oa ob oc"><p id="bd8b" class="kf kg ml kh b ki kj jr kk kl km ju kn od kp kq kr oe kt ku kv of kx ky kz la ij bi translated"><strong class="kh ir">定理(最优性原理)<br/> </strong> <em class="iq"> A 策略</em> π(a|s) <em class="iq">从状态 s 达到最优值，</em>v _π(s)= v∫(s)<em class="iq">当且仅当:</em> <br/> - <em class="iq">对于从</em>s<em class="iq">-</em>-<em class="iq">可达的任意状态 s '达到最优值</em></p></blockquote><p id="d65c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="ml">值迭代(应用)</em> </strong></p><p id="a248" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="ml">问题</em> </strong> <em class="ml"> : </em>求给定 MDP 的最优策略<em class="ml">【π*】</em>。<br/> <strong class="kh ir"> <em class="ml">解</em> </strong>:贝尔曼最优备份的迭代应用</p><p id="4828" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="ml">方法:<br/> </em> </strong>利用同步备份更新价值函数，直到计算出最优价值函数而不计算动作价值函数。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi og"><img src="../Images/95576e478481307e5ac5a5704cfc8c14.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*sYD53ERrczzr5roDRAORxw.png"/></div></figure><ol class=""><li id="746b" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la nn ms mt mu bi translated">在每次迭代 k+1 时，对于所有状态 s ∈ S</li><li id="ae94" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la nn ms mt mu bi translated">更新<em class="ml"> vₖ₊₁(s) </em>从<em class="ml">vₖ(s')</em>，其中<em class="ml">s’</em>是<em class="ml"> s </em>的后继状态</li></ol><p id="c60b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ml"> vₖ(s') </em>使用第 2 部分中讨论的贝尔曼最优方程进行更新:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/e79d3791386390ee885077790f2327dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*dRQy7U8YeCtRj2fP0nxFMA.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Bellman Optimality Equation</figcaption></figure><p id="a016" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过重复上述过程，最终收敛到<em class="ml"> v*。</em>注意，该过程不同于策略迭代，因为中间值函数可能不对应于任何策略。</p><h1 id="856f" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">摘要</h1><p id="0d83" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">这篇文章中描述的所有算法都是强化学习中规划问题的解决方案(这里我们给出了 MDP)。这些规划问题(预测和控制)可以使用同步动态规划算法来解决。</p><p id="a362" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">预测</strong>问题可以用<em class="ml">贝尔曼期望方程迭代</em> ( <strong class="kh ir"> <em class="ml">政策评估</em> </strong>)来解决。</p><p id="b3c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">控制</strong>问题可以用<em class="ml">贝尔曼期望方程策略迭代&amp;贪婪</em> ( <strong class="kh ir"> <em class="ml">策略改进</em> </strong>)或<em class="ml">贝尔曼最优方程</em> ( <strong class="kh ir"> <em class="ml">值迭代</em> </strong>)来解决。</p><h1 id="b00b" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">参考</h1><ul class=""><li id="7a30" class="mm mn iq kh b ki mg kl mh ko oi ks oj kw ok la mr ms mt mu bi translated"><a class="ae lb" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf" rel="noopener ugc nofollow" target="_blank">关于 RL 的 UCL 课程——第 3 讲</a></li><li id="c033" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">《强化学习导论》，萨顿和巴尔托，1998 年</li></ul></div><div class="ab cl ol om hu on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="ij ik il im in"><p id="f3dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你喜欢这篇文章，并想看到更多，不要忘记关注和/或留下掌声。</p></div></div>    
</body>
</html>