<html>
<head>
<title>Tell Me a Story: Thoughts on Model Interpretability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">给我讲个故事:关于模型可解释性的思考</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tell-me-a-story-thoughts-on-model-interpretability-37a03ed41440?source=collection_archive---------7-----------------------#2018-01-09">https://towardsdatascience.com/tell-me-a-story-thoughts-on-model-interpretability-37a03ed41440?source=collection_archive---------7-----------------------#2018-01-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="fe28" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最近，我的思考围绕着感觉像是机器学习的一些最大的元对话:学习一个普遍智能的演员的潜力和局限性，算法公平性的细微差别和真正的规范挑战，以及现在，模型对人类来说是可解释和可理解的意味着什么。</p><p id="527f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着思想市场被越来越多的<a class="ae kl" href="https://devblogs.nvidia.com/wp-content/uploads/2015/08/image6-624x172.png" rel="noopener ugc nofollow" target="_blank"/>-<a class="ae kl" href="https://cdn-images-1.medium.com/max/1200/1*2ns4ota94je5gSVjrpFq3A.png" rel="noopener">-</a>-<a class="ae kl" href="https://cdn-images-1.medium.com/max/1600/1*KOjUX1ST5RnDOZWWLWRGkw.png" rel="noopener"/>复杂架构充斥，似乎在所有这些层的重压下摇摇欲坠，呼吁可解释机器学习的声音越来越大。NIPS在过去两年里主办了一个关于这个主题的研讨会，DARPA正在资助一个可解释的人工智能项目，欧洲许多国家都在争先恐后地应对欧盟的新要求，即模型做出的任何“重大决定”都必须向用户解释。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/6ac2bd9258bb04fde42046b0c32606c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sQS7PLi3yfNypQTTSWBfAw.jpeg"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Oh no! I think that heatmap of pixel importances is in the shape of a Grim</figcaption></figure><p id="cf26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我发现这些争论通常分为三大类。</p><ol class=""><li id="fc1f" class="lc ld iq jp b jq jr ju jv jy le kc lf kg lg kk lh li lj lk bi translated">对模型稳定性的担心，以及检查脆弱或不正确的概括的愿望:这种心态主要是担心，在缺乏对模型决策进行反思的能力的情况下，它可能会以最终不适当的方式拾取模式。这方面的一个常见例子是一个模型，该模型了解到哮喘患者的死亡率较低，因为医院的政策是总是将这些患者紧急送入重症监护。当模型被用在关键场景中时，这种检查为什么模型知道它拥有什么的“原因”的能力是至关重要的。事实也是如此，因为这种模型经常由熟练的专业人员实施，他们对部署一个他们认为无法担保的系统感到不舒服。</li><li id="ae56" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated"><strong class="jp ir">希望可以从这些模型中提炼出新的知识:以给人类新的概念洞察力的方式:</strong>这主要是你从应用科学中使用机器学习的人那里听到的观点。在这里，可解释性被视为一种强大的附加值，因为如果我们可以将这些机器抽象转化为人类有意义的概念，我们就可以将这些概念编织到我们现有的知识网中，并使用它来推动更多的研究。</li><li id="a172" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated">一种对我们所做的决定进行逻辑论证的权利感:这种感觉有点难以解释，但确实有一些情感上的显著性。这与关于机器偏见的争论有关，的确，许多从这个角度争论的人担心黑盒内的模型以我们社会认定不恰当的方式使用信息。但是，我认为这里有一个更广泛的公平概念:一个人应该能够在做出决定时要求理由的信念——贷款，缓刑听证会。当有人给你理由的时候，你可以抓住这些理由，潜在地反驳它们。当一个模型给了你一个不可接受的、不透明的决定时，我可以理解当你不能“为自己辩护”(如果有正当理由的话)反对所使用的逻辑时，你会有一种专制的感觉</li></ol><h1 id="ef6f" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated"><strong class="ak">为什么复杂性会降低可解释性？</strong></h1><p id="b2dd" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">冒着听起来迂腐的风险，为什么ResNet是不可解释的？它缺少什么根本素质？这个系统是一个完全确定的系统，一旦训练完毕，你就可以写出方程，将模型中的每一个量与其他量联系起来。我觉得这里根本缺失的一块，跟模型的缺点关系不大，跟人类认知的缺点关系更大。当我们“思考”时，我们需要有意义的概念作为思考的单位，我们通常需要抽象和概括这些概念——有些压缩——以便它们易于处理。给某人一个叙述化的散文解释与给他们三个充满重量的矩阵会导致非常不同层次的可解释性，因为我们不可能一次将这些矩阵中包含的信息保存在我们的大脑中。这激发了我认为可解释性中的一个关键思想:<strong class="jp ir">复杂模型的可解释表示几乎总是它的压缩表示。</strong></p><p id="e4e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这一概念，即概念在真正的可解释性中的必要性，提供了一个视角来解释为什么我们经常发现深度学习模型特别难“搜索”。当然，这部分归因于模型本身的架构。但我也认为，问题的一部分是深度学习在历史上对非常原始的输入数据最有效。作为对比，当您将经济普查数据作为输入要素时，根据定义，每个要素都代表一个对人类有意义的概念，因为因果链的方向是正在计算的要素，因为人类认为计算它是有意义和有价值的。对于非常原始的输入数据，您会遇到单个输入变量(在本例中为像素值)与概念不相关的问题；无论模型在多大程度上使用了更高层次的抽象，它们都是完全习得的抽象，而不是由人类输入系统的。每当有人执行神经元或层<a class="ae kl" href="https://distill.pub/2017/feature-visualization/" rel="noopener ugc nofollow" target="_blank">可视化</a>时，你都会看到这种对意义的争夺，我们不可避免地试图附加人类概念——这一个是寻找眼睛，这一个是建筑物等——即使在某种程度上，我们知道期望机器抽象巧妙地嫁接到人类思想上有点被误导了。</p><h1 id="5e21" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">有哪几种可解释性？</h1><p id="12f8" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">当阅读一篇旨在解决模型可解释性问题的论文——LIME、Shapley值、神经元可视化等——时，我发现将其分为几类是很有用的</p><p id="1997" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">特性属性与内部逻辑:</strong>对我来说，最有意义的区别是试图给特性赋值的方法和试图阐明模型实际内部工作的方法。沙普利值和石灰是前者的例子。他们的主要目标是将模型的行为投射回一组输入要素(或人类创建的替代输入要素)，以某种方式为那些在模型的所有曲折过程中具有更大影响的要素分配更多权重。相比之下，层模板可视化等方法属于后一类:试图理解模型在达到最终答案的过程中创建的中间抽象。虽然在某种意义上他们都在追求“可解释性”，但我认为更广泛地为这些不同的可解释性子目标采用某种清晰的名称是有价值的。</p><p id="26eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">通过模拟获得的知识与通过内省获得的知识:</strong>第二个更微妙的区别与给定的可解释性方法的目标无关，而是它用来得出答案的技术。基于模拟的知识意味着，我们通过生成某种形式的模拟数据，捕捉模型在这些数据点上的表现，并将其作为您的理解，来获得对模型的理解。这与上述轴的方向略有不同，因为LIME(在本地模拟数据样本，也使用本地内核)和神经元可视化(在数字上优化像素，以将内部状态推至高激活值)都属于这个等式的“模拟”侧。相比之下，通过内省获得的知识来自于获取模型的固定方面，并使用它们来获得知识，而不必进行这种模拟。第二种类型的一些示例是线性模型中的基本特征重要性(其中线性和固定项数意味着您可以通过分析计算FI)，以及随机森林总体中的基尼系数减少特征重要性，因为这两者都只是训练模型的属性。不过，总的来说，我认为后一类方法不太适用于更复杂的模型，因此大多数较新的论文属于前一类。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="206f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">构建一个我们并不完全理解的系统的想法——暗示着可能无法完全控制——基本上是傲慢的同义词，这是最近对复杂模型的建模者经常提出的指控。而且，虽然这种对人类可以理解的解释的渴望偶尔会从技术角度感到沮丧和勒德派风格，但我确实认为有令人信服的原因——最突出的是关于采用信任和测试内部表示的脆弱性——为什么这仍然是一个富有成效的研究领域。我的一个抱怨是，我认为我们把一系列期望的目标和潜在的动机混为一谈，这使得我们在这个问题上的讨论更加模糊不清。</p></div></div>    
</body>
</html>