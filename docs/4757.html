<html>
<head>
<title>Organizing Your First Text Analytics Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">组织你的第一个文本分析项目</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/organizing-your-first-text-analytics-project-ce350dea3a4a?source=collection_archive---------5-----------------------#2018-09-05">https://towardsdatascience.com/organizing-your-first-text-analytics-project-ce350dea3a4a?source=collection_archive---------5-----------------------#2018-09-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn jo jp"><p id="e2d9" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated">使用自然语言工具发现会话数据。</p></blockquote><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/fe488d6dd2ba67d6d2790f05b0ccf397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8gb5Ir-2ROQne0WN"/></div></div></figure><p id="ecfd" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated"><strong class="jt ir">文本分析</strong>或文本挖掘是使用各种方法、工具和技术对自然语言文本中包含的“非结构化”数据进行分析。</p><p id="3faa" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">如今，文本挖掘的流行是由统计数据和非结构化数据的可用性推动的。随着社交媒体的日益流行，以及互联网成为各种重要对话的中心位置，<strong class="jt ir">文本挖掘提供了一种低成本的方法来衡量公众意见。</strong></p><p id="c3e6" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">这是我学习文本分析和写这篇博客并与我的数据科学家同事分享我的学习的灵感！</p><p id="f4a2" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">我这篇博客的主要参考是 DataCamp 设计精美的课程<a class="ae le" href="https://www.datacamp.com/courses/intro-to-text-mining-bag-of-words" rel="noopener ugc nofollow" target="_blank">文本挖掘——单词袋</a>。</p><p id="7ee9" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">下面是文本挖掘项目的六个主要步骤。在这篇博客中，我将关注第 3、4、5 和 6 步，并讨论 R 中可用于这些步骤的关键包和函数。</p><h1 id="951c" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">1.问题定义</h1><p id="42e3" class="pw-post-body-paragraph jq jr iq jt b ju md jw jx jy me ka kb lb mf ke kf lc mg ki kj ld mh km kn ko ij bi translated">确定任何项目的具体目标是项目成功的关键。人们需要对领域有所了解，才能恰当地定义问题陈述。</p><p id="3940" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">在这篇文章中，我会问，根据在线评论，亚马逊和谷歌的薪酬水平更高，根据当前员工的评论，哪家的工作生活平衡更好。</p><h1 id="418c" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">2.识别文本来源</h1><p id="54a4" class="pw-post-body-paragraph jq jr iq jt b ju md jw jx jy me ka kb lb mf ke kf lc mg ki kj ld mh km kn ko ij bi translated">可以有多种方式来收集员工评论，从 Glassdoor 这样的网站，甚至是与工作场所评论一起发布的文章，甚至是通过员工的焦点小组访谈。</p><h1 id="5192" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">3.文本组织</h1><p id="e6c1" class="pw-post-body-paragraph jq jr iq jt b ju md jw jx jy me ka kb lb mf ke kf lc mg ki kj ld mh km kn ko ij bi translated">这涉及到清理和预处理文本的多个步骤。R 中有两个主要的包可以用来执行这个:<a class="ae le" href="https://cran.r-project.org/web/packages/qdap/qdap.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jt ir"> qdap </strong> </a>和<a class="ae le" href="https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jt ir"> tm </strong> </a> <strong class="jt ir">。</strong></p><p id="c8d1" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated"><em class="js">要点记住:</em></p><ul class=""><li id="a62d" class="mi mj iq jt b ju jv jy jz lb mk lc ml ld mm ko mn mo mp mq bi translated">tm 包处理文本语料库对象</li><li id="0c93" class="mi mj iq jt b ju mr jy ms lb mt lc mu ld mv ko mn mo mp mq bi translated">qdap 包直接应用于文本向量</li></ul><p id="a507" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated"><strong class="jt ir"> x - &gt;对亚马逊有正面评价的向量</strong></p><pre class="kq kr ks kt gt mw mx my mz aw na bi"><span id="30a8" class="nb lg iq mx b gy nc nd l ne nf"><em class="js"># qdap cleaning function<br/></em>&gt; qdap_clean &lt;- function(x)  {<br/>  x &lt;- replace_abbreviations(x)<br/>  x &lt;- replace_contractions(x)<br/>  x &lt;- replace_number(x)<br/>  x &lt;-  replace_ordinal(x)<br/>  x &lt;-  replace_symbol(x)<br/>  x &lt;-  tolower(x)<br/>  return(x)<br/>}</span></pre><p id="84ce" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">* *根据具体要求，您还可以在上述功能的基础上增加更多清洁功能。</p><p id="132a" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated"><strong class="jt ir">语料库-&gt;v corpus(vector source(x))</strong></p><p id="0f76" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">然后使用<em class="js"> tm_map() </em>函数——由<em class="js"> tm </em>包提供——对语料库应用清理函数。<strong class="jt ir">将这些函数映射到一个完整的语料库使得清洗步骤的缩放变得非常容易。</strong></p><pre class="kq kr ks kt gt mw mx my mz aw na bi"><span id="4421" class="nb lg iq mx b gy nc nd l ne nf"><em class="js"># tm cleaning function</em><br/>&gt; clean_corpus &lt;- function(corpus){<br/> corpus &lt;- tm_map(corpus, stripWhitespace)<br/> corpus &lt;- tm_map(corpus, removePunctuation)<br/> corpus &lt;- tm_map(corpus, content_transformer(tolower))<br/> corpus &lt;- tm_map(corpus, removeWords, c(stopwords("en"), "Google", "Amazon", "company))<br/> return(corpus)<br/>}</span></pre><p id="a688" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated"><a class="ae le" href="https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html" rel="noopener ugc nofollow" target="_blank">单词词干</a>和使用 tm 包完成句子上的词干</p><p id="0fd7" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated"><em class="js"> tm </em>包提供了<em class="js"> stemDocument() </em>函数来获取单词的词根。这个函数要么接受一个字符向量并返回一个字符向量，要么接受一个 PlainTextDocument 并返回一个 PlainTextDocument。</p><pre class="kq kr ks kt gt mw mx my mz aw na bi"><span id="1eaf" class="nb lg iq mx b gy nc nd l ne nf"><em class="js"># Remove punctuation</em><br/>&gt; rm_punc &lt;- <strong class="mx ir">removePunctuation</strong>(text_data)<br/><br/><em class="js"># Create character vector</em><br/>&gt; n_char_vec &lt;- <strong class="mx ir">unlist(strsplit</strong>(rm_punc, split = ' '))<br/><br/><em class="js"># Perform word stemming: stem_doc</em><br/>&gt; stem_doc &lt;- <strong class="mx ir">stemDocument</strong>(n_char_vec)<br/><br/><em class="js"># Re-complete stemmed document: complete_doc</em><br/>&gt; complete_doc &lt;- <strong class="mx ir">stemCompletion</strong>(stem_doc, comp_dict)</span></pre><p id="c08f" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated"><em class="js">点记:</em></p><p id="7440" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">定义您自己的<strong class="jt ir"> comp_dict </strong>，这是一个自定义词典，包含您想要用来重新完成词干的单词。</p><h1 id="063f" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">4.特征抽出</h1><p id="25ac" class="pw-post-body-paragraph jq jr iq jt b ju md jw jx jy me ka kb lb mf ke kf lc mg ki kj ld mh km kn ko ij bi translated">在完成了对文本的基本清理和预处理后，下一步就是提取关键特征，可以通过<strong class="jt ir">情感评分</strong>或提取<strong class="jt ir"> n 元图</strong>并绘制的形式来完成。为此，<em class="js">术语文档矩阵</em> (TDM)或<em class="js">文档术语矩阵</em> (DTM)函数非常方便。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ng"><img src="../Images/a2f85824d56e18f0ece0d35f7a5f2c9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GunyL-BKcADb2aDl.png"/></div></div></figure><pre class="kq kr ks kt gt mw mx my mz aw na bi"><span id="d913" class="nb lg iq mx b gy nc nd l ne nf"><em class="js"># Generate TDM</em><br/>&gt; coffee_tdm &lt;- <strong class="mx ir">TermDocumentMatrix</strong>(clean_corp)<br/><br/><em class="js"># Generate DTM</em><br/>&gt; coffee_dtm &lt;- <strong class="mx ir">DocumentTermMatrix</strong>(clean_corp)</span></pre><p id="10a9" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated"><em class="js">要点记住:</em></p><p id="2215" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">当你要复习的单词比文档多的时候，你可以使用 TDM，因为阅读大量的行比阅读大量的列更容易。</p><p id="c426" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">然后，您可以使用 as.matrix ()函数将结果转换成矩阵，然后对这些矩阵的各个部分进行切片和检查。</p><p id="6910" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated"><strong class="jt ir">让我们看一个为二元模型创建 TDM 的简单例子:</strong></p><p id="1309" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">为了创建一个二元模型 TDM，我们使用<em class="js"> TermDocumentMatrix </em>()以及一个接收控制函数列表的控制参数(更多细节请参考<a class="ae le" href="https://www.rdocumentation.org/packages/tm/versions/0.7-3/topics/TermDocumentMatrix" rel="noopener ugc nofollow" target="_blank"> TermDocumentMatrix </a>)。这里使用了一个名为<em class="js">标记器</em>的内置函数，它有助于将单词标记为二元模型。</p><pre class="kq kr ks kt gt mw mx my mz aw na bi"><span id="5377" class="nb lg iq mx b gy nc nd l ne nf"><em class="js"># Create bigram TDM</em><br/>&gt; amzn_p_tdm &lt;- TermDocumentMatrix(<br/>amzn_pros_corp,<br/>control = list(tokenize = tokenizer))</span><span id="d737" class="nb lg iq mx b gy nh nd l ne nf"><em class="js"># Create amzn_p_tdm_m</em><br/>&gt; amzn_p_tdm_m &lt;- as.matrix(amzn_p_tdm) <br/><br/><em class="js"># Create amzn_p_freq</em> <br/>&gt; amzn_p_freq &lt;- rowSums(amzn_p_tdm_m)</span></pre><h1 id="f547" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">5.特征分析</h1><p id="920c" class="pw-post-body-paragraph jq jr iq jt b ju md jw jx jy me ka kb lb mf ke kf lc mg ki kj ld mh km kn ko ij bi translated">有多种方法来分析文本特征。下面讨论其中的一些。</p><h2 id="cc4a" class="nb lg iq bd lh ni nj dn ll nk nl dp lp lb nm nn lt lc no np lx ld nq nr mb ns bi translated">a.条形图</h2><pre class="kq kr ks kt gt mw mx my mz aw na bi"><span id="35bb" class="nb lg iq mx b gy nc nd l ne nf"><em class="js"># Sort term_frequency in descending order</em><br/>&gt; amzn_p_freq &lt;- sort(amzn_p_freq, decreasing = TRUE) &gt; <br/><br/><em class="js"># Plot a barchart of the 10 most common words</em><br/>&gt; barplot(amzn_p_freq[1:10], col = "tan", las = 2)</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/58d37721eaffca3d465008ce3d97c041.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/0*g-8_lyh96PJ98tUD.png"/></div></figure><h2 id="e505" class="nb lg iq bd lh ni nj dn ll nk nl dp lp lb nm nn lt lc no np lx ld nq nr mb ns bi translated">b.WordCloud</h2><pre class="kq kr ks kt gt mw mx my mz aw na bi"><span id="3668" class="nb lg iq mx b gy nc nd l ne nf"><em class="js"># Plot a wordcloud using amzn_p_freq values</em><br/>&gt; wordcloud(names(amzn_p_freq), amzn_p_freq, max.words = 25, color = "red")</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/f9a01be025fe375a97f3a80cf546b1ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/0*LzeRoB0P6lr74sz8.png"/></div></figure><p id="aa36" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">要进一步了解绘制 wordcloud 的不同方法，请参考这篇<a class="ae le" href="http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know" rel="noopener ugc nofollow" target="_blank">文章</a>，我发现它相当有用。</p><p id="6d7d" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated"><strong class="jt ir"> c .聚类树状图</strong></p><p id="c483" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">这是一个简单的聚类技术，用于执行层次聚类并创建一个树状图来查看不同短语之间的联系。</p><pre class="kq kr ks kt gt mw mx my mz aw na bi"><span id="2a48" class="nb lg iq mx b gy nc nd l ne nf"><em class="js"># Create amzn_p_tdm2 by removing sparse terms</em><br/>&gt; amzn_p_tdm2 &lt;- removeSparseTerms(amzn_p_tdm, sparse = .993) &gt; <br/><br/><em class="js"># Create hc as a cluster of distance values</em><br/>&gt; hc &lt;- hclust(dist(amzn_p_tdm2, method = "euclidean"), method = "complete") &gt; <br/><br/><em class="js"># Produce a plot of hc</em><br/>&gt; plot(hc)</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nv"><img src="../Images/c23784bf5a0fc6542b3f0e3099946a8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-G8VytWfu7oCdpeI.png"/></div></div></figure><p id="3ba5" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">你可以在整个树形图中看到类似的主题，如“巨大的利益”、“高薪”、“聪明人”等。</p><p id="4c1c" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated"><strong class="jt ir"> d .单词联想</strong></p><p id="56b1" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">这用于检查出现在单词云中的顶级短语，并使用来自<em class="js"> tm </em>包的<em class="js"> findAssocs() </em>函数找到相关术语。</p><p id="e271" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">下面的代码用于查找与亚马逊正面评论中最频繁出现的词语最相关的单词。</p><pre class="kq kr ks kt gt mw mx my mz aw na bi"><span id="7d1b" class="nb lg iq mx b gy nc nd l ne nf"><em class="js"># Find associations with Top 2 most frequent words</em><br/>&gt; findAssocs(amzn_p_tdm, "great benefits", 0.2)<br/> $`great benefits`<br/> stock options  options four four hundred vacation time<br/>     0.35          0.28         0.27          0.26<br/> benefits stock competitive pay   great management   time vacation<br/>     0.22              0.22                  0.22          0.22</span><span id="af3d" class="nb lg iq mx b gy nh nd l ne nf">&gt; findAssocs(amzn_p_tdm, "good pay", 0.2)<br/> $`good pay`<br/> pay benefits  pay good  good people  work nice<br/>     0.31        0.23        0.22       0.22</span></pre><p id="bb33" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated"><strong class="jt ir"> e .比较云</strong></p><p id="b3d2" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">当您希望一次检查两个不同的单词库，而不是分别分析它们(这可能更耗时)时，可以使用这种方法。</p><p id="1566" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">下面的代码比较了对谷歌的正面和负面评价。</p><pre class="kq kr ks kt gt mw mx my mz aw na bi"><span id="e434" class="nb lg iq mx b gy nc nd l ne nf"><em class="js"># Create all_goog_corp</em><br/>&gt; all_goog_corp &lt;- tm_clean(all_goog_corpus) &gt; # Create all_tdm<br/>&gt; all_tdm &lt;- TermDocumentMatrix(all_goog_corp)<br/><br/>&lt;&gt;<br/>Non-/sparse entries: 2845/1713<br/>Sparsity : 38%<br/>Maximal term length: 27<br/>Weighting : term frequency (tf)<br/><br/><em class="js">&gt; # Name the columns of all_tdm</em><br/>&gt; colnames(all_tdm) &lt;- c("Goog_Pros", "Goog_Cons") &gt; # Create all_m<br/>&gt; all_m &lt;- as.matrix(all_tdm) &gt; # Build a comparison cloud<br/>&gt; comparison.cloud(all_m, colors = c("#F44336", "#2196f3"), max.words = 100)</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nw"><img src="../Images/d82a78f3a5e976f4148165c5a60fb9cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bBRhR9f3qoDVV1WW.png"/></div></div></figure><p id="58f9" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated"><strong class="jt ir"> f .金字塔图</strong></p><p id="4f2e" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">金字塔图用于显示金字塔(相对于水平条)图，有助于根据相似的短语进行简单的比较。</p><p id="ced7" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">下面的代码比较了亚马逊和谷歌正面短语的出现频率。</p><pre class="kq kr ks kt gt mw mx my mz aw na bi"><span id="d6ed" class="nb lg iq mx b gy nc nd l ne nf"><em class="js"># Create common_words</em><br/>&gt; common_words &lt;- subset(all_tdm_m, all_tdm_m[,1] &gt; 0 &amp; all_tdm_m[,2] &gt; 0)<br/>&gt; str(common_words)<br/> num [1:269, 1:2] 1 1 1 1 1 3 2 2 1 1 ...<br/> - attr(*, "dimnames")=List of 2<br/> ..$ Terms: chr [1:269] "able work" "actual work" "area traffic" "atmosphere little" ...<br/> ..$ Docs : chr [1:2] "Amazon Pro" "Google Pro"<br/><br/><em class="js"># Create difference</em><br/>&gt; difference &lt;- abs(common_words[,1]- common_words[,2]) &gt;<br/><br/><em class="js"># Add difference to common_words</em><br/>&gt; common_words &lt;- cbind(common_words, difference) &gt; head(common_words)<br/> Amazon Pro Google Pro difference<br/> able work 1 1 0<br/> actual work 1 1 0<br/> area traffic 1 1 0<br/> atmosphere little 1 1 0<br/> back forth 1 1 0<br/> bad work 3 1 2<br/><br/><em class="js"># Order the data frame from most differences to least</em><br/>&gt; common_words &lt;- common_words[order(common_words[,"difference"],decreasing = TRUE),]<br/><br/><em class="js"># Create top15_df</em><br/>&gt; top15_df &lt;- data.frame(x = common_words[1:15,1], y = common_words[1:15,2], labels = rownames(common_words[1:15,]))<br/><br/><em class="js"># Create the pyramid plot</em><br/>&gt; pyramid.plot(top15_df$x, top15_df$y,<br/> labels = top15_df$labels, gap = 12,<br/> top.labels = c("Amzn", "Pro Words", "Google"),<br/> main = "Words in Common", unit = NULL)<br/> [1] 5.1 4.1 4.1 2.1</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/45881d771e40333fa22f741b0d038de2.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/0*kK0bXyr_aFPcxfGC.png"/></div></figure><h1 id="46f7" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">6.得出结论</h1><p id="47c1" class="pw-post-body-paragraph jq jr iq jt b ju md jw jx jy me ka kb lb mf ke kf lc mg ki kj ld mh km kn ko ij bi translated">基于上述视觉(“共同语言”金字塔图)，总体而言，亚马逊看起来比谷歌有更好的工作环境和工作生活平衡。亚马逊的工作时间似乎更长，但也许它们为恢复工作与生活的平衡提供了其他好处。我们需要收集更多的评论来做出更好的结论。</p><p id="8274" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">所以，最后我们来到这个博客的结尾。我们学习了如何组织我们的文本分析项目，清理和预处理中涉及的不同步骤，以及最终如何可视化功能并得出结论。我正在完成我的文本分析项目，这是基于我的博客和从 DataCamp 学到的东西。我将很快贴出我的 GitHub 项目库来进一步帮助你。我们的下一个目标应该是进行情感分析。在那之前继续编码！！</p><p id="2152" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">希望你喜欢这个博客。请在我的下一篇博客中分享你喜欢的内容和希望我改进的地方。</p><p id="9b05" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated">请继续关注这个空间，了解更多信息。干杯！</p><p id="838a" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lb kd ke kf lc kh ki kj ld kl km kn ko ij bi translated"><em class="js">(首发@www.datacritics.com) </em></p></div></div>    
</body>
</html>