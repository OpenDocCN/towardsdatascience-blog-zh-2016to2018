<html>
<head>
<title>Self Learning AI-Agents Part II: Deep Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自学习人工智能代理第二部分:深度 Q 学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47?source=collection_archive---------1-----------------------#2018-10-28">https://towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47?source=collection_archive---------1-----------------------#2018-10-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bbe4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深度 Q 学习的数学指南。在这个关于深度强化学习的多部分系列的第二部分中，我将向您展示一种有效的方法，来说明 AI 智能体如何在具有离散动作空间的环境中学习行为。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/45aa3fff077fca0202e6988e0cf0e6b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*loU1qji8ISq2CGHMxWVqAQ.gif"/></div></figure><h2 id="c33b" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">自学习人工智能代理系列—目录</h2><ul class=""><li id="423e" class="lm ln it lo b lp lq lr ls kz lt ld lu lh lv lw lx ly lz ma bi translated"><a class="ae mb" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f">第一部分:马尔可夫决策过程</a></li><li id="35e1" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">第二部分:深度 Q-Learning ( <strong class="lo iu">本文</strong>)</li><li id="dbdb" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated"><a class="ae mb" rel="noopener" target="_blank" href="/deep-double-q-learning-7fca410b193a">第三部分:深度(双)Q 学习</a></li><li id="fa8f" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">第四部分:持续行动空间的政策梯度</li><li id="ae79" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">第五部分:决斗网络</li><li id="9477" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">第六部分:异步演员-评论家代理</li><li id="f4c9" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi">…</li></ul><h2 id="ad5e" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">深度 Q-学习-目录</h2><ul class=""><li id="07da" class="lm ln it lo b lp lq lr ls kz lt ld lu lh lv lw lx ly lz ma bi translated"><strong class="lo iu"> 0。简介</strong></li><li id="c75a" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated"><strong class="lo iu"> 1。时间差异学习</strong></li><li id="33d4" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated"><strong class="lo iu"> 2。q-学习</strong></li><li id="4ce4" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">2.1 开启和关闭策略</li><li id="d78f" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">2.2 贪婪政策</li><li id="1eb9" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated"><strong class="lo iu"> 3。深度 Q 学习</strong></li><li id="0fd8" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">3.1 目标网络和 Q 网络</li><li id="163e" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">3.2ε-贪婪政策</li><li id="5dde" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">3.3 勘探/开采困境</li><li id="b842" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">3.4 体验回放</li><li id="f9ba" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated"><strong class="lo iu"> 4。具有经验重放伪算法的深度 Q 学习</strong></li></ul></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="03ec" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">如果你喜欢这篇文章，想分享你的想法，问问题或保持联系，请随时通过 LinkedIn 与我联系。</h2></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="ceac" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">0.介绍</h2><blockquote class="mo mp mq"><p id="696f" class="mr ms mt lo b lp mu ju mv lr mw jx mx my mz na nb nc nd ne nf ng nh ni nj lw im bi translated">在本系列的第一篇文章中，我向您介绍了<em class="it">马尔可夫决策过程</em>的概念，这是深度强化学习的基础。为了完全理解下面的主题，我建议你重温一下第一篇文章。T25】</p></blockquote><p id="d552" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">通过深度 Q 学习，我们可以编写能够在具有离散动作空间的环境中运行的人工智能代理。离散动作空间是指定义明确的动作，例如向左或向右、向上或向下移动。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3f12d23480383ae73c042534194033c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*wXIZwFnRNmSbO4QWj0cjbg.gif"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">FIg. 1. Atari’s Breakthrough as an example for discrete action spaces.</figcaption></figure><p id="f07e" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated"><em class="mt">雅达利的突破</em>是一个具有离散行动空间的环境的典型例子。人工智能代理可以向左或向右移动。每个方向的运动都有一定的速度。</p><p id="bcd6" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">如果代理可以决定速度，那么我们将有一个连续的动作空间，有无限多的可能动作(不同速度的运动)。这一案件将在今后审议。</p><h2 id="b159" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">1.动作值函数</h2><p id="f2af" class="pw-post-body-paragraph mr ms it lo b lp lq ju mv lr ls jx mx kz np na nb ld nq ne nf lh nr ni nj lw im bi translated">在上一篇文章中，我介绍了动作值函数的概念<strong class="lo iu"> <em class="mt"> Q(s，a)</em></strong>由<strong class="lo iu"> <em class="mt"> Eq 给出。1.</em> </strong>提醒一下，动作值函数被定义为 AI 智能体通过从状态<strong class="lo iu"> s </strong>开始，采取动作<strong class="lo iu"> a </strong>然后遵循策略<strong class="lo iu"> π而获得的预期回报。</strong></p><blockquote class="mo mp mq"><p id="cfe1" class="mr ms mt lo b lp mu ju mv lr mw jx mx my mz na nb nc nd ne nf ng nh ni nj lw im bi translated"><strong class="lo iu">记住</strong>:直观地说，策略π可以描述为代理根据当前状态<strong class="lo iu"> s </strong>选择某些动作的策略。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f84e1458b4f09eb9b543b519384d34b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*V3vK3A03COwFuzu_TyWI1w.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Eq. 1 Action-Value function.</figcaption></figure><p id="e9df" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated"><strong class="lo iu"><em class="mt">【s，a】</em></strong>告诉代理人一个可能动作的值(或质量)<strong class="lo iu"><em class="mt"/></strong><strong class="lo iu"><em class="mt">s</em></strong>。给定一个状态<strong class="lo iu"> <em class="mt"> s </em> </strong>，动作值函数计算该状态下每个可能动作<strong class="lo iu"> <em class="mt"> a_i </em> </strong>的质量/值，作为标量值(图 1)。更高的质量意味着对于给定的目标更好的行动。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/bbfd70d34ad123fc8d7317c16d55cf60.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*mxKfAeu9xcagHnCs6F178Q.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Fig. 1 Given a state <strong class="bd nu">s</strong>, there are many actions and appropriate values of Q(s,a)</figcaption></figure><p id="a11f" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">如果我们执行<strong class="lo iu">等式中的期望运算符<em class="mt"> E </em>。1 </strong>我们获得了一种新形式的行动价值函数，在这里我们处理概率。<strong class="lo iu"><em class="mt">【Pss’</em></strong>是从一个状态<strong class="lo iu"><em class="mt"/></strong>到下一个状态<strong class="lo iu"><em class="mt">【s’</em></strong>的转移概率，由环境决定。<strong class="lo iu"><em class="mt">【π(a ')</em></strong>是政策或<strong class="lo iu"> <em class="mt"> </em> </strong>从数学上讲是给定状态下所有行动的分配<strong class="lo iu"><em class="mt"/></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/09d7591ad4122aff555eb5c302f6f787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*Ixm-Agq8A1Ci_mQCr3UGSw.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Eq. 2 Another form of <strong class="bd nu">Q(s,a)</strong> incorporates probabilities.</figcaption></figure><h2 id="074b" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">1.时间差异学习</h2><p id="9298" class="pw-post-body-paragraph mr ms it lo b lp lq ju mv lr ls jx mx kz np na nb ld nq ne nf lh nr ni nj lw im bi translated">我们在深度 Q-Learning 中的目标是求解动作值函数<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong>。我们为什么想要这个？如果 AI 智能体知道<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong>，那么给定的目标(比如赢得一场与人类玩家的象棋比赛或者玩<em class="mt"> Atari 的</em>突破)可以被认为是已解决的。这样做的原因是这样一个事实，即<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong>的知识将使代理人能够确定在任何给定状态下任何可能行动的质量。因此，代理可以相应地行动。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi nw"><img src="../Images/0e85c02b3929df290ea17128c308e93f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*uqmkic0Za5vrwg9yEMa0nQ.gif"/></div></div></figure><p id="eb32" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated"><strong class="lo iu">情商。2 </strong>还给出了一个递归解，可以用来计算<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong>。但是因为我们在考虑递归，而且用这个方程来处理概率是不实际的。而是必须使用所谓的<em class="mt">时间差分</em> ( <em class="mt"> TD </em>)学习算法来迭代求解<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong>。</p><p id="2b84" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">在 TD 学习中我们更新<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong>对于每一个动作<strong class="lo iu"> <em class="mt"> a </em> </strong>处于一种状态<strong class="lo iu"> <em class="mt"> s </em> </strong>朝向估计返回<strong class="lo iu"> <em class="mt"> R(t+1)+γQ(s(t+1)，a(t+1)) </em> </strong> ( <strong class="lo iu"> Eq。3 </strong>)。估计回报也称为 TD 目标。对每个状态<strong class="lo iu"><em class="mt"/></strong>和动作<strong class="lo iu"><em class="mt"/></strong>多次迭代地执行该更新规则，对于环境中的任何状态-动作对，产生正确的动作值<strong class="lo iu"><em class="mt">【Q(s，a)</em></strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi ob"><img src="../Images/ba561ca9b940506efda1d784e4ab4576.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xYWnN5goNuwZUC2ofFIg_A.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Eq. 3 Update rule for <strong class="bd nu">Q(s,a)</strong></figcaption></figure><p id="8e40" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">TD-Learning 算法可以总结为以下步骤:</p><ul class=""><li id="a1d7" class="lm ln it lo b lp mu lr mw kz oc ld od lh oe lw lx ly lz ma bi translated">对于状态<strong class="lo iu"> <em class="mt"> s_t </em> </strong>中的动作<strong class="lo iu"> a_t </strong>计算<strong class="lo iu"> <em class="mt"> Q(s_t，a_t) </em> </strong></li><li id="d50b" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">转到下一个状态<strong class="lo iu"><em class="mt">【t+1】</em></strong>，在那里采取一个动作<strong class="lo iu"><em class="mt">【t+1】</em></strong>并计算值<strong class="lo iu"> <em class="mt"> Q( s_(t+1)，a(t+1)) </em> </strong></li><li id="28b6" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">使用<strong class="lo iu"> <em class="mt"> Q( s_(t+1)，a(t+1)) </em> </strong>和即时奖励<strong class="lo iu"> <em class="mt"> R(t+1) </em> </strong>进行动作<strong class="lo iu"> <em class="mt"> a_t </em> </strong>最后状态<strong class="lo iu"> <em class="mt"> s_t </em> </strong>计算 TD-Target</li><li id="fef0" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">通过将<strong class="lo iu"><em class="mt">【Q(s _ t，a _ t)】</em></strong>添加到 TD-Target 和<strong class="lo iu"><em class="mt">【Q(s _ t，a _ t)】</em></strong><strong class="lo iu"><em class="mt">α</em></strong>之间的差值来更新先前的<strong class="lo iu"><em class="mt">【s _ t，a _ t】</em></strong>。</li></ul><h2 id="2e77" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">1.1 时间差异</h2><p id="f676" class="pw-post-body-paragraph mr ms it lo b lp lq ju mv lr ls jx mx kz np na nb ld nq ne nf lh nr ni nj lw im bi translated">让我们更详细地讨论 TD 算法的概念。在 TD- learning 中我们考虑了<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong> — <strong class="lo iu"> <em class="mt"> </em> </strong>两个“版本”之间的差异<strong class="lo iu">【a】</strong>Q(s，】<strong class="lo iu"> <em class="mt"> </em> </strong>一旦<strong class="lo iu">在</strong>之前我们采取行动<strong class="lo iu"><em class="mt"/></strong>处于状态<strong class="lo iu"><em class="mt"/></strong></p><h2 id="43f3" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">采取行动前:</h2><p id="e7d4" class="pw-post-body-paragraph mr ms it lo b lp lq ju mv lr ls jx mx kz np na nb ld nq ne nf lh nr ni nj lw im bi translated">请看图 2。假设 AI 代理处于状态<strong class="lo iu"> <em class="mt"> s </em> </strong>(蓝色箭头)。在<strong class="lo iu"><em class="mt"/></strong><em class="mt"/>状态下，他可以采取两种不同的动作<strong class="lo iu"><em class="mt">【a _ 1】</em></strong><strong class="lo iu"><em class="mt">【a _ 2】</em></strong>。基于来自一些先前时间步骤的计算，代理知道该状态下两个可能动作的动作值<strong class="lo iu"><em class="mt">【s，a _ 1】</em></strong>和<strong class="lo iu"> <em class="mt"> Q(s，a_2) </em> </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/38882073b428090c1ab4bb72080cd1e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*yyFqkWxG0PGz8MCWfOs2-A.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Fig. 2 Agent in state <strong class="bd nu">s </strong>knows every possible <strong class="bd nu">Q(s,a)</strong>.</figcaption></figure><h2 id="ace3" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">采取行动后:</h2><p id="334a" class="pw-post-body-paragraph mr ms it lo b lp lq ju mv lr ls jx mx kz np na nb ld nq ne nf lh nr ni nj lw im bi translated">基于这个知识，代理决定采取行动<strong class="lo iu"> <em class="mt"> a_1 </em> </strong>。采取此操作后，代理处于下一个状态<strong class="lo iu"><em class="mt">‘s’</em></strong>。因为采取了行动<strong class="lo iu"> <em class="mt"> a_1 </em> </strong>他获得了直接奖励<strong class="lo iu"> <em class="mt"> R </em> </strong>。处于状态<strong class="lo iu"><em class="mt">s’</em></strong>时，代理可以再次采取两种可能的动作<strong class="lo iu"><em class="mt">a’_ 1</em></strong>和<strong class="lo iu"><em class="mt">a’_ 2</em></strong>，他从先前的一些计算中再次知道了这些动作的值。</p><p id="fe3a" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">如果从方程<strong class="lo iu"> Eq 中<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong>的定义上看。1 </strong>你会发现，在状态<strong class="lo iu"><em class="mt">s’</em></strong>中，我们现在有了新的信息，可以用来计算<strong class="lo iu"> <em class="mt"> Q(s，a_1) </em> </strong>的新值。该信息是针对上一个状态中的上一个动作而接收到的即时奖励<strong class="lo iu"> <em class="mt"> R </em> </strong>以及针对动作<strong class="lo iu"><em class="mt">a’</em></strong>而接收到的<strong class="lo iu"><em class="mt">Q(s’，a’)</em></strong>代理将在这个新状态中获取。根据图 3 中的等式可以计算出<strong class="lo iu"> <em class="mt"> Q(s，a_1) </em> </strong>的新值。等式的右边也是我们所说的 TD 目标。TD-target 与旧值或<strong class="lo iu"><em class="mt">【s，a _ 1】</em></strong>的'<em class="mt">时态版本</em>之差称为时态差。</p><blockquote class="mo mp mq"><p id="80a9" class="mr ms mt lo b lp mu ju mv lr mw jx mx my mz na nb nc nd ne nf ng nh ni nj lw im bi translated"><strong class="lo iu">记住</strong>:在 TD-learning 期间，我们为任何可能的动作值<strong class="lo iu"><em class="it"/></strong>Q(s，a)<em class="it">【s，a】</em><em class="it">计算时间差，并同时使用它们来更新<strong class="lo iu"><em class="it">【Q(s，a) </em> </strong> <em class="it">，直到</em> <strong class="lo iu"> <em class="it"> Q(s，a) </em> </strong>收敛到其真实值。</em></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/adf8cae54958c206aba888dbd22ecbd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*EBXlRiR8CThYeJeTGOsCpA.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Fig, 3 Agent in state <strong class="bd nu">s’ </strong>after taking action <strong class="bd nu">a_1</strong>.</figcaption></figure></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="a301" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">2.萨尔萨</h2><p id="f43d" class="pw-post-body-paragraph mr ms it lo b lp lq ju mv lr ls jx mx kz np na nb ld nq ne nf lh nr ni nj lw im bi translated">应用于<strong class="lo iu"><em class="mt">【S，A】</em></strong>的 TD-Learning 算法俗称<em class="mt"> SARSA </em>算法(<strong class="lo iu">S</strong>tate—<strong class="lo iu">A</strong>action—<strong class="lo iu">R</strong>eward—<strong class="lo iu">S</strong>tate—<strong class="lo iu">A</strong>action)。<em class="mt"> SARSA </em>是被称为<em class="mt"> on-policy </em> <strong class="lo iu"> </strong>算法的特殊学习算法的一个很好的例子。</p><p id="5c6b" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">前面我介绍了策略<strong class="lo iu"> π(a|s) </strong>作为从状态<strong class="lo iu"> <em class="mt"> s </em> </strong>到动作<strong class="lo iu"> <em class="mt"> a </em> </strong>的映射。此时需要记住的一点是，<em class="mt"> on-policy </em>算法使用与<strong class="lo iu">相同的</strong>策略来获取 TD-Target 中<strong class="lo iu"><em class="mt">【s _ t，a _ t】</em></strong>以及<strong class="lo iu"> <em class="mt"> Q(s(t+1)，a_(t+1)) </em> </strong>的动作。这意味着我们正在<strong class="lo iu">跟随</strong>和<strong class="lo iu">同时改进</strong>同样的政策。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi oh"><img src="../Images/58ab4b199bb5123f16292c91cb38decf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OoBd56C86SrnYWPkiHXm9Q.png"/></div></div></figure><h2 id="e8c9" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">3.q 学习</h2><p id="d5d3" class="pw-post-body-paragraph mr ms it lo b lp lq ju mv lr ls jx mx kz np na nb ld nq ne nf lh nr ni nj lw im bi translated">我们最终到达文章的核心，在这里我们将讨论 Q-Learning 的概念。但在此之前，我们必须先看看第二种特殊类型的算法，称为<em class="mt">非策略</em>算法。正如你可能已经想到的，Q-Learning 属于这种算法，这是一个区别<strong class="lo iu"> </strong>到<em class="mt"> on-policy </em>算法，如<em class="mt"> SARSA </em>。</p><p id="d016" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">为了理解非策略算法，我们必须引入另一个策略<strong class="lo iu"> <em class="mt"> (a|s) </em> </strong>，并将其称为<em class="mt">行为策略</em>。行为策略决定行动<strong class="lo iu"><em class="mt">a _ t ~(a | s)</em></strong>for<strong class="lo iu"><em class="mt">Q(s _ t，a _ t)</em></strong>for all<strong class="lo iu"><em class="mt"/></strong>在<em class="mt"> SARSA、</em>的情况下，行为策略将是我们同时遵循并试图优化的策略。</p><p id="a3d4" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">在<strong class="lo iu"> <em class="mt">非策略</em> </strong>算法中，我们有两种不同的策略<strong class="lo iu"><em class="mt">(a | s)</em></strong><em class="mt"/>和<strong class="lo iu"> π(a|s) </strong>、<strong class="lo iu"> <em class="mt"> (a|s) </em> </strong>是行为，而<em class="mt"> </em> <strong class="lo iu"> π(a|s) </strong>是所谓的<em class="mt">目标策略。</em>行为策略用于计算<strong class="lo iu"> <em class="mt"> Q(s_t，a _ t)</em></strong>目标策略<strong class="lo iu"> </strong>用于计算<strong class="lo iu"> <em class="mt"> Q(s_t，a_t) </em>只有</strong>中的<em class="mt"/>TD-Target。(这一概念将在下一节进行实际计算时更加全面)</p><blockquote class="mo mp mq"><p id="c490" class="mr ms mt lo b lp mu ju mv lr mw jx mx my mz na nb nc nd ne nf ng nh ni nj lw im bi translated"><strong class="lo iu">记住</strong>:行为策略为所有<strong class="lo iu"> <em class="it"> Q(s，a) </em> </strong>挑选动作。相比之下，目标策略只为 TD 目标的计算确定行动。</p></blockquote><p id="6b42" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">我们实际上称之为 Q 学习算法的算法是一个特例，其中目标策略<strong class="lo iu"><em class="mt">【π(a | s)</em></strong>是一个贪婪的 w.r.t. <strong class="lo iu"> <em class="mt"> Q(s，a)</em></strong>这意味着我们的策略正在采取导致最高值<strong class="lo iu"> <em class="mt"> Q </em> </strong>的行动。这产生了以下目标策略:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/af51eeccef58b7e06d5e4c1c2ca77038.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*aO7_J2JYv80tZcKCbOzReg.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Eq. 4 Greedy target policy w.r.t <strong class="bd nu">Q(s,a)</strong>.</figcaption></figure><p id="7556" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">在这种情况下，目标策略被称为<em class="mt">贪婪策略</em>。贪婪策略意味着我们只选择产生最高<strong class="lo iu"><em class="mt">【s，a】</em></strong>值的行为。这个贪婪的目标策略可以被插入到行动等式中-值<strong class="lo iu"><em class="mt">【Q(s，a)】</em></strong>其中我们之前已经遵循了随机策略<strong class="lo iu"><em class="mt">【π(a | s)</em></strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/d3bbf62757d655776529e324013424a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*3wPaaU4P0WAhaU-vVg_Z8Q.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Eq. 5 Insertion of greedy policy into <strong class="bd nu">Q(s,a)</strong>.</figcaption></figure><p id="6482" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">贪婪策略为我们提供了<em class="mt">最优</em>动作值<strong class="lo iu"> <em class="mt"> Q*(s，a) </em> </strong>，因为根据定义<strong class="lo iu"> <em class="mt"> Q*(s，a) </em> </strong>是<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong>遵循最大化动作值的策略:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi ok"><img src="../Images/5f6a870b6cd1567a0d78f4b9602eae4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*eqVL4WEBaLBGndDuODGI5A.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Eq. 6 Definition of optimal <strong class="bd nu">Q(s,a)</strong>.</figcaption></figure><p id="7cc9" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated"><strong class="lo iu"> Eq 中的最后一行。5 </strong>就是我们在上一篇文章中推导出的<em class="mt">贝尔曼最优方程</em>。<em class="mt"> </em>这个方程作为递归更新规则，用来估计<em class="mt">最优</em>动作值函数<strong class="lo iu"> <em class="mt"> Q*(s，a)。</em>T25】</strong></p><p id="b5a9" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">然而，TD-learning 仍然是找到<strong class="lo iu"> <em class="mt"> Q*(s，a) </em> </strong>的最佳方法。利用贪婪目标策略，对等式中的<strong class="lo iu"> Q(s，a) </strong>的 TD 学习更新步骤。3 变得更加简单，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi ol"><img src="../Images/df27e15fd9a12b5fe432c0ebce618a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fq9J1FFuWHdhfPdnZQHRoA.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Eq. 7 TD-learning update rule with greedy policy.</figcaption></figure><p id="b3aa" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">用于具有贪婪目标策略的<strong class="lo iu"><em class="mt">【Q(s，a)】</em></strong>的 TD-学习算法可以概括为以下步骤:</p><ul class=""><li id="f020" class="lm ln it lo b lp mu lr mw kz oc ld od lh oe lw lx ly lz ma bi translated">对于状态<strong class="lo iu"> <em class="mt"> s_t </em> </strong>中的动作<strong class="lo iu"> a_t </strong>计算<strong class="lo iu"> <em class="mt"> Q(s_t，a_t) </em> </strong></li><li id="2e30" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">转到下一个状态<strong class="lo iu"><em class="mt">【s _(t+1)</em></strong>，采取动作<strong class="lo iu"><em class="mt">‘a’</em></strong>，产生最高值<strong class="lo iu"> <em class="mt"> Q </em> </strong>，并计算<strong class="lo iu"> <em class="mt"> Q( s_(t+1)，a’)</em></strong></li><li id="6611" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">使用<strong class="lo iu"> <em class="mt"> Q( s_(t+1)，a’)</em></strong>和即时奖励<strong class="lo iu"> <em class="mt"> R </em> </strong>进行动作<strong class="lo iu"> <em class="mt"> a_t </em> </strong>最后状态<strong class="lo iu"> <em class="mt"> s_t </em> </strong>计算 TD-Target</li><li id="fb28" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">更新先前的<strong class="lo iu"> <em class="mt"> Q(s_t，a_t) </em> </strong>，将<strong class="lo iu"> <em class="mt"> Q(s_t，a_t) </em> </strong>添加到 TD-Target 和<strong class="lo iu"> <em class="mt"> Q(s_t，a_t)之间的差值，α </em> </strong>为学习率。</li></ul><p id="55c9" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">考虑前面的图(图 3)，其中代理处于状态<strong class="lo iu"><em class="mt">【s’</em></strong>，并且知道该状态下可能动作的动作值。遵循贪婪目标策略，代理将采取具有最高动作值的动作(图 4 中的蓝色路径)。这个策略还为我们提供了一个新的值<strong class="lo iu"> <em class="mt"> Q(s，a_1) </em> </strong>(图中的等式)，这就是定义的 TD 目标。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi om"><img src="../Images/41b858fa41345582daa31234415e37cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*g4W0u-ECWQqRWoDVO8vlag.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Eq. 8 Calculation of <strong class="bd nu">Q(s,a_1)</strong> following the greedy policy.</figcaption></figure><h2 id="c9b7" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">3.深度 Q 学习</h2><p id="3de9" class="pw-post-body-paragraph mr ms it lo b lp lq ju mv lr ls jx mx kz np na nb ld nq ne nf lh nr ni nj lw im bi translated">我们终于到达了这篇文章的标题得到其序言<em class="mt">深度的地方— </em>我们终于利用了深度学习。如果您查看<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong>的更新规则，您可能会认识到，如果 TD-Target 和<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong>具有相同的值，我们不会得到任何更新。在这种情况下<strong class="lo iu"> <em class="mt"> Q(s，a)</em></strong>收敛到真实的动作值，目标达到。</p><p id="7bf9" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">这意味着我们的目标是最小化 TD-Target 和<strong class="lo iu"><em class="mt">【Q(s，a)】</em></strong>之间的距离，这可以通过平方误差损失函数(等式 1)来表示。10).这种损失函数的最小化可以通过通常的梯度下降算法来实现。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi on"><img src="../Images/d4e0036c7a35f141ea08c1b1bc9f7a6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zwdaxLofeJlrWTTcRZBcxg.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Eq. 10 Squared error loss function.</figcaption></figure><h2 id="3e71" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">3.1 目标网络和 Q 网络</h2><p id="97c1" class="pw-post-body-paragraph mr ms it lo b lp lq ju mv lr ls jx mx kz np na nb ld nq ne nf lh nr ni nj lw im bi translated">在深度 Q 学习中，TD-Target<strong class="lo iu"><em class="mt">y _ I</em></strong>和<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong>分别由两个不同的神经网络估计，这两个网络通常称为 Target-和 Q-网络(图 4)。目标网络的参数<strong class="lo iu"> <em class="mt"> θ(i-1) </em> </strong>(权重、偏差)对应于 Q 网络在较早时间点的参数<strong class="lo iu"><em class="mt">【θ(I)</em></strong>。意味着目标网络参数在时间上被冻结。在用 Q 网络的参数进行了<em class="mt"> n </em>次迭代之后，它们被更新。</p><blockquote class="mo mp mq"><p id="0b85" class="mr ms mt lo b lp mu ju mv lr mw jx mx my mz na nb nc nd ne nf ng nh ni nj lw im bi translated"><strong class="lo iu"> <em class="it">记住</em> </strong>:给定当前状态<strong class="lo iu"> <em class="it"> s </em> </strong>，Q 网络计算动作值<strong class="lo iu"> Q(s，a) </strong>。同时，目标网络使用下一个状态<strong class="lo iu">s’</strong>来计算 TD 目标的<strong class="lo iu">Q(s’，a) </strong>。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/db63fd8f2274be512e825e31a3895f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*Vd1kcpLoQDnM5vrKnvzxbw.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Fig. 4 Target,- and Q-Network. <strong class="bd nu">s</strong> being the current and <strong class="bd nu">s’</strong> the next state.</figcaption></figure><p id="7ea3" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">研究表明，对 TD-Target 和<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong>计算采用两种不同的神经网络，模型的稳定性更好。</p><h2 id="6f2c" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">3.2ε-贪婪政策</h2><p id="1367" class="pw-post-body-paragraph mr ms it lo b lp lq ju mv lr ls jx mx kz np na nb ld nq ne nf lh nr ni nj lw im bi translated">虽然目标策略<strong class="lo iu"><em class="mt">【π(a | s)</em></strong>仍然是贪婪策略，但是行为策略<strong class="lo iu"> <em class="mt"> (a|s) </em> </strong>决定了 AI 代理采取的动作<strong class="lo iu"> <em class="mt"> a_i </em> </strong>，因此必须将哪个<strong class="lo iu"> <em class="mt"> Q(s，a_i) </em> </strong>(由 Q-网络计算)插入到平方误差中</p><p id="bfae" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">行为方针<strong class="lo iu"> <em class="mt"> </em> </strong>通常选择<em class="mt">ε-贪婪。使用ε-贪婪策略，代理在每个时间步选择一个具有固定概率ε的随机动作。如果ε具有比随机生成的数<em class="mt"> p </em>，0 ≤ <em class="mt"> p </em> ≤ 1 更高的值，则 AI 代理从动作空间中选取一个随机动作。否则，根据学习的动作值<strong class="lo iu"> <em class="mt"> Q(s，a): </em> </strong>贪婪地选择动作</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/eb7c934608391b99ce7f37fe26cff3ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*WIkudlfJOU5d17TCiTZbNw.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Eq. 11 Definition of the ε-Greedy policy.</figcaption></figure><p id="6ba4" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">选择ε-贪婪策略作为行为策略<strong class="lo iu"> <em class="mt"> </em> </strong>解决了探索/剥削权衡的困境。</p><h2 id="0dd2" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">3.3 勘探/开采</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/a009a695d2948ac292690b5d6476b27d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*UAsULa57_2mVTMj4pwechg.png"/></div></figure><p id="54c6" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">关于采取何种行动的决策涉及一个基本选择:</p><ul class=""><li id="fe96" class="lm ln it lo b lp mu lr mw kz oc ld od lh oe lw lx ly lz ma bi translated"><strong class="lo iu">利用</strong>:根据当前信息做出最佳决策</li><li id="57b3" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated">探索:收集更多信息，探索可能的新路径</li></ul><p id="a79d" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">就<strong class="lo iu">利用而言，</strong>代理根据行为策略<strong class="lo iu"> <em class="mt"> </em> </strong>采取可能的最佳行动。但是这可能会导致一个问题。也许有时可以采取另一种(替代的)行动，在状态序列中产生(长期)更好的路径，但是如果我们遵循行为策略，这种替代行动可能不会被采取。在这种情况下，我们<strong class="lo iu">利用</strong>当前政策，但不探索其他替代行动。</p><p id="1f94" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">ε-贪婪策略通过允许 AI 代理以一定的概率从动作空间中采取随机动作来解决这个问题。这叫<strong class="lo iu">探索</strong>。通常，根据等式，ε的值随着时间而减小。12.这里<strong class="lo iu"> <em class="mt"> n </em> </strong>是迭代次数。减小ε意味着在训练开始时，我们试图探索更多的可选路径，而在最后，我们让政策决定采取何种行动。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi or"><img src="../Images/39dc0fe563183494bbcd8e7a8f4266e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*zYd_G6ckENfRdQKvmJgYkA.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Eq. 12 Decreasing of <em class="os">ε over time.</em></figcaption></figure><h2 id="8147" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">3.4 体验回放</h2><p id="77a1" class="pw-post-body-paragraph mr ms it lo b lp lq ju mv lr ls jx mx kz np na nb ld nq ne nf lh nr ni nj lw im bi translated">过去，可以表明，如果 Deep-Q 学习模型实现<strong class="lo iu">经验重放</strong>，则估计 TD-Target 和<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong>的神经网络方法变得更加稳定。经验回放无非是存储<strong class="lo iu"> <em class="mt"> &lt; s，s’，a’，r&gt;</em>T51】元组的记忆，其中</strong></p><ul class=""><li id="17a1" class="lm ln it lo b lp mu lr mw kz oc ld od lh oe lw lx ly lz ma bi translated"><strong class="lo iu"><em class="mt">s</em></strong>:AI 代理的状态</li><li id="6bd5" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated"><strong class="lo iu"><em class="mt">a’</em></strong>:代理在<strong class="lo iu"> <em class="mt"> s </em> </strong>状态下采取的动作</li><li id="9c92" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated"><strong class="lo iu"> <em class="mt"> r </em> </strong>:在<strong class="lo iu"> <em class="mt"> s </em> </strong>状态下行动<strong class="lo iu"><em class="mt">a’</em></strong>获得即时奖励</li><li id="e387" class="lm ln it lo b lp mc lr md kz me ld mf lh mg lw lx ly lz ma bi translated"><strong class="lo iu"> <em class="mt"> s' </em> </strong>:状态<strong class="lo iu"> <em class="mt"> s </em> </strong>后代理的下一个状态</li></ul><p id="96e4" class="pw-post-body-paragraph mr ms it lo b lp mu ju mv lr mw jx mx kz mz na nb ld nd ne nf lh nh ni nj lw im bi translated">在训练神经网络时，我们通常不使用最近的<strong class="lo iu"> <em class="mt"> &lt; s，s’，a’，r &gt; </em> </strong>元组。而是我们从经验回放<strong class="lo iu"> <em class="mt"> </em> </strong>中随机取一批<strong class="lo iu"> <em class="mt"> &lt; s，s’，a’，r &gt; </em> </strong>来计算 TD-Target，<strong class="lo iu"> <em class="mt"> Q(s，a) </em> </strong>最后应用梯度下降。</p><h2 id="9dd5" class="kq kr it bd ks kt ku dn kv kw kx dp ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">3.5 具有经验重放伪算法的深度 Q 学习</h2><p id="49c9" class="pw-post-body-paragraph mr ms it lo b lp lq ju mv lr ls jx mx kz np na nb ld nq ne nf lh nr ni nj lw im bi translated">下面的伪算法实现了具有经验重放的深度 Q 学习。我们之前讨论的所有主题都以正确的顺序包含在这个算法中，就像它在代码中是如何实现的一样。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi ot"><img src="../Images/b8cd17efbb999225f1e85f8ef2134f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2wOzh6K4NMMrWYvZ0G5KUA.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Pseudo-Algorithm for Deep-Q Learning with Experience Replay.</figcaption></figure></div></div>    
</body>
</html>