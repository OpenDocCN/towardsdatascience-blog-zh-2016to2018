<html>
<head>
<title>Olive Oil is Made of Olives, Baby Oil is Made for Babies [Paper Summary]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">橄榄油是橄榄做的，婴儿油是给婴儿做的[论文摘要]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/olive-oil-is-made-of-olives-baby-oil-is-made-for-babies-paper-summary-265bea33605b?source=collection_archive---------10-----------------------#2018-04-20">https://towardsdatascience.com/olive-oil-is-made-of-olives-baby-oil-is-made-for-babies-paper-summary-265bea33605b?source=collection_archive---------10-----------------------#2018-04-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="b720" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文总结了一种新颖的技术，用于自然语言处理中的一项非常复杂的任务，即名词复合分类。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/142f9df9c7d5cabd71a2354e02ea1898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nP9flAk6joT7AQEfNZQTVA.png"/></div></div></figure><p id="7f2b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">论文题目</em> </strong></p><p id="0fd6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1803.08073.pdf" rel="noopener ugc nofollow" target="_blank">橄榄油是由橄榄制成的，婴儿油是为婴儿制造的:用神经模型中的释义解释名词复合词</a><em class="kl">——Vered shw artz 和 Chris Waterson </em></p><p id="a23a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">基本概述</em> </strong></p><p id="fd4c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇<a class="ae ky" href="https://arxiv.org/pdf/1803.08073.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>解决了一个重要的自然语言处理任务——自动解释名词复合成分之间的关系。</p><p id="2e96" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">动机</em> </strong></p><p id="6f93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑下面的名词复合词例子:<strong class="jp ir"> <em class="kl">橄榄油</em> </strong>和<strong class="jp ir"> <em class="kl">婴儿油</em> </strong>。可以观察到，<strong class="jp ir"> <em class="kl">【橄榄油】</em> </strong>中的<strong class="jp ir"> <em class="kl">【橄榄】</em> </strong>一词描述的是来源关系，<strong class="jp ir"> <em class="kl">【婴儿油】</em> </strong>中的<strong class="jp ir"> <em class="kl">【婴儿】</em> </strong>一词描述的是目的关系。换句话说，就婴儿在现实世界中所代表的意义而言，他们永远不应该被放在与橄榄相同的环境中。这种区别很重要，因为它可以用于需要复杂文本理解能力的各种应用程序。</p><p id="0b7b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">例子<em class="kl">例子</em>例子</strong></p><p id="61a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想象一下，你问谷歌搜索橄榄油是由什么组成的。如果谷歌搜索是聪明的，它应该回应“橄榄”。现在想象一下，你问谷歌婴儿油是由什么组成的。绝对不是婴儿！答案应该是油的其他成分或者油的主要成分。这是一个非常重要的区别！这是一项具有挑战性的任务，因为这两种油的含义都不容易解释给定其组成单词的含义。</p><p id="d89b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你能想出更多的例子吗？试一试，你就会明白为什么 NLP 研究的这个领域很重要。作为一名 NLP 研究人员，我甚至可以看到这对于消除情感短语之间的歧义是多么有用。(在另一篇文章中有更多关于这方面的内容。)</p><p id="f18b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">文献综述</em> </strong></p><p id="1a18" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">两种非常常见的方法被用来解决这个问题:<strong class="jp ir"> <em class="kl">释义</em> </strong>和<strong class="jp ir"> <em class="kl">名词复合表示</em> </strong>。第一种方法映射成分之间的关系，后一种方法利用单个成分的分布表示。(不要慌，我一会儿解释他们的意思。)</p><p id="8454" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最近的一项工作<a class="ae ky" href="https://doi.org/10.18653/v1/W16-1604" rel="noopener ugc nofollow" target="_blank">(马頔，2016) </a>表明，成分嵌入可以有效地表示名词性复合词(以下简称 NCs)。这种方法有效的主要原因在于一种被称为<a class="ae ky" href="http://www.aclweb.org/anthology/N15-1098" rel="noopener ugc nofollow" target="_blank">词汇记忆</a>的现象。</p><p id="43fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">贡献</em> </strong></p><p id="d6db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文提出了一种结合路径嵌入(代表名词短语之间的关系)和分布信息(直接从单词嵌入中获得)的神经释义方法来执行名词短语分类任务。作者还对避免词汇记忆的设置进行了实验，以表明他们的方法更加稳健，他们的结果与这一现象无关。</p><p id="64ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">型号</em> </strong></p><p id="dcad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作者使用<a class="ae ky" href="http://www.aclweb.org/anthology/P16-1226" rel="noopener ugc nofollow" target="_blank"> HypeNET </a>来学习连接语料库中成分实例的联合出现的模式。这些也被称为<em class="kl">路径嵌入</em>。</p><p id="6c58" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">结合三种模型进行 NCs 关系分类:<em class="kl">基于路径的</em>、<em class="kl">集成的</em>和<em class="kl">集成的-NC </em>。每个模型递增地添加新的特征(在这种情况下是不同的分布式输入),这实质上向整个输入向量添加了更多上下文化的信息。在下图中可以更清楚地看到这一过程:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi kz"><img src="../Images/80951117e520759c2c5adae3f9a83b50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*r0BEqUrZiFffAPR1.png"/></div></div></figure><p id="5042" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">路径嵌入(图中紫色部分)是用一个普通的 LSTM 学习的，输入向量表示下列向量的连接:<em class="kl">引理</em>、<em class="kl">词性标签</em>、<em class="kl">依存标签</em>和<em class="kl">方向向量</em>。(详见论文)。NC 标签(关系)通过 LSTM 的输出使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Semi-supervised_learning" rel="noopener ugc nofollow" target="_blank">远程监控</a>方法获得。</p><p id="f30e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">评价</em> </strong></p><p id="a67c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从<a class="ae ky" href="http://digitallibrary.usc.edu/cdm/ref/collection/p15799coll3/id/176191" rel="noopener ugc nofollow" target="_blank"> Tratz (2011) </a>获得的两个数据集用于评估提出的神经释义模型。提出了几个比较模型，包括几个基线模型和从以前的工作和最新方法中采用的重新训练模型。(有关实验设置的更多详细信息，请参见论文)。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi la"><img src="../Images/1ac29c64462f28d47164bd31573ed03c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jjHfUovECjz4P0AZ.png"/></div></div></figure><p id="1b44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">表 1 显示，在大多数情况下，集成模型(<em class="kl"> Int </em>和<em class="kl"> Int-NC </em>)在使用不同的数据拆分策略方面优于所有其他模型(显示在拆分列中)。从<strong class="jp ir"> <em class="kl"> Int </em> </strong>模型和<strong class="jp ir"> <em class="kl"> Int-NC </em> </strong>模型获得的结果之间几乎没有差别，表明 NC 嵌入对分类任务没有太大贡献。</p><p id="7747" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">分析</em> </strong></p><p id="066a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对随机分裂策略进行了进一步的分析，以分析不同模型的结果的变化。在表 3 中，您可以观察到产生合理性能的一些关系(例如，度量和个人头衔)</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi lb"><img src="../Images/edd8894fa262df47dbde401fd26fcf12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XzCcaTCLNxLVxZFY.png"/></div></div></figure><p id="7fde" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作者还发现复杂关系表现不佳，如 NC 的词汇化，“<em class="kl">肥皂剧”</em>和 NC 的目标，“<em class="kl">恢复计划”。</em>(见文中更有趣的例子)。</p><p id="cd1c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面的表 4 提供了从测试集获得的 NC 嵌入的例子(<em class="kl">左</em>)和嵌入中最相似 NC 的例子(<em class="kl">右</em>)。作者观察到只有 27.61%的 NCs 与具有相同标签的 NCs 非常相似。他们将这种行为归因于不一致的注释，而不是嵌入的质量。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi lc"><img src="../Images/07959db3b53ff75d30e523df8133ed84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WZazmYwh2tk7_0B3.png"/></div></div></figure><p id="a3ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">我的</em> </strong> <strong class="jp ir"> <em class="kl">进一步的想法和结论</em> </strong></p><ul class=""><li id="89c3" class="ld le iq jp b jq jr ju jv jy lf kc lg kg lh kk li lj lk ll bi translated">可视化 NC 向量嵌入，以观察可能具有相似属性的关系簇和模式。</li><li id="a363" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk li lj lk ll bi translated">更仔细地研究词汇记忆现象，本文使用整个模型中基于路径的部分来帮助<em class="kl">【轻微】</em>解决这个问题。</li><li id="4a63" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk li lj lk ll bi translated">总体而言，性能得到了提高，但在数据质量和建模方面仍有很大的改进空间。</li><li id="f3ac" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk li lj lk ll bi translated">NC 嵌入对当前模型没有帮助，因此它在不改变模型整体结构的情况下提供了一个可行的未来研究方向。</li></ul><p id="f6ae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">资源</em> </strong></p><ul class=""><li id="cef7" class="ld le iq jp b jq jr ju jv jy lf kc lg kg lh kk li lj lk ll bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1803.08073.pdf" rel="noopener ugc nofollow" target="_blank">原文</a>|<a class="ae ky" href="https://github.com/tensorflow/models/tree/master/research/lexnet_nc" rel="noopener ugc nofollow" target="_blank">|<em class="kl">代码库</em> </a> <em class="kl"> (Tensorflow 实现)</em></li></ul></div></div>    
</body>
</html>