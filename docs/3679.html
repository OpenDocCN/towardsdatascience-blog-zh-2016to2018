<html>
<head>
<title>Transfer Learning in Tensorflow (VGG19 on CIFAR-10): Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Tensorflow 中的迁移学习(CIFAR-10 上的 VGG19):第 1 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transfer-learning-in-tensorflow-9e4f7eae3bb4?source=collection_archive---------2-----------------------#2018-06-07">https://towardsdatascience.com/transfer-learning-in-tensorflow-9e4f7eae3bb4?source=collection_archive---------2-----------------------#2018-06-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/ec0a53dfcd671ca2341e6e0112e07242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*MTPz5zdrPPcPAx7a1jIUNw.jpeg"/></div></figure><h1 id="37c2" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">简介</strong></h1><p id="e99c" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><em class="lq">这个故事的第二部分可以在这里找到</em><a class="ae lr" rel="noopener" target="_blank" href="/transfer-learning-in-tensorflow-5d2b6ad495cb"><em class="lq"/></a><em class="lq">。</em></p><p id="9041" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">这个故事介绍了如何用预训练的 VGG19 模型训练 CIFAR-10 数据集。我将使用<a class="ae lr" href="https://github.com/taehoonlee/tensornets" rel="noopener ugc nofollow" target="_blank"> tensornets </a>中包含的 VGG19。本文的主要目标是理解迁移学习的概念，以及在这个过程中应该关注哪些步骤。你可以在这里找到这个故事的笔记本。</p><p id="ee6c" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">我之前写过一个<a class="ae lr" href="https://github.com/deep-diver/CIFAR10-img-classification-tensorflow" rel="noopener ugc nofollow" target="_blank">笔记本</a>和一个<a class="ae lr" href="https://medium.com/@parkchansung/cifar-10-image-classification-in-tensorflow-5b501f7dc77c" rel="noopener">故事</a>关于建立经典 CNN 模型来训练 CIFAR-10 数据集。这样，我可以比较最先进的 CNN 模型和基本的 CNN 模型之间的性能。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="f862" class="ju jv iq bd jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn mi kp kq kr bi translated"><strong class="ak">迁移学习</strong></h1><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mj"><img src="../Images/fa35a7f20b2e5660cae3cea405aa6281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C462zlZ_1yaiZxzQ1S3a-A.jpeg"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk"><strong class="bd mw">Winners of ILSVRC since ‘10</strong></figcaption></figure><p id="fac3" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">自 2012 年 AlexNet 出现以来，基于深度学习的图像分类任务得到了极大的改善。一路走来，已经提出了很多 CNN 模型。即使其中一些没有赢得 ILSVRC，但它们如 VGG16 因其简单和低损失率而受到欢迎。</p><p id="956d" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">即使使用高端 GPU，针对整个 ImageNet 数据库训练模型也需要几周时间。如果我们能够重用来自 ILSVRC 的经过验证的模型，这将是理想的，这样我们就不需要在训练和验证上花费大量时间。迁移学习正是我们想要的。</p><p id="79e2" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">迁移学习就是从别人那里借用 CNN 架构及其预先训练好的参数。当我们在预训练参数的基础上训练我们自己的数据时，我们可以很容易地达到目标精度。</p><h1 id="cb8e" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">型号选择</h1><p id="4f46" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在潜入之前，你必须选择选择哪种型号。这完全与你的物理环境和你的团队的目标有关。</p><p id="c2c3" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">由于 ILSVRC 要求模型对 1000 种图像进行分类，因此一些建议的模型无法显示出超强的性能。然而，如果你的目标是对 10 到 100 类图像进行分类，这些模型可能适合你的情况。</p><p id="0325" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">随着 CNN 模型的发展，最近的模型显示出相当复杂的结构。如果我们想要改变/修改整个层的某些部分，或者如果我们想要解决哪个部分是我们问题的瓶颈，理解模型在幕后如何工作是至关重要的。</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/6318bbfccfbce02be69364b1d3d60161.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*didUfSkj3y_q_gQT9lhdnA.jpeg"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">from <a class="ae lr" href="https://arxiv.org/abs/1605.07678" rel="noopener ugc nofollow" target="_blank">AN ANALYSIS OF DEEP NEURAL NETWORK MODELS FOR PRACTICAL APPLICATIONS</a></figcaption></figure><p id="2be2" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">模型有多大？这通常与参数数量或层数有关。他们会提示你需要花多长时间进行训练。特别是，当你使用 GPU 硬件时，应该充分考虑参数的数量，因为 GPU 的内存资源有限。</p><p id="aa8c" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">对于我来说，出于某些原因，我选择了 VGG19 型号。<strong class="ku ir">第一名</strong>，尽管它没有赢得 ILSVRC，但它取得了第二名的好成绩。我只需要 10 类图像，所以我认为 VGG19 对于 CIFAR-10 已经足够了。<strong class="ku ir">第二</strong>，VGG19 架构很简单。如果你了解基本的 CNN 模型，你会立刻注意到 VGG19 看起来很相似。<strong class="ku ir">第三个</strong>，我有 11GB 内存的英伟达 GTX 1080Ti。这不是最好的选择，但我认为即使 VGG19 的尺寸很大，运行 VGG19 也足够了。<strong class="ku ir">最后</strong>，由于很多人使用 VGG16，我想尝试一下 VGG19。</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/a1bded4074c3cd8ccd871a0c405441e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*cufAO77aeSWdShs3ba5ndg.jpeg"/></div></figure></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="6622" class="ju jv iq bd jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn mi kp kq kr bi translated">模型实现的选择</h1><p id="3c0b" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">实际上，您可以为所选的 CNN 模型使用自己的实现。因为来自 ILSVRC 的模型在他们的网页上分享他们的成果，包括权重，你可以下载(像<a class="ae lr" href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" rel="noopener ugc nofollow" target="_blank"> VGG </a>)并把权重注入到你的实现中。</p><p id="6ff3" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">然而，不实现模型本身，而是从文件和验证任务中转换/注入权重，这需要相当长的时间。幸运的是，Github 中有一些开源的实现。只需要在搜索你想要哪个 CNN 模型，你要像 Tensorflow 或者 PyTorch 一样达到哪个平台(框架)就可以了。</p><p id="06e7" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">对于我来说，我用的是 Tensorflow，碰巧选择了 VGG19。我发现了两个开源实现，<a class="ae lr" href="https://github.com/taehoonlee/tensornets" rel="noopener ugc nofollow" target="_blank"> tensornets </a>和<a class="ae lr" href="https://github.com/machrisaa/tensorflow-vgg" rel="noopener ugc nofollow" target="_blank"> vgg-tensorflow </a>。有趣的是，tensornets 提供了几乎所有流行的 CNN 模型，包括像 YOLO 这样的本地化模型。我想我将来可能会经常使用它们中的一些，所以这就是为什么我选择 tensornets 来熟悉它。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="7ade" class="ju jv iq bd jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn mi kp kq kr bi translated">知道如何创建模型</h1><p id="13a0" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">没有比仔细阅读 README 文档更简单的方法来了解所选实现的用法。一般来说，第一步是知道如何创建/构建模型图。建立模型后，您可以将预先训练的参数(权重)加载到模型中。不用说，没有预先训练好的参数，就不是迁移学习而只是借用架构。</p><figure class="mk ml mm mn gt jr"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="e878" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">具体来说，对于张量网，VGG19()创建模型。你只需要指定两个自定义参数，<strong class="ku ir"> <em class="lq"> is_training </em> </strong>，和<strong class="ku ir"> <em class="lq"> classes </em> </strong>。当您要针对 ImageNet 以外的数据集训练模型时，应将<strong class="ku ir"> <em class="lq"> is_training </em> </strong>设置为<strong class="ku ir"> <em class="lq"> True </em> </strong>。<strong class="ku ir"> <em class="lq"> classes </em> </strong>是要预测的图像的类别数，因此设置为 10，因为数据集来自 CIFAR-10。</p><p id="e9ac" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">需要记住的一点是，输入张量的形状应该始终是<strong class="ku ir"> <em class="lq">【无，224，224，3】</em></strong>。因为来自 ILSVRC 的每个模型都是在 ImageNet 提供的形状为(224，224，3)的图像上训练的，所以即使您自己的数据具有不同大小的图像，这也不应该改变。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="5ef9" class="ju jv iq bd jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn mi kp kq kr bi translated">了解最后一层</h1><p id="f790" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">您选择的模型的实际实现通常会有所不同。您应该知道模型的最后一层是什么，以便应用损失函数、优化器和执行准确性测试。通过代码本身，你可以很容易地发现模型是如何构建的。以下截图摘自 tensornets github repo。</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi na"><img src="../Images/89fbcaf364d4a7850c90d7e7c9de0f64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*buQWdhHsmhPpgpus2hWOPw.png"/></div></div></figure><p id="e202" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">可以看到，tensornets 的 VGG19 返回最后一层作为 softmax 激活函数。稍微了解 Tensorflow 的话，<strong class="ku ir"><em class="lq">TF . nn . soft max _ cross _ entropy _ with _ logits()</em></strong>函数用的有些重。然而，这个函数是为了同时应用 softmax 函数和交叉熵损失函数，所以这里不应该用于张量网。相反，可以使用仅提供交叉熵损失函数的<strong class="ku ir"><em class="lq">TF . losses . soft max _ cross _ entropy()</em></strong>函数。除了应用内部 softmax 函数之外，这一个与前一个等效。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><p id="28cd" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">这是第一部分的全部内容。<a class="ae lr" rel="noopener" target="_blank" href="/transfer-learning-in-tensorflow-5d2b6ad495cb">在下一部分</a>中，我会解释以下问题。</p><ol class=""><li id="ceee" class="nb nc iq ku b kv ls kz lt ld nd lh ne ll nf lp ng nh ni nj bi translated">如何进行预训练的负重？</li><li id="d52c" class="nb nc iq ku b kv nk kz nl ld nm lh nn ll no lp ng nh ni nj bi translated">如何重新缩放图像大小以适合？</li><li id="d86b" class="nb nc iq ku b kv nk kz nl ld nm lh nn ll no lp ng nh ni nj bi translated">怎么训练？如何衡量准确度？</li><li id="92c3" class="nb nc iq ku b kv nk kz nl ld nm lh nn ll no lp ng nh ni nj bi translated">表演</li></ol></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><p id="573e" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">我的深度学习背景是<a class="ae lr" href="https://www.udacity.com/course/deep-learning-nanodegree--nd101" rel="noopener ugc nofollow" target="_blank">uda city { Deep Learning N</a>D&amp;<a class="ae lr" href="https://www.udacity.com/course/ai-artificial-intelligence-nanodegree--nd898" rel="noopener ugc nofollow" target="_blank">AI-nd</a>with contentrations(<a class="ae lr" href="https://www.udacity.com/course/computer-vision-nanodegree--nd891" rel="noopener ugc nofollow" target="_blank">CV</a>，<a class="ae lr" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank"> NLP </a>，VUI)}，<a class="ae lr" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank">Coursera Deep Learning . AI Specialization</a>(AI-ND 被拆分成了 4 个不同的部分，我是和之前版本的 ND 一起完成的)。还有，我目前正在服用<a class="ae lr" href="https://www.udacity.com/course/data-analyst-nanodegree--nd002" rel="noopener ugc nofollow" target="_blank"> Udacity 数据分析师 ND </a>，已经 80%完成。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><p id="fbbd" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">最近在<a class="ae lr" href="https://neptune.ai/" rel="noopener ugc nofollow" target="_blank"> neptune.ai </a>上发现了一篇非常有用的关于迁移学习的博文。由于我的帖子有些过时，如果你有机会，请查看更多最新技术，更深入的解释。</p><ul class=""><li id="d6e1" class="nb nc iq ku b kv ls kz lt ld nd lh ne ll nf lp np nh ni nj bi translated"><a class="ae lr" href="https://neptune.ai/blog/transfer-learning-guide-examples-for-images-and-text-in-keras" rel="noopener ugc nofollow" target="_blank">迁移学习指南:包含 Keras 中图像和文本示例的实用教程</a></li></ul></div></div>    
</body>
</html>