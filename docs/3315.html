<html>
<head>
<title>Build simple artificial neural networks with popular deep learning frameworks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用流行的深度学习框架构建简单的人工神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3?source=collection_archive---------13-----------------------#2018-05-01">https://towardsdatascience.com/building-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3?source=collection_archive---------13-----------------------#2018-05-01</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="a694" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">使用TensorFlow、Keras、PyTorch和MXNet/Gluon</h2></div><p id="9ba0" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">几周前，我<a class="ae lf" rel="noopener" target="_blank" href="/how-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37">完成了构建一个非常简单的神经网络</a>的步骤，并在go中从头开始实现了它。然而，已经有许多深度学习框架可用，所以如果你想使用深度学习作为解决问题的工具，从头开始通常不是你要做的。</p><p id="f655" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">问题是，在众多深度学习框架中，我应该使用哪一个？比较深度学习框架的方法有很多。这是数据孵化器最近(2017年9月)<a class="ae lf" href="https://blog.thedataincubator.com/2017/10/ranking-popular-deep-learning-libraries-for-data-science/" rel="noopener ugc nofollow" target="_blank">的排名，根据他们的Github、Stack Overflow和Google搜索结果得分，给出了一个有趣的人气排名。</a></p><figure class="lh li lj lk gu ll gi gj paragraph-image"><div class="gi gj lg"><img src="../Images/75ce5ceb00cb2306b6103df70f1f62e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*Pw1CB8YDACzUDwwg9SXHTA.png"/></div><figcaption class="lo lp gk gi gj lq lr bd b be z dk">Deep learning frameworks popularity (data from <a class="ae lf" href="https://blog.thedataincubator.com/2017/10/ranking-popular-deep-learning-libraries-for-data-science/" rel="noopener ugc nofollow" target="_blank">https://blog.thedataincubator.com/2017/10/ranking-popular-deep-learning-libraries-for-data-science/</a>)</figcaption></figure><p id="83ac" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">从结果来看，很明显TensorFlow无疑是最受欢迎的框架，随着Keras现在成为TensorFlow本身的一部分，事情在不久的将来不会有太大变化。此外，几乎所有流行的深度学习框架都有Python APIs，因此TensorFlow/Keras与Python的结合似乎是正确的选择。</p><p id="19a8" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">尽管如此，我对其他一些框架很好奇，所以我开始了一段小小的旅程，编写与我在这些框架中所做的相同(或几乎相同)的简单人工神经网络进行比较。</p><p id="ff13" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">作为快速复习，我创建的神经网络是一个简单的前馈神经网络，通常也称为多级感知器(MLP)。使用这个简单的MLP，我获得了MNIST 6万个手写数字的数据集，并用它来训练神经网络。之后，我使用10，000个手写数字的测试数据集来测试神经网络的准确性。</p><figure class="lh li lj lk gu ll gi gj paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gi gj ls"><img src="../Images/3d8a62d6c610545509913f4883ceef3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*msFjfisfJ1D5Ok9WI3079Q.png"/></div></div><figcaption class="lo lp gk gi gj lq lr bd b be z dk">Our simple artificial neural network</figcaption></figure><p id="5458" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">神经网络有3层，第一(输入)层有784个神经元(28×28像素)，第二(隐藏)层有200个神经元，最后(输出)层有10个神经元。我用sigmoid函数作为激活函数，也用均方差作为损失函数。最后，我用0.1作为学习率，完全不使用任何偏向神经元。</p><p id="c234" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">以下所有实施都遵循相同的一般步骤:</p><ol class=""><li id="2fbc" class="lx ly iu kl b km kn kp kq ks lz kw ma la mb le mc md me mf bi translated">设置参数并加载数据集(大多数框架都有加载标准数据集的方法，如MNIST)</li><li id="2e9a" class="lx ly iu kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">通过创建一个创建并返回神经网络的<code class="fe ml mm mn mo b">mlp</code>函数来定义神经网络</li><li id="6724" class="lx ly iu kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">定义<code class="fe ml mm mn mo b">train</code>功能</li><li id="be39" class="lx ly iu kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">定义<code class="fe ml mm mn mo b">predict</code>功能</li><li id="0c01" class="lx ly iu kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">创建一个main，允许用户首先使用训练数据集(60，000个图像)进行训练，然后使用测试数据集(10，000个图像)进行预测</li></ol><p id="e5a9" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这种用MNIST数据集对数字进行的手写识别在深度学习教程中经常使用，它几乎是编写深度学习程序的“hello world”。不过，作为免责声明，您在下面看到的实现没有经过任何方式的优化，也不是最终的实现方式。事实上，还有许多其他更好的方法，这只是几个例子。</p><p id="5c96" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">现在我们开始，先看看如何在TensorFlow中实现这个。</p><h1 id="e897" class="mp mq iu bd mr ms mt mu mv mw mx my mz ka na kb nb kd nc ke nd kg ne kh nf ng bi translated">张量流</h1><p id="652a" class="pw-post-body-paragraph kj kk iu kl b km nh jv ko kp ni jy kr ks nj ku kv kw nk ky kz la nl lc ld le in bi translated">TensorFlow最初由从事谷歌大脑项目的研究人员和工程师开发，供内部使用，并于2015年开源。这是迄今为止最受欢迎的深度学习框架。</p><p id="7319" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">在TensorFlow上运行的更著名的项目包括<a class="ae lf" href="https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BDeepMind%5D(https://deepmind.com/)" rel="noopener ugc nofollow" target="_blank"> DeepMind </a>(开发AlphaGo的谷歌公司)，它在2016年从Torch转换为TensorFlow。</p><p id="f46d" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这个实现使用TensorFlow 1.6。我们开始吧。</p><h2 id="40e1" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">设置参数并加载数据集</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="f83b" class="nm mq iu mo b gz oc od l oe of">import TensorFlow as tf<br/>import argparse<br/>import numpy as np<br/>from TensorFlow.examples.tutorials.mnist import input_data<br/><br/># parameters<br/>inputs, hiddens, outputs = 784, 200, 10<br/>learning_rate = 0.01<br/>epochs = 50<br/>batch_size = 20<br/><br/>#loading the datasets<br/>mnist = input_data.read_data_sets("./mnist/", one_hot=True)</span></pre><p id="c2b3" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">相当简单，不言自明。请注意，我们将数据输出设置为<a class="ae lf" href="https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BOne-hot%20-%20Wikipedia%5D(https://en.wikipedia.org/wiki/One-hot)" rel="noopener ugc nofollow" target="_blank">单次触发</a>。这仅仅意味着具有最高值的ndarray元素的位置是正确的。</p><h2 id="499f" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">定义神经网络</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="73cf" class="nm mq iu mo b gz oc od l oe of"># a random generator using uniform<br/>def random(r, c, v):<br/>    return tf.random_uniform([r,c], minval=-1/tf.sqrt(float(v)), maxval=1/tf.sqrt(float(v)))<br/><br/># the neural network<br/>def mlp(x, hidden_weights=None, output_weights=None):<br/>    if hidden_weights == None:<br/>        hidden_weights = tf.Variable(random(inputs, hiddens, inputs), name="hidden_weights")<br/>    if output_weights == None:<br/>        output_weights = tf.Variable(random(hiddens, outputs, hiddens), name="output_weights")<br/>    hidden_outputs = tf.matmul(x, hidden_weights)<br/>    hidden_outputs = tf.nn.sigmoid(hidden_outputs)  <br/>    final_outputs = tf.matmul(hidden_outputs, output_weights)<br/>    final_outputs = tf.nn.sigmoid(final_outputs)<br/>    return final_outputs</span></pre><p id="e577" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这是我们定义神经网络的地方。相对来说比较直接。如果没有传入隐藏和输出权重，则使用<code class="fe ml mm mn mo b">tf.random_uniform</code>函数随机生成权重。这发生在我们训练神经网络的时候。</p><figure class="lh li lj lk gu ll gi gj paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gi gj og"><img src="../Images/7692f04617b30bf85769dfaba8d26425.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDPSdCcvYyuFSwSRo_aJ1g.png"/></div></div><figcaption class="lo lp gk gi gj lq lr bd b be z dk">How a neuron works</figcaption></figure><p id="9531" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">正如在<a class="ae lf" href="https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/" rel="noopener ugc nofollow" target="_blank">我之前创建的神经网络</a>一样，我们首先将输入<code class="fe ml mm mn mo b">x</code>与隐藏权重相乘(使用<code class="fe ml mm mn mo b">tf.matmul</code>)以获得隐藏输出。请记住，我们正在处理矩阵，因此<code class="fe ml mm mn mo b">tf.matmul</code>实际上是一个点积函数，隐藏权重和输入都是矩阵。</p><p id="2f04" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">隐藏的输出然后通过一个激活函数，在这个例子中，是一个<code class="fe ml mm mn mo b">sigmoid</code>函数。然后将输出乘以输出权重，得到最终输出。</p><p id="a3a5" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">最终输出在再次通过sigmoid激活函数后返回。</p><h2 id="01c2" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">定义列车功能</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="1ec4" class="nm mq iu mo b gz oc od l oe of"># training with the train dataset<br/>def train(x, y):<br/>    final_outputs = mlp(x)<br/>    errors = tf.reduce_mean(tf.squared_difference(final_outputs, y))<br/>    optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(errors)<br/>    init_op = tf.global_variables_initializer()<br/>    saver = tf.train.Saver()<br/>    with tf.Session() as sess:<br/>        sess.run(init_op)<br/>        total_batch = int(len(mnist.train.labels) / batch_size)<br/>        for epoch in range(epochs):<br/>            avg_error = 0<br/>            for i in range(total_batch):<br/>                batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)<br/>                _, c = sess.run([optimiser, errors], feed_dict={x: batch_x, y: batch_y})<br/>                avg_error += c / total_batch<br/>            print("Epoch [%d/%d], error: %.4f" %(epoch+1, epochs, avg_error))<br/>        print("\nTraining complete!")<br/>        saver.save(sess, "./model")</span></pre><p id="6443" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">让我们看看我们如何训练我们的神经网络模型。首先，我们使用<code class="fe ml mm mn mo b">mlp</code>函数创建它，向它传递输入。我们还将我们的误差函数恰当地定义为目标和输出之间的平方差(均方误差)。</p><p id="adb0" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">接下来，我们定义优化器，我们在这里使用<a class="ae lf" href="https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BEverything%20you%20need%20to%20know%20about%20Adam%20Optimizer%20%E2%80%93%20Nishant%20Nikhil%20%E2%80%93%20Medium%5D(https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218)" rel="noopener ugc nofollow"> Adam </a>优化器，向它传递学习率和我们的误差函数。当我第一次尝试这个的时候，我使用了梯度下降优化器，但是这些值需要很长时间才能收敛。当我切换到Adam优化器时，它收敛得很好，所以我改用Adam优化器。</p><p id="8db9" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">现在我们有了优化器，我们初始化所有的变量并定义一个保护程序，这样我们就可以保存模型了。我们启动一个会话，并按时期运行小批量，将我们之前加载的训练数据集传递给它。</p><p id="9eb4" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">一旦我们完成训练，我们保存模型。张量流模型由两部分组成。第一个是元图，它保存了张量流图上的信息。这被保存到一个扩展名为<code class="fe ml mm mn mo b">.meta</code>的文件中，在这种情况下，它将是<code class="fe ml mm mn mo b">model.meta</code>。</p><p id="3a32" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">第二个是一堆检查点文件。<code class="fe ml mm mn mo b">model.index</code>存储变量名称和形状的列表，而<code class="fe ml mm mn mo b">model.data-00000-of-00001</code>存储变量的实际值。</p><p id="008e" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">稍后，当我们想要加载模型进行预测时，我们将重用这些文件。</p><h2 id="c829" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">定义预测函数</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="9a7b" class="nm mq iu mo b gz oc od l oe of"># predicting with the test dataset<br/>def predict(x):    <br/>    saver = tf.train.import_meta_graph("./model.meta")<br/>    with tf.Session() as sess:<br/>        saver.restore(sess, tf.train.latest_checkpoint("./"))<br/>        graph = tf.get_default_graph()<br/>        hidden_weights = graph.get_tensor_by_name("hidden_weights:0")<br/>        output_weights = graph.get_tensor_by_name("output_weights:0")<br/>        final_outputs = mlp(x, hidden_weights, output_weights)       <br/>        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(final_outputs, 1))<br/>        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))<br/>        tf.summary.scalar('accuracy', accuracy)          <br/>        print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))</span></pre><p id="4c4a" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">在我们训练好模型之后，我们会想要一些可以用来预测值的东西。在这种情况下，我们实际上想要的是对测试数据集中的10，000张图像运行我们的<code class="fe ml mm mn mo b">predict</code>函数，看看我们的训练模型正确地得到了多少张图像。</p><p id="2a7a" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们从导入来自<code class="fe ml mm mn mo b">model.meta</code>文件的元图开始。接下来，我们恢复检查点，并使用默认的图来获得隐藏的权重和输出权重各自的名称。</p><p id="90b3" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">最后，我们通过调用<code class="fe ml mm mn mo b">mlp</code>函数并向其传递保存的权重来恢复训练好的模型。</p><p id="a583" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">有了训练好的模型，当我们通过测试数据集时，我们试图预测输出，并获得模型的准确性。<code class="fe ml mm mn mo b">predict</code>功能打印出所有测试图像的预测精度。</p><h2 id="d249" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">训练然后预测</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="fb20" class="nm mq iu mo b gz oc od l oe of">if __name__ == "__main__":<br/>    x = tf.placeholder(tf.float32, [None, inputs])<br/>    y = tf.placeholder(tf.float32, [None, outputs])       <br/>    parser = argparse.ArgumentParser()<br/>    parser.add_argument("--action", type=str, default="predict" )<br/>    FLAGS, unparsed = parser.parse_known_args()<br/>    if FLAGS.action == "predict":<br/>        predict(x)<br/>    if FLAGS.action == "train":<br/>        train(x, y)</span></pre><p id="59f9" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">最后一点很简单，它只是一个主要的功能，允许用户进行预测或训练。这一部分在其他实现中实际上是相同的，所以我以后不会再展示这段代码了。</p><p id="8c9b" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这是结果。</p><figure class="lh li lj lk gu ll gi gj paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gi gj oh"><img src="../Images/26efddd48b2d5f500aa01e244939081c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pWLecqj6VwaiLd3owbuY9Q.png"/></div></div></figure><p id="5444" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">模型预测正确率为97.25%，不算太好但还可以。现在我们接下来看看Keras。</p><h1 id="3596" class="mp mq iu bd mr ms mt mu mv mw mx my mz ka na kb nb kd nc ke nd kg ne kh nf ng bi translated">Keras(在张量流上)</h1><p id="3a81" class="pw-post-body-paragraph kj kk iu kl b km nh jv ko kp ni jy kr ks nj ku kv kw nk ky kz la nl lc ld le in bi translated">Keras不是一个独立的框架，而是一个建立在TensorFlow、Theano和CNTK之上的接口。Keras设计用于快速原型制作，易于使用和用户友好。</p><p id="c4e5" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">2017年，TensorFlow决定<a class="ae lf" href="https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BBig%20deep%20learning%20news:%20Google%20TensorFlow%20chooses%20Keras%20%C2%B7%20fast.ai%5D(http://www.fast.ai/2017/01/03/keras/)" rel="noopener ugc nofollow" target="_blank">在TensorFlow的核心库</a>中支持Keras，尽管Keras本身没有任何变化。</p><p id="a97b" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">让我们看看Keras的情况有什么不同。</p><h2 id="a375" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">设置参数并加载数据集</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="97d8" class="nm mq iu mo b gz oc od l oe of">import argparse<br/>from keras.models import Sequential<br/>from keras.datasets import mnist<br/>from keras.layers import Dense<br/>from keras.models import load_model<br/>from keras import optimizers<br/>from keras import utils<br/><br/># parameters<br/>inputs, hiddens, outputs = 784, 100, 10<br/>learning_rate = 0.01<br/>epochs = 50<br/>batch_size = 20<br/><br/># loading datasets<br/>(train_images, train_labels), (test_images, test_labels) = mnist.load_data()<br/>train_images = train_images.reshape(60000, 784).astype('float32')/255<br/>train_labels = utils.to_categorical(train_labels, outputs)<br/>test_images = test_images.reshape(10000, 784).astype('float32')/255<br/>test_labels = utils.to_categorical(test_labels, outputs)</span></pre><p id="732d" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">设置数据集似乎比以前更加复杂，但这没什么大不了的，事实上，更清楚的是，我们正在将训练和测试数据集重塑为正确的形状和大小。</p><h2 id="3cd5" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">定义列车功能</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="8cde" class="nm mq iu mo b gz oc od l oe of"># training with the train dataset<br/>def train():<br/>    model = Sequential()<br/>    model.add(Dense(hiddens, activation='sigmoid', input_shape=(inputs,)))<br/>    model.add(Dense(outputs, activation='sigmoid'))<br/>    sgd = optimizers.Adam(lr=learning_rate)<br/>    model.compile(optimizer=sgd, loss='mean_squared_error')<br/>    model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs)<br/>    model.save('mlp_model.h5')</span></pre><p id="d326" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">你可能会注意到，我没有在这里定义神经网络。我本来可以创建一个单独的<code class="fe ml mm mn mo b">mlp</code>函数来完成这项工作，但这并不是真正必要的，因为我使用了一个内置的Keras模型，名为<code class="fe ml mm mn mo b">Sequential</code>，并简单地在它上面堆叠层来构建网络。</p><p id="3b7d" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">前两行添加了隐藏层和输出层(根据给定的隐藏层的输入形状，默认情况下会采用后面的输入)。这包括激活功能<code class="fe ml mm mn mo b">sigmoid</code>。</p><p id="a698" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">接下来，我们使用内置的Adam优化器<code class="fe ml mm mn mo b">optimizers.Adam</code>来定义优化器。</p><p id="d099" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">该模型由优化器编译，并被赋予一个误差(或损失)函数<code class="fe ml mm mn mo b">mean_squared_error</code>，该函数也是内置的。</p><p id="245d" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">最后，我们使用<code class="fe ml mm mn mo b">fit</code>方法，使用图像和标签训练模型，具有给定的批量大小和时期数。</p><p id="914f" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">和以前一样，我们在训练模型之后保存它。</p><h2 id="3fde" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">定义预测函数</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="1701" class="nm mq iu mo b gz oc od l oe of"># predicting the test dataset<br/>def predict():<br/>    model = load_model("mlp_model.h5")<br/>    error = model.evaluate(test_images, test_labels)<br/>    print("accuracy:", 1 - error)</span></pre><p id="d7de" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">如果您认为训练函数相当简单，请查看预测函数！您只需要加载模型，然后用它来评估测试图像和标签！</p><h2 id="0854" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">训练然后预测</h2><p id="d815" class="pw-post-body-paragraph kj kk iu kl b km nh jv ko kp ni jy kr ks nj ku kv kw nk ky kz la nl lc ld le in bi translated">以下是你训练时看到的。</p><figure class="lh li lj lk gu ll gi gj paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gi gj oi"><img src="../Images/a1af468268de6829ea30c95085b64cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xPtgn_l3Lqusb0By5M0hUw.png"/></div></div></figure><p id="bbbd" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这是预测的结果。</p><figure class="lh li lj lk gu ll gi gj paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gi gj oi"><img src="../Images/12a3474699c92d2bca4464c5eeb85169.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f16euUlYhABcGc20EuZeKA.png"/></div></div></figure><p id="c39c" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这里的准确性要好得多，我们在检测正确的图像时有99.42%的准确性。</p><h1 id="db61" class="mp mq iu bd mr ms mt mu mv mw mx my mz ka na kb nb kd nc ke nd kg ne kh nf ng bi translated">PyTorch</h1><p id="5b68" class="pw-post-body-paragraph kj kk iu kl b km nh jv ko kp ni jy kr ks nj ku kv kw nk ky kz la nl lc ld le in bi translated"><a class="ae lf" href="https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BPyTorch%5D(http://pytorch.org/)" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>顾名思义，就是Torch框架的Python版本。Torch最初是用C开发的，有一个使用Lua编程语言的包装器。PyTorch主要是由脸书的人工智能研究小组开发的，而是用Python包装了Torch二进制文件。</p><p id="2893" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">PyTorch的一个关键特性是能够使用动态计算图修改现有的神经网络，而不必从头开始重建。PyTorch将其描述为使用和重放录音机，它的灵感来自其他作品，如<a class="ae lf" href="https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BGitHub%20-%20HIPS/autograd:%20Efficiently%20computes%20derivatives%20of%20numpy%20code.%5D(https://github.com/HIPS/autograd)" rel="noopener ugc nofollow" target="_blank">亲笔签名</a>和<a class="ae lf" href="https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BChainer:%20A%20flexible%20framework%20for%20neural%20networks%5D(https://chainer.org/)" rel="noopener ugc nofollow" target="_blank"> Chainer </a>。</p><p id="ad76" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">在实现简单的神经网络的过程中，我没有机会正确地使用这个特性，但这似乎是一个建立神经网络的有趣方法，我想在以后进行更多的探索。</p><p id="653c" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">让我们看看PyTorch如何为我们的简单神经网络工作。</p><h2 id="8c16" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">设置参数并加载数据集</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="757c" class="nm mq iu mo b gz oc od l oe of">import torch<br/>import argparse<br/>import torch.nn as nn<br/>import torch.optim as optim<br/>from torchvision import datasets, transforms<br/>from torch.autograd import Variable<br/><br/># parameters<br/>inputs, hiddens, outputs = 784, 200, 10<br/>learning_rate = 0.01<br/>epochs = 50<br/>batch_size = 20<br/><br/>transformation = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])<br/>train_dataset = datasets.MNIST('mnist/',train=True,transform=transformation, download=False)<br/>train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)<br/>test_dataset = datasets.MNIST('mnist/',train=False,transform=transformation, download=False)<br/>test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)</span></pre><p id="0b29" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">加载数据集需要几个步骤，但它们相当简单。值得注意的是，变换的平均值为0.1307，标准差为0.3081，这是MNIST数据集的平均值和标准差。</p><h2 id="9298" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">定义神经网络</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="dc78" class="nm mq iu mo b gz oc od l oe of">class mlp(nn.Module):<br/>    def __init__(self):<br/>        super(MLP, self).__init__()<br/>        self.sigmoid = nn.Sigmoid()<br/>        self.hidden_layer = nn.Linear(inputs, hiddens)<br/>        self.output_layer = nn.Linear(hiddens, outputs)<br/><br/>    def forward(self, x):<br/>        out = self.sigmoid(self.hidden_layer(x))<br/>        out = self.sigmoid(self.output_layer(out))<br/>        return out<br/><br/>    def name(self):<br/>        return "mlp"</span></pre><p id="23b4" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">定义神经网络很简单。我们在类中定义了一些方法，<code class="fe ml mm mn mo b">sigmoid</code>是<code class="fe ml mm mn mo b">nn.Sigmoid</code>，<code class="fe ml mm mn mo b">hidden_layer</code>和<code class="fe ml mm mn mo b">output_layer</code>是适当大小的线性层。</p><p id="bd23" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">然后，<code class="fe ml mm mn mo b">forward</code>方法将输入<code class="fe ml mm mn mo b">x</code>传递给隐藏层，然后传递给<code class="fe ml mm mn mo b">sigmoid</code>激活函数。之后，它进入输出层，并在返回输出之前再次调用<code class="fe ml mm mn mo b">sigmoid</code>激活函数。</p><h2 id="9f43" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">定义列车功能</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="17c9" class="nm mq iu mo b gz oc od l oe of">def train():<br/>    model = mlp()<br/>    loss = nn.MSELoss(size_average=False)<br/>    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)<br/>    for epoch in range(epochs):<br/>        avg_error = 0<br/>        for i, (images, labels) in enumerate(train_loader):<br/>            images = Variable(images.view(-1, inputs))<br/>            # Convert class label to one hot vector <br/>            one_hot = torch.FloatTensor(labels.size(0), 10).zero_()<br/>            target = one_hot.scatter_(1, labels.view((labels.size(0),1)), 1)            <br/>            target = Variable(target)<br/>            # Compute loss and gradient<br/>            optimizer.zero_grad()<br/>            out = model(images)<br/>            error = loss(out, target)<br/>            error.backward()<br/>            # Apply gradient<br/>            optimizer.step()<br/>            avg_error += error.data[0]<br/><br/>        avg_error /= train_loader.dataset.train_data.shape[0]<br/>        print ('Epoch [%d/%d], error: %.4f' %(epoch+1, epochs, avg_error))<br/>    # Save model to file<br/>    torch.save(model.state_dict(), 'model.pkl')</span></pre><p id="5543" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">与其他实现一样，我们首先创建神经网络模型、误差函数<code class="fe ml mm mn mo b">loss</code>(我们将其定义为均方误差损失函数)以及Adam优化器。</p><p id="c939" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们照常运行50个时期的训练。因为训练标签的格式不正确，所以我们需要将其转换为一个热点向量，<code class="fe ml mm mn mo b">target</code>。然后，我们使用<code class="fe ml mm mn mo b">loss</code>函数计算误差，将实际输出值和目标值传递给它，然后对其应用反向传播。</p><p id="2811" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">最后，我们在结束训练之前保存模型。有几种方法可以保存PyTorch模型。更通用的Python方式是将其保存为pickle文件，扩展名为<code class="fe ml mm mn mo b">.pkl</code>。这是我在这个实现中使用的。另一种方法是使用PyTorch自己的序列化机制，它保存到一个扩展名为<code class="fe ml mm mn mo b">.pth</code>的文件中。</p><h2 id="935c" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">定义预测函数</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="9994" class="nm mq iu mo b gz oc od l oe of">def predict():<br/>    model = mlp()<br/>    model.load_state_dict(torch.load('model.pkl'))<br/>    correct, total = 0, 0<br/>    for images, labels in test_loader:<br/>        images = Variable(images.view(-1, inputs))<br/>        out = model(images)<br/>        _, predicted = torch.max(out.data, 1)<br/>        total += labels.size(0)<br/>        correct += (predicted == labels).sum()<br/>    print('accuracy: %0.2f %%' % (100.0 * correct / total))</span></pre><p id="5ea9" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">预测比训练简单。这里我们需要首先创建一个神经网络，并用保存的状态加载它，以重现训练好的模型。然后使用训练好的模型预测输出，然后使用标签检查它是否正确。最后，我们将所有正确预测的值加起来，并得到准确率的百分比。</p><h2 id="397e" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">训练然后预测</h2><p id="46fa" class="pw-post-body-paragraph kj kk iu kl b km nh jv ko kp ni jy kr ks nj ku kv kw nk ky kz la nl lc ld le in bi translated">这是结果。</p><figure class="lh li lj lk gu ll gi gj paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gi gj oj"><img src="../Images/72cb2168355605e3655c551afc15ffb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SWk4DC77CLrJhsxf56rWFw.png"/></div></div></figure><p id="ef30" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">正如你所看到的，网络不能以相同的学习速率在50个时期内完全收敛。这里的预测准确率相当差，只有95.17%。另一方面，当我切换到使用SGD优化器时，准确率提高到了98.29%。</p><h1 id="3359" class="mp mq iu bd mr ms mt mu mv mw mx my mz ka na kb nb kd nc ke nd kg ne kh nf ng bi translated">带有胶子的MXNet</h1><p id="3dfb" class="pw-post-body-paragraph kj kk iu kl b km nh jv ko kp ni jy kr ks nj ku kv kw nk ky kz la nl lc ld le in bi translated"><a class="ae lf" href="https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BMXNet:%20A%20Scalable%20Deep%20Learning%20Framework%5D(https://mxnet.incubator.apache.org/)" rel="noopener ugc nofollow" target="_blank"> MXNet </a>是Apache基金会的一个项目，目前正在Apache中孵化。它支持多种语言，并得到了许多大型行业参与者的支持，主要包括亚马逊和微软。</p><p id="f0fd" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated"><a class="ae lf" href="https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BWhy%20Amazon%20picked%20MXNet%20for%20deep%20learning%20%7C%20InfoWorld%5D(https://www.infoworld.com/article/3144025/cloud-computing/why-amazon-picked-mxnet-for-deep-learning.html)" rel="noopener ugc nofollow" target="_blank">亚马逊选择MXNet作为深度学习框架的首选</a>，因为它声称MXNet比其他框架更好地扩展和运行。MXNet模型是可移植的，也可以部署在设备上。2017年10月，亚马逊和微软为MXNet推出了一个名为Gluon 的新界面，以使深度学习更容易。</p><p id="eba1" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">胶子相对容易使用，从我的角度来看，建立我们简单的神经网络看起来也差不多。诚然，我可能还没有使用它的最佳能力。</p><p id="f649" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">让我们看看它是如何工作的。</p><h2 id="a446" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">设置参数并加载数据集</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="10ef" class="nm mq iu mo b gz oc od l oe of">import argparse<br/>import numpy as np<br/>import mxnet as mx<br/>from mxnet import nd, autograd, gluon<br/>from mxnet.gluon import nn<br/>from mxnet.gluon.data import vision<br/><br/># parameters<br/>inputs, hiddens, outputs = 784, 200, 10<br/>learning_rate = 0.01<br/>epochs = 50<br/>batch_size = 20<br/><br/>ctx = mx.cpu()<br/><br/>def transform(data, label):<br/>    return data.astype(np.float32)/255, label.astype(np.float32)<br/>    <br/>train_data = mx.gluon.data.DataLoader(vision.MNIST(train=True, transform=transform), batch_size, shuffle=True)<br/>test_data = mx.gluon.data.DataLoader(vision.MNIST(train=False, transform=transform), batch_size, shuffle=False)</span></pre><p id="8fba" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">与其他框架不同，您必须更加明确您希望操作的上下文在哪里运行。在这种情况下，我只在CPU上运行，所以我创建了一个基于CPU的上下文<code class="fe ml mm mn mo b">ctx</code>。</p><p id="25ae" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">加载数据集与其他框架没有太大区别。</p><h2 id="cc40" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">定义神经网络</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="8fb4" class="nm mq iu mo b gz oc od l oe of">def mlp():<br/>    model = nn.Sequential()<br/>    with model.name_scope():<br/>        model.add(nn.Dense(hiddens, activation="sigmoid"))<br/>        model.add(nn.Dense(outputs, activation="sigmoid"))<br/>        dist = mx.init.Uniform(1/np.sqrt(float(inputs)))<br/>        model.collect_params().initialize(dist, ctx=ctx) <br/>    return model</span></pre><p id="2823" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">定义神经网络相对简单，与Keras非常相似。我们简单地使用一个内置的模型，用适当的激活函数在它上面添加层，然后用从均匀分布中采样的随机值的上下文和权重初始化它。我在这里使用均匀分布是为了与早期的实现保持一致。我尝试了其他的发行版，但是结果有些相同，所以至少在这篇文章中我坚持使用这个发行版。</p><h2 id="f589" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">定义列车功能</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="16a1" class="nm mq iu mo b gz oc od l oe of">def train():<br/>    model = mlp()   <br/>    loss = gluon.loss.L2Loss()<br/>    optimizer = gluon.Trainer(model.collect_params(), "adam", {"learning_rate": learning_rate})<br/><br/>    for e in range(epochs):<br/>        cumulative_error = 0<br/>        for i, (data, labels) in enumerate(train_data):<br/>            data = data.as_in_context(ctx).reshape((-1, inputs))<br/>            labels = nd.one_hot(labels, 10, 1, 0).as_in_context(ctx)<br/>            with autograd.record():<br/>                output = model(data)<br/>                error = loss(output, labels)<br/>            error.backward()<br/>            optimizer.step(data.shape[0])<br/>            cumulative_error += nd.sum(error).asscalar()<br/>        print("Epoch [%d/%d]: error: %.4f" % (e+1, epochs, cumulative_error/len(train_data)))    <br/>    model.save_params("mxnet.model")</span></pre><p id="ed29" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">为了训练模型，我们首先用我们的<code class="fe ml mm mn mo b">mlp</code>函数创建它。我们使用<code class="fe ml mm mn mo b">L2Loss</code>定义一个误差函数<code class="fe ml mm mn mo b">loss</code>，它本质上是一个均方误差函数。</p><p id="3f84" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们还定义了一个优化器(在MXNet中称为<code class="fe ml mm mn mo b">Trainer</code>),它使用Adam优化器算法。</p><p id="8d99" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">接下来，我们枚举训练数据集，并将其整形为一个独热n数组。我们将训练数据集传递给经过训练的模型，以获得输出。输出和标签被传递给错误函数。</p><p id="b143" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">训练结束后，我们保存网络模型。MXNet允许我们用简单的<code class="fe ml mm mn mo b">save_params</code>方法保存参数。对文件名没有太大的要求，所以我们可以使用任何我们喜欢的名字。</p><h2 id="2a4d" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">定义预测函数</h2><pre class="lh li lj lk gu ny mo nz oa aw ob bi"><span id="87d0" class="nm mq iu mo b gz oc od l oe of">def predict():<br/>    model = mlp()<br/>    model.load_params("mxnet.model", ctx)<br/>    acc = mx.metric.Accuracy()<br/>    for i, (data, label) in enumerate(test_data):<br/>        data = data.as_in_context(ctx).reshape((-1, inputs))<br/>        label = label.as_in_context(ctx)<br/>        output = model(data)<br/>        predictions = nd.argmax(output, axis=1)<br/>        acc.update(preds=predictions, labels=label)<br/>    print("accuracy: %.2f %%" % (acc.get()[1] * 100))</span></pre><p id="00f2" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated"><code class="fe ml mm mn mo b">predict</code>函数通过从我们之前保存的文件中加载它来重新创建我们的训练模型。我们重塑测试数据集中的数据，并将其传递给已加载的训练模型，然后我们将预测作为输出。然后使用标签，我们发现预测的准确性。</p><h2 id="8bb8" class="nm mq iu bd mr nn no dn mv np nq dp mz ks nr ns nb kw nt nu nd la nv nw nf nx bi translated">训练然后预测</h2><p id="cc6e" class="pw-post-body-paragraph kj kk iu kl b km nh jv ko kp ni jy kr ks nj ku kv kw nk ky kz la nl lc ld le in bi translated">下面是使用MXNet框架和Gluon进行预测的结果。</p><figure class="lh li lj lk gu ll gi gj paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gi gj ok"><img src="../Images/0b74338f45baa9273d01c8d2d2d7865d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PrVarPpq4JwLU2r3HcJv6g.png"/></div></div></figure><p id="cd08" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">准确率为97.49%，与其他框架相当。</p><h1 id="4c85" class="mp mq iu bd mr ms mt mu mv mw mx my mz ka na kb nb kd nc ke nd kg ne kh nf ng bi translated">一些想法</h1><p id="cb60" class="pw-post-body-paragraph kj kk iu kl b km nh jv ko kp ni jy kr ks nj ku kv kw nk ky kz la nl lc ld le in bi translated">显然这个帖子没有所有的深度学习框架。这更像是在我探索各种框架时，漫无边际地浏览了几个激发我想象力的选定框架。我错过了很多流行的，包括Caffe和Caffe2，CNTK，Theano，Torch，Sonnet等等。</p><p id="786d" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我也没有做任何比较——这不是我的目的，任何比较都需要对这些框架有更深入的理解，也需要更多的时间。从某种意义上说，由于所有这些框架都在增长(当我在过去几周写这篇文章时，TensorFlow连续发布了1.7和1.8！)并且改变任何比较将会很快变得不准确。相反，我的目的是弄清楚实际编写深度学习软件有多容易，以及这些框架能在多大程度上帮助我做到这一点。</p><p id="5fbb" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">当我使用这些框架时，我意识到就目标而言，它们基本上是相同的。在每个框架中，目标总是有一个简单的方法来加载数据集，定义模型，训练模型，然后使用它来预测结果。实现的方式可能因框架而异，潜在的哲学可能不同，但目标是相同的。</p><p id="16b9" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">从某种意义上说，它非常类似于我过去20年一直使用的所有web框架。虽然这些年来已经创建了令人惊叹的web应用程序，但是web框架基本上以相同的方式工作，具有相同的视图、控制器和服务，并且使用HTTP。</p><p id="eb43" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">毫无疑问，我把一切都过于简单化了，但在某种意义上，我同时也感到安慰。</p><h1 id="6fd1" class="mp mq iu bd mr ms mt mu mv mw mx my mz ka na kb nb kd nc ke nd kg ne kh nf ng bi translated">源代码</h1><p id="7af3" class="pw-post-body-paragraph kj kk iu kl b km nh jv ko kp ni jy kr ks nj ku kv kw nk ky kz la nl lc ld le in bi translated">您可以在这里找到所有源代码:</p><p id="36e1" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated"><a class="ae lf" href="https://github.com/sausheong/pynn" rel="noopener ugc nofollow" target="_blank">https://github.com/sausheong/pynn</a></p></div></div>    
</body>
</html>