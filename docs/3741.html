<html>
<head>
<title>[ NIPS 2011 / Andrew Ng ] First Glance of Sparse Filtering in Tensorflow with Interactive Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[ NIPS 2011 /吴恩达 Tensorflow 中带交互代码的稀疏滤波一瞥</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nips-2011-andrew-ng-first-glance-of-sparse-filtering-in-tensorflow-with-interactive-code-659c4e84658e?source=collection_archive---------6-----------------------#2018-06-13">https://towardsdatascience.com/nips-2011-andrew-ng-first-glance-of-sparse-filtering-in-tensorflow-with-interactive-code-659c4e84658e?source=collection_archive---------6-----------------------#2018-06-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/c90a2729135791fe2ffb9a01d4ca7fe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/1*SI2jmQqX_ed2MLdyR7nxLQ.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/coding-fKsSCtWQ6MR7q" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="cc55" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">【Neil Bruce 博士，(<a class="ae jy" href="https://scholar.google.com/citations?user=Gnezf-4AAAAJ&amp;hl=en" rel="noopener ugc nofollow" target="_blank"> Google scholar Page </a>)我的导师推荐我阅读这篇论文，我真的很想获得对稀疏滤波更实际的理解。(所以才发这个帖子。)</p><p id="7136" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这篇文章中，我将首先看到不同降维技术的效果，包括…</p><p id="13ee" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="kx"> 1。</em> <a class="ae jy" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank"> <em class="kx">主成分分析</em> </a> <em class="kx"> <br/> 2。</em> <a class="ae jy" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html" rel="noopener ugc nofollow" target="_blank"> <em class="kx">独立成分分析。</em> </a> <em class="kx"> <br/> 3。</em> <a class="ae jy" href="http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit_transform" rel="noopener ugc nofollow" target="_blank"> <em class="kx">线性判别分析</em> </a> <em class="kx"> <br/> 4。</em><a class="ae jy" href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="noopener ugc nofollow" target="_blank"><em class="kx">t-分布随机邻居嵌入。</em> </a> <em class="kx"> <br/> 5。</em> <a class="ae jy" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html#examples-using-sklearn-decomposition-factoranalysis" rel="noopener ugc nofollow" target="_blank"> <em class="kx">因素分析</em> </a> <em class="kx"> <br/> 6。</em> <a class="ae jy" href="https://papers.nips.cc/paper/4334-sparse-filtering" rel="noopener ugc nofollow" target="_blank"> <em class="kx">稀疏过滤</em> </a></p><blockquote class="ky kz la"><p id="f9d4" class="jz ka kx kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated"><strong class="kb ir">请注意这篇帖子是为了我对稀疏滤波的理解。</strong></p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="1d94" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">降维</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/e7a96959ea6382acb7fd397a6acf53af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*MFH6z10P-VmWdVlzLcQLUw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from this <a class="ae jy" href="http://bigdata.csail.mit.edu/node/277" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="aea0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在继续之前，我想简单回顾一下降维的效果。但是请注意，这些算法背后的数学不在本文讨论范围之内，但是如果有人感兴趣，我会在下面链接一些好的资源。</p><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="ls lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Video from <a class="ae jy" href="https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw" rel="noopener ugc nofollow" target="_blank">StatQuest with Josh Starmer</a></figcaption></figure><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="ls lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Video from <a class="ae jy" href="https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw" rel="noopener ugc nofollow" target="_blank">StatQuest with Josh Starmer</a></figcaption></figure><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="ls lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Video from <a class="ae jy" href="https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw" rel="noopener ugc nofollow" target="_blank">StatQuest with Josh Starmer</a></figcaption></figure><p id="f8ab" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果你想知道 LDA 和 PCA 的区别，请<a class="ae jy" href="https://www.quora.com/What-is-the-difference-between-LDA-and-PCA-for-dimension-reduction" rel="noopener ugc nofollow" target="_blank">点击这里</a>或<a class="ae jy" href="https://www.youtube.com/watch?v=e4woe8GRjEI" rel="noopener ugc nofollow" target="_blank">点击这里</a>。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="178b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">示例:清晰分离的数据</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/968b2f341fef142dfccd072ba4a2e2e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/1*uG4M82-BC-iLIyx4exYw-A.gif"/></div></figure><p id="7b27" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">简单地说，我们有上面的数据点，不同的颜色代表不同的类别，每个数据点有 3 个轴(X，Y，Z)。现在让我们先来看看每个分解算法是如何分解这些数据的。(作为开始，我们将只把维数减少 1 倍。)</p><div class="ll lm ln lo gt ab cb"><figure class="lu jr lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/550994d57c05a7154997a4e68705e3f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*lXSX6CQ7PKGhm3jkKkeHKA.png"/></div></figure><figure class="lu jr lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/a5e8246f8c54558f01a0bfb1536bc396.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ux0bAa0Vxyx7HSkJlkk8Mg.png"/></div></figure><figure class="lu jr lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/72cb497d22b48ac329269cb61f9c390a.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*pRo2NaxFjBfzbEldGJhpPA.png"/></div></figure></div><div class="ab cb"><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/e4469f0e68f57305d6b6e54397fddcc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*39ToUWEbVIYRE3OmuT8tZw.png"/></div></figure><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/22cf8f525309fcbfa1d07986699a92b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*-5GSbZZhDzq0yI09-I_FmA.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk mf di mg mh">Different decomposition algorithm to reduce the dimension to 2</figcaption></figure></div><p id="35eb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">他们都做得很好，将相同的类聚集在一起，现在让我们看看何时将维度减少到 1。(删除二维)。</p><div class="ll lm ln lo gt ab cb"><figure class="lu jr lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/fc5fc2c6d285acb1d87def835eb43590.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*iOhTH37ekyXr0rAowYGnrA.png"/></div></figure><figure class="lu jr lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/67278672f8613a523c2c354033408836.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*EtnKE6Xn8tx9m4KQEN-oeA.png"/></div></figure><figure class="lu jr lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/6b7e63d77dbc0123f639d194ddc6c01f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ZbqPP3vA6lSxuw7rzVQGbA.png"/></div></figure></div><div class="ab cb"><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/b7111b4b38758d20344a215f4b7d8a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*Bm8zoeGz5jxU0NhLIoHC1A.png"/></div></figure><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/49588fb6baf011cf5f904151a1300f2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*vXziJ2t-Fmk-w1QKpOQKAQ.png"/></div></figure></div><p id="a67a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同样，当原始数据彼此明显分离时，我们可以看到大多数算法在降低维度方面做得很好，同时对每个类进行聚类。(虽然这里那里有一些离群值。)</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="445c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">示例:没有清晰分离的数据</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/3e2593610ee92acedf2f7cf9091ee802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/1*h4mxsMjt5MSDJ6wSO7Ji8Q.gif"/></div></figure><p id="9405" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在为了让事情变得更难，让我们来看一个没有从每个类中明确分离的例子。</p><div class="ll lm ln lo gt ab cb"><figure class="lu jr lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/cb00cc27ccd282156ba350b4e156b109.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*UDgRZew1FDhrtE_gJH-JHA.png"/></div></figure><figure class="lu jr lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/7915fb3cad7cf9dc48130f599bfbbd6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*telFOz_SzRYzD_FOZhTfsA.png"/></div></figure><figure class="lu jr lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/c96c2380f0e2c8fc1e27397697f9a7c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*xCEyUADU09KJMX0zjHYiFw.png"/></div></figure></div><div class="ab cb"><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/11565cf320d077764f210924c785b81c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*Ft9AoT3j6p3tXD1DE9n7Sg.png"/></div></figure><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/a4bccccf37111e4ac3cec00174963fbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*YE01wllF2oywGwtWTJrscA.png"/></div></figure></div><p id="0544" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从表面上看，大多数算法只是移除了 Z(深度)轴，最后让我们来看看将维度减少到 1 的情况。</p><div class="ll lm ln lo gt ab cb"><figure class="lu jr lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/e453c1ecd9cc95f4946415f0123f6e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*sErUerR15CbaH2ExplAATw.png"/></div></figure><figure class="lu jr lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/e707d5959e3e3f72fa22d6bbfd089428.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*w9lxYsXPt41A2o0PFm51lg.png"/></div></figure><figure class="lu jr lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/42b9e16743c04112c90701482eed4194.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*6pGH2a3M9IHjd6ccjmf7sg.png"/></div></figure></div><div class="ab cb"><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/a427c648d4d76d63b93fedeb5153e4b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*UgI2pimNqmyjVQxdQPDmtg.png"/></div></figure><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/ca7fa5931e3343aecce95dffba71b362.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*oD_MxTI8AV-CuQZvJEFL_g.png"/></div></figure></div><p id="0a93" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从一瞥中，我们可以注意到一个模式，当与紫色点相比时，黄色点更倾向于聚集(t-SNE 或 LDA 不是这种情况)。然而，我们得到的是一个非常有趣的结果。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="2883" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">特征分布的理想性质</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mi"><img src="../Images/d2674c7c6626642b8972565aa7c1debd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x52zlWT6EAUy8OQkc2jcig.png"/></div></div></figure><p id="aa2b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在继续之前，我想介绍一下特征分布的一些期望的性质。(我个人认为这个话题很重要)</p><p id="78c1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">每个示例的稀疏特征(总体稀疏)</strong> →矩阵中的每个元素(或每个示例)应该仅由少数(非零)特征来表示。</p><p id="ad8c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">跨示例的稀疏特征(生命周期稀疏)</strong> →每个特征应该允许我们区分不同的示例。(换句话说，特征应该足够有效，让我们能够区分不同的例子。)</p><p id="0f45" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">活动分布均匀(高度分散)</strong> →对于每个示例，活动特征的数量应与其他示例相似。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="1563" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">稀疏滤波</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mj"><img src="../Images/2d22a13421e4a6935a8274991ccf8607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mho1mbrNAz-A3FhzuEWM1w.png"/></div></div></figure><p id="3a2e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">蓝线</strong> →稀疏滤波算法</p><p id="1328" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">稀疏过滤的主要思想非常简单，通过无监督的方式学习数据的新特征(更紧凑的表示)。我们实现这一点的方法是简单地对矩阵的所有例子取<a class="ae jy" href="http://mathworld.wolfram.com/L2-Norm.html" rel="noopener ugc nofollow" target="_blank"> L2 范数</a>并执行归一化。然后我们再次规范化这些例子，但是这次是每个例子。最后，我们通过<a class="ae jy" href="http://www.statisticshowto.com/regularization/" rel="noopener ugc nofollow" target="_blank"> L1 罚函数</a>最小化特征的总和。关于这个方法为什么有效的更多理论理由<a class="ae jy" href="https://arxiv.org/abs/1603.08831" rel="noopener ugc nofollow" target="_blank">请点击这里。</a></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mk"><img src="../Images/1df501c8e831cc04d4739d8d93a56c66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6jGURvhFVhwkI3gwcRVv_Q.png"/></div></div></figure><p id="3fcc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，对于这个实验，我们也将遵循论文的原始作者所做的，将软绝对函数作为我们的激活函数。对于 Numpy 实现，我们将使用<a class="ae jy" href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" rel="noopener ugc nofollow" target="_blank">有限内存 BFGS 优化</a>算法，对于 tensorflow，我们将使用 Adam 优化器。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="2030" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:稀疏滤波(Numpy) </strong></p><div class="ll lm ln lo gt ab cb"><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/1e8ce1a11c4d8f2217954d059ed22093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*KfMIaRSLA7joZEANRm-ZJg.png"/></div></figure><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/19b768796f5b77e4803597010659f069.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*n2g1LodJDva9v7186kuKVw.png"/></div></figure></div><div class="ab cb"><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/e247ab291d0f43b24200c7e5b2f2ae78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*VveuU1_-78je_18c7b_UTA.png"/></div></figure><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/2d9e13ff79194868107e6df5b88442c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*Zf0V66CEG4eRa8fhTPghew.png"/></div></figure></div><p id="239b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左上/右下</strong> →清除分色数据 2D // 1D <br/> <strong class="kb ir">左下/右下</strong> →不清除分色数据 2D // 1D</p><p id="2cb5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从<a class="ae jy" href="https://github.com/subramgo" rel="noopener ugc nofollow" target="_blank"> subramgo </a>中取代码，我们可以观察稀疏滤波是如何降维的。总的来说，它在聚类相似的类方面做得很好，但是，对于具有明显分离的数据的 1D 归约，我们可以观察到它不能清楚地将每个类聚类到不同的 x 轴点。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="3b0a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:稀疏滤波(张量流)</strong></p><div class="ll lm ln lo gt ab cb"><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/4b2d0140e08fb10979182680f3a4b066.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*u-SVKKyi-5KgsfMUl5Kf-g.png"/></div></figure><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/90ff060b2c42d85b13e08e94bfbcb3dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*Ajm1yJkAIqtPS3j-mevv8w.png"/></div></figure></div><div class="ab cb"><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/836c0e11e9cf7d4a5c2115aa9b77ce60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*5lPyQAkVZzzglFvFZWXXyw.png"/></div></figure><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><img src="../Images/51929e46759538dc7292030c96a8d249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*V7uLeNkn571KdDl3pk70TA.png"/></div></figure></div><p id="7741" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左上/右下</strong> →清除分色数据 2D // 1D<br/>T22】左下/右下 →不清除分色数据 2D//1D</p><p id="98b0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">既然我喜欢 tensorflow，我就直接复制了 Numpy 实现的代码。然而，与 Numpy 实现的一个区别是，我们将使用 Adam Optimizer。(而不是<a class="ae jy" href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" rel="noopener ugc nofollow" target="_blank">有限内存 BFGS </a>)。).</p><p id="072d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">令人惊讶的是，(我个人认为)，它比 Numpy 版本做得更好。特别是当我们看一下清晰分离数据的 1D 缩减时。此外，对于没有明确分离的数据，我们可以观察到大多数绿色圆圈聚集在一起，而其他紫色和黄色圆圈更加分散。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="f22e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">互动码</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ml"><img src="../Images/b608acec3d0f3154e43246ff16abeea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IY_AwFqOtmUdd4D8qoi-Bw.png"/></div></div></figure><p id="539e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于 Google Colab，你需要一个 Google 帐户来查看代码，而且你不能在 Google Colab 中运行只读脚本，所以在你的操场上复制一份。最后，我永远不会请求允许访问你在 Google Drive 上的文件，仅供参考。编码快乐！同样为了透明，我在 github 上上传了所有的训练日志。</p><p id="63ff" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要访问 PCA、LDA、ICA 等的代码… <a class="ae jy" href="https://colab.research.google.com/drive/1rP7VW9XigXnSgj0ECOvOVswKOEeDwPRt" rel="noopener ugc nofollow" target="_blank">请点击此处。</a> <br/>访问 Numpy 的代码<a class="ae jy" href="https://colab.research.google.com/drive/1YSI_OIWy3Lsst2SGl03AD4IR892wzSEl" rel="noopener ugc nofollow" target="_blank">稀疏点击这里</a>，<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/Understanding_Concepts/Sparse/sparse_np.txt" rel="noopener ugc nofollow" target="_blank">训练日志点击这里。</a> <br/>访问<a class="ae jy" href="https://colab.research.google.com/drive/1Jz0jhV5ozYzokgIHVywY3j98By-Uysq0" rel="noopener ugc nofollow" target="_blank"> tensorflow 稀疏代码点击此处，</a> <a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/Understanding_Concepts/Sparse/sparse.txt" rel="noopener ugc nofollow" target="_blank">训练日志点击此处。</a></p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="d1ec" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="7b5a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我为这篇文章缺乏结构而道歉。</p><p id="9e5d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有写作的列表，请在这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的推特上关注我<a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文 pos </a> t。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="3692" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="0951" class="mm mn iq kb b kc kd kg kh kk mo ko mp ks mq kw mr ms mt mu bi translated">Ngiam，j .、Chen，z .、Bhaskar，s .、Koh，p .、和 Ng，A. (2011 年)。稀疏过滤。Papers.nips.cc 于 2018 年 6 月 12 日检索，来自<a class="ae jy" href="https://papers.nips.cc/paper/4334-sparse-filtering" rel="noopener ugc nofollow" target="_blank">https://papers.nips.cc/paper/4334-sparse-filtering</a></li><li id="2e89" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">, (2018).[在线]可在:<a class="ae jy" href="https://www.linkedin.com/in/neil-bruce-19292417/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/neil-bruce-19292417/</a>【2018 年 6 月 12 日访问】。</li><li id="8899" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">[在线]可在:<a class="ae jy" href="https://scholar.google.com/citations?user=Gnezf-4AAAAJ&amp;hl=en" rel="noopener ugc nofollow" target="_blank">https://scholar.google.com/citations?user=Gnezf-4AAAAJ&amp;HL = en</a>【2018 年 6 月 12 日访问】。</li><li id="8b73" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">控制图形美学-seaborn 0 . 8 . 1 文档。(2018).Seaborn.pydata.org。检索于 2018 年 6 月 12 日，来自 https://seaborn.pydata.org/tutorial/aesthetics.html<a class="ae jy" href="https://seaborn.pydata.org/tutorial/aesthetics.html" rel="noopener ugc nofollow" target="_blank"/></li><li id="3f42" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">分区，L. (2018)。LDA 作为划分之前或之后的维度缩减。交叉验证。检索于 2018 年 6 月 12 日，来自<a class="ae jy" href="https://stats.stackexchange.com/questions/305691/lda-as-the-dimension-reduction-before-or-after-partitioning" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/305691/LDA-as-the-dimension-reduction-before-or-after-partitioning</a></li><li id="8bcd" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">sk learn . decomposition . PCA—sci kit—学习 0.19.1 文档。(2018).Scikit-learn.org。检索于 2018 年 6 月 12 日，来自<a class="ae jy" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/modules/generated/sk learn . decomposition . PCA . html</a></li><li id="38ae" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">sk learn . decomposition . fa stica—sci kit—学习 0.19.1 文档。(2018).Scikit-learn.org。检索于 2018 年 6 月 12 日，来自<a class="ae jy" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/modules/generated/sk learn . decomposition . fa stica . html</a></li><li id="d329" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">Python？，H. (2018)。如何用 Python 制作 3D 散点图？。堆栈溢出。检索于 2018 年 6 月 12 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/1985856/how-to-make-a-3d-scatter-plot-in-python" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/1985856/how-to-make-a-3d-scatter-plot-in-python</a></li><li id="649b" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">k 均值聚类和低秩近似的降维。(2018).Bigdata.csail.mit.edu。检索于 2018 年 6 月 12 日，来自<a class="ae jy" href="http://bigdata.csail.mit.edu/node/277" rel="noopener ugc nofollow" target="_blank">http://bigdata.csail.mit.edu/node/277</a></li><li id="9e83" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">sklearn.discriminant _ analysis。线性判别分析-sci kit-学习 0.19.1 文档。(2018).Scikit-learn.org。检索于 2018 年 6 月 12 日，来自<a class="ae jy" href="http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit_transform" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/modules/generated/sk learn . discriminant _ analysis。lineardiscriminantanalysis . html # sk learn . discriminant _ analysis。linear discriminant analysis . fit _ transform</a></li><li id="e9b5" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">sk learn . manifold . tsne—sci kit-learn 0 . 19 . 1 文档。(2018).Scikit-learn.org。检索于 2018 年 6 月 12 日，来自<a class="ae jy" href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/modules/generated/sk learn . manifold . tsne . html</a></li><li id="3250" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">sk learn . decomposition . factor analysis-sci kit-learn 0 . 19 . 1 文档。(2018).Scikit-learn.org。检索于 2018 年 6 月 12 日，来自<a class="ae jy" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html#examples-using-sklearn-decomposition-factoranalysis" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/modules/generated/sk learn . decomposition . factor analysis . html # examples-using-sk learn-decomposition-factor analysis</a></li><li id="4750" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">StatQuest:主成分分析(PCA)解释清楚(2015)。(2018).YouTube。检索于 2018 年 6 月 12 日，来自 https://www.youtube.com/watch?v=_UVHneBUBW0<a class="ae jy" href="https://www.youtube.com/watch?v=_UVHneBUBW0" rel="noopener ugc nofollow" target="_blank"/></li><li id="f979" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">StatQuest: t-SNE，解释清楚。(2018).YouTube。检索于 2018 年 6 月 12 日，来自<a class="ae jy" href="https://www.youtube.com/watch?v=NEaUSP4YerM" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=NEaUSP4YerM</a></li><li id="e8d8" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">StatQuest:线性判别分析(LDA)解释清楚..(2018).YouTube。检索于 2018 年 6 月 12 日，发自<a class="ae jy" href="https://www.youtube.com/watch?v=azXCzI57Yfc&amp;lc=z221ubiy3qzoizzubacdp431wog0fwhls5iiwovere5w03c010c.1528837997136780" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=azXCzI57Yfc&amp;LC = z 221 ubiy 3 qzoizzubacd p 431 wog 0 fwhls 5 iiwovere 5 w 03 c 010 c</a></li><li id="a524" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">sk learn . datasets . make _ classification-sci kit-learn 0 . 19 . 1 文档。(2018).Scikit-learn.org。检索于 2018 年 6 月 12 日，来自<a class="ae jy" href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/modules/generated/sk learn . datasets . make _ classification . html</a></li><li id="35a3" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">尺寸，S. (2018)。sk learn LDA-分析不会生成 2 维。堆栈溢出。检索于 2018 年 6 月 12 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/49955592/sklearn-lda-analysis-wont-generate-2-dimensions" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/49955592/sk learn-LDA-analysis-wont-generate-2-dimensions</a></li><li id="deae" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">主成分分析与独立成分分析继续-佐治亚理工学院-机器学习。(2018).YouTube。检索于 2018 年 6 月 12 日，来自<a class="ae jy" href="https://www.youtube.com/watch?v=e4woe8GRjEI" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=e4woe8GRjEI</a></li><li id="0c4a" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">你的参考书目:Anon，(2018)。[在线]可从以下网址获取:<a class="ae jy" href="https://www.quora.com/What-is-the-difference-between-LDA-and-PCA-for-dimension-reduction" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/What-is-the-difference-than-the-LDA-and-PCA-for-dimension-reduction</a>【2018 年 6 月 12 日获取】。</li><li id="9d0e" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">subramgo/SparseFiltering。(2018).GitHub。检索于 2018 年 6 月 13 日，来自 https://github.com/subramgo/SparseFiltering<a class="ae jy" href="https://github.com/subramgo/SparseFiltering" rel="noopener ugc nofollow" target="_blank"/></li><li id="c259" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">过滤，U. (2014)。无监督特征学习-稀疏过滤。Vaillab.blogspot.com。检索于 2018 年 6 月 13 日，来自<a class="ae jy" href="http://vaillab.blogspot.com/2014/08/unsupervised-feature-learning-sparse.html" rel="noopener ugc nofollow" target="_blank">http://vail lab . blogspot . com/2014/08/unsupervised-feature-learning-sparse . html</a></li><li id="6f82" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">Metzen，J. (2018 年)。基于稀疏滤波的无监督特征学习。Jmetzen.github.io .检索于 2018 年 6 月 13 日，来自<a class="ae jy" href="http://jmetzen.github.io/2014-09-14/sparse_filtering.html" rel="noopener ugc nofollow" target="_blank">http://jmetzen.github.io/2014-09-14/sparse_filtering.html</a></li><li id="c9f5" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">资料 D. (2018)。禁用 Tensorflow 调试信息。堆栈溢出。检索于 2018 年 6 月 13 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/35911252/disable-tensor flow-debugging-information</a></li><li id="7f61" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">tensorflow？，W. (2018)。tensorflow 中 numpy.newaxis 的替代品是什么？。堆栈溢出。检索于 2018 年 6 月 13 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/42344090/what-is-the-alternative-of-numpy-newaxis-in-tensorflow" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/42344090/what-the-alternative-of-numpy-newaxis-in-tensor flow</a></li><li id="398a" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">类，P. (2018)。Python 调用类中的函数。堆栈溢出。检索于 2018 年 6 月 13 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/5615648/python-call-function-within-class" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/5615648/python-call-function-within-class</a></li><li id="aed9" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">(TensorFlow)？，D. (2018)。GradientDescentOptimizer 和 AdamOptimizer (TensorFlow)的区别？。交叉验证。检索于 2018 年 6 月 13 日，来自<a class="ae jy" href="https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensor flow</a></li><li id="3aa1" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">导入多个张量流模型(图表)Breta Hajek。(2017).布雷塔·哈杰克。检索于 2018 年 6 月 13 日，来自<a class="ae jy" href="https://bretahajek.com/2017/04/importing-multiple-tensorflow-models-graphs/" rel="noopener ugc nofollow" target="_blank">https://bretahajek . com/2017/04/importing-multiple-tensor flow-models-graphs/</a></li><li id="2f40" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">TensorFlow，W. (2018)。在 TensorFlow 中使用多个图形。堆栈溢出。检索于 2018 年 6 月 13 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/35955144/working-with-multiple-graphs-in-tensorflow" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/35955144/working-with-multiple-graphs-in-tensor flow</a></li><li id="04a9" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">TensorFlow，W. (2018)。在 TensorFlow 中使用多个图形。堆栈溢出。检索于 2018 年 6 月 13 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/35955144/working-with-multiple-graphs-in-tensorflow" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/35955144/working-with-multiple-graphs-in-tensor flow</a></li><li id="288c" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">有限记忆 BFGS。(2018).En.wikipedia.org。检索于 2018 年 6 月 13 日，来自<a class="ae jy" href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Limited-memory_BFGS</a></li><li id="4520" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">正规化:简单的定义，L1 和 L2 处罚。(2016).统计学如何？检索于 2018 年 6 月 13 日，来自<a class="ae jy" href="http://www.statisticshowto.com/regularization/" rel="noopener ugc nofollow" target="_blank">http://www.statisticshowto.com/regularization/</a></li><li id="dcfd" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">L -Norm —来自 Wolfram MathWorld。(2018).Mathworld.wolfram.com。检索于 2018 年 6 月 13 日，来自<a class="ae jy" href="http://mathworld.wolfram.com/L2-Norm.html" rel="noopener ugc nofollow" target="_blank">http://mathworld.wolfram.com/L2-Norm.html</a></li><li id="fa62" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">Zennaro，f .，&amp; Chen，K. (2016)。理解稀疏滤波:理论观点。Arxiv.org。检索于 2018 年 6 月 13 日，来自<a class="ae jy" href="https://arxiv.org/abs/1603.08831" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1603.08831</a></li></ol></div></div>    
</body>
</html>