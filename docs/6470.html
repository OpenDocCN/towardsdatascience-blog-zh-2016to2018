<html>
<head>
<title>Feature Selection Using Random forest</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用随机森林的特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f?source=collection_archive---------1-----------------------#2018-12-15">https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f?source=collection_archive---------1-----------------------#2018-12-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6dd6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">群体的智慧</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/fb9a46b613c59f1623bcd9c8776363e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*M7rQdINzo9ctoCNL"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@noahsilliman?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Noah Silliman</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="2726" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随机森林是最流行的机器学习算法之一。它们之所以如此成功，是因为它们总体上提供了良好的预测性能、较低的过拟合和易于解释的能力。这种可解释性是由这样一个事实给出的，即很容易推导出每个变量在决策树中的重要性。换句话说，很容易计算出每个变量对决策的影响程度。</p><p id="30c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用随机森林的特征选择属于嵌入式方法的范畴。嵌入式方法结合了过滤器和包装器方法的优点。它们是由算法实现的，这些算法有自己内置的特征选择方法。嵌入式方法的一些好处是:</p><ul class=""><li id="9731" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">它们非常精确。</li><li id="d8d9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">他们概括得更好。</li><li id="2ce5" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">它们是可以解释的</li></ul><p id="570b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">随机森林如何选择特征？</strong></p><p id="be57" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随机森林由 4 到 1200 个决策树组成，每个决策树都是基于从数据集中随机提取的观察值和随机提取的特征构建的。不是每棵树都能看到所有的特征或所有的观察结果，这保证了树是不相关的，因此不容易过度拟合。每个树也是一系列基于单一或组合特征的是非问题。在每个节点处(这是在每个问题处)，这三个节点将数据集分成两个桶，每个桶存放的观察值彼此更相似，但与另一个桶中的观察值不同。因此，每个特征的重要性来源于每个桶的“纯净”程度。</p><p id="01b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">分类和回归的工作方式不同吗？</strong></p><p id="4add" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于<em class="mg">分类</em>，杂质的度量或者是<em class="mg">基尼杂质</em>或者是<em class="mg">信息增益/熵。</em></p><p id="e6bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于<em class="mg">回归</em>，杂质<em class="mg">的度量是方差。</em></p><p id="7ead" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，在训练树时，可以计算每个特征减少杂质的程度。一个特征减少杂质越多，该特征就越重要。在随机森林中，每个特征的杂质减少可以跨树进行平均，以确定变量的最终重要性。</p><blockquote class="mh mi mj"><p id="118e" class="kw kx mg ky b kz la jr lb lc ld ju le mk lg lh li ml lk ll lm mm lo lp lq lr ij bi translated">为了给出更好的直觉，在树的顶部选择的特征通常比在树的末端节点选择的特征更重要，因为通常顶部分裂导致更大的信息增益。</p></blockquote><p id="df60" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="mg">让我们来看一些关于如何使用随机森林选择要素的 Python 代码。</em> </strong></p><p id="fcc5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我不会将随机森林应用于实际的数据集，但它可以很容易地应用于任何实际的数据集。</p><ol class=""><li id="81a6" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr mn ly lz ma bi translated"><em class="mg">导入库</em></li></ol><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="4034" class="mt mu iq mp b gy mv mw l mx my">import pandas as pd<br/>from sklearn.ensemble import RandomForestClassfier<br/>from sklearn.feature_selection import SelectFromModel</span></pre><p id="2802" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.<em class="mg">在所有特征选择过程中，通过仅检查训练集来选择特征是一个好的实践。这是为了避免过度拟合。</em></p><p id="0bfb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑到我们有一个训练和一个测试数据集。我们从训练集中选择特征，然后将变化转移到测试集中。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="599c" class="mt mu iq mp b gy mv mw l mx my">X_train,y_train,X_test,y_test = train_test_split(data,test_size=0.3)</span></pre><p id="f7cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.<em class="mg">在这里，我将在一行代码中完成模型拟合和特征选择。</em></p><ul class=""><li id="7efe" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">首先，我指定了随机森林实例，指出了树的数量。</li><li id="fedd" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">然后我用 sklearn 的<code class="fe mz na nb mp b">selectFromModel</code>对象自动选择特征。</li></ul><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="a4fc" class="mt mu iq mp b gy mv mw l mx my">sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))<br/>sel.fit(X_train, y_train)</span></pre><p id="a0b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">默认情况下，<code class="fe mz na nb mp b">SelectFromModel</code>将选择重要性大于所有特征的平均重要性的那些特征，但我们可以根据需要改变该阈值。</p><p id="be81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.<em class="mg">为了查看哪些特征是重要的，我们可以在拟合的模型上使用</em> <code class="fe mz na nb mp b">get_support</code> <em class="mg">方法。</em></p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="1f03" class="mt mu iq mp b gy mv mw l mx my">sel.get_support()</span></pre><p id="581e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它将返回一个由<em class="mg">布尔</em>值组成的数组。<strong class="ky ir">重要性大于平均重要性的特征为真</strong>，其余为假<strong class="ky ir"/>。</p><p id="f0e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.<em class="mg">我们现在可以列出并统计所选的特性。</em></p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="902a" class="mt mu iq mp b gy mv mw l mx my">selected_feat= X_train.columns[(sel.get_support())]<br/>len(selected_feat)</span></pre><p id="5ffa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它将返回一个整数，表示随机森林选择的要素数量。</p><p id="7b46" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">6.<em class="mg">获取所选特征的名称</em></p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="264d" class="mt mu iq mp b gy mv mw l mx my">print(selected_feat)</span></pre><p id="862b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它将返回所选特征的名称。</p><p id="3404" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">7.我们还可以检查并绘制重要性的分布。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="a395" class="mt mu iq mp b gy mv mw l mx my">pd.series(sel.estimator_,feature_importances_,.ravel()).hist()</span></pre><p id="8057" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它将返回一个直方图，显示使用该特征选择技术选择的特征的分布。</p><blockquote class="mh mi mj"><p id="59e4" class="kw kx mg ky b kz la jr lb lc ld ju le mk lg lh li ml lk ll lm mm lo lp lq lr ij bi translated">我们当然可以调整决策树的参数。我们选择特性的截止点有点武断。一种方法是选择前 10、20 个特征。或者，前 10%的人。为此，我们可以将 mutual info 与 sklearn 的 SelectKBest 或 SelectPercentile 结合使用。</p></blockquote><p id="0a6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">随机森林的局限性有:</strong></p><ul class=""><li id="4c32" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">相关特征将被赋予相同或相似的重要性，但是与没有相关对应物而构建的相同树相比，整体重要性降低。</li><li id="b0c9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">一般来说，随机森林和决策树优先选择基数高的特征(树偏向于这些类型的变量)。</li></ul><p id="8398" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过使用树导出的特征重要性来选择特征是为机器学习选择良好特征的一种非常直接、快速且通常准确的方式。特别是，如果我们要构建树方法。</p><p id="a157" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，正如我所说的，如果树是在没有相关对应物的情况下构建的，那么与它们的重要性相比，相关的特征将在树中显示出相似和较低的重要性。</p><p id="e407" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，最好递归地选择特性，而不是像我们在这里所做的那样一起选择。</p></div></div>    
</body>
</html>