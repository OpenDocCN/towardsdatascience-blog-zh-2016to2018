<html>
<head>
<title>Comparative Study on Classic Machine learning Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">经典机器学习算法的比较研究</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comparative-study-on-classic-machine-learning-algorithms-24f9ff6ab222?source=collection_archive---------0-----------------------#2018-12-06">https://towardsdatascience.com/comparative-study-on-classic-machine-learning-algorithms-24f9ff6ab222?source=collection_archive---------0-----------------------#2018-12-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3579" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">各种 ML 算法的快速总结</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/53c6d84d28fcff17293a38ef907b80c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*leIOPoQEemUQgZBATkDDRw.jpeg"/></div></div></figure><p id="b12a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">机器学习是一种科学技术，在这种技术中，计算机学习如何解决一个问题，而不需要对它们进行显式编程。在更好的算法、计算能力和大数据的推动下，深度学习目前正在引领 ML 竞赛。尽管如此，最大似然经典算法在该领域仍有其强大的地位。</p><p id="dbcf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我将对不同的机器学习监督技术进行比较研究，如本故事中的<em class="ln"> </em> <strong class="kt ir"> <em class="ln">线性回归、逻辑回归、K 近邻和决策树</em></strong><strong class="kt ir"><em class="ln">。</em> </strong>在<a class="ae lo" href="https://medium.com/@dannymvarghese/comparative-study-on-classic-machine-learning-algorithms-part-2-5ab58b683ec0" rel="noopener">的下一个故事</a>中，我将覆盖<strong class="kt ir"> <em class="ln">支持向量机、随机森林和天真八爷</em> s </strong>。关于算法的深入细节有那么多比较好的博客，我们就只着重比较研究一下。我们将研究它们的<strong class="kt ir"> <em class="ln">基本逻辑、优点、缺点、假设、共线效应&amp;异常值、超参数</em> </strong> <em class="ln">、</em> <strong class="kt ir"> <em class="ln">相互比较</em> </strong> <em class="ln">等</em>。</p><p id="0ee2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">其余算法请参考本系列的第 2 部分。 </p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="68b9" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated"><strong class="ak"> 1。线性回归</strong></h1><p id="ab97" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">如果你想开始机器学习，线性回归是最好的起点。线性回归是一种回归模型，也就是说，它将获取特征并预测连续输出，例如:股票价格、工资等。线性回归顾名思义，找到一个线性曲线解决每个问题。</p><h2 id="4c68" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">基础理论</strong>:</h2><p id="f12e" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">LR 为每个训练特征分配权重参数θ。预测输出(h(θ))将是特征和θ系数的线性函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/e45067d832c7400c7c0db22e4470168c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*A87wqka924CHHwL97LsdMQ.png"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">linear regression output. Eqn (1)</figcaption></figure><p id="e6a5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在训练开始时，每个θ被随机初始化。但是在训练期间，我们校正对应于每个特征的θ，使得损失(预期和预测输出之间的偏差的度量)最小化。梯度下降算法将用于在正确的方向上对齐θ值。在下图中，每个红点代表训练数据，蓝线显示导出的解决方案。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/53ea5302af72313cfa205f775e9b93b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*L9OqETMWSa0zYzrR2t1ZAw.gif"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk"><em class="nl">gif credits : </em><a class="ae lo" href="https://medium.com/@kabab/linear-regression-with-python-d4e10887ca43" rel="noopener">https://medium.com/@kabab/linear-regression-with-python-d4e10887ca43</a></figcaption></figure><h2 id="b8aa" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">损失函数:</strong></h2><p id="2e91" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">在 LR 中，我们使用均方误差作为损失的度量。预期和实际输出的偏差将被平方并求和。梯度下降算法将使用该损失的导数。</p><h2 id="6729" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">优点:</strong></h2><ul class=""><li id="870f" class="nm nn iq kt b ku mo kx mp la no le np li nq lm nr ns nt nu bi translated">容易和简单的实现。</li><li id="d4e4" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">空间复解。</li><li id="23c0" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">快速训练。</li><li id="7567" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">θ系数的值给出了特征重要性的假设。</li></ul><h2 id="7d71" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">缺点:</strong></h2><ul class=""><li id="7339" class="nm nn iq kt b ku mo kx mp la no le np li nq lm nr ns nt nu bi translated">仅适用于线性解决方案。在很多现实生活场景中，可能并非如此。</li><li id="fbd8" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">算法假设输入残差(误差)为正态分布，但可能并不总是满足。</li><li id="e3da" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">算法假设输入特征相互独立(无共线)。</li></ul><h2 id="75b8" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">超参数:</strong></h2><ul class=""><li id="17ad" class="nm nn iq kt b ku mo kx mp la no le np li nq lm nr ns nt nu bi translated">正则化参数(λ):正则化用于避免对数据的过拟合。λ越大，正则化程度越高，解的偏差越大。λ越低，解的方差越大。中间值是优选的。</li><li id="ba57" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">学习率(α):它估计在训练期间应用梯度下降算法时，θ值应该被校正多少。α也应该是一个适中的值。</li></ul><h2 id="c49a" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">LR 的假设:</strong></h2><ul class=""><li id="7bd2" class="nm nn iq kt b ku mo kx mp la no le np li nq lm nr ns nt nu bi translated">自变量和因变量之间的线性关系。</li><li id="04dc" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">训练数据是同方差的，这意味着误差的方差应该是恒定的。</li><li id="07d3" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">自变量不应该是共线的。</li></ul><h2 id="0c7b" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">共线性&amp;异常值:</strong></h2><p id="f79c" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">当一个特征可以从另一个特征以一定的精度线性预测时，称两个特征共线。</p><ul class=""><li id="b8ca" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">共线性只会扩大标准误差，并导致一些重要特征在训练过程中变得不重要。理想情况下，我们应该在训练之前计算共线性，并且只保留高度相关的特征集中的一个特征。</li></ul><p id="09fb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">离群值是训练中面临的另一个挑战。它们是对正常观察极端的数据点，并影响模型的准确性。</p><ul class=""><li id="7ed9" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">异常值会增大误差函数，并影响曲线函数和线性回归的精度。正则化(尤其是 L1)可以通过不允许θ参数剧烈变化来校正异常值。</li><li id="81a1" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">在探索性数据分析阶段本身，我们应该注意异常值并纠正/消除它们。箱线图可用于识别它们。</li></ul><h2 id="dd6b" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">与其他车型对比:</strong></h2><p id="7f97" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">由于线性回归是一种回归算法，我们将它与其他回归算法进行比较。线性回归的一个基本区别是，LR 只能支持线性解。机器学习中不存在胜过所有其他模型的最佳模型(没有免费的午餐)，效率是基于训练数据分布的类型。</p><p id="868d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> LR vs 决策树</strong>:</p><ul class=""><li id="d896" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">决策树支持非线性，而 LR 只支持线性解决方案。</li><li id="889a" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">当具有较少数据集(低噪声)的大量特征时，线性回归可能优于决策树/随机森林。在一般情况下，决策树会有更好的平均准确率。</li><li id="80eb" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">对于分类独立变量，决策树优于线性回归。</li><li id="970c" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">决策树比 LR 更好地处理共线性。</li></ul><p id="521c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> LR vs SVM : </strong></p><ul class=""><li id="7c85" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">SVM 支持使用内核技巧的线性和非线性解决方案。</li><li id="fc67" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">SVM 比 LR 更好地处理异常值。</li><li id="2cb4" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">两者在训练数据较少且特征数量较多的情况下表现良好。</li></ul><p id="c1c6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> LR vs KNN : </strong></p><ul class=""><li id="92da" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">KNN 是一个非参数模型，而 LR 是一个参数模型。</li><li id="1f09" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">KNN 的实时速度很慢，因为它必须跟踪所有训练数据并找到邻居节点，而 LR 可以很容易地从调谐的θ系数中提取输出。</li></ul><p id="fa65" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> LR vs 神经网络:</strong></p><ul class=""><li id="c65a" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">与 LR 模型相比，神经网络需要大量的训练数据，而 LR 即使使用较少的训练数据也可以工作得很好。</li><li id="dedb" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">NN 相比 LR 会比较慢。</li><li id="a4ad" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">神经网络的平均精度总是更好。</li></ul></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="f1ec" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated"><strong class="ak"> 2。逻辑回归</strong></h1><p id="4076" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">就像线性回归一样，逻辑回归是从分类算法开始的正确算法。尽管“回归”这个名字出现了，但它不是一个回归模型，而是一个分类模型。它使用逻辑函数来构建二进制输出模型。逻辑回归的输出将是一个概率(0≤x≤1)，并可用于预测二进制 0 或 1 作为输出(如果 x &lt;0.5, output= 0, else output=1).</p><h2 id="4917" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">基本理论:</strong></h2><p id="73c9" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">逻辑回归的作用有点类似于线性回归。它还计算线性输出，然后对回归输出执行存储函数。Sigmoid 函数是常用的逻辑函数。从下面可以清楚地看到，z 值与等式(1)中的线性回归输出值相同。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/1dfb0997a2019a0a6d81614c0c4161d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*z2T1AMny9mqNKYnT_1-v1g.png"/></div></figure><p id="a60a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这里的 h(θ)值对应于 P(y=1|x)，即给定输入 x，输出为二进制 1 的概率。P(y=0|x)将等于 1-h(θ)。</p><p id="5833" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当 z 的值为 0 时，g(z)将为 0.5。每当 z 为正时，h(θ)将大于 0.5，输出将为二进制 1。同样，每当 z 为负时，y 的值将为 0。当我们使用线性方程来寻找分类器时，输出模型也将是线性的，这意味着它将输入维度分成两个空间，其中一个空间中的所有点对应于相同的标签。</p><p id="4438" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下图显示了 sigmoid 函数的分布。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/d7025d160f64e8637f516c7cb6687423.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*BwZmaMBaVW8f8lBAQUoytw.png"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">sigmoid function Eqn(3)</figcaption></figure><h2 id="d473" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">损失函数:</strong></h2><p id="bf3a" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">我们不能用均方差作为损失函数(像线性回归)，因为我们在最后用了一个非线性的 sigmoid 函数。MSE 函数会引入局部极小值，影响梯度下降算法。</p><p id="7d0b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">所以我们用交叉熵作为损失函数。将使用两个等式，对应于 y=1 和 y=0。这里的基本逻辑是，每当我的预测严重错误时，(例如:y' =1 &amp; y = 0)，成本将是-log(0)，这是无穷大。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/dc31d8d6be97f97ccfe799a87a69cd63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*F0EQjGDHQ9oTcecabu_L8A.png"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">cross-entropy loss Eqn(4)</figcaption></figure><p id="ff12" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在给出的等式中，m 代表训练数据大小，y’代表预测输出，y 代表实际输出。</p><h2 id="66f5" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">优点:</strong></h2><ul class=""><li id="7e17" class="nm nn iq kt b ku mo kx mp la no le np li nq lm nr ns nt nu bi translated">简易、快速、简单的分类方法。</li><li id="fcce" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">θ参数解释了自变量相对于因变量的显著性的方向和强度。</li><li id="31bd" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">也可用于多类分类。</li><li id="05ed" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">损失函数总是凸的。</li></ul><h2 id="57b0" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">缺点:</strong></h2><ul class=""><li id="1ea5" class="nm nn iq kt b ku mo kx mp la no le np li nq lm nr ns nt nu bi translated">不能应用于非线性分类问题。</li><li id="2231" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">需要正确选择功能。</li><li id="e886" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">期望有好的信噪比。</li><li id="c6ac" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">共线性和异常值会影响 LR 模型的准确性。</li></ul><h2 id="3504" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">超参数:</strong></h2><p id="438a" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">逻辑回归超参数类似于线性回归超参数。学习率(α)和正则化参数(λ)必须适当调整以实现高精度。</p><h2 id="8531" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">LR 的假设:</strong></h2><p id="81ff" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">逻辑回归假设类似于线性回归模型的假设。请参考上述部分。</p><h2 id="fbad" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">与其他车型对比:</strong></h2><p id="2b05" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated"><strong class="kt ir">逻辑回归 vs SVM : </strong></p><ul class=""><li id="c09d" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">SVM 可以处理非线性解，而逻辑回归只能处理线性解。</li><li id="dc45" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">线性 SVM 能更好地处理异常值，因为它能导出最大利润解。</li><li id="b6dc" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">SVM 的铰链损耗优于 LR 的原木损耗。</li></ul><p id="064e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">逻辑回归 vs 决策树:</strong></p><ul class=""><li id="ea6e" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">决策树比 LR 更好地处理共线性。</li><li id="0a45" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">决策树不能得出特征的重要性，但是 LR 可以。</li><li id="60be" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">决策树比 LR 更适合分类值。</li></ul><p id="1470" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">逻辑回归 vs 神经网络:</strong></p><ul class=""><li id="d507" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">NN 可以支持 LR 不能支持的非线性解。</li><li id="d9e6" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">LR 具有凸损失函数，所以它不会陷入局部极小值，而 NN 可能会陷入局部极小值。</li><li id="406b" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">当训练数据较少且特征较多时，LR 优于 NN，而 NN 需要大量训练数据。</li></ul><p id="7acf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">逻辑回归 vs 朴素贝叶斯:</strong></p><ul class=""><li id="00b0" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">朴素贝叶斯是一种生成模型，而 LR 是一种判别模型。</li><li id="df78" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">朴素贝叶斯适用于小数据集，而 LR+正则化可以实现类似的性能。</li><li id="4c61" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">LR 在共线性方面比朴素贝叶斯表现得更好，因为朴素贝叶斯期望所有特征都是独立的。</li></ul><p id="f3b1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">逻辑回归 vs KNN : </strong></p><ul class=""><li id="9b6d" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">KNN 是非参数模型，而 LR 是参数模型。</li><li id="e4d9" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">KNN 比逻辑回归相对慢。</li><li id="2921" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">KNN 支持非线性解决方案，而 LR 仅支持线性解决方案。</li><li id="6048" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">LR 可以导出置信度(关于它的预测)，而 KNN 只能输出标签。</li></ul></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="6776" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated"><strong class="ak"> 3。k-最近邻</strong></h1><p id="3431" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">k-最近邻法是一种用于分类和回归的非参数方法。这是最简单的 ML 技术之一。它是一个懒惰的学习模型，具有局部近似性。</p><h2 id="351f" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated">基本理论:</h2><p id="dfb3" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">KNN 背后的基本逻辑是探索你的邻居，假设测试数据点与他们相似，并得到输出。在 KNN，我们寻找 k 个邻居并得出预测。</p><p id="c78c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在 KNN 分类的情况下，在 k 个最近的数据点上应用多数投票，而在 KNN 回归中，k 个最近的数据点的平均值被计算为输出。根据经验，我们选择奇数作为 k。KNN 是一个懒惰的学习模型，计算只在运行时发生。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/47fb55eeeb110e94570c217fc1d9eee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*1MY16tfFYjM6RwzDwXrrdw.png"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">image credits : <a class="ae lo" href="https://www.fromthegenesis.com/pros-and-cons-of-k-nearest-neighbors/" rel="noopener ugc nofollow" target="_blank">https://www.fromthegenesis.com/pros-and-cons-of-k-nearest-neighbors/</a></figcaption></figure><p id="2d9e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在上图中，黄色和紫色点对应于训练数据中的 A 类和 B 类。红星指的是要分类的测试数据。当 k = 3 时，我们预测 B 类作为输出，当 K=6 时，我们预测 A 类作为输出。</p><h2 id="8fe5" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">损失函数:</strong></h2><p id="4979" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">在 KNN 没有培训。在测试期间，具有最小距离的 k 个邻居将参与分类/回归。</p><h2 id="6d8b" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated">优势:</h2><ul class=""><li id="1d5b" class="nm nn iq kt b ku mo kx mp la no le np li nq lm nr ns nt nu bi translated">简易的机器学习模型。</li><li id="64a4" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">需要调整的超参数很少。</li></ul><h2 id="4c02" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated">缺点:</h2><ul class=""><li id="3815" class="nm nn iq kt b ku mo kx mp la no le np li nq lm nr ns nt nu bi translated">应该明智地选择 k。</li><li id="94b7" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">如果样本量很大，则运行时的计算成本很大。</li><li id="2e4b" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">应该提供适当的缩放，以便在特性之间进行公平处理。</li></ul><h2 id="0a69" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated">超参数:</h2><p id="0cd4" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">KNN 主要涉及两个超参数，K 值和距离函数。</p><ul class=""><li id="bf35" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">k 值:有多少邻居参与 KNN 算法。k 应该根据验证误差进行调整。</li><li id="ed46" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">距离函数:欧几里德距离是最常用的相似性函数。曼哈顿距离、海明距离、闵可夫斯基距离是不同的选择。</li></ul><h2 id="aa4d" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated">假设:</h2><ul class=""><li id="4e15" class="nm nn iq kt b ku mo kx mp la no le np li nq lm nr ns nt nu bi translated">应该对输入域有清晰的理解。</li><li id="6d08" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">可行的中等样本量(由于空间和时间的限制)。</li><li id="37e9" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">共线性和异常值应在训练前处理。</li></ul><h2 id="c5e8" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated">与其他型号的比较:</h2><p id="59a6" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">KNN 和其他模型之间的一个普遍区别是，与其他模型相比，KNN 需要大量的实时计算。</p><p id="4cd9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> KNN vs 朴素贝叶斯:</strong></p><ul class=""><li id="7934" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">由于 KNN 的实时执行，朴素贝叶斯比 KNN 快得多。</li><li id="94f2" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">朴素贝叶斯是参数化的，而 KNN 是非参数化的。</li></ul><p id="9048" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> KNN vs 线性回归:</strong></p><ul class=""><li id="d803" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">当数据信噪比较高时，KNN 优于线性回归。</li></ul><p id="9db1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> KNN 对 SVM : </strong></p><ul class=""><li id="d7c2" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">SVM 比 KNN 更关心离群值。</li><li id="4fc3" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">如果训练数据远大于特征数(m&gt;&gt;n)，KNN 比 SVM 好。当有大量特征和较少训练数据时，SVM 胜过 KNN。</li></ul><p id="891c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> KNN vs 神经网络:</strong></p><ul class=""><li id="1d31" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">与 KNN 相比，神经网络需要大量的训练数据来达到足够的精度。</li><li id="e33a" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">与 KNN 相比，神经网络需要大量的超参数调整。</li></ul></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="7602" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated"><strong class="ak"> 4。决策树</strong></h1><p id="61b1" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">决策树是一种基于树的算法，用于解决回归和分类问题。构造一个倒置的树，它从一个均匀概率分布的根节点分支到高度不均匀的叶节点，用于导出输出。回归树用于具有连续值的因变量，分类树用于具有离散值的因变量。</p><p id="0f5b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">基础理论:</strong></p><p id="81cc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">决策树是从独立变量中导出的，每个节点都有一个特征条件。节点根据条件决定下一步导航哪个节点。一旦到达叶节点，就可以预测输出。正确的条件顺序使树有效率。熵/信息增益被用作选择节点条件的标准。使用递归的、基于贪婪的算法来导出树结构。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/ac05a9753d5d6b7f8f4d01ed5cf9be8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*bElBhenfI-g3NFiM5nCxnw.png"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">credits : <a class="ae lo" href="https://brookewenig.github.io" rel="noopener ugc nofollow" target="_blank">https://brookewenig.github.io</a></figcaption></figure><p id="150b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在上图中，我们可以看到一个树，它有一组内部节点(条件)和带标签的叶节点(拒绝/接受提议)。</p><h2 id="65b9" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated">选择条件的算法:</h2><ul class=""><li id="d8c1" class="nm nn iq kt b ku mo kx mp la no le np li nq lm nr ns nt nu bi translated">对于 CART(分类和回归树)，我们使用基尼指数作为分类度量。这是计算数据点混合程度的指标。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/2bf19488375ad3448acdab051c72a842.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/1*AKTGXFOunaGJICSmN48Gzg.gif"/></div></figure><p id="b2e4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在创建决策树的每个阶段，具有最大基尼系数的属性被选为下一个条件。当集合是不平等混合时，基尼系数将是最大的。</p><ul class=""><li id="64c8" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">对于迭代二分法 3 算法，我们使用熵和信息增益来选择下一个属性。在下面的等式中，H(s)代表熵，IG(s)代表信息增益。信息增益计算父节点和子节点的熵差。选择具有最大信息增益的属性作为下一个内部节点。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/b0064edc0b4149de3b25799af9b814e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*Ebgpf7PL73oRKKM4yhhzuQ.png"/></div></figure><h2 id="10f7" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">优点:</strong></h2><ul class=""><li id="f52d" class="nm nn iq kt b ku mo kx mp la no le np li nq lm nr ns nt nu bi translated">数据不需要预处理。</li><li id="7810" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">没有关于数据分布的假设。</li><li id="b8e7" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">有效处理共线性。</li><li id="3e09" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">决策树可以对预测提供可理解的解释。</li></ul><h2 id="ebb5" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">缺点:</strong></h2><ul class=""><li id="794f" class="nm nn iq kt b ku mo kx mp la no le np li nq lm nr ns nt nu bi translated">如果我们继续构建树以达到高纯度，则有可能过度拟合模型。决策树剪枝可以用来解决这个问题。</li><li id="c452" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">倾向于离群值。</li><li id="16f0" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">在训练复杂数据集时，树可能会变得非常复杂。</li><li id="aade" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">处理连续变量时丢失有价值的信息。</li></ul><h2 id="3e5e" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">超参数:</strong></h2><p id="f3ef" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">决策树包括许多超参数，我将列出其中的几个。</p><ul class=""><li id="4e0e" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated"><strong class="kt ir">准则</strong>:选择下一个树节点的代价函数。常用的有基尼/熵。</li><li id="f33e" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated"><strong class="kt ir">最大深度:</strong>决策树允许的最大深度。</li><li id="e50f" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated"><strong class="kt ir">最小样本分裂:</strong>分裂一个内部节点所需的最小节点。</li><li id="aa5b" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated"><strong class="kt ir">最小样本叶:</strong>要求在叶节点的最小样本。</li></ul><h2 id="877b" class="mt lx iq bd ly mu mv dn mc mw mx dp mg la my mz mi le na nb mk li nc nd mm ne bi translated"><strong class="ak">与其他车型对比:</strong></h2><p id="3dbc" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated"><strong class="kt ir">决策树 vs 随机森林:</strong></p><ul class=""><li id="f651" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">随机森林是决策树的集合，选择森林的平均/多数投票作为预测输出。</li><li id="d54d" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">随机森林模型将比决策树更不容易过拟合，并给出更一般化的解决方案。</li><li id="f90c" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">随机森林比决策树更加健壮和准确。</li></ul><p id="b51f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">决策树 vs KNN : </strong></p><ul class=""><li id="3887" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">两者都是非参数方法。</li><li id="2d2e" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">决策树支持自动特征交互，而 KNN 不支持。</li><li id="039f" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">由于 KNN 昂贵的实时执行，决策树更快。</li></ul><p id="0487" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">决策树 vs 朴素贝叶斯:</strong></p><ul class=""><li id="a3d4" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">决策树是一种判别模型，而朴素贝叶斯是一种生成模型。</li><li id="2a25" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">决策树更加灵活和简单。</li><li id="5bc1" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">决策树剪枝可能会忽略训练数据中的一些关键值，这会导致准确性下降。</li></ul><p id="2c23" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">决策树 vs 神经网络:</strong></p><ul class=""><li id="bcfe" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">两者都能找到非线性解，并且独立变量之间有相互作用。</li><li id="8bd9" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">当训练数据中有大量类别值时，决策树更好。</li><li id="0597" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">当场景要求对决策进行解释时，决策树比神经网络更好。</li><li id="1679" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">当有足够的训练数据时，神经网络优于决策树。</li></ul><p id="e0a0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">决策树 vs SVM : </strong></p><ul class=""><li id="7e4f" class="nm nn iq kt b ku kv kx ky la oa le ob li oc lm nr ns nt nu bi translated">SVM 使用核技巧来解决非线性问题，而决策树在输入空间中导出超矩形来解决问题。</li><li id="5316" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">决策树更适合分类数据，它比 SVM 更好地处理共线性。</li></ul></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="d83f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在下一个故事中，我将介绍剩余的算法，如朴素贝叶斯、随机森林和支持向量机。如有任何建议或修改，请赐教。</p><p id="251b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">快乐学习:)</p></div></div>    
</body>
</html>