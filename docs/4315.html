<html>
<head>
<title>Aligning hand-written digits with Convolutional Autoencoders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用卷积自动编码器对齐手写数字</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/aligning-hand-written-digits-with-convolutional-autoencoders-99128b83af8b?source=collection_archive---------6-----------------------#2018-08-07">https://towardsdatascience.com/aligning-hand-written-digits-with-convolutional-autoencoders-99128b83af8b?source=collection_archive---------6-----------------------#2018-08-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/dee1275b71e6b0289d4623366bacb54f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZXixptvL4rzkx3EDuj38xw.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/photos/ieic5Tq8YMk?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Chris Ried</a> on <a class="ae jd" href="https://unsplash.com/search/photos/python-programming?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="4b17" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自动编码器是广泛使用的神经网络的无监督应用，其原始目的是发现数据集的潜在低维状态空间，但它们也能够解决其他问题，如图像去噪、增强或着色。</p><p id="8f2b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我想分享我对卷积自动编码器的实验，我训练它将 MNIST 数据集中随机旋转的手写数字对齐到它们的原始位置。请注意，这篇文章不是用来介绍自动编码器的，而是一个应用的展示和不同解码架构的比较。在接下来的部分中，我将展示一个案例，该案例强化了具有卷积层的上采样块比去卷积层性能更好的共识，并且还展示了卷积层与全连接层的组合在简单自动编码器和仅具有卷积层的卷积自动编码器上都具有优势。</p><p id="167b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您已经熟悉卷积自动编码器和上采样技术，可以跳过下一节，如果不熟悉，我推荐您阅读它和相关文章。</p><h1 id="2a2f" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">介绍</h1><p id="97c2" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">自动编码器背后的主要思想是将输入减少到具有更少维度的潜在状态空间中，然后尝试从该表示中重构输入。第一部分称为编码，第二步是解码阶段。通过减少代表数据的变量数量，我们迫使模型学习如何只保留有意义的信息，从这些信息中输入是可重构的。它也可以被视为一种压缩技术。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi me"><img src="../Images/da7d1fbe84b58a5775c77ad6e4c86313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nmnm2rRJjaBD1TCUgL4dmA.png"/></div></div></figure><p id="7a41" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面这个帖子是我推荐的一个很棒的介绍:</p><div class="ip iq gp gr ir mj"><a href="https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jh gy z fp mo fr fs mp fu fw jf bi translated">自动编码器—深度学习比特#1</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">特色:数据压缩，图像重建和分割(附实例！)</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">hackernoon.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx ix mj"/></div></div></a></div><p id="e69f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您也对实现感兴趣，下一篇文章是基于第一个链接编写的，但它也包括 Tensorflow 中的详细实现:</p><div class="ip iq gp gr ir mj"><a rel="noopener follow" target="_blank" href="/autoencoders-introduction-and-implementation-3f40483b0a85"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jh gy z fp mo fr fs mp fu fw jf bi translated">自动编码器-TF 中的介绍和实现。</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">简介和概念:</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="my l mu mv mw ms mx ix mj"/></div></div></a></div><p id="801f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个实现的一个有趣部分是上采样的使用。由于内核的重叠，反转卷积层可能相当具有挑战性。大多数深度学习框架都包括反卷积层(有人称之为转置卷积层)，它只是一个反向卷积层。尽管这些层在重建输入时直观上有意义，但它们也有产生棋盘假象的缺点。为了解决这个问题，它们已经被上采样和简单卷积层所取代。以下两个链接的帖子都是对这个问题的详细解释。</p><div class="ip iq gp gr ir mj"><a href="https://distill.pub/2016/deconv-checkerboard/" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jh gy z fp mo fr fs mp fu fw jf bi translated">去卷积和棋盘伪影</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">当我们仔细观察神经网络生成的图像时，我们经常会看到一种奇怪的棋盘图案…</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">蒸馏. pub</p></div></div><div class="ms l"><div class="mz l mu mv mw ms mx ix mj"/></div></div></a></div><div class="ip iq gp gr ir mj"><a rel="noopener follow" target="_blank" href="/up-sampling-with-transposed-convolution-9ae4f2df52d0"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jh gy z fp mo fr fs mp fu fw jf bi translated">转置卷积上采样</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">如果你听说过转置卷积，并对它的实际含义感到困惑，这篇文章是为…</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="na l mu mv mw ms mx ix mj"/></div></div></a></div></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="bb91" class="lb lc jg bd ld le ni lg lh li nj lk ll lm nk lo lp lq nl ls lt lu nm lw lx ly bi translated">该项目</h1><p id="7982" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">正如我前面提到的，我使用了 MNIST 数据集，并将每张图像旋转了某个随机角度。模型的任务是将它们重新排列成原始状态。在实验过程中，我比较了以下架构；</p><ul class=""><li id="0e47" class="nn no jg kf b kg kh kk kl ko np ks nq kw nr la ns nt nu nv bi translated">一个简单的自动编码器，有三个隐藏层，我用它作为基准</li><li id="0a66" class="nn no jg kf b kg nw kk nx ko ny ks nz kw oa la ns nt nu nv bi translated">一种卷积自动编码器，仅由编码器中的卷积层和解码器中的转置卷积层组成</li><li id="3d2f" class="nn no jg kf b kg nw kk nx ko ny ks nz kw oa la ns nt nu nv bi translated">另一种卷积模型，在编码器部分使用卷积块和最大池，在解码器中使用卷积层的上采样</li><li id="2c94" class="nn no jg kf b kg nw kk nx ko ny ks nz kw oa la ns nt nu nv bi translated">最后一个模型是卷积层和全连接层的组合</li></ul><p id="02e7" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有模型都在具有相同超参数的相同数据集上训练，并且在瓶颈层中具有 256 个变量。</p><h2 id="d291" class="ob lc jg bd ld oc od dn lh oe of dp ll ko og oh lp ks oi oj lt kw ok ol lx om bi translated">基线模型</h2><p id="2531" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">首先，让我们看看基线自动编码器，它具有以下结构。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi on"><img src="../Images/b0ffbfb91c56b45cc5eabb2b5f73f15b.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*dbFHNVxLH5-ybmYNrAM4OA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Architecture of the Baseline Model</figcaption></figure><p id="0c9b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我在训练中使用了均方误差，最佳模型在训练数据上达到了 0.0158，在验证数据上达到了 0.0208，正如预期的那样，在测试数据上达到了 0.0214。下图是测试数据集中的一个示例，显示了模型如何成功地重新排列数字 4。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/f0d15f5df08cc5ac3d8ee4af203ce7cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*X5eWSWTuM65x2bNnNpyDnA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">From left to right: input image, target image and the image predicted by the model</figcaption></figure><p id="1d2d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型意识到图像显示的是数字 4，并将其旋转回原来的位置。预测仍然是一个明显的数字 4，然而，边缘有点模糊，顶部两条线之间的差距几乎消失了，这可能是低维瓶颈的结果。总的来说，该模型能够完成任务并产生可接受的结果，但是，它在一些输入方面存在问题。所有模型中最具挑战性的一张图片如下。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/3ab601a2f09c970ddaaf03a29f6f08a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BWYP5D808Ya1_aj6LNp68w.png"/></div></div></figure><p id="9ca3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">中间的真实图像清楚地显示了数字 4，然而，由于长的水平线，旋转后的图像有点像数字 5。该模型努力识别数字，最终得到一个水平质量，它显示的是数字 5 而不是数字 4。</p><h2 id="7dda" class="ob lc jg bd ld oc od dn lh oe of dp ll ko og oh lp ks oi oj lt kw ok ol lx om bi translated">具有转置卷积的卷积自动编码器</h2><p id="92a0" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">第二种模型是卷积自动编码器，它只包括卷积层和解卷积层。在编码器中，输入数据通过 12 个具有 3×3 内核的卷积层，滤波器大小从 4 开始增加到 16。由于卷积层没有填充，并且步长大小为 1，瓶颈的大小为 16x4x4，这意味着瓶颈中的变量数量与基线模型的数量相匹配。解码器利用转置卷积层来反映这种架构。仅使用卷积层可能看起来不寻常，但在这种情况下，目标是比较技术，而不是实现出色的结果。</p><p id="2e93" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就度量标准而言，这种架构无法接近基准模型。训练 MSE 损失为 0.0412，验证损失为 0.0409，测试损失为 0.0407。较大的损失可能是由于可训练参数的尺寸较小。使用卷积层将基准测试中的参数数量从大约 100 万减少到只有 23000 个。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/d09e2641cc7909008c96bdfe440e18fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*_kK4VBQkoCsxNKHHeigVRw.png"/></div></figure><p id="f496" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在许多情况下，如上面的数字 9 所示，模型能够解决问题并预测与所需输出匹配的可识别数字，但一般来说，它无法生成更好性能所需的如此精细和狭窄的线条，并且生成模糊的图像，其数字难以识别，如下所示。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/41d1273a9f2daf7d016314e1f416f532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*S6RK9q5BHz9E1MYBaTtmvw.png"/></div></figure><p id="aaa7" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种效果可以归因于去卷积层，因为除了边缘之外的每个像素都是作为重叠滤波器的总和而生成的。这个模型在上面考虑的复杂例子中表现得更差，不仅产生了模糊的输出，而且产生了一个类似于数字 3 而不是期望的数字 4 的数字。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/10680b643735a069944d2681bf9a3f8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yKm-U9v-RyZ95x3FzvoQdQ.png"/></div></div></figure><h2 id="ca0d" class="ob lc jg bd ld oc od dn lh oe of dp ll ko og oh lp ks oi oj lt kw ok ol lx om bi translated">带上采样的卷积自动编码器</h2><p id="fbc3" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我考虑的下一个架构是具有卷积层、最大池层和上采样层的卷积自动编码器。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/4af08d2b9a076905c83b61f7524d7922.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*ONvzSKv6Kg5ZKftNxRDpqA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Architecture of the Convolutional Autoencoder with Upsampling</figcaption></figure><p id="6712" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在训练指标方面，它获得了比基准模型稍大的 MSE 值；0.0293 关于训练，0.0293 关于验证，0.0297 关于测试数据集。但与之前的模型类似，更差的分数也带来了好处，该模型的规模不到基准的三分之一，只有 2.9 万个可训练参数，但它的表现仍然可以接受。此外，度量的狭窄分布表明，它在没有额外正则化的情况下概括得很好。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/d4e6d3d1898326f8c766d7165937db24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*HNNyMwI7TVQSNoFFE51O-w.png"/></div></figure><p id="37c8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的图片上，我们可以看到它仍然能够识别数字并重新排列它。它仍然稍微模糊了边缘，但没有以前的模型那么多。在这种情况下，它甚至删除了顶部循环中不重要的部分。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/e1522a6b5213ff8f8ccdd6f89279f906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m0cPexMkfEDDZPJPhJp9zw.png"/></div></div></figure><p id="0a7c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个模型也不能识别之前讨论的硬例子，但是它比之前的模型更简单。它的输出部分类似于数字 5，这表明它在识别数字和重建数字时有问题。</p><h2 id="6fdc" class="ob lc jg bd ld oc od dn lh oe of dp ll ko og oh lp ks oi oj lt kw ok ol lx om bi translated">组合模型</h2><p id="7f92" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">在看到上采样提供了更好的结果和更精确的输出后，我制作了另一个架构，它以下面的方式组合了上采样模型中的块和完全连接的层。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi or"><img src="../Images/8710b079b9cb5c96219eb0deda980721.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*NbPF0uWYwLheMRSV74CtBA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Architecture of the Combined model</figcaption></figure><p id="9d70" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">卷积层和池层成功地取代了基准测试的第一个密集层，并产生了迄今为止最好的模型，只有 40 万个可训练参数，这仍然远远少于基准测试的大约一百万个参数。它在训练数据上实现了 0.0151 MSE 损失，在验证数据上实现了 0.0151 MSE 损失，在测试数据上实现了 0.0151 MSE 损失。它生成的情节也比以前的好。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/65b9820e7244a0d476d564e1850654ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*cyWwOeN_lySIo0nsPjzqDA.png"/></div></figure><p id="6347" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过混合这两种类型的图层，模型能够生成更精细的线条和更模糊的图像，但重建仍然不完美，如下图所示。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/6beb9062a8dc4fafb7d418299a051c50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*vUarfBrG4fdhx1Sq81U62Q.png"/></div></figure><p id="5ab3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个硬例子的问题在这个模型的输出上更加明显。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/10536f8db4cf4571c73357e3f58da4b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*008gfUJju41GD1NqJ1fDUw.png"/></div></div></figure><p id="5d48" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">显然，该模型试图重新排列输入的数字，就好像它是数字 5 一样，这加强了这样一种假设，即该模型在这个例子中遇到了困难，因为这个数字是不可识别的。</p><h1 id="1aee" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">评论</h1><p id="df4c" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">即使我使用 MSE 作为性能的度量，关于任务本身的一个有趣的评论是，在某些情况下，高 MSE 并不意味着错误的输出。例如，以下预测是由组合模型做出的。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/d545fce517df6cee1adac202590036a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ACFBGFW5lv8xSB4mZVj8RQ.png"/></div></div></figure><p id="b4c9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">预测图像是基于输入图像预期的绘制得很好的零，但是目标图像上的数字不是以常规方式书写的，因此预测 MSE 误差很高。自然，我们不能通过神经网络来了解这些信息。此外，我之前预计模型会混淆数字 6 和数字 9，反之亦然，但输出图像显示这种情况只是偶尔发生。例如，下图显示了来自验证数据集的 upsamplig 模型的输出之一，该模型已被很好地识别，并且除了尾部模糊之外，被重构为正常的数字 9。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/859411224d81bd43c3e476297b6263c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*5mS6qn66JZoMYsYpsOavWQ.png"/></div></figure><p id="79d0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一方面，我用来演示数据集中的困难样本的问题似乎是一个更大挑战的一部分，因为许多模型在一些情况下很难将数字 4 旋转回来。特别是，具有转置卷积层的模型很难解决这个问题，并生成了如下图像。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/cd85f62560de17e03c6cd1c1190beefb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fzl2_j3ZnNPiYPzurgPwCA.png"/></div></div></figure><p id="3125" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，其他模型也没有什么不同。例如，基线模型犯了以下错误。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/acc64418b286c67a1c14977ffc8ded8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JCRtL1gW7N2qTPmlzOq_Ig.png"/></div></div></figure><p id="6d06" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">令人惊讶的是，旋转数字 4 产生的图像与数字 5 甚至数字 2 有更多的相似之处。</p><h1 id="0e73" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结论</h1><p id="9e29" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">正如该项目所示，仔细选择模型不一定会提高其性能，但它有助于构建最简单的合适模型，从而减少过度拟合的可能性，正如我们在训练指标的较小分布中看到的那样。一般来说，使用简单的自动编码器似乎是一个合适的选择，因为它以令人满意的方式解决了这个问题，但在指标中可以观察到过度拟合的痕迹，并且它的大小明显大于其他模型。我们可以尝试通过减少中间层的节点数量或简单地省略它们来简化这个模型，但是，如果我们仍然保持 256 个大瓶颈，可实现的最低参数数量大约是 40 万个。此外，额外的正则化技术可能有助于泛化，但这似乎是不必要的，因为具有上采样层的卷积自动编码器能够在网络小十倍以上的情况下实现几乎同样好的结果。这两种类型的层的组合最终以合理的架构提供了最佳的性能。</p><p id="652a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我只挑选了一些图片来展示这些架构的性能。在链接的 Github 存储库中可以找到更多图片的更详细的评估，其中也包括 Keras 中的实现。</p><p id="6462" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae jd" href="https://github.com/pasztorb/Rotational_CAD" rel="noopener ugc nofollow" target="_blank">https://github.com/pasztorb/Rotational_CAD</a></p></div></div>    
</body>
</html>