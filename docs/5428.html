<html>
<head>
<title>Review: YOLOv1 — You Only Look Once (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习:YOLOv1 —你只看一次(物体检测)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=collection_archive---------4-----------------------#2018-10-17">https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=collection_archive---------4-----------------------#2018-10-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="73fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>这个故事里，<strong class="jp ir">的 YOLOv1 </strong>由艾研究中心(FAIR)审查。<strong class="jp ir">该网络只需查看图像一次即可检测多个物体。</strong>因此，它被称为<strong class="jp ir"> YOLO </strong>，<strong class="jp ir">你只看一次</strong>。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ku"><img src="../Images/800f63cf6434199cd1762624a5bbd67d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YXP2MUhtyAo6bRWIFm1YAg.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk"><strong class="bd lk">YOLOv1 without Region Proposals Generation Steps</strong></figcaption></figure><p id="9a23" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过只看一次图像，<strong class="jp ir">检测速度是实时的(45 fps) </strong>。<strong class="jp ir">快 YOLOv1 达到 155 fps。</strong>这是另一种最先进的深度学习对象检测方法，在我写这个故事的时候已经发表在<strong class="jp ir"> 2016 CVPR </strong>上，被引用<strong class="jp ir">2000 多次</strong>。(<a class="ll lm ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----e1f3ffec8a89--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p><p id="11f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是作者提供的 YOLOv1 示例:</p><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="ln lo l"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">YOLO Watches Sports</figcaption></figure><p id="46ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果感兴趣，他们还提供了其他 YOLOv1 示例:</p><ul class=""><li id="f5ed" class="lp lq iq jp b jq jr ju jv jy lr kc ls kg lt kk lu lv lw lx bi translated"><a class="ae ly" href="https://www.youtube.com/watch?v=U9c1gXO8xEU" rel="noopener ugc nofollow" target="_blank"> YOLO 看 YouTube 第一集</a></li><li id="3407" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk lu lv lw lx bi translated"><a class="ae ly" href="https://www.youtube.com/watch?v=r6ZzopHEO1U" rel="noopener ugc nofollow" target="_blank"> YOLO 看 YouTube 第二部</a></li><li id="3611" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk lu lv lw lx bi translated"><a class="ae ly" href="https://www.youtube.com/watch?v=A5QgsNNdphU" rel="noopener ugc nofollow" target="_blank"> YOLO 看 YouTube 第三部</a></li><li id="b95a" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk lu lv lw lx bi translated"><a class="ae ly" href="https://www.youtube.com/watch?v=dTcfAuCEV3A" rel="noopener ugc nofollow" target="_blank"> YOLO 手表自然第一部</a></li><li id="f02a" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk lu lv lw lx bi translated"><a class="ae ly" href="https://www.youtube.com/watch?v=K9a6mGNmhbc" rel="noopener ugc nofollow" target="_blank"> YOLO 手表自然第二部</a></li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="b1b3" class="ml mm iq bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">涵盖哪些内容</h1><ol class=""><li id="37c9" class="lp lq iq jp b jq nj ju nk jy nl kc nm kg nn kk no lv lw lx bi translated"><strong class="jp ir">统一检测</strong></li><li id="444f" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk no lv lw lx bi translated"><strong class="jp ir">网络架构</strong></li><li id="d82d" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk no lv lw lx bi translated"><strong class="jp ir">损失函数</strong></li><li id="6676" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk no lv lw lx bi translated"><strong class="jp ir">结果</strong></li></ol></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="9596" class="ml mm iq bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">1.统一检测</h1><h2 id="a7a0" class="np mm iq bd mn nq nr dn mr ns nt dp mv jy nu nv mz kc nw nx nd kg ny nz nh oa bi translated">1.1 现有技术:R-CNN</h2><p id="c3cb" class="pw-post-body-paragraph jn jo iq jp b jq nj js jt ju nk jw jx jy ob ka kb kc oc ke kf kg od ki kj kk ij bi translated">像 R-CNN 这样的现有技术首先生成 2K 个区域提议(边界框候选)，然后如下检测每个区域提议内的对象:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi oe"><img src="../Images/36421452827a7c800f42fa3993eaf7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UdiAmikev_39REgUgiqwGA.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk"><strong class="bd lk">R-CNN (First, extract region proposals; Then detect object for each region proposals)</strong></figcaption></figure><p id="093d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为需要两步，所以它们非常慢。</p><h2 id="cf66" class="np mm iq bd mn nq nr dn mr ns nt dp mv jy nu nv mz kc nw nx nd kg ny nz nh oa bi translated">1.2 YOLO</h2><p id="4aa5" class="pw-post-body-paragraph jn jo iq jp b jq nj js jt ju nk jw jx jy ob ka kb kc oc ke kf kg od ki kj kk ij bi translated">YOLO 建议有一个统一的网络来一次性完成所有任务。同样，端到端的培训网络也可以实现。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi of"><img src="../Images/c0bd8af8823ab8e01b8ec1cb4d60842f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9nikM2b0u-m67SJpQXftKA.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk"><strong class="bd lk">YOLO Unified Detection</strong></figcaption></figure><p id="edcb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">输入图像被分成一个 S×S 网格(S=7) </strong>。如果对象的中心落入网格单元，则该网格单元负责检测该对象。</p><p id="b095" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">每个网格单元预测 B 个边界框(B=2)和这些框的置信度得分。</strong>这些置信度得分反映了模型对盒子包含对象<strong class="jp ir">的置信度，即盒子中的任何对象 P(对象)。</strong></p><p id="da1a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">每个边界框由 5 个预测组成:x、y、w、h 和置信度。</strong></p><ul class=""><li id="4ea5" class="lp lq iq jp b jq jr ju jv jy lr kc ls kg lt kk lu lv lw lx bi translated">(x，y)坐标表示相对于网格单元边界的盒子中心。</li><li id="825e" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk lu lv lw lx bi translated">相对于整个图像预测宽度 w 和高度 h。</li><li id="c6a5" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk lu lv lw lx bi translated">置信度表示预测框和任何基础真值框之间的联合交集(IOU)。</li></ul><p id="1c53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每个网格单元还预测条件类概率 P(Classi|Object)。(班级总数=20) </p><p id="5b55" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下图说明了网络的输出:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi og"><img src="../Images/284cd313dabcdfcccaff4ba4a5966c68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YG6heD55fEmZeUKRSlsqlA.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk"><strong class="bd lk">The output from YOLO</strong></figcaption></figure><p id="7508" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">输出大小变成:7×7×(2×5+20)=1470 </strong></p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="4d02" class="ml mm iq bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">2.网络体系结构</h1><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi oh"><img src="../Images/4253dd8e15bca0102b37ae575d0ae305.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q5feieizWKYq7dpWjYvCOw.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk"><strong class="bd lk">YOLO model network architecture</strong></figcaption></figure><p id="ecf5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">该模型由 24 个卷积层和 2 个全连接层组成。</strong>交替的 1×1 卷积层减少了来自前面层的特征空间。(GoogLeNet 中使用了 1×1 conv 以减少参数数量。)</p><p id="d8ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">快速 YOLO 更少的卷积层(9 层而不是 24 层)以及这些层中更少的滤波器。</strong>网络管道总结如下:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi oi"><img src="../Images/e3b7711d1f5b361d7856488ccdadeff8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z5lLvL3r8MeOmgcGkYnT2A.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk"><strong class="bd lk">Whole Network Pipeline</strong></figcaption></figure><p id="2617" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我们可以看到，输入图像通过网络一次，然后可以检测对象。我们可以进行<strong class="jp ir">端到端学习</strong>。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="2fa6" class="ml mm iq bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">3.损失函数</h1><h2 id="f68d" class="np mm iq bd mn nq nr dn mr ns nt dp mv jy nu nv mz kc nw nx nd kg ny nz nh oa bi translated">3.1 损失函数解释</h2><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi oj"><img src="../Images/df006ba8d37a27b123d06ad2ac1e2c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1noFJp_rnXsG0TJvtoghFA.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk"><strong class="bd lk">Loss Function</strong></figcaption></figure><p id="95d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上图所示，损失函数中有<strong class="jp ir"> 5 项。</strong></p><ol class=""><li id="b731" class="lp lq iq jp b jq jr ju jv jy lr kc ls kg lt kk no lv lw lx bi translated"><strong class="jp ir">第一项(x，y) </strong>:边界框 x 和 y 坐标被参数化为特定网格单元位置的偏移量，因此它们也被限制在 0 和 1 之间。并且只有在有目标的情况下才估计误差平方和(SSE)。</li><li id="cad3" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk no lv lw lx bi translated"><strong class="jp ir">第二项(w，h) </strong>:边界框的宽度和高度由图像的宽度和高度归一化，使它们落在 0 和 1 之间。SSE 只有在有对象的情况下才被估计。因为大盒子中的小偏差不如小盒子中的小偏差重要。边界框宽度 w 和高度 h 的平方根，而不是宽度和高度直接解决这个问题。</li><li id="a15f" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk no lv lw lx bi translated"><strong class="jp ir">第三项和第四项(置信度)</strong>(即预测框和任何地面真实框之间的 IOU):在每个图像中，许多网格单元不包含任何对象。这将这些单元的“置信度”分数推向零，通常会压倒包含对象的单元的梯度，并使模型不稳定。因此，不包含对象的盒子的置信度预测的损失减少，即<strong class="jp ir"> λnoobj </strong> =0.5。</li><li id="c8db" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk no lv lw lx bi translated"><strong class="jp ir">第五项(类概率)</strong>:有对象时类概率的 SSE。</li><li id="abbb" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk no lv lw lx bi translated"><strong class="jp ir"> λcoord </strong>:由于第三和第四项中提到的相同原因，λcoord = 5，以增加边界框坐标预测的损失。</li></ol><h2 id="4bc3" class="np mm iq bd mn nq nr dn mr ns nt dp mv jy nu nv mz kc nw nx nd kg ny nz nh oa bi translated">3.2 其他细节</h2><p id="3710" class="pw-post-body-paragraph jn jo iq jp b jq nj js jt ju nk jw jx jy ob ka kb kc oc ke kf kg od ki kj kk ij bi translated">除了最后一层，其他所有层都使用 leaky ReLU 作为激活函数。<strong class="jp ir">ImageNet</strong>对前 20 个卷积层进行预训练，使其在一周内达到 88%的前 5 名准确率。<strong class="jp ir">然后，根据来自 PASCAL VOC 2007 和 2012 的训练和验证数据集，对网络进行大约 135 个时期的训练。</strong>在对 2012 年进行测试时，我们还纳入了 VOC 2007 年的测试数据用于培训。使用的批次大小为 64。还使用了第一个完全连接的层的丢弃和数据增加。</p><p id="a4d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 PASCAL VOC 上，<strong class="jp ir">预测了每幅图像 98 个包围盒</strong>。</p><p id="614a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一些大的物体或者多个小区边界附近的物体可以同时被多个小区很好的定位。<strong class="jp ir">使用非最大抑制。</strong></p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="2389" class="ml mm iq bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">4.结果</h1><h2 id="0458" class="np mm iq bd mn nq nr dn mr ns nt dp mv jy nu nv mz kc nw nx nd kg ny nz nh oa bi translated"><strong class="ak"> 4.1 VOC 2007 </strong></h2><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/5d99eb76c9d3ee603a36c172f945cb63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*zxDddLucPHk2sM5DHHEWvw.png"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk"><strong class="bd lk">mAP and FPS Results (VOC 2007)</strong></figcaption></figure><ul class=""><li id="be1a" class="lp lq iq jp b jq jr ju jv jy lr kc ls kg lt kk lu lv lw lx bi translated"><strong class="jp ir"> YOLO </strong> : 63.4 mAP(均值平均预测)和 45 FPS。与 DPM、R-CNN、快速 R-CNN 和更快 R-CNN 相比，YOLO 可以在相似的 mAP 下获得实时性能。</li><li id="16a4" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk lu lv lw lx bi translated"><strong class="jp ir">快速 YOLO </strong> : 52.7%贴图，155 FPS。如此高的 FPS，与 100Hz DPM 相比，它也有非常高的 mAP。</li><li id="1591" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk lu lv lw lx bi translated"><strong class="jp ir"> YOLO VGG-16 </strong> : YOLO 使用 VGG016 架构，由于没有 1×1 卷积来减少模型大小，所以速度很慢，即使有 66.4%的 mAP 也只有 21 FPS。</li></ul><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/7e73f785a302bee78cd8a39989a7cce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*3dYkVN2FHkykV9gKsnDW2g.png"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk"><strong class="bd lk">Error Analysis (VOC 2007)</strong></figcaption></figure><p id="57b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">物体定位</strong>:与快速 R-CNN 相比，YOLO 很难正确定位物体。</p><p id="66e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">背景误差</strong> : YOLO 背景误差较小。快速 R-CNN 有 13.6%的顶部检测是假阳性。</p><p id="2772" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于 YOLO 和快速 R-CNN 各有利弊，所以可以将它们结合起来以具有更高的准确性。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi om"><img src="../Images/64dde1aaadb2d2ba6a4b8fe289544ffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*Xn7b_pfx-vnjeuUcXCq3tA.png"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk"><strong class="bd lk">Model combination with Fast R-CNN (VOC 2007)</strong></figcaption></figure><p id="0ff3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型组合后，获得了 75.0%的 mAP，与其他组合相比具有相对较高的精度。</p><h2 id="2746" class="np mm iq bd mn nq nr dn mr ns nt dp mv jy nu nv mz kc nw nx nd kg ny nz nh oa bi translated"><strong class="ak">2012 年 4.2 VOC</strong></h2><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi on"><img src="../Images/42fa31009767081fc2e25fbd6ae957fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MtsGFQrSmoU5n-KBjjN_Rg.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk"><strong class="bd lk">Fast R-CNN + YOLO (VOC 2012)</strong></figcaption></figure><p id="992c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">快速 R-CNN + YOLO 具有 70.7%的 mAP，这是最高性能的检测方法之一。</p><h2 id="d7d1" class="np mm iq bd mn nq nr dn mr ns nt dp mv jy nu nv mz kc nw nx nd kg ny nz nh oa bi translated">4.3 普遍性</h2><p id="d10d" class="pw-post-body-paragraph jn jo iq jp b jq nj js jt ju nk jw jx jy ob ka kb kc oc ke kf kg od ki kj kk ij bi translated">在 Picasso 数据集和 People-Art 数据集上也尝试了艺术品上的人物检测。毕加索的模特接受 VOC 2012 培训，而人物艺术的模特接受 VOC 2010 培训。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi oo"><img src="../Images/db9cca078480d049af761fb7a8d3f06e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Sms7_zQhjjIB8GRD8oujg.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk"><strong class="bd lk">Generalization Results</strong></figcaption></figure><ul class=""><li id="be0a" class="lp lq iq jp b jq jr ju jv jy lr kc ls kg lt kk lu lv lw lx bi translated"><strong class="jp ir"> R-CNN </strong>在 VOC 2007 上 AP 高。然而，R-CNN <strong class="jp ir">在应用到艺术品上时，就大大地减少了</strong>。<strong class="jp ir"> R-CNN 使用</strong> <strong class="jp ir">选择性搜索为自然图像调整的边界框提议。</strong></li><li id="659d" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk lu lv lw lx bi translated"><strong class="jp ir"> DPM 将其 AP </strong>很好地应用于艺术品。先前的工作理论上认为 DPM 性能良好，因为它具有对象形状和布局的<strong class="jp ir">强空间模型。</strong></li><li id="10b7" class="lp lq iq jp b jq lz ju ma jy mb kc mc kg md kk lu lv lw lx bi translated"><strong class="jp ir"> YOLO </strong>在 VOC 2007 上有很好的表现，当应用于艺术品时，它的 AP 降解比其他方法少。<strong class="jp ir">艺术作品和自然图像在像素级别上非常不同，但它们在对象的大小和形状方面相似</strong>，因此 YOLO 仍然可以预测良好的边界框和检测。</li></ul><p id="98e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面显示了一些非常有趣的可视化结果:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi op"><img src="../Images/29220beaf7c75a033eae3f7c1678c875.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nmud9CO0cOUNyLlwxxCc7g.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk"><strong class="bd lk">Some detection results</strong></figcaption></figure><p id="665c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以后再讲 YOLOv2 和 YOLOv3。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="88b4" class="ml mm iq bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">参考</h1><ol class=""><li id="c6cc" class="lp lq iq jp b jq nj ju nk jy nl kc nm kg nn kk no lv lw lx bi translated">【2016 CVPR】【yolov 1】<br/>T5【你只看一次:统一的，实时的物体检测</li></ol><h1 id="c2d9" class="ml mm iq bd mn mo oq mq mr ms or mu mv mw os my mz na ot nc nd ne ou ng nh ni bi translated">我的评论</h1><p id="0a40" class="pw-post-body-paragraph jn jo iq jp b jq nj js jt ju nk jw jx jy ob ka kb kc oc ke kf kg od ki kj kk ij bi translated">[ <a class="ae ly" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener"> R-CNN </a> ] [ <a class="ae ly" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快速 R-CNN </a> ] [ <a class="ae ly" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快 R-CNN</a>][<a class="ae ly" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener">Google net</a>]</p></div></div>    
</body>
</html>