<html>
<head>
<title>Disentanglement with Variational Autoencoder: A Review</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变分自动编码器的解惑:综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/disentanglement-with-variational-autoencoder-a-review-653a891b69bd?source=collection_archive---------5-----------------------#2018-11-28">https://towardsdatascience.com/disentanglement-with-variational-autoencoder-a-review-653a891b69bd?source=collection_archive---------5-----------------------#2018-11-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c53a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可解释的因子分解表示的学习在机器学习中已经存在了很长时间。但是随着最近像变分自动编码器(VAE)这样的深度生成模型的进步，对学习这种解开的表示的兴趣出现了爆炸。由于任何生成模型的目标本质上都是捕捉潜在的数据生成因素，因此<strong class="jp ir">解开的表示将意味着单个潜在单元对单个生成因素的变化敏感。</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/457e6cb560b5af06a7655272c6d496c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UD_nM-q83NzWeL1niwekhg.jpeg"/></div></div></figure><p id="699d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于香草 VAE 鼓励生成因子上的后验分布<em class="kx"> q(z|x) </em>更接近各向同性高斯<em class="kx"> N(0，I) </em>，它促进了潜在生成因子的解开。这是因为各向同性高斯的协方差∑等于单位矩阵 I，这意味着所有维度都是独立的。在 ELBO 中，这是由第二个术语推动的:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ky"><img src="../Images/1bd8e032becace98a7ff012112c72df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2saAvYAdB3Xsd48BxGJx-Q.png"/></div></div></figure><p id="663e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，有效解纠缠所需的学习压力可能还不够，因为在 VAE，我们还想对输入信号进行适当的自动编码(或重建)，而重建损失(第一项)与第二项相比可能太大。受此启发，[<a class="ae kz" href="https://openreview.net/pdf?id=Sy2fzU9gl" rel="noopener ugc nofollow" target="_blank">β-VAE</a>通过赋予第二项β &gt; 1 权重，对潜在瓶颈产生了更强的约束。因此，他们的目标函数是这样的:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi la"><img src="../Images/555031732581fe491fe0420af954c768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-YXwwBQB2j5LMGeYLlJlCg.png"/></div></div></figure><p id="0796" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于增加了第二项的权重，重建精度开始下降。这给许多研究者带来了重要的研究问题:“<strong class="jp ir">如何在不损失重构能力的情况下实现更好的解纠缠？</strong>“ELBO 的[ <a class="ae kz" href="http://approximateinference.org/accepted/HoffmanJohnson2016.pdf" rel="noopener ugc nofollow" target="_blank">手术极大地帮助了寻找这个答案的道路，其中第二个术语被分解为:</a></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lb"><img src="../Images/839d502c297489d098adc8684e25ab91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vu763kXsKhIo_-0CjAC2Hw.png"/></div></div></figure><p id="d387" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，第一项是索引码互信息(MI ),第二项是对先验的边际 KL。这种分解提供了一种观点，即实际上第二项对学习解纠缠表示更重要，惩罚 MI(比常规 ELBO 更多)可能是重建不佳的原因。此外，[ <a class="ae kz" href="https://arxiv.org/pdf/1606.03657.pdf" rel="noopener ugc nofollow" target="_blank"> InfoGAN </a> ](不是基于 VAE 的模型)最大化了相同的 MI 以实现更好的解缠。</p><p id="a087" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有了这个理论基础，本文[ <a class="ae kz" href="https://arxiv.org/pdf/1711.00848.pdf" rel="noopener ugc nofollow" target="_blank"> link </a> ]在正则 ELBO 上增加了(-1) <em class="kx"> λ </em>加权<em class="kx"> KL(q(z)||p(z)) </em>。但是，由于<em class="kx"> KL(q(z)||p(z)) </em>已经存在于 ELBO 中，它们实际上是最小化(<em class="kx"> λ </em> + 1)加权<em class="kx"> KL(q(z)||p(z)) </em>以鼓励解纠缠。注意[ <a class="ae kz" href="https://arxiv.org/pdf/1511.05644.pdf" rel="noopener ugc nofollow" target="_blank"> adversarialAE </a> ]也使用对抗性损失最小化这个 KL(不是 KL(q(z|x)||p(z)))。</p><p id="af2b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更深入地说，【<a class="ae kz" href="https://arxiv.org/pdf/1802.04942.pdf" rel="noopener ugc nofollow" target="_blank"> TC-βVAE </a>】进一步将这个边际 KL 分解为总相关性(TC) <em class="kx">【第一项】</em>和维度 KL <em class="kx">【第二项】</em>:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lc"><img src="../Images/7ecdd523c21b03c8f899d775e173cc0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mf3R-9uysnbtya5j0HjxTQ.png"/></div></div></figure><p id="5284" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过这种分解，他们认为 TC (Watanabe 1960)是一种流行的多随机变量相关性度量，是学习解纠缠表示的最重要的术语，因此用一些β权重惩罚 TC，因此他们的总体目标看起来像:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ld"><img src="../Images/cc2eecbf3c00fb71ffd56e1085b63c78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l8bZf-WdXnV4N8DNAbUSIQ.png"/></div></div></figure><p id="6350" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与此同时，[<a class="ae kz" href="https://arxiv.org/pdf/1802.05983.pdf" rel="noopener ugc nofollow" target="_blank">dfactoring</a>]论文也承认了 TC 对于解纠缠的重要性，并在 ELBO 中增加了这个术语的权重(- <em class="kx"> λ) </em>。同样，由于 TC <em class="kx"> </em>已经存在于 ELBO 中，他们实际上是最小化(<em class="kx"> λ </em> + 1)加权 TC 以鼓励解开。</p><p id="1435" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，根本的挑战在于对<em class="kx"> q(z) </em>(聚合后验分布)的估计，这取决于整个数据集(不仅仅是一个小批量)。这导致所有这些工作在估计<em class="kx"> q(z) </em>或任何涉及它的项时采取不同的方法。例如，[<a class="ae kz" href="https://arxiv.org/pdf/1802.05983.pdf" rel="noopener ugc nofollow" target="_blank">dfactoring</a>]使用了带有独立鉴别器的密度比技巧。</p><p id="e4d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总的来说，我相信，在不久的将来，解开与 VAE 的纠葛会变得更加有趣。(更新)针对这一点，在[ <a class="ae kz" href="https://arxiv.org/pdf/1909.01839.pdf" rel="noopener ugc nofollow" target="_blank"> IBP-VAE </a> ]上，我们认为，随着生成因素复杂性的增加，这些讨论过的方法中的一些方法的解开能力下降，并提出 VAE 与非参数潜在因素模型(IBP-VAE)，潜在密度可以随着数据复杂性的增加而增加，表明解开能力提高。</p></div></div>    
</body>
</html>