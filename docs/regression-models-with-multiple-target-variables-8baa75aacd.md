# 多目标变量回归模型

> 原文：<https://towardsdatascience.com/regression-models-with-multiple-target-variables-8baa75aacd?source=collection_archive---------0----------------------->

![](img/4a661b7336092765cad2296cd75baee1.png)

Photo by [Annie Spratt](https://unsplash.com/@anniespratt?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

在 Datacraft，我们最近有机会研究一个监督机器学习问题，其中目标变量是实数和多值的。对机器学习库的初步搜索显示，流行的开源 ML 库对此几乎没有支持。

这里有一个主题的快速介绍，以及(在后面的文章中)对 JVM 生态系统中的一些库的探究。

# 多目标回归

机器学习分类器通常支持单个目标变量。在回归模型中，目标是实值，而在分类模型中，目标是二元或多值的。

对于分类模型，一个有多个目标变量的问题叫做*多标签分类*。在回归模型领域，作为初学者，我发现术语有点混乱。

*   *简单回归*模型是一种试图用单一解释变量/独立变量拟合线性回归模型的模型。
*   *多元回归*模型是一种试图根据两个或更多自变量的值来预测因变量的模型。例如:可以根据吸烟持续时间、开始吸烟的年龄、收入、性别等来预测每天的香烟消费量吗？
*   *多目标回归*是有多个因变量时使用的术语。如果目标变量是分类的，那么它被称为多标签或多目标分类，如果目标变量是数值的，那么多目标(或多输出)回归是常用的名称。

# 几种 MTR 回归模型方法的探讨

[本文](http://cig.fi.upm.es/articles/2015/Borchani-2015-WDMKD.pdf)对多目标回归的模型方法进行了很好的概述。

它将方法分为以下几类:

问题转化:

*   诸如单目标回归、回归变量链

算法转换:

*   多输出支持向量回归
*   多输出回归树和规则方法

# 使用聚类和决策树的多目标回归。

在接下来的讨论中，我们将集中讨论一种方法，即用于 MTR 的决策树和决策树集合。

# 使用树进行聚类，也称为预测聚类树(PCT)

我们需要首先看一下预测聚类树(PCT ),它是 MTR 决策树的基础。

[聚类树的自顶向下归纳](http://arxiv.org/pdf/cs/0011032.pdf)是讨论使用决策树进行聚类的论文。在分类角色中，决策树的叶子是类或标签。

然而，从集群的角度来看

*   每个叶节点对应于一个概念或一个集群，
*   树中的节点表示分类法或层次结构。

该文件指出:

`A clustering tree is a decision tree where leaves don't contain classes and where each node & leaf correspond to a cluster.`

沿着树向下走(从根开始，然后向下到叶节点)可以被比作从大的集群(在顶部)移动到越来越小的集群，当我们接近叶节点时。

# 聚类之间距离的度量

在监督分类中，距离度量是目标变量的类。

在无监督或半监督学习中，聚类是基于距离度量(例如欧几里得距离)来完成的。如果小样本被标记，则节点中的所有(未标记的)样本被分配大多数标记样本的类别。聚类可用于预测许多或所有目标属性。

对于一个有 5 个目标变量的问题，叶节点将包含一个长度为 5 的向量，每个向量代表一个目标变量。

# 衡量 PCT 的质量

对于回归，使用的默认度量是相对误差，即

*   预测的均方误差除以始终预测平均值的默认假设的均方误差。

# 如何建造这棵树

为了构建一棵树，我们首先决定:

*   距离度量(比较两个实例之间的距离)
*   一个原型函数(它计算一个聚类的质心，以便可以比较两个聚类)

给定树的每个节点对应于一个聚类，然后决策树算法适于在每个节点中选择将最大化其子节点中的结果聚类之间的距离的测试。

*   给定聚类 C 和测试 T，最佳测试 T 是使子聚类 C1 和 C2 之间的距离最大化的测试。
*   如果原型(类似于聚类质心)是平均值，最大化聚类间距离对应于最小聚类内距离。这意味着树构建彼此远离的簇，并且簇内几乎没有分散。
*   停止标准:当子树明显不同时，树的生长(或者更确切地说，节点分裂)停止。对于回归，使用 f 检验(测量两个群体之间的方差差异)。
*   修剪:树从训练数据中生长，并且使用验证集来修剪树。

`For each node of the tree the quality of the tree if it were pruned at that node Q′ is compared with the quality Q of the unpruned tree. If Q′ > Q then the tree is pruned.`

# 用于 MTR 的决策树集成方法的比较

在上一节中，我们学习了如何构建一个决策树。然而，当使用树的森林时，性能会大大提高。

[本文](https://pdfs.semanticscholar.org/ca94/c65320c6023bf00ef6db30e6815f3aa07aa4.pdf)讨论了集成在多目标决策树中的应用。

# 装袋:

*   从数据集中选取引导样本，并使用这些样本构建一个树。引导样本是通过对训练集进行替换采样而获得的。

# 随机森林

*   除了 bagging 之外，在决策树的每个节点上，使用随机特征子集来分割节点，而不是 Bagging 中的所有特征。

论文的结果，简而言之

*   (MODTs 的)集成比单个决策树表现更好。
*   (MODTs 的)随机森林集成比 Bagging 集成表现更好。

# 为什么要用多输出模型，而不是回归这样的单输出模型组合？

*   使用单一输出的模型需要更长的训练时间，并且计算量很大
*   它们针对单个目标而不是所有目标一起进行优化
*   它们不使用目标变量(Y)之间的关系
*   MTR 回归模型比一堆单一目标模型更简单
*   MTR 树提供了人类可读的预测模型，这些模型易于解释。

# 中期审查可用的工具

我发现 [Clus 工具包](http://clus.sourceforge.net/)很好地融合了高性能和健壮性

*   文档是优秀的
*   该工具包有多种用于多目标分类和回归的方法
*   它还支持基于规则归纳和聚类。
*   我使用的集合模型(Bagging，RandomForest)易于阅读和解释。

一些更新的方法(2012 年后)已经作为木兰工具包的扩展实现了，可以在这个 [Github 链接](https://github.com/lefman/mulan-extended)获得。虽然这些方法如随机线性目标组合比集合模型报告了更好的性能，但我发现该工具包不如 Clus 工具包成熟。

这里有一个[关于相同主题的 StackExchange](https://stats.stackexchange.com/questions/176515/resources-for-learning-about-multiple-target-techniques) 的讨论。

在后面的文章中，我们将讨论如何使用 Clus 工具包来运行 MTR 模型。