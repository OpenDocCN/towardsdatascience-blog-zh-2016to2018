<html>
<head>
<title>Speed Up Your Algorithms Part 3 — Parallel-ization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">加速您的算法第 3 部分—并行化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speed-up-your-algorithms-part-3-parallelization-4d95c0888748?source=collection_archive---------5-----------------------#2018-11-27">https://towardsdatascience.com/speed-up-your-algorithms-part-3-parallelization-4d95c0888748?source=collection_archive---------5-----------------------#2018-11-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3cb7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Python 并行编程简介</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a2310bfbe8f026398bfbeba44dc96ab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MaLFPqNuX4pWF3ES"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@drmakete?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">drmakete lab</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="c0a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我写的系列文章中的第三篇。所有帖子都在这里:</p><ol class=""><li id="20d0" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/speed-up-your-algorithms-part-1-pytorch-56d8a4ae7051">加速您的算法第 1 部分— PyTorch </a></li><li id="b935" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/speed-up-your-algorithms-part-2-numba-293e554c5cc1">加速你的算法第二部分——Numba</a></li><li id="3ca5" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/speed-up-your-algorithms-part-3-parallelization-4d95c0888748">加速您的算法第三部分——并行化</a></li><li id="0afc" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">加速你的算法第 4 部分— Dask </li></ol><p id="083c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而这些与<strong class="ky ir"> <em class="mg">相配套的 Jupyter 笔记本</em> </strong>可在此处获得:</p><p id="c6ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[<a class="ae kv" href="https://github.com/PuneetGrov3r/MediumPosts/tree/master/SpeedUpYourAlgorithms" rel="noopener ugc nofollow" target="_blank">Github-speedupyourlightms</a>和<strong class="ky ir">[</strong><a class="ae kv" href="https://www.kaggle.com/puneetgrover/kernels" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">ka ggle</strong></a><strong class="ky ir">]</strong></p><h1 id="5133" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">索引</h1><ol class=""><li id="8ba8" class="ls lt iq ky b kz mz lc na lf nb lj nc ln nd lr lx ly lz ma bi translated"><a class="ae kv" href="#9ad0" rel="noopener ugc nofollow">简介</a></li><li id="0819" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="#7e6e" rel="noopener ugc nofollow">池和过程</a></li><li id="4632" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="#43ca" rel="noopener ugc nofollow">穿线</a></li><li id="e906" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="#be41" rel="noopener ugc nofollow">达斯克</a></li><li id="1542" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="#aec3" rel="noopener ugc nofollow"> torch.multiprocessing </a></li><li id="76c6" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="#fea9" rel="noopener ugc nofollow">延伸阅读</a></li><li id="c4d7" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="#0b7f" rel="noopener ugc nofollow">参考文献</a></li></ol><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="04cc" class="nj mi iq nf b gy nk nl l nm nn"><strong class="nf ir"><em class="mg">NOTE:<br/></em></strong>This post goes with <strong class="nf ir"><em class="mg">Jupyter Notebook</em></strong> available in my Repo on <strong class="nf ir">Github</strong>:[<a class="ae kv" href="https://nbviewer.jupyter.org/github/PuneetGrov3r/MediumPosts/blob/master/SpeedUpYourAlgorithms/3%29%20Prallelization.ipynb" rel="noopener ugc nofollow" target="_blank">SpeedUpYourAlgorithms-Parallelization</a>]<br/>and on <strong class="nf ir">Kaggle</strong>:<br/>[<a class="ae kv" href="https://www.kaggle.com/puneetgrover/speed-up-your-algorithms-prallelization" rel="noopener ugc nofollow" target="_blank">SpeedUpYourAlgorithms-Parallelization</a>]</span></pre><h1 id="9ad0" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">1.简介<a class="ae kv" href="#5133" rel="noopener ugc nofollow"> ^ </a></h1><p id="d92a" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">随着时间的推移，数据呈指数级增长，而处理器计算能力的增长却停滞不前，我们需要找到高效处理数据的方法。我们做什么呢</p><p id="0548" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">GPU 是一种解决方案，而且非常有效。但是，GPU 不是为了机器学习的目的而制造的，它们是专门为复杂的图像处理和游戏而制造的。我们让我们的算法在现有的 GPU 上工作，它实际上得到了回报。现在，谷歌推出了一款名为 TPU(张量处理单元)的新设备，它是为 TensorFlow 上的机器学习工作负载量身定制的，结果看起来很有希望。英伟达也没有退缩。<a class="ae kv" href="#f403" rel="noopener ugc nofollow"> </a></p><p id="4367" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是我们会在未来的某个时候碰到天花板。即使我们采用当今可用的任何巨大数据集，单个机器或计算单元也不足以处理这样的负载。我们将不得不使用多台机器来完成任务。我们将不得不<strong class="ky ir">并行化</strong>我们的任务。</p><p id="1b2a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本帖中，我们将探讨一些你在 Python 中大部分时间会用到的方法。然后稍微介绍一下<strong class="ky ir"> Dask </strong>和<strong class="ky ir"> torch.multiprocessing </strong>。</p><h1 id="7e6e" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">2.池和进程<a class="ae kv" href="#5133" rel="noopener ugc nofollow"> ^ </a></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/b5ca8995c5495856452cf12d6859c2cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*n8MCV9Z5HThV9DZg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@5tep5?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Alexander Popov</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="f7d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Python 的<code class="fe ns nt nu nf b">multiprocessing</code>库的<code class="fe ns nt nu nf b">Pool</code>和<code class="fe ns nt nu nf b">Process</code>方法都为我们的任务启动了一个新的进程，但是方式不同。<code class="fe ns nt nu nf b">Process</code>每次呼叫只进行一个过程:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="b2ce" class="nj mi iq nf b gy nk nl l nm nn">import multiprocessing as mp<br/>p = mp.Process(target= ##target-function,<br/>               args=   ##args-to-func)<br/># This call will make only one process, which will process<br/># target-function with given arguments in background.</span></pre><p id="0480" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是这个过程还没有开始。要启动它，您必须:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="a513" class="nj mi iq nf b gy nk nl l nm nn">p.start()</span></pre><p id="d849" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，您可以将它留在这里，或者通过以下方式检查流程是否完成:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="6514" class="nj mi iq nf b gy nk nl l nm nn">p.join()<br/># Now it will wait for process to complete.</span></pre><p id="6cd1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不检查进程是否已经完成有许多用途。例如，在客户端-服务器应用程序中，数据包丢失或无响应进程的概率非常低，我们可以忽略它，这可以为我们带来可观的加速。[取决于应用程序的流程]</p><p id="4c92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于多个进程，你将不得不制作多个<code class="fe ns nt nu nf b">Process</code>。你可以随意制作多个。当您调用它们上的<code class="fe ns nt nu nf b">.start()</code>时，它们都将启动。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="a7e7" class="nj mi iq nf b gy nk nl l nm nn">processes =[mp.Process(target=func, args=(a, b)) for (a, b) in list]</span><span id="2c52" class="nj mi iq nf b gy nv nl l nm nn">for p in processes: p.start()<br/>for p in processes: p.join()</span></pre><p id="86c7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面,<code class="fe ns nt nu nf b">Pool</code>启动固定数量的进程，然后我们可以给这些进程分配一些任务。因此，在特定的时刻，只有固定数量的进程在运行，其余的都在等待。进程的数量通常被选择为设备的内核数量，如果将该参数留空，这也是默认行为。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="03bc" class="nj mi iq nf b gy nk nl l nm nn">pool = mp.Pool(processes=2)</span></pre><p id="ee82" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在有很多方法可以让你使用这个<code class="fe ns nt nu nf b">Pool</code>。在数据科学中，我们可以不用担心的是<code class="fe ns nt nu nf b">Pool.apply</code>和<code class="fe ns nt nu nf b">Pool.map</code>，因为它们在任务完成后立即返回结果。<code class="fe ns nt nu nf b">Pool.apply</code>只接受一个参数，只使用一个进程，而<code class="fe ns nt nu nf b">Pool.map</code>接受许多参数，并将它们放到我们的<code class="fe ns nt nu nf b">Pool</code>进程中。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="930c" class="nj mi iq nf b gy nk nl l nm nn">results = [pool.apply(func, (x)) for x in X]<br/># Or <br/>results = pool.map(func, (arg)) <strong class="nf ir">#</strong> Takes only one argument</span></pre><p id="5a29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是<code class="fe ns nt nu nf b">Pool.map</code>只接受一个参数(iterable ),它将这个参数分成若干块。要发送许多参数，你可以像<a class="ae kv" href="https://stackoverflow.com/questions/5442910/python-multiprocessing-pool-map-for-multiple-arguments" rel="noopener ugc nofollow" target="_blank"> this </a>这样做。</p><p id="bb06" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑我们前面的客户机-服务器应用程序的例子，这里要运行的最大进程数是预定义的，所以如果我们有很多请求/包，一次只有其中的<code class="fe ns nt nu nf b">n</code>(池中的最大进程数)会运行，而其他的会在队列中的进程槽中等待轮到它。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/0bda1035746a9eb6fac75c778ad4808e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*sL2K_aARQ6-EJ1WiWLVL9w.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Squaring of all elements of a vector</figcaption></figure><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="0f61" class="nj mi iq nf b gy nk nl l nm nn"><strong class="nf ir"># How can we use it with Data Frame?<br/># A: </strong>You can use some parallelizable function</span><span id="98fa" class="nj mi iq nf b gy nv nl l nm nn">df.shape<br/># (100, 100)<br/>dfs = [df.iloc[i*25:i*25+25, 0] for i in range(4)]</span><span id="65e9" class="nj mi iq nf b gy nv nl l nm nn">with Pool(4) as p:<br/>    res = p.map(np.exp, dfs)<br/>for i in range(4): df.iloc[i*25:i*25+25, 0] = res[i]</span><span id="af18" class="nj mi iq nf b gy nv nl l nm nn"><strong class="nf ir">#</strong> It can come in handy for preprocessing of data.</span></pre><p id="3df8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">用什么，什么时候用？</strong> <a class="ae kv" href="#d923" rel="noopener ugc nofollow"> <strong class="ky ir"> </strong> </a></p><p id="69aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你有很多任务，但其中没有多少是计算密集型的，你应该使用<code class="fe ns nt nu nf b">Process</code>。因为如果它们是计算密集型的，它们可能会阻塞您的 CPU，您的系统可能会崩溃。如果您的系统可以一次性处理它们，它们就不必排队等待机会。</p><p id="8422" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当你有固定数量的任务并且它们是计算密集型的，你应该使用一个<code class="fe ns nt nu nf b">Pool</code>。因为如果你一下子放开它们，你的系统可能会崩溃。</p><h1 id="43ca" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">3.穿线<a class="ae kv" href="#5133" rel="noopener ugc nofollow"> ^ </a></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/26d64d434027f65255b345f260cf55dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ioftZ-8wEaDzxmYJ"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@helloimnik?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Hello I'm Nik</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ee56" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">穿线！用 python？</strong></p><p id="cb5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">python 中的线程名声不好。人们是正确的。实际上，线程在大多数情况下并没有正常工作。那么问题是什么呢？</p><p id="b944" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">问题是<strong class="ky ir"> GIL(全局解释器锁)</strong>。GIL 是在 Python 开发的早期引入的，当时操作系统中甚至没有线程的概念。选择它是因为它简单。</p><p id="ee16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">GIL 一次只允许一个 CPU 绑定的进程。也就是说，它一次只允许一个线程访问 python 解释器。因此，线程<code class="fe ns nt nu nf b">Lock</code>是整个解释器，直到它完成。<a class="ae kv" href="#76b1" rel="noopener ugc nofollow"> </a></p><p id="2afc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于单线程程序来说，它很快，因为只有一个<code class="fe ns nt nu nf b">Lock</code>需要维护。随着 python 的流行，很难有效地消除 GIL 而不损害所有相关的应用程序。这就是它还在的原因。</p><p id="3cd0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">但是</strong>，如果你的任务不受 CPU 限制，你仍然可以使用多线程并行(y)。也就是说，如果你的任务是 I/O 受限的，你可以使用多线程并获得加速。因为大部分时间这些任务都在等待其他代理(如磁盘等)的响应。)在此期间，他们可以释放锁，让其他任务同时获取它。<a class="ae kv" href="#5984" rel="noopener ugc nofollow"> ⁴ </a></p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="bd06" class="nj mi iq nf b gy nk nl l nm nn"><strong class="nf ir">NOTE: (From official page </strong><a class="ae kv" href="https://wiki.python.org/moin/GlobalInterpreterLock" rel="noopener ugc nofollow" target="_blank"><strong class="nf ir">here</strong></a><strong class="nf ir">)<br/></strong>The GIL is controversial because it prevents multithreaded CPython programs from taking full advantage of multiprocessor systems in certain situations. Note that potentially blocking or long-running operations, such as <strong class="nf ir">I/O</strong>, <strong class="nf ir">image processing</strong>, and <a class="ae kv" href="https://wiki.python.org/moin/NumPy" rel="noopener ugc nofollow" target="_blank"><strong class="nf ir">NumPy</strong></a> <strong class="nf ir">number crunching</strong>, happen <strong class="nf ir"><em class="mg">outside</em></strong> the GIL. Therefore it is only in multithreaded programs that spend a lot of time inside the GIL, interpreting CPython bytecode, that the GIL becomes a bottleneck.</span></pre><p id="ab0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，如果你的任务是 IO 绑定的，比如从服务器下载一些数据，读/写磁盘等等。，您可以使用多线程并获得加速。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="76eb" class="nj mi iq nf b gy nk nl l nm nn">from threading import Thread as t<br/>import queue<br/>q = queue.Queue()  # For putting and getting results of thread</span><span id="a85d" class="nj mi iq nf b gy nv nl l nm nn">func_ = lambda q, args: q.put(func(args))</span><span id="af5c" class="nj mi iq nf b gy nv nl l nm nn">threads = [t(target=func_, args=(q, args)) for args in args_array]<br/>for t in threads: t.start()<br/>for t in threads: t.join()<br/>res = []<br/>for t in threads: res.append(q.get()) <br/><strong class="nf ir">#</strong> These results won't necessarily be in order</span></pre><p id="b1bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了保存线程的结果，你可以使用类似<code class="fe ns nt nu nf b">Queue</code>的东西。为此，你必须像上面那样定义你的函数，或者你可以在你的函数中使用<code class="fe ns nt nu nf b">Queue.put()</code>，但是你必须改变你的函数定义，以包含<code class="fe ns nt nu nf b">Queue</code>作为参数。<a class="ae kv" href="#fc5f" rel="noopener ugc nofollow"> ⁶ </a></p><p id="9fa6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，你在队列中的结果不一定是有序的。如果你希望你的结果是有序的，你可以传入一些计数器作为参数，作为 id，然后使用这些 id 来识别结果来自哪里。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="2db2" class="nj mi iq nf b gy nk nl l nm nn">threads = [t(func_, args = (i, q, args)) for i, args in <br/>                                         enumerate(args_array)]<br/># And update function accordingly</span><span id="4edc" class="nj mi iq nf b gy nv nl l nm nn"><strong class="nf ir"><em class="mg">NOTE:<br/></em></strong>Multiprocessing with Pandas 'read.csv' method doesn't give much speedup for some reason. As an alternative you can use <a class="ae kv" rel="noopener" target="_blank" href="/speeding-up-your-algorithms-part-4-dask-7c6ed79994ef"><strong class="nf ir">Dask</strong></a>.</span></pre><p id="1cd3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">线程 vs 进程？</strong> <a class="ae kv" href="#9540" rel="noopener ugc nofollow"> <strong class="ky ir"> ⁷ </strong> </a></p><p id="1973" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个进程是重量级的，因为它可能包含许多自己的线程(至少包含一个)，并且它有自己分配的内存空间，而线程是重量级的，因为它在父进程的内存区域工作，因此执行速度更快。</p><p id="29b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">进程内线程之间的通信更容易，因为它们共享相同的内存空间。，而进程间的通信(<strong class="ky ir">IPC</strong>-进程间通信)较慢。不过话说回来，共享相同数据的线程可能会进入<a class="ae kv" href="https://en.wikipedia.org/wiki/Race_condition#Software" rel="noopener ugc nofollow" target="_blank">竞争状态</a>，应该使用<code class="fe ns nt nu nf b">Locks</code>或类似的解决方案来处理。</p><h1 id="be41" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">4.达斯克<a class="ae kv" href="#5133" rel="noopener ugc nofollow"> ^ </a></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/0d7e6838c286a150730199d686a5860f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QMy3Sud7sTjSyPQy"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@trevcole?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Trevor Cole</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="fba9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ns nt nu nf b">Dask</code>是一个并行计算库，它不仅帮助并行化现有的机器学习工具(<code class="fe ns nt nu nf b">Pandas</code>和<code class="fe ns nt nu nf b">Numpy</code> )[ <strong class="ky ir">，即使用高级集合</strong> ]，还帮助并行化低级任务/功能，并可以通过制作任务图来处理这些功能之间的复杂交互。[ <strong class="ky ir">即使用低级调度程序</strong> ]这类似于 Python 的线程或多处理模块。</p><p id="abcd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">他们还有一个独立的机器学习库<code class="fe ns nt nu nf b">dask-ml</code>，它与现有的库如<code class="fe ns nt nu nf b">sklearn</code>、<code class="fe ns nt nu nf b">xgboost</code>和<code class="fe ns nt nu nf b">tensorflow</code>集成在一起。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="cd01" class="nj mi iq nf b gy nk nl l nm nn">from dask import delayed as delay</span><span id="3da0" class="nj mi iq nf b gy nv nl l nm nn">@delay<br/>def add(x, y):<br/>    return x+y<br/>@delay<br/>def sq(x):<br/>    return x**2</span><span id="dbb8" class="nj mi iq nf b gy nv nl l nm nn"># Now you can use these functions any way you want, Dask will <br/># parallelize your execution. And as the name suggest Dask <br/># will not execute your function callings right away, rather<br/># it will make a computational graph depending on the way you are<br/># calling functions on inputs and intermediate results. To compute<br/># final result:<br/>result.compute()</span></pre><p id="4345" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Dask 做任何事情都有一种天生的并行性。对于它如何处理数据帧，您可以将它看作是一种分而治之的方法，它将您的数据帧分成块，然后并行应用您给定的函数。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="bf0e" class="nj mi iq nf b gy nk nl l nm nn">df = dask.DataFrame.read_csv("BigFile.csv", chunks=50000)<br/># Your DataFrame has been divided into chunks and every function<br/># you apply will be applied to all chunks separately and in <br/># parallel.</span><span id="5660" class="nj mi iq nf b gy nv nl l nm nn"># It has most of Pandas functions which you can use:<br/>agg = df.groupby(["column"]).aggregate(["sum", "mean"])<br/>agg.columns = new_column_names</span><span id="cbb8" class="nj mi iq nf b gy nv nl l nm nn">df_new = df.merge(agg.reset_index(), on="column", how="left")<br/># It have not compute result up until now,</span><span id="6c3b" class="nj mi iq nf b gy nv nl l nm nn"># but with .compute() it will compute now in parallel.<br/>df_new.compute().head()</span></pre><p id="66cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它们也有在机器集群上运行它们接口。</p><p id="8820" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于<code class="fe ns nt nu nf b">Dask</code>的完整介绍，请看我的帖子<a class="ae kv" rel="noopener" target="_blank" href="/speeding-up-your-algorithms-part-4-dask-7c6ed79994ef">这里</a>。</p><h1 id="aec3" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">5.火炬.多重处理<a class="ae kv" href="#5133" rel="noopener ugc nofollow"> ^ </a></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/92903b1bb39a0a0af12158958a86743b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hvIAUhW7gtdomsQk"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@mjhphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Matthew Hicks</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="3065" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ns nt nu nf b">torch.multiprocessing</code>是 Python <code class="fe ns nt nu nf b">multiprocessing</code>模块的包装器，其 API 与原始模块 100%兼容。所以可以用<code class="fe ns nt nu nf b">Queue</code>的、<code class="fe ns nt nu nf b">Pipe</code>的、<code class="fe ns nt nu nf b">Array</code>的等等。这些都在 Python 的多重处理模块中。除此之外，为了使它更快，他们增加了一个方法<code class="fe ns nt nu nf b">share_memory_()</code>，它允许数据进入任何进程都可以直接使用它的状态，因此将该数据作为参数传递给不同的进程不会复制该数据。</p><p id="54f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以分享<code class="fe ns nt nu nf b">Tensors</code>，model 的<code class="fe ns nt nu nf b">parameters</code>，你可以随心所欲的在 CPU 或者 GPU 上分享。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="451c" class="nj mi iq nf b gy nk nl l nm nn"><strong class="nf ir">Warning from Pytorch: (Regarding sharing on GPU)<br/></strong>  CUDA API requires that the allocation exported to other processes remains valid as long as it’s used by them. You should be careful and ensure that CUDA tensors you shared don’t go out of scope as long as it’s necessary. This shouldn’t be a problem for sharing model parameters, but passing other kinds of data should be done with care. Note that this restriction doesn’t apply to shared CPU memory.</span></pre><p id="eea5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在这里的“池和进程”部分使用上面的方法，为了获得更快的速度，你可以使用<code class="fe ns nt nu nf b">share_memory_()</code>方法在所有进程之间共享一个<code class="fe ns nt nu nf b">Tensor</code>(比方说)而不被复制。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="4629" class="nj mi iq nf b gy nk nl l nm nn"><strong class="nf ir"># Training a model using multiple processes:</strong></span><span id="95d4" class="nj mi iq nf b gy nv nl l nm nn">import torch.multiprocessing as mp<br/>def train(model):<br/>    for data, labels in data_loader:<br/>        optimizer.zero_grad()<br/>        loss_fn(model(data), labels).backward()<br/>        optimizer.step()  <strong class="nf ir"><em class="mg">#</em></strong><em class="mg"> This will update the shared parameters</em></span><span id="b17a" class="nj mi iq nf b gy nv nl l nm nn">model = nn.Sequential(nn.Linear(n_in, n_h1),<br/>                      nn.ReLU(),<br/>                      nn.Linear(n_h1, n_out))</span><span id="b49c" class="nj mi iq nf b gy nv nl l nm nn">model.share_memory() <strong class="nf ir">#</strong> Required for 'fork' method to work</span><span id="c412" class="nj mi iq nf b gy nv nl l nm nn">processes = []<br/>for i in range(4): # No. of processes<br/>    p = mp.Process(target=train, args=(model,))<br/>    p.start()<br/>    processes.append(p)</span><span id="3614" class="nj mi iq nf b gy nv nl l nm nn">for p in processes: p.join()</span></pre><p id="b007" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您也可以使用一组机器。更多信息请参见<a class="ae kv" href="https://pytorch.org/docs/stable/distributed.html" rel="noopener ugc nofollow" target="_blank">此处</a>。</p></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="9da3" class="nj mi iq nf b gy nk nl l nm nn"><strong class="nf ir">NOTE:<br/></strong>For little introduction (kind of) on usage of <strong class="nf ir">Pycuda</strong>, see Jupyter Notebook's PyCuda section <a class="ae kv" href="https://nbviewer.jupyter.org/github/PuneetGrov3r/MediumPosts/blob/master/SpeedUpYourAlgorithms/3%29%20Prallelization.ipynb#5.-Pycuda-(Optional)" rel="noopener ugc nofollow" target="_blank">here</a>.</span></pre></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><h1 id="fea9" class="mh mi iq bd mj mk oh mm mn mo oi mq mr jw oj jx mt jz ok ka mv kc ol kd mx my bi translated">6.延伸阅读<a class="ae kv" href="#5133" rel="noopener ugc nofollow"> ^ </a></h1><ol class=""><li id="f403" class="ls lt iq ky b kz mz lc na lf nb lj nc ln nd lr lx ly lz ma bi translated"><a class="ae kv" href="https://blog.riseml.com/comparing-google-tpuv2-against-nvidia-v100-on-resnet-50-c2bbb6a51e5e" rel="noopener ugc nofollow" target="_blank">https://blog . rise ml . com/comparising-Google-TPU v2-对抗-NVIDIA-v100-on-resnet-50-C2 BBB 6 a 51 e 5 e</a></li><li id="35af" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://medium.com/syncedreview/googles-tpu-chip-goes-public-in-challenge-to-nvidia-s-gpu-78ced56776b5" rel="noopener">https://medium . com/synced review/Google s-TPU-chip-goes-public-in-challenge-to-NVIDIA-s-GPU-78 ced 56776 b5</a></li><li id="bab9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://sebastianraschka.com/Articles/2014_multiprocessing.html" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka . com/Articles/2014 _ multi processing . html</a></li><li id="80d1" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/how-i-learned-to-love-parallelized-applies-with-python-pandas-dask-and-numba-f06b0b367138">https://towards data science . com/how-I-learn-to-love-parallelised-apply-with-python-pandas k-and-numba-f 06 b0b 367138</a></li><li id="7b5d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://www.geeksforgeeks.org/multiprocessing-python-set-2/" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/multiprocessing-python-set-2/</a></li><li id="aa93" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://www.geeksforgeeks.org/multithreading-in-python-set-2-synchronization/" rel="noopener ugc nofollow" target="_blank">https://www . geeksforgeeks . org/multi threading-in-python-set-2-synchron ization/</a></li><li id="f43d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://medium.com/idealo-tech-blog/parallelisation-in-python-an-alternative-approach-b2749b49a1e" rel="noopener">https://medium . com/idealo-tech-blog/parallelisation-in-python-an-alternative-approach-b 2749 b 49 a 1 e</a></li><li id="b3c0" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/questions/990102/python-global-interpreter-lock-gil-workaround-on-multi-core-systems-using-task" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/990102/python-global-interpreter-lock-Gil-workrance-on-multi-core-systems-using-task</a></li><li id="68c6" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackoverflow.com/questions/38666078/fast-queue-of-read-only-numpy-arrays" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/38666078/fast-queue-of-read-only-numpy-arrays</a></li><li id="a4bb" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://medium.com/@rvprasad/data-and-chunk-sizes-matter-when-using-multiprocessing-pool-map-in-python-5023c96875ef" rel="noopener">https://medium . com/@ rvprasad/data-and-chunk-size-matter-when-using-multi processing-pool-map-in-python-5023 c 96875 ef</a></li><li id="dcfc" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://stackabuse.com/parallel-processing-in-python/" rel="noopener ugc nofollow" target="_blank">https://stackabuse.com/parallel-processing-in-python/</a></li></ol><h1 id="0b7f" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">7.参考文献<a class="ae kv" href="#5133" rel="noopener ugc nofollow"> ^ </a></h1><p id="c0a5" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated"><strong class="ky ir"> a)池和过程:</strong></p><ol class=""><li id="0fde" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" href="https://docs.python.org/3/library/multiprocessing.html" rel="noopener ugc nofollow" target="_blank">https://docs.python.org/3/library/multiprocessing.html</a></li><li id="d923" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://www.ellicium.com/python-multiprocessing-pool-process/" rel="noopener ugc nofollow" target="_blank">https://www . elli cium . com/python-multi processing-pool-process/</a></li></ol><p id="17f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> b)穿线:</strong></p><p id="76b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.<a class="ae kv" href="https://realpython.com/python-gil/" rel="noopener ugc nofollow" target="_blank">https://realpython.com/python-gil/</a></p><p id="5984" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.<a class="ae kv" href="https://stackoverflow.com/questions/29270818/why-is-a-python-i-o-bound-task-not-blocked-by-the-gil" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/29270818/why-a-python-I-o-bound-task-not-blocked-by-the-Gil</a></p><p id="9c08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.<a class="ae kv" href="https://stackoverflow.com/questions/27455155/python-multiprocessing-combined-with-multithreading" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/27455155/python-multi processing-combined-with-threading</a></p><p id="fc5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">6.<a class="ae kv" href="https://stackoverflow.com/questions/6893968/how-to-get-the-return-value-from-a-thread-in-python" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/6893968/how-to-get-the-return-value-from-a-thread-in-python</a></p><p id="9540" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">7.<a class="ae kv" href="https://stackoverflow.com/questions/200469/what-is-the-difference-between-a-process-and-a-thread" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/200469/a-process-and-a-thread 的区别是什么</a></p><p id="7a03" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> c)达斯克</strong></p><p id="6b11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">8.【https://ml.dask.org T4】</p><p id="65c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">9.<a class="ae kv" href="https://docs.dask.org/en/latest/" rel="noopener ugc nofollow" target="_blank">https://docs.dask.org/en/latest/</a></p><p id="031f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> d)火炬.多重处理:</strong></p><p id="09a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">9.<a class="ae kv" href="https://pytorch.org/docs/stable/multiprocessing.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/multiprocessing.html</a></p><p id="17d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">10.<a class="ae kv" href="https://pytorch.org/docs/stable/notes/multiprocessing.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/notes/multiprocessing.html</a></p><p id="d1fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> d) Pycuda: </strong></p><p id="6ec8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">11.<a class="ae kv" href="https://documen.tician.de/pycuda/tutorial.html" rel="noopener ugc nofollow" target="_blank">https://documen.tician.de/pycuda/tutorial.html</a></p><p id="00e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">12.<a class="ae kv" href="https://github.com/inducer/pycuda/tree/master/examples" rel="noopener ugc nofollow" target="_blank">https://github.com/inducer/pycuda/tree/master/examples</a></p><p id="5f5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">13.<a class="ae kv" href="https://www3.nd.edu/~zxu2/acms60212-40212-S12/Lec-12-02.pdf" rel="noopener ugc nofollow" target="_blank">https://www3.nd.edu/~zxu2/acms60212-40212-S12/Lec-12-02.pdf</a></p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="dd8b" class="nj mi iq nf b gy nk nl l nm nn">Suggestions and reviews are welcome.<br/>Thank you for reading!</span></pre><p id="813b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">签名:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/ca01c1d315400c09978fb5e62da01d87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*N7tbEUmEr0wEqsdlZNQ5iA.png"/></div></div></figure></div></div>    
</body>
</html>