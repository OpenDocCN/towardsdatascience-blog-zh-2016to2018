<html>
<head>
<title>Neural Networks: Forming Analogies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络:形成类比</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-forming-analogies-587557c3b26e?source=collection_archive---------17-----------------------#2018-08-24">https://towardsdatascience.com/neural-networks-forming-analogies-587557c3b26e?source=collection_archive---------17-----------------------#2018-08-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="4085" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的大脑具有人工神经网络仍然缺乏的能力:我们可以形成类比，将不同的输入联系起来，并使用相同的启发式方法处理它们。官方的行话是“迁移学习”。从根本上说，类比是压缩的一种形式，允许大脑以最小的空间模拟许多不同系统的动态。当两个系统行为相似时，可以用一个简单的类比来描述它们。</p><p id="f0a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">学习新东西</strong></p><p id="075e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">人工神经网络现在可以避免<a class="ae kl" href="https://www.groundai.com/project/online-structured-laplace-approximations-for-overcoming-catastrophic-forgetting/" rel="noopener ugc nofollow" target="_blank">灾难性遗忘</a>，这是一个主要的绊脚石。以前，当一个神经网络被训练完成一项新任务时，要么是<em class="km">太有可塑性</em>，学习新任务的同时忘记了旧任务，要么是<em class="km">太死板</em>，记住了第一个任务，却从未学习第二个任务。这一进步是向迁移学习迈出的重要一步，然而它只是将<em class="km">新任务划分到</em>网络中不太有用的区域——它没有使用共享试探法将不同的任务组合起来。</p><p id="974f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我认为，为了形成类比并压缩用于许多类似任务的空间，神经网络必须将时间投入到一个<em class="km">独特的状态:假设类比</em>。当假设类比时，神经网络必须<strong class="jp ir">忽略新的感觉输入，同时比较来自不同专家的输出</strong>。这样的网络有三种状态:接收和处理输入，反向传播突触权重的更新，以及假设类比。网络在所有三种状态之间交替；正如它在前馈和反馈之间交替一样，如果它希望发现不同子系统之间的联系，它必须花时间“思考”。</p><p id="a8aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">为专家提供制图投入</strong></p><p id="2f6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了了解类比形成是如何工作的，想象一下所有可能输入的空间。在这个高维空间中，每个点对应不同的输入。混合专家神经网络的任务是将这些输入映射到各种专家聚类上。假设棋盘是输入的空间，每个专家占据棋盘上的一个样本，这样棋盘上的每个方格都与一个特定的样本相关联。神经网络接收输入，检查哪个专家在那里应用，并将输入数据发送给该专家进行处理。从输入到专家的映射是对这些输入的<strong class="jp ir">解析</strong>。</p><p id="92ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当网络假设一个类比时，它接受从输入空间到专家空间的新的映射为正确的。通常接收{X，Y，Z}作为输入的专家可能改为接收{A，B，C},以查看新的分配是否产生正确的预测。测试每个可能的重新分配的<em class="km">过程</em>是<em class="km">假设</em>状态；网络花在“思考”上的时间越多，就越有可能找到准确模拟不同子系统的现有专家群。当这种“思考”发生时，感觉输入被搁置，反向传播没有发生——网络“陷入思考”。</p><p id="374d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">增长理解力</strong></p><p id="ca47" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，一个不熟悉的输入被赋予它自己的专家聚类——它被视为全新的，处理起来效率低下且缓慢。然而，随着更多的输入出现在输入空间的该区域中，由于反向传播，其专家聚类变得更加准确。一旦新任务的专家足够准确，神经网络就可以开始思考——它<em class="km">模拟</em>输入，并查看其现有专家集群的答案是否与类似集群的答案匹配。如果它们足够相似，那么类比是有效的，并且笨拙的新专家群被更成熟的模块所取代。(注意，网络是<strong class="jp ir">而不是</strong>将类似的专家集群与<em class="km">现实</em>进行比较；它是相对于特定任务的专家来说的！比较的是<em class="km">想象的</em>。)</p><p id="0445" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种行为类似于我们在大脑中看到的——当学习一项新任务时，我们很笨拙，容易分心。然而，一旦我们消化了新的任务，我们就会变得流畅和敏感。这种消化的发生是因为对任务的反思，以及将该任务与更熟悉的任务联系起来的过程。我们必须<em class="km">思考</em>以找到有助于理解的联系和相似之处。寻找这些类比是一项非常有价值的工作，我们花了很多时间思考假设的类比，我预计高级人工神经网络也需要花很多时间思考。</p><p id="68fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">好奇心</strong></p><p id="53a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一组研究人员已经在雅达利游戏的人工智能方面取得了很大进展，完全依靠<a class="ae kl" href="https://arxiv.org/abs/1808.04355" rel="noopener ugc nofollow" target="_blank">好奇心</a>。这台机器不知道奖励和死亡；它只是试图用一种新的、意想不到的事情发生的方式来玩。这可能非常像新大脑皮层，它不断地试图预测未来，并非常注意其预测出错的时间和方式。新大脑皮层不太受不常见的愉悦信号的驱动。相反，大脑皮层渴望理解。</p><p id="fba4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，还有另一种形式的好奇心。</p><p id="57d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设你的神经网络形成了一个类比，X →A，Y →B，Z →C，它想看看这个类比是否成立。两个专家系统，一个在数据{A，B，C}上被明确训练，另一个在{X，Y，Z}上被训练并且被假定为类似的，具有可以被模拟以比较那些专家群的边缘情况。</p><p id="e4e6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">测试这些边缘情况是<em class="km">一种不同的好奇心</em>——神经网络不会搜索新的输入。相反，它怀疑通过系统地寻找矛盾来进行类比是否恰当。“边缘案例”测试在发现任务的潜在动力方面可能超过“新经验”的好奇心；新体验的好奇心会落入陷阱，就像不断地<a class="ae kl" href="https://arxiv.org/abs/1808.04355" rel="noopener ugc nofollow" target="_blank">换</a>一个电视频道。<em class="km">新体验的好奇心是一台分心机器</em>。相比之下，测试边缘案例更类似于科学家的工作，系统地假设，然后在这些假设的边界上进行实验。</p><p id="7ca3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">需要时间</strong></p><p id="0919" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，神经网络使其感觉静音，停止反向传播，并开始思考假设的类比。不幸的是，没有找到可靠类比的硬性快速规则。我们自己的大脑似乎使用稀疏分布表示来编码信息，如果两种表示在许多方面重叠，它们可能会受到相同的动力学影响。所以，我们有一个粗略的相似性度量，给我们一个好的开始。然而，思考仍然缓慢。</p><p id="b1d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于每一个假设的类比，必须想象许多边缘情况。每个实例都呈现给特定任务专家<em class="km">和类似专家集群</em>。<strong class="jp ir">如果许多想象的实例产生匹配的输出，那么类似的专家是一个很好的选择，并取代特定任务的专家</strong>。这可能很少见。因此，必须尝试许多假设，每个假设都需要多个想象的实例来验证。这可能是神经网络工作的很大一部分！我们自己对睡眠的需求，以及睡眠在消化新概念中的作用，似乎反映了这种行为。我们梦想在子系统之间形成新的连接，当我们的传感器静音，行动只是想象。</p></div></div>    
</body>
</html>