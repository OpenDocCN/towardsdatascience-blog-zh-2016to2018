<html>
<head>
<title>Review: DeepLabv1 &amp; DeepLabv2 — Atrous Convolution (Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习:DeepLabv1 和 DeepLabv2 —阿特鲁卷积(语义分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=collection_archive---------4-----------------------#2018-11-09">https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=collection_archive---------4-----------------------#2018-11-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="05fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>这个故事中，<strong class="jp ir"> DeepLabv1 </strong>和<strong class="jp ir"> DeepLabv2 </strong>被放在一起回顾，因为它们都使用了<strong class="jp ir">阿特鲁卷积</strong>和<strong class="jp ir">全连通条件随机场(CRF) </strong>，除了 DeepLabv2 多了一项技术叫做<strong class="jp ir"> Atous 空间金字塔池(ASPP) </strong>，这是与 DeepLabv1 的主要区别。(当然也有其他的区别，比如:DeepLabv2 用<a class="ae ku" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>和<a class="ae ku" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a>做实验，而 DeepLabv1 只用<a class="ae ku" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a>。)</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi kv"><img src="../Images/61eeea2215224f184e94a88331394b96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MVLmei6xOqScKjwffk4ZXg.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk"><strong class="bd ll">DeepLab Model</strong></figcaption></figure><p id="4e62" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上图是 DeepLab 模型架构。首先，输入图像通过使用 atrous 卷积和 ASPP 的网络。然后对网络输出进行双线性插值，并通过全连接 CRF 对结果进行微调，得到最终输出。</p><p id="5d6b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">DeepLabv1 和 DeepLabv2 已经发表在<strong class="jp ir"> 2015 ICLR 和 2018 TPAMI </strong>上，分别有大约<strong class="jp ir"> 400 次和 2000 次引用</strong>在我写这个故事的时候。(<a class="lm ln ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----b51c5fbde92d--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="76d3" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">涵盖哪些内容</h1><ol class=""><li id="1c4a" class="mt mu iq jp b jq mv ju mw jy mx kc my kg mz kk na nb nc nd bi translated"><strong class="jp ir">阿特鲁卷积</strong></li><li id="efde" class="mt mu iq jp b jq ne ju nf jy ng kc nh kg ni kk na nb nc nd bi translated"><strong class="jp ir">阿特鲁空间金字塔汇集(ASPP) </strong></li><li id="3199" class="mt mu iq jp b jq ne ju nf jy ng kc nh kg ni kk na nb nc nd bi translated"><strong class="jp ir">全连通条件随机场</strong></li><li id="fa86" class="mt mu iq jp b jq ne ju nf jy ng kc nh kg ni kk na nb nc nd bi translated"><strong class="jp ir">结果</strong></li></ol></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="bc4d" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated"><strong class="ak"> 1。阿特鲁卷积</strong></h1><p id="bc6e" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy nj ka kb kc nk ke kf kg nl ki kj kk ij bi translated">术语“<strong class="jp ir">阿特鲁</strong>”确实来源于法语“<strong class="jp ir"> à trous </strong>”意思是洞。因此，它也被称为“<strong class="jp ir">算法à trous </strong>”和“<strong class="jp ir">洞算法</strong>”。有些论文还称之为“<strong class="jp ir">膨胀卷积</strong>”。它通常用于小波变换，现在它被应用于深度学习的卷积中。</p><p id="5f1a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是 atrous 卷积的等式:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/8ba891463ef8cc25435c4adb68e54cea.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*3Vo4jPJdN6f9xWApzj8BNw.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk"><strong class="bd ll">1D Atrous Convolution (r&gt;1: atrous convolution, r=1: standard convolution)</strong></figcaption></figure><ul class=""><li id="65cc" class="mt mu iq jp b jq jr ju jv jy nn kc no kg np kk nq nb nc nd bi translated">当 r=1 时，就是我们通常使用的标准卷积。</li><li id="21a4" class="mt mu iq jp b jq ne ju nf jy ng kc nh kg ni kk nq nb nc nd bi translated"><strong class="jp ir">当 r &gt; 1 时，为 atrous 卷积，即卷积过程中对输入样本进行采样的步长。</strong></li></ul><p id="ba3f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下图说明了这个想法:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0163f2494c0c099ec6b6afca2f6dac1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*O5B0IRWewitfivGklGDJjA.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk"><strong class="bd ll">Standard Convolution (Top) Atrous Convolution (Bottom)</strong></figcaption></figure><p id="390a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">阿特鲁斯卷积的思想很简单。上图顶部，是标准卷积。</p><p id="b7a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在图的底部，是阿特鲁斯卷积。我们可以看到，当 rate = 2 时，输入信号是交替采样的。首先，pad=2 意味着我们在左右两边都填充 2 个零。然后，当 rate=2 时，我们每隔 2 个输入对输入信号进行采样以进行卷积。因此，在输出端，我们将有 5 个输出，<strong class="jp ir">使输出特征图更大。</strong></p><p id="1476" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们还记得<a class="ae ku" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>，一系列的卷积和池化使得输出的特征图非常小，需要 32 倍的上采样，这是一种激进的上采样。</p><p id="b669" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，atrous 卷积<strong class="jp ir">允许我们扩大过滤器的视野，以纳入更大的背景。</strong>因此，它提供了一种有效的机制来控制视野，并在精确定位(小视野)和上下文同化(大视野)之间找到最佳折衷。</p><p id="2ec7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 DeepLab 中，使用 VGG-16 或 ResNet-101，最后一个池(pool5)或卷积 conv5_1 的步幅分别设置为 1，以避免信号抽取过多。并且使用 rate = 2，使用 atrous 卷积来替换所有后续的卷积层。<strong class="jp ir">产量大很多</strong>。我们只需要<strong class="jp ir"> 8 倍上采样</strong>来对输出进行上采样。而<strong class="jp ir">双线性插值</strong>对于 8 倍上采样有相当不错的表现。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="c9c2" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated"><strong class="ak"> 2。阿特鲁空间金字塔池(ASPP) </strong></h1><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/32a28c22a4074678600c716145719060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*_8p_KTPr5N0HSeIKV35G_g.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk"><strong class="bd ll">Atrous Spatial Pyramid Pooling (ASPP)</strong></figcaption></figure><p id="a1a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ASPP 实际上是 SPP 的一个<strong class="jp ir">老版，其中的概念已经在<a class="ae ku" href="https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679" rel="noopener"> SPPNet </a>中使用。在 ASPP，<strong class="jp ir">不同速率的并行 atrous 卷积</strong>应用于输入特征图，并融合在一起。</strong></p><p id="e0fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于同一类别的对象在图像中可能具有不同的比例，<strong class="jp ir"> ASPP 有助于说明不同的对象比例</strong>，这可以提高准确性。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="24ca" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">3.<strong class="ak">全连通条件随机场</strong></h1><p id="12a7" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy nj ka kb kc nk ke kf kg nl ki kj kk ij bi translated">双线性插值后，全连接 CRF 应用于网络输出:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/fe85ba1fd66db6e03d8ca4ac64f7d90a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*r7dUU5Gb3LaWCr7tnKYm2w.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk"><strong class="bd ll">Fully Connected CRF</strong></figcaption></figure><p id="5748" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">x 是像素的标签分配。P(xi)是像素 I 处的标签分配概率。因此，第一项θi 是对数概率。</p><p id="ec33" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于第二项，θij，它是一个滤波器。= 1 当 xi！= xj。当 xi = xj 时= 0。在括号中，它是两个核的加权和。<strong class="jp ir">第一核</strong>依赖于像素值差和像素位置差，这就是<strong class="jp ir">一种双边滤波器</strong>。<strong class="jp ir"> </strong>双边滤波器具有<strong class="jp ir">保持边缘的特性。</strong><strong class="jp ir">第二核</strong>只依赖像素位置差，是一个<strong class="jp ir">高斯滤波器</strong>。σ和 w 是通过交叉验证得到的。迭代次数为 10 次。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/f295487d01ee425784182c7ab1510130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*0omVBDSf5sucAqsFBastiQ.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk"><strong class="bd ll">Top: Score map (input before softmax function), Bottom: belief map (output of softmax function)</strong></figcaption></figure><p id="5991" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用 10 倍的 CRF，飞机周围那些不同颜色的小区域被平滑掉了。</p><p id="9615" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是，CRF 是一个<strong class="jp ir">后处理过程</strong>使得 DeepLabv1 和 DeepLabv2 成为<strong class="jp ir">而不是一个端到端的学习框架</strong>。而且是<strong class="jp ir">在 DeepLabv3 和 DeepLabv3+已经不使用</strong>。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="9c8b" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">4.<strong class="ak">结果</strong></h1><h2 id="eaa5" class="nv lw iq bd lx nw nx dn mb ny nz dp mf jy oa ob mj kc oc od mn kg oe of mr og bi translated">4.1.消融研究</h2><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi oh"><img src="../Images/10673fba45d20eacbf763a4362d6d039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*o3180gQ3t3sDC4mz2nGQsA.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk"><strong class="bd ll">DeepLab-LargeFOV (Left: i.e. only single atrous conv), DeepLab-ASPP (Right, i.e. ASPP)</strong></figcaption></figure><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7cae4f1326103f2eaf50f4b2179e04ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*z1PIiwrIe-HVAcGaVDnahA.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk"><strong class="bd ll">Results of Each Component Using ResNet-101 in PASCAL VOC 2012 Validation Set</strong></figcaption></figure><ul class=""><li id="b39d" class="mt mu iq jp b jq jr ju jv jy nn kc no kg np kk nq nb nc nd bi translated">最简单的 ResNet-101  : 68.72%</li><li id="32cc" class="mt mu iq jp b jq ne ju nf jy ng kc nh kg ni kk nq nb nc nd bi translated"><strong class="jp ir"> MSC </strong>:多刻度输入</li><li id="338e" class="mt mu iq jp b jq ne ju nf jy ng kc nh kg ni kk nq nb nc nd bi translated"><strong class="jp ir">COCO</strong>:COCO 数据集预处理的模型</li><li id="e2d5" class="mt mu iq jp b jq ne ju nf jy ng kc nh kg ni kk nq nb nc nd bi translated"><strong class="jp ir"> Aug </strong>:通过随机缩放输入图像(从 0.5 到 1.5)进行数据扩充</li><li id="e037" class="mt mu iq jp b jq ne ju nf jy ng kc nh kg ni kk nq nb nc nd bi translated"><strong class="jp ir"> LargeFOV </strong>:使用单遍 atrous 卷积的 DeepLab</li><li id="bbd9" class="mt mu iq jp b jq ne ju nf jy ng kc nh kg ni kk nq nb nc nd bi translated"><strong class="jp ir"> ASPP: </strong>使用并行 atrous 卷积的深度实验室</li><li id="5efc" class="mt mu iq jp b jq ne ju nf jy ng kc nh kg ni kk nq nb nc nd bi translated"><strong class="jp ir"> CRF </strong>:后处理全连接 CRF。</li></ul><p id="0508" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后得到了<strong class="jp ir"> 77.69% </strong>。可以看出，MSC、COCO 和 Aug 贡献了从 68.72%到 74.87%的提高，这与 LargeFOV、ASPP 和 CRF 同样重要。</p><h2 id="dfae" class="nv lw iq bd lx nw nx dn mb ny nz dp mf jy oa ob mj kc oc od mn kg oe of mr og bi translated">4.2.与最先进方法的比较</h2><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi oj"><img src="../Images/b9ae96150a8f10c56cfaa2aa6bfff1ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fKEhNqDwudBvGYOhJje-HQ.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk"><strong class="bd ll">PASCAL VOC 2012 Test Set (Leftmost) PASCAL-Context (2nd Left) PASCAL-Person-Part (2nd Right) Cityscape (Rightmost)</strong></figcaption></figure><p id="c824" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">也如上测试了四个数据集。结果表明，与最先进的方法相比，DeepLabv2 具有竞争力的结果。</p><h2 id="8e18" class="nv lw iq bd lx nw nx dn mb ny nz dp mf jy oa ob mj kc oc od mn kg oe of mr og bi translated">4.3.定性结果</h2><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ok"><img src="../Images/4ddd2db121888d70e39f94232d74f19d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IT9f_7B8aBEn-ABnRHFlpA.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk"><strong class="bd ll">Qualitative Results: PASCAL-Context</strong></figcaption></figure><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ol"><img src="../Images/48f8ba56afc999e7eb48dfd147e85b3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qU4IU-Xf5Qs5XuHJScrNwQ.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk"><strong class="bd ll">Qualitative Results: Cityscape</strong></figcaption></figure><p id="bc5e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但 DeepLab 也有一些失败的例子，其中自行车和椅子由多个薄零件组成，如自行车和椅子腿的零件:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi om"><img src="../Images/3aea8109a8964536ca42c504e213b643.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QCS7SDey733Sy9qo_25b9w.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk"><strong class="bd ll">Failure Examples</strong></figcaption></figure></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><p id="658c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">希望以后能覆盖 DeepLabv3 和 DeepLabv3+。</p><h1 id="6303" class="lv lw iq bd lx ly on ma mb mc oo me mf mg op mi mj mk oq mm mn mo or mq mr ms bi translated">参考</h1><ol class=""><li id="d54b" class="mt mu iq jp b jq mv ju mw jy mx kc my kg mz kk na nb nc nd bi translated">【2015 ICLR】【DeepLabv1】<br/><a class="ae ku" href="https://arxiv.org/abs/1412.7062" rel="noopener ugc nofollow" target="_blank">深度卷积网和全连通 CRF 的语义图像分割</a></li><li id="c5a6" class="mt mu iq jp b jq ne ju nf jy ng kc nh kg ni kk na nb nc nd bi translated">【2018 TPAMI】【deeplabv 2】<br/><a class="ae ku" href="https://arxiv.org/abs/1606.00915" rel="noopener ugc nofollow" target="_blank">DeepLab:深度卷积网、阿特鲁卷积、全连通 CRFs 的语义图像分割</a></li></ol><h1 id="9fe1" class="lv lw iq bd lx ly on ma mb mc oo me mf mg op mi mj mk oq mm mn mo or mq mr ms bi translated">我的相关评论</h1><p id="5619" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy nj ka kb kc nk ke kf kg nl ki kj kk ij bi translated">[<a class="ae ku" href="https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679" rel="noopener">SPPNet</a>][<a class="ae ku" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener">VGGNet</a>][<a class="ae ku" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">ResNet</a>][<a class="ae ku" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a>]</p></div></div>    
</body>
</html>