<html>
<head>
<title>A practitioner's guide to PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch实践指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-practitioners-guide-to-pytorch-1d0f6a238040?source=collection_archive---------0-----------------------#2017-11-12">https://towardsdatascience.com/a-practitioners-guide-to-pytorch-1d0f6a238040?source=collection_archive---------0-----------------------#2017-11-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/ce9afa7753c68397d1541c4cd96902ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KKADWARPMxHb-WMxCgW_xA.png"/></div></div></figure><div class=""/><p id="8c8d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">几天前我开始使用PyTorch。下面我概述了PyTorch的关键概念，以及一些我在接触这个框架时发现特别有用的观察结果(如果你没有意识到它们，这可能会导致很多挫折！)</p><h2 id="8d2e" class="kw kx jb bd ky kz la dn lb lc ld dp le kj lf lg lh kn li lj lk kr ll lm ln lo bi translated">词汇表</h2><p id="81e0" class="pw-post-body-paragraph jy jz jb ka b kb lp kd ke kf lq kh ki kj lr kl km kn ls kp kq kr lt kt ku kv ij bi translated"><strong class="ka jc">张量</strong>——(喜欢)一个numpy.ndarray但是可以住在GPU上。</p><p id="2719" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">变量</strong>——通过将张量自身包裹起来，使其成为计算的一部分。如果创建时requires_grad = True，将在向后阶段计算梯度。</p><h2 id="5794" class="kw kx jb bd ky kz la dn lb lc ld dp le kj lf lg lh kn li lj lk kr ll lm ln lo bi translated">pytorch如何工作</h2><p id="1530" class="pw-post-body-paragraph jy jz jb ka b kb lp kd ke kf lq kh ki kj lr kl km kn ls kp kq kr lt kt ku kv ij bi translated">您可以通过写出它们来执行计算，就像这样:</p><figure class="lv lw lx ly gt is gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/f6f6c150e155bc12687be41c02399c8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*UtBi-OpqD7RD5KROQ2GJ_g.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">Taking a dot product of x with w using the new Python 3.5 syntax. torch.cuda creates the tensor on the GPU.</figcaption></figure><p id="1ce7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦完成，您需要做的就是对结果调用#backward()。这将计算梯度，并且您将能够访问使用requires_grad = True创建的变量的梯度。</p><figure class="lv lw lx ly gt is gh gi paragraph-image"><div class="gh gi md"><img src="../Images/540277190ae0865bfb833878090bb3ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*yr4vYVe1Fd0TT_HbLVYVWg.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">Accessing the gradient of w.</figcaption></figure><h2 id="c8d3" class="kw kx jb bd ky kz la dn lb lc ld dp le kj lf lg lh kn li lj lk kr ll lm ln lo bi translated">要记住的事情(否则有发疯的危险)</h2><ol class=""><li id="fc59" class="me mf jb ka b kb lp kf lq kj mg kn mh kr mi kv mj mk ml mm bi translated"><strong class="ka jc">数据类型很重要！<br/> </strong>如果把一个<a class="ae mn" href="http://pytorch.org/docs/master/tensors.html#" rel="noopener ugc nofollow" target="_blank">字节或者</a>除以50会怎么样？如果你试图将torch.exp(12)的结果存储在一个半张量中会怎样？实际上，这比乍看起来要简单得多，但确实需要一些思考。</li><li id="f70a" class="me mf jb ka b kb mo kf mp kj mq kn mr kr ms kv mj mk ml mm bi translated"><strong class="ka jc">如果能上溢或下溢，就会。<br/> </strong>数值稳定性很重要——当存储在合成张量中时，除法能产生零吗？如果你试着把它记录下来呢？<br/>再一次——这没什么可详细说明的，但肯定是需要注意的。如果你想更深入地了解这一点，让我向你介绍终极，同时在这一问题上非常平易近人的<a class="ae mn" href="https://www.youtube.com/watch?v=XlYD8jn1ayE&amp;feature=youtu.be&amp;list=PLoWh1paHYVRfygApBdss1HCt-TFZRXs0k&amp;t=1607" rel="noopener ugc nofollow" target="_blank">来源</a>。</li><li id="adee" class="me mf jb ka b kb mo kf mp kj mq kn mr kr ms kv mj mk ml mm bi translated"><strong class="ka jc">渐变默认累加！我把最好的留到了最后——与我们现在要看的野生动物相比，上面两只是友好的吉娃娃！<br/>默认情况下，渐变会累积。你运行一次计算，你逆向运行——一切都很好。但是，对于第二次运行，渐变被添加到第一次操作的渐变中！这很重要，也很容易忘记，请看看我们之前的例子:</strong></li></ol><figure class="lv lw lx ly gt is gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/6dc3cf8a32f2564581d314a4f6b3b3b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*Ct5PnZAuD2ku6fd4AsswRA.png"/></div></figure><p id="df88" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">解决方案是在运行之间手动调零梯度。</p><figure class="lv lw lx ly gt is gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/080b57369dbe1b73854df096a921f1db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*feHsLP2YkGuuzOqqbuIhNg.png"/></div></figure><p id="b1b9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后但同样重要的是，我想推荐<a class="ae mn" href="http://pytorch.org/tutorials/" rel="noopener ugc nofollow" target="_blank">官方教程</a>——不管你的经验水平如何，它们都是值得一去的好地方。</p><p id="b889" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我希望这能对你有所帮助，并能省去我在开始学习PyTorch时所经历的一些挣扎。祝我们俩好运，因为我们将继续掌握这个令人惊叹的框架！</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="68b7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="nc">如果你觉得这篇文章很有趣并且想保持联系，你可以在Twitter上找到我</em> <a class="ae mn" href="https://twitter.com/radekosmulski" rel="noopener ugc nofollow" target="_blank"> <em class="nc">这里</em> </a> <em class="nc">。</em></p></div></div>    
</body>
</html>