<html>
<head>
<title>Handling overfitting in deep learning models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">处理深度学习模型中的过拟合</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e?source=collection_archive---------2-----------------------#2018-08-23">https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e?source=collection_archive---------2-----------------------#2018-08-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/533db13c8aa9b850f60f95bf0e11a38b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XXtWMdH-YVL8z1VtnfG_iw.jpeg"/></div></div></figure><p id="ab16" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当您在训练数据上实现了模型的良好拟合，而在新的、看不见的数据上没有很好地概括时，就会发生过度拟合。换句话说，模型学习了特定于训练数据的模式，而这些模式在其他数据中是不相关的。</p><p id="7284" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以通过查看验证指标(如损失或准确性)来识别过度拟合。通常，验证度量在一定数量的时期后停止提高，之后开始下降。训练指标不断改进，因为模型试图找到最适合训练数据的方法。</p><p id="85d2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有几种方式可以减少深度学习模型中的过度拟合。最好的选择是<strong class="ka ir"> <em class="kw">获取更多的训练数据</em> </strong>。不幸的是，在现实世界中，由于时间、预算或技术限制，您通常没有这种可能性。</p><p id="05d5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">另一种减少过拟合的方法是<strong class="ka ir"> <em class="kw">降低模型记忆训练数据的容量</em> </strong>。因此，模型将需要关注训练数据中的相关模式，这将导致更好的泛化。在本帖中，我们将讨论实现这一目标的三个选项。</p><h1 id="e0b4" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">项目的建立</h1><p id="a948" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们首先导入必要的包并配置一些参数。我们将使用<a class="ae ma" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>来拟合深度学习模型。训练数据是来自 Kaggle 的<a class="ae ma" href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment" rel="noopener ugc nofollow" target="_blank"> Twitter 美国航空公司情绪数据集。</a></p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="135f" class="mk ky iq mg b gy ml mm l mn mo"># Basic packages<br/>import pandas as pd <br/>import numpy as np<br/>import re<br/>import collections<br/>import matplotlib.pyplot as plt<br/>from pathlib import Path</span><span id="a061" class="mk ky iq mg b gy mp mm l mn mo"># Packages for data preparation<br/>from sklearn.model_selection import train_test_split<br/>from nltk.corpus import stopwords<br/>from keras.preprocessing.text import Tokenizer<br/>from keras.utils.np_utils import to_categorical<br/>from sklearn.preprocessing import LabelEncoder</span><span id="fbca" class="mk ky iq mg b gy mp mm l mn mo"># Packages for modeling<br/>from keras import models<br/>from keras import layers<br/>from keras import regularizers</span><span id="76f3" class="mk ky iq mg b gy mp mm l mn mo">NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary<br/>NB_START_EPOCHS = 20  # Number of epochs we usually start to train with<br/>BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent<br/>MAX_LEN = 20  # Maximum number of words in a sequence</span><span id="0a47" class="mk ky iq mg b gy mp mm l mn mo">root = Path('../')<br/>input_path = root / 'input/' <br/>ouput_path = root / 'output/'<br/>source_path = root / 'source/'</span></pre><h1 id="621f" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">一些助手功能</h1><p id="8a7e" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">在本文中，我们将使用一些助手函数。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="4f1b" class="mk ky iq mg b gy ml mm l mn mo">def deep_model(model, X_train, y_train, X_valid, y_valid):<br/>    '''<br/>    Function to train a multi-class model. The number of epochs and <br/>    batch_size are set by the constants at the top of the<br/>    notebook. <br/>    <br/>    Parameters:<br/>        model : model with the chosen architecture<br/>        X_train : training features<br/>        y_train : training target<br/>        X_valid : validation features<br/>        Y_valid : validation target<br/>    Output:<br/>        model training history<br/>    '''<br/>    model.compile(optimizer='rmsprop'<br/>                  , loss='categorical_crossentropy'<br/>                  , metrics=['accuracy'])<br/>    <br/>    history = model.fit(X_train<br/>                       , y_train<br/>                       , epochs=NB_START_EPOCHS<br/>                       , batch_size=BATCH_SIZE<br/>                       , validation_data=(X_valid, y_valid)<br/>                       , verbose=0)<br/>    return history<br/></span><span id="728b" class="mk ky iq mg b gy mp mm l mn mo">def eval_metric(model, history, metric_name):<br/>    '''<br/>    Function to evaluate a trained model on a chosen metric. <br/>    Training and validation metric are plotted in a<br/>    line chart for each epoch.<br/>    <br/>    Parameters:<br/>        history : model training history<br/>        metric_name : loss or accuracy<br/>    Output:<br/>        line chart with epochs of x-axis and metric on<br/>        y-axis<br/>    '''<br/>    metric = history.history[metric_name]<br/>    val_metric = history.history['val_' + metric_name]</span><span id="ab08" class="mk ky iq mg b gy mp mm l mn mo">    e = range(1, NB_START_EPOCHS + 1)</span><span id="d91e" class="mk ky iq mg b gy mp mm l mn mo">    plt.plot(e, metric, 'bo', label='Train ' + metric_name)<br/>    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)<br/>    plt.xlabel('Epoch number')<br/>    plt.ylabel(metric_name)<br/>    plt.title('Comparing training and validation ' + metric_name + ' for ' + model.name)<br/>    plt.legend()<br/>    plt.show()</span><span id="0414" class="mk ky iq mg b gy mp mm l mn mo">def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):<br/>    '''<br/>    Function to test the model on new data after training it<br/>    on the full training data with the optimal number of epochs.<br/>    <br/>    Parameters:<br/>        model : trained model<br/>        X_train : training features<br/>        y_train : training target<br/>        X_test : test features<br/>        y_test : test target<br/>        epochs : optimal number of epochs<br/>    Output:<br/>        test accuracy and test loss<br/>    '''<br/>    model.fit(X_train<br/>              , y_train<br/>              , epochs=epoch_stop<br/>              , batch_size=BATCH_SIZE<br/>              , verbose=0)<br/>    results = model.evaluate(X_test, y_test)<br/>    print()<br/>    print('Test accuracy: {0:.2f}%'.format(results[1]*100))<br/>    return results</span><span id="c587" class="mk ky iq mg b gy mp mm l mn mo">    <br/>def remove_stopwords(input_text):<br/>    '''<br/>    Function to remove English stopwords from a Pandas Series.<br/>    <br/>    Parameters:<br/>        input_text : text to clean<br/>    Output:<br/>        cleaned Pandas Series <br/>    '''<br/>    stopwords_list = stopwords.words('english')<br/>    # Some words which might indicate a certain sentiment are kept via a whitelist<br/>    whitelist = ["n't", "not", "no"]<br/>    words = input_text.split() <br/>    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) &gt; 1] <br/>    return " ".join(clean_words) <br/>    <br/>def remove_mentions(input_text):<br/>    '''<br/>    Function to remove mentions, preceded by @, in a Pandas Series<br/>    <br/>    Parameters:<br/>        input_text : text to clean<br/>    Output:<br/>        cleaned Pandas Series <br/>    '''<br/>    return re.sub(r'@\w+', '', input_text)<br/></span><span id="62eb" class="mk ky iq mg b gy mp mm l mn mo">def compare_models_by_metric(model_1, model_2, model_hist_1, model_hist_2, metric):<br/>    '''<br/>    Function to compare a metric between two models <br/>    <br/>    Parameters:<br/>        model_hist_1 : training history of model 1<br/>        model_hist_2 : training history of model 2<br/>        metrix : metric to compare, loss, acc, val_loss or val_acc<br/>        <br/>    Output:<br/>        plot of metrics of both models<br/>    '''<br/>    metric_model_1 = model_hist_1.history[metric]<br/>    metric_model_2 = model_hist_2.history[metric]</span><span id="7c8e" class="mk ky iq mg b gy mp mm l mn mo">    e = range(1, NB_START_EPOCHS + 1)<br/>    <br/>    metrics_dict = {<br/>        'acc' : 'Training Accuracy',<br/>        'loss' : 'Training Loss',<br/>        'val_acc' : 'Validation accuracy',<br/>        'val_loss' : 'Validation loss'<br/>    }<br/>    <br/>    metric_label = metrics_dict[metric]</span><span id="888d" class="mk ky iq mg b gy mp mm l mn mo">    plt.plot(e, metric_model_1, 'bo', label=model_1.name)<br/>    plt.plot(e, metric_model_2, 'b', label=model_2.name)<br/>    plt.xlabel('Epoch number')<br/>    plt.ylabel(metric_label)<br/>    plt.title('Comparing ' + metric_label + ' between models')<br/>    plt.legend()<br/>    plt.show()<br/>    <br/>def optimal_epoch(model_hist):<br/>    '''<br/>    Function to return the epoch number where the validation loss is<br/>    at its minimum<br/>    <br/>    Parameters:<br/>        model_hist : training history of model</span><span id="9dfb" class="mk ky iq mg b gy mp mm l mn mo">    Output:<br/>        epoch number with minimum validation loss<br/>    '''<br/>    min_epoch = np.argmin(model_hist.history['val_loss']) + 1<br/>    print("Minimum validation loss reached in epoch {}".format(min_epoch))<br/>    return min_epoch</span></pre><h1 id="36c8" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">数据准备</h1><h2 id="8c56" class="mk ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">数据清理</h2><p id="1b72" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们用 tweets 加载 CSV 并执行随机洗牌。在分割训练集和测试集之前，对数据进行洗牌是一个很好的做法。这样，情感类别在训练集和测试集中平均分布。我们只将<strong class="ka ir"> <em class="kw">文本</em> </strong>列作为输入，将<strong class="ka ir"> <em class="kw">航空公司 _ 情绪</em> </strong>列作为目标。</p><p id="329a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来我们要做的是<strong class="ka ir"> <em class="kw">去除停用词</em> </strong>。停用词对于预测情绪没有任何价值。此外，由于我们希望建立一个同样适用于其他航空公司的模型，我们<strong class="ka ir"> <em class="kw">删除了提到的</em> </strong>。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="c5fc" class="mk ky iq mg b gy ml mm l mn mo">df = pd.read_csv(input_path / 'Tweets.csv')<br/>df = df.reindex(np.random.permutation(df.index))  <br/>df = df[['text', 'airline_sentiment']]<br/>df.text = df.text.apply(remove_stopwords).apply(remove_mentions)</span></pre><h2 id="0a83" class="mk ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">列车测试分离</h2><p id="94f6" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">模型性能的评估需要在单独的测试集上完成。因此，我们可以估计模型的概括程度。这是通过 scikit-learn 的<strong class="ka ir"><em class="kw">train _ test _ split</em></strong>方法完成的。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="9c7f" class="mk ky iq mg b gy ml mm l mn mo">X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)</span></pre><h2 id="880e" class="mk ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">将单词转换成数字</h2><p id="4db0" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">要使用文本作为模型的输入，我们首先需要将单词转换成记号，这仅仅意味着将单词转换成引用字典中索引的整数。这里我们将只保留训练集中最常用的单词。</p><p id="4666" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们通过应用<strong class="ka ir"> <em class="kw">滤镜</em> </strong>将文字整理成<strong class="ka ir"> <em class="kw">小写</em> </strong>。单词由空格分隔。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="784b" class="mk ky iq mg b gy ml mm l mn mo">tk = Tokenizer(num_words=NB_WORDS,<br/>               filters='!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{"}~\t\n',<br/>               lower=True,<br/>               char_level=False,<br/>               split=' ')<br/>tk.fit_on_texts(X_train)</span></pre><p id="6323" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">创建字典后，我们可以将 tweet 的文本转换为带有 NB_WORDS 值的向量。用<strong class="ka ir"> <em class="kw"> mode=binary </em> </strong>表示，它包含了一个指示词是否出现在推文中。这是通过记号赋予器的<strong class="ka ir"><em class="kw">texts _ to _ matrix</em></strong>方法完成的。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="70cb" class="mk ky iq mg b gy ml mm l mn mo">X_train_oh = tk.texts_to_matrix(X_train, mode='binary')<br/>X_test_oh = tk.texts_to_matrix(X_test, mode='binary')</span></pre><h2 id="7c1c" class="mk ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">将目标类转换为数字</h2><p id="cf37" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们还需要将目标类转换成数字，然后用 Keras 中的<strong class="ka ir"><em class="kw">to _ categorial</em></strong>方法对数字进行一次性编码</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="f3b0" class="mk ky iq mg b gy ml mm l mn mo">le = LabelEncoder()<br/>y_train_le = le.fit_transform(y_train)<br/>y_test_le = le.transform(y_test)<br/>y_train_oh = to_categorical(y_train_le)<br/>y_test_oh = to_categorical(y_test_le)</span></pre><h2 id="f6f9" class="mk ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">分离验证集</h2><p id="70ff" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">现在我们的数据已经准备好了，我们分离出一个验证集。当我们调整模型的参数时，这个验证集将用于评估模型的性能。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="a115" class="mk ky iq mg b gy ml mm l mn mo">X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1, random_state=37)</span></pre><h1 id="54df" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">深度学习</h1><h2 id="55bd" class="mk ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">创建一个过度拟合的模型</h2><p id="817d" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们从一个过度拟合的模型开始。它有两层紧密相连的 64 个元素。第一层的<strong class="ka ir"> <em class="kw"> input_shape </em> </strong>等于我们保存在字典中的单词数，我们为其创建了一个热编码特征。</p><p id="f5ca" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因为我们需要预测 3 个不同的情感类别，所以最后一层有 3 个元素。<strong class="ka ir"> <em class="kw"> softmax </em> </strong>激活功能确保三个概率总和为 1。</p><p id="77ed" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要训练的参数数量计算为<strong class="ka ir"> <em class="kw"> (nb 输入 x 隐藏层中的 nb 元素)+ nb 偏差项</em> </strong>。第一层的输入数量等于我们语料库中的单词数量。后续层将前一层的输出数量作为输入。因此每层的参数数量为:</p><ul class=""><li id="87fa" class="nb nc iq ka b kb kc kf kg kj nd kn ne kr nf kv ng nh ni nj bi translated">第一层:(10000 x 64) + 64 = 640064</li><li id="65ec" class="nb nc iq ka b kb nk kf nl kj nm kn nn kr no kv ng nh ni nj bi translated">第二层:(64 x 64) + 64 = 4160</li><li id="f5cd" class="nb nc iq ka b kb nk kf nl kj nm kn nn kr no kv ng nh ni nj bi translated">最后一层:(64 x 3) + 3 = 195</li></ul><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="313c" class="mk ky iq mg b gy ml mm l mn mo">base_model = models.Sequential()<br/>base_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))<br/>base_model.add(layers.Dense(64, activation='relu'))<br/>base_model.add(layers.Dense(3, activation='softmax'))<br/>base_model.name = 'Baseline model'</span></pre><p id="3004" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因为这个项目是多类单标签预测，所以我们用<strong class="ka ir"><em class="kw">categorial _ cross entropy</em></strong>作为损失函数，用<strong class="ka ir"> <em class="kw"> softmax </em> </strong>作为最终激活函数。我们在训练数据上拟合模型，并在验证集上验证。我们运行预定数量的历元，并观察模型何时开始过度拟合。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="c301" class="mk ky iq mg b gy ml mm l mn mo">base_history = deep_model(base_model, X_train_rest, y_train_rest, X_valid, y_valid)<br/>base_min = optimal_epoch(base_history)</span><span id="819f" class="mk ky iq mg b gy mp mm l mn mo">eval_metric(base_model, base_history, 'loss')</span></pre><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/beebaa9a526163445c4c6b93510d8470.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/0*ZwKhGQkYF3FqQlhe"/></div></figure><p id="caea" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">开始时，<strong class="ka ir"> <em class="kw">验证损失</em> </strong>下降。但是在时期 3，这停止并且验证损失开始快速增加。这是模型开始过度拟合的时候。</p><p id="a081" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="kw">训练损耗</em> </strong>继续下降，在纪元 20 几乎归零。这是正常的，因为模型被训练为尽可能好地拟合训练数据。</p><h1 id="d846" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">处理过拟合</h1><p id="a37b" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">现在，我们可以试着做一些关于过度拟合的事情。有不同的方法可以做到这一点。</p><ul class=""><li id="d335" class="nb nc iq ka b kb kc kf kg kj nd kn ne kr nf kv ng nh ni nj bi translated"><strong class="ka ir"> <em class="kw">通过移除图层或减少隐藏图层中的元素数量来减少网络容量</em> </strong></li><li id="8c26" class="nb nc iq ka b kb nk kf nl kj nm kn nn kr no kv ng nh ni nj bi translated">应用<strong class="ka ir"> <em class="kw">正则化</em> </strong>，这归结于为大权重的损失函数增加了一个成本</li><li id="549c" class="nb nc iq ka b kb nk kf nl kj nm kn nn kr no kv ng nh ni nj bi translated">使用<strong class="ka ir"> <em class="kw">脱落层</em> </strong>，将某些特征设置为零，随机删除</li></ul><h2 id="190b" class="mk ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">降低网络容量</h2><p id="ad2f" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们的第一个模型有大量可训练的参数。这个数字越高，模型就越容易记住每个训练样本的目标类。显然，这对于新数据的推广并不理想。</p><p id="bf18" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过降低网络容量，您迫使它学习重要的模式或将损失降至最低。另一方面，过多地减少网络的容量会导致<strong class="ka ir"> <em class="kw">配不上</em> </strong>。该模型将无法学习列车数据中的相关模式。</p><p id="a19c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们通过移除一个隐藏层并将剩余层中的元素数量减少到 16 个来减少网络容量。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="1fa2" class="mk ky iq mg b gy ml mm l mn mo">reduced_model = models.Sequential()<br/>reduced_model.add(layers.Dense(16, activation='relu', input_shape=(NB_WORDS,)))<br/>reduced_model.add(layers.Dense(3, activation='softmax'))<br/>reduced_model.name = 'Reduced model'</span><span id="dbed" class="mk ky iq mg b gy mp mm l mn mo">reduced_history = deep_model(reduced_model, X_train_rest, y_train_rest, X_valid, y_valid)<br/>reduced_min = optimal_epoch(reduced_history)</span><span id="0232" class="mk ky iq mg b gy mp mm l mn mo">eval_metric(reduced_model, reduced_history, 'loss')</span></pre><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/7c75d8aa82f765edbbdac8557d524dc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/0*ZDi9EJn6dORo4LCY"/></div></figure><p id="9486" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以看到，在简化模型开始过度拟合之前，需要更多的时期。验证损失也比我们的第一个模型上升得慢。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="d6df" class="mk ky iq mg b gy ml mm l mn mo">compare_models_by_metric(base_model, reduced_model, base_history, reduced_history, 'val_loss')</span></pre><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/12a7c53f28eb32839218cf7b0b9b8151.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/0*W8RSZtaBR-SDIGn5"/></div></figure><p id="57fb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当我们比较基线模型的验证损失时，很明显，简化模型在以后开始过度拟合。验证损失低于基线模型的时间要长得多。</p><h2 id="9b96" class="mk ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">应用正则化</h2><p id="75fd" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">为了解决过拟合问题，我们可以对模型应用权重正则化。对于较大的权重(或参数值)，这将增加网络损耗函数的成本。因此，您会得到一个更简单的模型，它将被迫只学习列车数据中的相关模式。</p><p id="9387" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">有<em class="kw"> L1 正则化和</em>L2 正则化</strong>。</p><ul class=""><li id="7cf8" class="nb nc iq ka b kb kc kf kg kj nd kn ne kr nf kv ng nh ni nj bi translated">L1 正则化将会增加一个关于<strong class="ka ir"> <em class="kw">参数的绝对值</em> </strong>的费用。这将导致一些权重等于零。</li><li id="234b" class="nb nc iq ka b kb nk kf nl kj nm kn nn kr no kv ng nh ni nj bi translated">L2 正则化将增加参数 的<strong class="ka ir"> <em class="kw">平方值的成本。这将导致较小的重量。</em></strong></li></ul><p id="6c04" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们试试 L2 正则化。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="bb9c" class="mk ky iq mg b gy ml mm l mn mo">reg_model = models.Sequential()<br/>reg_model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(NB_WORDS,)))<br/>reg_model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu'))<br/>reg_model.add(layers.Dense(3, activation='softmax'))<br/>reg_model.name = 'L2 Regularization model'</span><span id="f53a" class="mk ky iq mg b gy mp mm l mn mo">reg_history = deep_model(reg_model, X_train_rest, y_train_rest, X_valid, y_valid)<br/>reg_min = optimal_epoch(reg_history)</span></pre><p id="d299" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于正则化模型，我们注意到它在与基线模型相同的时期开始过拟合。然而，随后的损失增加速度要慢得多。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="13e1" class="mk ky iq mg b gy ml mm l mn mo">eval_metric(reg_model, reg_history, 'loss')</span></pre><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/f3ffcdde97054c0254fbb551e0992fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/0*4UV4tgegrn6UOdRX"/></div></figure><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="0c03" class="mk ky iq mg b gy ml mm l mn mo">compare_models_by_metric(base_model, reg_model, base_history, reg_history, 'val_loss')</span></pre><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/4d49f3ca7956cd3453ff2bd2b25edd9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/0*5ybMcTqkOXyC7xc4"/></div></figure><h2 id="b097" class="mk ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">添加脱落层</h2><p id="fde4" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们尝试的最后一个选项是添加脱落层。脱落图层会将图层的输出要素随机设置为零。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="4689" class="mk ky iq mg b gy ml mm l mn mo">drop_model = models.Sequential()<br/>drop_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))<br/>drop_model.add(layers.Dropout(0.5))<br/>drop_model.add(layers.Dense(64, activation='relu'))<br/>drop_model.add(layers.Dropout(0.5))<br/>drop_model.add(layers.Dense(3, activation='softmax'))<br/>drop_model.name = 'Dropout layers model'</span><span id="0a04" class="mk ky iq mg b gy mp mm l mn mo">drop_history = deep_model(drop_model, X_train_rest, y_train_rest, X_valid, y_valid)<br/>drop_min = optimal_epoch(drop_history)</span><span id="5e2c" class="mk ky iq mg b gy mp mm l mn mo">eval_metric(drop_model, drop_history, 'loss')</span></pre><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/41cbfc4d39110b838e9d8b6f8a52e092.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/0*B0SuqLpCTYGP4Bwz"/></div></figure><p id="54e1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">具有脱落层的模型比基线模型晚开始过度拟合。损耗的增加也比基线模型慢。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="c590" class="mk ky iq mg b gy ml mm l mn mo">compare_models_by_metric(base_model, drop_model, base_history, drop_history, 'val_loss')</span></pre><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/679bbdfc5a7697ccbf5334c0c52277dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/0*ykk8IT9v3wgsq1Wf"/></div></figure><p id="23e4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">具有脱落层的模型稍后开始过度拟合。与基线模型相比，损耗也保持低得多。</p><h1 id="cb0f" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">全训练数据的训练和测试数据的评估</h1><p id="e804" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">乍一看，简化的模型似乎是最好的泛化模型。但是让我们在测试集上检查一下。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="7706" class="mk ky iq mg b gy ml mm l mn mo">base_results = test_model(base_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, base_min)<br/>reduced_results = test_model(reduced_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, reduced_min)<br/>reg_results = test_model(reg_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, reg_min)<br/>drop_results = test_model(drop_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, drop_min)</span></pre><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/558a72c9c3e5c2df1a7f6ac57500edb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*faz4dZBCsh3yB6tAZzSXkg.png"/></div></figure><h1 id="bc78" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">结论</h1><p id="824b" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">如上所示，所有三个选项都有助于减少过度拟合。我们设法大幅提高测试数据的准确性。在这三个选项中，具有脱落层的模型在测试数据上表现最好。</p><p id="9319" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你可以在<a class="ae ma" href="https://github.com/bertcarremans/TwitterUSAirlineSentiment/blob/master/source/Handling%20overfitting%20in%20deep%20learning%20models.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到笔记本。好好享受吧！欢迎任何反馈。</p></div></div>    
</body>
</html>