<html>
<head>
<title>How to use TorchText for neural machine translation, plus hack to make it 5x faster</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用 TorchText 进行神经机器翻译，加上 hack 使其速度提高 5 倍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95?source=collection_archive---------8-----------------------#2018-09-27">https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95?source=collection_archive---------8-----------------------#2018-09-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/eb0418ffd245acee2a80ae1274795162.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ct-w6Y9mkutE83nJBB8NKg.jpeg"/></div></div></figure><div class=""/><p id="14d4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">处理文本是 NLP 中任何成功项目的第一步，幸运的是，帮助我们的工具正在变得越来越复杂。</p><p id="6de9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于我的第一个深度学习项目，我构建了一个高功能的英法翻译器，如果没有 TorchText 库，我不可能做到这一点。好吧，也许这是一个有点戏剧性的说法…我至少会被大大减慢。</p><p id="af21" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">TorchText 非常方便，因为它允许您快速标记和打包(这些是单词吗？)你的数据。原本需要你自己的功能和大量思考的东西现在只需要几行代码，然后，你就可以构建你的人工智能了，它将接管这个世界(或者只是翻译一种语言什么的，我不知道你在计划什么)。</p><p id="6c03" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然而，它也不是没有缺点，在这篇文章中，我们还将探讨它最大的低效率以及如何快速规避它。</p><p id="e1b2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，现在让我们通过浏览我在越来越多的互联网上找到的一些英语-法语数据来探索如何使用 TorchText。</p><h1 id="833d" class="kw kx jb bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">所有翻译数据的始祖</h1><p id="2653" class="pw-post-body-paragraph jy jz jb ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">这是我用来训练我的翻译的数据集。</p><p id="f285" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">虽然法语和英语之间有许多小型并行数据集，但我想创建一个尽可能强大的翻译器，于是我选择了 big kah una:the European Parliament Proceedings Parallel Corpus 1996–2011(可从这里下载<a class="ae lz" href="http://www.statmt.org/europarl/" rel="noopener ugc nofollow" target="_blank"/>)。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ma"><img src="../Images/5ad1c56ca1f135d38a5859988cf34050.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iDPRWDeLbh2nfg6FSBiV_w.jpeg"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">15 years of EU proceedings makes an enthralling read for our seq2seq model!</figcaption></figure><p id="344a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个坏男孩包含了 15 年来来自欧盟程序的评论，总计 2007724 个句子，50265039 个单词。</p><p id="6fc7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">解压缩可下载文件将产生两个文本文件，可以通过以下方式加载:</p><pre class="mb mc md me gt mj mk ml mm aw mn bi"><span id="4d88" class="mo kx jb mk b gy mp mq l mr ms">europarl_en = open('{YOUR_PATH}/europarl-v7.fr-en.en', encoding='utf-8').read().split('\n')<br/>europarl_fr = open('{YOUR_PATH}/europarl-v7.fr-en.fr', encoding='utf-8').read().split('\n')</span></pre><p id="10b8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这给我们留下了两个 python 句子列表，其中每个句子都是另一个句子的翻译。</p><h1 id="6175" class="kw kx jb bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">使用分词器快速处理文本</h1><p id="1aaa" class="pw-post-body-paragraph jy jz jb ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">我们需要做的第一件事是为每种语言创建一个标记器。这是一个将文本分割成独立单词并给它们分配唯一数字(索引)的功能。当我们讨论嵌入时，这个数字将会发挥作用。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mt"><img src="../Images/ba2b743b41a8ba230d1bbf5f6f859e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lANyhSqRj-6FFqcqTKWclA.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Sentences turned into tokens, which are then given individual indexes</figcaption></figure><p id="5f26" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面的代码展示了如何同时使用 Torchtext 和 Spacy 来标记文本。Spacy 是一个专门构建的库，用于获取各种语言的句子，并将它们拆分成不同的标记(更多信息，请参见<a class="ae lz" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank">此处</a>)。没有 Spacy，Torchtext 默认为一个简单的<em class="mu">。</em>拆分(“”)的方法进行标记化。这比 Spacy 的方法要少得多，Spacy 的方法也将像“不”这样的词分成“做”和“不”，等等。</p><pre class="mb mc md me gt mj mk ml mm aw mn bi"><span id="bc13" class="mo kx jb mk b gy mp mq l mr ms">import spacy<br/>import torchtext<br/>from torchtext.data import Field, BucketIterator, TabularDataset</span><span id="ddd5" class="mo kx jb mk b gy mv mq l mr ms">en = spacy.load('en')<br/>fr = spacy.load('fr')</span><span id="d7b7" class="mo kx jb mk b gy mv mq l mr ms">def tokenize_en(sentence):<br/>    return [tok.text for tok in en.tokenizer(sentence)]</span><span id="4793" class="mo kx jb mk b gy mv mq l mr ms">def tokenize_fr(sentence):<br/>    return [tok.text for tok in fr.tokenizer(sentence)]</span><span id="e07b" class="mo kx jb mk b gy mv mq l mr ms">EN_TEXT = Field(tokenize=tokenize_en)</span><span id="0bac" class="mo kx jb mk b gy mv mq l mr ms">FR_TEXT = Field(tokenize=tokenize_fr, init_token = "&lt;sos&gt;", eos_token = "&lt;eos&gt;")</span></pre><p id="947b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在短短几行代码中，我们创建了两个 field 对象，稍后它们将能够处理/标记我们的文本。</p><h1 id="bfdc" class="kw kx jb bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">使用 Torchtext TabularDataset 构建迭代器</h1><p id="295f" class="pw-post-body-paragraph jy jz jb ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">也许与直觉相反，使用 Torchtext 的最佳方式是将数据转换成电子表格格式，不管数据文件的原始格式是什么。</p><p id="0459" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这要归功于 Torchtext TabularDataset 函数令人难以置信的多功能性，它可以从电子表格格式创建数据集。</p><p id="f8b0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，首先要将我们的数据转换成一个合适的 CSV 文件，我们使用了一点熊猫的魔法:</p><pre class="mb mc md me gt mj mk ml mm aw mn bi"><span id="9d01" class="mo kx jb mk b gy mp mq l mr ms">import pandas as pd</span><span id="331b" class="mo kx jb mk b gy mv mq l mr ms">raw_data = {'English' : [line for line in europarl_en], 'French': [line for line in europarl_fr]}</span><span id="e2a3" class="mo kx jb mk b gy mv mq l mr ms">df = pd.DataFrame(raw_data, columns=["English", "French"])</span><span id="0b76" class="mo kx jb mk b gy mv mq l mr ms"># remove very long sentences and sentences where translations are <br/># not of roughly equal length</span><span id="88b3" class="mo kx jb mk b gy mv mq l mr ms">df['eng_len'] = df['English'].str.count(' ')<br/>df['fr_len'] = df['French'].str.count(' ')<br/>df = df.query('fr_len &lt; 80 &amp; eng_len &lt; 80')<br/>df = df.query('fr_len &lt; eng_len * 1.5 &amp; fr_len * 1.5 &gt; eng_len')</span></pre><p id="d891" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们现在必须创建一个验证集。这一步很关键！例如，seq2seq 上的<a class="ae lz" href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html" rel="noopener ugc nofollow" target="_blank">这个 pytorch 教程</a>没有做到这一点，在我自己构建它并使用验证集后，我发现它过度拟合了。</p><p id="743d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">幸运的是，Sklearn 和 Torchtext 一起让这个过程变得异常简单:</p><pre class="mb mc md me gt mj mk ml mm aw mn bi"><span id="25a2" class="mo kx jb mk b gy mp mq l mr ms">from sklearn.model_selection import train_test_split</span><span id="f3f5" class="mo kx jb mk b gy mv mq l mr ms"># create train and validation set <br/>train, val = train_test_split(df, test_size=0.1)</span><span id="fd30" class="mo kx jb mk b gy mv mq l mr ms">train.to_csv("train.csv", index=False)<br/>val.to_csv("val.csv", index=False)</span></pre><p id="86d5" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这将创建一个 train 和 validation csv，每个 csv 都有两列(英语、法语)，其中每行在“英语”列中包含一个英语句子，在“法语”列中包含其法语翻译。</p><p id="a962" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">调用 magic TabularDataset.splits 然后返回一个训练和验证数据集，其中加载了相应的数据，并根据我们前面定义的字段进行了处理(/标记化)。</p><pre class="mb mc md me gt mj mk ml mm aw mn bi"><span id="2c69" class="mo kx jb mk b gy mp mq l mr ms"># associate the text in the 'English' column with the EN_TEXT field, # and 'French' with FR_TEXT<br/>data_fields = [('English', EN_TEXT), ('French', FR_TEXT)]</span><span id="bb5c" class="mo kx jb mk b gy mv mq l mr ms">train,val = data.TabularDataset.splits(path='./', train='train.csv', validation='val.csv', format='csv', fields=data_fields)</span></pre><p id="52cb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">处理几百万字可能需要一段时间，所以在这里喝杯茶吧…</p><p id="e18c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦最终完成，我们可以索引所有的令牌:</p><pre class="mb mc md me gt mj mk ml mm aw mn bi"><span id="48be" class="mo kx jb mk b gy mp mq l mr ms">FR_TEXT.build_vocab(train, val)<br/>EN_TEXT.build_vocab(train, val)</span></pre><p id="2753" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要查看每个字段中的令牌被分配了哪些数字，反之亦然，我们可以使用 self.vocab.stoi 和 self.vocab.itos…这在以后会很方便。</p><pre class="mb mc md me gt mj mk ml mm aw mn bi"><span id="c929" class="mo kx jb mk b gy mp mq l mr ms">example input: print(EN_TEXT.vocab.stoi['the'])<br/>example_output: 11</span><span id="d0d0" class="mo kx jb mk b gy mv mq l mr ms">example input: print(EN_TEXT.vocab.itos[11])<br/>example_output: 'the'</span></pre><p id="c150" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">只需多一行代码，我们就有了一个迭代器…</p><pre class="mb mc md me gt mj mk ml mm aw mn bi"><span id="aded" class="mo kx jb mk b gy mp mq l mr ms">train_iter = BucketIterator(train, batch_size=20, \<br/>sort_key=lambda x: len(x.French), shuffle=True)</span></pre><p id="8a90" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们来看一个批次的例子，这样我们就可以看到我们到底用所有这些 Torchtext 魔法做了什么…</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mw"><img src="../Images/3103c4a85b73d320d965cc4f8ca5b09b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hFfhvfhM5T_GiNiJlmu0IA.png"/></div></div></figure><p id="a2bf" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在每一批中，句子已经被转置，所以它们是垂直下降的(<strong class="ka jc">重要:</strong>我们将需要再次转置这些句子以使用转换器)。每个索引代表一个单词，每个列代表一个句子。我们有 10 列，因为 10 是我们指定的 batch_size。</p><p id="3bb5" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你可能会注意到所有的“1 ”,并想这是哪个极其常见的单词的索引？这个“1”当然不是一个词，而是纯粹的填充。事实上，它太多了(这是我们将在下一节中解决的问题)。</p><pre class="mb mc md me gt mj mk ml mm aw mn bi"><span id="cdab" class="mo kx jb mk b gy mp mq l mr ms">..., sort_key = lambda x: len(x.French), ...</span></pre><p id="fc4b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，这个排序关键字位决定了如何形成每一批。lambda 函数告诉迭代器尝试查找相同长度的句子(这意味着矩阵中更多的是有用的数据，更少的是填充)。</p><p id="97d9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">用这么少的几行代码完成了这么多的工作…我们已经将数据处理成了标记，并得到了一个迭代器，它返回匹配的、成批的法语和英语句子，所有的句子都是填充的，等等。</p><h1 id="f5a7" class="kw kx jb bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">黑客入侵 TorchText 使其效率大大提高</h1><pre class="mb mc md me gt mj mk ml mm aw mn bi"><span id="99d3" class="mo kx jb mk b gy mp mq l mr ms"># code from <a class="ae lz" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a> <br/># read text after for description of what it does</span><span id="7506" class="mo kx jb mk b gy mv mq l mr ms">global max_src_in_batch, max_tgt_in_batch</span><span id="9f4a" class="mo kx jb mk b gy mv mq l mr ms">def batch_size_fn(new, count, sofar):<br/>    "Keep augmenting batch and calculate total number of tokens + padding."<br/>    global max_src_in_batch, max_tgt_in_batch<br/>    if count == 1:<br/>        max_src_in_batch = 0<br/>        max_tgt_in_batch = 0<br/>    max_src_in_batch = max(max_src_in_batch,  len(new.English))<br/>    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.French) + 2)<br/>    src_elements = count * max_src_in_batch<br/>    tgt_elements = count * max_tgt_in_batch<br/>    return max(src_elements, tgt_elements)</span><span id="c26a" class="mo kx jb mk b gy mv mq l mr ms">class MyIterator(data.Iterator):<br/>    def create_batches(self):<br/>        if self.train:<br/>            def pool(d, random_shuffler):<br/>                for p in data.batch(d, self.batch_size * 100):<br/>                    p_batch = data.batch(<br/>                        sorted(p, key=self.sort_key),<br/>                        self.batch_size, self.batch_size_fn)<br/>                    for b in random_shuffler(list(p_batch)):<br/>                        yield b<br/>            self.batches = pool(self.data(), self.random_shuffler)<br/>            <br/>        else:<br/>            self.batches = []<br/>            for b in data.batch(self.data(), self.batch_size,<br/>                                          self.batch_size_fn):<br/>                self.batches.append(sorted(b, key=self.sort_key))</span></pre><p id="8e7d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">虽然 Torchtext 很出色，但它基于 sort_key 的批处理还有一些不足之处。通常，句子的长度根本不一样，你最终会向你的网络中输入大量的填充(正如你在最后一个数字的<a class="ae lz" href="#8a90" rel="noopener ugc nofollow">中看到的所有 1)。</a></p><p id="3235" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，如果您的 RAM 每次迭代可以处理 1500 个令牌，并且您的 batch_size 是 20，那么只有当您的批处理长度为 75 时，您才会利用所有的内存。一个有效的批处理机制将根据序列长度改变批处理大小，以确保每次迭代处理大约 1500 个令牌。</p><p id="7753" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是通过上面在 Torchtext 上运行的补丁批处理代码实现的。我们现在可以创建一个更有效的迭代器，如下所示:</p><pre class="mb mc md me gt mj mk ml mm aw mn bi"><span id="6c6d" class="mo kx jb mk b gy mp mq l mr ms">train_iter = MyIterator(trn, batch_size=1300, device=0,<br/>                        repeat=False, sort_key= lambda x:<br/>                        (len(x.English), len(x.French)),<br/>                        batch_size_fn=batch_size_fn, train=True,<br/>                        shuffle=True)</span></pre><h1 id="4921" class="kw kx jb bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">准备出发！</h1><p id="ca6b" class="pw-post-body-paragraph jy jz jb ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">现在你有了它，一个迭代器，可以部署在任何神经机器翻译模型上。看我的 github <a class="ae lz" href="https://github.com/SamLynnEvans/Transformer" rel="noopener ugc nofollow" target="_blank">这里</a>，我在我的<a class="ae lz" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">变形金刚模型</a>的实现中使用了这个代码，你可以在你自己的数据集上尝试这个最先进的 NMT 模型。</p></div></div>    
</body>
</html>