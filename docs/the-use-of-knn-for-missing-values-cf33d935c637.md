# 对缺失值使用 KNN

> 原文：<https://towardsdatascience.com/the-use-of-knn-for-missing-values-cf33d935c637?source=collection_archive---------0----------------------->

![](img/36b38c12d3142d2c3370526d4d74a665.png)

# **更新**

代码的主要更新，以提高计算加权汉明距离的性能。该方法从逐点距离计算改为矩阵计算，从而节省了多个数量级的时间。

# 访问代码:

[Github 链接](https://gist.github.com/YohanObadia/b310793cd22a4427faaadd9c381a5850)

# 问题:

在处理真实世界的数据时，您经常会在数据集中遇到缺失值。你如何处理它们对你的分析和你将得出的结论至关重要。

缺失值有三种常见类型:

1.  **完全随机缺失(MCAR):** 当缺失数据为 MCAR 时，数据的有/无完全独立于感兴趣的可观测变量和参数。在这种情况下，对数据进行的分析是无偏的。实际上，这是极不可能的。
2.  **随机缺失(MAR):** 当缺失数据为**非**随机，但可以与一个变量完全相关，且有完整信息时。一个例子是，男性不太可能填写抑郁调查，但在考虑男性因素后，这与他们的抑郁水平无关。这种缺失数据会导致您的分析出现偏差，尤其是当您的数据因为某个类别中的许多缺失值而不平衡时。
3.  **非随机缺失(MNAR):** 当缺失值既不是 MCAR 也不是马尔时。在前面的示例中，如果人们倾向于根据自己的抑郁水平不回答调查，就会出现这种情况。

在本教程中，我们将使用一种称为 k-最近邻(KNN)的非参数算法来替换丢失的值。该算法适用于前面三种情况中的任何一种，只要具有缺失值的变量与其他变量之间存在关系。

# 为什么用 KNN？

KNN 是一种算法，用于在多维空间中将一个点与其最近的 k 个邻居进行匹配。它可以用于连续、离散、有序和分类的数据，这使得它在处理各种缺失数据时特别有用。

对缺失值使用 KNN 背后的假设是，基于其他变量，点的值可以由与其最接近的点的值来近似。

让我们保留之前的例子，并添加另一个变量，人的收入。现在，我们有三个变量，性别(T0)、收入(T2)、抑郁程度(T4)和缺失值(T5)。然后，我们假设收入相似、性别相同的人往往会有相同程度的抑郁。对于给定的缺失值，我们将查看该人的性别、收入，寻找其最近的 k 个邻居，并获得他们的抑郁水平。然后我们可以估计出我们想要的人的抑郁程度。

# KNN 参数校准

使用 KNN 时，您必须考虑许多参数。您将在下面找到本文提供的函数所允许的内容:

*   **要寻找的邻居数量**。采用低 k 值**会增加噪声的影响，并且结果不太具有普遍性。另一方面，采用高 k 值会模糊局部效果，而这正是我们想要的。也建议二进制类取一个**奇 k** 避免平局。**
*   **聚合方法**使用。在这里，我们允许数字变量的算术平均值、中值和众数以及分类变量的众数。
*   **归一化数据**是一种方法，当计算某种类型的距离时，如**欧几里德**距离，该方法允许在识别邻居时给予每个属性相同的影响。当比例没有意义和/或比例不一致(如厘米和米)时，应该对数据进行规范化。它意味着对数据的先验知识，以知道哪一个更重要。当同时提供了数字变量和分类变量时，该算法会自动对数据进行规范化。
*   **数字属性距离:**在各种可用的距离度量中，我们将关注主要的度量，欧几里德和曼哈顿。如果输入变量的类型相似(例如，所有测量的宽度和高度)，欧氏距离是一种很好的距离测量方法。如果输入变量的类型不相似(如年龄、身高等)，曼哈顿距离是一种很好的度量方法。
*   **分类属性距离:**没有先验变换，适用距离与频率和相似度有关。这里我们允许使用两种距离:汉明距离和加权汉明距离。

    - **汉明距离:**取所有的分类属性，对于每个属性，如果两点之间的值不相同，则计 1。那么汉明距离就是值不同的属性的数量。

    - **加权海明距离:**如果值不同也返回 1，但如果匹配则返回属性中值的频率，值越频繁则增加距离。当多个属性是分类属性时，应用**调和平均值**。结果保持在 0 和 1 之间，但是与**算术平均值**相比，平均值向较低值移动。
*   **二元属性距离:**这些属性通常是通过转换成虚拟变量的分类变量获得的。如前所述，对于连续变量，**欧几里德距离**也可应用于此。然而，也可以使用另一个基于相异度的度量，即 **Jaccard 距离**。

为了确定用于您的数据的最佳距离度量，您可以使用上面给出的提示，但最重要的是，您必须进行实验，以找到最佳改进您的模型的方法。

# 在大型数据集上的测试

目标:预测哪位乘客在泰坦尼克号上幸存。你可以在 [Kaggle](https://www.kaggle.com/c/titanic/data) 上下载数据。为了能够快速进行多项测试，我还下载了完整的数据集，包括每位乘客的名单以及他们是否幸存。

**程序:**

*   为了构建一个简单的模型，我删除了“姓名”、“客舱”和“机票”这三列。
*   然后，我发现两列缺少值，“年龄”和“上船”。第一个有很多缺失值，而第二个只有几个。对于这两列，我应用了两种方法:
    1-对数字列使用全局平均值，对分类列使用全局模式。
    2-应用 knn_impute 函数。
*   构建一个简单的随机森林模型

**简单逼近结果:** 灵敏度= 66%；特异性= 79%；精度= 63%

**KNN 用最佳模型插补结果:** 灵敏度= 69%；特异性= 80%；精度= 66%

**代码示例:**

对于这个数据集来说，这两种方法之间的结果差异不是很大，但是在 Kaggle 比赛中，人们可以花很多时间来获得这几个额外的百分比。

如果你在其他数据集上测试这个函数，我期待着听到你的反馈，关于这个函数在帮助你改善结果方面有多大的帮助！

# 资源:

*   [https://en.wikipedia.org/wiki/Missing_data](https://en.wikipedia.org/wiki/Missing_data)
*   【http://citeseerx.ist.psu.edu/viewdoc/download? doi = 10 . 1 . 1 . 140 . 8831&rep = re P1&type = pdf
*   [http://machine learning mastery . com/k-nearest-neighbors-for-machine-learning/](http://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/)
*   [http://jamesxli . blogspot . co . il/2013/03/on-weighted-hamming-distance . html](http://jamesxli.blogspot.co.il/2013/03/on-weighted-hamming-distance.html)