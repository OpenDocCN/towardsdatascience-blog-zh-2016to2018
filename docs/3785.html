<html>
<head>
<title>Hyper-parameters in Action! Part II — Weight Initializers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数在起作用！第二部分—权重初始值设定项</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404?source=collection_archive---------0-----------------------#2018-06-18">https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404?source=collection_archive---------0-----------------------#2018-06-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/a685fc54eb31ab5135351ad659a7fb54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bXkY3UaSDaXKyMqE"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@aggergakker?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jesper Aggergaard</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="ddef" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h1><p id="2fac" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这是我关于<strong class="lg iu">超参数</strong>系列的第二篇文章。在这篇文章中，我将向你展示正确初始化你的深度神经网络的<strong class="lg iu">权重</strong>的<strong class="lg iu">重要性</strong>。我们将从一个<strong class="lg iu">简单的初始化方案</strong>开始，<strong class="lg iu">解决它的问题</strong>，就像<strong class="lg iu">消失</strong> / <strong class="lg iu">爆炸渐变</strong>，直到我们<strong class="lg iu">(重新)发现</strong>两个流行的初始化方案:<strong class="lg iu">Xavier</strong>/<strong class="lg iu">Glorot</strong>和<strong class="lg iu"> He </strong>。</p><p id="e087" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我假设你已经熟悉了一些关键概念(Z 值、激活函数及其梯度)，我在本系列的第一篇<a class="ae kf" rel="noopener" target="_blank" href="/hyper-parameters-in-action-a524bf5bf1c">帖子</a>中已经提到过。</p><blockquote class="mh mi mj"><p id="a948" class="le lf mk lg b lh mc lj lk ll md ln lo ml me lr ls mm mf lv lw mn mg lz ma mb im bi translated">说明这篇文章的情节是用我的包<strong class="lg iu"> DeepReplay </strong>生成的，你可以在<a class="ae kf" href="https://github.com/dvgodoy/deepreplay" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到它，并在这篇<a class="ae kf" rel="noopener" target="_blank" href="/hyper-parameters-in-action-introducing-deepreplay-31132a7b9631">文章</a>上了解更多。</p></blockquote><h1 id="494c" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">动机</h1><p id="8fa4" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在我寻求更深入地理解每一个不同的<strong class="lg iu">超参数</strong>对训练深度神经网络的影响时，是时候研究一下<strong class="lg iu">权重初始化器</strong>了。</p><p id="115c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果您曾经搜索过这个特定的主题，您可能会遇到一些常见的<strong class="lg iu">初始化方案</strong>:</p><ul class=""><li id="a844" class="mo mp it lg b lh mc ll md lp mq lt mr lx ms mb mt mu mv mw bi translated"><strong class="lg iu">随机</strong></li><li id="c31e" class="mo mp it lg b lh mx ll my lp mz lt na lx nb mb mt mu mv mw bi translated"><strong class="lg iu">泽维尔</strong> / <strong class="lg iu">格洛特</strong></li><li id="5589" class="mo mp it lg b lh mx ll my lp mz lt na lx nb mb mt mu mv mw bi translated"><strong class="lg iu">何</strong></li></ul><p id="70f7" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果你挖得更深一点，你可能还会发现，如果激活函数是一个<strong class="lg iu"> Tanh </strong>，那么应该使用<strong class="lg iu"> Xavier / Glorot 初始化</strong>，如果激活函数是一个<strong class="lg iu"> ReLU </strong>，那么推荐使用<strong class="lg iu"> He 初始化</strong>。</p><p id="2b7b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">顺便说一下，澄清一些事情:<em class="mk"> Xavier Glorot </em>和<em class="mk"> Yoshua Bengio </em>是“<a class="ae kf" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">理解训练深度前馈神经网络</strong> </a>的困难”论文的作者，该论文概述了将<strong class="lg iu">作为其第一作者的</strong>开头(<em class="mk"> Xavier </em>或最后(<em class="mk"> Glorot </em>)的初始化方案。因此，有时这种方案将被称为<strong class="lg iu"> Xavier 初始化、</strong>和其他一些时候(如在<em class="mk"> Keras </em>中)，它将被称为<strong class="lg iu"> Glorot 初始化</strong>。不要被这个迷惑，因为我是第一次知道这个话题。</p><p id="8f9e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">弄清楚这一点后，我再问你一次:你有没有想过引擎盖下到底发生了什么？为什么初始化这么重要<strong class="lg iu"/>？初始化方案之间的<strong class="lg iu">差异</strong>是什么？我的意思是，不仅是他们对方差应该是什么的不同定义，而且是在训练一个深度神经网络时使用其中一个的总体效果！</p><p id="9bef" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在深入研究之前，我想给它应有的信任:这些情节在很大程度上受到了安德烈·皮卢尼奇关于同一主题的令人敬畏的帖子的启发。</p><p id="19ec" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">好了，<strong class="lg iu">现在</strong>让我们开始吧！</p><h1 id="d9e6" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">设置</strong></h1><blockquote class="mh mi mj"><p id="95a5" class="le lf mk lg b lh mc lj lk ll md ln lo ml me lr ls mm mf lv lw mn mg lz ma mb im bi translated">确保您使用的是 Keras 2.2.0 或更新版本-旧版本有一个问题，生成的权重集的方差低于预期！</p></blockquote><p id="e5ea" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在这篇文章中，我将使用一个具有<strong class="lg iu"> 5 个隐藏层</strong>和<strong class="lg iu"> 100 个单位</strong>的模型，以及一个典型的<strong class="lg iu">二进制分类</strong>任务中的单个单位输出层(即使用<em class="mk"> sigmoid </em>作为激活函数，使用<em class="mk">二进制交叉熵</em>作为损失)。我将这个模型称为<strong class="lg iu">模块</strong>模型，因为它有相同大小的连续层。我使用以下代码来构建我的模型:</p><figure class="nc nd ne nf gt ju"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Model builder function</figcaption></figure><h2 id="65bd" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">块状模型</h2><p id="f872" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这是块模型的架构，不考虑我将用来构建图的激活函数和/或初始化器。</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/e4e77bd3a9abea4f32fc576b7f82e006.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*onIgJE7niQjF5fgW0HblEA.png"/></div></figure><h2 id="4160" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">输入</h2><p id="0bad" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">输入是从<strong class="lg iu"> 10 维球</strong>中抽取的<strong class="lg iu">1000 个随机点</strong>(这看起来<em class="mk">比实际上的</em>更好，你可以把它想象成一个有 1000 个样本的数据集，每个样本有 10 个特征)这样样本就有了<strong class="lg iu">零均值</strong>和<strong class="lg iu">单位标准差</strong>。</p><p id="5df9" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在该数据集中，位于球半径一半内的<em class="mk">点被标记为<em class="mk">阴性情况</em> (0)，而剩余的点被标记为<em class="mk">阳性情况</em> (1)。</em></p><figure class="nc nd ne nf gt ju"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Loading 10-dimensional ball dataset using DeepReplay</figcaption></figure><h1 id="dfdd" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">朴素初始化方案</h1><p id="8a15" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在<strong class="lg iu">开始的时候，有一个<em class="mk"> sigmoid 激活函数</em>和<strong class="lg iu">随机初始化的权重</strong>。而且训练是<em class="mk">硬</em>，收敛是<em class="mk">慢</em>，成绩是<em class="mk">不好</em>。</strong></p><p id="5869" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">但是，<strong class="lg iu">为什么呢？</strong></p><p id="7848" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">那时，通常的程序是<em class="mk">从一个<strong class="lg iu">正态分布</strong>(零均值，单位标准偏差)中抽取随机值</em>，然后将它们乘以一个小数字，比如说<strong class="lg iu"> 0.01 </strong>。结果是一组标准偏差约为<strong class="lg iu"> 0.01 </strong>的砝码。这导致了一些问题…</p><p id="8ad0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在进入更深的<em class="mk">钻头</em>之前(只是一个<em class="mk">钻头</em>，我保证！)进入为什么这是一个<strong class="lg iu">坏的</strong>初始化方案的数学原因，让我展示在带有<strong class="lg iu"> sigmoid </strong>激活功能的<strong class="lg iu">模块</strong>模型中使用它的结果:</p><figure class="nc nd ne nf gt ju"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Code to build the plots! Just change the initializer and have fun! :-)</figcaption></figure><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/03ebe59f363671d7b3b40218562aa982.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YtDFipeLEa5u9rCIcEcHMg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 1. BLOCK model using sigmoid and naive initialization — don’t try this at home!</figcaption></figure><p id="179b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这个<strong class="lg iu">不</strong>好看吧？你能发现所有正在变坏的东西吗？</p><ul class=""><li id="b89b" class="mo mp it lg b lh mc ll md lp mq lt mr lx ms mb mt mu mv mw bi translated"><strong class="lg iu"> Z 值</strong>(记住，这些是应用激活功能之前的输出<em class="mk">)和<strong class="lg iu">激活</strong>都在<strong class="lg iu">窄范围内；</strong></em></li><li id="d425" class="mo mp it lg b lh mx ll my lp mz lt na lx nb mb mt mu mv mw bi translated"><strong class="lg iu">梯度</strong>几乎为零<strong class="lg iu">；</strong></li><li id="6c29" class="mo mp it lg b lh mx ll my lp mz lt na lx nb mb mt mu mv mw bi translated">左栏的<strong class="lg iu">奇怪的分布</strong>是怎么回事？！</li></ul><p id="d3a7" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">不幸的是，如果我们选择尝试和训练它，这个网络可能不会很快学到很多东西。<strong class="lg iu">为什么是</strong>？因为它的渐变<strong class="lg iu">消失了</strong>！</p><p id="322e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们甚至还没有开始培训过程！这是<strong class="lg iu">纪元 0 </strong>！那么，到目前为止发生了什么？让我们看看:</p><ol class=""><li id="1937" class="mo mp it lg b lh mc ll md lp mq lt mr lx ms mb nw mu mv mw bi translated">使用<strong class="lg iu">简单方案</strong> ( <em class="mk">右上子情节</em>)初始化<strong class="lg iu">权重</strong></li><li id="3ac8" class="mo mp it lg b lh mx ll my lp mz lt na lx nb mb nw mu mv mw bi translated">1000 个样本被用于通过网络的正向传递，并为所有层(输出层不包括在图中)生成了<strong class="lg iu"> Z 值</strong> ( <em class="mk">左上子图</em>)和<strong class="lg iu">激活</strong> ( <em class="mk">左下子图</em>)</li><li id="61ad" class="mo mp it lg b lh mx ll my lp mz lt na lx nb mb nw mu mv mw bi translated">针对通过网络反向传播的和<strong class="lg iu">真实标签<em class="mk">计算<em class="mk">损失</em>，生成所有层的<strong class="lg iu">梯度</strong>(<em class="mk">右下方子图</em></em></strong></li></ol><p id="c13b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">就是这样！<strong class="lg iu">单遍</strong>通过网络！</p><p id="cd51" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">接下来，我们应该<strong class="lg iu">相应地更新权重</strong>，然后<strong class="lg iu">重复</strong>这个过程，对吗？但是，等等…如果<strong class="lg iu">梯度</strong>几乎为零，那么<strong class="lg iu">更新后的权重</strong>将几乎与<strong class="lg iu">相同</strong>，对吗？</p><p id="f2be" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">是什么意思</strong>？这意味着我们的网络几乎毫无用处，因为<strong class="lg iu">它无法在合理的时间内学习任何东西</strong>(即，更新其权重以执行所提议的分类任务)。</p><p id="9cfe" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">欢迎来到<strong class="lg iu">消失渐变</strong>的极端案例！</p><p id="eed5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">你可能会想:“<em class="mk">是的，当然，标准差太低了，不可能像</em>那样工作”。那么，尝试不同的<strong class="lg iu">值</strong>怎么样，比如说，<strong class="lg iu"> 10x </strong>或者<strong class="lg iu"> 100x 大</strong>？</p><h2 id="a30b" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">标准偏差大 10 倍= 0.10</h2><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/d0afcf8366508899241e42a02aa45542.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-GgzN49zpw6IDVY7PfCNyQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 2. BLOCK model using 10x bigger standard deviation</figcaption></figure><p id="317b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">好了，这看起来好一点了… <strong class="lg iu"> Z 值</strong>和<strong class="lg iu">激活</strong>在<em class="mk">合适的范围内</em>，倒数第二个隐藏层的<strong class="lg iu">渐变</strong>显示出一些改善，但是<strong class="lg iu">仍然朝着初始层消失</strong>。</p><p id="ccae" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">也许把<strong class="lg iu">变得更大</strong>可以修正渐变，让我们看看…</p><h2 id="6e70" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">标准偏差大 100 倍= 1.00</h2><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/1d8b4037f04904d337bbd2f7b654b4b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XrQvOkuWdsRRkVDcuDwsWA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 3. BLOCK model using 100x bigger standard deviation</figcaption></figure><p id="77fd" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">好的，看起来我们在<strong class="lg iu">消失渐变</strong>问题中有了<em class="mk"> som </em> e <em class="mk"> progress </em>，因为所有层的范围变得彼此更加相似。耶！<strong class="lg iu">但是</strong> …我们<em class="mk">毁了</em><strong class="lg iu">Z 值</strong>和<strong class="lg iu">激活</strong>……现在<strong class="lg iu"> Z 值</strong>显示出<em class="mk">太宽的范围</em>，迫使<strong class="lg iu">激活</strong>几乎变成<em class="mk">二进制模式</em>。</p><h2 id="88fe" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">尝试不同的激活功能</h2><p id="ef61" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果你读了我关于超参数的第一篇文章，你会记得一个<strong class="lg iu"> Sigmoid 激活函数</strong>有一个基本的问题，就是以 0.5 为中心的<strong class="lg iu">。所以，让我们继续遵循神经网络的进化路径，使用一个<strong class="lg iu"> Tanh 激活函数</strong>来代替！</strong></p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/27fea0c5206948acfa1b50b502e0c355.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XooZz9xPQvzIjKnwO_gAKg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 4. BLOCK model using Tanh and naive initialization</figcaption></figure><p id="d5d7" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">用来自正态分布的小随机值替换<strong class="lg iu"> Tanh </strong>激活函数的<strong class="lg iu"> Sigmoid </strong>，同时保持<strong class="lg iu">初始初始化方案</strong>将我们带到了另一种<strong class="lg iu">消失渐变</strong>的情况(它可能<em class="mk">看起来</em>不<em class="mk">像</em>，毕竟，它们<strong class="lg iu">沿着所有层都是</strong> <strong class="lg iu">相似的</strong>，但是检查一下<strong class="lg iu">比例、</strong>渐变)，伴随着<strong class="lg iu">消失的 Z 值</strong>和<strong class="lg iu">消失的激活</strong>(只是为了明确，这两个是<em class="mk">而不是</em>真正的术语)！肯定，<strong class="lg iu">不是</strong>该走的路！</p><p id="bdfe" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">让我们一直使用一个<strong class="lg iu">大的</strong>标准差，然后看看结果如何(是的，我把最好的留到最后……)。</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/a02de599c2404cc7b344275750fee52c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RAjPZ0X-mkv5dGNl-iK3Ww.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 5. BLOCK model using Tanh and a BIG standard deviation</figcaption></figure><p id="9136" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在这个设置中，我们可以观察到<strong class="lg iu">爆炸渐变</strong>的问题。看到渐变值如何随着我们从最后一个隐藏层到第一个隐藏层越来越大了吗？此外，就像使用 Sigmoid 激活函数时发生的一样，<strong class="lg iu"> Z 值</strong>具有<em class="mk">太宽的范围</em>和<strong class="lg iu">激活</strong>在大多数情况下都崩溃为<em class="mk">零或一个</em>。还是那句话，不好！</p><p id="26cb" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">并且，如承诺的那样，获胜者是… <strong class="lg iu"> Tanh </strong>标准差<strong class="lg iu"> 0.10 </strong>！</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/5cdf9e6652d1f0b0c5a4f186936fb722.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zm_mIPNkgbvlHRx7EZSqLQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 6. BLOCK model looking good!</figcaption></figure><p id="90a1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">为什么<strong class="lg iu">这个</strong>一个是赢家？让我们来看看它的特点:</p><ul class=""><li id="f8bc" class="mo mp it lg b lh mc ll md lp mq lt mr lx ms mb mt mu mv mw bi translated">首先，<strong class="lg iu">梯度</strong>沿着所有层合理地<strong class="lg iu">相似</strong> <strong class="lg iu">(并且在<em class="mk">适当的比例</em>——大约比权重<em class="mk">小 20 倍</em>)</strong></li><li id="8bf9" class="mo mp it lg b lh mx ll my lp mz lt na lx nb mb mt mu mv mw bi translated">第二，<strong class="lg iu"> Z 值</strong>在<em class="mk">合适的范围</em> (-1，1)内，并且沿着所有层相当<strong class="lg iu">相似(尽管一些收缩是明显的)</strong></li><li id="2b50" class="mo mp it lg b lh mx ll my lp mz lt na lx nb mb mt mu mv mw bi translated">第三，<strong class="lg iu">激活</strong>没有<em class="mk">折叠成二进制模式</em>，并且沿着所有层合理地<strong class="lg iu">相似(再次，<em class="mk">有一些</em>收缩)</strong></li></ul><p id="66d1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果你还没有注意到，在所有层上<strong class="lg iu">相似是一件大事！简而言之，这意味着我们可以<strong class="lg iu">在网络的末端堆叠另一层</strong>，并期待类似的<strong class="lg iu">分布</strong>的<strong class="lg iu"> Z 值</strong>、<strong class="lg iu">激活</strong>，当然还有<strong class="lg iu">梯度</strong>。在我们的网络中，我们绝对做<strong class="lg iu">而不是</strong>像<em class="mk">折叠</em>、<em class="mk">消失</em>或<em class="mk">爆炸</em>的行为，不，先生！</strong></p><p id="7432" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">但是，所有层的<strong class="lg iu">相似是结果，而不是原因……正如你可能已经猜到的，关键<strong class="lg iu">是权重</strong>的<strong class="lg iu">标准偏差！</strong></strong></p><p id="2141" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">因此，我们需要一个<strong class="lg iu">初始化方案</strong>，它使用<strong class="lg iu">最佳可能标准偏差</strong>来抽取随机权重！进入<em class="mk">泽维尔·格洛特</em>和<em class="mk">约舒阿·本吉奥</em> …</p><h1 id="0aa9" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">Xavier / Glorot 初始化方案</h1><p id="11f8" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Glorot 和 Bengio 设计了一个<strong class="lg iu">初始化方案</strong>，试图保持所有<em class="mk">获胜特征</em>被列出，即<strong class="lg iu">渐变</strong>、<strong class="lg iu"> Z 值</strong>和<strong class="lg iu">激活</strong>、<em class="mk">沿着所有层</em>相似。另一种说法是:保持<strong class="lg iu">所有层</strong>的变化相似。</p><p id="75d8" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">你可能会问，他们是怎么把这个拉出来的？一会儿我们会看到，我们只需要做一个<strong class="lg iu">真正的</strong> <strong class="lg iu">简要的</strong>回顾一下<strong class="lg iu">方差</strong>的一个基本性质。</p><h2 id="5d0f" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">非常简要地回顾一下</h2><p id="2bc1" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">假设我们有<strong class="lg iu"> <em class="mk"> x </em> </strong>值(来自前一层的<strong class="lg iu">输入</strong>或<strong class="lg iu">激活值</strong>)和<strong class="lg iu"> <em class="mk"> W </em> </strong>权重。两个独立变量的<a class="ae kf" href="https://en.wikipedia.org/wiki/Variance#Product_of_independent_variables" rel="noopener ugc nofollow" target="_blank">乘积的方差由以下公式给出:</a></p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e3f6b5ffcfe7765e07e11d9acbdb63f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*ymtKF1gZUPrl-DTTMJM-Hw.png"/></div></figure><p id="2eff" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">那么，我们假设<strong class="lg iu"> <em class="mk"> x </em> </strong>和<strong class="lg iu"> <em class="mk"> W </em> </strong>都有<strong class="lg iu">零均值</strong>。上面的表达式变成了两个方差<strong class="lg iu"> <em class="mk"> x </em> </strong>和<strong class="lg iu"> <em class="mk"> W </em> </strong>的简单乘积。</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/f6acaaa795ed74e95304de7ea90c426e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*z-H7y4IYE2atp5WAo1Trlg.png"/></div></figure><p id="e35d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这里有两点很重要:</p><ol class=""><li id="aa77" class="mo mp it lg b lh mc ll md lp mq lt mr lx ms mb nw mu mv mw bi translated"><strong class="lg iu">输入</strong>应该有<strong class="lg iu">零均值</strong>以保持在第一层，因此<em class="mk">总是</em>缩放和居中您的输入！</li><li id="8a6b" class="mo mp it lg b lh mx ll my lp mz lt na lx nb mb nw mu mv mw bi translated"><strong class="lg iu"> Sigmoid 激活函数</strong>对此提出了一个问题，因为激活值的平均值为 0.5，<strong class="lg iu">不是</strong> <strong class="lg iu">零</strong>！关于如何补偿的更多细节，请查看这个<a class="ae kf" href="https://mnsgrg.com/2017/12/21/xavier-initialization/#logistic-activation" rel="noopener ugc nofollow" target="_blank">帖子</a>。</li></ol><p id="be0f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">鉴于第二点，坚持使用<strong class="lg iu"> Tanh </strong>才有意义，对吗？所以，这正是我们要做的！现在，是时候将这些知识应用到一个<strong class="lg iu">小例子</strong>中了，所以我们到了(希望如此！)得出与<em class="mk"> Glorot </em>和<em class="mk"> Bengio </em>相同的结论。</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/3c8c44734d852a1c33395dcdb746455e.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*M-ScBnXLxVXs4OnXfRwoRg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 7. Two hidden layers of a network</figcaption></figure><h2 id="3f1c" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">微小的例子</h2><p id="fee8" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这个例子由两个完全连接的隐藏层组成，<strong class="lg iu"> X </strong>和<strong class="lg iu"> Y，</strong>(我把通常的约定抛到九霄云外，以保持数学符号最少！).</p><p id="ccf9" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们只关心连接这两层的<strong class="lg iu">权重</strong>以及<strong class="lg iu">激活</strong>和<strong class="lg iu">渐变</strong>的<strong class="lg iu">差异</strong>。</p><p id="0db5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">对于<strong class="lg iu">激活</strong>，我们需要在网络中通过一个<strong class="lg iu">转发通道</strong>。对于<strong class="lg iu">渐变</strong>，我们需要<strong class="lg iu">反向传播</strong>。</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/fad105931738ac8b81647d2eedd8cfe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*SeCBB7lfA7KPJ-T1Mi7GRg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 8. Tanh activation function</figcaption></figure><p id="abcd" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">并且，为了<em class="mk">保持数学简单</em>，我们将假设<strong class="lg iu">激活函数在等式中是线性的</strong>(而不是<strong class="lg iu"> Tanh </strong>)，这意味着<strong class="lg iu">激活值</strong>是与 Z 值相同的<strong class="lg iu">。</strong></p><p id="6e65" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">虽然这看起来有点夸张，但是图 8 向我们展示了在区间[-1，1] 中，Tanh 大致是线性的，所以结果应该成立，至少在这个区间内。</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/e25fb934c1a69501db3598b03d05140b.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*ljhK3TSC4cESFbqNvo7s1w.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 9. Forward Pass</figcaption></figure><h2 id="b47c" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">前进传球</h2><p id="6042" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">因此，不使用矢量化方法，我们将挑出<strong class="lg iu">一个</strong>单元、<strong class="lg iu"> <em class="mk"> y1 </em> </strong>，并计算它。</p><p id="5ae8" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">图 9 </strong>提供了所涉及零件的清晰图片，即:</p><ul class=""><li id="8571" class="mo mp it lg b lh mc ll md lp mq lt mr lx ms mb mt mu mv mw bi translated">三个单元在<strong class="lg iu">前一层</strong> ( <strong class="lg iu">扇入</strong>)</li><li id="f8ed" class="mo mp it lg b lh mx ll my lp mz lt na lx nb mb mt mu mv mw bi translated"><strong class="lg iu">重量</strong> ( <em class="mk"> w11 </em>、<em class="mk"> w21 </em>和<em class="mk"> w31 </em>)</li><li id="0eba" class="mo mp it lg b lh mx ll my lp mz lt na lx nb mb mt mu mv mw bi translated">我们要计算的单位<strong class="lg iu">方差</strong>为，<strong class="lg iu">y1T79】</strong></li></ul><p id="6beb" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">假设<strong class="lg iu"> <em class="mk"> x </em> </strong>和<strong class="lg iu"> W </strong>是<strong class="lg iu">独立</strong>和<strong class="lg iu">同分布</strong>，我们可以对<strong class="lg iu"> <em class="mk"> y1 </em> : </strong>的<strong class="lg iu">方差</strong>进行一些简单的数学运算</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/141d31645ca699b4d74ca87fb5d197fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*BuTkUhX7RMB285PTHbvLFQ.png"/></div></figure><p id="51b0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">还记得关于<strong class="lg iu">差异</strong>的<em class="mk">简要回顾</em>吗？是时候好好利用它了！</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/9b515a87c075de836ab16fb9c74e9c11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*hPRBN8tvvXhJIvNhv7j_mw.png"/></div></figure><p id="edc7" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">好了，快到了！记住，我们的<strong class="lg iu">目标</strong>是保持<strong class="lg iu">方差沿着所有层</strong>相似。换句话说，我们应该致力于使<strong class="lg iu"><em class="mk"/></strong>x 的<strong class="lg iu">方差</strong>与<strong class="lg iu"><em class="mk"/></strong>y 的方差相同。</p><p id="d289" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">对于我们的单个单元，<strong class="lg iu"> <em class="mk"> y1 </em> </strong>，这可以通过选择其连接<strong class="lg iu">权重</strong>的<strong class="lg iu">方差</strong>为:</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c360c1a777b91832a41daafe84de2a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*S4Mu530arS0GWRkCrXfajw.png"/></div></figure><p id="2de7" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">并且，对于<strong class="lg iu">所有</strong>隐藏层<strong class="lg iu"> X </strong>和<strong class="lg iu"> Y </strong>之间的连接<strong class="lg iu">权重</strong>，我们有:</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/8a91bfa84d4e664ac378e6a66669ed63.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*eYT8rAK3OEK_SAl6mDdsUw.png"/></div></figure><p id="2130" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">顺便说一下，如果我们从一个<strong class="lg iu">正态</strong> <strong class="lg iu">分布</strong>中抽取随机<strong class="lg iu">权重</strong>，这就是要使用的<strong class="lg iu">方差</strong>！</p><p id="7f93" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果我们想用一个<strong class="lg iu">均匀分布</strong>怎么办？我们只需计算(对称的)下限和上限<strong class="lg iu"/>，如下所示:</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi of"><img src="../Images/8957bb7607e07be3883526bba2244edd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*aUJU8I0kVvxBIUb7uJ7ZiA.png"/></div></figure><p id="95ae" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">完事了吗？！还没有…不要忘了<strong class="lg iu">反向传播</strong>，我们也想保持<strong class="lg iu">的渐变沿着所有的层</strong>(它的方差，更精确)。</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c95bff0b34a68684da8e626d49287194.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*fAXXm98mVKpOmThg9rCPjQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 10. Backward Pass</figcaption></figure><h2 id="c132" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">反向传递(反向传播)</h2><p id="cddf" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">还是那句话，我们挑出<strong class="lg iu">一个</strong>单位，<strong class="lg iu"> <em class="mk"> x1 </em> </strong>，用于后向传递。</p><p id="e6e5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">图 10 </strong>提供了所涉及零件的清晰图片，即:</p><ul class=""><li id="5f6c" class="mo mp it lg b lh mc ll md lp mq lt mr lx ms mb mt mu mv mw bi translated"><strong class="lg iu">以下五层</strong> ( <strong class="lg iu">扇出</strong>)</li><li id="e3ac" class="mo mp it lg b lh mx ll my lp mz lt na lx nb mb mt mu mv mw bi translated"><strong class="lg iu">重量</strong> ( <em class="mk"> w11 </em>、<em class="mk"> w12 </em>、<em class="mk"> w13 </em>、<em class="mk"> w14 </em>和<em class="mk"> w15 </em>)</li><li id="11e7" class="mo mp it lg b lh mx ll my lp mz lt na lx nb mb mt mu mv mw bi translated">我们要计算<strong class="lg iu">梯度</strong>相对于它的<strong class="lg iu"> <em class="mk"> x1 </em> </strong>的<strong class="lg iu">方差</strong>的单位</li></ul><p id="de2e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">基本上，我们将做出<strong class="lg iu">相同的假设</strong>，并遵循<strong class="lg iu">与正向传递中相同的步骤</strong>。对于<strong class="lg iu">梯度</strong> s 相对于<strong class="lg iu"> <em class="mk"> x1 </em> </strong>的<strong class="lg iu">方差</strong>，我们可以用同样的方法计算出来:</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/b462604a755a03d36d3dd8cea2e85d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7OMqC35U4rrAilbuWJwHaQ.png"/></div></div></figure><p id="e197" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">再次利用我们在<em class="mk">简要回顾</em>中学到的知识:</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/32212efc420ce04f5543e098a6d92b2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*1OW1BhiXuYlGZc_2NNS5xw.png"/></div></figure><p id="5e84" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">并且，为了保持<strong class="lg iu">沿所有层</strong>的梯度方差相似，我们发现其连接<strong class="lg iu">权重</strong>所需的<strong class="lg iu">方差</strong>为:</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/50bb1dbdde13e7b85c16451813e1c284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*uwwhDYhamTkaBk3lHYPZvg.png"/></div></figure><p id="8cdb" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">好了，我们已经走了很长一段路了！中的“<strong class="lg iu">扇出”的逆运算给出了<strong class="lg iu">正向传递</strong>的<strong class="lg iu">权重</strong>的期望<strong class="lg iu">方差</strong>，而“<strong class="lg iu">扇出</strong>的逆运算给出了(<strong class="lg iu"> <em class="mk">同</em> </strong>)的期望<strong class="lg iu">方差</strong>！)<strong class="lg iu">用于<strong class="lg iu">反向传播</strong>的权重</strong>。</strong></p><p id="f6cf" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">但是…如果“<strong class="lg iu">扇入</strong>”和“<strong class="lg iu">扇出</strong>”有<strong class="lg iu">非常不同的</strong>值呢？</p><h2 id="ac8a" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">协调向前和向后传球</h2><p id="e50a" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">无法决定选择哪一个，“<strong class="lg iu">扇入</strong>”或“<strong class="lg iu">扇出</strong>”，来计算你的网络<strong class="lg iu">权重</strong>的<strong class="lg iu">方差</strong>？没问题，就拿<strong class="lg iu">平均值</strong>吧！</p><p id="ca35" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">因此，我们最终得出了<strong class="lg iu">权重</strong>的<strong class="lg iu">方差</strong>的表达式，如<em class="mk"> Glorot </em>和<em class="mk"> Bengio </em>所示，用于<strong class="lg iu">正态分布</strong>:</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/15698f38a07c4b863f7440e2e8df3e0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oADR40XKsqUz3MdGOvaZkg.png"/></div></div></figure><p id="47b2" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">并且，对于<strong class="lg iu">均匀分布</strong>，我们相应地计算极限:</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oj"><img src="../Images/076439866e03adfae0403d44ab7c992f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uHKjF9HH65N4dTSWnECd8A.png"/></div></div></figure><p id="bacd" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">恭喜</strong>！你(重新)发现了<strong class="lg iu"> Xavier / Glorot 初始化方案</strong>！</p><p id="2c93" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">但是，仍然有一个<em class="mk">小的</em>细节，你应该选择一个<strong class="lg iu">正态分布</strong>来提取权重吗…</p><h2 id="ce7f" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">截断正态和 Keras 方差标度</h2><p id="f78a" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">当谈到神经网络的<strong class="lg iu">权重</strong>时，我们希望它们整齐地分布在零附近，甚至更重要的是，我们<strong class="lg iu">不希望有任何<strong class="lg iu">异常值</strong>！所以，我们<strong class="lg iu">截断</strong>它！</strong></p><p id="0def" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">截断是什么意思？只要<strong class="lg iu">去掉</strong>任何比两倍标准差远的值<strong class="lg iu">！因此，如果您使用标准偏差<strong class="lg iu"> 0.1 </strong>，截尾正态分布<strong class="lg iu">将绝对<strong class="lg iu">没有低于-0.2 或高于 0.2 </strong>的值(如图 11 </strong>左侧图<strong class="lg iu">)。</strong></strong></p><p id="2fec" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">事情是这样的，一旦你切掉了正态分布的尾部，剩下的值有一个<strong class="lg iu">稍微低一点的</strong>标准差…准确的说是原始值的 0.87962566103423978。</p><blockquote class="mh mi mj"><p id="a1f2" class="le lf mk lg b lh mc lj lk ll md ln lo ml me lr ls mm mf lv lw mn mg lz ma mb im bi translated">在 Keras 2 . 2 . 0 之前的版本中，截断正态分布的这种差异在<a class="ae kf" href="https://keras.io/initializers/#variancescaling" rel="noopener ugc nofollow" target="_blank">方差缩放</a>初始化器中没有考虑，而这是 Glorot 和 he 初始化器的基础。因此，在更深层次的模型中，基于均匀分布的初始化器可能会比它的正常对应物表现得更好，正常对应物会遭受一层又一层缓慢缩小的方差…</p></blockquote><p id="77d9" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">到今天为止，这不再是一个问题，我们可以在图 11 的右侧图<strong class="lg iu">中观察到补偿</strong>截断的<strong class="lg iu">效果，其中<strong class="lg iu">方差缩放</strong>初始值的分布明显更宽。</strong></p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/4dfbbedc9a1254eabeaefcb1ea470eb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qsOjnuKya_emoe97P7Hwrw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 11. Truncated normal and Keras’ Variance Scaling</figcaption></figure><h1 id="c76a" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">求一些剧情！</h1><p id="918d" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">非常感谢你陪我看完了更多的<em class="mk">数学</em>部分。你的耐心将会得到丰厚的回报！</p><p id="c28e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">让我们看看<strong class="lg iu"> Glorot 初始化器</strong>(正如它在<strong class="lg iu"> Keras </strong>中被调用的)如何执行，使用<strong class="lg iu">正常</strong>和<strong class="lg iu">统一</strong>分布。</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/aae1f3ffe011d56307a3e98f21548a2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sWf8h_9Yrs0SCMeXr85wkA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 12. BLOCK model with Glorot Normal initializer</figcaption></figure><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/c4ee9d34519a51eb835e3c0f60d52ef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D6RNdSEtxZze2EKz1P5IqA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 13. BLOCK model with Glorot Uniform initializer</figcaption></figure><p id="30a8" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">看起来我们有两个赢家了！</p><p id="4e14" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">还记得我们之前的<strong class="lg iu">赢家</strong>吗，在<strong class="lg iu">图 6 </strong>中使用<em class="mk">朴素初始化方案</em>标准差<em class="mk"> 0.1 </em>的 BLOCK 模型？结果非常相似，对吧？</p><p id="3c33" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">事实证明，根据<strong class="lg iu"> Glorot 初始化方案</strong>，当“<strong class="lg iu">扇入</strong>”和“<strong class="lg iu">扇出</strong>”等于 100 时，我们使用的 0.1 标准偏差正是正确的值。这是<strong class="lg iu">而不是</strong>使用截尾正态分布，虽然…</p><p id="33cc" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">所以，这个初始化方案解决了我们的<em class="mk">消失</em>和<em class="mk">爆发</em>渐变的问题…但是它是否适用于<strong class="lg iu">不同于<strong class="lg iu"> Tanh </strong>的激活函数</strong>？让我想想…</p><h1 id="acb1" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">整流线性单元(ReLU)激活功能</h1><p id="4ab1" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们能坚持使用<strong class="lg iu">相同的</strong>初始化方案，而使用<strong class="lg iu"> ReLU </strong>作为激活函数吗？</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/1e091ffb7fda48be63137e62154c5e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FiltrmgR2Ig6yIgaSl20Kg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 14. BLOCK model with ReLU and Glorot Normal initializer — they don’t mix well…</figcaption></figure><p id="079f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">答案是:<strong class="lg iu">没有</strong>！</p><p id="dac9" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">回到起点…我们需要一个新的改进的初始化方案。进入<em class="mk">何</em>等人，带着他们的“<a class="ae kf" href="https://arxiv.org/pdf/1502.01852.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">钻研整流器</strong> </a>”论文…</p><h1 id="2b68" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">初始化方案</h1><p id="8cc0" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">幸运的是，我们在(重新)发现<strong class="lg iu"> Glorot 初始化方案</strong>时得到的一切仍然有效。只有一个<strong class="lg iu">微调</strong>我们需要做… <strong class="lg iu">将权重的方差乘以 2 </strong>！真的，这就是全部的代价！</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/b6816ae8239c8e7e293f5954921cc0cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*piSRCnAIA2paTMd8kVDfKg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 15. ReLU activation function</figcaption></figure><p id="19cd" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">很简单，对吧？但是，<strong class="lg iu">为什么是</strong>？</p><p id="e03c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">原因也很简单:<strong class="lg iu"> ReLU </strong>将<strong class="lg iu"> Z 值</strong>(负一)的<strong class="lg iu">一半</strong>变成<strong class="lg iu">零</strong>，有效地去除了<strong class="lg iu">差异</strong>的大约<strong class="lg iu">一半</strong>。因此，我们需要将权重的方差加倍来补偿它。</p><p id="1f45" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">既然我们知道<strong class="lg iu"> Glorot 初始化方案</strong>保留了<strong class="lg iu">方差</strong> ( <strong class="lg iu"> 1 </strong>)，那么<strong class="lg iu">如何补偿</strong>对于<strong class="lg iu"> ReLU </strong> ( <strong class="lg iu"> 2 </strong>)的<strong class="lg iu">方差减半</strong>效应？结果(<strong class="lg iu"> 3 </strong>)不出所料，是<strong class="lg iu">将方差</strong>翻倍。</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ok"><img src="../Images/08ae90e3f57a3e5f0aa76619b88a511e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7aKXYL4-xBYaUfDkehm5zw.png"/></div></div></figure><p id="438e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">因此，与<strong class="lg iu">正态分布</strong>一起使用的<strong class="lg iu">权重</strong>的<strong class="lg iu">方差</strong>的表达式为:</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/bf17a65b736336b5c30f6a77eceadde5.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*0j2957jFfzJ3Dw6LFukmRA.png"/></div></figure><p id="5f89" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">并且，对于<strong class="lg iu">均匀分布</strong>，我们相应地计算极限:</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div class="gh gi om"><img src="../Images/c76fbbcec9d87cc704fe30fe88ff6481.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*xxPDyUDztxWWitj48K2t2w.png"/></div></figure><p id="28bc" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">恭喜</strong>！你(重新)发现了<strong class="lg iu"> He 初始化方案</strong>！</p><p id="4cf9" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">但是</strong> …那么<strong class="lg iu">反向传播</strong>呢？难道我们不应该再次使用两个“<em class="mk">粉丝</em>的平均值吗？实际上，没有<strong class="lg iu">也不需要</strong>了。<em class="mk">何</em>等人在他们的论文中指出，对于常见的网络设计，如果初始化方案在<strong class="lg iu"> <em class="mk"> </em> </strong> <em class="mk">正向传递</em>期间缩放激活值，那么对于<em class="mk">反向传播</em> <strong class="lg iu">以及</strong>也是如此！此外，它以两种方式工作<strong class="lg iu">，所以我们甚至可以使用<strong class="lg iu">扇出</strong>，而不是<strong class="lg iu">扇入</strong>。</strong></p><p id="a290" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在，是更多剧情的时候了！</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/da242fee90fbc90018dd483ddc8dec06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*msO3sXXsnPwPCrkiDXD-bQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 15. BLOCK model with ReLU and He Normal initializer</figcaption></figure><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/415a01eeb1c14c8c8857eeb7e86dd951.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HDVmIEaiWBlyZFDG67HKzg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 16. BLOCK model with ReLU and He Uniform initializer</figcaption></figure><p id="740f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">再来两个<strong class="lg iu">获奖者</strong>！当谈到 Z 值的分布时，它们看起来非常相似！至于<strong class="lg iu">渐变</strong>，它们现在看起来比我们用<strong class="lg iu">Tanh</strong>/<strong class="lg iu">Glorot</strong>duo 的时候多了一点<em class="mk">vanish</em>…这是否意味着<strong class="lg iu"> Tanh </strong> / <strong class="lg iu"> Glorot </strong>比<strong class="lg iu"> ReLU </strong> / <strong class="lg iu"> He </strong>更好？我们<em class="mk">知道</em>这是<em class="mk">不是</em>真的…</p><p id="396a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">但是，那么，<em class="mk">为什么</em>它的<strong class="lg iu">渐变</strong>在<strong class="lg iu">图 16 </strong>上看起来不那么好看呢？嗯，再一次，别忘了看一下<strong class="lg iu">刻度</strong>！即使梯度<strong class="lg iu">的<strong class="lg iu">方差</strong>随着我们通过网络<em class="mk">反向传播</em>而减少</strong>，其值也<strong class="lg iu">无处</strong>靠近的<strong class="lg iu">消失(如果你记得<em class="mk">图 4 </em>，情况正好相反——沿层的方差<em class="mk">相似，但它<strong class="lg iu">).</strong></em></strong></p><p id="efa7" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">因此，我们不仅需要沿着所有层的<strong class="lg iu">相似的变化，还需要一个合适的<strong class="lg iu">渐变比例</strong>。<strong class="lg iu">比例</strong>非常重要，因为它将与<em class="mk">学习率</em>一起定义<em class="mk">多快</em>更新<strong class="lg iu">权重</strong><strong class="lg iu">更新</strong>。如果坡度<strong class="lg iu">太小</strong>，则<em class="mk">学习</em>(即<em class="mk">权重</em>的<em class="mk">更新</em>)将<strong class="lg iu">极慢</strong>。</strong></p><p id="cae5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">你会问，<strong class="lg iu">有多小才算太小</strong>？一如既往，它<em class="mk">取决于</em> …重量<strong class="lg iu">的<strong class="lg iu">大小</strong></strong>。所以，过小是<em class="mk">不是</em>绝对<em class="mk">度量，而是<strong class="lg iu">相对</strong>度量。</em></p><p id="8e54" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果我们计算梯度的<strong class="lg iu">方差和相应<strong class="lg iu">权重</strong>的<strong class="lg iu">方差</strong>之间的<strong class="lg iu">比率</strong>(或其<em class="mk">标准差</em>，就可以大致<em class="mk">比较</em>不同<strong class="lg iu">初始化方案</strong>的<em class="mk">学习速度</em>及其底层<strong class="lg iu">分布</strong>(假设学习速率不变)。</strong></p><p id="7560" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">所以，是时候…</p><h1 id="7ac7" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">摊牌——普通对制服，格洛特对贺！</h1><p id="559e" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">说实话，<strong class="lg iu">Glorot</strong><em class="mk">vs</em><strong class="lg iu">He</strong>其实就是指<strong class="lg iu">Tanh</strong><em class="mk">vs</em><strong class="lg iu">ReLU</strong>这场比赛的答案我们都知道(<em class="mk">剧透预警</em>！):<strong class="lg iu"> ReLU 胜</strong>！</p><p id="240f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">而<strong class="lg iu">普通</strong> <em class="mk"> vs </em> <strong class="lg iu">制服</strong>呢？让我们来看看下面的情节:</p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi on"><img src="../Images/f760ff9c52adeb5edfb7ce48377d252a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WLUL_bcjsNK9sXNw6nC-cg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 17. How big are the gradients, after all?</figcaption></figure><p id="1dbb" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">而<strong class="lg iu">赢家</strong>是……<strong class="lg iu">制服</strong>！很明显，至少对于我们的特定块模型和输入，使用<em class="mk">均匀分布</em>比使用<em class="mk">正态分布</em>产生相对更大的<strong class="lg iu"/><strong class="lg iu">梯度</strong>。</p><p id="388a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">此外，正如所料，使用<em class="mk"> ReLU </em>比使用<em class="mk"> Tanh </em>产生相对更大的梯度<strong class="lg iu">。对于我们的特定示例，这对于第一层<em class="mk">不成立，因为其在</em></strong>中的<strong class="lg iu">扇形仅为<strong class="lg iu"> 10 </strong>(输入的尺寸)。如果我们使用<em class="mk"> 100 维</em>输入，那么<em class="mk"> ReLU </em>的<strong class="lg iu">梯度</strong>也会比该层的<strong class="lg iu">大</strong>。</strong></p><p id="d777" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">而且，即使看起来<em class="mk">像<strong class="lg iu">一样</strong>有点"<em class="mk">消失</em>"当使用<em class="mk"> ReLU </em>时，只要看一下<strong class="lg iu">图 17 </strong>最右边的紫色小条…我将<em class="mk">简单初始化的</em>和<em class="mk"> Sigmoid 激活的</em>网络滑入了情节</em></p><figure class="nc nd ne nf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oo"><img src="../Images/1f724f6708a58d54a95766b65aea37e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WrZecC13bJxIk4jDgi737g.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Ratio: standard deviation of gradients over standard deviation of weights</figcaption></figure><p id="8075" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">综上所述，对于一个<strong class="lg iu"> ReLU </strong>激活的网络，使用<strong class="lg iu">均匀分布</strong>的<strong class="lg iu"> He 初始化方案</strong>是一个<strong class="lg iu">不错的选择</strong>；-)</p><blockquote class="mh mi mj"><p id="f7c3" class="le lf mk lg b lh mc lj lk ll md ln lo ml me lr ls mm mf lv lw mn mg lz ma mb im bi translated">有很多很多方法可以分析选择特定初始化方案的影响…我们可以尝试不同的网络架构(如“漏斗”或“沙漏”形状)，更深的网络，改变标签的分布(因此，损失)…我尝试了很多组合，他/统一总是优于其他初始化方案，但这篇文章已经太长了！</p></blockquote><h1 id="ce07" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">最后的想法</h1><p id="01d0" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这是一篇<em class="mk"> looong </em>的帖子，尤其是对于一个如此理所当然的话题，如<strong class="lg iu">重量</strong> <strong class="lg iu">初始者</strong>！但是我觉得，一个人要真正理解它的重要性，就应该遵循步骤，碰到导致现在使用的方案发展的问题。</p><p id="529c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">尽管作为一名从业者，你知道用于初始化你的网络的"<em class="mk">正确的</em>"组合，我真的希望这篇文章能够给你一些<strong class="lg iu">洞察</strong>真正发生了什么，最重要的是，<strong class="lg iu">为什么</strong> <em class="mk">那个</em>特定组合是"<em class="mk">正确的</em>"一:-)</p><p id="39f1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><em class="mk">如果你有什么想法、评论或者问题，请在下方留言或者联系我</em> <a class="ae kf" href="https://twitter.com/dvgodoy" rel="noopener ugc nofollow" target="_blank"> <em class="mk">推特</em> </a> <em class="mk">。</em></p></div></div>    
</body>
</html>