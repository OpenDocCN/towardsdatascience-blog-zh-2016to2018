<html>
<head>
<title>Teaching Cars To See — Vehicle Detection Using Machine Learning And Computer Vision</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">教汽车看东西——使用机器学习和计算机视觉进行车辆检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/teaching-cars-to-see-vehicle-detection-using-machine-learning-and-computer-vision-54628888079a?source=collection_archive---------4-----------------------#2017-10-25">https://towardsdatascience.com/teaching-cars-to-see-vehicle-detection-using-machine-learning-and-computer-vision-54628888079a?source=collection_archive---------4-----------------------#2017-10-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ad4f4e761ec62353566eaa9ae31299c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ef6QU6hyuyvvqVfl87a1qw.png"/></div></div></figure><p id="17e4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw">这是</em> <a class="ae kx" href="https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013" rel="noopener ugc nofollow" target="_blank"> <em class="kw"> Udacity自动驾驶汽车工程师纳米学位</em> </a> <em class="kw">第一学期的期末项目。你可以在</em><a class="ae kx" href="https://github.com/kenshiro-o/CarND-Vehicle-Detection" rel="noopener ugc nofollow" target="_blank"><em class="kw">github</em></a><em class="kw">上找到与这个项目相关的所有代码。你也可以阅读我以前项目的帖子:</em></p><ul class=""><li id="e13e" class="ky kz iq ka b kb kc kf kg kj la kn lb kr lc kv ld le lf lg bi translated"><em class="kw">项目1: </em> <a class="ae kx" href="https://medium.com/computer-car/udacity-self-driving-car-nanodegree-project-1-finding-lane-lines-9cd6a846c58c" rel="noopener"> <em class="kw">利用计算机视觉检测车道线</em> </a></li><li id="42dd" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated"><em class="kw">项目二:</em> <a class="ae kx" href="https://medium.com/towards-data-science/recognizing-traffic-signs-with-over-98-accuracy-using-deep-learning-86737aedc2ab" rel="noopener"> <em class="kw">交通标志分类使用深度学习</em> </a></li><li id="5ed6" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated"><em class="kw">项目三:</em> <a class="ae kx" href="https://medium.com/towards-data-science/teaching-cars-to-drive-using-deep-learning-steering-angle-prediction-5773154608f2" rel="noopener"> <em class="kw">转向角度预测利用深度学习</em> </a></li><li id="9241" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated"><em class="kw">项目4: </em> <a class="ae kx" href="https://medium.com/towards-data-science/teaching-cars-to-see-advanced-lane-detection-using-computer-vision-87a01de0424f" rel="noopener"> <em class="kw">利用计算机视觉进行高级车道检测</em> </a></li></ul></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><p id="0955" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当我们开车时，我们不断关注我们的环境，因为这关系到我们和其他许多人的安全。我们特别注意潜在障碍物的位置，无论是其他汽车、行人还是路上的物体。同样，随着我们开发为自动驾驶汽车提供动力所必需的智能和传感器，这种汽车也能检测障碍物是至关重要的，因为这可以加强汽车对环境的理解。最重要的一种ostacles是检测道路上的其他车辆，因为它们很可能是我们车道或邻近车道上最大的物体，因此构成了潜在的危险。</p><p id="9a81" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从传统的计算机视觉技术到深度学习技术，在整个文献中已经开发了许多障碍检测技术。在本练习中，我们通过采用一种称为<em class="kw">梯度方向直方图(HOG) </em>的传统计算机视觉技术，结合一种称为<em class="kw">支持向量机(SVM) </em>的机器学习算法，来构建一个车辆检测器。</p><h1 id="0b7a" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">资料组</h1><p id="5365" class="pw-post-body-paragraph jy jz iq ka b kb mr kd ke kf ms kh ki kj mt kl km kn mu kp kq kr mv kt ku kv ij bi translated">Udacity慷慨地提供了一个具有以下特征的<em class="kw">平衡</em>数据集:</p><ul class=""><li id="6ab2" class="ky kz iq ka b kb kc kf kg kj la kn lb kr lc kv ld le lf lg bi translated">约9K的车辆图像</li><li id="3e15" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">非车辆的~ 9K图像</li><li id="8850" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">所有图像都是64x64</li></ul><p id="ef42" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">数据集来自<a class="ae kx" href="http://www.gti.ssr.upm.es/data/Vehicle_database.html" rel="noopener ugc nofollow" target="_blank"> GTI车辆图像数据库</a>、<a class="ae kx" href="http://www.cvlibs.net/datasets/kitti/" rel="noopener ugc nofollow" target="_blank"> KITTI Vision基准套件</a>，以及从项目视频本身提取的例子。后者要大得多，没有用于这个项目。然而，这在未来将是一个很好的补充，特别是当我们计划使用深度学习建立一个分类器时。您可以从下面的数据集中看到一个图像示例:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/b3c90c2bec193c83c036850855693008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4YA07rUW8DTBs4ji3mDy0A.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Sample of vehicles and non-vehicles from dataset</figcaption></figure><p id="14ee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以清楚地看到车辆和非车辆图像。非车辆图像往往是道路的其他元素，如沥青、路标或路面。区别非常明显。大多数图像也将车辆显示在中央，但方向不同，这很好。此外，还有各种各样的汽车类型和颜色，以及照明条件。</p><h1 id="fdcb" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">探索特性</h1><h2 id="fab1" class="nf lu iq bd lv ng nh dn lz ni nj dp md kj nk nl mh kn nm nn ml kr no np mp nq bi translated">方向梯度直方图(HOG)</h2><p id="519a" class="pw-post-body-paragraph jy jz iq ka b kb mr kd ke kf ms kh ki kj mt kl km kn mu kp kq kr mv kt ku kv ij bi translated">Navneet Dalal和Bill Triggs在他们的论文《人类检测的方向梯度直方图<a class="ae kx" href="https://hal.inria.fr/inria-00548512/document" rel="noopener ugc nofollow" target="_blank">中展示了令人印象深刻的结果后，使用HOG进行检测受到了欢迎。Satya Mallick在这篇</a><a class="ae kx" href="https://www.learnopencv.com/histogram-of-oriented-gradients/" rel="noopener ugc nofollow" target="_blank">文章</a>中很好地解释了这个算法，对于那些想要更好地掌握HOG的人来说。</p><p id="23b7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们首先在RGB图像上探索了HOG算法中以下值的不同配置:</p><ul class=""><li id="86f1" class="ky kz iq ka b kb kc kf kg kj la kn lb kr lc kv ld le lf lg bi translated">方位数量(用<em class="kw"> o表示)</em></li><li id="847c" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">每个单元格的像素(用<em class="kw"> px/c表示)</em></li></ul><p id="28d6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">每个块的单元最初固定为2(用<em class="kw"> c/bk表示)</em>。下图显示了在RGB格式的样本车辆图像上获得的结果:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/0ba212dd62ee4daedec0ff2d7614d086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mKHugn2b9BY00azFrGcsFw.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Results of different configurations for HOG</figcaption></figure><p id="57b3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从纯粹的观察来看，它看起来像一个猪配置:</p><ul class=""><li id="99c7" class="ky kz iq ka b kb kc kf kg kj la kn lb kr lc kv ld le lf lg bi translated">11个方向</li><li id="da86" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">每个单元格14个像素</li><li id="7c4f" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">每个区块2个单元</li></ul><p id="a1c4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">产生车辆最独特的坡度。我们还没有对每个模块的不同单元进行实验，所以现在让我们来尝试一下。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/20209bdba3d5094d3c89b25cc79a2e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yD8vTD2BlHN0KNDNkPDhAw.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">HOG results with different cells per block</figcaption></figure><p id="3aeb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于人眼来说，我们在视觉上没有注意到明显的差异。理想情况下，我们希望减少特征空间以加快计算速度。我们现在决定每块3个细胞。</p><h2 id="32ac" class="nf lu iq bd lv ng nh dn lz ni nj dp md kj nk nl mh kn nm nn ml kr no np mp nq bi translated">色彩空间</h2><p id="d695" class="pw-post-body-paragraph jy jz iq ka b kb mr kd ke kf ms kh ki kj mt kl km kn mu kp kq kr mv kt ku kv ij bi translated">我们现在必须为我们的配置探索最合适的颜色空间，因为看起来我们在3个RGB通道上的HOG特征太相似了，因此感觉我们没有生成具有足够变化的特征。</p><p id="25da" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们在众多色彩空间中生成以下输出:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/b6ecab310df8e6268abbd44cdf5ffec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Eq1SkPzIojaPaQuTsseCA.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">HOG image across all channels in different color spaces</figcaption></figure><p id="20f2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于某些颜色通道，很难解释HOG的结果。有趣的是，YUV、YCrCb和LAB中的第一个颜色通道似乎足以捕捉我们正在寻找的渐变。在HSV和HLS中，HOG分别在<em class="kw">值</em>和<em class="kw">亮度</em>通道上捕捉车辆的最重要特征。</p><p id="20db" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了证实我们的假设，让我们尝试一个不同的车辆图像:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/576729d54a25dd6b80bd9ae032a7fe42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ja7tlM7I57uzbTJiBSCNsQ.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Same HOG settings but on a different image</figcaption></figure><p id="c5ad" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw">休斯顿，我们这里有一个问题</em> …在如上图这样的暗图像上，我们可以观察到携带最多光信息的信道上的HOG产生了不好的结果。<strong class="ka ir">因此，我们必须考虑所有的颜色通道，以捕捉最多的特征。</strong>最后，我们的配置如下:</p><ul class=""><li id="3ea5" class="ky kz iq ka b kb kc kf kg kj la kn lb kr lc kv ld le lf lg bi translated"><strong class="ka ir">ycr CB色彩空间的所有通道</strong></li><li id="163c" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated"><strong class="ka ir">11</strong>的拱起方向</li><li id="9b0b" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated"><strong class="ka ir">每14个单元的HOG像素</strong></li><li id="9f83" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated"><strong class="ka ir">每块猪细胞2个</strong></li></ul><p id="698d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们还将添加颜色信息来增强我们的功能集。为此，我们只需使用32个箱生成所有颜色通道的直方图，如下所示:</p><pre class="mx my mz na gt nu nv nw nx aw ny bi"><span id="2f68" class="nf lu iq nv b gy nz oa l ob oc">def color_histogram(img, nbins=32, bins_range=(0, 256)):<br/>    """<br/>    Returns the histograms of the color image across all channels, as a concatenanted feature vector<br/>    """<br/>    # Compute the histogram of the color channels separately<br/>    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)<br/>    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)<br/>    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)</span><span id="5d90" class="nf lu iq nv b gy od oa l ob oc">    # Concatenate the histograms into a single feature vector and return it<br/>    return np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))<br/>    </span></pre><h1 id="0156" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">分类者</h1><p id="63b5" class="pw-post-body-paragraph jy jz iq ka b kb mr kd ke kf ms kh ki kj mt kl km kn mu kp kq kr mv kt ku kv ij bi translated">分类器负责将我们提交的图像分为<em class="kw">车辆</em>或<em class="kw">非车辆</em>类别。为此，我们必须采取以下步骤:</p><ul class=""><li id="1949" class="ky kz iq ka b kb kc kf kg kj la kn lb kr lc kv ld le lf lg bi translated">从数据集中加载我们的图像</li><li id="cb9d" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">提取我们想要的特征</li><li id="13c8" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">使这些特征正常化</li><li id="5d38" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">分割用于<em class="kw">训练</em>和<em class="kw">测试</em>的数据集</li><li id="fb28" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">使用适当的参数构建分类器</li><li id="1a9f" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">在<em class="kw">训练</em>数据上训练分类器</li></ul><p id="aff2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">正如上一节所讨论的，我们决定只保留一个特征:在YCrCb图像的Y通道上计算的HOG特征向量。</p><p id="8e7d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们随机分割数据集，留下20%用于测试。此外，我们通过使用一个<a class="ae kx" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank">sk learn . preprocessing . standard scaler</a>normalizer来缩放数据。</p><p id="7ba7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们没有足够的时间对许多分类器进行实验，所以选择使用<em class="kw">支持向量机</em> (SVMs)，因为它们通常与HOG结合用于对象检测问题。此外，我们使用了带有内核的SVC，因为它提供了最好的精度，但是比线性SVC慢。我们接受了这种折衷，因为当我们在一系列图像上测试时，使用rbf核的SVC的检测更强。</p><p id="94cc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用<a class="ae kx" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" rel="noopener ugc nofollow" target="_blank"> GridSearchCV </a>函数获得了核类型(<em class="kw">线性</em>或<em class="kw"> rbf </em>)、<em class="kw"> C </em> ( <em class="kw"> 1，100，1000，1000 </em>)和<em class="kw">伽马</em> ( <em class="kw">自动，0.01，0.1，1 </em>)中的理想参数。最佳配置实现了超过99%的准确度，并具有以下参数:</p><ul class=""><li id="70ea" class="ky kz iq ka b kb kc kf kg kj la kn lb kr lc kv ld le lf lg bi translated"><strong class="ka ir">内核</strong> = rbf</li><li id="07c8" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated"><strong class="ka ir"> C </strong> = 100</li><li id="7059" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated"><strong class="ka ir">伽玛</strong> =自动</li></ul><h1 id="7ba8" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">推拉窗</h1><p id="789b" class="pw-post-body-paragraph jy jz iq ka b kb mr kd ke kf ms kh ki kj mt kl km kn mu kp kq kr mv kt ku kv ij bi translated">我们创建了多个维度的滑动窗口，范围从64x64到256x256像素，以针对分类器测试图像的部分，并仅保留正面预测。我们通常从屏幕底部滑动较大的窗口，因为这将对应于车辆出现最大的位置。较小的窗口会在屏幕上滑得更高。此外，我们能够配置小区重叠，并且当前已经将它设置为1以获得最大覆盖(即，每14像素*比例重叠，其中比例1的最小窗口是64x64)。我们停止尝试检测y方向上任何低于350像素的车辆(即屏幕上图像的较高部分)。下图显示了单元格重叠设置为4的重叠滑动窗口示例:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/7f5c981807bbc3575f5645072f1ce99e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dmo2UFF9fFqCSo1DxethEA.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Sliding windows of different sizes with cell overlap = 4</figcaption></figure><h1 id="e2b3" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">热图和阈值</h1><p id="e874" class="pw-post-body-paragraph jy jz iq ka b kb mr kd ke kf ms kh ki kj mt kl km kn mu kp kq kr mv kt ku kv ij bi translated">分类器有时会误分类图像中实际上不是车辆的部分。为了避免突出显示视频中的车辆，我们利用我们通过多尺寸滑动窗口创建的冗余，并计算我们的分类器在图像的给定部分的所有窗口中预测车辆的次数。我们首先使用<em class="kw">scipy . ndimage . measurements</em>'<a class="ae kx" href="https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.ndimage.measurements.label.html" rel="noopener ugc nofollow" target="_blank">label</a>函数来标记具有重叠窗口的对象。然后，我们通过确定我们检测到的对象能够适合的最小边界框来提取每个标签的位置。我们只保留图像中检测到的阈值被设置为特定值的部分。通过实验，我们发现阈值4足以在项目视频上获得稳定的结果。下图说明了热图和阈值处理的工作原理:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/48adcb3f76780e35b7c7e3b4be33f945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ofuw5YFXd2-Vjfprdu65sQ.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Detected vehicles with heatmaps and thresholding</figcaption></figure><p id="ab21" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第一个迷你热图代表来自分类器的原始原始检测，而第二个显示阈值区域，其中红色的强度随着重叠窗口数量的增加而增加。右边最后一个小图像显示了我们的分类器预测到<em class="kw">车辆</em>的所有窗口。在这个例子中，我们实际上使用了<em class="kw">线性SVC </em>，它比<em class="kw"> rbf SVC更容易预测错误。</em></p><h1 id="bac9" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">帧聚合</h1><p id="140d" class="pw-post-body-paragraph jy jz iq ka b kb mr kd ke kf ms kh ki kj mt kl km kn mu kp kq kr mv kt ku kv ij bi translated">为了进一步加强我们的管道，我们决定每隔<em class="kw"> n </em>帧平滑所有检测到的窗口。为此，我们累积帧(<em class="kw"> n-1)*f+1 </em>到<em class="kw"> n*f </em>之间的所有检测到的窗口，其中<em class="kw"> n </em>是表示我们所在的帧的<em class="kw">组</em>的正标量。我们已经创建了以下封装检测到的对象的类:</p><pre class="mx my mz na gt nu nv nw nx aw ny bi"><span id="2bca" class="nf lu iq nv b gy nz oa l ob oc">class DetectedObject:<br/>    """<br/>    The DetectedObject class encapsulates information about an object identified by our detector<br/>    """<br/>    def __init__(self, bounding_box, img_patch, frame_nb):<br/>        self.bounding_box = bounding_box<br/>        self.img_patch = img_patch <br/>        self.frame_nb = frame_nb<br/>        self.centroid = (int((bounding_box[0][0] + bounding_box[1][0]) / 2), int((bounding_box[0][1] + bounding_box[1][1]) / 2))<br/>        self.similar_objects = []<br/>...</span></pre><p id="06c7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">每当我们在组中的当前帧或下一帧上检测到新的对象时，我们检查我们过去是否检测到类似的对象，如果是，我们附加类似的对象，从而增加该对象在多个帧上的计数。在帧<em class="kw"> n*f </em>处，我们仅保留具有超过<em class="kw"> m </em>个检测计数的检测到的对象(及其相关联的边界框)，从而在流水线中实现某种类型的<em class="kw">双重过滤</em>(第一次过滤是关于重叠边界框的数量的阈值)。</p><p id="e602" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在下面的gif上，你可以看到当我们有一个单一的边界框覆盖两辆汽车时和当每辆汽车都有自己的边界框时之间有一瞬间:帧聚合逻辑必须等到两个窗口出现足够的次数后才显示它们:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi of"><img src="../Images/4a58be4ecfe1e535d13f46d084576583.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*Maq3ZwsL3hAqWV3x-2ATpA.gif"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">gif of vehicle detection with frame sampling</figcaption></figure><h1 id="ddf4" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">决赛成绩</h1><p id="dbeb" class="pw-post-body-paragraph jy jz iq ka b kb mr kd ke kf ms kh ki kj mt kl km kn mu kp kq kr mv kt ku kv ij bi translated">下面的视频链接显示了对车辆的成功检测。由于这是第一学期的最后一个项目，我刚刚<em class="kw">让</em>使用Tron Legacy的OST的音轨<em class="kw">片尾标题</em>作为背景音乐——没有什么比这更合适的了😎。像往常一样享受吧！</p><figure class="mx my mz na gt jr"><div class="bz fp l di"><div class="og oh l"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Video montage of vehicle detection</figcaption></figure><h1 id="634d" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">丰富</h1><p id="3e2c" class="pw-post-body-paragraph jy jz iq ka b kb mr kd ke kf ms kh ki kj mt kl km kn mu kp kq kr mv kt ku kv ij bi translated">这是一个棘手的项目，特别是对于那些选择更传统的计算机视觉和机器学习方法而不是深度学习的人来说。以下步骤相当耗时:</p><ul class=""><li id="3713" class="ky kz iq ka b kb kc kf kg kj la kn lb kr lc kv ld le lf lg bi translated">确定最合适的特征(HOG、图像颜色直方图等)</li><li id="cbd8" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">探索HOG参数+色彩空间的组合</li><li id="684a" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">应用网格搜索寻找最合适的分类器</li></ul><p id="849f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，在我们的渠道中，我们还面临以下问题:</p><ul class=""><li id="86e2" class="ky kz iq ka b kb kc kf kg kj la kn lb kr lc kv ld le lf lg bi translated">确定滑动窗口和重叠的正确位置</li><li id="e5f9" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">为重叠检测识别合适的<em class="kw">阈值</em></li><li id="ce44" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">采用合适的帧采样率</li><li id="1f6f" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">在多个帧中找到足够好的最小检测计数</li><li id="c0f2" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">聚集重叠检测的组合窗口维度</li></ul><p id="5cb2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于不是车辆但被分类器检测到的对象，流水线将失败，并且在足够多的重叠窗口上出现这种错误检测以突破所配置的阈值，并且在每组的最小数量的帧上始终如此。绘制的边界框并不总是完全适合车辆，并且每隔n帧重画一次，因此造成缺乏平滑度的印象。此外，与每n个帧进行批量聚合相反，可以通过使用n个帧的滚动窗口来改进帧聚合。</p><p id="f6a6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后一个问题是我们的流水线<em class="kw">太慢</em>。我们不应该在整个屏幕上滑动窗口，只需要查看屏幕的一部分:例如，将来我们可以使用决策树来识别感兴趣的区域。我们还可以考虑减少滑动窗口的数量，以及采用像<em class="kw"> LinearSVC </em>这样的更快的分类器来加速检测(但准确性也会显著下降)。尽管如此，这种车辆检测管道不太可能实时工作。</p><p id="ce05" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在未来，将采用深度学习方法，例如使用<a class="ae kx" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank">更快的R-CNN </a>或<a class="ae kx" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank"> YOLO </a>架构，因为这些架构现在是检测问题的最先进技术，并且可以实时运行。然而，这是一个有价值的练习，可以更好地理解传统的机器学习技术，并在特征选择上建立直觉。此外，像HOG这样的技术的美丽和简单给我留下了深刻的印象，它仍然能够产生可靠的结果。</p><h1 id="42c3" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">感谢</h1><p id="4962" class="pw-post-body-paragraph jy jz iq ka b kb mr kd ke kf ms kh ki kj mt kl km kn mu kp kq kr mv kt ku kv ij bi translated">我要再次感谢我的导师迪伦，感谢他在这一学期对我的支持和建议。我也非常感谢Udacity设立了这样一个令人兴奋和具有挑战性的纳米学位，有着伟大的项目和优秀的材料。</p><p id="2235" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们站在巨人的肩膀上，因此我感谢人工智能、计算机视觉等领域的所有研究人员和爱好者，感谢他们通过论文和代码所做的工作和分享。没有这些资源，我就无法“借用”他们的想法和技术，并成功完成这个项目。</p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><p id="79e1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第一学期已经结束了🎉🎉。我将于2018年1月开始第二学期，与此同时，我将致力于深化我刚刚起步的人工智能技能，并恢复兼职项目的工作。感谢阅读，敬请关注！</p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><p id="dcca" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">感谢你阅读这篇文章。希望你觉得有用。我现在正在建立一个新的创业公司，叫做 <a class="ae kx" href="https://envsion.io" rel="noopener ugc nofollow" target="_blank"> <em class="kw"> EnVsion </em> </a> <em class="kw">！在EnVsion，我们正在为UX的研究人员和产品团队创建一个中央存储库，以从他们的用户采访视频中挖掘见解。当然我们用人工智能来做这个。).</em></p><p id="cbaa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw">如果你是一名UX研究员或产品经理，对与用户和客户的视频通话感到不知所措，那么EnVsion就是为你准备的！</em></p><p id="ea69" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw">你也可以关注我的</em> <a class="ae kx" href="https://twitter.com/Ed_Forson" rel="noopener ugc nofollow" target="_blank"> <em class="kw">推特</em> </a> <em class="kw">。</em></p></div></div>    
</body>
</html>