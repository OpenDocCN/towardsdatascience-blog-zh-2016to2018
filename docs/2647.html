<html>
<head>
<title>Online Learning with Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归在线学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/online-training-with-linear-regression-35645007e980?source=collection_archive---------4-----------------------#2018-02-16">https://towardsdatascience.com/online-training-with-linear-regression-35645007e980?source=collection_archive---------4-----------------------#2018-02-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3d06" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用 Python 实现线性回归和梯度下降在线学习的简单方法。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/7214f8cfdab31ae9becf82633be0a542.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_gg1Te-7SJfk9E2D-mORfw.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Source <a class="ae lb" href="http://tomtunguz.com/key-ingredient-machine-learning/" rel="noopener ugc nofollow" target="_blank">http://tomtunguz.com/key-ingredient-machine-learning</a></figcaption></figure></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="6178" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi lj translated">你有没有想过，机器学习应用程序如何在少数情况下实时给出结果？机器学习从业者如何在容纳新样本/或训练数据的同时，一次又一次地训练大数据？如果是的话，这里有一个幼稚的指南。我将解释一些关于我们如何使用 Python 实现在线学习机器学习算法的基本概念。</p><p id="dc54" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">下面是 python 中源代码的 Github 链接和一个样本数据集，按照 Coursera 吴恩达机器学习课程给出的问题集</strong>。</p><div class="ls lt gp gr lu lv"><a href="https://github.com/bmonikraj/linear-regression-scratch/blob/master/LinearGD.py" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">bmonikraj/线性回归-擦除</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">这是一个从头开始线性回归的 Python 代码，带有梯度下降实现和…</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">github.com</p></div></div><div class="me l"><div class="mf l mg mh mi me mj kv lv"/></div></div></a></div><p id="7e3e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大多数机器学习应用程序遵循以下方法。这是一个迭代过程，每一步都可能根据需要执行。这三个基本步骤是</p><ol class=""><li id="de26" class="mk ml iq jp b jq jr ju jv jy mm kc mn kg mo kk mp mq mr ms bi translated">数据收集和预处理</li><li id="32f8" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">数据集上的计算</li><li id="6763" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">可视化和分析</li></ol><p id="6d18" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="my">在本文中，我将主要关注上面提到的第二个子模块——数据集上的</em> <strong class="jp ir"> <em class="my">计算。</em>T9】</strong></p><p id="61a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，为了更好地理解，我将用代码同时解释这些概念。</p><p id="369c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">在线训练</strong> :-这是一种学习步骤的改进方法，其中所用模型的权重根据新到达的数据进行更新，而不是重新训练整个模型。这基本上是一种循序渐进的学习方法。这种训练的应用主要可以在实时应用中找到，实时应用为此目的使用预测或分类模型。</p><pre class="km kn ko kp gt mz na nb nc aw nd bi"><span id="e925" class="ne nf iq na b gy ng nh l ni nj">import pandas as pd<br/>import numpy as np <br/>data = pd.read_csv("path_to_file.csv")<br/>Y = data["Name of the Column"]<br/>del data["Profit"]<br/>bias = pd.Series(1,index=range(len(Y))) <br/>data["Bias"] = bias<br/>Header_X_Bias = list(data.columns.values)<br/>Header_X_Bias = Header_X_Bias[:-1]<br/>Header_X_Bias.insert(0,"Bias")<br/>data = data[Header_X_Bias] <br/>X = np.array(data)<br/>Y = np.array(Y)<br/>Y = Y.reshape(len(Y),1)<br/>Theta = [0,0]<br/>Theta = np.array(Theta)<br/>Theta = Theta.reshape(2,1)</span><span id="b830" class="ne nf iq na b gy nk nh l ni nj">#Here Theta is the global variable</span></pre><p id="77ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用 Pandas 库从文件中读取数据集，使用 NumPy 模块执行线性代数计算。</p><p id="2064" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在导入模块之后，我将数据集读入“数据”变量。从那里，我分离出了数据集的因变量“y”(在我们的例子中是“利润”(更多细节请参考 GitHub 参考资料))，并在 X 集中添加了一个值为 1 的 Bias 列。给定的数据集只有一个要素。</p><blockquote class="nl nm nn"><p id="3e2d" class="jn jo my jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated">y:利润</p><p id="9749" class="jn jo my jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated">X : Xi 是 X 矩阵的第 I 行向量(X 的维数是‘m’X ^ 2 ),其中 Xi 的维数是 2。</p><p id="c55d" class="jn jo my jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated">X[0]:偏置；X[1]:原始单一特征</p></blockquote><p id="be83" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于 numpy 计算，X 和 Y 都以适当的矩阵格式被重新整形，并且初始θ也被声明为值[0，0]，因为特征维数是 2，包括偏差。</p><pre class="km kn ko kp gt mz na nb nc aw nd bi"><span id="1935" class="ne nf iq na b gy ng nh l ni nj">def cost(X,Y,Theta):    <br/>     Hypothesis = np.dot(X,Theta)    <br/>     Error = Y - Hypothesis    <br/>     #Matrix method for calculating Cost    <br/>     Cost = np.dot(Error.T,Error)/(2*len(Error))    <br/>     return Cost[0][0]</span></pre><p id="ceb0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，定义代价函数，它以三个参数作为输入，第一个是 X，第二个是 Y，第三个是θ。在函数内部，它首先计算假设，然后以矢量化的方式计算成本。</p><pre class="km kn ko kp gt mz na nb nc aw nd bi"><span id="8ed4" class="ne nf iq na b gy ng nh l ni nj">Let <br/>m = Total number of training examples<br/>d = Total number of features including bias = 2<br/>Dimension of X : (m,d)<br/>Dimension of Y : (m,1)<br/>Dimension of Theta : (d,1)<br/>Dimension of Loss : (m,1)</span></pre><p id="6beb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="my">注意:总是建议尽可能使用矢量化形式的计算，而不是循环，使用像 numpy 这样的线性代数库，以便大大优化计算时间。</em></p><pre class="km kn ko kp gt mz na nb nc aw nd bi"><span id="9525" class="ne nf iq na b gy ng nh l ni nj">alpha = 0.01<br/>Iterations = 1500</span></pre><p id="fa8c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“alpha”和“迭代”步骤被声明为常量。</p><p id="68fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在算法中，我们把迭代次数作为收敛的决定因素。你也可以把θ和ε阈值的变化作为收敛的决定因素。</p><pre class="km kn ko kp gt mz na nb nc aw nd bi"><span id="a99d" class="ne nf iq na b gy ng nh l ni nj">def gradient(X,Y,Theta,Iterations,alpha):    <br/>     for i in range(Iterations):        <br/>          Loss = Y - np.dot(X,Theta) + (np.dot(Theta.T,Theta)*0.001)<br/>          Cost = cost(X,Y,Theta)        <br/>          Loss = Loss*(-1)        <br/>          dJ = (np.dot(X.T,Loss)*2)/len(Y)<br/>          Theta = Theta - (alpha*dJ)<br/>     return Theta</span></pre><p id="82e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的代码片段是梯度下降函数的定义。它以 X，Y，Theta，迭代次数和 alpha 为参数。</p><p id="3283" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它循环到“迭代”步骤，并且在每一步中，它根据成本值修改它的θ值。在每个循环中，都要计算损耗。</p><blockquote class="nl nm nn"><p id="31b6" class="jn jo my jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated">损失= Y 假设</p><p id="4d80" class="jn jo my jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated">(np.dot(Theta。t，Theta)*0.001) =这是正则项</p><p id="50b1" class="jn jo my jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated">假设=点积(X，θ)</p></blockquote><p id="3c39" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">计算损失后，在每个循环中，每个参数的偏导数(对应于特征的θ值)以矢量化格式计算如下</p><blockquote class="nl nm nn"><p id="ec43" class="jn jo my jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated">(2 *点积(X.T，损失))/m</p></blockquote><p id="f7a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后θ被更新为</p><blockquote class="nl nm nn"><p id="2af8" class="jn jo my jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated">θ=θ-(α*偏导数)</p></blockquote><p id="4f3a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上述类似的过程中，θ是更新“迭代”次数，梯度函数返回最终更新的θ。</p><pre class="km kn ko kp gt mz na nb nc aw nd bi"><span id="9bd0" class="ne nf iq na b gy ng nh l ni nj">Theta = gradient(X,Y,Theta,Iterations,alpha)</span><span id="a5c2" class="ne nf iq na b gy nk nh l ni nj">#Here Theta is the global variable</span></pre><p id="8631" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">到目前为止，我们已经得到了一种计算更新后的θ的方法，其中考虑了损失和迭代次数。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="85c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi lj translated"><span class="l lk ll lm bm ln lo lp lq lr di">现在我们的任务是拥有在线学习的功能。基本上，在线学习是一个改进的中间步骤，我们只更新“K”次损失的权重，认为“K”是 x 中没有的新数据。</span></p><p id="ac34" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为，我们将使用从梯度函数中获得的更新和训练的θ，在每个在线学习实例中，θ将总是更新的。下面我将向你展示一个定义在线学习功能的代码片段。</p><pre class="km kn ko kp gt mz na nb nc aw nd bi"><span id="df30" class="ne nf iq na b gy ng nh l ni nj">def online_train(Xn, Yn, Theta):<br/>     Loss = Yn - np.dot(Xn,Theta) + (np.dot(Theta.T,Theta)*0.001)<br/>     Loss = Loss*(-1)        <br/>     dJ = (np.dot(Xn.T,Loss)*2)/len(Y)<br/>     Theta = Theta - (alpha*dJ)<br/>     return Theta</span></pre><p id="445c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">online_train 函数类似于梯度函数，只是差别不大。区别在于传递给函数的θ。在梯度函数的情况下，传递的θ是随机初始化值[0，0]，而在 online_train 函数的情况下，θ是从梯度函数返回的预训练的θ。</p><pre class="km kn ko kp gt mz na nb nc aw nd bi"><span id="642b" class="ne nf iq na b gy ng nh l ni nj">K = Total number of new data (Generally K = 1)<br/>Dimension of Xn : (K,d)<br/>Dimension of Yn : (K,1)</span></pre><p id="89ae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每当有新数据到来时，就会调用 online_train 函数，比如</p><pre class="km kn ko kp gt mz na nb nc aw nd bi"><span id="fafd" class="ne nf iq na b gy ng nh l ni nj">#On Arrival a new single data vector<br/>Theta = online_train(X_new, Y_new, Theta)<br/>#X_new : new data vector<br/>#Y_new : Y value corresponding to X_new</span></pre><p id="16a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于预测，使用θ，可以预测测试数据的值。</p><blockquote class="nl nm nn"><p id="880d" class="jn jo my jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated">在线学习最好的部分是它非常适应新的训练集，并且它可以基于新的训练集更新 Theta，甚至不需要重新训练整个数据集。</p></blockquote><p id="f573" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在线学习的一个主要优点是，在实时应用程序中实现时，它非常节省空间。在第一个训练阶段之后，可以丢弃 X 和 Y，因为我们已经得到了θ，它可以在将来用于进一步的计算。</p><p id="e2cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，在线学习也可以用于完整的训练，每次在循环中传递单个数据向量，并在每个循环步骤中更新 Theta(权重)。通过这种方式，可以可视化收敛，并且如果感觉收敛已经实现，可以停止迭代以实现计算优化。该方法在某种程度上类似于神经网络概念。</p><p id="0747" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi lj translated">如果你听说过在线学习，那么我想你一定也听说过批量培训。批量训练是在线学习的修订版，其中<em class="my">‘K’&gt;1</em>，主要是<em class="my">‘K’=批量</em>，这取决于<em class="my">‘m’(训练集中的观察次数)</em>。两种算法在收敛期间访问相同的最小值点，只是它们在训练数据样本的不同点更新它们的θ。</p><p id="8946" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们列出在线学习的好处</p><ul class=""><li id="d91e" class="mk ml iq jp b jq jr ju jv jy mm kc mn kg mo kk nr mq mr ms bi translated">这对于空间优化非常有用，因为空间是一个问题。</li><li id="e321" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk nr mq mr ms bi translated">它可以在实时应用中使用，其中模型应该适应新的数据点。</li><li id="36bd" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk nr mq mr ms bi translated">在线学习最重要的应用可以在<em class="my">网络应用</em>中找到，这些应用基于<em class="my">预测建模、分类和推荐系统的原理。</em></li></ul><p id="9832" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着对机器学习算法的更好理解，并深入研究权重更新的数学概念，如梯度下降、反向传播、<a class="ae lb" href="http://ruder.io/optimizing-gradient-descent/index.html#adam" rel="noopener ugc nofollow" target="_blank"> adam </a>等，人们可以很容易地设计出针对特定机器学习算法的在线训练解决方案。关键是以尽可能好的方式从数学上理解算法。</p><blockquote class="nl nm nn"><p id="8bb1" class="jn jo my jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated"><strong class="jp ir">在线学习的进步可以为在产品中实时实现机器学习方法带来巨大的转变，并为产品的用户提供更好的用户体验。</strong>最后，<strong class="jp ir">向着理想的人工智能只差一步。</strong></p></blockquote></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="3d75" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">进一步阅读</p><ul class=""><li id="9030" class="mk ml iq jp b jq jr ju jv jy mm kc mn kg mo kk nr mq mr ms bi translated">优化者—<a class="ae lb" href="http://ruder.io/optimizing-gradient-descent/index.html" rel="noopener ugc nofollow" target="_blank">http://ruder.io/optimizing-gradient-descent/index.html</a></li><li id="7d53" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk nr mq mr ms bi translated">在线学习—<a class="ae lb" href="https://www.coursera.org/learn/machine-learning/lecture/ABO2q/online-learning" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/machine-learning/lecture/ABO2q/online-learning</a></li></ul></div></div>    
</body>
</html>