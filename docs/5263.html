<html>
<head>
<title>Residual Networks (ResNets)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">剩余网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/residual-networks-resnets-cb474c7c834a?source=collection_archive---------4-----------------------#2018-10-07">https://towardsdatascience.com/residual-networks-resnets-cb474c7c834a?source=collection_archive---------4-----------------------#2018-10-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/79bfb0fd229e39fff6fd9e41e4de2dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*sqrv-H06hNPP5c75eIFIgg.jpeg"/></div></figure><p id="efff" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在之前的帖子中，我们看到了深度<a class="ae ks" href="https://engmrk.com/convolutional-neural-network-3/" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a>的<a class="ae ks" href="https://engmrk.com/lenet-5-a-classic-cnn-architecture/" rel="noopener ugc nofollow" target="_blank"> LeNet-5 </a>、<a class="ae ks" href="https://engmrk.com/alexnet-implementation-using-keras/" rel="noopener ugc nofollow" target="_blank"> AlexNet </a>、<a class="ae ks" href="https://engmrk.com/vgg16-implementation-using-keras/" rel="noopener ugc nofollow" target="_blank"> VGG16 </a>的实现。同样，我们可以在理论上建立自己的超过 100 层的深度神经网络，但在现实中，它们很难训练。何、、任和在他们的研究<a class="ae ks" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">图像识别的深度残差学习</a>中引入了残差网络(ResNets)的概念。ResNets 允许更深的网络有效地训练。</p><p id="0faa" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">非常深度的神经网络有什么问题？</strong> <br/>在训练神经网络的每次迭代期间，所有权重接收与误差函数相对于当前权重的偏导数成比例的更新。如果梯度非常小，那么权重将不能有效地改变，并且它可能完全停止神经网络的进一步训练。这种现象被称为消失梯度。更具体地说，我们可以说，由于非常缓慢的梯度下降，数据正在通过深度神经网络的各层消失。[1]</p><p id="96e6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">微软研究发现，将深度网络分成三层块，并将每个块的输入直接传递到下一个块，以及该块的剩余输出减去重新引入的块的输入，有助于消除这种信号消失问题。不需要额外的参数或改变学习算法[1]。换句话说，ResNets 将一个非常深的平面神经网络分解为通过跳过或快捷连接连接的小块网络，以形成一个更大的网络。</p><figure class="ku kv kw kx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi kt"><img src="../Images/1af46d1ce362db24cb818742d5950cc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ME62WUBfdIcLIWoCHJekhg.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">ResNets Vs Plain Neural Network</figcaption></figure><p id="1e4a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">ResNet 中使用两种主要类型的块，主要取决于输入/输出维度是相同还是不同。</p><h2 id="680a" class="lg lh iq bd li lj lk dn ll lm ln dp lo kf lp lq lr kj ls lt lu kn lv lw lx ly bi translated">a-身份块</h2><p id="cb65" class="pw-post-body-paragraph ju jv iq jw b jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr ij bi translated">标识块是 ResNets 中使用的标准块，对应于输入激活(比如 a[l])与输出激活(比如 a[l+2])具有相同维数的情况。下面是一个标识块的例子，上面的路径是“快捷路径”，下面的路径是“主路径”。</p><figure class="ku kv kw kx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi me"><img src="../Images/4e9282cb528a38466f7a846009cd479c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dhQQdqZ_XciBou1yAPL8ow.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">ResNets Identity block</figcaption></figure><figure class="ku kv kw kx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi mf"><img src="../Images/7aaae0f5986708e64b50e3f0f901dfd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aWNuZBQZ70Pk-9fQWp9APA.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Identity block implementation using keras</figcaption></figure><h2 id="d692" class="lg lh iq bd li lj lk dn ll lm ln dp lo kf lp lq lr kj ls lt lu kn lv lw lx ly bi translated">b 卷积块</h2><p id="67a5" class="pw-post-body-paragraph ju jv iq jw b jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr ij bi translated">当输入和输出维度不匹配时，我们在捷径路径中添加一个卷积层。这种排列称为卷积块。</p><figure class="ku kv kw kx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi mg"><img src="../Images/6781c8fae72411ca8d90a791ec695b8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NX7Bhwa5yVbA27LRKFLiIQ.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">ResNets convolutional block</figcaption></figure><figure class="ku kv kw kx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi mh"><img src="../Images/82669bc6c06a1bada549c1190cff917b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SK1Pw5BOCoKyU0VPGIwRBw.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Convolutional block implementation using keras</figcaption></figure><h1 id="c3c7" class="mi lh iq bd li mj mk ml ll mm mn mo lo mp mq mr lr ms mt mu lu mv mw mx lx my bi translated">构建完整的 ResNet 模型(50 层)</h1><p id="8343" class="pw-post-body-paragraph ju jv iq jw b jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr ij bi translated">现在你已经有了构建一个非常深的 ResNet 所必需的模块。下图详细描述了该神经网络的体系结构。图中的“ID BLOCK”代表“身份块”，而“ID BLOCK x3”意味着您应该将 3 个身份块堆叠在一起。</p><figure class="ku kv kw kx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi mz"><img src="../Images/c9f4ceb9683f6be64bdb7098eb6839b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CjqE4DjN1ALx5tKK1zDV7Q.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">50 layers ResNets Architecture</figcaption></figure><p id="5898" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">上述 ResNet-50 型号的详细信息如下:</p><ul class=""><li id="d660" class="na nb iq jw b jx jy kb kc kf nc kj nd kn ne kr nf ng nh ni bi translated">零填充:用(3，3)填充输入</li><li id="1e7f" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">阶段 1:2D 卷积具有 64 个形状为(7，7)的滤波器，并使用(2，2)的步长。它的名字叫“conv1”。BatchNorm 应用于输入的通道轴。最大池使用(3，3)窗口和(2，2)步距。</li><li id="efbc" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">阶段 2:卷积块使用三组大小为 64×64×256 的滤波器，f=3，s=1，块是“a”。这两个单位块使用三组大小为 64×64×256 的滤波器，f=3，块是“b”和“c”。</li><li id="3b4d" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">阶段 3:卷积块使用三组大小为 128×128×512 的滤波器，f=3，s=2，块为“a”。3 个单位块使用三组大小为 128×128×512 的滤波器，f=3，块为“b”、“c”和“d”。</li><li id="ed27" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">阶段 4:卷积块使用三组大小为 256×256×1024 的滤波器，f=3，s=2，块为“a”。这 5 个单位块使用三组大小为 256×256×1024 的滤波器，f=3，块是“b”、“c”、“d”、“e”和“f”。</li><li id="54b9" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">阶段 5:卷积块使用三组大小为 512×512×2048 的滤波器，f=3，s=2，块为“a”。这两个单位块使用三组大小为 256×256×2048 的滤波器，f=3，块是“b”和“c”。</li><li id="0f95" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">2D 平均池使用形状为(2，2)的窗口，其名称为“avg_pool”。</li><li id="c871" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">展平没有任何超参数或名称。</li><li id="da69" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">全连接(密集)层使用 softmax 激活将其输入减少到类的数量。它的名字应该是' fc' + str(类)。</li></ul><pre class="ku kv kw kx gt no np nq nr aw ns bi"><span id="3352" class="lg lh iq np b gy nt nu l nv nw">from keras import layers </span><span id="6507" class="lg lh iq np b gy nx nu l nv nw">from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D </span><span id="ae28" class="lg lh iq np b gy nx nu l nv nw">from keras.layers import AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D </span><span id="d3a0" class="lg lh iq np b gy nx nu l nv nw">from keras.models import Model</span><span id="159f" class="lg lh iq np b gy nx nu l nv nw">from keras.initializers import glorot_uniform</span><span id="09a6" class="lg lh iq np b gy nx nu l nv nw">input_shape = (64, 64, 3)<br/> classes = 6</span><span id="78f9" class="lg lh iq np b gy nx nu l nv nw"># Define the input as a tensor with shape input_shape<br/> X_input = Input(input_shape)</span><span id="b5ea" class="lg lh iq np b gy nx nu l nv nw"># Zero-Padding<br/> X = ZeroPadding2D((3, 3))(X_input)</span><span id="386d" class="lg lh iq np b gy nx nu l nv nw"># Stage 1 X = Conv2D(64, (7, 7), strides = (2, 2), name = ‘conv1’,)(X) X = BatchNormalization(axis = 3, name = ‘bn_conv1’)(X) X = Activation(‘relu’)(X)</span><span id="d48b" class="lg lh iq np b gy nx nu l nv nw">X = MaxPooling2D((3, 3), strides=(2, 2))(X)</span><span id="b811" class="lg lh iq np b gy nx nu l nv nw"># Stage 2 X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block=’a’, s = 1) X = identity_block(X, 3, [64, 64, 256], stage=2, block=’b’)</span><span id="c78b" class="lg lh iq np b gy nx nu l nv nw">X = identity_block(X, 3, [64, 64, 256], stage=2, block=’c’)</span><span id="4451" class="lg lh iq np b gy nx nu l nv nw"># Stage 3 X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block=’a’, s = 2) X = identity_block(X, 3, [128, 128, 512], stage=3, block=’b’) X = identity_block(X, 3, [128, 128, 512], stage=3, block=’c’)</span><span id="a8db" class="lg lh iq np b gy nx nu l nv nw">X = identity_block(X, 3, [128, 128, 512], stage=3, block=’d’)</span><span id="b276" class="lg lh iq np b gy nx nu l nv nw"># Stage 4 X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block=’a’, s = 2) X = identity_block(X, 3, [256, 256, 1024], stage=4, block=’b’) X = identity_block(X, 3, [256, 256, 1024], stage=4, block=’c’) X = identity_block(X, 3, [256, 256, 1024], stage=4, block=’d’) X = identity_block(X, 3, [256, 256, 1024], stage=4, block=’e’)</span><span id="5065" class="lg lh iq np b gy nx nu l nv nw">X = identity_block(X, 3, [256, 256, 1024], stage=4, block=’f’)</span><span id="b998" class="lg lh iq np b gy nx nu l nv nw"># Stage 5 X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block=’a’, s = 2) X = identity_block(X, 3, [512, 512, 2048], stage=5, block=’b’)</span><span id="5b0a" class="lg lh iq np b gy nx nu l nv nw">X = identity_block(X, 3, [512, 512, 2048], stage=5, block=’c’)</span><span id="f852" class="lg lh iq np b gy nx nu l nv nw"># AVGPOOL<br/> X = AveragePooling2D((2,2), name=’avg_pool’)(X)</span><span id="3583" class="lg lh iq np b gy nx nu l nv nw"># output layer X = Flatten()(X)</span><span id="7fac" class="lg lh iq np b gy nx nu l nv nw">X = Dense(classes, activation=’softmax’, name=’fc’ + str(classes))(X)</span><span id="7195" class="lg lh iq np b gy nx nu l nv nw"># Create model<br/> model = Model(inputs = X_input, outputs = X, name=’ResNet50′)</span><span id="7fab" class="lg lh iq np b gy nx nu l nv nw">#Compile the model<br/> model.compile(optimizer=’adam’, loss=’categorical_crossentropy’, metrics=[‘accuracy’])</span></pre><h2 id="929e" class="lg lh iq bd li lj lk dn ll lm ln dp lo kf lp lq lr kj ls lt lu kn lv lw lx ly bi translated">摘要</h2><ul class=""><li id="6a98" class="na nb iq jw b jx lz kb ma kf ny kj nz kn oa kr nf ng nh ni bi translated">非常深的神经网络(普通网络)实现起来不实际，因为它们由于消失梯度而难以训练。</li><li id="99fb" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">跳跃连接有助于解决消失梯度问题。它们还使得 ResNet 块很容易学习标识函数。</li><li id="533c" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">有两种主要类型的 ResNets 块:身份块和卷积块。</li><li id="986e" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">通过将这些块堆叠在一起，构建非常深的剩余网络。</li></ul><h2 id="9821" class="lg lh iq bd li lj lk dn ll lm ln dp lo kf lp lq lr kj ls lt lu kn lv lw lx ly bi translated">参考</h2><ul class=""><li id="c4dd" class="na nb iq jw b jx lz kb ma kf ny kj nz kn oa kr nf ng nh ni bi translated"><a class="ae ks" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Vanishing_gradient_problem</a></li><li id="8825" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">吴恩达卷积神经网络(Coursera)</li><li id="4362" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">何，，，，任，—深度残差学习在图像识别中的应用(2015)</li><li id="41a5" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">Francois Chollet 的 GitHub 资源库:<a class="ae ks" href="https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/fchollet/deep-learning-models/blob/master/resnet 50 . py</a></li></ul></div></div>    
</body>
</html>