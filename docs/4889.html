<html>
<head>
<title>Where did the least-square come from?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最小二乘法从何而来？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/where-did-the-least-square-come-from-3f1abc7f7caf?source=collection_archive---------3-----------------------#2018-09-13">https://towardsdatascience.com/where-did-the-least-square-come-from-3f1abc7f7caf?source=collection_archive---------3-----------------------#2018-09-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1de4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在机器学习面试中，如果被问及最小二乘损失函数的数学基础，你会怎么回答？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c44f5484b41ff4c1b4f3ccfa79c88375.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DxfQbIRcWBDuy1O5cGkEaw.png"/></div></div></figure><h2 id="db6c" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">承认</h2><p id="49a7" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">感谢我在佐治亚理工学院的<a class="ae mj" href="https://www.omscs.gatech.edu/cs-7641-machine-learning" rel="noopener ugc nofollow" target="_blank"> CS7641 课程，在我的</a><a class="ae mj" href="https://pe.gatech.edu/degrees/analytics" rel="noopener ugc nofollow" target="_blank"> MS 分析项目</a>中，我发现了这个概念，并受到了写这个概念的启发。感谢<a class="ae mj" href="https://www.linkedin.com/in/brohrer/" rel="noopener ugc nofollow" target="_blank"> Brandon </a>让我在本文中使用他美丽的贝叶斯推断视频。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="840b" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated"><strong class="ls iu"> <em class="mw">问题</em> </strong>:为什么要对一个回归机器学习任务中的误差进行平方？</p><p id="d3e2" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated"><strong class="ls iu"> <em class="mw"> Ans </em> </strong>:“为什么，当然是把所有的误差(残差)变成正的量！”</p><p id="fa71" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated"><strong class="ls iu"> <em class="mw">问题</em> </strong>:“好吧，为什么不用一个更简单的绝对值函数| <em class="mw"> x </em> |使所有误差为正？”</p><p id="da3d" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated"><strong class="ls iu"> <em class="mw"> Ans </em> </strong>:“啊哈，你在骗我。绝对值函数不是处处可微的！”</p><p id="1739" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated"><strong class="ls iu"> <em class="mw">问题</em> </strong>:“那对数值算法应该没多大关系。套索回归使用一个有绝对值的项，可以处理。还有，为什么不是<em class="mw"> x </em>的 4 次方或者 log(1+ <em class="mw"> x </em>)？<strong class="ls iu">平方误差有什么特别的？</strong></p><p id="b9d1" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated"><strong class="ls iu"> <em class="mw"> Ans </em> </strong>:嗯……</p><h2 id="ca7f" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">贝叶斯论证</h2><p id="2034" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">请记住，对于机器学习中所有棘手的问题，如果你在你的论点中混合使用单词“<strong class="ls iu"> <em class="mw">贝叶斯</em> </strong>”，你就可以很快找到一个听起来很严肃的答案。</p><p id="f47a" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">好吧，我是在开玩笑。</p><p id="618c" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">但是，是的，我们应该明确准备好关于像最小二乘和交叉熵这样的流行损失函数来自哪里的论点——至少当我们试图使用贝叶斯论点为监督学习问题找到最可能的假设时。</p><p id="3ff9" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">请继续阅读…</p><h2 id="78b2" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">基础知识:贝叶斯定理和“最可能假设”</h2><p id="855a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">贝叶斯定理可能是对现代机器学习和人工智能系统最有影响的概率论恒等式。关于这个话题的超级直观的介绍，<a class="ae mj" href="https://brohrer.github.io/how_bayesian_inference_works.html" rel="noopener ugc nofollow" target="_blank">请参见由<a class="ae mj" href="https://brohrer.github.io/blog.html" rel="noopener ugc nofollow" target="_blank"> Brandon Rohrer </a>撰写的这个伟大的教程</a>。我会专注于方程式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/3c689e10c2188f6bf18f2fb189995c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/1*Cfqw_wOjfrQPNeS1JQbgjQ.gif"/></div></figure><p id="96c4" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">这实质上是告诉你在看到数据/证据(<em class="mw">可能性</em>)后更新你的信念(<em class="mw">先验概率</em>，并将更新后的信念度赋予术语<em class="mw">后验概率</em>。你可以从一个信念开始，但是每一个数据点都会加强或削弱这个信念，并且你一直在更新你的假设。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="b161" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">现在让我们用不同的符号——属于数据科学的符号——来改写贝叶斯定理。让我们用<strong class="ls iu"> <em class="mw"> D </em> </strong>表示数据，用<strong class="ls iu"><em class="mw"/></strong>表示假设。这意味着在给定数据 的情况下，我们应用贝叶斯公式来尝试确定<strong class="ls iu"> <em class="mw">数据来自什么假设。我们将定理改写为，</em></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/5a3abbad4e5561bb25def861f3ce9c99.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*SZZaRHJM6eYZu3W2HytPiA.png"/></div></figure><p id="3e39" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">现在，一般来说，我们有一个大的(通常是无限的)假设空间，也就是说，有许多假设可供选择。<a class="ae mj" href="https://www.datascience.com/blog/introduction-to-bayesian-inference-learn-data-science-tutorials" rel="noopener ugc nofollow" target="_blank">贝叶斯推断</a>的本质是我们想要检查数据，以最大化最有可能产生观察数据的一个假设的概率。我们主要是想确定<strong class="ls iu"><em class="mw"/></strong>中的<strong class="ls iu"><em class="mw">arg max</em></strong>(<strong class="ls iu"><em class="mw">h</em></strong>|<strong class="ls iu"><em class="mw">D</em></strong>)<strong class="ls iu"/>也就是说，给定观察到的<strong class="ls iu"><em class="mw"/></strong>，我们想知道哪个<strong class="ls iu"> <em class="mw"> h </em> </strong>可能性最大。</p><h2 id="6508" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">一个捷径:最大可能性</h2><p id="f259" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">上面的等式看起来很简单，但在实践中计算起来却非常棘手——因为在复杂的概率分布函数上计算积分时，假设空间非常大且非常复杂。</p><p id="c2c9" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">然而，在我们寻求'<em class="mw">给定数据的最可能假设时，</em>'我们可以进一步简化它。</p><ul class=""><li id="2b3c" class="nb nc it ls b lt mr lw ms ld nd lh ne ll nf mi ng nh ni nj bi translated">我们可以把分母中的项去掉，它没有任何包含<strong class="ls iu"> <em class="mw"> h </em> </strong>的项即假设。我们可以把它想象成使总概率总和为 1 的规格化器。</li><li id="a290" class="nb nc it ls b lt nk lw nl ld nm lh nn ll no mi ng nh ni nj bi translated"><a class="ae mj" href="https://www.quora.com/What-is-an-example-of-a-uniform-prior" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu">一致先验假设</strong> </a> <strong class="ls iu"> </strong> —这实质上是通过使其一致来放松对<strong class="ls iu"><em class="mw">P</em></strong>(<strong class="ls iu"><em class="mw">h</em></strong>)性质的任何假设，即所有假设都是可能的。那么它是一个常数 1/|Vsd|其中|Vsd|是<a class="ae mj" href="http://www2.cs.uregina.ca/~dbd/cs831/notes/ml/vspace/3_vspace.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu">版本空间</strong>的大小，即与训练数据</a>一致的所有假设的集合。那么它在确定最大可能假设时实际上并不重要。</li></ul><p id="6ff3" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">在这两个简化的假设之后，<strong class="ls iu">最大似然(ML) </strong>假设可以由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/2465ad5c6bca8a3e75331d0a88d92944.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/1*DqvpTdRj6CLXmttJ8byb7w.gif"/></div></figure><blockquote class="nq nr ns"><p id="8bb2" class="lq lr mw ls b lt mr ju lv lw ms jx ly nt mt ma mb nu mu md me nv mv mg mh mi im bi translated">这仅仅意味着<em class="it">最可能的</em>假设是观测数据(给定假设)的<a class="ae mj" href="https://www.mathsisfun.com/data/probability-events-conditional.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu">条件概率</strong> </a> <strong class="ls iu">达到最大的假设。</strong></p></blockquote><h2 id="d280" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">拼图的下一块:数据中的噪音</h2><p id="eb49" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">我们通常在学习 Stats 101 中的<a class="ae mj" href="https://en.wikipedia.org/wiki/Simple_linear_regression" rel="noopener ugc nofollow" target="_blank">简单线性回归</a>时开始使用<a class="ae mj" href="https://en.wikipedia.org/wiki/Least_squares" rel="noopener ugc nofollow" target="_blank">最小二乘误差</a>，但这个看起来简单的损失函数牢牢地存在于几乎每个监督机器学习算法中，即。线性模型、样条、决策树或深度学习网络。</p><p id="7448" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">那么，它有什么特别之处呢？这和贝叶斯推理有什么关系吗？</p><blockquote class="nq nr ns"><p id="c77b" class="lq lr mw ls b lt mr ju lv lw ms jx ly nt mt ma mb nu mu md me nv mv mg mh mi im bi translated">事实证明，最小二乘误差和贝叶斯推断之间的关键联系是通过<strong class="ls iu">误差或残差</strong>的假定性质。</p></blockquote><p id="574c" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated"><strong class="ls iu">测量/观察数据从来都不是无误差的</strong>，并且总是有<strong class="ls iu">随机噪声</strong>与数据相关联，这可以被认为是感兴趣的信号。机器学习算法的任务是通过从噪声中分离信号来估计/近似可能产生数据的函数。</p><p id="ad20" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">但是我们能对这种噪音的本质说些什么呢？原来噪声可以被建模为一个<a class="ae mj" href="https://en.wikipedia.org/wiki/Random_variable" rel="noopener ugc nofollow" target="_blank">随机变量</a>。因此，我们可以将我们选择的一个<a class="ae mj" href="https://en.wikipedia.org/wiki/Probability_distribution" rel="noopener ugc nofollow" target="_blank">概率分布</a>关联到这个随机变量上。</p><blockquote class="nq nr ns"><p id="dc23" class="lq lr mw ls b lt mr ju lv lw ms jx ly nt mt ma mb nu mu md me nv mv mg mh mi im bi translated"><strong class="ls iu">最小二乘优化的关键假设之一是残差的概率分布是我们信任的老朋友——</strong><a class="ae mj" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank"><strong class="ls iu">高斯正态。</strong>T11】</a></p></blockquote><p id="881c" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">这意味着监督学习训练数据集中的每个数据点(<strong class="ls iu"> <em class="mw"> d </em> </strong>)可以写成未知函数<strong class="ls iu"><em class="mw">f</em></strong>(<strong class="ls iu"><em class="mw">x</em></strong>)(学习算法试图近似的)和从零均值(<strong class="ls iu"> <em class="mw"> μ </em> </strong>)和未知方差<strong class="ls iu"><em class="mw">σ【t3t】的正态分布中得出的误差项之和这就是我的意思，</em></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/fab8e5aeeaddf0c6b3c0c6bf6aad7616.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/1*4yNEEcVLb8hDYXiTux7mYg.gif"/></div></figure><p id="9463" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">从这个断言，我们可以很容易地得出，最大可能的假设是最小化最小平方误差的假设。</p><p id="8903" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi nx translated">M 而且没有好的方法在 Medium 中输入数学。所以，我必须粘贴一个图像来显示推导过程。请随意跳过这一部分，我将在下一部分总结关键结论。</p><h2 id="dd55" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">从最大似然假设推导最小二乘</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/693e3f9db8794110a3cd7358ea6ea95e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a4aWlL7HWk-L-kxm-3q80g.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/d812eaab892040bfe994c0c30b0f272c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6JbEfTyfsI68BW1_b94Hsg.png"/></div></div></figure><blockquote class="nq nr ns"><p id="3371" class="lq lr mw ls b lt mr ju lv lw ms jx ly nt mt ma mb nu mu md me nv mv mg mh mi im bi translated">瞧！最后一项只不过是简单的最小二乘最小化。</p></blockquote><h2 id="e2a2" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">那么，这些数学显示了什么？</h2><p id="a0cf" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">它表明，从监督训练数据集的误差分布在高斯正态分布的假设出发，该训练数据的<strong class="ls iu">最大似然假设是最小化最小平方误差损失函数的假设。</strong></p><p id="e5c4" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">没有关于学习算法类型的假设。<strong class="ls iu">这同样适用于任何事情，从简单的线性回归到深度神经网络。</strong></p><blockquote class="nq nr ns"><p id="d559" class="lq lr mw ls b lt mr ju lv lw ms jx ly nt mt ma mb nu mu md me nv mv mg mh mi im bi translated">这就是贝叶斯推理的力量和统一性。</p></blockquote><p id="0882" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">这是线性回归拟合的典型场景。贝叶斯推理论证在这个模型上起作用，并使选择误差的平方作为最佳损失函数变得可信。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/8b3920228ea221ece3b3791077230bff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iiq550rQoziWDLKJKG94Yg.png"/></div></div></figure><h2 id="beb8" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">常态的假设是否足够合理？</h2><p id="c1fc" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">你可以质疑关于正态分布误差项的假设的有效性。<a class="ae mj" href="https://www.quora.com/Why-is-the-normal-distribution-important" rel="noopener ugc nofollow" target="_blank">但在大多数情况下，它是有效的</a>。这是根据<a class="ae mj" href="https://en.wikipedia.org/wiki/Central_limit_theorem" rel="noopener ugc nofollow" target="_blank">中心极限定理</a> (CLT)得出的，在某种意义上，错误或噪声从来不是由单个底层过程产生的，而是由多个子过程的综合影响产生的。当大量随机子过程组合时，它们的平均值服从正态分布(来自 CLT)。因此，对于我们承担的大多数机器学习任务来说，假设这样的分布并不困难。</p><h2 id="7539" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">通过高斯-马尔可夫定理的另一个(更强的)论证</h2><p id="e8ad" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">事实证明，在没有误差分布信息的情况下，最小二乘法仍然是所有线性估计中最好的线性无偏估计(根据<a class="ae mj" href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem" rel="noopener ugc nofollow" target="_blank">高斯-马尔可夫定理</a>)。唯一的要求是误差的期望值为零，不相关，方差相等。</p><h2 id="7ab3" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">类似的论点适用于分类问题吗？</h2><p id="b311" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">最小平方损失函数用于回归任务。但是当我们处理类别和概率，而不是任意的实数时，分类问题又如何呢？</p><p id="bcdc" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">令人惊讶的是，使用 ML 假设和简单的类定义选择可以完成类似的推导，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/90d4e455cc2456f229fe0490639ab905.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*WI7hbLz9oDCiy5ruQYHTyg.gif"/></div></div></figure><p id="9c8e" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">…也就是<strong class="ls iu">交叉熵损失函数。</strong></p><p id="b580" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi translated">因此，相同的贝叶斯推理产生交叉熵作为在分类问题中获得最大可能假设的损失函数的优选选择。</p><h2 id="1572" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">总结和结论</h2><p id="a298" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">我们可以通过以下几点来总结和延伸我们在文章中的讨论和论点，</p><ul class=""><li id="303a" class="nb nc it ls b lt mr lw ms ld nd lh ne ll nf mi ng nh ni nj bi translated"><strong class="ls iu">最大似然估计(MLE) </strong>是一种强大的技术，如果我们能够做出一致的先验假设，即在开始时，所有假设都是同等可能的，则可以得出给定数据集的最可能假设。</li><li id="69a9" class="nb nc it ls b lt nk lw nl ld nm lh nn ll no mi ng nh ni nj bi translated">如果我们可以假设机器学习任务中的每个数据点都是真实函数和一些正态分布的随机噪声变量之和，那么我们可以推导出这样一个事实，即<strong class="ls iu">最大可能假设是最小化平方损失函数</strong>的假设。</li><li id="5bb1" class="nb nc it ls b lt nk lw nl ld nm lh nn ll no mi ng nh ni nj bi translated">无论机器学习算法的性质如何，这个结论都是正确的。</li><li id="d17b" class="nb nc it ls b lt nk lw nl ld nm lh nn ll no mi ng nh ni nj bi translated">然而，另一个隐含的假设是数据点的<strong class="ls iu">相互独立性</strong>，这使得我们能够将联合概率写成单个概率的简单乘积。这也<strong class="ls iu">强调了在建立机器学习模型之前消除训练样本间共线性的重要性。</strong></li></ul><blockquote class="nq nr ns"><p id="c9e9" class="lq lr mw ls b lt mr ju lv lw ms jx ly nt mt ma mb nu mu md me nv mv mg mh mi im bi translated">最终，你可以说最小二乘最小化非常特别，因为事实上，它与宇宙中最著名的<a class="ae mj" href="http://www.askamathematician.com/2010/02/q-whats-so-special-about-the-gaussian-distribution-a-k-a-a-normal-distribution-or-bell-curve/" rel="noopener ugc nofollow" target="_blank">分布函数</a> :-)密切相关</p></blockquote></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="bdb1" class="pw-post-body-paragraph lq lr it ls b lt mr ju lv lw ms jx ly ld mt ma mb lh mu md me ll mv mg mh mi im bi nx translated"><span class="l ny nz oa bm ob oc od oe of di">如果</span>你有任何问题或想法要分享，请联系作者<a class="ae mj" href="mailto:tirthajyoti@gmail.com" rel="noopener ugc nofollow" target="_blank"><strong class="ls iu">tirthajyoti【AT】Gmail . com</strong></a>。此外，您可以查看作者的<a class="ae mj" href="https://github.com/tirthajyoti?tab=repositories" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu"> GitHub 资源库</strong> </a>中其他有趣的 Python、R 或 MATLAB 代码片段和机器学习资源。如果你像我一样对机器学习/数据科学充满热情，请随时<a class="ae mj" href="https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/" rel="noopener ugc nofollow" target="_blank">在 LinkedIn 上添加我</a>或<a class="ae mj" href="https://twitter.com/tirthajyotiS" rel="noopener ugc nofollow" target="_blank">在 Twitter 上关注我。</a></p><div class="ok ol gp gr om on"><a href="https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd iu gy z fp os fr fs ot fu fw is bi translated">Tirthajyoti Sarkar -高级首席工程师-低压设计工程-半导体上|…</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">查看 Tirthajyoti Sarkar 在世界上最大的职业社区 LinkedIn 上的个人资料。Tirthajyoti 有 8 份工作…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">www.linkedin.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb ks on"/></div></div></a></div></div></div>    
</body>
</html>