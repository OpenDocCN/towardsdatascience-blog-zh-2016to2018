<html>
<head>
<title>The Capricious Models of Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习的多变模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-capricious-models-of-machine-learning-23cd2f36dbbe?source=collection_archive---------5-----------------------#2018-06-20">https://towardsdatascience.com/the-capricious-models-of-machine-learning-23cd2f36dbbe?source=collection_archive---------5-----------------------#2018-06-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e459b1c2299236f11273a7936e183aa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3tAxNGPEol7rW4IhhA2GAA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/photos/uAFjFsMS3YY?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Marten Newhall</a> on <a class="ae kc" href="https://unsplash.com/search/photos/funny?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="kd ke kf"><p id="fb6f" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">这个标题充分说明了机器学习中模型拟合的问题。伙计，统计学对建模有着非常不同的看法，这与我们的认知并不完全一致。</p></blockquote><h1 id="7801" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated"><span class="l md me mf bm mg mh mi mj mk di"> T </span>何发出:</h1><p id="e999" class="pw-post-body-paragraph kg kh iq kj b kk ml km kn ko mm kq kr mn mo ku kv mp mq ky kz mr ms lc ld le ij bi translated">机器学习中的这些模型有一个很好地拟合数据的问题。这两个问题是:</p><p id="225d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mn kt ku kv mp kx ky kz mr lb lc ld le ij bi mt translated"><span class="l md me mf bm mg mh mi mj mk di"> O </span>拟合:<strong class="kj ir">模型很好地吸收了数据</strong>，并完美地拟合了数据，有效地使<em class="ki">平均误差</em>比例非常小，几乎完美。听起来像是天赐之福，但事实并非如此。我们不希望我们的模型过于完美。数据集就像腐败的政客一样，隐藏着一些肮脏的东西。它们通常既不是线性的，也不是弯曲的。它们有某种粗糙的形状，但几乎从来没有完全符合几何形状。数据中一定有一些值与其他值相差太大。例如，考虑机器学习社区中使用的最喜欢的数据集:<em class="ki">房价数据集</em>。现在假设数据集的形状大致随大小线性增加。但是有一个特定的房子，它以前的房客自杀了，因此被当地人归类为“闹鬼”。对于这样的房子来说，价格太低了。再想想另一栋房子，它曾是某个死于车祸的非常受欢迎的电影明星的家。这样的房子会有很好的市场价值。这两种数据是我们的数据集的异常，记录在某个特定的区域(或 x 轴)值。我们不希望我们的模型从这些数据中做出推论，因为当我们预测不同社区的房价时，没有必要预测相同区域的类似房屋会闹鬼或以前由名人拥有。我们希望尽量减少模型对这些异常的拟合。为了做到这一点，我们需要找到完美数量的叶节点(如果使用决策树)或神经元(如果使用深度学习)。</p><p id="38a0" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mn kt ku kv mp kx ky kz mr lb lc ld le ij bi mt translated">nderfitting:顾名思义，模型无法从数据中吸取任何东西。因此<strong class="kj ir">没有学到我们想要的那么多</strong>。(哑巴模特！).为了形象化这个场景，你可以想象一下，房价曲线在某一点之前是线性增长的，但是从这一点开始急剧上升，导致该地区的房价开始变得更加昂贵。一个很好的拟合是，我们的回归线几乎是直线，然后向上弯曲。一个哑拟合将是回归线始终是线性的，完全忽略上升曲线，直接在它下面。或者甚至是一条直线，从起点一直走，穿过曲线。</p><p id="a557" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mn kt ku kv mp kx ky kz mr lb lc ld le ij bi translated">下图直观地展示了这一场景。每种情况下的橙色线代表数据的最佳拟合(这是我们希望达到的)。左侧第一幅图像中的蓝线描述了模型的欠拟合情况。最右边图像中的蓝线描述了模型如何过度拟合。中间的图像描述了一个最佳匹配。我们可以观察到拟合没有完全覆盖橙色线，但这没关系。(在数据建模中，这种程度的误差总是意料之中的)</p><figure class="mv mw mx my gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mu"><img src="../Images/54a32f2afbcfc6ffd6a37a7e591fe88c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*94PW-U1fK7LoDCNKGMMBsw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Source: Scikit-Learn</figcaption></figure><p id="9e58" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mn kt ku kv mp kx ky kz mr lb lc ld le ij bi translated">为了解决这两个主要问题，<em class="ki">使用绘图可视化数据是很重要的</em>。(哦，很美的情节！)</p><p id="e29a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mn kt ku kv mp kx ky kz mr lb lc ld le ij bi translated">同样重要的是，我们要找到节点的最佳值，以便模型与数据达到最佳匹配。</p><p id="2a09" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mn kt ku kv mp kx ky kz mr lb lc ld le ij bi translated">有了神经网络，这个问题变得非常复杂。但值得庆幸的是，我们有许多高级库(<em class="ki">像</em> <a class="ae kc" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> <em class="ki"> Keras </em> </a> <em class="ki">和</em> <a class="ae kc" href="http://caffe.berkeleyvision.org/" rel="noopener ugc nofollow" target="_blank"> <em class="ki"> Caffe </em> </a>)来抽象所有远离我们的工作(对于大多数通用时间)。</p><figure class="mv mw mx my gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/20267b8924a64d2fa99fe38723e766b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*GIscS9fYvlX8T7udtwv6lw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Source: <a class="ae kc" href="https://www.neuraldesigner.com" rel="noopener ugc nofollow" target="_blank">https://www.neuraldesigner.com</a></figcaption></figure><p id="6e5c" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mn kt ku kv mp kx ky kz mr lb lc ld le ij bi translated">下面是一段代码，它包含了一个函数，我们用它来寻找决策树中最适合模型的节点数。当我们运行这个脚本时，我们可以观察到它不断地遍历节点数并打印出值。然后，我们可以使用视觉检查来决定我们应该在决策树中使用的最佳拟合的正确节点数，因为我们可以观察到平均误差在开始时很高<strong class="kj ir">(节点数较少，拟合不足)，最终<strong class="kj ir">继续减少到点</strong>(最佳点)，然后<strong class="kj ir">返回增加</strong>(节点数过多，拟合过度)。</strong></p><p id="622f" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mn kt ku kv mp kx ky kz mr lb lc ld le ij bi translated">一些基本的解释已经作为注释提供。</p><pre class="mv mw mx my gt na nb nc nd aw ne bi"><span id="c6be" class="nf lg iq nb b gy ng nh l ni nj">#First, import all the libraries. Ok, this is too formal<br/>import pandas as pd<br/>from sklearn.tree import DecisionTreeRegressor<br/>from sklearn.metrics import mean_absolute_error<br/>from sklearn.model_selection import train_test_split</span><span id="2a30" class="nf lg iq nb b gy nk nh l ni nj">#We import the dataset that we will be using. You can import any dataset you want that probably have 5-6 columns with numerical data(preferably something to do with housing prices)</span><span id="71d6" class="nf lg iq nb b gy nk nh l ni nj">#The data will be imported in the form of a pandas dataframe. Pandas provide an excellent dataframe for all statistical application by formatting all the data in a spreadsheet like table</span><span id="39d0" class="nf lg iq nb b gy nk nh l ni nj">my_data = pd.read_csv('*The file destination here without asterisk*')</span><span id="715f" class="nf lg iq nb b gy nk nh l ni nj"># print the column names of the dataset as we'll be calling names in a while<br/>print(my_data.columns)</span><span id="3830" class="nf lg iq nb b gy nk nh l ni nj">#Now select the columns that we'll be using for making our decision tree and form a new dataframe with those values<br/>interest_data = my_data['col1','col2',...,'coln']<br/>interest_data.describe()<br/>X = interest_data</span><span id="a606" class="nf lg iq nb b gy nk nh l ni nj">#Load the column that we need to predict into a new variable called y(maths!)<br/>y = my_data.col_name</span><span id="b9d2" class="nf lg iq nb b gy nk nh l ni nj">#Define the data model that we'll be using, in this case, decision tree regressor<br/>data_model = DecisionTreeRegressor()</span><span id="f39f" class="nf lg iq nb b gy nk nh l ni nj">#Fit the data on the model<br/>data_model.fit(X,y)</span><span id="7b5b" class="nf lg iq nb b gy nk nh l ni nj">#It's not good to use the same values for testing the dataset so we use the train_test_split function by Scikit-Learn to randomly split the dataset into a training set and validation set<br/>train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)</span><span id="6d58" class="nf lg iq nb b gy nk nh l ni nj">#Fitting the data model using the training values<br/>data_model.fit(train_X,train_y)</span><span id="3fa8" class="nf lg iq nb b gy nk nh l ni nj">#We use the validation set we splitted earlier to check the accuracy of our model by printing the MAE of the predictions<br/>predictions = my_model.predict(val_X)<br/>print(mean_absolute_error(val_y, predictions))</span><span id="4503" class="nf lg iq nb b gy nk nh l ni nj">#Now, to fit the entire model optimally, we'll have to change the number of leaf nodes to find the optimal requirement for which the MAE would be the lowest. For this, we develop one function to calculate the MAE</span><span id="4ea8" class="nf lg iq nb b gy nk nh l ni nj">def get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):<br/>    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)<br/>    model.fit(predictors_train, targ_train)<br/>    preds_val = model.predict(predictors_val)<br/>    mae = mean_absolute_error(targ_val, preds_val)<br/>    return(mae)</span><span id="fd91" class="nf lg iq nb b gy nk nh l ni nj">#Now take some random values in an array that denote the number of leaf nodes. We will be using these values to test the fit so try out a good amount of them <br/>n = [10,50,100,200,400,500,800,1000]</span><span id="4f66" class="nf lg iq nb b gy nk nh l ni nj">#We define another function to iterate over all the leaf node values we described above and find the optimal nodes for which MAE would be the least.</span><span id="9682" class="nf lg iq nb b gy nk nh l ni nj">def optimal_nodes(n, train_X, val_X, train_y, val_y):<br/>    on = 1000000 #Just a really high random value<br/>    leaf = 0 #The variable which will save the minimum nodes we need<br/>    for max_leaf_nodes in n:<br/>        my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)<br/>        if my_mae &lt; on:<br/>            on = my_mae<br/>            leaf = max_leaf_nodes<br/>        print("Max leaf nodes: %d  \t\t Mean Absolute Error:  %d" %(max_leaf_nodes, my_mae))<br/>    return(leaf)</span><span id="f360" class="nf lg iq nb b gy nk nh l ni nj">#By running the above function, we derive the optimal leaf nodes we need in our decision tree to have the least MAE. For that number of nodes, we can say that the model fits optimally.</span><span id="f1e0" class="nf lg iq nb b gy nk nh l ni nj">print("The optimal number of nodes for our model would be: "+str(optimal_nodes(n, train_X, val_X, train_y, val_y)))</span></pre><blockquote class="kd ke kf"><p id="3d00" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">如果你需要更好地理解拟合的概念，请参考伊恩·古德菲勒写的这本关于深度学习的优秀书籍:<a class="ae kc" href="http://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">深度学习书籍</a>，可在线免费获得。</p></blockquote></div></div>    
</body>
</html>