<html>
<head>
<title>Learning Rate Scheduler</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习率调度程序</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-rate-scheduler-d8a55747dd90?source=collection_archive---------10-----------------------#2017-08-02">https://towardsdatascience.com/learning-rate-scheduler-d8a55747dd90?source=collection_archive---------10-----------------------#2017-08-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="0c96" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">适应性学习率</p><p id="7bbd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在训练深度网络时，随着训练时期数量的增加，降低学习速率是有帮助的。这是基于直觉，即在高学习率的情况下，深度学习模型将拥有高动能。因此，它的参数向量会乱跳。因此，它不能停留在损失函数的更深和更窄的部分(局部最小值)。另一方面，如果学习速率非常小，那么系统的动能就会很低。因此，它会停留在损失函数的较浅和较窄的部分(假最小值)。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/df438fabf9dfa874ca31cf9bd21b34e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*iYWyu8hemMyaBlK6V-2vqg.png"/></div></figure><p id="5b24" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上图描述了高学习率将导致向量在局部最小值附近的随机往复运动，而低学习率将导致陷入错误的最小值。因此，知道何时降低学习速度可能很难找到答案。</p><p id="6c6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的实验基于<strong class="jp ir">阶跃衰减的原理。</strong>这里，我们每隔几个历元就用一个常数因子降低学习率。典型值可能是每5个时期将学习速率减少一半，或者每20个时期减少0.1。这些数字很大程度上取决于问题的类型和模型。你在实践中可能看到的一个启发是，在以固定的学习速率训练时观察验证误差，并且每当验证误差停止改善时，以一个常数(例如0.5)降低学习速率。</p><p id="1f2d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在实践中，<strong class="jp ir">步长衰减</strong>是首选，因为它更容易解释超参数，如衰减分数和以历元为单位的步长计时。此外，还发现它提供了学习率值的稳定性，这反过来有助于随机梯度下降显示出快速收敛和高成功率。</p><p id="98dc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实验步骤</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kt"><img src="../Images/47aa16088efeb5b36451be3f87c61f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*a_jdgq7zsB1235zI6BtaSw.png"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ku"><img src="../Images/1eaf41c6273fb23316f958e20e8ccf02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*BTBeFUSwyVvC-KyetULHlQ.png"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kv"><img src="../Images/3bf8f7e4a3c8d17a4eda9e470f529767.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*nV3it42i4i7hqRXtLClm7Q.png"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kw"><img src="../Images/3ebd7fb3e47f3716df0a0f7e4dc86473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*lW3vXDiO9tInSd9seZ2mlA.png"/></div></figure><p id="1d29" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实验结果</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kx"><img src="../Images/0ab04d532d69cdb80829f3cb1e6f1e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*zqtrQmWl4Rx-VTeIuULXww.png"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ky"><img src="../Images/d0f15a07bf302a8e80005b2b35a7db6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*IAqx2WgJJjIE5umnzOFU-A.png"/></div></figure><p id="6e1b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">源代码</p><div class="kz la gp gr lb lc"><a href="https://github.com/shree6791/Deep-Learning/blob/master/CNN/Cats%20and%20Dogs/keras/src/Intern%20Task%203.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ld ab fo"><div class="le ab lf cl cj lg"><h2 class="bd ir gy z fp lh fr fs li fu fw ip bi translated">shree 6791/深度学习</h2><div class="lj l"><h3 class="bd b gy z fp lh fr fs li fu fw dk translated">深度学习——这个知识库由Shreenidhi Sudhakar实施的深度学习项目组成。</h3></div><div class="lk l"><p class="bd b dl z fp lh fr fs li fu fw dk translated">github.com</p></div></div><div class="ll l"><div class="lm l ln lo lp ll lq kr lc"/></div></div></a></div></div></div>    
</body>
</html>