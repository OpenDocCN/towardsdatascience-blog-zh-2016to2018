<html>
<head>
<title>How Neural Networks “Learn”</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络如何“学习”</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-neural-network-learn-3b56c175b5ca?source=collection_archive---------11-----------------------#2018-10-12">https://towardsdatascience.com/how-neural-network-learn-3b56c175b5ca?source=collection_archive---------11-----------------------#2018-10-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/13dc15ec3716385196aa66fdfc50def4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9NOYioT9eMcl2EIP.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Source: <a class="ae kc" href="https://stats385.github.io/assets/img/grad_descent.png" rel="noopener ugc nofollow" target="_blank">https://stats385.github.io/assets/img/grad_descent.png</a></figcaption></figure><p id="0462" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我的第一个故事中，我解释了神经网络如何处理你的输入。在神经网络能够像前一篇文章那样进行预测之前，它必须经过一个预处理阶段。这个阶段控制神经网络在处理输入时使用的权重和偏差值。</p><p id="5293" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在神经网络生命周期中有两个阶段，一般来说，所有机器学习算法都是训练阶段和预测阶段。寻找权重和偏差值的过程发生在<strong class="kf ir">训练阶段</strong>。与此同时，神经网络处理我们的输入以产生预测的阶段发生在预测阶段，如在<a class="ae kc" href="https://medium.com/datadriveninvestor/how-neural-network-process-your-input-trained-neural-network-fd48f1bf310" rel="noopener">之前的</a>中。这一次，我将讨论<strong class="kf ir">神经网络如何在训练阶段获得正确的权重和偏差</strong>也称为“学习”以做出准确的预测(阅读:回归或分类)。</p><p id="8e6b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，神经网络如何获得最优的权重和偏差值呢？答案是通过一个误差梯度。在固定当前权重和偏移(最初随机生成)时，我们想知道的是当前权重和偏移值是太大还是太小<strong class="kf ir"> </strong>(我们需要<strong class="kf ir">减少还是增加</strong>我们的当前值吗？)<strong class="kf ir"> </strong>关于它们的最优值？以及它偏离了多少(<strong class="kf ir">我们需要减少或增加当前值多少</strong>？)从它们的最优值。我们寻找的梯度是误差相对于权重和偏差的导数。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/21973a31c94a5ab5d928afd81e92b5e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/1*7rVaUi_UgZmI4hFpQz4qTQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">where E is an error, W is weight and b is bias</figcaption></figure><p id="82e9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是为什么呢？因为我们想知道我们当前的权重和偏差如何影响神经网络误差的值，作为回答上段 2 个问题(减少或增加以及增加多少)的参考。<strong class="kf ir">我们如何得到梯度值</strong>是通过一个众所周知的算法叫做<strong class="kf ir">反向传播</strong>。<strong class="kf ir">我们如何利用通过反向传播获得的梯度</strong>来改善权重值和偏差是通过<strong class="kf ir">优化算法</strong>实现的。优化算法的一个例子是梯度下降，这是最简单和最常用的优化算法。它只是用获得的梯度值乘以<em class="lg">学习速率</em>常数来减少最近的权重和偏差值。什么是<em class="lg">学习率</em>以及更多细节，我们将在本帖中立即讨论。</p><p id="307f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们有一个如下的神经网络。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/5e42f4a8730ef6ee6f44ff4f9b579372.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*NebkovuJG8xVa_uUfAzNkw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Our neural network has a structure 3–2–2</figcaption></figure><p id="9277" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们有一个输入向量、偏差向量、权重矩阵和真值，如下所示</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi li"><img src="../Images/572db28d6344e3ebc4dd50e3a2ce859d.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*EjEq8ugzXD1vLVobQnuaMA.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/575043fda7013a0408ed68ddad42763d.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*AsJt46_XmWrtWDoGXnsrUA.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/fa443b5ac5323a84a43f4a043e9ac700.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*Rj5jSnB2fH7g8dt5W_RF-w.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/5042b30a45dba26305831587838da621.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*KMNUx9MnJYtgW2rMZPOnyQ.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/0f17b2054c00002e4ee14676f90b9285.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*tapLARBLfquFNNJ9k4a9dQ.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/f2cc837a1e35ebc8a7829287186e8179.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*Jw5VlhBD7RsVNds4qSG3mA.png"/></div></figure><p id="9697" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了明确起见，权重值的顺序为</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/361ebcd771613e8e57336ff30e573019.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*tf86F13pGnLthcM_a2Tndg.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/9fb7c8e6630a3f5e118d05d6707ad309.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*sPUQfS4oi1ZbW7dpIvnr1g.png"/></div></figure><p id="fdab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们做一下<em class="lg">向前传球</em> 。流程与<a class="ae kc" href="https://medium.com/datadriveninvestor/how-neural-network-process-your-input-trained-neural-network-fd48f1bf310" rel="noopener">上一篇</a>相同。在本演示中，我们对所有神经元使用的激活函数是 sigmoid 函数。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/c8c204a628b0feacfacef1d54a7f5908.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*kDnpaM1rL929zZrkpe8ftA.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/b13720ff74021ca9b41fd3df7d0764b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*FQaGl4siJGQjKn7VcprPmg.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/81bcec79a7fb1d4eaa6ba4ed41354aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*KZX0SSPjE4A93k7TLcsP1Q.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/2397b533b546623639fe16bf05c29a94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*xK1yCaA5m7qQOjnmVNkoRQ.png"/></div></figure><p id="02d6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们将 sigmoid 函数的输出值四舍五入到 4 位小数。在实际计算中，<strong class="kf ir">这样一轮会大大降低神经网络的精度</strong>。小数的个数对神经网络的准确性至关重要。我们这样做是为了简化计算，这样写作就不会太长。</p><p id="d4d5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们进行下一层之前，请注意下一层是最后一层。意味着<strong class="kf ir">下一层是输出层</strong>。在这一层，我们只做<em class="lg">纯线性</em>运算。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/3a11d15b28014766120c5645aae498e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*h0R8iM2xCp3cjZgtT46jEg.png"/></div></figure><p id="81ab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">是时候计算误差了。在这种情况下，我们使用<em class="lg">均方误差</em> (MSE)来计算输出神经元的误差。MSE 方程如下</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/374402379c4be635e7f34d1b9a006308.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*qXrMB555lBchMB5vs2OhJg.png"/></div></figure><p id="cc5a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的例子中，N = 1，因为我们只有 1 个数据，所以等式简化为</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/f8b4efccfdfc468b4b3cf4f58ba35103.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/format:webp/1*Wf3-o0fPjNRxIDEYyK964Q.png"/></div></figure><p id="faba" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们根据前面定义的真值(T)来计算输出层神经元的误差。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/e61421c0b52805e2b0e7231e7e0fdd6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*Ff4Lvg2y8ZWHD7ezFjY2RA.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lx"><img src="../Images/4d8d91a77fed534b3b814d51e2ae3ed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-YFbX72NhbzDGJT-9SQFRg.png"/></div></div></figure><p id="ca28" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是我们当前在输出层的误差。现在是通过反向传播(也称为反向传递)在层<strong class="kf ir">之间的每次交互中寻找相对于权重和偏差的误差梯度，然后应用梯度下降来最小化误差的时候了。<strong class="kf ir">反向传播仅仅是一个链式法则</strong>，它是如何工作的将立即讨论。现在，让我们找出我们在正向传递中使用的所有方程的导数。</strong></p><ol class=""><li id="717a" class="ly lz iq kf b kg kh kk kl ko ma ks mb kw mc la md me mf mg bi translated">E 关于 O 的导数</li></ol><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/3a27d7fe1429103aaf1066045089385f.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*iBwHlIE7OWdvPBJVeIZPTg.png"/></div></figure><p id="ff84" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.sigmoid (h)函数关于 P 的导数(纯线性运算的输出)</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/e31ac157beab6f2116530a7e4e49775f.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*IaOqxQmcDgQVmTOhVmoejw.png"/></div></figure><p id="efaa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">h 在哪里</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/2e5a958c2dfc8c8c62a54f2697a90058.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*GvQBmIdaqHtfrXCYO948fw.png"/></div></figure><p id="a582" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.关于权重(W)、偏差(b)和输入(h)的纯线性导数。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/36cac82236eac8b7ae9ecce067ee89de.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*wHl6HwRYlzDzW-pYGXapEA.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/468ba2813eba49bb7f41ad8f0089f89b.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*nmlbPgOPZuFImfK6ZiyAUw.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/1f6531a410a5b921145ace2bbc84dd3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*OBB3ZS-qycYPVQUn8FP7nA.png"/></div></figure><p id="dcc4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">purelin 在哪里</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/d247c2500a2176d7e6cf7014f9e0444f.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*G4TIVH8jsxfKCqK_RrFTVA.png"/></div></figure><p id="7bb9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中 l 是从 1 到 m 的数。</p><p id="c4a6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是我们所需要的，是时候应用反向传播了。我们首先寻找隐藏层和输出层之间的权重和偏差的梯度。为了寻找梯度，我们使用链式法则。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/fc7016a86a0c79764f1c94b4a6a93566.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*lTHWwARsx0NxZJgXsDdJ9A.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/a5e0fc09ad82fa8bb36c030f7791eee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*xgbHUDJTBe3gf9PBTWRSMA.png"/></div></figure><p id="9a57" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过应用这些，我们得到</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/875c5be7686a571e45ad33995692180c.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*X1qZBxWlpOdOe-KfCLEIKA.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/51ef1734d6c181f366bad9595604ac66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*Q8WnJ8PA3tRNGQIfZ1Ub6A.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/35418a65757b18c0b21389b7d37e6552.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*QJt_mHJuBpOyjzZ2_mzd9A.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/af1257a7234c32fd522fa56172c80873.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*Ht3nKi1QMq7OZRkNlxgOsw.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/7baf96f8ec62555e53b51849a556a17f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*SP0KKv7AafrQ7bN539SURQ.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/ec41e81fbefaa2bfa7df9be8f40c69df.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*umFJP9sb9J6KNb8KYcLJqg.png"/></div></figure><p id="e718" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是我们在隐藏层和输出层之间的渐变。现在，进入下一层。这里真正的挑战(不那么挑战)！但是不要担心，在这之后一切都会变得清晰和容易:)。</p><p id="f661" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">反向传播中的链式法则完全是关于神经元之间的路径</strong>。让我们收集信息吧！</p><ol class=""><li id="f2fc" class="ly lz iq kf b kg kh kk kl ko ma ks mb kw mc la md me mf mg bi translated">隐含层有<strong class="kf ir"> 2 个神经元，每个神经元在左侧(输入层和隐含层之间)用 3 个权值和 1 个偏置</strong>连接<strong class="kf ir">。</strong></li><li id="1122" class="ly lz iq kf b kg mv kk mw ko mx ks my kw mz la md me mf mg bi translated">在右侧，隐藏层中的每个神经元都与输出层中的 2 个神经元相连<strong class="kf ir">。</strong></li></ol><p id="56ac" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">这些信息对于求 W1 的梯度非常重要</strong>。从这些，我们想要找到的梯度是</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/0629afb997349d0032f9f390f5be73de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*qhueJ5rrQiYQyxTyJ9x5JQ.png"/></div></figure><p id="01da" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在哪里</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/27735703bd12a8f92b47eeace14a45c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*WP7kFaGCbPJ7stJFJacjjA.png"/></div></figure><p id="475f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">添加了从我们关注的权重到输出层的所有可能路径。这就是为什么上面的等式中有两项之和。现在，让我们计算 W1 的真实梯度。</strong></p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/6f6cb0c498bafa2db67d56e103d26e1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jh7_6xkY3sF2hpAw6bXFYQ.png"/></div></div></figure><p id="82f4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代入 E 对 h 的偏导数，我们得到</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nd"><img src="../Images/ac91e4d4165edb9db1d5004dd556a7d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*hD74NHdIpptOK9NcilZuEQ.png"/></div></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/9c7e3376a94d59a35a1697614f5ed525.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*gPNLPxuqLibYPxUFRaR7IQ.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nf"><img src="../Images/eb62a659db238e70a4dd531e16878974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hOTMQQvDRkn071xU-mU5NA.png"/></div></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/d8059c26eca285f3b5d4a944ac24d6ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*ksnZVf60w6i1fkDfzQrBsw.png"/></div></figure><p id="771b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在谈谈偏见，又称 b1</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/3c573eef42e34ae0dc0beb19cb9ae6ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*VN7EFTOUS6LUL-QuAoUChw.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/6c1c3affb0e37fbb22e0f6c0ae9c87a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*JeLcCTcpSjyuVWToSxhSUQ.png"/></div></div></figure><p id="2f92" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">这就是反向传播算法作用的终点</strong>。现在，让我们来看看优化算法。优化算法是关于如何利用我们已经获得的梯度来校正现有的权重和偏差。我们选择的优化算法是梯度下降法。梯度下降法通过下面的等式校正权重和偏差。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/a2613d6ef84e96d8def82ccc699257ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*--bZXF2wfVjYVt-HXQ7s7A.png"/></div></figure><p id="495c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中 W’是新的权重，W 是权重，a 是学习常数，梯度是我们从反向传播获得的梯度。<strong class="kf ir">学习常数是至关重要的常数</strong>，因为如果这个常数太大，结果将不会收敛，如果太小，需要更多的迭代，这意味着训练阶段将更加耗时。假设我们有一个等于 0.02 的<strong class="kf ir">学习常数。</strong></p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/b5ac10c4fde309e9145e70a3996e4653.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*he-zet8r8oxBYUNDRaaS5A.png"/></div></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/5bd9661eedae2f8937cd7dd9b11cc9bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*MNKgkwFCQGMs56n_vfzNpA.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/c48b0627e55b6f311b6c9ee9b1c36f3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Uk9JIsB77KuIW_XwOCJXQ.png"/></div></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/e56569c8bfb6ee1db5807c42303e39ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*33AtmOU70E70xTLKAlzCUA.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/9e261e4b67ad8a8b9546a1acd52cd068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*kDa1uspEFw74fiZRmRcBxw.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/47eb4009aa8a491c00047d4c825e8010.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*gnBwKx7amwwe28-huDcD8w.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/fb114fcdaceccb8cbcb9a0120cc3be09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*0zbpEUxxHx099B31UakOvw.png"/></div></figure><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/837e65ac25dba91900e1931c1efde5f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*5S4iqB-YFWvFTc1t__6TsQ.png"/></div></figure><p id="8e36" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">依此类推，<strong class="kf ir">该过程将被重复</strong>(具有将被输入的相同输入)直到达到所需的迭代次数<strong class="kf ir">或目标误差</strong>。</p><p id="6490" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是神经网络一般是如何“学习”的。如果我有更多的空闲时间(当然还有好心情)，我会用 numpy 分享 python 中多层感知器(另一种“普通神经网络”的名称，这是我们这里的重点)的源代码。再见。</p><p id="1276" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">我的另一个神经网络系列:</strong></p><ol class=""><li id="30fe" class="ly lz iq kf b kg kh kk kl ko ma ks mb kw mc la md me mf mg bi translated"><a class="ae kc" href="https://medium.com/datadriveninvestor/how-neural-network-process-your-input-trained-neural-network-fd48f1bf310" rel="noopener">神经网络如何处理你的输入(经过训练的神经网络)</a></li><li id="bb8e" class="ly lz iq kf b kg mv kk mw ko mx ks my kw mz la md me mf mg bi translated"><strong class="kf ir">神经网络如何“学习”</strong></li><li id="9e67" class="ly lz iq kf b kg mv kk mw ko mx ks my kw mz la md me mf mg bi translated"><a class="ae kc" href="https://medium.com/datadriveninvestor/a-simple-way-to-know-how-important-your-input-is-in-neural-network-86cbae0d3689" rel="noopener">知道你的输入在神经网络中有多重要的简单方法</a></li></ol></div></div>    
</body>
</html>