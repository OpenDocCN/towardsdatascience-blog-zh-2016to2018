<html>
<head>
<title>Radial Basis Functions Neural Networks — All we need to know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">径向基函数神经网络—我们需要知道的一切</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/radial-basis-functions-neural-networks-all-we-need-to-know-9a88cc053448?source=collection_archive---------0-----------------------#2017-08-18">https://towardsdatascience.com/radial-basis-functions-neural-networks-all-we-need-to-know-9a88cc053448?source=collection_archive---------0-----------------------#2017-08-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jp jq jr js gh gi paragraph-image"><div class="gh gi jo"><img src="../Images/9ff0de44b43025acecda2f2a990c13b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*U5y_-UuqGe1lxdgisM32xw.png"/></div></figure><p id="cf8f" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated"><strong class="jx ir">单感知器</strong> / <strong class="jx ir">多层感知器(MLP) </strong>中的⁃，我们只有线性可分性，因为它们是由输入层和输出层(MLP中的一些隐藏层)组成的</p><p id="f335" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">比如⁃，与、或函数是<strong class="jx ir">线性</strong>可分的&amp;异或函数是<strong class="jx ir">不</strong>线性可分的。</p><figure class="ku kv kw kx gt js gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi kt"><img src="../Images/52d2a39ee57948a48129828c06ab4857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CyGlr8VjwtQGeNsuTUq3HA.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Linear-separability of AND, OR, XOR functions</figcaption></figure><p id="70c1" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃我们至少需要<strong class="jx ir">一个隐藏层</strong>来导出非线性<strong class="jx ir">分离</strong>。</p><p id="5932" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃我们的RBNN它做的是，它将输入信号转换成另一种形式，然后可以<strong class="jx ir">馈送</strong>到网络中<strong class="jx ir">以获得线性可分性。</strong></p><p id="e4a0" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃ RBNN在结构上与感知器(MLP)相同。</p><figure class="ku kv kw kx gt js gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/8024d173ad9fb692d3d2b1ab5fcdace3.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*aNeBDke8kLPPCuu29tq-Nw.jpeg"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Distinction between MLP and RBF</figcaption></figure><p id="510b" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃ RBNN由<strong class="jx ir">输入、</strong>和<strong class="jx ir">输出</strong>层组成。RBNN被<strong class="jx ir">严格限制</strong>为只有<strong class="jx ir">一个隐含层</strong>。我们称这个隐藏层为<strong class="jx ir">特征向量。</strong></p><p id="8750" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃ RBNN <strong class="jx ir">增加特征向量的维数</strong>。</p><figure class="ku kv kw kx gt js gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/8aaa96b8d2c3bd93aaf2ecbe62a81aea.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/1*umho3WaIXmB2qBoHTgw0MA.gif"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Simplest diagram shows the architecture of RBNN</figcaption></figure><figure class="ku kv kw kx gt js gh gi paragraph-image"><div class="gh gi li"><img src="../Images/79208092d46566e3f052386d9fa83c6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*kBCuq8ckCo67oyEiPW9Pdg.jpeg"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Extended diagram shows the architecture of RBNN with hidden functions.</figcaption></figure><p id="fd6f" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃:在我们进行<strong class="jx ir">分类问题之前，我们将<strong class="jx ir">非线性传递函数</strong>应用于特征向量。</strong></p><p id="8b4f" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃当我们增加特征向量的维数时，特征向量的线性可分性增加。</p><blockquote class="lj"><p id="6d7a" class="lk ll iq bd lm ln lo lp lq lr ls ks dk translated">非线性可分离问题(模式分类问题)在高维空间中比在低维空间中高度可分离。</p><p id="49fd" class="lk ll iq bd lm ln lo lp lq lr ls ks dk translated">【<strong class="ak"> <em class="jn">盖兹定理</em> </strong>】</p></blockquote><p id="42dc" class="pw-post-body-paragraph jv jw iq jx b jy lt ka kb kc lu ke kf kg lv ki kj kk lw km kn ko lx kq kr ks ij bi translated">⁃什么是径向基函数？</p><blockquote class="lj"><p id="482c" class="lk ll iq bd lm ln lo lp lq lr ls ks dk translated">⁃我们定义一个受体= t</p><p id="a052" class="lk ll iq bd lm ln lo lp lq lr ls ks dk translated">⁃:我们在受体周围画正面图。</p><p id="95e1" class="lk ll iq bd lm ln lo lp lq lr ls ks dk translated">⁃高斯函数通常用于弧度基函数(正射映射)。所以我们定义径向距离r = ||x- t||。</p></blockquote><figure class="lz ma mb mc md js gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/1f49a46bb8e3d1bd259440598a0ae6ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*AdPrlGRh6usbZNgFWTWaEQ.jpeg"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Radial distance and Radial Basis function with confrontal map</figcaption></figure><p id="e9ed" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">高斯径向函数:=</p><blockquote class="lj"><p id="968c" class="lk ll iq bd lm ln lo lp lq lr ls ks dk translated">ϕ(r) = exp (- r /2 <strong class="ak"> σ </strong>)</p></blockquote><blockquote class="me mf mg"><p id="f2e9" class="jv jw mh jx b jy lt ka kb kc lu ke kf mi lv ki kj mj lw km kn mk lx kq kr ks ij bi translated">其中σ &gt; 0</p></blockquote><figure class="ku kv kw kx gt js gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/9f23c7709d84951677a83fe3b2a99d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*c6KMMqfhmXdJda9LBGmhJw.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Classification only happens on the second phase, where linear combination of hidden functions are driven to output layer.</figcaption></figure><h1 id="8929" class="mm mn iq bd mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj bi translated">⁃的例子。XOR函数:-</h1><p id="c0eb" class="pw-post-body-paragraph jv jw iq jx b jy nk ka kb kc nl ke kf kg nm ki kj kk nn km kn ko no kq kr ks ij bi translated">⁃，我有4个输入，我不会在这里增加特征向量的维数。因此，我将在这里选择2个受体。对于每个变换函数ϕ(x)，我们将有每个受体t</p><p id="0f00" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃现在考虑RBNN结构，</p><blockquote class="lj"><p id="54cb" class="lk ll iq bd lm ln lo lp lq lr ls ks dk translated">⁃ P := #个输入特征/值。</p><p id="2373" class="lk ll iq bd lm ln lo lp lq lr ls ks dk translated">⁃ M = #变换的矢量维数(隐藏层宽度)。所以M ≥ P通常为。</p><p id="089c" class="lk ll iq bd lm ln lo lp lq lr ls ks dk translated">⁃在隐藏层的每个节点上，执行一组非线性的弧度基函数。</p><p id="9ec7" class="lk ll iq bd lm ln lo lp lq lr ls ks dk translated">⁃输出c将保持与分类问题相同(预先定义一定数量的类别标签)。</p></blockquote><figure class="lz ma mb mc md js gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi np"><img src="../Images/88d9e29b3145ac0469113dfa9579a6dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tHaXRnx9Tj5LkxqynnGoSw.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Architecture of XOR RBNN</figcaption></figure><figure class="ku kv kw kx gt js gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi nq"><img src="../Images/037fd77629974a1e55824bcfce9faaf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8nLnA4vPDRJOBV7H4tv5cQ.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Transformation function with receptors and variances.</figcaption></figure><figure class="ku kv kw kx gt js gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi nr"><img src="../Images/27c82b79f44e2626b58f83e2ca12992b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MVVBo_E4SBodjPXreMbJVw.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Output → linear combination of transformation function is tabulated.</figcaption></figure><p id="7e11" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">只有隐藏层中的⁃节点执行弧度基变换功能。</p><p id="db8c" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃输出层执行隐藏层输出的线性组合，以给出输出层的最终概率值。</p><p id="39df" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃所以分类只做了@ <strong class="jx ir"> <em class="mh">(隐藏层→输出层)</em> </strong></p><h1 id="a4cb" class="mm mn iq bd mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj bi translated">训练RBNN :-</h1><p id="ab12" class="pw-post-body-paragraph jv jw iq jx b jy nk ka kb kc nl ke kf kg nm ki kj kk nn km kn ko no kq kr ks ij bi translated">⁃ <strong class="jx ir">首先</strong>，我们要用<strong class="jx ir">反向传播训练<strong class="jx ir">隐层</strong>。</strong></p><p id="d981" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃神经网络训练(反向传播)是一种<strong class="jx ir">曲线拟合方法</strong>。在<strong class="jx ir">训练</strong>阶段<strong class="jx ir">拟合</strong>一条<strong class="jx ir">非线性曲线</strong>。它通过随机逼近，我们称之为反向传播。</p><p id="3eab" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃对于隐藏层中的每个节点，我们必须找到<strong class="jx ir"> t </strong>(受体)&amp;方差(<strong class="jx ir"> σ </strong>)【方差——径向基函数的扩散】</p><p id="2896" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃在<strong class="jx ir">第二个</strong>训练阶段，我们要<strong class="jx ir">更新</strong>中<strong class="jx ir">隐藏层&amp;输出层<strong class="jx ir">之间的</strong>加权向量。</strong></p><p id="4305" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">在隐藏层中，<strong class="jx ir">的每个</strong>节点代表<strong class="jx ir">的每个</strong>变换基函数。<strong class="jx ir">函数的任意</strong>可以满足非线性可分性，甚至函数集合的<strong class="jx ir">组合</strong>也可以满足非线性可分性。</p><p id="3605" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃:所以在我们的隐藏层变换中，所有的非线性项都包括在内。比如说X+Y+5XY；它全部包含在超曲面方程中(X和Y是输入)。</p><p id="2ccd" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃因此，第一阶段的训练是由<strong class="jx ir">聚类算法完成的。</strong>我们定义我们需要的<strong class="jx ir">个聚类中心</strong>。通过聚类算法，我们计算聚类中心，然后将其指定为每个隐藏神经元的<strong class="jx ir">受体</strong>。</p><p id="2681" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃:我必须将n个样本或观测值聚类成m个聚类(N &gt; M)。</p><p id="243a" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃所以输出“集群”就是“受体”。</p><p id="7682" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">对于每个受体，我可以找到的方差为"<strong class="jx ir">各个受体之间距离的平方和&amp;每个聚类最近的样本</strong> " := 1/N * ||X — t||</p><p id="c993" class="pw-post-body-paragraph jv jw iq jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks ij bi translated">⁃对第一个训练阶段的解释是<strong class="jx ir">“特征向量被投影到变换的空间上”。</strong></p><figure class="ku kv kw kx gt js gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/65b65f416df8c1e909291358dcb491a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/1*cwfZauLq5T97ha_5-kQQag.gif"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Complex diagram depicting the RBNN</figcaption></figure><h2 id="5bce" class="ns mn iq bd mo nt nu dn ms nv nw dp mw kg nx ny na kk nz oa ne ko ob oc ni od bi translated">使用径向基神经网络比MLP的优势:-</h2><blockquote class="me mf mg"><p id="fe23" class="jv jw mh jx b jy jz ka kb kc kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ks ij bi translated">1.RBNN中的训练比多层感知器(MLP)中的<strong class="jx ir">快</strong>→需要<strong class="jx ir">许多交互</strong>在MLP。</p><p id="c3cb" class="jv jw mh jx b jy jz ka kb kc kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ks ij bi translated">2.我们可以很容易地解释RBNN隐含层中每个节点的意义/ <strong class="jx ir">功能是什么。这是MLP的<strong class="jx ir">难</strong>。</strong></p><p id="6fad" class="jv jw mh jx b jy jz ka kb kc kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ks ij bi translated">3.(什么应该是隐藏层的节点数<strong class="jx ir"> &amp; </strong>隐藏层的节点数)这个<strong class="jx ir">参数化</strong>在MLP很难。但这在RBNN中是没有的。</p><p id="fa85" class="jv jw mh jx b jy jz ka kb kc kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ks ij bi translated">4.<strong class="jx ir">分类</strong>在RBNN比在MLP要花更多的时间。</p></blockquote></div></div>    
</body>
</html>