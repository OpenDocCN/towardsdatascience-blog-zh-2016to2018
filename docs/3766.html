<html>
<head>
<title>Support Vector Machines(SVM) — An Overview</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机(SVM)——概述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989?source=collection_archive---------1-----------------------#2018-06-16">https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989?source=collection_archive---------1-----------------------#2018-06-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/3f936f2d062dd4e71a7c1edf9bf52c7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dh0lzq0QNCOyRlX1Ot4Vow.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">SVM classifier</figcaption></figure><p id="f3b7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">机器学习涉及预测和分类数据，为此，我们根据数据集采用各种机器学习算法。</p><p id="d1d8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">SVM 或支持向量机是用于分类和回归问题的线性模型。它可以解决线性和非线性问题，并能很好地解决许多实际问题。SVM 的想法很简单:算法创建一条线或一个超平面，将数据分类。</p><p id="6b84" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在这篇博文中，我打算提供一个关于支持向量机的高层次概述。我将讨论支持向量机背后的理论，它在非线性可分数据集上的应用，以及用 Python 实现支持向量机的一个快速示例。在接下来的文章中，我将探索算法背后的数学，并挖掘引擎盖下。</p><blockquote class="la lb lc"><p id="af18" class="kc kd ld ke b kf kg kh ki kj kk kl km le ko kp kq lf ks kt ku lg kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="iq">论</em> </strong></p></blockquote><p id="0258" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在第一级近似下，支持向量机所做的是找到两类数据之间的分隔线(或超平面)。SVM 是一种算法，它将数据作为输入，如果可能的话，输出一条线来分隔这些类。</p><p id="9bd2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们从一个问题开始。假设您有一个如下所示的数据集，您需要将红色矩形从蓝色椭圆中分类出来(让我们从负面中找出正面)。所以你的任务是找到一条理想的线，将这个数据集分成两类(比如红色和蓝色)。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/581736dc0836e94fe04c42753cf07e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*VDATmWG1E1ZNg7hdasOh5g.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Find an ideal line/ hyperplane that separates this dataset into red and blue categories</figcaption></figure><p id="c9a7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">不是什么大任务，对吧？</p><p id="f984" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">但是，正如您注意到的，并没有一个独特的行来完成这项工作。事实上，我们有一条无限长的线可以把这两个阶级分开。那么 SVM 是如何找到理想伴侣的呢？？？</p><p id="593c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们采取一些可能的候选人，自己找出答案。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/4fc171b3a4bf4337abe655ad6d28a80f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*AMR3v-jCvUMXPUtQskzxmQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Which line according to you best separates the data???</figcaption></figure><p id="df98" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这里有两条候选线，绿色的线和黄色的线。根据你的说法，哪条线最适合分隔数据？</p><p id="7ae6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果你选择了黄线，那么恭喜你，因为那是我们正在寻找的线。在这种情况下，黄线分类更好，这在视觉上非常直观。但是，我们需要一些具体的东西来固定我们的线。</p><p id="3fed" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">上图中的绿线与红色类相当接近。虽然它对当前数据集进行分类，但它不是一条一般化的线，在机器学习中，我们的目标是获得一个更一般化的分隔符。</p><blockquote class="la lb lc"><p id="5e22" class="kc kd ld ke b kf kg kh ki kj kk kl km le ko kp kq lf ks kt ku lg kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="iq"> SVM 寻找最佳路线的方法</em> </strong></p></blockquote><p id="2f46" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">根据 SVM 算法，我们从两个类中找到最接近直线的点。这些点称为支持向量。现在，我们计算直线和支持向量之间的距离。这个距离叫做边缘。我们的目标是利润最大化。边缘最大的超平面是最优超平面。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/f27b3a2a6cc24364220287211af152f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*irg_jfdAar9gfe0j-Q04vQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Optimal Hyperplane using the SVM algorithm</figcaption></figure><p id="aad2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因此，SVM 试图以这样一种方式做出一个决定边界，即两个阶级(那条街)之间的间隔尽可能宽。</p><p id="17be" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">很简单，不是吗？让我们考虑一个有点复杂的数据集，它不是线性可分的。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/eb29260a6e23ea6d18dbd275bd8a511d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*YY8BOq-WPjRp4QkO1Xoulw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Non-linearly separable data</figcaption></figure><p id="b24a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这个数据显然不是线性可分的。我们无法画出一条直线来对这些数据进行分类。但是，这些数据可以转换成高维的线性可分数据。让我们增加一个维度，称之为 z 轴。让 z 轴上的坐标受约束支配，</p><p id="9413" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">z = x +y</p><p id="b76a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">所以，基本上 z 坐标是该点到原点距离的平方。让我们把数据标在 z 轴上。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/aac2390e8c4f9c968c9c311609d32159.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*a_TQSZ_H1UOA3BV299qtJQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Dataset on higher dimension</figcaption></figure><p id="c62a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在数据显然是线性可分的。设高维空间中分隔数据的紫线为 z=k，其中 k 为常数。因为，z=x +y 我们得到 x+y = k；这是一个圆的方程式。所以，我们可以用这种变换，把高维空间中的线性分隔符投射回原始维度。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/8eef83f25519773345dff5815282297d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*WTg1NgtzaoUoQP7N5HucSA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Decision boundary in original dimensions</figcaption></figure><p id="5ef0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因此，我们可以通过向数据添加一个额外的维度来对数据进行分类，使其成为线性可分的，然后使用数学变换将决策边界投影回原始维度。但是为任何给定的数据集找到正确的转换并不容易。谢天谢地，我们可以在 sklearn 的 SVM 实现中使用内核来完成这项工作。</p><blockquote class="la lb lc"><p id="d1bd" class="kc kd ld ke b kf kg kh ki kj kk kl km le ko kp kq lf ks kt ku lg kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="iq">超平面</em> </strong></p></blockquote><p id="888a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在我们理解了 SVM 逻辑，让我们正式定义超平面。</p><blockquote class="la lb lc"><p id="eb39" class="kc kd ld ke b kf kg kh ki kj kk kl km le ko kp kq lf ks kt ku lg kw kx ky kz ij bi translated"><strong class="ke ir">n 维欧几里得空间中的超平面是该空间的平坦的 n-1 维子集，它将该空间分成两个不相连的部分。</strong></p></blockquote><p id="8671" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">例如，让我们假设一条线是我们的一维欧几里得空间(也就是说，我们的数据集位于一条线上)。现在在这条线上选择一个点，这个点将这条线分成两部分。线有 1 维，而点有 0 维。所以一个点就是这条线的超平面。</p><p id="a4c3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于二维，我们看到分隔线是超平面。类似地，对于三维，具有二维的平面将 3d 空间分成两部分，因此充当超平面。因此，对于一个 n 维空间，我们有一个 n-1 维的超平面把它分成两部分</p><blockquote class="la lb lc"><p id="0a53" class="kc kd ld ke b kf kg kh ki kj kk kl km le ko kp kq lf ks kt ku lg kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="iq">代码</em> </strong></p></blockquote><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="920a" class="lr ls iq ln b gy lt lu l lv lw">import numpy as np<br/>X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])<br/>y = np.array([1, 1, 2, 2])</span></pre><p id="1d84" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们在 X 中有我们的点，在 y 中有它们所属的类。</p><p id="c7fa" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在我们用上面的数据集训练我们的 SVM 模型。对于这个例子，我使用了线性核。</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="69ef" class="lr ls iq ln b gy lt lu l lv lw">from sklearn.svm import SVC<br/>clf = SVC(kernel='linear')<br/>clf.fit(X, y)</span></pre><p id="330d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">预测新数据集的类别</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="750d" class="lr ls iq ln b gy lt lu l lv lw">prediction = clf.predict([[0,6]])<br/></span></pre><blockquote class="la lb lc"><p id="5729" class="kc kd ld ke b kf kg kh ki kj kk kl km le ko kp kq lf ks kt ku lg kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="iq">调谐参数</em> </strong></p></blockquote><p id="c051" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">参数是在创建分类器时传递的参数。以下是 SVM 的重要参数-</p><p id="1fc8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">1】C:</strong></p><p id="4b2c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">它控制平滑决策边界和正确分类训练点之间的折衷。大的 c 值意味着你将正确地得到更多的训练点。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/ec45e64585d3f1fbde7631a325095ebe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*KsO48nIV0ohZzQ_ZPstoOA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Smooth decision boundary vs classifying all points correctly</figcaption></figure><p id="b2de" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">考虑一个如上图所示的例子。我们可以为这个数据集绘制许多决策边界。考虑一条直线(绿色)决策边界，这非常简单，但代价是几个点被错误分类。这些错误分类的点称为异常值。我们也可以做一些更摇摆不定的东西(天蓝色的决策边界),但我们可能会得到所有正确的训练点。当然，像这样非常错综复杂的东西的代价是，它很可能不能很好地推广到我们的测试集。因此，如果你着眼于精度，更简单、更直接的方法可能是更好的选择。较大的 c 值意味着你将得到更复杂的决策曲线来拟合所有的点。弄清楚你有多希望有一个平滑的决策边界，而不是一个正确的决策边界，是机器学习艺术的一部分。因此，为您的数据集尝试不同的 c 值，以获得完美平衡的曲线并避免过度拟合。</p><p id="1c87" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 2】伽玛:</strong></p><p id="d316" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">它定义了单个训练示例的影响范围。如果伽马值较低，则意味着每个点都有很远的范围，反之，伽马值较高，则意味着每个点都有很近的范围。</p><p id="90ca" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果γ值非常高，那么决策边界将只取决于非常靠近该线的点，这实际上会导致忽略一些离决策边界非常远的点。这是因为更近的点得到更多的权重，这导致了曲线的摆动，如上图所示。另一方面，如果伽玛值很低，即使远点也会得到相当大的权重，我们会得到更线性的曲线。</p><blockquote class="la lb lc"><p id="0f95" class="kc kd ld ke b kf kg kh ki kj kk kl km le ko kp kq lf ks kt ku lg kw kx ky kz ij bi translated"><strong class="ke ir">中的<em class="iq">即将出现</em>中的</strong></p></blockquote><p id="652a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我将探索 SVM 算法和最优化问题背后的数学。</p><blockquote class="la lb lc"><p id="bbee" class="kc kd ld ke b kf kg kh ki kj kk kl km le ko kp kq lf ks kt ku lg kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="iq">结论</em> </strong></p></blockquote><p id="a701" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我希望这篇博文有助于理解支持向量机。<strong class="ke ir"> <em class="ld">写下你的想法、反馈或建议</em> </strong>如果有的话。</p><figure class="li lj lk ll gt jr"><div class="bz fp l di"><div class="lx ly l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="ak">Connect with the Raven team on </strong><a class="ae lz" href="https://t.me/ravenprotocol" rel="noopener ugc nofollow" target="_blank"><strong class="ak">Telegram</strong></a></figcaption></figure></div></div>    
</body>
</html>