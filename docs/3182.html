<html>
<head>
<title>How to use PySpark on your computer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在你的电脑上使用PySpark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-pyspark-on-your-computer-9c7180075617?source=collection_archive---------2-----------------------#2018-04-17">https://towardsdatascience.com/how-to-use-pyspark-on-your-computer-9c7180075617?source=collection_archive---------2-----------------------#2018-04-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1000" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我发现对于大多数人来说，在本地机器上开始使用Apache Spark(这里将重点介绍PySpark)有点困难。有了这个简单的教程，你会很快到达那里！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/fe3e106c18e1a481ba2e125c6ef3afe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J6jWDf1eYu3U6SHvfAEt1A.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="https://www.mytectra.com/apache-spark-and-scala-training.html" rel="noopener ugc nofollow" target="_blank">https://www.mytectra.com/apache-spark-and-scala-training.html</a></figcaption></figure><p id="5c3c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我假设您知道Apache Spark是什么，PySpark也是什么，但是如果您有问题，请不要介意问我！哦，你可以在这里查看我不久前做的一个快速介绍。</p><p id="a096" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"><em class="ls">$符号将表示在shell中运行(但不要复制该符号)。</em> </strong></p><h1 id="96c8" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">在Jupyter运行PySpark</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ml"><img src="../Images/13a807dfaf05666131f2d01c698339df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m87_Htb_9Pstq0UcvNJ49w.png"/></div></div></figure><ol class=""><li id="dfe5" class="mm mn iq ky b kz la lc ld lf mo lj mp ln mq lr mr ms mt mu bi translated">安装Jupyter笔记本</li></ol><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="86d7" class="na lu iq mw b gy nb nc l nd ne">$ pip install jupyter</span></pre><p id="6e77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.安装PySpark</p><p id="0665" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">确保您的计算机上安装了<strong class="ky ir"> Java 8 </strong>或更高版本。当然，你也会需要Python(我推荐<a class="ae kv" href="https://www.continuum.io/downloads" rel="noopener ugc nofollow" target="_blank"> Anaconda </a>的&gt; Python 3.5)。</p><p id="d487" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在访问<a class="ae kv" href="http://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank"> Spark下载页面</a>。选择最新的Spark版本，这是一个针对Hadoop的预构建包，并直接下载。</p><p id="f4ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你想得到蜂巢的支持或者更多有趣的东西，你必须自己建立你的星火分布-&gt; <a class="ae kv" href="http://spark.apache.org/docs/latest/building-spark.html" rel="noopener ugc nofollow" target="_blank">建立星火</a>。</p><p id="74fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将其解压缩并移动到/opt文件夹中:</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="9882" class="na lu iq mw b gy nb nc l nd ne">$ tar -xzf spark-2.3.0-bin-hadoop2.7.tgz<br/>$ mv spark-2.3.0-bin-hadoop2.7 /opt/spark-2.3.0</span></pre><p id="b12a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">创建一个符号链接(这将让您拥有多个spark版本):</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="a335" class="na lu iq mw b gy nb nc l nd ne">$ ln -s /opt/spark-2.3.0 /opt/spark̀</span></pre><p id="5817" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，告诉你的bash(或zsh等。)哪里找火花。为此，通过在~/中添加以下行来配置$PATH变量。bashrc(或者~/。zshrc)文件:</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="5eef" class="na lu iq mw b gy nb nc l nd ne">export SPARK_HOME=/opt/spark<br/>export PATH=$SPARK_HOME/bin:$PATH</span></pre><p id="82b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在要在Jupyter中运行PySpark，您需要更新PySpark驱动程序环境变量。就把这几行加到你的~/。bashrc(或者~/。zshrc)文件:</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="4616" class="na lu iq mw b gy nb nc l nd ne">export PYSPARK_DRIVER_PYTHON=jupyter<br/>export PYSPARK_DRIVER_PYTHON_OPTS='notebook'</span></pre><p id="d56b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">重启你的终端并启动PySpark:</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="c8ae" class="na lu iq mw b gy nb nc l nd ne">$ pyspark</span></pre><p id="c0b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，这个命令应该在您的web浏览器中启动一个Jupyter笔记本。点击“新建”&gt;“笔记本Python[默认]”，创建一个新的笔记本。瞧，您的计算机中有一个SparkContext和SqlContext(或Spark &gt; 2.x的SparkSession ),可以在笔记本中运行PySpark(运行一些<a class="ae kv" href="http://spark.apache.org/examples.html" rel="noopener ugc nofollow" target="_blank">示例</a>来测试您的环境)。</p><h1 id="4f08" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">在您最喜欢的IDE上运行PySpark</h1><p id="45c8" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">有时候你需要一个完整的IDE来创建更复杂的代码，PySpark默认不在sys.path上，但这并不意味着它不能作为一个常规库使用。您可以通过在运行时将PySpark添加到sys.path来解决这个问题。findspark包<a class="ae kv" href="https://github.com/minrk/findspark" rel="noopener ugc nofollow" target="_blank">会帮你做到这一点。</a></p><p id="0e6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要安装findspark，只需输入:</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="2142" class="na lu iq mw b gy nb nc l nd ne">$ pip install findspark</span></pre><p id="e233" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后在您的IDE上(我使用PyCharm)初始化PySpark，只需调用:</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="9510" class="na lu iq mw b gy nb nc l nd ne">import findspark<br/>findspark.init()</span><span id="5bf3" class="na lu iq mw b gy nk nc l nd ne">import pyspark<br/>sc = pyspark.SparkContext(appName="myAppName")</span></pre><p id="3b00" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">仅此而已。很简单，对吧？下面是一个完整的独立应用程序示例，用于在本地测试PySpark(使用上面解释的conf):</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="b5ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你有什么要补充的，或者只是有问题，请提出来，我会尽力帮助你。</p></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><p id="8981" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">编辑由于贡献巨大:)——————&gt;&gt;</strong></p><h1 id="a8fa" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">Jupyter笔记本Python，Scala，R，Spark，Mesos栈</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/9b8a997040d00845d3bfe0ed164ecf6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2QYim4bJ9LyO1pziQNJXMA.jpeg"/></div></div></figure><p id="faa1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Docker就像一个轻量级的“虚拟机”(Docker技术上提供的是“映像”和“容器”而不是虚拟机。)，就好像您有一台完整的第二台计算机，它有自己的操作系统和文件，就在您的真实计算机中。您可以从您的真实计算机登录到这台机器，并像通过ssh登录到另一台远程计算机一样使用它。</p><div class="nv nw gp gr nx ny"><a href="https://github.com/jupyter/docker-stacks/tree/master/all-spark-notebook" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd ir gy z fp od fr fs oe fu fw ip bi translated">jupyter/docker-stack</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">Docker-stacks——Docker中现成的Jupyter应用程序的自以为是的堆栈。</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">github.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om kp ny"/></div></div></a></div><p id="e729" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">只需到那里并按照步骤操作，就可以获得Spark的完整“容器化”版本(2.3和Hadoop 2.7)</p><h2 id="ba10" class="na lu iq bd lv on oo dn lz op oq dp md lf or os mf lj ot ou mh ln ov ow mj ox bi translated">基本用途</h2><p id="3919" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">以下命令启动一个容器，笔记本服务器使用随机生成的身份验证令牌在端口8888上侦听HTTP连接。</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="db49" class="na lu iq mw b gy nb nc l nd ne">$ docker run -it --rm -p 8888:8888 jupyter/pyspark-notebook</span></pre><h1 id="255d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">Pip安装</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/cf06f20b715373fab3b55dd1bab3310a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*AMf_w5QH_YydSGncTRgchw.png"/></div></figure><p id="3c5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种打包目前是试验性的，在未来的版本中可能会改变(尽管我们会尽最大努力保持兼容性)。</p><p id="8742" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Spark的Python打包并不打算取代所有其他用例。Spark的Python打包版本适合与现有集群(无论是Spark standalone、YARN还是Mesos)进行交互，但不包含设置您自己的独立Spark集群所需的工具。</p><p id="eeb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要安装它，只需运行:</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="e56e" class="na lu iq mw b gy nb nc l nd ne">$ pip install pyspark</span></pre></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><p id="5cb3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最初发布在我的<a class="ae kv" href="https://www.linkedin.com/pulse/how-use-pyspark-your-computer-favio-v%C3%A1zquez/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上。</p><p id="44f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您想联系我，请务必在twitter上关注我:</p><div class="nv nw gp gr nx ny"><a href="https://twitter.com/faviovaz" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd ir gy z fp od fr fs oe fu fw ip bi translated">法维奥·巴斯克斯(@法维奥·巴斯克斯)|推特</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">Favio Vázquez的最新推文(@FavioVaz)。数据科学家。物理学家和计算工程师。我有一个…</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">twitter.com</p></div></div><div class="oh l"><div class="oz l oj ok ol oh om kp ny"/></div></div></a></div><p id="f7f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">资源:</p><ul class=""><li id="46a0" class="mm mn iq ky b kz la lc ld lf mo lj mp ln mq lr pa ms mt mu bi translated"><a class="ae kv" href="https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594" rel="noopener ugc nofollow" target="_blank">希卡拉</a></li><li id="d557" class="mm mn iq ky b kz pb lc pc lf pd lj pe ln pf lr pa ms mt mu bi translated"><a class="ae kv" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇火花</a></li><li id="2ec1" class="mm mn iq ky b kz pb lc pc lf pd lj pe ln pf lr pa ms mt mu bi translated"><a class="ae kv" href="https://github.com/minrk/findspark" rel="noopener ugc nofollow" target="_blank"> FindSpark </a></li></ul></div></div>    
</body>
</html>