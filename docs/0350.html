<html>
<head>
<title>Neural Networks = Black Box?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络=黑盒？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-black-box-b20723f9a417?source=collection_archive---------4-----------------------#2017-04-20">https://towardsdatascience.com/neural-networks-black-box-b20723f9a417?source=collection_archive---------4-----------------------#2017-04-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="de17" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如你所知，我是一个机器学习爱好者。具体地说，是神经网络。</p><p id="0c20" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在过去的几个月里，我读了很多关于 NNs 的文章。我得出了一个结论:</p><blockquote class="kl"><p id="436c" class="km kn iq bd ko kp kq kr ks kt ku kk dk translated">没有一个真正让你看到 NNs 是怎么学习的。</p></blockquote><h2 id="981a" class="kv kw iq bd kx ky kz dn la lb lc dp ld jy le lf lg kc lh li lj kg lk ll lm ln bi translated">示例#1</h2><p id="5db8" class="pw-post-body-paragraph jn jo iq jp b jq lo js jt ju lp jw jx jy lq ka kb kc lr ke kf kg ls ki kj kk ij bi translated"><a class="ae lt" href="http://news.mit.edu/2017/explained-neural-networks-deep-learning-0414" rel="noopener ugc nofollow" target="_blank">福布斯</a>，<em class="lu">解释:神经网络</em></p><p id="6347" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，作者说:</p><blockquote class="lv lw lx"><p id="ecfb" class="jn jo lu jp b jq jr js jt ju jv jw jx ly jz ka kb lz kd ke kf ma kh ki kj kk ij bi translated">“训练数据被输入到底层，即输入层，然后通过后续层，以复杂的方式相乘并相加，直到最终到达输出层，进行彻底的转换。</p></blockquote><p id="b999" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">哇，<em class="lu">情结</em>确实帮助我理解了 NNs 是如何学习的…然后:</p><blockquote class="lv lw lx"><p id="750f" class="jn jo lu jp b jq jr js jt ju jv jw jx ly jz ka kb lz kd ke kf ma kh ki kj kk ij bi translated">在训练期间，不断调整权重和阈值，直到具有相同标签的训练数据始终产生相似的输出。"</p></blockquote><p id="131f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我有这个想法…模糊地。</p><h2 id="058b" class="kv kw iq bd kx ky mb dn la lb mc dp ld jy md lf lg kc me li lj kg mf ll lm ln bi translated">实施例 2</h2><p id="878e" class="pw-post-body-paragraph jn jo iq jp b jq lo js jt ju lp jw jx jy lq ka kb kc lr ke kf kg ls ki kj kk ij bi translated"><a class="ae lt" href="https://hbr.org/2017/01/deep-learning-will-radically-change-the-ways-we-interact-with-technology" rel="noopener ugc nofollow" target="_blank">哈佛商业评论</a>，深度学习将从根本上…</p><blockquote class="lv lw lx"><p id="d330" class="jn jo lu jp b jq jr js jt ju jv jw jx ly jz ka kb lz kd ke kf ma kh ki kj kk ij bi translated">“然而，要使神经网络有用，它需要训练。为了训练神经网络，一组虚拟神经元被绘制出来，并被分配一个随机的数字“权重”，该权重决定神经元如何对新数据(数字化的对象或声音)做出反应。</p></blockquote><p id="f975" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嗯，我想这是有道理的。</p><blockquote class="lv lw lx"><p id="ddf7" class="jn jo lu jp b jq jr js jt ju jv jw jx ly jz ka kb lz kd ke kf ma kh ki kj kk ij bi translated">“最终，经过充分的训练，神经网络将始终如一地识别语音或图像中的正确模式。”</p></blockquote><p id="8eb7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">哦，再一次，它关上了正确解释神经网络如何学习的大门…</p><h2 id="b9d5" class="kv kw iq bd kx ky mb dn la lb mc dp ld jy md lf lg kc me li lj kg mf ll lm ln bi translated">实施例 3</h2><p id="4888" class="pw-post-body-paragraph jn jo iq jp b jq lo js jt ju lp jw jx jy lq ka kb kc lr ke kf kg ls ki kj kk ij bi translated"><a class="ae lt" href="https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html?_r=0" rel="noopener ugc nofollow" target="_blank"> NYT </a>，伟大的人工智能觉醒</p><blockquote class="lv lw lx"><p id="c4e9" class="jn jo lu jp b jq jr js jt ju jv jw jx ly jz ka kb lz kd ke kf ma kh ki kj kk ij bi translated">“训练是一个过程，通过这个过程，在 blob 中挖掘出一系列错综复杂的隧道，这些隧道将任何给定的输入连接到其正确的输出。”</p></blockquote><p id="792b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我想我不喜欢隐喻数学方法。在这种情况下，一个 NN。</p><blockquote class="lv lw lx"><p id="609a" class="jn jo lu jp b jq jr js jt ju jv jw jx ly jz ka kb lz kd ke kf ma kh ki kj kk ij bi translated">“他们<em class="iq">【权重】</em>必须独立证明他们是否也擅长挑选狗和除颤器，(…)</p></blockquote><p id="cdee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对(？).</p><blockquote class="lv lw lx"><p id="eaff" class="jn jo lu jp b jq jr js jt ju jv jw jx ly jz ka kb lz kd ke kf ma kh ki kj kk ij bi translated">(…)但让神经网络如此灵活的一点是，每个单独的单元都可以对不同的预期结果做出不同的贡献。"</p></blockquote><p id="aab3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尽管如此，我还是没有亲眼看到真正的学习。总之。</p><h1 id="5f27" class="mg kw iq bd kx mh mi mj la mk ml mm ld mn mo mp lg mq mr ms lj mt mu mv lm mw bi translated">我的观点是:</h1><blockquote class="kl"><p id="b29c" class="km kn iq bd ko kp kq kr ks kt ku kk dk translated">了解机器如何学习。</p></blockquote><p id="2b3a" class="pw-post-body-paragraph jn jo iq jp b jq mx js jt ju my jw jx jy mz ka kb kc na ke kf kg nb ki kj kk ij bi translated">真的！</p><p id="4d59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经网络训练(涉及梯度下降、反向传播等方法)很有意思。这也有点不可思议，因为数字开始自我调整，试图找到最大预测的最佳路径。</p><p id="8bd0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我知道这些文章并不是要向你展示这一点，因为它们的目标受众(这显然不是技术或教育)。</p><p id="42a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但我想，即便如此，我还是想看看里面的一些东西。</p></div></div>    
</body>
</html>