<html>
<head>
<title>Word2Vec (skip-gram model): PART 2 — Implementation in TF</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2Vec(跳格模型):第 2 部分—在 TF 中的实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word2vec-skip-gram-model-part-2-implementation-in-tf-7efdf6f58a27?source=collection_archive---------1-----------------------#2017-06-15">https://towardsdatascience.com/word2vec-skip-gram-model-part-2-implementation-in-tf-7efdf6f58a27?source=collection_archive---------1-----------------------#2017-06-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="6218" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Jupyter 笔记本:<a class="ae kl" href="https://github.com/mchablani/deep-learning/blob/master/embeddings/Skip-Gram_word2vec.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/mchablani/deep-learning/blob/master/embeddings/Skip-Gram _ word 2 vec . ipynb</a></p><p id="42b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">张量流内置了对 skip-gram word 2 ect 所需的大部分支架的支持，包括嵌入查找和负采样。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="9a25" class="kv kw iq kr b gy kx ky l kz la">tf.nn.embedding_lookup<br/>tf.nn.sampled_softmax_loss</span></pre><h1 id="bd93" class="lb kw iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">预处理</h1><p id="d0c8" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">将输入标记化，并将输入转换为 int 表示。从单词到 int 查找，反之亦然。</p><h1 id="3c64" class="lb kw iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">二次抽样</h1><p id="9445" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">经常出现的单词，如“the”、“of”和“for ”,不会为附近的单词提供太多的上下文信息。如果我们丢弃其中一些，我们可以从数据中去除一些噪声，反过来得到更快的训练和更好的表示。这个过程被米科洛夫称为子采样。对于训练集中的每个单词 wi，我们将以如下概率将其丢弃</p><figure class="km kn ko kp gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi md"><img src="../Images/306744f394b2c8722e19361b988f0dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5dcMgnu2UPo8j-2Eprm4wA.jpeg"/></div></div></figure><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="7e29" class="kv kw iq kr b gy kx ky l kz la">from collections import Counter<br/>import random</span><span id="7210" class="kv kw iq kr b gy ml ky l kz la">threshold = 1e-5<br/>word_counts = Counter(int_words)<br/>total_count = len(int_words)<br/>freqs = {word: count/total_count for word, count in word_counts.items()}<br/>p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}<br/>train_words = [word for word in int_words if random.random() &lt; (1 - p_drop[word])]</span></pre><h2 id="e1c7" class="kv kw iq bd lc mm mn dn lg mo mp dp lk jy mq mr lo kc ms mt ls kg mu mv lw mw bi translated">构建图表</h2><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="0475" class="kv kw iq kr b gy kx ky l kz la">inputs = tf.placeholder(tf.int32, [None], name=’inputs’)<br/>labels = tf.placeholder(tf.int32, [None, None], name=’labels’)</span></pre><p id="5992" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意标签是二维的，如用于负采样的 tf.nn.sampled_softmax_loss 所要求的。</p><p id="97e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嵌入矩阵的大小为单词数乘以隐藏层中的单元数。因此，如果您有 10，000 个单词和 300 个隐藏单元，矩阵的大小将为 10，000×300。请记住，我们对输入使用了标记化的数据，通常是整数，其中标记的数量就是我们词汇表中的单词数量。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="da0f" class="kv kw iq kr b gy kx ky l kz la">n_vocab = len(int_to_vocab)<br/>n_embedding =  300<br/>embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))<br/>embed = tf.nn.embedding_lookup(embedding, inputs)</span></pre><h1 id="eced" class="lb kw iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">负采样</h1><p id="c551" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">我们将更新正确标签的权重，但只有少量不正确的标签。这就是所谓的<a class="ae kl" href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank">【负抽样】</a>。Tensorflow 有一个方便的函数可以做到这一点，<code class="fe mx my mz kr b"><a class="ae kl" href="https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss" rel="noopener ugc nofollow" target="_blank">tf.nn.sampled_softmax_loss</a></code>。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="5c87" class="kv kw iq kr b gy kx ky l kz la"># Number of negative labels to sample<br/>n_sampled = 100<br/>softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding))) softmax_b = tf.Variable(tf.zeros(n_vocab), name="softmax_bias")<br/>    <br/># Calculate the loss using negative sampling<br/>loss = tf.nn.sampled_softmax_loss(<br/>    weights=softmax_w,<br/>    biases=softmax_b,<br/>    labels=labels,<br/>    inputs=embed,<br/>    num_sampled=n_sampled,<br/>    num_classes=n_vocab)<br/>    <br/>cost = tf.reduce_mean(loss)<br/>optimizer = tf.train.AdamOptimizer().minimize(cost)</span></pre><h1 id="9a96" class="lb kw iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">培养</h1><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="0a66" class="kv kw iq kr b gy kx ky l kz la">batches = get_batches(train_words, batch_size, window_size)<br/>for x, y in batches:<br/>    feed = {inputs: x, labels: np.array(y)[:, None]}<br/>    train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)</span></pre><h1 id="649f" class="lb kw iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">使用 T-SNE 可视化单词向量</h1><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="54ab" class="kv kw iq kr b gy kx ky l kz la">%matplotlib inline<br/>%config InlineBackend.figure_format = 'retina'</span><span id="983d" class="kv kw iq kr b gy ml ky l kz la">import matplotlib.pyplot as plt<br/>from sklearn.manifold import TSNE</span><span id="63f8" class="kv kw iq kr b gy ml ky l kz la">embed_mat = sess.run(embedding)</span><span id="60cd" class="kv kw iq kr b gy ml ky l kz la">viz_words = 500<br/>tsne = TSNE()<br/>embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])<br/>fig, ax = plt.subplots(figsize=(14, 14))<br/>for idx in range(viz_words):<br/>    plt.scatter(*embed_tsne[idx, :], color='steelblue')<br/>    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)</span></pre><p id="37e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">学分:来自课堂讲稿:<a class="ae kl" href="https://classroom.udacity.com/nanodegrees/nd101/syllabus" rel="noopener ugc nofollow" target="_blank">https://classroom.udacity.com/nanodegrees/nd101/syllabus</a></p></div></div>    
</body>
</html>