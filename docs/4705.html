<html>
<head>
<title>How Do You Find the Partial Derivative of a Function?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何求函数的偏导数？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-ac15e178bbd?source=collection_archive---------4-----------------------#2018-09-01">https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-ac15e178bbd?source=collection_archive---------4-----------------------#2018-09-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="591e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一步一步:神经网络背后的数学</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c12bd24dbdbebb8bbe3aa3ef060afbd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5NZdHietqSZjF59KGzMCwQ.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Title image: <a class="ae kv" href="https://pixabay.com/en/geometry-mathematics-volume-surface-1044090/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="806d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9">第一部分</a>中，我们被给了一个问题:计算这个损失函数的梯度:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/187621d8d6992b6835ff8872b8776854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WGnJteHoZFMRLjF82Nibqg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 1: Loss function</figcaption></figure><p id="a413" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">求梯度本质上就是求函数的导数。然而，在我们的例子中，因为有许多我们可以调整的独立变量(所有的权重和偏差)，我们必须找到每个变量的导数。这就是所谓的偏导数，符号是∂.</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="d27e" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">偏导数:</h1><p id="4fa0" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">计算简单函数的偏导数很容易:只需将方程中的其他变量视为常数，然后找到通常的标量导数。以下是一些标量导数规则作为提醒:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/55c0b93c45467fc3fea1b61ccf606ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wnq_XjJ82u_BrZt05HM2Og.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 2: Scalar derivative rules // <a class="ae kv" href="http://explained.ai/matrix-calculus/index.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="140b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑函数<em class="my"> f(x，y) = 3x y </em>中关于<em class="my"> x </em>的偏导数(即<em class="my"> y </em>如何随着<em class="my"> x </em>的变化而变化)。将<em class="my"> y </em>视为常数，我们可以求出 x 的偏导数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/974f547477bdb31cb63229bf41b63b91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nB0YWdb4NkPJXi176ghOIQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 3: Partial with respect to x</figcaption></figure><p id="13e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，我们可以找到 y 的偏导数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/18c213d25848c7b2c7c6b7d5d6aa192c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8fucVacasM_yMQqSY6sQ4A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 4: Partial with respect to y</figcaption></figure><p id="a0b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">函数<em class="my"> f(x，y) = 3x y </em>的梯度是一个水平向量，由两个分向量组成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/4d5ce1b16f8708b13840c09552ba421d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*viaJL_btfd26n_dXt2uI9Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 5: Gradient of <em class="nc">f(x,y) // </em><a class="ae kv" href="http://explained.ai/matrix-calculus/index.html" rel="noopener ugc nofollow" target="_blank"><em class="nc">Source</em></a></figcaption></figure><p id="f6ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这应该很清楚:因为相对于<em class="my"> x </em>的偏导数是函数在 x 方向上的梯度，相对于<em class="my"> y </em>的偏导数是函数在 y 方向上的梯度，所以总梯度是由两个偏导数组成的向量。这个可汗学院的视频提供了一个非常简洁的偏导数的图形解释，如果你想形象化我们正在做的事情。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="3a2e" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">链规则:</h1><p id="aa2d" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">对于像<em class="my"> f(x，y) = 3x y </em>这样的简单函数，我们只需要知道这些。然而，如果我们想要计算更复杂函数的偏导数——例如那些具有嵌套表达式的函数，如<em class="my"> max(0，</em><strong class="ky ir"><em class="my">w</em></strong><em class="my">∙</em><strong class="ky ir"><em class="my">X</em></strong><em class="my">+b)</em>——我们需要能够利用多元链规则，在本文中称为<em class="my">单变量全导数链规则</em>。</p><h2 id="23b9" class="nf mb iq bd mc ng nh dn mg ni nj dp mk lf nk nl mm lj nm nn mo ln no np mq nq bi translated">单变量链规则</h2><p id="581d" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">我们先来回顾一下单变量链规则。考虑函数<em class="my"> y=f(g(x))=sin(x)。为了得到这个表达式的导数，我们将外部表达式的导数与内部表达式的导数相乘，或者“将各个部分连接在一起”。换句话说:</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/cf6e16a919bf1bd93edf072c6c1b6cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*ZlB0S8csDjy8efvQzNdzDQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 6: Single-variable chain rule where u is the intermediate variable for nested subexpressions</figcaption></figure><p id="57d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于我们的例子，<em class="my"> u=x </em>和<em class="my"> y=sin(u) </em>。因此:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/8823275606405605833e2612e94ef3f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*HESfCbVLO3NU567CdCXVOw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 7: Derivatives // <a class="ae kv" href="http://explained.ai/matrix-calculus/index.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="4701" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">和</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/5a28e978fa0690b454a819be92e97a20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TihovLfKm8oNXHCgnVP3UQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 8: Derivative of the whole expression // <a class="ae kv" href="http://explained.ai/matrix-calculus/index.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="f534" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">把单变量链规则想象成<em class="my"> x </em>经过的操作图是很好的，就像这样:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/92c4a0e8bc8f827a8d58249396d2fbb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*lbVNzoDU8TV9Em7VhBKEDw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 9: Diagram of chain of operations for y=sin(x<em class="nc">²)</em></figcaption></figure><p id="f175" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当处理多变量链式法则时，将方程可视化为图表的概念将会非常方便。此外，如果您使用 Tensorflow(或 Keras)和 TensorBoard，在您构建模型和编写训练代码时，您可以看到与此类似的操作图。</p><h2 id="9b06" class="nf mb iq bd mc ng nh dn mg ni nj dp mk lf nk nl mm lj nm nn mo ln no np mq nq bi translated">多变量链式法则</h2><p id="e694" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">多变量链规则，也称为<em class="my">单变量全导数链规则</em>，如文中所称，是标量链规则的变体。与它的名字所暗示的不同，它可以应用于只有一个变量的表达式。但是，表达式应该有多个中间变量。</p><p id="d089" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了说明这一点，让我们考虑等式<em class="my"> y=f(x)=x+x </em>。使用标量附加导数规则，我们可以立即计算导数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/409e4720fb56da484607e44a81c359af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FVS31lOdc_-wxD0Z9nm4QQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 10: Derivative of x+x<em class="nc">²</em></figcaption></figure><p id="9927" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们试着用链式法则来做。首先，我们引入中间变量:<em class="my"> u₁(x) = x </em>和<em class="my"> u₂(x，u₁) = x + u₁.</em>如果我们应用单变量链规则，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/98aec4133b7cb574452ef8d69c69757e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z1ntlKPXz2ZwnknefhLP7A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 11: Using the single-variable chain rule</figcaption></figure><p id="8667" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很明显，2x≠1+2x，所以这里有问题。让我们画出方程式的图表:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/4dfe3a30324db8a7eb1b092b3db6e984.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*ejmIce56KHfhbCnLoD89nA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 12: Diagram of chain of operations for y = x+x<em class="nc">² // </em>// <a class="ae kv" href="http://explained.ai/matrix-calculus/index.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="b567" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图 12 中的图不再是线性的，所以我们必须考虑图中所有导致最终结果的路径。由于<em class="my"> u₂ </em>有两个参数，偏导数开始起作用。为了计算这个函数的导数，我们必须计算关于 u₁). u₂(x<em class="my">的<em class="my"> x </em>的偏导数</em>这里，<em class="my"> x </em>的变化以两种方式反映在<em class="my"> u₂ </em>中:作为加法的操作数和作为平方运算符的操作数。在符号中，<em class="my">ŷ=(x+δx)+(x+δx)</em>和<em class="my">δy = ŷ-y</em>，其中<em class="my"> ŷ </em>是调整后的<em class="my"> x. </em>处的 y 值</p><p id="1dd4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，为了计算<em class="my"> u₂(x，u₁) </em>的偏差值，我们需要合计从<em class="my"> x </em>的变化到<em class="my"> y </em>的变化的所有可能的贡献。<em class="my"> u₂(x，u₁) </em>的总导数由下式给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/a356689370dabf20cf04f22525925a7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ieczzI3NSKaHyyNIeiQRnw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 13: Derivative of y = x+x<em class="nc">² </em>// <a class="ae kv" href="http://explained.ai/matrix-calculus/index.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="9632" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更简单地说，你<strong class="ky ir">把 x 的变化直接对 u₂的影响和 x 的变化通过 u₁对 u₂.的影响加起来</strong>我发现通过图表更容易形象化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/df4987bc52dbaea11a705fb7b89eda03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*HnQk0WP7v0K29JTwZFj0EA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 14: Graph of y = x+x<em class="nc">², with partials included</em></figcaption></figure><p id="8bb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！我们得到了正确答案:1+2x。我们现在可以用一个规则来概括这个过程，即多变量链式规则(或单变量全导数链式规则):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/7359bb41a5686ccd49f6448a632fd395.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ldU2GTsOj6YpX6oj6T71cg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 15: Multivariable chain rule // <a class="ae kv" href="http://explained.ai/matrix-calculus/index.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="6eb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们为 x 引入一个别名，如 x=u(n+1)，那么我们可以将公式改写成最终形式，这样看起来更简洁:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/b4f7ff79dc167bb2254b90524f1ef6f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*-Ro6QL5A5hYpyN5TCKvvhw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 16: Multivariable chain rule // <a class="ae kv" href="http://explained.ai/matrix-calculus/index.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="e36f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">仅此而已！复习一下，我们再做一个例子:<em class="my"> f(x)=sin(x+x ) </em>。我们的 3 个中间变量是:<em class="my"> u₁(x) = x，u₂(x，u₁)=x+u₁，</em>和<em class="my"> u₃(u₂) = sin(u₂) </em>。我们可以再次绘制图表:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/4f26345c13181c3575b0a05519cd072a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x78Xl6xxYKtPRDrRu3F6tQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 17: Graph of y = sin(x+x<em class="nc">²), with partials included</em></figcaption></figure><p id="e38b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">计算我们的偏角:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/2462809ee0d562ef88807fbb3e214052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VoEayqaKB9q2DE0djGzgzA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 18: Partials for the function y = sin(x+x<em class="nc">²)</em></figcaption></figure><p id="ee02" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此<em class="my"> f(x)=sin(x+x ) </em>的导数为<em class="my"> cos(x+x )(1+2x) </em>。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="8914" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这和我们的问题有什么关系？记住，我们需要找到损失函数相对于<strong class="ky ir"> w </strong>(我们所有权重的向量)和<em class="my"> b </em>(偏差)的偏导数。然而，我们的损失函数并不那么简单——有多个嵌套的子表达式(即多个中间变量)需要我们使用链规则。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/187621d8d6992b6835ff8872b8776854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WGnJteHoZFMRLjF82Nibqg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 19: Loss function</figcaption></figure><p id="c4a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">还有一个问题。如你所见，我们的损失函数不仅仅接受标量作为输入，它也接受向量。如何计算向量方程的偏导数，向量链规则是什么样的？</p><p id="c710" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看看<a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-d002440227fb">第三部</a>就知道了！</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="31a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你还没有，点击<a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9">这里</a>阅读第 1 部分！</p><p id="4da9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">跳到其他文章:</p><ul class=""><li id="de9f" class="oe of iq ky b kz la lc ld lf og lj oh ln oi lr oj ok ol om bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-d002440227fb">第三部分:向量微积分</a></li><li id="5de3" class="oe of iq ky b kz on lc oo lf op lj oq ln or lr oj ok ol om bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/calculating-gradient-descent-manually-6d9bee09aa0b">第 4 部分:将所有内容整合在一起</a></li></ul><p id="2076" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此处下载原文<a class="ae kv" href="https://arxiv.org/abs/1802.01528" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="606c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你喜欢这篇文章，别忘了留下一些掌声！如果您有任何问题或建议，请在下面留下您的评论:)</p></div></div>    
</body>
</html>