<html>
<head>
<title>What the Hell is Perceptron?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">感知器是什么鬼？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53?source=collection_archive---------0-----------------------#2017-09-09">https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53?source=collection_archive---------0-----------------------#2017-09-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a940" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">神经网络的基础</h2></div><blockquote class="kf kg kh"><p id="247e" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kl ir">感知器是单层神经网络</strong>，多层感知器称为神经网络。</p></blockquote><p id="cd91" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">感知器是一个线性分类器(二进制)。此外，它还用于监督学习。它有助于对给定的输入数据进行分类。但是它究竟是如何工作的呢？</p><p id="ae6d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">众所周知，正常的神经网络是这样的</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi li"><img src="../Images/62089121ca0997c329a9fb5a3dcb258e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*eEKb2RxREV6-MtLz2DNWFQ.gif"/></div></div></figure><p id="927d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">得到这本书👇</p><p id="9b0c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><a class="ae lu" href="https://amzn.to/2AXjHRZ" rel="noopener ugc nofollow" target="_blank"><strong class="kl ir">Python机器学习简介:数据科学家指南</strong> </a></p><p id="c630" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这对我帮助很大。🙌 👍</p><p id="4326" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">正如你所看到的，它有多层。</p><p id="7191" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">感知器由4部分组成。</p><ol class=""><li id="0433" class="lv lw iq kl b km kn kp kq lf lx lg ly lh lz le ma mb mc md bi translated">输入值或一个输入图层</li><li id="5c54" class="lv lw iq kl b km me kp mf lf mg lg mh lh mi le ma mb mc md bi translated">权重和偏差</li><li id="1ac6" class="lv lw iq kl b km me kp mf lf mg lg mh lh mi le ma mb mc md bi translated">净和</li><li id="e9bb" class="lv lw iq kl b km me kp mf lf mg lg mh lh mi le ma mb mc md bi translated"><a class="ae lu" href="https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6" rel="noopener">激活功能</a></li></ol><blockquote class="kf kg kh"><p id="75a1" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">仅供参考:神经网络的工作方式与感知器相同。所以，如果你想知道神经网络是如何工作的，那就学习一下感知器是如何工作的。</p></blockquote><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi mj"><img src="../Images/3e3efb352adbf3ff7a31f72ef7325a01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n6sJ4yZQzwKL9wnF5wnVNg.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Fig : Perceptron</strong></figcaption></figure><h2 id="f165" class="mp mq iq bd mr ms mt dn mu mv mw dp mx lf my mz na lg nb nc nd lh ne nf ng nh bi translated">但是它是如何工作的呢？</h2><p id="78be" class="pw-post-body-paragraph ki kj iq kl b km ni jr ko kp nj ju kr lf nk ku kv lg nl ky kz lh nm lc ld le ij bi translated">感知器工作在这些简单的步骤上</p><p id="c58d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">a.所有输入<strong class="kl ir"> <em class="kk"> x </em> </strong>乘以其权重<strong class="kl ir"> <em class="kk"> w </em> </strong>。姑且称之为<strong class="kl ir"> <em class="kk"> k. </em> </strong></p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi nn"><img src="../Images/02ac83efba0c1f00ea651a0b29b49de9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Zy1C83cnmYUdETCeQrOgA.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Fig: Multiplying inputs with weights for 5 inputs</strong></figcaption></figure><p id="66d8" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">b.<strong class="kl ir"> <em class="kk">将</em> </strong>所有相乘后的值相加，称之为<strong class="kl ir"> <em class="kk">加权和。</em> </strong></p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi no"><img src="../Images/1c9711cf0217a23864f0a381da18eb71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/1*xFd9VQnUM1H0kiCENsoYxg.gif"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo"><em class="np">Fig: Adding with Summation</em></strong></figcaption></figure><p id="1836" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">c.<strong class="kl ir"> <em class="kk">将</em> </strong>加权和应用于正确的<a class="ae lu" href="https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6" rel="noopener"> <strong class="kl ir"> <em class="kk">激活函数</em> </strong>。</a></p><p id="1602" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">例如:单位步长激活功能。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/56eb7c60d849084c98c3127d26b40eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*0iOzeMS3s-3LTU9hYH9ryg.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><strong class="bd mo">Fig: Unit Step Activation Function</strong></figcaption></figure></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><h2 id="c4d4" class="mp mq iq bd mr ms mt dn mu mv mw dp mx lf my mz na lg nb nc nd lh ne nf ng nh bi translated">为什么我们需要权重和偏差？</h2><blockquote class="ny"><p id="dff4" class="nz oa iq bd ob oc od oe of og oh le dk translated"><strong class="ak">权重</strong>显示特定节点的强度。</p><p id="8667" class="nz oa iq bd ob oc od oe of og oh le dk translated"><strong class="ak"> <em class="np">一个</em> </strong>偏置值允许你向上或向下移动激活函数曲线。</p></blockquote><figure class="oj ok ol om on ln gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a39d7ab70e1d6afa3aa83b0f6e61a25b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/1*ztXU57QEETPHGXczHrSWSA.gif"/></div></figure><h2 id="9e72" class="mp mq iq bd mr ms mt dn mu mv mw dp mx lf my mz na lg nb nc nd lh ne nf ng nh bi translated">为什么我们需要激活功能？</h2><blockquote class="ny"><p id="2c48" class="nz oa iq bd ob oc od oe of og oh le dk translated">简而言之，<strong class="ak">激活函数用于映射所需值之间的输入，如(0，1)或(-1，1) </strong>。</p></blockquote><p id="b415" class="pw-post-body-paragraph ki kj iq kl b km oo jr ko kp op ju kr lf oq ku kv lg or ky kz lh os lc ld le ij bi translated">为了更好的解释，去我以前的故事<a class="ae lu" href="https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6" rel="noopener">激活功能:神经网络</a>。</p><h2 id="279b" class="mp mq iq bd mr ms mt dn mu mv mw dp mx lf my mz na lg nb nc nd lh ne nf ng nh bi translated">我们用感知器。</h2><blockquote class="ny"><p id="bf35" class="nz oa iq bd ob oc od oe of og oh le dk translated">感知器通常用于将数据分为两部分。因此，它也被称为<a class="ae lu" href="https://medium.com/towards-data-science/linear-regression-the-easier-way-6f941aa471ea" rel="noopener">线性二元分类器</a>。</p></blockquote><figure class="oj ok ol om on ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi ot"><img src="../Images/87785a56906437eeba5905594409c25d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xsR57_PO8U7PB_ItLslLmA.png"/></div></div></figure><blockquote class="ny"><p id="88c8" class="nz oa iq bd ob oc ou ov ow ox oy le dk translated">如果你想离线更好地理解机器学习。</p></blockquote><p id="f5b9" class="pw-post-body-paragraph ki kj iq kl b km oo jr ko kp op ju kr lf oq ku kv lg or ky kz lh os lc ld le ij bi translated">我将每周发布2个帖子，所以不要错过教程。</p><p id="97e0" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">所以，跟着我上<a class="ae lu" href="https://medium.com/@sagarsharma4244" rel="noopener">中</a>、<a class="ae lu" href="https://www.facebook.com/profile.php?id=100003188718299" rel="noopener ugc nofollow" target="_blank">脸书</a>、<a class="ae lu" href="https://twitter.com/SagarSharma4244" rel="noopener ugc nofollow" target="_blank">推特</a>、<a class="ae lu" href="https://www.linkedin.com/in/sagar-sharma-232a06148/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae lu" href="https://plus.google.com/u/0/+SAGARSHARMA4244" rel="noopener ugc nofollow" target="_blank"> Google+ </a>、<a class="ae lu" href="https://www.quora.com/profile/Sagar-Sharma-71" rel="noopener ugc nofollow" target="_blank"> Quora </a>看看类似的帖子。</p><p id="5271" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">如果你有任何意见或问题，请写在评论里。</p><p id="9b4a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kl ir">鼓掌吧！分享一下！跟我来。</strong></p><p id="5390" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">乐意帮忙。荣誉……..</p></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><h1 id="93df" class="oz mq iq bd mr pa pb pc mu pd pe pf mx jw pg jx na jz ph ka nd kc pi kd ng pj bi translated">你会喜欢的以前的故事:</h1><ol class=""><li id="2649" class="lv lw iq kl b km ni kp nj lf pk lg pl lh pm le ma mb mc md bi translated"><a class="ae lu" href="https://medium.com/towards-data-science/cross-validation-code-visualization-kind-of-fun-b9741baea1f8" rel="noopener">交叉验证代码可视化:有点意思</a></li></ol><p id="e873" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">2.<a class="ae lu" href="https://medium.com/towards-data-science/linear-regression-the-easier-way-6f941aa471ea" rel="noopener">线性回归:更简单的方法</a></p><p id="23f8" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">3.<a class="ae lu" href="https://medium.com/towards-data-science/linear-regression-the-easier-way-6f941aa471ea" rel="noopener"> DIY Arduino无线键盘</a></p><p id="6bba" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">4.<a class="ae lu" href="https://theffork.com/activation-functions-in-neural-networks/" rel="noopener ugc nofollow" target="_blank"> <strong class="kl ir">神经网络的激活函数及其类型</strong> </a></p><div class="pn po gp gr pp pq"><a href="https://theffork.com/activation-functions-in-neural-networks/" rel="noopener  ugc nofollow" target="_blank"><div class="pr ab fo"><div class="ps ab pt cl cj pu"><h2 class="bd ir gy z fp pv fr fs pw fu fw ip bi translated">神经网络中的激活函数及其类型</h2><div class="px l"><h3 class="bd b gy z fp pv fr fs pw fu fw dk translated">它是一条曲线(sigmoid，tanH，ReLU ),用于映射有界值之间的网络值。这就完成了…</h3></div><div class="py l"><p class="bd b dl z fp pv fr fs pw fu fw dk translated">theffork.com</p></div></div><div class="pz l"><div class="qa l qb qc qd pz qe ls pq"/></div></div></a></div><p id="99c9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">5.<a class="ae lu" href="https://hackernoon.com/what-the-hell-is-tensor-in-tensorflow-e40dbf0253ee" rel="noopener ugc nofollow" target="_blank">Tensor flow中的“张量”是什么鬼？</a></p></div></div>    
</body>
</html>