<html>
<head>
<title>Artificial Neural Networks and Neuroscience</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工神经网络和神经科学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/artifician-neural-networks-and-neuroscience-e4852b10d7a9?source=collection_archive---------3-----------------------#2017-04-25">https://towardsdatascience.com/artifician-neural-networks-and-neuroscience-e4852b10d7a9?source=collection_archive---------3-----------------------#2017-04-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="cd1d" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">TL；博士</strong></h1><p id="f43c" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">众所周知，人工神经网络受到不同学科的影响，如数学、物理和神经科学(顾名思义)。</p><p id="1515" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">人工神经网络能提供任何线索回到神经科学去理解一些大脑结构吗？</p><h1 id="7714" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">长版</strong></h1><p id="f428" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">人工神经网络是数学、物理(如统计力学)和神经科学交叉领域的结果。</p><p id="8c71" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">人工神经网络最初的神经科学灵感可以追溯到40年代，因为它收到了上述领域的大量贡献:</p><ul class=""><li id="0c45" class="lo lp iq kn b ko lj ks lk kw lq la lr le ls li lt lu lv lw bi translated">从数学的角度来看，让我们考虑一下函数优化和误差反向传播(梯度计算等)的所有影响</li><li id="9a6e" class="lo lp iq kn b ko lx ks ly kw lz la ma le mb li lt lu lv lw bi translated">从物理的角度，我们来思考一下受限玻尔兹曼机和自旋玻璃之间的联系</li><li id="6062" class="lo lp iq kn b ko lx ks ly kw lz la ma le mb li lt lu lv lw bi translated">从神经科学的角度来看，让我们想想最近生物启发的卷积神经网络(CNN)的成功</li></ul><p id="a657" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">所以问题是<strong class="kn ir">安能给这些领域什么回报</strong>？</p><p id="65ea" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">许多臭名昭著的深度神经网络应用于数据密集型领域，如应用数学、物理和显然的工程，目前已经开发并正在开发中，但神经科学呢？</p><p id="9c06" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">人工神经网络的最新发展是否有助于提高我们对大脑的了解，反之亦然，从而引发良性循环？</p><p id="2fbf" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">似乎是这样的:计算神经科学领域的一些最新推测可以被定义为“人工神经网络启发的”,事实也的确如此</p><ol class=""><li id="f366" class="lo lp iq kn b ko lj ks lk kw lq la lr le ls li mc lu lv lw bi translated">动态学习相当于大脑中发生的成本函数优化</li><li id="0757" class="lo lp iq kn b ko lx ks ly kw lz la ma le mb li mc lu lv lw bi translated">对于架构和成本函数来说，任务特异性对于实现足够好的性能水平是必要的</li><li id="0d0f" class="lo lp iq kn b ko lx ks ly kw lz la ma le mb li mc lu lv lw bi translated">计算专用结构存在:不同种类的存储器(例如，内容可寻址、缓冲区等)，不同种类的信息路由系统(例如，注意力、高级门控系统等)</li></ol><p id="e36e" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">大脑神经元和人工神经元的区别</strong></p><p id="c3c5" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">首先，重要的是观察到，即使一些ANN / DL架构具有某种生物灵感，但人工神经元和生物神经元之间仍然存在很大差异。</p><p id="baec" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">最臭名昭著的差异之一是关于网络上的物理时间效应:虽然目前最常用的人工神经网络没有物理时间的概念(因为推理和训练计算发生在离散的计算时间内)，但生物网络深受物理时间的影响，因为神经元通过发送<strong class="kn ir">尖峰信号</strong>工作，网络状态取决于时间。</p><p id="114e" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这就是说，如果ANN / DL体系结构和生物学体系结构在某些方面看起来相似，它们在许多其他方面，甚至是基本方面仍然非常不同。</p><p id="a057" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">大脑中的成本函数优化</strong></p><p id="866e" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">在当前的人工神经网络中，典型的优化算法类是依赖于成本函数局部梯度计算的<strong class="kn ir">梯度下降</strong>类。由于一系列的原因，大脑不太可能利用这样的机制，因此已经形成了其他的假说，如1) <strong class="kn ir">赫布边可塑性</strong>和2) <strong class="kn ir">系列扰动</strong>。</p><p id="226b" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir"> Hebbian可塑性</strong>基本上是一个基于成对神经元尖峰活动相关性的神经元权重学习规则:神经元对连接越强，其尖峰活动相关性越强，减弱越弱。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi md"><img src="../Images/e5f2e33d7579d1c30a81ef24dadebb59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*l1_A3KFY06EqOPnflXdaHQ.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Fig.1. Hebbian Plasticity: the connection between 2 neurons is strenghtened the more their spiking activity is correlated and weakened the less their spiking activity is correlated.</figcaption></figure><p id="8d9e" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">串行扰动</strong>是一种基于噪声的策略，旨在探索神经元参数状态空间:对神经元连接权值随机执行扰动，并根据假设使用的参考成本函数评估其效果。</p><p id="f806" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">任务特定成本函数</strong></p><p id="10d6" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">第二个假设是关于大脑中存在任务特定的架构和成本函数，根据特定的算法进行优化(即学习)，其中一些算法在前一点中已经介绍过。</p><p id="b7b7" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这种假设是基于“一刀切”的成本函数和架构无法提供足够好的性能水平，因此需要某种程度的专业化。</p><p id="9e59" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">根据这种假设，理解如何定义任务特定的成本函数也是有趣的，从而执行一种“元学习”(即，决定哪个成本函数是最佳优化的，以便在特定任务中实现良好的性能)。</p><p id="af5f" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这些话题将在一个专门的文章系列中讨论。</p><p id="3d96" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">计算专用结构</strong></p><p id="27fd" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">类似于前一点，一些“计算启发”的结构也被认为存在于大脑中，如:记忆和信息路由。</p><p id="dc4c" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">众所周知，记忆是大脑中发生的复杂现象，存在不同种类的记忆，如:内隐记忆(即，不可明确恢复的记忆)、内容可寻址记忆和短期记忆。</p><p id="5b54" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">对于这些类型的现象，存在一些DNN模型，如通过脉冲网络实现的内容可寻址存储器的Hopfield网络。</p><p id="bf3f" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">信息路由机制允许高效和有效的信息处理:注意机制导致效率，使得处理系统聚焦于最相关的信息，而门控系统允许在处理系统的不同区域之间提供远程连接的效率(在没有完全连接的拓扑的情况下)。</p><p id="4c10" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这些话题也将在专门的文章系列中讨论。</p></div></div>    
</body>
</html>