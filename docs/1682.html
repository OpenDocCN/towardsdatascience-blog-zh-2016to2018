<html>
<head>
<title>[Learning Note] Dropout in Recurrent Networks — Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[学习笔记]循环网络中的辍学—第 3 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-note-dropout-in-recurrent-networks-part-3-1b161d030cd4?source=collection_archive---------5-----------------------#2017-10-03">https://towardsdatascience.com/learning-note-dropout-in-recurrent-networks-part-3-1b161d030cd4?source=collection_archive---------5-----------------------#2017-10-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9357" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一些实证结果比较</h2></div><p id="42a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在第一部分中，我们介绍了循环网络中变分丢失的理论基础。第二部分研究了 Keras 和 PyTorch 中的实现细节。现在是最后一部分，我们在公共数据集上比较不同的模型。</p><h2 id="f281" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">MC 辍学实施</h2><p id="78f1" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated"><em class="lz"> Keras </em>没有一个参数或函数来启用测试时间的丢失。<a class="ae ma" href="https://github.com/fchollet/keras/issues/1606" rel="noopener ugc nofollow" target="_blank">虽然你可以使用一个总是活跃的 lambda 层，</a>我们不容易比较标准漏失近似和蒙特卡罗漏失的结果。</p><p id="f27f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">撰写核心论文[1]，<a class="ae ma" href="https://medium.com/p/1b161d030cd4/edit" rel="noopener">的 Yarin Gal 提供了一个 2017 年 2 月在 Github </a>上实现 MC dropout 的例子。这也是 Keras 开始为经常性辍学提供内置支持的时候。仅仅过了几个月，它就很快过时了。我挖了一下<a class="ae ma" href="https://github.com/ceshine/recurrent-dropout-experiments/blob/master/yaringal_callbacks.py" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> <em class="lz">似乎</em> </strong>已经让它与<em class="lz"> Kera </em> s 的最新版本<em class="lz"> Tensorflow 1.3.0 </em>一起工作了。</a></p><p id="30a5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于 PyTorch 来说，事情就简单多了。在前馈前使用<code class="fe mb mc md me b">model.eval()</code>使能标准压差近似值，使用<code class="fe mb mc md me b">model.train()</code>使能蒙特卡洛压差。</p><p id="dbfb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，一些超参数是根据框架经验调整的。例如，在<em class="lz"> Keras </em>中，学习率被设置为 1e-3，但在<em class="lz"> PyTorch </em>中，它被设置为 1e-4。目的是避免模型永远收敛。因此，建议将重点放在同一框架中实现的模型之间的比较上。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="ae76" class="mm lc iq bd ld mn mo mp lg mq mr ms lj jw mt jx lm jz mu ka lp kc mv kd ls mw bi translated"><a class="ae ma" href="http://www.cs.cornell.edu/people/pabo/movie-review-data/" rel="noopener ugc nofollow" target="_blank">康奈尔电影评论数据集</a></h1><p id="a9f6" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">该数据集在论文[1]中用于评估不同的模型。有 10，620 个训练样本和 2，655 个测试/验证样本。这是一个回归问题。给定一个单词序列，模型需要预测用户给这部电影的分数。将<a class="ae ma" href="https://github.com/ceshine/recurrent-dropout-experiments/blob/master/yaringal_dataset.py#L51" rel="noopener ugc nofollow" target="_blank">分数归一化，使其以零为中心，单位方差为</a>。在下面的讨论中，<strong class="kh ir">【原始 MSE】</strong>将对应于归一化分数的均方误差，<strong class="kh ir"/>将对应于原始分数的均方根误差(文中使用 RMSE)。</p><p id="2a9b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">批量大小设置为 128，序列长度为 200(填充)，与论文中相同。所有模型中隐藏单元的数量保持不变。唯一的区别是如何应用辍学。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/69584befe7035053763a7b3c6bc9d33b.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*uWDdCEaQdWcZBM7fUlw7Gw.png"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">Fig 3(b) in [1]: Naive Dropout LSTM over-fits eventually</figcaption></figure><p id="1494" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">论文中使用的退出概率似乎大多是<em class="lz"> 0.5 </em>。正如你在上图中看到的，这可能与缓慢的收敛速度有关。变分 LSTM 需要数百个历元才能胜过其他两个模型。我没有太多的时间来训练这么多的纪元，所以在允许一些过度拟合的同时，降低了退出概率以提高速度。关于确切的概率，请参考本节末尾的电子表格。</p><h2 id="4f54" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">克拉斯</h2><p id="b845" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">评估了三种模型:</p><ol class=""><li id="4545" class="nj nk iq kh b ki kj kl km ko nl ks nm kw nn la no np nq nr bi translated"><em class="lz">变量:</em>使用输入压差、循环压差和输出压差。</li><li id="2ebd" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi translated"><em class="lz">无脱落:</em>只有极低的重量衰减。</li><li id="ddae" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi translated"><em class="lz">朴素漏失:</em>使用时间步长独立输入漏失，输出漏失。</li></ol><p id="5086" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有三种模型都没有嵌入丢失。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi nx"><img src="../Images/1e173b684d62adecf80c55b5bd096f89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qzs1Gi-uswT0Aoznf_25Pg.png"/></div></div></figure><p id="ab90" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lz">(向右修正图表的坐标轴标签:X 轴:"</em> <strong class="kh ir"> <em class="lz">原始 MSE 差异</em> </strong> <em class="lz">"，Y 轴:"</em> <strong class="kh ir"> <em class="lz">计数</em></strong><em class="lz">"】</em></p><p id="3aa6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图表“(MC —近似值)直方图”是每个样本的 MC 差值的原始 MSE 减去标准差值近似值的直方图。注:在变分和简单的 LSTM 模型中，MC 下降通常产生较低的原始 MSE。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/88214cb395fe44579e014d7dc2fcf54a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*NEWrUHTMvEJE7DGlV3tpAA.png"/></div></figure><p id="5271" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">天真的辍学者似乎是最好的执行者，并且不会随着时间的推移而过度适应。</p><h2 id="83b2" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">PyTorch</h2><p id="e8bb" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">测试了五个模型:</p><ol class=""><li id="eabd" class="nj nk iq kh b ki kj kl km ko nl ks nm kw nn la no np nq nr bi translated"><em class="lz">重量下降</em> [ <em class="lz"> 2 </em> ]:使用输入下降、重量下降、输出下降、嵌入下降。</li><li id="fb2a" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi translated"><em class="lz">无脱落</em>:香草单层 LSTM，无重量衰减。</li><li id="ec02" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi translated"><em class="lz">朴素漏失</em>:使用时间步长独立输入漏失，输出漏失。</li><li id="ba01" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi translated"><em class="lz">变重量下降</em>:与<strong class="kh ir"> <em class="lz">变重量下降</em> </strong>相同，但<em class="lz">变</em>参数设置为真。</li><li id="b69e" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi translated"><em class="lz">无经常性脱落的变化</em> ( <em class="lz">变量-2 </em>，<em class="lz"> v w/o r-drop </em>):与重量脱落相同，但重量脱落概率设置为零。(无经常性辍学)</li></ol><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi od"><img src="../Images/78d1bf33a0fb4f8938bc2d68ddea6349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XmbE4Ezb4hWcpS3dpl9Ppg.png"/></div></div></figure><p id="3214" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于重量下降的 LSTM，MC 压差低于标准压差近似值。这也适用于没有经常性辍学的变分 LSTM。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/783128622a15730df7f0dffdbdb5b24f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*gSP34MLH0GLYQ-ry14hc9Q.png"/></div></figure><p id="acd6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，“无辍学”模型严重过度拟合。与 Keras 的结果相比，我们也许可以把 RMSE 的陡峭上升归因于没有重量衰减。然而，PyTorch 的“天真辍学”模式在 RMSE 也有缓慢上升的趋势。“重量下降变化”模型似乎有一些不合适的问题。</p><h2 id="5ff2" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">摘要</h2><p id="9e7d" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">以下是实验及其结果的详细记录:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi of"><img src="../Images/85fbd757432df5843b1bc03bcd16e7f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5c9mLh9vv28unCrstGkdew.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><a class="ae ma" href="https://docs.google.com/spreadsheets/d/e/2PACX-1vQ_3NX8kEe3hoYGXac4QaUeo5HDAT9__FfhTiqc_sGwG0HoWbeLlGYZXEfDfOnpcTB9n2dBkVmixCNQ/pubhtml?gid=0&amp;single=true" rel="noopener ugc nofollow" target="_blank">Spreadsheet Link</a></figcaption></figure><p id="69d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lz">(似乎没有办法把谷歌电子表格嵌入到一个中等的帖子里。请使用表格下方的链接获取表格的网页版本。)</em></p><p id="131d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(尽管 MC dropout 在减重模型中的性能较差，但为了简单起见，我坚持使用 MC dropout:<strong class="kh ir">所有预测(除了“无脱落”模型)都是使用 10 轮 MC dropout 创建的。</strong> <em class="lz"> </em></p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="924c" class="mm lc iq bd ld mn mo mp lg mq mr ms lj jw mt jx lm jz mu ka lp kc mv kd ls mw bi translated"><a class="ae ma" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank"> IMDB 电影评论数据集</a></h1><p id="3bfb" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">这个数据集是一个二元分类问题。有 25000 个训练样本和 25000 个测试样本。目标是从给定的单词序列中预测用户是否喜欢这部电影。</p><p id="727d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">批量大小为 128，序列长度为 80(填充)。</p><p id="0706" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">模型结构和之前差不多，这里就不赘述了。</p><h2 id="5905" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">克拉斯</h2><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi og"><img src="../Images/3ef8e66d9f4218b9cf53851aa406d95d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jKkRpN9GwXC1XiWAScP2Qg.png"/></div></div></figure><p id="ae7e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">和以前一样，MC 丢失通常产生较低的对数损失。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi oh"><img src="../Images/3e0ae22b32f254da7a232fdca316c1f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z_5_Ps_i1mkrorpZoAz4ww.png"/></div></div></figure><p id="df04" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lz">(epoch 17 后有一些数值问题，生成 NaNs 为 log loss。图表中省略了这些数据点。)</em></p><p id="0ff1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个数据集中，所有模型都随着时间的推移而过度拟合。</p><h2 id="e046" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">PyTorch</h2><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi og"><img src="../Images/b6259ffe9923fdf81f826535d1a77b44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uwJyuO51Kl43JYtuJCNQZw.png"/></div></div></figure><p id="9f67" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">和以前一样，权重下降模型在 MC 丢失时表现更差，就像变分模型在经常性丢失时一样(不在直方图中)。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi oi"><img src="../Images/c8e0158f270523b6098ef1b2f4fc2b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zv03ixb0JRACdLR_Bh4zQg.png"/></div></div></figure><p id="2747" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下,“天真辍学”模型明显过度拟合。其他三个辍学模型都得到了类似的表现。</p><h2 id="1a63" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">摘要</h2><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi oj"><img src="../Images/15bb938d237859c8312406bf4b8500f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DV5jhZBj5ucxyk0cf9nYzg.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk"><a class="ae ma" href="https://docs.google.com/spreadsheets/d/e/2PACX-1vQ_3NX8kEe3hoYGXac4QaUeo5HDAT9__FfhTiqc_sGwG0HoWbeLlGYZXEfDfOnpcTB9n2dBkVmixCNQ/pubhtml?gid=490942750&amp;single=true" rel="noopener ugc nofollow" target="_blank">Spreadsheet Link</a></figcaption></figure><p id="8939" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">“变权重下降”在这个数据集中表现得令人惊讶，具有体面的验证损失，并且相当抗过拟合。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h2 id="ed78" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">总体评论</h2><ol class=""><li id="4a74" class="nj nk iq kh b ki lu kl lv ko ok ks ol kw om la no np nq nr bi translated">我们可以从减肥模特身上得到一些不错的表现。然而，退出概率需要仔细调整。</li><li id="f353" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi translated">我怀疑 MC dropout 性能的下降与嵌入 dropout 的使用有关。我会尝试使用更多的采样轮或只是在测试时间禁用嵌入丢失。</li><li id="dbcf" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi translated">从 IMDB 数据集的结果来看，嵌入丢失似乎在防止过拟合方面有适度的贡献。</li><li id="b42d" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi translated">在 Jupyter notebook 中进行实验(链接到下面的代码)需要大量的手工劳动，并且只能在一定程度上进行管理。如果需要更全面的评估，我会使用类似于<a class="ae ma" href="http://sacred.readthedocs.io/en/latest/quickstart.html" rel="noopener ugc nofollow" target="_blank">神圣的</a>来自动化这个过程。</li></ol><div class="on oo gp gr op oq"><a href="https://github.com/ceshine/recurrent-dropout-experiments" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">ces hine/经常性辍学实验</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">通过在 GitHub 上创建一个帐户，为经常辍学实验的发展做出贡献。</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">github.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe nd oq"/></div></div></a></div></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h2 id="f327" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">参考资料:</h2><ol class=""><li id="2434" class="nj nk iq kh b ki lu kl lv ko ok ks ol kw om la no np nq nr bi translated">Gal，y .，&amp; Ghahramani，Z. (2016)。<a class="ae ma" href="https://arxiv.org/pdf/1506.02142.pdf" rel="noopener ugc nofollow" target="_blank">作为贝叶斯近似的辍学:表示深度学习中的模型不确定性</a>。</li><li id="4370" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi translated">梅里蒂，s .，凯斯卡尔，N. S .，&amp;索彻，R. (2017)。<a class="ae ma" href="http://arxiv.org/abs/1708.02182" rel="noopener ugc nofollow" target="_blank">规范和优化 LSTM 语言模型</a>。</li></ol><h2 id="63f7" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">以前的零件</h2><div class="on oo gp gr op oq"><a href="https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">[学习笔记]循环网络中的辍学—第 1 部分</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">理论基础</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">becominghuman.ai</p></div></div><div class="oz l"><div class="pf l pb pc pd oz pe nd oq"/></div></div></a></div><div class="on oo gp gr op oq"><a href="https://medium.com/towards-data-science/learning-note-dropout-in-recurrent-networks-part-2-f209222481f8" rel="noopener follow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">[学习笔记]循环网络中的辍学—第 2 部分</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">喀拉斯和 PyTorch 的经常性辍学实施情况</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">medium.com</p></div></div><div class="oz l"><div class="pg l pb pc pd oz pe nd oq"/></div></div></a></div></div></div>    
</body>
</html>