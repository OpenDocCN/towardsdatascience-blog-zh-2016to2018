<html>
<head>
<title>M2M Day 191: Deconstructing a self-driving car model (based on my current knowledge)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">M2M 第 191 天:解构自动驾驶汽车模型(基于我目前的知识)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/m2m-day-191-deconstructing-a-self-driving-car-model-based-on-my-current-knowledge-69def7e8e6c0?source=collection_archive---------6-----------------------#2017-05-11">https://towardsdatascience.com/m2m-day-191-deconstructing-a-self-driving-car-model-based-on-my-current-knowledge-69def7e8e6c0?source=collection_archive---------6-----------------------#2017-05-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn"><p id="86dc" class="jo jp iq bd jq jr js jt ju jv jw jx dk translated">这篇文章是为期 12 个月的加速学习项目<a class="ae jy" href="https://medium.com/@maxdeutsch/m2m-day-1-completing-12-ridiculously-hard-challenges-in-12-months-9843700c741f" rel="noopener">月掌握</a>的一部分。今年五月，<a class="ae jy" href="https://medium.com/@maxdeutsch/m2m-day-182-attempting-to-build-a-self-driving-car-809fab9e4723" rel="noopener">我的目标是打造无人驾驶汽车的软件部分</a>。</p></blockquote><p id="f83a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv jx ij bi translated">现在我已经有了可以工作的<em class="kw">无人驾驶汽车</em>代码(<a class="ae jy" href="https://medium.com/@maxdeutsch/m2m-day-190-the-car-is-driving-itself-74490ae509de" rel="noopener">见昨天的视频</a>)，在接下来的几天里，我计划解构代码，并试图理解它到底是如何工作的。</p><p id="bb0b" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">今天，我将特别关注“模型”，它可以被认为是代码的<em class="kw">部分:模型定义了如何将输入图像转换成转向指令。</em></p><p id="ece8" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">今天我没有太多的时间，所以我不会完整地描述代码是如何工作的(因为我还不知道，还需要做大量的研究)。相反，我会对代码行的含义做一些假设，然后记录我需要进一步研究的开放性问题。</p><p id="4036" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">这将使我有条理地学习材料。</p><p id="18b2" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">这是自动驾驶模型的完整代码。它只有 50 行代码加上注释和空格(这是相当疯狂的，因为它在驾驶汽车和其他东西……)</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="ee77" class="ll lm iq lh b gy ln lo l lp lq">import tensorflow as tf<br/>import scipy</span><span id="f783" class="ll lm iq lh b gy lr lo l lp lq">def weight_variable(shape):<br/>  initial = tf.truncated_normal(shape, stddev=0.1)<br/>  return tf.Variable(initial)</span><span id="5e34" class="ll lm iq lh b gy lr lo l lp lq">def bias_variable(shape):<br/>  initial = tf.constant(0.1, shape=shape)<br/>  return tf.Variable(initial)</span><span id="bada" class="ll lm iq lh b gy lr lo l lp lq">def conv2d(x, W, stride):<br/>  return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='VALID')</span><span id="4b96" class="ll lm iq lh b gy lr lo l lp lq">x = tf.placeholder(tf.float32, shape=[None, 66, 200, 3])<br/>y_ = tf.placeholder(tf.float32, shape=[None, 1])</span><span id="bda4" class="ll lm iq lh b gy lr lo l lp lq">x_image = x</span><span id="056e" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#first convolutional layer</strong><br/>W_conv1 = weight_variable([5, 5, 3, 24])<br/>b_conv1 = bias_variable([24])</span><span id="50de" class="ll lm iq lh b gy lr lo l lp lq">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, 2) + b_conv1)</span><span id="4f94" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#second convolutional layer</strong><br/>W_conv2 = weight_variable([5, 5, 24, 36])<br/>b_conv2 = bias_variable([36])</span><span id="8be1" class="ll lm iq lh b gy lr lo l lp lq">h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, 2) + b_conv2)</span><span id="58d5" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#third convolutional layer</strong><br/>W_conv3 = weight_variable([5, 5, 36, 48])<br/>b_conv3 = bias_variable([48])</span><span id="a853" class="ll lm iq lh b gy lr lo l lp lq">h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 2) + b_conv3)</span><span id="dbaf" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#fourth convolutional layer</strong><br/>W_conv4 = weight_variable([3, 3, 48, 64])<br/>b_conv4 = bias_variable([64])</span><span id="49fa" class="ll lm iq lh b gy lr lo l lp lq">h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4, 1) + b_conv4)</span><span id="21d3" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#fifth convolutional layer</strong><br/>W_conv5 = weight_variable([3, 3, 64, 64])<br/>b_conv5 = bias_variable([64])</span><span id="4bc0" class="ll lm iq lh b gy lr lo l lp lq">h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5, 1) + b_conv5)</span><span id="191d" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#FCL 1</strong><br/>W_fc1 = weight_variable([1152, 1164])<br/>b_fc1 = bias_variable([1164])</span><span id="4d1b" class="ll lm iq lh b gy lr lo l lp lq">h_conv5_flat = tf.reshape(h_conv5, [-1, 1152])<br/>h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)</span><span id="acb9" class="ll lm iq lh b gy lr lo l lp lq">keep_prob = tf.placeholder(tf.float32)<br/>h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><span id="905e" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#FCL 2</strong><br/>W_fc2 = weight_variable([1164, 100])<br/>b_fc2 = bias_variable([100])</span><span id="b83b" class="ll lm iq lh b gy lr lo l lp lq">h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><span id="2635" class="ll lm iq lh b gy lr lo l lp lq">h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)</span><span id="cf86" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#FCL 3</strong><br/>W_fc3 = weight_variable([100, 50])<br/>b_fc3 = bias_variable([50])</span><span id="07ee" class="ll lm iq lh b gy lr lo l lp lq">h_fc3 = tf.nn.relu(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)</span><span id="0e07" class="ll lm iq lh b gy lr lo l lp lq">h_fc3_drop = tf.nn.dropout(h_fc3, keep_prob)</span><span id="4520" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#FCL 4</strong><br/>W_fc4 = weight_variable([50, 10])<br/>b_fc4 = bias_variable([10])</span><span id="0c8e" class="ll lm iq lh b gy lr lo l lp lq">h_fc4 = tf.nn.relu(tf.matmul(h_fc3_drop, W_fc4) + b_fc4)</span><span id="4886" class="ll lm iq lh b gy lr lo l lp lq">h_fc4_drop = tf.nn.dropout(h_fc4, keep_prob)</span><span id="61f2" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#Output</strong><br/>W_fc5 = weight_variable([10, 1])<br/>b_fc5 = bias_variable([1])</span><span id="b2a0" class="ll lm iq lh b gy lr lo l lp lq">y = tf.mul(tf.atan(tf.matmul(h_fc4_drop, W_fc5) + b_fc5), 2)</span></pre><h1 id="29d7" class="ls lm iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">逐行评论</h1><p id="e715" class="pw-post-body-paragraph jz ka iq kb b kc mp ke kf kg mq ki kj kk mr km kn ko ms kq kr ks mt ku kv jx ij bi translated">现在，我将分块研究代码，并描述我认为每个块的意思/作用。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="39e0" class="ll lm iq lh b gy ln lo l lp lq">import tensorflow as tf<br/>import scipy</span></pre><p id="62c5" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">前两行很简单。</p><p id="e9d0" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">我们正在导入 TensorFlow 库(我们将在代码的其他地方将其称为“tf ”)和 SciPy 库。TensorFlow 是由 Google 编写的 python 库，它将帮助抽象出大多数地面级机器学习实现。SciPy 会在数学方面帮忙。</p><p id="6293" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">这里没什么新东西可学。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="9239" class="ll lm iq lh b gy ln lo l lp lq">def weight_variable(shape):<br/>  initial = tf.truncated_normal(shape, stddev=0.1)<br/>  return tf.Variable(initial)</span><span id="6e34" class="ll lm iq lh b gy lr lo l lp lq">def bias_variable(shape):<br/>  initial = tf.constant(0.1, shape=shape)<br/>  return tf.Variable(initial)</span></pre><p id="272b" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">好的，这里我认为我们正在定义新的<em class="kw">对象</em>，这基本上意味着我们可以在代码的其他地方使用“weight_variable”和“bias_variable”的概念，而不必一次重新定义它们。</p><p id="822f" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">在机器学习中，我们试图求解的函数通常表示为 Wx+b = y，其中给定了 x(输入图像列表)和 y(对应的转向指令列表)，想要找到 W 和 b 的最佳组合，使方程平衡。</p><p id="a8b4" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">w 和 b 实际上不是单一的数字，而是系数的集合。这些集合是多维的，并且这些集合的大小对应于机器学习网络中的节点数量。(至少，我现在是这么理解的)。</p><p id="7193" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">所以，在上面的代码中，<em class="kw"> weight_variable </em>对象代表 W，<em class="kw"> bias_variable </em>对象代表 b，在广义上。</p><p id="b6c4" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">这些对象接受一个称为“形状”的输入，它基本上定义了 W 和 b 的维数。</p><p id="6971" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">这些 W 和 b 对象由一个名为“normal”的函数初始化。我很确定这意味着…当最初创建 W 和 b 的集合时，单个系数的值应该基于标准偏差为 0.1 的正态分布(即钟形曲线)随机分配。标准偏差或多或少定义了我们希望初始系数有多随机。</p><p id="0ff4" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">所以，令人惊讶的是，我想我基本上理解了这段代码。乍一看，我不确定这是怎么回事，但写出来有助于我整理思绪。</p><p id="aabd" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated"><strong class="kb ir">我还需要学习的:</strong>我需要学习更多关于 Wx + b = y 结构的知识，为什么使用它，它是如何工作的，等等。，但我理解代码的基本原理。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="17cd" class="ll lm iq lh b gy ln lo l lp lq">def conv2d(x, W, stride):<br/>  return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='VALID')</span></pre><p id="8952" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">我相信这个<em class="kw"> conv2d </em>是一个对某些输入执行核卷积的函数。内核卷积是我几天前在<a class="ae jy" href="https://medium.com/towards-data-science/m2m-day-185-my-attempt-to-intuitively-explain-how-this-self-driving-car-algorithm-works-7422eb2b135e" rel="noopener">中学到的一种更通用的图像操作。</a></p><p id="c8d6" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">就我而言，核卷积操纵图像来突出图像的某些特征，无论是图像的边缘、角落等等。</p><p id="3872" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">这个特殊的特征是由“内核”定义的，它似乎是用上面的<em class="kw">步幅=[1，步幅，步幅，1] </em>定义的。虽然，我不知道大步是什么意思，也不知道这到底是怎么回事。</p><p id="a26d" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">这个图像处理函数似乎有三个输入:1 .内核/步幅(说明如何操作图像)；2.x(也就是图像本身)；第三。w(我猜它是一组系数，用于在某种程度上将不同的图像操作混合在一起)。</p><p id="fc81" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">我必须更多地了解 W 在这一切中的角色。</p><p id="1895" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">不过，在高层次上，该函数以某种方式操纵图像，以自动将图像缩减为更有助于<em class="kw">训练模型</em>的独特特征。</p><p id="17d9" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated"><strong class="kb ir">我还需要学习的:</strong>卷积函数在数学上到底是如何定义的，W 在其中又是如何发挥作用的？</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="f82e" class="ll lm iq lh b gy ln lo l lp lq">x = tf.placeholder(tf.float32, shape=[None, 66, 200, 3])<br/>y_ = tf.placeholder(tf.float32, shape=[None, 1])</span><span id="184a" class="ll lm iq lh b gy lr lo l lp lq">x_image = x</span></pre><p id="0be3" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">接下来的几行看起来很简单。我们再一次回到等式 Wx + b = y。</p><p id="afd1" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">这里我们本质上是为 x 和 y 变量定义了占位符<em class="kw">。这些占位符设置了变量的维度(记住:这些变量代表一个值的集合，而不仅仅是一个数字)。</em></p><p id="b3f8" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">我们将 x 设置为接收特定尺寸的图像，将 y 设置为输出一个数字(即转向角)。</p><p id="edb7" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">然后我们把 x 改名为“x_image”来提醒自己，x 是一个图像，因为……为什么不是。</p><p id="17c2" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">这里没什么新东西可学。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="fb50" class="ll lm iq lh b gy ln lo l lp lq"><strong class="lh ir">#first convolutional layer</strong><br/>W_conv1 = weight_variable([5, 5, 3, 24])<br/>b_conv1 = bias_variable([24])</span><span id="400e" class="ll lm iq lh b gy lr lo l lp lq">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, 2) + b_conv1)</span></pre><p id="add7" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">好了，我们现在进入第一个卷积层。</p><p id="41c2" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">我们定义<em class="kw"> W_conv1 </em>，它只是我上面解释的 weight_variable 的一个具体实例(用 shape [5，5，3，24])。我不确定这个形状是如何或为什么以这种特殊的方式设置的。</p><p id="2b15" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">然后我们定义<em class="kw"> b_conv1 </em>，它只是我上面解释的 bias_variable 的一个具体实例(具有形状[24])。这个 24 可能需要匹配 W_conv1 形状的 24，但我不确定为什么(除了这将有助于使<em class="kw">矩阵乘法</em>工作)。</p><p id="b7c2" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">h_conv1 是一个中间对象，它将卷积函数应用于输入 x_image 和 W_conv1，将 bconv1 添加到卷积的输出，然后通过一个名为<em class="kw"> relu </em>的函数处理一切。</p><p id="dbb9" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">这个<em class="kw"> relu </em>的东西听起来很熟悉，但是我不记得它到底是做什么的了。我的猜测是，这是某种“squasher”或规范化功能，在某种程度上平滑一切，无论这意味着什么。我得调查一下。</p><p id="c25c" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">虽然我可以<em class="kw">阅读</em>大部分代码，但我不太确定为什么“卷积层”要以这种方式设置。</p><p id="39e2" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated"><strong class="kb ir">我还需要学习的:</strong>什么是卷积层，它应该做什么，它是如何做到的？</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="4ba7" class="ll lm iq lh b gy ln lo l lp lq"><strong class="lh ir">#second convolutional layer</strong><br/>W_conv2 = weight_variable([5, 5, 24, 36])<br/>b_conv2 = bias_variable([36])</span><span id="8b7b" class="ll lm iq lh b gy lr lo l lp lq">h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, 2) + b_conv2)</span><span id="1a37" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#third convolutional layer</strong><br/>W_conv3 = weight_variable([5, 5, 36, 48])<br/>b_conv3 = bias_variable([48])</span><span id="6488" class="ll lm iq lh b gy lr lo l lp lq">h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 2) + b_conv3)</span><span id="23c0" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#fourth convolutional layer</strong><br/>W_conv4 = weight_variable([3, 3, 48, 64])<br/>b_conv4 = bias_variable([64])</span><span id="ba21" class="ll lm iq lh b gy lr lo l lp lq">h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4, 1) + b_conv4)</span><span id="8be4" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#fifth convolutional layer</strong><br/>W_conv5 = weight_variable([3, 3, 64, 64])<br/>b_conv5 = bias_variable([64])</span><span id="4794" class="ll lm iq lh b gy lr lo l lp lq">h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5, 1) + b_conv5)</span></pre><p id="691b" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">我们继续有四个以上的卷积层，其功能与第一层完全相同，但不是使用 x_image 作为输入，而是使用来自前一层的输出(即 h_conv 的东西)。</p><p id="7af2" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">我不确定我们是如何决定使用五层的，也不知道为什么每个 conv 的形状都不一样。</p><p id="07ba" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated"><strong class="kb ir">我还需要学习的是:</strong>为什么有五层，我们如何<em class="kw">为每一层选择</em>形状？</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="e2d0" class="ll lm iq lh b gy ln lo l lp lq"><strong class="lh ir">#FCL 1</strong><br/>W_fc1 = weight_variable([1152, 1164])<br/>b_fc1 = bias_variable([1164])</span><span id="2744" class="ll lm iq lh b gy lr lo l lp lq">h_conv5_flat = tf.reshape(h_conv5, [-1, 1152])<br/>h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)</span><span id="edbb" class="ll lm iq lh b gy lr lo l lp lq">keep_prob = tf.placeholder(tf.float32)<br/>h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><span id="cc4e" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#FCL 2</strong><br/>W_fc2 = weight_variable([1164, 100])<br/>b_fc2 = bias_variable([100])</span><span id="2622" class="ll lm iq lh b gy lr lo l lp lq">h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><span id="52f3" class="ll lm iq lh b gy lr lo l lp lq">h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)</span><span id="e994" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#FCL 3</strong><br/>W_fc3 = weight_variable([100, 50])<br/>b_fc3 = bias_variable([50])</span><span id="b4a2" class="ll lm iq lh b gy lr lo l lp lq">h_fc3 = tf.nn.relu(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)</span><span id="5140" class="ll lm iq lh b gy lr lo l lp lq">h_fc3_drop = tf.nn.dropout(h_fc3, keep_prob)</span><span id="2826" class="ll lm iq lh b gy lr lo l lp lq"><strong class="lh ir">#FCL 4</strong><br/>W_fc4 = weight_variable([50, 10])<br/>b_fc4 = bias_variable([10])</span><span id="6c6c" class="ll lm iq lh b gy lr lo l lp lq">h_fc4 = tf.nn.relu(tf.matmul(h_fc3_drop, W_fc4) + b_fc4)</span><span id="0205" class="ll lm iq lh b gy lr lo l lp lq">h_fc4_drop = tf.nn.dropout(h_fc4, keep_prob)</span></pre><p id="aa09" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">接下来，我们有四个 fcl，我相信它们代表“全连接层”。</p><p id="4b9f" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">这些层的设置似乎类似于卷积步骤，但我不确定这里发生了什么。我认为这只是普通的神经网络的东西(我写它是为了假装我完全理解“普通的神经网络的东西”)。</p><p id="4bb8" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">不管怎样，我会更深入地调查这件事。</p><p id="b45b" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated"><strong class="kb ir">我还需要学习的:</strong>什么是 FCL，每个 FCL 步骤中发生了什么？</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="fe3e" class="ll lm iq lh b gy ln lo l lp lq"><strong class="lh ir">#Output</strong><br/>W_fc5 = weight_variable([10, 1])<br/>b_fc5 = bias_variable([1])</span><span id="0dbb" class="ll lm iq lh b gy lr lo l lp lq">y = tf.mul(tf.atan(tf.matmul(h_fc4_drop, W_fc5) + b_fc5), 2)</span></pre><p id="55dc" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">最后，我们采用最终 FCL 层的输出，进行一些疯狂的三角运算，然后输出 y，即预测的转向角。</p><p id="5f69" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">这一步似乎只是“把数学算出来”，但我不确定。</p><p id="2926" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">我还需要学习的:产量是如何以及为什么以这种方式计算的？</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><p id="cfb7" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">完成了。</p><p id="37b9" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">这比预期的要长——主要是因为我能够解析的东西比预期的多。</p><p id="0006" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">TensorFlow 库抽象出了多少实现，而构建一个完全有能力的自动驾驶汽车模型所需的底层数学知识又是如此之少，这有点疯狂。</p><p id="ef61" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">对于我们作为模型构造者来说，似乎唯一重要的事情就是如何设置模型的深度(例如层数)、每层的形状以及层的类型。</p><p id="2b76" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">我的猜测是，这可能更像是一门艺术，而不是科学，但很可能是一门受过教育的艺术。</p><p id="eac5" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">我明天开始钻研我的开放式问题。</p><blockquote class="jn"><p id="c980" class="jo jp iq bd jq jr nb nc nd ne nf jx dk translated">阅读<a class="ae jy" href="https://medium.com/@maxdeutsch/m2m-day-192-taking-a-break-my-favorite-youtube-channels-856348098dac" rel="noopener">下一篇</a>。看了<a class="ae jy" href="https://medium.com/@maxdeutsch/m2m-day-190-the-car-is-driving-itself-74490ae509de" rel="noopener">以前的帖子</a>。</p></blockquote></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h2 id="9e19" class="ll lm iq bd lt ng nh dn lx ni nj dp mb kk nk nl mf ko nm nn mj ks no np mn nq bi translated"><a class="ae jy" href="http://max.xyz" rel="noopener ugc nofollow" target="_blank">马克斯·多伊奇</a>是一名痴迷的学习者、产品建造者、为期<a class="ae jy" href="http://MonthToMaster.com" rel="noopener ugc nofollow" target="_blank">个月以掌握</a>的试验品，以及<a class="ae jy" href="http://OpenmindLearning.com" rel="noopener ugc nofollow" target="_blank"> Openmind </a>的创始人。</h2><h2 id="1e64" class="ll lm iq bd lt ng nh dn lx ni nj dp mb kk nk nl mf ko nm nn mj ks no np mn nq bi translated">如果你想跟随 Max 长达一年的加速学习项目，请确保跟随<a class="ae jy" href="https://medium.com/@maxdeutsch" rel="noopener">这个媒介账户</a>。</h2></div></div>    
</body>
</html>