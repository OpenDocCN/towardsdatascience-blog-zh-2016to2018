<html>
<head>
<title>Introduction to Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Q学习简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-q-learning-88d1c4f2b49c?source=collection_archive---------1-----------------------#2017-03-13">https://towardsdatascience.com/introduction-to-q-learning-88d1c4f2b49c?source=collection_archive---------1-----------------------#2017-03-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="d4c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想象自己在迷宫中寻宝。游戏如下:</p><p id="d2bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你从一个给定的位置开始，起始状态。从任何状态你都可以向左、向右、向上或向下走，或者停留在同一个地方，只要你不穿过迷宫的前提。每个动作将带你到网格的一个单元(不同的状态)。现在，在其中一个州(目标州)有一个宝箱。此外，迷宫在某些位置/状态有一个蛇坑。您的目标是沿着一条没有蛇的路径从起始状态行进到目标状态。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/6a802fd56639276cd0c37dfea0fd0870.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*xl9tXwGCZpqWaSy26A6SIg.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Grid outline of the maze</figcaption></figure><p id="9735" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当您将一个代理放入网格(我们称之为我们的环境)时，它将首先探索。它不知道什么是蛇，也不知道宝藏在哪里。所以，为了给它蛇和宝箱的概念，我们会在它完成每个动作后给它一些奖励。它每踏上一个蛇穴，我们将给予它-10的奖励。对于宝藏，我们将给予+10的奖励。现在我们希望我们的代理尽快完成任务(走最短的路线)。为此，我们将给予其他州-1的奖励。然后我们会告诉它最大化分数。现在，随着代理探索，它了解到蛇对它是有害的，宝藏对它是有益的，它必须尽快得到宝藏。图中的'-'路径表示奖励最大的最短路径。</p><h2 id="608c" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated">Q-Learning试图了解处于特定状态的价值，并在那里采取特定的行动。</h2><p id="9927" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">我们要做的是开发一个表格。其中行是状态，列是它可以采取的动作。因此，我们有一个16x5 (80个可能的状态-动作)对，其中每个状态是迷宫网格的一个单元。</p><p id="719b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们首先将该表初始化为统一的(全零)，然后当我们观察到我们从各种行为中获得的奖励时，我们相应地更新该表。我们将使用<strong class="jp ir"> <em class="lv">贝尔曼方程</em>来更新表格。</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/41dbe4f79dc7b9dae1acfb8d41e892b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*2RjuuBUk5ArFAGjaU4jiwA.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">eqn.1</figcaption></figure><p id="c57f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“s”代表当前状态。‘a’表示代理从当前状态采取的动作。s”表示该操作产生的状态。“r”是你采取行动得到的奖励，“γ”是折扣系数。所以，采取行动a的状态的Q值是即时奖励和未来奖励的折扣之和(结果状态的值)。折扣系数‘γ’决定了你对未来奖励的重视程度。比方说，你去了一个离目标州更远的州，但是从那个州开始，遇到有蛇的州的机会就更少了，所以，这里未来的奖励更多，尽管即时奖励更少。</p><p id="18c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将每次迭代(代理所做的尝试)称为一个情节。对于每一集，代理将试图达到目标状态，并且对于每个转换，它将继续更新Q表的值。</p><p id="2b2e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看如何计算Q表:</p><p id="8d28" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为此，为了方便起见，我们将采用一个较小的迷宫网格。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/35b5402336b9ca151183668d9134e2af.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*Wb_F7jzXP5RIERP8sAT2Mw.png"/></div></figure><p id="2650" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最初的Q表看起来像这样(状态沿着行，动作沿着列) :</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/3fc753fb1b6a1de8067dc889a6fee1fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*90FiDykQMduVcKHs4PYuXg.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Q Matrix</figcaption></figure><p id="2284" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上、下、左、右</p><p id="bad7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">奖励表应该是这样的:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/849ed75c1644b642d2ef9a1ebbb6a00e.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*kL-5AYkojm0d5dyK6h1Y9A.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">R Matrix</figcaption></figure><p id="398d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，E表示零，即，不可能有这样的转变。</p><p id="d067" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">算法:</p><ol class=""><li id="fd7d" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">用全零初始化Q矩阵。设置“γ”的值。填写奖励矩阵。</li><li id="a0c3" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">每集。选择一个随机的起始状态(这里我们将起始状态限制为state-1)。</li><li id="d8bf" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">从当前状态的所有可能操作中选择一个。</li><li id="d158" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">作为动作(a)的结果，移动到下一个状态(S’)。</li><li id="7c8e" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">对于状态中所有可能的动作，选择具有最高Q值的一个。</li><li id="0788" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">使用公式1更新Q表。</li><li id="21d2" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">将下一个状态设置为当前状态。</li><li id="4ba0" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">如果达到目标状态，则结束。</li></ol><p id="25f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如:假设我们从状态1开始。我们可以选择D或r。比如说，我们选择了D。然后我们将到达3(蛇坑)。因此，我们可以选择U或R，取γ = 0.8，我们有:</p><p id="ecea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Q(1，D) = R(1，D) + γ*[max(Q(3，U) &amp; Q(3，R))]</p><p id="6a39" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Q(1，D) = -10 + 0.8*0 = -10</p><p id="33d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，max(Q(3，U) &amp; Q(3，R)) = 0，因为Q矩阵尚未更新。-10代表踩蛇。因此，新的Q表看起来像:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/15b4a259087691176e7368b70b6c5287.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*7tcb3zFTBM6xiDad0Nue2w.png"/></div></figure><p id="e1e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，3是起始状态。从3开始，假设我们去r。所以，我们去4。从4开始，我们可以去U或L。</p><p id="401b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Q(3，R) = R(3，R) + 0.8*[max(Q(4，U) &amp; Q(4，L))]</p><p id="3dd4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Q(3，R) = 10 + 0.8*0 = 10</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/f22eff2dfb08153238d3e0bd27197348.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*AkzYfU-mrl4HxLiWrLCUIw.png"/></div></figure><p id="a82b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，现在我们已经达到了目标状态4。所以，我们终止和更多的传递，让我们的代理理解每个状态和动作的值。继续传递，直到值保持不变。这意味着您的代理已经尝试了所有可能的状态-动作对。</p><p id="6189" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">python中的实现:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mq"><img src="../Images/7bf853886ff37f04bfc4d658d3125267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c-jNu4CKjHPydCIciTkucA.png"/></div></div></figure><p id="f915" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后一个q_matrix的输出:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/8ac6484aa8f04747fb220dc626539a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*c-7yZFLzX8KllguwykUkJg.png"/></div></figure><p id="3dbc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在下一篇文章中，我将介绍使用神经网络进行Q学习，以及使用表格方法的缺点。此外，我们将致力于开放人工智能健身房的游戏测试。在那之前，再见。</p></div></div>    
</body>
</html>