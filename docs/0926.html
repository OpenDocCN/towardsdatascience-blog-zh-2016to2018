<html>
<head>
<title>K-Means Data Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-均值数据聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-data-clustering-bce3335d2203?source=collection_archive---------5-----------------------#2017-07-10">https://towardsdatascience.com/k-means-data-clustering-bce3335d2203?source=collection_archive---------5-----------------------#2017-07-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="e584" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>当今世界，随着互联网使用的增加，产生的数据量惊人地庞大。即使单个数据的性质很简单，但要处理的数据量之大，甚至连计算机都难以处理。</p><p id="bd12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们需要大数据分析工具来处理这样的流程。数据挖掘算法和技术以及机器学习为我们提供了以可理解的方式解释大数据的方法。k-means 是一种数据聚类算法，可用于无监督的机器学习。它能够将相似的未标记数据分组到预定数量的簇(k)中。需要注意的是，我们需要事先知道数据中的自然聚类数，尽管可以对代码进行轻微的修改，以计算为代价找到最优的聚类数。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ku"><img src="../Images/1a96d7494434e9f5bfcaa5773794e79a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vNng_oOsNRHKrlh3pjSAyA.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">Source: <a class="ae lk" href="https://i.stack.imgur.com/cIDB3.png" rel="noopener ugc nofollow" target="_blank">https://i.stack.imgur.com/cIDB3.png</a></figcaption></figure><p id="cde0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">K-means 聚类问题可以正式表述为<em class="ll">“给定一个整数 k 和 R^d 中的一组 n 个数据点，目标是选择 k 个中心，以最小化每个数据点与其最近中心之间的总平方距离”</em>。精确地解决这个问题是 NP 难的，但是有算法可以局部搜索它的解。标准算法是由 Stuart Lloyd 在 1957 年首先提出的，它依赖于迭代改进。</p><p id="b74b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，必须确定聚类中心的初始集合。这可以随机分配。但是这不能保证准确性。这可以在 python 中完成，如下所示:</p><pre class="kv kw kx ky gt lm ln lo lp aw lq bi"><span id="d50e" class="lr ls iq ln b gy lt lu l lv lw">#datapoints is a 2D array initialized with the points to be clustered<br/>centroids = []<br/>for i in range(1,k+1): <br/>    uniqueNumber = False <br/>    while(uniqueNumber==False): <br/>    a = randint(0, len(dataPoints)-1) <br/>    if (dataPoints[a] not in centroids): <br/>        uniqueNumber = True <br/>    centroids.append(dataPoints[a])</span></pre><p id="0d48" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么算法通过在两个步骤之间交替进行。</p><p id="cd1c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">步骤 1: </strong>(分配步骤)将每个数据点分配给根据欧几里德距离具有最接近该数据点的中心的聚类。</p><p id="2365" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">步骤 2: </strong>(更新步骤)使用每个聚类的所有成员重新计算每个聚类的质心。</p><p id="8b0d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">保证每个点与其最近的中心之间的总平方距离单调递减，以便配置不会重复。此外，由于只有 k^n 可能的集群，该过程将总是终止。由于该算法是局部搜索，不能保证全局最优解，初始种子在很大程度上影响结果。取决于初始种子的质量，聚类可能是任意差的。有一些选择初始种子的方法可以增加找到全局最优解的概率。</p><p id="6a0e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">选择初始中心后，数据点可以聚类为:</p><pre class="kv kw kx ky gt lm ln lo lp aw lq bi"><span id="9d6e" class="lr ls iq ln b gy lt lu l lv lw">closestCluster = [0,euclidean_distance(point,centroids[0])] <br/>for i in range(1,k): <br/>    dist = euclidean_distance(point,centroids[i])<br/>    if (dist&lt;closestCluster[1]): <br/>        closestCluster=[i,dist] clusters[closestCluster[0]].append(point)</span></pre><p id="dee9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在第一次聚类之后，使用如下聚类来找到新的质心:</p><pre class="kv kw kx ky gt lm ln lo lp aw lq bi"><span id="2644" class="lr ls iq ln b gy lt lu l lv lw">#np is numpy<br/>newCentroids = []<br/>for i in range(k): <br/>    newCentroids.append(np.mean(clusters[i],axis=0).tolist())</span></pre><p id="a2b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我们比较以前的聚类中心和新的质心，如果有任何变化，我们回到分配步骤。如果质心没有变化，我们就终止循环。</p><p id="b144" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请务必访问我的回购在<a class="ae lk" href="https://github.com/niruhan/k-means-clustering" rel="noopener ugc nofollow" target="_blank">https://github.com/niruhan/k-means-clustering</a>的完整代码。标准算法有许多改进，有望提高性能。有些是 k-means++，k-means#。此外，标准算法是一个批处理算法，这意味着我们应该在开始处理之前拥有所有的数据点。但实际上，数据是以流的形式到达处理引擎的，需要进行实时分析。k-means 算法有在线版本，可以对流数据集进行聚类。顺序 k-means 是一种标准算法。还有小批量 k-means，以及使用衰减率处理非平稳数据的方法。</p><p id="fe89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">T3】参考文献:T5】</strong></p><ol class=""><li id="3f5c" class="lx ly iq jp b jq jr ju jv jy lz kc ma kg mb kk mc md me mf bi translated"><em class="ll">“k-means ++小心播种的好处”——大卫·亚瑟和谢尔盖·瓦西里维茨基</em></li><li id="f2a3" class="lx ly iq jp b jq mg ju mh jy mi kc mj kg mk kk mc md me mf bi translated"><em class="ll">“k 均值聚类”在 https://en.wikipedia.org/wiki/K-means_clustering</em></li></ol></div></div>    
</body>
</html>