<html>
<head>
<title>Learn Word2Vec by implementing it in tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过在tensorflow中实现来学习Word2Vec</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac?source=collection_archive---------0-----------------------#2017-07-09">https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac?source=collection_archive---------0-----------------------#2017-07-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/f47503e358b6b95b1d69a95285dd3b01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TQc7Xk8_N6SzpqZSBVq69A.jpeg"/></div></div></figure><div class=""/><p id="1fb0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">嗨！</p><p id="9558" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我觉得理解一个算法最好的方法就是实现它。因此，在这篇文章中，我将通过在张量流<em class="kw">中实现来教你单词嵌入。</em></p><p id="07d3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw">这篇文章背后的想法是避免所有的介绍和通常与word embeddeds/word 2 vec相关的闲聊，直接进入事情的实质。因此，许多国王-男人-女人-王后的例子将被跳过。</em></p><h1 id="b313" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">我们如何进行这些嵌入？</h1><p id="004a" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">有许多获得单词嵌入的技术，我们将讨论一种已经很出名的技术，唯一的一种，word2vec。与普遍的看法相反，word2vec不是一个<em class="kw">深</em>的网络，它只有3层！</p><p id="e2b5" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">注意:word2vec有很多技术细节，为了更容易理解，我将跳过这些细节。</p><h1 id="41b3" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">word2vec的工作原理:</h1><p id="f3a7" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">word2vec背后的想法是:</p><ol class=""><li id="d697" class="ma mb jb ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">以一个3层神经网络为例。(1个输入层+ 1个隐藏层+ 1个输出层)</li><li id="1175" class="ma mb jb ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">给它一个单词，训练它预测它的相邻单词。</li><li id="255e" class="ma mb jb ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">移除最后一层(输出层)，保留输入层和隐藏层。</li><li id="3eae" class="ma mb jb ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">现在，从词汇表中输入一个单词。隐藏层给出的输出是输入单词的<em class="kw">‘单词嵌入’</em>。</li></ol><p id="4900" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">就是这样！仅仅做这个简单的任务就能让我们的网络学习有趣的单词表达。</p><p id="1acf" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们开始实现它来充实这种理解。</p><p id="5530" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(完整代码可在<a class="ae mo" href="https://gist.github.com/aneesh-joshi/c8a451502958fa367d84bf038081ee4b" rel="noopener ugc nofollow" target="_blank">这里</a>获得。我建议您理解这篇文章中的内容，然后使用其中的代码。)</p><p id="1b0d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是我们将要处理的原始文本:</p><p id="50e7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(<em class="kw">为了简化，我故意把句号隔开，并在最后避开了它们。一旦你理解了，就可以随意使用记号赋予器</em></p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="4f4d" class="my ky jb mu b gy mz na l nb nc">import numpy as np<br/>import tensorflow as tf</span><span id="5621" class="my ky jb mu b gy nd na l nb nc">corpus_raw = 'He is the king . The king is royal . She is the royal  queen '</span><span id="a07a" class="my ky jb mu b gy nd na l nb nc"># convert to lower case<br/>corpus_raw = corpus_raw.lower()</span></pre><p id="9bb6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们需要将它转换成一个输入输出对，这样如果我们输入一个单词，它应该预测相邻的单词:它前后的n个单词，其中n是参数，这里有一个例子，来自Chris McCormick在word2vec上发表的一篇令人惊叹的文章。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/8dbce9a4692ad87b0ef2a048c2f1e3ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yiH5sZI-IBxDSQMKhvbcHw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">A training sample generation with a window size of 2.</figcaption></figure><p id="ad49" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw">注意:如果单词在句子的开头或结尾，窗口将忽略外面的单词。</em></p><p id="5d04" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在此之前，我们将创建一个字典，将单词翻译成整数，将整数翻译成单词。这个以后会派上用场的。</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="2914" class="my ky jb mu b gy mz na l nb nc">words = []</span><span id="45d6" class="my ky jb mu b gy nd na l nb nc">for word in corpus_raw.split():<br/>    if word != '.': # because we don't want to treat . as a word<br/>        words.append(word)</span><span id="fb2b" class="my ky jb mu b gy nd na l nb nc">words = set(words) # so that all duplicate words are removed</span><span id="bef2" class="my ky jb mu b gy nd na l nb nc">word2int = {}<br/>int2word = {}</span><span id="b306" class="my ky jb mu b gy nd na l nb nc">vocab_size = len(words) # gives the total number of unique words</span><span id="c77f" class="my ky jb mu b gy nd na l nb nc">for i,word in enumerate(words):<br/>    word2int[word] = i<br/>    int2word[i] = word</span></pre><p id="14b3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这些字典允许我们做的是:</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="60a2" class="my ky jb mu b gy mz na l nb nc">print(word2int['queen'])<br/>-&gt; 42 (say)</span><span id="c696" class="my ky jb mu b gy nd na l nb nc">print(int2word[42])<br/>-&gt; 'queen'</span></pre><p id="f2c6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，我们想要一个句子列表作为单词列表:</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="b55f" class="my ky jb mu b gy mz na l nb nc"># raw sentences is a list of sentences.<br/>raw_sentences = corpus_raw.split('.')</span><span id="0fbc" class="my ky jb mu b gy nd na l nb nc">sentences = []</span><span id="8138" class="my ky jb mu b gy nd na l nb nc">for sentence in raw_sentences:<br/>    sentences.append(sentence.split())</span></pre><p id="beb8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这会给我们一个句子列表，每个句子都是一个单词列表。</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="81f4" class="my ky jb mu b gy mz na l nb nc">print(sentences)<br/>-&gt; [['he', 'is', 'the', 'king'], ['the', 'king', 'is', 'royal'], ['she', 'is', 'the', 'royal', 'queen']]</span></pre><p id="6512" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，我们将生成我们的训练数据:</p><p id="7f12" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw">(这可能会变得难以阅读。参考代码链接)</em></p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="cb01" class="my ky jb mu b gy mz na l nb nc">data = []</span><span id="83fd" class="my ky jb mu b gy nd na l nb nc">WINDOW_SIZE = 2</span><span id="d751" class="my ky jb mu b gy nd na l nb nc">for sentence in sentences:<br/>    for word_index, word in enumerate(sentence):<br/>        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : <br/>            if nb_word != word:<br/>                data.append([word, nb_word])</span></pre><p id="fa2e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这基本上给出了一个单词列表，单词对。<em class="kw">(我们考虑窗口大小为2) </em></p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="b350" class="my ky jb mu b gy mz na l nb nc">print(data)</span><span id="7fac" class="my ky jb mu b gy nd na l nb nc">[['he', 'is'],<br/> ['he', 'the'],</span><span id="0444" class="my ky jb mu b gy nd na l nb nc"> ['is', 'he'],<br/> ['is', 'the'],<br/> ['is', 'king'],</span><span id="4416" class="my ky jb mu b gy nd na l nb nc"> ['the', 'he'],<br/> ['the', 'is'],<br/> ['the', 'king'],</span><span id="d993" class="my ky jb mu b gy nd na l nb nc">.<br/>.<br/>.<br/>]</span></pre><p id="c998" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们有训练数据。但是它需要用计算机能理解的方式来表达，比如用数字。这就是我们的格言派上用场的地方。</p><p id="7037" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们更进一步，把这些数字转换成一个热矢量。</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="8bf3" class="my ky jb mu b gy mz na l nb nc">i.e., <br/>say we have a vocabulary of 3 words : pen, pineapple, apple<br/>where <br/>word2int['pen'] -&gt; 0 -&gt; [1 0 0]<br/>word2int['pineapple'] -&gt; 1 -&gt; [0 1 0]<br/>word2int['apple'] -&gt; 2 -&gt; [0 0 1]</span></pre><p id="184f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">为什么是一个热点载体？:稍后告知</strong></p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="80ae" class="my ky jb mu b gy mz na l nb nc"># function to convert numbers to one hot vectors<br/>def to_one_hot(data_point_index, vocab_size):<br/>    temp = np.zeros(vocab_size)<br/>    temp[data_point_index] = 1<br/>    return temp</span><span id="6080" class="my ky jb mu b gy nd na l nb nc">x_train = [] # input word<br/>y_train = [] # output word</span><span id="a5c9" class="my ky jb mu b gy nd na l nb nc">for data_word in data:<br/>    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))<br/>    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))</span><span id="6e8e" class="my ky jb mu b gy nd na l nb nc"># convert them to numpy arrays<br/>x_train = np.asarray(x_train)<br/>y_train = np.asarray(y_train)</span></pre><p id="3634" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以现在我们有x_train和y_train:</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="3183" class="my ky jb mu b gy mz na l nb nc">print(x_train)<br/>-&gt;<br/>[[ 0.  0.  0.  0.  0.  0.  1.]<br/> [ 0.  0.  0.  0.  0.  0.  1.]<br/> [ 0.  0.  0.  0.  0.  1.  0.]<br/> [ 0.  0.  0.  0.  0.  1.  0.]<br/> [ 0.  0.  0.  0.  0.  1.  0.]<br/> [ 0.  0.  0.  0.  1.  0.  0.]<br/> [ 0.  0.  0.  0.  1.  0.  0.]<br/> [ 0.  0.  0.  0.  1.  0.  0.]<br/> [ 0.  0.  0.  1.  0.  0.  0.]<br/> [ 0.  0.  0.  1.  0.  0.  0.]<br/> [ 0.  0.  0.  0.  1.  0.  0.]<br/> [ 0.  0.  0.  0.  1.  0.  0.]<br/> [ 0.  0.  0.  1.  0.  0.  0.]<br/> [ 0.  0.  0.  1.  0.  0.  0.]<br/> [ 0.  0.  0.  1.  0.  0.  0.]<br/> [ 0.  0.  0.  0.  0.  1.  0.]<br/> [ 0.  0.  0.  0.  0.  1.  0.]<br/> [ 0.  0.  0.  0.  0.  1.  0.]<br/> [ 0.  1.  0.  0.  0.  0.  0.]<br/> [ 0.  1.  0.  0.  0.  0.  0.]<br/> [ 0.  0.  1.  0.  0.  0.  0.]<br/> [ 0.  0.  1.  0.  0.  0.  0.]<br/> [ 0.  0.  0.  0.  0.  1.  0.]<br/> [ 0.  0.  0.  0.  0.  1.  0.]<br/> [ 0.  0.  0.  0.  0.  1.  0.]<br/> [ 0.  0.  0.  0.  1.  0.  0.]<br/> [ 0.  0.  0.  0.  1.  0.  0.]<br/> [ 0.  0.  0.  0.  1.  0.  0.]<br/> [ 0.  0.  0.  0.  1.  0.  0.]<br/> [ 0.  1.  0.  0.  0.  0.  0.]<br/> [ 0.  1.  0.  0.  0.  0.  0.]<br/> [ 0.  1.  0.  0.  0.  0.  0.]<br/> [ 1.  0.  0.  0.  0.  0.  0.]<br/> [ 1.  0.  0.  0.  0.  0.  0.]]</span></pre><p id="b586" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">两者都有形状:</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="d6d4" class="my ky jb mu b gy mz na l nb nc">print(x_train.shape, y_train.shape)<br/>-&gt;<br/>(34, 7) (34, 7)</span><span id="33ee" class="my ky jb mu b gy nd na l nb nc"># meaning 34 training points, where each point has 7 dimensions</span></pre><h1 id="ab87" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">制作张量流模型</h1><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="1563" class="my ky jb mu b gy mz na l nb nc"># making placeholders for x_train and y_train</span><span id="a9b6" class="my ky jb mu b gy nd na l nb nc">x = tf.placeholder(tf.float32, shape=(None, vocab_size))<br/>y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))</span></pre><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/1c99a00ce367be068dde2e1fa077b6ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Os5hj9qg1t6sr0S3DF4gyA.jpeg"/></div></div></figure><p id="99fc" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从上图中可以看出，我们将训练数据转换成嵌入式表示。</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="f0d2" class="my ky jb mu b gy mz na l nb nc">EMBEDDING_DIM = 5 # you can choose your own number</span><span id="75f4" class="my ky jb mu b gy nd na l nb nc">W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))</span><span id="5c21" class="my ky jb mu b gy nd na l nb nc">b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) #bias</span><span id="104f" class="my ky jb mu b gy nd na l nb nc">hidden_representation = tf.add(tf.matmul(x,W1), b1)</span></pre><p id="2614" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我们利用嵌入维度中的信息，对邻居进行预测。为了进行预测，我们使用softmax。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/c17c7803e3ddbfa14d9d45b86c380523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KxWiUoe-FXPpBdATP-IHOw.jpeg"/></div></div></figure><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="3de2" class="my ky jb mu b gy mz na l nb nc">W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))</span><span id="6b6d" class="my ky jb mu b gy nd na l nb nc">b2 = tf.Variable(tf.random_normal([vocab_size]))</span><span id="7259" class="my ky jb mu b gy nd na l nb nc">prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))</span></pre><p id="ceeb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">总结一下:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/92a58fcce2dbc322ddca8d51ad0f1c29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cnzY08TWRxG3lMKExbslHw.jpeg"/></div></div></figure><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="2358" class="my ky jb mu b gy mz na l nb nc">input_one_hot  ---&gt;  embedded repr. ---&gt; predicted_neighbour_prob</span><span id="5f0d" class="my ky jb mu b gy nd na l nb nc">predicted_prob will be compared against a one hot vector to correct it.</span></pre><p id="0078" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，剩下的就是训练它:</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="b6eb" class="my ky jb mu b gy mz na l nb nc">sess = tf.Session()</span><span id="af35" class="my ky jb mu b gy nd na l nb nc">init = tf.global_variables_initializer()</span><span id="8861" class="my ky jb mu b gy nd na l nb nc">sess.run(init) #make sure you do this!</span><span id="89ef" class="my ky jb mu b gy nd na l nb nc"># define the loss function:<br/>cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))</span><span id="e6e1" class="my ky jb mu b gy nd na l nb nc"># define the training step:<br/>train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)</span><span id="3428" class="my ky jb mu b gy nd na l nb nc">n_iters = 10000</span><span id="05b9" class="my ky jb mu b gy nd na l nb nc"># train for n_iter iterations</span><span id="0661" class="my ky jb mu b gy nd na l nb nc">for _ in range(n_iters):<br/>    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})</span><span id="35af" class="my ky jb mu b gy nd na l nb nc">    print('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))</span></pre><p id="ed42" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在训练中，你会得到损失的显示:</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="7c5c" class="my ky jb mu b gy mz na l nb nc">loss is :  2.73213<br/>loss is :  2.30519<br/>loss is :  2.11106<br/>loss is :  1.9916<br/>loss is :  1.90923<br/>loss is :  1.84837<br/>loss is :  1.80133<br/>loss is :  1.76381<br/>loss is :  1.73312<br/>loss is :  1.70745<br/>loss is :  1.68556<br/>loss is :  1.66654<br/>loss is :  1.64975<br/>loss is :  1.63472<br/>loss is :  1.62112<br/>loss is :  1.6087<br/>loss is :  1.59725<br/>loss is :  1.58664<br/>loss is :  1.57676<br/>loss is :  1.56751<br/>loss is :  1.55882<br/>loss is :  1.55064<br/>loss is :  1.54291<br/>loss is :  1.53559<br/>loss is :  1.52865<br/>loss is :  1.52206<br/>loss is :  1.51578<br/>loss is :  1.50979<br/>loss is :  1.50408<br/>loss is :  1.49861<br/>.<br/>.<br/>.</span></pre><p id="29da" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">它最终在持续亏损的基础上稳定下来。即使不能得到高精度，我们也不在乎。我们感兴趣的只是W1和b1，也就是隐藏的表示。</p><p id="ca2f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们来看看它们:</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="ba3a" class="my ky jb mu b gy mz na l nb nc">print(sess.run(W1))<br/>print('----------')<br/>print(sess.run(b1))<br/>print('----------')</span><span id="221f" class="my ky jb mu b gy nd na l nb nc">-&gt;</span><span id="cc1e" class="my ky jb mu b gy nd na l nb nc">[[-0.85421133  1.70487809  0.481848   -0.40843448 -0.02236851]<br/> [-0.47163373  0.34260952 -2.06743765 -1.43854153 -0.14699034]<br/> [-1.06858993 -1.10739779  0.52600187  0.24079895 -0.46390489]<br/> [ 0.84426647  0.16476244 -0.72731972 -0.31994426 -0.33553854]<br/> [ 0.21508843 -1.21030915 -0.13006891 -0.24056002 -0.30445012]<br/> [ 0.17842589  2.08979321 -0.34172744 -1.8842833  -1.14538431]<br/> [ 1.61166084 -1.17404735 -0.26805425  0.74437028 -0.81183684]]<br/>----------</span><span id="6fe0" class="my ky jb mu b gy nd na l nb nc">[ 0.57727528 -0.83760375  0.19156453 -0.42394346  1.45631313]</span><span id="7399" class="my ky jb mu b gy nd na l nb nc">----------</span></pre><h1 id="1e61" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">为什么是一个热点载体？</h1><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/c4a9eb2ce090d834f5c35a647dcba260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*neaOXEbp6h6kgOKVsMwLhw.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">again from Chris McCormick’s article (do read it)</figcaption></figure><p id="ec62" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当我们将one hot vectors乘以<code class="fe ne nf ng mu b">W1</code>时，我们基本上可以访问<code class="fe ne nf ng mu b">W1</code>的行，它实际上是由输入one hot vector表示的单词的嵌入式表示。所以<code class="fe ne nf ng mu b">W1</code>本质上是作为一个查找表。</p><p id="ccfc" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我们的例子中，我们还包括了一个偏见术语<code class="fe ne nf ng mu b">b1</code>，所以你必须添加它。</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="ef09" class="my ky jb mu b gy mz na l nb nc">vectors = sess.run(W1 + b1)</span><span id="9a57" class="my ky jb mu b gy nd na l nb nc"><br/># if you work it out, you will see that it has the same effect as running the node hidden representation</span><span id="b9b3" class="my ky jb mu b gy nd na l nb nc">print(vectors)<br/>-&gt;<br/>[[-0.74829113 -0.48964909  0.54267412  2.34831429 -2.03110814]<br/> [-0.92472583 -1.50792813 -1.61014366 -0.88273793 -2.12359881]<br/> [-0.69424796 -1.67628145  3.07313657 -1.14802659 -1.2207377 ]<br/> [-1.7077738  -0.60641652  2.25586247  1.34536338 -0.83848488]<br/> [-0.10080346 -0.90931684  2.8825531  -0.58769202 -1.19922316]<br/> [ 1.49428082 -2.55578995  2.01545811  0.31536022  1.52662396]<br/> [-1.02735448  0.72176981 -0.03772151 -0.60208392  1.53156447]]</span></pre><p id="3422" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果我们想要“女王”的代表，我们要做的就是:</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="006b" class="my ky jb mu b gy mz na l nb nc">print(vectors[ word2int['queen'] ])</span><span id="01db" class="my ky jb mu b gy nd na l nb nc"># say here word2int['queen'] is 2</span><span id="e6ec" class="my ky jb mu b gy nd na l nb nc">-&gt; <br/>[-0.69424796 -1.67628145  3.07313657 -1.14802659 -1.2207377 ]</span></pre><h1 id="90fb" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">那么我们能用这些美丽的载体做什么呢？</h1><p id="90a7" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">这里有一个快速函数，可以找到与给定向量最近的向量。请注意，这是一个肮脏的实现。</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="d154" class="my ky jb mu b gy mz na l nb nc">def euclidean_dist(vec1, vec2):<br/>    return np.sqrt(np.sum((vec1-vec2)**2))<br/></span><span id="9942" class="my ky jb mu b gy nd na l nb nc">def find_closest(word_index, vectors):<br/>    min_dist = 10000 # to act like positive infinity<br/>    min_index = -1</span><span id="df7c" class="my ky jb mu b gy nd na l nb nc">    query_vector = vectors[word_index]</span><span id="2e90" class="my ky jb mu b gy nd na l nb nc">    for index, vector in enumerate(vectors):</span><span id="d5f0" class="my ky jb mu b gy nd na l nb nc">        if euclidean_dist(vector, query_vector) &lt; min_dist and not np.array_equal(vector, query_vector):</span><span id="425e" class="my ky jb mu b gy nd na l nb nc">            min_dist = euclidean_dist(vector, query_vector)<br/>            min_index = index</span><span id="9400" class="my ky jb mu b gy nd na l nb nc">    return min_index</span></pre><p id="0a80" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们现在将使用“king”、“queen”和“royal”查询这些向量</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="3d5e" class="my ky jb mu b gy mz na l nb nc">print(int2word[find_closest(word2int['king'], vectors)])<br/>print(int2word[find_closest(word2int['queen'], vectors)])<br/>print(int2word[find_closest(word2int['royal'], vectors)])</span><span id="4914" class="my ky jb mu b gy nd na l nb nc">-&gt;</span><span id="2bb8" class="my ky jb mu b gy nd na l nb nc">queen<br/>king<br/>he</span></pre><p id="a99f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有趣的是，我们的嵌入了解到</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="99d7" class="my ky jb mu b gy mz na l nb nc">king is closest to queen<br/>queen is closest to king<br/>royal is closest to he</span></pre><p id="5853" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第三个是由于我们的语料库(还是蛮不错的)。更大的语料库将导致更好的结果。(<em class="kw">注意:由于权重的随机初始化，您可能会得到不同的结果。如果需要，运行几次</em></p><p id="5bf3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们画出它们的向量！</p><p id="0ef2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，让我们用我们最喜欢的降维技术将维数从5减少到2:tSNE(tee SNE！)</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="94c4" class="my ky jb mu b gy mz na l nb nc">from sklearn.manifold import TSNE</span><span id="476c" class="my ky jb mu b gy nd na l nb nc">model = TSNE(n_components=2, random_state=0)<br/>np.set_printoptions(suppress=True)<br/>vectors = model.fit_transform(vectors)</span></pre><p id="7d72" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，我们需要对结果进行规范化，以便在matplotlib中更方便地查看它们</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="99ad" class="my ky jb mu b gy mz na l nb nc">from sklearn import preprocessing</span><span id="c141" class="my ky jb mu b gy nd na l nb nc">normalizer = preprocessing.Normalizer()<br/>vectors =  normalizer.fit_transform(vectors, 'l2')</span></pre><p id="8790" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，我们将绘制2D归一化向量</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="5812" class="my ky jb mu b gy mz na l nb nc">import matplotlib.pyplot as plt</span><span id="4ae3" class="my ky jb mu b gy nd na l nb nc">fig, ax = plt.subplots()</span><span id="02bb" class="my ky jb mu b gy nd na l nb nc">for word in words:<br/>    print(word, vectors[word2int[word]][1])<br/>    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))</span><span id="a904" class="my ky jb mu b gy nd na l nb nc">plt.show()</span></pre><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi no"><img src="../Images/eb1f1e80c248e1beffa9c1b67e873678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*AGYlE8FyiM2pjbKlnl0zug.png"/></div></figure><p id="d483" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">哇！<code class="fe ne nf ng mu b">she</code>离<code class="fe ne nf ng mu b">queen</code>近而<code class="fe ne nf ng mu b">king</code>离<code class="fe ne nf ng mu b">royal</code>和<code class="fe ne nf ng mu b">queen</code>等距离我们需要一个更大的语料库来看一些更复杂的关系。</p><p id="69b2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw">注意:在发表这篇文章后，我意识到这个例子是不正确的，因为要得到有意义的向量收敛，我们需要一个非常大的语料库。数据量小，容易受到突然的“冲击”。然而，出于教学目的，我将保持这种写作的完整性。为了有效地实现word2vec，请尝试使用一些类似于</em><a class="ae mo" href="https://gist.github.com/robotcator/1fb0cdc1437515f5662d33368554f4c8" rel="noopener ugc nofollow" target="_blank"><em class="kw">text 8</em></a><em class="kw">的语料库</em> <a class="ae mo" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"> <em class="kw"> gensim </em> </a> <em class="kw">。</em></p><h1 id="f22b" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">为什么会这样？</h1><p id="ff80" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们给了神经网络预测邻居的任务。但是我们还没有具体说明网络应该如何预测它。因此，神经网络计算出单词的隐藏表示形式，以帮助它完成预测相邻单词的任务。预测相邻单词本身并不是一项有趣的任务。我们关心这个隐藏的表示。</p><p id="b87e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了形成这些表示，网络使用上下文/邻居。在我们的语料库中，<code class="fe ne nf ng mu b">king</code>和<code class="fe ne nf ng mu b">royal</code>作为邻居出现，<code class="fe ne nf ng mu b">royal</code>和<code class="fe ne nf ng mu b">queen</code>作为邻居出现。</p><h1 id="4465" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">为什么预测邻居是一项任务？</h1><p id="82d1" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">嗯，其他任务也设法形成一个好的表示。预测这个单词是否是一个有效的n-gram，如这里的<a class="ae mo" href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/" rel="noopener ugc nofollow" target="_blank">所示</a>也可以得到好的向量！</p><p id="934d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们试图预测给定单词的相邻单词。这就是所谓的跳过克模型。我们可以使用中间词的相邻词作为输入，并要求网络预测中间词。这就是所谓的连续词汇袋模型。</p><h1 id="d69e" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">延伸阅读:</h1><p id="37ce" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">这绝不是对word2vec的完整理解。w2v的部分魅力在于它对我刚刚谈到的内容做了两处修改。这些是:</p><ul class=""><li id="3b99" class="ma mb jb ka b kb kc kf kg kj mc kn md kr me kv np mg mh mi bi translated">负采样</li><li id="67cc" class="ma mb jb ka b kb mj kf mk kj ml kn mm kr mn kv np mg mh mi bi translated">分级Softmax</li></ul><p id="4fd7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">负采样:</strong>它表明，不是反向传播正确输出向量中的所有0(对于10密耳的vocab大小，有10密耳减去1个零)，而是我们只反向传播它们中的一些(比如14个)</p><p id="4011" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">分级Softmax: </strong>计算10mill的vocab的Softmax非常耗时且计算量大。分级Softmax提出了一种使用霍夫曼树的更快的计算方法</p><p id="ee8f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了保持这篇文章的简洁，我避免了过多的探究。我绝对推荐多读点进去。</p><h1 id="71b3" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">结束语:</h1><ul class=""><li id="a982" class="ma mb jb ka b kb lv kf lw kj nq kn nr kr ns kv np mg mh mi bi translated">单词向量超级酷</li><li id="47bd" class="ma mb jb ka b kb mj kf mk kj ml kn mm kr mn kv np mg mh mi bi translated">不要把这个tensorflow代码用于实际使用。只是为了理解。使用像<a class="ae mo" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"> gensim </a>这样的库</li></ul><p id="5c52" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我希望这能帮助人们更好地理解这些美景。如果是的话，让我知道！</p><p id="f233" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果我犯了错误，<strong class="ka jc">请</strong>告诉我。</p><p id="1c98" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我很乐意通过<a class="ae mo" href="https://twitter.com/aneesh_joshi" rel="noopener ugc nofollow" target="_blank">推特</a>、<a class="ae mo" href="https://www.linkedin.com/in/aneesh-joshi-594602135/" rel="noopener ugc nofollow" target="_blank"> linkedin </a>或/和<a class="ae mo" href="mailto:aneeshyjoshi@gmail.com" rel="noopener ugc nofollow" target="_blank">电子邮件</a>与你联系。</p><p id="96df" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">再见！</p></div></div>    
</body>
</html>