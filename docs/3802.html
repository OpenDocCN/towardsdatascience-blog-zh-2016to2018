<html>
<head>
<title>Unfair Back Propagation with Tensorflow [ Manual Back Propagation with TF ]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Tensorflow 的不公平反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unfair-back-propagation-with-tensorflow-manual-back-propagation-with-tf-c166115988fd?source=collection_archive---------9-----------------------#2018-06-19">https://towardsdatascience.com/unfair-back-propagation-with-tensorflow-manual-back-propagation-with-tf-c166115988fd?source=collection_archive---------9-----------------------#2018-06-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/9460df6053249ca03b72600668578229.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*AXQzoZmFhagHax9hvBrrfQ.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/not-fair-picket-ymwKHQ44WnAoU" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="ecd7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我一直在思考<a class="ae jy" href="https://brilliant.org/wiki/backpropagation/" rel="noopener ugc nofollow" target="_blank">反向传播</a>，在传统的神经网络中，我们似乎总是线性地执行前馈操作和反向传播。(1:1 的比例)但是我对自己说，我们真的不需要这么做。所以我想做些实验。</p><p id="052c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="kx">案例 a)反向传播(无数据增强)<br/>案例 b)反向传播(数据增强)<br/>案例 c)不公平的后支撑(从后面)(无数据增强)<br/>案例 d)不公平的后支撑(从后面)(数据增强)<br/>案例 e)不公平的后支撑(从前面)(无数据增强)<br/>案例 f)不公平的后支撑(从前面)(数据增强)</em></p><blockquote class="ky kz la"><p id="a1be" class="jz ka kx kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated"><strong class="kb ir">请注意，这个帖子是为了好玩，也是为了表达我的创意。因此不是面向性能的。</strong></p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="1f8c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">网络架构/数据集/典型反向传播</strong></p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ll"><img src="../Images/b2e0079f62ec9583876622c66d8566b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o1B-xTudq0DkelCVEFpcWA.png"/></div></div></figure><p id="5761" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红色方块</strong> →输入图像批次<br/> <strong class="kb ir">黑色方块</strong> →有/无均值合并的卷积层<br/> <strong class="kb ir">橙色方块</strong> →用于分类的 Softmax 层<br/> <strong class="kb ir">橙色箭头</strong> →前馈操作方向<br/> <strong class="kb ir">紫色箭头</strong> →反向传播方向。</p><p id="ff3e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，我们将使用我的旧帖子“<a class="ae jy" rel="noopener" target="_blank" href="/iclr-2015-striving-for-simplicity-the-all-convolutional-net-with-interactive-code-manual-b4976e206760">全卷积网</a>”中的基本网络，这是一个仅由卷积运算组成的 9 层网络。此外，我们将使用<a class="ae jy" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR 10 数据集</a>。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="061e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">不公平反向传播</strong></p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lu"><img src="../Images/77b0a55d5e1020b3e868412c283bb4db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YG5v1yb5DPflfIHDu588rg.png"/></div></div></figure><p id="b602" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在为了解释不公平反向传播的概念，让我们首先假设我们刚刚完成了前馈操作(如上所示)。通过查看粉色箭头，我们已经可以知道我们已经反向传播到第 9 层。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lv"><img src="../Images/f9d1f6bb2fd7f8bf6c8c7410c5f4690c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V5Z-F7NzS5vFdvu0v-UhyQ.png"/></div></div></figure><p id="ce01" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在，我们可以再次执行前馈操作来获得另一轮分类，而不是继续反向传播到第八层等等。(但是这次第 9 层已经更新了一次它的权重。)</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lw"><img src="../Images/2d99381dd972511f35d6ffe099bec3b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UO6i_501NT1X-kj_ka2GWg.png"/></div></div></figure><p id="2c3d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们可以再次重复这个过程，但是这次我们也要更新第 8 层的权重。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lx"><img src="../Images/3321a5c30b4e4ba82ecde97dad1b7306.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tJz8Wm8cPLzLc5G5ExSwQQ.png"/></div></div></figure><p id="62b6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们可以一次又一次地遵循这个概念。直到我们到达第一层，完成整个反向传播。总之，我们对网络的开始层不公平，因为后面的部分会更新更多。我们也可以在 GIF 格式中看到不同。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ly"><img src="../Images/e02bb4bfd371fd15d049885bf860d6f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*k_-xZU3gjGPzHSiRHzt7Qg.gif"/></div></div></figure><p id="feea" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，我们可以把对网络开始部分不公平的概念反过来，对网络的后面部分不公平。(换句话说，我们网络的开始部分比后面部分更新得更多。)</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lz"><img src="../Images/b31bfa1a0877ae456a0caf49a62090ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IdhUm551EquuFJJDuiKbsA.png"/></div></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="b985" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:情况 a)反向传播(无数据增加)(50 个时期)</strong></p><div class="lm ln lo lp gt ab cb"><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/4a0fc4fff3c422766fbced922cb80d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*hCCa85gXKAzLTTc0TcztYQ.png"/></div></figure><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/82ef6633e088fd9bad85951a823f8356.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*axKvgwKIfMOv6_8n837ldw.png"/></div></figure></div><p id="4b57" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →一段时间内测试图像的精度/成本<br/>T5】右图 →一段时间内列车图像的精度/成本</p><p id="bfcc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，网络测试图像的准确率已经开始停滞在 83%左右。然而，这里要注意的是，网络需要 13 个历元才能使测试图像的准确率达到 80%以上。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mg"><img src="../Images/38e78e5d095f539e66bf353385fdd5b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVCzoHNTvp52cBGh9oHKcA.png"/></div></div></figure><p id="d16f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">训练图像的最终准确率为 98 %,而测试图像的准确率为 83 %,这表明网络正遭受过拟合。</p><div class="lm ln lo lp gt ab cb"><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/ec7fd3bc362377abbd0432ec91e14c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*mhz89m04Ks5wLpOqgKR2jg.png"/></div></figure><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/ea0a9a17a02236ffc25afac88c2bdc8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*emjqvKq3Is89slwF159_hw.png"/></div></figure></div><p id="104c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →训练后权重直方图<br/> <strong class="kb ir">右图</strong> →训练前权重直方图</p><p id="01dd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所示，所有的权重范围都从-0.1/0.1 增加到-0.5/0.5。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="f90e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:情况 b)反向传播(数据扩充)(10 个时期)</strong></p><div class="lm ln lo lp gt ab cb"><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/83d4a3a60fca9353fff6685370dc8743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*tzXiKUOjtpAXN2iNMkPwOA.png"/></div></figure><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/2184da919a742f7a4fdffc77f82b6e1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*W-xq_3Pq7HEXDyMHZgZaMw.png"/></div></figure></div><p id="f98e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左侧图像</strong> →一段时间内测试图像的精度/成本<br/> <strong class="kb ir">右侧图像</strong> →一段时间内训练图像的精度/成本</p><p id="d5da" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在，由于我们执行了数据扩充，我们可以清楚地观察到训练图像的准确性下降(因为数据中有更多的差异。).</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mh"><img src="../Images/1618a875a7042f0e97ce21b07879e71e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k3PQ7y6_NCbsmg-cIa01rw.png"/></div></div></figure><p id="877c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所示，在训练 10 个时期后，网络最终达到了 79%的准确率(在测试图像上)。值得庆幸的是，网络并没有过度拟合。</p><div class="lm ln lo lp gt ab cb"><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/9f7f6b7385019ff01a04d931e66b2010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*6S2NlaWy6Bs844uVXVPgww.png"/></div></figure><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/ea0a9a17a02236ffc25afac88c2bdc8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*emjqvKq3Is89slwF159_hw.png"/></div></figure></div><p id="80b8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →训练后权重直方图<br/>T22】右图 →训练前权重直方图</p><p id="2ea0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">就像我们没有执行数据扩充的情况一样，权重的范围也增加了。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="86d0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:案例 c)不公平的后道具(从后面) (无数据增强)(10 个历元)</strong></p><div class="lm ln lo lp gt ab cb"><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/75fa2fcfdff914b72907377b9370fce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*lBR8T5NaoVBlt0C1FbCVwA.png"/></div></figure><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/0c09893e595b3d9b1aeab590342ef287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*lfSk6bCL9SR_sF2AmGde4A.png"/></div></figure></div><p id="3c1e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左侧图像</strong> →一段时间内测试图像的精度/成本<br/> <strong class="kb ir">右侧图像</strong> →一段时间内列车图像的精度/成本</p><p id="d50b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">与用普通(典型)反向传播方法训练的网络相比，该网络表现得更好。在测试图像上完成训练的准确率约为 82 %,在训练图像上完成训练的准确率约为 89%。表明仍然存在过度拟合，但是与情况 a)相比没有那么多。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mi"><img src="../Images/7d063ba36310b55f15468f6935510a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dhFBKkvMoZrUyB37YYkDSQ.png"/></div></div></figure><p id="cb2f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">观察到不公平反向传播似乎具有某种正则化效应的事实是相当令人惊讶的。</p><div class="lm ln lo lp gt ab cb"><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/f57cc490aa627c866ffe2d6108a32301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*YoIHahSa_uApk61Xs7qr7Q.png"/></div></figure><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/ea0a9a17a02236ffc25afac88c2bdc8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*emjqvKq3Is89slwF159_hw.png"/></div></figure></div><p id="a47a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →训练后权重直方图<br/> <strong class="kb ir">右图</strong> →训练前权重直方图</p><p id="e07b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">到目前为止，最有趣的观察是权重的直方图。如上所述，当我们将注意力集中到网络最终层生成的直方图时，我们可以观察到直方图偏向一侧。(-左侧范围为-0.5，而右侧没有 0.5。)表明分布的对称性有些受阻。中间层的直方图也是如此。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="87ac" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:案例 d)不公平背道具(从后面) (数据增强)(10 Epoch) </strong></p><div class="lm ln lo lp gt ab cb"><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/614c41aae05dfd3e51a35e19310c77ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*CnaK0oX2g8PTGoYBr5QJkQ.png"/></div></figure><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/f729919b0d5a9e2c45adaf16b71de365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*dMnM8QhYF70YAyEbF_dMCA.png"/></div></figure></div><p id="cbfa" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →一段时间内测试图像的精度/成本<br/>T5】右图 →一段时间内列车图像的精度/成本</p><p id="f919" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同样，当我们执行数据扩充时，我们可以观察到训练图像的准确性下降。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mh"><img src="../Images/243d3018737618136f119ce2f59e2f57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GyPJMuq3zIdqfmld86GoKg.png"/></div></div></figure><p id="c7a5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">并且当比较来自情况 b)的结果(10 个历元的正常反向支持)时，我们可以观察到训练图像上的准确度彼此相似(49%)的事实。然而，不管是什么原因，由不公平反向支持训练的网络在测试图像上具有更高的准确性。</p><div class="lm ln lo lp gt ab cb"><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/5c0bde032b4cace8cdde5e140f3cd586.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*1ESsDmkuvM_h1qAklzYuHA.png"/></div></figure><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/ea0a9a17a02236ffc25afac88c2bdc8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*emjqvKq3Is89slwF159_hw.png"/></div></figure></div><p id="8872" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →训练后权重直方图<br/> <strong class="kb ir">右图</strong> →训练前权重直方图</p><p id="8942" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">与情况 c)不同，我们可以观察到这种情况下权重分布的一些对称性。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="c730" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:情况 e)不公平的后道具(来自前方) (无数据增强)(10 个历元)</strong></p><div class="lm ln lo lp gt ab cb"><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/3d979c75cf6564820fafec64a5a4dcae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*20n0YiOq4Fm2m_Hzy8y0fA.png"/></div></figure><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/e05ad42e89787fa78d55eaed23db8771.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*5HnPEbtly6-QduatPXPVEw.png"/></div></figure></div><p id="05ca" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左侧图像</strong> →一段时间内测试图像的精度/成本<br/> <strong class="kb ir">右侧图像</strong> →一段时间内训练图像的精度/成本</p><p id="5f5e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，我想看看，当我们对网络的后半部分不公平时，是否有区别。从表面上看，对我们网络的开始部分不公平比后面部分不公平要好。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mj"><img src="../Images/02b5aec65b078d957807893c2d0be8c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zpBlS-x5sud52bOyONfq0Q.png"/></div></div></figure><p id="a5ea" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我这么说的一个原因是因为测试图像的最终准确性。我们可以观察到在训练图像上的准确性是相似的，但是该网络在测试图像上具有较低的准确性。</p><div class="lm ln lo lp gt ab cb"><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/d51cec54068fc0952fd7fca91e7cbe8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*FpKBQNrKrfhb1qB0nOmShg.png"/></div></figure><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/ea0a9a17a02236ffc25afac88c2bdc8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*emjqvKq3Is89slwF159_hw.png"/></div></figure></div><p id="d44e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →训练后权重直方图<br/>T22】右图 →训练前权重直方图</p><p id="d293" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">有趣的是，不管我们对网络的开始部分还是后面部分不公平，权重的分布似乎是相似的。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="66a5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:案例 f)不公平的后道具(来自正面) (数据增强)(10 个历元)</strong></p><div class="lm ln lo lp gt ab cb"><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/4c7c020ad43d35e036fceb38dd9884f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*lgAfKmPmWVBqDxchGB4BEA.png"/></div></figure><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/aa1d5e2e2a68dda32ce2263f4aecfeb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Is1H9C8hUS2oMgAJuTsdpw.png"/></div></figure></div><p id="b6cb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左侧图像</strong> →一段时间内测试图像的精度/成本<br/> <strong class="kb ir">右侧图像</strong> →一段时间内列车图像的精度/成本</p><p id="2506" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，我在训练来自案例 e 的相同网络时执行了数据扩充。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mk"><img src="../Images/bb1c3ae705646d6d51d47216a03cad7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EQA_xjGnkYaA87aLn8x0eg.png"/></div></div></figure><p id="9960" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，这里也可以观察到精度没有提高。</p><div class="lm ln lo lp gt ab cb"><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/83c4974b7512c45b8574d0afaef5381c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*vBKA1xQ3BNgpIGHYEfd6nw.png"/></div></figure><figure class="ma jr mb mc md me mf paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><img src="../Images/ea0a9a17a02236ffc25afac88c2bdc8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*emjqvKq3Is89slwF159_hw.png"/></div></figure></div><p id="d16d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →训练后权重直方图<br/> <strong class="kb ir">右图</strong> →训练前权重直方图</p><p id="5482" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">重量分布也有类似的结果。在普通的 CNN 中，我们知道后面的层捕捉更高级的特征。我怀疑网络更努力训练比较好？捕捉更高层次的特征。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="dbeb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">交互代码</strong></p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ml"><img src="../Images/e375f065d6c63e4d0d13648c531c0293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YS6h4vLuWOYpgsSUfa0Wbg.png"/></div></div></figure><p id="36b1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于 Google Colab，你需要一个 Google 帐户来查看代码，而且你不能在 Google Colab 中运行只读脚本，所以在你的操场上复制一份。最后，我永远不会请求允许访问你在 Google Drive 上的文件，仅供参考。编码快乐！同样为了透明，我在 github 上上传了所有的训练日志。</p><p id="114a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/16F_6dXCNCmO-e5IfGIGq2mneE7amihb_" rel="noopener ugc nofollow" target="_blank"> a 的代码，请点击此处</a>，要查看<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/Unfair/a_case(no%20data)/casea.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处。</a> <br/>访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1SVbAtyignjfUyTlCH9u8sdjIY8or-ShH" rel="noopener ugc nofollow" target="_blank"> b 的代码请点击此处，<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/Unfair/a_case(aug)/a_case(aug).txt" rel="noopener ugc nofollow" target="_blank">日志的</a>请点击此处。</a> <br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1Nrbal3TDr6NxUcyq7YJh_KZxpDhB1coU" rel="noopener ugc nofollow" target="_blank"> c 的代码，请点击此处</a>，要查看<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/Unfair/d_case(no%20data)/cased.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处。</a> <br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1_9pVlKNUFmmBx-Y1g91TKRtaH4JTMIYw" rel="noopener ugc nofollow" target="_blank"> d 的代码，请点击此处</a>，要查看<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/Unfair/d_case(aug)/d_case(aug).txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处。</a> <br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1Glk8SerYf-qfBUMQHJMEDLcOqj-6UxLy" rel="noopener ugc nofollow" target="_blank"> e 的代码，请点击此处</a>，要查看<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/Unfair/e_case(no%20data)/casee.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处。</a> <br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1QB51lGy22STowW_AeamzKnaXeW-E-xmG" rel="noopener ugc nofollow" target="_blank"> f 的代码，请点击此处</a>，要查看<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/Unfair/e_case(aug)/e_case(aug).txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处</a>。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="ddee" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="e0b6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">关于我为什么发这篇博文，我其实有更深层次的原因，很快就会揭晓。这种不成比例的反向传播的想法真的让我很感兴趣。</p><p id="eedc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你希望看到我所有写作的列表，请<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">在这里查看我的网站</a>。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的 twitter 上关注我<a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文</a> t。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="11d1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="3932" class="mm mn iq kb b kc kd kg kh kk mo ko mp ks mq kw mr ms mt mu bi translated">tf.cond |张量流。(2018).张量流。检索于 2018 年 6 月 17 日，来自<a class="ae jy" href="https://www.tensorflow.org/api_docs/python/tf/cond" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/cond</a></li><li id="3753" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">TensorFlow？，H. (2018)。如何在 TensorFlow 中打印一个张量对象的值？。堆栈溢出。检索于 2018 年 6 月 17 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/33633370/how-to-print-the-value-of-a-tensor-object-in-tensorflow" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/33633370/how-to-print-the-value-of-a-tensor-object-in-tensor flow</a></li><li id="a289" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">数学|张量流。(2018).张量流。检索于 2018 年 6 月 17 日，来自<a class="ae jy" href="https://www.tensorflow.org/api_guides/python/math_ops" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_guides/python/math_ops</a></li><li id="975c" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">tf.floormod | TensorFlow。(2018).张量流。检索于 2018 年 6 月 17 日，来自<a class="ae jy" href="https://www.tensorflow.org/api_docs/python/tf/floormod" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/floormod</a></li><li id="b956" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">tf.cond |张量流。(2018).张量流。检索于 2018 年 6 月 17 日，来自<a class="ae jy" href="https://www.tensorflow.org/api_docs/python/tf/cond" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/cond</a></li><li id="4dca" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">使用 tf。TensorFlow 中的 print()—走向数据科学。(2018).走向数据科学。检索于 2018 年 6 月 17 日，来自<a class="ae jy" rel="noopener" target="_blank" href="/using-tf-print-in-tensorflow-aa26e1cff11e">https://towards data science . com/using-TF-print-in-tensor flow-aa 26 E1 cf F11 e</a></li><li id="92aa" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">Tensorflow？，H. (2018)。如何在 Tensorflow 中给 tf.cond 内部的函数传递参数？。堆栈溢出。检索于 2018 年 6 月 17 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/38697045/how-to-pass-parmeters-to-functions-inside-tf-cond-in-tensorflow" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/38697045/how-to-pass-parameters-to-functions-inside-TF-cond-in-tensor flow</a></li><li id="dc99" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">TensorFlow？，H. (2018)。如何在 TensorFlow 中打印一个张量对象的值？。堆栈溢出。检索于 2018 年 6 月 17 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/33633370/how-to-print-the-value-of-a-tensor-object-in-tensorflow" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/33633370/how-to-print-the-value-of-a-tensor-object-in-tensor flow</a></li><li id="6e83" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">规范化的方向保持亚当，从亚当到 SGD 的转换，和内斯特罗夫动量亚当与…(2018).走向数据科学。检索于 2018 年 6 月 17 日，来自<a class="ae jy" rel="noopener" target="_blank" href="/normalized-direction-preserving-adam-switching-from-adam-to-sgd-and-nesterov-momentum-adam-with-460be5ddf686">https://towards data science . com/normalized-direction-preserving-Adam-switching-from-Adam-to-SGD-and-nesterov-momentum-Adam-with-460 be 5 ddf 686</a></li><li id="81b9" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">谷歌合作实验室。(2018).Colab.research.google.com。检索于 2018 年 6 月 17 日，来自<a class="ae jy" href="https://colab.research.google.com/drive/1Okr4jfqBMoQ8q4ctZdJMp8jqeA4XDQbK" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/drive/1 okr 4 jfqbmoq 8 q 4 ctzdjmp 8 jqea 4 xdqbk</a></li><li id="11ab" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">反向传播|杰出的数学和科学维基。(2018).Brilliant.org。检索于 2018 年 6 月 17 日，来自<a class="ae jy" href="https://brilliant.org/wiki/backpropagation/" rel="noopener ugc nofollow" target="_blank">https://brilliant.org/wiki/backpropagation/</a></li><li id="645c" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">tf.while_loop，I. (2018)。tf.while_loop 中是否可以定义多个条件。堆栈溢出。检索于 2018 年 6 月 17 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/45595419/is-it-possible-to-have-multiple-conditions-defined-in-tf-while-loop" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/45595419/is-possible-have-multiple-conditions-defined-in-TF-while-loop</a></li><li id="ad86" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">Python，E. (2018)。Python 中每 N 次迭代执行一次语句。堆栈溢出。检索于 2018 年 6 月 17 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/5628055/execute-statement-every-n-iterations-in-python" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/5628055/execute-statement-every-n-iterations-in-python</a></li><li id="4cb1" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">示例:基础— imgaug 0.2.5 文档。(2018).img aug . readthe docs . io . 2018 年 6 月 18 日检索，来自<a class="ae jy" href="http://imgaug.readthedocs.io/en/latest/source/examples_basics.html" rel="noopener ugc nofollow" target="_blank">http://img aug . readthe docs . io/en/latest/source/examples _ basics . html</a></li><li id="5f46" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">CIFAR-10 和 CIFAR-100 数据集。(2018).Cs.toronto.edu。检索于 2018 年 6 月 19 日，来自<a class="ae jy" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">https://www.cs.toronto.edu/~kriz/cifar.html</a></li><li id="a051" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">[ ICLR 2015 ]追求简单:具有交互码的全卷积网。(2018).走向数据科学。检索于 2018 年 6 月 19 日，来自<a class="ae jy" rel="noopener" target="_blank" href="/iclr-2015-striving-for-simplicity-the-all-convolutional-net-with-interactive-code-manual-b4976e206760">https://towards data science . com/iclr-2015-forwards-for-simplicity-the-all-convolutional-net-with-interactive-code-manual-b 4976 e 206760</a></li></ol></div></div>    
</body>
</html>