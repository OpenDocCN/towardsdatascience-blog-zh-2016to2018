<html>
<head>
<title>10 Stochastic Gradient Descent Optimisation Algorithms + Cheatsheet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">10 种随机梯度下降优化算法+备忘单</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/10-gradient-descent-optimisation-algorithms-86989510b5e9?source=collection_archive---------3-----------------------#2018-11-22">https://towardsdatascience.com/10-gradient-descent-optimisation-algorithms-86989510b5e9?source=collection_archive---------3-----------------------#2018-11-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/74bede0215709387d701e308d0dae017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GG4hzBs60_5QHZjvTS316Q.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/photos/y8iR4t4MTF8?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Julian Zett</a> on <a class="ae jg" href="https://unsplash.com/search/photos/mountain?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="649f" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">深度学习你应该知道的随机梯度下降优化算法</h2></div><p id="7674" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">(我在我的博客</em> <a class="ae jg" href="https://remykarem.github.io/blog/gradient-descent-optimisers.html" rel="noopener ugc nofollow" target="_blank"> <em class="lu">这里</em> </a> <em class="lu">里维护了一张包括 RAdam 在内的这些优化者的小抄。)</em></p><p id="a1e4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">变更日志:【2022 年 1 月 5 日—2020 年 5 月 4 日<br/>修正附录 2 中那达慕公式的错别字<br/>2020 年 3 月 21 日—分别用</em> m <em class="lu">和</em> v <em class="lu">替换</em> V <em class="lu">和</em> S <em class="lu">，更新死链接，回顾学习率和梯度组件的概念，更新直觉<br/>2019 年 10 月 6 日—改进</em></p><p id="0c71" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lv translated">梯度下降是一种寻找函数最小值的优化方法。它通常用于深度学习模型中，通过反向传播来更新神经网络的权重。</p><p id="b083" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，我将总结流行的深度学习框架(如 TensorFlow，Keras，PyTorch)中使用的常见梯度下降优化算法。这篇文章的目的是使使用一致的命名法阅读和消化公式变得容易，因为没有很多这样的总结。在这篇文章的末尾有一张小抄供你参考。</p><p id="e5af" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本帖假设读者对<a class="ae jg" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a> / <a class="ae jg" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a>有所了解。</p><p id="780c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(有关使用梯度下降优化器(如 SGD、momentum 和 Adam)的线性回归问题的演示，请单击此处的<a class="ae jg" href="https://remykarem.github.io/backpropagation-demo/" rel="noopener ugc nofollow" target="_blank"/>。)</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="ff91" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">随机梯度下降优化器是做什么的？</h2><p id="68f0" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">回想一下，标准随机梯度下降(SGD)通过用其梯度的因子(即<em class="lu"> α </em>，学习率)减去当前权重来更新权重。</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/a4c288704cca63508d8299cbabdd6de7.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*Bjh6q72YAx9RD-z4i3h3pA@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Eqn. 1: The terms in stochastic gradient descent</figcaption></figure><p id="3b7c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个方程的变体通常被称为随机梯度下降优化器。它们的区别主要有三个方面:</p><ol class=""><li id="95b3" class="no np jj la b lb lc le lf lh nq ll nr lp ns lt nt nu nv nw bi translated"><strong class="la jk">调整“梯度分量”(</strong><em class="lu">【∂l/∂w】</em><strong class="la jk">)</strong><br/>不是像在随机香草梯度下降中那样仅使用一个单一梯度来更新权重，而是采用多个梯度的<em class="lu">集合</em><em class="lu"/>。具体来说，这些优化器使用梯度的<a class="ae jg" href="https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average" rel="noopener ugc nofollow" target="_blank">指数移动平均值</a>。</li><li id="eae1" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated"><strong class="la jk">调整“学习率分量”(</strong><em class="lu">α</em><strong class="la jk">)</strong><em class="lu"><br/></em>，根据梯度的<em class="lu">大小</em>调整学习率，而不是保持恒定的学习率。</li><li id="41dc" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated"><strong class="la jk">(1)和(2) <br/> </strong>适应梯度分量和学习率分量。</li></ol><p id="d2b6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如您将在后面看到的，这些优化器试图改进用于更新权重的信息量，主要是通过使用先前(和未来)的梯度，而不仅仅是当前可用的梯度。</p><p id="0354" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下表总结了正在调整的“组件”:</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oc"><img src="../Images/ab910f61734fe8f196c1fc06cf0212cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0xqraYy8AMQ6OvjbC3WP8w@2x.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Fig. 2: Gradient descent optimisers, the year in which the papers were published, and the components they act upon</figcaption></figure><p id="0845" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><change log="" sep="" removed="" evolutionary="" map=""/></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="813a" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">内容</h2><ol class=""><li id="0326" class="no np jj la b lb ne le nf lh od ll oe lp of lt nt nu nv nw bi translated">随机梯度下降</li><li id="1f40" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">动力</li><li id="7d3d" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">阿达格拉德</li><li id="1923" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">RMSprop</li><li id="1598" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">阿达德尔塔</li><li id="1b2a" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">困扰</li><li id="3d71" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</li><li id="c368" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">阿达马克斯</li><li id="27c2" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">那达慕</li><li id="f2c7" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">阿姆斯格勒</li></ol><p id="40c9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">附录 1:备忘单</p><p id="1def" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">附录 2:直觉</p><p id="54bb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">附录 3:学习率调度器与随机梯度下降优化器</p><h2 id="81ab" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated"><strong class="ak">符号</strong></h2><ul class=""><li id="2d50" class="no np jj la b lb ne le nf lh od ll oe lp of lt og nu nv nw bi translated"><em class="lu"> t — </em>时间步长</li><li id="b529" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> w — </em>我们要更新的权重/参数，其中下标<em class="lu"> t </em>索引时间步长<em class="lu"> t </em>的权重。</li><li id="bb81" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> α — </em>学习率</li><li id="8114" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu">∂l/∂w—</em>l<em class="lu">的梯度</em>，最小化的损失函数，w.r.t .到<em class="lu"> w </em></li><li id="7427" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated">我还标准化了本文中使用的符号和希腊字母(因此可能与论文不同)，以便我们可以在滚动时探索乐观主义者是如何“进化”的。</li></ul></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="89a8" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">1.随机梯度下降</h2><p id="538f" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">正如我们前面看到的，标准 SGD 使用当前梯度<em class="lu"> ∂L/∂w </em>乘以某个称为学习率的因子<em class="lu"> α来更新当前权重。</em></p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/161ea5ac0ea38db0b43f640e43a5e02c.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*kvwgxqwUskRcuXUCr_GFZA@2x.png"/></div></figure><h2 id="e802" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">2.动力</h2><p id="74af" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">带动量的梯度下降(<a class="ae jg" href="https://www.researchgate.net/publication/243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods" rel="noopener ugc nofollow" target="_blank"> Polyak，1964 </a>)不是仅依赖于当前梯度来更新权重，而是用<em class="lu"> m </em>(“动量”)替换当前梯度，这是梯度的集合。该集合是当前和过去梯度的指数移动平均值(即，直到时间<em class="lu"> t </em>)。在这篇文章的后面，你会看到这个动量更新成为大多数优化者的渐变组件的标准更新。</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/2a43d4de324c6238a9d02649aee926d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*auq2VqAGyo3BJQQbaOcjDw@2x.png"/></div></figure><p id="7d64" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在哪里</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/eaf9832c3bb6a5ae2109441f4d18e002.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*QNfNDqVKASSLjZsmetgWHg@2x.png"/></div></figure><p id="32ca" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">并且<em class="lu"> m </em>被初始化为 0。</p><p id="74ec" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通用默认值:</p><ul class=""><li id="e29e" class="no np jj la b lb lc le lf lh nq ll nr lp ns lt og nu nv nw bi translated"><em class="lu"> β = </em> 0.9</li></ul><blockquote class="ok ol om"><p id="23ff" class="ky kz lu la b lb lc kk ld le lf kn lg on li lj lk oo lm ln lo op lq lr ls lt im bi translated"><strong class="la jk"> <em class="jj">论动量的起源</em> </strong> <em class="jj"> <br/>注意，许多文章引用动量法来发表由</em> <a class="ae jg" href="https://www.semanticscholar.org/paper/On-the-momentum-term-in-gradient-descent-learning-Qian/735d4220d5579cc6afe956d9f6ea501a96ae99e2" rel="noopener ugc nofollow" target="_blank"> <em class="jj">、1999 年</em> </a> <em class="jj">。然而，名为</em> <a class="ae jg" href="http://proceedings.mlr.press/v28/sutskever13.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="jj"> Sutskever 等人</em> </a> <em class="jj">的论文将经典动量归因于 Polyak 在 1964 年发表的更早的出版物，如上所述。(感谢</em> <a class="ae jg" href="https://news.ycombinator.com/item?id=18525494#18528682" rel="noopener ugc nofollow" target="_blank"> <em class="jj">詹姆斯</em> </a> <em class="jj">指出这一点。)</em></p></blockquote><h2 id="1c7a" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">3.阿达格拉德</h2><p id="9eb2" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">自适应梯度或 AdaGrad ( <a class="ae jg" href="http://jmlr.org/papers/v12/duchi11a.html" rel="noopener ugc nofollow" target="_blank"> Duchi 等人，2011 </a>)通过将学习率除以<em class="lu"> v </em>的平方根来作用于学习率分量，该平方根是当前和过去平方梯度的累积和(即，直到时间<em class="lu"> t </em>)。请注意，与 SGD 中一样，梯度分量保持不变。</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/b91c6ae7bb40ed21f4ac065532f83c71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*TKM5MMoyEb7xQMGK65H7_Q@2x.png"/></div></figure><p id="d11b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在哪里</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/eb9995f830977e1638b88179788a3749.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*P3dSbZEqnqxPdRgz9D-M0A@2x.png"/></div></figure><p id="be16" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">并且<em class="lu"> v </em>被初始化为 0。</p><p id="b525" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意<em class="lu"> ε </em>被加到分母上。Keras 称之为<em class="lu">模糊因子</em>，一个小的浮点值，以确保我们永远不会遇到被零除的情况。</p><p id="484a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">默认值(来自<a class="ae jg" href="https://keras.io/optimizers/#adagrad" rel="noopener ugc nofollow" target="_blank"> Keras </a>):</p><ul class=""><li id="ad73" class="no np jj la b lb lc le lf lh nq ll nr lp ns lt og nu nv nw bi translated"><em class="lu"> α </em> = 0.01</li><li id="1b99" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> ε = </em> 10⁻⁷</li></ul><h2 id="ef85" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">4.RMSprop</h2><p id="9bbc" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">均方根 prop 或 RMSprop ( <a class="ae jg" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" rel="noopener ugc nofollow" target="_blank"> Hinton 等人，2012 </a>)是另一种试图提高 AdaGrad 的自适应学习速率。我们不像 AdaGrad 那样采用梯度平方的累积和，而是采用指数移动平均(再次！)的这些梯度。与 momentum 类似，我们将慢慢看到这个更新成为大多数优化器的学习率组件的标准更新。</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/0f13a56a9a9a601748ce807a69816e6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*fMEgNlvwMWH1Zt3sm5SYSw@2x.png"/></div></figure><p id="3ea0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在哪里</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/6f02701573b6b3c3589a804420dfd2a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*01Uje2lCzvkMf1kfN5IdPw@2x.png"/></div></figure><p id="9b73" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">并且<em class="lu"> v </em>初始化为 0。</p><p id="17e0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">默认值(来自<a class="ae jg" href="https://keras.io/optimizers/#rmsprop" rel="noopener ugc nofollow" target="_blank"> Keras </a>):</p><ul class=""><li id="9507" class="no np jj la b lb lc le lf lh nq ll nr lp ns lt og nu nv nw bi translated"><em class="lu"> α </em> = 0.001</li><li id="1bc7" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> β </em> = 0.9(论文作者推荐)</li><li id="9332" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> ε = </em> 10⁻⁶</li></ul><h2 id="8320" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">5.阿达德尔塔</h2><p id="adaa" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">像 RMSprop 一样，Adadelta ( <a class="ae jg" href="https://arxiv.org/abs/1212.5701" rel="noopener ugc nofollow" target="_blank">泽勒，2012 </a>)也是 AdaGrad 的另一个改进，专注于学习率部分。Adadelta 可能是“自适应 delta”的简称，这里的<em class="lu"> delta </em>指的是当前权重和新更新的权重之间的差值。</p><p id="0183" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Adadelta 和 RMSprop 之间的区别在于，Adadelta 通过用<em class="lu"> D、</em>平方的指数移动平均值<em class="lu"> deltas </em>来代替学习率参数，从而完全取消了学习率参数的使用。</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/3601fc1cb238515b233b5b19795b56a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3NOrGUPHzcB-qI1jxWjf9Q@2x.png"/></div></div></figure><p id="afac" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在哪里</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/2dac958dc0bf8f4f41dd9db267c2b6af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i3OuE84PF_a1bvMtV7autA@2x.png"/></div></div></figure><p id="6158" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将<em class="lu"> D </em>和<em class="lu"> v </em>初始化为 0，并且</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/0738cdd8c1192a785b9e2932f5d1fc17.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*TscI4WyJTVJaBOXILBoO3A@2x.png"/></div></figure><p id="7669" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">默认值(来自<a class="ae jg" href="https://keras.io/optimizers/#adadelta" rel="noopener ugc nofollow" target="_blank"> Keras </a>):</p><ul class=""><li id="e480" class="no np jj la b lb lc le lf lh nq ll nr lp ns lt og nu nv nw bi translated"><em class="lu"> β </em> = 0.95</li><li id="3797" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> ε = </em> 10⁻⁶</li></ul><h2 id="ab07" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">6.内斯特罗夫加速梯度(NAG)</h2><p id="809a" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">在 Polyak 获得了他的势头(双关语😬)，使用内斯特罗夫加速梯度进行了类似的更新(<a class="ae jg" href="http://proceedings.mlr.press/v28/sutskever13.html" rel="noopener ugc nofollow" target="_blank"> Sutskever 等人，2013 </a>)。这次更新利用了<em class="lu"> m </em>，我称之为<em class="lu">投影梯度的指数移动平均。</em></p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/31cea6abb795ec433626a1f50f709d75.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*7EaQcjGvjmM1OFhFLn5jdA@2x.png"/></div></figure><p id="c4b7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在哪里</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/f840d7ed7d25beb295036d1e5439d06b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*A12Mz8xoZd7eaQkhg_waYQ@2x.png"/></div></figure><p id="ae98" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">并且<em class="lu"> m </em>被初始化为 0。</p><p id="acb2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第二个等式的最后一项是投影梯度。这个值可以通过使用先前的速度“向前一步”获得(公式。4).这意味着对于这个时间步骤<em class="lu"> t </em>，我们必须在最终执行反向传播之前执行另一个正向传播。事情是这样的:</p><ol class=""><li id="002c" class="no np jj la b lb lc le lf lh nq ll nr lp ns lt nt nu nv nw bi translated">使用之前的速度将当前重量<em class="lu"> w </em>更新为预计重量<em class="lu"> w* </em>。</li></ol><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/108943f56d2fd396867c8886ac3b4974.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*kUuyhU2idoUYBgl3lKIYZw@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Eqn. 4</figcaption></figure><p id="4290" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.执行向前传播，但是使用这个投影权重。</p><p id="3391" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu"> 3。</em>获得预计坡度<em class="lu"> ∂L/∂w* </em>。</p><p id="cf8c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4.相应地计算<em class="lu"> V </em>和<em class="lu"> w </em>。</p><p id="2fe3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通用默认值:</p><ul class=""><li id="c59a" class="no np jj la b lb lc le lf lh nq ll nr lp ns lt og nu nv nw bi translated"><em class="lu"> β = </em> 0.9</li></ul><blockquote class="ok ol om"><p id="d1a4" class="ky kz lu la b lb lc kk ld le lf kn lg on li lj lk oo lm ln lo op lq lr ls lt im bi translated"><strong class="la jk"> <em class="jj">关于 NAG 的起源</em> </strong> <em class="jj"> <br/>注意，最初的内斯特罗夫加速梯度论文(</em> <a class="ae jg" href="http://www.cis.pku.edu.cn/faculty/vision/zlin/1983-A%20Method%20of%20Solving%20a%20Convex%20Programming%20Problem%20with%20Convergence%20Rate%20O(k%5E(-2))_Nesterov.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="jj">内斯特罗夫，1983 </em> </a> <em class="jj">)并不是关于随机梯度下降的，也没有明确使用梯度下降方程。因此，更合适的参考是上面提到的 Sutskever 等人在 2013 年的出版物，该出版物描述了 NAG 在随机梯度下降中的应用。(再次感谢 HackerNews 上詹姆斯的</em> <a class="ae jg" href="https://news.ycombinator.com/item?id=18525494#18528682" rel="noopener ugc nofollow" target="_blank"> <em class="jj">评论</em> </a> <em class="jj">指出了这一点。)</em></p></blockquote><h2 id="2027" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">7.圣经》和《古兰经》传统中）亚当（人类第一人的名字</h2><p id="cdec" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">自适应矩估计，或 Adam ( <a class="ae jg" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank"> Kingma &amp; Ba，2014 </a>)，就是动量和 RMSprop 的简单结合。它作用于</p><ol class=""><li id="b814" class="no np jj la b lb lc le lf lh nq ll nr lp ns lt nt nu nv nw bi translated">使用<em class="lu"> m </em>的梯度分量，梯度的指数移动平均(如动量)，以及</li><li id="61d9" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">通过将学习率<em class="lu"> α </em>除以<em class="lu"> v </em>的平方根得到的学习率分量，即平方梯度的指数移动平均值(类似于 RMSprop 中的)。</li></ol><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/727131d883ff4bb2e1e56709d852b09a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*Be5WZOzllc0mhYmnN-YcEw@2x.png"/></div></figure><p id="8a78" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在哪里</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/41ee3f80551d05200ccef90657cbe900.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*swR2mA_nV3hWWUTqPgPEJg@2x.png"/></div></figure><p id="0254" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">是偏差修正，和</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/d08fa4b3ac1dc55e6841885175254fa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pvzuTtAZ2Hzb65-dv5nqGQ@2x.png"/></div></div></figure><p id="3115" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将<em class="lu"> m </em>和<em class="lu"> v </em>初始化为 0。</p><p id="8c44" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者建议的默认值:</p><ul class=""><li id="4fbe" class="no np jj la b lb lc le lf lh nq ll nr lp ns lt og nu nv nw bi translated"><em class="lu"> α </em> = 0.001</li><li id="4dfa" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> β </em> ₁ = 0.9</li><li id="78b0" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> β </em> ₂ = 0.999</li><li id="f8ac" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> ε </em> = 10⁻⁸</li></ul><h2 id="2b3f" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">8.阿达马克斯</h2><p id="d728" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">AdaMax ( <a class="ae jg" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank"> Kingma &amp; Ba，2015 </a>)是由相同的作者使用<a class="ae jg" href="https://en.wikipedia.org/wiki/Norm_(mathematics)#Maximum_norm_(special_case_of:_infinity_norm,_uniform_norm,_or_supremum_norm)" rel="noopener ugc nofollow" target="_blank">无穷范数</a>改编的亚当优化器(因此称为“Max”)。<em class="lu"> m </em>是梯度的指数移动平均值，而<em class="lu"> v </em>是过去<em class="lu"> p </em>的指数移动平均值——梯度的范数，近似于最大值函数，如下所示。请参考他们的收敛性证明文件。</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/d0d1a986bb8a7433acb664df649b0616.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*9xBlFyq1LxL6HX4aSxD5Fw@2x.png"/></div></figure><p id="bcae" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在哪里</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/208a043ae44ba9b2c7565e48db4d8e99.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*lNx8uE_cEHrA942J_wh6Fg@2x.png"/></div></figure><p id="3cd0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">是对<em class="lu"> m </em>的偏差校正</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/544d376f9e2c2d3e54b4c3956a106212.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*BsoE9V9NGdtsjP5pjwdxqQ@2x.png"/></div></figure><p id="211e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将<em class="lu"> m </em>和<em class="lu"> v </em>初始化为 0。</p><p id="5394" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者建议的默认值:</p><ul class=""><li id="d30c" class="no np jj la b lb lc le lf lh nq ll nr lp ns lt og nu nv nw bi translated"><em class="lu"> α </em> = 0.002</li><li id="4143" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> β </em> ₁ = 0.9</li><li id="4d6e" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> β </em> ₂ = 0.999</li></ul><h2 id="6d4c" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">9.那达慕</h2><p id="dddb" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">Nadam ( <a class="ae jg" href="http://cs229.stanford.edu/proj2015/054_report.pdf" rel="noopener ugc nofollow" target="_blank"> Dozat，2015 </a>)是内斯特罗夫和亚当乐观者的缩写。然而，内斯特罗夫组件是比其原始实现更有效的修改。</p><p id="1892" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我想说明亚当乐观主义者也可以写成:</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/c89a261a48904e965a130cd6e049300c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hjrob5eosQstbBCTPFxi_Q@2x.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Eqn. 5: Weight update for Adam optimiser</figcaption></figure><p id="4242" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那达慕使用内斯特罗夫提前一步更新梯度，将上面等式中之前的<em class="lu"> m_hat </em>替换为当前的<em class="lu"> m_hat </em>:</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/71482c2509bf0e6da588ea185c00e1c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ooyKi81UnH1TR0uNuhNXQw@2x.png"/></div></div></figure><p id="cc9b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在哪里</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/06d8779ba1a160142779092b9b2470a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*iqzMu0ZhGckMbrhAbNJXQQ@2x.png"/></div></figure><p id="a1ff" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">和</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/89f65e83965cbdf5d7ea0228bfd4b05d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kFozluIfAjkusAUtjLVf3w@2x.png"/></div></div></figure><p id="6395" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将<em class="lu"> m </em>和<em class="lu"> v </em>初始化为 0。</p><p id="2dd5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">默认值(取自<a class="ae jg" href="https://keras.io/optimizers/#nadam" rel="noopener ugc nofollow" target="_blank"> Keras </a>):</p><ul class=""><li id="fd45" class="no np jj la b lb lc le lf lh nq ll nr lp ns lt og nu nv nw bi translated"><em class="lu"> α </em> = 0.002</li><li id="7f02" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> β </em> ₁ = 0.9</li><li id="f03d" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> β </em> ₂ = 0.999</li><li id="cbe3" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> ε </em> = 10⁻⁷</li></ul><h2 id="de85" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">10.阿姆斯格勒</h2><p id="3606" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">亚当的另一个变体是 AMSGrad ( <a class="ae jg" href="https://openreview.net/pdf?id=ryQu7f-RZ" rel="noopener ugc nofollow" target="_blank"> Reddi 等人，2018 </a>)。该变体重新访问 Adam 中的自适应学习率组件并对其进行更改，以确保当前的<em class="lu"> v </em>始终大于前一时间步<em class="lu">的<em class="lu"> v </em>。</em></p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/544a10a3392da8323fcf190922fd84c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*lhGhPFLhmBo22QQJMbc7bw@2x.png"/></div></figure><p id="29b9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在哪里</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/20c1386ab0d9c3e651fac6511fdf26e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*FJ8e2V2s1MUX_TaXq_DnIg@2x.png"/></div></figure><p id="5fc6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">和</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/6802c9941a7cd98475dbd41933f57cc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H-KgH3PHlsdWY45OksgwDw@2x.png"/></div></div></figure><p id="5523" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将<em class="lu"> m </em>和<em class="lu"> v </em>初始化为 0。</p><p id="aea7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">默认值(取自<a class="ae jg" href="https://keras.io/optimizers/#adam" rel="noopener ugc nofollow" target="_blank"> Keras </a>):</p><ul class=""><li id="025d" class="no np jj la b lb lc le lf lh nq ll nr lp ns lt og nu nv nw bi translated"><em class="lu"> α </em> = 0.001</li><li id="00a1" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> β </em> ₁ = 0.9</li><li id="ab3a" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> β </em> ₂ = 0.999</li><li id="647b" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt og nu nv nw bi translated"><em class="lu"> ε </em> = 10⁻⁷</li></ul><p id="c9e5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果有什么不对的地方，或者这篇文章中有什么可以改进的地方，请联系我！✌🏼</p><h2 id="5442" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">附录 2:直觉</h2><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/14bb7a6241ae98979d1de2d01bff8211.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eNw93ANXBQzby0eINLpA_Q@2x.png"/></div></div></figure><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/8dd0fb0090516d672bee173d34d30168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*igkglW58Fkf_S0jiDeYvAg@2x.png"/></div></div></figure><p id="4920" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以上要点可在<a class="ae jg" href="https://gist.github.com/remykarem/d5eff32a67ceeec8e653d1016525a649" rel="noopener ugc nofollow" target="_blank">这里</a>找到。用<a class="ae jg" href="https://quicklatex.com/" rel="noopener ugc nofollow" target="_blank"> QuickLaTeX </a>生成的图像。(感谢拉维指出那达慕更新中的错别字。)</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="a521" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">附录 2:直觉</h2><p id="b7a7" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">在这里，我想和你分享一些直觉，为什么梯度下降优化器对梯度分量使用指数移动平均，对学习率分量使用均方根。</p><p id="1710" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">为什么要取梯度的指数移动平均？</strong></p><p id="7ba9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">回想一下，我们需要更新权重，为此我们需要利用<em class="lu">的某个值</em>。我们拥有的唯一值是当前的梯度，所以我们只使用这个信息来更新权重。</p><p id="56ba" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是只取当前的梯度值是不够的。我们希望我们的更新得到“更好的引导”这是通过使用先前关于梯度的信息来实现的。因此，让我们通过<em class="lu">聚合</em>当前渐变和过去渐变来包含以前的渐变。</p><p id="7bb8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">聚合这些梯度的一种方法是对所有过去和当前梯度进行简单平均。但是等等，这意味着这些梯度的权重是相等的。这公平吗？也许吧。也许不是。</p><p id="5f4b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以做的是采用指数移动平均，其中过去的梯度值被赋予比当前值更高的权重(重要性)。直观上，不考虑当前梯度的重要性将确保权重更新对当前梯度不敏感。</p><p id="596b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">为什么要用平方梯度的指数平均值的根来除学习率？</strong></p><p id="c3ea" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">调整学习率的目标是通过将学习率除以多个梯度的均方根来使优化器“更聪明”。所以让我们问自己这些问题:</p><ol class=""><li id="d5bb" class="no np jj la b lb lc le lf lh nq ll nr lp ns lt nt nu nv nw bi translated">为什么要走多重渐变？</li><li id="7cb6" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">为什么要分？</li><li id="0355" class="no np jj la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">为什么要取平方梯度的指数移动平均的根？</li></ol><p id="2979" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">前一节已经回答了第一个问题——除了当前梯度值，我们还想利用过去梯度的信息。</p><p id="8be7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了回答第二个问题，首先，考虑一个简单的情况，其中过去几次迭代的梯度的平均幅度为 0.01。由于该值接近于 0，这意味着我们一直在一个近似平坦的表面上(想象一段平坦的 3D 损失景观)。我们现在所在的地方，地球很平，所以我们很有信心在这个区域移动。事实上，我们想尽快离开这个区域，寻找一个向下的斜坡，这个斜坡可能会把我们带到一个全球性的最小值。(你可能会发现一些文章提到这有“加速”的效果。)因此，当梯度的幅度较小时，我们想要增加学习速率分量(学习得更快)。为了建立这种反比关系，我们取固定的学习速率<em class="lu"> α </em>并除以梯度的平均幅度。这个适应的学习率(现在是一个大值)然后乘以梯度分量，给我们一个大的幅度的权重更新(不管是正的还是负的)。</p><p id="ba12" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">直觉也适用于相反的情况。假设我们的梯度平均值很高，大约是 2.7。这意味着我们一直在陡坡上。我们希望谨慎行事，所以我们采取较小的步骤，这可以通过执行相同的划分来实现。</p><p id="0931" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在最后一个问题中，我们采用指数移动平均线的原因在前面已经很明显了。我们取梯度的平方的原因很简单，当处理学习率分量时，我们关心的是它的大小。“抵消”这一点的自然选择是扎根。这背后可能有数学，但现在让我们用这种直觉来说服自己。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="1e2f" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">附录 3: <strong class="ak"> <em class="ph">学习率调度器与随机梯度下降优化器</em> </strong></h2><p id="d65f" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">有些人可能会问——学习率调度器和随机梯度下降优化器之间有什么区别？这两者之间的主要区别在于，随机梯度下降优化器通过将学习率乘以一个因子来调整学习率分量，该因子是梯度的函数，而学习率调度器将学习率乘以一个因子，该因子是时间步长(或者甚至是常数)的函数。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="42a0" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">参考</h2><p id="041f" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">上面提到的每一个乐观主义者的论文</p><p id="84c4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" href="http://ruder.io/optimizing-gradient-descent" rel="noopener ugc nofollow" target="_blank">梯度下降优化算法概述</a> (ruder.io)</p><p id="a007" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" href="https://distill.pub/2017/momentum/" rel="noopener ugc nofollow" target="_blank">为什么动量真的有效</a></p><h2 id="0ff0" class="ml mm jj bd mn mo mp dn mq mr ms dp mt lh mu mv mw ll mx my mz lp na nb nc nd bi translated">深度学习相关文章</h2><p id="2c08" class="pw-post-body-paragraph ky kz jj la b lb ne kk ld le nf kn lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/animated-rnn-lstm-and-gru-ef124d06cf45">RNN、LSTM 和 GRU 的动画</a></p><p id="5307" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281">逐行 Word2Vec 实现</a>(关于单词嵌入)</p><p id="7124" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/step-by-step-tutorial-on-linear-regression-with-stochastic-gradient-descent-1d35b088a843">关于随机梯度下降线性回归的分步指南</a></p><p id="698e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889">统计深度学习模型中的参数数量</a></p><p id="3d8c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/attn-illustrated-attention-5ec4ad276ee3">收件人:图文并茂的注意事项</a></p><p id="34bf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/illustrated-self-attention-2d627e33b20a">图文并茂:自我关注</a></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="6966" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">感谢</em> <a class="ae jg" href="https://medium.com/@renjietan" rel="noopener"> <em class="lu">【任杰】</em></a><em class="lu"/><a class="ae jg" href="https://medium.com/@derekchia" rel="noopener"><em class="lu">德里克</em> </a> <em class="lu">、威廉 Tjhi、陈凯、澄净、</em> <a class="ae jg" href="https://news.ycombinator.com/item?id=18525494#18528682" rel="noopener ugc nofollow" target="_blank"> <em class="lu">詹姆斯</em> </a> <em class="lu">对本文的想法、建议和更正。</em></p><p id="42c8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">关注我上</em> <a class="ae jg" href="https://www.twitter.com/remykarem" rel="noopener ugc nofollow" target="_blank"> <em class="lu">推特</em> </a> <em class="lu"> @remykarem 或者</em><a class="ae jg" href="http://www.linkedin.com/in/raimibkarim" rel="noopener ugc nofollow" target="_blank"><em class="lu">LinkedIn</em></a><em class="lu">。你也可以通过 raimi.bkarim@gmail.com 联系我。欢迎访问我的网站</em><a class="ae jg" href="https://remykarem.github.io/" rel="noopener ugc nofollow" target="_blank"><em class="lu">remykarem . github . io</em></a><em class="lu">。</em></p></div></div>    
</body>
</html>