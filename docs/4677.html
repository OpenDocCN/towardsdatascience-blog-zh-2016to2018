<html>
<head>
<title>Regularization in Machine Learning: Connect the dots</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的正则化:将点连接起来</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regularization-in-machine-learning-connecting-the-dots-c6e030bfaddd?source=collection_archive---------3-----------------------#2018-08-30">https://towardsdatascience.com/regularization-in-machine-learning-connecting-the-dots-c6e030bfaddd?source=collection_archive---------3-----------------------#2018-08-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="4261" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">故障</h1><p id="5738" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">以下是我们将一起走的各个步骤，并试图获得理解。</p><ol class=""><li id="83b9" class="lj lk iq kn b ko ll ks lm kw ln la lo le lp li lq lr ls lt bi translated">语境</li></ol><p id="47b6" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">2.先决条件</p><p id="4354" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">3.过度拟合问题</p><p id="881d" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">4.目标</p><p id="c549" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">5.什么是正规化？</p><p id="4c9f" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">6.L2 范数或岭正则化</p><p id="9f3c" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">7.L1 范数或拉索正则化</p><p id="3b3d" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">8.拉索(L1) vs 里奇(L2)</p><p id="93a0" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">9.结论</p><h1 id="0696" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">1.语境</h1><p id="0166" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在本文中，我们将把线性回归视为一种算法，其中目标变量“y”将由系数为β1 和β2 的两个特征“x1”和“x2”来解释。</p><h1 id="ce4c" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">2.先决条件</h1><p id="ba1b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">首先，让我们先弄清楚一些次要的先决条件，以便理解它们的用法。</p><h2 id="ce52" class="ly jo iq bd jp lz ma dn jt mb mc dp jx kw md me kb la mf mg kf le mh mi kj mj bi translated">线性回归</h2><p id="8bbd" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">可选:参考下面链接中的第 3 章，了解线性回归。</p><div class="mk ml gp gr mm mn"><a href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd ir gy z fp ms fr fs mt fu fw ip bi translated">统计学习的要素:数据挖掘、推理和预测。第二版。</h2><div class="mu l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">web.stanford.edu</p></div></div></div></a></div><h2 id="cb8a" class="ly jo iq bd jp lz ma dn jt mb mc dp jx kw md me kb la mf mg kf le mh mi kj mj bi translated">普通最小二乘(OLS)回归的成本函数</h2><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/f09803b9ef42eeb4e7f414048e2245de.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*i5Wr0EgyjmKLbxXw2oPeNQ.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Source: <a class="ae nh" href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/~hastie/ElemStatLearn/</a></figcaption></figure><p id="7ebe" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">N —样本数量</p><p id="d2c8" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">p——独立变量或特征的数量</p><p id="21ed" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">x-特征</p><p id="dd1b" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">y —实际目标或因变量</p><p id="0637" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">f(x) —估计目标</p><p id="d5b3" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">β——对应于每个特征或独立变量的系数或权重。</p><h2 id="aaff" class="ly jo iq bd jp lz ma dn jt mb mc dp jx kw md me kb la mf mg kf le mh mi kj mj bi translated">表示为等高线图的梯度下降</h2><p id="eee1" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在下面的图 1(a)中，梯度下降以三维形式表示。J (β)代表相应β1 和β2 的误差。红色代表误差高的区域，蓝色代表误差最小的区域。使用梯度下降，将在误差最小的地方识别系数β1 和β2。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/1981ae677537e086aa63b63ee24ac94b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GagrtxPolqxlCR6ZP6CB7w.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Fig 1: Gradient Descent Projection as Contour</figcaption></figure><p id="f786" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">在上面的图 1(b)中，来自梯度下降的成本函数的相应误差被投影到具有相应颜色的 2-dim 上。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e8800ab1fdac4146b75835963763a378.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*XoMLocfIZd_HF73M45DQeA.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Fig 2: Gradient Descent on axes of β1 and β2</figcaption></figure><p id="af69" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">在左侧的图 2 中，简单线性回归的梯度下降轮廓在系数β1 和β2 的坐标系中以二维格式表示。</p><p id="1531" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated"><em class="lx">让我们暂时把这个理解放在一边，继续讨论其他一些基本问题。</em></p><h1 id="6446" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">3.过度拟合问题</h1><p id="9ad6" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">假设我们已经在训练集上使用线性回归训练了一个模型，并将性能提高到令人满意的水平，我们的下一步是根据测试集/看不见的数据进行验证。在大多数情况下，准确性水平会下降。<em class="lx">发生了什么事？</em></p><p id="5d59" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">为了在训练过程中努力提高模型的准确性，我们的模型将尝试拟合和学习尽可能多的数据点。换句话说，随着拟合优度的增加，模型从线性到二次再到多项式(即模型复杂性在增加)。这是由于过度拟合。下图来自吴恩达的机器学习课程，有助于直观地理解这一点。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi no"><img src="../Images/b533a7e533d293b6b11b97461db1f8ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZQ2KPQa4kOiAoTvfSTd1Ew.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Source: <a class="ae nh" href="https://youtu.be/u73PU6Qwl1I?t=211" rel="noopener ugc nofollow" target="_blank">https://youtu.be/u73PU6Qwl1I?t=211</a></figcaption></figure><p id="878d" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated"><em class="lx">高次模型和大系数显著增加了方差，导致过度拟合。</em></p><h1 id="a87e" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">4.目标:</h1><p id="8bf2" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们的模型需要稳健，以便在现实世界中进行良好的预测，即从样本数据或训练期间未见过的数据中进行预测。训练过程中的过度适应是阻止它变得健壮的原因。为了避免这种过度拟合的情况并提高模型的稳健性，我们尝试</p><p id="cda8" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">1.缩小模型中要素的系数或权重</p><p id="0ad2" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">2.从模型中去除高次多项式特征</p><p id="d2c6" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">我们如何实现这一目标？随之而来的是正规化。现在就来探索一下吧。</p><h1 id="c1e7" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">5.什么是正规化？</h1><p id="f2fd" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">假设说，我们已经训练了我们的线性回归模型，并且在全局最小值处识别了系数β1 和β2。为了达到我们的目标；如果我们尝试缩小先前学习的特征的系数，模型会失去准确性。这种准确度的损失需要用其他东西来解释，以保持准确度水平。这个责任将由模型方程的偏差部分承担(<em class="lx">偏差:不依赖于特征数据</em>的模型方程的一部分)。</p><p id="005d" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated"><em class="lx">偏置部分将按以下方式调整:</em></p><p id="4d7b" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">1.<em class="lx">方差需要减少，偏差需要增加:</em>由于方差是系数β1，β2 的函数；偏差也将被标记，并作为系数β1、β2 的函数而改变。使用系数β1、β2 的 L1 和 L2 范数的原因。</p><p id="f49e" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">2.<em class="lx">推广:</em>这个系数β1，β2 的函数需要推广，在整个坐标空间(坐标轴上的正值和负值)都起作用。这就是我们在 L1 和 L2 范数方程中发现模算子的原因。</p><p id="9596" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated"><em class="lx"> 3。</em> <em class="lx">正则化参数‘λ’:</em>由于方差和偏差都是系数β1、β2 的函数，所以它们将成正比。这样不行。因此，我们需要一个额外的参数来调节偏差项的大小。这个调节器是正则化参数‘λ’</p><p id="4488" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">4.<em class="lx">‘λ’是一个超参数:</em>如果‘λ’是一个参数，梯度下降会很好地将其设置为 0，并移动到全局最小值。因此，对‘λ’的控制不能用于梯度下降，需要排除在外。它将不是一个参数，而是一个超参数。</p><p id="bb35" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated"><em class="lx">在理解了偏差项的变化后，让我们进入将用于正则化的系数β1、β2 的一些函数。</em></p><h1 id="90c0" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">6.L2 范数或岭回归</h1><p id="7ab7" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">L2 范数是|β1| + |β2|形式的欧氏距离范数。</p><p id="aae4" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">具有 L2 正则化的修改的成本函数如下:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi np"><img src="../Images/279335476ac1a3ae1bb7eb7d6c0e91ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*4qK-8138XQK9QyKPmBojvw.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Source: <a class="ae nh" href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/~hastie/ElemStatLearn/</a></figcaption></figure><p id="d053" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated"><em class="lx">让我们假设λ = 1 并且暂时不在画面中</em></p><p id="a343" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">在寻找全局最小误差的过程中，β1 和β2 可以变化。正则项(在上面的等式中用蓝色突出显示)；特定值(β1，β2)将产生偏置输出β1 + β2。可以有多个值产生相同的偏差，如(1，0)、(-1，0)、(0，-1)、(0.7，0.7)和(0，1)。在 L2 范数的情况下，产生<em class="lx">特定相同</em>偏差的β1 和β2 的各种组合形成一个圆。对于我们的例子，让我们考虑一个偏差项= 1。图 6(a)表示这个圆。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/41a4c67be8f22776440b3cd9927adc28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*VNq12fddW4O25KyLy-JgBw.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Fig 6: L2 Norm</figcaption></figure><p id="3d9c" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">图 6(b)表示线性回归问题的梯度下降等高线图。现在，这里有两股力量在起作用。</p><p id="6255" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated"><em class="lx">力 1:偏项，将β1 和β2 拉至仅位于黑色圆圈上的某处。</em></p><p id="d67d" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated"><em class="lx">力 2:梯度下降试图行进到绿点指示的全局最小值。</em></p><p id="8109" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">这两个力都在拉动，最后停在由“红十字”表示的交点附近。</p><p id="8532" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated"><em class="lx">记住我们之前对图 6(b)的假设是偏差项= 1 且λ = 1。现在去掉一个关于偏差项的假设。</em></p><p id="d4db" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">想象一下“黑色圆圈”的大小在变化，梯度下降落在成本最低的不同点上。λ = 1 的假设仍然存在。观察到的最小成本是λ = 1 时的</p><p id="2abf" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated"><em class="lx">接下来去掉另一个假设λ = 1。提供一个不同的值，比如λ = 0.5。</em></p><p id="863a" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">对λ = 0.5 重复梯度下降过程，并检查成本。可以对不同的λ值重复该过程，以便识别成本函数给出最小误差的最佳λ值。</p><p id="7571" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">总的目标是在坡度下降后保持低成本。因此，λ、β1、β2 的值被确定为保持客观。</p><p id="92be" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated"><em class="lx">在该过程结束时，方差将会减小，即系数的大小减小。让我们转到β1，β2 的另一类函数。</em></p><h1 id="4ea7" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">7.L1 范数或拉索回归</h1><p id="70e7" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">L1 范数的形式是|β1| + |β2|。</p><p id="41be" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">L1 正则化的修正成本函数如下:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/36a3de1747fa4aa3ac39888f30ae4045.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*WEGhpA3qrHZKkrFmaa2Otw.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Source: <a class="ae nh" href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/~hastie/ElemStatLearn/</a></figcaption></figure><p id="c7aa" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">图 7(a)显示了 L1 范数的形状。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/5a3d354a3088c7c36a45a5f8cf6f91b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*iz4AZmNJGWNzOX5P-SmW4A.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Fig 7: L1 Norm</figcaption></figure><p id="0f9e" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">图 7(b)显示了具有梯度下降等高线图的 L1 范数。从减少差异的角度来看，上一节讨论的相同逻辑在这里也是有效的。</p><p id="0cd2" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated"><em class="lx">让我们转到套索正则化的另一个重要方面，我们将在下一节讨论。</em></p><h1 id="c0c6" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">8.拉索(L1)对岭(L2)正规化</h1><p id="a888" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">图 8(a)显示了 L1 和 L2 标准的面积。对于产生的相同量的偏置项，L1 范数占据的面积很小。但是 L1·诺姆不允许任何靠近轴线的空间。这是导致 L1 范数和梯度下降轮廓之间的交点在轴附近<em class="lx">会聚的原因，从而导致特征选择。</em></p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b95d86e417f6a7b29a698cf29cefdbb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*GSQfWiZolZzSJfGTRaDnMg.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Fig 8: L1 vs L2 Norms</figcaption></figure><p id="7016" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">图 8(b)显示了 L1 和 L2 标准以及不同线性回归问题的梯度下降轮廓。除了一种情况，L1 范数收敛于或非常接近轴，因此从模型中移除特征。</p><p id="8ff4" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">这里的每个象限都有一个梯度下降轮廓，用于不同的线性回归问题。绿色、蓝色、棕色表示它们与不同的线性回归问题有关。每个等高线中的红色圆圈与山脊或 L2 范数相交。每个轮廓中的黑色圆圈与套索或 L1 范数相交。</p><p id="4147" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">这表明 L1 范数或 Lasso 正则化作为特征选择器，同时减少方差。</p><h1 id="d329" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">9.结论</h1><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ns"><img src="../Images/2b2b9eafb7991f58af710f5843fc168b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jF-8CxB59tcfuoB2Kw_SQQ.png"/></div></div></figure><p id="2d65" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw lu ky kz la lv lc ld le lw lg lh li ij bi translated">方差减少等同于偏差方差权衡。</p><h2 id="8793" class="ly jo iq bd jp lz ma dn jt mb mc dp jx kw md me kb la mf mg kf le mh mi kj mj bi translated">学分:</h2><ol class=""><li id="3f8b" class="lj lk iq kn b ko kp ks kt kw nt la nu le nv li lq lr ls lt bi translated"><em class="lx">《统计学习的要素》,作者:特雷弗·哈斯蒂、罗布·蒂伯拉尼、杰罗姆·弗里德曼。链接:</em>【https://web.stanford.edu/~hastie/ElemStatLearn/】T4</li><li id="d61d" class="lj lk iq kn b ko nw ks nx kw ny la nz le oa li lq lr ls lt bi translated"><em class="lx">正规化话题由吴恩达提出。链接:</em><a class="ae nh" href="https://youtu.be/u73PU6Qwl1I?t=211" rel="noopener ugc nofollow" target="_blank">https://youtu.be/u73PU6Qwl1I?t=211</a></li></ol></div></div>    
</body>
</html>