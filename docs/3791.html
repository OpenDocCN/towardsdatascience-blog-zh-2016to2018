<html>
<head>
<title>[ Paper Summary ] Emergence of Structured Behaviors from Curiosity-Based Intrinsic Motivation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">【论文摘要】基于好奇心的内在动机引发结构化行为</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-summary-emergence-of-structured-behaviors-from-curiosity-based-intrinsic-motivation-3d2b49f516f1?source=collection_archive---------6-----------------------#2018-06-18">https://towardsdatascience.com/paper-summary-emergence-of-structured-behaviors-from-curiosity-based-intrinsic-motivation-3d2b49f516f1?source=collection_archive---------6-----------------------#2018-06-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/39f525e24cd8f43fb9f983c1f570daf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*WmYIHZHMVNayWYrIWSn66Q.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/heart-crystal-reiki-xThuWl7sV12hg7TXWg" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="815d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我从 Carlos E. Perez  ( <a class="ae jy" href="https://medium.com/intuitionmachine/ego-motion-in-self-aware-deep-learning-91457213cfc4" rel="noopener">自我意识深度学习中的自我运动</a>)的一篇博客文章中看到了这篇论文，我立刻就想读它。另外，如果你有时间，我强烈建议你阅读这篇博文，因为它非常有趣。</p><blockquote class="kx ky kz"><p id="89a0" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated"><strong class="kb ir">请注意，这篇文章是为我未来的自己写的，目的是回顾这篇论文上的内容，而不是从头再看一遍。</strong></p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="https://arxiv.org/pdf/1802.07461.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="2ec4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">摘要</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/30fe9e3c3fbf7e79c9255ade65dd97e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dkgvS_Q7-ftMjRa8D_81FA.png"/></div></div></figure><p id="abaf" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">婴儿是在非结构化环境中产生新的结构化行为的专家，在这种环境中没有明确的奖励系统。(我不知道这到底是什么意思，但一般来说，我的理解是，当婴儿出生时，他们通过观察周围的人或事物开始不断学习。他们慢慢地但肯定地获得了世界是如何在他们周围构建的知识。问题是，当婴儿学习时，没有明确的解释或奖励系统，但不知何故，他们或我们能够学习。)</p><p id="faca" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">作者通过使用神经网络复制了这些能力，神经网络是一种由好奇心驱动的代理。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="4bcc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">简介</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/46b44da054745168f0cc72c513305104.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F-CvwATmKE7pRATLrP9ygg.png"/></div></div></figure><p id="a0f1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从出生的那一刻起，我们人类就擅长驾驭我们的环境。(而这些环境可以是随机的、自发的、缺乏一般结构的。)换句话说，我们非常擅长模拟我们周围的世界，谢天谢地，与最先进的机器人相比，即使是婴儿在这方面也做得更好。(所以他们还没有接管世界。)</p><p id="e3a9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">但是主要的问题是我们怎样才能学会这样做？总的来说，我们有非常强大的内置系统(如物体注意/定位和数字感等)，帮助我们完成对周围世界建模的任务。另一个想法(本文研究的)是，我们的动机是对周围世界的好奇。以及通过探索新任务(其中的任务是新颖的，但仍然可以理解)。)我们能够围绕我们的周围开发出一个非常好的模型。这里要指出的另一个重要事实是循环的概念，一旦我们习惯了这个任务，我们就会寻找更多新的令人兴奋的东西。由于这种循环，我们能够将我们的模型发展成更复杂的模型，所以这是自我监督的学习。另外，作者介绍了“婴儿床里的科学家”这个主题，我找到了一个与这个主题相关的好视频。(见下图)</p><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lx lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Video from <a class="ae jy" href="https://www.youtube.com/channel/UCU1csP9quygVRsPQuAfNEZQ" rel="noopener ugc nofollow" target="_blank">ECETP</a></figcaption></figure><p id="76b0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，作者介绍了论文的主要主题，其中他们创建了一个由基于好奇心的内在动机驱动的代理。</p><p id="e5b6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">神经网络 A →训练预测代理行为的结果<br/>神经网络 B →训练对抗挑战模型的当前状态(代理策略)(我猜这就是作者所说的好奇心)</p><p id="0562" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">通过成功地训练所有的网络，作者证明了创造一个能够理解自我产生的自我运动的代理是可能的，在这种运动中，他们有选择地关注、定位和与物体互动，而不必内置任何这些概念。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="d950" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">代理架构和环境</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ly"><img src="../Images/ac63fcc2a8f6f5a3d536ace633d6fb34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V7uCBfossR5uEhjfkBiQlA.png"/></div></div></figure><p id="811f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">作者使用<a class="ae jy" href="https://unity3d.com/" rel="noopener ugc nofollow" target="_blank"> Unity </a>来训练一个智能体，其中它有一个世界模型，试图理解智能体周围世界的动态，还有一个损失模型，试图逼近未来时间戳的世界模型损失，以对抗世界模型的学习。最后，作者没有加载任何预先训练的权重，使代理从零开始学习。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="c6a4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">交互环境</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lz"><img src="../Images/c4d9925ee3992a8f7140af7d3fc21463.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oNXv3vK2Xf9p27O64Q73VA.png"/></div></div></figure><p id="40c0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这一节中，作者描述了实验是如何设置的。例如，代理是一个正方形房间中的球体，它接收离散时间戳中的 RGB 图像等……此外，作者还详细介绍了他们如何定义状态空间(在时间戳 t 和 t -1 捕获的图像)和动作空间(x，y，z 力/扭矩和向前/向后运动)。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="a6d5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">世界模特</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/f781edf1d35900689a883010b0e19f9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*hKuZo860qeyEhTgzBj_Zmg.png"/></div></figure><p id="4155" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">(这部分对我来说很难，因为我不知道什么是广义动力学问题。)首先，给定状态和动作的历史片段(H ),有两个函数。A(H) →输入映射函数// T(H) →真实映射函数并且其中一个神经网络正在尝试将 A(H)映射到 T(H)。总之，代理正试图学习一个与真实世界相似的世界模型。最后，作者强调他们把问题公式化为逆动力学预测。与其预测未来，不如填写缺失的动作。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="b1b7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">损失模型</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mb"><img src="../Images/8a6f9428e14a8182944b9f050f1a38b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lqZt7EXrdBd4z9MSowDKGA.png"/></div></div></figure><p id="2020" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这一节中，作者描述了他们如何创建一个损失模型。它由卷积神经网络构成，在 CNN 的最后几层增加了多层感知。他们使用 softmax 交叉熵损失进行分类。然而，我现在还不能 100%确定有哪些不同的职业。(所以我将不得不学习更多的 RL 并回到这个问题上来，如果任何人确切地知道他们在谈论什么，请在下面评论。)</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mc"><img src="../Images/28bda6b50eb3b833add1372bbf88ef91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3m9JIE_XVN17GJEN3p4PjQ.png"/></div></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="f86b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">行动策略</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi md"><img src="../Images/ff2c241bb7e4a5eee2d29f6f531a537a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4zI0RzDKMxI4yosgOD02QQ.png"/></div></div></figure><p id="afa6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这里，作者描述了模型的动作策略。我们知道，当给定 T 中的状态以及建议的下一个动作 a 时，损失函数能够为我们提供 T(概率分布)。似乎作者创造了另一个函数σ，它接受这些分布并给出一个实值。接下来，我们将通过一些β项来缩放真实值，最后取最终结果的指数。(这部分和损失模型对我来说很难理解。)</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="0a61" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">实验</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi me"><img src="../Images/9d17e387a30a316da1e092c9367819f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9KnVwcCuublIABpaSW14bw.png"/></div></div></figure><p id="e326" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">实验从将试剂随机放置在 10*10 平方的房间中开始。(有一定的播放距离，场景每 8000 到 30000 步重置一次。).关于这个实验的一个非常有趣的事实是，随着(一个代理可以与之交互的)对象数量的增加，代理实际上更喜欢与所有的对象进行交互。(我猜这就是好奇心的动力来源，例如，在玩了圆形之后，代理可以移动到球形，然后移动到三角形。因为它是由新的令人兴奋的形状等激发的。)最后，作者将学习权重/好奇心策略(LW/CP-40)与随机权重/随机策略(RW/RP)和学习权重/随机策略(LW/RP)进行了比较。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="633d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">自我运动学习</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mf"><img src="../Images/cd72b8abd9832d5a05ef4ab806b02bc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D3iBROOwtSNGEW9Mzkjk0Q.png"/></div></div></figure><p id="e6d0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，RW/RP(绿线)不能很好地学习，损失值很高。LW/RP 能够快速收敛到一个较低的值，因为它从一个恒定的随机分布中学习，而没有对抗性的策略。(无对抗性政策)。但是 LW/CP-40 是两者的混合，首先它能够收敛到一个较低的值，但是由于敌对政策，损失增加。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="7fb9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">物体注意的出现</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/19b4380a0d372f620ec3d2e96e5eba59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*QkcCVs407QBE8JnQMUWjDQ.png"/></div></figure><p id="8499" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">随着 LW/CP-40 损失值的增加，其目标播放频率也增加，如上所示。(出于好奇)。我们可以观察到其他权重和策略根本不与对象交互。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="4f64" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">改进的逆动力学预测</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mh"><img src="../Images/5fcb756aeedcd369b206470422de15c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SY8TnhQ4m6GmJxR8qC1fVA.png"/></div></div></figure><p id="da27" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，当作者测量不同代理在 1)自我运动确认损失 2)对象动作上的表现时，我们可以清楚地观察到 LW/CP-40 优于这两个任务。(不是因为自我运动验证损失，因为 LW/RP 也具有相当低的值。)</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="4d3d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">改进的目标检测和定位</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mi"><img src="../Images/ed21098b54fbe2158fa7631045b6eac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n4z6LY-_kV9ncUdASj6gZA.png"/></div></div></figure><p id="038a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">此外，为了证明 LW/CP-40 在目标定位和存在方面的性能优势，他们训练了一个线性/逻辑回归模型。(训练/验证数据都来自代理所处的环境。)如上所述，LW/CP-40 策略具有最低的误差，表明代理能够学习更好的视觉特征。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="3d12" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">导航和规划</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mj"><img src="../Images/61e01961f2998a3547b56ea7da3b4cb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i3c6fXFkIA3olkLSy6F3FA.png"/></div></div></figure><p id="c8c3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">此外，为了证明代理的导航和规划能力，作者已经可视化了损失地图，如上所示。当红色区域表示更多更高的损失时，接受过 LW/CP-40 政策培训的代理将实际采取更接近目标的行动。(非常有趣，尽管错误率很高，但代理实际上会朝那个方向移动…)</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="895b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">多对象交互的出现</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mk"><img src="../Images/95865cc41886381fef85571710540e63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nvY-SD8b8bzqzlJmQMq07g.png"/></div></div></figure><p id="471c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，即使对于两个对象交互实验，我们也可以观察到，在 LW/CP-(40 或 20)策略上训练的代理倾向于与不同的对象进行交互。(而且他们似乎更乐于接受学习新东西的想法。)具体来说，我们可以看到 LW/CP-40 策略比 LW/CP-20 策略更好地学会了使用 2 个对象。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="e1f9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">讨论及未来工作</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/8049a1cb5aed7bbcc4e2cea2539658e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*lICAdNhXaSo5IYhUvYvRGQ.png"/></div></figure><p id="ace9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">总之，作者能够创建一个由好奇心驱动的代理，通过自我监督学习系统，代理能够适应周围的复杂世界。例如，在开始时，代理人集中精力学习自己的自我运动的动力学。之后，它开始学习物体的存在或定位，因此放弃了无聊的自我运动预测任务，接受新的挑战。</p><p id="1c60" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">此外，作者还指出了当前工作的一些不足之处。(例如适当具体化的带有手臂的代理，以模拟更真实的交互或更好的策略等)</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="f95a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">遗言</strong></p><p id="ad44" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">作为强化学习的初学者，我发现这篇论文上的材料非常难掌握。我迫不及待地想深入研究 RL :D。另一篇与这个主题相关的好论文是“<a class="ae jy" href="https://arxiv.org/pdf/1802.07442.pdf" rel="noopener ugc nofollow" target="_blank">学习使用内在激励的自我意识代理</a> s”</p><p id="2406" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有写作的列表，请在这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的 twitter <a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>关注我，并访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文 pos </a> t。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="709c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="a9d8" class="mm mn iq kb b kc kd kg kh kk mo ko mp ks mq kw mr ms mt mu bi translated">自我意识深度学习中的自我运动-直觉机器-媒介。(2018).中等。检索于 2018 年 6 月 17 日，来自<a class="ae jy" href="https://medium.com/intuitionmachine/ego-motion-in-self-aware-deep-learning-91457213cfc4" rel="noopener">https://medium . com/intuition machine/ego-motion-in-self-aware-deep-learning-91457213 CFC 4</a></li><li id="6077" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">北达科他州哈伯市、姆罗卡特区、飞飞市和亚明斯特区(2018 年)。基于好奇心的内在动机的结构化行为的出现。Arxiv.org。检索于 2018 年 6 月 17 日，来自 https://arxiv.org/abs/1802.07461<a class="ae jy" href="https://arxiv.org/abs/1802.07461" rel="noopener ugc nofollow" target="_blank"/></li><li id="6865" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">婴儿床里的科学家。(2018).YouTube。检索于 2018 年 6 月 17 日，来自 https://www.youtube.com/watch?v=HnRVbWsqHLw<a class="ae jy" href="https://www.youtube.com/watch?v=HnRVbWsqHLw" rel="noopener ugc nofollow" target="_blank"/></li><li id="0aee" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">(2018).Arxiv.org。检索于 2018 年 6 月 17 日，来自<a class="ae jy" href="https://arxiv.org/pdf/1802.07442.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1802.07442.pdf</a></li><li id="fbc1" class="mm mn iq kb b kc mv kg mw kk mx ko my ks mz kw mr ms mt mu bi translated">团结。(2018).团结。检索于 2018 年 6 月 18 日，来自<a class="ae jy" href="https://unity3d.com/" rel="noopener ugc nofollow" target="_blank">https://unity3d.com/</a></li></ol></div></div>    
</body>
</html>