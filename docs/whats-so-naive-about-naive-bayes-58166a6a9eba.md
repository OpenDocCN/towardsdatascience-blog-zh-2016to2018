# 朴素贝叶斯有什么幼稚的？

> 原文：<https://towardsdatascience.com/whats-so-naive-about-naive-bayes-58166a6a9eba?source=collection_archive---------2----------------------->

![](img/0ed157c0ad253530ed76c0c91695261b.png)

Naive art

朴素贝叶斯(NB)是“朴素”的，因为它假设测量的特征是相互独立的。这是天真的，因为它(几乎)永远不会是真的。以下是 NB 无论如何都管用的原因。

NB 是一个非常直观的分类算法。它会问这样一个问题，"*考虑到这些特征，这个测量值属于 A 类还是 B 类？*"，并通过将属于 A 类的具有相同特征的所有先前测量的比例乘以 A 类中所有测量的比例来回答它。如果该数字大于 B 类的相应计算，则我们说该测量属于 A 类。简单，对吗？

当然，在实践中，我们很少会看到许多测量具有相同的特征集。事实上，如果我们必须依赖一个测量值来与一些先前测量的数据点相同，我们将只能对完全相同的数据点进行分类，这使得贝叶斯规则实际上对分类毫无用处。

现在，如果我们做一个*天真的*假设，所有的特征都是相互独立的，那么我们就不需要依靠训练数据集中的精确副本来进行分类。我们可以简单地分别获取每个特征，并确定属于 A 类的先前测量值的比例，这些测量值仅对此特征具有相同的值。然后，我们对所有其他功能进行同样的操作，并获得产品。我们再次将其乘以 A 类在数据集中的比例，看看这个数字是否大于我们对 b 类进行相应计算的结果。这是作弊，但确实有效。

NB 的伟大之处在于天真的假设实际上有助于分类。可以这样想:如果两个特征实际上是相关的，比如说，头发长度和性别，那么假设它们是独立的就意味着你要重复计算证据。如果性别和长头发与成为贾斯汀比伯的粉丝有更多的联系，那么假设独立会让你更加确定她是一个信仰者。或许有点天真。