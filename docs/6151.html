<html>
<head>
<title>TD in Reinforcement Learning, the Easy Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TD 在强化学习中，最简单的方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce?source=collection_archive---------4-----------------------#2018-11-28">https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce?source=collection_archive---------4-----------------------#2018-11-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="04ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">更新:学习和练习 TD 方法的最好方式是去<a class="ae ko" href="http://rl-lab.com/gridworld-td" rel="noopener ugc nofollow" target="_blank">http://rl-lab.com/gridworld-td</a></p><p id="d26e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设你正在驾驶装有 GPS 的汽车。在旅程开始时，GPS 会给你一个到达时间的估计值(基于统计数据)，当你开车遇到交通堵塞时(或者没有)，它会改进估计值并给你其他到达时间。</p><p id="971d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您会注意到，在旅程的每一段，您都会得到一些关于到达时间的估计。</p><p id="2525" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在假设你的全球定位系统没有给你任何估计，但存储的数据，直到你到达，然后给你一个详细的报告，每段路花了多少时间。这对你有用吗？</p><p id="bbe8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">答案会是:这取决于你想做什么。<br/>但肯定的是，你会感谢早期的反馈，即使不是很准确。</p><p id="ea32" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是蒙特卡罗和时态差的区别。<br/>示例中的后一种方法是基于蒙特卡罗的，因为它会等到到达目的地后再计算行程各部分的估计值。而前者是时间差异。</p><p id="a4bd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">事实上，如果将蒙特卡罗(MC)方法和动态规划(DP)方法结合起来，就得到时间差分(TD)方法。</p><p id="4467" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="kp">注意:TD 将被记为 TD(0 ),这意味着它将向前看一步。TD(0)是 TD(n)的特例。</em></p><p id="70f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">回想一下，在 MC 中，我们播放一整集，直到结束，然后我们计算该集中出现的每个州的折扣奖励。我们做了大量的事件，然后我们平均每个状态的不同值。</p><p id="a814" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 DP 中，我们随机初始化所有状态，然后根据周围状态的(先前计算的)值迭代计算每个状态的值。我们一直这样做，直到我们注意到任何状态值都没有显著的改善。</p><h1 id="fe54" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">政策预测</h1><p id="2a74" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">我们已经看到，在 MC 中，我们播放一集直到结束，然后我们向后移动，给每个状态分配该集的贴现收益 G。但这意味着我们必须等到最后才能知道 G. <br/>的值，然而在 TD(0)中，我们根据下一个状态的<strong class="js iu"> <em class="kp">估计值</em> </strong>来更新当前状态。还记得 GPS 的例子吗，在某一点上，GPS 可能会注意到您的速度下降到 10Km/h，因此它会将其到达时间的估计值更新+30 分钟，但这可能是一个非常短暂的减速，几分钟后您会再次加速，GPS 会将其估计值更新-20 分钟。<br/>与 TD(0)相同，V(s)根据以下公式更新:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/8f75ff90beabf722a32ba1cccd0c01b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*s80b7_pfTReXENYNaCUK4g.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">where α is the step size ∈ ]0,1], 𝛄 is the discount factor</figcaption></figure><p id="2a47" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是一个增量平均计算。查看文章末尾的“增量平均计算”了解详情。</p><p id="452c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">很明显，仅基于一个事件，对 V(s)的估计是不准确的。和你那天的汽车旅行一样！只做一次，你不会很好地估计总时间和每一部分的时间。也许那天你是幸运的，没有交通堵塞，或者相反，你是不幸的，由于车祸你被困在一个不寻常的堵塞中。<br/> <strong class="js iu">但是如果你每天都这样做(多放几集)，你就能每天都精确你的估计。</strong></p><p id="9170" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">TD(0)中策略评估的算法(伪代码)如下:</p><pre class="lu lv lw lx gt mf mg mh mi aw mj bi"><span id="b668" class="mk kr it mg b gy ml mm l mn mo">Evaluate_Policy(policy):<br/>  randomly_initialize_non_terminal_states_values()</span><span id="5675" class="mk kr it mg b gy mp mm l mn mo">Loop number_of_episodes:<br/>  let s = start_state()</span><span id="be69" class="mk kr it mg b gy mp mm l mn mo">  # Play episode until the end<br/>  Loop until game_over():</span><span id="8756" class="mk kr it mg b gy mp mm l mn mo">    let a = get_action(policy, s, 0.1) <br/>                      # get action to perform on state s according <br/>                      # to the given policy 90% of the time, and a<br/>                      # random action 10% of the time.</span><span id="f056" class="mk kr it mg b gy mp mm l mn mo">    let (s', r) = make_move(s, a) #make move from s using a and get <br/>                                  #the new state s' and the reward r<br/></span><span id="499c" class="mk kr it mg b gy mp mm l mn mo">     # incrementally compute the average at V(s). Notice that V(s)<br/>     # depends on an estimate of V(s') and not on the return <br/>     # G as in MC <br/>     let V(s) = V(s) + alpha * [r + gamma * V(s') - V(s)]</span><span id="67e5" class="mk kr it mg b gy mp mm l mn mo">    let s = s'</span><span id="20b4" class="mk kr it mg b gy mp mm l mn mo"> End Loop<br/>End Loop</span></pre><h1 id="0b86" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">政策控制</h1><p id="6151" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">TD(0)中的策略控制有两种实现:<strong class="js iu"> <em class="kp"> SARSA </em> </strong>和<strong class="js iu"> <em class="kp"> Q-Learning </em> </strong>。</p><p id="e7a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="kp"> SARSA </em> </strong>是一个<strong class="js iu"> <em class="kp"> On-Policy </em> </strong>方法，这意味着它根据某个策略来计算<strong class="js iu"> <em class="kp"> Q 值</em> </strong>，然后代理遵循该策略。</p><p id="a5fb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="kp"> Q-Learning </em> </strong>是一种<strong class="js iu"> <em class="kp"> Off-Policy </em> </strong>方法。它包括根据贪婪策略计算<strong class="js iu"> <em class="kp"> Q 值</em> </strong>，但是代理不一定遵循贪婪策略。</p><h2 id="ccf6" class="mk kr it bd ks mq mr dn kw ms mt dp la kb mu mv le kf mw mx li kj my mz lm na bi translated">萨尔萨</h2><p id="eaf8" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">像往常一样，当执行动作时，你需要计算动作-状态函数(<strong class="js iu"> <em class="kp"> Q 值</em> </strong>)，因为它将状态和动作映射到估计。<br/>在<a class="ae ko" href="https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511" rel="noopener">蒙特卡洛</a>的文章中，我们解释了为什么单独的<strong class="js iu"><em class="kp">【V(s)】</em></strong>无助于确定最优政策(计划，或在每个状态下采取的行动)。<br/>因此，假设我们处于状态<strong class="js iu"> <em class="kp"> s </em> </strong>并且我们想要基于状态<strong class="js iu"> <em class="kp"> s </em> </strong>和动作<strong class="js iu"><em class="kp"/></strong>来计算<strong class="js iu"> <em class="kp"> Q 值</em> </strong>，正如我们之前看到的，TD(0)使用增量平均值来计算任何状态的值。这个平均计算，用下一个状态的值来表示。<br/>既然我们在计算<strong class="js iu"> <em class="kp"> Q 值</em> </strong>，那么我们就得到了下一个状态<strong class="js iu"><em class="kp">【s’</em></strong>的<strong class="js iu"> <em class="kp"> Q 值</em> </strong>。然而<strong class="js iu"> <em class="kp"> Q </em> </strong>需要状态和动作两个参数。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/b52d8f65adbb3424f5a896b9eec0b708.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*w80Z80WdgPm-4uYquWVhvg.png"/></div></figure><p id="00be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="kp"> SARSA </em> </strong>解决这个问题的方法是，为了得到<strong class="js iu"> <em class="kp"> Q 值，</em> </strong>在状态<strong class="js iu"><em class="kp">【s’</em></strong>选择一个动作<strong class="js iu"><em class="kp">【a’</em></strong>(基于ε贪婪方法)，然后当代理到达<strong class="js iu"><em class="kp">【s’</em></strong>时，我们将执行动作<strong class="js iu"><em class="kp">【a’</em></strong>。</p><p id="70b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下图给出了一个 SARSA 示例。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nc"><img src="../Images/bc5e97a39bc042eef3fb6aa0c4d9b22c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fDNG8FOhPb7_0xmjHy3wiw.png"/></div></div></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/8f1d8a5a21da1130ec973a0847ec3ff8.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/1*13Uj03tfOmlmi7M4traVJg.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Four actions per state: North, South, West, East</figcaption></figure><p id="fa3d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在左侧网格中，代理处于状态<strong class="js iu"> <em class="kp"> s </em> </strong>，它计算向北移动的值(蓝色箭头)，为了能够进行计算，它需要向东移动的<strong class="js iu"> <em class="kp"> Q 值</em></strong><strong class="js iu"><em class="kp">s’</em></strong>(灰色箭头)。<br/>右边的网格显示当代理移动到状态<strong class="js iu"><em class="kp">‘s’</em></strong>时，它遵循策略先前决定的动作，并计算向东(蓝色箭头)动作的<strong class="js iu"> <em class="kp"> Q 值</em></strong>…</p><p id="b672" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下是<strong class="js iu"> <em class="kp"> SARSA </em> </strong>的伪代码:</p><pre class="lu lv lw lx gt mf mg mh mi aw mj bi"><span id="3247" class="mk kr it mg b gy ml mm l mn mo">SARRA():<br/>  #initialization<br/>  for each state s in AllNonTerminalStates:<br/>     for each action a in Actions(s):<br/>         Q(s,a) = random()<br/>  for each s in TerminalStates:<br/>      Q(s,_) = 0 #Q(s) = 0 for all actions in s</span><span id="9bf7" class="mk kr it mg b gy mp mm l mn mo">  Loop number_of_episodes:<br/>    let s = start_state()</span><span id="089b" class="mk kr it mg b gy mp mm l mn mo">    # get action to perform on state s according <br/>    # to the given policy 90% of the time, and a<br/>    # random action 10% of the time.    <br/>    let a = get_epsilon_greedy_action(s, 0.1)</span><span id="6aff" class="mk kr it mg b gy mp mm l mn mo">    # Play episode until the end<br/>    Loop until game_over():</span><span id="c714" class="mk kr it mg b gy mp mm l mn mo">       # make move from s using a and get the new state s'<br/>       # and the reward r<br/>       let (s', r) = make_move(s, a)</span><span id="349a" class="mk kr it mg b gy mp mm l mn mo">      # choose action to perform on state s'<br/>      # a' will be used executed in the next iteration<br/>      # but for the moment it will be used to get Q(s', a')<br/>      let a' = get_epsilon_greedy_action(s', 0.1)</span><span id="3701" class="mk kr it mg b gy mp mm l mn mo">     # incrementally compute the average at Q(s,a)<br/>     let Q(s, a) = Q(s, a) + alpha*[r + gamma * Q(s', a') - Q(s, a)]</span><span id="4c7e" class="mk kr it mg b gy mp mm l mn mo">     let s = s'  # move to the next state<br/>     let a = a'  # use the same action a' as determined above</span><span id="b45c" class="mk kr it mg b gy mp mm l mn mo">    End Loop<br/>  End Loop</span></pre><h2 id="5995" class="mk kr it bd ks mq mr dn kw ms mt dp la kb mu mv le kf mw mx li kj my mz lm na bi translated">q 学习</h2><p id="b25d" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated"><strong class="js iu"> <em class="kp"> Q-learning </em> </strong>与<strong class="js iu"><em class="kp"/></strong>类似，只是在计算<strong class="js iu"><em class="kp">【s，a】</em></strong>Q(s，a)<strong class="js iu"><em class="kp">【s’，a’</em></strong>时，它使用贪婪策略从下一个状态<strong class="js iu"><em class="kp">s’</em></strong>中确定<em class="kp">Q(s’，a’</em>。<br/>记住贪婪策略选择给出最高<br/> <strong class="js iu"> <em class="kp"> Q 值</em> </strong>的动作。然而，这一点很重要，它不一定遵循那种贪婪的政策。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/c649ef3d85de2baf51f067d5bba643c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*Aq-UZZ0QtkAal3rLF_08Zw.png"/></div></figure><p id="1751" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个形象的打击说明了<strong class="js iu"> <em class="kp"> Q-Learning </em> </strong>的机理:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nc"><img src="../Images/4fd32733eb5f853989fef9784045d8b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*12Qa9TcWGRXIR9PeomRrbA.png"/></div></div></figure><p id="9b06" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">左侧网格显示了处于状态<strong class="js iu"> <em class="kp"> s </em> </strong>的代理在向北行驶时计算 Q 值(蓝色箭头)。为此，它在计算中使用由状态 s(橙色箭头)的贪婪策略确定的<strong class="js iu"> <em class="kp"> Q 值</em> </strong>。<br/>右边的网格显示代理移动到状态<strong class="js iu"><em class="kp">‘s’</em></strong>，但不一定遵循贪婪策略确定的动作(橙色箭头)，而是选择随机动作(蓝色箭头)。</p><p id="bf1f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="kp"> Q-learning </em> </strong>的算法如下:</p><pre class="lu lv lw lx gt mf mg mh mi aw mj bi"><span id="2518" class="mk kr it mg b gy ml mm l mn mo"><br/>QLearning():<br/>  #initialization<br/>  for each state s in AllNonTerminalStates:<br/>     for each action a in Actions(s):<br/>         Q(s,a) = random()<br/>  for each s in TerminalStates:<br/>      Q(s,_) = 0 #Q(s) = 0 for all actions in s</span><span id="4629" class="mk kr it mg b gy mp mm l mn mo">  Loop number_of_episodes:<br/>    let s = start_state()</span><span id="1b99" class="mk kr it mg b gy mp mm l mn mo">    # Play episode until the end<br/>    Loop until game_over():</span><span id="5008" class="mk kr it mg b gy mp mm l mn mo">      # get action to perform on state s according <br/>      # to the given policy 90% of the time, and a<br/>      # random action 10% of the time.    <br/>      let a = get_epsilon_greedy_action(s, 0.1)</span><span id="a6ca" class="mk kr it mg b gy mp mm l mn mo">      # make move from s using a and get the new state s'<br/>      # and the reward r<br/>      let (s', r) = make_move(s, a)</span><span id="3034" class="mk kr it mg b gy mp mm l mn mo">      # choose the max Q-value (qmax) on state s'<br/>      let qmax = get_max_qvalue_on_state(s')</span><span id="18d2" class="mk kr it mg b gy mp mm l mn mo">      # incrementally compute the average at Q(s,a)<br/>      let Q(s, a) = Q(s, a) + alpha*[r + gamma * qmax - Q(s, a)]</span><span id="490c" class="mk kr it mg b gy mp mm l mn mo">     let s = s'  # move to the next state</span><span id="2214" class="mk kr it mg b gy mp mm l mn mo">   End Loop<br/>End Loop</span></pre><h1 id="6a05" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">增量平均计算</h1><p id="2d78" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">这一段说明了增量平均值的计算方法。<br/>平均值的项以既有 A(n+1)又有 A(n)的方式排列。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e32b8fb231783b18b6b8ad37ed90b057.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*ESn4_Qqd3qlafNHQh8x5yw.jpeg"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Incremental Average Computation</figcaption></figure><p id="db47" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请注意，1/(n+1)表示状态值和动作值函数中的α项。</p><h1 id="cf0b" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">结论</h1><p id="bc7c" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">时间差异比动态规划方法更好，因为它不需要环境的模型，也不需要报酬和概率分布。TD 也比蒙特卡罗方法有优势，因为不需要等到一集结束才知道回报，只需要一个时间步。</p><h1 id="668b" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">相关文章</h1><ul class=""><li id="316a" class="nk nl it js b jt lo jx lp kb nm kf nn kj no kn np nq nr ns bi translated"><a class="ae ko" href="https://medium.com/@zsalloum/revisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182" rel="noopener">开发人员强化学习政策</a></li><li id="6320" class="nk nl it js b jt nt jx nu kb nv kf nw kj nx kn np nq nr ns bi translated"><a class="ae ko" href="https://medium.com/p/9350e1523031" rel="noopener">强化学习中的 Q vs V，最简单的方法</a></li><li id="b9e3" class="nk nl it js b jt nt jx nu kb nv kf nw kj nx kn np nq nr ns bi translated"><a class="ae ko" href="https://medium.com/p/1b7ed0c030f4" rel="noopener">数学背后的强化学习，最简单的方法</a></li><li id="6318" class="nk nl it js b jt nt jx nu kb nv kf nw kj nx kn np nq nr ns bi translated"><a class="ae ko" href="https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511" rel="noopener">蒙特卡洛强化学习，简单易行</a></li><li id="e2ca" class="nk nl it js b jt nt jx nu kb nv kf nw kj nx kn np nq nr ns bi translated"><a class="ae ko" href="https://medium.com/@zsalloum/dynamic-programming-in-reinforcement-learning-the-easy-way-359c7791d0ac" rel="noopener">动态编程在强化学习中的简便方法</a></li></ul></div></div>    
</body>
</html>