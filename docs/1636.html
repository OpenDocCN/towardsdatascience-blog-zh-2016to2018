<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://towardsdatascience.com/gaussian-discriminant-analysis-an-example-of-generative-learning-algorithms-2e336ba7aa5c?source=collection_archive---------1-----------------------#2017-09-29">https://towardsdatascience.com/gaussian-discriminant-analysis-an-example-of-generative-learning-algorithms-2e336ba7aa5c?source=collection_archive---------1-----------------------#2017-09-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><p id="25f8" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">高斯判别分析<br/>生成学习算法的一个例子</p><figure class="jo jp jq jr gt js gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/f195cb6f20f39a0c465138306a65ebd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*51e4Y6kZIEi-iKiuI0_wqQ.jpeg"/></div></figure><p id="7061" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">生成学习算法:<br/>在<a class="ae jv" href="https://medium.com/@paragradke/simple-linear-regression-2421076a5892" rel="noopener">线性回归</a>和<a class="ae jv" href="https://medium.com/@paragradke/logistic-regression-2b555e5f80e6" rel="noopener">逻辑回归</a>中我们都建模了给定x的y的条件分布，如下。</p><figure class="jo jp jq jr gt js gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi jw"><img src="../Images/a22958d68bdabb97738bd9a0eb0cdbf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*krT9h3D-fiagy1ssurPCDA.png"/></div></div></figure><p id="2002" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">直接从训练集中建模p(y|x)的算法被称为<strong class="ir kb">判别算法</strong>。<br/>同样的问题可以有不同的解决方法，考虑同样的二进制分类问题，我们希望学习根据一些特征区分两个类别，类别A (y=1)和类别B (y=0)。现在，我们采用标签为A的所有示例，并尝试学习功能，为A类构建一个模型。然后，我们采用标签为B的所有示例，并尝试学习它的功能，为B类构建一个单独的模型。最后，为了对新元素进行分类，我们将它与每个模型进行匹配，并查看哪一个更适合(为概率生成高值)。在这种方法中，我们尝试对p(x|y)和p(y)建模，而不是我们之前所做的p(y|x ),这被称为<strong class="ir kb">生成学习算法</strong>。<br/>一旦我们使用训练集学习了模型p(y)和p(x|y ),我们就使用贝叶斯规则来导出p(y|x)为</p><figure class="jo jp jq jr gt js gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi jw"><img src="../Images/16c83d83b2b608f0de5f373ea77057dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fD5M2fz-h3LSJpu9eknTyg.png"/></div></div></figure><p id="93cb" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir kb">高斯判别分析模型<br/> </strong>当我们遇到输入特征为连续随机变量的分类问题时，我们可以使用GDA，它是一种生成学习算法，我们假设p(x|y)按照<strong class="ir kb">多元正态分布</strong>分布，p(y)按照<strong class="ir kb">伯努利</strong>分布。所以这个模型是</p><figure class="jo jp jq jr gt js gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi kc"><img src="../Images/864ccb430df817ed64ede5a29a35f19c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Eiy_EsrtbPwaTJNooDdAtA.png"/></div></div></figure><p id="6e54" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">现在，正如我们在<a class="ae jv" href="https://medium.com/@paragradke/simple-linear-regression-2421076a5892" rel="noopener">线性回归</a>和<a class="ae jv" href="https://medium.com/@paragradke/logistic-regression-2b555e5f80e6" rel="noopener">逻辑回归</a>中所做的，我们需要定义对数似然函数<strong class="ir kb"> L </strong>，然后通过最大化<strong class="ir kb"> L </strong>的模型参数，找到最大似然参数。</p><figure class="jo jp jq jr gt js gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi kd"><img src="../Images/62c5cf2c3856db04d556e449114c3d9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mw42sVL7b9rKl6qcQpdYew.png"/></div></div></figure><figure class="jo jp jq jr gt js gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi jw"><img src="../Images/442a60b6b91d99a58d479e0b0bcd9fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FXkiaZ8d1pBEfSkP58tSYg.png"/></div></div></figure><figure class="jo jp jq jr gt js gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi kd"><img src="../Images/8d51437aad2b9735b3ad8bc29e54e388.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FItEShlB9YO1f5lLx6Vvrw.png"/></div></div></figure><figure class="jo jp jq jr gt js gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi kd"><img src="../Images/f1e78bd8c591099a7a7ce4bbadd43d49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mImNZxXtGT98KrjqHFTOPg.png"/></div></div></figure><p id="b3c5" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">因此，等式(2)、等式(4)和等式(5)将GDA的所有最大似然参数定义如下</p><figure class="jo jp jq jr gt js gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi ke"><img src="../Images/c56345d8e386dced4ae2c82447c588aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NeeSLnPEoPI8f0p6Y0oCcQ.png"/></div></div><figcaption class="kf kg gj gh gi kh ki bd b be z dk">Model parameters for GDA</figcaption></figure><p id="8a29" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">例如:在这个例子中，我使用了一个定制的虹膜数据集(减少了特征以适应二维，并从数据集中删除了第三类以进行二进制分类)。数据图看起来像</p><figure class="jo jp jq jr gt js gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi kj"><img src="../Images/9c429ae645a590205eef61b4296540b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CKLTHuYnUts8VSTI3CM5JQ.png"/></div></div></figure><figure class="jo jp jq jr gt js"><div class="bz fp l di"><div class="kk kl l"/></div></figure><p id="94f3" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">对于<a class="ae jv" href="https://gist.github.com/paragradke/4044ad2d567f6b0cd62888afb104c177" rel="noopener ugc nofollow" target="_blank">数据集</a>，计算的模型参数如下</p><figure class="jo jp jq jr gt js gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi jw"><img src="../Images/bb092e7745c699e41ab931073f571d65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pzMbEp3tLVRFFnx1pxKTSw.png"/></div></div></figure><p id="9412" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">基于该算法，每个模型的概率密度图看起来如下</p><figure class="jo jp jq jr gt js gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi km"><img src="../Images/ed3f928a40a589b5a2a48700167de6ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5guogPhnt2vIzEm17JGS-Q.png"/></div></div><figcaption class="kf kg gj gh gi kh ki bd b be z dk">3D Surface Plot for p(x|y=1)</figcaption></figure><figure class="jo jp jq jr gt js gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi kn"><img src="../Images/6bcc96d3268a0fc2d973b5992c3f9a95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5Rm6u6aN_-p90QopZW25w.png"/></div></div><figcaption class="kf kg gj gh gi kh ki bd b be z dk">3D Surface Plot for p(x|y=0)</figcaption></figure><p id="400e" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">从它们的等高线图中，你可以看到模型解释了测试数据。模型的表面并不光滑(因为假设p(x|y)是高斯的对于这个特定的试验数据并不完全正确。并且将讨论它如何影响模型的精度),因此轮廓也将不是平滑的而是有噪声的。</p><figure class="jo jp jq jr gt js gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi ko"><img src="../Images/097ce872fdd45a74ab6e628a24db86b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y1X1iYz14FLUpwkqD8uCRA.png"/></div></div><figcaption class="kf kg gj gh gi kh ki bd b be z dk">Contour Plot for p(x|y=0) with data points for class y=0</figcaption></figure><figure class="jo jp jq jr gt js gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi kp"><img src="../Images/42da71350a0d0aaa45c78ffde1370415.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jxpSP8Nq3jSJc6z_UbVReQ.png"/></div></div><figcaption class="kf kg gj gh gi kh ki bd b be z dk">Contour Plot for p(x|y=1) with data points for class y=1</figcaption></figure><p id="3940" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">让我们用两个测试点来测试我们的算法<br/><strong class="ir kb">x0 =【4，4】<br/>x1 =【6.5，2.25】</strong></p><figure class="jo jp jq jr gt js"><div class="bz fp l di"><div class="kk kl l"/></div></figure><p id="34c3" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">您可以看到，正如预期的那样，该算法支持<strong class="ir kb"> x0 </strong>的<strong class="ir kb">类0 </strong>和<strong class="ir kb"> x1 </strong>的<strong class="ir kb">类1 </strong>。用于分类的<a class="ae jv" href="https://medium.com/@paragradke/logistic-regression-2b555e5f80e6" rel="noopener">逻辑回归</a>和高斯判别分析都将给出略微不同的<strong class="ir kb">决策边界</strong>因此使用哪一个以及何时使用。<br/> GDA对p(x|y=k)的概率分布做了一个假设，其中k是其中一个类。而且很容易证明，对于GDA，如果关于p(x|y=k) ( <strong class="ir kb">高斯</strong>)和p(y)( <strong class="ir kb">伯努利)</strong>分布的初始假设为真，那么p(y|x)可以表示为<strong class="ir kb"> Sigmoid。</strong>但反之则不然。<strong class="ir kb">这意味着GDA对数据集做出比</strong> <a class="ae jv" href="https://medium.com/@paragradke/logistic-regression-2b555e5f80e6" rel="noopener"> <strong class="ir kb">逻辑回归</strong> </a> <strong class="ir kb">更具体的假设，如果这些假设为真，那么它比LR </strong>更有效。当您更好地了解您正在处理的数据集的性质并且该数据集是高斯数据集，并且您没有大型训练集时，这一事实会很有用，在这种情况下，GDA会比LR <strong class="ir kb">执行得更好。但是另一方面，LR做出了更一般的假设，并且在特征集的概率分布不是高斯分布的许多其他地方可能更有用。</strong></p></div></div>    
</body>
</html>