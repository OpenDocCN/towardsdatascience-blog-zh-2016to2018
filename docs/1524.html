<html>
<head>
<title>Deep Learning II L9: Generative Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习II L9:生成模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-ii-l9-generative-models-dcd599ad6e0b?source=collection_archive---------4-----------------------#2017-09-14">https://towardsdatascience.com/deep-learning-ii-l9-generative-models-dcd599ad6e0b?source=collection_archive---------4-----------------------#2017-09-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/1ea1db97f96903c4bbf41095825c23ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KXKzaDsXKq4bj2hbQpwSWw.jpeg"/></div></div></figure><p id="1aab" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="http://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai </a>的深度学习课程中的<a class="ae kw" href="http://course.fast.ai/lessons/lesson9.html" rel="noopener ugc nofollow" target="_blank">第九课</a>继续从制作艺术的角度深入到生成对抗网络(GANs)中。这主要涉及(截至编写时)课程库中的<a class="ae kw" href="https://github.com/fastai/courses/blob/master/deeplearning2/neural-style.ipynb" rel="noopener ugc nofollow" target="_blank">神经风格</a>和<a class="ae kw" href="https://github.com/fastai/courses/blob/master/deeplearning2/neural-sr.ipynb" rel="noopener ugc nofollow" target="_blank">神经-sr </a> Jupyter笔记本。<em class="kx">注</em>:这篇帖子比较长。从简单开始，不断深入。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="8cc6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在<a class="ae kw" href="http://course.fast.ai/lessons/lesson8.html" rel="noopener ugc nofollow" target="_blank">第8课</a>中，我们学习了获取一幅图像，将它放入由风格损失和内容损失组成的损失函数中，输出损失和梯度，并使用这些梯度来更新图像并在下一次迭代中减少损失——冲洗并重复。像这样:</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lf"><img src="../Images/687377b9c37cb3efc17a668136fbabd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OP7fNtVuU3xuIlmpyOOiPQ.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">copyright: my notebook …</figcaption></figure><p id="c434" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">根据杰瑞米·霍华德在讲座中的说法，这是对许多不同的风格和图像进行风格转换的最好方法，比如在web应用程序中。然而，如果你想应用一个单一的风格到任何图像:这是我们可以改变事情做得更好的地方。原因是你不需要做上面的优化循环来产生一些东西。相反，你可以只训练CNN来学习如何以特定的风格输出图像(并且通过CNN向前传递是非常快的——权衡是CNN被训练成特定的风格)。</p><p id="809b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这一次我们开始像以前一样将多个图像输入到同一个损失函数中(样式+内容)。在这个例子中，对于风格损失，我们使用梵高的虹膜，对于内容损失，我们使用当前输入到函数中的图像。这里最大的不同是，我们将在输入图像和损失函数之间放置一个CNN。CNN将学习输出一个图像，这样当它进入损失函数时，它将输出一个小的损失值。在英语中，这意味着CNN已经学会制作内容与原始输入图像相似、风格与所选风格相似的图像。</p><p id="d94a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里有一些注释。有了CNN，你可以选择任何你喜欢的损失函数(你的里程可能会有所不同)。<a class="ae kw" href="http://course.fast.ai/" rel="noopener ugc nofollow" target="_blank">深度学习I </a> &amp; <a class="ae kw" href="http://course.fast.ai/part2.html" rel="noopener ugc nofollow" target="_blank"> II </a>到目前为止已经使用了相当简单的，比如<a class="ae kw" href="https://en.wikipedia.org/wiki/Mean_squared_error#Loss_function" rel="noopener ugc nofollow" target="_blank"> MSE </a>和<a class="ae kw" href="http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function" rel="noopener ugc nofollow" target="_blank">交叉熵</a>。不过，这里我们将使用样式+内容损失，就像上面的第一个例子一样。<em class="kx">注意</em>:因为样式+内容损失函数是由神经网络生成的:它是可微的；并且可以优化任何可微的损失函数。</p><p id="b52b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，如果我们采用相对于CNN权重的输出梯度，我们可以使用它们来更新这些权重，这样CNN就可以更好地将图像转变为与风格相匹配的图像。</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lq"><img src="../Images/80e1849fef5774426aaedd78ebb9743b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NhdZFwBp2Lt95Ja1dNOH2Q.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">something like this</figcaption></figure><p id="3a57" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这一点的强大之处在于，没有优化步骤来减缓推理——或者模型的“预测”操作。通过CNN的单次向前传递几乎是瞬时的。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="6725" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你还能做更多。</p><figure class="lg lh li lj gt jr"><div class="bz fp l di"><div class="lr ls l"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">the future is here. ENHANCE.</figcaption></figure><p id="6ea0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">事实证明，深度学习神经网络可以用来放大图像和推断细节。</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lt"><img src="../Images/cfd1d0a66959832ac32abcd5538c4e4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0ghl-iJdCJvf8RrIlOCJtw.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">we have the technology</figcaption></figure><p id="79fa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们从在传说和雇佣图像之间放一个CNN开始。CNN将LoRes图像作为输入，并将其输出发送到仅计算内容损失的损失函数中。内容损失是通过将其从CNN的输入与从HiRes图像的激活进行比较来计算的。换句话说，损失函数是检查CNN是否从原始知识中创建了一个更大的图像，该图像与雇佣图像具有相同的激活。</p><p id="3292" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这场讨论引发了我在课堂上听到的最好的问题之一:</p><blockquote class="lu lv lw"><p id="5a43" class="jy jz kx ka b kb kc kd ke kf kg kh ki lx kk kl km ly ko kp kq lz ks kt ku kv ij bi translated">我们能不能在任何两个事物之间放一个CNN，它就能知道它们之间的关系？</p></blockquote><p id="9c36" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="lo lp ep" href="https://medium.com/u/34ab754f8c5e?source=post_page-----dcd599ad6e0b--------------------------------" rel="noopener" target="_blank">杰瑞米·霍华德</a>:</p><blockquote class="lu lv lw"><p id="51d7" class="jy jz kx ka b kb kc kd ke kf kg kh ki lx kk kl km ly ko kp kq lz ks kt ku kv ij bi translated">是的，绝对的。</p></blockquote><p id="255a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这就涉及到了我们与CNN之前不同的做法。一篇<a class="ae kw" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank"> 2016论文</a>详细介绍了CNN使用不同的损失函数来做超分辨率。在此之前，<a class="ae kw" href="https://en.wikipedia.org/wiki/Mean_squared_error#Loss_function" rel="noopener ugc nofollow" target="_blank"> MSE </a>通常用于升级网络像素输出和HiRes图像之间。问题是:模糊图像在<a class="ae kw" href="https://en.wikipedia.org/wiki/Mean_squared_error#Loss_function" rel="noopener ugc nofollow" target="_blank"> MSE </a>下仍然表现良好，但这对于图像任务来说有点不可接受。另一方面:</p><blockquote class="lu lv lw"><p id="64ea" class="jy jz kx ka b kb kc kd ke kf kg kh ki lx kk kl km ly ko kp kq lz ks kt ku kv ij bi translated">…如果你拿下VGG的第二或第三个conv街区，那么它需要知道<em class="iq">这是一个眼球</em>，否则它不会好看。</p></blockquote><p id="c1c3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">教训:使用内容损失而不是像素损失。<em class="kx">(题外话:从在像素上使用</em><a class="ae kw" href="https://en.wikipedia.org/wiki/Mean_squared_error#Loss_function" rel="noopener ugc nofollow" target="_blank"><em class="kx">MSE</em></a><em class="kx">到内容丢失，显然花了1年时间。回顾过去，发现直观事物所花费的时间是很有趣的。)</em></p><p id="b2ec" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">论文</a>列出了这种方法的一些有用应用——将它们框定为<em class="kx">图像变换</em>任务:</p><ul class=""><li id="20f5" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">图像处理:去噪、超分辨率、彩色化</li><li id="7494" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">计算机视觉:语义分割，深度估计</li></ul><p id="de63" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(<em class="kx">这里的另一个题外话是，使用生成模型，你可以通过添加噪声或使彩色图像灰度化等方式产生尽可能多的标签数据。)</em></p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mo"><img src="../Images/1f811c7923322f43f3555819b9c9166a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ppIssdsBMyuvxwfJiJQQ0g.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">high-level view of Content (Perceptual) Loss for Super Resolution or Style Transfer</figcaption></figure></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="27d9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi mp translated"><span class="l mq mr ms bm mt mu mv mw mx di"> 2。</span>现在进入代码。如果您想继续学习，请参见<a class="ae kw" href="https://github.com/fastai/courses/blob/master/deeplearning2/neural-style.ipynb" rel="noopener ugc nofollow" target="_blank"> neural-style.ipynb </a>(注意:课程的未来更新可能会完全更改代码或笔记本)。</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi my"><img src="../Images/15bf5e510318c2102626ccd32d5975c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QisWeJie5zqHebUyhYS-vQ.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">the final network — “The Upsampling Network”</figcaption></figure><p id="8329" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">上面的代码首先接收一批低分辨率图像，作为一个维度为<code class="fe mz na nb nc b">arr_lr.shape[1:]</code>(这里是:<code class="fe mz na nb nc b">(72, 72, 3)</code>)的张量。在笔记本中，<code class="fe mz na nb nc b">arr_lr</code>和<code class="fe mz na nb nc b">arr_hr</code>是NumPy数组。</p><p id="c9da" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe mz na nb nc b">inp = Input(arr_lr.shape[1:])</code></p><p id="49c5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该输入然后被放入具有64个大小为9的滤波器和单位步幅(即输入大小没有变化)。</p><p id="42f7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe mz na nb nc b">x = conv_block(inp, 64, 9, (1,1))</code></p><p id="345e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是一个大的过滤器尺寸，因为我们想要一个大的初始感受野。显然，拥有64个滤波器可以确保没有信息丢失(来自3个颜色通道)，但我不知道输入滤波器与滤波器的信息丢失曲线是什么样的。但显然这也变得越来越普遍。</p><p id="0615" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来是大部分计算将要完成的地方。这是GAN必须弄清楚的部分，例如，<em class="kx">那个黑点是眼球</em>，然后<em class="kx">一个更高分辨率的眼球看起来会是什么样子</em>。</p><p id="5c45" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe mz na nb nc b">for i in range(4): x = res_block(x)</code> # <code class="fe mz na nb nc b">res_block()</code>是在别处定义的<a class="ae kw" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> ResNet </a>块。</p><p id="7941" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在创成式模型中，我们希望以低分辨率完成这个主要的计算步骤。低分辨率意味着工作量更少，因此计算速度更快，但最重要的是:我们将有一个更大的感受域。为什么这很重要？你可以很容易地想象出一张放大版的某人脸部照片是什么样子，因为你知道脸是什么样子的。你知道那个黑色的东西是一只眼睛，而那个黑色的区域是它们下颚投下的阴影。你一开始就知道这些，因为你能一眼看到整张照片，并认出它是一张脸，而不是餐盘或抽象艺术作品。GAN必须做同样的事情。</p><p id="9a35" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">决定感受域的有几个因素:卷积层深度、滤波器大小和下采样(非单位步长、最大池等)。</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nd"><img src="../Images/2f9698b18c45fdf924fc1e5d0b5b6b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7WFhZUpQth9SR-KLK_b0Jw.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">hopefully this makes some amount of sense</figcaption></figure><p id="1ac3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">回到代码。ResNet块是这样定义的:</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/83024fda1193450007527c006f3d5e1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*I7H9GGWA3OmgmsbZgQPmyQ.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">ip: input; nf: num. filters</figcaption></figure><p id="8c7d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个块接受一个输入，用它来构建两个卷积块(通过<a class="ae kw" href="https://keras.io/getting-started/functional-api-guide/" rel="noopener ugc nofollow" target="_blank"> Keras的函数式API </a>)，然后将这些Conv块的结果添加回原始输入:<code class="fe mz na nb nc b">return merge([x, ip], mode='sum')</code>最后的Conv块没有激活(<code class="fe mz na nb nc b">act=False</code>)，因为最近的一篇论文(不确定是哪一篇)发现没有ResNet块通常会执行得更好。</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nf"><img src="../Images/111154edd5e3c71322954c90633a74c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RlbNIotYehatxsG-a9MVug.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">1st conv_block has ReLU activation; 2nd block no activ. ip is input. merge using sum operation.</figcaption></figure><p id="9a07" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从我目前对<a class="ae kw" href="https://youtu.be/UlnYEWXoxOY?t=600" rel="noopener ugc nofollow" target="_blank">深度残差网络</a>的理解来看，ResNet的优势在于它给了网络的每一层一些关于原始输入的上下文。在讲座中，<a class="lo lp ep" href="https://medium.com/u/34ab754f8c5e?source=post_page-----dcd599ad6e0b--------------------------------" rel="noopener" target="_blank">杰瑞米·霍华德</a>说，一堆ResNet块可以磨练他们需要做什么来完成一项任务(这里:放大图像)。</p><p id="ac62" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe mz na nb nc b">y = f(x) + x;</code> → <code class="fe mz na nb nc b">f(x) = y — x;</code>其中<code class="fe mz na nb nc b">y — x = the residual</code></p><p id="d03c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">那么，如果函数是输出y和输入x之差，即残差，这就给出了一些关于如何最小化损失函数的上下文信息？我将不得不及时了解更多关于那件事。</p><p id="c7a4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe mz na nb nc b">conv_block</code>相当标准:</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/62650b37d671598c7f1b6302b9cadc64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ogsVWV1MWXNvmqQyG1-pQw.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">a convolution, then batch-normalization, and optionally an activation</figcaption></figure><p id="5423" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这一点上，我们实际上需要改变图像的尺寸。最初的讲座使用了反卷积模块，执行<a class="ae kw" href="https://youtu.be/I-P363wSv0Q?t=3154" rel="noopener ugc nofollow" target="_blank">反卷积/转置/分数阶卷积</a>来重建图像的外观，如果它的当前版本(输入)是卷积的结果。简而言之:它是对(零)填充输入的卷积。如果你想深入了解，这是讲座中提到的链接。在课程的github存储库中，课程笔记本的当前版本、<a class="ae kw" href="https://github.com/fastai/courses/blob/master/deeplearning2/neural-style.ipynb" rel="noopener ugc nofollow" target="_blank"> neural-style.ipynb </a>以及<a class="ae kw" href="https://github.com/fastai/courses/blob/master/deeplearning2/neural-sr.ipynb" rel="noopener ugc nofollow" target="_blank"> neural-sr.ipynb </a>都使用一个定义为:</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/648e96b21feb6a100bef5a07f9e33efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KIVtvq-zGbbk8fcfS4A7Sg.png"/></div></div></figure><p id="d869" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果我理解正确的话，这是课程代码的一点逻辑发展。转置/分形步长/去卷积的一个问题是棋盘:根据步长，卷积滤波器重叠的地方会出现棋盘图案的假象。谷歌大脑和蒙特利尔大学的一些研究人员写了一篇关于这个问题的很棒的论文。</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/7d5f46c9ba084e933c16d6ac5907a093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kcGviYL1piLtunQlWVPnhA.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Figure from Odena, et al., “Deconvolution and Checkerboard Artifacts”, Distill, 2016. <a class="ae kw" href="http://doi.org/10.23915/distill.00003" rel="noopener ugc nofollow" target="_blank">http://doi.org/10.23915/distill.00003</a></figcaption></figure><p id="48d6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">简而言之:步长为2 — <code class="fe mz na nb nc b">stride=(2,2)</code> —意味着过滤器在扫描输入时以2个像素或单元为步长移动，<code class="fe mz na nb nc b">size=3</code>意味着过滤器是一个<code class="fe mz na nb nc b">3x3</code>正方形。看一下上图，问题就很明显了。纠正这个问题的一个方法是确保你的过滤器大小是你步幅的倍数。另一种方法是<em class="kx">根本不</em>使用去卷积。</p><p id="cf34" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">相反，该论文建议首先进行上采样，这本质上与最大池操作相反:例如，每个像素都成为同一像素的<code class="fe mz na nb nc b">2x2</code>网格。在常规卷积之后进行上采样不会产生棋盘效应。这看起来就像杰瑞米·霍华德在新版代码中所做的一样。酷毙了。</p><p id="3d89" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们再深入一点:原始代码使用了两个去卷积模块来放大低分辨率图像，从<code class="fe mz na nb nc b">72x72</code> → <code class="fe mz na nb nc b">144x144</code> → <code class="fe mz na nb nc b">288x288</code>。较新的代码通过两个上采样模块来实现这一点，每个模块执行一个<code class="fe mz na nb nc b">Upsampling2D()</code>操作，然后进行卷积和批量归一化。<code class="fe mz na nb nc b">UpSampling2D()</code>的<a class="ae kw" href="https://keras.io/layers/convolutional/#upsampling2d" rel="noopener ugc nofollow" target="_blank"> Keras文档</a>显示:</p><p id="ab34" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe mz na nb nc b">keras.layers.convolutional.UpSampling2D(size=(2,2), data_format=None)</code></p><p id="86ab" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对我们来说重要的是<code class="fe mz na nb nc b">size</code>参数:</p><blockquote class="lu lv lw"><p id="7c10" class="jy jz kx ka b kb kc kd ke kf kg kh ki lx kk kl km ly ko kp kq lz ks kt ku kv ij bi translated"><strong class="ka ir"> size </strong> : int，或者2个整数的元组。行和列的上采样因子。</p></blockquote><p id="1704" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以默认的行为是将图像的大小增加一倍，这就是为什么对<code class="fe mz na nb nc b">UpSampling2D()</code>的两次空调用实现了预期的效果。</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi my"><img src="../Images/b5ba31a5490b4f1cf6d7ca68583b5f43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uSE1Bie8xKnYBeU1ZQlI5g.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Just a reminder for what’s going on: the super-resolution network we’re building. inp is the original LoRes input tensor. outp is the HiRes output tensor. The Upsampling Network.</figcaption></figure><p id="7c33" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在放大之后，我们有一个最终的<code class="fe mz na nb nc b">9x9</code>卷积层，以便回到3个颜色通道。讲座中的解释是，在图像输出从64个过滤器或通道减少到3个时，我们希望大量的上下文(因此是<code class="fe mz na nb nc b">9x9</code>)能够做到这一点。因此，将前面的<code class="fe mz na nb nc b">(288, 288, 64)</code>张量通过最后的卷积层输出一个<code class="fe mz na nb nc b">(288, 288, 3)</code>张量:一个图像。最后一条<code class="fe mz na nb nc b">outp = Lambda(lambda x: (x+1) * 127.5)(x)</code>线需要将每个像素从之前的tanh激活中恢复到正确的<code class="fe mz na nb nc b">0,255</code>颜色强度等级。Tanh在范围<code class="fe mz na nb nc b">0±1</code>我觉得，所以→ <code class="fe mz na nb nc b">([-1:+1] + 1) * 127.5 = [0:255]</code>。<code class="fe mz na nb nc b">Lambda(..)</code>是一个Keras包装器，它将内部的所有东西都视为自定义层。<em class="kx">注意</em>:<a class="ae kw" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">感知损失论文的</a>作者之一在reddit上提到，他们在没有tanh激活和没有最终Lambda处理层的情况下运行网络，并获得了同样好或更好的结果(无链接)。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="91e4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在可以训练网络，但是我们仍然需要定义我们的损失函数。Ie:网络自我评估的标准是什么？我们首先将我们的上采样网络连接到VGG。VGG将仅被用作内容损失的损失函数。在将上采样网络的输出张量输入VGG之前，需要对其进行预处理(VGG模型中使用的归一化)。我们通过定义一个定制的Keras预处理层，并定义一个新的输出张量<code class="fe mz na nb nc b">outp_λ</code>作为预处理层运行<code class="fe mz na nb nc b">outp</code>的结果，来实现这一点:</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="0f98" class="nn no iq nc b gy np nq l nr ns">vgg_λ = Lambda(preproc)<br/>outp_λ = vgg_λ(outp)<br/># I just discovered code blocks in Medium :D (```)</span></pre><p id="3d46" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我们创建一个VGG16网络，并将其所有层设置为不可训练。这…有点重要。因为这个网络<em class="kx">是</em>我们的损失函数——我们的测量棒——它必须是静态的。想象一下如果千克或米改变了。如果你想看现场混乱，只要看看外汇市场。</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="c9b7" class="nn no iq nc b gy np nq l nr ns">shp = arr_hr.shape[1:]</span><span id="d79f" class="nn no iq nc b gy nt nq l nr ns">vgg_inp = Input(shp)<br/>vgg     = VGG16(include_top=False, input_tensor=vgg_λ(vgg_inp))<br/>for λ in vgg.layers: λ.trainable=False</span></pre><p id="bb84" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">VGG网络的输入张量形状<code class="fe mz na nb nc b">shp</code>是HiRes图像维度。于是:<code class="fe mz na nb nc b">shp = arr_hr.shape[1:]</code>和<code class="fe mz na nb nc b">vgg_inp = Input(shp)</code>。<code class="fe mz na nb nc b">arr_hr</code>是从磁盘上的bcolz压缩数组创建的NumPy数组，其维数(形状)为:(num-elements，dim1，dim2，dim3，…)。在我们的例子中是:(图像数量，高度，宽度，颜色通道)。所以取<code class="fe mz na nb nc b">arr_hr.shape[1:]</code>会返回一个形状张量:<code class="fe mz na nb nc b">(288, 288 , 3)</code>。<code class="fe mz na nb nc b">VGG16(..)</code>的<code class="fe mz na nb nc b">include_top=False</code>将省略网络末端的全连接分类块。这是VGG16 w/ Average-Pooling &amp;批处理规范化的fast.ai实现，位于:<a class="ae kw" href="https://github.com/fastai/courses/blob/master/deeplearning2/vgg16_avg.py" rel="noopener ugc nofollow" target="_blank"> vgg16_avg.py </a>(如果课程目录结构发生变化，链接可能会断开)。</p><p id="aadd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了获得我们想要的VGG网络的部分内容损失，有几个实现。最初的课堂讲授通过选择提取特征的单个块(特别是块2中的conv第2层)来创建单个模型。在课程存储库中，这已经发展成为从多个块中检索特征的功能。</p><p id="6d23" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">讲座的实施:</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="14b8" class="nn no iq nc b gy np nq l nr ns">vgg_content = Model(vgg_inp, vgg.get_layer('block2_conv2').output)<br/>vgg1 = vgg_content(vgg_inp)<br/>vgg2 = vgg_content(outp_λ)</span></pre><p id="2825" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">更新的课程repo在<a class="ae kw" href="https://github.com/fastai/courses/blob/master/deeplearning2/neural-sr.ipynb" rel="noopener ugc nofollow" target="_blank"> neural-sr.ipynb </a>中的实现:</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/059225d656a9b13699c9946440428f45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iy6aiK5day2LZOjgbo6ZyQ.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">note how similar the 1 &amp; l look. Wait, in this font which is an I and which is an l? → Making my case for the λ. =) vgg_content here is a <a class="ae kw" href="https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models" rel="noopener ugc nofollow" target="_blank">Multi-Output model</a> (covered in Deep Learning I)</figcaption></figure><p id="c675" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这两种情况下，我们都使用模型中相对较早的激活。在L8中，我们发现我们完全可以使用早期层激活来重建图像。然后我们使用Keras的功能API创建VGG输出的两个版本。这个Keras API的一个伟大之处在于，任何层(模型被视为层)都可以被视为一个函数。然后，我们可以将任何我们喜欢的张量传递到这个“函数”中，Keras将创建一个新的模型，将这两个部分连接在一起。因此<code class="fe mz na nb nc b">vgg2</code>是<code class="fe mz na nb nc b">outp</code>，底部是上采样网络，顶部是<code class="fe mz na nb nc b">vgg_content</code>，VGG16中我们希望用作损失函数的部分。</p><p id="6aee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">旁白:你可能会有点困惑。我不应该说<code class="fe mz na nb nc b">outp_λ</code>而不是<code class="fe mz na nb nc b">outp</code>吗？在讲座的代码中，它是<code class="fe mz na nb nc b">outp_λ</code>:这是经过预处理λ层后的输出张量。然而，在更新的课程代码中，该步骤包含在VGG16模型的初始化调用中。新的号召是:</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="3d17" class="nn no iq nc b gy np nq l nr ns">vgg = VGG16(include_top=False, input_tensor=Lambda(preproc)(vgg_inp))</span></pre></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="bb11" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，现在我们有两个版本的VGG图层输出。<code class="fe mz na nb nc b">vgg1</code>基于雇佣输入，而<code class="fe mz na nb nc b">vgg2</code>基于升级网络的输出(接受了知识输入)。我们将比较招聘目标图像和招聘汇总结果，因此两个张量都具有高分辨率形状(288，288，3)。需要明确的是:<code class="fe mz na nb nc b">vgg1</code>是雇佣内容/感知激活，<code class="fe mz na nb nc b">vgg2</code>是知识增采样内容/感知激活。</p><p id="573b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kx">注意:对于损失函数和最终模型的定义，原始讲座和更新的存储库代码之间存在显著差异。我将先浏览一下讲座版本，然后浏览一下新内容。</em></p><p id="79de" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来我们计算它们之间的均方误差。在Keras中，任何时候你想把某个东西放到一个网络中，你都需要把它变成一个层，而这是通过把它扔进一个<code class="fe mz na nb nc b">Lambda(..)</code>来完成的:</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="dcc7" class="nn no iq nc b gy np nq l nr ns">loss = Lambda(lambda x: K.sqrt(K.mean((x[0]-x[1])**2, (1,2))))([vgg1, vgg2])</span></pre><p id="1def" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最终模型将采用LoRes输入和HiRes输入作为其两个输入，并返回刚刚定义为其输出的损失函数:</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="c8ab" class="nn no iq nc b gy np nq l nr ns">m_final = Model([inp, vgg_inp], loss)</span></pre><p id="6d24" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，当您试图在Keras中适应事物时，它会假设您试图获取一些输出并使其接近目标。这里的损失是我们想要的实际损失函数；没有目标，我们只是希望它越小越好。由于我们的损失函数使用的是<a class="ae kw" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank"> MSE </a>，我们可以说我们的目标是零。然而，Keras中的目标是标签，所以每行(每个输入)都必须有一个标签，所以我们需要一个零数组:</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="66a2" class="nn no iq nc b gy np nq l nr ns">targ = np.zeros((arr_hr.shape[0], 128))</span></pre><p id="e51e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为什么每个图像有128个零？<a class="ae kw" href="https://youtu.be/I-P363wSv0Q?t=4298" rel="noopener ugc nofollow" target="_blank">讲座</a>回答:Lambda层，<code class="fe mz na nb nc b">loss</code>，对应目标数组有128个滤镜。128，因为该层包含了<code class="fe mz na nb nc b">vgg1</code>和<code class="fe mz na nb nc b">vgg2</code>，两者都有64个过滤器。</p><p id="d591" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我们使用Adam优化器和MSE损失编译模型，并对其进行拟合:</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="24b5" class="nn no iq nc b gy np nq l nr ns">m_final.compile('adam', 'mse')<br/>m_final.fit([arr_lr, arr_hr], targ, 8, 2, **pars)</span></pre><p id="8062" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kx">注意:</em> <code class="fe mz na nb nc b">pars</code>包含一对这样定义的参数变量:<code class="fe mz na nb nc b">pars = {'verbose': 0, 'callbacks': [TQDMNotebookCallback(leave_inner=True)]}</code> —这关闭了冗长性，并使用<a class="ae kw" href="https://pypi.python.org/pypi/tqdm" rel="noopener ugc nofollow" target="_blank"> TQDM </a>进行状态报告。</p><p id="96fc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在第一轮训练后，学习率有所降低(称为“学习率退火”):</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="376a" class="nn no iq nc b gy np nq l nr ns">K.set_value(m_final.optimizer.lr, 1e-4)<br/>m_final.fit([arr_lr, arr_hr], targ, 16, 2, **pars)</span></pre><p id="0548" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">训练之后，我们对模型中的上行采样网络感兴趣。一旦我们完成训练&amp;试图最小化损失函数，我们不再关心损失:我们关心的是来自上采样网络的输出图像。所以现在我们定义了一个模型，它接受知识输入<code class="fe mz na nb nc b">inp</code>并返回雇佣输出<code class="fe mz na nb nc b">outp</code>:</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="1b5e" class="nn no iq nc b gy np nq l nr ns">top_model = Model(inp, outp)</span></pre><p id="7336" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当我第一次为它做任务时，这让我有点困惑:<code class="fe mz na nb nc b">outp</code>在我们上面的训练步骤中已经被训练过了。事实上，如果你记得:<code class="fe mz na nb nc b">inp</code>和<code class="fe mz na nb nc b">outp</code>只是我们的上采样网络的输入层和输出激活。我们基本上建立了一台机器来对图像进行上采样，使用现有的模块(VGG16)来训练这台机器，方法是将它放在上面，然后在完成后断开该模块，现在我们有了一台经过训练的机器来对图像进行上采样。</p><p id="a66b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kx">现在</em>，我做这件事的方式与课程库中的方式有些不同。定义<code class="fe mz na nb nc b">vgg1</code>和<code class="fe mz na nb nc b">vgg2</code>后，训练前的部分如下:</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="242f" class="nn no iq nc b gy np nq l nr ns">def mean_sqr_b(diff):<br/>    dims = list(range(1, K.ndim(diff)))<br/>    return K.expand_dims(K.sqrt(K.mean(diff**2, dims)), 0)</span><span id="d994" class="nn no iq nc b gy nt nq l nr ns">w = [0.1, 0.8, 0.1]</span><span id="b634" class="nn no iq nc b gy nt nq l nr ns">def content_fn(x):<br/>    res = 0; n=len(w)<br/>    for i in range(n): res += mean_sqr_b(x[i]-x[i+n]) * w[i]<br/>    return res</span><span id="7cbd" class="nn no iq nc b gy nt nq l nr ns">m_sr = Model([inp, vgg_inp], Lambda(content_fn)(vgg1+vgg2)<br/>m_sr.compile('adam', 'mae')</span></pre><p id="ad01" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">还记得在VGG16中，新代码是如何使用多输出模型在不同的块上进行激活的吗？我们将对每个块对损失函数的相对影响进行加权:<code class="fe mz na nb nc b">w = [0.1, 0.8, 0.1]</code>表示<code class="fe mz na nb nc b">block1_conv2</code>的权重为10%，<code class="fe mz na nb nc b">block_2_conv2</code>的权重为80%，&amp;等等。</p><p id="a29c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的损失函数本身就是函数中的函数。<code class="fe mz na nb nc b">content_fn(x)</code>取张量<code class="fe mz na nb nc b">x</code>，输出标量损失值<code class="fe mz na nb nc b">res</code>。对于<code class="fe mz na nb nc b">w</code>中的每个权重，<code class="fe mz na nb nc b">content_fn(.)</code>根据<code class="fe mz na nb nc b">x</code>的第I个元素和<code class="fe mz na nb nc b">x</code>的第I+n个元素之差调用<code class="fe mz na nb nc b">mean_sqr_b(.)</code>(其中n =在<code class="fe mz na nb nc b">w</code>中权重的数量)，将结果乘以<code class="fe mz na nb nc b">w</code>中的第I个权重，并将该值加到<code class="fe mz na nb nc b">res</code>。</p><p id="94ea" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你认为进入<code class="fe mz na nb nc b">content_fn(.)</code>的<code class="fe mz na nb nc b">x</code>是我们的雇佣张量和我们的LoRes上采样张量的连接，如果这两个张量都是多输出模型(3个不同的conv块激活)的结果，那么<code class="fe mz na nb nc b">content_fn(.)</code>所做的只是两个张量的相应加权块激活的<a class="ae kw" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank"> MSE </a>。</p><p id="4799" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果这仍然有点难以理解，试试这个:</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/664e24633a7bf79bc97087c9155d84a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AlQuDLjhkFQdraZFTbpEQw.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">yes I see it too.</figcaption></figure><p id="9097" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我要在这里暂停一会儿，希望我没有让它变得更糟。</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/485b8ed0e34e521e8486a99b80fdfec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*swwHhvAesaCK87XcStr2Pg.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Terek</figcaption></figure><p id="140d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">没错。这就是损失函数。</p><p id="8396" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最终的模型，这次称为<code class="fe mz na nb nc b">m_sr</code>的超分辨率模型，将一组张量作为输入:LoRes和HiRes originals，并输出应用于<code class="fe mz na nb nc b">vgg1</code> + <code class="fe mz na nb nc b">vgg2</code>的损失函数<code class="fe mz na nb nc b">content_fn</code>的结果。</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="37ee" class="nn no iq nc b gy np nq l nr ns">m_sr = Model([inp, vgg_inp], Lambda(content_fn)(vgg1+vgg2)</span></pre><p id="245c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">想象这是一个温和的噩梦:想象多重计算图形的不同之处仅在于它们将VGG的哪个块作为输出，它们的输出是如何被编织到损失函数中的；这还没有考虑每个上采样网络内部发生了什么，或者反向传播实际上是如何更新网络的。这可以被抽象成几行代码，这很酷。我想过画一个巨大的图表…过了某个点，它是光明会遇到矩阵，你在比赛找出情节。</p><p id="2a86" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">尽管使用了<a class="ae kw" href="https://www.kaggle.com/wiki/MeanAbsoluteError" rel="noopener ugc nofollow" target="_blank">平均绝对误差</a>而不是<a class="ae kw" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank"> MSE </a> : <code class="fe mz na nb nc b">m_sr.compile('adam', 'mae')</code>，但编译还是很简单的。另一个区别是训练。加载大约40k的图像<em class="kx">刚好</em>刚好适合我工作站的内存，<em class="kx">有时</em>。如果你想要更多呢？这就是bcolz库的用武之地。你把数据保存在磁盘上，然后分批加载。代码是这样的:</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/659c462e4e25eb0bf2549cce8ca256d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*HQALi9NKWvhXxn2mVrAE4A.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">see: <a class="ae kw" href="https://github.com/fastai/courses/blob/master/deeplearning2/bcolz_array_iterator.py" rel="noopener ugc nofollow" target="_blank">bcolz_array_iterator.py</a></figcaption></figure><p id="0b9f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe mz na nb nc b">bs</code>是批量大小。它必须是压缩时定义的bcolz carray的块长度的倍数。bcolz数组有一个问题，它的块长度是64，而我的工作站只能处理6的批量大小；8如果行星排成一行。我通过创建新数组并显式设置参数<code class="fe mz na nb nc b">chunklen</code>解决了这个问题。这可能是或可能不是犹太人。</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/9a825ae33fc61fea8498a6926530ba7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D6EI5VI_3ZIOlYTLRDi0lg.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk"><a class="ae kw" href="https://github.com/WNoxchi/Kaukasos/blob/master/FAI02/Lesson9/neural_sr_attempt3.ipynb" rel="noopener ugc nofollow" target="_blank">neural_sr_attempt3.ipynb</a></figcaption></figure><p id="8bdb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe mz na nb nc b">niter</code>是迭代次数。<code class="fe mz na nb nc b">targ</code>和之前一样是目标向量，但是这次是一个0的数组，一个0代表批处理中的一个元素。<code class="fe mz na nb nc b">bc</code>是一个迭代器，它在每次调用<code class="fe mz na nb nc b">next(bc)</code>时返回下一批<code class="fe mz na nb nc b">arr_hr</code>和<code class="fe mz na nb nc b">arr_lr</code>。我真的很喜欢这个，因为我在一个深度学习任务中遇到了系统内存限制的问题，我拼凑了一堆类似的函数来批量从磁盘中提取数据，并正确处理最后的剩余部分。<a class="ae kw" href="https://github.com/fastai/courses/blob/master/deeplearning2/bcolz_array_iterator.py" rel="noopener ugc nofollow" target="_blank">代码</a>更先进一些，把它拆开看看它是如何工作的会很有趣。它使用了线程和<code class="fe mz na nb nc b">with</code>语句，这些我还没有用过。</p><p id="924e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于<code class="fe mz na nb nc b">niter</code>迭代，<code class="fe mz na nb nc b">train(..)</code>加载下一批雇佣和知识图像，然后调用<code class="fe mz na nb nc b">m_sr.train_on_batch([lr[:bs], hr[:bs]], targ)</code>。为什么当<code class="fe mz na nb nc b">lr</code>是长度<code class="fe mz na nb nc b">bs</code>时，确切地说是<code class="fe mz na nb nc b">lr[:bs]</code>而不仅仅是<code class="fe mz na nb nc b">lr</code>呢，反正我不确定——也许不是？在最后一批中，我遇到了输入和目标数组长度不匹配的问题。如果您有16，007个图像，批处理大小为8，那么您的目标数组大小被设置为批处理大小→您会得到一个减一的错误。我通过将目标指定为<code class="fe mz na nb nc b">targ[:len(hr)]</code>来解决这个问题，这样最终的输入和目标数组长度相同。至于<code class="fe mz na nb nc b">niter</code>，我将其设置为:<code class="fe mz na nb nc b">niter = len(arr_hr)//6 + 1</code>以说明余数，尽管回想起来，只有在余数存在时才添加<code class="fe mz na nb nc b">1</code>会更好。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="2e8c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">毕竟，结束是容易的。训练几个纪元，降低学习速度，然后再训练几个纪元。然后获得您训练有素的工作模型:</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="6b73" class="nn no iq nc b gy np nq l nr ns">top_model = Model(inp, outp)</span></pre><p id="0a9c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">并开始向上采样。我得到了这个(<em class="kx">注</em>:这里的数据一次全部加载到内存中，模型通过<code class="fe mz na nb nc b">model.fit(..)</code>进行训练):</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/a517edec02f2aa653ab3ff926977969e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*ze1uXOwSvrFHIfcg3-M2-g.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">it enhanced the mashed potatoes</figcaption></figure><p id="739c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">虽然正如课程笔记本暗示的那样，这种良好的表现可能部分是由于过度拟合。以下是木星照片的结果:</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/56b6d8ccdbd6a59b256babc5cc8fc61f.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*AoUdv9HPTGtvzSdS1UtMMA.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">LoRes — SuperRes — HiRes</figcaption></figure><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ob"><img src="../Images/5c526932514b00cf3a7502e5d5b5a136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2nbN0uaThCR9_hPO0woG3w.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">LoRes — SuperRes — HiRes (used batch-loading method)</figcaption></figure><p id="e37e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这并不奇怪。首先，我肯定忘记了至少一个实现中的学习率退火部分——我更感兴趣的是让它工作起来:大量的资源耗尽错误和bcolz迭代器的故障排除——而我正在处理的数据集只有19，439幅图像。也许这已经足够了，但是如果<code class="fe mz na nb nc b">In: len(arr_hr)//16; Out: 62277</code>有任何线索的话，课程中的模型至少训练了996，432张图像，尽管那可能是在讲座被录制之后。不管是哪种情况，你都应该在ImageNet上进行训练，而不是一个小的子集，如果你有发言权的话。理想情况下，你也应该正确地进行训练。顺便说一下，我的两次成功尝试总共花费了大约12个小时的计算时间。所以20k就可以了。</p><p id="ea50" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">关于木星和过度拟合的一个注意事项:在所有图片中，似乎确实存在对比度增加和褪色的问题，但据我所知，ImageNet中也没有行星类别。所以那里可能发生了什么。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="3f4b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi mp translated"><span class="l mq mr ms bm mt mu mv mw mx di"> 3。</span>快速风格转移。我们可以使用来自超分辨率的相同基本思想，利用感知(内容)损失进行快速风格转换。这次我们拍摄一张照片，通过CNN，然后通过一个损失函数对一张固定风格的图片进行风格和内容损失。关于这个话题有一篇<a class="ae kw" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">论文</a>和<a class="ae kw" href="http://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16Supplementary.pdf" rel="noopener ugc nofollow" target="_blank">补充材料</a>。</p><p id="bfbe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">论文</a>在输入上使用反射填充而不是零填充——也就是说:反射边界周围的图像而不是只有黑色像素。杰瑞米·霍华德用一个定制层实现了这个，在Keras中这个定制层是作为一个Python类来实现的。</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oc"><img src="../Images/887d3b82bb84b462bcd080f5b6cb96b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d5mTnbU3rN4ogI2836WVvw.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk"><a class="ae kw" href="https://github.com/fastai/courses/blob/master/deeplearning2/neural-style.ipynb" rel="noopener ugc nofollow" target="_blank">neural-style.ipnb</a></figcaption></figure><p id="503e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第一部分是构造函数。此后，在定义自定义Keras层时需要做两件事:您需要定义<code class="fe mz na nb nc b">get_output_shape_for(self, input)</code>，它接受输入张量的形状，并返回输出张量的形状——在这种情况下，它与<code class="fe mz na nb nc b">s</code> ( <code class="fe mz na nb nc b">s[0]</code>)具有相同的批处理大小，相同的通道数(<code class="fe mz na nb nc b">s[3]</code>)，以及两倍的行和列填充量(<code class="fe mz na nb nc b">s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1],</code>)。当你把东西放在一起时，Keras“神奇地”知道中间层的输入/输出有多大，因为每一层都包含这种信息。第二个要定义的是<code class="fe mz na nb nc b">call(self, x, mask=None)</code>。<code class="fe mz na nb nc b">call(..)</code>获取你的层数据<code class="fe mz na nb nc b">x</code>，并返回你的层所做的一切。<a class="ae kw" href="https://keras.io/layers/writing-your-own-keras-layers/" rel="noopener ugc nofollow" target="_blank"> Keras的文件规定</a>:</p><blockquote class="lu lv lw"><p id="4ae3" class="jy jz kx ka b kb kc kd ke kf kg kh ki lx kk kl km ly ko kp kq lz ks kt ku kv ij bi translated">这是层的逻辑所在。</p></blockquote><p id="c2d0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们希望这一层添加反射填充。TensorFlow有一个名为<code class="fe mz na nb nc b">tf.pad(..)</code>的内置函数。</p><p id="c830" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该图层现在可用于网络定义:</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="847a" class="nn no iq nc b gy np nq l nr ns">inp = Input((288, 288, 3))<br/>ref_model = Model(inp, ReflectionPadding2D((40, 2))(inp)<br/>ref_model.compile('adam', 'mse')</span><span id="4a8e" class="nn no iq nc b gy nt nq l nr ns">p = ref_model.predict(arr_hr[10:11]) # run model on 10th img</span></pre><figure class="lg lh li lj gt jr gh gi paragraph-image"><div class="gh gi od"><img src="../Images/e41377fb4cbf4e5726010bc343327c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*xLGdGRKLNHhE0X3THplq1w.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk"><a class="ae kw" href="https://github.com/fastai/courses/blob/master/deeplearning2/neural-style.ipynb" rel="noopener ugc nofollow" target="_blank">neural-style.ipynb</a></figcaption></figure><p id="99f8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">论文作者使用<code class="fe mz na nb nc b">40x40</code>像素对图像进行反射填充。因为它们在它们的ResNet块中使用“有效”卷积，所以图像通过层被逐渐裁剪，所以反射填充的存在是为了保留图像。</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/004c8a4a876aa1326fc5ad3925d6ae6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*3XM_aR3IwMbpCeo-5O27MA.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">From: <a class="ae kw" href="http://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16Supplementary.pdf" rel="noopener ugc nofollow" target="_blank">Perceptual Loss Supplementary Material</a></figcaption></figure><p id="4a9e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其实作者的架构和我们的很像。一般模式是:通过卷积对输入进行下采样，并增加通道和感受野的数量，从而创建更复杂的数据表示(第3-5行)，对这些表示进行计算(剩余层)，然后对其进行上采样(最后3行)。1/2步是去卷积:它与分数步卷积相同。</p><p id="1947" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">没有进入太多的细节，实现这一点(主要遵循更新的课程笔记本)我采取了这种风格的形象:</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/9089340073d4db8671858155b9236a74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5oK8NHIPD9HPmRzeRSNCpQ.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">artist: <a class="ae kw" href="https://www.artstation.com/aenamiart" rel="noopener ugc nofollow" target="_blank">Alena Aenami</a></figcaption></figure><p id="4657" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">并把它应用到这幅Waynakh中世纪塔的图片上:</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div class="gh gi og"><img src="../Images/719c015f91deacfb1b37ce4e76fa96d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*c4_-aJOOO71NgLvYlS3fww.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">image: Ahmed Osmiev</figcaption></figure><p id="4415" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">两幅图像都被调整到<code class="fe mz na nb nc b">500x500</code>。回头看看代码，我看到了一些愚蠢的错误:样式图像被裁剪为<code class="fe mz na nb nc b">288x288</code>以适应样式张量…因此丢弃了33%的样式图像。在将塔的内容图片调整到<code class="fe mz na nb nc b">333x333</code> …那不适合，所以我只是裁剪它以适合输入张量。尽管如此，在用学习速率退火(2p @ default→2p @ 0.0001)在4个时期中训练快速风格转移模型大约6小时之后，该模型在大约1秒或更短时间内完成了其风格转移，并产生了以下结果:</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/0479a40a0ab841d3531a2c1129c02355.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*EU4HcTIs5WxoI7wbLR2WOA.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">resized in Apple Pages to (kind of) match original proportions</figcaption></figure><p id="fd30" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有点像角色扮演游戏的卡片。闹鬼的城堡什么的。显然，如果我使用原始内容和风格图像的全分辨率会好得多，但我还没有研究它。由于这种模型首先进行训练，而不是像以前的风格转移模型那样在运行时进行训练……感觉要让它在计算资源有限的情况下处理大型图像会很困难。你可以尝试简单的极端情况，重新压缩bcolz数组，使其块长度为1，这样你就可以使用批量大小为1的数组，以低得多的学习速率开始，只需接受更长的训练时间，然后就可以了……但是这种解决方案感觉<em class="kx">太</em>简单了。在给定固定资源的情况下，应该有一种方法可以缩放到任何图像大小，并且只在计算时间上付出代价。嗯..</p><p id="e0ba" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你可以试着把图像分成一个网格马赛克，在每一块上运行，然后把它们拼接起来。但现在你要付出一个定性的代价:如果这是一张雇佣照片，那么有一些重要信息你的社交网络不会知道，因为它实际上会被放大到只见树木不见森林。</p><p id="b885" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">等等，有没有一种方法可以使用<code class="fe mz na nb nc b">288x288x3</code>张量作为视野，并扫描更大的图像，将视野中的内容加载到内存中，并将其余内容保存在磁盘上，也许可以使用bcolz？<em class="kx">嗯..</em></p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="df32" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi mp translated"><span class="l mq mr ms bm mt mu mv mw mx di"> 4。</span> <a class="ae kw" href="https://research.google.com/pubs/pub41869.html" rel="noopener ugc nofollow" target="_blank">设计</a>:深度视觉语义嵌入模型。第九课的最后一部分，也是这篇博文的开始。Andrea Frome在一篇论文中描述了这一点。)，DeVISE试图通过用单词嵌入取代一键编码来更接近我们在图像识别方面所做的事情。</p><p id="4501" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">作为一个(非常)粗略的概述:在你传统的一键编码模型中，鲨鱼和鱼之间几乎没有相似之处，或者“鲨鱼”的概念是所有类型鲨鱼的超集。当然，这不是我们在现实生活中感知事物的方式。在一个不同的领域，自然语言处理，有一种方法可以捕捉到类别之间的相互关系，叫做单词嵌入。它基本上是一个矩阵，每行(每列？？)由映射一个单词与所有其他单词的相关性的向量组成。我们在<a class="ae kw" href="http://course.fast.ai/lessons/lesson4.html" rel="noopener ugc nofollow" target="_blank">深度学习I </a>中使用它来创建一种类似尼采的机器人，也用于电影推荐(嵌入可以将电影映射到类别，而不仅仅是单词到单词)。</p><p id="a613" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，对于图像，pug的向量不是1000个元素，pug类别的索引是1，其他地方都是0，而是用pug的密集单词向量来替换它(向量长度取决于我们使用的嵌入)。我们将使用一个Softmax层来进行一键编码(答案是<code class="fe mz na nb nc b">1</code>其他都是<code class="fe mz na nb nc b">0</code>)，但是现在我们将使用线性层。</p><p id="8a0b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">棘手的是要通过ImageNet。150万张左右的图片已经很多了。这就是bcolz和数组迭代器(在前面使用过)的用武之地。</p><p id="c7fa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里有一个来自<a class="lo lp ep" href="https://medium.com/u/34ab754f8c5e?source=post_page-----dcd599ad6e0b--------------------------------" rel="noopener" target="_blank">杰瑞米·霍华德</a>的提示:他定义了两条数据路径——一条用于高速固态硬盘/非易失性存储器上的数据，另一条用于大型硬盘。他使用RAID 1 SSD来快速访问调整大小的图像和功能阵列，以及用于批量数据的HDD路径。好主意。</p><p id="b9f0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用的单词嵌入是word2vec。</p><p id="2035" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">讲座中提到的<a class="ae kw" href="https://youtu.be/I-P363wSv0Q?t=6172" rel="noopener ugc nofollow" target="_blank">另一个有趣的事情是Python的single-star (zip-star？)语法。如果您有一个将单词映射到向量的数据结构，并且想要获得单词列表和向量列表，如果您能够以迭代器的形式获得该数据结构，那么:</a></p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="ffb5" class="nn no iq nc b gy np nq l nr ns">words, vectors = zip(*wordvec_iterator)</span></pre><p id="2645" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">将所有的单词放入<code class="fe mz na nb nc b">words</code>，所有对应的向量放入<code class="fe mz na nb nc b">vectors</code>。</p><p id="8eb6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以在这个讲座的例子中，如果<code class="fe mz na nb nc b">wordvec_iterator</code>包含<code class="fe mz na nb nc b">"fox:", array01, "wolf", array02, ...</code>，那么当你压缩<code class="fe mz na nb nc b">wordvec_iterator</code>时，这与把这些内容放入<code class="fe mz na nb nc b">zip(.)</code>是一样的，就像这样:</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="ae4e" class="nn no iq nc b gy np nq l nr ns">zip("fox", array01, "wolf", array02)</span></pre><p id="e939" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">那么测试一下这个:</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="0cd2" class="nn no iq nc b gy np nq l nr ns">&gt;&gt;&gt; words = ["fox:","wolf:"]; vecs = [[1,2,3],[4,5,6]]; <br/>&gt;&gt;&gt; zipped = zip(words, vecs)<br/>&gt;&gt;&gt; list(zipped)</span><span id="0a60" class="nn no iq nc b gy nt nq l nr ns">Out: [('fox:', [1,2,3]), ('wolf', [4,5,6])]</span><span id="66ad" class="nn no iq nc b gy nt nq l nr ns">&gt;&gt;&gt; w, v = zip(*zip(words, vecs)) # or: zip(*zipped) ## for fresh `zipped`<br/>&gt;&gt;&gt; words == list(w) and vecs == list(v)</span><span id="40dc" class="nn no iq nc b gy nt nq l nr ns">Out: True</span><span id="69d9" class="nn no iq nc b gy nt nq l nr ns">&gt;&gt;&gt; w, v</span><span id="2e96" class="nn no iq nc b gy nt nq l nr ns">Out: (('fox:', 'wolf:'), ([1,2,3], [4,5,6]))</span></pre><p id="47a2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">或者说得迂腐一点:</p><pre class="lg lh li lj gt nj nc nk nl aw nm bi"><span id="76a3" class="nn no iq nc b gy np nq l nr ns">&gt;&gt;&gt; word, vecs</span><span id="6fa9" class="nn no iq nc b gy nt nq l nr ns">Out: (['fox:', 'wolf:'], [[1, 2, 3], [4, 5, 6]])</span><span id="999c" class="nn no iq nc b gy nt nq l nr ns">&gt;&gt;&gt; list(w), list(v)</span><span id="0156" class="nn no iq nc b gy nt nq l nr ns">Out: (['fox:', 'wolf:'], [[1, 2, 3], [4, 5, 6]])</span></pre><p id="ad63" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以我今天学到了一些新东西。基本经验:如果你想把一个元组列表转换成一个列表元组，使用zip-star (Python)。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="11ec" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="lo lp ep" href="https://medium.com/u/34ab754f8c5e?source=post_page-----dcd599ad6e0b--------------------------------" rel="noopener" target="_blank">杰瑞米·霍华德</a>做了更多设置笔记本的工作，将文字嵌入应用到ImageNet图片。我不会在这里详述，因为这篇文章遭受了它自己的任务爬行形式——而且我还没有为一个完全解压缩的ImageNet腾出TB左右的自由空间。Howard对单词列表做了一些处理，对它们进行了重新排序，这样更常见的单词就会排在前面——并且还将所有单词都变成了小写，因为大写在这里并不重要。他通过使用<code class="fe mz na nb nc b">np.corrcoef(..)</code>来查看他的名字的大写和小写版本与他的名字和不相关的对象之间的相关系数，从而进行了几次健全性检查。</p><p id="d75f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，他创建了两个单词向量映射(Python字典),分别映射到1，000个Imagenet类别和WordNet中的82，000个名词——目标是创建一个可以超越ImageNet的1，000个类别的系统。</p><p id="4521" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，他创建了一个在WordNet中找不到的ImageNet类别列表，并将这些图像文件夹移出目录。</p><p id="fd0f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">另一个有用的提示:Howard保存所有花费大量时间的输出。获取一百万张图像的文件名需要一段时间，尤其是当它们在旋转的磁盘上时。</p><p id="d7d4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">他还利用了我在<a class="ae kw" href="https://github.com/fastai/numerical-linear-algebra/blob/master/README.md" rel="noopener ugc nofollow" target="_blank">计算/数值线性代数</a>课上看到的三种技术:内存局部性、SIMD/向量化和并行处理。【<a class="ae kw" href="https://youtu.be/I-P363wSv0Q?t=6729" rel="noopener ugc nofollow" target="_blank">讲座环节</a>】。我觉得关于SIMD的那段很有趣。</p><p id="5629" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">到目前为止，对我来说，这是这门课最长的一课。有很多事情要做，但我喜欢从大规模架构的角度来看代码的想法。在黑暗的森林中很容易迷路，并且忘记你的模型的一部分来自哪里。人们也很容易害怕做出任何改变，因为害怕再也无法完全理解。但是如果你有一个连续的主题“我通过CNN输入图像，CNN的输出通过一个损失函数与我想要它产生的相比较”，记住你实际上在做什么就变得容易多了。这就差不多结束了！</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="90ad" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">关于符号的几个注意事项:我将经常大写某些术语，如卷积或生成等。我发现，在阅读新的技术主题时，将一些关键术语强调得恰到好处，就像路径标记一样，会有所帮助，可以避免只是通读像散文这样的文本墙而迷失方向。</p><p id="40ae" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，每当代码中有一个单独的<code class="fe mz na nb nc b">l</code>时，我会用<code class="fe mz na nb nc b">λ</code>替换它。除了<code class="fe mz na nb nc b">λ</code>漂亮之外，在许多字体中<code class="fe mz na nb nc b">1 I l</code>看起来完全相同或者非常接近，足以让人迷惑；所以它有一个额外的用途——保持理智。</p></div></div>    
</body>
</html>