<html>
<head>
<title>[Learning Note] Dropout in Recurrent Networks — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[学习笔记]循环网络中的辍学—第2部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-note-dropout-in-recurrent-networks-part-2-f209222481f8?source=collection_archive---------1-----------------------#2017-09-30">https://towardsdatascience.com/learning-note-dropout-in-recurrent-networks-part-2-f209222481f8?source=collection_archive---------1-----------------------#2017-09-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="086e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">喀拉斯和PyTorch的经常性辍学实施</h2></div><p id="da03" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在进行实验之前，我想详细检查一下实现，以便更好地理解和将来参考。由于篇幅限制，我用<code class="fe lb lc ld le b">#...</code>省略了对当前讨论不重要的行。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/e8faf81912e7badd74c8eaa2936b37ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*S9okmCRVqlPExjYiOeHjvA.jpeg"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><a class="ae lr" href="https://keras.io/img/keras-logo-small.jpg" rel="noopener ugc nofollow" target="_blank">https://keras.io/img/keras-logo-small.jpg</a></figcaption></figure><h2 id="8a6c" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">克拉斯</h2><p id="4dea" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">这里我用的是<em class="mq"> Tensorflow 1.3.0 </em>自带的Keras。</p><p id="bab9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实现主要驻留在<code class="fe lb lc ld le b">LSTM</code>类中。我们从<code class="fe lb lc ld le b">LSTM.get_constants</code>类方法开始。为<code class="fe lb lc ld le b">Recurrent.call</code>方法中的每个批处理调用它，以提供丢弃掩码。(输入辍学率和经常性辍学率已作为实例属性存储在<code class="fe lb lc ld le b">__init__</code>。)</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="140c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输入以<strong class="kh ir"> <em class="mq">(样本，时间(用零填充)，input_dim) </em> </strong>的形式排列。上面的代码块创建形状为<strong class="kh ir"> <em class="mq"> (samples，input_dim) </em> </strong>的输入掩码，然后随机将元素设置为零。因此，对于每个序列/样本，新的掩码<strong class="kh ir">被采样，与论文【1】中描述的一致。</strong></p><p id="cab8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意<strong class="kh ir">创建了四个不同的掩模</strong>，对应于LSTM的四个门。只有LSTM 支持这个设置。(更多细节见下文)。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="f3a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，上面创建了四个形状为<strong class="kh ir"> <em class="mq"> (samples，hidden_units) </em> </strong>的循环遮罩。</p><p id="bfdd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来我们转向<code class="fe lb lc ld le b">LSTM.step</code>方法，该方法在每个时间步依次执行:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="9b69" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Keras有<a class="ae lr" href="https://github.com/tensorflow/tensorflow/blob/v1.3.0/tensorflow/contrib/keras/python/keras/layers/recurrent.py#L140" rel="noopener ugc nofollow" target="_blank"> 3个LSTM </a>实现，默认为实现0:</p><blockquote class="mt mu mv"><p id="e7a4" class="kf kg mq kh b ki kj jr kk kl km ju kn mw kp kq kr mx kt ku kv my kx ky kz la ij bi translated">实现:{0、1或2}之一。如果设置为0，RNN将使用使用更少、更大矩阵乘积的实现，从而在CPU上运行更快，但消耗更多内存。如果设置为1，RNN将使用更多的矩阵产品，但较小的，因此运行速度较慢(实际上可能在GPU上更快)，同时消耗较少的内存。如果设置为2(仅适用于LSTM/GRU)，RNN会将输入门、忽略门和输出门组合成一个矩阵，从而在GPU上实现更省时的并行化。注意:RNN辍学必须为所有门共享，导致稍微减少正规化。</p></blockquote><p id="d21d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实现2对应于捆绑权重LSTM。上面的代码块实现了dropout，就像这个公式[1]一样:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/38e4fd20be584fce9b8987d77ccd1851.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*z8SvkSfrdwBm7y5V66T8Eg.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Dropout in Tied-weight LSTM</figcaption></figure><p id="9566" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意它是如何只获取第一个遮罩并丢弃其余的(三个遮罩)的。这是因为这个公式要求所有门共享RNN辍学。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="a33e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看来实施<em class="mq"> 0 </em>和<em class="mq"> 1 </em>在如何应用输入压差方面有所不同。在实现<em class="mq"> 0 </em>中，转换后的输入在<code class="fe lb lc ld le b">step </code>方法之外预计算，而在实现<em class="mq"> 1 </em>中，输入被丢弃并在<code class="fe lb lc ld le b">step</code>内转换。</p><p id="a974" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(注意每个门如何使用它自己的下降掩码，以及如何为每个门组合转换的输入和隐藏状态。)</p><p id="9e0d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就是这样。实现没有任何惊喜，所以你<a class="ae lr" href="https://keras.io/layers/recurrent/#lstm" rel="noopener ugc nofollow" target="_blank">可以放心使用</a> <code class="fe lb lc ld le b"><a class="ae lr" href="https://keras.io/layers/recurrent/#lstm" rel="noopener ugc nofollow" target="_blank">dropout</a></code> <a class="ae lr" href="https://keras.io/layers/recurrent/#lstm" rel="noopener ugc nofollow" target="_blank">和</a> <code class="fe lb lc ld le b"><a class="ae lr" href="https://keras.io/layers/recurrent/#lstm" rel="noopener ugc nofollow" target="_blank">recurrent_dropout</a></code> <a class="ae lr" href="https://keras.io/layers/recurrent/#lstm" rel="noopener ugc nofollow" target="_blank">参数</a>。你唯一需要考虑的可能是是否使用实现<em class="mq"> 2 </em>而不是<em class="mq"> 0 </em>来加快速度。</p><p id="2cb1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(目前Keras似乎不提供[1]中描述的嵌入丢失。不过，我认为你绝对可以为此编写一个自定义层。)</p></div><div class="ab cl na nb hu nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ij ik il im in"><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/4f5966531b196e58bff151d201019794.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*ncASVTTDQ2cWDf3LJxv1NQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><a class="ae lr" href="http://pytorch.org/tutorials/_images/pytorch-logo-flat.png" rel="noopener ugc nofollow" target="_blank">http://pytorch.org/tutorials/_images/pytorch-logo-flat.png</a></figcaption></figure><h2 id="c529" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">PyTorch</h2><p id="b1b8" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">正如在第1部分中提到的，PyTorch不提供对变化的辍学的本地支持。我们将使用来自<a class="ae lr" href="https://github.com/salesforce/awd-lstm-lm/" rel="noopener ugc nofollow" target="_blank"><em class="mq">sales force/awd-lstm-lm</em></a><em class="mq"/>项目的实现。(此部分针对PyTorch 0.2.0版本)</p><p id="e751" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="mq">锁定输出</em> </strong>可用于对每个时间步长应用相同的下降掩码(如输入下降):</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="ac5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">PyTorch一般支持两种序列张量排列:<strong class="kh ir"> <em class="mq">(样本，时间，输入_ dim)</em></strong><strong class="kh ir"><em class="mq">(时间，样本，输入_dim) </em> </strong>。上述代码块是为后一种安排而设计的。您可以很容易地修改它来支持这两种安排。<code class="fe lb lc ld le b">m</code>被创建为一个单一时间步长的下降遮罩，形状为<strong class="kh ir"> <em class="mq"> (1，样本，输入_尺寸)</em> </strong>。因此，每个序列都要采样一个新的掩码，与Keras中的一样。</p><p id="4609" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来是<strong class="kh ir"> <em class="mq"> WeightDrop </em> </strong>类。在[2]中提出的这种压差形式更简单，性能更好，即使在捆绑权重设置中，每个门也允许不同的压差。相比之下，要实现传统的变分丢失，可能需要在For循环中将LSTM/RNN分解成单独的时间步长。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="9424" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<code class="fe lb lc ld le b">_setup</code><strong class="kh ir"><em class="mq">weight drop</em></strong>中禁用参数展平(否则无法与CUDA配合使用)，并将目标权重矩阵(通常为<code class="fe lb lc ld le b">weight_hh_l0</code>)重命名为带有<code class="fe lb lc ld le b">_raw </code>后缀的矩阵。在<code class="fe lb lc ld le b">forward</code>中，目标权重被应用了一个删除遮罩，被复制并重命名为原始属性名(<code class="fe lb lc ld le b">weight_hh_l0</code>)。注意注册的参数是带有<code class="fe lb lc ld le b">_raw</code>后缀的权重矩阵(删除操作不会影响该权重矩阵)。</p><p id="58b4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有两种类型的重量下降，由<code class="fe lb lc ld le b">variational</code>参数控制。在我们能够理解代码之前，我们首先需要知道权重矩阵是如何工作的:对于LSTM <code class="fe lb lc ld le b">nn.LSTM</code>，有四个相关联的参数:<code class="fe lb lc ld le b">weight_ih_l0</code>、<code class="fe lb lc ld le b">weight_hh_l0</code>、<code class="fe lb lc ld le b">bias_ih_l0</code>、<code class="fe lb lc ld le b">bias_hh_l0</code>。命名要足够明显:<code class="fe lb lc ld le b">ih</code>表示输入要隐藏；<code class="fe lb lc ld le b">hh</code>意为隐来隐去；<code class="fe lb lc ld le b">l0</code>表示第一层。如果以<code class="fe lb lc ld le b">nn.LSTM(2, 8, num_layers=1) </code>为例，<code class="fe lb lc ld le b">weight_hh_l0</code> <strong class="kh ir"> (U) </strong>会有一个<em class="mq"> (32，8) </em>的形状，对应四个门和八个隐藏单元(32 = 8 * 4)。你应该能认出这是一只负重LSTM。这暗示了隐藏状态矩阵<strong class="kh ir"> (h) </strong>被整形<em class="mq"> (8，batch_size) </em>。矩阵乘法<strong class="kh ir"> Uh </strong>将产生一个<em class="mq"> 32 x batch_size </em>矩阵。每列代表一个单个序列的八个隐藏单元及其四个内部门的转换后的循环输入(注意PyTorch使用<strong class="kh ir"> Uh </strong>而不是<strong class="kh ir"> hU </strong>):</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ee2b8db06bdf762793efd2bc7c179e85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*wx12gHrq2F-o_9lkJB9frQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Internal Definition of LSTM</figcaption></figure><p id="5870" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于<code class="fe lb lc ld le b">variational=True</code>，创建形状为<em class="mq"> (4 * hidden_units，1) </em>的遮罩。将任何一个元素设置为零意味着<strong class="kh ir">切断隐藏单元</strong>中一个门的所有循环连接。<em class="mq">这似乎有点不可思议。</em>如果我们希望dropout out与Keras绑定权重实现(下面的公式)一致，我们需要使用形状为<em class="mq"> (1，hidden_units) </em>的遮罩。将任何一个元素设置为零意味着<strong class="kh ir">切断所有源自隐藏单元</strong>的循环连接。(记住，Keras中的单个循环漏失遮罩是成形的<em class="mq">(示例，hidden_units)。</em>)有可能是我弄错了，或者是bug，或者是作者故意这样做的。我还不确定。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/38e4fd20be584fce9b8987d77ccd1851.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*z8SvkSfrdwBm7y5V66T8Eg.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">(Reprise) Dropout in Tied-weight LSTM</figcaption></figure><p id="eea9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于<code class="fe lb lc ld le b">variational=False</code>，创建形状<em class="mq"> (4 * hidden_units，hidden_units) </em>的遮罩。因此，不同的隐藏单元和不同的门使用不同的掩模。</p><p id="4568" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="mq"> WeightDrop </em> </strong>和<strong class="kh ir"> <em class="mq"> Keras </em> </strong>实现的一个重要区别是权重矩阵的丢弃掩码只能在<strong class="kh ir">每小批量</strong>采样一次。如果您试图对每个序列采样一次，那么它实际上是使用大小为1的小批量，失去了使用小批量的目的。根据小批量的大小，这种限制导致每个小批量内不同程度的变化减少。(记住，在Keras中，对每个序列采样新的缺失掩码。)因此，总是从<code class="fe lb lc ld le b">variational=False</code>配置开始似乎是个好主意。</p><p id="cbee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，嵌入漏失:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="9f00" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这应该很简单。漏音遮罩的形状为<strong class="kh ir"> <em class="mq"> (num_words，1) </em> </strong>，漏音应用于字级。如[1]中所述，当字数和嵌入维数很大时，这种实现可能会有一些性能问题。但是我猜作者认为这是代码简单性和性能之间的一个适当的折衷。</p><h2 id="310b" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">把它放在一起</h2><p id="f110" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">使用所有三个讨论过的辍学的示例模型:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="d76a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(第11–15行)虽然<strong class="kh ir"> <em class="mq"> WeightDrop </em> </strong>不需要分割时间步，但是它需要分割RNN层。这是我们可以在图层之间应用<strong class="kh ir"><em class="mq">locked roup</em></strong>的唯一方法。</p><p id="3502" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(第30行)此处<code class="fe lb lc ld le b">forward</code>应用了嵌入dropout。</p><p id="d785" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(第31，35行)<strong class="kh ir"><em class="mq">locked rout</em></strong>通过简单地传递张量和辍学率来应用。</p></div><div class="ab cl na nb hu nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ij ik il im in"><h2 id="5c9b" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">待续</h2><p id="3816" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">这个帖子很乱，很抱歉。用简单的英语从技术上写作很难…无论如何，在最后一部分，我将记录一些实验结果。所得结果与文献[1]的结果有些不同。我也会试着对此做一些解释。</p><div class="nj nk gp gr nl nm"><a href="https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">[学习笔记]循环网络中的辍学—第1部分</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">理论基础</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">becominghuman.ai</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa ll nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a href="https://medium.com/@ceshine/learning-note-dropout-in-recurrent-networks-part-3-1b161d030cd4" rel="noopener follow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">[学习笔记]循环网络中的辍学—第3部分</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">一些实证结果比较</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">medium.com</p></div></div><div class="nv l"><div class="ob l nx ny nz nv oa ll nm"/></div></div></a></div><h2 id="81dc" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">参考</h2><ol class=""><li id="9f77" class="oc od iq kh b ki ml kl mm ko oe ks of kw og la oh oi oj ok bi translated">y . gal和z . Ghahramani(2015年)。<a class="ae lr" href="http://arxiv.org/abs/1512.05287" rel="noopener ugc nofollow" target="_blank">在递归神经网络中基于理论的辍学应用。</a></li><li id="d204" class="oc od iq kh b ki ol kl om ko on ks oo kw op la oh oi oj ok bi translated">梅里蒂，s .，凯斯卡尔，N. S .，&amp;索彻，R. (2017)。<a class="ae lr" href="http://arxiv.org/abs/1708.02182" rel="noopener ugc nofollow" target="_blank">规范和优化LSTM语言模型</a>。</li></ol></div></div>    
</body>
</html>