<html>
<head>
<title>NLP | Sequence to Sequence Networks| Part 2|Seq2seq Model (EncoderDecoder Model)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP |序列到序列网络|第 2 部分|Seq2seq 模型(编码器/解码器模型)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1?source=collection_archive---------2-----------------------#2018-11-15">https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1?source=collection_archive---------2-----------------------#2018-11-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/7c690c25c32393c8c8ba860e1fe0d4eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RspNg2-o0dL8IoSgBCkqQQ.png"/></div></div></figure></div><div class="ab cl kb kc hx kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="im in io ip iq"><p id="c11f" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">在[<a class="ae lg" rel="noopener" target="_blank" href="/nlp-sequence-to-sequence-networks-part-1-processing-text-data-d141a5643b72">NLP | Sequence to Sequence Networks | Part 1 | Processing text data</a>中，我们学习了如何处理文本数据，在这一部分中，我们将创建一个模型，该模型将获取我们处理的数据，并使用它来训练将英语句子翻译成法语。</p><p id="1845" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">我们将使用一种称为(seq2seq)或(Encoder Decoder)的架构，它适用于输入序列(在我们的例子中是英语句子)的长度与输出数据(在我们的例子中是法语句子)的长度不同的情况</p></div><div class="ab cl kb kc hx kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="im in io ip iq"><h2 id="40c4" class="lh li it bd lj lk ll dn lm ln lo dp lp kt lq lr ls kx lt lu lv lb lw lx ly lz bi translated">什么是编码器解码器架构？</h2><figure class="mb mc md me gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ma"><img src="../Images/c8261eccbe94eb873623a607b9dd65af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*BpmYIit1tmLKlpDm.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk"><a class="ae lg" href="https://smerity.com/articles/2016/google_nmt_arch.html" rel="noopener ugc nofollow" target="_blank">Encoder Decoder Architecture</a></figcaption></figure><p id="ae00" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">编码器解码器架构由两个主要部分组成:</p><ol class=""><li id="e9da" class="mj mk it kk b kl km kp kq kt ml kx mm lb mn lf mo mp mq mr bi translated"><strong class="kk iu">编码器</strong>:</li></ol><p id="2ced" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">编码器简单地获取输入数据，并对其进行训练，然后将其循环层的最后状态作为初始状态传递给解码器部分的第一循环层。</p><pre class="mb mc md me gt ms mt mu mv aw mw bi"><span id="5a32" class="lh li it mt b gy mx my l mz na">Encoder input : English sentences</span><span id="69db" class="lh li it mt b gy nb my l mz na">Encoder initial state : It depends on the initializer we use</span></pre><p id="45f6" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated"><strong class="kk iu"> 2。解码器:</strong></p><p id="299c" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">解码器获取编码器最后一个递归层的最后一个状态，并将其用作其第一个递归层的初始状态，解码器的输入是我们想要得到的序列(在我们的例子中是法语句子)。</p><pre class="mb mc md me gt ms mt mu mv aw mw bi"><span id="06aa" class="lh li it mt b gy mx my l mz na">Decoder input : French sentences</span><span id="11d5" class="lh li it mt b gy nb my l mz na">Decoder initial state : The last state of encoder’s last recurrent layer</span></pre><p id="4d0e" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated"><strong class="kk iu">其他一些解释编码器解码器的图片:</strong></p><figure class="mb mc md me gt ju gh gi paragraph-image"><div class="ab gu cl nc"><img src="../Images/c1a4170b61a1bddef95dfe85baa3c397.png" data-original-src="https://miro.medium.com/v2/format:webp/1*3lj8AGqfwEE5KCTJ-dXTvg.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk"><a class="ae lg" rel="noopener" target="_blank" href="/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d">Simple Representation</a></figcaption></figure><figure class="mb mc md me gt ju gh gi paragraph-image"><div class="ab gu cl nc"><img src="../Images/b9eaf2602b9dba5b3641ad4d991f8126.png" data-original-src="https://miro.medium.com/v2/format:webp/1*sO-SP58T4brE9EHazHSeGA.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk"><a class="ae lg" href="https://medium.com/botsupply/generative-model-chatbots-e422ab08461e" rel="noopener">Generative Model Chatbots</a></figcaption></figure><p id="707c" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">了解有关编码器/解码器架构的更多信息[<a class="ae lg" href="https://smerity.com/articles/2016/google_nmt_arch.html" rel="noopener ugc nofollow" target="_blank">0</a>]、<a class="ae lg" href="https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/" rel="noopener ugc nofollow" target="_blank">1</a>、<a class="ae lg" href="https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/" rel="noopener ugc nofollow" target="_blank">2</a>、<a class="ae lg" href="https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/" rel="noopener ugc nofollow" target="_blank">3</a>、<a class="ae lg" href="https://www.liip.ch/en/blog/sentiment-detection-with-keras-word-embeddings-and-lstm-deep-learning-networks" rel="noopener ugc nofollow" target="_blank">4</a></p></div><div class="ab cl kb kc hx kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="im in io ip iq"><h2 id="351b" class="lh li it bd lj lk ll dn lm ln lo dp lp kt lq lr ls kx lt lu lv lb lw lx ly lz bi translated">构建模型:</h2><p id="b31a" class="pw-post-body-paragraph ki kj it kk b kl nd kn ko kp ne kr ks kt nf kv kw kx ng kz la lb nh ld le lf im bi translated">首先，导入所需的依赖项:</p><figure class="mb mc md me gt ju"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="0648" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated"><strong class="kk iu">构建编码器:</strong></p><figure class="mb mc md me gt ju"><div class="bz fp l di"><div class="ni nj l"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">The Encoder</figcaption></figure><p id="3972" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">我们使用(<a class="ae lg" href="https://keras.io/getting-started/functional-api-guide/" rel="noopener ugc nofollow" target="_blank"> Keras Functional API </a>)来构建这样复杂的模型，如果你想了解更多关于 Keras Functional API 的知识:[ <a class="ae lg" href="https://www.youtube.com/watch?v=elOLCEJV-dc" rel="noopener ugc nofollow" target="_blank"> 1 </a>，[ <a class="ae lg" href="https://machinelearningmastery.com/keras-functional-api-deep-learning" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]</p><p id="adfd" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated"><strong class="kk iu">编码器由以下部分组成:</strong></p><ol class=""><li id="f17c" class="mj mk it kk b kl km kp kq kt ml kx mm lb mn lf mo mp mq mr bi translated">输入层:获取英语句子并将其传递给嵌入层。</li><li id="db33" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf mo mp mq mr bi translated">嵌入层:获取英语句子并将每个单词转换成固定大小的向量</li><li id="0289" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf mo mp mq mr bi translated">第一个 LSTM 层:每一个时间步，它采用一个代表一个单词的向量，并将其输出传递给下一层，我们使用 CuDNNLSTM 层而不是 LSTM，因为它快得多。</li><li id="e546" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf mo mp mq mr bi translated">第二 LSTM 层:它与前一层做同样的事情，但不是传递其输出，而是将其状态传递给解码器的第一 LSTM 层。</li></ol><figure class="mb mc md me gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi np"><img src="../Images/7ddc3ff1de4775429a28f6d6f0314c0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3pH2NH_8i7QMxpV0TFOdxw.jpeg"/></div></div></figure><p id="0f99" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated"><strong class="kk iu">构建解码器:</strong></p><figure class="mb mc md me gt ju"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="4b6e" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated"><strong class="kk iu">解码器由:</strong>组成</p><ol class=""><li id="d57e" class="mj mk it kk b kl km kp kq kt ml kx mm lb mn lf mo mp mq mr bi translated">输入层:获取法语句子并将其传递给嵌入层。</li><li id="fa0d" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf mo mp mq mr bi translated">嵌入层:获取法语句子，并将每个单词转换为固定大小的向量</li><li id="9fee" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf mo mp mq mr bi translated">第一个 LSTM 层:每一个时间步，它都采用一个表示单词的向量，并将其输出传递给下一层，但在解码器中，我们将这一层的状态初始化为解码器中最后一个 LSTM 层的最后一个状态。</li><li id="d306" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf mo mp mq mr bi translated">第二 LSTM 层:处理前一层的输出，并将其输出传递给密集层。</li><li id="70d0" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf mo mp mq mr bi translated">密集层(输出层) :获取前一层的输出，并输出一个表示目标法语单词的 hot vector</li></ol><p id="98b6" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated"><strong class="kk iu">注:</strong></p><p id="0221" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">我们必须知道，我们不能在一个时间步长内将每一个英语句子转换成法语，我们要在多个时间步长内完成，这些时间步长等于最长的英语句子的字数。</p><p id="defc" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">因此，如果最长的英语句子有 10 个单词，我们必须采取 10 个时间步骤来获得它的法语翻译。</p><figure class="mb mc md me gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi np"><img src="../Images/f4ebf2d7927dd3aa247eb40276ca6da7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sDlV9_-PXBlt8jol-7Xjhg.jpeg"/></div></div></figure><p id="9871" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">我使用前面的模块简化了编码器和解码器模型，实际上，我们有许多时间步长，而不是简化表示中的一个，最终输出不是我们可以直接阅读的句子，实际情况是，每个时间步长我们都会获得一个代表目标单词的热点向量。</p></div><div class="ab cl kb kc hx kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="im in io ip iq"><p id="05e6" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated"><strong class="kk iu">将编码器和解码器整合到一个模型中:</strong></p><figure class="mb mc md me gt ju"><div class="bz fp l di"><div class="ni nj l"/></div></figure><figure class="mb mc md me gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nq"><img src="../Images/de3d9594c75bc3e223bf40f39e0d1ae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SMZbHBbXyhhpneqjRqlg6g.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Model Summary</figcaption></figure></div><div class="ab cl kb kc hx kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="im in io ip iq"><h2 id="c3ed" class="lh li it bd lj lk ll dn lm ln lo dp lp kt lq lr ls kx lt lu lv lb lw lx ly lz bi translated">训练模型:</h2><figure class="mb mc md me gt ju"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="6e9c" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">我只使用了 9000 个样本，因为我没有足够的内存来使用整个数据集，正因为如此，结果不够好。</p><p id="f578" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">我们使用的优化器是<a class="ae lg" href="https://keras.io/optimizers" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> rmsprop </strong> </a> <strong class="kk iu">，</strong>它用在有循环层的模型中。</p><blockquote class="nr"><p id="2a75" class="ns nt it bd nu nv nw nx ny nz oa lf dk translated">这个优化器通常是递归神经网络的好选择。</p></blockquote><p id="5e02" class="pw-post-body-paragraph ki kj it kk b kl ob kn ko kp oc kr ks kt od kv kw kx oe kz la lb of ld le lf im bi translated">我们使用<strong class="kk iu">categorial _ cross entropy</strong>作为损失函数，因为我们将每个单词视为一个类别(我们的输出是一个代表单词的热点向量)</p></div><div class="ab cl kb kc hx kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="im in io ip iq"><h2 id="8ce5" class="lh li it bd lj lk ll dn lm ln lo dp lp kt lq lr ls kx lt lu lv lb lw lx ly lz bi translated">回想一下:</h2><ul class=""><li id="df7b" class="mj mk it kk b kl nd kp ne kt og kx oh lb oi lf oj mp mq mr bi translated">我们必须知道什么是编码器解码器模型。</li><li id="8451" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf oj mp mq mr bi translated">如何创建编码器</li><li id="6b98" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf oj mp mq mr bi translated">如何使用嵌入层</li><li id="48d1" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf oj mp mq mr bi translated">如何制作解码器</li><li id="9467" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf oj mp mq mr bi translated">我们了解编码器解码器模型是如何工作的</li><li id="612c" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf oj mp mq mr bi translated">我们训练了这个模型</li></ul><h1 id="debf" class="ok li it bd lj ol om on lm oo op oq lp or os ot ls ou ov ow lv ox oy oz ly pa bi translated">接下来是什么:</h1><p id="89a9" class="pw-post-body-paragraph ki kj it kk b kl nd kn ko kp ne kr ks kt nf kv kw kx ng kz la lb nh ld le lf im bi translated">在本系列的下一部分，我们将使用训练好的模型将英语句子翻译成法语。</p><h1 id="bc60" class="ok li it bd lj ol om on lm oo op oq lp or os ot ls ou ov ow lv ox oy oz ly pa bi translated">NLP |序列到序列网络:</h1><p id="9410" class="pw-post-body-paragraph ki kj it kk b kl nd kn ko kp ne kr ks kt nf kv kw kx ng kz la lb nh ld le lf im bi translated"><a class="ae lg" rel="noopener" target="_blank" href="/nlp-sequence-to-sequence-networks-part-1-processing-text-data-d141a5643b72"> 1- NLP |序列到序列网络|第 1 部分|处理文本数据</a></p><p id="780b" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">2- <a class="ae lg" href="https://medium.com/@mamarih1/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1" rel="noopener"> NLP |序列到序列网络|第 2 部分|Seq2seq 模型(编码器解码器模型)</a></p><h2 id="7f3a" class="lh li it bd lj lk ll dn lm ln lo dp lp kt lq lr ls kx lt lu lv lb lw lx ly lz bi translated">参考资料:</h2><p id="58e8" class="pw-post-body-paragraph ki kj it kk b kl nd kn ko kp ne kr ks kt nf kv kw kx ng kz la lb nh ld le lf im bi translated">这个系列的所有参考资料将在最后一部分的结尾。</p></div><div class="ab cl kb kc hx kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="im in io ip iq"><p id="cabe" class="pw-post-body-paragraph ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">你可以在推特上关注我<a class="ae lg" href="https://twitter.com/ModMaamari" rel="noopener ugc nofollow" target="_blank"> @ModMaamari </a></p></div><div class="ab cl kb kc hx kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="im in io ip iq"><h2 id="4c60" class="lh li it bd lj lk ll dn lm ln lo dp lp kt lq lr ls kx lt lu lv lb lw lx ly lz bi translated">您可能还喜欢:</h2><ul class=""><li id="f420" class="mj mk it kk b kl nd kp ne kt og kx oh lb oi lf oj mp mq mr bi translated"><a class="ae lg" href="https://blog.goodaudience.com/ai-generates-taylor-swifts-song-lyrics-6fd92a03ef7e" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> AI 生成泰勒斯威夫特的歌词</strong> </a></li><li id="8e72" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf oj mp mq mr bi translated"><a class="ae lg" href="https://medium.com/@mamarih1/deep-neural-networks-for-regression-problems-81321897ca33" rel="noopener"> <strong class="kk iu">深度神经网络用于回归问题</strong> </a></li><li id="91e6" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf oj mp mq mr bi translated"><a class="ae lg" href="https://medium.com/@mamarih1/machine-learning-crash-course-with-tensorflow-apis-summary-524e0fa0a606" rel="noopener"> <strong class="kk iu">带 TensorFlow APIs 的机器学习速成班汇总</strong> </a></li><li id="5133" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf oj mp mq mr bi translated"><a class="ae lg" href="https://medium.com/@mamarih1/how-to-make-a-cnn-using-tensorflow-and-keras-dd0aaaed8ab4" rel="noopener"> <strong class="kk iu">如何使用 Tensorflow 和 Keras 制作 CNN</strong></a></li><li id="086b" class="mj mk it kk b kl nk kp nl kt nm kx nn lb no lf oj mp mq mr bi translated"><a class="ae lg" href="https://medium.com/@mamarih1/how-to-choose-the-best-machine-learning-model-e1dbb46bdd4d" rel="noopener"> <strong class="kk iu">如何选择最好的机器学习模型？</strong>T25】</a></li></ul></div></div>    
</body>
</html>