<html>
<head>
<title>Sequence to sequence tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">序列到序列教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sequence-to-sequence-tutorial-4fde3ee798d8?source=collection_archive---------3-----------------------#2017-10-01">https://towardsdatascience.com/sequence-to-sequence-tutorial-4fde3ee798d8?source=collection_archive---------3-----------------------#2017-10-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="73e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是深度学习中最强大的概念之一，它始于翻译，但后来转移到了问答系统(Siri、Cortana等)。)、音频转录等。顾名思义，它对从一个序列转换到另一个序列很有用。</p><p id="93ae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">背后的主要思想是，它包含一个编码器RNN (LSTM)和一个解码器RNN。一个用来“理解”输入序列，解码器用来“解码”“思想向量”并构建输出序列。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/e3f96fe4173c9fb9e4a5254fd7b43ed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sO-SP58T4brE9EHazHSeGA.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Seq2Seq LSTMs</figcaption></figure><p id="0066" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">思想载体真正强大的地方在于，你可以插入输出解码器，将其转换成任何语言。也就是说，你不需要在英语到法语的数据集上训练来从英语转换到西班牙语。你可以简单地插入一个解码器，这个解码器是从一个不同的设备(比如葡萄牙语到西班牙语)预先训练好的。至少这是背后的理论。可能取决于你训练它的时间，你的数据量等等。</p><p id="091c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我发现tensorflow中缺少seq2seq模型背后的文档，所以我编写了自己的模型，如下所示。我在Tensorflow文档中找到的最接近的是<code class="fe lb lc ld le b">embedding_rnn_seq2seq</code>,然而，它在实现这个函数方面并不明显。因此，我写了自己的<code class="fe lb lc ld le b">seq2seq</code>模型如下:</p><pre class="km kn ko kp gt lf le lg lh aw li bi"><span id="f93d" class="lj lk iq le b gy ll lm l ln lo"><em class="lp"># Tensor where we will feed the data into graph</em><br/>inputs = tf.placeholder(tf.int32, (<strong class="le ir">None</strong>, x_seq_length), 'inputs')<br/>outputs = tf.placeholder(tf.int32, (<strong class="le ir">None</strong>, <strong class="le ir">None</strong>), 'output')<br/>targets = tf.placeholder(tf.int32, (<strong class="le ir">None</strong>, <strong class="le ir">None</strong>), 'targets')<br/><br/><em class="lp"># Embedding layers</em><br/>input_embedding = tf.Variable(tf.random_uniform((len(char2numX), embed_size), -1.0, 1.0), name='enc_embedding')<br/>output_embedding = tf.Variable(tf.random_uniform((len(char2numY), embed_size), -1.0, 1.0), name='dec_embedding')<br/>date_input_embed = tf.nn.embedding_lookup(input_embedding, inputs)<br/>date_output_embed = tf.nn.embedding_lookup(output_embedding, outputs)<br/><br/><strong class="le ir">with</strong> tf.variable_scope("encoding") <strong class="le ir">as</strong> encoding_scope:<br/>    lstm_enc = tf.contrib.rnn.BasicLSTMCell(nodes)<br/>    _, last_state = tf.nn.dynamic_rnn(lstm_enc, inputs=date_input_embed, dtype=tf.float32)<br/><br/><strong class="le ir">with</strong> tf.variable_scope("decoding") <strong class="le ir">as</strong> decoding_scope:<br/>    lstm_dec = tf.contrib.rnn.BasicLSTMCell(nodes)<br/>    dec_outputs, _ = tf.nn.dynamic_rnn(lstm_dec, inputs=date_output_embed, initial_state=last_state)<br/><em class="lp">#connect outputs to </em><br/>logits = tf.contrib.layers.fully_connected(dec_outputs, num_outputs=len(char2numY), activation_fn=<strong class="le ir">None</strong>) <br/><strong class="le ir">with</strong> tf.name_scope("optimization"):<br/>    <em class="lp"># Loss function</em><br/>    loss = tf.contrib.seq2seq.sequence_loss(logits, targets, tf.ones([batch_size, y_seq_length]))<br/>    <em class="lp"># Optimizer</em><br/>    optimizer = tf.train.RMSPropOptimizer(1e-3).minimize(loss)</span></pre><p id="eedd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">完整的教程可以在这个视频(和<a class="ae lq" href="https://github.com/sachinruk/deepschool.io/tree/master/DL-Keras_Tensorflow" rel="noopener ugc nofollow" target="_blank">代码</a>(第19课)如果你想跳过视频):</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="lr ls l"/></div></figure></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="9c47" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看这里是我的关于机器学习和深度学习的<a class="ae lq" href="https://www.udemy.com/course/machine-learning-and-data-science-2021/?referralCode=E79228C7436D74315787" rel="noopener ugc nofollow" target="_blank">课程</a>(使用代码DEEPSCHOOL-MARCH到85折)。</p></div></div>    
</body>
</html>