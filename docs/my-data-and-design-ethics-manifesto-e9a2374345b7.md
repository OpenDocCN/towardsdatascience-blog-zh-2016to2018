# 我的数据和设计道德宣言

> 原文：<https://towardsdatascience.com/my-data-and-design-ethics-manifesto-e9a2374345b7?source=collection_archive---------11----------------------->

***本帖原载于***[***LinkedIn 2018 年 5 月 30 日***](https://www.linkedin.com/pulse/my-data-design-ethics-manifesto-ovetta-sampson/?published=t)

作为一名设计研究负责人，我参与了许多数据驱动的项目，我一直在思考很多关于“大数据”的伦理问题。

尽管基于数学和统计学，但数据驱动的模型丝毫不受偏见、成见和人类彻头彻尾的恶意本性的影响。无论这些不道德的价值观是由它们的创造者传递给数学模型的，还是它们使用(或滥用)的函数，没有观点的数学在大数据世界中是不存在的。因为每一个数据点、每一个二元表达式、每一个公式、函数、统计模型的背后，都是实际的人。有时我们会忘记这一点。

要想找到证据证明事情可能会变得非常糟糕，你可以从这种反乌托邦式的科学出错小说中得到启示，比如赫胥黎的《美丽新世界》，或者阿西莫夫的《我是机器人》系列，或者你也可以看看我们最近的数学被用作武器的历史，在凯茜·奥尼尔的书《数学毁灭的武器》中有如此直白的描述。

奥尼尔写得像一个痛苦的离婚女人，发现她曾经深爱的伴侣欺骗她，描绘了一幅道德和大数据的惨淡画面。她抨击算法的仲裁者，这些算法用扭曲的半生不熟的代理预测模型将美丽优雅的数学变成“数学毁灭武器”，用真实数据代替观点和半真半假的事实。

奥尼尔编造了大量的故事，讲述了推动算法引发的纸牌屋住房危机的邪恶，以及坏演员致富所使用的糟糕数学，让我们其余的人感到震惊。她揭穿了标准化测试(它既不是标准的，也不符合公平测试)作为评估教师工具的隐含公平性。她指出，在刑事司法系统中，预测性量刑模型中使用的算法工具存在固有的偏见。她的清单还在继续。

对于我们这些深知人类有能力将最温和的工具变成邪恶的仲裁者的人来说，奥尼尔的案例研究并不令人惊讶。

十多年来，大数据占据了中心舞台，人工智能、有知觉的生物和机器人等领域被推入主流，我们越来越善于识别利用数据力量的坏人。然而，下一个挑战从危险信号转移到了一个明显灰色的领域。

对全世界的脸书人、亚马逊人和谷歌人来说，当然，对我们来说，当我们设计以数据为燃料的系统时，这就是我们真正开始看到棘手的伦理问题甚至不需要尝试就迅速变成膝盖深的危机的地方。

数据驱动的问题从看似无害到完全令人不快——事实上，谷歌的一个搜索算法向男性而不是女性显示了更高报酬的招聘广告，或者声称[亚马逊的搜索算法似乎倾向于自己的产品，拒绝给客户最好的东西，或者](https://www.investopedia.com/news/amazons-pricing-algorithm-favors-its-own-products-says-propublica-amzn/)[脸书玩弄你的新闻提要，从只向你显示悲伤的消息以衡量你的反应(令人讨厌)到压制保守或其他意识形态的消息(不确定)。给你看所有的假新闻(悲伤)。更不用说我称之为机器学习的“一上效应”了。虽然上面提到的怪癖经常发生在有偏见的数据驱动模型中，但是完全缺乏对人性的理解，再加上对编程代码优点的盲目相信，这可能是一种危险的民主崩溃的组合。](https://www.theguardian.com/technology/2014/jun/29/facebook-users-emotions-news-feeds)

# 算法的扩展

脸书俄罗斯选举丑闻将很快成为《法律与秩序》的一集，但让我们重温一下十年来科技领域最大的故事之一。如果你已经看到了大量的数据和道德博客帖子，煤矿中的许多金丝雀哭诉科技缺乏同理心及其危险，这可能是因为世界上最大的科技公司之一被少数购买广告的俄罗斯人操纵，他们利用该公司的产品影响国家总统选举。我们现在知道这是真的，而不仅仅是猜测。

当你这样写的时候，听起来很奇怪。世界上最大的平台之一，拥有这个星球上最聪明的工程师，怎么会被如此愚弄？却不知道吗？却不去阻止？

从某种意义上说，脸书在不知不觉中成为了一个经典案例，说明为什么你需要的不仅仅是工程师，而是创造一个几乎无害的数据驱动产品。

为了理解为什么，让我们看看智能系统是如何构建的。

## 关于算法的简短入门(如果你以前听过，跳过)

首先，事情第一——像脸书创建的帮助用户创建和定位信息的智能系统中有什么？主要成分是算法。

算法是智能系统模型的组成部分。简单来说，算法就是一个解决问题的过程。构成智能编程或人工智能主干的大多数算法都是学习返回结果的决策模型。

它们被编程为筛选信息并返回结果，就像谷歌的搜索引擎一样。这些结果有标准，允许他们选择超过其他信息。这是由人，通常是计算机工程师或数据科学家编写的。

如果它们是有机物，我会给算法贴上病毒的标签。算法一旦被编程，在完成任务时会非常高效，尤其是一旦它们学会了基本的任务。让我们看一个例子。

假设你想在你遇到的每一篇文学作品中找到“种子”这个词。你可以做 CTRL+F 一百万次，但更有效的方法是让计算机解决这个问题。因此，您可以创建一个算法来梳理信息，找到单词“seed”

但是一个算法怎么知道什么是种子呢？比如 seed 和 meed 有什么不同？他们都很亲近。好吧，你可以训练它，所以机器，你的电脑，学会识别“种子”，但避免像“杂草”、“需要”和“meed”这样的词

你训练它，给它一堆信息，包括很多单词，包括“种子”，你训练它识别单词“种子”是什么。然后你告诉它不断重复，直到它停留在单词“seed”上，足够多次而不会把它误认为另一个单词。这就是图像处理、自然语言处理和其他基于算法的机器学习模型所发生的事情。

但是，一旦算法学习了一项任务，一旦学习曲线跳跃，程序就可以更快地识别，并猛烈攻击这些新发现的知识。

算法可以增长知识，你可以教它的知识，或者它可以自己学习的知识。现在，它可以将图像与单词“seed”进行匹配接下来，它可以匹配几乎没有提到“seed”但与该单词相关的整个段落。它可以快速且全面准确地做到这一点。然后，在“深度学习”中，它可以开始自己决定与“种子”这个词的相关性它已经超越了你的训练和解释，给你和其他人的不是他们要求的，而是它认为你想要的。

## 好了，回到我们定期安排的媒体文章

但是一个失控和被操纵的算法是脸书首席执行官马克·扎克伯格在国会作证的原因。再加上几个糟糕的演员——呆在家里的俄罗斯黑客——你就有了滥用和灾难的配方。[仅仅通过几个“训练”广告，俄罗斯黑客就利用脸书的算法，用虚假的、未经证实的、耸人听闻的、但受欢迎的内容来攻击共和党人和右倾观众。](https://www.wired.com/story/russian-facebook-ads-targeted-us-voters-before-2016-election/)其他坏人利用脸书的 PageRank 算法对喜欢、分享和评论的偏好，在社交媒体上像野火一样传播虚假、阴谋的内容。

但是脸书不是唯一的一个。通过向消费其内容的眼球销售来赚钱的社交媒体平台使用基于量化的指标来增加收入。算法的量化追求焦点导致这些平台分发嵌入了种族主义、性别歧视和几乎所有人类已知的其他主义偏见的内容。

脸书的 PageRank 系统追逐喜欢、评论和分享，创造了一个字面上和比喻上的“过滤泡沫”，提供主流新闻无法渗透的单音符精彩内容。

## 将人工智能模型武器化以创造新的现实

前奥巴马特工伊莱·帕里泽[在 2011 年](https://www.ted.com/talks/eli_pariser_beware_online_filter_bubbles)警告说，在线平台正在为个人生活在另一个现实中创造条件，在这个现实中，只有他们的观点和信仰得到加强。2016 年的选举不仅仅是帕里泽的威胁成真。事实上，情况要糟糕得多。

威斯康辛大学新闻学教授杨·米金(Young Mie Kim)在 2016 年选举期间，对脸书平台上制作和分发的有针对性的俄罗斯广告做了广泛的[研究。她的研究既有启发性又令人沮丧，用赤裸裸的黑暗结果展示了算法模型的放大。](https://www.wired.com/story/russian-facebook-ads-targeted-us-voters-before-2016-election/)

她发现俄罗斯支持的互联网研究机构在脸书大量购买和分发包含极端、煽动性和未经证实的右倾指控的广告。据《连线》杂志报道，这里有一个例子:

> 多个可疑团体共享的一则广告写道:“老兵优先于非法移民。300，000 退伍军人死于等待退伍军人管理局的检查。非法移民每年的医疗费用为 11 亿美元。

俄罗斯的广告购买额约为 10 万美元，结果只有 3000 个广告。但同样，我们在谈论数据驱动工具的放大。由于脸书算法，这些广告得到了喜欢和分享，因此比实际的新闻内容更受欢迎。

事实上，哥伦比亚大学 Tow 数字新闻中心的研究主任 Jonathan Albright 发表了一份令人惊讶的研究报告，研究了脸书平台上有多少俄罗斯广告被分享。

使用脸书自己的分析工具和俄罗斯广告购买账户的名称，奥尔布赖特估计他们的内容在平台上被分享了 3.4 亿次。这超出了过滤气泡的范围，是尼亚加拉大瀑布大小的洪水。

下个月，我将写一篇文章，分析由于数据驱动平台，仅仅 3000 个广告如何影响数百万人及其选举决定。我将在今年年底结束我的关于对抗和击败 AI 武器化的方法的系列文章。

但除了俄罗斯的坏演员之外，如果该平台的数据模型没有被设计成篡夺人类行为，那么在我们上次选举中主导社交媒体平台的俄罗斯广告、虚假和阴谋内容的野火蔓延就永远不会发生。这不是偶然。*这些模型的设计是基于一些最普遍的、激励人类行为的偏见。*他们被设计成利用我们的偏见来对付我们。

如果没有一个名为可用性级联的非常真实的认知偏差，俄罗斯的广告可能会失败。简而言之，可用性级联是人们仅仅因为看到重复而相信某件事的倾向。谚语“重复某事足够长的时间，它就会成真，”描述了人们仅仅因为听到很多就不得不相信他们所听到的倾向。

前脸书工程师 Antonio Garcia Martinez 对 Buzzfeed 谈到脸书在 2016 年选举中的共谋时，放大了 Pariser 的金丝雀叫声，w [:](https://www.buzzfeed.com/charliewarzel/how-people-inside-facebook-are-reacting-to-the-companys?utm_term=.lmJ3V0VOx#.qtbyJ5Jqo)

> “我认为，民主是否能在脸书和所有其他类似脸书的平台上生存下来，这是一个真正的问题，”他说。“在脸书这样的平台出现之前，人们通常认为你有权保留自己的观点。现在，它更像是对自己现实的权利。”Buzzfeed 前脸书工程师。

你可以说这些都是利用工具做坏事的坏演员。事实上，这是脸书一开始反复说的。

> 雪莉·桑德伯格，首席运营官脸书公司说，直到 2017 年 10 月，“在我们的内心，我们是一家科技公司，我们雇佣工程师。我们不雇佣记者，没有人是记者，我们不报道新闻。”

但是越来越清楚的是，不仅仅是坏演员搞乱了脸书的模式。它远不止如此。

脸书坚称自己是一个平台，而不是内容出版商，这显然是错误的。一旦平台决定选择人们消费的内容，它就成了出版商。内容是否由平台创建并不重要。脸书创造了它的消费渠道。该公司使用数据驱动模型来决定哪些内容更好地提供给用户。

正是人们消费内容的欲望，大概是为了让他们留在平台上，并帮助他们赚取广告费，让该公司陷入了未知的道德困境，纠结于如何在保持平台纯净但受欢迎的同时，停止向他人传播仇恨言论、种族主义内容甚至辱骂性语言。

脸书现在正在创建程序来防止滥用其平台，但要解决这个问题，还需要发展一些基本信念。

一个信念的改变是根本性的:使用这些数据驱动工具的公司必须将自己视为世界和现实的设计师、创造者。

作为设计师，我们必须看到我们在这一切中的积极作用，不要再认为自己是一个被动的平台。

另一个需要发生的范式转变是更深入地理解人性，以及人类如何互动、使用数据驱动的工具以及受其影响。

当然，这也是需要道德准则的地方。

专家表示，科技公司及其“以量化为中心”的文化让社交媒体巨头对其数据驱动模式的后果视而不见。但是也有一种危险的想法，认为数据驱动的工具本身是无害的。坦率地说，这不是真的。这个工具并不比创造它的人更无害。正如我所展示的，这些模型是由人类创造、设计、训练和测试的。这意味着所有制造它们的东西都带有人类的特征。以不同的方式断言，就是无视事实。

但是，除了过滤泡沫和被操纵的产品，数据驱动的模型本身也可能造成大破坏。他们对效率的关注导致一些决策数据驱动模型将极端主义直接注入未知的地方。这里的关键是它是工具本身的一个公开动作。不是什么意外的后果。当然，这是真正的危险。

# 人工思维创造真正的行动

以谷歌的 YouTube 算法为例。嗯，它是专有的，所以你必须根据专家认为它是如何工作的来使用它。数据驱动模型不仅仅是根据用户的请求向他们分发内容。而是一旦算法学会了。它决定胜过用户。YouTube 的算法开始推荐与搜索请求相关的视频，但让直到几年前还被留在上午电台黑暗角落的内容浮出水面。

正如《纽约时报》、《T2》、《财富》杂志和[《华尔街日报》、](https://www.wsj.com/articles/how-youtube-drives-viewers-to-the-internets-darkest-corners-1518020478?utm_content=bufferf25b7&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)所详述的，谷歌的 YouTube 算法产生的内容结果远远超出了用户的要求。由于无法辨别推荐内容的类型，这种数据驱动的模式提供了用户原始搜索请求的极端版本。

它不再只是返回你所要求的，而是开始“推荐”它认为是内容的东西。《华尔街日报》的一项调查发现，这些内容比用户要求的要极端得多。

因此，像亚历克斯·琼斯、迈克尔·萨维奇、迈克·德拉吉这样的人，这些只被小部分听众所知的人，通过早上的广播和电视购物节目，突然开始在 YouTube 上显示足球妈妈的推荐列表，寻找当天的新闻信息。

例如，[《华尔街日报》的工作人员进行了一项搜索“联邦调查局备忘录”的实验，](https://www.wsj.com/articles/how-youtube-drives-viewers-to-the-internets-darkest-corners-1518020478?utm_content=bufferf25b7&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)当天的新闻都是关于共和党发布一份关于特朗普竞选期间情报官员行为的备忘录。结果是:

> 在 YouTube 上，在主流新闻来源的小缩略图之后，最热门的结果来自 BPEarthWatch，它自称“致力于观看导致我们的主耶稣基督回归的末日事件。彗星、小行星、地震、太阳耀斑和末日力量。”

他们还指出:

> 还有来自 Styxhexenhammer666 的视频，其信息页面上简单地写着，“我是上帝”，还有来自经常宣传阴谋论的网站 Infowars 的创始人亚历克斯·琼斯的视频。

如果我错了，请纠正我，但我从未听说过 BPEarthWatch，它们听起来肯定不像有人在键入“FBI 备忘录”时要找的东西。

脸书、推特和谷歌并没有恶意构建这些算法。但是他们忽视潜在危害的能力是一个重大错误。那么，我们究竟如何防止这种情况发生呢？

# 防止 AI“不可知无害”

除非你坚定地在军事、炮兵、轰炸或其他以死亡为中心的行业工作，否则你设计一个故意伤害人类、以数据为燃料的工具的可能性很小，但并非完全不可能。

这是无意的，无意的后果，对我们这些没有生活在边缘的人来说是危险的。这就是一套共享的道德准则最有用的地方。

如果没有一个有意的伦理讨论，无论是项目还是设计，技术人员、工程师和设计师都有落入狂妄自大的人性陷阱的风险，我把艾称为“不可知论无害”

不可知论无害是这种信念，因为模型使用数据，它免除了人类的弱点。在这篇我写的关于[人类理想为中心的设计的中间文章中，阅读更多关于不可知论无害以及如何设计我们摆脱这种哲学的方法。](https://medium.com/@writingprincess/human-ideal-centered-ai-design-bdda5953a9ef)

在这个领域工作多年后，首先作为一名记者做数据驱动的报道，现在作为一名设计研究员研究数据驱动的系统，我发现要想设计对人类的伤害最小，必须将道德准则融入创造过程。

伦理，即控制创造的是非观念，必须尽快解决。整个创作过程，然后把我的作品展示给一个伦理小组是不好的。一旦项目、想法、平台或产品实现，需要一个对人类行为有深刻理解的人在房间里，卷起袖子，准备与工程师、技术专家和数据科学家一起创造。

对一些人来说，答案是教会算法如何应对伦理困境。对我来说，这没有抓住重点。设计这些数据驱动模型的人必须首先进行伦理讨论。机器向我们学习。(至少目前如此……)

我研究这些模型已经四年多了，我发现有一些实用的、具体的道德准则，我试图遵循这些准则来确保无意的行为不会从无害变成有害。

从完全自愿但绝对有效的《新闻道德颂》(感谢职业记者协会)中得到启示，我精心制作了一个类似的道德框架，帮助我穿越大数据和设计的雷区。

这些道德准则并不意味着涵盖所有可能的情况。事实上，它们根本不是严格的指导方针。它们更像是谈话的引子和开始探索的话题，让你和你的设计团队走上无害的道路。

这些指导原则旨在当你抓住一个数据驱动的设计项目时充当煤矿中的金丝雀。

它们源于一个基础的地方，在那里善意是假定的，而故意消除有目的的坏行为者的愿望是你的核心。换句话说，我假设你从一个善良的地方开始。

如果数据挖掘收入数据的想法试图将穷人从获得房屋贷款中划出来，这让你想“嗨”，那么你就是我们这种数据设计师。其他人都寻求心理咨询。

让我们开始吧。

# 什么是伦理，什么不是伦理

要发展伦理原则，最好对什么是伦理原则有一个很好的理解。简单来说，伦理原则就是一个群体、文化、组织、行业或职业普遍认同的行为规则。它们是有共同点的人同意遵守的规则。(关于“伦理简介”，请查阅西蒙·布莱克本的《做好人》。"

谈到道德，人们喜欢用的最著名的例子之一是希波克拉底誓言。但那甚至是有问题的。因为它犯了一个常见的错误:混淆伦理和道德。(旁注:你读过经典的[希波克拉底誓言](https://en.wikipedia.org/wiki/Hippocratic_Oath)？这是一个相当有趣的使命宣言。)

![](img/a327c0fdc9960b11469916187fb358dd.png)

Wikipedia.com

该誓言以希波克拉底命名，希波克拉底是一位生活在公元前 460-370 年的希腊医生，其中蕴含了一些有趣的原则。当想到誓言时，大多数人会想起“首先，不要伤害”这句话，但当然它并没有确切地说“不要伤害”更像是，“让[病人]远离伤害和不公。”听起来不错。继续读。

如果你想了解伦理和道德原则之间的区别，那么看看医学誓言的经典版本和 1964 年由塔夫茨大学医学院院长路易斯·拉萨纳写的新的现代版本。(美国医学协会已经提出了一个[更长的道德准则，在道德上不那么说教。)](https://www.ama-assn.org/delivering-care/ama-code-medical-ethics)

> 经典的誓言将道德融入到既定的行为准则中。它说，宣誓者“既不会给任何索要致命药物的人，也不会提出这方面的建议。”

等等什么？禁止医生协助的自杀被写进了誓言。这是一种道德立场。一种偏见认为，即使人们要求结束生命，医生也不应该结束生命，尽管他们可以这样做。请继续阅读。

> 下一句经典的誓言说，“同样，我不会给一个女人流产的补救措施。我将保持我的生命和艺术的纯洁和神圣。”

说什么？是的，最初的希波克拉底誓言禁止医生辅助堕胎。甚至更偏向。

多年来，人们对誓言中的这两句话争议颇多。学者们说，大多数医学院和医生甚至没有意识到誓言中蕴含了多少道德；包括反复提到上帝、神和女神。20 世纪 60 年代，一个更加现代的誓言版本诞生了，它淡化了宗教含义，转而使用了更具思想性的语言。它包含了像“在生死问题上我必须小心行事”这样的短语。

正如你所看到的，创建伦理原则是一门艺术，既要提倡不伤害他人的最佳实践，又要忽略特定的道德标准和依赖于不具普遍性的尖锐观点的行为。你必须清楚，但不要太具体。你必须包容一切；但仍然具有普遍性。

# 从设计伦理到数据设计伦理

考虑到这一点，我们如何才能设计出对社会利大于弊的道德数据驱动产品呢？

好吧，让我们从作为以人为中心的设计师的原则开始。我们作为以人为中心的设计师最看重的是什么？

在 IDEO，我们说我们希望通过设计创造积极的影响。我们通过以人为中心的设计方法做到这一点，我们的首席执行官蒂姆·布朗称之为“设计思维”这种方法产生了一些设计的[原则，包括:](https://designthinking.ideo.com/?p=409)

*   透明度
*   参与性
*   上下文的
*   可持续的
*   改变的
*   鼓舞人心的

我认为，当处理由智能数据系统的创建或使用支持的设计项目时，这些设计原则不会改变。

有了这些作为指引，我就有了我所谓的“数据设计原则”这些是开始广泛探索主题的特定桶，将帮助您得出关于是设计数据引擎还是放弃的结论。

如果这些类别提出的问题的答案不是你想要的，让你觉得恶心，或者违背了你作为设计师的职业道德，那么你可能不应该设计这个数据引擎。

如果你这样做了，你需要制定协议，故障保险和调整，以确保你创造的东西，当规模扩大时，不会回来困扰你或其他任何人。在这些必须探索的类别中，有一些是我在涉及智能系统及其与人类的交集的项目中必须问的问题。这些类别和问题是:

## 以人为本

我创造的东西是以人类需求为中心的吗？它会为人类服务吗？它会让人类做得更好，成为更好，变得更好吗？简而言之，是以人为中心吗？如果不是，重新思考解决方案，使之成为现实。

## 保护隐私

我创造的东西会侵犯他人的隐私权吗？如果会，他们是如何被通知的，会受到什么影响？他们可以使用该产品并维护其个人隐私权吗？如果没有，他们知道他们牺牲了什么吗？获得的东西真的和基本的隐私权一样有价值吗？或者是给予和获得的交换是不平衡的。

## 保留身份选择

我创造的东西会剥夺一个人的匿名性吗，如果会，为什么，如何，会有什么后果，这是可以接受的吗，如果不可以，有什么方法可以防止吗？

## 创造安全

我创造的东西会伤害别人吗？如果是，如何以什么方式，为什么？有人能轻易地利用我创造的东西去伤害别人吗？有办法防止这种情况吗？我如何建立制衡机制来防止滥用和误用？

## 交付股权

我构建的东西可以用来伤害一个受保护的类吗？以什么方式？我建造的东西不包括人吗？为什么？我的设计公平吗？它是否以特权为出发点，永不偏离？我的模型能同样适用于不同种族、民族、背景和收入的人吗？

## 从良好的预期用途开始

该工具是否有预期的无害用途？即便如此，它最终会造成伤害吗？对谁，以什么方式？你如何防范虐待？我是否有检查点和里程碑来迭代和检查嵌入在设计中的模型？五年后我的模型会被如何使用？10?我怎么知道？

## 强调透明度

别人能追踪到我是如何创造这个数据驱动的产品的吗？我的过程听得见吗？有没有人可以追溯一下，了解一下我的模型是怎么创建的？如果不是为什么？所有相关人员都知道我创造了什么吗？如果是为什么不呢？

## 坚持法律的精神

我的工具合法吗？它是否违反了法律的条文甚至精神？除隐私之外，它是否遵守其他国家、州、联邦和国际法律？

将伦理融入人工智能和智能系统设计还有很多工作要做。这是一个复杂的问题。但是我们不能再假装数据是无偏见的。是人创造的。它有它的问题。任何根据数据创建的模型都可能存在这些问题。