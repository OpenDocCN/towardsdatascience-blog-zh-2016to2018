<html>
<head>
<title>Topic Modeling and Latent Dirichlet Allocation (LDA) in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的主题建模和潜在狄利克雷分配</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24?source=collection_archive---------0-----------------------#2018-05-31">https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24?source=collection_archive---------0-----------------------#2018-05-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/adc4db055f1234a2d462becec79cd104.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LeJ3vMxW4ZvcS4ZsDyJDYA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo Credit: Pixabay</figcaption></figure><p id="8704" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae la" href="https://en.wikipedia.org/wiki/Topic_model" rel="noopener ugc nofollow" target="_blank"> <strong class="ke ir">主题建模</strong> </a>是一种用于发现文档集合中出现的抽象“主题”的统计建模。<a class="ae la" href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/" rel="noopener ugc nofollow" target="_blank"> <strong class="ke ir">潜在狄利克雷分配</strong> </a> (LDA)是主题模型的一个例子，用于将文档中的文本分类到特定主题。它建立了每个文档的主题模型和每个主题的单词模型，建模为 Dirichlet 分布。</p><p id="1cb4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这里我们将把 LDA 应用于一组文档，并把它们分成主题。我们开始吧！</p><h1 id="4515" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据</h1><p id="7e89" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">我们将使用的数据集是 15 年间出版的超过 100 万条新闻标题的列表，可以从<a class="ae la" href="https://www.kaggle.com/therohk/million-headlines/data" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="791f" class="mn lc iq mj b gy mo mp l mq mr">import pandas as pd</span><span id="cdb4" class="mn lc iq mj b gy ms mp l mq mr">data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);<br/>data_text = data[['headline_text']]<br/>data_text['index'] = data_text.index<br/>documents = data_text</span></pre><p id="9f85" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">看一眼数据。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9be9" class="mn lc iq mj b gy mo mp l mq mr">print(len(documents))<br/>print(documents[:5])</span></pre><p id="51eb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> 1048575 </em> </strong></p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/4dd9f551c4fc8943bb174a3c88bde9d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*9QcQgSW5CAlsJB1YiJMA2Q.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1</figcaption></figure><h1 id="9ebc" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据预处理</h1><p id="2ccb" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">我们将执行以下步骤:</p><ul class=""><li id="cb90" class="mv mw iq ke b kf kg kj kk kn mx kr my kv mz kz na nb nc nd bi translated"><strong class="ke ir">分词</strong>:将文本拆分成句子，句子拆分成单词。将单词小写，去掉标点符号。</li><li id="ccde" class="mv mw iq ke b kf ne kj nf kn ng kr nh kv ni kz na nb nc nd bi translated">少于 3 个字符的单词将被删除。</li><li id="d5c9" class="mv mw iq ke b kf ne kj nf kn ng kr nh kv ni kz na nb nc nd bi translated">所有<strong class="ke ir">停用字</strong>都被移除。</li><li id="38c7" class="mv mw iq ke b kf ne kj nf kn ng kr nh kv ni kz na nb nc nd bi translated">单词被词汇化了——第三人称的单词变成了第一人称，过去时态和将来时态的动词变成了现在时态。</li><li id="2d43" class="mv mw iq ke b kf ne kj nf kn ng kr nh kv ni kz na nb nc nd bi translated">单词被词干化——单词被还原成它们的词根形式。</li></ul><p id="5a5e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">加载 gensim 和 nltk 库</strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="ab89" class="mn lc iq mj b gy mo mp l mq mr">import gensim<br/>from gensim.utils import simple_preprocess<br/>from gensim.parsing.preprocessing import STOPWORDS<br/>from nltk.stem import WordNetLemmatizer, SnowballStemmer<br/>from nltk.stem.porter import *<br/>import numpy as np<br/>np.random.seed(2018)</span><span id="9cc6" class="mn lc iq mj b gy ms mp l mq mr">import nltk<br/>nltk.download('wordnet')</span></pre><p id="abe2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mt">【nltk _ data】下载包 wordnet 到<br/>【nltk _ data】C:\ Users \ Susan Li \ AppData \ Roaming \ nltk _ data…<br/>【nltk _ data】包 wordnet 已经是最新的了！</em>T29】</strong></p><p id="6e21" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">真</em> </strong></p><p id="9087" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">编写一个函数，对数据集</strong>执行词汇化和词干预处理步骤。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1261" class="mn lc iq mj b gy mo mp l mq mr">def lemmatize_stemming(text):<br/>    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))</span><span id="b1ad" class="mn lc iq mj b gy ms mp l mq mr">def preprocess(text):<br/>    result = []<br/>    for token in gensim.utils.simple_preprocess(text):<br/>        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:<br/>            result.append(lemmatize_stemming(token))<br/>    return result</span></pre><p id="d03d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">选择预处理后的文件进行预览</strong>。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="ab65" class="mn lc iq mj b gy mo mp l mq mr">doc_sample = documents[documents['index'] == 4310].values[0][0]</span><span id="a1bf" class="mn lc iq mj b gy ms mp l mq mr">print('original document: ')<br/>words = []<br/>for word in doc_sample.split(' '):<br/>    words.append(word)<br/>print(words)<br/>print('\n\n tokenized and lemmatized document: ')<br/>print(preprocess(doc_sample))</span></pre><p id="2ae2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">原始文档:</em> </strong></p><p id="7aed" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> ['雨'，'帮忙'，'浇灭'，'丛林大火'] </em> </strong></p><p id="08ea" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">标记化和词条化的文档:</em> </strong></p><p id="395a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> ['雨'，'救命'，'浇灭'，'灌木'] </em> </strong></p><p id="e0ae" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">成功了！</p><p id="d3ec" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">预处理标题文本，将结果保存为‘processed _ docs’</strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="6e69" class="mn lc iq mj b gy mo mp l mq mr">processed_docs = documents['headline_text'].map(preprocess)<br/>processed_docs[:10]</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/3d51ef5b7d3a18ca46a86a3adf54adc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*pjav3VLRNQIFe1eWAqsHRA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2</figcaption></figure><h1 id="62e8" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据集上的单词包</h1><p id="24bf" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">从“processed_docs”创建一个字典，其中包含一个单词在训练集中出现的次数。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5d75" class="mn lc iq mj b gy mo mp l mq mr">dictionary = gensim.corpora.Dictionary(processed_docs)</span><span id="ab4a" class="mn lc iq mj b gy ms mp l mq mr">count = 0<br/>for k, v in dictionary.iteritems():<br/>    print(k, v)<br/>    count += 1<br/>    if count &gt; 10:<br/>        break</span></pre><p id="0ff4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> 0 播出</em> </strong></p><p id="f260" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> 1 共产主义</em> </strong></p><p id="c732" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> 2 分钟</em> </strong></p><p id="2c16" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">T5 3 许可 T7】</strong></p><p id="ef40" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mt">4</em></strong></p><p id="4c34" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> 5 默认</em> </strong></p><p id="d2e2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> 6 机智</em> </strong></p><p id="6161" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mt">⑦谓</em> </strong></p><p id="d1da" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> 8 基础设施</em> </strong></p><p id="1eef" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> 9 保护</em> </strong></p><p id="d372" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> 10 峰会</em> </strong></p><p id="0cc4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> Gensim 过滤器 _ 极端情况</strong></p><p id="d538" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">过滤掉出现在</strong>中的令牌</p><ul class=""><li id="8198" class="mv mw iq ke b kf kg kj kk kn mx kr my kv mz kz na nb nc nd bi translated">少于 15 份文件(绝对数量)或</li><li id="8dda" class="mv mw iq ke b kf ne kj nf kn ng kr nh kv ni kz na nb nc nd bi translated">超过 0.5 个文档(总语料库大小的一部分，而不是绝对数量)。</li><li id="50bd" class="mv mw iq ke b kf ne kj nf kn ng kr nh kv ni kz na nb nc nd bi translated">完成上述两个步骤后，只保留前 100000 个最常用的令牌。</li></ul><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="b883" class="mn lc iq mj b gy mo mp l mq mr">dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)</span></pre><p id="3c47" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> Gensim doc2bow </strong></p><p id="7854" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于每个文档，我们创建一个字典，报告有多少<br/>单词以及这些单词出现了多少次。将此保存到“bow_corpus ”,然后检查我们之前选择的文档。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="63bf" class="mn lc iq mj b gy mo mp l mq mr">bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]<br/>bow_corpus[4310]</span></pre><p id="3b1e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> [(76，1)，(112，1)，(483，1)，(3998，1)] </em> </strong></p><p id="2410" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">我们的样本预处理文档</strong>的单词预览包。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9f63" class="mn lc iq mj b gy mo mp l mq mr">bow_doc_4310 = bow_corpus[4310]</span><span id="2a6f" class="mn lc iq mj b gy ms mp l mq mr">for i in range(len(bow_doc_4310)):<br/>    print("Word {} (\"{}\") appears {} time.".format(bow_doc_4310[i][0], <br/>                                               dictionary[bow_doc_4310[i][0]], <br/>bow_doc_4310[i][1]))</span></pre><p id="5fb7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">字 76(“bush fir”)出现 1 次。</em> </strong></p><p id="728d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">字 112(“帮助”)出现 1 次。</em>T56】</strong></p><p id="3b75" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">字 483(“雨”)出现 1 次。</em> </strong></p><p id="f674" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">字 3998(“挫伤”)出现 1 次。</em> </strong></p><h1 id="d3d5" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">TF-IDF</h1><p id="4b35" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">使用模型创建 tf-idf 模型对象。' bow_corpus '上的 TfidfModel 并将其保存到' tfidf '，然后对整个语料库应用转换并将其命名为' corpus_tfidf '。最后，我们预览了第一份文档的 TF-IDF 分数。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="3974" class="mn lc iq mj b gy mo mp l mq mr">from gensim import corpora, models</span><span id="ab83" class="mn lc iq mj b gy ms mp l mq mr">tfidf = models.TfidfModel(bow_corpus)<br/>corpus_tfidf = tfidf[bow_corpus]</span><span id="0af9" class="mn lc iq mj b gy ms mp l mq mr">from pprint import pprint</span><span id="38eb" class="mn lc iq mj b gy ms mp l mq mr">for doc in corpus_tfidf:<br/>    pprint(doc)<br/>    break</span></pre><p id="8593" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> [(0，0.5907943557842693)，</em> </strong></p><p id="3277" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> (1，0.3900924708457926)</em></strong></p><p id="18af" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> (2，0.49514546614015836)</em></strong></p><p id="224b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> (3，0.5036078441840635)】</em></strong></p><h1 id="6a0f" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">运行 LDA 使用的包字</strong></h1><p id="2d33" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">使用 gensim.models.LdaMulticore 训练我们的 lda 模型，并将其保存到“lda_model”</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9473" class="mn lc iq mj b gy mo mp l mq mr">lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)</span></pre><p id="6879" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于每个主题，我们将探索在该主题中出现的单词及其相对权重。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="0a92" class="mn lc iq mj b gy mo mp l mq mr">for idx, topic in lda_model.print_topics(-1):<br/>    print('Topic: {} \nWords: {}'.format(idx, topic))</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/1764b27d388574e2dabd75bf238eff9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yCd5BcHDDWMFF7emZu1VcA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3</figcaption></figure><p id="cb04" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">你能用每个题目中的单词和它们对应的权重来区分不同的题目吗？</p><h1 id="7b2f" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">使用 TF-IDF 运行 LDA</strong></h1><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="eeb4" class="mn lc iq mj b gy mo mp l mq mr">lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)</span><span id="ad30" class="mn lc iq mj b gy ms mp l mq mr">for idx, topic in lda_model_tfidf.print_topics(-1):<br/>    print('Topic: {} Word: {}'.format(idx, topic))</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nl"><img src="../Images/7aa5ca00aaa7c6ee60baf153f80de682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_HZ-9HChNGIOBcWmOD5Pnw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4</figcaption></figure><p id="b0cd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">还是那句话，能不能用每个题目里的词和对应的权重来区分不同的题目？</p><h1 id="96c1" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">使用 LDA 词袋模型对样本文档进行分类的性能评估</strong></h1><p id="f55a" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">我们将检查我们的测试文档将被分类到哪里。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="523d" class="mn lc iq mj b gy mo mp l mq mr">processed_docs[4310]</span></pre><p id="1451" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt"> ['雨'，'帮助'，'挫伤'，'灌木'] </em> </strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="a5aa" class="mn lc iq mj b gy mo mp l mq mr">for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):<br/>    print("\nScore: {}\t \nTopic: {}".format(score, lda_model.print_topic(index, 10)))</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/6fa6f62ae535613e790924220a7381e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z15r4Z41s63arOH4grauoA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 5</figcaption></figure><p id="1e9c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们的测试文档最有可能成为我们的模型分配的主题的一部分，这就是准确的分类。</p><h1 id="bafd" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">使用 LDA TF-IDF 模型对样本文档进行分类的性能评估。</strong></h1><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="b227" class="mn lc iq mj b gy mo mp l mq mr">for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):<br/>    print("\nScore: {}\t \nTopic: {}".format(score, lda_model_tfidf.print_topic(index, 10)))</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nn"><img src="../Images/ee405216fedd407b944ed3a0a43cd33d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D5Me29QJ-UEgXRWUXQcVYA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 6</figcaption></figure><p id="e291" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们的测试文档最有可能成为我们的模型分配的主题的一部分，这就是准确的分类。</p><h1 id="b144" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">在看不见的文档上测试模型</strong></h1><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="598f" class="mn lc iq mj b gy mo mp l mq mr">unseen_document = 'How a Pentagon deal became an identity crisis for Google'<br/>bow_vector = dictionary.doc2bow(preprocess(unseen_document))</span><span id="bd76" class="mn lc iq mj b gy ms mp l mq mr">for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):<br/>    print("Score: {}\t Topic: {}".format(score, lda_model.print_topic(index, 5)))</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/1bc07df1cb5ade6d4775ef48254fccff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wttNUhFvMOB11YFLv02bfg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 7</figcaption></figure><p id="e56e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">源代码可以在<a class="ae la" href="https://github.com/susanli2016/NLP-with-Python/blob/master/LDA_news_headlines.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。我期待听到任何反馈或问题。</p><p id="3f92" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">参考:</p><p id="266d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae la" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank"> Udacity — NLP </a></p></div></div>    
</body>
</html>