<html>
<head>
<title>A Year in Computer Vision — Part 4 of 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算机视觉一年——第 4 部分，共 4 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-year-in-computer-vision-part-4-of-4-515c61d41a00?source=collection_archive---------10-----------------------#2017-10-02">https://towardsdatascience.com/a-year-in-computer-vision-part-4-of-4-515c61d41a00?source=collection_archive---------10-----------------------#2017-10-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="15e9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">—第四部分:ConvNet 架构、数据集、不可分组的附加内容</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><blockquote class="km kn ko"><p id="4f57" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">下面这篇文章摘自我们的研究团队最近编辑的关于计算机视觉领域的出版物。所有零件现在都可以通过我们的网站以及媒体上的<a class="ae lm" href="https://medium.com/@info_84181/a-year-in-computer-vision-part-1-of-4-eaeb040b6f46" rel="noopener">第 1 部分</a>、<a class="ae lm" href="https://medium.com/@info_84181/a-year-in-computer-vision-part-2-of-4-893e18e12be0" rel="noopener">第 2 部分</a>和<a class="ae lm" href="https://medium.com/@info_84181/a-year-in-computer-vision-part-3-of-4-861216d71607" rel="noopener">第 3 部分</a>获得。</p></blockquote><p id="b6ae" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">现在可以通过</strong><a class="ae lm" href="http://www.themtank.org" rel="noopener ugc nofollow" target="_blank"><em class="kr">www.themtank.org</em></a>在我们的网站上免费获得完整的出版物</p><p id="fa1f" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">我们鼓励读者通过我们自己的网站查看这篇文章，因为我们包括嵌入的内容和简单的导航功能，使报告尽可能动态。我们的网站不会给团队带来任何收入，只是为了让读者尽可能地感受到这些材料的吸引力和直观性。我们竭诚欢迎对演示的任何反馈！</p><blockquote class="km kn ko"><p id="43c5" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">请关注、分享和支持我们的工作，无论你喜欢的渠道是什么(请尽情鼓掌！).如有任何问题或想了解对未来作品的潜在贡献，请随时联系编辑:info@themtank.com</p></blockquote></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="b74c" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">第四部分:ConvNet 架构、数据集、不可分组的附加内容</h1><h1 id="a012" class="lq lr iq bd ls lt mi lv lw lx mj lz ma jw mk jx mc jz ml ka me kc mm kd mg mh bi translated">ConvNet 架构</h1><p id="168b" class="pw-post-body-paragraph kp kq iq ks b kt mn jr kv kw mo ju ky ln mp lb lc lo mq lf lg lp mr lj lk ll ij bi translated">ConvNet 架构最近在计算机视觉之外发现了许多新颖的应用，其中一些将在我们即将出版的出版物中介绍。然而，它们继续在计算机视觉中占据显著地位，架构的进步为本文中提到的许多应用和任务提供了速度、精度和训练方面的改进。</p><p id="287e" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">因此，ConvNet 架构对整个计算机视觉至关重要。以下是 2016 年一些值得注意的 ConvNet 架构，其中许多架构都是从 ResNets 最近的成功中获得灵感的。</p><ul class=""><li id="4ca4" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated"><strong class="ks ir"> Inception-v4、Inception-ResNet 和剩余连接对学习的影响</strong>【131】—呈现 Inception v4，这是一个新的 Inception 架构，从 2015 年底开始建立在 Inception v2 和 v3 的基础上。[132]该论文还提供了使用剩余连接来训练初始网络以及一些剩余-初始混合网络的分析。</li><li id="3aa3" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">密集连接的卷积网络</strong>【133】或“DenseNets”直接从 ResNets 的身份/跳跃连接中获得灵感。该方法将这一概念扩展到 ConvNets，使每一层以前馈方式连接到每一层，共享来自先前层的特征地图作为输入，从而创建密集网络。</li></ul><p id="e866" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">DenseNets 有几个引人注目的优点:它们缓解了消失梯度问题，加强了特征传播，鼓励特征重用，并大大减少了参数的数量。[134]</p><p id="eb6d" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">图 16</strong>:dense net 架构示例</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/5be8cf68869ccdcfce55755a436d3e7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/0*b8-SI-okjMF5rvbK.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Note</strong>: A 5-layer dense block with a growth rate of <em class="nt">k = 4</em>. Each layer takes all preceding feature-maps as input. <strong class="bd ns">Source</strong>: Huang et al. (2016)[135]</figcaption></figure><p id="0126" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">在 CIFAR-10、CIFAR-100、SVHN 和 ImageNet 上对模型进行了评估；它在很多方面都达到了 SOTA。令人印象深刻的是，DenseNets 实现了这些结果，同时使用更少的内存和降低的计算要求。有多个实现(Keras，Tensorflow 等)<a class="ae lm" href="https://github.com/liuzhuang13/DenseNet" rel="noopener ugc nofollow" target="_blank">这里</a>。[136]</p><ul class=""><li id="a789" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated"><strong class="ks ir"> FractalNet </strong> <strong class="ks ir">无残差超深度神经网络</strong>【137】<strong class="ks ir"/>—利用不同长度的交互子路径，没有直通或残差连接，而是使用滤波器和非线性变换来改变内部信号。</li></ul><p id="8b28" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><em class="kr"> FractalNets 将多个平行层序列与不同数量的卷积块重复组合，以获得较大的标称深度，同时在网络中保持许多短路径</em>。[138]</p><p id="2717" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">该网络在 CIFAR 和 ImageNet 上实现了 SOTA 性能，同时展示了一些附加属性。例如，他们对残差在极深网络的成功中的作用提出了质疑，同时也提供了对通过各种子网深度获得的答案的本质的洞察。</p><ul class=""><li id="62e9" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated"><strong class="ks ir">让我们保持简单:使用简单的架构来超越更深层次的架构</strong>【139】专注于创建一个简化的母架构。该架构在“CIFAR10/100、MNIST 和 SVHN 等数据集上实现了 SOTA 结果，或与现有方法相当，只需简单或无需数据扩充”。我们认为他们的原话是对这一动机的最好描述:</li></ul><p id="efa5" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><em class="kr">在这项工作中，我们提出了一个非常简单的 13 层完全卷积网络架构，对新功能的依赖最小，其性能优于几乎所有更深层的架构，参数减少了 2 到 25 倍。我们的架构可以成为许多场景的非常好的候选，尤其是在嵌入式设备中的应用。”</em></p><p id="caa8" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><em class="kr">“可以使用深度压缩等方法对其进行进一步压缩，从而大幅降低其内存消耗。我们有意尝试创建一个对最近提出的新功能依赖最小的母架构，以展示一个精心制作但简单的卷积架构的有效性，该架构随后可以通过文献中介绍的现有或新方法进行增强。</em>【140】</p><p id="b884" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">以下是一些补充 ConvNet 架构的附加技术:</p><ul class=""><li id="c9b2" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated"><strong class="ks ir"> Swapout:学习深度架构的集合</strong> [141]概括了丢弃和随机深度方法，以防止特定层和跨网络层的单元的共同适应。集成训练方法从包括“<em class="kr">丢失、随机深度和剩余架构</em>”的多个架构中采样。Swapout 在 CIFAR-10 和 CIFAR-100 上的表现优于相同网络结构的 ResNets，可以归类为一种正则化技术。</li><li id="284a" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">SqueezeNet</strong>【142】<strong class="ks ir"/>认为，更小的 dnn 提供了各种好处，从更少的计算负担训练到更容易的信息传输和对存储或处理能力有限的设备的操作。SqueezeNet 是一个小型 DNN 架构，它实现了“AlexNet 级别的准确性，使用模型压缩技术显著降低了参数和内存需求，使其比 AlexNet 小 510 倍。”</li></ul><p id="45cc" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">传统上，校正线性单元(ReLU)是所有神经网络的主要激活函数。然而，这里有一些最近的选择:</p><ul class=""><li id="5039" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated"><strong class="ks ir">串接整流线性单元(CRelu)</strong>【143】</li><li id="e166" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">指数线性单位(ELUs)</strong>【144】从 2015 年收盘</li><li id="f2d7" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">参数指数线性单元(PELU)</strong>【145】</li></ul><p id="7f03" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">向等方差转换</strong></p><p id="8115" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">ConvNets 具有平移不变性，这意味着它们可以识别图像多个部分中的相同特征。然而，典型的 CNN 并不是旋转不变的——这意味着如果一个特征或整个图像被旋转，那么网络的性能就会受到影响。通常，ConvNets 通过数据扩充(例如，在训练期间有目的地以小的随机量旋转图像)来学习(某种程度上)处理旋转不变性。这意味着网络获得轻微的旋转不变属性，而无需将旋转不变性特别设计到网络中。这意味着旋转不变性在使用当前技术的网络中受到根本限制。这与人类有一个有趣的相似之处，人类通常在识别颠倒的字符方面表现更差，尽管机器没有理由遭受这种限制。</p><p id="9541" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">下面的论文解决了<strong class="ks ir">旋转不变变换</strong>。虽然每种方法都有新颖之处，但它们都通过更有效地使用参数来提高旋转不变性，从而最终实现全局旋转等变:</p><ul class=""><li id="e5c2" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated"><strong class="ks ir">谐波 CNN</strong>【146】用‘圆谐波’代替常规 CNN 滤波器。</li><li id="6347" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">分组等变卷积网络(G-CNNs)</strong>【147】使用 G-卷积，这是一种新型层，与常规卷积层相比，<em class="kr">享有更高程度的权重共享，并在不增加参数数量的情况下增加网络的表达能力。</em></li><li id="4cc3" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">利用卷积神经网络中的循环对称性</strong>【148】<strong class="ks ir"/>将四种操作呈现为层，这些层增强神经网络层以部分增加旋转等方差。</li><li id="1c99" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">可控 CNN</strong>【149】<strong class="ks ir">—</strong>Cohen 和 Welling 在他们对<strong class="ks ir"> G-CNNs </strong>所做工作的基础上，证明了“<em class="kr">可控架构”</em>在 CIFARs 上优于剩余和密集网络。它们还提供了不变性问题的简明概述:</li></ul><p id="9369" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><em class="kr">为了提高机器学习方法的统计效率，许多人寻求学习不变表示。然而，在深度学习中，中间层不应该完全不变，因为局部特征的相对姿态必须为进一步的层保留。因此，人们会想到</em> <strong class="ks ir"> <em class="kr">等变</em> </strong> <em class="kr">的概念:如果网络产生的表示在输入的变换下以可预测的线性方式变换，则网络是等变的。换句话说，等变网络产生可操纵的表示。可操控性使得不仅可以在每个位置(如在标准卷积层中)应用过滤器，还可以在每个姿势中应用过滤器，从而允许增加参数共享。</em>107</p><p id="1edf" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">剩余网络</strong></p><p id="79ed" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">图 17</strong>:CIFAR 数据集上的测试错误率</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/1ca462ee358d466113b7523fe112a6d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/0*yKr4q3WlSsX83K1q.jpg"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Note</strong>: Yellow highlight indicates that these papers feature within this piece. Pre-resnet refers to “<em class="nt">Identity Mappings in Deep Residual Networks</em>” (see following section). Furthermore, while not included in the table we believe that <em class="nt">“</em><strong class="bd ns"><em class="nt">Learning Identity Mappings with Residual Gates</em></strong>” produced some of the lowest error rates of 2016 with 3.65% and 18.27% on CIFAR-10 and CIFAR-100, respectively.</figcaption></figure><p id="37db" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">资料来源</strong>:阿卜迪和纳哈万迪(2016 年，第 6 页)[150]</p><p id="e7d0" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">继微软的 ResNet[151]取得成功之后，剩余网络及其变体在 2016 年变得非常受欢迎，现在有许多开源版本和预训练模型可用。2015 年，ResNet 在 ImageNet 的检测、定位和分类任务以及 COCO 的检测和分割挑战中获得第一名。虽然关于深度的问题仍然很多，但 ResNets 对消失梯度问题的处理为“<em class="kr">深度增加产生高级抽象</em>”哲学提供了更多动力，这种哲学支撑着目前的许多深度学习。</p><p id="99af" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">ResNets 通常被认为是较浅网络的集合，它通过运行与其卷积层平行的快捷连接，在某种程度上抵消了深度神经网络(DNNs)的分层性质。这些快捷方式或<strong class="ks ir"> <em class="kr">跳过连接</em> </strong>通过允许梯度在整个网络层中更容易地反向传播，减轻了与 DNNs 相关联的消失/爆炸梯度问题。更多信息，这里有一个 Quora 线程可用<a class="ae lm" href="https://www.quora.com/What-is-an-intuitive-explanation-of-Deep-Residual-Networks" rel="noopener ugc nofollow" target="_blank"/>。[152]</p><p id="3fd6" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">剩余学习、理论和改进</strong></p><ul class=""><li id="4257" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated"><strong class="ks ir">广泛的剩余网络</strong>【153】现在是一种极其常见的 ResNet 方法。作者对 ResNet 块的体系结构进行了实验研究，并通过增加网络的宽度和减少网络的深度来改善剩余网络的性能，从而缓解了特征重用问题。这种方法在多个基准上产生了新的 SOTA，包括在 CIFAR-10 和 CIFAR-100 上分别为 3.89%和 18.3%。作者表明，16 层深宽的 ResNet 在准确性和效率方面与许多其他 ResNet(包括 1000 层网络)一样好，甚至更好。</li><li id="2fef" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">具有随机深度的深度网络</strong> [154]本质上是将丢弃应用于整个神经元层，而不是单个神经元束。"<em class="kr">我们从非常深的网络开始，但在训练过程中，对于每个小批量，随机丢弃一个子集的层，并用身份函数绕过它们。</em>“随机深度允许更快的训练和更好的准确性，即使训练网络超过 1200 层。</li><li id="6c92" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">利用剩余门学习身份映射</strong>【155】—“<em class="kr">通过使用标量参数来控制每个门，我们提供了一种仅通过优化一个参数来学习身份映射的方法。</em>“作者使用这些门控结果来改善深度模型的优化，同时提供‘对全层移除的高容忍度’,使得 90%的性能在随机显著移除后保持不变。使用宽门控结果，该模型在 CIFAR- 10 和 CIFAR-100 上分别实现了 3.65%和 18.27%的误差。</li><li id="635b" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">剩余网络的行为类似于相对较浅的网络的集合</strong>【156】<strong class="ks ir"/>——剩余网络可以被视为许多路径的集合，这些路径并不强烈依赖于彼此，因此强化了集合行为的概念。此外，剩余路径在长度上有所不同，短路径在训练过程中会产生梯度，而较深的路径在此阶段不会产生梯度。</li><li id="c928" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">深度剩余网络中的身份映射</strong> [157]是对原 Resnet 作者、何、、任、的改进。标识映射显示为允许“当用作跳过连接和添加后激活时，在任何 ResNet 块之间传播前向和后向信号”。该方法在 CIFAR-10 (4.62%误差)和 CIFAR-100 上使用 1001 层 ResNet，在 ImageNet 上使用 200 层 ResNet，改进了推广、训练和结果"<em class="kr">。</em></li><li id="6bf2" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">多重残差网络:提高残差网络的速度和准确性</strong> [158]再次倡导 ResNet 的整体行为，并支持对 ResNet 架构采用更广泛、更深入的方法。<em class="kr">所提出的多残差网络增加了残差块中残差函数的数量。</em>“精度提高后，CIFAR-10 和 CIFAR-100 的误差分别为 3.73%和 19.45%。图 17 中的表格摘自本文，考虑到 2017 年迄今所做的工作，有更多最新版本可用。</li></ul><p id="5f3a" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">其他剩余理论和改进<br/> </strong>虽然这是一个相对较新的概念，但目前有相当多的工作是围绕剩余理论创建的。以下是我们希望向感兴趣的读者强调的一些额外的理论和改进:</p><ul class=""><li id="5488" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated"><a class="ae lm" href="https://arxiv.org/abs/1612.07771" rel="noopener ugc nofollow" target="_blank">高速公路和剩余网络学习展开迭代估计</a>【159】</li><li id="7670" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><a class="ae lm" href="https://arxiv.org/abs/1609.05672" rel="noopener ugc nofollow" target="_blank">残差网络的残差网络:多级残差网络</a>【160】</li><li id="299a" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><a class="ae lm" href="https://arxiv.org/abs/1603.08029" rel="noopener ugc nofollow" target="_blank"> Resnet 中的 Resnet:一般化剩余架构</a>【161】</li><li id="0c40" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><a class="ae lm" href="https://arxiv.org/abs/1611.10080" rel="noopener ugc nofollow" target="_blank">更宽或更深:重新审视用于视觉识别的 ResNet 模型</a>【162】</li><li id="b779" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><a class="ae lm" href="https://arxiv.org/abs/1604.03640" rel="noopener ugc nofollow" target="_blank">弥合剩余学习、递归神经网络和视觉皮层之间的差距</a>【163】</li><li id="1331" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><a class="ae lm" href="https://arxiv.org/abs/1606.05262" rel="noopener ugc nofollow" target="_blank">卷积剩余记忆网络</a>【164】</li><li id="c099" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><a class="ae lm" href="https://arxiv.org/abs/1611.04231" rel="noopener ugc nofollow" target="_blank">深度学习中的身份问题</a>【165】</li><li id="a160" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><a class="ae lm" href="https://arxiv.org/abs/1604.04112" rel="noopener ugc nofollow" target="_blank">具有指数线性单元的深度残差网络</a>【166】</li><li id="1bbe" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><a class="ae lm" href="https://arxiv.org/abs/1605.08831" rel="noopener ugc nofollow" target="_blank">深度网络的加权残差</a>【167】</li></ul><h1 id="498a" class="lq lr iq bd ls lt mi lv lw lx mj lz ma jw mk jx mc jz ml ka me kc mm kd mg mh bi translated">数据集</h1><p id="d0e8" class="pw-post-body-paragraph kp kq iq ks b kt mn jr kv kw mo ju ky ln mp lb lc lo mq lf lg lp mr lj lk ll ij bi translated">丰富的数据集对于机器学习的所有方面的重要性不能被夸大。因此，我们认为包含该领域中一些最大的进步是谨慎的。套用 Kaggle 的联合创始人兼首席技术官 Ben Hamner 的话，“一个新的数据集可以让一千篇论文繁荣发展”，[168]也就是说，数据的可用性可以促进新的方法，也可以为以前无效的技术注入新的活力。</p><p id="fbd3" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">2016 年，ImageNet[169]、上下文中的公共对象(COCO)[170]、CIFARs[171]和 MNIST[172]等传统数据集加入了大量新条目。我们还注意到，图形技术的进步刺激了合成数据集的兴起。合成数据集是人工神经网络(ann)的大数据需求的一个有趣的解决方案。为了简洁起见，我们选择了(主观上)最重要的 2016 年<em class="kr">新</em>数据集:</p><ul class=""><li id="1244" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated"><strong class="ks ir">地点 2</strong>【173】是一个场景分类数据集，即任务是给一幅图像标注一个场景类别，如“体育场”、“公园”等。虽然预测模型和图像理解无疑将通过 Places2 数据集得到改善，但根据该数据集训练的网络的一个有趣发现是，在学习对场景进行分类的过程中，网络学会了在没有明确学习的情况下检测其中的对象。例如，卧室里有床，水槽可以在厨房和浴室里。这意味着对象本身是场景分类的抽象层次中的较低级特征。</li></ul><p id="b758" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">图 18</strong>:scene net RGB-D 的示例</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi nv"><img src="../Images/f646208dfd0c5a5a581776168372778e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xTFk_5ZC3Iz_3BWp.jpg"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Note</strong>: Examples taken from SceneNet RGB-D, a dataset with 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth. The photo (a) is rendered through computer graphics with available ground truth for specific tasks from (b) to (e). Creation of synthetic datasets should aid the process of domain adaptation. Synthetic datasets are somewhat pointless if the knowledge learned from them cannot be applied to the real world. This is where domain adaptation comes in, which refers to this transfer learning process of moving knowledge from one domain to another, e.g. from synthetic to real-world environments. Domain adaptation has recently been improving very rapidly again highlighting the recent efforts in transfer learning. Columns © vs (d) show the difference between instance and semantic/class segmentation.</figcaption></figure><p id="550c" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">资料来源</strong>:麦科马克等人(2017)[174]</p><ul class=""><li id="d12e" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated"><strong class="ks ir">SceneNet RGB-D</strong>【175】<strong class="ks ir"/>—该合成数据集扩展了原始 scene net 数据集，并为场景理解问题(如语义分割、实例分割和对象检测)以及几何计算机视觉问题(如光流、深度估计、相机姿态估计和 3D 重建)提供了像素级的真实情况。数据集通过提供像素完美的表示来细化所选的环境。</li><li id="750e" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">cm places</strong>【176】是麻省理工学院的跨模态场景数据集。任务是识别自然图像之外的许多不同模态的场景，并在此过程中有希望跨模态传递该知识。一些模态是:真实、剪贴画、草图、空间文本(与对象的空间位置相对应的文字)和自然语言描述。本文还讨论了如何用交叉模态卷积神经网络处理这类问题的方法。</li></ul><p id="2539" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">图 19 </strong> : CMPlaces 跨模态场景表示</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi oa"><img src="../Images/8733ca504e8828c66bc69443f9b640cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rNjC606rr19KS9LB.jpg"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Note</strong>: Taken from the CMPlaces paper showing two examples, bedrooms and kindergarten classrooms, across different modalities. Conventional Neural Network approaches learn representations that don’t transfer well across modalities and this paper attempts to generate a shared representation “agnostic of modality”.</figcaption></figure><p id="af9c" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">资料来源</strong> : Aytar 等人(2016)[177]</p><p id="c829" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">在 CMPlaces 中，我们看到明确提到迁移学习、领域不变表示、领域适应和多模态学习，所有这些都有助于进一步展示当前计算机视觉研究的趋势。作者专注于试图找到“<em class="kr">领域/模态独立表示</em>”，这可能对应于人类从中得出统一表示的更高级抽象。例如，以“猫”的各种形态为例，人类在书写中看到“猫”这个词，在速写本中画出一幅画，在现实世界中看到一幅图像，或者在讲话中提到，但我们仍然在这些形态之上的更高层次上抽象出相同的统一表示。</p><p id="e06b" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><em class="kr">“人类能够独立地利用他们感知的知识和经验，机器的类似能力将使检索和识别方面的一些重要应用成为可能”</em>。</p><ul class=""><li id="8b04" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated"><strong class="ks ir">MS-Cele b-1M</strong>【178】包含一百万个名人的图像，在面部识别的训练集中有一千万个训练图像。</li><li id="12c5" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">开放图像</strong>【179】由谷歌公司提供，包含大约 900 万个带有多个标签的图像的 URL，这是对典型的单标签图像的巨大改进。开放图像涵盖 6000 个类别，比 ImageNet 之前提供的 1000 个类别(较少关注犬科动物)有了很大的改进，应该证明对机器学习社区是不可或缺的。</li><li id="7fb1" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">YouTube-8M</strong>【180】<strong class="ks ir"/>也是谷歌提供的 800 万个视频网址，50 万小时的视频，4800 个课程，平均。每个视频 1.8 个标签。标签的一些例子是:“艺术&amp;娱乐”、“购物”和“宠物&amp;动物”。视频数据集更难以标记和收集，因此该数据集提供了巨大的价值。</li></ul><p id="331c" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">也就是说，图像理解方面的进步，如分割、对象分类和检测，已经将视频理解带到了研究的前沿。然而，在这个数据集发布之前，现实世界中可用的视频数据集的种类和规模都非常缺乏。此外，该数据集最近刚刚更新，[181]今年，谷歌与 Kaggle 联合举办了一场视频理解比赛，作为 2017 年 CVPR 的一部分。[182]</p><p id="342c" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">关于 YouTube-8M 的一般信息:<a class="ae lm" href="https://research.google.com/youtube8m/" rel="noopener ugc nofollow" target="_blank">此处</a>【183】</p><h1 id="a20e" class="lq lr iq bd ls lt mi lv lw lx mj lz ma jw mk jx mc jz ml ka me kc mm kd mg mh bi translated">无法组合的额外项目和有趣的趋势</h1><p id="dccf" class="pw-post-body-paragraph kp kq iq ks b kt mn jr kv kw mo ju ky ln mp lb lc lo mq lf lg lp mr lj lk ll ij bi translated">当这个作品接近尾声时，我们感叹我们不得不在这样的限制下创作它。事实上，计算机视觉的领域太过广阔，无法涵盖任何真实的、有意义的深度，因此出现了许多疏漏。不幸的是，其中一个遗漏是几乎所有不使用神经网络的东西。我们知道在 NNs 之外有很好的工作，我们承认我们自己的偏见，但是我们认为目前的动力在于这些方法，并且我们对纳入材料的主观选择主要基于从整个研究社区收到的接收(结果不言自明)。</p><p id="76e8" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">我们还要强调的是，在上述主题中还有数百篇其他论文，这些主题并不是权威的，而是希望鼓励感兴趣的人沿着我们提供的入口进一步阅读。因此，这最后一节是对我们喜欢的一些其他应用程序、我们希望强调的趋势以及我们希望向读者提出的理由的总括。</p><p id="c476" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">应用/用例</strong></p><ul class=""><li id="6751" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated">脸书的盲人应用程序[184]和百度的硬件。[185]</li><li id="32a8" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated">情感检测结合了面部检测和语义分析，发展迅速。目前有 20 多个可用的 API。[186]</li><li id="94b3" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated">从航空影像中提取道路，[187]从航空地图和人口密度地图中进行土地利用分类。[188]</li><li id="1469" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated">Amazon Go 通过展示无队列购物体验进一步提升了计算机视觉的形象，[189]尽管目前仍存在一些功能问题。[190]</li><li id="c268" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated">对于自动驾驶汽车，有大量的工作正在进行，而我们基本上没有触及。然而，对于那些希望深入研究总体市场趋势的人来说，Moritz Mueller-Freitag 有一篇关于德国汽车行业和自动驾驶汽车影响的 200 亿神经元的优秀文章。[191]</li><li id="f4af" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated">其他感兴趣的领域:图像检索/搜索，[192]手势识别，修复和面部重建。</li><li id="2436" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated">围绕医学中的数字成像和通信(DICOM)以及其他医学应用，尤其是与成像相关的应用，有大量的工作要做。例如，已经存在(并且仍然存在)许多 Kaggle 检测竞赛(肺癌、宫颈癌)，其中一些具有大量金钱激励，在这些竞赛中，算法试图在所讨论的分类/检测任务中胜过专家。</li></ul><p id="0bb5" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">然而，当改进这些算法的错误率的工作继续进行时，它们作为医疗从业者的工具的价值越来越明显。当我们考虑到通过将人工智能系统<a class="ae lm" href="http://localhost:3000/a-year-in-computer-vision#ftnt193" rel="noopener ugc nofollow" target="_blank">【193】</a>与医学专家相结合而实现的乳腺癌检测性能改善时，这一点尤为引人注目。[194]在这种情况下，机器人-人类共生产生的准确度远远大于其各部分的总和，达到 99.5%。</p><p id="6e6e" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">这只是深度学习/机器学习社区目前正在追求的医疗应用洪流的一个例子。我们团队中一些愤世嫉俗的成员开玩笑地将这些尝试视为一种手段，以迎合社会对人工智能研究作为一种无处不在的慈善力量的想法。但是，只要这项技术有助于医疗保健行业，并且是以安全和慎重的方式引入的，我们就全心全意地欢迎这种进步。</p><p id="f18e" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">硬件/市场</strong></p><ul class=""><li id="1f8c" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated">机器人视觉/机器视觉(独立领域)的增长市场和物联网的潜在目标市场。我们个人最喜欢的是一个农民的儿子使用深度学习、覆盆子 Pi 和 TensorFlow 在日本根据独特的生产者启发式质量(如形状、大小和颜色)对黄瓜进行分类。[195]这大大减少了他母亲在挑选黄瓜上花费的时间。</li><li id="79ad" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated">计算需求的缩减和向移动设备迁移的趋势是显而易见的，但同时也伴随着急剧的硬件加速。很快，我们将到处看到口袋大小的 CNN 和视觉处理单元(vpu)。例如，Movidius Myriad2 用于谷歌的 Tango 和无人机项目。[196]</li></ul><p id="4038" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">m ovidius Fathom stick[197]也使用了 Myriad2 的技术，允许用户将 SOTA 计算机视觉性能添加到消费设备中。Fathom 棒具有 USB 棒的物理属性，它将神经网络的能力带到了几乎任何设备上:棒上的大脑。</p><ul class=""><li id="4277" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated">使用非可见光的传感器和系统。例子包括雷达、热成像相机、超光谱成像、声纳、磁共振成像等。</li><li id="a56d" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated">降低激光雷达的成本，它使用光和雷达来测量距离，并提供许多优于普通 RGB 相机的优势。目前有许多价格低于 500 美元的激光雷达设备。</li><li id="9110" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated">Hololens 和几乎无数的其他增强现实耳机[198]进入市场。</li><li id="bb41" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">谷歌的探戈项目</strong>【199】代表了 SLAM 的下一个重大商业化。Tango 是一个增强现实计算平台，包括新颖的软件和硬件。Tango 允许在不使用 GPS 或其他外部信息的情况下检测移动设备相对于世界的位置，同时以 3D 方式绘制设备周围的区域。</li></ul><p id="ac21" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">企业合作伙伴联想在 2016 年向市场推出了价格实惠的 Tango 手机，使数百名开发者开始为该平台创建应用程序。Tango 采用了以下软件技术:运动跟踪、区域学习和深度感知。</p><p id="9c88" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">更新 2019 年 10 月</strong> : <a class="ae lm" href="https://www.dailywireless.org/mobile/what-happened-to-google-tango/" rel="noopener ugc nofollow" target="_blank">谷歌的 Tango 怎么了？</a></p><p id="1ca1" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">基于即将出版的出版物的遗漏</strong></p><p id="8163" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">在机器学习和人工智能中，计算机视觉技术和其他领域之间也有相当多且越来越多的重叠。这些其他领域和混合用例是 M Tank 即将发布的出版物的主题，与本文的全部内容一样，我们基于自己的启发对内容进行了划分。</p><p id="782e" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">例如，我们决定将两个完整的计算机视觉任务，图像字幕和视觉问答，与视觉语音识别一起放在我们即将推出的 NLP 作品中，因为涉及到 CV 和 NLP 的结合。而将生成模型应用于图像则是我们在生成模型方面的工作。这些未来作品中包括的例子有:</p><ul class=""><li id="7965" class="ms mt iq ks b kt ku kw kx ln mu lo mv lp mw ll mx my mz na bi translated"><strong class="ks ir">唇读</strong>:2016 年，我们看到了 LipNet[200]等程序在唇读方面的巨大进步，这些程序将计算机视觉和 NLP 结合到视觉语音识别中。</li><li id="84f1" class="ms mt iq ks b kt nb kw nc ln nd lo ne lp nf ll mx my mz na bi translated"><strong class="ks ir">应用于图像的生成模型</strong>将成为我们对自回归模型(PixelRNN、PixelCNN、ByteNet、VPN、WaveNet)、生成对抗网络(GANs)、变分自动编码器之间激烈*斗争的描述的一部分，正如你在此阶段所预期的，所有这些模型的变体、组合和混合体。</li></ul><p id="8f60" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">*免责声明:监测组希望指出，他们不宽恕任何形式的网络对网络(非)暴力，并且同情向生殖性非对话网络(枪支)发展的运动。<a class="ae lm" href="http://localhost:3000/a-year-in-computer-vision#ftnt201" rel="noopener ugc nofollow" target="_blank">【201】</a></p><p id="98f9" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">在最后一部分，我们将提供一些总结性的评论，并对我们发现的一些趋势进行概括。我们希望我们能够足够全面地展示计算机视觉领域的大致位置和近期发展方向的鸟瞰图。我们还要特别提请注意，我们的工作不涵盖 2017 年 1 月至 8 月。研究产出的快速增长意味着大部分工作可能已经过时；我们鼓励读者去看看，看看是否适合自己。但这种快速增长也带来了有利可图的机会，因为计算机视觉硬件和软件市场预计到 2022 年将达到 486 亿美元。</p><p id="1b89" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated"><strong class="ks ir">图 20 </strong>:应用市场计算机视觉收入【202】</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/6e5c0dfba404a606c625abdf0f537475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/0*RPX7yk0VCzmcXP_j.jpg"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Note</strong>: Estimation of Computer Vision revenue by application market spanning the period from 2015–2022. The largest growth is forecasted to come from applications within the automotive, consumer, robotics and machine vision sectors. <br/><strong class="bd ns">Source</strong>: Tractica (2016)[203]</figcaption></figure><h1 id="6c19" class="lq lr iq bd ls lt mi lv lw lx mj lz ma jw mk jx mc jz ml ka me kc mm kd mg mh bi translated">结论</h1><p id="2229" class="pw-post-body-paragraph kp kq iq ks b kt mn jr kv kw mo ju ky ln mp lb lc lo mq lf lg lp mr lj lk ll ij bi translated">最后，我们想强调在我们的研究回顾过程中反复出现的一些趋势和反复出现的主题。首先，我们想提请注意机器学习研究社区对优化的贪婪追求。这在准确率的逐年变化中最为明显，尤其是在准确率的年内变化中。我们想强调这一点，过一会儿再回到这一点。</p><p id="afa1" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">错误率并不是唯一狂热优化的参数，研究人员正在努力提高速度、效率，甚至是以全新的方式推广到其他任务和问题的算法能力。我们敏锐地意识到，随着一次性学习、生成模型、迁移学习以及最近的进化学习等方法的出现，研究开始崭露头角，我们认为这些研究原则正在逐渐对最佳表现工作的方法产生更大的影响。</p><p id="e901" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">虽然最后一点毫无疑问是对这一趋势的赞扬，而不是诋毁，但人们不禁将注意力投向人工通用智能(非常)遥远的幽灵，无论是否值得思考。我们绝不是危言耸听，我们只是希望向专家和外行人强调，这种担忧来自这里，来自计算机视觉和其他人工智能子领域已经显而易见的惊人进展。只有通过关于这些进步及其总体影响的教育，才能恰当地表达公众的关切。这可能会反过来平息媒体情绪的力量和人工智能中的错误信息。</p><p id="b565" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">我们选择关注一年时间表有两个原因。第一个因素与正在制作的作品数量有关。即使对于非常关注该领域的人来说，随着出版物数量呈指数级增长，跟上研究的步伐也变得越来越困难。第二个让我们回到年内变化的观点。</p><p id="8af6" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">通过对一年的进展进行快照，读者可以开始理解目前的研究进度。我们在如此短的时间内看到了一个又一个的改善，但是为什么呢？研究人员已经培育了一个全球社区，在这个社区中，基于以前的方法(架构、元架构、技术、想法、技巧、古怪的黑客、结果等)进行构建。)，以及基础设施(像 Keras、TensorFlow 和 PyTorch、GPU 等库。)，不仅是鼓励，也是庆祝。一个以开源为主的社区，鲜有相似之处，它不断吸引新的研究人员，其技术被经济学、物理学和无数其他领域重新引用。</p><p id="47de" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">对于那些还没有注意到的人来说，重要的是要理解，在已经疯狂的不同声音的合唱中，宣告对这项技术的真实性质的神圣洞察力，至少有一致意见；一致认为这项技术将以令人兴奋的新方式改变世界。然而，对于这些变化将在什么时间展开，仍然存在很多分歧。</p><p id="fbe7" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">在我们能够准确模拟这些进展之前，我们将继续尽最大努力提供信息。通过这个资源，我们希望迎合人工智能经验的范围，从研究人员到任何只想获得计算机视觉和人工智能基础的人。我们的项目希望以此为开源革命增加一些价值，这场革命在一生的技术之下悄然进行。</p><p id="c54a" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">非常感谢，</p><blockquote class="km kn ko"><p id="cb7d" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">M 坦克</p></blockquote><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/1e4ee110ffe53de7137a11f5580b117c.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*Q_zw5-Mu8eksQCveTK_4vQ.png"/></div></figure><blockquote class="km kn ko"><p id="c6f0" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">请随意将所有反馈和建议放在评论区，我们会尽快回复。或者，您可以通过以下方式直接联系我们:info@themtank.com</p></blockquote><p id="2a08" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">完整版本可在:<a class="ae lm" href="http://www.themtank.org/a-year-in-computer-vision" rel="noopener ugc nofollow" target="_blank">www.themtank.org/a-year-in-computer-vision</a>获得</p><h2 id="c806" class="od lr iq bd ls oe of dn lw og oh dp ma ln oi oj mc lo ok ol me lp om on mg oo bi translated">按出现顺序排列的参考文献</h2><p id="c657" class="pw-post-body-paragraph kp kq iq ks b kt mn jr kv kw mo ju ky ln mp lb lc lo mq lf lg lp mr lj lk ll ij bi translated">[131] Szegedy 等人，2016 年。Inception-v4，Inception-ResNet 和剩余连接对学习的影响。<em class="kr">【在线】arXiv: 1602.07261 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1602.07261v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1602.07261 v2</strong></a></p><p id="ba41" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[132] Szegedy 等人，2015 年。重新思考计算机视觉的初始架构。<em class="kr">【在线】arXiv: 1512.00567 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1512.00567v3" rel="noopener ugc nofollow" target="_blank">T11】arXiv:1512.00567 v3T13】</a></p><p id="5733" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[133]黄等，2016。密集连接的卷积网络。<em class="kr">【在线】arXiv: 1608.06993 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1608.06993v3" rel="noopener ugc nofollow" target="_blank">T17】arXiv:1608.06993 v3T19】</a></p><p id="167e" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">134 同上</p><p id="3b9f" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">135 同上</p><p id="0f64" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[136]刘庄 13。2017.密集连接卷积网络的代码。<em class="kr">【在线】github.com</em>。可用:<a class="ae lm" href="https://github.com/liuzhuang13/DenseNet" rel="noopener ugc nofollow" target="_blank">https://github.com/liuzhuang13/DenseNet</a>【访问时间:2017 年 03 月 04 日】。</p><p id="a3e8" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[137] Larsson 等人，2016 年。FractalNet:无残差超深度神经网络。<em class="kr">【在线】arXiv: 1605.07648 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1605.07648v2" rel="noopener ugc nofollow" target="_blank">T27】arXiv:1605.07648 v2T29】</a></p><p id="d67b" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[138]黄等，2016 年。密集连接的卷积网络。<em class="kr">【在线】arXiv: 1608.06993 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1608.06993v3" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1608.06993 v3</strong></a>，pg。1.</p><p id="18be" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[139] Hossein HasanPour 等人，2016 年让我们保持简单:使用简单的架构来超越更深层次的架构。<em class="kr">【在线】arXiv: 1608.06037 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1608.06037v3" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1608.06037 v3</strong></a></p><p id="bc58" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">140 同上</p><p id="6b7c" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[141] Singh 等人，2016 年。Swapout:学习深层架构的合奏。<em class="kr">【在线】arXiv: 1605.06465 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1605.06465v1" rel="noopener ugc nofollow" target="_blank">T15】arXiv:1605.06465 v1T17】</a></p><p id="9874" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[142] Iandola 等人，2016 年。SqueezeNet: AlexNet 级精度，参数少 50 倍&lt;0.5MB model size. <em class="kr">【在线】arXiv: 1602.07360 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1602.07360v4" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1602.07360 v4</strong></a></p><p id="19d7" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[143]尚等，2016。通过级联校正线性单元理解和改进卷积神经网络。<em class="kr">【在线】arXiv: 1603.05201 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1603.05201v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1603.05201 v2</strong></a></p><p id="5894" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[144] Clevert 等人，2016 年。通过指数线性单元(ELUs)进行快速准确的深度网络学习。<em class="kr">【在线】arXiv: 1511.07289 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1511.07289v5" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1511.07289 V5</strong></a></p><p id="aec0" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[145] Trottier 等人，2016 年。深度卷积神经网络的参数指数线性单元。<em class="kr">【在线】arXiv: 1605.09332 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1605.09332v3" rel="noopener ugc nofollow" target="_blank">T39】arXiv:1605.09332 v3T41】</a></p><p id="ad92" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[146]沃拉尔等人，2016 年。谐波网络:深度平移和旋转等值。<em class="kr">【在线】arXiv: 1612.04642 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1612.04642v1" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1612.04642 v1</strong></a></p><p id="61c7" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[147]科恩&amp;韦林。2016.群等变卷积网络。<em class="kr">【在线】arXiv: 1602.07576 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1602.07576v3" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1602.07576 v3</strong></a></p><p id="111c" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[148] Dieleman 等人，2016 年。利用卷积神经网络中的循环对称性。<em class="kr">【在线】arXiv: 1602.02660 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1602.02660v2" rel="noopener ugc nofollow" target="_blank">T3】arXiv:1602.02660 v2T5】</a></p><p id="5c66" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[149]科恩&amp;韦林。2016.可操纵 CNN。<em class="kr">【在线】arXiv: 1612.08498 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1612.08498v1" rel="noopener ugc nofollow" target="_blank">T9】arXiv:1612.08498 v1T11】</a></p><p id="1900" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[150] Abdi，m .，Nahavandi，S. 2016 年。多重残差网络:提高残差网络的速度和精度。<em class="kr">【在线】arXiv: 1609.05672 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1609.05672v3" rel="noopener ugc nofollow" target="_blank">T15】arXiv:1609.05672 v3T17】</a></p><p id="2300" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[151]何等，2015。用于图像识别的深度残差学习。<em class="kr">【在线】arXiv: 1512.03385 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1512.03385v1" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1512.03385 v1</strong></a></p><p id="4b27" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[152] Quora。2017.深度剩余网络的直观解释是什么？<em class="kr">【网站】</em><a class="ae lm" href="http://www.quora.com/" rel="noopener ugc nofollow" target="_blank"><em class="kr">www.quora.com</em></a>。可用:<a class="ae lm" href="https://www.quora.com/What-is-an-intuitive-explanation-of-Deep-Residual-Networks" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/What-a-intuitive-an-explain-of-Deep-Residual-Networks</a>【访问时间:2017 年 03 月 04 日】。</p><p id="3739" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[153] Zagoruyko，s .和 Komodakis，N. 2017。广泛的剩余网络。<em class="kr">【在线】arXiv: 1605.07146 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1605.07146v3" rel="noopener ugc nofollow" target="_blank">T35】arXiv:1605.07146 v3T37】</a></p><p id="572b" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[154]黄等，2016。具有随机深度的深度网络。<em class="kr">【在线】arXiv: 1603.09382 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1603.09382v3" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1603.09382 v3</strong></a></p><p id="12b3" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[155] Savarese 等人，2016 年。用剩余门学习身份映射。<em class="kr">【在线】arXiv: 1611.01260 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1611.01260v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1611.01260 v2</strong></a></p><p id="6f90" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">156 韦特、威尔伯和贝隆吉。2016.剩余网络的行为类似于相对较浅的网络的集合。<em class="kr">【在线】arXiv: 1605.06431 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1605.06431v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1605.06431 v2</strong></a></p><p id="62c5" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[157]他在阿尔 2016.深剩余网络中的身份映射。<em class="kr">【在线】arXiv: 1603.05027 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1603.05027v3" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1603.05027 v3</strong></a></p><p id="e4fb" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[158] Abdi，m .，Nahavandi，S. 2016。多重残差网络:提高残差网络的速度和精度。<em class="kr">【在线】arXiv: 1609.05672 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1609.05672v3" rel="noopener ugc nofollow" target="_blank">T9】arXiv:1609.05672 v3T11】</a></p><p id="814d" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[159] Greff 等人，2017 年。公路和残差网络学习展开迭代估计。<em class="kr">【在线】arXiv: 1612。07771 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1612.07771v3" rel="noopener ugc nofollow" target="_blank">T15】arXiv:1612.07771 v3T17】</a></p><p id="e1bb" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">160 阿布迪和纳哈万迪。2017.多重残差网络:提高残差网络的速度和精度。<em class="kr">【在线】1609.05672 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1609.05672v4" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1609.05672 v4</strong></a></p><p id="37ed" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[161] Targ 等人，2016 年。概化剩余架构。<em class="kr">【在线】arXiv: 1603.08029 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1603.08029v1" rel="noopener ugc nofollow" target="_blank">T27】arXiv:1603.08029 v1T29】</a></p><p id="bde9" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[162]吴等，2016。更广还是更深:重新审视视觉识别的 ResNet 模型。<em class="kr">【在线】arXiv: 1611.10080 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1611.10080v1" rel="noopener ugc nofollow" target="_blank">T33】arXiv:1611.10080 v1T35】</a></p><p id="e741" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[163]廖和波焦。2016.弥合剩余学习、递归神经网络和视觉皮层之间的差距。<em class="kr">【在线】arXiv: 1604.03640 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1604.03640v1" rel="noopener ugc nofollow" target="_blank">T39】arXiv:1604.03640 v1T41】</a></p><p id="d711" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">164 莫尼斯和帕尔。2016.卷积剩余记忆网络。<em class="kr">【在线】arXiv: 1606.05262 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1606.05262v3" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1606.05262 v3</strong></a></p><p id="18cd" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[165]哈特和马。2016.身份在深度学习中很重要。<em class="kr">【在线】arXiv: 1611.04231 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1611.04231v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1611.04231 v2</strong></a></p><p id="2107" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[166] Shah 等人，2016 年。具有指数线性单位的深度剩余网络。<em class="kr">【在线】arXiv: 1604.04112 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1604.04112v4" rel="noopener ugc nofollow" target="_blank">T3】arXiv:1604.04112 v4T5】</a></p><p id="7512" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[167]沈、曾。2016.甚深网络的加权残差。<em class="kr">【在线】arXiv: 1605.08831 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1605.08831v1" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1605.08831 v1</strong></a></p><p id="ca83" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">168 本·哈姆纳。2016.推特状态。<em class="kr">【在线】推特。</em>可用:<a class="ae lm" href="https://twitter.com/benhamner/status/789909204832227329" rel="noopener ugc nofollow" target="_blank">https://twitter.com/benhamner/status/789909204832227329</a></p><p id="d1b9" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">169 ImageNet。2017.主页。<em class="kr">【在线】</em>可用:<a class="ae lm" href="http://image-net.org/index" rel="noopener ugc nofollow" target="_blank">http://image-net.org/index</a>【访问时间:2017 年 04 月 01 日】</p><p id="2e40" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[170]可可。2017.公共主页中的公共对象。<em class="kr">【在线】</em>可用:<a class="ae lm" href="http://mscoco.org/" rel="noopener ugc nofollow" target="_blank">http://mscoco.org/</a>【访问时间:2017 年 04 月 01 日】</p><p id="049f" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">171 CIFARs。2017.CIFAR-10 数据集。<em class="kr">【在线】</em>可用:<a class="ae lm" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">https://www.cs.toronto.edu/~kriz/cifar.html</a>【访问时间:2017 年 04 月 01 日】</p><p id="6031" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">172 MNIST。2017.MNIST 手写数字数据库。<em class="kr">【在线】</em>可用:<a class="ae lm" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">http://yann.lecun.com/exdb/mnist/</a>【访问时间:2017 年 04 月 01 日】</p><p id="741c" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[173]周等，2016。地点 2。<em class="kr">【在线】</em>可用:<a class="ae lm" href="http://places2.csail.mit.edu/" rel="noopener ugc nofollow" target="_blank">http://places2.csail.mit.edu/</a>【访问时间:2017 年 06 月 01 日】</p><p id="6045" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">174 同上</p><p id="b328" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[175]麦科马克等人，2017 年。SceneNet RGB-D:具有地面真实性的合成室内轨迹的 5M 照片级真实感图像。<em class="kr">【在线】arXiv: 1612.05079 </em> v3。可用:<a class="ae lm" href="https://arxiv.org/abs/1612.05079v3" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1612.05079 v3</strong></a></p><p id="5c91" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[176] Aytar 等人，2016 年。跨模态场景网络。<em class="kr">【在线】arXiv: 1610.09003 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1610.09003v1" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1610.09003 v1</strong></a></p><p id="d265" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">177 同上</p><p id="dc23" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[178]郭等，2016。MS-Celeb-1M:大规模人脸识别的数据集和基准。<em class="kr">【在线】arXiv: 1607.08221 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1607.08221v1" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1607.08221 v1</strong></a></p><p id="6c42" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[179]开放图像。2017.打开图像数据集。<em class="kr">【在线】Github </em>。可用:【https://github.com/openimages/dataset T2】【访问时间:2017 年 8 月 1 日】</p><p id="62e4" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[180] Abu-El-Haija 等人，2016 年。YouTube-8M:大规模视频分类基准。<em class="kr">【在线】arXiv: 1609.08675 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1609.08675v1" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1609.08675 v1</strong>T9】</a></p><p id="f77d" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[181]纳采夫，第 2017 页。更新的 YouTube-8M、视频理解挑战和 CVPR 研讨会。我的天啊。。<em class="kr">【在线】谷歌研究博客</em>。可用:<a class="ae lm" href="https://research.googleblog.com/2017/02/an-updated-youtube-8m-video.html" rel="noopener ugc nofollow" target="_blank">https://research . Google blog . com/2017/02/an-updated-YouTube-8m-video . html</a>【访问时间:26/02/2017】。</p><p id="4ea7" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[182] YouTube-8M。2017.CVPR 17 年关于 YouTube 的研讨会-8M 大规模视频理解。<em class="kr">【在线】谷歌研究</em>。可用:<a class="ae lm" href="https://research.google.com/youtube8m/workshop.html" rel="noopener ugc nofollow" target="_blank">https://research.google.com/youtube8m/workshop.html</a>【访问时间:26/02/2017】。</p><p id="2edc" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[183]谷歌。2017.YouTube-8M 数据集。<em class="kr">【在线】research.google.com</em>。可用:<a class="ae lm" href="https://research.google.com/youtube8m/" rel="noopener ugc nofollow" target="_blank">https://research.google.com/youtube8m/</a>【访问时间:2017 年 04 月 03 日】。</p><p id="d5c2" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[184]吴、皮克和。2016.使用人工智能帮助盲人“看见”脸书。<em class="kr">【在线】脸书编辑部</em>。可用:<a class="ae lm" href="http://newsroom.fb.com/news/2016/04/using-artificial-intelligence-to-help-blind-people-see-facebook/" rel="noopener ugc nofollow" target="_blank">http://news room . FB . com/news/2016/04/using-artificial-intelligence-to-help-blind-people-see-Facebook/</a>【访问时间:2017 年 02 月 03 日】。</p><p id="3fa5" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[185]梅斯。2016.人工智能终于进入了我们的日常世界。<em class="kr">【在线】连线</em>。可用:<a class="ae lm" href="https://www.wired.com/2016/01/2015-was-the-year-ai-finally-entered-the-everyday-world/" rel="noopener ugc nofollow" target="_blank">https://www . wired . com/2016/01/2015-was-the-year-ai-finally-entry-the-daily-world/</a>【访问时间:2017 年 02 月 03 日】。</p><p id="4a0e" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[186]多尔费尔德。2015.20 多个情感识别 API 会让你印象深刻，并关注。<em class="kr">【在线】北欧 API</em>。可用:<a class="ae lm" href="http://nordicapis.com/20-emotion-recognition-apis-that-will-leave-you-impressed-and-concerned/" rel="noopener ugc nofollow" target="_blank">http://nordicapis . com/20-emotion-recognition-APIs-that-leave-you-impressed-and-concerned/</a>【访问时间:2017 年 02 月 03 日】。</p><p id="b692" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[187]约翰逊，2016 年。trail behind/deep osm-使用 OpenStreetMap 功能和卫星图像训练深度学习网络。<em class="kr">【在线】Github.com</em>。可用:【https://github.com/trailbehind/DeepOSM T2】【访问时间:29/03/2017】。</p><p id="84d7" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">188 格罗和蒂埃克。2016.用更好的地图连接世界。<em class="kr">【在线】脸书电码</em>。可用:<a class="ae lm" href="https://code.facebook.com/posts/1676452492623525/connecting-the-world-with-better-maps/" rel="noopener ugc nofollow" target="_blank">https://code . Facebook . com/posts/1676452492623525/connecting-the-world with-better-maps/</a>【访问时间:2017 年 02 月 03 日】。</p><p id="927b" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[189]亚马逊。2017.常见问题— Amazon Go。<em class="kr">【网址】Amazon.com</em>。可用:<a class="ae lm" href="https://www.amazon.com/b?node%3D16008589011" rel="noopener ugc nofollow" target="_blank">https://www.amazon.com/b?node=16008589011</a>【访问时间:29/03/2017】。</p><p id="2da6" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[190]雷辛格特区，2017 年。亚马逊的免收银商店可能很容易被攻破。<em class="kr">【在线】财富科技</em>。可用:<a class="ae lm" href="http://fortune.com/2017/03/28/amazon-go-cashier-free-store/" rel="noopener ugc nofollow" target="_blank">http://fortune.com/2017/03/28/amazon-go-cashier-free-store/</a>【访问时间:29/03/2017】。</p><p id="b3d3" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[191]穆勒-弗莱塔格，M. 2017 年。德国在车轮上睡着了？<em class="kr">[博客]两百亿神经元——Medium.com</em>。可用:<a class="ae lm" href="https://medium.com/twentybn/germany-asleep-at-the-wheel-d800445d6da2" rel="noopener">https://medium . com/twenty bn/Germany-sleep-at-the-wheel-d 800445 d6da 2</a></p><p id="e36a" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[192]戈多等人，2016 年。深度图像检索:学习图像搜索的全局表示。<em class="kr">【在线】arXiv: 1604.01325 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1604.01325v2" rel="noopener ugc nofollow" target="_blank">T23】arXiv:1604.01325 v2T25】</a></p><p id="6863" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[193]王等，2016。用于识别转移性乳腺癌的深度学习。<em class="kr">【在线】arXiv: 1606.05718 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1606.05718v1" rel="noopener ugc nofollow" target="_blank">T29】arXiv:1606.05718 v1T31】</a></p><p id="369c" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[194]罗森菲尔德，J. 2016。人工智能实现了接近人类的乳腺癌检测。<em class="kr">【在线】Mentalfloss.com</em>。可用:<a class="ae lm" href="http://mentalfloss.com/article/82415/ai-achieves-near-human-detection-breast-cancer" rel="noopener ugc nofollow" target="_blank">http://mental loss . com/article/82415/ai-achieves-near-human-detection-breast-cancer</a>【访问时间:27/03/2017】。</p><p id="0212" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[195]佐藤，K. 2016 年。一个日本黄瓜农民如何使用深度学习和 TensorFlow。<em class="kr">【博客】谷歌云平台</em>。可用:<a class="ae lm" href="https://cloud.google.com/blog/big-data/2016/08/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow" rel="noopener ugc nofollow" target="_blank">https://cloud . Google . com/blog/big-data/2016/08/how-a-Japanese-cucumber-farmer-is-use-deep-learning-and-tensor flow</a></p><p id="dd0b" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[196]班纳吉，第 2016 页。VPUs 的崛起:给机器眼睛。<em class="kr">【在线】</em> <a class="ae lm" href="http://www.digit.in/" rel="noopener ugc nofollow" target="_blank"> <em class="kr"> www.digit.in </em> </a>。可用:<a class="ae lm" href="http://www.digit.in/general/the-rise-of-vpus-giving-eyes-to-machines-29561.html" rel="noopener ugc nofollow" target="_blank">http://www . digit . in/general/the-rise-of-vpus-giving-eyes-to-machines-29561 . html</a>【访问时间:22/03/2017。</p><p id="4b31" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">197m ovidius。2017.嵌入式神经网络计算框架。<em class="kr">【在线】Movidius.com</em>。可用:<a class="ae lm" href="https://www.movidius.com/solutions/machine-vision-algorithms/machine-learning" rel="noopener ugc nofollow" target="_blank">https://www . m ovidius . com/solutions/machine-vision-algorithms/machine-learning</a>【访问时间:03/03/2017】。</p><p id="1f25" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[198] Dzyre，N. 2016。你可以买到的 10 款即将上市的增强现实和智能眼镜。<em class="kr">【博客】</em> <em class="kr">洪家</em>。可用:<a class="ae lm" href="http://www.hongkiat.com/blog/augmented-reality-smart-glasses/" rel="noopener ugc nofollow" target="_blank">http://www . hongkiat . com/blog/augmented-reality-smart-glasses/</a>【访问时间:2017 年 3 月 3 日】。</p><p id="ce98" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[199]谷歌。2017.探戈。<em class="kr">【网址】get.google.com</em>。可用:<a class="ae lm" href="https://get.google.com/tango/" rel="noopener ugc nofollow" target="_blank">https://get.google.com/tango/</a>【访问时间:23/03/2017】。</p><p id="01e8" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[200] Assael 等人，2016 年。LipNet:端到端的句子级唇读。<em class="kr">【在线】arXiv: 1611.01599 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1611.01599v2" rel="noopener ugc nofollow" target="_blank"> arXiv:1611.01599v2 </a></p><p id="2ae4" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[201] Albanie 等人，2017 年。制止 GAN 暴力:生成性非通用网络。<em class="kr">【在线】arXiv: 1703.02528 </em>。可用:<a class="ae lm" href="https://arxiv.org/abs/1703.02528v1" rel="noopener ugc nofollow" target="_blank">T33】arXiv:1703.02528 v1T35】</a></p><p id="214b" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">[202] Tractica。2016.到 2022 年，计算机视觉硬件和软件市场将达到 486 亿美元。<em class="kr">【网址】</em><a class="ae lm" href="http://www.tractica.com/" rel="noopener ugc nofollow" target="_blank"><em class="kr">【www.tractica.com】</em></a>。可用:<a class="ae lm" href="https://www.tractica.com/newsroom/press-releases/computer-vision-hardware-and-software-market-to-reach-48-6-billion-by-2022/" rel="noopener ugc nofollow" target="_blank">https://www . tractica . com/news room/press-releases/computer-vision-hardware-and-software-market-to-reach-4860 亿-by-2022/ </a>【访问日期:2017 年 12 月 3 日】。</p><p id="0470" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky ln la lb lc lo le lf lg lp li lj lk ll ij bi translated">203 同上</p></div></div>    
</body>
</html>