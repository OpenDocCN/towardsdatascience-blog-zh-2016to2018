<html>
<head>
<title>Vehicle Detection and Tracking</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">车辆检测和跟踪</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/vehicle-detection-and-tracking-44b851d70508?source=collection_archive---------0-----------------------#2017-05-14">https://towardsdatascience.com/vehicle-detection-and-tracking-44b851d70508?source=collection_archive---------0-----------------------#2017-05-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn"><p id="75d6" class="jo jp iq bd jq jr js jt ju jv jw jx dk translated"><a class="ae jy" href="https://youtu.be/zIwLWfaAg-8?t=14m48s" rel="noopener ugc nofollow" target="_blank">“只要有摄像头，你绝对可以成为超人”</a>。埃隆·马斯克在TED演讲。</p></blockquote><figure class="ka kb kc kd ke kf gh gi paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="gh gi jz"><img src="../Images/d74fa2da5877eb676dc489592a45821b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*92cbnEUUwABYUiw-ItGfbQ.png"/></div></div></figure></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><p id="5494" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">这是Udacity的自动驾驶汽车工程师纳米学位项目 第一学期的期末项目<a class="ae jy" href="https://www.udacity.com/drive" rel="noopener ugc nofollow" target="_blank"> <strong class="kv ir">。源代码和更详细的技术文章可以在GitHub<a class="ae jy" href="https://github.com/antevis/CarND-Project5-Vehicle_Detection_and_Tracking" rel="noopener ugc nofollow" target="_blank"><strong class="kv ir"/></a>上找到</strong></a></p><h2 id="907a" class="lq lr iq bd ls lt lu dn lv lw lx dp ly le lz ma mb li mc md me lm mf mg mh mi bi translated"><strong class="ak">目标</strong></h2><p id="9349" class="pw-post-body-paragraph kt ku iq kv b kw mj ky kz la mk lc ld le ml lg lh li mm lk ll lm mn lo lp jx ij bi translated"><strong class="kv ir"> <em class="mo">编写一个软件管道，从汽车上的前置摄像头识别视频中的车辆。</em>T15】</strong></p><p id="49ac" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">在我的实现中，我使用了深度学习方法来进行图像识别。具体来说，我利用卷积神经网络(CNN)的非凡能力来识别图像。</p><p id="c706" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">然而，手头的任务不仅仅是检测车辆的存在，而是指出它的位置。事实证明，CNN也适用于这类问题。在<a class="ae jy" href="http://cs231n.github.io/" rel="noopener ugc nofollow" target="_blank"> <strong class="kv ir"> CS231n课程</strong> </a>中有一个专门针对本地化的<a class="ae jy" href="https://youtu.be/wFG_JMQ6_Sk?list=PLLvH2FwAQhnpj1WEB-jHmPuUeQ8mX-XXG" rel="noopener ugc nofollow" target="_blank"> <strong class="kv ir">讲座</strong> </a>，我在我的解决方案中采用的原则基本上反映了该讲座中讨论的区域提议的思想，并在诸如<a class="ae jy" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank"><strong class="kv ir">fast R-CNN</strong></a>等架构中实现。</p><p id="4b34" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">主要思想是，由于存在二进制分类问题(车辆/非车辆)，模型可以以这样的方式构建，即它将具有小训练样本的输入大小(例如，64×64)和在顶部的1×1的单特征卷积层，其输出可以用作分类的概率值。</p><p id="5d38" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">训练了这种类型的模型后，输入的宽度和高度维度可以任意扩展，将输出层的维度从1x1转换为纵横比近似匹配新的大输入的<strong class="kv ir">地图</strong>。</p><p id="e2ad" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">本质上，这在某种程度上相当于:</p><ol class=""><li id="fd6d" class="mp mq iq kv b kw kx la lb le mr li ms lm mt jx mu mv mw mx bi translated">将新的大输入图像切割成模型初始输入尺寸的正方形(例如，64×64)</li><li id="41ed" class="mp mq iq kv b kw my la mz le na li nb lm nc jx mu mv mw mx bi translated">在每一个方块中探测目标</li><li id="7023" class="mp mq iq kv b kw my la mz le na li nb lm nc jx mu mv mw mx bi translated">将得到的检测结果拼接回去，将与源输入中的相应正方形相同的顺序保存到一个<strong class="kv ir">图</strong>中，其边的纵横比近似匹配新的大输入图像的纵横比。</li></ol><figure class="nd ne nf ng gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="gh gi jz"><img src="../Images/fe524b5235010adfc401824d46fdcc44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kkgZZ-Tn5RX0imD3aKKlvQ.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Consider each of these squares to be processed individually by its own dedicated CNN, producing a 4x20 detection map</figcaption></figure><h2 id="ef17" class="lq lr iq bd ls lt lu dn lv lw lx dp ly le lz ma mb li mc md me lm mf mg mh mi bi translated">数据</h2><p id="b7dd" class="pw-post-body-paragraph kt ku iq kv b kw mj ky kz la mk lc ld le ml lg lh li mm lk ll lm mn lo lp jx ij bi translated">Udacity为学生提供了训练分类器的强大资源。<a class="ae jy" href="https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip" rel="noopener ugc nofollow" target="_blank"> <strong class="kv ir">车辆</strong> </a>和<a class="ae jy" href="https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles.zip" rel="noopener ugc nofollow" target="_blank"> <strong class="kv ir">非车辆</strong> </a>已使用<a class="ae jy" href="http://www.cvlibs.net/datasets/kitti/" rel="noopener ugc nofollow" target="_blank"><strong class="kv ir">KITTI vision benchmark suite</strong></a>的样本进行训练。</p><p id="b107" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">最终的模型在检测项目视频中的白色雷克萨斯时遇到了困难，所以我用大约200个样本增加了数据集。此外，我使用了与<a class="ae jy" href="https://github.com/antevis/CarND-Project2-Traffic-signs-classifier" rel="noopener ugc nofollow" target="_blank"> <strong class="kv ir">项目2中相同的随机图像增强技术对交通标志进行分类</strong> </a>，从项目视频中产生了大约1500幅车辆图像。用于训练、验证和测试的车辆图像总数约为7500张。显然，每个样本都被水平翻转过，使数据集膨胀了2倍。结果，我有大约15000个数据点。</p><figure class="nd ne nf ng gt kf gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/edf9866e03f2120ab646ca39f633a365.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*oqgTgfAfSRdo-hUTyHB0-Q.png"/></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Class IDs are vehicles and non-vehicles</figcaption></figure><p id="0353" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">增加了相同数量的非车辆图像作为反面例子。</p><figure class="nd ne nf ng gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="gh gi nm"><img src="../Images/867ef93381718bb99e25a1490f931752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jzOrbyN5THJe4dbzxvv7zA.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Typical vehicle and non-vehicle samples with their corresponding labels</figcaption></figure><h2 id="185f" class="lq lr iq bd ls lt lu dn lv lw lx dp ly le lz ma mb li mc md me lm mf mg mh mi bi translated">模型</h2><p id="1ed2" class="pw-post-body-paragraph kt ku iq kv b kw mj ky kz la mk lc ld le ml lg lh li mm lk ll lm mn lo lp jx ij bi translated">我从<a class="ae jy" href="https://github.com/maxritter/SDC-Vehicle-Lane-Detection" rel="noopener ugc nofollow" target="_blank">Max Ritter<strong class="kv ir"/></a>的实现中借用了构建网络顶部的技术，他显然采用了相同的方法。</p><p id="8e47" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">已经测试了许多具有不同复杂性的模型架构，以获得最终的模型。</p><p id="6c96" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">我开始从<a class="ae jy" href="http://www.robots.ox.ac.uk/%7Evgg/research/very_deep/" rel="noopener ugc nofollow" target="_blank"> <strong class="kv ir"> VGG16 </strong> </a>架构转移学习，权重在<a class="ae jy" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="kv ir"> ImageNet </strong> </a>上训练。VGG是一个伟大的和经过充分测试的架构，ImageNet weights显然认为它应该对车辆的功能有所了解。我添加了我的顶级<em class="mo">单特征二元分类器</em>并对模型进行了微调。正如所料，它产生了相当高的测试精度，大约为<strong class="kv ir"> 99，5% </strong>。VGG的另一面是它相当复杂，使得预测的计算量很大。</p><p id="83ba" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">然后，我测试了一些不同层数和形状的定制CNN配置，逐步降低复杂性并评估测试精度，最终得到了只有大约<strong class="kv ir"> 28，000个可训练参数</strong>的模型，测试精度仍然约为<strong class="kv ir"> 99.4% </strong>:</p><pre class="nd ne nf ng gt nn no np nq aw nr bi"><span id="f885" class="lq lr iq no b gy ns nt l nu nv">Epoch 5/5<br/>607/607 [==============================] - 48s - loss: 0.0063 - acc: 0.9923 - val_loss: 0.0073 - val_acc: 0.9926<br/><br/>Evaluating accuracy on test set.<br/>test accuracy:  [0.0065823850340600764, 0.99373970345963758]</span></pre><p id="9bd1" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">将模型的复杂性降低到极致对预测的<strong class="kv ir">计算成本</strong>和<strong class="kv ir">过拟合</strong>都有好处。虽然数据集可能看起来不太大，但很难假设28000个参数的模型可能能够记住它。此外，我还积极利用<strong class="kv ir">辍学</strong>来进一步降低过度适应的风险。</p><p id="8dfe" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">该模型已经使用<a class="ae jy" href="https://keras.io" rel="noopener ugc nofollow" target="_blank"> <strong class="kv ir"> Keras </strong> </a>和<a class="ae jy" href="https://www.tensorflow.org" rel="noopener ugc nofollow" target="_blank"> <strong class="kv ir"> TensorFlow </strong> </a>后端实现和训练。</p><p id="a8e5" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">样本预测结果:</p><figure class="nd ne nf ng gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="gh gi nw"><img src="../Images/268d11ddfd2a8dbaea5402c6d2d3791e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yKNXd4ixRf4Nv1S8-ECRwA.png"/></div></div></figure><h2 id="a340" class="lq lr iq bd ls lt lu dn lv lw lx dp ly le lz ma mb li mc md me lm mf mg mh mi bi translated">使用训练好的模型进行车辆检测</h2><p id="e91f" class="pw-post-body-paragraph kt ku iq kv b kw mj ky kz la mk lc ld le ml lg lh li mm lk ll lm mn lo lp jx ij bi translated">视频流中的原始帧如下所示:</p><figure class="nd ne nf ng gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="gh gi nx"><img src="../Images/38a8d51c9ec372510205aca1d0ac760c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1v9U-qzGnSwfbCVLbp262g.png"/></div></div></figure><p id="c9ef" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">严格来说，它不是原创的，因为它已经受到了不失真的影响，但这值得一个属于它自己的故事。对于手头的任务，这是要由车辆检测管道处理的图像。</p><p id="1748" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">车辆检测的感兴趣区域从顶部起大约第<strong class="kv ir">400</strong>个像素开始，垂直跨越大约<strong class="kv ir"> 260 </strong>个像素。因此，我们有一个尺寸为<strong class="kv ir">260×1280</strong>的感兴趣区域，从垂直方向的第<strong class="kv ir">400个</strong>像素开始。</p><figure class="nd ne nf ng gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="gh gi nx"><img src="../Images/260839326c40440b230e604274d3be5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-InHI1Mc-lamry5ujrguQw.png"/></div></div></figure><p id="5779" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">这就把顶层卷积层的维度从<strong class="kv ir">(？，1，1，1) </strong>到<strong class="kv ir">(？，25，153，1) </strong>，其中<strong class="kv ir"> 25 </strong>和<strong class="kv ir"> 153 </strong>是预测的微型<strong class="kv ir">图</strong>的<strong class="kv ir">高度</strong>和<strong class="kv ir">宽度</strong>尺寸，反过来，最终将投影到原始高分辨率图像上。</p><p id="2e87" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated"><strong class="kv ir">车辆扫描流水线包括以下步骤:</strong></p><ol class=""><li id="d656" class="mp mq iq kv b kw kx la lb le mr li ms lm mt jx mu mv mw mx bi translated">获取感兴趣的区域(见上文)</li></ol><p id="6a1b" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated"><strong class="kv ir"> 2。</strong>使用<strong class="kv ir"> </strong>训练好的CNN模型生成<strong class="kv ir">检测图</strong>:</p><figure class="nd ne nf ng gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="gh gi ny"><img src="../Images/523f0a26c3d5033b65b97400b660e487.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w-uFNEXeAWdOVEF0Iqkh7g.png"/></div></div></figure><p id="18c3" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated"><strong class="kv ir"> 3。</strong>应用置信度阈值生成二值图:</p><figure class="nd ne nf ng gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="gh gi ny"><img src="../Images/9e8ef51d4109df76cd8ba5b04b6916ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*90nzF1EP8Kc33TG_tIOtfw.png"/></div></div></figure><p id="ddae" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">预测<strong class="kv ir">非常</strong>两极化，即对于车辆和非车辆点大多坚持<strong class="kv ir">一</strong>和<strong class="kv ir">零</strong>。因此，即使是置信度阈值的中点<strong class="kv ir"> 0.5 </strong>也可能是一个可靠的选择。为了安全起见，我坚持使用0 <strong class="kv ir"> .7 </strong></p><p id="a7be" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated"><strong class="kv ir"> 4。用<code class="fe nz oa ob no b">scipy.ndimage.measurements</code>包的<code class="fe nz oa ob no b">label()</code>功能标记</strong>获得的检测区域。这一步可以勾勒出标注的边界，从而有助于在构建热点图时将每个检测到的“孤岛”保持在其要素标注的边界内。</p><blockquote class="oc od oe"><p id="74ed" class="kt ku mo kv b kw kx ky kz la lb lc ld of lf lg lh og lj lk ll oh ln lo lp jx ij bi translated"><strong class="kv ir"> <em class="iq">这也是被检测车辆</em> </strong> <em class="iq">的第一近似值。</em></p></blockquote><p id="fb8b" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated"><strong class="kv ir"> 5。</strong>将检测点的特征标签投影到原始图像的坐标空间，将每个点转换成64x64的正方形，并将这些正方形保持在特征的区域边界内。</p><p id="b0e5" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">为了说明这种点到平方的转换投射到原始图像上的结果:</p><figure class="nd ne nf ng gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="gh gi nx"><img src="../Images/b7c83ce941bf1356f079870529bbe607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rseJQ3i-UiJMLQjOGl4UjQ.png"/></div></div></figure><p id="a2e8" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">6。创建<strong class="kv ir">热图</strong>。上图中重叠的方块实际上是在积聚“热量”。</p><figure class="nd ne nf ng gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="gh gi nx"><img src="../Images/870d49f9975ba24d36da3eda9550bc19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dnxVy_OwJfwCscZqdEAKww.png"/></div></div></figure><p id="dbb9" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">7。再次标记<strong class="kv ir">热图</strong>，为实际车辆的边界框生成最终的“岛”。这个特定热图的标记创建了<strong class="kv ir"> 2 </strong>个检测“孤岛”。很明显。</p><p id="71b8" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated"><strong class="kv ir"> 8。</strong>将<strong class="kv ir">热图</strong>的标记特征保存到标记列表中，这些特征将被保存一定数量的后续帧。</p><p id="e46a" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated"><strong class="kv ir"> 9。</strong>最后一步是获得车辆的实际边界框。<code class="fe nz oa ob no b"><a class="ae jy" href="http://opencv.org" rel="noopener ugc nofollow" target="_blank"><strong class="kv ir">OpenCV</strong></a></code>提供了便捷的功能<code class="fe nz oa ob no b">cv2.groupRectangles()</code>。正如在<a class="ae jy" href="http://docs.opencv.org/3.0-beta/modules/objdetect/doc/cascade_classification.html?highlight=cv2.grouprectangles#cv2.groupRectangles" rel="noopener ugc nofollow" target="_blank">文档</a>中所说:“它使用矩形等价标准将所有的输入矩形聚类，该标准将具有相似大小和相似位置的矩形组合在一起。”正是需要的。该函数有一个<code class="fe nz oa ob no b">groupThreshold</code>参数，负责“<em class="mo">最小可能矩形数减1 </em>”。也就是说，它不会产生任何结果，直到历史积累了至少该数量的帧的边界框。</p><figure class="nd ne nf ng gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="gh gi nx"><img src="../Images/98d01485e100268e446ca266db951f39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hmkm5e16IjHQkuifKbJmJw.png"/></div></div></figure><h2 id="8bf4" class="lq lr iq bd ls lt lu dn lv lw lx dp ly le lz ma mb li mc md me lm mf mg mh mi bi translated">视频实现</h2><p id="2a62" class="pw-post-body-paragraph kt ku iq kv b kw mj ky kz la mk lc ld le ml lg lh li mm lk ll lm mn lo lp jx ij bi translated">我已经将<strong class="kv ir">车辆</strong>和<a class="ae jy" href="https://github.com/antevis/CarND-Project4-Advanced_Lane_Finding" rel="noopener ugc nofollow" target="_blank">T3】车道检测T5】合并到一个单独的管道中，生成一个包含车道投影和车辆边界框的组合镜头。</a></p><figure class="nd ne nf ng gt kf"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="afb7" class="lq lr iq bd ls lt lu dn lv lw lx dp ly le lz ma mb li mc md me lm mf mg mh mi bi translated">反光</h2><p id="55d5" class="pw-post-body-paragraph kt ku iq kv b kw mj ky kz la mk lc ld le ml lg lh li mm lk ll lm mn lo lp jx ij bi translated">我彻底研究了将<strong class="kv ir"> SVM </strong>分类器应用于<strong class="kv ir"> HOG </strong>特征的方法，该方法涵盖了项目课程，但我仍然非常自信地认为深度学习方法更适合车辆检测任务。在我开始提到的一个讲座中，猪只只从历史的角度来看。此外，还有一篇<a class="ae jy" href="https://arxiv.org/abs/1409.5403" rel="noopener ugc nofollow" target="_blank"> <strong class="kv ir">论文</strong> </a>认为<strong class="kv ir"> DPMs </strong>(那些基于<strong class="kv ir"> HOGs </strong>)可能被认为是某种类型的卷积神经网络。</p><p id="a5f2" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">花了一些时间来弄清楚如何导出一个模型，当扩展它以接受全尺寸感兴趣区域的输入图像时，该模型将产生可靠分辨率的检测图。</p><p id="50be" class="pw-post-body-paragraph kt ku iq kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp jx ij bi translated">即使是我最终选择的微型模型，也需要大约0.75秒才能在2014年中期的3GHz四核i7 MacBook Pro上生成260x1280输入图像的检测图。也就是每秒1.33帧。</p><h2 id="a11f" class="lq lr iq bd ls lt lu dn lv lw lx dp ly le lz ma mb li mc md me lm mf mg mh mi bi translated"><strong class="ak">致谢</strong></h2><p id="31e9" class="pw-post-body-paragraph kt ku iq kv b kw mj ky kz la mk lc ld le ml lg lh li mm lk ll lm mn lo lp jx ij bi translated">我想亲自感谢<a class="ok ol ep" href="https://medium.com/u/8190c86ea791?source=post_page-----44b851d70508--------------------------------" rel="noopener" target="_blank"> David Silver </a>和<a class="ok ol ep" href="https://medium.com/u/fd8bc37755d8?source=post_page-----44b851d70508--------------------------------" rel="noopener" target="_blank"> Oliver Cameron </a>为Udacity的自动驾驶汽车Nanodegree项目开发的精彩内容，以及所有Udacity团队提供的非凡学习体验。</p></div></div>    
</body>
</html>