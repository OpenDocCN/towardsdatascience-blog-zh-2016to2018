<html>
<head>
<title>Super Simple Machine Learning — Simple Linear Regression Part 2 [Math and Python]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超级简单的机器学习—简单线性回归第 2 部分【数学和 Python】</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/super-simple-machine-learning-simple-linear-regression-part-2-math-and-python-1137acb4c352?source=collection_archive---------3-----------------------#2018-01-15">https://towardsdatascience.com/super-simple-machine-learning-simple-linear-regression-part-2-math-and-python-1137acb4c352?source=collection_archive---------3-----------------------#2018-01-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="83e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是我正在撰写的一系列文章的一部分，涵盖了 ML 算法，以简单轻松的方式进行了解释，以便于理解。 <a class="ae km" href="https://medium.com/@bernie.low93/super-simple-machine-learning-by-me-simple-linear-regression-part-1-concept-and-r-4b5b39bbdb5d" rel="noopener"> <em class="kl"> </em> </a> <em class="kl">我确实会掩饰更多的技术方面和术语，因为这里的目标是创造一些以最基本的方式推动理解概念的东西，而不是仅仅遵循步骤和盲目地抛出术语。然而，如果我的解释根本不正确，请告诉我。</em></p><p id="a6e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae km" href="https://medium.com/@bernie.low93/super-simple-machine-learning-by-me-simple-linear-regression-part-1-concept-and-r-4b5b39bbdb5d" rel="noopener"> <em class="kl">第 1 部分</em> </a> <em class="kl">对于简单的线性回归可以在这里找到。</em></p><p id="5817" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，您已经从第 1 部分中了解了简单线性回归的基本概念，让我们来看看本质。</p><p id="79a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将深入一些 Python 编码及其背后的数学知识，并触及数据集的某些特征。</p><p id="b7ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们开始吧！</p><h1 id="f4ca" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">奇妙的参数/统计数据以及在哪里可以找到它们</h1><blockquote class="ll"><p id="5d33" class="lm ln iq bd lo lp lq lr ls lt lu kk dk translated">***编辑！！！！！</p></blockquote><p id="6838" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">我想先弄清楚一些术语。</p><p id="12dc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个朋友告诉我，我之前对参数的定义不正确。</p><p id="b050" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参数是群体的特征(例如所有<em class="kl">可能的</em>结果)。很可能无法导出。</p><p id="78f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">统计数据是样本的特征(例如我们可以记录的结果)。统计数据允许您估计参数:<a class="ae km" href="https://www.cliffsnotes.com/study-guides/statistics/sampling/populations-samples-parameters-and-statistics" rel="noopener ugc nofollow" target="_blank">“推断统计数据允许您对总体参数进行有根据的猜测”</a></p><p id="3c7e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">到目前为止，您应该已经熟悉的特征示例如下:</p><ul class=""><li id="fd81" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated"><strong class="jp ir">平均值:</strong>平均值</li><li id="8358" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><strong class="jp ir">中位数:</strong>中间值</li><li id="2d4d" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><strong class="jp ir">方差</strong>:每个 x 与平均值 x 之间的<strong class="jp ir">平方</strong>差的平均值。它描述了数据的分布范围。如果方差很大，你的“低数值”很低，你的“高数值”很高，想象一个弹性带被拉得越来越远，方差相应地增加。方差越低，越“稳定”,因为它收敛于均值。</li><li id="db3b" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><strong class="jp ir">标准差:方差的平方根</strong>。基本上找到 x 的分布有多宽，这与方差<strong class="jp ir">相同，但</strong>是单位的<strong class="jp ir">问题。如果你在看一个身高(厘米)的数据集，方差会给你平方厘米，但标准差是它的平方根，会给你一个厘米的答案，这有时更好计算，更适合你的强迫症。</strong></li></ul><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mo"><img src="../Images/29bb68c314bb1bbadf7c6f37656092cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uTeZabsAvb5lSLNHQNQnDQ.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">the two different standard deviation formulae</figcaption></figure><p id="15e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先弄清楚方差和标准差，因为你会在统计建模中遇到 ALOT。<a class="ae km" href="http://www.mathsisfun.com/data/standard-deviation.html" rel="noopener ugc nofollow" target="_blank">这个解释</a>挺好的……另外还有小狗！</p><p id="4316" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">超参数:</p><blockquote class="ne nf ng"><p id="3ede" class="jn jo kl jp b jq jr js jt ju jv jw jx nh jz ka kb ni kd ke kf nj kh ki kj kk ij bi translated">值得注意的是，机器学习中有称为超参数的参数，这些参数基本上是您必须决定使用的值，这些值无法从算法中“学习”，但会影响您的模型。</p><p id="ecd7" class="jn jo kl jp b jq jr js jt ju jv jw jx nh jz ka kb ni kd ke kf nj kh ki kj kk ij bi translated">超参数优化/调优是一个完全不同的话题，关于你如何决定你的超参数应该是什么值。我将在下一次解释 K-最近邻分类时谈到这一点。或者你可以在这里阅读<a class="ae km" href="https://datascience.stackexchange.com/questions/14187/what-is-the-difference-between-model-hyperparameters-and-model-parameters" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote><p id="6695" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">非常感谢 Michael Chia Wei Aun 澄清了什么是参数！</em></p><h1 id="f3be" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">好吧，那么…回归？</h1><p id="26af" class="pw-post-body-paragraph jn jo iq jp b jq nk js jt ju nl jw jx jy nm ka kb kc nn ke kf kg no ki kj kk ij bi translated">还记得在第 1 部分<a class="ae km" href="https://medium.com/@bernie.low93/super-simple-machine-learning-by-me-simple-linear-regression-part-1-concept-and-r-4b5b39bbdb5d" rel="noopener">中，我说过如何绘制不同的线以找到误差平方最小的线，以及 R 和 Python 包如何能帮你解决这个问题吗？</a></p><p id="c5e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">好吧，让我们看看这些包在做什么。</p><p id="a277" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请记住这一点:</p><blockquote class="ll"><p id="333c" class="lm ln iq bd lo lp lq lr ls lt lu kk dk translated">y = ax + b</p></blockquote><p id="c2e5" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">直线方程。</p><p id="52d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看寻找最佳拟合线背后的数学原理。</p><p id="35ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">普通最小二乘法背后的等式如下:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/e27d6b53b9876ef9ba6fe289c9015238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*o3fASxpR5_CpqPqjnITWoA.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Figure 1: ??????</figcaption></figure><p id="d20c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一个方程是不是很眼熟？那是因为它和我在上面提到的方程完全一样——一条线的方程，只是写法不同</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/4df98d28e596d040960328efebbfad4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*gLht1ksmeONKfA40fVnu7A.png"/></div></figure><p id="a42b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，第二个和第三个方程是您需要找到<strong class="jp ir"> a </strong>和<strong class="jp ir"> b、</strong>的方程，这基本上是我们在 R 和 python 包中尝试做的事情。</p><p id="aa37" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">a 和 b 在线性回归中被称为“β”,并被视为<strong class="jp ir">学习参数</strong>,因为它们在运行线性回归算法后最终被“学习”。</p><blockquote class="ne nf ng"><p id="1a16" class="jn jo kl jp b jq jr js jt ju jv jw jx nh jz ka kb ni kd ke kf nj kh ki kj kk ij bi translated"><em class="iq">你可以在这里找到上面的方程式</em><a class="ae km" href="https://mubaris.com/2017/09/28/linear-regression-from-scratch/" rel="noopener ugc nofollow" target="_blank"><em class="iq"/></a><em class="iq">及其背后的数学可以在这里找到</em><a class="ae km" href="http://people.revoledu.com/kardi/tutorial/Regression/OLS.html" rel="noopener ugc nofollow" target="_blank"><em class="iq"/></a><em class="iq">。</em></p><p id="4554" class="jn jo kl jp b jq jr js jt ju jv jw jx nh jz ka kb ni kd ke kf nj kh ki kj kk ij bi translated">那些公式/公式是怎么回事？</p><p id="33c2" class="jn jo kl jp b jq jr js jt ju jv jw jx nh jz ka kb ni kd ke kf nj kh ki kj kk ij bi translated">他们试图最小化误差平方和(实际值和预测值的平方差)。如果不确定，请参考<a class="ae km" href="https://medium.com/@bernie.low93/super-simple-machine-learning-by-me-simple-linear-regression-part-1-concept-and-r-4b5b39bbdb5d" rel="noopener">第 1 部分</a>您是否注意到了？！).</p><p id="0389" class="jn jo kl jp b jq jr js jt ju jv jw jx nh jz ka kb ni kd ke kf nj kh ki kj kk ij bi translated">要做到这一点，a 和 b 的偏导数必须为 0。因为 0 是曲线最小时的拐点。是的是的衍生品。</p></blockquote><p id="1a46" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">导数不是我要解释的，但是你可以在这里快速修改一下。</p><h1 id="a4c8" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">x 和 Y 有比你妈妈更酷的头饰</h1><p id="91c6" class="pw-post-body-paragraph jn jo iq jp b jq nk js jt ju nl jw jx jy nm ka kb kc nn ke kf kg no ki kj kk ij bi translated">你会注意到图 1 的第二个和第三个方程，x 和 y 的头上有有趣的东西。</p><blockquote class="ll"><p id="915d" class="lm ln iq bd lo lp lq lr ls lt lu kk dk translated">x̄ = x 酒吧</p><p id="eca3" class="lm ln iq bd lo lp lq lr ls lt lu kk dk translated">̅y = y 酒吧</p></blockquote><p id="fb4a" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">除了是一个总屁股打字，酒吧基本上意味着，意味着。所以 x̄指的是 x 的均值</p><p id="4952" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以(x- x̄)是 x 的值和所有不同 x 值的平均值之差。</p><p id="295f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这被称为偏差分数，意思是偏离值有多远。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nr"><img src="../Images/d3909f9c7a82f5f5aca20ca13037296e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*28iWZQpKSlpgGwJwigx0pg.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">A quick explanation on x- xbar</figcaption></figure><p id="c4aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(xi-x̄)看起来真的很熟悉，不是吗？这是因为它也用于计算方差和标准差。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/4995e050adb116730b31fbd44ee8e784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*AkOE0J4rlZV5tv3fD3Fq3w.png"/></div></figure><p id="5aa9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看看平均值有多有用！这解释了为什么参数/统计如此重要。</p><p id="032a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一个需要注意的符号是帽子</p><blockquote class="ll"><p id="dd2d" class="lm ln iq bd lo lp lq lr ls lt lu kk dk translated">ŷ = y 帽</p></blockquote><figure class="nu nv nw nx ny mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nt"><img src="../Images/bfe3fa3232112b6ee41e5d6210e34336.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hwQ5lo4u8I-FKfVHSENetA.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">how I remember y-hat: guy with fedora, always assuming, trying to predict things, most likely wrong</figcaption></figure><p id="eda3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是指预测方程中 y 的预测值。</p><p id="be1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">换句话说，更准确地说，</p><blockquote class="ll"><p id="e470" class="lm ln iq bd lo lp lq lr ls lt lu kk dk translated">y = ax + b</p></blockquote><p id="931f" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">应该是</p><blockquote class="ll"><p id="fadb" class="lm ln iq bd lo lp lq lr ls lt lu kk dk translated">ŷ = ax + b</p></blockquote><p id="e28f" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">误差基本上是</p><blockquote class="ll"><p id="5a71" class="lm ln iq bd lo lp lq lr ls lt lu kk dk translated">真实 Y -预测 Y</p></blockquote><p id="6aea" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">可以写成</p><blockquote class="ll"><p id="b356" class="lm ln iq bd lo lp lq lr ls lt lu kk dk translated">y - ŷ</p></blockquote><p id="9339" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">这也被称为<strong class="jp ir">残差。(</strong> <em class="kl">还记得</em> <a class="ae km" href="https://medium.com/@bernie.low93/super-simple-machine-learning-by-me-simple-linear-regression-part-1-concept-and-r-4b5b39bbdb5d" rel="noopener"> <em class="kl">第一部分</em> </a> <em class="kl">中关于检查你的残差是随机的，不应该显示出一个模式的那一步吗？)</em></p><p id="1259" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总之，因为生活是艰难而复杂的——预测的误差平方和被写成<strong class="jp ir"> SSE </strong></p><p id="7b3f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但也可以称为:</p><ul class=""><li id="3ab6" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated"><strong class="jp ir">残差平方和</strong></li><li id="8f31" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><strong class="jp ir">残差平方和(SSR) </strong></li></ul><h1 id="dd5b" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">好了，酷的编码从这里开始</h1><p id="c989" class="pw-post-body-paragraph jn jo iq jp b jq nk js jt ju nl jw jx jy nm ka kb kc nn ke kf kg no ki kj kk ij bi translated">既然我们已经解决了所有这些数学问题，让我们来编写代码吧。</p><p id="f3b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将从使用 Python 中的<strong class="jp ir"> sklearn </strong>库中的<strong class="jp ir">线性回归</strong>模块开始。这类似于我在第 1 部分中给出的 R 的一行代码，其中我使用已经预先编码的东西来寻找我的回归线。</p><p id="8620" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我不会在这里讨论 Python 编码，因为我不认为我是合格的 lol，但我尽力在代码本身的#注释中解释了每个步骤。</p><figure class="mp mq mr ms gt mt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="9659" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">耶，你完成了建模！</p><p id="0ad6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看那条<em class="kl">华丽的</em>线。这是不是一个好的线还没有决定(等待第三部分..就等着吧)，不过目前已经决定了这一行有<strong class="jp ir">最少的 SSE(或者 RSS，或者 SSR) </strong>。</p><p id="ffa4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，由于我花了大量时间研究 LinearRegression()方法背后的等式，我想证明它实际上是我们刚刚使用的 python 模块背后的数学。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/e27d6b53b9876ef9ba6fe289c9015238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*o3fASxpR5_CpqPqjnITWoA.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Figure 1 makes a 2nd appearance!</figcaption></figure><figure class="mp mq mr ms gt mt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="6933" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/8ce8b12ffde47007ffdaf2078b730b67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*yfm7aAIVuw8oJoeWbLGxgw.png"/></div></figure><blockquote class="ll"><p id="0256" class="lm ln iq bd lo lp oc od oe of og kk dk translated">python 里的权力符号不是“^”，是“**”</p></blockquote><p id="5507" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">两者给出完全相同的结果:</p><p id="574b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回归方程为 y = 1.37x + 4.27</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/9b37e53744a638226338988c98256653.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*rpkcJxqjXL1mc1rJYzb6Wg.png"/></div></figure><h1 id="0283" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">我们结束了。</h1><p id="9d91" class="pw-post-body-paragraph jn jo iq jp b jq nk js jt ju nl jw jx jy nm ka kb kc nn ke kf kg no ki kj kk ij bi translated">希望你现在对简单线性回归的工作原理有了更好的了解:)我当然知道。</p><p id="e600" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这只是线性回归的第一步，但是你可以自己尝试。我用<a class="ae km" href="https://mubaris.com/2017/09/28/linear-regression-from-scratch/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>作为指南，它被证明是非常全面的，尤其是数学部分。</p><p id="3f15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以在<a class="ae km" href="https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>中设置和编码，或者只使用<a class="ae km" href="https://www.datacamp.com/community/tutorials/data-science-python-ide" rel="noopener ugc nofollow" target="_blank"> Python IDE </a>。</p><p id="a9b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在下一集，我将会谈到评估模型的准确性，以及如何从中得出预测。</p><p id="75fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">敬请期待！</p><h1 id="cae7" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">额外额外！！语境很重要！！</h1><p id="5c4d" class="pw-post-body-paragraph jn jo iq jp b jq nk js jt ju nl jw jx jy nm ka kb kc nn ke kf kg no ki kj kk ij bi translated">在第一部分中，我举的关于‘流泪’和‘考试分数’的例子事后看来很糟糕。</p><p id="e77d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尽管统计数据可以证明这种相关性，但请永远记住</p><blockquote class="ll"><p id="e406" class="lm ln iq bd lo lp lq lr ls lt lu kk dk translated">**相关性不是因果关系* *</p></blockquote><p id="16c1" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated"><em class="kl">对着镜子念 6 遍，希望相关的鬼魂出现给你祝福~ </em></p><p id="f7c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">也许流泪的次数<em class="kl">会影响分数，因为我花在学习上的时间越多，哭得越厉害(在这种情况下，更好的方法是建立学习时间与考试分数的模型)，但这都是假设，可能只是巧合。</em></p><p id="95d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">流泪次数与考试成绩之间的相关性可能是一个重要的因素。</p><p id="0e5f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">*鼓声*</p><blockquote class="ll"><p id="ef8a" class="lm ln iq bd lo lp lq lr ls lt lu kk dk translated"><a class="ae km" href="https://en.wikipedia.org/wiki/Spurious_relationship" rel="noopener ugc nofollow" target="_blank">虚假相关</a></p></blockquote><p id="bffd" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">这就是商业知识和常识发挥作用的地方，也是防止机器偷走你工作的地方。什么是相关的，什么是不相关的，不仅仅是由程序定义的。特征选择在很大程度上是人类和计算机的工作。</p><p id="f524" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">查看泰勒·维根<a class="ae km" href="http://www.tylervigen.com/spurious-correlations" rel="noopener ugc nofollow" target="_blank">的超级有趣的虚假相关性。</a></p><p id="f929" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们已经到了第 2 部分的结尾！谢谢你坚持到现在。请在第 3 部分继续关注，如果发现任何错误，记得告诉我。</p></div></div>    
</body>
</html>