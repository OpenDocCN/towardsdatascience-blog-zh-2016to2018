# 函数式编程有多棒:尾部递归消除

> 原文：<https://towardsdatascience.com/what-is-tail-recursion-elimination-or-why-functional-programming-can-be-awesome-43091d76915e?source=collection_archive---------9----------------------->

![](img/d0622cc0e607a4c5fc6109829dd752f4.png)

Iguanas are experts at tail elimination. I am not sure about the recursion though. Source: [Pixabay](https://pixabay.com/en/iguana-reptile-lizard-animal-2039719/)

尾部递归消除是函数式编程语言中一个非常有趣的特性，比如 Haskell 和 Scala。它使得递归函数调用几乎和循环一样快。

在我的关于 Python 中函数式编程特性的最新文章中，我说鉴于列表理解的存在，T2 映射有点多余，并且也没有很好地描绘 T4 表达式。我觉得我总体上没有做好函数式编程，因为我确实喜欢它作为一种构造程序的优雅方式。它与 Python 的风格和哲学不太合拍。作为越位评论，我提到了缺少尾部递归消除，这是 Python 实现中另一个有争议的设计决策。

老实说，当我写这个的时候，我并不完全确定什么是尾部递归消去法(TRE，从现在开始)。我知道这是一个与递归函数调用有关的优化，它出现在 Haskell 中，但并不多。

所以我决定用我能做到的最好的方式来弥补:学习并写一篇关于它的文章，这样这就不会发生在你身上了！

## 什么是递归调用？

当一个函数调用在被调用的函数范围内进行时，我们说它是递归的。所以基本上它是一个调用自身的函数。

许多问题(实际上任何你可以用循环解决的问题，和许多你不能解决的问题)都可以通过递归调用一个函数直到满足某个条件来解决。
例如，这里有一个以命令式和函数式两种风格编写的 Python 函数:

Note that the last two clauses in the second function could be merged with an ‘or’.

这两个函数在理论上做同样的事情:给定一个列表和一个元素，查看该元素是否存在，并将其作为 bool 返回。但是在较低的层次上，第二个实现进行了大量的函数调用，并且直到最后一个调用完成后才真正返回。为什么这是一个问题？

## 递归消除的动机

由于函数调用会占用计算机堆栈的空间，所以在遇到堆栈溢出之前，我们可以调用的函数数量有一个硬性限制:填满整个堆栈。不仅如此:由于每个函数调用都是从建立堆栈开始的(将东西推到内存和其他高成本的操作)，第二段代码要慢很多。

正如我之前说过的，有些问题你无法用不使用递归的解决方案来解决，或者至少不那么优雅。因此，如果我们能以第二种方式编写函数，并使它们像第一种方式一样快，那将是非常好的——特别是如果这也允许我们避免堆栈溢出的话。

幸运的是，有人已经找到了解决方案——但首先，让我们澄清一些事情。

## 什么是尾部递归？

我们已经看到了为什么我们想要以一种有效的方式实现递归，但我一直在谈论消除尾部递归，而不是所有类型的递归。那么是什么让尾部递归变得特别呢？尾递归只是递归的一个特殊实例，函数的返回值被计算为对自身的调用，除此之外别无其他。

例如，下面是阶乘函数的两个版本。一个是尾递归，一个不是。

请注意，即使第一个函数的返回行包含对自身的调用，它也对其输出做了一些事情(在这个特殊的例子中是计算一个产品),所以返回值并不是递归调用的返回值。通常我们可以通过使用累加器参数使一个常规递归函数尾递归，就像我在第二次声明*阶乘*时所做的那样。

**引入尾部递归消除**

TRE 背后的整个思想是尽可能避免函数调用和堆栈帧，因为它们耗费时间，并且是递归和迭代程序之间的关键区别。你没看错:函数式语言很棒，部分原因是它们找到了调用更少函数的方法。

为了理解下一部分，重要的是回退一步，理解每次函数调用时到底发生了什么。

无论我们的代码是编译的(如 C 或 Golang)还是解释的(如 Python)，它总是以机器语言指令的形式结束。这些通常用汇编或其他类似的语言编码，它们代表了最低层次的抽象，因此是对内存和硬件的最细粒度的控制。

下面是每次函数调用时发生的情况:

*   所有寄存器——存储数据的变量的硬件等价物——都被推到堆栈上(写入内存，但不是以最慢的方式)。
*   您的计算机开始从不同的内存地址读取指令(对应于被调用函数的第一行代码)。
*   代码从该地址开始执行，做函数实际做的事情。通常以某种方式改变寄存器值。
*   所有的寄存器值都从堆栈中弹出/取回，所以我们返回的函数有它的数据。
*   运行 return 语句，并再次开始从前面的函数中读取指令。

就时间而言，第二步和第四步的运行成本更高，就像大多数处理内存的操作一样。每次 push 或 pop 通常需要十倍于“常规”(仅处理寄存器)指令的时间。然而，如果跳过这些步骤，一个函数可以在寄存器中写入值，潜在地覆盖调用者函数已经写入的值。想象一下，如果每次调用 *print，*时，所有变量都变成任意值，会发生什么。

然而，在函数调用自身的特殊情况下，我们可以使用一些技巧:

*   我们可以存储函数开始的内存地址，而不是调用函数，只是在最后将“内存读取器”移回到它那里。
*   我们可以自己写入寄存器，知道前一个函数期望从我们这里得到哪些值，而不必使用堆栈来恢复前一个状态。我们知道“前一个函数”期望的是什么，因为它就是这个函数。不仅如此:我们甚至不需要保存和恢复我们不会改变的寄存器。

这样我们可以避免来回推动和弹出我们的寄存器，这将花费很多时间。但这还不是全部——因为没有实际的函数调用发生(我们只使用了 *jump* 语句——移动我们的指令读取器—),我们没有填充我们的堆栈，也不会发生堆栈溢出。我们不需要首先在堆栈中保存以前的上下文，因为我们只是一次又一次地返回到同一个函数。我们需要保存的唯一上下文是第一次调用我们的函数时的上下文。

总而言之，TRE 是一种优化，它利用了函数调用的一种非常特殊的情况:函数调用自己，并返回它们的输出，而不做任何进一步的处理。它使用函数自身的知识，因此它可以将合适的值写入相关的寄存器，而不必恢复它在运行期间没有进行任何修改的值。然后，当它调用自己时，它就跳到自己的起点，而不必在堆栈中移动任何东西。

由于这个特性，像 Haskell 这样的语言可以运行递归算法的实现，这对于函数式编程来说是至关重要的(特别是对于纯函数式语言)，就像它们的命令式对应物一样快。

这是 Haskell 中一个非常精简的线性搜索，看看它在两行代码中是多么优雅！(对于那些实际上擅长 Haskell 的人，请原谅我的糟糕做法或可怕的代码):

This code does the same thing as the second Python function defined above, only in Haskell

我希望你现在对 TRE 有了更好的理解，也许对函数式语言也有了更好的理解。如果你认为这个解释中的任何部分不够清楚，或者太详细，请在评论中告诉我，因为我还在学习写作。

*如果你想要更多的编程教程、技巧和诀窍，请关注我！
并请考虑* [*表示对我写作的支持*](http://buymeacoffee.com/strikingloo) *。*