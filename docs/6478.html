<html>
<head>
<title>Introducing K-FAC</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">介绍 K-FAC</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introducing-k-fac-and-its-application-for-large-scale-deep-learning-4e3f9b443414?source=collection_archive---------9-----------------------#2018-12-15">https://towardsdatascience.com/introducing-k-fac-and-its-application-for-large-scale-deep-learning-4e3f9b443414?source=collection_archive---------9-----------------------#2018-12-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="41cf" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">大规模深度学习的二阶优化方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2e17528eaa0c307fdea3cb04070c7d65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*khjOBr_j5b61uFyC"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@splashabout?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Nareeta Martin</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5de7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我总结了<a class="ae kv" href="https://arxiv.org/abs/1503.05671" rel="noopener ugc nofollow" target="_blank"> Kronecker-factored 近似曲率(K-FAC) </a> (James Martens et al .，2015)，深度学习最高效的二阶优化方法之一。</p><h2 id="24e3" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">概观</h2><p id="ab49" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">曲率的繁重计算限制了二阶优化方法在深度学习中的应用数量。<a class="ae kv" href="https://arxiv.org/abs/1503.05671" rel="noopener ugc nofollow" target="_blank">Kronecker-factored approximated Curvature(K-FAC)</a>是由多伦多大学的<a class="ae kv" href="http://www.cs.toronto.edu/~jmartens/" rel="noopener ugc nofollow" target="_blank"> James Martens </a>和<a class="ae kv" href="https://www.cs.toronto.edu/~rgrosse/" rel="noopener ugc nofollow" target="_blank"> Roger Grosse </a>在<a class="ae kv" href="https://icml.cc/2015/" rel="noopener ugc nofollow" target="_blank"> ICML2015 </a>中提出的一种深度学习的二阶优化方法，通过<em class="mq"> Kronecker 因子分解</em>来逼近曲率，降低了参数更新的计算复杂度。得益于包括 K-FAC 在内的高效二阶方法，ML 研究人员现在开始重新审视二阶方法快速收敛对于减少深度学习训练时间的好处。</p><h2 id="d672" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">自然梯度下降</h2><p id="0afd" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated"><a class="ae kv" href="https://www.mitpressjournals.org/doi/10.1162/089976698300017746" rel="noopener ugc nofollow" target="_blank">自然梯度下降(NGD) </a>是由甘利顺一(Shun-Ichi Amari)在 1998 年提出的一种基于信息几何的优化方法。NGD 通过使用<strong class="ky ir">费希尔信息矩阵(FIM) </strong>作为损失函数的曲率，正确地获得损失情况，并且在“迭代”方面比简单的一阶方法(例如随机梯度下降)收敛得更快。因此，人们可以把 NGD 看作二阶优化的有效实现。</p><p id="9815" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">给定<strong class="ky ir"> x </strong>，输出条件概率为<strong class="ky ir"> y </strong>的概率模型的 FIM 定义如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/e65bc58efebdb4758772d95c8a533f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*IaIS8UZy7b40Atn--J9rqA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Fisher information matrix (Empirical Fisher)</figcaption></figure><p id="4fc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个取数据期望值的定义叫做<a class="ae kv" href="https://arxiv.org/abs/1503.05671" rel="noopener ugc nofollow" target="_blank"> <em class="mq">经验费希尔</em> </a> <em class="mq"> </em>(当你使用小批量时，你计算其中数据的平均值来得到 FIM)。在图像分类任务中，由于人们经常使用负对数似然的平均值作为损失函数，因此可以将 FIM 视为损失函数的曲率的近似。下面的等式显示了 FIM 和负对数似然损失 E(θ)的<em class="mq"> Hessian </em>之间的关系:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/22bb823198065ba69c9cb812bc936a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*kjQfO6OgHiVj5KK59SD8jQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The loss function</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/5cfc6dafe27996c9330ea1d2ac128aa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*WWVTDvcK6pwDBWOU_bkQDw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The Hessian of the loss function and the relationship between the FIM</figcaption></figure><p id="35c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NGD 的更新规则是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/6f73f32a0ad47c2a3d8aa86015907aed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2tG3VbY0jF4vOPgEOqD7UA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The update rule of Natural Gradient Descent (NGD)</figcaption></figure><p id="cbb6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，FIM 的逆过程应用于损失梯度，FIM 预处理的梯度称为<em class="mq">自然梯度</em>。对于 N 的参数，FIM 的大小是 N×N，深度学习中使用的神经网络往往具有大量的参数(例如，用于 ImageNet 分类的 AlexNet 中的 6000 万个参数)，因此 FIM 的逆是难以处理的，并且它限制了 NGD 在深度学习中的应用数量。</p><h2 id="c9f0" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">自然梯度近似方法</h2><p id="6bd7" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">近年来，一些作品提出了近似(或避免)反演 FIM 的方法，深度学习研究者<a class="ae kv" href="https://openreview.net/forum?id=vz8AumxkAfz5U" rel="noopener ugc nofollow" target="_blank">重温了 NGD </a>的“快速收敛”。</p><p id="aa7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">粗略地说，有三种近似方法(我引用了<a class="ae kv" href="https://www.jstage.jst.go.jp/article/pjsai/JSAI2017/0/JSAI2017_1A2OS05b4/_article/-char/ja" rel="noopener ugc nofollow" target="_blank">这篇文章</a>进行这种分类。)</p><ol class=""><li id="f224" class="mv mw iq ky b kz la lc ld lf mx lj my ln mz lr na nb nc nd bi translated">近似 Fisher 信息矩阵(以便逆矩阵易于计算)</li><li id="e663" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">重新参数化，使 FIM 更接近单位矩阵。</li><li id="a27d" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">直接逼近自然梯度。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl nj"><img src="../Images/05585b7687525ff83c1964ed7e437f49.png" data-original-src="https://miro.medium.com/v2/format:webp/1*N0jZ4FdEZaJuYfNr3x17Zg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The three approaches for approximating NGD</figcaption></figure><h2 id="9343" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">使用克罗内克因子分解(K-FAC)的近似</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/5220a3ba005d70c44635e855c7a57cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TOuIHgzShRisHkL8oBHl_A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Approximation of (inverse of) FIM by K-FAC</figcaption></figure><p id="6327" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以将 K-FAC 视为自然梯度近似方法之一，其对应于“1。近似 Fisher 信息矩阵(这样逆矩阵容易计算)”。特别地，与其他自然梯度近似方法相比，它是基于数学原理的最有效的近似方法。</p><p id="7bd3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，K-FAC 块对角化 FIM，其中每个对角块对应于神经网络每层的参数。例如，K-FAC 将三层网络的 FIM 近似为具有三个块的块对角矩阵。</p><p id="b874" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，K-FAC 用两个矩阵的 Kronecker 积来近似每个块(称为<strong class="ky ir"> Kronecker 因式分解</strong>)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/7b6fb65c6fc5835d8e2db6343a9e14eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*taNWlC7F87Wd1rzePxjzNQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Kronecker product of two matrices</figcaption></figure><p id="3784" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，K-FAC 使用矩阵的 Kronecker 积的临界性质:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/4c369028addee0cce2200af05adc9fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZnvaHHfVvTloXKSKAwC9GQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The inverse of Kronecker product = Kronecker product of the inverse matrices</figcaption></figure><p id="b953" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简而言之，K-FAC 将 FIM 的逆近似为块对角矩阵，其中每个对角块是微小克罗内克因子的逆(与 FIM 相比)。</p><p id="0dad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了阐明克罗内克因子的大小(你可以降低多少求逆的复杂度)，我解释一下克罗内克因子分解的机制。以一个全连接层为例，你可以看到如何因式分解 FIM 的一个对角块(为方便起见，称为<em class="mq"> Fisher 块</em>)对应于这个层。第 I 层中的 Fisher 块表示为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/1c127da91aa1afe732799f47e105ee5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*koEmVYPNDMwlOul--Wva8Q.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The Fisher block of i-th layer (i-th diagonal block of the FIM)</figcaption></figure><p id="5f6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(期望值的符号被简化。)其中∇i 是第 I 层参数的梯度。通过使用<em class="mq">反向传播方法</em>，这是一种在深度神经网络中计算梯度的有效方法，对数似然的梯度(对于每个样本)被表示为两个向量的克罗内克乘积:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/f8c5fdcfdfe3da382b04c58be0af457a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cuYYyK8dIW7PK12ENpAydA.png"/></div></div></figure><p id="7d5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">利用这种关系，费希尔区块可以转化为“克罗内克乘积的期望值”的形式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/12db3f9756976f7f80c3482b674f2edb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*QLAbAksnvVGPbVPzvRMt1g.png"/></div></div></figure><p id="d639" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">K-FAC 将“克罗内克积的期望值”近似为“期望值的克罗内克积”(克罗内克因式分解)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/956e52e106cdd00e37f9ca6ca41b5481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*KRiHgUqOPJcZInmdPnFCng.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Kronecker factorization of a Fisher block by K-FAC</figcaption></figure><p id="3dff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如上所述，Kronecker 因式分解显著降低了 Fisher 块逆运算的计算复杂度。以图像分类领域经常使用的架构<a class="ae kv" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" rel="noopener ugc nofollow" target="_blank"> AlexNet </a>为例，可以更清晰地看到这种效果。下图显示了 Alex net for ImageNet(1000 类分类)的所有层和最后一层(全连通层)的矩阵大小的比较结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/e00fdd8106cc48d537347b6471feeab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rBqvwD_e9nFng5467x6yNg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">AlexNet for Imagenet (1,000 classification)</figcaption></figure><p id="532f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总结到目前为止，可以说 K-FAC 是一种自然梯度近似方法，它执行以下三个过程。</p><ol class=""><li id="a5a9" class="mv mw iq ky b kz la lc ld lf mx lj my ln mz lr na nb nc nd bi translated">通过费希尔信息矩阵的块对角化(每个对角块对应于每一层)，忽略“跨层参数”的相关性。<br/> * <a class="ae kv" href="https://arxiv.org/abs/1503.05671" rel="noopener ugc nofollow" target="_blank">还有一种使用分块三对角</a>的方法(考虑相邻层参数的相关性)。</li><li id="05de" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">通过对每个对角块(Fisher 块)的 Kronecker 分解，忽略每层中“输入”和“输出梯度”之间的相关性。</li><li id="a9d0" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">通过 1，2 的近似，有效地计算费希尔信息矩阵的逆矩阵，以产生自然梯度。</li></ol><p id="fbe4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我为全连接层引入了 K-FAC，但是在卷积层的 K-FAC 中，除了 2 之外，还应用了更多的近似。(参考<a class="ae kv" href="https://arxiv.org/abs/1602.01407" rel="noopener ugc nofollow" target="_blank">论文</a>。)</p><p id="b839" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，以图像数据集 CIFAR-10 的分类任务(10 类分类)为例，说明了 K-FAC 的有效性。下图显示了随机梯度下降法(SGD)、无任何近似的自然梯度下降法(NGD)和 K-FAC 的训练曲线的比较。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/174daf93c29cd03f8ac4389c69250d77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8DFBB-JrhKbojgmfXn2H-w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The comparison of training of ConvNet for CIFAR-10 dataset. Solid line: train, dashed line: validation</figcaption></figure><p id="8407" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以看到 NGD 在“迭代次数”上比 SGD 收敛得快，但是在 NGD，每次迭代的计算时间很长，所以你也可以看到“经过的时间”比 SGD 晚。另一方面，K-FAC 在“迭代次数”方面很好地再现了 NGD 的训练曲线，并且在“经过的时间”方面也比“SGD”快</p><p id="e5ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种快速收敛促使引入了 K-FAC 等自然梯度近似方法，但 K-FAC 在 ImageNet 等大规模深度学习中的应用受到限制，之前没有人针对 SGD 验证过有效性。</p><h2 id="0695" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">K-FAC 的应用</h2><ul class=""><li id="18eb" class="mv mw iq ky b kz ml lc mm lf ns lj nt ln nu lr nv nb nc nd bi translated">递归神经网络(RNN)<br/><a class="ae kv" href="https://openreview.net/forum?id=HyMTkQZAb" rel="noopener ugc nofollow" target="_blank"/>，<br/>詹姆斯·马滕斯，吉米·巴，<br/> ICLR2018。</li><li id="1d01" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr nv nb nc nd bi translated">强化学习<br/> <a class="ae kv" href="https://arxiv.org/abs/1801.05566" rel="noopener ugc nofollow" target="_blank">基于克罗内克因子自然梯度的最近策略优化实证分析</a>，<br/>宋家明，吴，<br/>arXiv:1801.05566【cs .AI]，2018 年 1 月。</li><li id="d9ec" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr nv nb nc nd bi translated">贝叶斯深度学习<br/> <a class="ae kv" href="https://arxiv.org/abs/1712.02390" rel="noopener ugc nofollow" target="_blank">噪声自然梯度作为变分推理</a>，<br/>张国栋，孙升阳，大卫·杜文瑙，罗杰·格罗斯，<br/>arXiv:1712.02390【cs .LG]，2018 年 12 月。</li></ul><h2 id="ec4a" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">K-FAC 的实现</h2><ul class=""><li id="d328" class="mv mw iq ky b kz ml lc mm lf ns lj nt ln nu lr nv nb nc nd bi translated">张量流<br/><a class="ae kv" href="https://github.com/tensorflow/kfac" rel="noopener ugc nofollow" target="_blank">https://github.com/tensorflow/kfac</a></li><li id="4cb6" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr nv nb nc nd bi translated">py torch<br/>T19】https://github.com/yaroslavvb/kfac_pytorchT21【在<a class="ae kv" href="https://medium.com/@yaroslavvb/optimizing-deeper-networks-with-kfac-in-pytorch-4004adcba1b0" rel="noopener">本帖</a>中介绍)</li><li id="a1cc" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr nv nb nc nd bi translated">链轮<br/>T25】https://github.com/tyohei/chainerkfac</li></ul><h2 id="91cd" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">结论</h2><p id="4ba2" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在本文中，我解释了自然梯度法的近似方法之一 K-FAC 的概要。我牺牲了数学的严谨性，致力于直观的理解。</p></div></div>    
</body>
</html>