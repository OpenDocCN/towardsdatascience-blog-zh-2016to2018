<html>
<head>
<title>Identifying duplicate questions on Quora | Top 12% on Kaggle!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">识别Quora上的重复问题| ka ggle上的前12%！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/identifying-duplicate-questions-on-quora-top-12-on-kaggle-4c1cf93f1c30?source=collection_archive---------0-----------------------#2017-06-08">https://towardsdatascience.com/identifying-duplicate-questions-on-quora-top-12-on-kaggle-4c1cf93f1c30?source=collection_archive---------0-----------------------#2017-06-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/7c4f0075621c43598ae5972f6dacadca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*v6hu3LvLlVrK-8JX.png"/></div></div></figure><blockquote class="jy jz ka"><p id="5b10" class="kb kc kd ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果你像我一样是一个普通的Quoran人，你很可能会无意中发现重复的问题，问同样的基本问题。</p></blockquote><p id="f51e" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">这对于作者和搜索者来说都是一个糟糕的用户体验，因为答案在同一问题的不同版本中变得支离破碎。其实这是一个在StackOverflow等其他社交问答平台上非常明显的问题。一个现实世界的问题，求人工智能解决:)</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div class="gh gi ld"><img src="../Images/72482987ba4ce935b96d0e5a23fc6d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*S1F2iEBGRFqGLsGS.png"/></div></figure><p id="b482" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">在过去的一个月左右的时间里，我和其他3000名卡格勒人一起，整夜都在为这个问题绞尽脑汁。这是我连续第二次参加严肃的比赛，有时会有压力，但总体来说我学到了很多——获得了前12%的位置，一枚讨论金牌和几枚核心铜牌:)</p><p id="f495" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi li translated"><span class="l lj lk ll bm lm ln lo lp lq di">一个</span> <strong class="ke ir">关于问题</strong>——Quora已经给出了一个(几乎)真实世界的问题对数据集，每个问题对都带有is_duplicate的标签。目标是最小化测试数据集中重复预测的对数损失。训练集中大约有40万个问题对，而测试集中大约有250万个问题对。对，250万！这些问题中的大部分是计算机生成的问题，以防止作弊，但有250万，天啊！我每隔一个小时就把我那可怜的8GB机器用到了极限</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lr"><img src="../Images/22ee00c78819e185d157936fc34a46f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CSGUzKsKxQTDFwGx.png"/></div></div></figure><p id="99c8" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated"><a class="ae ls" href="https://github.com/shubh24/QuoraDuplicate/blob/master/run_quora.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ke ir">我的方法</strong></a>——我从@anokas的xgboost <a class="ae ls" href="https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb" rel="noopener ugc nofollow" target="_blank"> starter </a>开始，并逐渐在它的基础上构建。我的特征集包含了大约70个特征，与顶级Kagglers的方法相比，这是一个相当低的范围。我的特征可以大致分为基于NLP的特征、基于单词嵌入的距离和基于图形的特征。让我详细说明一下:</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/496880b4cd72709fa3cfd832f7510994.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*LLBKWLw8wUoVJsw4.gif"/></div></figure><p id="03c4" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi li translated"><span class="l lj lk ll bm lm ln lo lp lq di"> N </span>我尝试继续使用tfidf分数，但这既没什么用，又计算量大。相反，最重要的特征之一是加权单词匹配份额，其中每个单词的权重是该单词在语料库中的频率的倒数(基本上是IDF)——如果我在两个问题中都有一个罕见的单词，他们可能会讨论类似的话题。我还有余弦距离、jaccard距离、jarowinkler距离、hamming距离和n-gram匹配(shingling)等特性。</p><p id="09fd" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">我在NLP任务中广泛使用的一个库是Spacy，它最近开发了一些很棒的功能——Spacy相似性也是一个很好的特性。我想到的一些有创意的问题与问题的类型有关——无论是“如何”的问题还是“为什么”的问题——取决于句子的第一个单词。奇怪的是，在建模的时候，我本应该想到建立“最后一个单词的相似度”,但是完全没有想到！命名实体是理解问题上下文的关键——因此common_named_entity得分和common_noun_chunk得分是显而易见的选择。我推出了一个关于通过wordnet语料库计算相似度的<a class="ae ls" href="https://www.kaggle.com/shubh24/wordnet-similarity-matrix-a-naive-implementation" rel="noopener ugc nofollow" target="_blank">内核</a>，但是wordnet在易用性、速度和词汇量方面都有所欠缺。</p><h1 id="26ad" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated"><a class="ae ls" href="https://github.com/shubh24/QuoraDuplicate/blob/master/word2vec_features.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">字嵌入基础距离</strong> </a>:</h1><p id="aa03" class="pw-post-body-paragraph kb kc iq ke b kf ms kh ki kj mt kl km la mu kp kq lb mv kt ku lc mw kx ky kz ij bi translated">做NLP比赛的时候，Word2Vec能不能留下！我觉得word2vec可能是我读过的最酷的计算机科学概念，我总是被它的有效性所震撼。每时每刻。</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mx"><img src="../Images/113d937c882d6ba275dfe3a749b35f4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Poznxd7JdxdbqH_N.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Word2Vec Magic!</figcaption></figure><p id="2cd3" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">总之，我将问题映射到Sent2Vec格式的300维向量中，结果每个问题都有一个向量。自然地，基于距离的矢量特征被建立起来——余弦、城市街区、jacard、堪培拉、欧几里得和布雷柯蒂斯。必须提到@abhishek的<a class="ae ls" href="https://github.com/abhishekkrthakur/is_that_a_duplicate_quora_question" rel="noopener ugc nofollow" target="_blank">脚本</a>作为这些功能的灵感来源。不幸的是，我不能构建真正的嵌入层，我可以在keras中传递到lstm层——我的RAM不允许我这样做。我很快就会有真正的硬件了！</p><p id="a3f6" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi li translated"><span class="l lj lk ll bm lm ln lo lp lq di"> G </span> <strong class="ke ir">图形特征</strong>:在NLP比赛中，这些图形特征玩得相当扫兴！然而，这很好地提醒了我们，社交网络的理论可能如何应用于像Quora的问题对这样的数据集。</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/21e5795696b18d0de92733c895f57956.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iwbul3K-1z2z9Q8Q.jpg"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">A typical graph structure in social media</figcaption></figure><p id="85d5" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">这里，每个问题都是图中的一个节点，数据集中的一个问题对表示两个节点之间的一条边。我们也可以使用测试数据的图结构，因为我们没有在任何地方考虑is_duplicate标签——这200万条边对图贡献了很多！卡格勒夫妇就基于图表的功能是否应该是“神奇的功能”展开了激烈的讨论，这些功能应该被释放出来以创造公平的竞争环境。总之，所有这些特性都极大地提升了大多数模型:</p><ul class=""><li id="e70d" class="nd ne iq ke b kf kg kj kk la nf lb ng lc nh kz ni nj nk nl bi translated">一个节点的度:本质上，问题的频率，这个问题在数据集中出现多少次，就有多少条边。这个特性带来了巨大的收益，因为Quora所做的问题抽样(对重复项进行上抽样)很可能依赖于这个频率。</li><li id="7ecf" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">邻居交集:问题对的一级邻居的百分比，例如，Q1的邻居是Q2，Q3，Q4，Q2的邻居是Q1，Q3。(Q2 Q1)的共同邻居是Q3，占所有一级邻居的一半。</li><li id="22d3" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">分离度:这是我通过广度优先搜索想到并实现的一个特性，但并没有带来很大的改进。</li><li id="470d" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">PageRank:我实现了这个功能，甚至在kaggle上发布了一个<a class="ae ls" href="https://www.kaggle.com/shubh24/pagerank-on-quora-a-basic-implementation" rel="noopener ugc nofollow" target="_blank">内核</a>——page rank较高的问题链接到重要的(page rank较高的)问题，而垃圾问题链接到垃圾问题。</li><li id="2a6f" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">kcore/kclique: K-core基本上是最大的子图，其中每个节点都连接到至少“K”个节点，但没有给我太多的收获。我曾经想过kclique，但是因为时间不够而没有实现:(结果证明这是一个相当重要的想法！</li><li id="4d2b" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">加权图:在比赛的后期，kagglers的同事分享了一个加权图的想法，其中每个节点的权重是weighted_word_share(我们之前讨论过)。这个加权图中的邻居交集是有用的。</li></ul><p id="7c31" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated"><strong class="ke ir">传递性魔法</strong>——继续图结构，对问题之间的传递性建模是一种显而易见的方法。例如，如果Q1与Q2相似，Q2与Q3相似，这意味着Q1与Q3更相似(根据我们的数据集)。这是我开始构建的功能之一，但中途放弃了，这是一个代价高昂的错误。许多顶级解决方案以某种形式使用了这一功能，一个更简单的版本是平均每个问题与其对应的邻居之间的重复概率。</p><h1 id="9c5a" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated"><strong class="ak">我的白板会议</strong></h1><figure class="le lf lg lh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mx"><img src="../Images/fe5953848ac8ee2e4e2659bb4f7ad3a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MhJ8-vwVoDdwR1M2.jpg"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Part 1 of 2</figcaption></figure><figure class="le lf lg lh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mx"><img src="../Images/fcaa0e579c31bf6d3f29fed6589dd692.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UU9UdhqUl0i4_-Kb.jpg"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Part 2 of 2</figcaption></figure><h1 id="39d3" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated"><strong class="ak">一些混蛋</strong></h1><figure class="le lf lg lh gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/5f502c7936d26a5f507ab473e0111ea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*a_-ipUIki2YRx9C6.gif"/></div></figure><p id="0eac" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">当一个像我这样的菜鸟一头扎进一个大圈子时，肯定会有麻烦。尽管我有意识地努力保持管道的模块化和版本控制，但由于管道中的一个严重错误，我损失了几乎一个周末的工作。经过艰苦的学习，我在ipython <a class="ae ls" href="https://github.com/shubh24/QuoraDuplicate/blob/master/run_quora.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>上恢复并改造了管道。我在构建特性时面临的一个主要挑战是编写内存高效的代码，而不是重新构建以前的特性，模块化是关键。这导致了另一个混乱——因为我无法在我的RAM中处理我的整个测试数据集，我将它分成六个子集，并迭代地构建特性。这意味着我在旧的数据框架和新的特性之间做一个pandas concat，我很少关注索引:(花了几天时间对所有NaN特性挠头。没有错。</p><h1 id="a857" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated"><strong class="ak">QID难题:</strong></h1><p id="69a1" class="pw-post-body-paragraph kb kc iq ke b kf ms kh ki kj mt kl km la mu kp kq lb mv kt ku lc mw kx ky kz ij bi translated">训练数据集的每个问题都有一个ID，因此每行都有一个QID1和QID2。然而，测试数据集的情况并非如此，这意味着“QID”不能直接用作一个功能。一位kaggler的同事发布了一个令人难以置信的创造性的<a class="ae ls" href="https://www.kaggle.com/ashhafez/temporal-pattern-in-train-response-rates" rel="noopener ugc nofollow" target="_blank">观察</a>,随着QID的增加，平均重复率(滚动平均值)下降——很可能是Quora随着时间的推移改进算法的迹象，从而随着ID的增加减少重复问题的数量。这一推断基于这样的假设，即QID值没有被屏蔽，并且真正代表了发布问题的时间。为了在我们的测试数据集中对qid建模，我有一个将问题文本映射到qid的哈希表。现在迭代test_df中的所有问题——如果我遇到一个存在的问题，相应的QID被分配给它。否则，我们假设这是一个新问题，按照发布的时间顺序，将QID加1。这导致了各种各样的特征，如QID差异、平均QID、最小QID，以期模拟随着时间的推移重复率的下降。</p><h1 id="deed" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated"><strong class="ak">阶层失衡:</strong></h1><p id="7ce0" class="pw-post-body-paragraph kb kc iq ke b kf ms kh ki kj mt kl km la mu kp kq lb mv kt ku lc mw kx ky kz ij bi translated">讨论的主要部分集中在猜测测试数据框架中的类别划分，这与测试划分并不明显相似。数学专家通过几个恒定值的提交，这里的<a class="ae ls" href="https://www.kaggle.com/davidthaler/how-many-1-s-are-in-the-public-lb" rel="noopener ugc nofollow" target="_blank"/>和这里的<a class="ae ls" href="https://www.kaggle.com/c/quora-question-pairs/discussion/31179" rel="noopener ugc nofollow" target="_blank"/>，计算出一个狭窄的分割范围。训练集中大约有34%的正重复，而测试集中估计有16%-17%的正重复——这可能是改进的Quora算法的结果，或者是计算机生成的问题对的结果。无论如何，一个错误的训练数据集不会有所帮助——人们想出了过采样的解决方案(复制训练中的负行)，或者通过适当的因子重新调整他们的预测。</p><blockquote class="jy jz ka"><p id="cd12" class="kb kc kd ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">我的模特——XGBoost是爱情，XG boost是生活</strong></p></blockquote><figure class="le lf lg lh gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/100dcf1eda37f765268a249fed793c19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*rP3ut4o3oGA--7pv.jpg"/></div></figure><p id="6419" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">承认这一点非常尴尬，但我提交的只是一个单一模型解决方案，一个2000轮的xgboost。公平地说，我没有在参数调整或构建多样化模型上花太多时间，因为太晚了。我尝试了默认的随机森林和GBM，效果并不比XGB好。堆叠和集成也在路线图中，但只是停留在那里:(</p><p id="24ee" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">相反，我花了一个很长的周末来学习Keras和建立密集的神经网络——这是我的第一次！通过了解各种超参数和理想的架构，激活函数，辍学层和优化背后的理论！非常有趣的东西！我用我的两层sigmoid神经网络和我的70个特性来完善它，但它从未接近xgboost。正如我前面提到的，我无法用我的硬件构建嵌入层或LSTMs，那肯定会有帮助。很快。</p><p id="ded1" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">我为自己的建模管道感到自豪，不必为每个模型都创建一个函数而烦恼——当我构建数百个模型进行堆叠时，这会很有帮助。很快。</p><h1 id="a9a5" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated"><strong class="ak">顶级解决方案:</strong></h1><p id="f084" class="pw-post-body-paragraph kb kc iq ke b kf ms kh ki kj mt kl km la mu kp kq lb mv kt ku lc mw kx ky kz ij bi translated">金牌得主的赛后评论让我觉得自己完全是个菜鸟！我必须真正提高我的比赛水平，更加努力地去达到那个高度，铜牌离我不远了！从赢家解决方案中获得的一些主要经验:</p><ul class=""><li id="09e5" class="nd ne iq ke b kf kg kj kk la nf lb ng lc nh kz ni nj nk nl bi translated">大多数团队根据加权图的边或节点的频率来重新调整他们的最终预测</li><li id="e83f" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">每个顶级团队都建造了一个有数百个模型的堆垛机，并有一个可重复使用的模型构建管道。</li><li id="c103" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">使用了最先进的神经网络架构(Siamese/Attention NNs)、LightGBMs，但甚至像ExtraTrees或Random Forests这样的低性能模型也有帮助！</li><li id="6f62" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">图形特性是许多解决方案的核心，因为团队使用各种技术从中获取价值。一些去除了虚假的(不太频繁的)节点，一些使用邻居权重的平均值/中值，而大多数模拟了传递性。</li><li id="637b" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">显然，问题1或问题2也很重要。令人惊讶！</li><li id="32df" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">NLP:以多种不同的方式处理文本——小写和不变，以不同的方式替换标点，包含和排除停用词，词干化和不词干化，等等。</li><li id="06f6" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">堆叠很重要，但人们也用单个xgb型号实现了0.14倍。</li><li id="d37d" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">第一名<a class="ae ls" href="https://www.kaggle.com/c/quora-question-pairs/discussion/34355" rel="noopener ugc nofollow" target="_blank">后</a>。真令人羞愧。</li></ul><h1 id="c0aa" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated"><strong class="ak">结论</strong>:</h1><p id="f576" class="pw-post-body-paragraph kb kc iq ke b kf ms kh ki kj mt kl km la mu kp kq lb mv kt ku lc mw kx ky kz ij bi translated">我应该更好地管理我的时间，为探索、特征工程、模型构建和堆叠分配适当的时间。上周我确实感到了压力，因为我提交的材料不够了。</p><ul class=""><li id="272e" class="nd ne iq ke b kf kg kj kk la nf lb ng lc nh kz ni nj nk nl bi translated">我的建模管道很适合测试，但是我需要在不同的超参数/数据子集上建立更多的模型。</li><li id="3e33" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">不要中途放弃构建特性，有些特性会导致代价高昂的失误。</li><li id="f582" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">不要仅仅因为你花了时间在一个功能上，就过于沉迷于它。这有点好笑，哈哈！</li><li id="c7e7" class="nd ne iq ke b kf nm kj nn la no lb np lc nq kz ni nj nk nl bi translated">花大量时间在Kaggle内核和讨论上。他们太酷了:)</li></ul><p id="5973" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated">所以是的，就是这样。一场比赛结束，另一场比赛开始。Kaggle会上瘾！</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d115f78494c48243b33dc5d61944ba2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*vFNJtzLhigcqOIfWtUWrQA.gif"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">It’s true!</figcaption></figure><p id="c5b1" class="pw-post-body-paragraph kb kc iq ke b kf kg kh ki kj kk kl km la ko kp kq lb ks kt ku lc kw kx ky kz ij bi translated"><em class="kd">这篇文章最初发表在我的</em> <a class="ae ls" href="https://shubh24.github.io" rel="noopener ugc nofollow" target="_blank"> <em class="kd">博客</em> </a> <em class="kd">上。每周一篇博客</em></p></div></div>    
</body>
</html>