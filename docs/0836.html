<html>
<head>
<title>GANS — PART2: DCGANs (deep convolution GANS) for generating images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GANS —第二部分:用于生成图像的深度卷积GANS</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gans-part2-dcgans-deep-convolution-gans-for-generating-images-c5d3c7c3510e?source=collection_archive---------1-----------------------#2017-06-28">https://towardsdatascience.com/gans-part2-dcgans-deep-convolution-gans-for-generating-images-c5d3c7c3510e?source=collection_archive---------1-----------------------#2017-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="21b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">深度卷积GAN或DCGAN在生成器和鉴别器中使用卷积层。在的<a class="ae kl" href="https://arxiv.org/pdf/1511.06434.pdf" rel="noopener ugc nofollow" target="_blank">论文中首次探讨了DCGAN架构。还需要使用批量标准化来训练卷积网络。</a></p><h2 id="cded" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">发电机</h2><p id="24d0" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">第一层是一个完全连接的层，它被重新塑造成一个又深又窄的层，有点像原始DCGAN纸中的4x4x1024。然后我们使用批处理规范化和一个泄漏的ReLU激活。接下来是转置卷积，通常情况下，您会将前一层的深度减半，宽度和高度加倍。同样，我们使用批处理规范化和泄漏ReLU。对于这些层中的每一层，一般的方案是卷积&gt;批范数&gt;泄漏ReLU。</p><p id="7338" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您继续像这样堆叠层，直到您获得形状为32x32x3的最终转置卷积层。下面是最初的DCGAN论文中使用的结构:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/379e18d5bb564d20ceeec2112a852a1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tv7wjpBTB0Pg6rWfLm4YSA.png"/></div></div></figure><pre class="ll lm ln lo gt lw lx ly lz aw ma bi"><span id="dd87" class="km kn iq lx b gy mb mc l md me">def conv_transpose_layer(prev_layer, filter, kernel_size, strides, is_training, alpha):<br/>    x = tf.layers.conv2d_transpose(prev_layer, filter, kernel_size, strides, 'same')<br/>    x = tf.layers.batch_normalization(x, training=is_training)<br/>    x = tf.maximum(x, alpha*x)<br/>    return x</span><span id="6965" class="km kn iq lx b gy mf mc l md me">def generator(z, output_dim, reuse=False, alpha=0.2, training=True):<br/>    with tf.variable_scope('generator', reuse=reuse):<br/>        <br/>        # First fully connected layer<br/>        x1 = tf.layers.dense(z, 4*4*512)<br/>        # Reshape it to start the convolutional stack<br/>        x1 = tf.reshape(x1, (-1, 4, 4, 512))<br/>        x1 = tf.layers.batch_normalization(x1, training=training)<br/>        x1 = tf.maximum(x1, alpha*x1)<br/>        # 4x4x512 now</span><span id="e85a" class="km kn iq lx b gy mf mc l md me">        x2 = conv_transpose_layer(x1, 256, 5, 2, training, alpha)<br/>        # 8x8x256 now<br/>        <br/>        x3 = conv_transpose_layer(x2, 128, 5, 2, training, alpha)<br/>        # 16x16x128 now<br/>               <br/>        # Output layer, 32x32x3<br/>        logits = tf.layers.conv2d_transpose(x3, output_dim, 5, 2, 'same')<br/>        # 32x32x3 now<br/>        <br/>        out = tf.tanh(logits)<br/>        return out</span></pre><h2 id="9ca5" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">鉴别器</h2><p id="1af6" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">鉴别器基本上只是一个卷积分类器。请注意，在DCGAN论文中，他们仅使用步长卷积层进行了所有下采样，没有使用最大池层。</p><p id="5e84" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在除第一卷积和输出层之外的每一层<strong class="jp ir">上使用<code class="fe mg mh mi lx b">tf.layers.batch_normalization</code>进行批量归一化。同样，每一层应该看起来像卷积&gt;批处理规范&gt;泄漏ReLU。</strong></p><pre class="ll lm ln lo gt lw lx ly lz aw ma bi"><span id="0a5d" class="km kn iq lx b gy mb mc l md me">def conv_layer(prev_layer, filters, is_training, alpha, batch_norm=True):<br/>    conv_layer = tf.layers.conv2d(prev_layer, filters, 5, 2, 'same', use_bias=False, activation=None)<br/>    if batch_norm:<br/>        conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)<br/>    conv_layer = tf.maximum(conv_layer, alpha*conv_layer)<br/>    return conv_layer</span><span id="92fd" class="km kn iq lx b gy mf mc l md me">def discriminator(x, reuse=False, alpha=0.2):<br/>    with tf.variable_scope('discriminator', reuse=reuse):<br/>        # Input layer is 32x32x3<br/>        x1 = conv_layer(x, 64, True, alpha, False)<br/>        # 16x16x64</span><span id="5959" class="km kn iq lx b gy mf mc l md me">        x2 = conv_layer(x1, 128, True, alpha, True)<br/>        # 8x8x128</span><span id="b540" class="km kn iq lx b gy mf mc l md me">        x3 = conv_layer(x2, 256, True, alpha, True)<br/>        # 4x4x256</span><span id="62eb" class="km kn iq lx b gy mf mc l md me">        flat = tf.reshape(x3, (-1, 4*4*256))</span><span id="6db2" class="km kn iq lx b gy mf mc l md me">        logits = tf.layers.dense(flat, 1)<br/>        out = tf.sigmoid(logits)<br/>        <br/>        return out, logits</span></pre><blockquote class="mj mk ml"><p id="2c0c" class="jn jo mm jp b jq jr js jt ju jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj kk ij bi translated">gan对超参数非常敏感。为了找到最佳的超参数，进行了大量的实验，以使发生器和鉴别器不会相互干扰。</p></blockquote><pre class="ll lm ln lo gt lw lx ly lz aw ma bi"><span id="8c2d" class="km kn iq lx b gy mb mc l md me">real_size = (32,32,3)<br/>z_size = 100<br/>learning_rate = 0.0002<br/>batch_size = 128<br/>epochs = 25<br/>alpha = 0.2<br/>beta1 = 0.5</span></pre><p id="da98" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://github.com/mchablani/deep-learning/blob/master/dcgan-svhn/DCGAN_Exercises.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/mchablani/deep-learning/blob/master/DCGAN-svhn/DCGAN _ exercises . ipynb</a></p><p id="5537" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">学分:来自课堂讲稿:<a class="ae kl" href="https://classroom.udacity.com/nanodegrees/nd101/syllabus" rel="noopener ugc nofollow" target="_blank">https://classroom.udacity.com/nanodegrees/nd101/syllabus</a></p></div></div>    
</body>
</html>