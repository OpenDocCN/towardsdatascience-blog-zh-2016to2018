<html>
<head>
<title>Understanding Swift for TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解面向TensorFlow的Swift</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-with-swift-for-tensorflow-9167df128912?source=collection_archive---------4-----------------------#2018-04-28">https://towardsdatascience.com/machine-learning-with-swift-for-tensorflow-9167df128912?source=collection_archive---------4-----------------------#2018-04-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="24eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di"> S </span>用于TensorFlow的wift由<a class="ae ku" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjg-pT0n93aAhWMQI8KHSvnAfkQFggoMAA&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FChris_Lattner&amp;usg=AOvVaw1P92WCcalUe6I19h7fIJUG" rel="noopener ugc nofollow" target="_blank">克里斯·拉特纳</a>在2018年TensorFlow开发峰会上推出。2018年4月27日，谷歌团队在他们的<a class="ae ku" href="https://github.com/tensorflow/swift" rel="noopener ugc nofollow" target="_blank"> GitHub知识库</a>上首次向公众社区发布。但Swift for TensorFlow仍处于起步阶段。而且开发人员/研究人员在项目中使用它似乎还为时过早。如果您仍有兴趣试用，请从Swift官网<a class="ae ku" href="https://swift.org" rel="noopener ugc nofollow" target="_blank">为TensorFlow的快照安装Swift。</a></p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi kv"><img src="../Images/6671b19cc0a05cc100a4c7d853c5f042.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bnmoBKbmztTkbv8ObMP_CA.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Swift for TensorFlow (<a class="ae ku" href="https://medium.com/tensorflow/introducing-swift-for-tensorflow-b75722c58df0" rel="noopener">Image Source</a>)</figcaption></figure><p id="c0f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本文中，我将重点解释以下主题:</p><ul class=""><li id="d5dc" class="ll lm iq jp b jq jr ju jv jy ln kc lo kg lp kk lq lr ls lt bi translated">访问Python APIs和<code class="fe lu lv lw lx b">PyValue</code> (Python的Swift动态系统)</li><li id="2527" class="ll lm iq jp b jq ly ju lz jy ma kc mb kg mc kk lq lr ls lt bi translated">Swift中的自动区分系统</li><li id="ad1a" class="ll lm iq jp b jq ly ju lz jy ma kc mb kg mc kk lq lr ls lt bi translated">对TensorFlow的Swift中的<code class="fe lu lv lw lx b">Tensor</code> s执行计算</li><li id="eaf1" class="ll lm iq jp b jq ly ju lz jy ma kc mb kg mc kk lq lr ls lt bi translated">训练神经网络</li></ul><blockquote class="md me mf"><p id="d368" class="jn jo mg jp b jq jr js jt ju jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj kk ij bi translated">Swift for TensorFlow很可能会像Swift和TensorFlow独立完成的那样，与开源社区一起快速发展。因此，这可能是一个值得努力去了解它的东西。</p></blockquote><p id="3903" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="mg">注意</em> </strong>:关于TensorFlow的Swift需要注意的一个主要问题是，它是一个<em class="mg">由运行定义的</em>框架。这意味着，尽管Swift for TensorFlow在后台创建图表(如TensorFlow ),但您不必为执行这些图表创建会话。这种方法类似于TensorFlow中的急切执行。</p><h1 id="28fa" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated">1.主要特点</h1><ul class=""><li id="8d43" class="ll lm iq jp b jq ni ju nj jy nk kc nl kg nm kk lq lr ls lt bi translated">自动反向微分(正向尚未实现)</li><li id="2af8" class="ll lm iq jp b jq ly ju lz jy ma kc mb kg mc kk lq lr ls lt bi translated">运行定义设计(不需要会议)</li><li id="e61f" class="ll lm iq jp b jq ly ju lz jy ma kc mb kg mc kk lq lr ls lt bi translated">Swift经过优化，包含机器学习特定功能</li><li id="d05a" class="ll lm iq jp b jq ly ju lz jy ma kc mb kg mc kk lq lr ls lt bi translated">允许Python APIs以Python方式访问</li><li id="a079" class="ll lm iq jp b jq ly ju lz jy ma kc mb kg mc kk lq lr ls lt bi translated">包括用于Python的动态系统行为的<code class="fe lu lv lw lx b">PyValue</code>类型</li></ul><h1 id="2500" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated">2.Python互操作性</h1><p id="b279" class="pw-post-body-paragraph jn jo iq jp b jq ni js jt ju nj jw jx jy nn ka kb kc no ke kf kg np ki kj kk ij bi translated">借助Swift for TensorFlow，我们可以以最Python化的方式使用Python APIs。要访问Python APIs，必须进入程序，如下面的示例代码片段所示。</p><pre class="kw kx ky kz gt nq lx nr ns aw nt bi"><span id="882c" class="nu ml iq lx b gy nv nw l nx ny">import Python</span><span id="93c1" class="nu ml iq lx b gy nz nw l nx ny">let np = Python.import("numpy")  // akin to `import numpy as np`<br/>let pickle = Python.import("pickle")<br/>let gzip = Python.import("gzip")</span></pre><p id="0053" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">而用于TensorFlow的Swift也有一个名为<code class="fe lu lv lw lx b">PyValue</code>的新类型，它展示了Python在Swift中的完整动态类型系统行为，而不影响Swift中其他类型的行为。</p><pre class="kw kx ky kz gt nq lx nr ns aw nt bi"><span id="9230" class="nu ml iq lx b gy nv nw l nx ny">var x: PyValue = 3.14159<br/>print(x * 2)  // Prints "6.28318"<br/>x = "string"<br/>print("now a " + x)  // Prints "now a string"</span></pre><p id="78f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更多信息请参考官方的<a class="ae ku" href="https://github.com/tensorflow/swift/blob/master/docs/PythonInteroperability.md" rel="noopener ugc nofollow" target="_blank"> Python互操作性</a>文档。</p><h1 id="8ef4" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated">3.自动微分</h1><p id="625a" class="pw-post-body-paragraph jn jo iq jp b jq ni js jt ju nj jw jx jy nn ka kb kc no ke kf kg np ki kj kk ij bi translated">Swift for TensorFlow内置了对计算函数相对于其他变量的梯度的支持。该功能已被直接整合到Swift的编译器中，以优化行为。它支持两种差分功能:<code class="fe lu lv lw lx b">#gradient(of:withRespectTo:)</code>和<code class="fe lu lv lw lx b">#valueAndGradient(of:)</code>。虽然它对我不起作用😩(还为时过早)但是文档中说要遵循下面的语法。这段代码片段来自官方的d <a class="ae ku" href="https://github.com/tensorflow/swift/blob/master/docs/AutomaticDifferentiation.md" rel="noopener ugc nofollow" target="_blank">文档。</a></p><pre class="kw kx ky kz gt nq lx nr ns aw nt bi"><span id="cb4c" class="nu ml iq lx b gy nv nw l nx ny">@differentiable(reverse, adjoint: dTanh)<br/>func tanh(_ x: Float) -&gt; Float {<br/>  // ... some super low-level assembly tanh implementation ...<br/>}<br/>func dTanh(x: Float, y: Float, seed: Float) -&gt; Float {<br/>  return (1.0 - (y * y)) * seed<br/>}</span><span id="0d7d" class="nu ml iq lx b gy nz nw l nx ny">// Get the gradient function of tanh.<br/>let dtanh_dx = #gradient(of: tanh)<br/>dtanh_dx(2)<br/>// Get the gradient function of foo with respect to the first parameter.<br/>let dfoo_dx = #gradient(of: foo, withRespectTo: .0)<br/>dfoo_dx(3, 4)</span></pre><p id="7751" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">目前仅允许自动<em class="mg">反向</em>微分，正向微分正在讨论中。</p><h1 id="2ed0" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated">4.使用张量</h1><p id="5abe" class="pw-post-body-paragraph jn jo iq jp b jq ni js jt ju nj jw jx jy nn ka kb kc no ke kf kg np ki kj kk ij bi translated">作为一个简单的例子，我们将创建一个<code class="fe lu lv lw lx b">Tensor</code>实例，在其上我们使用TensorFlow应用一些操作。</p><pre class="kw kx ky kz gt nq lx nr ns aw nt bi"><span id="7733" class="nu ml iq lx b gy nv nw l nx ny">import TensorFlow</span><span id="3768" class="nu ml iq lx b gy nz nw l nx ny">var x = Tensor([[1, 2], [3, 4]])<br/>for _ in 1...5 {<br/>  x += x<br/>}<br/>print(x)  // Prints "[[32.0, 64.0], [96.0, 128.0]]"</span></pre><p id="be3e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的代码中使用了基本的<code class="fe lu lv lw lx b">+</code>操作符，它在一个循环中将<code class="fe lu lv lw lx b">Tensor</code>添加到自身中。这之所以成为可能，是因为<a class="ae ku" href="https://developer.apple.com/library/content/documentation/Swift/Conceptual/Swift_Programming_Language/AdvancedOperators.html" rel="noopener ugc nofollow" target="_blank"> Swift的高级操作符</a>功能为<code class="fe lu lv lw lx b">Tensor</code>实例提供了过载功能。</p><h1 id="3eab" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated">5.训练一个简单的前馈神经网络</h1><p id="2e6b" class="pw-post-body-paragraph jn jo iq jp b jq ni js jt ju nj jw jx jy nn ka kb kc no ke kf kg np ki kj kk ij bi translated">转向神经网络——机器学习在这个时代流行的真正原因。在本节中，我们将教授我们的3层完全连接的<em class="mg">前馈神经网络</em>来预测来自MNIST数据集的图像中的数字。从Swift文件中的tensor flow<code class="fe lu lv lw lx b">import TensorFlow</code>开始。</p><p id="e89c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="mg">注</em> : </strong>训练代码可以在<a class="ae ku" href="https://github.com/tensorflow/swift-models/blob/master/MNIST/MNIST.swift" rel="noopener ugc nofollow" target="_blank">这里</a>找到如果有人不耐烦的话。</p><p id="d62a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的神经网络将有3层:</p><ul class=""><li id="f33c" class="ll lm iq jp b jq jr ju jv jy ln kc lo kg lp kk lq lr ls lt bi translated"><strong class="jp ir">输入层</strong>:它将输入数据(在我们的例子中是像素值)呈现给神经网络。在我们的例子中，每个图像有784个值。</li><li id="1ad3" class="ll lm iq jp b jq ly ju lz jy ma kc mb kg mc kk lq lr ls lt bi translated"><strong class="jp ir">隐藏层</strong>:它用权重和偏差计算我们输入数据的仿射变换。然后将一个<em class="mg"> sigmoid </em>激活函数应用于变换。我们示例中的隐藏层将有30个单元(神经元)。</li><li id="fd18" class="ll lm iq jp b jq ly ju lz jy ma kc mb kg mc kk lq lr ls lt bi translated"><strong class="jp ir">输出层</strong>:隐藏层的数据再次经过<em class="mg">仿射变换</em>和一个<em class="mg"> sigmoid </em>函数的应用，就像形成输出层之前一样。这是预测发生的地方。我们在这一层有10个单元，每个单元代表在图像中成为特定数字的概率。还要注意，我们在该层中使用<em class="mg">one-hot</em>/<em class="mg">1-of-k</em>编码，其中在一维张量中，除了所有其他值为0之外，单个值为1。例如，[0，0，1，0，0，0，0，0，0，0]表示输出[预测]层图像中的两位数。</li></ul><p id="2194" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mg">仿射变换</em>基本上是数据与权重的点积，然后是偏差的增加，随后是激活函数的逐元素应用。以下是输入数据的仿射变换的方程式<em class="mg"> x. </em></p><blockquote class="md me mf"><p id="9cca" class="jn jo mg jp b jq jr js jt ju jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj kk ij bi translated">o(x；W，b)= f(wx+b)</p></blockquote><p id="a982" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，<em class="mg"> O(。)</em>是输出函数，<em class="mg"> f(。)</em>是激活函数(我们这里是sigmoid)，<em class="mg"> W </em>是权重矩阵，<em class="mg"> b </em>是偏置向量，代表点积。</p><p id="adb0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用<em class="mg"> sigmoid </em>激活函数，因为它将值压缩到限制输出范围的范围[0，1],从而提供输出层图像中可能数字的概率。</p><h2 id="1785" class="nu ml iq bd mm oa ob dn mq oc od dp mu jy oe of my kc og oh nc kg oi oj ng ok bi translated">5.1读取MNIST数据</h2><p id="ae77" class="pw-post-body-paragraph jn jo iq jp b jq ni js jt ju nj jw jx jy nn ka kb kc no ke kf kg np ki kj kk ij bi translated">让我们读取数据集并构建这些图像和标签的<code class="fe lu lv lw lx b">Tensor</code>实例。我们必须创建<code class="fe lu lv lw lx b">Tensor</code>对象，因为这是TensorFlow模型(神经网络)允许流经的对象，因此得名。</p><pre class="kw kx ky kz gt nq lx nr ns aw nt bi"><span id="fa9c" class="nu ml iq lx b gy nv nw l nx ny">let (images, numericLabels) = readMnist(imagesFile: imagesFile,labelsFile: labelsFile)<br/>let labels = Tensor&lt;Float&gt;(oneHotAtIndices: numericLabels, depth: 10)</span></pre><h2 id="ffdf" class="nu ml iq bd mm oa ob dn mq oc od dp mu jy oe of my kc og oh nc kg oi oj ng ok bi translated">5.2超参数</h2><p id="0400" class="pw-post-body-paragraph jn jo iq jp b jq ni js jt ju nj jw jx jy nn ka kb kc no ke kf kg np ki kj kk ij bi translated">我们定义了3个超参数:学习率、训练损失和迭代步骤。</p><pre class="kw kx ky kz gt nq lx nr ns aw nt bi"><span id="bfb9" class="nu ml iq lx b gy nv nw l nx ny">let iterationCount: Int32 = 20<br/>let learningRate: Float = 0.2<br/>var loss = Float.infinity</span></pre><h2 id="5724" class="nu ml iq bd mm oa ob dn mq oc od dp mu jy oe of my kc og oh nc kg oi oj ng ok bi translated">5.3可训练参数</h2><p id="e826" class="pw-post-body-paragraph jn jo iq jp b jq ni js jt ju nj jw jx jy nn ka kb kc no ke kf kg np ki kj kk ij bi translated">接下来，我们根据张量流的<code class="fe lu lv lw lx b">Tensor</code>类型创建2个权重矩阵、2个偏置向量，如下所示。</p><pre class="kw kx ky kz gt nq lx nr ns aw nt bi"><span id="339f" class="nu ml iq lx b gy nv nw l nx ny">var w1 = Tensor&lt;Float&gt;(randomUniform: [784, 30])<br/>var w2 = Tensor&lt;Float&gt;(randomUniform: [30, 10])<br/>var b1 = Tensor&lt;Float&gt;(zeros: [1, 30])<br/>var b2 = Tensor&lt;Float&gt;(zeros: [1, 10])</span></pre><h2 id="4345" class="nu ml iq bd mm oa ob dn mq oc od dp mu jy oe of my kc og oh nc kg oi oj ng ok bi translated">5.4训练循环</h2><p id="095b" class="pw-post-body-paragraph jn jo iq jp b jq ni js jt ju nj jw jx jy nn ka kb kc no ke kf kg np ki kj kk ij bi translated">训练循环是神经网络进行学习的代码块。我们通过网络传递图像和标签<code class="fe lu lv lw lx b">Tensor</code>(正向传递)。然后计算预测中的误差，然后将它们反向传播以计算可训练参数的梯度。接下来，我们在学习率的帮助下，使用相应的梯度来降低这些参数。最后计算损失，给出我们离图像的真实标签有多远的概念。每个步骤描述如下。</p><p id="ec8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 5.4.1向前传球</strong></p><p id="d6e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所述，输入图像像素值经过仿射变换。这里的值是带权重的点积，然后加上偏差，偏差进一步通过s形激活函数(按元素方式应用)。</p><pre class="kw kx ky kz gt nq lx nr ns aw nt bi"><span id="8e3b" class="nu ml iq lx b gy nv nw l nx ny">let z1 = images ⊗ w1 + b1<br/>let h1 = sigmoid(z1)<br/>let z2 = h1 ⊗ w2 + b2<br/>let predictions = sigmoid(z2)</span></pre><p id="ec63" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里需要注意的一点是Swift使用⊗ unicode来表示点积，这表明Swift语言实际上是多么酷！坦白说，我真的很喜欢♥️这种编程语言。</p><p id="4f2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 5.4.2反向传递(计算梯度)</strong></p><p id="be25" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">反向传递计算预测和真实标签之间的误差。这些误差然后通过网络反向传播，计算可学习参数的梯度。</p><pre class="kw kx ky kz gt nq lx nr ns aw nt bi"><span id="3176" class="nu ml iq lx b gy nv nw l nx ny">let dz2 = predictions - labels<br/>let dw2 = h1.transposed(withPermutations: 1, 0) ⊗ dz2<br/>let db2 = dz2.sum(squeezingAxes: 0)<br/>let dz1 = dz2.dot(w2.transposed(withPermutations: 1, 0)) * h1 * (1 - h1)<br/>let dw1 = images.transposed(withPermutations: 1, 0) ⊗ dz1<br/>let db1 = dz1.sum(squeezingAxes: 0)</span></pre><p id="469f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 5.4.3下降参数</strong></p><p id="5fa2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们用它们的梯度和决定神经网络学习它的参数的速度的学习速率来降低参数，以便在下一次输入图像被馈送给它时预测真实值。</p><pre class="kw kx ky kz gt nq lx nr ns aw nt bi"><span id="903a" class="nu ml iq lx b gy nv nw l nx ny">w1 -= dw1 * learningRate<br/>b1 -= db1 * learningRate<br/>w2 -= dw2 * learningRate<br/>b2 -= db2 * learningRate</span></pre><p id="cf46" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 5.4.4更新损失</strong></p><p id="df50" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们更新损失值以查看我们与真实标签的接近程度，以便下次更正确地预测数字图像。</p><pre class="kw kx ky kz gt nq lx nr ns aw nt bi"><span id="6dbe" class="nu ml iq lx b gy nv nw l nx ny">loss = dz2.squared().mean(squeezingAxes: 1, 0).scalarized()</span></pre><p id="a291" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们现在打印我们的损失，它告诉我们如何从我们的训练集中学习识别数字图像。越低的损失越好是我们网络识别的任务。</p><pre class="kw kx ky kz gt nq lx nr ns aw nt bi"><span id="81a7" class="nu ml iq lx b gy nv nw l nx ny">print("Loss: \(loss)")  // Prints "0.1"</span></pre><h1 id="ee21" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated">6.摘要</h1><p id="cfcc" class="pw-post-body-paragraph jn jo iq jp b jq ni js jt ju nj jw jx jy nn ka kb kc no ke kf kg np ki kj kk ij bi translated">在本文中，我们了解了用于TensorFlow的Swift，以及它的易用性，因为Swift与Python非常相似，看起来像脚本语言，但速度非常快。我们看到Swift for TensorFlow允许我们使用Python APIs，而且Swift的编译器已经过深度优化，内置了对自动微分的支持，这对机器学习任务非常重要。我们还看到了如何在Swift中使用TensorFlow，我们创建了自己的<code class="fe lu lv lw lx b">Tensor</code>实例，并对它们进行了一些处理(使用基本操作符<code class="fe lu lv lw lx b">+</code>)。最后，我们训练了三层神经网络来解决传统的数字图像识别问题。</p><h1 id="6495" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated">7.讨论</h1><p id="d0d1" class="pw-post-body-paragraph jn jo iq jp b jq ni js jt ju nj jw jx jy nn ka kb kc no ke kf kg np ki kj kk ij bi translated">似乎Swift的名称应该是<em class="mg">tensor flow</em>，而不是TensorFlow 的<em class="mg"> Swift。事实并非如此，因为实际上Swift的编译器已被修改为支持TensorFlow，因此Swift不仅充当Python库和TensorFlow的包装器，现在更像是机器学习语言。为了在整个机器学习和数据科学社区中保持工作流的一致性(因为Python被大量使用)，它还允许以Python的方式访问Python APIs，并且还为Python的动态系统类型行为实例提供了一种新的类型。</em></p><p id="268d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后一句话，用于TensorFlow的Swift是由Google开发的，因此它很有可能在未来的时代变得著名。它还试图利用原始TensorFlow实现的最佳功能，如eager-execution。</p><h1 id="c511" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated">8.参考</h1><p id="acac" class="pw-post-body-paragraph jn jo iq jp b jq ni js jt ju nj jw jx jy nn ka kb kc no ke kf kg np ki kj kk ij bi translated">[1] <a class="ae ku" href="https://github.com/tensorflow/swift" rel="noopener ugc nofollow" target="_blank"> Swift for TensorFlow </a>，谷歌</p><p id="b166" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae ku" href="https://swift.org" rel="noopener ugc nofollow" target="_blank">Swift.org</a>，苹果</p><p id="6d14" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[3] <a class="ae ku" href="https://github.com/tensorflow/swift/blob/master/docs/PythonInteroperability.md" rel="noopener ugc nofollow" target="_blank"> Python的互操作性</a></p><p id="622d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[4]<a class="ae ku" href="https://github.com/tensorflow/swift/blob/master/docs/AutomaticDifferentiation.md" rel="noopener ugc nofollow" target="_blank">Swift中的自动微分</a></p><p id="9d8d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[5]<a class="ae ku" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwi55bKTp-TaAhUDPI8KHbJeBn4QFggoMAA&amp;url=https%3A%2F%2Fdeveloper.apple.com%2Flibrary%2Fcontent%2Fdocumentation%2FSwift%2FConceptual%2FSwift_Programming_Language%2FAdvancedOperators.html&amp;usg=AOvVaw1LBV8xPgX9PCn4lc67weZs" rel="noopener ugc nofollow" target="_blank">Swift编程语言(Swift 4.1):高级操作员</a></p><p id="05da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[6]<a class="ae ku" href="https://github.com/tensorflow/swift-models/blob/master/MNIST/MNIST.swift" rel="noopener ugc nofollow" target="_blank">Swift for tensor flow:MNIST示例</a></p><p id="c766" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你觉得这篇文章有用/有见识，请鼓掌👏这样其他人也可以找到它，或者你也可以在社交网络上分享它。如果你在我的解释中发现了一些错误(也许我解释错了)，或者你从这篇文章中有什么不清楚的地方，你也可以在下面发表评论。T11】</p><p id="6e4f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="mg">保持【机器】学习，直到你化石燃料！🤘🤖</em>T15】</strong></p></div></div>    
</body>
</html>