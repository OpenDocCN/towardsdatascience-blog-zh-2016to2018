<html>
<head>
<title>Introduction to Image Processing: Building a Simple Digit Recognizer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图像处理简介:构建一个简单的数字识别器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-image-processing-building-a-simple-digit-recognizer-b0c74d70d85d?source=collection_archive---------11-----------------------#2018-11-14">https://towardsdatascience.com/introduction-to-image-processing-building-a-simple-digit-recognizer-b0c74d70d85d?source=collection_archive---------11-----------------------#2018-11-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/06a815a5bfddc585d8a977c9f1568a8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u7dyl_NYyv1ze1QwACIt5Q.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/photos/MK4QKBqG_lA?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Toa Heftiba</a> on <a class="ae jd" href="https://unsplash.com/search/photos/numbers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="2c72" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">它必须从某个地方开始</h2></div><p id="78ab" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数字识别并不是什么困难或高级的事情。有点像“你好，世界！”程序——不是很酷，但是你可以从这里开始。所以我决定分享我的工作，同时更新知识——这是我很久以前玩的图像。</p><h1 id="8b56" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">数据导入和探索</h1><p id="d6c2" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">我们从导入所有必需的包开始。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="c298" class="mx ls jg mt b gy my mz l na nb">import pandas as pd<br/>import random<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from tqdm import tqdm</span><span id="de39" class="mx ls jg mt b gy nc mz l na nb">%matplotlib inline</span></pre><p id="b278" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">包含 2 万个手写数字的 MNIST 数据集是这项任务的“Hello World”数据集，它已经预加载在 Colaboratory(基于云的 Python 笔记本，奇妙的东西 BTW)中，所以我们将使用它。这里不需要发明轮子。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="c2aa" class="mx ls jg mt b gy my mz l na nb"># load data<br/>df = pd.read_csv('sample_data/mnist_train_small.csv', header=None)</span><span id="2797" class="mx ls jg mt b gy nc mz l na nb">df.head()</span><span id="63fa" class="mx ls jg mt b gy nc mz l na nb">Out[7]: <br/>   0    1    2    3    4    5    6   ...   778  779  780  781  782  <br/>0    6    0    0    0    0    0    0 ...     0    0    0    0    0    <br/>1    5    0    0    0    0    0    0 ...     0    0    0    0    0    <br/>2    7    0    0    0    0    0    0 ...     0    0    0    0    0    <br/>3    9    0    0    0    0    0    0 ...     0    0    0    0    0    <br/>4    5    0    0    0    0    0    0 ...     0    0    0    0    0    </span><span id="9f91" class="mx ls jg mt b gy nc mz l na nb"><br/>len(df)<br/>Out[9]: 20000</span></pre><p id="a180" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如我们从 head()方法中看到的，数据集中的第一列包含标签和图像的其余像素 28×28——这就是为什么我们多了 784 列。每次修改后检查数据集的长度也是有用的，以确保我们做的一切都是正确的。</p><p id="5ffd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，让我们可视化我们的像素，并观看我们的图像。每次运行下面的代码时，我们都使用 randint()来选择随机图像。此外，我们必须将我们的像素转换为 numpy 数组(现在它的类型是 Series ),并将其调整为 28×28 的大小，以便能够绘制它们。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="480b" class="mx ls jg mt b gy my mz l na nb">ix = random.randint(0, len(df)-1)<br/>label, pixels = df.loc[ix][0], df.loc[ix][1:]<br/>img = np.array(pixels).reshape((28,28))<br/>print('label: ' + str(label))<br/>plt.imshow(img)</span><span id="8dba" class="mx ls jg mt b gy nc mz l na nb">label: 9<br/>&lt;matplotlib.image.AxesImage at 0x7ff9ac6fda20&gt;</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/47ff7de01b992d14174af4569d29e4ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*ZuoCpddR7i6q1dXlZcuHKg.png"/></div></figure><h1 id="e8bb" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">数据预处理</h1><p id="13a9" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">现在，为了让我们的生活稍微轻松一点，我们将把我们的数据帧转换成只有两列——标签和图像，其中图像是一个像素的<em class="ne"> numpy </em>数组。此外，我们将减少数据帧的大小，以加快计算速度(首先，我们要确保一切正常，然后我们开始玩模型)</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="c141" class="mx ls jg mt b gy my mz l na nb"># transforming df for easier manipulation<br/>labels, imgs = [], []<br/>for index, row in df.iterrows():<br/>    label, pixels = row[0], row[1:]<br/>    img = np.array(pixels)<br/>    labels.append(label)<br/>    imgs.append(img)<br/><br/>df2 = pd.DataFrame({'label': labels, 'img': imgs})<br/>df2 = df2[:1000]<br/></span><span id="37ab" class="mx ls jg mt b gy nc mz l na nb"># checking images using new df structure<br/>ix = random.randint(0, len(df2)-1)<br/>img = df2.loc[ix].img.reshape((28,28))<br/>label = df2.loc[ix].label<br/>print('label: ' + str(label))<br/>plt.imshow(img)</span><span id="4ed2" class="mx ls jg mt b gy nc mz l na nb">label: 9<br/>&lt;matplotlib.image.AxesImage at 0x7ff9a9b997f0&gt;</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/295ce0888fe8ee6841864e3a73f25c42.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*CeT3cCnBM7CFAxZQXdrX7A.png"/></div></figure><p id="c358" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当我们准备好数据后，我们想把它分成两个数据集:一个用来训练我们的模型，另一个用来测试它的性能。最好的方法是使用<em class="ne"> sklearn </em>。我们设置了一个<em class="ne"> test_size=0.2 </em>，这是该操作的标准值(通常对于测试，我们会留下 20–30%的数据)，这意味着对于训练，仍然是 80%。设置<em class="ne"> shuffle=True </em>也是一个很好的做法，因为一些数据集可能有有序的数据，所以模型将学习识别 0 和 1，但不会知道例如 8 的存在。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="8fc0" class="mx ls jg mt b gy my mz l na nb">from sklearn.model_selection import train_test_split<br/>train_df, test_df = train_test_split(df2, test_size=0.2, shuffle=True)</span><span id="7475" class="mx ls jg mt b gy nc mz l na nb">print(len(train_df), len(test_df))<br/>800 200</span><span id="edeb" class="mx ls jg mt b gy nc mz l na nb">train_df.head()</span><span id="8cec" class="mx ls jg mt b gy nc mz l na nb">Out[12]: <br/>     label                                                img<br/>296      9  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<br/>488      2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<br/>124      7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<br/>862      7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<br/>421      9  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</span></pre><h1 id="c7bd" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">建立模型</h1><p id="38e1" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">我们检查了数据集的长度和头部——都很好，我们可以开始构建我们的模型了。为此，我们需要安装<em class="ne"> pytorch </em>。如果我们转到“代码片段”并开始在那里键入<em class="ne">“pyt”</em>，它将显示“安装[pytorch]”，因此我们可以将其插入到我们的笔记本中。如果有人已经安装了<em class="ne"> pytorch </em>，可以跳过这一步。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="7b46" class="mx ls jg mt b gy my mz l na nb"># http://pytorch.org/<br/>from os.path import exists<br/>from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag<br/>platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())<br/>cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\.\([0-9]*\)\.\([0-9]*\)$/cu\1\2/'<br/>accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'<br/><br/>!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision</span><span id="5a47" class="mx ls jg mt b gy nc mz l na nb"># importing torch and setting up the device<br/>import torch<br/>device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")</span><span id="0971" class="mx ls jg mt b gy nc mz l na nb">print(device)</span></pre><p id="3fc2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我们必须将数据转换成 pytorch 数据集。<code class="fe nf ng nh mt b">torch.utils.data.Dataset</code>是表示数据集的抽象类。自定义数据集应继承数据集并重写以下方法:</p><ul class=""><li id="12c1" class="ni nj jg kx b ky kz lb lc le nk li nl lm nm lq nn no np nq bi translated"><code class="fe nf ng nh mt b">__len__</code>以便<code class="fe nf ng nh mt b">len(dataset)</code>返回数据集的大小。</li><li id="5aed" class="ni nj jg kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated"><code class="fe nf ng nh mt b">__getitem__</code>支持索引，以便<code class="fe nf ng nh mt b">dataset[i]</code>可用于获取其样本</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="4be8" class="mx ls jg mt b gy my mz l na nb"># create torch dataset<br/>from torch.utils.data import Dataset<br/><br/><br/>class MNISTDataset(Dataset):<br/>    def __init__(self, imgs, labels):<br/>        super(MNISTDataset, self).__init__()<br/>        self.imgs = imgs<br/>        self.labels = labels<br/><br/>    def __len__(self):<br/>        return len(self.imgs)<br/><br/>    def __getitem__(self, ix):<br/>        img = self.imgs[ix]<br/>        label = self.labels[ix]<br/>        return torch.from_numpy(img).float(), label</span><span id="616c" class="mx ls jg mt b gy nc mz l na nb">dataset = {<br/>    'train': MNISTDataset(train_df.img.values, train_df.label.values),<br/>    'test': MNISTDataset(test_df.img.values, test_df.label.values)<br/>} <br/><br/>len(dataset['train'])<br/>800</span><span id="a626" class="mx ls jg mt b gy nc mz l na nb"># again checking image, now based on torch dataset<br/>ix = random.randint(0, len(dataset['train'])-1)<br/>img, label = dataset['train'][ix]<br/>print(img.shape, img.dtype)<br/>print(label)<br/>plt.imshow(img.reshape((28,28)))</span><span id="460a" class="mx ls jg mt b gy nc mz l na nb">torch.Size([784]) torch.float32<br/>6<br/>&lt;matplotlib.image.AxesImage at 0x7ff99eeeed30&gt;</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/f4234d09383518937da6736f3c23d534.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*mUTfGid2rUNxh-klqjf8_Q.png"/></div></figure><p id="1d53" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">pytorch 的美妙之处在于它定义模型的简单性。我们用输入和输出定义我们的层，我们添加一些批量标准化来改进我们的模型(这是一种向神经网络中的任何层提供零均值/单位方差输入的技术)和激活函数，在这种情况下 ReLU。</p><p id="0917" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于第一个输入，我们有 784 个神经元(每个像素一个神经元)和 512 个输出(这个几乎是随机的——我尝试了几个不同的值，这个表现得很好，所以我离开了)。下一层将有 512 个输入(<em class="ne">input _ layer[n+1]= = output _ layer[n]</em>)和 256 个输出，接下来有 256 个输入和 128 个输出，最后一层有 128 个输入和 10 个输出(每个神经元代表 10 个数字中的一个)</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="deed" class="mx ls jg mt b gy my mz l na nb"># create model<br/>import torch.nn as nn<br/><br/>def block(in_f, out_f):<br/>  return nn.Sequential(<br/>      nn.Linear(in_f, out_f),<br/>      nn.BatchNorm1d(out_f),<br/>      nn.ReLU(inplace=True),<br/>      #nn.Dropout(),<br/>  )<br/><br/>model = nn.Sequential(<br/>  block(784,512),<br/>  block(512,256),<br/>  block(256,128),<br/>  nn.Linear(128, 10)<br/>)<br/><br/>model.to(device)</span></pre><p id="fd4b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们需要为我们的模型创建一些额外的参数:</p><ul class=""><li id="8651" class="ni nj jg kx b ky kz lb lc le nk li nl lm nm lq nn no np nq bi translated">标准—计算损失函数，在我们的例子中是<a class="ae jd" href="https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss" rel="noopener ugc nofollow" target="_blank"> CrossEntropyLoss </a></li><li id="f14c" class="ni nj jg kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">优化器—设置学习率</li><li id="97ec" class="ni nj jg kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">调度器——如果模型没有随时间改进，更新学习率(非常强大的技术，允许我们随时调整系统)</li><li id="8a7c" class="ni nj jg kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">data loader——py torch 的类，为数据集提供单进程或多进程迭代器</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="e8c5" class="mx ls jg mt b gy my mz l na nb">from torch.utils.data import DataLoader<br/>import torch.optim as optim<br/>from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau<br/><br/>criterion = nn.CrossEntropyLoss()<br/>optimizer = optim.Adam(model.parameters(), lr=0.1)<br/>scheduler = ReduceLROnPlateau(optimizer, 'max', factor=0.1, patience=3, min_lr=0.0001, verbose=True)<br/><br/>dataloader = {<br/>    'train': DataLoader(dataset['train'], batch_size=32, shuffle=True, num_workers=4),<br/>    'test': DataLoader(dataset['test'], batch_size=32, shuffle=False, num_workers=4),<br/>}</span></pre><h1 id="b90d" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">训练和评估模型</h1><p id="879b" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">有了这些，我们就可以开始训练和评估我们的模型了。虽然我们定义了 100 个时期，但是如果模型没有随着时间的推移而改进，那么停止循环也是有用的。这里我们已经设置了<code class="fe nf ng nh mt b">early_stop = 10</code>，所以如果模型连续 10 个纪元没有改变，我们将停止训练过程。</p><p id="862a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">训练过程:我们通过将每个图像和标签分配给先前定义的设备来迭代我们的训练数据，我们给我们的模型一个图像，它试图找到正确的类(<code class="fe nf ng nh mt b">preds</code>)，我们清除所有梯度(<code class="fe nf ng nh mt b">zero_grad()</code>)并计算损失函数和梯度(<code class="fe nf ng nh mt b">loss</code>)，执行优化步骤并将新值附加到<code class="fe nf ng nh mt b">total_loss</code>数组。</p><p id="2650" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">测试过程:我们迭代测试数据，进行预测，计算模型的损失和准确性。在<code class="fe nf ng nh mt b">torch.max()</code>中，我们寻找最大值的索引，因为它将代表一个数字的类别，在我们的例子中，它将匹配标签。然后，通过比较标签和预测，我们计算我们的模型的准确性。</p><p id="102e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每当我们找到最佳模型时，我们就保存它，如果我们点击<code class="fe nf ng nh mt b">early_stop</code>，我们就退出并报告结果。通常它不需要所有的 100 个纪元。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="d1b3" class="mx ls jg mt b gy my mz l na nb"># train<br/>best_acc, stop, early_stop = 0, 0, 10<br/>for e in range(100):<br/><br/>    model.train()<br/>    total_loss = []<br/>    for imgs, labels in tqdm(dataloader['train']):<br/>        imgs, labels = imgs.to(device), labels.to(device)<br/>        preds = model(imgs)<br/>        optimizer.zero_grad()<br/>        loss = criterion(preds, labels)<br/>        loss.backward()<br/>        optimizer.step()<br/>        total_loss.append(loss.data)<br/><br/>    model.eval()<br/>    val_loss, acc = [], 0.<br/>    with torch.no_grad():<br/>        for imgs, labels in tqdm(dataloader['test']):<br/>            imgs, labels = imgs.to(device), labels.to(device)<br/>            preds = model(imgs)<br/>            loss = criterion(preds, labels)<br/>            val_loss.append(loss.data)<br/>            _, preds = torch.max(preds, 1)<br/>            acc += (preds == labels).sum().item()<br/><br/>    acc /= len(dataset['test'])<br/>    if acc &gt; best_acc:<br/>        print('\n Best model ! saved.')<br/>        torch.save(model.state_dict(), 'best_model.pt')<br/>        best_acc = acc<br/>        stop = -1<br/><br/>    stop += 1<br/>    if stop &gt;= early_stop:<br/>        break<br/><br/>    scheduler.step(acc)<br/><br/>    print('\n Epoch {}, Training loss: {:4f}, Val loss: {:4f}, Val acc: {:4f}'.format(<br/>        e + 1, np.array(total_loss).mean(), np.array(val_loss).mean(), acc))<br/><br/>print('\n Best model with acc: {}'.format(best_acc))</span><span id="cde1" class="mx ls jg mt b gy nc mz l na nb"><br/>Out[n]:<br/>Epoch 30, Training loss: 0.015759, Val loss: 0.397337, Val acc: 0.910000<br/>100%|██████████| 25/25 [00:01&lt;00:00, 22.10it/s]<br/>100%|██████████| 7/7 [00:00&lt;00:00, 73.41it/s]</span><span id="ed9a" class="mx ls jg mt b gy nc mz l na nb">Best model with acc: 0.91</span></pre><p id="f7d2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当我们找到我们的最佳模型并保存它时，我们可以通过向它输入新数据来玩它，并观察它的表现。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="5bdc" class="mx ls jg mt b gy my mz l na nb"># test<br/>model.load_state_dict(torch.load('best_model.pt'))<br/>model.to(device)<br/>model.eval()<br/><br/>ix = random.randint(0, len(dataset['test'])-1)<br/>img, label = dataset['test'][ix]<br/>pred = model(img.unsqueeze(0).to(device)).cpu()<br/>pred_label = torch.argmax(pred)<br/>print('Ground Truth: {}, Prediction: {}'.format(label, pred_label))<br/>plt.imshow(img.reshape((28,28)))</span><span id="fa37" class="mx ls jg mt b gy nc mz l na nb">Ground Truth: 5, Prediction: 5<br/>&lt;matplotlib.image.AxesImage at 0x7ff9a9ced748&gt;</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/bbf6f0b262d8a8c3b5c0982c3b2b1f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*QE3Hcm8cdG02v69eG5PmBg.png"/></div></figure><p id="6c9c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">就像一开始说的，这是一个图像识别的“Hello World ”,我们没有使用卷积神经网络，它通常用于这样的任务，只是入门级别的理解流程。我通常不处理图像，所以如果有错误，请告诉我。对我来说，这是一次很好的复习，希望对其他人也有帮助。</p></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><p id="4777" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可以在<a class="ae jd" href="https://github.com/slehkyi/notebooks-for-articles/blob/master/digit-recognizer-colab.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到 Python 笔记本的代码</p></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><p id="41e5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="ne">原载于 2018 年 11 月 14 日</em><a class="ae jd" href="http://sergilehkyi.com/introduction-to-image-recognition-building-a-simple-digit-detector/" rel="noopener ugc nofollow" target="_blank"><em class="ne">【sergilehkyi.com】</em></a><em class="ne">。</em></p></div></div>    
</body>
</html>