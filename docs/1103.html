<html>
<head>
<title>What is a Decision Tree?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是决策树？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-a-decision-tree-22975f00f3e1?source=collection_archive---------2-----------------------#2017-07-29">https://towardsdatascience.com/what-is-a-decision-tree-22975f00f3e1?source=collection_archive---------2-----------------------#2017-07-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/6d9a4bb5db22e3421f532971bd51a52b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*EFCePNEkqoGmxm5qR-nqrA.gif"/></div></figure><p id="e918" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">银行在考虑是否向某人提供贷款时，通常会考虑一系列问题，以判断向个人提供贷款是否安全。那些问题可以从人有什么样的收入这样简单的问题开始。如果介于 30-70，000 美元之间，他们会继续回答下一个问题。他们目前的工作做了多长时间？如果 1-5 年后，这会导致他们的下一个问题:他们会用信用卡付款吗？如果是，他们会提供贷款，如果不是，他们不会。这个过程最基本的形式是决策树。</p><p id="7864" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">决策树是用于回归和分类问题的大量使用的非参数有效机器学习建模技术。为了找到解决方案，决策树根据预测数据对结果变量进行连续的、分层的决策。</p><p id="c756" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">那么这一切意味着什么呢？</p><p id="a4fd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">分层意味着模型由一系列问题定义，当应用于任何观察时，这些问题导致一个类别标签或一个值。一旦建立起来，模型就像一系列“如果这发生，那么这发生”条件中的协议，从输入数据产生特定的结果。</p><p id="dcfa" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">非参数方法意味着没有关于误差或数据分布的潜在假设。它基本上意味着模型是基于观察到的数据构建的。</p><p id="9481" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">目标变量使用一组离散值的决策树模型被分类为分类树。在这些树中，每个节点或叶子代表类标签，而分支代表导致类标签的特征的合取。目标变量取连续值(通常是数字)的决策树称为回归树。这两种类型通常在 CART(分类和回归树)中一起被提及。</p><p id="4c46" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">每个购物车模型都是有向无环图的一个例子。这些图具有表示关于给定预测值的主变量的决策点的节点，边是节点之间的连接。在上面的贷款场景中，30-70 美元是一个优势，而“工作年限”是节点。</p><p id="9ad5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">由于决策树的目标是在每个节点的末端做出最佳选择，因此它需要一种能够做到这一点的算法。这种算法被称为亨特算法，它既贪婪又递归。贪婪意味着在步骤中它做出最优决策，递归意味着它将较大的问题分成较小的问题，并以相同的方式解决它们。根据称为<strong class="jw ir">纯度</strong>的度量标准，决定在每个节点进行拆分。当一个节点被 50/50 平均分割时，它是 100%不纯的，当它的所有数据都属于一个类时，它是 100%纯的。</p><p id="2213" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了优化我们的模型，我们需要达到最大纯度并避免杂质。为了衡量这一点，我们使用了基尼系数，它衡量随机选择的元素被错误标记的频率，如果它是根据分布随机标记的。它的计算方法是将标签为 I 的物品被选中的概率 pi 乘以时间分类错误的概率(1–pi)。我们的目标是让它达到 0，在这里它将是最小不纯的，最大纯的，属于一个类别。</p><p id="59fd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">使用的另一个度量是信息增益，它用于决定在树的每一步拆分什么特征。这是通过维基百科精心设计的方程式计算出来的，</p><pre class="ks kt ku kv gt kw kx ky kz aw la bi"><span id="d4e4" class="lb lc iq kx b gy ld le l lf lg">Information Gain = Entropy(parent) - Weighted Sum of Entropy(Children).</span></pre><p id="8bc7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">虽然这是一个很好的模型，但它确实存在一个很大的问题，即当所有信息都在一个类或属性中时，它会停止运行。以偏差为代价，这个模型的方差是巨大的，肯定会导致过度拟合。“决策树学习者可以创建过于复杂的树，这些树不能很好地从训练数据中归纳出来。”那么网络如何应对这种情况呢？我们可以设置决策树的最大深度(即，它将深入多少个节点(上面的贷款树的深度为 3 )),和/或另一种方法是指定做出每个决策所需的最小数据点数。</p><p id="cad6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">决策树的其他缺点是什么:它使用贪婪算法进行局部优化，我们不能保证返回到全局最优的决策树。如果单个类接受一个数据集，这是一个令人难以置信的有偏差的模型，除非数据集在放入树中之前是平衡的。</p><p id="0527" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">虽然有缺点，但决策树也有很多优点。</p><p id="4574" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">由于它们的可视化表示，它们非常容易理解，它们需要很少的数据，可以处理定性和定量数据，可以使用统计集进行验证，可以处理大量数据，并且计算成本非常低。</p><p id="c4d1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我希望这篇文章能帮助你更好地理解决策树。对于编码和更多关于它们的知识，我强烈建议您查看关于决策树的 Scikit-Learns 文档。</p></div></div>    
</body>
</html>