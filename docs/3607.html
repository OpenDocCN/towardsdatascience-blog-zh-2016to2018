<html>
<head>
<title>Soft Sign Activation Function with Tensorflow [ Manual Back Prop with TF ]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带 Tensorflow 的软标志激活功能[带 TF 的手动后撑]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/soft-sign-activation-function-with-tensorflow-manual-back-prop-with-tf-5a04f3c8e9c1?source=collection_archive---------13-----------------------#2018-05-29">https://towardsdatascience.com/soft-sign-activation-function-with-tensorflow-manual-back-prop-with-tf-5a04f3c8e9c1?source=collection_archive---------13-----------------------#2018-05-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/93b48b9955215dab1bce41e1b610919f.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*0N4VQt77-Em-USy4b-mNiQ.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" rel="noopener" target="_blank" href="/basic-data-cleaning-engineering-session-twitter-sentiment-data-95e5bd2869ec">website</a></figcaption></figure><p id="ea9a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">因此，论文“<a class="ae jy" href="https://dl.acm.org/citation.cfm?id=1620921" rel="noopener ugc nofollow" target="_blank">二次特征和分块的深度架构</a>”自 2009 年就已经存在，然而我最近在阅读“<a class="ae jy" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank">理解训练深度前馈神经网络</a>的困难”时发现了该论文中介绍的激活函数(我也就此事做了一篇论文总结，如果有人对<a class="ae jy" href="https://medium.com/@SeoJaeDuk/paper-summary-understanding-the-difficulty-of-training-deep-feed-forward-neural-networks-ee34f6447712" rel="noopener">感兴趣，请点击此处</a>)。)</p><p id="70ba" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">只是为了好玩，让我们比较许多不同的情况，看看哪个给我们最好的结果。</p><p id="35a8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> <em class="kx">案例 a) Tanh 激活函数</em> </strong> <a class="ae jy" href="https://medium.com/@SeoJaeDuk/implementation-of-optimization-for-deep-learning-highlights-in-2017-feat-sebastian-ruder-61e2cbe9b7cb" rel="noopener"> <strong class="kb ir"> <em class="kx">与 AMS Grad</em></strong></a><strong class="kb ir"><em class="kx"><br/>案例 b)软签激活函数</em> </strong> <a class="ae jy" href="https://medium.com/@SeoJaeDuk/implementation-of-optimization-for-deep-learning-highlights-in-2017-feat-sebastian-ruder-61e2cbe9b7cb" rel="noopener"> <strong class="kb ir"> <em class="kx">与 AMS Grad</em></strong></a><strong class="kb ir"><em class="kx"><br/>案例 c) ELU 激活函数</em> </strong> <a class="ae jy" href="https://medium.com/@SeoJaeDuk/implementation-of-optimization-for-deep-learning-highlights-in-2017-feat-sebastian-ruder-61e2cbe9b7cb" rel="noopener"> <strong class="kb ir"> <em class="kx">与 AMS Grad </em> </strong></a></p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><figure class="lf lg lh li gt jr"><div class="bz fp l di"><div class="lj lk l"/></div></figure></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="ee18" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">软标志激活功能</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ll"><img src="../Images/7dbdff260b675e65c9d02e9928671c7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_SZerd2U5bbFOBez0X8Leg.png"/></div></div></figure><p id="70be" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红线</strong> →软标志激活功能<br/> <strong class="kb ir">蓝线</strong> →双曲正切激活功能<br/> <strong class="kb ir">绿线</strong> →软标志功能的导数<br/> <strong class="kb ir">橙线</strong> →双曲正切激活功能的导数</p><p id="6eed" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，我们可以直接观察到软符号激活函数比双曲正切激活函数更平滑的事实。(具体来说，该函数名义上是多元增长，而不是指数增长。)并且这种更温和的非线性实际上导致更好更快的学习。在更详细的解释中，研究人员发现，柔和的手势可以防止神经元饱和，从而提高学习效率。(要了解更多信息，请阅读这篇<a class="ae jy" href="https://medium.com/@SeoJaeDuk/paper-summary-understanding-the-difficulty-of-training-deep-feed-forward-neural-networks-ee34f6447712" rel="noopener">博文。)</a></p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="2fcb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">网络架构/数据集</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lq"><img src="../Images/5caa7354bcff2b9bb6a5c8de2998ab65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X5dMtW_xjqHFUBxbHXmITg.png"/></div></div></figure><p id="6634" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红框</strong> →输入图像<br/>T5】黑框 →与不同激活函数的卷积运算<br/> <strong class="kb ir">橙框</strong> →分类的软 Max 运算</p><p id="0d3f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，我们将使用的基础网络架构是 ICLR 2015 中展示的<a class="ae jy" rel="noopener" target="_blank" href="/iclr-2015-striving-for-simplicity-the-all-convolutional-net-with-interactive-code-manual-b4976e206760">全卷积网络。最后，我们将用来评估我们网络的数据集是</a><a class="ae jy" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR 10 数据。</a></p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="7282" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:情况 a) Tanh 激活功能</strong> <a class="ae jy" href="https://medium.com/@SeoJaeDuk/implementation-of-optimization-for-deep-learning-highlights-in-2017-feat-sebastian-ruder-61e2cbe9b7cb" rel="noopener"> <strong class="kb ir">与 AMS Grad (CNN) </strong> </a></p><div class="lf lg lh li gt ab cb"><figure class="lr jr ls lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/0f362f3d412eda1aa2d180dc6c81e6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*YkL02i9MNJzzaMcCGZApUw.png"/></div></figure><figure class="lr jr ls lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/2323d7c1c23926a6e7fffeaa012bde36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*uR-f9KvrMRDnl77btTVHCw.png"/></div></figure></div><p id="0d77" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →一段时间内的训练精度/成本<br/> <strong class="kb ir">右图</strong> →一段时间内的测试精度/成本</p><p id="a1b4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，对所有卷积网络使用 tanh 的图像分类任务似乎需要更长的时间来收敛。在 21 世纪末期，该模型只能在训练图像上达到 69%的准确度(在测试图像上达到 68%的准确度)，然而该模型似乎在正则化方面做得很好。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lx"><img src="../Images/6fa19ceceb7006677930093d17530c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C7IuicRC0u8cUlFkq8lp6A.png"/></div></div></figure></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="4952" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:情况 b)软标志激活功能</strong> <a class="ae jy" href="https://medium.com/@SeoJaeDuk/implementation-of-optimization-for-deep-learning-highlights-in-2017-feat-sebastian-ruder-61e2cbe9b7cb" rel="noopener"> <strong class="kb ir">与 AMS Grad (CNN) </strong> </a></p><div class="lf lg lh li gt ab cb"><figure class="lr jr ls lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/d3779ab94e00427e66309503a49d6f11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*16M1m0ML-V5zYsO6UFkjDA.png"/></div></figure><figure class="lr jr ls lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/5e09bbc7c437806526232b93612564e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*BhZ3COZUuhZA3NaPsN1HSg.png"/></div></figure></div><p id="46dd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →一段时间内的训练精度/成本<br/> <strong class="kb ir">右图</strong> →一段时间内的测试精度/成本</p><p id="5fb6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我其实很惊讶，软标志激活功能的表现不如 tanh。花费更长的时间来收敛并不总是意味着激活函数不好，然而对于这种设置，软符号激活函数可能不是最佳选择。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ly"><img src="../Images/6dc1312ec35ed07860a50b823bc81535.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VtCOH-6kYyJyMXaZ4KSXPA.png"/></div></div></figure></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="583b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:情况 c) ELU 激活功能</strong> <a class="ae jy" href="https://medium.com/@SeoJaeDuk/implementation-of-optimization-for-deep-learning-highlights-in-2017-feat-sebastian-ruder-61e2cbe9b7cb" rel="noopener"> <strong class="kb ir">与 AMS Grad (CNN) </strong> </a></p><div class="lf lg lh li gt ab cb"><figure class="lr jr ls lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/884100fc78dfadb54676b27aea7e2d49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*R98mUs0_nUxZWBg4_Rbcnw.png"/></div></figure><figure class="lr jr ls lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/239a3c0d50117d8bd85ba8d418f2fcb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*K6YyNvH-tfvwn6rV-7ngdQ.png"/></div></figure></div><p id="96ef" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →一段时间内的训练精度/成本<br/> <strong class="kb ir">右图</strong> →一段时间内的测试精度/成本</p><p id="0e5c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于图像分类，传统的 relu 类激活似乎是最好的选择。因为它们不仅在训练/测试图像上具有最高的精度，而且收敛得更快。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lz"><img src="../Images/11662c8819d97ec258e268b6c0efff74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BYfksorDu2PgSjlimlRr1w.png"/></div></div></figure></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="404a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">交互代码</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ma"><img src="../Images/1b987e36c3c64e9f7335f21bced07948.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-DRC_r4tIUhR7rozVcFGFw.png"/></div></div></figure><p id="2b29" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于 Google Colab，你需要一个 Google 帐户来查看代码，而且你不能在 Google Colab 中运行只读脚本，所以在你的操场上复制一份。最后，我永远不会请求允许访问你在 Google Drive 上的文件，仅供参考。编码快乐！同样为了透明，我在 github 上上传了所有的训练日志。</p><p id="88a1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要访问<a class="ae jy" href="https://colab.research.google.com/drive/1QDfuDXl2LGYxwdjz6W1iHmsvzbXm4Snh" rel="noopener ugc nofollow" target="_blank">案例的代码，点击这里，<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/softsign/a1/casea.txt" rel="noopener ugc nofollow" target="_blank">日志的</a>点击这里。</a> <br/>点击此处查看<a class="ae jy" href="https://colab.research.google.com/drive/1zSfBmrTBH0sX4LOPXkn48lUC6dz3xXg6" rel="noopener ugc nofollow" target="_blank">案例 b 的代码</a>，点击此处查看<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/softsign/b1/caseb.txt" rel="noopener ugc nofollow" target="_blank">日志。</a> <br/>要访问<a class="ae jy" href="https://colab.research.google.com/drive/126dgOgW2wNbnxKEzdrDbifZB7JWbDU0x" rel="noopener ugc nofollow" target="_blank">案例 c 的代码，请点击此处</a>，要查看<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/softsign/c1/casec.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处。</a></p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="6040" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">遗言</strong></p><p id="a3b3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">另外，<a class="ae jy" rel="noopener" target="_blank" href="/deep-study-of-a-not-very-deep-neural-network-part-2-activation-functions-fd9bd8d406fc">这篇博文</a>在比较许多其他激活功能方面做得非常出色，所以如果有人感兴趣，请查看一下。</p><p id="dc76" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有写作的列表，请在这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的推特<a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>关注我，并访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文 pos </a> t。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="53b8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="d8a9" class="mb mc iq kb b kc kd kg kh kk md ko me ks mf kw mg mh mi mj bi translated">图瑞安、伯格斯特拉和本吉奥(2009 年)。组块的二次特征和深层结构。人类语言技术会议录:计算语言学协会北美分会 2009 年度会议，配套卷:短文，245–248。从<a class="ae jy" href="https://dl.acm.org/citation.cfm?id=1620921" rel="noopener ugc nofollow" target="_blank">https://dl.acm.org/citation.cfm?id=1620921</a>取回</li><li id="7b67" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">作为神经网络激活函数的软设计。(2017).赛菲克·伊尔金·塞伦吉尔。检索于 2018 年 5 月 28 日，来自<a class="ae jy" href="https://sefiks.com/2017/11/10/softsign-as-a-neural-networks-activation-function/" rel="noopener ugc nofollow" target="_blank">https://sefiks . com/2017/11/10/soft sign-as-a-neural-networks-activation-function/</a></li><li id="1205" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">对一个不很深的神经网络的深入研究。第 2 部分:激活函数。(2018).走向数据科学。2018 年 5 月 28 日检索，来自<a class="ae jy" rel="noopener" target="_blank" href="/deep-study-of-a-not-very-deep-neural-network-part-2-activation-functions-fd9bd8d406fc">https://towards data science . com/deep-study-of-a-not-very-deep-neural-network-part-2-activation-functions-FD 9 BD 8d 406 fc</a></li><li id="0922" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">(2018).Proceedings.mlr.press .检索于 2018 年 5 月 28 日，来自<a class="ae jy" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a></li><li id="3b11" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">【论文摘要】了解训练深度前馈神经网络的难度。(2018).中等。检索于 2018 年 5 月 28 日，来自<a class="ae jy" href="https://medium.com/@SeoJaeDuk/paper-summary-understanding-the-difficulty-of-training-deep-feed-forward-neural-networks-ee34f6447712" rel="noopener">https://medium . com/@ SeoJaeDuk/paper-summary-understanding-the-difference-of-training-deep-feed-forward-neural-networks-ee34f 6447712</a></li><li id="ccc1" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">导数双曲函数。(2018).Math2.org。检索于 2018 年 5 月 28 日，来自<a class="ae jy" href="http://math2.org/math/derivatives/more/hyperbolics.htm" rel="noopener ugc nofollow" target="_blank">http://math2.org/math/derivatives/more/hyperbolics.htm</a></li><li id="e0f8" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">2017 年深度学习优化实现亮点(feat。塞巴斯蒂安·鲁德)。(2018).中等。检索于 2018 年 5 月 28 日，来自<a class="ae jy" href="https://medium.com/@SeoJaeDuk/implementation-of-optimization-for-deep-learning-highlights-in-2017-feat-sebastian-ruder-61e2cbe9b7cb" rel="noopener">https://medium . com/@ SeoJaeDuk/implementation-of-optimization-for-deep-learning-highlights-in-2017-feat-sebastian-ruder-61e 2 CBE 9 b 7 CB</a></li><li id="8aca" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">Wolfram|Alpha:让世界知识可计算。(2018).Wolframalpha.com。检索于 2018 年 5 月 28 日，来自<a class="ae jy" href="http://www.wolframalpha.com/input/?i=f(x)+%3D+x%2F(1%2B%7Cx%7C)" rel="noopener ugc nofollow" target="_blank">http://www.wolframalpha.com/input/?I = f(x)+% 3D+x % 2F(1% 2B % 7Cx % 7C)</a></li><li id="723e" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">Numpy 向量(N，)。(2018).Numpy 向量(N，1)维-&gt; (N，)维转换。堆栈溢出。检索于 2018 年 5 月 29 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/17869840/numpy-vector-n-1-dimension-n-dimension-conversion" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/17869840/numpy-vector-n-1-dimension-n-dimension-conversion</a></li><li id="bb74" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">CIFAR-10 和 CIFAR-100 数据集。(2018).Cs.toronto.edu。检索于 2018 年 5 月 29 日，来自 https://www.cs.toronto.edu/~kriz/cifar.html<a class="ae jy" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"/></li><li id="6236" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">[ ICLR 2015 ]追求简单:具有交互码的全卷积网。(2018).走向数据科学。检索于 2018 年 5 月 29 日，来自<a class="ae jy" rel="noopener" target="_blank" href="/iclr-2015-striving-for-simplicity-the-all-convolutional-net-with-interactive-code-manual-b4976e206760">https://towards data science . com/iclr-2015-力争简单-所有卷积网与交互式代码手册-b4976e206760 </a></li><li id="7513" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">[谷歌]连续可微分指数线性单位与互动代码。(2018).中等。检索于 2018 年 5 月 29 日，来自<a class="ae jy" href="https://medium.com/@SeoJaeDuk/google-continuously-differentiable-exponential-linear-units-with-interactive-code-manual-back-2d0a56dd983f" rel="noopener">https://medium . com/@ SeoJaeDuk/Google-continuously-differentiable-index-linear-units-with-interactive-code-manual-back-2d 0a 56 DD 983 f</a></li></ol></div></div>    
</body>
</html>