<html>
<head>
<title>Review: DeepID-Net — Def-Pooling Layer (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习:DeepID-Net-Def-Pooling 层(对象检测)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=collection_archive---------3-----------------------#2018-09-28">https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=collection_archive---------3-----------------------#2018-09-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="5bf4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个故事里，<strong class="jp ir"> DeepID-Net </strong>简单回顾一下。介绍了一种基于可变形零件的细胞神经网络。一个新的<strong class="jp ir">可变形约束池(def-pooling)层</strong>被用于<strong class="jp ir">建模具有几何约束和惩罚的物体部分的变形。</strong></p><p id="1fa7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这意味着，除了直接检测整个对象之外，检测对象的部分也是至关重要的，然后这些部分可以帮助检测整个对象。是 ILSVRC 2014 中 <strong class="jp ir">任务</strong>的<strong class="jp ir">亚军。并在<strong class="jp ir">2015 CVPR</strong>【1】和<strong class="jp ir">2017 TPAMI</strong>【2】发表论文，共引用<strong class="jp ir">约 300 篇</strong>。(<a class="kl km ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----f72486f1a0f6--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</strong></p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi kn"><img src="../Images/418efcda59b208cf8e1275b1794b72b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dNerMhXXaxqeXKziGCCbrA.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk"><strong class="bd ld">DeepID-Net</strong></figcaption></figure><p id="58db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">黑色</strong>彩色的台阶实际上是 R-CNN 中存在的<strong class="jp ir">老东西</strong> <strong class="jp ir">。<strong class="jp ir">红色</strong>颜色的步骤实际上是<strong class="jp ir">没有出现在 R-CNN </strong>中。</strong></p><p id="c0c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我会提到上图中的每一步，并在故事的结尾给出结果。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="de93" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">步伐</h1><ol class=""><li id="f9a7" class="mj mk iq jp b jq ml ju mm jy mn kc mo kg mp kk mq mr ms mt bi translated"><strong class="jp ir">选择性搜索</strong></li><li id="7df3" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk mq mr ms mt bi translated"><strong class="jp ir">箱子拒收</strong></li><li id="1dda" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk mq mr ms mt bi translated"><strong class="jp ir">使用对象级注释进行预训练</strong></li><li id="6be6" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk mq mr ms mt bi translated"><strong class="jp ir">定义池层</strong></li><li id="f8e7" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk mq mr ms mt bi translated"><strong class="jp ir">情境建模</strong></li><li id="25bc" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk mq mr ms mt bi translated"><strong class="jp ir">模型平均</strong></li><li id="f134" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk mq mr ms mt bi translated"><strong class="jp ir">包围盒回归</strong></li></ol></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="2111" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak"> 1。选择性搜索</strong></h1><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="ab gu cl mz"><img src="../Images/10d848b6ff0f21d90e52dbba48b3e191.png" data-original-src="https://miro.medium.com/v2/format:webp/1*NXZoM83IKAM9NZzRTJk1jw.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk"><strong class="bd ld">Selective Search</strong></figcaption></figure><ol class=""><li id="ef85" class="mj mk iq jp b jq jr ju jv jy na kc nb kg nc kk mq mr ms mt bi translated">首先，颜色相似性、纹理相似性、区域大小和区域填充被用作<strong class="jp ir">非基于对象的分割</strong>。因此，我们得到了许多小的分割区域，如上图左下方所示。</li><li id="5dc9" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk mq mr ms mt bi translated">然后，使用自下而上的方法，将<strong class="jp ir">小的分割区域合并在一起，形成更大的分割区域。</strong></li><li id="3589" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk mq mr ms mt bi translated">因此，如图像所示，生成了大约 2K 个 <strong class="jp ir">区域提议(边界框候选)<strong class="jp ir">。</strong></strong></li></ol></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="a3bb" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">2.盒子拒绝</h1><p id="b799" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy nd ka kb kc ne ke kf kg nf ki kj kk ij bi translated">R-CNN 用于<strong class="jp ir">拒绝最有可能是背景的包围盒</strong>。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="5286" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">3.使用对象级注释进行预训练<strong class="ak"/></h1><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi ng"><img src="../Images/d4aa4ad5a7bb908e34f81d7bcf28c8f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0jYY-fySExV8B5mIT7_Qsg.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk"><strong class="bd ld">Object-Level Annotation (Left), Image-Level Annotation (Right)</strong></figcaption></figure><p id="9df5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通常，预训练在<strong class="jp ir">图像级注释</strong>上进行。当图像中的对象太小时<strong class="jp ir">是不好的，因为对象应该在选择性搜索创建的边界框中占据大的区域。</strong></p><p id="8738" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，<strong class="jp ir">预训练是在对象级注释</strong>上进行的。而<strong class="jp ir">深度学习模型可以是 ZFNet、VGGNet、GoogLeNet 等任何模型</strong>。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="574b" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak"> 4。Def-Pooling 层</strong></h1><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi nh"><img src="../Images/2da41fa056e6a15a782af5861f8744c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tedEyExBZLnGtFXb5eQfhA.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk"><strong class="bd ld">Overall Architecture with More Details</strong></figcaption></figure><p id="8224" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，我们使用 ZFNet，在 conv5 之后，输出将经过原始 FC 层 fc6 和 fc7，以及一组 conv 和建议的 def-pooling 层。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi ni"><img src="../Images/9d2b3d35c419eb36d26b50d713cf3a1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H5-MN7FN1Watvu_3KW_Viw.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk"><strong class="bd ld">Def-Pooling Layers (Deformable Constrained Pooling), High Activation Value for the Circle Center of Each Light</strong></figcaption></figure><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/a48700ac4aabf554333f409c57389e73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*8kgCCev4kbBYWfMO0s2jYQ.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk"><strong class="bd ld">Def-Pooling Equations</strong></figcaption></figure><p id="33a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于 def-pooling 路径，conv5 的输出经过 conv 层，然后经过 def-pooling 层，最后是 max pooling 层。</p><p id="b1ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简而言之，<strong class="jp ir">AC 乘以 dc，n 的总和就是上图中的 5×5 变形罚分</strong>。<strong class="jp ir">惩罚是从假定的锚位置放置目标部分的惩罚。</strong></p><p id="1ae9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">def-pooling 层<strong class="jp ir">学习具有不同大小和语义含义的对象部分的变形。</strong></p><p id="9a80" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过训练该 def-pooling 层，要检测的对象的对象部分将在 def-pooling 层之后给出高的激活值，如果它们靠近它们的锚位置的话。并且这个输出将连接到 200 级分数以进行改进。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="5db3" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">5.<strong class="ak">情境建模</strong></h1><p id="3d52" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy nd ka kb kc ne ke kf kg nf ki kj kk ij bi translated">在 ILSVRC 的目标检测任务中，只有 200 个类。并且在 ILSVRC 中还有一个分类竞争任务，用于对 1000 类对象进行分类和定位。与目标检测任务相比，内容更加多样化。因此，<strong class="jp ir">通过分类网络获得的 1000 类分数被用于细化 200 类分数。</strong></p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="6ec0" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">6.<strong class="ak">模型平均</strong></h1><p id="f752" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy nd ka kb kc ne ke kf kg nf ki kj kk ij bi translated"><strong class="jp ir">使用多个模型</strong>来提高精确度，并且<strong class="jp ir">对所有模型的结果进行平均</strong>。这种技术从 LeNet、AlexNet 等开始使用。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="9998" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">7.<strong class="ak">包围盒回归</strong></h1><p id="8bee" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy nd ka kb kc ne ke kf kg nf ki kj kk ij bi translated">包围盒回归只是为了<strong class="jp ir">微调包围盒位置</strong>，在 R-CNN 中已经使用。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="45ad" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">结果</h1><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi nk"><img src="../Images/9fc70c0b3329ad39f18270320409c2f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PCyhF4KHdlG8_T37JkOUwg.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk"><strong class="bd ld">Incremental Results</strong></figcaption></figure><ul class=""><li id="fbc7" class="mj mk iq jp b jq jr ju jv jy na kc nb kg nc kk nl mr ms mt bi translated">具有选择性搜索的 R-CNN(步骤 1): 29.9% mAP(平均平均预测)</li><li id="7996" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk nl mr ms mt bi translated">+ <strong class="jp ir">包围盒拒绝</strong>(第二步):30.9%</li><li id="17bc" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk nl mr ms mt bi translated">从 AlexNet 改为 ZFNet(第三步):31.8%</li><li id="010a" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk nl mr ms mt bi translated">从 ZFNet 更改为 VGGNet(步骤 3): 36.6%</li><li id="89c3" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk nl mr ms mt bi translated">从 VGGNet 更改为 GoogLeNet(第三步):37.8%</li><li id="9396" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk nl mr ms mt bi translated">+ <strong class="jp ir">对象级标注的预处理</strong>(第三步):40.4%</li><li id="cc83" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk nl mr ms mt bi translated">+边缘有更多的边界框建议来自[参考文件 60]: 42.7%</li><li id="12ab" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk nl mr ms mt bi translated">+ <strong class="jp ir"> Def-Pooling 层</strong>(第 4 步):44.9%</li><li id="4ecc" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk nl mr ms mt bi translated">+建议在 VGGNet 进行多尺度培训:47.3%</li><li id="6c30" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk nl mr ms mt bi translated">+ <strong class="jp ir">情境建模</strong>(第五步):47.8%</li><li id="827e" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk nl mr ms mt bi translated">+边界框回归(第 7 步):48.2%</li><li id="9626" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk nl mr ms mt bi translated">+模型平均(第六步):<strong class="jp ir"> 50.7%！</strong></li></ul><p id="43bf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与多模型多作物 GoogLeNet 相比，DeepID-Net 的地图高出 6.1%。然而，正如我们所看到的，有些投稿实际上来自其他论文。然而，有两个最新颖的想法是<strong class="jp ir"/><strong class="jp ir">对对象级注释进行预处理，以及定义池层。</strong></p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="9358" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">参考</h1><ol class=""><li id="a367" class="mj mk iq jp b jq ml ju mm jy mn kc mo kg mp kk mq mr ms mt bi translated">【2015 CVPR】【DeepID-Net】<br/><a class="ae nm" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ouyang_DeepID-Net_Deformable_Deep_2015_CVPR_paper.pdf" rel="noopener ugc nofollow" target="_blank">DeepID-Net:用于物体检测的可变形深度卷积神经网络</a></li><li id="586d" class="mj mk iq jp b jq mu ju mv jy mw kc mx kg my kk mq mr ms mt bi translated">【2017 TPAMI】【DeepID-Net】<br/><a class="ae nm" href="https://ieeexplore.ieee.org/document/7298854" rel="noopener ugc nofollow" target="_blank">DeepID-Net:用于物体检测的可变形深度卷积神经网络</a></li></ol><h1 id="b95c" class="ll lm iq bd ln lo nn lq lr ls no lu lv lw np ly lz ma nq mc md me nr mg mh mi bi translated">我的评论</h1><p id="dd14" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy nd ka kb kc ne ke kf kg nf ki kj kk ij bi translated">[<a class="ae nm" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a>][<a class="ae nm" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener">AlexNet</a>][<a class="ae nm" href="https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103" rel="noopener">ZFNet</a>][<a class="ae nm" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener">VGGNet</a>][<a class="ae nm" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener">Google net</a>]</p></div></div>    
</body>
</html>