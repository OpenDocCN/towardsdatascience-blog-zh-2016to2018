<html>
<head>
<title>Bias -Variance &amp; Precision-Recall Trade-offs: How to aim for the sweet spot.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">偏差-方差和精确-回忆权衡:如何瞄准最佳点？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tradeoffs-how-to-aim-for-the-sweet-spot-c20b40d5e6b6?source=collection_archive---------6-----------------------#2018-12-21">https://towardsdatascience.com/tradeoffs-how-to-aim-for-the-sweet-spot-c20b40d5e6b6?source=collection_archive---------6-----------------------#2018-12-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="50b7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何在<strong class="ak">偏差-方差&amp;精确-召回权衡</strong>中找到甜蜜点？理解在此过程中起主要作用的所有重要参数。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/7031cc298eee4d14ad914df3cb9de113.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*6iLcvaGfD10XRFD2faxatw.png"/></div></figure><p id="6415" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">权衡是介于理智但无聊的生活和复杂、冒险但冒险的生活之间的一件事。生活中的每一点，甚至每一秒我们都做出某种‘取舍’。冒险与否的权衡总是帮助我们找到最佳平衡点或中间地带。当我们让机器像人类一样思考时，它们也受到“权衡”的困扰。</p><p id="49c0" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">机器学习通常需要处理两个权衡，</p><ol class=""><li id="5880" class="lj lk iq kp b kq kr kt ku kw ll la lm le ln li lo lp lq lr bi translated">偏差-方差权衡</li><li id="7393" class="lj lk iq kp b kq ls kt lt kw lu la lv le lw li lo lp lq lr bi translated">精确召回的权衡</li></ol></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="1ec2" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">第 1 部分:偏差-方差权衡</h1><h2 id="c291" class="mw mf iq bd mg mx my dn mk mz na dp mo kw nb nc mq la nd ne ms le nf ng mu nh bi translated">1.1 首先，什么是偏差，什么是方差？</h2><h2 id="99d8" class="mw mf iq bd mg mx my dn mk mz na dp mo kw nb nc mq la nd ne ms le nf ng mu nh bi translated">偏置:</h2><p id="b975" class="pw-post-body-paragraph kn ko iq kp b kq ni jr ks kt nj ju kv kw nk ky kz la nl lc ld le nm lg lh li ij bi translated">要理解它，就要知道它的大概意思。剑桥词典称，</p><blockquote class="nn no np"><p id="b165" class="kn ko nq kp b kq kr jr ks kt ku ju kv nr kx ky kz ns lb lc ld nt lf lg lh li ij bi translated">以不公平的方式支持或反对特定的人或事的行为，因为允许个人观点影响你的判断。</p></blockquote><p id="c70c" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→所以在统计学的世界里，它被定义为，</p><blockquote class="nn no np"><p id="dedc" class="kn ko nq kp b kq kr jr ks kt ku ju kv nr kx ky kz ns lb lc ld nt lf lg lh li ij bi translated">统计偏差是统计技术或其结果的一个特征，由此结果的期望值不同于被估计的真实的潜在定量参数。</p></blockquote><p id="de82" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">受够了“书生气”的定义，让我们通过与现实世界更相关的类比来理解它。</p><p id="eddb" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→用简单的英语来说，“机器学习技术无法捕捉真实关系是<strong class="kp ir">偏见”。</strong></p><ul class=""><li id="a827" class="lj lk iq kp b kq kr kt ku kw ll la lm le ln li nu lp lq lr bi translated"><strong class="kp ir">低偏差</strong>:预测数据点接近目标。此外，该模型对目标函数的形式提出了更少的假设。</li><li id="f3ad" class="lj lk iq kp b kq ls kt lt kw lu la lv le lw li nu lp lq lr bi translated"><strong class="kp ir">高偏差</strong>:预测数据点远离目标。此外，该模型对目标函数的形式提出了更多的假设。</li><li id="2a59" class="lj lk iq kp b kq ls kt lt kw lu la lv le lw li nu lp lq lr bi translated"><strong class="kp ir">低偏机器学习算法的例子:</strong>决策树、k 近邻、支持向量机。</li><li id="c45e" class="lj lk iq kp b kq ls kt lt kw lu la lv le lw li nu lp lq lr bi translated"><strong class="kp ir">高偏差机器学习算法的例子:</strong>线性回归，线性判别分析，逻辑回归。</li></ul><p id="4145" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→因此，理想情况下，我们的主要目标是整体低偏置(但不总是如此)。具有高偏差的模型很少关注训练数据，并且过度简化了模型。它总是导致训练和测试数据的高误差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/55db01aae291f5bb58bd024b17b15e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*rOd_56UwyAvNnpf5CMvjnQ.jpeg"/></div></figure><h2 id="1dbc" class="mw mf iq bd mg mx my dn mk mz na dp mo kw nb nc mq la nd ne ms le nf ng mu nh bi translated">1.1.2 差异:</h2><p id="ccbe" class="pw-post-body-paragraph kn ko iq kp b kq ni jr ks kt nj ju kv kw nk ky kz la nl lc ld le nm lg lh li ij bi translated">再次要理解它，我们必须知道它的大致意思。剑桥词典称，</p><blockquote class="nn no np"><p id="0a06" class="kn ko nq kp b kq kr jr ks kt ku ju kv nr kx ky kz ns lb lc ld nt lf lg lh li ij bi translated">两个或多个事物不同的事实，或它们不同的数量或数目。</p></blockquote><p id="6eb3" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→所以在统计学的世界里，它被定义为，</p><blockquote class="nn no np"><p id="bd85" class="kn ko nq kp b kq kr jr ks kt ku ju kv nr kx ky kz ns lb lc ld nt lf lg lh li ij bi translated">在概率论和统计学中，<strong class="kp ir">方差</strong>是随机变量与其均值的方差平方的期望值。通俗地说，它测量一组(随机)数从它们的平均值分散开多远。</p></blockquote><p id="8763" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这又是一个“书生气”的定义。因此，让我们通过它在现实世界中的类比来弄清楚这个想法。</p><p id="2d8f" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→用简单的英语来说，“它是给定数据点或数值的模型预测的可变性，它告诉我们数据的分布”。这里的数据分布就是数据点和平均值之差的平方(即偏差的平方，σ)</p><ul class=""><li id="10aa" class="lj lk iq kp b kq kr kt ku kw ll la lm le ln li nu lp lq lr bi translated"><strong class="kp ir">低方差</strong>:数据点相互接近，结果接近函数。此外，该模型建议随着训练数据集的改变而对目标函数的估计进行小的改变。</li><li id="bbff" class="lj lk iq kp b kq ls kt lt kw lu la lv le lw li nu lp lq lr bi translated"><strong class="kp ir">高方差</strong>:数据点分散，结果远离函数。建议随着训练数据集的改变而对目标函数的估计进行大的改变。</li><li id="4062" class="lj lk iq kp b kq ls kt lt kw lu la lv le lw li nu lp lq lr bi translated"><strong class="kp ir">低方差机器学习算法的例子:</strong>线性回归、线性判别分析、逻辑回归。</li><li id="bf44" class="lj lk iq kp b kq ls kt lt kw lu la lv le lw li nu lp lq lr bi translated"><strong class="kp ir">高方差机器学习算法的例子:</strong>决策树、k 近邻、支持向量机。</li></ul><p id="004c" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→因此，理想情况下，我们的总体目标是低偏置。(但并不总是)。高方差模型非常重视训练数据，不会对以前没有见过的数据进行归纳。结果，这样的模型在训练数据上表现得非常好，但是在测试数据上有很高的错误率。</p><h2 id="5f50" class="mw mf iq bd mg mx my dn mk mz na dp mo kw nb nc mq la nd ne ms le nf ng mu nh bi translated">1.2.到目前为止，一切似乎都很好，那么偏差-方差权衡的结果是什么呢？</h2><p id="a156" class="pw-post-body-paragraph kn ko iq kp b kq ni jr ks kt nj ju kv kw nk ky kz la nl lc ld le nm lg lh li ij bi translated">→要理解这一点，首先我们必须理解误差，因为它们或多或少只与误差有关。不管使用什么算法，都不能减少不可约误差&amp;从问题的选择框架中引入。由影响输入变量到输出变量的映射的未知变量等因素引起。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl nw"><img src="../Images/657d9d9018ad745d7bd6a86b438154ed.png" data-original-src="https://miro.medium.com/v2/format:webp/1*e7VaoBh5apjaM2p4afkFyg.png"/></div></figure><p id="fb32" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→因此，为了最小化误差，我们必须最小化偏差和方差。</p><p id="e48b" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→任何有监督的机器学习算法的目标都是实现低偏差和低方差。反过来，该算法应该实现良好的预测性能。你可以在上面的例子中看到一个总的趋势:</p><ul class=""><li id="76e7" class="lj lk iq kp b kq kr kt ku kw ll la lm le ln li nu lp lq lr bi translated">参数或线性机器学习算法通常具有高偏差但低方差。</li><li id="16e6" class="lj lk iq kp b kq ls kt lt kw lu la lv le lw li nu lp lq lr bi translated">非参数或非线性机器学习算法通常具有低偏差但高方差。</li></ul><blockquote class="nn no np"><p id="858f" class="kn ko nq kp b kq kr jr ks kt ku ju kv nr kx ky kz ns lb lc ld nt lf lg lh li ij bi translated">机器学习算法的参数化通常是一场平衡偏差和方差的战斗。</p></blockquote><p id="014f" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→在机器学习中，偏差和方差之间的关系是不可避免的。</p><ul class=""><li id="4738" class="lj lk iq kp b kq kr kt ku kw ll la lm le ln li nu lp lq lr bi translated">增加偏差会减少方差。</li><li id="c700" class="lj lk iq kp b kq ls kt lt kw lu la lv le lw li nu lp lq lr bi translated">增加方差将减少偏差。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/3254738e690177349af051527c823a96.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/0*YzZ5PH2qqk3Yg-6a"/></div><figcaption class="ny nz gj gh gi oa ob bd b be z dk"><em class="oc">Bias-Variance Trade-off as a Function of Model Capacity</em></figcaption></figure><p id="0611" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→ <em class="nq">从上图我们可以解读如下:</em></p><p id="c3f7" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">I .高偏差和低方差使数据欠拟合，因为它错过了许多假设。此外，点数与预期预测值有偏差。</p><p id="fb7a" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">二。低偏差和高方差过度拟合数据，因为模型过于关注训练数据，不能很好地概括。</p><p id="7bea" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">三。当两者都找到中间点或最佳点时，误差最小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/818c66acb4353ab6621d6683de0afa95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*3OQwDzmNGLben3Js"/></div></figure><p id="eeec" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">左上是我们的目标。</p><p id="9e83" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">二。左下具有高偏差，因此它们远离目标，但由于低方差而彼此接近。</p><p id="1924" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">三。右上点由于高方差而展开，但是由于低偏差而接近目标。</p><p id="087e" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">四。右下方离目标很远，而且点数本身也是因为高方差和高偏差。</p><p id="b556" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→为了建立一个好的模型，我们需要在偏差和方差之间找到一个好的平衡，这样可以使总误差最小化。偏差和方差的最佳平衡不会使模型过拟合或欠拟合。这是通过在反复试验的基础上进行超参数调整来实现的</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="8f97" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">第 2 部分:精确召回权衡</h1><h2 id="a7e0" class="mw mf iq bd mg mx my dn mk mz na dp mo kw nb nc mq la nd ne ms le nf ng mu nh bi translated">2.1.首先，什么是精确，什么是回忆，或者其他术语？</h2><p id="c04e" class="pw-post-body-paragraph kn ko iq kp b kq ni jr ks kt nj ju kv kw nk ky kz la nl lc ld le nm lg lh li ij bi translated">它们是<em class="nq">分类问题</em>的性能矩阵。我认为<em class="nq">分类</em>的每一个概念都可以用一个例子来解释。</p><p id="3118" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir">例子:</strong>假设你正在考虑给可能会再次光顾的顾客额外赠送一块方糖。但是你当然想避免不必要的分发方糖，所以你只把方糖给那些模型显示至少有 30%可能会再次光顾的顾客。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/7c1c3a1c05f2e84751219e3dfb7a0dec.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*7ittNFbbbYjxIFeALk051A.png"/></div><figcaption class="ny nz gj gh gi oa ob bd b be z dk">Confusion Matrix</figcaption></figure><ul class=""><li id="26de" class="lj lk iq kp b kq kr kt ku kw ll la lm le ln li nu lp lq lr bi translated">TP:被模型分类为<em class="nq">的将返回</em>，而实际上<em class="nq">已经返回了</em></li><li id="a659" class="lj lk iq kp b kq ls kt lt kw lu la lv le lw li nu lp lq lr bi translated">FP:被模型归类为<em class="nq">将返回</em>，但实际上<em class="nq">没有返回</em> <em class="nq">(类型 1 错误或告警)</em></li><li id="0fa7" class="lj lk iq kp b kq ls kt lt kw lu la lv le lw li nu lp lq lr bi translated">TN:被模型归类为<em class="nq">不会返回</em>，实际上<em class="nq">并没有返回</em></li><li id="ba17" class="lj lk iq kp b kq ls kt lt kw lu la lv le lw li nu lp lq lr bi translated">FN:被模型归类为<em class="nq">不会返回</em>，但实际上<em class="nq">已经返回</em>。<em class="nq">(类型 2 错误或遗漏)</em></li></ul><p id="6b33" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><em class="nq">(注:我是假设读者了解‘混淆矩阵’，如果不了解，请先过一遍。)</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl nw"><img src="../Images/06efcd4bf164a7abad98e3035fcc7f51.png" data-original-src="https://miro.medium.com/v2/format:webp/1*WlTKk0RmvgcjWs2ikxG0fQ.jpeg"/></div><figcaption class="ny nz gj gh gi oa ob bd b be z dk">Venn Dig for Precision &amp; Recall</figcaption></figure><p id="a3ca" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> 2.1.1 精度:</strong></p><blockquote class="nn no np"><p id="cbf9" class="kn ko nq kp b kq kr jr ks kt ku ju kv nr kx ky kz ns lb lc ld nt lf lg lh li ij bi translated">检索到的实例中相关实例的比例。</p></blockquote><p id="65dd" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→计算如下:TP / (TP + FP)</p><p id="655a" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→从上面的例子，我们可以解读为，“<em class="nq">被归类为会回归的，实际做了的比例是多少？”</em></p><p id="40e2" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→因此，精度表示我们的模型所说的相关数据点的比例实际上是相关的。</p><p id="fe17" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> 2.1.2 召回(又称灵敏度):</strong></p><blockquote class="nn no np"><p id="6ff3" class="kn ko nq kp b kq kr jr ks kt ku ju kv nr kx ky kz ns lb lc ld nt lf lg lh li ij bi translated">已检索的相关实例占相关实例总数的比例。</p></blockquote><p id="3578" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→计算如下:TP /( TP + FN)</p><p id="0ae1" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→从上面的例子中，我们可以将它解释为，“<em class="nq">在那些实际返回的人中，有多少比例是这样分类的？”</em></p><p id="e02e" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→一般来说，灵敏度告诉我们<em class="nq"/><strong class="kp ir"><em class="nq"/></strong><em class="nq"/><strong class="kp ir"><em class="nq">目标</em> </strong> <em class="nq">被正确识别的百分比。</em>也称为“真阳性率”，可互换使用。在正确识别阳性的基础上选择模型时很有用，在这种情况下，就是给额外的方糖。</p><p id="e77f" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→因此，回忆表示在数据集中找到所有相关实例的能力</p><p id="aa28" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><em class="nq">在继续之前，我们应该了解一些其他有用的术语。</em></p><p id="8ba0" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> 2.1.3 特异性(又称假阴性率):</strong></p><p id="f5e3" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> → </strong> <em class="nq">被正确识别的</em> <strong class="kp ir"> <em class="nq">阴性靶</em> </strong> <em class="nq">的百分比。</em>在正确识别负面因素的基础上选择模型时很有用，在这种情况下，不会给出额外的方糖。</p><p id="f787" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→计算为:TN/(TN + FP)</p><p id="0163" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">2 . 1 . 4 F1-分数:</p><p id="093a" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→精确度和召回率的最佳结合我们可以使用所谓的<a class="ae of" href="https://en.wikipedia.org/wiki/F1_score" rel="noopener ugc nofollow" target="_blank"> F1 分数</a>来组合这两个指标。</p><p id="2104" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→F1 分数是精确度和召回率的调和平均值，在以下等式中考虑了这两个指标:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/2f260ddc7f32bdd82cd5fad7daea11f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*UJxVqLnbSj42eRhasKeLOA.png"/></div></figure><p id="b719" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→我们使用<a class="ae of" href="https://stackoverflow.com/questions/26355942/why-is-the-f-measure-a-harmonic-mean-and-not-an-arithmetic-mean-of-the-precision" rel="noopener ugc nofollow" target="_blank">调和平均值而不是简单平均值，因为它会惩罚极值</a>。精度为 1.0、召回率为 0.0 的分类器的简单平均值为 0.5，但 f 1 值为 0。</p><p id="1415" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> 2.2 回到最初的问题，精确召回权衡还是精确<em class="nq"> vs </em>召回？</strong></p><p id="f541" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→在任何一个模型中，您也可以决定强调精确度或召回率。也许你很缺方糖，所以你只想把方糖给那些你很有信心会再次光顾的人，所以你决定只把方糖给那些有 60%可能会再次光顾的顾客(而不是 30%)。</p><p id="647a" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们的精确度会提高，因为只有当你确信有人会回来的时候，你才会分发方糖。我们的召回率将会下降，因为将会有很多人最终会退回那些你没有足够信心给他们一块方糖的人。</p><p id="407a" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">精度:62% → 80% <br/>召回:60% → 30%</p><p id="87b5" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→或者，如果你觉得方糖很丰富，你可以把它们分发给任何一个至少有 10%机会再次购买的人。</p><p id="8feb" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">精确度:62% → 40% <br/>召回率:60% → 90%</p><p id="37f5" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">您可以使用此图表来跟踪精确度和召回率之间的权衡:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/07394daa4a19ed373a42e65520f3cacf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/0*lo1WwfnMRBAWZgDh.png"/></div></figure><p id="7060" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">→更好的模型具有更高的精度和召回值。</p><ul class=""><li id="4a34" class="lj lk iq kp b kq kr kt ku kw ll la lm le ln li nu lp lq lr bi translated">你可以想象一个模型有 94%的准确率(几乎所有被标识为<em class="nq">的都将返回</em> do 事实上)和 97%的召回率(几乎所有<em class="nq">返回</em>的都被标识为这样)。</li><li id="fc1c" class="lj lk iq kp b kq ls kt lt kw lu la lv le lw li nu lp lq lr bi translated">一个较弱的模型可能有 95%的精度，但 50%的召回率(当它识别某人为<em class="nq">将返回</em>，这在很大程度上是正确的，但它错误地标记为<em class="nq">不会返回</em>，而事实上是那些后来返回<em class="nq">的人中的一半返回</em>)。</li><li id="aaab" class="lj lk iq kp b kq ls kt lt kw lu la lv le lw li nu lp lq lr bi translated">或者这个模型有 60%的准确率和 60%的召回率</li></ul><p id="7579" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这些数字会让你对你的模型有多精确有一个很好的感觉，即使你实际上从来不想做任何预测。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="e695" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated"><strong class="ak"> <em class="oc">包装完毕:</em> </strong></h1><p id="27e9" class="pw-post-body-paragraph kn ko iq kp b kq ni jr ks kt nj ju kv kw nk ky kz la nl lc ld le nm lg lh li ij bi translated"><em class="nq">就像现实世界中的权衡决策是基于当前环境做出的一样，这两种权衡也是如此。一天结束时，我们所要做的就是根据我们的需求做出判断。</em></p></div></div>    
</body>
</html>