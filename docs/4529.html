<html>
<head>
<title>Mapping My Facebook Data — Part 1: Simple NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">绘制我的脸书数据—第 1 部分:简单的 NLP</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mapping-my-facebook-data-part-1-simple-nlp-98ce41f7f27d?source=collection_archive---------13-----------------------#2018-08-21">https://towardsdatascience.com/mapping-my-facebook-data-part-1-simple-nlp-98ce41f7f27d?source=collection_archive---------13-----------------------#2018-08-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="f85c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">NLP 看看我的写作风格(以及如何处理你自己的脸书数据！)</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/8a93dc14dbdac03113fc3a12a39239dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CASptphU0s3PufgZCN3VDA.jpeg"/></div></div></figure></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="fca5" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">介绍</h1><p id="d9e9" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">不久前，我被自己一天生成的文本数据量深深吸引住了。如果你像我一样，你可能会写很多东西。电子邮件。短信。脸书。也许你还有其他一些创造性的出路。也许你写日记或写音乐什么的。也许你是个学生，有一些写作作业。对我来说，我真的很想深入了解我正在生成的所有这些数据，并认为使用自然语言处理来分析这些数据会很酷。</p><p id="3d0d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个系列将记录我是如何做的，如果你感兴趣的话，你也可以做同样的事情。</p><h1 id="b96c" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">查找我的数据</h1><p id="fd41" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">当我仔细思考所有的数据时，我决定关注几个来源。他们是:</p><ul class=""><li id="27b5" class="mm mn iq jp b jq jr ju jv jy mo kc mp kg mq kk mr ms mt mu bi translated">我的书面学校作业</li><li id="f8a4" class="mm mn iq jp b jq mv ju mw jy mx kc my kg mz kk mr ms mt mu bi translated">我写的日记</li><li id="0d99" class="mm mn iq jp b jq mv ju mw jy mx kc my kg mz kk mr ms mt mu bi translated">我写的歌曲集</li><li id="8402" class="mm mn iq jp b jq mv ju mw jy mx kc my kg mz kk mr ms mt mu bi translated">我的脸书数据(我的评论、帖子和聊天)</li></ul><p id="01fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然对于我的整个项目，我使用了所有的资料来源，但对于这个系列，我只打算使用我在脸书的数据。</p><p id="04f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于大多数数据，我只是把它放在一个文本文件中，然后就收工了。对于脸书的数据，我必须做更多的预处理。</p><h2 id="4f99" class="na lf iq bd lg nb nc dn lk nd ne dp lo jy nf ng ls kc nh ni lw kg nj nk ma nl bi translated">如何获得你的脸书数据</h2><p id="0141" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">我们如何真正得到我们的脸书数据？实际上，这比你想象的要简单。截至目前(2018 年 8 月 20 日)，您可以通过以下方式下载数据:</p><ul class=""><li id="6664" class="mm mn iq jp b jq jr ju jv jy mo kc mp kg mq kk mr ms mt mu bi translated">登录脸书</li><li id="e01f" class="mm mn iq jp b jq mv ju mw jy mx kc my kg mz kk mr ms mt mu bi translated">点击右上角朝下的三角形</li><li id="6dbf" class="mm mn iq jp b jq mv ju mw jy mx kc my kg mz kk mr ms mt mu bi translated">单击设置</li><li id="440b" class="mm mn iq jp b jq mv ju mw jy mx kc my kg mz kk mr ms mt mu bi translated">在左上角，第三个选项是“你的脸书信息”点击这个</li><li id="6057" class="mm mn iq jp b jq mv ju mw jy mx kc my kg mz kk mr ms mt mu bi translated">下一个菜单中的选项#2 将是“下载您的信息”</li><li id="27d4" class="mm mn iq jp b jq mv ju mw jy mx kc my kg mz kk mr ms mt mu bi translated">从这里，您可以决定想要什么数据，什么时间段，什么格式</li></ul><p id="bbfd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我选择下载所有的东西，而且是 JSON 格式的。当我的下载准备好了，我得到了一堆这样的文件夹:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/18c77854f8386c53185ba1dbf9063ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*wQ7U7z3w2DbrIEDc_fm-tQ.png"/></div></figure><p id="1eb1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">装满了我要求的 JSON。</p><h2 id="ab79" class="na lf iq bd lg nb nc dn lk nd ne dp lo jy nf ng ls kc nh ni lw kg nj nk ma nl bi translated">预处理您的脸书数据</h2><p id="6abc" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">我想下载我所有的脸书数据，但实际上我并不想要这个项目的所有脸书数据。对于这个项目，我只关心我的帖子、评论和聊天记录。为了帮助解决这个问题，我为其中的每一个都编写了一个预处理脚本，以便将我想要的内容转储到文本文件中。</p><p id="55d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我处理了我的信息:</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="b93b" class="na lf iq no b gy ns nt l nu nv">import os<br/>import json<br/>import datetime<br/><br/>chat_root = 'messages'<br/>writing_dir = 'data/facebook_chat'<br/><br/>for f in os.listdir(chat_root):<br/>    file = os.path.join(chat_root, f, 'message.json')<br/>    data = json.load(open(file, 'r'))<br/><br/>    for msg in data['messages']:<br/>        try:<br/>            if msg['sender_name'] == 'Hunter Heidenreich':<br/>                if msg['content'] != "You are now connected on Messenger.":<br/>                    content = msg['content']<br/>                    ts = datetime.datetime.fromtimestamp(msg['timestamp_ms'] / 1000)<br/><br/>                    filename = os.path.join(writing_dir,<br/>                                            str(ts.year) + '.' + str(ts.month) + '.' + str(ts.day) + '.txt')<br/><br/>                    with open(filename, 'a+') as out_file:<br/>                        out_file.write(content + '\n')<br/>                        print('wrote.')<br/>        except KeyError:<br/>            pass</span></pre><p id="6d11" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您将看到，我正在遍历 messages 文件夹中的所有子文件夹。我在那里做的是读取消息 JSON。对于每条可用的消息，我都会检查它是否是我发送的消息。如果不是脸书默认的“您现在已在 Messenger 上连接”那我要了。我给消息打上时间戳，然后将它添加到一个采用<code class="fe nw nx ny no b">year.month.day.txt</code>格式的文件中，这是我给所有文本文件打上时间戳的格式，这样我就可以记录词汇随时间的变化。</p><p id="4172" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果由于某种原因 JSON 的键不起作用，我就忽略它。</p><p id="b6ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我对我写的两个帖子做了非常相似的事情:</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="0721" class="na lf iq no b gy ns nt l nu nv">import os<br/>import datetime<br/>import json<br/><br/>in_data = 'posts/your_posts.json'<br/>writing_dir = 'data/facebook_posts'<br/><br/>data = json.load(open(in_data, 'r'))<br/><br/>for post in data['status_updates']:<br/>    try:<br/>        ts = datetime.datetime.fromtimestamp(post['timestamp'])<br/>        post_text = post['data'][0]['post']<br/><br/>        filename = os.path.join(writing_dir, str(ts.year) + '.' + str(ts.month) + '.' + str(ts.day) + '.txt')<br/><br/>        with open(filename, 'a+') as out_file:<br/>            out_file.write(post_text + '\n')<br/>    except KeyError:<br/>        print(post)</span></pre><p id="d2a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我的评论是:</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="5b8b" class="na lf iq no b gy ns nt l nu nv">import os<br/>import datetime<br/>import json<br/><br/>comment_path = 'comments/comments.json'<br/>writing_dir = 'data/facebook_comments'<br/><br/>data = json.load(open(comment_path, 'r'))<br/>for comment in data['comments']:<br/>    try:<br/>        for d in comment['data']:<br/>            if d['comment']['author'] == "Hunter Heidenreich":<br/>                ts = datetime.datetime.fromtimestamp(d['comment']['timestamp'])<br/>                com = d['comment']['comment']<br/><br/>                filename = os.path.join(writing_dir, str(ts.year) + '.' + str(ts.month) + '.' + str(ts.day) + '.txt')<br/><br/>                with open(filename, 'a+') as out_file:<br/>                    out_file.write(com + '\n')<br/>    except KeyError:<br/>        print(comment)</span></pre><p id="da8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从那里，我准备好了我的脸书数据。</p><h1 id="51d4" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">加载我们的数据</h1><p id="a5a8" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">首先，我们将编写一个简单的函数来获取某个类别中所有文件的列表。这将使我们能够轻松地跟踪哪个是哪个，并且我们将在处理和分析数据时保留这些命名方案。</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="36ee" class="na lf iq no b gy ns nt l nu nv">import os<br/><br/><br/>def get_file_list(text_dir):<br/>    files = []<br/>    for f in os.listdir(text_dir):<br/>        files.append(os.path.join(text_dir, f))<br/>    return files<br/><br/><br/>comments = 'data/facebook_comments'<br/>posts = 'data/facebook_posts'<br/>chats = 'data/facebook_chat'<br/><br/>comment_files = get_file_list(comments)<br/>post_files = get_file_list(posts)<br/>chat_files = get_file_list(chats)<br/><br/>all_files = comment_files + post_files + chat_files</span></pre><p id="dae6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们实际读入数据之前，我们将编写一个函数，用几种不同的方式对数据进行预处理。</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="719c" class="na lf iq no b gy ns nt l nu nv">import string<br/><br/>import nltk<br/>from nltk.corpus import stopwords<br/>from nltk.stem import SnowballStemmer<br/>from nltk.stem import WordNetLemmatizer<br/><br/><br/>def preprocess_text(lines):<br/><br/>    def _flatten_list(two_list):<br/>        one_list = []<br/>        for el in two_list:<br/>          one_list.extend(el)<br/>        return one_list<br/><br/>    translator = str.maketrans('', '', string.punctuation)<br/>    upd = []<br/>    for line in lines:<br/>        upd.extend(nltk.sent_tokenize(line))<br/>    lines = [line.translate(translator) for line in upd]<br/>    lines = [nltk.word_tokenize(line) for line in lines]<br/>    lines = [[word.lower() for word in line if word not in [<br/>        '\'', '’', '”', '“']] for line in lines]<br/><br/>    raw = lines<br/><br/>    stop = [[word for word in line if word not in set(<br/>        stopwords.words('english'))] for line in raw]<br/><br/>    snowball_stemmer = SnowballStemmer('english')<br/>    stem = [[snowball_stemmer.stem(word) for word in line] for line in stop]<br/><br/>    wordnet_lemmatizer = WordNetLemmatizer()<br/>    lemma = [[wordnet_lemmatizer.lemmatize(<br/>        word) for word in line] for line in stop]<br/><br/>    raw = _flatten_list(raw)<br/>    stop = _flatten_list(stop)<br/>    stem = _flatten_list(stem)<br/>    lemma = _flatten_list(lemma)<br/><br/>    return raw, stop, stem, lemma</span></pre><p id="ff96" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们在这里所做的是产生我们文本的 4 个变体。我们正在生产:</p><ul class=""><li id="ed31" class="mm mn iq jp b jq jr ju jv jy mo kc mp kg mq kk mr ms mt mu bi translated">我们的原始数据去掉了标点符号，用了小写字母</li><li id="b67c" class="mm mn iq jp b jq mv ju mw jy mx kc my kg mz kk mr ms mt mu bi translated">删除了停用词的数据</li><li id="d6be" class="mm mn iq jp b jq mv ju mw jy mx kc my kg mz kk mr ms mt mu bi translated">我们的数据来源于</li><li id="e3bd" class="mm mn iq jp b jq mv ju mw jy mx kc my kg mz kk mr ms mt mu bi translated">我们的数据被假设了</li></ul><p id="1b22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑到这一点，我们现在可以创建一个基本的对象来保存我们的文件数据数据，并允许使用聚合来自脸书的不同来源的写作发生在同一天:</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="8e73" class="na lf iq no b gy ns nt l nu nv">class FileObject:<br/>    def __init__(self):<br/>        self._datetime = None<br/><br/>        self._lines = []<br/><br/>        self._raw = []<br/>        self._stop = []<br/>        self._stem = []<br/>        self._lemma = []<br/><br/>    @property<br/>    def lines(self):<br/>        return self._lines<br/><br/>    def preprocess_text(self):<br/>        self._raw, self._stop, self._stem, self._lemma = preprocess_text(<br/>            self._lines)</span></pre><p id="6031" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们加载数据并对其进行预处理。我将演示聚合数据上的代码，但它也适用于其他输入文件列表:</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="12e9" class="na lf iq no b gy ns nt l nu nv">all_text = {}<br/><br/>for f in all_files:<br/>    base = os.path.basename(f)<br/><br/>    if base not in all_text:<br/>        all_text[base] = FileObject()<br/><br/>    with open(f, 'r') as in_file:<br/>        all_text[base]._lines += in_file.readlines()<br/><br/>for k, v in all_text.items():<br/>    v.preprocess_text()</span></pre><p id="dfd3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这可能需要很短的时间，但当我们完成后，我们将能够开始看看关于我们的文本的一些基本的东西！</p><h1 id="4a47" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">我最喜欢的词是什么？</h1><p id="cf9d" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">让我们从最基本的开始。我们把这些单词列表加载到各种资源中。让我们做一个统计，看看我们最常用的词是什么。让我们看看我们的 20 强。</p><p id="e279" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以这样写:</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="584d" class="na lf iq no b gy ns nt l nu nv">from collections import Counter<br/><br/>def file_objects_to_words(fo_text):<br/>    raw, stop, stem, lemma = [], [], [], []<br/>    for key, value in fo_text.items():<br/>        raw.append(value._raw)<br/>        stop.append(value._stop)<br/>        stem.append(value._stem)<br/>        lemma.append(value._lemma)<br/>    return raw, stop, stem, lemma<br/><br/><br/>def top_20_words(list_of_word_lists):<br/>    full_count = Counter()<br/><br/>    for word_list in list_of_word_lists:<br/>        for word in word_list:<br/>            full_count[word] += 1<br/><br/>    return full_count.most_common(20)<br/><br/>raw, stop, stem, lemma = file_objects_to_words(all_text)<br/>print('All words -- raw')<br/>print(top_20_words(raw))<br/>print('All words -- stop')<br/>print(top_20_words(stop))<br/>print('All words -- stem')<br/>print(top_20_words(stem))<br/>print('All words -- lemma')<br/>print(top_20_words(lemma))<br/><br/>print('Chat words -- lemma')<br/>_, _, _, lemma = file_objects_to_words(chat_text)<br/>print(top_20_words(lemma))<br/>print('Post words -- lemma')<br/>_, _, _, lemma = file_objects_to_words(post_text)<br/>print(top_20_words(lemma))<br/>print('Comment words -- lemma')<br/>_, _, _, lemma = file_objects_to_words(comment_text)<br/>print(top_20_words(lemma))</span></pre><p id="ddef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将得到一个很好的输出:</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="53c3" class="na lf iq no b gy ns nt l nu nv">All words -- raw<br/>[('i', 47935), ('you', 43131), ('to', 24551), ('and', 20882), ('the', 18681), ('that', 16725), ('a', 15727), ('it', 15140), ('haha', 13516), ('me', 12597), ('like', 10149), ('so', 9945), ('im', 9816), ('but', 8942), ('do', 8599), ('is', 8319), ('of', 8266), ('just', 8154), ('be', 8092), ('my', 8017)]<br/>All words -- stop<br/>[('haha', 13516), ('like', 10149), ('im', 9816), ('na', 6528), ('yeah', 5970), ('dont', 5361), ('okay', 5332), ('good', 5253), ('know', 4896), ('would', 4702), ('think', 4555), ('thats', 4163), ('want', 4148), ('cause', 3922), ('really', 3803), ('get', 3683), ('wan', 3660), ('hahaha', 3518), ('well', 3332), ('feel', 3231)]<br/>All words -- stem<br/>[('haha', 13516), ('like', 10580), ('im', 9816), ('na', 6528), ('yeah', 5970), ('think', 5772), ('want', 5396), ('dont', 5364), ('okay', 5333), ('good', 5261), ('know', 5078), ('would', 4702), ('get', 4535), ('feel', 4270), ('that', 4163), ('caus', 3992), ('go', 3960), ('realli', 3803), ('make', 3727), ('wan', 3660)]<br/>All words -- lemma<br/>[('haha', 13516), ('like', 10207), ('im', 9816), ('na', 6530), ('yeah', 5970), ('dont', 5361), ('okay', 5333), ('good', 5254), ('know', 5038), ('would', 4702), ('think', 4631), ('want', 4419), ('thats', 4163), ('cause', 3934), ('get', 3901), ('really', 3803), ('wan', 3660), ('hahaha', 3518), ('feel', 3358), ('well', 3332)]<br/>Chat words -- lemma<br/>[('haha', 13464), ('like', 10111), ('im', 9716), ('na', 6497), ('yeah', 5957), ('okay', 5329), ('dont', 5316), ('good', 5226), ('know', 4995), ('would', 4657), ('think', 4595), ('want', 4384), ('thats', 4150), ('cause', 3913), ('get', 3859), ('really', 3760), ('wan', 3649), ('hahaha', 3501), ('feel', 3336), ('well', 3299)]<br/>Post words -- lemma<br/>[('game', 68), ('video', 41), ('check', 38), ('like', 36), ('birthday', 36), ('happy', 34), ('im', 33), ('noah', 29), ('song', 28), ('play', 24), ('music', 21), ('one', 20), ('get', 19), ('guy', 18), ('time', 18), ('made', 18), ('evans', 18), ('hey', 17), ('make', 17), ('people', 16)]<br/>Comment words -- lemma<br/>[('noah', 120), ('im', 67), ('like', 60), ('evans', 60), ('thanks', 51), ('game', 50), ('haha', 47), ('cote', 44), ('one', 43), ('coby', 40), ('wilcox', 38), ('would', 33), ('man', 31), ('dont', 29), ('really', 29), ('know', 28), ('think', 28), ('na', 24), ('want', 23), ('get', 23)]</span></pre><p id="a2ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我喜欢只看我的词条，所以这就是为什么我只记录我个人的来源。我觉得有趣的是，我在聊天中经常使用“哈哈”的变体。我的大部分评论都是某人的名字。</p><h1 id="fe4d" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">我的单个单词用法是什么样的？</h1><p id="d028" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">那么，如果我们想用图表来表示我们的单个单词，看看我们的用法是如何从顶部单词到底部单词衰减的呢？我们可以编写一个一般的条形图函数，如下所示:</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="e3a3" class="na lf iq no b gy ns nt l nu nv">from matplotlib import pyplot as plt<br/><br/><br/>def plot_bar_graph(x, y, x_label='X', y_label='Y', title='Title', export=False, export_name='default.png'):<br/>    plt.bar(x, y)<br/>    plt.xlabel(x_label)<br/>    plt.ylabel(y_label)<br/>    plt.title(title)<br/>    plt.xlim(0, len(x))<br/>    plt.ylim(0, max(y))<br/><br/>    if export:<br/>        plt.savefig(export_name)<br/><br/>    plt.show()</span></pre><p id="ed5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从那里，我们可以修改我们的前 20 个函数，并可以将我们的注释引理列表直接输入到图形中:</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="ac8d" class="na lf iq no b gy ns nt l nu nv">def aggregate_words_counts(list_of_word_lists):<br/>    full_count = Counter()<br/><br/>    for word_list in list_of_word_lists:<br/>        for word in word_list:<br/>            full_count[word] += 1<br/><br/>    return full_count<br/><br/>counter = aggregate_words_counts(lemma)<br/>word_counts_raw = list(counter.values())<br/>word_counts_sorted = sorted(word_counts_raw)<br/><br/>cap = len(word_counts_raw)<br/><br/>plot_bar_graph(range(len(word_counts_raw[:cap])), word_counts_raw[:cap],<br/>                      x_label='Words', y_label='Counts', title='Word Frequencies (Raw)',<br/>                      export=True, export_name='visualizations/word_freq_bar_raw.png')<br/>plot_bar_graph(range(len(word_counts_sorted[:cap])), word_counts_sorted[-cap:],<br/>                      x_label='Words', y_label='Counts', title='Word Frequencies (Sorted)',<br/>                      export=True, export_name='visualizations/word_freq_bar_sorted.png')</span></pre><p id="f649" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们得到了两张非常好的图表:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/9aae78f359ce5829e2c0ae81e9c0a8b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*hiuh00BzfE88RlS8DYLt1A.png"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/d287d78e748c1e2bf237f625c9033ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*2FvhfCGOTelk-lq517knWg.png"/></div></figure><h1 id="12d9" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">我的数据有哪些基本统计数据？</h1><p id="77fb" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">让我们根据数据生成一些基本的统计数据。让我们设置一个函数来创建一个表:</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="82b1" class="na lf iq no b gy ns nt l nu nv">def plot_table(cell_data, row_labels=None, col_labels=None, export=False, export_name='default.png'):<br/>    _ = plt.figure(figsize=(6, 1))<br/><br/>    _ = plt.table(cellText=cell_data, rowLabels=row_labels,<br/>                  colLabels=col_labels, loc='center')<br/><br/>    plt.axis('off')<br/>    plt.grid(False)<br/><br/>    if export:<br/>        plt.savefig(export_name, bbox_inches='tight')<br/><br/>    plt.show()</span></pre><p id="a205" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后生成要转储到该函数中的数据:</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="8b48" class="na lf iq no b gy ns nt l nu nv">import numpy as np<br/><br/><br/>def top_k_words(list_of_word_lists, k=10):<br/>    full_count = Counter()<br/><br/>    for word_list in list_of_word_lists:<br/>        for word in word_list:<br/>            full_count[word] += 1<br/><br/>    return full_count.most_common(k)<br/><br/>raw, stop, stem, lemma = file_objects_to_words(all_text)<br/><br/>counter = aggregate_words_counts(lemma)<br/>data = [[len(all_files)],<br/>        [top_k_words(lemma, k=1)[0][0] + ' (' + str(top_k_words(lemma, k=1)[0][1]) + ')'],<br/>        [top_k_words(lemma, k=len(list(counter.keys())))[-1][0] + ' (' + str(top_k_words(lemma, k=len(list(counter.keys())))[-1][1]) + ')'],<br/>        [len(counter.items())],<br/>        [np.mean(list(counter.values()))],<br/>        [sum([len(word_list) for word_list in raw])],<br/>        [sum([sum([len(w) for w in word_list]) for word_list in raw])]]<br/>row_labels = ['Collection size: ', 'Top word: ', 'Least common: ', 'Vocab size: ', 'Average word usage count: ',<br/>              'Total words: ', 'Total characters: ']<br/><br/>plot_table(cell_data=data, row_labels=row_labels,<br/>                  export=True, export_name='visualizations/basic_stats_table.png')</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/79160c4ade99c5b8502c8864504ff4c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*co6bxrF5NSrMPlimndy9tQ.png"/></div></figure><p id="7df7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些只是我认为有趣的一些数据。这次我把所有的数据都放了进去，因为我觉得这是最有趣的。</p><p id="7516" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，我在脸书有 2147 天的文本活动。</p><p id="ef76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我最喜欢的词是“哈哈”(这并不奇怪)。</p><p id="24bc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总共 19，508 个单词</p><p id="778d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我用了将近 400 万个字符。</p><p id="0ecb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是大量的文本数据！</p><h1 id="c03c" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">随着时间的推移，我的 Vocab 使用情况如何？</h1><p id="224c" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">我想知道随着时间的推移，我的 vocab 使用情况如何变化。我们怎样才能产生这种效果呢？好吧，幸运的是我们给所有的文件都打上了时间戳！</p><p id="e003" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，让我们创建我们的绘图函数:</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="e43b" class="na lf iq no b gy ns nt l nu nv">def plot(x, y, x_label='X', y_label='Y', title='Title', export=False, export_name='default.png'):<br/>    plt.plot(x, y)<br/>    plt.xlabel(x_label)<br/>    plt.ylabel(y_label)<br/>    plt.title(title)<br/><br/>    if export:<br/>        plt.savefig(export_name)<br/><br/>    plt.show()</span></pre><p id="534c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们写一些函数来绘制我们的单词使用情况:</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="b354" class="na lf iq no b gy ns nt l nu nv">import datetime<br/><br/><br/>def get_datetimes(list_of_files):<br/>    base_files = [os.path.basename(f) for f in list_of_files]<br/>    no_ext = [os.path.splitext(f)[0] for f in base_files]<br/>    splits = [f.split('.') for f in no_ext]<br/>    times = np.array(<br/>        [datetime.datetime(int(t[0]), int(t[1]), int(t[2])) for t in splits])<br/>    return times<br/><br/>def unique_vocab(word_list):<br/>    cnt = Counter()<br/>    for word in word_list:<br/>        cnt[word] += 1<br/>    return cnt<br/><br/>raws = []<br/>names = []<br/>for key, value in all_text.items():<br/>    raws.append(value._raw)<br/>    names.append(key)<br/><br/>raw_wc = [len(word_list) for word_list in raws]<br/>labels = get_datetimes(names)<br/><br/>labels, raw_wc = zip(*sorted(zip(labels, raw_wc)))<br/><br/>plot(labels, raw_wc,<br/>            x_label='Date', y_label='Word Count', title='Word Count Over Time',<br/>            export=True, export_name='visualizations/word_count_by_time.png')<br/><br/>raw_wc_u = [len(list(unique_vocab(word_list).items())) for word_list in raws]<br/>plot(labels, raw_wc_u,<br/>            x_label='Date', y_label='Word Count', title='Unique Word Count Over Time',<br/>            export=True, export_name='visualizations/unique_word_count_by_time.png')</span></pre><p id="324a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们开始吧:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/aaea798b61a8470bae55e0a873532588.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*7n98r4w-WjkrxVBSmztGuw.png"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/44f80c99a3783e0f54ef77833ec6175a.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*miWLxnKLHK3kKFiYZYxzyA.png"/></div></figure><p id="a836" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我觉得真的很有意思，2013 年年中，我用了很多词。我不太确定我在做什么，但是当你把它分解成独特的词时，在我那天使用的 20，000 个词中，没有多少是非常独特的…</p><p id="2be6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更不用说，在 2017 年后，你肯定可以看到我的脸书使用量下降。</p><p id="e3ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我觉得这真的很酷！</p><h1 id="fb8f" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">包扎</h1><p id="42ac" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">我们做到了！对我们的一些脸书数据的基本分析。希望你从你的脸书数据中学到了一两个技巧，也许还有一些关于你自己的东西！我知道当我开始分析我自己的时候，我确实是这样想的。如果你有很酷的视觉效果或者你想分享的东西，给我留言吧！很好奇看看别人在自己的数据里发现了什么。</p><p id="f20e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一次，我想我们会尝试对我们的脸书数据进行一些情绪分析，看看我们是否能从中找到任何有趣的花絮。</p><p id="5050" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你喜欢这篇文章，或者发现它在任何方面都有帮助，如果你给我一两美元来资助我的机器学习教育和研究，我会永远爱你！每一美元都让我离成功更近一步，我永远心存感激。</p><p id="47f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请继续关注更多的脸书 NLP 分析！</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="a152" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最初发布于 hunterheidenreich.com 的<a class="ae ob" href="http://hunterheidenreich.com/blog/map-my-writing-part-1/" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>