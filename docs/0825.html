<html>
<head>
<title>Autoencoders — Introduction and Implementation in TF.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动编码器-TF中的介绍和实现。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/autoencoders-introduction-and-implementation-3f40483b0a85?source=collection_archive---------2-----------------------#2017-06-26">https://towardsdatascience.com/autoencoders-introduction-and-implementation-3f40483b0a85?source=collection_archive---------2-----------------------#2017-06-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><h1 id="7023" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">简介和概念:</h1><p id="b412" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><em class="lm">自动编码器(AE) </em>是一个神经网络家族，其输入<strong class="kq iu">与输出</strong>相同(它们实现一个身份函数)。他们的工作原理是将输入压缩成一个<em class="lm">潜在空间表示</em>，然后从这个表示中重建输出。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/e20371366abaecd0632dad7d40afad92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LSYNW5m3TN7xRX61BZhoZA.png"/></div></div></figure><p id="2791" class="pw-post-body-paragraph ko kp it kq b kr lz kt ku kv ma kx ky kz mb lb lc ld mc lf lg lh md lj lk ll im bi translated">自动编码器真正流行的用途是将它们应用于图像。<strong class="kq iu">的诀窍</strong>是用<em class="lm">卷积</em>层代替<em class="lm">全连接</em>层。这些图层与合并图层一起，将输入从宽<strong class="kq iu">宽</strong>和薄<strong class="kq iu">宽</strong>(假设100 x 100像素，3个通道— RGB)转换为窄宽<strong class="kq iu">宽</strong>。这有助于网络从图像中提取<strong class="kq iu">视觉特征</strong>，从而获得更加准确的潜在空间表示。重建过程使用<em class="lm">上采样</em>和卷积。</p><p id="5d68" class="pw-post-body-paragraph ko kp it kq b kr lz kt ku kv ma kx ky kz mb lb lc ld mc lf lg lh md lj lk ll im bi translated">由此产生的网络被称为<em class="lm">卷积自动编码器</em> ( <em class="lm"> CAE </em>)。</p><h2 id="7e6a" class="me jr it bd js mf mg dn jw mh mi dp ka kz mj mk ke ld ml mm ki lh mn mo km mp bi translated">CAEs的使用</h2><h2 id="0725" class="me jr it bd js mf mg dn jw mh mi dp ka kz mj mk ke ld ml mm ki lh mn mo km mp bi translated">示例:超基本图像重建</h2><p id="6b8b" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">卷积自动编码器可以用于重建。例如，他们可以学习从图片中去除噪声，或者重建丢失的部分。</p><p id="2d03" class="pw-post-body-paragraph ko kp it kq b kr lz kt ku kv ma kx ky kz mb lb lc ld mc lf lg lh md lj lk ll im bi translated">为此，我们不使用相同的图像作为输入和输出，而是使用<strong class="kq iu">噪声版本作为输入</strong>和<strong class="kq iu">干净版本作为输出</strong>。通过这个过程，网络学会填补图像中的空白。</p><p id="f466" class="pw-post-body-paragraph ko kp it kq b kr lz kt ku kv ma kx ky kz mb lb lc ld mc lf lg lh md lj lk ll im bi translated">让我们看看CAE能做什么来替换眼睛图像的一部分。<em class="lm">假设有一个十字准线，我们想移除它</em>。我们可以手动创建数据集，这非常方便。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mr"><img src="../Images/483b2d5994a29a08369bcb1ae021303d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PX5e64QQw-RZOmG9EWWsVA.png"/></div></div></figure><p id="5c8c" class="pw-post-body-paragraph ko kp it kq b kr lz kt ku kv ma kx ky kz mb lb lc ld mc lf lg lh md lj lk ll im bi translated">现在我们的自动编码器已经训练好了，我们可以用它来移除我们从未见过的眼睛图片上的十字准线！</p><h1 id="ed8f" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">TF中的实现:</h1><p id="0693" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">让我们来看一个在tensorflow中使用MNIST数据集的示例实现。</p><p id="aede" class="pw-post-body-paragraph ko kp it kq b kr lz kt ku kv ma kx ky kz mb lb lc ld mc lf lg lh md lj lk ll im bi translated">笔记本:<a class="ae mq" href="https://github.com/mchablani/deep-learning/blob/master/autoencoder/Convolutional_Autoencoder.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/mchablani/deep-learning/blob/master/auto encoder/convolatile _ auto encoder . ipynb</a></p><h2 id="f67c" class="me jr it bd js mf mg dn jw mh mi dp ka kz mj mk ke ld ml mm ki lh mn mo km mp bi translated">网络体系结构</h2><p id="1ef2" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">网络的编码器部分将是典型的卷积金字塔。每个卷积层后面都有一个最大池层，以减少各层的维数。解码器需要从窄的表示转换成宽的重建图像。</p><p id="6f79" class="pw-post-body-paragraph ko kp it kq b kr lz kt ku kv ma kx ky kz mb lb lc ld mc lf lg lh md lj lk ll im bi translated">通常，你会看到<strong class="kq iu">转置卷积</strong>层用于增加层的宽度和高度。它们的工作原理与卷积层几乎完全相同，但方向相反。输入层中的步幅导致转置卷积层中的步幅更大。例如，如果您有一个3x3内核，输入层中的一个3x3补丁将减少到卷积层中的一个单元。相比之下，输入层中的一个单元将在转置卷积层中扩展为3×3路径。TensorFlow API为我们提供了一种创建层的简单方法。</p><blockquote class="mw mx my"><p id="938c" class="ko kp lm kq b kr lz kt ku kv ma kx ky mz mb lb lc na mc lf lg nb md lj lk ll im bi translated">然而，转置卷积层会导致最终图像中出现伪影，例如棋盘图案。这是由于内核重叠造成的，可以通过将步幅和内核大小设置为相等来避免这种重叠。在<a class="ae mq" href="http://distill.pub/2016/deconv-checkerboard/" rel="noopener ugc nofollow" target="_blank">这篇摘自Augustus Odena，<em class="it">等人</em>的文章</a>中，作者展示了通过使用最近邻或双线性插值(上采样)后接卷积层来调整层的大小，可以避免这些棋盘状伪像。在TensorFlow中，这很容易用<code class="fe ms mt mu mv b"><a class="ae mq" href="https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/image/resize_images" rel="noopener ugc nofollow" target="_blank">tf.image.resize_images</a></code>完成，然后是卷积。Odena等人声称最近邻插值最适合上采样</p></blockquote><p id="26ba" class="pw-post-body-paragraph ko kp it kq b kr lz kt ku kv ma kx ky kz mb lb lc ld mc lf lg lh md lj lk ll im bi translated">自动编码器仅通过在有噪声的图像上训练网络，就可以非常成功地用于图像去噪。我们可以通过向训练图像添加高斯噪声，然后将值剪切到0和1之间，来自己创建有噪声的图像。我们将使用噪声图像作为输入，原始的、干净的图像作为目标。</p><blockquote class="mw mx my"><p id="29d4" class="ko kp lm kq b kr lz kt ku kv ma kx ky mz mb lb lc na mc lf lg nb md lj lk ll im bi translated">注意，我们使用sigmoid _ cross _ entropy _ with _ logit来表示损失。根据TF文档:它测量离散分类任务中的概率误差，其中每个类都是独立的，并且不互相排斥。例如，可以执行多标签分类，其中一幅图片可以同时包含一头大象和一只狗。</p></blockquote><p id="57ac" class="pw-post-body-paragraph ko kp it kq b kr lz kt ku kv ma kx ky kz mb lb lc ld mc lf lg lh md lj lk ll im bi translated">模型定义:</p><pre class="lo lp lq lr gt nc mv nd ne aw nf bi"><span id="e7cd" class="me jr it mv b gy ng nh l ni nj">learning_rate = 0.001<br/>inputs_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs')<br/>targets_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets')</span><span id="1bca" class="me jr it mv b gy nk nh l ni nj">### Encoder<br/>conv1 = tf.layers.conv2d(inputs=inputs_, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br/># Now 28x28x32<br/>maxpool1 = tf.layers.max_pooling2d(conv1, pool_size=(2,2), strides=(2,2), padding='same')<br/># Now 14x14x32<br/>conv2 = tf.layers.conv2d(inputs=maxpool1, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br/># Now 14x14x32<br/>maxpool2 = tf.layers.max_pooling2d(conv2, pool_size=(2,2), strides=(2,2), padding='same')<br/># Now 7x7x32<br/>conv3 = tf.layers.conv2d(inputs=maxpool2, filters=16, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br/># Now 7x7x16<br/>encoded = tf.layers.max_pooling2d(conv3, pool_size=(2,2), strides=(2,2), padding='same')<br/># Now 4x4x16</span><span id="14ea" class="me jr it mv b gy nk nh l ni nj">### Decoder<br/>upsample1 = tf.image.resize_images(encoded, size=(7,7), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br/># Now 7x7x16<br/>conv4 = tf.layers.conv2d(inputs=upsample1, filters=16, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br/># Now 7x7x16<br/>upsample2 = tf.image.resize_images(conv4, size=(14,14), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br/># Now 14x14x16<br/>conv5 = tf.layers.conv2d(inputs=upsample2, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br/># Now 14x14x32<br/>upsample3 = tf.image.resize_images(conv5, size=(28,28), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br/># Now 28x28x32<br/>conv6 = tf.layers.conv2d(inputs=upsample3, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br/># Now 28x28x32</span><span id="c708" class="me jr it mv b gy nk nh l ni nj">logits = tf.layers.conv2d(inputs=conv6, filters=1, kernel_size=(3,3), padding='same', activation=None)<br/>#Now 28x28x1</span><span id="887b" class="me jr it mv b gy nk nh l ni nj"># Pass logits through sigmoid to get reconstructed image<br/>decoded = tf.nn.sigmoid(logits)</span><span id="6f1c" class="me jr it mv b gy nk nh l ni nj"># Pass logits through sigmoid and calculate the cross-entropy loss<br/>loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)</span><span id="847a" class="me jr it mv b gy nk nh l ni nj"># Get cost and define the optimizer<br/>cost = tf.reduce_mean(loss)<br/>opt = tf.train.AdamOptimizer(learning_rate).minimize(cost)</span></pre><p id="f6aa" class="pw-post-body-paragraph ko kp it kq b kr lz kt ku kv ma kx ky kz mb lb lc ld mc lf lg lh md lj lk ll im bi translated">培训:</p><pre class="lo lp lq lr gt nc mv nd ne aw nf bi"><span id="f0a0" class="me jr it mv b gy ng nh l ni nj">sess = tf.Session()<br/>epochs = 100<br/>batch_size = 200<br/># Set's how much noise we're adding to the MNIST images<br/>noise_factor = 0.5<br/>sess.run(tf.global_variables_initializer())<br/>for e in range(epochs):<br/>    for ii in range(mnist.train.num_examples//batch_size):<br/>        batch = mnist.train.next_batch(batch_size)<br/>        # Get images from the batch<br/>        imgs = batch[0].reshape((-1, 28, 28, 1))<br/>        <br/>        # Add random noise to the input images<br/>        noisy_imgs = imgs + noise_factor * np.random.randn(*imgs.shape)<br/>        # Clip the images to be between 0 and 1<br/>        noisy_imgs = np.clip(noisy_imgs, 0., 1.)<br/>        <br/>        # Noisy images as inputs, original images as targets<br/>        batch_cost, _ = sess.run([cost, opt], feed_dict={inputs_: noisy_imgs,<br/>                                                         targets_: imgs})</span><span id="4513" class="me jr it mv b gy nk nh l ni nj">print("Epoch: {}/{}...".format(e+1, epochs),<br/>              "Training loss: {:.4f}".format(batch_cost))</span></pre><p id="a2b2" class="pw-post-body-paragraph ko kp it kq b kr lz kt ku kv ma kx ky kz mb lb lc ld mc lf lg lh md lj lk ll im bi translated">学分:<a class="ae mq" href="https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694" rel="noopener ugc nofollow" target="_blank">https://hacker noon . com/auto encoders-deep-learning-bits-1-11731 e 200694</a></p></div></div>    
</body>
</html>