<html>
<head>
<title>Understanding a TensorFlow program in simple steps.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用简单的步骤理解张量流程序。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-fundamentals-of-tensorflow-program-and-why-it-is-necessary-94cf5b60e255?source=collection_archive---------4-----------------------#2017-09-26">https://towardsdatascience.com/understanding-fundamentals-of-tensorflow-program-and-why-it-is-necessary-94cf5b60e255?source=collection_archive---------4-----------------------#2017-09-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="a4a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated">ensorFlow是一个库，可以应用于所有的机器学习算法，尤其是用神经网络进行深度学习。机器学习就是那些被编写来处理大型数据集以从中发现模式并提取信息的程序。它实际上从数据中学习，以做出准确或接近准确的预测，并更好地自我修正。就像我们希望有人做的那样。图像识别及其增强甚至识别是ML的一些应用。</p><p id="ca43" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自从我在TF上写了第一个故事后，我很高兴看到了可喜的变化。这个故事着重于理解TensorFlow程序的一些基础知识。我建议在你的机器上试试这段代码，看看它是如何工作的。</p><p id="5bd7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了能够在不同的问题集上运行相同的模型，我们需要占位符和提要词典。随着我们的张量流程序变得越来越复杂，我们的可视化也需要跟上。回归试图对因果关系进行建模。原因是发生的独立变量，结果取决于原因。线性回归是一种直线回归，它是在对X导致y的回归进行建模时产生的。原因也称为解释变量</p><p id="0b42" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">财富假说增加了预期寿命。</p><p id="ed84" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于方程Y=A+BX。为了找到最佳拟合，ML算法将为A和b提供初始值。它将运行回归并找出A和b的这些值的误差。然后将这些误差反馈到输入中以获得A和b的新值。线性回归包括找到最佳拟合线</p><p id="87de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过最小化最小平方误差来找到最佳拟合线。最佳拟合线是从底部连接到图中数据点的线的长度的平方和最小的线。</p><h2 id="c051" class="ku kv iq bd kw kx ky dn kz la lb dp lc jy ld le lf kc lg lh li kg lj lk ll lm bi translated">占位符</h2><p id="beab" class="pw-post-body-paragraph jn jo iq jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj kk ij bi translated">TensorFlow中的占位符类似于变量，您可以使用tf.placeholder来声明它。您不必提供初始值，您可以在运行时使用Session.run中的feed_dict参数来指定它，而在tf。变量，可以在声明它时提供初始值。</p><p id="7bfb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用占位符的示例程序</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="d09a" class="ku kv iq lx b gy mb mc l md me">import tensorflow as tf</span><span id="31b9" class="ku kv iq lx b gy mf mc l md me">#setup placeholder using tf.placeholder<br/>x = tf.placeholder(tf.int32, shape=[3],name='x')<br/>'''it is of type integer and it has shape 3 meaning it is a 1D vector with 3 elements in it<br/>we name it x. just create another placeholder y with same dimension. we treat the <br/>placeholders like we treate constants. '''<br/>y = tf.placeholder(tf.int32, shape=[3],name='y')</span><span id="38bb" class="ku kv iq lx b gy mf mc l md me">sum_x = tf.reduce_sum(x,name="sum_x")<br/>prod_y = tf.reduce_prod(y,name="prod_y")<br/>'''we dont know what values x and y holds till we run the graph'''<br/>final_div = tf.div(sum_x,prod_y, nwe give fetches and feed_dict pass into every session.run commandame="final_div")</span><span id="847b" class="ku kv iq lx b gy mf mc l md me">final_mean = tf.reduce_mean([sum_x, prod_y], name="final_mean")</span><span id="3627" class="ku kv iq lx b gy mf mc l md me">sess = tf.Session()</span><span id="cb51" class="ku kv iq lx b gy mf mc l md me">print ("sum(x): ", sess.run(sum_x, feed_dict={x: [100,200,300]}))<br/>print ("prod(y): ", sess.run(prod_y, feed_dict={y: [1,2,3]}))</span><span id="3ab8" class="ku kv iq lx b gy mf mc l md me">writer = tf.summary.FileWriter('./tensorflow_example',sess.graph)</span><span id="65bf" class="ku kv iq lx b gy mf mc l md me">writer.close()<br/>sess.close()</span></pre><h2 id="838b" class="ku kv iq bd kw kx ky dn kz la lb dp lc jy ld le lf kc lg lh li kg lj lk ll lm bi translated">取送字典</h2><p id="9685" class="pw-post-body-paragraph jn jo iq jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj kk ij bi translated">我们将fetches和feed_dict传递给每个session.run命令。获取参数指示我们想要计算什么，而提要字典为该计算指定占位符值</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="be66" class="ku kv iq lx b gy mb mc l md me">import tensorflow as tf</span><span id="eb67" class="ku kv iq lx b gy mf mc l md me">W = tf.constant([10,100], name='const_W')</span><span id="a3ba" class="ku kv iq lx b gy mf mc l md me">#these placeholders can hold tensors of any shape<br/>#we will feed these placeholders later<br/>x = tf.placeholder(tf.int32, name='x')<br/>b = tf.placeholder(tf.int32,name='b')</span><span id="9fed" class="ku kv iq lx b gy mf mc l md me">#tf.multiply is simple multiplication and not matrix<br/>Wx = tf.multiply(W,x, name="Wx")<br/>y = tf.add(Wx,b,name='y')</span><span id="d39c" class="ku kv iq lx b gy mf mc l md me">with tf.Session() as sess:<br/> '''all the code which require a session is writer here<br/> here Wx is the fetches parameter. fetches refers to the node of the graph we want to compute<br/> feed_dict is used to pass the values for the placeholders<br/> '''<br/> print( "Intermediate result Wx: ", sess.run(Wx, feed_dict={x: [3,33]}))<br/> print( "Final results y: ",sess.run(y, feed_dict={x:[5,50],b:[7,9]}))</span><span id="41b0" class="ku kv iq lx b gy mf mc l md me">writer = tf.summary.FileWriter('./fetchesAndFeed',sess.graph)<br/>writer.close()</span></pre><h2 id="253b" class="ku kv iq bd kw kx ky dn kz la lb dp lc jy ld le lf kc lg lh li kg lj lk ll lm bi translated">变量</h2><p id="79f0" class="pw-post-body-paragraph jn jo iq jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj kk ij bi translated">变量是允许你改变存储在那里的值的结构。监督学习算法在到达最终结论之前执行多次迭代，使用变量来存储随着模型收敛而变化的值。我们的目标是最小化回归线和数据集中的点之间的误差。所以我们在每次迭代中调整回归线来获得新的值。为了得到方程y=A+Bx的最佳拟合线，我们不断调整A和b的值。</p><p id="4403" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">变量是可变的张量值，在多次调用sesssion.run()时保持不变。我用与上面相同的演示代码来解释这一点。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="fd09" class="ku kv iq lx b gy mb mc l md me">import tensorflow as tf</span><span id="309e" class="ku kv iq lx b gy mf mc l md me">W = tf.Variable([2.5,4.0],tf.float32, name='var_W')<br/>#here W is a Variable<br/>x = tf.placeholder(tf.float32, name='x')<br/>b = tf.Variable([5.0,10.0],tf.float32, name='var_b')<br/>#b is also a variable with initial value 5 and 10<br/>y = W * x + b</span><span id="42a2" class="ku kv iq lx b gy mf mc l md me">#initialize all variables defined<br/>init = tf.global_variables_initializer()<br/>#global_variable_initializer() will declare all the variable we have initilized<br/># use with statement to instantiate and assign a session<br/>with tf.Session() as sess:<br/> sess.run(init)<br/> #this computation is required to initialize the variable<br/> print("Final result: Wx + b = ", sess.run(y,feed_dict={x:[10,100]}))<br/># changing values <br/>number = tf.Variable(2)<br/>multiplier = tf.Variable(1)</span><span id="93ed" class="ku kv iq lx b gy mf mc l md me">init = tf.global_variables_initializer()<br/>result = number.assign(tf.multiply(number,multiplier))</span><span id="113f" class="ku kv iq lx b gy mf mc l md me">with tf.Session() as sess:<br/> sess.run(init)</span><span id="14d1" class="ku kv iq lx b gy mf mc l md me">for i in range(10):<br/>  print("Result number * multiplier = ",sess.run(result))<br/>  print("Increment multiplier, new value = ",sess.run(multiplier.assign_add(1)))</span></pre><h2 id="01f9" class="ku kv iq bd kw kx ky dn kz la lb dp lc jy ld le lf kc lg lh li kg lj lk ll lm bi translated">一个TensorFlow程序的多个图形</h2><p id="f31e" class="pw-post-body-paragraph jn jo iq jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj kk ij bi translated">我们可以在TensorFlow程序中显式创建尽可能多的图形。任何TensorFlow程序都有一个默认的图形，其中包含了您已经实例化的所有占位符和变量。但是我们可以通过使用tf.graph()显式地实例化一个图来对图进行逻辑分段。下面的节目可能会解答你的一些疑惑。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="ddea" class="ku kv iq lx b gy mb mc l md me">import tensorflow as tf</span><span id="3083" class="ku kv iq lx b gy mf mc l md me">g1 = tf.Graph()<br/>'''set g1 as default to add tensors to this graph using default methord'''<br/>with g1.as_default():<br/> with tf.Session() as sess:<br/>  A = tf.constant([5,7],tf.int32, name='A')<br/>  x = tf.placeholder(tf.int32, name='x')<br/>  b = tf.constant([3,4],tf.int32, name='b')</span><span id="3b50" class="ku kv iq lx b gy mf mc l md me">y = A * x + b</span><span id="2271" class="ku kv iq lx b gy mf mc l md me">print( sess.run(y, feed_dict={x: [10,100]}))<br/>  '''to ensure all the tensors and computations are within the graph g1, we use assert'''<br/>  assert y.graph is g1</span><span id="99d2" class="ku kv iq lx b gy mf mc l md me">g2 = tf.Graph()</span><span id="0ee9" class="ku kv iq lx b gy mf mc l md me">with g2.as_default():<br/> with tf.Session() as sess:<br/>  A = tf.constant([5,7],tf.int32, name='A')<br/>  x = tf.placeholder(tf.int32, name='x')<br/>  y = tf.pow(A,x,name='y')<br/>  print( sess.run(y, feed_dict={x: [3,5]}))<br/>  assert y.graph is g2</span><span id="cf97" class="ku kv iq lx b gy mf mc l md me">'''same way you can access defaut graph '''<br/>default_graph = tf.get_default_graph()<br/>with tf.Session() as sess:<br/> A = tf.constant([5,7],tf.int32, name='A')<br/> x = tf.placeholder(tf.int32, name='x')<br/> y = A + x<br/> print(sess.run(y, feed_dict={x: [3,5]}))</span><span id="12fa" class="ku kv iq lx b gy mf mc l md me">assert y.graph is default_graph</span></pre><h2 id="095b" class="ku kv iq bd kw kx ky dn kz la lb dp lc jy ld le lf kc lg lh li kg lj lk ll lm bi translated">命名范围</h2><p id="a5dd" class="pw-post-body-paragraph jn jo iq jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj kk ij bi translated">TensorBoard可能是最有用的调试工具，但是随着你的图表尺寸爆炸，你需要一些方法来获得更大的图片中的细节。现在使用TensorFlow运行下面的程序，并在TensorBoard中查看它的图形</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="4ba5" class="ku kv iq lx b gy mb mc l md me">import tensorflow as tf</span><span id="d931" class="ku kv iq lx b gy mf mc l md me">A = tf.constant([4], tf.int32, name='A')<br/>B = tf.constant([4], tf.int32, name='B')<br/>C = tf.constant([4], tf.int32, name='C')</span><span id="6860" class="ku kv iq lx b gy mf mc l md me">x = tf.placeholder(tf.int32, name='x')</span><span id="5f44" class="ku kv iq lx b gy mf mc l md me"># y = Ax^2 + Bx + C<br/>Ax2_1 = tf.multiply(A, tf.pow(x,2), name="Ax2_1")<br/>Bx = tf.multiply(A,x, name="Bx")<br/>y1 = tf.add_n([Ax2_1, Bx, C], name='y1')</span><span id="8602" class="ku kv iq lx b gy mf mc l md me"># y = Ax^2 + Bx^2<br/>Ax2_2 = tf.multiply(A, tf.pow(x,2),name='Ax2_2')<br/>Bx2 = tf.multiply(B, tf.pow(x,2),name='Bx2')<br/>y2 = tf.add_n([Ax2_2,Bx2],name='y2')</span><span id="fe83" class="ku kv iq lx b gy mf mc l md me">y = y1 + y2</span><span id="b4da" class="ku kv iq lx b gy mf mc l md me">with tf.Session() as sess:<br/> print(sess.run(y, feed_dict={x:[10]}))</span><span id="772b" class="ku kv iq lx b gy mf mc l md me">writer = tf.summary.FileWriter('./named_scope',sess.graph)<br/> writer.close()</span></pre><figure class="ls lt lu lv gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mg"><img src="../Images/7206826a9dee5b704b24fa7e1ded073d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6mt64zFaQAipgrPTnaw6hQ.png"/></div></div></figure><p id="bbcf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这张图表看起来真的很复杂！</p><p id="bf20" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们可以使用命名作用域在tensorboard中组织事物。使用定义名称范围</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="04f4" class="ku kv iq lx b gy mb mc l md me">with tf.name_scope("name the scope"):</span></pre><p id="2f6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">并在这个范围内编写代码。上述程序可以使用如下所示的命名空间进行安排</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="8d83" class="ku kv iq lx b gy mb mc l md me">import tensorflow as tf</span><span id="5ade" class="ku kv iq lx b gy mf mc l md me">A = tf.constant([4], tf.int32, name='A')<br/>B = tf.constant([4], tf.int32, name='B')<br/>C = tf.constant([4], tf.int32, name='C')</span><span id="c938" class="ku kv iq lx b gy mf mc l md me">x = tf.placeholder(tf.int32, name='x')</span><span id="2c72" class="ku kv iq lx b gy mf mc l md me"># y = Ax^2 + Bx + C<br/>with tf.name_scope("Equation1"):<br/> Ax2_1 = tf.multiply(A, tf.pow(x,2), name="Ax2_1")<br/> Bx = tf.multiply(A,x, name="Bx")<br/> y1 = tf.add_n([Ax2_1, Bx, C], name='y1')</span><span id="a107" class="ku kv iq lx b gy mf mc l md me"># y = Ax^2 + Bx^2<br/>with tf.name_scope("Equation2"):<br/> Ax2_2 = tf.multiply(A, tf.pow(x,2),name='Ax2_2')<br/> Bx2 = tf.multiply(B, tf.pow(x,2),name='Bx2')<br/> y2 = tf.add_n([Ax2_2,Bx2],name='y2')</span><span id="d9db" class="ku kv iq lx b gy mf mc l md me">with tf.name_scope("final_sum"):<br/> y = y1 + y2</span><span id="4468" class="ku kv iq lx b gy mf mc l md me">with tf.Session() as sess:<br/> print(sess.run(y, feed_dict={x:[10]}))</span><span id="2594" class="ku kv iq lx b gy mf mc l md me">writer = tf.summary.FileWriter('./named_scope',sess.graph)<br/> writer.close()</span></pre><figure class="ls lt lu lv gt mh gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/01281af8cc9ddf985fee4456ef831f94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*HAmvXEPvsDt0NXoSVnA5vw.png"/></div></figure><h2 id="17cf" class="ku kv iq bd kw kx ky dn kz la lb dp lc jy ld le lf kc lg lh li kg lj lk ll lm bi translated">将在接下来的故事中继续介绍图像识别。请分享帮助别人找到。欢迎发表评论。</h2></div></div>    
</body>
</html>