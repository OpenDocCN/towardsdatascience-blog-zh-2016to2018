<html>
<head>
<title>Review: Xception — With Depthwise Separable Convolution, Better Than Inception-v3 (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:例外—深度方向可分离卷积，优于 Inception-v3(图像分类)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568?source=collection_archive---------3-----------------------#2018-09-25">https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568?source=collection_archive---------3-----------------------#2018-09-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="66de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>这个故事中，谷歌的<strong class="jp ir">exception【1】，代表《盗梦空间》的极致版，进行了回顾。通过修改的<strong class="jp ir">深度方向可分离卷积</strong>，它在 ImageNet ILSVRC 和 JFT 数据集上都比 Inception-v3</strong>【2】(也是由谷歌在 ILSVRC 2015 中获得亚军)更好。虽然这是一篇去年刚刚发表的<strong class="jp ir"> 2017 CVPR </strong>论文，但在我写这篇文章的时候，它已经被引用了<strong class="jp ir"> 300 多次</strong>。(<a class="ku kv ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----dc967dd42568--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="45b3" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">涵盖哪些内容</h1><ol class=""><li id="c047" class="mb mc iq jp b jq md ju me jy mf kc mg kg mh kk mi mj mk ml bi translated"><strong class="jp ir">原始深度方向可分离卷积</strong></li><li id="5956" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><strong class="jp ir">除</strong>外的改进深度方向可分离卷积</li><li id="8d90" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><strong class="jp ir">整体架构</strong></li><li id="fc9f" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><strong class="jp ir">与最先进结果的比较</strong></li></ol></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="ad1e" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated"><strong class="ak"> 1。原始深度方向可分离卷积</strong></h1><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mr"><img src="../Images/62371b7307d3453e23fe3fa3c6223f0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VvBTMkVRus6bWOqrK1SlLQ.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nh">Original Depthwise Separable Convolution</strong></figcaption></figure><p id="f991" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">原始深度方向可分离卷积是<strong class="jp ir">深度方向卷积后跟一个点方向卷积</strong>。</p><ol class=""><li id="a2fa" class="mb mc iq jp b jq jr ju jv jy ni kc nj kg nk kk mi mj mk ml bi translated"><strong class="jp ir">深度方向卷积</strong>是<strong class="jp ir">通道方向 n×n 空间卷积</strong>。假设在上图中，我们有 5 个通道，那么我们将有 5 个 n×n 空间卷积。</li><li id="65ea" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><strong class="jp ir">逐点卷积</strong>实际上是改变尺寸的<strong class="jp ir"> 1×1 卷积</strong>。</li></ol><p id="5386" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与常规卷积相比，我们不需要在所有通道上执行卷积。这意味着<strong class="jp ir">连接数量更少，型号更轻。</strong></p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="d3c1" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated"><strong class="ak"> 2。除</strong>外的改进深度可分卷积</h1><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nl"><img src="../Images/c6f90347945ae6e885cbc45c0549bf11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8dborzVBRBupJfvR7YhuA.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nh">The Modified Depthwise Separable Convolution used as an Inception Module in Xception, so called “extreme” version of Inception module (n=3 here)</strong></figcaption></figure><p id="3076" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">修改的深度方向可分离卷积是<strong class="jp ir">点方向卷积，后面是深度方向卷积</strong>。这种修改是由 inception-v3 中的 Inception 模块引起的，即在任何 n×n 空间卷积之前首先进行 1×1 卷积。因此，它与原来的有点不同。(<strong class="jp ir">这里 n=3 </strong>因为在 Inception-v3 中使用了 3×3 空间卷积。)</p><p id="6456" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">两个小区别:</strong></p><ol class=""><li id="c1a9" class="mb mc iq jp b jq jr ju jv jy ni kc nj kg nk kk mi mj mk ml bi translated"><strong class="jp ir">操作顺序</strong>:如上所述，通常实现的原始深度方向可分离卷积(例如在 TensorFlow 中)首先执行通道方向空间卷积，然后执行 1×1 卷积，而修改的深度方向可分离卷积<strong class="jp ir">首先执行 1×1 卷积，然后执行通道方向空间卷积</strong>。这被认为是不重要的，因为当它被用在堆叠设置中时，在所有链接的初始模块的开始和结束处只有很小的差别。</li><li id="e76f" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><strong class="jp ir">存在/不存在非线性</strong>:在初始模块中，第一次操作后存在非线性。<strong class="jp ir">除了</strong>修改的深度方向可分离卷积<strong class="jp ir">之外，没有中间 ReLU 非线性</strong>。</li></ol><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/64b900bae05cbb4a84484bcd801304d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*x99M0RaK9i2X9kYrEYtYdg.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nh">The modified depthwise separable convolution with different activation units</strong></figcaption></figure><p id="eda1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">测试了具有不同激活单元的改进深度可分卷积。如上图所示，<strong class="jp ir">与使用 ELU 或 r ELU 的情况相比，没有任何中间激活的例外具有最高的准确性</strong>。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="68a4" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated"><strong class="ak"> 3。整体架构</strong></h1><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nn"><img src="../Images/277cefd3841483ece83545e4271d8530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hOcAEj9QzqgBXcwUzmEvSg.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nh">Overall Architecture of Xception (Entry Flow &gt; Middle Flow &gt; Exit Flow)</strong></figcaption></figure><p id="5cb1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上图所示，<strong class="jp ir"> SeparableConv </strong>是修正的深度方向可分离卷积。我们可以看到，SeparableConvs 被<strong class="jp ir">视为初始模块</strong>，并被放置在整个深度学习架构中。</p><p id="1e97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还有最初由 ResNet [3]提出的针对所有流的<strong class="jp ir">剩余(或快捷方式/跳过)连接</strong>。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi no"><img src="../Images/8d9a89458b45e3880a68bf63b093e716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*gDlRiUcFEfNmTOGOoZFIXg.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nh">ImageNet: Validation Accuracy Against Gradient Descent Steps</strong></figcaption></figure><p id="6d42" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如在架构中看到的，存在剩余连接。这里，它使用非剩余版本测试异常。从上图可以看出，使用残差连接时，精度要高得多。<strong class="jp ir">由此可见，剩余连接极其重要！！！</strong></p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="4b7a" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated"><strong class="ak"> 4。与最先进结果的比较</strong></h1><p id="0da7" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy np ka kb kc nq ke kf kg nr ki kj kk ij bi translated">测试了 2 个数据集。一个是 ILSVRC。一个是 JFT。</p><h2 id="96d6" class="ns le iq bd lf nt nu dn lj nv nw dp ln jy nx ny lr kc nz oa lv kg ob oc lz od bi translated">4.1.ImageNet — ILSVRC</h2><p id="bfc1" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy np ka kb kc nq ke kf kg nr ki kj kk ij bi translated">ImageNet 是一个数据集，包含超过 1500 万张带有标签的高分辨率图像，大约有 22，000 个类别。</p><p id="c79a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ILSVRC 在 1000 个类别中的每个类别中使用大约 1000 个图像的 ImageNet 子集。总的来说，大约有 130 万幅训练图像、50，000 幅验证图像和 100，000 幅测试图像。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oe"><img src="../Images/0671073c58fdf8fbec2455d13222dc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X_Mh854bmKovWBuLX_an2g.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nh">ImageNet: Xception has the highest accuracy</strong></figcaption></figure><p id="7b53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Xception 优于 VGGNet [4]、ResNet [3]和 Inception-v3 [2]。(如果感兴趣，也请访问我关于他们的评论，广告了，lol)</p><p id="4dc2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">需要注意的是，在错误率方面，而不是准确率方面，相对提升不小！！！</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/91fc8b5d6fa8890e0408db91cfc760ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*zsJZU90QxWkxMlmr_6Mg-g.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nh">ImageNet: Validation Accuracy Against Gradient Descent Steps</strong></figcaption></figure><p id="1e79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当然，从上图来看，沿着梯度下降的台阶，Xception 比 Inception-v3 有更好的准确率。</p><p id="bb87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但如果用非残差版本和 Inception-v3 对比，Xception 表现不如 Inception-v3。为了公平比较，是不是应该有一个残版的 Inception-v3 比较好？无论如何，Xception 告诉我们，利用深度方向可分离卷积和残差连接，确实有助于提高精度。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/7495e4bec456334b8e57cbfa50ef8a7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*XpWbCahzdMOTHiHogqz0nw.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nh">Model Size/Complexity</strong></figcaption></figure><p id="2966" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">据称，Xception 与 Inception-v3 的模型大小相似。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h2 id="bf34" class="ns le iq bd lf nt nu dn lj nv nw dp ln jy nx ny lr kc nz oa lv kg ob oc lz od bi translated">4.2.JFT—fast val 14k</h2><p id="16fb" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy np ka kb kc nq ke kf kg nr ki kj kk ij bi translated">JFT 是一个用于大规模图像分类数据集的内部谷歌数据集，由 Hinton <em class="oh">教授等人</em>首先介绍，它包括超过 3.5 亿张高分辨率图像，标注有来自 17，000 个类别的标签。</p><p id="e836" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用辅助数据集 FastEval14k。FastEval14k 是一个包含 14，000 幅图像的数据集，具有来自大约 6，000 个类的密集注释(平均每幅图像 36.5 个标签)。</p><p id="7d5c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于多个目标密集地出现在一幅图像中，因此使用平均精度预测(mAP)进行测量。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/d59d466b399b724a39717c4cc9c1bb77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*Sn-HE8Us8VwbnF7f-qyiTg.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nh">FastEval14k: Xception has highest mAP@100</strong></figcaption></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/eada3442618a8f45bd22b7c59924a53c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*yFfHbSOQ-me-cyNVNbLZJw.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nh">FastEval14k: Validation Accuracy Against Gradient Descent Steps</strong></figcaption></figure><p id="7ddb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同样，与 Inception-v3 相比，Xception 的 mAP 更高。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="7df7" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">参考</h1><ol class=""><li id="a042" class="mb mc iq jp b jq md ju me jy mf kc mg kg mh kk mi mj mk ml bi translated">【2017 CVPR】【例外】<br/> <a class="ae ok" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf" rel="noopener ugc nofollow" target="_blank">例外:深度可分卷积深度学习</a></li><li id="dcb7" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">【2016 CVPR】【盗梦空间-v3】<br/><a class="ae ok" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">重新思考计算机视觉的盗梦空间架构</a></li><li id="e174" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">【2016 CVPR】【ResNet】<br/><a class="ae ok" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习</a></li><li id="287b" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">【2015 ICLR】【VGGNet】<br/><a class="ae ok" href="https://arxiv.org/pdf/1409.1556" rel="noopener ugc nofollow" target="_blank">用于大规模图像识别的极深度卷积网络</a></li></ol><h1 id="debd" class="ld le iq bd lf lg ol li lj lk om lm ln lo on lq lr ls oo lu lv lw op ly lz ma bi translated">我的评论</h1><ol class=""><li id="eb34" class="mb mc iq jp b jq md ju me jy mf kc mg kg mh kk mi mj mk ml bi translated"><a class="ae ok" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener">回顾:Inception-v3–ILSVRC 2015 亚军(图像分类)</a></li><li id="e201" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><a class="ae ok" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">回顾:ResNet—ils vrc 2015(图像分类、定位、检测)获奖者</a></li><li id="fe6b" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><a class="ae ok" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener">点评:VGGNet—ils vrc 2014 亚军(图像分类)、冠军(本地化)</a></li></ol></div></div>    
</body>
</html>