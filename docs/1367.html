<html>
<head>
<title>Experiments on different loss configurations for style transfer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">风格转换的不同损失配置实验</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/experiments-on-different-loss-configurations-for-style-transfer-7e3147eda55e?source=collection_archive---------4-----------------------#2017-08-28">https://towardsdatascience.com/experiments-on-different-loss-configurations-for-style-transfer-7e3147eda55e?source=collection_archive---------4-----------------------#2017-08-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="868c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我参加<a class="ae kl" href="https://medium.com/@jeremyhoward" rel="noopener">杰瑞米·霍华德的</a>优秀的<a class="ae kl" href="http://course.fast.ai/part2.html" rel="noopener ugc nofollow" target="_blank">程序员前沿深度学习</a>时，我对风格转移很感兴趣。我想探究改变损耗配置如何改变生成的图像。我想看看使用不同的损耗网络如何产生不同的图像。我用的不同损耗网络是vgg-16和vgg-19。这是一篇关于我的发现的博文。</p><p id="4830" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那些不熟悉风格转移的人可以阅读<a class="ae kl" href="https://medium.com/@singlasahil14/introduction-to-style-transfer-faa60f3f5257" rel="noopener">这篇</a>介绍其工作原理的博客文章。</p><p id="1abd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所有实验都是使用以下内容和样式图像完成的:</p><div class="km kn ko kp gt ab cb"><figure class="kq kr ks kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/aee20768c01b9a606065a67ab1ff6351.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*gahvpeitypqK0fTW1Enx0g.jpeg"/></div></figure><figure class="kq kr ld kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/e587d82b91695c0f7f4a9afd9b6de7c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*IT6LKaQgMmNA-VoT4JqXmw.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk li di lj lk">left (content image), right (style image) used in these experiments</figcaption></figure></div><p id="272e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些实验中使用的所有预训练损耗网络都是从<a class="ae kl" href="https://github.com/tensorflow/models/tree/master/slim" rel="noopener ugc nofollow" target="_blank"> tensorflow瘦模型库</a>下载的。除非特别说明，否则初始图像是内容图像，使用的预训练网络是vgg-16。要找出conv1_2、conv2_2等指的是哪些层，请运行</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/8c7a22de76d6c518b4b593660f5cc2e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*BRqKFvueyoRyRY07KeuvoQ.png"/></div></figure><p id="b666" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://github.com/singlasahil14/style-transfer" rel="noopener ugc nofollow" target="_blank">在本次回购中。</a>以下所有实验均使用本报告中给出的代码进行。</p><p id="c0b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">实验1:一次从一层使用风格损失进行训练</strong></p><p id="a3a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我使用这5层的输出来计算样式损耗:conv1_2、conv2_2、conv3_3、conv4_1和conv5_1。作为一个开始实验，我将内容权重和电视权重设置为零，将图像初始化为“噪声”。并使用这些层中的每一层生成样式图像(每次使用一层来计算样式损失)。生成的风格图像应该与绘画的仿作相对应。</p><p id="66d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是产出:</p><div class="km kn ko kp gt ab cb"><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/35d4394c9981281332fb2a0507702329.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*fPfayZWxOVTZzzBqAaf07g.jpeg"/></div></figure><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/084125f2b363d7eccc9a2cc49f3acfba.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*99VwGfvjv7UbVmeabPokMg.jpeg"/></div></figure><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/18bc461513b45a64a24ecb184b69ca51.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0kAHKOz904u22yvm_x8Y_g.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk ln di lo lk">image created using loss from conv1_2, conv2_2 and conv3_3 (left to right)</figcaption></figure></div><div class="ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/3653ac93e2d905b353aa351d28d34476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*RzYQPu8mrFgEMjzu2G5Y0g.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/25d15e1a2b8ce8b0057a6e4cfd87ae8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Moj70pP3NEglcbFGWd9AWg.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">image created using loss from conv4_1 (left) and conv5_1 (right)</figcaption></figure></div><p id="77e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在早期层(conv1_2和conv2_2)的输出中，风格图像中需要较小感受野(如咖啡色背景)的图案是显著的。在后面的层(conv3_3和conv4_1)中，更大的图案更明显。conv5_1的输出看起来更像垃圾。我认为这是因为在该层很少激活。所以对损失贡献不大。下面的初始损失也显示了同样的情况。</p><p id="751e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了生成风格化的图像，我使用conv3_3来计算内容损失。</p><p id="0c50" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我保持所有5层的权重等于1时，以下是图像初始化为“内容”时的层内容和样式损失值:</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ls"><img src="../Images/237eb93a42c680b8f83895fe27100c5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g3pAs6UAzv7GTGcqEH2rzw.png"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">layer-wise style loss values for initial image ‘content’</figcaption></figure><p id="9f13" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我保持所有5层的权重相等时，以下是图像初始化为“噪波”时的层内容和样式损失值:</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lt"><img src="../Images/8c46c083b62b8e8b82b90aa4d2d4cf03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mmz3OoQIZuFflxVJjAh_zQ.png"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">layer-wise style loss values for initial image ‘noise’</figcaption></figure><p id="17b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">显然，在这两种情况下，后面的层对样式损失的贡献都非常小。在大多数出版的关于风格转换的作品中，所有层的权重都是一样的，这没有意义。</p><p id="6bbf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我想看看如果我只使用其中一层来训练，会产生什么样的风格化图像。</p><div class="km kn ko kp gt ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/b41b623a444cab8f87296914f3b7b166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*CjCDz_i4sSfubuaUBbnW6w.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/ad607e01336d6eb92aeadb178db5db64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*-rdP75clydoqnRXcO_0kUg.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">Output image when trained with gram matrix from layer conv1_2 and conv2_2 (left to right)</figcaption></figure></div><div class="ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/d07750c25621f798ca2eba2716e6271a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*mhtI75eCRUDMXHStyt0cSQ.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/2f284d652c1aeb2310e6674ae7ee615c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ahDCsti1kcsW0X3W__frrA.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">Output image when trained with gram matrix from layer conv3_3 and conv4_1 (left to right)</figcaption></figure></div><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lu"><img src="../Images/4a6e9cfe2d7c246c18c6684df13ad1d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2eWQkW9ySlnFeRpEG_-Ckg.jpeg"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">Output image when trained with gram matrix from layer conv5_1</figcaption></figure><p id="6d33" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">从上面生成的图像可以明显看出，当我们从conv1_2移动到conv5_1时，笔触的大小会增加。【conv5 _ 1的输出中有一些奇怪的模式。很可能是因为conv5_1没有包含太多的信息(我不得不将这一层的损失缩放1e6来生成此图像)。所以这只是噪音。这5个中我最喜欢的输出？conv3_3的输出。</strong></p><p id="e774" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">实验2:利用一次一层的内容损失进行训练</strong></p><p id="5de5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我使用3层的输出来逐一计算内容损失:conv2_2、conv3_3、conv4_1。</p><p id="f94a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我首先尝试通过将样式权重和电视权重设置为0来查找生成的图像，并计算每个内容层的损失，即仅使用内容损失。</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lu"><img src="../Images/b87bea8dc00ca7622ef5de9a6f5bda27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zsXEbxTpATWrhBYsQl3URA.jpeg"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">Image created using content loss from conv2_2</figcaption></figure><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lu"><img src="../Images/53a53f6529a5823335f3a9807846c1bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-7QmadMiDBgMyW0cf8NUdw.jpeg"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">Image created using content loss from conv3_3</figcaption></figure><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lu"><img src="../Images/4b37c406a73a6667dcef4199443266a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JNv4lhd9pAVqBWzwxOSNpA.jpeg"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">Image created using content loss from conv4_1</figcaption></figure><p id="734c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以看出，当我们从conv2_2移至conv4_1时，仍能捕捉到窗口等高层信息，但边缘、拐角等低层信息越来越模糊。</p><p id="7fef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我生成了风格化的图像(使用内容，风格和电视损失)。对于风格损失，我使用层conv3_3。我想检查一下，如果我更改内容层，上一个实验中conv3_3层的图像输出会有什么变化。</p><p id="bada" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是生成的图像:</p><div class="km kn ko kp gt ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/d04bc58d1434fb915d041e29e906d04b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Mubbw9h0fOvhWZCcB0DKmA.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/477d7c39434b5536e96fb872ca50182a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*9AqZZlWbZn0Ib3ashMcA4Q.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">Output image when trained with features from layer conv2_2 and conv3_3 (left to right)</figcaption></figure></div><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lu"><img src="../Images/56ea4fb583999b5aeadace843b11cfb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GY8363Ryy10BRlhwDzBDrg.jpeg"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">Output image when trained with features from layer conv4_1</figcaption></figure><p id="5926" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">很明显。<strong class="jp ir">当我们从conv2_2移到conv4_1时，建筑的边缘变得更钝，颜色变得更淡。捕获的原始内容要少得多。</strong></p><p id="39c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">实验3:使用损失网络中两个最大汇集层之间不同层的风格损失进行训练</strong></p><p id="3f53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我想看看在两个最大池层之间使用不同的层来计算样式损失是否会改变生成的风格化图像。为此，我使用了以下层“conv3_1”、“conv3_2”和“conv3_3”。</p><div class="km kn ko kp gt ab cb"><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/be2f2277dbd92d366347d77385ceaf7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*or_Glk7boy3HzfN76hjMtQ.jpeg"/></div></figure><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/e10b742fced4d1623256041d82e687e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*8uQwyI3VNgT837uaKA4T4w.jpeg"/></div></figure><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/6ac38c02d5f7302bbd63aa7f98ee1320.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*cp73mX5PhuwZdVyusE7uGw.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk ln di lo lk">images generated by training using only the style loss: conv3_1, conv3_2, conv3_3 (left to right)</figcaption></figure></div><p id="ef88" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以上是仅使用风格损失进行训练时的输出。conv3_3的输出与conv3_1和conv3_2的输出截然不同。</p><p id="50d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我尝试通过使用这些图层来计算风格损失，从而创建风格化的图像。用于内容丢失的层是conv2_2。</p><p id="5d14" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是生成的图像:</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lu"><img src="../Images/79367dc7c6827022d65a28f894c2b902.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Efh3-Q77xiMYIAhWwMII1g.jpeg"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">image generated for loss calculated from layer conv3_1</figcaption></figure><div class="km kn ko kp gt ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/8cc164eaacffecdca16aacdca79c1467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*yBJkGBpC_bWdOofv7XEX0g.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/c0555dcd9f3a1162a09f26028b33906f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*hpRZTABZSUsY1kBGp6cpsg.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">image generated for loss calculated from layer conv3_2, conv3_3 (left, right)</figcaption></figure></div><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lu"><img src="../Images/79367dc7c6827022d65a28f894c2b902.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Efh3-Q77xiMYIAhWwMII1g.jpeg"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">magnified view of image generated using style loss from conv3_1</figcaption></figure><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lu"><img src="../Images/67741c724c082ae77c86f107fb0f74d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WzFqR9yiZNLfV4bPTuB59A.jpeg"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">artifacts shown with black underlining (the image is the same as conv3_3 shown above)</figcaption></figure><p id="0e6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在conv3_1和conv3_3之间，上图(conv3_3输出)中带黑色下划线的伪像明显更高。<strong class="jp ir">有趣的是，conv3_3的仿体不同于conv3_1和conv3_2。生成的图像也是如此。</strong></p><p id="8a8d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">实验4:使用不同大小的风格图像进行训练</strong></p><p id="7815" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我使用conv2_2层的输出来计算内容损失:conv2_2。对于风格损失，我使用权重为1的conv2_2。我没有使用原始尺寸(928x514)的样式图像，而是调整了图像的大小，使短边等于特定的尺寸，并在中间裁剪了较大的尺寸，以提取一个正方形。我试的尺码是128，256，384和512。</p><p id="67b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是生成的图像:</p><div class="km kn ko kp gt ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/50a7dea5debdfdd5c0c7cb92f664dc72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*CT3rzUzW5MeGNKH_9IE8JA.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/b328cf2a9043be6b1d7b72671920252c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*N3xoatDnoG3F1XNAxjL05A.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">stylized images for style image sizes 128 (left) and 256 (right)</figcaption></figure></div><div class="ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/db0c7f28b5240b74346f64c05e595a53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Y1Bc7EloB3zMfol8H1xivQ.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/a39a30520a1fee50abdf4d25fa86d6b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Hh62C64BgLWK-CYixClMsw.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">stylized images for style image sizes 384 (left) and 512 (right)</figcaption></figure></div><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lu"><img src="../Images/40d70981e0f88e1ab2464f06aaaa5c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VWwDa-L_dZD27G5NgPb7IQ.jpeg"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">stylized image for default style image size : 514x928</figcaption></figure><p id="b585" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用较小尺寸的样式图像生成的图像不好看，甚至不能显示样式图像的笔触。</p><p id="fdcd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">同样，很明显，随着风格图像尺寸的增加，生成的图像更好地捕捉了画家的笔触。</strong></p><p id="c528" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">实验5:使用不同的vgg网络(vgg16和vgg19)进行训练</strong></p><p id="c8cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我从vgg16和vgg19生成图像。内容损耗用conv2_2计算，风格损耗用conv3_1计算。</p><p id="3d5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我只生成了模仿作品(保持内容和电视权重为零)。生成了以下仿作:</p><div class="km kn ko kp gt ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/b68d98808f98843f8d9326c6f28e7d08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*nLQt5UnMPaWTdh0Mn2TaJg.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/8c006036b950e14de9e88f0a1ab5a9e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*_GmUNxFswGyqTM8MRpx9XQ.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">pastiches generated left: vgg-16, right: vgg-19</figcaption></figure></div><p id="6e85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">生成的仿作几乎没有任何不同。因此，我期望程式化的图像几乎没有什么不同，但我完全错了。这些是生成的图像:</p><div class="km kn ko kp gt ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/7bd8e825a50e09e2a116e1b54c5dbca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*nkvVYd4NP95E0DgJLKYf8w.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/af99809bb5662dbbf1ecdc256203a0ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*zV_WM5QXp9PhIJ-jgsYjaA.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">Output by training using vgg-16 (left) and vgg-19 (right) loss networks</figcaption></figure></div><p id="335b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用vgg-19生成的图像看起来更好，捕捉内容更好。我对此很感兴趣，并决定绘制图像在训练过程中的内容损失、风格损失和总损失。</p><div class="km kn ko kp gt ab cb"><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/39961cf445c012d97c227a0a49ab3140.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*c6B2e3EavfY1rF6xqONsog.png"/></div></figure><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/271efd8be1bdfb86f79271ee9a8f925c.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*7xbOuV75r2W3fZoXI-Zm6A.png"/></div></figure><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/d4dbb0834a7acd2136512369304bea76.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*XlSdQw6HqmEZ2OgEcmhR0g.png"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk ln di lo lk">content loss, style loss and total loss of image as it gets trained</figcaption></figure></div><p id="d3af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">更重要的是，vgg-19的起始风格损失值远低于vgg-16。我的猜测是，生成的图像之所以不同，是因为网络对内容和风格损失赋予了不同的“隐含”权重。</strong>所谓“隐含”权重，我指的是网络本身赋予内容和风格损失的权重，而不是因为我指定的内容风格和电视权重。</p><p id="09e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，这里vgg16和vgg19表示不同的功能。因此，这种比较并不完全意味着“vgg-19”的绝对含量损失低于“vgg-16”。这只是表明，在优化图像的同时，vgg-16的风格损失变得更加重要。</p><p id="d3ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注:-对于后续实验，使用conv2_2计算(6，7，8)含量损失，conv3_1计算风格损失。</p><p id="29cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">实验6:使用不同初始化(“噪声”/“内容”)的训练</strong></p><div class="km kn ko kp gt ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/97e9f71964b357d1dfd061212fa4243f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*nWpyNDbFkBwBLpp5CDj63w.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/a0b3096c633b352ce06be3fc35c88190.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*gRNiB3gUHHqkrosulBxZPQ.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">image generated with initial ‘content’ (left) and initial ‘noise’ (right)</figcaption></figure></div><div class="ab cb"><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/369d786b5102b5b9e68e1074a4d9cf98.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*zW6Uef4o8rKzwwlutC7PZg.png"/></div></figure><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/417fd296b7ff864e42b6984902d85e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*paOrBrT2j8Y6kC9rlBTsOQ.png"/></div></figure><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/871602906c87ec6737036224b4d22a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Ekjb6AhPJ9xPYZWhMsza3g.png"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk ln di lo lk">content loss, style loss and total loss of image for both these initial conditions as it gets trained</figcaption></figure></div><p id="337a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以看出，内容和噪声图像都收敛到几乎相同的损失值。尽管生成的图像不同。<strong class="jp ir">使用“内容”初始化时，损耗收敛更快。</strong></p><p id="f056" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">实验7:使用不同类型的衬垫进行训练(相同/有效)</strong></p><div class="km kn ko kp gt ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/702660b72487f66bc3011b7cc71145ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ewXkENPxrK6NIz0a3MU07Q.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/f8e26ee131b9ee68fa203cd913ea4c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*61mF0yKdp44wIJStxwgkcA.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">images generated using padding ‘SAME’ (left) and ‘VALID’ (right) in the loss network</figcaption></figure></div><div class="ab cb"><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/82850bf4a1c6a340fbe92601fbdb304e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*lFIpabcylNP6pR0ST7HZuQ.png"/></div></figure><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/7b947decaa2c79c3e74554b81e5f42a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ALJeh2zimtufPFlGZ80-8Q.png"/></div></figure><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/7097133a1286ddd42c30b6827e0f64d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*QYHmDwjgVjK3cyqne9bbcA.png"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk ln di lo lk">content loss, style loss and total loss of image for both these types of padding as it gets trained</figcaption></figure></div><p id="f887" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">损失值相差不大。结果完全符合我的预期。</strong></p><p id="00d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">实验8:使用不同类型的汇集(最大/平均)进行训练</strong></p><div class="km kn ko kp gt ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/ffd4fdde48bcd4e107fc49efbede55f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*F4jwBa_9TzVPMSVRdKSbOQ.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/c412c66523eaed9049a15b7e3fb5f5fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*gm88Y1rWW2QJXn_YhnKPrA.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">image generated with pooling ‘avg’ (left) and pooling ‘max’ (right)</figcaption></figure></div><div class="ab cb"><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/687bac7f4d4401a7de04d8c50ab5a5e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*zsTOobyPxvhbty2A6uHhjQ.png"/></div></figure><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/8774df10cdd7350cb0b6fcf5c6addf75.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*RmFWhO7yNIUqjrgW7V-rYg.png"/></div></figure><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/8ec260f76eef43c34b9d5530fd21f5e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*tGddkXMoUw3LugvV554kvA.png"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk ln di lo lk">content loss, style loss and total loss of image for both these types of pooling as it gets trained</figcaption></figure></div><p id="4c54" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以看到，使用“max”生成的图像有奇怪的点。<strong class="jp ir">使用“avg”生成的图像更加均匀。更重要的是,“平均值”的总损失收敛速度比“最大值”快得多。</strong></p><p id="e899" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">实验9:使用不同风格权重值进行训练</strong></p><p id="c69c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我想看看样式权重对生成的图像有什么影响。为此，我决定对内容层conv2_2和样式层conv3_1使用vgg-19，样式权重设置为200、400、800、1600和3200。生成了以下图像:</p><div class="km kn ko kp gt ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/771a589c5506b6a3f2fa16db2687047f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*uyUyOY23q576HIlCtZEUhg.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/962f5a95f08d1fa3825683e5cc001a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*AVaR-JZ2NtyY53YCeb8BYA.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">left image: style-weight 200, right image: style-weight 400</figcaption></figure></div><div class="ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/9685fe386654f57035f6fe2abc9a42ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*GAvdGjFyr5OlF9UztljEDw.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/c2ea0c9726d165918cbac37e890856fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Xf1AwrjXrlmi7_A56gzxOQ.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">left image: style-weight 800, right image: style-weight 1600</figcaption></figure></div><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lu"><img src="../Images/602587b1e3ed042cf696e4dc3004ecfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fLp6vih8GUDN1LneCUOt7w.jpeg"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">style-weight 3200</figcaption></figure><p id="bb95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">可以看出，在使用风格权重3200生成的图像中，画家的笔触要突出得多。</strong>原始内容(颜色等)不太突出。</p><p id="bb29" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">实验10:使用不同的总变化损失值进行训练</strong></p><p id="f51e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我想看看总变差损失对生成的图像有什么影响。为此，我决定对内容层conv2_2和样式层conv3_1使用vgg-19，总变差损失为零，下一个电视重量设置为200。生成了以下图像:</p><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lu"><img src="../Images/48d9be470597cdb4c6c8441a1a5d59c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jJre-zQDJAZ6s1joz9YdQw.jpeg"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">Image created using default content and style weight and tv weight = 0. On careful observation, it becomes visible that image is quite rough.</figcaption></figure><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lu"><img src="../Images/fb150d76eddd1fd2eb65891c8271dbd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uyUyOY23q576HIlCtZEUhg.jpeg"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">Image created using default content and style weight and tv weight = 200. Roughness present in the previous image is gone</figcaption></figure><p id="1d89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总变差损失有助于去除生成图像的粗糙纹理。<strong class="jp ir">从上面两幅图像中可以观察到，一幅有总变差损失，另一幅没有，总变差损失非零的图像非常平滑。</strong></p><p id="3bbf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">实验11:使用风格图像的不同裁剪区域进行训练</strong></p><p id="39f0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我裁剪了样式图像的不同区域，以了解它如何影响生成的图像的质量。</p><p id="abb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是提取的样式图像部分:</p><div class="km kn ko kp gt ab cb"><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/3b733cccef7bf23e481e4bbc92d349c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*J5HF_ce2Rm0OvBWmz7kFSQ.jpeg"/></div></figure><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/b821a03c876c0fa61c3258e757a80182.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ozMarcZHQpuONVgRD9AlCw.jpeg"/></div></figure><figure class="kq kr lm kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/7333b924c883c5f267946f359e53c678.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*-p--OicZjcNMRHpF3opTLw.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk ln di lo lk">style image portions used for training the image: left region, center region, right region of the style image (left to right)</figcaption></figure></div><figure class="km kn ko kp gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lv"><img src="../Images/fdca8e5ac346634f639372e68fce4118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IT6LKaQgMmNA-VoT4JqXmw.jpeg"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk">complete style image</figcaption></figure><p id="d456" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">和以前一样，我首先为左侧区域、中间区域、右侧区域和完整的图像生成了仿制品。</p><p id="e760" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用了vgg-16网络。使用的样式层是conv3_1。</p><div class="km kn ko kp gt ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/94e9c01ff9d5c25a185450f7383ea53c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*FjiujCdyfHjk7Z_3v9Jn0g.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/28526f776a87ccf3db743cf43f6da02d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*WlXce7CumxA9i5CsN7mYuA.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">left region (left), center region (right)</figcaption></figure></div><div class="ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/24de0c987594db6bd7ced9e6d29f5610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*_BFkY1Ozr6UoyQYNRLduMA.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/1cf7cbbd9ee507b8256f7770656946cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*jk-yylino0yX4E5kX8BzkQ.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">right region (left) and complete image (right)</figcaption></figure></div><p id="f8a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">右侧区域的仿作明显不同于左侧、中间区域和完整图像的仿作。这是预料之中的，因为样式图像的右边区域明显不同于所有其他区域，甚至不同于整个样式图像本身。</p><p id="7a7b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来生成风格化的图像，内容层是conv2_2，样式层是conv3_1。内容重量、风格重量和电视重量在规范中给出(分别为8和200)。</p><p id="971a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">生成了以下图像:</p><div class="km kn ko kp gt ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/e38453e5ecdfdf570a91a9482507a85f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*TbyB_SnIpBm80eIfuPEeGQ.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/8ab31a9eaa5bbd757f8dab14cbfbe9ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*J2RYCEeZNJwLhPDlHT7kSw.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">stylized image generated using left region (left) and center region (right)</figcaption></figure></div><div class="ab cb"><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/e2093729db4a00d6d4cafdf781662078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*tbEBBvU8Lsg6Zvgi1KJRkw.jpeg"/></div></figure><figure class="kq kr lp kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/2a74cf4304739acb3b0087d9a5693391.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*zDQckr6NKvMXU_jvaucZgw.jpeg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk lq di lr lk">stylized image generated using right region (left) and using whole style image (right)</figcaption></figure></div><p id="66e6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如我们所见，从右侧区域生成的图像明显不同于从左侧、中间区域和完整图像生成的图像。</p><blockquote class="lw lx ly"><p id="5944" class="jn jo lz jp b jq jr js jt ju jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="iq">如果您喜欢这篇文章，请点击下面的小拍手图标帮助他人找到它。非常感谢！</em>T15】</strong></p></blockquote></div></div>    
</body>
</html>