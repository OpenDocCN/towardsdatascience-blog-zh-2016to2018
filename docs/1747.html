<html>
<head>
<title>Vehicle Detection and Tracking From a Front-Face Camera</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于前置摄像头的车辆检测和跟踪</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/vehicle-detection-and-tracking-from-a-front-face-camera-a9b83c842f58?source=collection_archive---------3-----------------------#2017-10-13">https://towardsdatascience.com/vehicle-detection-and-tracking-from-a-front-face-camera-a9b83c842f58?source=collection_archive---------3-----------------------#2017-10-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="246f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Udacity自动驾驶工程师Nanodegree —学期1，作业5。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/3ad495aea5f8028d1948cc7365d8dc58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QXjGxV4ZrJ4WbCrM5rOeXg.png"/></div></div></figure><h1 id="cc6c" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">目标</h1><p id="fc36" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">这是为Udacity自动驾驶汽车工程师Nanodegree第一学期的第五次也是最后一次任务创建的报告。挑战在于创建一种算法，利用前置摄像头获取的视频来检测道路上的其他车辆。</p><p id="2cb7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是<a class="ae ma" href="https://github.com/cacheop/CARND-AllTerms-projects/tree/master/CARND-Term1-projects/P5_CarND-Vehicle-Detection" rel="noopener ugc nofollow" target="_blank"> Github库</a>。</p><h1 id="bd1f" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">特征抽出</h1><p id="45a4" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">为了检测车辆或任何其他物体，我们需要知道它们与相机拍摄的其他图像有何不同。颜色和渐变是很好的区分因素，但最重要的特征将取决于对象的外观。</p><p id="1ecb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">颜色单独作为一个特征可能是有问题的。依赖于颜色值(或颜色直方图)的分布可能最终在图像的不需要的区域中找到匹配。渐变可以提供更强大的演示。它们在围绕中心的特定方向上的存在可以解释为形状的概念。然而，使用梯度的一个问题是，它们使签名过于敏感</p><h2 id="8a85" class="mb ky iq bd kz mc md dn ld me mf dp lh jy mg mh ll kc mi mj lp kg mk ml lt mm bi translated">HOG方向梯度直方图</h2><p id="e577" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">HOG是一种计算图像局部梯度方向出现次数的计算机视觉技术。如果我们计算每个像素的梯度<br/>大小和方向，然后将它们分组为小单元，我们可以使用这个“星形”直方图来建立该单元的主要梯度方向。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mn"><img src="../Images/67a79a6d5caa185658ed95c5c909f38f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDnYKofIo8ZwbAwLTMhsPA.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">“Star” histogram of gradient directions</figcaption></figure><p id="1faa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这样，即使很小的形状变化也会保持签名的唯一性。各种参数，如方向箱的数量、网格(单元)的大小、单元的大小、单元之间的重叠，对于微调算法非常重要。</p><h2 id="315c" class="mb ky iq bd kz mc md dn ld me mf dp lh jy mg mh ll kc mi mj lp kg mk ml lt mm bi translated">HOG、色彩空间、空间和彩色宁滨参数</h2><p id="7991" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">在最终的特征提取函数中，我们对YCrCb应用了颜色变换。然后，我们将装箱的颜色特征向量、颜色向量的直方图与HOG特征向量连接起来。对所有颜色通道进行HOG计算。</p><pre class="km kn ko kp gt ms mt mu mv aw mw bi"><span id="a821" class="mb ky iq mt b gy mx my l mz na">color_space = ‘YCrCb’ <br/>spatial_size = (16, 16)               # Spatial binning dimensions<br/>hist_bins = 32                        # Number of histogram bins<br/>orient = 9                            # HOG orientations<br/>pix_per_cell = 8                      # HOG pixels per cell<br/>cell_per_block = 2                    # HOG cells per block<br/>hog_channel = “ALL”                   # Can be 0, 1, 2, or “ALL”</span></pre><h1 id="4d53" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">训练分类器</h1><p id="6374" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">一旦提取了特征，就该建立和训练分类器了。课程中介绍的方法是建立一个分类器，可以区分汽车和非汽车图像。然后，这个分类器通过对小块进行采样而在整个图片上运行。每个补丁然后被分类为汽车或非汽车。</p><h2 id="1118" class="mb ky iq bd kz mc md dn ld me mf dp lh jy mg mh ll kc mi mj lp kg mk ml lt mm bi translated">数据集</h2><p id="7c31" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">为了训练分类器，我使用了<a class="ae ma" href="https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip" rel="noopener ugc nofollow" target="_blank">车辆</a>和<a class="ae ma" href="https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles.zip" rel="noopener ugc nofollow" target="_blank">非车辆</a>示例的标记数据。这些示例图像来自GTI车辆图像数据库、KITTI vision基准套件和从项目视频本身提取的示例的组合。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nb"><img src="../Images/c50d4262072e19b61ed43a6e47a9c243.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9dpdIYpQeEMUqAaRwbUmVg.png"/></div></div></figure><p id="005b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据被随机打乱后分成训练集和测试集，以避免数据中可能出现的排序效应<br/>。</p><p id="658c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练基本上是为训练集中的每幅图像提取特征向量。这些向量和它们各自的标签(<em class="nc">汽车</em>或<em class="nc">非汽车</em>)提供给训练算法，该算法迭代地改变模型，直到预测和实际标签之间的误差足够小。(或者在多次迭代后误差停止减小。)</p><h2 id="b39e" class="mb ky iq bd kz mc md dn ld me mf dp lh jy mg mh ll kc mi mj lp kg mk ml lt mm bi translated">归一化特征向量的大小</h2><p id="6e9e" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">在开始训练分类器之前，我们将特征向量归一化为零均值和单位方差。这是必要的，因为基于颜色的特征和基于渐变的特征之间存在大小差异，这可能会导致问题。</p><pre class="km kn ko kp gt ms mt mu mv aw mw bi"><span id="e8be" class="mb ky iq mt b gy mx my l mz na"># Fit a per-column scaler<br/>X_scaler = StandardScaler().fit(X)</span><span id="494e" class="mb ky iq mt b gy nd my l mz na"># Apply the scaler to X<br/>scaled_X = X_scaler.transform(X)</span></pre><h2 id="f3a5" class="mb ky iq bd kz mc md dn ld me mf dp lh jy mg mh ll kc mi mj lp kg mk ml lt mm bi translated">支持向量机分类器</h2><p id="c7b0" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">正如课程中所建议的，我们使用支持向量机对数据进行分类。我们也实现了一个决策树分类器，但是准确性看起来不太好，所以我们决定继续微调SVM。</p><pre class="km kn ko kp gt ms mt mu mv aw mw bi"><span id="924b" class="mb ky iq mt b gy mx my l mz na">3.26 Seconds to train SVC...<br/>Test Accuracy of SVC =  0.9926</span></pre><h1 id="46d9" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">滑动窗口方法</h1><p id="0442" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">既然我们已经训练了分类器，我们将不得不在帧中搜索汽车。前提是我们在图像中定义小块，然后对每个小块运行分类器。然后，分类器将决定该小块是否“是”汽车。</p><p id="8e11" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在滑动窗口技术中，我们在图像上定义一个网格，并在其上移动以提取训练的特征。分类器将在每一步给出预测，并判断网格元素是否包含汽车特征。</p><p id="c640" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">鉴于图像的视角，远离汽车摄像头的物体会显得较小，而靠近的汽车会显得较大。因此，网格细分根据图像上的位置考虑不同的大小是有意义的。我们也不考虑地平线以上的任何区域，忽略有天空、山脉和树木的区域。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ne"><img src="../Images/32838fd1cc836d6ec68235afeb8f03df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3flMK7yofbfVSzzkQ4iU9g.png"/></div></div></figure><h2 id="0412" class="mb ky iq bd kz mc md dn ld me mf dp lh jy mg mh ll kc mi mj lp kg mk ml lt mm bi translated">跟踪问题</h2><p id="02ef" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">不足为奇的是，分类器将返回大量的假阳性。这些区域没有汽车，但是图像中的照明或纹理会欺骗分类器称其为汽车。对于自动驾驶汽车应用程序来说，这显然是一个大问题。假阳性可能导致汽车改变方向或启动刹车，这是事故的潜在原因。</p><p id="377a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了过滤掉误报，我们记录每一帧的所有检测的位置，并与在后续帧中发现的检测进行比较。检测的群集很可能是一辆真实的汽车。另一方面，在一帧中出现而在下一帧中没有再次出现的检测将是误报。</p><h2 id="e271" class="mb ky iq bd kz mc md dn ld me mf dp lh jy mg mh ll kc mi mj lp kg mk ml lt mm bi translated">热图和边界框</h2><p id="9759" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">搜索<code class="fe nf ng nh mt b">find_cars</code>返回分类器预测包含车辆的热盒阵列。我们创建了一个热图来识别重叠盒子的集群。使用<code class="fe nf ng nh mt b">apply_threshold</code>,我们可以移除假设的误报。</p><pre class="km kn ko kp gt ms mt mu mv aw mw bi"><span id="a21e" class="mb ky iq mt b gy mx my l mz na">import collections<br/><strong class="mt ir">heatmaps</strong> = collections.deque(maxlen=10) <br/>    <br/>def process_frame(source_img):</span><span id="ea1c" class="mb ky iq mt b gy nd my l mz na">     out_img, boxes = find_cars(source_img, ystart, <br/>                           ystop, scale, svc, <br/>                           X_scaler, orient, pix_per_cell, <br/>                           cell_per_block, spatial_size, <br/>                           hist_bins, False)</span><span id="1e84" class="mb ky iq mt b gy nd my l mz na">    current_heatmap =   <br/>                 np.zeros_like(source_img[:,:,0]).astype(np.float)<br/>    current_heatmap = <strong class="mt ir">add_heat</strong>(current_heatmap, boxes)<br/>    <strong class="mt ir">heatmaps.append</strong>(current_heatmap)<br/>    heatmap_sum = sum(heatmaps)<br/>  <br/>    heat_map = <strong class="mt ir">apply_threshold</strong>(heatmap_sum, 2)<br/>    heatmap = np.clip(heat_map, 0, 255)<br/>    labels = label(heatmap)  <br/>    labeled_box_img = draw_labeled_bboxes(source_img, labels)</span><span id="0833" class="mb ky iq mt b gy nd my l mz na">return labeled_box_img</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ne"><img src="../Images/b02c11e7645d9e89fb43408eebce8bff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S8XO3DrsMRtArUdDVPzesg.png"/></div></div></figure><p id="bfb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的例子是针对单个帧的。如前所述，我们要考虑时间顺序，以避免一次性考虑。我们使用了一个<code class="fe nf ng nh mt b">deque</code>数据结构来保存最后10帧的盒子。</p><h1 id="1712" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">管道和视频</h1><p id="f827" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">总结我们的整体跟踪渠道:</p><ul class=""><li id="457a" class="ni nj iq jp b jq jr ju jv jy nk kc nl kg nm kk nn no np nq bi translated">处理带标签的数据集以<strong class="jp ir">定义训练和测试集</strong>。</li><li id="839a" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated"><strong class="jp ir">从数据集图像中提取特征</strong>。</li><li id="4533" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated"><strong class="jp ir">训练</strong>分类器。</li><li id="ded2" class="ni nj iq jp b jq nr ju ns jy nt kc nu kg nv kk nn no np nq bi translated">对于每个视频帧:使用<strong class="jp ir">滑动窗口</strong>技术运行搜索，并过滤掉<strong class="jp ir">误报</strong>。</li></ul><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nw nx l"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Generated video</figcaption></figure><h1 id="0ad2" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">考虑</h1><p id="0168" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">值得注意的是，一些检测到的是高速公路对面的汽车。虽然这不是一件坏事，但我们可以通过缩小考虑范围来避免它们。</p><p id="8ea8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当汽车进入视频帧时，检测会有一点延迟，这种情况可以通过调整平均框计算中考虑的帧数来解决。</p><p id="9165" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最近使用深度神经网络的技术可以通过提高准确性、减少假阳性的发生和提高性能来改善特征检测。微小的YOLO似乎是一种常见的方法。</p><p id="f9c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae ma" href="https://arxiv.org/abs/1704.05519" rel="noopener ugc nofollow" target="_blank"> Janai等人(2017) </a>发表了一份关于技术如何改进以及仍需解决的挑战的评估。</p><p id="231b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是一个总结第一学期的伟大项目。我可以看到我从使用OpenCV寻找车道的第一个任务<a class="ae ma" href="https://github.com/cacheop/CARND-AllTerms-projects/tree/master/CARND-Term1-projects/P1_CarND-LaneLines" rel="noopener ugc nofollow" target="_blank">中学到了多少，我期待着在第二学期学到更多。感谢出色的Udacity团队让这一切成为可能。</a></p></div></div>    
</body>
</html>