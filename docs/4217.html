<html>
<head>
<title>Industrial Classification of Websites by Machine Learning with hands-on Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用机器学习和动手 Python 对网站进行行业分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/industrial-classification-of-websites-by-machine-learning-with-hands-on-python-3761b1b530f1?source=collection_archive---------2-----------------------#2018-07-30">https://towardsdatascience.com/industrial-classification-of-websites-by-machine-learning-with-hands-on-python-3761b1b530f1?source=collection_archive---------2-----------------------#2018-07-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="dbd2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嘿伙计们，欢迎来到我的第一个技术教程。在本教程中，我想解释提取，清理和分类网站到不同的类别。我会用 python 环境运行我的代码进行数据抓取，用神经网络对网站进行分类。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/8d46049d538a21259be7acfc001743bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RWAmxUsenPRcXDtFVeRaYg.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Text classification</figcaption></figure><p id="2fe5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">文本分类是在数据科学的许多不同领域中广泛使用的自然语言处理任务之一。一个有效的文本分类器可以使用 NLP 算法有效地自动将数据分类。</p><p id="953b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">文本分类</strong>是监督机器学习任务的一个例子，因为包含文本文档及其标签的标记数据集用于训练分类器。</p><p id="c41a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一些常见的文本分类技术有:</p><ol class=""><li id="08e5" class="lb lc iq jp b jq jr ju jv jy ld kc le kg lf kk lg lh li lj bi translated"><a class="ae lk" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯分类器</a></li><li id="f259" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated"><a class="ae lk" href="https://en.wikipedia.org/wiki/Linear_classifier" rel="noopener ugc nofollow" target="_blank">线性分类器</a></li><li id="f661" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated"><a class="ae lk" href="https://en.wikipedia.org/wiki/Support_vector_machine" rel="noopener ugc nofollow" target="_blank">支持向量机</a></li><li id="3ea9" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated"><a class="ae lk" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" rel="noopener ugc nofollow" target="_blank">装袋模型</a></li><li id="880c" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated"><a class="ae lk" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">增压车型</a></li><li id="6387" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated"><a class="ae lk" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank">深度神经网络</a></li></ol><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/a1b220d1100ca79614fad3eb3dfee5df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EEUIKPkdqR2O2lggQ4fA6g.jpeg"/></div></div></figure><p id="b5d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lk" href="https://en.wikipedia.org/wiki/Web_scraping" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">网页抓取</strong> </a>、<strong class="jp ir">网页抓取</strong>或<strong class="jp ir">网页数据提取</strong>是用于从网站中提取数据的数据抓取。一般来说，这是通过模拟人类网上冲浪的软件从不同的网站收集特定的信息来完成的。</p><p id="b00b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可用于网页抓取的一些技术有:</p><ol class=""><li id="a85f" class="lb lc iq jp b jq jr ju jv jy ld kc le kg lf kk lg lh li lj bi translated">人类复制粘贴</li><li id="0ae9" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated">文本模式匹配</li><li id="4be2" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated">HTTP 编程</li><li id="345c" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated">HTML 解析</li><li id="57c2" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated">DOM 解析</li><li id="80a0" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated">垂直聚集</li><li id="b4ba" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated">语义标注识别</li><li id="dbb3" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated">计算机视觉网页分析</li></ol><p id="0c6e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本教程中，我们将尝试将完整的模型实现为三个不同的模块:</p><ol class=""><li id="24ca" class="lb lc iq jp b jq jr ju jv jy ld kc le kg lf kk lg lh li lj bi translated">数据抓取</li><li id="0585" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated">用于创建训练数据集的基于关键字的分类</li><li id="c716" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated">将神经网络应用于实际测试模型</li></ol><h1 id="a482" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">模块 1:数据搜集</h1><p id="367f" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">在本模块中，我将使用 Python 3.5 环境来实现我的脚本。因此，请跟随以获取完整的参考资料。</p><h2 id="7780" class="mt lr iq bd ls mu mv dn lw mw mx dp ma jy my mz me kc na nb mi kg nc nd mm ne bi translated">步骤 1:从网站请求数据</h2><p id="e43a" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">为了提取 web 数据，可以使用许多不同的包，但是在本教程中，我将使用<a class="ae lk" href="https://pypi.org/project/requests/" rel="noopener ugc nofollow" target="_blank"> <em class="nf">请求</em> </a> <em class="nf">。</em></p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="ec53" class="mt lr iq nh b gy nl nm l nn no"><strong class="nh ir">import</strong> <strong class="nh ir">requests<br/>url='</strong>https://medium.com/<strong class="nh ir">'<br/>try:<br/>    </strong>page = requests.get(url)        #to extract page from website<br/>    html_code = page.content        #to extract html code from page<br/>except <strong class="nh ir">Exception</strong> <strong class="nh ir">as</strong> e:<br/>    print(e)</span></pre><p id="cc18" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的代码中，requests.get()方法将使用<em class="nf"> https </em>协议从网站请求页面，并将页面加载到对象“<em class="nf"> page”中。</em>下一行代码将把 HTML 代码移动到字符串<em class="nf"> html_code 中。</em>因此，到目前为止，我们已经从网站上提取了数据，但它仍然是 HTML 格式，与实际文本相差甚远。</p><h2 id="8c64" class="mt lr iq bd ls mu mv dn lw mw mx dp ma jy my mz me kc na nb mi kg nc nd mm ne bi translated">步骤 2:从 HTML 页面中提取文本</h2><p id="1aef" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">为了从 HTML 页面中提取完整的文本数据，我们有两个非常受欢迎的包，<a class="ae lk" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir">beautiful soup</strong></a>和<a class="ae lk" href="https://pypi.org/project/html2text/" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> html2text </strong> </a>。使用上一步找到的 html_code 字符串，我们可以应用下面两种方法中的任何一种。</p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="3779" class="mt lr iq nh b gy nl nm l nn no"><strong class="nh ir">from</strong> <strong class="nh ir">bs4</strong> <strong class="nh ir">import</strong> BeautifulSoup<br/>try:<br/>    soup = BeautifulSoup(html_code, 'html.parser')  #Parse html code<br/>    texts = soup.findAll(text=<strong class="nh ir">True</strong>)                 #find all text<br/>    text_from_html = <!-- -->' '.join(texts)                   #join all text<br/>except <strong class="nh ir">Exception</strong> <strong class="nh ir">as</strong> e:<br/>    print(e)</span></pre><p id="431d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的代码片段中，<strong class="jp ir"> BeautifulSoup </strong>包将解析 HTML 代码并将数据分配给<em class="nf"> soup </em>对象。findall <em class="nf"> () </em>函数从代码中找到所有可见的文本，并返回一个字符串列表，我们将它存储在<em class="nf">文本中。</em>最后，我们使用<em class="nf"> join() </em>函数将所有单独的文本连接成一个公共字符串。</p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="87c8" class="mt lr iq nh b gy nl nm l nn no"><strong class="nh ir">import</strong> <strong class="nh ir">html2text<br/></strong>h = html2text.HTML2Text()                 #Initializing object<br/>h.ignore_links = <strong class="nh ir">True                     </strong>#Giving attributes <strong class="nh ir"><br/>try:<br/>    </strong>text = h.handle(html_code)            #handling the HTML code <br/>    text_from_html=text.replace("<strong class="nh ir">\n</strong>"," ") #replacing next line char<br/>except <strong class="nh ir">Exception</strong> <strong class="nh ir">as</strong> e:<br/>    print(e)</span></pre><p id="b921" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个替换块中，我们使用 package <strong class="jp ir"> html2text </strong>来解析字符串，并直接从 html 代码中获取文本。我们还需要用空格替换空行，最后找到<em class="nf"> text_from_html。</em></p><p id="6b87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">类似地，我们可以在大约 1000 多个 URL 上使用循环，并从这些站点提取数据，并将它们存储为 csv(逗号分隔文件)格式，我们可以在分类模块中进一步使用该格式。</p><h1 id="8696" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated"><strong class="ak">模块 2:基于关键字的分类</strong></h1><p id="f9fa" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">对于任何机器学习算法，我们都需要一些训练集和测试集来训练模型和测试模型的准确性。因此，为了创建模型的数据集，我们已经有了来自不同网站的文本，我们将根据关键字对它们进行分类，然后在下一个模块中应用结果。</p><p id="e878" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本教程中，我们将把网站分为三类，即:</p><ol class=""><li id="8acf" class="lb lc iq jp b jq jr ju jv jy ld kc le kg lf kk lg lh li lj bi translated">技术、办公和教育产品网站(Class_1)</li><li id="474d" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated">消费品网站(Class_2)</li><li id="f532" class="lb lc iq jp b jq ll ju lm jy ln kc lo kg lp kk lg lh li lj bi translated">工业工具和五金产品网站(Class_3)</li></ol><p id="069a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里的方法是，我们将拥有属于特定类别的某些关键字，我们将这些关键字与文本进行匹配，并找到具有最大<em class="nf"> Matching_value 的类别。</em></p><blockquote class="np nq nr"><p id="fc14" class="jn jo nf jp b jq jr js jt ju jv jw jx ns jz ka kb nt kd ke kf nu kh ki kj kk ij bi translated"><em class="iq"> Matching_value </em> =(与一个行业匹配的关键字数)/(匹配的关键字总数)</p></blockquote><p id="7179" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，相应地，我们有一个单独类别的关键字列表，如下所示:</p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="1e35" class="mt lr iq nh b gy nl nm l nn no">Class_1_keywords = ['Office', 'School', 'phone', 'Technology', 'Electronics', 'Cell', 'Business', 'Education', 'Classroom']</span><span id="eb61" class="mt lr iq nh b gy nv nm l nn no">Class_1_keywords = ['Restaurant', 'Hospitality', 'Tub', 'Drain', 'Pool', 'Filtration', 'Floor', 'Restroom', 'Consumer', 'Care', 'Bags', 'Disposables']</span><span id="c5d6" class="mt lr iq nh b gy nv nm l nn no">Class_3_keywords = ['Pull', 'Lifts', 'Pneumatic', 'Emergency', 'Finishing', 'Hydraulic', 'Lockout', 'Towers', 'Drywall', 'Tools', 'Packaging', 'Measure', 'Tag ']</span><span id="4b4d" class="mt lr iq nh b gy nv nm l nn no">keywords=Class_1_keywords + Class_2_keywords + Class_3_keywords</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/599a60a6e7e2a47b69cf717d505612d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VFncY3rfxgnl_no4EOMcPQ.jpeg"/></div></div></figure><p id="e0f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们将使用<a class="ae lk" href="https://pypi.org/project/flashtext/" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">关键字处理器</strong> </a>来查找从 URL 接收的文本中的关键字。</p><p id="3b2c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lk" href="https://pypi.org/project/flashtext/" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">关键字处理器</strong> </a> <strong class="jp ir"> </strong>在 pypi 上的 flashtext 包中有。</p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="aaaf" class="mt lr iq nh b gy nl nm l nn no"><strong class="nh ir">from</strong> <strong class="nh ir">flashtext.keyword</strong> <strong class="nh ir">import</strong> KeywordProcessor<br/>kp0=KeywordProcessor()<br/><strong class="nh ir">for</strong> word <strong class="nh ir">in</strong> keywords:<br/>    kp0.add_keyword(word)</span><span id="ee52" class="mt lr iq nh b gy nv nm l nn no">kp1=KeywordProcessor()<br/><strong class="nh ir">for</strong> word <strong class="nh ir">in</strong> Class_1_keywords:<br/>    kp1.add_keyword(word)</span><span id="fd52" class="mt lr iq nh b gy nv nm l nn no">kp2=KeywordProcessor()<br/><strong class="nh ir">for</strong> word <strong class="nh ir">in</strong> Class_2_keywords:<br/>    kp2.add_keyword(word)</span><span id="4ebd" class="mt lr iq nh b gy nv nm l nn no">kp3=KeywordProcessor()<br/><strong class="nh ir">for</strong> word <strong class="nh ir">in</strong> Class_3_keywords:<br/>    kp3.add_keyword(word)</span></pre><p id="99fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的代码中，我们将使用关键字加载<strong class="jp ir"> KeywordProcessor </strong>对象，我们将进一步使用这些关键字来查找匹配的关键字。</p><p id="f466" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了找到<em class="nf">匹配值</em>的百分比值，我们定义一个函数百分比如下:</p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="c434" class="mt lr iq nh b gy nl nm l nn no"><strong class="nh ir">def</strong> percentage1(dum0,dumx):<br/>    <strong class="nh ir">try</strong>:<br/>        ans=float(dumx)/float(dum0)<br/>        ans=ans*100<br/>    <strong class="nh ir">except</strong>:<br/>        <strong class="nh ir">return</strong> 0<br/>    <strong class="nh ir">else</strong>:<br/>        <strong class="nh ir">return</strong> ans</span></pre><p id="1cf8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在将使用 extract_keywords(string)方法来查找文本中出现的关键字。我们将查找该列表的长度，以找到文本中匹配关键字的数量。以下函数将查找百分比，并选择百分比最大的类。</p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="cd74" class="mt lr iq nh b gy nl nm l nn no">def find_class:<br/>    x=str(text_from_html)<br/>    y0 = len(kp0.extract_keywords(x))<br/>    y1 = len(kp1.extract_keywords(x))<br/>    y2 = len(kp2.extract_keywords(x))<br/>    y3 = len(kp3.extract_keywords(x))<br/>    Total_matches=y0   <br/>    per1 = float(percentage1(y0,y1))<br/>    per2 = float(percentage1(y0,y2))<br/>    per3 = float(percentage1(y0,y3))<br/>    <strong class="nh ir">if</strong> y0==0:<br/>        Category='None'<br/>    <strong class="nh ir">else</strong>:<br/>        <strong class="nh ir">if</strong> per1&gt;=per2 <strong class="nh ir">and</strong> per1&gt;=per3:<br/>            Category='Class_1'<br/>        <strong class="nh ir">elif</strong> per2&gt;=per3 <strong class="nh ir">and</strong> per2&gt;=per1:<br/>            Category='Class_2'<br/>        <strong class="nh ir">elif</strong> per3&gt;=per1 <strong class="nh ir">and</strong> per3&gt;=per2:<br/>            Category='Class_3'<br/>    return Category</span></pre><p id="8218" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的函数中使用一个循环，我们可以根据关键字找到所有网站的类别。我们将分类数据保存到文件<em class="nf"> Data.csv </em>中，我们将进一步使用该文件。因此，现在我们已经为应用神经网络进行分类准备好了数据集。</p><h1 id="9da0" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">模块 3:应用神经网络</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nw"><img src="../Images/d8a4e13dbc9b86b5a0820fa226a57652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P1Cxt9uu8RkD2YZWqbLrWw.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Classification of websites</figcaption></figure><p id="b090" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在下面的实现中，我们将从头开始创建一个神经网络，并将使用 NLTK 单词标记化器进行预处理。首先，我们需要导入从上述步骤中获得的数据集，并将其加载到一个列表中。</p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="1ab4" class="mt lr iq nh b gy nl nm l nn no"><strong class="nh ir">import</strong> <strong class="nh ir">pandas</strong> <strong class="nh ir">as</strong> <strong class="nh ir">pd<br/></strong>data=pd.read_csv('Data.csv')<br/>data = data[pd.notnull(data['tokenized_source'])]<br/>data=data[data.Category != 'None']</span></pre><p id="1a89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的代码将加载和清理分类数据。空值将被删除。</p><p id="24b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面的代码将根据它的类创建一个数据字典。</p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="3444" class="mt lr iq nh b gy nl nm l nn no"><strong class="nh ir">for</strong> index,row <strong class="nh ir">in</strong> data.iterrows():<br/>    train_data.append({"class":row["Category"], "sentence":row["text"]})</span></pre><p id="ad5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了应用神经网络，我们需要将语言文字转换成数学符号，用于计算。我们将列出所有字符串中的所有单词。</p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="af72" class="mt lr iq nh b gy nl nm l nn no">words = []<br/>classes = []<br/>documents = []<br/>ignore_words = ['?']<br/><em class="nf"># loop through each sentence in our training data</em><br/><strong class="nh ir">for</strong> pattern <strong class="nh ir">in</strong> training_data:<br/>    <em class="nf"># tokenize each word in the sentence</em><br/>    w = nltk.word_tokenize(pattern['sentence'])<br/>    <em class="nf"># add to our words list</em><br/>    words.extend(w)<br/>    <em class="nf"># add to documents in our corpus</em><br/>    documents.append((w, pattern['class']))<br/>    <em class="nf"># add to our classes list</em><br/>    <strong class="nh ir">if</strong> pattern['class'] <strong class="nh ir">not</strong> <strong class="nh ir">in</strong> classes:<br/>        classes.append(pattern['class'])<br/><br/><em class="nf"># stem and lower each word and remove duplicates</em><br/>words = [stemmer.stem(w.lower()) <strong class="nh ir">for</strong> w <strong class="nh ir">in</strong> words <strong class="nh ir">if</strong> w <strong class="nh ir">not</strong> <strong class="nh ir">in</strong> ignore_words]<br/>words = list(set(words))<br/><br/><em class="nf"># remove duplicates</em><br/>classes = list(set(classes))<br/><br/>print (len(documents), "documents")<br/>print (len(classes), "classes", classes)<br/><em class="nf">print (len(words), "unique stemmed words", words)</em></span></pre><p id="a39a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，输出将是:</p><blockquote class="np nq nr"><p id="864a" class="jn jo nf jp b jq jr js jt ju jv jw jx ns jz ka kb nt kd ke kf nu kh ki kj kk ij bi translated">1594 个文档<br/> 3 个类['Class_1 '，' Class_3 '，' Class_2'] <br/> <em class="iq">唯一词干 40000 个</em></p></blockquote><p id="01ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们将为该模式创建一个<em class="nf">单词列表，并使用 NLTK Lancaster 斯特梅尔创建一个单词包。</em></p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="d503" class="mt lr iq nh b gy nl nm l nn no"><strong class="nh ir">from</strong> <strong class="nh ir">nltk.stem.lancaster</strong> <strong class="nh ir">import</strong> LancasterStemmer</span><span id="0c12" class="mt lr iq nh b gy nv nm l nn no">stemmer = LancasterStemmer()<br/><em class="nf"># create our training data</em><br/>training = []<br/>output = []<br/><em class="nf"># create an empty array for our output</em><br/>output_empty = [0] * len(classes)<br/><br/><em class="nf"># training set, bag of words for each sentence</em><br/><strong class="nh ir">for</strong> doc <strong class="nh ir">in</strong> documents:<br/>    <em class="nf"># initialize our bag of words</em><br/>    bag = []<br/>    <em class="nf"># list of tokenized words for the pattern</em><br/>    pattern_words = doc[0]<br/>    <em class="nf"># stem each word</em><br/>    pattern_words = [stemmer.stem(word.lower()) <strong class="nh ir">for</strong> word <strong class="nh ir">in</strong> pattern_words]<br/>    <em class="nf"># create our bag of words array</em><br/>    <strong class="nh ir">for</strong> w <strong class="nh ir">in</strong> words:<br/>        bag.append(1) <strong class="nh ir">if</strong> w <strong class="nh ir">in</strong> pattern_words <strong class="nh ir">else</strong> bag.append(0)<br/><br/>    training.append(bag)<br/>    <em class="nf"># output is a '0' for each tag and '1' for current tag</em><br/>    output_row = list(output_empty)<br/>    output_row[classes.index(doc[1])] = 1<br/>    output.append(output_row)<br/><br/>print ("# words", len(words))<br/>print ("# classes", len(classes))</span></pre><p id="49d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输出:</p><blockquote class="np nq nr"><p id="eebc" class="jn jo nf jp b jq jr js jt ju jv jw jx ns jz ka kb nt kd ke kf nu kh ki kj kk ij bi translated"># word 41468<br/># class 3</p></blockquote><p id="6551" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们对数据进行最后的预处理，并创建一些函数。</p><p id="eb8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">乙状结肠功能</strong></p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="b0f4" class="mt lr iq nh b gy nl nm l nn no"><strong class="nh ir">def</strong> sigmoid(x):<br/>    output = 1/(1+np.exp(-x))<br/>    <strong class="nh ir">return</strong> output<br/><br/><em class="nf"># convert output of sigmoid function to its derivative</em><br/><strong class="nh ir">def</strong> sigmoid_output_to_derivative(output):<br/>    <strong class="nh ir">return</strong> output*(1-output)</span></pre><p id="b7b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">清洁功能</strong></p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="4022" class="mt lr iq nh b gy nl nm l nn no"><strong class="nh ir">def</strong> clean_up_sentence(sentence):<br/>    <em class="nf"># tokenize the pattern</em><br/>    sentence_words = nltk.word_tokenize(sentence)<br/>    <em class="nf"># stem each word</em><br/>    sentence_words = [stemmer.stem(word.lower()) <strong class="nh ir">for</strong> word <strong class="nh ir">in</strong> sentence_words]<br/>    <strong class="nh ir">return</strong> sentence_words</span></pre><p id="1146" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">包话功能</strong></p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="5cd4" class="mt lr iq nh b gy nl nm l nn no"><strong class="nh ir">def</strong> bow(sentence, words, show_details=<strong class="nh ir">False</strong>):<br/>    <em class="nf"># tokenize the pattern</em><br/>    sentence_words = clean_up_sentence(sentence)<br/>    <em class="nf"># bag of words</em><br/>    bag = [0]*len(words)  <br/>    <strong class="nh ir">for</strong> s <strong class="nh ir">in</strong> sentence_words:<br/>        <strong class="nh ir">for</strong> i,w <strong class="nh ir">in</strong> enumerate(words):<br/>            <strong class="nh ir">if</strong> w == s: <br/>                bag[i] = 1<br/>                <strong class="nh ir">if</strong> show_details:<br/>                    print ("found in bag: <strong class="nh ir">%s</strong>" % w)<br/><br/>    <strong class="nh ir">return</strong>(np.array(bag))</span></pre><p id="f6b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最终将用于神经网络的函数:<strong class="jp ir">思考函数</strong></p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="9bee" class="mt lr iq nh b gy nl nm l nn no"><strong class="nh ir">def</strong> think(sentence, show_details=<strong class="nh ir">False</strong>):<br/>    x = bow(sentence.lower(), words, show_details)<br/>    <strong class="nh ir">if</strong> show_details:<br/>        print ("sentence:", sentence, "<strong class="nh ir">\n</strong> bow:", x)<br/>    <em class="nf"># input layer is our bag of words</em><br/>    l0 = x<br/>    <em class="nf"># matrix multiplication of input and hidden layer</em><br/>    l1 = sigmoid(np.dot(l0, synapse_0))<br/>    <em class="nf"># output layer</em><br/>    l2 = sigmoid(np.dot(l1, synapse_1))<br/>    <strong class="nh ir">return</strong> l2</span></pre><p id="2100" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们已经准备好训练我们的神经网络模型。我们将通过 scratch 实现它，并将对每个神经元使用逻辑回归。只有一层，但有 50000 个历元，我们将训练我们的模型。完整的训练示例将在 CPU 上运行。</p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="192b" class="mt lr iq nh b gy nl nm l nn no"><strong class="nh ir">def</strong> train(X, y, hidden_neurons=10, alpha=1, epochs=50000, dropout=<strong class="nh ir">False</strong>, dropout_percent=0.5):<br/><br/>    print ("Training with <strong class="nh ir">%s</strong> neurons, alpha:<strong class="nh ir">%s</strong>, dropout:<strong class="nh ir">%s</strong> <strong class="nh ir">%s</strong>" % (hidden_neurons, str(alpha), dropout, dropout_percent <strong class="nh ir">if</strong> dropout <strong class="nh ir">else</strong> '') )<br/>    print ("Input matrix: <strong class="nh ir">%s</strong>x<strong class="nh ir">%s</strong>    Output matrix: <strong class="nh ir">%s</strong>x<strong class="nh ir">%s</strong>" % (len(X),len(X[0]),1, len(classes)) )<br/>    np.random.seed(1)<br/><br/>    last_mean_error = 1<br/>    <em class="nf"># randomly initialize our weights with mean 0</em><br/>    synapse_0 = 2*np.random.random((len(X[0]), hidden_neurons)) - 1<br/>    synapse_1 = 2*np.random.random((hidden_neurons, len(classes))) - 1<br/><br/>    prev_synapse_0_weight_update = np.zeros_like(synapse_0)<br/>    prev_synapse_1_weight_update = np.zeros_like(synapse_1)<br/><br/>    synapse_0_direction_count = np.zeros_like(synapse_0)<br/>    synapse_1_direction_count = np.zeros_like(synapse_1)<br/>        <br/>    <strong class="nh ir">for</strong> j <strong class="nh ir">in</strong> iter(range(epochs+1)):<br/><br/>        <em class="nf"># Feed forward through layers 0, 1, and 2</em><br/>        layer_0 = X<br/>        layer_1 = sigmoid(np.dot(layer_0, synapse_0))<br/>                <br/>        <strong class="nh ir">if</strong>(dropout):<br/>            layer_1 *= np.random.binomial([np.ones((len(X),hidden_neurons))],1-dropout_percent)[0] * (1.0/(1-dropout_percent))<br/><br/>        layer_2 = sigmoid(np.dot(layer_1, synapse_1))<br/><br/>        <em class="nf"># how much did we miss the target value?</em><br/>        layer_2_error = y - layer_2<br/><br/>        <strong class="nh ir">if</strong> (j% 10000) == 0 <strong class="nh ir">and</strong> j &gt; 5000:<br/>            <em class="nf"># if this 10k iteration's error is greater than the last iteration, break out</em><br/>            <strong class="nh ir">if</strong> np.mean(np.abs(layer_2_error)) &lt; last_mean_error:<br/>                print ("delta after "+str(j)+" iterations:" + str(np.mean(np.abs(layer_2_error))) )<br/>                last_mean_error = np.mean(np.abs(layer_2_error))<br/>            <strong class="nh ir">else</strong>:<br/>                print ("break:", np.mean(np.abs(layer_2_error)), "&gt;", last_mean_error )<br/>                <strong class="nh ir">break</strong><br/>                <br/>        <em class="nf"># in what direction is the target value?</em><br/>        <em class="nf"># were we really sure? if so, don't change too much.</em><br/>        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)<br/><br/>        <em class="nf"># how much did each l1 value contribute to the l2 error (according to the weights)?</em><br/>        layer_1_error = layer_2_delta.dot(synapse_1.T)<br/><br/>        <em class="nf"># in what direction is the target l1?</em><br/>        <em class="nf"># were we really sure? if so, don't change too much.</em><br/>        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)<br/>        <br/>        synapse_1_weight_update = (layer_1.T.dot(layer_2_delta))<br/>        synapse_0_weight_update = (layer_0.T.dot(layer_1_delta))<br/>        <br/>        <strong class="nh ir">if</strong>(j &gt; 0):<br/>            synapse_0_direction_count += np.abs(((synapse_0_weight_update &gt; 0)+0) - ((prev_synapse_0_weight_update &gt; 0) + 0))<br/>            synapse_1_direction_count += np.abs(((synapse_1_weight_update &gt; 0)+0) - ((prev_synapse_1_weight_update &gt; 0) + 0))        <br/>        <br/>        synapse_1 += alpha * synapse_1_weight_update<br/>        synapse_0 += alpha * synapse_0_weight_update<br/>        <br/>        prev_synapse_0_weight_update = synapse_0_weight_update<br/>        prev_synapse_1_weight_update = synapse_1_weight_update<br/><br/>    now = datetime.datetime.now()<br/><br/>    <em class="nf"># persist synapses</em><br/>    synapse = {'synapse0': synapse_0.tolist(), 'synapse1': synapse_1.tolist(),<br/>               'datetime': now.strftime("%Y-%m-<strong class="nh ir">%d</strong> %H:%M"),<br/>               'words': words,<br/>               'classes': classes<br/>              }<br/>    synapse_file = "synapses.json"<br/><br/>    <strong class="nh ir">with</strong> open(folder_path+synapse_file, 'w') <strong class="nh ir">as</strong> outfile:<br/>        json.dump(synapse, outfile, indent=4, sort_keys=<strong class="nh ir">True</strong>)<br/>    print ("saved synapses to:", synapse_file)</span></pre><p id="8edd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们将训练模型:</p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="33f5" class="mt lr iq nh b gy nl nm l nn no">import time<br/>X = np.array(training)<br/>y = np.array(output)<br/><br/>start_time = time.time()<br/><br/>train(X, y, hidden_neurons=10, alpha=0.1, epochs=50000, dropout=<strong class="nh ir">False</strong>, dropout_percent=0.2)<br/><br/>elapsed_time = time.time() - start_time<br/>print ("processing time:", elapsed_time, "seconds")</span></pre><p id="0c39" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输出:</p><blockquote class="np nq nr"><p id="e407" class="jn jo nf jp b jq jr js jt ju jv jw jx ns jz ka kb nt kd ke kf nu kh ki kj kk ij bi translated">用 10 个神经元训练，alpha:0.1，dropout:False <br/>输入矩阵:1594x41468 输出矩阵:1 x3<br/>10000 次迭代后的 delta:0.0665105275385<br/>20000 次迭代后的 delta:0.0610711168863<br/>30000 次迭代后的 delta:0.0561908365355<br/>40000 次迭代后的 delta</p></blockquote><p id="6cc9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们所看到的，训练这个模型花了将近 11 个小时。在如此密集的计算之后，我们准备测试数据。</p><p id="31e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">测试数据的函数:</p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="4903" class="mt lr iq nh b gy nl nm l nn no"><em class="nf"># probability threshold</em><br/>ERROR_THRESHOLD = 0.2<br/><em class="nf"># load our calculated synapse values</em><br/>synapse_file = 'synapses.json' <br/><strong class="nh ir">with</strong> open(synapse_file) <strong class="nh ir">as</strong> data_file: <br/>    synapse = json.load(data_file) <br/>    synapse_0 = np.asarray(synapse['synapse0']) <br/>    synapse_1 = np.asarray(synapse['synapse1'])<br/><br/><strong class="nh ir">def</strong> classify(sentence, show_details=<strong class="nh ir">False</strong>):<br/>    results = think(sentence, show_details)<br/><br/>    results = [[i,r] <strong class="nh ir">for</strong> i,r <strong class="nh ir">in</strong> enumerate(results) <strong class="nh ir">if</strong> r&gt;ERROR_THRESHOLD ] <br/>    results.sort(key=<strong class="nh ir">lambda</strong> x: x[1], reverse=<strong class="nh ir">True</strong>) <br/>    return_results =[[classes[r[0]],r[1]] <strong class="nh ir">for</strong> r <strong class="nh ir">in</strong> results]<br/>    <em class="nf">#print ("\n classification: %s" % ( return_results))</em><br/>    <strong class="nh ir">return</strong> return_results</span></pre><p id="010a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们测试模型的准确性:</p><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="4071" class="mt lr iq nh b gy nl nm l nn no">classify("Switchboards Help KA36200 About Us JavaScript seems to be disabled in your browser You must have JavaScript enabled in your browser to utilize the functionality of this website Help Shopping Cart 0 00 You have no items in your shopping cart My Account My Wishlist My Cart My Quote Log In BD Electrical Worldwide Supply Remanufacturing the past SUSTAINING THE FUTURE Hours and Location Michigan Howell")</span></pre><p id="ef15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输出:</p><blockquote class="np nq nr"><p id="b21f" class="jn jo nf jp b jq jr js jt ju jv jw jx ns jz ka kb nt kd ke kf nu kh ki kj kk ij bi translated">[['Class_3 '，0.9766343788614435]]</p></blockquote><pre class="km kn ko kp gt ng nh ni nj aw nk bi"><span id="d5d9" class="mt lr iq nh b gy nl nm l nn no">classify("  New Website Testimonial Policies Parts Catalog Contact Support Forum Documentation Themes WordPress Blog Products Spindle Parts Latest News Kennard Parts Suggest Ideas Legal/Disclaimers WordPress Planet News About CDT Home Latest News Testimonial Products Parts Catalog About CDT History Staff Policies Centrum Legal Disclaimers Contact About CDT Custom Drilling Technologies established in 1990 has been providing superior customer service to the printed circuit board industry for almost 20 years We specialize in Excellon Drilling and Routing Equipment Parts and Service Our staff has over sixty years of combined experience in the design building troubleshooting operation programming")</span></pre><p id="aae4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输出:</p><blockquote class="np nq nr"><p id="5cea" class="jn jo nf jp b jq jr js jt ju jv jw jx ns jz ka kb nt kd ke kf nu kh ki kj kk ij bi translated">[['Class_1 '，0.96297535870017]]</p></blockquote><p id="d3c1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如你所见，我们在这些测试中获得了相当高的准确度。我在不同的数据上尝试了这个模型，发现它有相当高的准确性。</p><p id="ead0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这种只有一层的模型中，大约 95%以上的精度被认为是非常准确的。对于不同模型的进一步分类，我们可以使用 Keras 或 Tensorflow。为了减少训练模型的时间，我们可以使用 NVIDIA GPU。</p><p id="bd70" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，借助反向传播的深度神经网络，我们可以很容易地收集数据并对其进行分类。</p><p id="4568" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在后续教程中，我将尝试解释 Keras 和 Tensorflow 的工作和实践。</p><p id="d557" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请在下面的评论区或通过我的 LinkedIn 页面分享您对本教程的反馈:<a class="ae lk" href="https://www.linkedin.com/in/ridhamdave/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/ridhamdave/</a>。也分享一下你对这个教程的疑惑。</p></div></div>    
</body>
</html>