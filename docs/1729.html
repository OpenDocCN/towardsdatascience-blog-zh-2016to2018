<html>
<head>
<title>Artificial Intelligence: Hyperparameters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能:超参数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/artificial-intelligence-hyperparameters-48fa29daa516?source=collection_archive---------4-----------------------#2017-10-11">https://towardsdatascience.com/artificial-intelligence-hyperparameters-48fa29daa516?source=collection_archive---------4-----------------------#2017-10-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="342d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">深度学习神经网络<strong class="jp ir">模型</strong>有很多<strong class="jp ir">参数</strong>(如权重和偏差)，也有不少<a class="ae kl" href="https://en.wikipedia.org/wiki/Hyperparameter" rel="noopener ugc nofollow" target="_blank">超参数</a>。我们从高中就知道什么是<strong class="jp ir">参数</strong>。它们是你插入到函数中的数字。但是什么是<a class="ae kl" href="https://www.youtube.com/watch?v=ttE0F7fghfk" rel="noopener ugc nofollow" target="_blank">超参数</a>？嗯，它们基本上是用来创建包含参数的模型的选项。</p><p id="492f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">学习哪个超参数设置适合哪个模型行为需要一些时间。谢天谢地，keras的默认值是一个很好的起点。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/0e85546960a8e5fa8cb62b4afca3fb71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/0*xMWaKlmLBHcB4AfP.jpg"/></div></figure><p id="2cb1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，让我们从<a class="ae kl" href="https://medium.com/towards-data-science/artificial-intelligence-get-your-users-to-label-your-data-b5fa7c0c9e00" rel="noopener">以前的文章</a>中记住，我们的深度学习模型正在试图逼近一个函数<strong class="jp ir"> <em class="ku"> f </em> </strong>，该函数将输入特征<strong class="jp ir"> <em class="ku"> X </em> </strong>映射到输出决策<strong class="jp ir"> <em class="ku"> Y </em> </strong>。或者，换一种说法，我们在试图寻找一个误差很小，但不需要记忆训练数据的函数<strong class="jp ir"><em class="ku">Y = f(X)</em></strong>(<a class="ae kl" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank">过拟合</a>)。</p><p id="e858" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">我们的模型中的参数</strong>像<a class="ae kl" href="https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/" rel="noopener ugc nofollow" target="_blank">权重和偏差</a>在参数优化过程中被调整(即反向传播)以得到我们的映射函数<strong class="jp ir"> <em class="ku"> f </em> </strong>的越来越好的版本。基于<a class="ae kl" href="https://www.youtube.com/watch?v=bLqJHjXihK8&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank">超级令人兴奋的最新工作</a>，事实证明，当神经网络学习逼近<strong class="jp ir"> <em class="ku"> f </em> </strong>时，它们实际上在做一种信息压缩。这将我们的老朋友<a class="ae kl" href="https://en.wikipedia.org/wiki/Information_theory" rel="noopener ugc nofollow" target="_blank">信息论</a>与我们的新朋友深度神经网络联系起来。</p><p id="e8be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="http://colinraffel.com/wiki/neural_network_hyperparameters" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">超参数</strong> </a>是元设置，我们可以选择(希望以某种聪明的方式)来调整模型如何形成<strong class="jp ir"> <em class="ku"> f </em> </strong>。换句话说，我们设置超参数是为了挑选我们想要的模型类型。例如，<a class="ae kl" href="https://distill.pub/2016/misread-tsne/" rel="noopener ugc nofollow" target="_blank"> t-SNE有超参数设置</a>称为困惑，ε(学习率)，<a class="ae kl" href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE" rel="noopener ugc nofollow" target="_blank">和其他几个</a>，像迭代次数。</p><p id="8082" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想想配置和构建深度学习模型，比如<strong class="jp ir">点寿司</strong>:寿司是成卷的。有的卷8块，有的卷6、4块。控制味道的超参数，以及当你从菜单上要一个面包卷时，你会得到多少个面包卷，就是面包卷类型。你可以选择辣味卷、蔬菜卷、油炸卷等等。在任何情况下，你都可以吃到寿司。改变的是寿司师傅用来制作寿司的配置。每卷味道都不一样。回到机器学习领域，当我们选择超参数时，我们需要做出一些非常大的决定(例如回归器或分类器，CNN或LSTM或DNN或甘)，以及许多小决定(例如批量大小，测试/训练分割，正则化，辍学，噪声等)。在某些情况下，预训练的神经网络(如VGG-19)或预定义的神经网络形状(如自动编码器)会比从头开始更接近解决方案。对于完全定制的神经网络配置，我们在<a class="ae kl" href="https://keras.io/regularizers/" rel="noopener ugc nofollow" target="_blank"> keras </a>中获得了许多超酷的超参数选项，如L1和L2的正则化、DNN层宽度、网络形状(自动编码器、固定宽度、…)、学习速率等等。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi kv"><img src="../Images/5f72f75407cc069c29f64585cf3fd6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/0*nf_jBtkTIY-u-vl6.jpg"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">As you go through the design space exploration, you find that many of the possible hyperparameter settings are <strong class="bd la">very useless</strong>.</figcaption></figure><p id="d185" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">程序员喜欢使用配置参数，例如生产/开发设置。我们对这个使用<a class="ae kl" href="https://wiki.python.org/moin/ConfigParserExamples" rel="noopener ugc nofollow" target="_blank"> ConfigParser。然而，深度学习中的超参数更类似于一系列嵌套for循环的</a><a class="ae kl" href="https://www.tutorialspoint.com/python/python_nested_loops.htm" rel="noopener ugc nofollow" target="_blank"/>，在爆发之前搜索“好”的配置。该搜索必须扫描可用的机器学习模型以找到一个具有低误差的模型(或者无论目标函数是什么)。您可以将这些模型超参数视为配置。然而，将超参数选择搜索视为<a class="ae kl" href="https://en.wikipedia.org/wiki/Multi-objective_optimization" rel="noopener ugc nofollow" target="_blank">帕累托优化</a>更准确，其中约束是GPU的大小，目标是损失/准确性、通用性(精度、召回、F分数)和其他模型性能标准。有很多模型约束是没有问题的。真正糟糕的是，你有多个目标，有些约束是整数。当面对一个优化问题中的多个目标时，你需要或者创建这些目标的线性组合(线性模型)，可能做一些疯狂的数学(参见<a class="ae kl" href="http://www.litislab.fr/wp-content/uploads/2015/12/Canu-S.pdf" rel="noopener ugc nofollow" target="_blank">混合整数线性规划</a>)，或者只是将此作为<a class="ae kl" href="https://en.wikipedia.org/wiki/Meta_learning_(computer_science)" rel="noopener ugc nofollow" target="_blank">元级机器学习</a>问题(研究！).因为多目标Pareto东西是如此丑陋和缓慢(读作<strong class="jp ir"> <em class="ku"> expen$ive </em> </strong>)，所以基本上规则是尝试有意义的东西，直到你达到可接受的模型性能水平。我的硕士学位是设计空间探索，所以我直接知道在多重约束下选择一个给定的配置有多困难。</p><p id="0c36" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我结束之前，我今天得到一些非常有趣的消息。<strong class="jp ir"> API。AI </strong>，已经改名为<strong class="jp ir"> Dialogflow </strong>。他们重定向了域名和一切。我认为在某个时候，谷歌会把它建成dialogflow.google.com，就像他们对AdWords和inbox等其他谷歌产品所做的那样。或者，它可能会被谷歌云平台吞噬，就像亚马逊对其AWS云服务所做的那样。</p><p id="211b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">好的。回去工作！如果你喜欢这篇关于人工智能的文章，那么请尝试一下<strong class="jp ir">拍手工具</strong>。轻点那个。跟着我们走。去吧。我也很高兴在评论中听到你的反馈。你怎么想呢?我是不是用了太多的<a class="ae kl" href="http://thecopyfox.com/2012/11/writing-tip-do-you-overuse-parentheses/" rel="noopener ugc nofollow" target="_blank">括号</a>？我应该写些什么？写了一堆业务端的文章，最近兴趣更多在技术端。不如这样:把你的数据科学用例或问题发给我，我会挑选一个条目来写一篇关于它的文章。去吧:<a class="ae kl" href="mailto:daniel@lsci.io" rel="noopener ugc nofollow" target="_blank">丹尼尔@lsci.io </a></p><p id="77b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">编码快乐！</p><p id="d405" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">-丹尼尔<br/> <a class="ae kl" href="mailto:daniel@lemay.ai" rel="noopener ugc nofollow" target="_blank">丹尼尔@lemay.ai </a> ←打个招呼。<br/><a class="ae kl" href="https://lemay.ai" rel="noopener ugc nofollow" target="_blank">LEMAY . AI</a><br/>1(855)LEMAY-AI</p><p id="8427" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可能喜欢的其他文章:</p><ul class=""><li id="167e" class="lb lc iq jp b jq jr ju jv jy ld kc le kg lf kk lg lh li lj bi translated"><a class="ae kl" rel="noopener" target="_blank" href="/artificial-intelligence-and-bad-data-fbf2564c541a">人工智能和不良数据</a></li><li id="d59c" class="lb lc iq jp b jq lk ju ll jy lm kc ln kg lo kk lg lh li lj bi translated"><a class="ae kl" rel="noopener" target="_blank" href="/artificial-intelligence-hyperparameters-48fa29daa516">人工智能:超参数</a></li><li id="4172" class="lb lc iq jp b jq lk ju ll jy lm kc ln kg lo kk lg lh li lj bi translated"><a class="ae kl" href="https://medium.com/towards-data-science/artificial-intelligence-get-your-users-to-label-your-data-b5fa7c0c9e00" rel="noopener">人工智能:让你的用户给你的数据贴上标签</a></li></ul></div></div>    
</body>
</html>