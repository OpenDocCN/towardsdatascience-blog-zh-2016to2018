<html>
<head>
<title>Weight Initialization Techniques in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的权重初始化技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78?source=collection_archive---------0-----------------------#2018-11-09">https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78?source=collection_archive---------0-----------------------#2018-11-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="2e36" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">即使构建一个简单的神经网络也是一项令人困惑的任务，在此基础上调整它以获得更好的结果是极其乏味的。但是，在建立神经网络时要考虑的第一步是参数的初始化，如果做得正确，那么优化将在最少的时间内实现，否则使用梯度下降收敛到最小值将是不可能的。</p><p id="2af3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文假设读者已经熟悉神经网络的概念、权重、偏差、激活函数、前向和后向传播等。</p><h2 id="dd43" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">基本符号</h2><p id="1c9f" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">考虑一个 L 层神经网络，它有 L-1 个隐藏层和 1 个输入和输出层。层 l 的参数(权重和偏差)表示为</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/d2e949c47da0ddd812351182b7ccd1f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M3Ja24g0cK22gVqG6HFQbw.png"/></div></div></figure><p id="805b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我们将看看一些基本的初始化方法，以及一些为了达到更好的效果而必须使用的改进技术。以下是初始化参数的一些常用技术:</p><ul class=""><li id="32ad" class="lv lw iq jp b jq jr ju jv jy lx kc ly kg lz kk ma mb mc md bi translated"><strong class="jp ir">零点初始化</strong></li><li id="3619" class="lv lw iq jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated"><strong class="jp ir">随机初始化</strong></li></ul><h2 id="2446" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">零初始化:</h2><p id="8255" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">在一般实践中，偏差用 0 初始化，权重用随机数初始化，如果权重用 0 初始化呢？</p><p id="b98a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了理解，让我们考虑对输出层应用 sigmoid 激活函数。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi mj"><img src="../Images/5322fd93bcf07db675f811b59e6f2c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6A3A_rt4YmumHusvTvVTxw.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">Sigmoid function (<a class="ae mo" rel="noopener" target="_blank" href="/derivative-of-the-sigmoid-function-536880cf918e">https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e</a>)</figcaption></figure><p id="3a4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果所有权重被初始化为 0，则对于 W[l]中的每个 W，关于损失函数的导数是相同的，因此所有权重在后续迭代中具有相同的值。这使得隐藏单元对称，并持续所有 n 次迭代，即，将权重设置为 0 并不会使其优于线性模型。需要记住的一件重要事情是，用 0 初始化时，偏差没有任何影响。</p><p id="d8ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mp mq mr ms b">W[l] = np.random.zeros((l-1,l))</code></p><p id="2b0b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们考虑一个只有三个隐藏层的神经网络，在隐藏层有 ReLu 激活函数，在输出层有 sigmoid。</p><p id="79b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在来自 sklearn.datasets 的数据集“make circles”上使用上述神经网络，获得如下结果:</p><p id="8308" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于 15000 次迭代，损失= 0.6931471805599453，准确度= 50 %</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi mt"><img src="../Images/297276c445bb2533fdcca2730663f4b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aaKYKl892E8v_dw24wDC1w.png"/></div></div></figure><p id="ab82" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">显然，零初始化在分类中是不成功的。</p><h2 id="b07c" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">随机初始化:</h2><p id="c9ed" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">为权重分配随机值比只分配 0 值要好。但有一点我要记住，如果权重被初始化为高值或非常低值会发生什么，什么是合理的权重值初始化。</p><p id="c193" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> a) </strong>如果权重初始化为非常高的值，则项<code class="fe mp mq mr ms b">np.dot(W,X)+b</code>变得显著更高，并且如果应用类似 sigmoid()的激活函数，则该函数将其值映射到 1 附近，其中梯度的斜率变化缓慢，并且学习需要大量时间。</p><p id="a3e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> b) </strong>如果权重被初始化为低值，它将被映射为 0，情况同上。</p><p id="3aa1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个问题通常被称为消失梯度。</p><p id="3c6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了看到这一点，让我们看看上面的例子，但是现在权重被初始化为非常大的值而不是 0:</p><p id="9748" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mp mq mr ms b">W[l] = np.random.randn(l-1,l)*10</code></p><p id="0e10" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经网络与前面的相同，使用来自 sklearn.datasets 的数据集“make circles”上的这种初始化，获得如下结果:</p><p id="fcd0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于 15000 次迭代，损失= 0.38278397192120406，准确度= 86 %</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi mu"><img src="../Images/14f87a26a0deb5b381b920760b9efb55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RzEieVSQ988z1vjJxioyEA.png"/></div></div></figure><p id="ff75" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种解决方案更好，但不能完全满足需要，因此，让我们看看一种新技术。</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h2 id="6032" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">新的初始化技术</h2><p id="1f9d" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">正如我们在上面看到的，对于大的或 0 的权重(W)初始化，即使我们使用适当的权重初始化，也不会获得显著的结果，训练过程很可能将花费更长的时间。有一些与之相关的问题:</p><p id="13b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">a)如果模型太大，需要花很多天来训练，那怎么办</p><p id="86d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">b)消失/爆炸梯度问题怎么办</p><p id="58c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些是多年来一直存在的一些问题，但在 2015 年，何等人(2015)提出了激活感知权重初始化(用于 ReLu)，能够解决这个问题。ReLu 和 leaky ReLu 还解决了渐变消失的问题。</p><p id="4b26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">他初始化:</strong>我们只是简单地将随机初始化与相乘</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/772bea096fff94da693ef721067db410.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zxD6Nr6TyAb8JEG6oXAjkg.png"/></div></div></figure><p id="472b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了了解该解决方案的有效性，让我们使用之前的数据集和神经网络进行上述初始化，结果如下:</p><p id="dfca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于 15000 次迭代，损失=0.07357895962677366，准确度= 96 %</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi nc"><img src="../Images/62ecd445ca6e20ed3e663f2a22f87e45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lCXe3BlmFR7JGui9JPctig.png"/></div></div></figure><p id="e631" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当然，这是对以前技术的改进。</p><p id="d077" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除了正在使用的初始化之外，还有一些比旧技术相对更好并且经常使用的其他技术。</p><p id="10da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Xavier 初始化:</strong>与 He 初始化相同，但用于 tanh()激活函数，在该方法中 2 被 1 代替。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/fef50b3851079643dbf4a19a298ec308.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lv9TNpAXffRnO0p0WMGJwQ.png"/></div></div></figure><p id="e9ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">有些人还使用以下技术进行初始化:</strong></p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/b8005f71ac72c01479ebfb98ecec84ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QIzXjH8uefVbcaycsjfdmw.png"/></div></div></figure><p id="d76d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些方法是初始化的良好起点，减少了渐变爆炸或消失的机会。他们设定的权重既不会比 1 大太多，也不会比 1 小太多。因此，梯度不会消失或爆炸得太快。它们有助于避免缓慢收敛，也确保我们不会一直偏离最小值。存在上述的其他变体，其中主要目标也是最小化参数的方差。谢谢你。</p><p id="5614" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">来源:神经网络和深度学习，吴恩达(Coursera.org)。</p></div></div>    
</body>
</html>