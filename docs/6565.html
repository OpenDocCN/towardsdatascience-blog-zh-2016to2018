<html>
<head>
<title>Review: DeepMask (Instance Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习:深度遮罩(实例分段)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-deepmask-instance-segmentation-30327a072339?source=collection_archive---------5-----------------------#2018-12-19">https://towardsdatascience.com/review-deepmask-instance-segmentation-30327a072339?source=collection_archive---------5-----------------------#2018-12-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0f49" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一种卷积神经网络驱动的实例分段建议方法</h2></div><p id="1243" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di"> T </span>他的时代，<strong class="kh ir"> DeepMask </strong>，由<strong class="kh ir">脸书 AI Research (FAIR) </strong>点评。从<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener"> AlexNet </a>开始，卷积神经网络(CNN)获得了图像分类的高精度，许多 CNN 方法被开发用于其他任务，如对象检测、语义分割和实例分割。DeepMask 是用于实例分割的 CNN 方法。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/4c79b4b4bd610a3b6c105fb26555d5b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JpMn7MNESnA3lYmpdIcpUw.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk"><strong class="bd mb">Semantic Segmentation vs Instance Segmentation</strong></figcaption></figure><ul class=""><li id="eb20" class="mc md iq kh b ki kj kl km ko me ks mf kw mg la mh mi mj mk bi translated"><strong class="kh ir">图像分类</strong>:对图像内的主要物体类别进行分类。</li><li id="b6f0" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la mh mi mj mk bi translated"><strong class="kh ir">对象检测</strong>:识别对象类别，并使用图像中每个已知对象的边界框定位位置。</li><li id="a8af" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la mh mi mj mk bi translated"><strong class="kh ir">语义分割</strong>:识别图像中每个已知物体的每个像素的物体类别。<strong class="kh ir">标签是类感知的。</strong></li><li id="1e86" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la mh mi mj mk bi translated"><strong class="kh ir">实例分割</strong>:识别图像中每个已知对象的每个像素的每个对象实例。<strong class="kh ir">标签是实例感知的。</strong></li></ul><h2 id="de5a" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">与语义分割的一些区别</h2><ul class=""><li id="86ba" class="mc md iq kh b ki nj kl nk ko nl ks nm kw nn la mh mi mj mk bi translated"><strong class="kh ir">对实例个体的更多理解。</strong></li><li id="3c63" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la mh mi mj mk bi translated"><strong class="kh ir">关于遮挡的推理。</strong></li><li id="1022" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la mh mi mj mk bi translated">对于计算物体数量等任务至关重要。</li></ul><h2 id="3d42" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">与对象检测的一些区别</h2><ul class=""><li id="486f" class="mc md iq kh b ki nj kl nk ko nl ks nm kw nn la mh mi mj mk bi translated">边界框是一个非常粗糙的对象边界，许多与被检测对象无关的像素也被包含在边界框中。</li><li id="7760" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la mh mi mj mk bi translated">而非最大抑制(NMS)将<strong class="kh ir">抑制遮挡对象或倾斜对象。</strong></li></ul><p id="de7c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，实例分段在难度上增加了一个级别！！！</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="8c19" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而 DeepMask 是<strong class="kh ir"> 2015 NIPS </strong>论文，引用<strong class="kh ir"> 300 多</strong>。虽然这是一篇发表于 2015 年的论文，但它是最早使用 CNN 进行实例分割的论文之一。为了了解基于深度学习的实例切分的发展，有必要对其进行研究。(<a class="nq nr ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----30327a072339--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p><p id="58a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为可以基于预测的分割掩模生成区域提议，所以也可以执行对象检测任务。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="69d5" class="nz mr iq bd ms oa ob oc mv od oe of my jw og jx nb jz oh ka ne kc oi kd nh oj bi translated">涵盖哪些内容</h1><ol class=""><li id="0b76" class="mc md iq kh b ki nj kl nk ko nl ks nm kw nn la ok mi mj mk bi translated"><strong class="kh ir">模型架构</strong></li><li id="d169" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la ok mi mj mk bi translated"><strong class="kh ir">联合学习</strong></li><li id="38f3" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la ok mi mj mk bi translated"><strong class="kh ir">全场景推理</strong></li><li id="616b" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la ok mi mj mk bi translated"><strong class="kh ir">结果</strong></li></ol></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="ec04" class="nz mr iq bd ms oa ob oc mv od oe of my jw og jx nb jz oh ka ne kc oi kd nh oj bi translated">1.模型架构</h1><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ol"><img src="../Images/a43b2169db76649fb86f8ee1265450be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cajhv-WJZgxILbQL-9bI2g.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk"><strong class="bd mb">Model Architecture (Top), Positive Samples (Green, Left Bottom), Negative Samples (Red, Right Bottom)</strong></figcaption></figure><h2 id="2435" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">左下方:阳性样本</h2><p id="60f5" class="pw-post-body-paragraph kf kg iq kh b ki nj jr kk kl nk ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated">第<em class="op"> k </em>个阳性样本被赋予标签<strong class="kh ir"> <em class="op"> yk </em> =1 </strong>。要成为阳性样本，需要满足两个标准:</p><ul class=""><li id="47a1" class="mc md iq kh b ki kj kl km ko me ks mf kw mg la mh mi mj mk bi translated"><strong class="kh ir">面片包含一个大致位于输入面片中心的对象</strong>。</li><li id="be9c" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la mh mi mj mk bi translated"><strong class="kh ir">物体完全包含在面片中，并在给定的比例范围内。</strong></li></ul><p id="963d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当<em class="op"> yk </em> =1 时，<strong class="kh ir">地面真实遮罩<em class="op"> mk </em>对于属于位于图像块中心的<strong class="kh ir">单个物体</strong>的像素具有正值</strong>。</p><h2 id="a205" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">右下方:阴性样本</h2><p id="a656" class="pw-post-body-paragraph kf kg iq kh b ki nj jr kk kl nk ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated">否则，即使物体部分存在，阴性样本也会被赋予标签<strong class="kh ir"> <em class="op"> yk </em> =-1 </strong>。<strong class="kh ir">当<em class="op"> yk </em> =-1 时，不使用掩膜。</strong></p><h2 id="ff68" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated"><strong class="ak">顶层，模型架构:主分支</strong></h2><p id="254b" class="pw-post-body-paragraph kf kg iq kh b ki nj jr kk kl nk ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated">如上图所示的模型，给定输入图像块<em class="op"> x </em>，经过<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a>的特征提取后，去除源于<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a>的全连通(FC)层。<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a>中的最后一个 max pooling 层也被删除，因此在分成两路之前的输出是输入的 1/16。例如如上，输入是 224×224 (3 是输入图像的通道数，即 RGB)，主分支末端的输出是(224/16)×(224/16) =14×14。(512 是卷积后的特征图个数。)</p><p id="ab17" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a>之后有两条路径:</p><ul class=""><li id="3e24" class="mc md iq kh b ki kj kl km ko me ks mf kw mg la mh mi mj mk bi translated"><strong class="kh ir">第一条路径是预测类别不可知的分割掩码，即<em class="op"> fsegm </em> ( <em class="op"> x </em>)。</strong></li><li id="4d77" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la mh mi mj mk bi translated"><strong class="kh ir">第二条路径是分配一个分数，该分数对应于补丁包含一个对象</strong>的可能性，即<strong class="kh ir"><em class="op">fs core</em>(<em class="op">x</em>)</strong>。</li></ul><h2 id="8c3e" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">顶部，第一条路径:预测分段图</h2><p id="98e4" class="pw-post-body-paragraph kf kg iq kh b ki nj jr kk kl nk ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated"><strong class="kh ir">先进行 1×1 卷积</strong>而不改变特征图的数量，<strong class="kh ir">这里进行的是没有降维的非线性映射</strong>。之后，执行<strong class="kh ir">两个 FC 层</strong>。(注意，在这两个 FC 层之间没有 ReLU！)</p><p id="d52d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与语义分割不同，即使存在多个对象，网络也必须输出单个对象的掩码。(就像上图输入图像中央的大象一样。)</p><p id="6f4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，<strong class="kh ir">生成一个 56×56 的分割图</strong>。而一个简单的<strong class="kh ir">双线性插值</strong>就是将分割图上采样到<strong class="kh ir"> 224×224 </strong>。</p><h2 id="ccfe" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">顶部，第二条路径:预测对象得分</h2><p id="6413" class="pw-post-body-paragraph kf kg iq kh b ki nj jr kk kl nk ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated">2×2 最大池，后跟两个 FC 层。最后，获得一个单值预测对象分数，<em class="op"> fscore </em> ( <em class="op"> x </em>)。由于正样本是基于上述两个标准给出的，<em class="op"> fscore </em> ( <em class="op"> x </em>)用于预测输入图像是否满足这两个标准。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="e1db" class="nz mr iq bd ms oa ob oc mv od oe of my jw og jx nb jz oh ka ne kc oi kd nh oj bi translated">2.联合学习</h1><h2 id="73b0" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">2.1.损失函数</h2><p id="babc" class="pw-post-body-paragraph kf kg iq kh b ki nj jr kk kl nk ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated">对网络进行训练，共同学习每个位置(I，j)的逐像素分割图<strong class="kh ir"><em class="op">fsegm</em>(<em class="op">xk</em>)</strong>和预测对象得分<strong class="kh ir"><em class="op">fs core</em>(<em class="op">xk</em>)。</strong>损失函数如下所示:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oq"><img src="../Images/39a21c0e7805dfdccd8010ad182e2473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iTLxm9l3wpuqrkLbA2TR9A.png"/></div></div></figure><p id="ee1b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简言之，损失函数是二元逻辑回归损失的和，一个用于分割网络<strong class="kh ir"><em class="op">fsegm</em>(<em class="op">xk</em>)</strong>的每个位置，一个用于对象分数<strong class="kh ir"><em class="op">fs core</em>(<em class="op">xk</em>)</strong>。<strong class="kh ir"> </strong>第一项意味着如果<em class="op"> yk </em> =1，我们仅在分割路径上反向传播误差。</p><p id="0718" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果<em class="op"> yk </em> =-1，即负样本，第一项变为 0，不会造成损失。只有第二项造成了损失。</p><p id="e6d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了数据平衡，使用相同数量的阳性和阴性样本。</p><h2 id="4fe0" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">2.2.其他详细信息</h2><p id="b4ec" class="pw-post-body-paragraph kf kg iq kh b ki nj jr kk kl nk ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated"><strong class="kh ir">使用 32 </strong>的批量。<strong class="kh ir">使用预训练的 ImageNet 模型</strong>。总共有<strong class="kh ir"> 75M 个参数</strong>。该模型需要大约<strong class="kh ir"> 5 天</strong>在 Nvidia Tesla K40m 上进行训练。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="77a5" class="nz mr iq bd ms oa ob oc mv od oe of my jw og jx nb jz oh ka ne kc oi kd nh oj bi translated">3.全场景推理</h1><h2 id="08ae" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">3.1.多个位置和规模</h2><p id="2fc7" class="pw-post-body-paragraph kf kg iq kh b ki nj jr kk kl nk ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated">推理(测试)时，模型在<strong class="kh ir">多个位置密集应用，步长</strong>为 16 个像素，<strong class="kh ir">从 1/4 到 2 的多个尺度，步长为 2 的平方根</strong>。这确保了至少有一个完全包含图像中每个对象的测试图像块。</p><h2 id="6c26" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">3.2.精细步幅最大汇集</h2><p id="ba7d" class="pw-post-body-paragraph kf kg iq kh b ki nj jr kk kl nk ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated">由于输入测试图像大于训练输入小块尺寸，我们需要一个相应的<strong class="kh ir"> 2D 评分图</strong>作为输出，而不是一个单一的评分值。在评分分支的最后一个最大池层之前使用了一个交织技巧，即<a class="ae lk" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">over fat</a>中提出的<strong class="kh ir">精细步长最大池</strong>。</p><p id="8c29" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之，多最大池是在特征图上完成的。在每次最大汇集之前执行像素移动。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="f200" class="nz mr iq bd ms oa ob oc mv od oe of my jw og jx nb jz oh ka ne kc oi kd nh oj bi translated">4.结果</h1><h2 id="f784" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">4.1.可可女士(盒子和分段遮罩)</h2><p id="1bcc" class="pw-post-body-paragraph kf kg iq kh b ki nj jr kk kl nk ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated">80，000 幅图像和总共近 500，000 个分割对象用于训练。2014 年可可小姐的前 5000 张照片用于验证。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi or"><img src="../Images/8ed0b135d537d8b96a75032cfef23906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mKXtnbuULRNZdYnknneFAQ.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk"><strong class="bd mb">Average Recall (AR) Detection Boxes (Left) and Segmentation Masks (Right) on MS COCO Validation Set (AR@n: the AR when <em class="os">n</em> region proposals are generated. AUCx: x is the size of objects)</strong></figcaption></figure><ul class=""><li id="6e90" class="mc md iq kh b ki kj kl km ko me ks mf kw mg la mh mi mj mk bi translated">DeepMask20 :只对属于 20 个帕斯卡类别之一的对象进行训练。与 DeepMask 相比，AR 较低，这意味着网络没有推广到看不见的类。(看不到的课分数低。)</li><li id="cceb" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la mh mi mj mk bi translated"><strong class="kh ir"> DeepMask20* </strong>:类似于 DeepMask，但评分路径使用原始的 DeepMask。</li><li id="ed8a" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la mh mi mj mk bi translated"><strong class="kh ir"> DeepMaskZoom </strong>:额外的小尺度提升 AR，但代价是增加了推理时间。</li><li id="5c6f" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la mh mi mj mk bi translated"><strong class="kh ir"> DeepMaskFull </strong>:预测分割掩膜路径上的两个 FC 层被一个从 512×14×14 特征图直接映射到 56×56 分割图的 FC 层代替。整个架构有超过 300 米的参数。比 DeepMask 略逊一筹，慢很多。</li></ul><h2 id="24ed" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">4.2.PASCAL VOC 2007(盒子)</h2><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/32048967f479d78ef68dcaae33938b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*VQt5LD04zCdJItsfV37aGA.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk"><strong class="bd mb">Average Recall (AR) for Detection Boxes on PASCAL VOC 2007 Test Set</strong></figcaption></figure><ul class=""><li id="6ac4" class="mc md iq kh b ki kj kl km ko me ks mf kw mg la mh mi mj mk bi translated">基于预测的分割掩模产生区域提议，其可以用作目标检测任务的第一步。</li><li id="1cf5" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la mh mi mj mk bi translated"><strong class="kh ir">使用深度屏蔽的快速 R-CNN</strong>优于使用选择性搜索的原始<a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快速 R-CNN </a>以及其他最先进的方法。</li></ul><h2 id="7978" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">4.3.推理时间</h2><ul class=""><li id="da1f" class="mc md iq kh b ki nj kl nk ko nl ks nm kw nn la mh mi mj mk bi translated">《可可小姐》中的推理时间是每张图片 1.6s。</li><li id="4473" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la mh mi mj mk bi translated">PASCAL VOC 2007 中的推理时间是每张图片 1.2s。</li><li id="786f" class="mc md iq kh b ki ml kl mm ko mn ks mo kw mp la mh mi mj mk bi translated">通过在单个批次中并行化所有秤，推断时间可以进一步下降约 30%。</li></ul><h2 id="80fa" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">4.4.定性结果</h2><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ou"><img src="../Images/e54686a8ed6774f393881a4c8b8aa536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tpLea9p3k3i3LNAsMg1rDA.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk"><strong class="bd mb">DeepMask proposals with highest IoU to the ground truth on selected images from COCO. Missed objects (no matching proposals with IoU &gt; 0.5) are marked with a red outline.</strong></figcaption></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ov"><img src="../Images/adb3cd53f40ff127414f34e95b03ccdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JUKJagr2KlzEuVHxj6CpqA.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk"><strong class="bd mb">More Results from COCO. Missed objects (no matching proposals with IoU &gt; 0.5) are marked with a red outline.</strong></figcaption></figure></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="5a6b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DeepMask 已经更新，在<a class="ae lk" href="https://github.com/facebookresearch/deepmask" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a>主干被<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>取代。</p><p id="edcf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">继 DeepMask 之后，FAIR 还发明了 SharpMask。希望我以后也能报道它。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h2 id="c465" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">参考</h2><p id="726b" class="pw-post-body-paragraph kf kg iq kh b ki nj jr kk kl nk ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated">【2015 NIPS】【deep mask】<br/><a class="ae lk" href="http://Learning to Segment Object Candidates" rel="noopener ugc nofollow" target="_blank">学习分割对象候选</a></p><h2 id="81b7" class="mq mr iq bd ms mt mu dn mv mw mx dp my ko mz na nb ks nc nd ne kw nf ng nh ni bi translated">我的相关评论</h2><p id="a085" class="pw-post-body-paragraph kf kg iq kh b ki nj jr kk kl nk ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae lk" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lk" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae lk" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">yolo v1</a><a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a><a class="ae lk" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">yolo v2/yolo 9000</a></p><p id="e227" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/>T23】[<a class="ae lk" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a>]</strong></p><p id="22bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">生物医学图像分割<br/></strong><a class="ae lk" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a></p></div></div>    
</body>
</html>