<html>
<head>
<title>‘Logit’ of Logistic Regression; Understanding the Fundamentals</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归的“Logit ”;了解基本原理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logit-of-logistic-regression-understanding-the-fundamentals-f384152a33d1?source=collection_archive---------1-----------------------#2018-10-21">https://towardsdatascience.com/logit-of-logistic-regression-understanding-the-fundamentals-f384152a33d1?source=collection_archive---------1-----------------------#2018-10-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="d2e0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我学习机器学习基础的旅程的最开始，我记得花了很多时间来清楚地理解逻辑回归的基础。希望这个冥想会给你留下更多的答案和正确的概念，而不是与逻辑回归相关的困惑。</p><p id="3fae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇文章中，我将尝试涵盖—</p><ul class=""><li id="a315" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated"><em class="kx">赔率和赔率比</em></li><li id="a997" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated"><em class="kx">理解逻辑回归，从线性回归开始。</em></li><li id="93a4" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated"><em class="kx">逻辑函数作为分类器；用伯努利分布连接 Logit。</em></li><li id="a087" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated"><em class="kx">关于癌症数据集和设置概率阈值以对恶性和良性进行分类的示例。</em></li></ul><h1 id="b3da" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">优势和优势比</h1><p id="aa78" class="pw-post-body-paragraph jq jr it js b jt mb jv jw jx mc jz ka kb md kd ke kf me kh ki kj mf kl km kn im bi translated">在我们深入研究逻辑回归之前，我们需要弄清楚一些概率的基础知识。为简单起见，我们将考虑一个<a class="ae mg" href="https://github.com/suvoooo/Machine_Learning/blob/master/gender_purchase.csv" rel="noopener ugc nofollow" target="_blank">数据集</a>，它告诉我们，客户是否会购买产品取决于性别。我们导入并检查数据集</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="8436" class="mq le it mm b gy mr ms l mt mu">import pandas as pd</span><span id="5656" class="mq le it mm b gy mv ms l mt mu">gender_df = pd.read_csv('gender_purchase.csv')</span><span id="09fd" class="mq le it mm b gy mv ms l mt mu">print gender_df.head(3)</span><span id="c53c" class="mq le it mm b gy mv ms l mt mu">&gt;&gt;&gt; Gender Purchase<br/>0  Female      Yes<br/>1  Female      Yes<br/>2  Female       No</span></pre><p id="a4ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将使用<code class="fe mw mx my mm b">pandas</code>的<code class="fe mw mx my mm b">crosstab</code>功能，根据性别创建一个“是”和“否”的频率表。这张表对理解以后的优势和优势比有很大的用处。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="376f" class="mq le it mm b gy mr ms l mt mu">table = pd.crosstab(gender_df['Gender'], gender_df['Purchase'])<br/>print table</span><span id="6905" class="mq le it mm b gy mv ms l mt mu">&gt;&gt;&gt; Purchase   No  Yes<br/>Gender            <br/>Female    106  159<br/>Male      125  121</span></pre><p id="2d8e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们现在准备定义<strong class="js iu">几率、</strong> <em class="kx">几率，它描述了成功与失败的比率</em>。考虑到女性群体，我们看到女性购买(成功)产品的概率= 159/265(是/女性总数)。女性失败(不购买)的概率为 106/265。在这种情况下，赔率定义为(159/265)/(106/265) = 1.5。可能性越大，成功的机会就越大。<em class="kx">赔率范围可以是</em>【0，∞】<em class="kx">之间的任意数字。</em>如果我们对这些数字取自然对数，范围会发生什么变化？log(x)定义为 x≥0，但范围从[-∞，∞]变化。您可以使用一段代码进行检查</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="a98a" class="mq le it mm b gy mr ms l mt mu">random=[]<br/>xlist = []<br/>for i in range(100):<br/> x = uniform(0,10)# choose numbers between 0 and 10 <br/> xlist.append(x)<br/> random.append(math.log(x))</span><span id="8867" class="mq le it mm b gy mv ms l mt mu">plt.scatter(xlist, random, c='purple',alpha=0.3,label=r'$log x$')<br/>plt.ylabel(r'$log \, x$', fontsize=17)<br/>plt.xlabel(r'$x$',fontsize=17)<br/>plt.legend(fontsize=16)<br/>plt.show()</span></pre><figure class="mh mi mj mk gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi mz"><img src="../Images/f2771dcccd3893b501b3e99a3fe18fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*misawZYESlVRiY8ArlUMew.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Figure 1: log x vs x; for all +’ve’ values of x, log x can vary between -∞ to + ∞.</figcaption></figure><p id="c5c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">到目前为止，我们已经理解了概率。我们来描述一下<strong class="js iu">赔率</strong>，顾名思义就是赔率的比值。考虑到上面的例子，<strong class="js iu">比值比</strong>，代表哪一组(男性/女性)有更好的成功几率，它是通过计算每一组的比值比给出的。所以女性成功购买的几率=女性成功购买的几率/男性成功购买的几率= (159/106)/(121/125)。男性的优势比是上述数字的倒数。</p><p id="299a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以清楚地认识到，虽然比值比可以在 0 到正无穷大之间变化，但 log(比值比)将在[-∞，∞]之间变化。特别是当比值比位于[0，1]之间时，log(比值比)为负。</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="2781" class="ld le it bd lf lg ns li lj lk nt lm ln lo nu lq lr ls nv lu lv lw nw ly lz ma bi translated">线性到逻辑回归</h1><p id="aafd" class="pw-post-body-paragraph jq jr it js b jt mb jv jw jx mc jz ka kb md kd ke kf me kh ki kj mf kl km kn im bi translated">由于“回归”一词在逻辑回归中容易引起混淆，我们可以花几秒钟来回顾一下回归。回归通常指连续性，即预测连续变量(药品价格、出租车费等。)取决于特性。然而，<strong class="js iu">逻辑回归是关于预测二元变量，即当目标变量是分类变量时。</strong> <em class="kx">逻辑回归可能是一个初露头角的数据科学家应该尝试掌握分类问题的第一件事。</em>我们将从线性回归模型开始，逐步理解实现 logistic 模型。</p><p id="426e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在线性回归中，特征变量可以取任何值，因此输出(标签)可以从负到正无穷大连续。</p><figure class="mh mi mj mk gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nx"><img src="../Images/747560fe6458b0b46651f12cc81775e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oHbibXiKD6TGWvfpvQNpbw.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Range of label and feature in linear regression case</figcaption></figure><p id="1d3b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为逻辑回归是关于分类的，即<em class="kx"> Y </em>是分类变量。很明显，用线性回归模型(方程式)不可能获得这样的输出。1.1)，因为两侧量程不匹配。我们的目标是以这样的方式变换 LHS，使得它匹配 RHS 的范围，RHS 的范围由特征变量的范围[-∞，∞]来控制。</p><p id="137b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将遵循一些直观的步骤来探索如何可能达到这样的结果。</p><figure class="mh mi mj mk gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ny"><img src="../Images/cc9d6cb46dddbd27965830356a844fc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GhDAo5e9VQuyWbUpvKpCYg.png"/></div></div></figure><ul class=""><li id="cd81" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">对于线性回归，<em class="kx"> X </em>和<em class="kx"> Y </em>的范围都是从负无穷大到正无穷大。<em class="kx">逻辑中的 Y </em>是绝对的，或者对于上面的问题，它取两个不同值 0，1 中的一个。首先，我们尝试使用回归模型预测概率。现在 LHS 可以取从 0 到 1 的任何值，而不是两个不同的值，但是范围仍然不同于 RHS。</li></ul><figure class="mh mi mj mk gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nz"><img src="../Images/664b42ac2cc6d8c9513ecc22b021a691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aLGPsH32uk2XElTKSpq0Ag.png"/></div></div></figure><ul class=""><li id="b16d" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">我上面讨论过，赔率和赔率比从[0，∞]不等。这比概率(限制在 0 和 1 之间)更好，并且更接近匹配 RHS 的范围。</li><li id="a35f" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">你们中的许多人已经明白，如果我们现在考虑(等式)的 LHS 的自然对数。1.3)那么两侧的范围匹配。</li></ul><figure class="mh mi mj mk gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oa"><img src="../Images/6348b8eced07db6a01e68dc5674f2267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ll7r3ThvvGYXHI_bOKM4IA.png"/></div></div></figure><p id="6b80" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这样，<strong class="js iu">我们实现了一个回归模型，其中输出是概率的自然对数，也称为 logit </strong>。对数的底数并不重要，但取赔率的对数才重要。</p><p id="12be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以从 eq 中检索成功的概率。1.4 如下。</p><figure class="mh mi mj mk gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ob"><img src="../Images/69ebd8b18d7ac073ff0ebd05943fe315.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kWELxwx_oXTy6XUV5rxNUw.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">From odds to probability where probability distribution resembles a sigmoid function</figcaption></figure><p id="4a01" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们知道自变量的系数<em class="kx"> X </em> s 和<em class="kx"> </em>截距 a，我们就可以预测概率。我们将使用软件(<code class="fe mw mx my mm b">sklearn</code>)进行优化。根据问题，我们可以从概率值中选择输出属于 A 类还是 b 类。当我们通过一个示例时，这将更加清楚。</p><h1 id="dde4" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">逻辑函数</h1><p id="711a" class="pw-post-body-paragraph jq jr it js b jt mb jv jw jx mc jz ka kb md kd ke kf me kh ki kj mf kl km kn im bi translated">如果你看到等式 1.5 的 RHS。也称为逻辑函数，与 sigmoid 函数非常相似。我们可以用一小段 python 代码来检查这个函数的行为。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="d5f8" class="mq le it mm b gy mr ms l mt mu">random1=[]<br/>random2=[]</span><span id="1464" class="mq le it mm b gy mv ms l mt mu">random3=[]</span><span id="ddca" class="mq le it mm b gy mv ms l mt mu">xlist = []<br/>theta=[10, 1,0.1]<br/>for i in range(100):<br/> x = uniform(-5,5)<br/> xlist.append(x)<br/> logreg1 = 1/(1+math.exp(-(theta[0]*x)))<br/> logreg2 = 1/(1+math.exp(-(theta[1]*x)))<br/> logreg3 = 1/(1+math.exp(-(theta[2]*x)))<br/> random1.append(logreg1)<br/> random2.append(logreg2)<br/> random3.append(logreg3)</span><span id="c933" class="mq le it mm b gy mv ms l mt mu">plt.scatter(xlist, random1, marker='*',s=40, c='orange',alpha=0.5,label=r'$\theta = %3.1f$'%(theta[0]))<br/>plt.scatter(xlist, random2, c='magenta',alpha=0.3,label=r'$\theta = %3.1f$'%(theta[1]))<br/>plt.scatter(xlist, random3, c='navy',marker='d', alpha=0.3,label=r'$\theta = %3.1f$'%(theta[2]))</span><span id="aa54" class="mq le it mm b gy mv ms l mt mu">plt.axhline(y=0.5, label='P=0.5')<br/>plt.ylabel(r'$P=\frac{1}{1+e^{-\theta \, x}}$', fontsize=19)<br/>plt.xlabel(r'$x$',fontsize=18)<br/>plt.legend(fontsize=16)<br/>plt.show()</span></pre><figure class="mh mi mj mk gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oc"><img src="../Images/bdbb44f7e609cb61ba26c4516eb24179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H64zrYN7-CtOPKrJnTfGTQ.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Figure 2: Probability vs independent variable <strong class="bd od">x</strong>; resembles sigmoid function plot.</figcaption></figure><p id="de00" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从上面的图中，注意自变量(这里是<em class="kx"> X </em>)的系数(橙色星星)值越高，它就能更好地表示两个不同的概率 0 和 1。对于较低的系数值，它基本上是一条直线，类似于一个简单的线性回归函数。对比等式(1.5)，图 2 中固定项<em class="kx"> a </em>取为 0。固定项对逻辑函数的影响也可以通过下图来理解</p><figure class="mh mi mj mk gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oe"><img src="../Images/14058797588111af71384747a01bbad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z79Do1Rs_pkToAefNJKANg.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Figure 3: Sigmoid function for different values of intercept (<strong class="bd od">a</strong>).</figcaption></figure><p id="c125" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就像线性回归中的常数项表示 Y 轴上的截距(因此沿 Y 轴移动)，这里对于逻辑函数，常数项沿 X 轴移动<em class="kx"> s </em>曲线。上面的数字(图 2，3)应该让你相信，使用可以分类数据的逻辑回归来优化模型确实是可能的，例如预测 0 或 1。</p><h1 id="4f59" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">伯努利和罗吉特</h1><p id="cf23" class="pw-post-body-paragraph jq jr it js b jt mb jv jw jx mc jz ka kb md kd ke kf me kh ki kj mf kl km kn im bi translated">逻辑回归的目的是针对独立变量(特征)的任何给定线性组合，预测成功事件的某个未知概率<em class="kx"> P </em>。那么如题所示，logit 和 Bernoulli 函数是如何联系的呢？回想一下二项式分布，它是在<em class="kx"> N 次</em>试验中有<em class="kx"> n 次</em>成功的概率分布，假设</p><figure class="mh mi mj mk gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi of"><img src="../Images/4eec72c743a622066e2d94e0e653c732.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rYE2xEBo2QnL-mdDp2Ws9w.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Binomial distribution</figcaption></figure><p id="d6c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每个试验以概率<em class="kx"> P </em>为真，以概率<em class="kx"> Q=1-P 为假。</em>伯努利分布另一方面是一个离散分布，有两种可能的结果，标记为<em class="kx"> n=0 </em>和<em class="kx"> n=1 </em>，其中<em class="kx"> n=1 </em>(成功事件)以概率<em class="kx"> P </em>发生，而失败即<em class="kx"> n=0 </em>以概率<em class="kx">发生</em></p><figure class="mh mi mj mk gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi og"><img src="../Images/0ac7f86e9ef5299450ed44b63e5b3b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DSZQeQvIa992fSsaGcep_g.png"/></div></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Bernoulli distribution as a special case of binomial distribution</figcaption></figure><p id="d358" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可以理解的是，伯努利分布是单次试验的二项分布的特例(等式 1.6 中<em class="kx"> N=1 </em>)。<strong class="js iu">最重要的是，我们看到逻辑回归中的因变量遵循具有未知概率 p 的伯努利分布。因此，logit，即几率的对数，将自变量(<em class="kx"> Xs </em>)与伯努利分布联系起来。</strong>在 logit 情况下，P 是未知的，但是在伯努利分布(eq。1.6)我们知道。让我们画出 logit 函数。</p><figure class="mh mi mj mk gt na gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/d67ccb4ba5433136ce3c3d19edbc2a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*vEiAxU_mVmFIOvf-xzTolw.png"/></div><figcaption class="nh ni gj gh gi nj nk bd b be z dk">Figure 4: Logit Function i.e. Natural logarithm of odds</figcaption></figure><p id="3819" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们看到函数的定义域位于 0 和 1 之间，函数的范围从负到正无穷大。我们需要逻辑回归的 y 轴上的概率<em class="kx"> P </em>，这可以通过取 logit 函数的反函数来实现。如果你以前注意过 sigmoid 函数曲线(图 2 和图 3)，你可能已经找到了联系。的确，<strong class="js iu"> sigmoid 函数是 logit 的逆(检查等式。1.5).</strong></p><h1 id="5cb9" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">癌症数据集和概率阈值的例子</h1><p id="ade4" class="pw-post-body-paragraph jq jr it js b jt mb jv jw jx mc jz ka kb md kd ke kf me kh ki kj mf kl km kn im bi translated">不再拖延，让我们看看逻辑回归在癌症数据集上的应用。这里我们将集中讨论如何设置概率阈值来对我们的模型进行分类。为了简单起见，我将使用数据集的所有特征，但你可以阅读关于使用<code class="fe mw mx my mm b">RFE </code>方法选择最佳特征的详细信息，我已经在<a class="ae mg" rel="noopener" target="_blank" href="/data-handling-using-pandas-machine-learning-in-real-life-be76a697418c">单独的帖子</a>中描述了该方法。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="2125" class="mq le it mm b gy mr ms l mt mu">from sklearn.datasets import load_breast_cancer</span><span id="fdad" class="mq le it mm b gy mv ms l mt mu">cancer = load_breast_cancer()<br/>cancer_df=pd.DataFrame(cancer.data,columns=cancer.feature_names)</span><span id="e49c" class="mq le it mm b gy mv ms l mt mu">X_trainc, X_testc, y_trainc, y_testc = train_test_split(cancer.data, cancer.target, test_size=0.3, stratify=cancer.target, random_state=30)</span><span id="f9b5" class="mq le it mm b gy mv ms l mt mu">cancerclf = LogisticRegression()<br/>cancerclf.fit(X_trainc, y_trainc)</span><span id="6aa9" class="mq le it mm b gy mv ms l mt mu">#print "Logreg score on cancer data set", cancerclf.score(X_testc, y_testc) # you can check the score if you want, which is not the main purpose. </span></pre><p id="efa5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将使用<code class="fe mw mx my mm b">predict_proba</code>方法进行逻辑回归，引用 scikit-learn 的话“返回按类别标签排序的所有类别的概率估计值”。我们在测试数据集上调用这个方法。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="30f0" class="mq le it mm b gy mr ms l mt mu">probac = cancerclf.predict_proba(X_testc)</span><span id="0bc5" class="mq le it mm b gy mv ms l mt mu">print probac[1:10] </span><span id="cc4c" class="mq le it mm b gy mv ms l mt mu">&gt;&gt;&gt; [[5.86216203e-02 9.41378380e-01]<br/> [7.25210884e-03 9.92747891e-01]<br/> [9.99938102e-01 6.18983128e-05]<br/> [4.75502091e-02 9.52449791e-01]<br/> [9.66861480e-01 3.31385203e-02]<br/> [3.09660805e-01 6.90339195e-01]<br/> [9.99687981e-01 3.12018784e-04]<br/> [6.80759215e-04 9.99319241e-01]<br/> [9.99998223e-01 1.77682663e-06]]</span></pre><p id="80ad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于我们的目标要么是 0，要么是 1，那么打印<code class="fe mw mx my mm b">predict_proba </code>会给我们维数为(N，2)的概率矩阵，N 是实例的数量。第一个指标指的是数据属于类 0 的概率，第二个指标指的是数据属于类 1 的概率。默认情况下，如果该概率大于 0.5，则该预测被归类为正面结果。对于每一行，两列相加应该等于 1，因为成功的概率<em class="kx"> (P) </em>和失败的概率<em class="kx"> (1-P) </em>应该等于 1。</p><p id="73f5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们现在可以转向<code class="fe mw mx my mm b">predict</code>方法，它预测类别标签，在默认情况下，对于二元分类，它将小于 0.5 的概率归类为 0，反之亦然。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="c2f0" class="mq le it mm b gy mr ms l mt mu">predict = cancerclf.predict(X_testc)</span><span id="f372" class="mq le it mm b gy mv ms l mt mu">print predict </span><span id="e123" class="mq le it mm b gy mv ms l mt mu">&gt;&gt;&gt; [1 1 1 0 1 0 1 0 1 0 1 1 1 1 .....]# didn't show the complete list</span></pre><p id="e808" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们考虑<code class="fe mw mx my mm b">probac=cancerclf.predict_proba(X_testc)</code>数组的第一列，它由 0 类概率组成(在癌症数据集中，这是恶性类)。我们用这个数组做了一个迷你数据框。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="de8e" class="mq le it mm b gy mr ms l mt mu">probability = probac[:,0]<br/>prob_df = pd.DataFrame(probability)<br/>print prob_df.head(10) # this should match the probac 1st column </span><span id="5fc5" class="mq le it mm b gy mv ms l mt mu">&gt;&gt;&gt;    0<br/>0  0.005366<br/>1  0.058622<br/>2  0.007252<br/>3  0.999938<br/>4  0.047550<br/>5  0.966861<br/>6  0.309661<br/>7  0.999688<br/>8  0.000681<br/>9  0.999998</span></pre><p id="a98b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们进一步修改这个数据帧，以理解改变阈值的影响。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="8625" class="mq le it mm b gy mr ms l mt mu">prob_df['predict'] = np.where(prob_df[0]&gt;=0.90, 1, 0)# create a new column<br/>print prob_df.head(10)</span><span id="cd0e" class="mq le it mm b gy mv ms l mt mu">&gt;&gt;&gt;    0        predict<br/>0  0.005366        0<br/>1  0.058622        0<br/>2  0.007252        0<br/>3  0.999938        1<br/>4  0.047550        0<br/>5  <strong class="mm iu">0.966861        1</strong><br/>6  0.309661        0<br/>7  0.999688        1<br/>8  0.000681        0<br/>9  0.999998        1</span></pre><p id="91c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们设定≥ 90%作为恶性分类选择的阈值。在打印出的示例中，我们看到值为 0.96，因此将阈值更改为 97%会将该样本从恶性类别中排除。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="a394" class="mq le it mm b gy mr ms l mt mu">prob_df['predict'] = np.where(prob_df[0]&gt;=0.97, 1, 0)<br/>print prob_df.head(10)</span><span id="17fa" class="mq le it mm b gy mv ms l mt mu">&gt;&gt;&gt;    0       predict<br/>0  0.005366        0<br/>1  0.058622        0<br/>2  0.007252        0<br/>3  0.999938        1<br/>4  0.047550        0<br/>5  <strong class="mm iu">0.966861        0 # here is the change</strong><br/>6  0.309661        0<br/>7  0.999688        1<br/>8  0.000681        0<br/>9  0.999998        1</span></pre><p id="ef02" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">还可以检查对测试样本总数的影响</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="352f" class="mq le it mm b gy mr ms l mt mu">prob_df['predict'] = np.where(prob_df[0]&gt;=0.50 1, 0)<br/>print len(prob_df[prob_df['predict']==1])</span><span id="2352" class="mq le it mm b gy mv ms l mt mu">&gt;&gt;&gt; 56</span><span id="95e3" class="mq le it mm b gy mv ms l mt mu">prob_df['predict'] = np.where(prob_df[0]&gt;=0.97 1, 0)<br/>print len(prob_df[prob_df['predict']==1])</span><span id="2373" class="mq le it mm b gy mv ms l mt mu">&gt;&gt;&gt; 45</span></pre><p id="dbcf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们已经看到如何改变概率阈值来选择或拒绝来自特定类别的样本。</p><p id="0a59" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="kx">逻辑回归默认使用 L2 正则化</em>，可以检查改变正则化参数的结果，并与线性回归进行比较。我之前和<a class="ae mg" rel="noopener" target="_blank" href="/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b">岭回归</a>讨论过这个问题，感兴趣的可以去看看。使用<code class="fe mw mx my mm b">RFE</code>选择最佳参数是逻辑回归的一个重要部分，因为最好有很少或没有多重共线性，这是确保选择少量可以描述模型的相关参数的方法之一。</p><p id="38fb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总而言之，<strong class="js iu">我们已经学习了一些关于开发可用于分类的回归模型的基本思想</strong>。</p><p id="56c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="kx">我推荐你去看看吴恩达的课堂笔记或者 YouTube 上的讲座。本帖的基本思想受到了 Kumar，a .的《用 Python 学习预测分析》一书的影响，该书明确描述了线性和逻辑回归的联系。将伯努利函数和 logit 函数之间的联系联系起来是受 B. Larget (UoW，麦迪森)的演示幻灯片的启发，该幻灯片可公开获得。</em></p><p id="9550" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">保持坚强，干杯！</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><p id="f4f4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="kx">如果你对更深入的基础机器学习概念感兴趣，可以考虑加盟 Medium 使用</em> </strong> <a class="ae mg" href="https://saptashwa.medium.com/membership" rel="noopener"> <strong class="js iu"> <em class="kx">我的链接</em> </strong> </a> <strong class="js iu"> <em class="kx">。你不用额外付钱，但我会得到一点佣金。感谢大家！！</em> </strong></p></div></div>    
</body>
</html>