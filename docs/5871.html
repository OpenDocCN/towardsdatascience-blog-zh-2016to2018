<html>
<head>
<title>KNN in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">蟒蛇皮 KNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/knn-in-python-835643e2fb53?source=collection_archive---------7-----------------------#2018-11-13">https://towardsdatascience.com/knn-in-python-835643e2fb53?source=collection_archive---------7-----------------------#2018-11-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/e57b5faea4bc5a55e11515cbdb5118e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LFlueKScaYRRQtxswgjRjg.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by &lt;a href=”<a class="ae kf" href="https://stocksnap.io/author/11667" rel="noopener ugc nofollow" target="_blank">https://stocksnap.io/author/11667</a>"&gt;Henry Lorenzatto&lt;/a&gt; from &lt;a href=”<a class="ae kf" href="https://stocksnap.io" rel="noopener ugc nofollow" target="_blank">https://stocksnap.io</a>"&gt;StockSnap&lt;/a&gt;</figcaption></figure><h1 id="1b38" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">摘要</h1><p id="1c82" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这篇文章中，你将了解到一个非常简单的但<strong class="lg iu">强大的</strong>算法，叫做<strong class="lg iu"> KNN </strong>或<strong class="lg iu">K-最近邻</strong>。第一部分将包含该算法的详细而清晰的解释。在本文的最后，你可以找到一个使用 KNN(用 python 实现)的例子。</p><h1 id="dcd5" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">KNN 解释道</h1><p id="e997" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">KNN 是一种非常流行的算法，它是十大人工智能算法之一(参见<a class="ae kf" rel="noopener" target="_blank" href="/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies-dde4edffae11">十大人工智能算法</a>)。它的流行源于这样一个事实，即它非常容易理解和解释，但很多时候它的准确性与其他更复杂的算法相当，甚至更好。</p><p id="e01c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> KNN </strong>是一种<strong class="lg iu">监督算法</strong>(这意味着训练数据被标记，参见<a class="ae kf" href="https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/" rel="noopener ugc nofollow" target="_blank">监督和非监督算法</a>)，它是<strong class="lg iu">非参数</strong>和<strong class="lg iu">懒惰(基于实例)</strong>。</p><p id="f001" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">为什么是懒？因为它<strong class="lg iu">不显式学习模型</strong>，但它<strong class="lg iu">保存所有训练数据</strong>并使用整个训练集进行分类或预测。这与其他技术形成对比，如<strong class="lg iu"> SVM，</strong>你可以毫无问题地丢弃所有非支持向量。</p><p id="8eb0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这意味着<strong class="lg iu">训练过程非常快</strong>，它只是保存数据集中的所有值。真正的问题是<strong class="lg iu">巨大的内存消耗</strong>(因为我们必须存储所有的数据)和<strong class="lg iu">在测试时间</strong>的时间复杂度(因为给定的观察分类需要运行整个数据集)。但总的来说，在小数据集的情况下(或者如果你有大量的时间和内存)或出于教育目的，这是一个非常有用的算法。</p><p id="9daf" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">另一个重要的假设是，该算法要求<strong class="lg iu">数据在度量空间</strong>中。这意味着我们<strong class="lg iu">可以定义</strong> <strong class="lg iu">度量来计算数据点之间的距离</strong>。定义距离度量可能是一个真正的挑战(参见<a class="ae kf" href="http://vlm1.uta.edu/~athitsos/nearest_neighbors/" rel="noopener ugc nofollow" target="_blank">最近邻分类和检索</a>)。一个有趣的想法是使用机器学习来寻找距离度量(主要是通过将数据转换到向量空间，将对象之间的差异表示为向量之间的距离并学习那些差异，但这是另一个主题，我们稍后会谈到这一点)。</p><p id="5467" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">最常用的距离度量是:</p><ul class=""><li id="6e18" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb mm mn mo mp bi translated"><strong class="lg iu">欧几里德距离</strong>:</li></ul><p id="4853" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是我们在日常生活中使用的几何距离。它的计算方法是两个感兴趣点的平方差之和的平方根。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/aadd6104aa8dd11b55ad73faff574939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*5HUfCCypI1-juH05.png"/></div></figure><p id="4bce" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">公式在 2D 空间中:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/5737bb95475cf4172d475c44d9888afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/0*Y2KWI53bE7WB5R-f.png"/></div></figure><ul class=""><li id="a25a" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb mm mn mo mp bi translated"><strong class="lg iu">曼哈顿距离</strong>:</li></ul><p id="5fca" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">使用绝对差的和计算实向量之间的距离。也叫<strong class="lg iu">城市街区距离</strong>。你可以把这想象成在一个组织成矩阵的城市中行走(或者在曼哈顿行走)。街道是矩阵中小方块的边缘。如果你想从方块 A 到方块 B，你必须走在小方块的边缘。这比欧几里得距离要长，因为你不是从 A 到 B 一直走，而是之字形。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mw"><img src="../Images/48e08cc95b4bb893bb34af1d38a8d59e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sFAWJ5WnT17pGCb8.png"/></div></div></figure><p id="4cf2" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">公式在 2D 空间中:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/0124b26f66dc2bf926bfb66c724e48a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/0*WifHj75MP5XttcQy.png"/></div></figure><ul class=""><li id="869c" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb mm mn mo mp bi translated"><strong class="lg iu">闵可夫斯基距离</strong>:欧几里德距离和曼哈顿距离的推广。计算 N 维距离的通用公式(见<a class="ae kf" href="https://en.wikipedia.org/wiki/Minkowski_distance" rel="noopener ugc nofollow" target="_blank">闵可夫斯基距离</a>)。</li><li id="35be" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb mm mn mo mp bi translated"><strong class="lg iu">汉明距离</strong>:计算二元向量之间的距离(参见<a class="ae kf" href="https://en.wikipedia.org/wiki/Hamming_distance" rel="noopener ugc nofollow" target="_blank">汉明距离</a>)。</li></ul><h1 id="2b98" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">KNN 用于分类</h1><p id="75be" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">非正式的<strong class="lg iu">分类</strong>意味着我们有一些<strong class="lg iu">标记的示例</strong>(训练数据)，对于新的<strong class="lg iu">未标记的示例</strong>(测试集)<strong class="lg iu">我们希望基于训练集的经验教训分配标签</strong>。</p><p id="608c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">正如我前面所说，KNN 算法是懒惰的，在训练阶段没有学习或推广。实际工作是在分类或预测时完成的。</p><p id="471b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">KNN 算法的步骤是(<strong class="lg iu">形式伪代码</strong>):</p><ol class=""><li id="4224" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb nd mn mo mp bi translated">为来自<strong class="lg iu">训练集</strong>的所有<strong class="lg iu"> <em class="ne"> i </em> </strong>数据点初始化<strong class="lg iu"> <em class="ne"> selectedi = 0 </em> </strong></li><li id="9eef" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated">选择一个<strong class="lg iu">距离度量</strong>(假设我们使用欧几里德距离)</li><li id="d6dd" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated">对于每个训练集数据点<strong class="lg iu"> <em class="ne"> i </em> </strong>计算<strong class="lg iu"><em class="ne">distance ei</em></strong>=新数据点与训练点<strong class="lg iu"> <em class="ne"> i </em> </strong>之间的距离</li><li id="cbe2" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated">选择算法的<strong class="lg iu"> K </strong>参数(<strong class="lg iu"> K =考虑的邻居数量</strong>)，通常是奇数，这样可以避免多数投票中出现平局</li><li id="ab75" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated">对于<strong class="lg iu"> <em class="ne"> j = 1 到 K </em> </strong>循环通过<strong class="lg iu"/><strong class="lg iu">所有的训练集数据点</strong>，并在每一步中选择与新观测值具有最小距离的点<strong class="lg iu">(最小<strong class="lg iu"> <em class="ne">距离</em> </strong>)</strong></li><li id="5718" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated">对于<strong class="lg iu">每个现有类别</strong>计算 K 个选定数据点中有多少是该类别的一部分(<strong class="lg iu">投票</strong></li><li id="3acf" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated">将具有最大<strong class="lg iu">计数(最高票数)的班级分配给新的观察——这是<strong class="lg iu">多数投票</strong>。</strong></li></ol><p id="c6cc" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">好吧，可能上面的正式伪代码有点难以理解，所以我们来看看<strong class="lg iu">非正式解释</strong>。</p><p id="61e3" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">主要思想是，对于新的观察，我们搜索 K 个最近点(具有最小距离)。这些点将通过多数表决来定义新观察的类别。</p><p id="f6c0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">例如，如果我们有两个类，红色和绿色，在计算距离并获得 3 个最近的点之后，其中 2 个是红色，1 个是绿色，那么通过多数投票选择的类是红色(2 &gt; 1)。</p><p id="6526" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果我们不得不面对以下情况:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/de16dd347baa7d9c2c5b88cff67682fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*-pGxDYol-UY9y2zY.png"/></div></figure><p id="648f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们有两个班，红色和绿色和一个新的观察，黑色的星星。我们选择 K = 3，因此我们将考虑距离新观测值最小的 3 个点。该恒星接近 3 个红点，因此很明显，这个新观测结果将被归类为红点。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/3bfc5d9705d8470ed7ebb6f1d082110b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/0*q0Xqkta3uKCkzV6o.png"/></div></figure><p id="5ede" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在上面的图片中，我把星星移近了绿点。在这种情况下，我们选择了 2 个红色点和 1 个绿色点。因为 2 &gt; 1，所以这颗星仍然被归类为红色。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/93f3e49006f1bc83076a3086c13dfba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/0*P8jLA0wyYE2dt0sb.png"/></div></figure><p id="3aa5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">随着我们越来越靠近绿点，选择红色作为标签的信心下降，直到新的观察结果将被分类为绿色。这是红色阶级和绿色阶级的<strong class="lg iu">界限</strong>，不同国家的情况也是如此。所以从不同的角度来看，我们可以说用这个算法我们<strong class="lg iu">建立了类的边界</strong>。边界由 K 值控制<strong class="lg iu">。小 K 将导致清晰的边界，而大 K 将导致平滑的边界。最有趣也是最重要的部分，就是在你的特定数据集的背景下，如何选择 K 的最佳值。在下面几节中，我们将看到如何选择 k 的最佳值。</strong></p><p id="9600" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我在上面描述的<strong class="lg iu">并不一定意味着</strong>KNN 算法将总是<strong class="lg iu">线性比较</strong>测试样本和训练数据，就好像它是一个列表一样。训练数据可以用不同的结构表示，如<a class="ae kf" href="https://en.wikipedia.org/wiki/K-d_tree" rel="noopener ugc nofollow" target="_blank"> K-D 树</a>或<a class="ae kf" href="https://en.wikipedia.org/wiki/Ball_tree" rel="noopener ugc nofollow" target="_blank">球树</a>。</p><p id="b06b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">另一个改进是我们可以<strong class="lg iu">为分类中更重要的属性</strong>分配权重。这样，如果我们知道在我们的数据集中有一些重要的属性需要考虑，我们可以给它们分配更高的权重。这将导致它们在给新的观察分配标签时具有<strong class="lg iu">更大的影响。权重可以是<strong class="lg iu">统一的</strong>(所有邻居的同等优势)或<strong class="lg iu">与邻居到测试样本的距离</strong>成反比。你也可以设计自己的权重分配算法(例如使用另一种人工智能算法来寻找最重要的属性，并给它们分配更高的权重)。</strong></p><h1 id="4bed" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">KNN 预测</h1><p id="e71a" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">KNN 算法也可用于<strong class="lg iu">预测新值</strong>。最常见的例子是用 KNN 预测某物(房子、汽车等)的价格。)基于训练数据。为了预测新值，KNN 算法几乎是相同的。在预测<strong class="lg iu">的情况下，我们计算 K 个最相似的点</strong>(相似性的度量必须由用户定义)，并且基于这些点，我们可以使用公式预测新的值<strong class="lg iu">，例如<strong class="lg iu">平均</strong>、<strong class="lg iu">加权平均</strong>等。所以思路是一样的，<strong class="lg iu">定义度量</strong>计算距离(这里是相似度)，<strong class="lg iu">选择 K 个最相似的点</strong>然后使用一个<strong class="lg iu">公式</strong>基于选择的 K 个点预测新值。</strong></p><h1 id="b813" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">计算的复杂性</h1><p id="6f09" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了计算 KNN 的计算复杂度，让我们考虑一个<strong class="lg iu"> d </strong>维空间，<strong class="lg iu"> k </strong>是邻居的数量，<strong class="lg iu"> n </strong>是训练数据点的总数。</p><p id="a153" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">要了解我们如何计算这个算法的复杂度，请看看正式的<a class="ae kf" href="#pseudocode" rel="noopener ugc nofollow">伪代码</a>！每次距离计算都需要<strong class="lg iu"> O(d) </strong>运行时，所以第三步需要<strong class="lg iu"> O(nd) </strong>工作。对于第五步中的每次迭代，我们通过循环训练集观察来执行<strong class="lg iu"> O(n) </strong>工作，因此整个步骤需要<strong class="lg iu"> O(nk) </strong>工作。第一步只需要<strong class="lg iu"> O(n) </strong>工作，所以我们得到一个<strong class="lg iu"> O(nd + kn) </strong>运行时。如果我们使用<a class="ae kf" href="https://en.wikipedia.org/wiki/Median_of_medians" rel="noopener ugc nofollow" target="_blank">快速选择</a>算法来选择具有最小距离的 K 个点，我们可以将这个运行时间复杂度降低到<strong class="lg iu"> O(nd) </strong>。</p><h1 id="f3ca" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">如何为你的数据集选择最好的 K</h1><p id="b0dd" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">您可能想知道，如何找到 K 的最佳值来最大化分类或预测的准确性。首先我必须提到的是，<strong class="lg iu"> K 是一个</strong> <strong class="lg iu">超参数</strong>，这意味着这个参数是由你，开发者选择的。我前面说过，你可以把 K 想象成控制决策边界的参数。例如:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ni"><img src="../Images/cbcf154fa9d00f9b4559dfc47bbde811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OkIIVp-nJSr3T46w.png"/></div></div></figure><p id="e6ef" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">正如你所看到的，如果 K=1，边界非常尖锐，呈之字形，但在 K = 7 的情况下，边界更平滑。因此，随着 K 值的增加，边界变得更加平滑。如果 K 是无穷大，那么所有的东西都是蓝色或红色的，基于总多数。</p><p id="1d16" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">训练误差率</strong>和<strong class="lg iu">验证误差率</strong>是我们在选择正确的 k 时需要考虑的两个参数，如果训练误差很低但测试误差很高，那么我们就要讨论关于<strong class="lg iu">过拟合</strong>的问题了。</p><p id="6ac0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">过拟合</strong>发生在<strong class="lg iu">模型学习</strong>训练数据中的<strong class="lg iu">细节</strong> <strong class="lg iu">和噪声</strong>到了<strong class="lg iu">对模型在新数据</strong>上的性能产生负面影响的程度。这意味着训练数据<strong class="lg iu">中的<strong class="lg iu">噪声或随机波动</strong>被模型拾取并学习为概念</strong>。</p><p id="898a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">比较过度拟合和常规边界:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/ae4b89af0d069935fa3d1e047e27576d.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*Ah7-aVZJq6XaRU-J.png"/></div></figure><p id="3169" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">绿色</strong> <strong class="lg iu">线</strong>代表<strong class="lg iu">过拟合</strong> <strong class="lg iu">模型</strong><strong class="lg iu">黑色</strong> <strong class="lg iu">线</strong>代表<strong class="lg iu">规则化模型</strong>。虽然绿线最符合训练数据，但它过于依赖该数据，并且与黑线相比，它可能对新的、看不见的数据具有更高的错误率。</p><p id="7763" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">欠拟合</strong>是指既不能对训练数据建模，也不能推广到新数据的模型。例如，对非线性数据使用线性算法将会有很差的性能。</p><p id="b1a5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们必须找到中庸之道，有一个模型，可以很好地推广到新的数据，但不是太好，避免学习噪音和避免过度拟合。</p><p id="6eb6" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果我们表示训练错误率和测试错误率，我们将得到如下一些图表:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nk"><img src="../Images/d552d34cf81c3d881e72776afc01cb2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GXpGf2DXU4ogtpiI.png"/></div></div></figure><p id="31bb" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如您所见，当 K=1 时，训练误差为 0，因为任何训练数据点的接近点都是其自身。如果我们看看测试误差，当 K=1 时，误差非常高，这是正常的，因为我们有过拟合。随着 K 值的增加，测试误差率下降，直到达到最小值，之后误差开始再次增加。所以基本上寻找最佳 K 的问题是一个最优化问题，在曲线上寻找最小值。这种叫做<strong class="lg iu">弯头</strong>T42 的方法，因为测试误差曲线看起来像一个弯头。</p><p id="e047" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">结论是为了找到最佳 K 值，使用肘形法并在曲线上找到最小值。这可以通过蛮力很容易地完成，通过多次运行模型，每次增加 K 的值。找到最佳 K 的有效方法是使用<strong class="lg iu"> K-Fold 交叉验证</strong>，但我们将在最后一章(提升 AI 算法)中讨论这一点。</p><h1 id="c899" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">使用 Python 的 KNN 示例</h1><p id="659b" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在本例中，我们将使用 Social_Networks_Ads.csv 文件，该文件包含有关用户的信息，如性别、年龄、工资。<strong class="lg iu">购买栏</strong>包含用户的<strong class="lg iu">标签</strong>。这是一个<strong class="lg iu">二元分类</strong>(我们有两个类)。如果<strong class="lg iu">标签为 1 </strong>则意味着用户<strong class="lg iu">已经购买了产品 X </strong>，而<strong class="lg iu"> 0 </strong>则意味着用户<strong class="lg iu">还没有购买</strong>该特定产品。</p><p id="6882" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这里可以下载文件:<a class="ae kf" href="http://dummyprogramming.com/wp-content/uploads/2018/08/Social_Network_Ads.csv" rel="noopener ugc nofollow" target="_blank">社交 _ 网络 _ 广告</a>。</p><p id="fb7e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在这个例子中，我们将使用以下库:<strong class="lg iu"> numpy、pandas、sklearn 和 motplotlib。</strong></p><p id="6bb0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">第一步是导出数据集。</p><pre class="mr ms mt mu gt nl nm nn no aw np bi"><span id="d193" class="nq kh it nm b gy nr ns l nt nu"># Importing the libraries<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span><span id="59da" class="nq kh it nm b gy nv ns l nt nu"># Importing the dataset<br/>dataset = pd.read_csv('Social_Network_Ads.csv')<br/>X = dataset.iloc[:, [2, 3]].values<br/>y = dataset.iloc[:, 4].values</span></pre><p id="027c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是一个简单的任务，因为 pandas 库包含了<strong class="lg iu"> read_csv </strong>方法，该方法读取我们的数据并将其保存在一个名为<a class="ae kf" href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html" rel="noopener ugc nofollow" target="_blank"> DataFrame </a>的数据结构中。</p><p id="0c89" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">来自<strong class="lg iu"> sklearn </strong>库<strong class="lg iu">的大多数算法要求</strong>属性和标签在单独的变量中，所以我们必须解析我们的数据。</p><p id="499a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在本例中(因为我们想用二维图表表示数据),我们将只使用年龄和工资来训练我们的模型。如果打开文件，可以看到前两列是用户的 ID 和性别。我们不想考虑这些属性。</p><p id="1655" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> X 包含属性</strong>。因为我们不想考虑前两列，所以我们将只复制第 2 列和第 3 列(见第 8 行)。</p><p id="745c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">标签</strong>在第 4 列，所以我们将把这一列复制到变量<strong class="lg iu"> y </strong>中(见第 9 行)。</p><p id="a93d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">下一步是<strong class="lg iu">将我们的数据分成两个不同的块</strong>，一个将用于<strong class="lg iu">训练</strong>我们的数据，另一个将用于<strong class="lg iu">测试</strong>我们模型的结果(测试属性将是新的观察结果，预测标签将与测试集中的标签进行比较)。</p><pre class="mr ms mt mu gt nl nm nn no aw np bi"><span id="b94a" class="nq kh it nm b gy nr ns l nt nu"># Splitting the dataset into the Training set and Test set<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)</span></pre><p id="3699" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是另一个简单的任务，因为 sklearn 有一个名为<a class="ae kf" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> train_test_split </strong> </a>的方法，它将拆分我们的数据集，返回 4 个值，即火车属性(X_train)、测试属性(X_test)、火车标签(y_train)和测试标签(y_test)。通常的设置是将<strong class="lg iu">数据集的 25%用于测试，75%用于训练</strong>。如果您愿意，可以使用其他设置。</p><p id="8a95" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在再看一遍数据集。您可以观察到“薪水”列中的值比“年龄”列中的值高得多。这可能是个问题，因为薪水栏的影响会高得多。你想想看，如果你有 10000 和 9000 这两个非常接近的工资，计算它们之间的距离，结果是 10000–9000 = 1000。现在，如果您将年龄列的值设为 10 和 9，则差值为 10–9 = 1，影响较小(与 1000 相比，10 非常小，就像您有加权属性一样)。</p><pre class="mr ms mt mu gt nl nm nn no aw np bi"><span id="6d7b" class="nq kh it nm b gy nr ns l nt nu">from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span></pre><p id="9d0c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">为了解决这个量级问题，我们必须缩放属性。为此我们使用了 sklearn 的<a class="ae kf" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">standard scaler</strong></a><strong class="lg iu"/>。</p><p id="020d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">下一步是训练模型:</p><pre class="mr ms mt mu gt nl nm nn no aw np bi"><span id="4c16" class="nq kh it nm b gy nr ns l nt nu"># Fitting classifier to the Training set<br/>from sklearn.neighbors import KNeighborsClassifier<br/>classifier = KNeighborsClassifier(n_neighbors = 2)<br/>classifier.fit(X_train, y_train)</span></pre><p id="e83e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们从 sklearn 导入<a class="ae kf" href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">KNeighborsClassifier</strong></a>。这需要多个参数。<strong class="lg iu">最重要的</strong> <strong class="lg iu">参数</strong>有:</p><ol class=""><li id="a072" class="mh mi it lg b lh mc ll md lp mj lt mk lx ml mb nd mn mo mp bi translated"><strong class="lg iu">n _ neighbors</strong>:k 的值，考虑的邻居数量</li><li id="c27f" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated"><strong class="lg iu">权重</strong>:如果您想使用加权属性，您可以在这里配置权重。这需要像统一、距离(到新点的反距离)或可调用这样的值，这些值应该由用户定义。默认值是统一的。</li><li id="ba39" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated"><strong class="lg iu">算法</strong>:如果你想要一个不同的数据表示，这里你可以使用 ball_tree，kd_tree 或者 brute 这样的值，默认为 auto，试图自动选择当前数据集的最佳表示。</li><li id="270a" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated"><strong class="lg iu">度量</strong>:距离度量(欧几里德、曼哈顿等)，默认为欧几里德。</li></ol><p id="29c8" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们保留所有默认参数，但对于 n_neighbors，我们将使用 2(默认值为 5)。</p><p id="6d59" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果您想要预测新观察值的类别，您可以使用以下代码:</p><pre class="mr ms mt mu gt nl nm nn no aw np bi"><span id="dd4e" class="nq kh it nm b gy nr ns l nt nu"># Predicting the Test set results<br/>y_pred = classifier.predict(X_test)</span></pre><p id="1819" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">下一步是评估我们的模型。为此，我们将使用<a class="ae kf" href="https://www.geeksforgeeks.org/confusion-matrix-machine-learning/" rel="noopener ugc nofollow" target="_blank">混淆矩阵</a>。</p><pre class="mr ms mt mu gt nl nm nn no aw np bi"><span id="e57d" class="nq kh it nm b gy nr ns l nt nu"># Making the Confusion Matrix<br/>from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y_test, y_pred)</span></pre><p id="ed61" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">混淆矩阵的结果是:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/d5632d80ba9dddcefd49a48c6b7db767.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/format:webp/0*PMt2VsOnyQl2mz-F.png"/></div></figure><p id="7cb6" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">正如你所看到的，我们只有 5 个假阳性和 7 个假阴性，这是一个非常好的结果(这里的准确率是 80%)。</p><p id="16c0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">最后一步是可视化决策边界。让我们从训练集的决策边界开始。</p><pre class="mr ms mt mu gt nl nm nn no aw np bi"><span id="773d" class="nq kh it nm b gy nr ns l nt nu"># Visualising the Training set results<br/>from matplotlib.colors import ListedColormap<br/>X_set, y_set = X_train, y_train<br/>X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),<br/>                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))<br/>plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),<br/>             alpha = 0.75, cmap = ListedColormap(('red', 'green')))<br/>plt.xlim(X1.min(), X1.max())<br/>plt.ylim(X2.min(), X2.max())<br/>for i, j in enumerate(np.unique(y_set)):<br/>    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],<br/>                c = ListedColormap(('red', 'green'))(i), label = j)<br/>plt.title('Classifier (Training set)')<br/>plt.xlabel('Age')<br/>plt.ylabel('Estimated Salary')<br/>plt.legend()<br/>plt.show()</span></pre><p id="a522" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> meshgrid </strong>函数用一个 x 值数组和一个 y 值数组创建一个<strong class="lg iu">矩形网格(这里 x = X1，y = X2)。</strong></p><p id="a777" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> contourf </strong>方法<strong class="lg iu">绘制填充轮廓</strong>，所以我们用这个方法<strong class="lg iu">用与之相关的类</strong>的颜色填充背景。诀窍在于，对于每个像素 <strong class="lg iu">，我们进行预测</strong>，并用与预测类别相关的颜色对该像素进行着色。这里我们有两个类，我们用红色和绿色表示 0 和 1。</p><p id="9f83" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在行 10 和 12 a 之间，我们<strong class="lg iu">循环所有训练数据点</strong>，我们<strong class="lg iu">预测它们的标签</strong>，如果预测值为 0，将它们涂成红色，如果预测值为 1，将它们涂成绿色。</p><p id="5377" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这段代码的结果是:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/62bbd643ec02c3367613bc005451bb0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/0*OsxZZEhguUfFABsP.png"/></div></figure><p id="951b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">为了可视化测试集的边界，我们使用相同的代码，但是将<strong class="lg iu"> X_set，y_set 改为 X_test，y_test。</strong>运行代码来可视化测试集的边界的结果是:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/ad713697edf9363af8bcc3c260f847d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/0*h3luFLzFmOgT2MUg.png"/></div></figure><p id="4a7f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">正如你在上图中看到的，我们有一个非常好的模型，只有几个点被错误分类。</p><h1 id="480a" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论</h1><ol class=""><li id="f534" class="mh mi it lg b lh li ll lm lp ny lt nz lx oa mb nd mn mo mp bi translated">KNN 算法非常<strong class="lg iu">直观</strong>和<strong class="lg iu">易于理解</strong></li><li id="2bb0" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated"><strong class="lg iu">训练时间最短</strong>，模型实际上不学习或推广</li><li id="40e7" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated"><strong class="lg iu">测试时间可能会很长</strong>，因为算法在整个训练数据集上循环并计算距离(基于距离度量的类型和数据集的类型，距离计算可能是一项艰巨的工作)</li><li id="a59a" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated">对于<strong class="lg iu">小 K 值</strong>，算法可能对<strong class="lg iu">噪声敏感并且容易过拟合</strong></li><li id="6065" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated"><strong class="lg iu">数据集应该是数字的</strong>或一个<strong class="lg iu">距离度量应该存在</strong>以计算点之间的距离</li><li id="e10c" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated">在不平衡数据上表现不佳</li></ol><figure class="mr ms mt mu gt ju gh gi paragraph-image"><a href="https://ko-fi.com/zozoczako"><div class="gh gi ob"><img src="../Images/b1bd188a08576c80bde7d92888d2e27c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tYE4n1ydG0AvXMn_-ADaXw@2x.png"/></div></a></figure><p id="9497" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我真的很喜欢咖啡，因为它给了我写更多文章的能量。</p><p id="5aee" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果你喜欢这篇文章，那么你可以请我喝杯咖啡来表达你的欣赏和支持！</p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><p id="b3e0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">成为媒介上的作家:</strong>【https://czakozoltan08.medium.com/membership】T2</p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="b86c" class="kg kh it bd ki kj oj kl km kn ok kp kq kr ol kt ku kv om kx ky kz on lb lc ld bi translated">参考</h1><ol class=""><li id="987f" class="mh mi it lg b lh li ll lm lp ny lt nz lx oa mb nd mn mo mp bi translated"><a class="ae kf" href="https://medium.com/data-science-group-iitr/k-nearest-neighbors-knn-500f0d17c8f1" rel="noopener"> KNearestNeighbors 解释</a></li><li id="f14d" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated"><a class="ae kf" href="https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/" rel="noopener ugc nofollow" target="_blank">KNN 完整指南</a></li><li id="b5a9" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated"><a class="ae kf" href="https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/" rel="noopener ugc nofollow" target="_blank">机器学习中的过拟合和欠拟合</a></li><li id="359b" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated"><a class="ae kf" href="https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/" rel="noopener ugc nofollow" target="_blank">KNN 详细介绍</a></li><li id="504a" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated"><a class="ae kf" href="https://www.udemy.com/machinelearning/learn/v4/t/lecture/5735502?start=0" rel="noopener ugc nofollow" target="_blank"> Udemy —从 A 到 Z 的机器学习</a></li><li id="b7b7" class="mh mi it lg b lh my ll mz lp na lt nb lx nc mb nd mn mo mp bi translated"><a class="ae kf" href="https://stats.stackexchange.com/questions/219655/k-nn-computational-complexity" rel="noopener ugc nofollow" target="_blank"> KNN 计算复杂度</a></li></ol><p id="8638" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果你<strong class="lg iu">喜欢</strong>这篇文章，并且你想<strong class="lg iu">为<strong class="lg iu">免费</strong>学习</strong>更多有趣的 AI 算法，请<strong class="lg iu">订阅</strong>！</p></div></div>    
</body>
</html>