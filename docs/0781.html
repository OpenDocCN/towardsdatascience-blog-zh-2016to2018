<html>
<head>
<title>A Soft Introduction to Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-soft-introduction-to-neural-networks-6986b5e3a127?source=collection_archive---------2-----------------------#2017-06-21">https://towardsdatascience.com/a-soft-introduction-to-neural-networks-6986b5e3a127?source=collection_archive---------2-----------------------#2017-06-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ee00829ec62b882ac71371498a8a99d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m2gDBT_nc-iE7R4AM3sHBQ.jpeg"/></div></div></figure><p id="8e70" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在过去的几年里，神经网络已经成为机器学习的同义词。最近，我们已经能够制造神经网络，它可以产生栩栩如生的面孔，转移动态艺术风格，甚至可以按年“老化”一个人的照片。如果你想知道神经网络背后的直觉是如何工作的，但不知道从哪里开始，你来对地方了！</p><p id="c371" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本教程结束时，您应该能够理解神经网络工作的方式和原因。您还将仅使用NumPy从头开始创建一个非常简单的神经网络。线性代数/多元微积分和Python编程的经验是本教程的先决条件。</p><h1 id="f81d" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">向前传球</h1><p id="a331" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">任何一个<strong class="ka ir">神经网络</strong>的项目都是<em class="lz">基于某个输入</em>生成一个预测。这听起来非常模糊，但这是因为每个神经网络的任务略有不同，所以这个定义是一刀切的。输入的一些例子可以是三维张量形式的图像、属性的输入特征向量或字典嵌入的向量；输出的一些示例可能是预测的分类标签(分类)或预测的非离散值(回归)。</p><h2 id="178b" class="ma kx iq bd ky mb mc dn lc md me dp lg kj mf mg lk kn mh mi lo kr mj mk ls ml bi translated">给我数据</h2><p id="5d8a" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">我们举个简单的例子，让这个概念不那么模糊。我们有这个16个数据点的玩具数据集，每个数据点有四个<strong class="ka ir">特征</strong>和两个可能的<strong class="ka ir">类别标签</strong> (0或1):</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mm"><img src="../Images/b5d94b80b087c4e875e6d1aa935bed74.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*csHz_FiDzGdhj5HDQaxbGg.png"/></div></div></figure><p id="778f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是数据集的代码:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="606f" class="ma kx iq ms b gy mw mx l my mz"><strong class="ms ir">import </strong>numpy <strong class="ms ir">as </strong>np<br/><br/>X = np.array([[1, 1, 0, 1], [0, 1, 0, 1], [0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 1, 0], [1, 0, 1, 1], [0, 0, 0, 0], [1, 1, 1, 0], [0, 0, 1, 1], [1, 1, 0, 1], [0, 0, 1, 0], [1, 0, 0, 0], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 0, 1], [1, 0, 0, 1]])</span><span id="eef5" class="ma kx iq ms b gy na mx l my mz">y = np.array([[0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [1], [0], [0]])</span></pre><p id="1d8e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如您所见，类标签与第三个特性的值完全相同。我们希望网络能够准确地将要素转换为预测的类别标注概率，也就是说，输出应该具有两个可能类别的概率。如果我们的网络是准确的，我们将对正确的类有高的预测概率，对不正确的类有低的预测概率。</p><p id="8499" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我们继续之前，有一点很重要:我们不能用所有的X来训练我们的网络。这是因为我们<em class="lz">期望</em>我们的网络在它看到的数据上表现良好；我们真的很有兴趣看看它在<em class="lz">还没有</em>看到的数据上表现如何。我们想要两种类型的看不见的数据:一个是<em class="lz">数据集，我们可以基于它定期评估我们的网络</em>，称为<strong class="ka ir">验证集</strong>，另一个是<em class="lz">数据集，它模拟“真实世界的数据”，我们只评估一次</em>，称为<strong class="ka ir">测试集</strong>。其余用于训练的数据被称为<strong class="ka ir">训练集</strong>。下面是执行该分割的代码:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="b2fa" class="ma kx iq ms b gy mw mx l my mz">X_train = X[:8, :]<br/>X_val = X[8:12, :]<br/>X_test = X[12:16, :]<br/><br/>y_train = y[:8]<br/>y_val = y[8:12]<br/>y_test = y[12:16]</span></pre><h2 id="efa0" class="ma kx iq bd ky mb mc dn lc md me dp lg kj mf mg lk kn mh mi lo kr mj mk ls ml bi translated">网络层</h2><p id="74a8" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">单词“<strong class="ka ir"> layers </strong>”在机器学习的上下文中经常出现。说实话，它只不过是一种数学运算<em class="lz">，经常涉及乘法权重矩阵和加法偏差向量</em>。</p><p id="3a04" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们只取其中一个数据点，它是一组四个特征值。我们希望将输入数据点(1×4维的矩阵)转换为标签概率向量(1×2维的向量)。为此，我们只需将输入乘以一个4乘2的权重矩阵。就是这样！不完全是这样，我们还在乘积中增加了一个1乘2的偏置向量，只是为了增加一个额外的学习参数。</p><p id="919a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">代码如下:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="c017" class="ma kx iq ms b gy mw mx l my mz">W = np.random.randn(4, 2)<br/>b = np.zeros(2)<br/><br/>linear_cache = {}<br/><strong class="ms ir">def </strong>linear(input):<br/>    output = np.matmul(input, W) + b<br/>    linear_cache[<strong class="ms ir">"input"</strong>] = input<br/>    <strong class="ms ir">return </strong>output</span></pre><p id="e44b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(暂时不用担心缓存；这将在下一节中解释)</p><p id="54b0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这种操作方式的一个非常酷的结果是，我们不需要只给图层一个数据点作为输入！如果我们取四个数据点，输入是4乘4，输出是4乘2(四组分类概率)。<em class="lz">在训练时间</em>使用的数据集子集被称为<strong class="ka ir">批次</strong>。</p><p id="686a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">概率最高的类是网络的猜测。太好了！除了网络会严重出错——毕竟所有的权重都是随机的！言简意赅地说:现在我们已经有了网络猜测<em class="lz"/>，我们需要让它正确地猜测<em class="lz"/>。</p><h1 id="0c7a" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">向后传球</h1><p id="1a59" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">为了使网络的猜测更好，我们需要找到权重和偏差的“好”值，以便正确猜测的数量最大化。我们通过找到<em class="lz">一个代表我们的猜测</em>有多“错误”的标量值(称为<strong class="ka ir">损失</strong>值)并使用多元微积分将其最小化来做到这一点。</p><h2 id="8404" class="ma kx iq bd ky mb mc dn lc md me dp lg kj mf mg lk kn mh mi lo kr mj mk ls ml bi translated">损失函数(或“取L”)</h2><p id="8aa4" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">在计算标量损失之前，我们希望将任意值的“概率”转换成适当的概率分布。我们通过对以下值计算softmax函数来实现这一点:</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/a387f27dcad0fbcf2aaeb781f7365c31.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*bdrNIeQs4LO1DDo3qtVmxg.png"/></div></figure><p id="98cc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于线性输出中的每个f_j值。为了得到标量损失值，计算<em class="lz">正确类</em>的交叉熵:</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/30e3646d094edf661329e3697c5e18c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*FCuWYnXiVGWjxqh8MKTU3w.png"/></div></figure><p id="f092" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">正确的类值f_i。下面是代码:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="9f91" class="ma kx iq ms b gy mw mx l my mz">softmax_cache = {}<br/><strong class="ms ir">def </strong>softmax_cross_entropy(input, y):<br/>    batch_size = input.shape[1]<br/>    indeces = np.arange(batch_size)<br/><br/>    exp = np.exp(input)<br/>    norm = (exp.T / np.sum(exp, axis=1)).T<br/>    softmax_cache[<strong class="ms ir">"norm"</strong>], softmax_cache[<strong class="ms ir">"y"</strong>], softmax_cache[<strong class="ms ir">"indeces"</strong>] = norm, y, indeces<br/><br/>    losses = -np.log(norm[indeces, y])<br/>    <strong class="ms ir">return </strong>np.sum(losses)/batch_size</span></pre><h2 id="fdc3" class="ma kx iq bd ky mb mc dn lc md me dp lg kj mf mg lk kn mh mi lo kr mj mk ls ml bi translated">反向传播</h2><p id="1bfc" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">我们现在的工作是最小化损失值，我们需要改变权重来做到这一点。我们利用链式法则的美丽来实现这一点。</p><p id="b36a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们举一个超级简单的例子来论证这个概念。我们来定义一个函数:L(x) = 3x + 2。下面是一个函数图:</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nd"><img src="../Images/42eb4015c6c93d77cb85b8c40d58bf81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AfsJSmR5Nl4XEr_EXrZIqw.png"/></div></div></figure><p id="1c1f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们想求L相对于x的导数。我们可以通过处理子函数处的每个图节点，求每个子函数处的偏导数，然后乘以引入的导数来实现:</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/37885fb68caf875748c6819db06c15dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HpSEqOjoo3B9mARwyoFazA.png"/></div></div></figure><p id="ec2b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们来分析一下。L对L的导数正好是1。n + 2的导数为1；乘以1(引入渐变)等于1。导数3n是3；乘以1等于3。最后，n的导数是2n；乘以3是6n。的确，我们知道3x +2的导数是6x。<em class="lz">通过递归使用链式法则</em>求关于参数的导数称为<strong class="ka ir">反向传播</strong>。我们可以对复杂的神经网络做同样的事情，首先将网络绘制成图形:</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nf"><img src="../Images/5d117838734d5c555c24ffa5d1e1404c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ppi68wLUvenbx5y_tqxI5Q.png"/></div></div></figure><p id="53f6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，我们可以通过网络反向传播，找到关于权重和偏差的导数(当变量是非标量时，称为<strong class="ka ir">梯度</strong>):</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/8db45272413de2db0ee92a70f4031f54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-HUFfnKZOGtZbVst1mPaCQ.png"/></div></div></figure><p id="fe4f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过该图的反向传播具有与上图相同的基本规则。然而，这里仍然有很多事情要做，所以让我们再来分析一下。第二个梯度是<em class="lz">整个</em>soft max/交叉熵函数的梯度；它基本上说明了导数与前向传递的softmax的输出相同，只是我们从正确的类中减去1。梯度的推导不在本文的讨论范围内，但是你可以在这里<a class="ae nh" href="https://stackoverflow.com/questions/37790990/derivative-of-a-softmax-function-explanation" rel="noopener ugc nofollow" target="_blank">阅读更多。此外，b和q的维数不同，所以我们需要对一个维数求和，以确保维数匹配。最后，x^T是输入矩阵x的转置。希望现在清楚了为什么我们需要缓存某些变量:在正向传递中使用/计算的一些值需要在反向传递中计算梯度。以下是向后传球的代码:</a></p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="aec6" class="ma kx iq ms b gy mw mx l my mz"><strong class="ms ir">def </strong>softmax_cross_entropy_backward():<br/>    norm, y, indeces = softmax_cache[<strong class="ms ir">"norm"</strong>], softmax_cache[<strong class="ms ir">"y"</strong>], softmax_cache[<strong class="ms ir">"indeces"</strong>]<br/>    dloss = norm<br/>    dloss[indeces, y] -= 1<br/>    <strong class="ms ir">return </strong>dloss<br/><br/><strong class="ms ir">def </strong>linear_backward(dout):<br/>    input = linear_cache[<strong class="ms ir">"input"</strong>]<br/>    dW = np.matmul(input.T, dout)<br/>    db = np.sum(dout, axis=0)<br/>    <strong class="ms ir">return </strong>dW, db</span></pre><h2 id="80ae" class="ma kx iq bd ky mb mc dn lc md me dp lg kj mf mg lk kn mh mi lo kr mj mk ls ml bi translated">更新规则</h2><p id="e8bc" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">相对于参数的反向传播给出了最陡的变化方向。所以，如果我们向与相反的<em class="lz">方向移动，那么我们将减少函数值。向最陡下降方向移动的最简单算法称为<strong class="ka ir">梯度下降</strong> — <strong class="ka ir"> </strong> <em class="lz">将梯度乘以某个值α并从参数中减去它:</em></em></p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/2a7f2ad52686da17e14bb360851919d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*qQCAZYiMVuttKzluT3eipA.png"/></div></figure><p id="7579" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">乘性值α非常重要，因为如果它太大，我们可能会超过最小值，但如果它太小，我们可能永远不会收敛。<em class="lz">我们在权重更新</em>中采取的步长被称为<strong class="ka ir">学习率</strong>。学习率是一个<strong class="ka ir">超参数</strong>，<em class="lz">一个我们可以改变的值，以在我们训练的网络中产生不同的结果</em>。</p><p id="81b2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请注意，在我们的训练制度的代码中，注释了“参数更新”一节:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="a283" class="ma kx iq ms b gy mw mx l my mz"><strong class="ms ir">def </strong>eval_accuracy(output, target):<br/>    pred = np.argmax(output, axis=1)<br/>    target = np.reshape(target, (target.shape[0]))<br/>    correct = np.sum(pred == target)<br/>    accuracy = correct / pred.shape[0] * 100<br/>    <strong class="ms ir">return </strong>accuracy<br/><br/><em class="lz"># Training regime<br/></em><strong class="ms ir">for </strong>i <strong class="ms ir">in </strong>range(4000):<br/>    indeces = np.random.choice(X_train.shape[0], 4)<br/>    batch = X_train[indeces, :]<br/>    target = y_train[indeces]<br/><br/>    <em class="lz"># Forward Pass<br/>    </em>linear_output = linear(batch)<br/>    loss = softmax_cross_entropy(linear_output, target)<br/><br/>    <em class="lz"># Backward Pass<br/>    </em>dloss = softmax_cross_entropy_backward()<br/>    dW, db = linear_backward(dloss)<br/><br/>    <em class="lz"># Weight updates<br/>    </em>W -= 1e-2 * dW<br/>    b -= 1e-2 * db<br/><br/>    <em class="lz"># Evaluation<br/>    </em><strong class="ms ir">if </strong>(i+1) % 100 == 0:<br/>        accuracy = eval_accuracy(linear_output, target)<br/>        print (<strong class="ms ir">"Training Accuracy: %f" </strong>% accuracy)<br/><br/>    <strong class="ms ir">if </strong>(i+1) % 500 == 0:<br/>        accuracy = eval_accuracy(linear(X_val), y_val)<br/>        print(<strong class="ms ir">"Validation Accuracy: %f" </strong>% accuracy)<br/><br/><em class="lz"># Test evaluation<br/></em>accuracy = eval_accuracy(linear(X_test), y_test)<br/>print(<strong class="ms ir">"Test Accuracy: %f" </strong>% accuracy)</span></pre><p id="6eac" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以下是全部代码的完整要点:</p><figure class="mn mo mp mq gt jr"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="dcf7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">也可以在这里找到:<a class="ae nh" href="https://gist.github.com/ShubhangDesai/72023174e0d54f8fb60ed87a3a58ec7c" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/ShubhangDesai/72023174 E0 d 54 f 8 FB 60 ed 87 a 3 a 58 EC 7 c</a></p><p id="ec09" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">就是这样！我们有一个非常简单的，一个隐藏层的神经网络，可以训练它在我们的玩具数据集上产生100%的验证和测试准确性。</p><h1 id="f1aa" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">后续步骤</h1><p id="ad81" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">我们使用的层类型被称为<strong class="ka ir">线性</strong>或<strong class="ka ir">全连接层</strong>。今年夏天，我将写更多关于其他类型的层和网络架构的文章，以及关于比玩具数据集更酷的应用程序的文章。小心那些东西！</p><p id="104c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有一些很棒的关于机器学习的在线课程是免费的。Coursera ML课程当然是经典，但我也推荐斯坦福CS 231n 的<a class="ae nh" href="http://cs231n.stanford.edu/" rel="noopener ugc nofollow" target="_blank">课程材料。这是一门硕士水平的课程，我有幸在上个季度修过；这是令人难以置信的教学和密集。</a></p><p id="1267" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我还会推荐查看一些关于<a class="ae nh" href="https://www.tensorflow.org/tutorials/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>和<a class="ae nh" href="http://pytorch.org/tutorials/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>的初学者教程。它们是最受欢迎的开源深度学习库，理应如此。教程深入浅出，易于理解。</p><p id="0777" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在你已经有了神经网络的基础，你已经正式进入了一个令人兴奋的快速变化的领域。去，探索领域！我真的希望机器学习能激发你和我一样的敬畏和惊奇。</p><p id="cc12" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你喜欢这篇文章，请一定给我一个掌声，并关注我，在你的feed中看到我未来的文章！另外，看看我的 <a class="ae nh" href="https://shubhangdesai.github.io/blog/" rel="noopener ugc nofollow" target="_blank"> <em class="lz">个人博客</em> </a> <em class="lz">，关注我的</em> <a class="ae nh" href="https://twitter.com/ShubhangDesai" rel="noopener ugc nofollow" target="_blank"> <em class="lz">推特</em> </a> <em class="lz">了解更多我的想法。</em></p></div></div>    
</body>
</html>