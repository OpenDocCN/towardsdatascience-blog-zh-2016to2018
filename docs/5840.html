<html>
<head>
<title>HandySpark: bringing pandas-like capabilities to Spark DataFrames</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">HandySpark:让熊猫般的能力点燃数据框架</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/handyspark-bringing-pandas-like-capabilities-to-spark-dataframes-5f1bcea9039e?source=collection_archive---------5-----------------------#2018-11-12">https://towardsdatascience.com/handyspark-bringing-pandas-like-capabilities-to-spark-dataframes-5f1bcea9039e?source=collection_archive---------5-----------------------#2018-11-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/f6a0a077359e40d81d251ad14888dbfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0fke2VC1RszzYaVu"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">“Panda statues on gray concrete stairs during daytime” by <a class="ae kc" href="https://unsplash.com/@chuttersnap?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">chuttersnap</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="f6ee" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">TLDR；</h1><p id="2ba3" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="ld ir"> HandySpark </strong>是一个新的 Python 包，旨在改善<em class="lz"> PySpark </em>的用户体验，特别是当涉及到<strong class="ld ir">探索性数据分析</strong>时，包括<strong class="ld ir">可视化</strong>功能。</p><blockquote class="ma mb mc"><p id="de1d" class="lb lc lz ld b le md lg lh li me lk ll mf mg lo lp mh mi ls lt mj mk lw lx ly ij bi translated">更新(2019 年 3 月 9 日):版本<strong class="ld ir"> 0.2.0 </strong>今天发布，包括分层操作的性能改进和 BinaryClassificationMetrics 的扩展版本——更多详细信息，请查看发布说明<a class="ae kc" href="https://github.com/dvgodoy/handyspark/releases/tag/v0.2.0a1" rel="noopener ugc nofollow" target="_blank">此处</a>。</p></blockquote><p id="58a0" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">使用 Google Colab 亲自尝试一下:</p><div class="ml mm gp gr mn mo"><a href="https://colab.research.google.com/github/dvgodoy/handyspark/blob/master/notebooks/Exploring_Titanic.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd ir gy z fp mt fr fs mu fu fw ip bi translated">谷歌联合实验室</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">使用 HandySpark 探索泰坦尼克号</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">colab.research.google.com</p></div></div><div class="mx l"><div class="my l mz na nb mx nc jw mo"/></div></div></a></div><p id="7b40" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">检查存储库:</p><div class="ml mm gp gr mn mo"><a href="https://github.com/dvgodoy/handyspark" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd ir gy z fp mt fr fs mu fu fw ip bi translated">dvgodoy/handyspark</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">HandySpark -带来熊猫般的能力来激发数据帧</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">github.com</p></div></div><div class="mx l"><div class="nd l mz na nb mx nc jw mo"/></div></div></a></div><h1 id="95f8" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">介绍</h1><p id="0277" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="ld ir"> Apache Spark </strong>是最流行的集群计算框架。约有<strong class="ld ir"> 30%的工作清单</strong> ( <a class="ae kc" rel="noopener" target="_blank" href="/the-most-in-demand-skills-for-data-scientists-4a4a8db896db">链接</a>)将其列为必备技能。</p><p id="cc2a" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">大多数数据科学家使用<strong class="ld ir"> Python </strong>和<strong class="ld ir">熊猫</strong>、事实上的标准<em class="lz">来操作数据。因此，他们想要使用<strong class="ld ir">py Spark</strong>——Spark Python API，当然还有<strong class="ld ir"> Spark DataFrames </strong>，这是合乎逻辑的。</em></p><p id="350c" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">但是，从<strong class="ld ir">熊猫</strong>到<strong class="ld ir">星火数据帧</strong>的过渡可能不像人们希望的那样顺利…</p><h1 id="1374" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">动机</h1><p id="8a6e" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">两年来，我一直在数据科学务虚会上使用 Apache Spark 向 100 多名学生教授应用机器学习。</p><p id="6a98" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">我的学生经常对<strong class="ld ir"> PySpark </strong>的一些<em class="lz">怪癖</em>感到困惑，还有一些时候，在使用传统的<em class="lz"> Pandas/Scikit-Learn </em>组合时，对<em class="lz">缺少一些科学家认为理所当然的功能</em>数据感到困惑。</p><p id="f5fb" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">我决定通过<strong class="ld ir">开发</strong>一个 Python 包来解决这些问题，这个包将使<strong class="ld ir">PySpark</strong>中的探索性数据分析变得更加容易……</p><h1 id="0069" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">HandySpark 简介</h1><p id="fcd3" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="ld ir"> HandySpark </strong>非常容易安装并集成到您的<em class="lz"> PySpark </em>工作流程中。只需 3 个步骤就能让你的数据框成为<strong class="ld ir">手办框</strong>:</p><ol class=""><li id="7573" class="ne nf iq ld b le md li me lm ng lq nh lu ni ly nj nk nl nm bi translated">使用<code class="fe nn no np nq b">pip install handyspark</code>安装<strong class="ld ir">手动停车</strong></li><li id="b053" class="ne nf iq ld b le nr li ns lm nt lq nu lu nv ly nj nk nl nm bi translated">用<code class="fe nn no np nq b">from handyspark import *</code>导入<strong class="ld ir">手动停车</strong></li><li id="ead2" class="ne nf iq ld b le nr li ns lm nt lq nu lu nv ly nj nk nl nm bi translated">用<code class="fe nn no np nq b">hdf = df.toHandy()</code>将你的<em class="lz">数据框</em>变成<strong class="ld ir">手持框</strong></li></ol><p id="ccae" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">导入<strong class="ld ir"> HandySpark </strong>后，方法<code class="fe nn no np nq b">toHandy</code>作为扩展被添加到<em class="lz"> Spark 的数据框架</em>中，所以你可以直接调用它。</p><p id="618a" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">让我们快速浏览一下你可以用<strong class="ld ir"> HandySpark </strong> :-)</p><h2 id="74b0" class="nw ke iq bd kf nx ny dn kj nz oa dp kn lm ob oc kr lq od oe kv lu of og kz oh bi translated">1.获取数据</h2><p id="383f" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">不再需要从<code class="fe nn no np nq b">Row</code>对象中进行繁琐的列选择、收集和手动提取！</p><p id="b2fc" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">现在你可以像在熊猫身上一样获取数据，使用<code class="fe nn no np nq b"><strong class="ld ir">cols</strong></code>:</p><p id="5434" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated"><code class="fe nn no np nq b">hdf.cols['Name'][:5]</code></p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="81f4" class="nw ke iq nq b gy oq or l os ot">0                              Braund, Mr. Owen Harris<br/>1    Cumings, Mrs. John Bradley (Florence Briggs Th...<br/>2                               Heikkinen, Miss. Laina<br/>3         Futrelle, Mrs. Jacques Heath (Lily May Peel)<br/>4                             Allen, Mr. William Henry<br/>Name: Name, dtype: object</span></pre><p id="2af6" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">简单多了，对吧？结果就是一个<em class="lz">熊猫系列！</em></p><blockquote class="ma mb mc"><p id="ad42" class="lb lc lz ld b le md lg lh li me lk ll mf mg lo lp mh mi ls lt mj mk lw lx ly ij bi translated">请记住，由于 Spark 中数据的分布式性质，<strong class="ld ir">只可能获取任何给定<strong class="ld ir"> HandyFrame </strong>的顶部行</strong>——所以，不，您仍然<em class="iq">不能</em>做类似于<code class="fe nn no np nq b">[3:5]</code>或<code class="fe nn no np nq b">[-1]</code>等等的事情…只有<code class="fe nn no np nq b">[:N]</code>。</p></blockquote><p id="e022" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">还有其他类似熊猫的方法:</p><p id="8964" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated"><code class="fe nn no np nq b">hdf.cols['Embarked'].value_counts(dropna=False)</code></p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="1411" class="nw ke iq nq b gy oq or l os ot">S      644<br/>C      168<br/>Q       77<br/>NaN      2<br/>Name: Embarked, dtype: int64</span></pre><p id="1939" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">如果你还没有猜到，上面的例子(以及这篇文章中的所有其他例子)是使用著名的<strong class="ld ir">泰坦尼克号</strong>数据集构建的:-)</p><h2 id="7d5e" class="nw ke iq bd kf nx ny dn kj nz oa dp kn lm ob oc kr lq od oe kv lu of og kz oh bi translated">2.绘图数据</h2><p id="972d" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">缺乏一种简单的可视化数据的方法总是困扰着我的学生。而且，当人们在网上搜索使用<em class="lz"> PySpark </em>绘制数据的例子时，更糟糕的是<strong class="ld ir">T42:<em class="lz">许多许多教程只是简单地将整个数据集转换成熊猫</em>然后以传统方式绘制。</strong></p><p id="94cd" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">求求你，<strong class="ld ir">千万别这么做</strong>！它肯定可以处理玩具数据集，但如果用于真正大的数据集(如果您使用 Spark，很可能会处理这些数据集)，它会失败得很惨。</p><p id="7268" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir"> HandySpark </strong>通过<em class="lz">使用 Spark 的分布式计算能力</em>正确计算统计数据，然后将结果转化为图表，从而解决了这个问题。然后，事情变得很简单:</p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="36c4" class="nw ke iq nq b gy oq or l os ot">fig, axs = plt.subplots(1, 4, figsize=(12, 4))<br/>hdf.cols['Embarked'].hist(ax=axs[0])<br/>hdf.cols['Age'].boxplot(ax=axs[1])<br/>hdf.cols['Fare'].boxplot(ax=axs[2])<br/>hdf.cols[['Fare', 'Age']].scatterplot(ax=axs[3])</span></pre><figure class="oi oj ok ol gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ou"><img src="../Images/6f9e9ba6f5579c5bfd8ffd3dbb2354b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JjfFmyDPNHmGoIZO.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Plotting with HandySpark!</figcaption></figure><p id="e942" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">是的，甚至还有<strong class="ld ir">散点图</strong>！这怎么可能呢？！<strong class="ld ir"> HandySpark </strong>将两个特征分别分成 30 个箱，计算 900 个组合中每一个的频率，并绘制相应大小的圆。</p><h2 id="6998" class="nw ke iq bd kf nx ny dn kj nz oa dp kn lm ob oc kr lq od oe kv lu of og kz oh bi translated">3.分层</h2><p id="5166" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如果您想使用<strong class="ld ir">分割-应用-组合</strong>方法执行<strong class="ld ir">分层</strong>操作，该怎么办？可能想到的第一个想法是使用<strong class="ld ir"> groupby </strong>操作…但是<strong class="ld ir"> groupby </strong>操作会在 Spark 中触发<strong class="ld ir">可怕的数据洗牌</strong>，所以应该避免使用。</p><p id="1e3a" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir"> HandySpark </strong>通过相应地过滤行，对数据的每个子集执行计算，然后组合结果来处理这个问题。例如:</p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="73f8" class="nw ke iq nq b gy oq or l os ot">hdf.stratify(['Pclass']).cols['Embarked'].value_counts()</span><span id="eb7c" class="nw ke iq nq b gy ov or l os ot">Pclass  Embarked<br/>1       C            85<br/>        Q             2<br/>        S           127<br/>2       C            17<br/>        Q             3<br/>        S           164<br/>3       C            66<br/>        Q            72<br/>        S           353<br/>Name: value_counts, dtype: int64</span></pre><p id="a1a1" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">您还可以通过利用<strong class="ld ir">桶</strong>或<strong class="ld ir">分位数</strong>对象，用非分类列对其进行<strong class="ld ir">分层。然后在<strong class="ld ir">分层图</strong>中使用它:</strong></p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="4ddd" class="nw ke iq nq b gy oq or l os ot">hdf.stratify(['Sex', Bucket('Age', 2)]).cols['Embarked'].hist()</span></pre><figure class="oi oj ok ol gt jr gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/ebcf1c35ed17933bfae86895721708ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/0*wiv3npqRjZzcqvCk.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Stratified histogram</figcaption></figure><h2 id="02c8" class="nw ke iq bd kf nx ny dn kj nz oa dp kn lm ob oc kr lq od oe kv lu of og kz oh bi translated">4.输入缺失值</h2><blockquote class="ma mb mc"><p id="6ce4" class="lb lc lz ld b le md lg lh li me lk ll mf mg lo lp mh mi ls lt mj mk lw lx ly ij bi translated">“你应该估算缺失的值”</p></blockquote><p id="b572" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">不过，首先要做的是。有多少缺失值？</p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="81b9" class="nw ke iq nq b gy oq or l os ot">hdf.isnull(ratio=True)</span><span id="e6f9" class="nw ke iq nq b gy ov or l os ot">PassengerId    0.000000<br/>Survived       0.000000<br/>Pclass         0.000000<br/>Name           0.000000<br/>Sex            0.000000<br/>Age            0.198653<br/>SibSp          0.000000<br/>Parch          0.000000<br/>Ticket         0.000000<br/>Fare           0.000000<br/>Cabin          0.771044<br/>Embarked       0.002245<br/>Name: missing(ratio), dtype: float64</span></pre><p id="1bf6" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">好了，现在我们知道有 3 列缺少值。让我们去掉<code class="fe nn no np nq b">Cabin</code>(毕竟，它的 77%的值都丢失了)，把注意力放在另外两列值的插补上:<code class="fe nn no np nq b">Age</code>和<code class="fe nn no np nq b">Embarked.</code></p><blockquote class="ma mb mc"><p id="dabd" class="lb lc lz ld b le md lg lh li me lk ll mf mg lo lp mh mi ls lt mj mk lw lx ly ij bi translated">在版本 2.2.0 发布<a class="ae kc" href="http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html#pyspark.ml.feature.Imputer" rel="noopener ugc nofollow" target="_blank">插补器</a>转换器之前，缺失值的插补无法集成到 Spark <em class="iq">管道</em>中。但是它仍然不能处理分类变量(如<code class="fe nn no np nq b">Embarked</code>)，更不用说分层插补了…</p></blockquote><p id="d7c4" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">让我们看看<strong class="ld ir"> HandySpark </strong>如何帮助我们完成这项任务:</p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="11c4" class="nw ke iq nq b gy oq or l os ot">hdf_filled = hdf.fill(categorical=['Embarked'])<br/>hdf_filled = (hdf_filled.stratify(['Pclass', 'Sex'])<br/>              .fill(continuous=['Age'], strategy=['mean']))</span></pre><p id="f2e9" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">首先，它使用<strong class="ld ir">最常见的值</strong>来填充<strong class="ld ir">分类</strong>列中缺失的值。然后，<strong class="ld ir">根据<code class="fe nn no np nq b">Pclass</code>和<code class="fe nn no np nq b">Sex</code>对数据集</strong>进行分层，以<strong class="ld ir">计算<code class="fe nn no np nq b">Age</code>的平均值</strong>，该平均值将用于插补。</p><p id="9964" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">它使用了哪些值进行插补？</p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="d931" class="nw ke iq nq b gy oq or l os ot">hdf_filled.statistics_</span><span id="5a89" class="nw ke iq nq b gy ov or l os ot">{'Age': {'Pclass == "1" and Sex == "female"': 34.61176470588235,<br/>  'Pclass == "1" and Sex == "male"': 41.28138613861386,<br/>  'Pclass == "2" and Sex == "female"': 28.722972972972972,<br/>  'Pclass == "2" and Sex == "male"': 30.74070707070707,<br/>  'Pclass == "3" and Sex == "female"': 21.75,<br/>  'Pclass == "3" and Sex == "male"': 26.507588932806325},<br/> 'Embarked': 'S'}</span></pre><p id="abe8" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">到目前为止，一切顺利！是时候<strong class="ld ir">将</strong>整合到一个火花<em class="lz">管道</em>中，生成一个<strong class="ld ir">自定义变压器</strong>与<strong class="ld ir"> </strong> <code class="fe nn no np nq b"><strong class="ld ir">transformers</strong></code>:</p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="5ea7" class="nw ke iq nq b gy oq or l os ot">imputer = hdf_filled.transformers.imputer()</span></pre><p id="5efc" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir">估算器</strong>对象现在是一个成熟的<strong class="ld ir">可序列化 PySpark 转换器</strong>！那是什么意思？你可以随意在你的<em class="lz">管道</em>和<em class="lz">保存/加载</em>中使用:-)</p><h2 id="a65a" class="nw ke iq bd kf nx ny dn kj nz oa dp kn lm ob oc kr lq od oe kv lu of og kz oh bi translated">5.检测异常值</h2><blockquote class="ma mb mc"><p id="081e" class="lb lc lz ld b le md lg lh li me lk ll mf mg lo lp mh mi ls lt mj mk lw lx ly ij bi translated">“你不能通过！”</p></blockquote><p id="1d81" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">我们不应该让多少异常值通过？</p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="6527" class="nw ke iq nq b gy oq or l os ot">hdf_filled.outliers(method='tukey', k=3.)</span><span id="dfbe" class="nw ke iq nq b gy ov or l os ot">PassengerId      0.0<br/>Survived         0.0<br/>Pclass           0.0<br/>Age              1.0<br/>SibSp           12.0<br/>Parch          213.0<br/>Fare            53.0<br/>dtype: float64</span></pre><p id="9e9d" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">目前只有<a class="ae kc" href="https://en.wikipedia.org/wiki/Outlier#Tukey's_fences" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir"> <em class="lz"> Tukey 的</em> </strong> </a> <strong class="ld ir">方法</strong>可用(我正在做<em class="lz"> Mahalanobis </em> <em class="lz">距离</em>！).该方法采用可选的<strong class="ld ir"> <em class="lz"> k </em> </strong>参数，您可以将其设置为更大的值(如 3)以允许更宽松的检测。</p><p id="36a7" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">以<code class="fe nn no np nq b">Fare</code>列为例。根据 Tukey 的方法，有<em class="lz"> 53 个异常值</em>。咱们<strong class="ld ir">栅栏</strong>他们！</p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="dadb" class="nw ke iq nq b gy oq or l os ot">hdf_fenced = hdf_filled.fence(['Fare'])</span></pre><p id="8a3c" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated"><em class="lz">下</em>和<em class="lz">上</em>栅栏值是多少？</p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="7ea8" class="nw ke iq nq b gy oq or l os ot">hdf_fenced.fences_</span><span id="3c0a" class="nw ke iq nq b gy ov or l os ot">{'Fare': [-26.7605, 65.6563]}</span></pre><p id="d056" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">记住，如果你愿意，你也可以进行<strong class="ld ir">分层击剑</strong> :-)</p><p id="cf9c" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">您可能已经猜到了，您也可以<strong class="ld ir">将</strong>这一步集成到您的<em class="lz">管道</em>中，生成相应的<strong class="ld ir">转换器</strong>:</p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="d890" class="nw ke iq nq b gy oq or l os ot">fencer = hdf_fenced.transformers.fencer()</span></pre><h2 id="2283" class="nw ke iq bd kf nx ny dn kj nz oa dp kn lm ob oc kr lq od oe kv lu of og kz oh bi translated">6.熊猫功能</h2><p id="b984" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在 Spark 2.3 中，<a class="ae kc" href="https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html" rel="noopener ugc nofollow" target="_blank">熊猫 UDF</a>发布了！这对我们<em class="lz"> PySpark </em>用户来说是一个<strong class="ld ir">重大改进</strong>，因为我们终于可以克服传统<em class="lz">用户定义函数</em>(UDF)带来的<em class="lz">性能瓶颈</em>。<strong class="ld ir">牛逼</strong>！</p><p id="4697" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir"> HandySpark </strong>更进一步，为你做所有繁重的工作:-)你只需要使用它的<code class="fe nn no np nq b"><strong class="ld ir">pandas</strong></code>对象和<em class="lz">voilà</em>——熊猫的许多功能立即可用！</p><p id="f9e6" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">例如，让我们使用<code class="fe nn no np nq b"><a class="ae kc" href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isin.html" rel="noopener ugc nofollow" target="_blank">isin</a></code>，就像你使用普通的<em class="lz">熊猫系列</em>一样:</p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="fa8b" class="nw ke iq nq b gy oq or l os ot">some_ports = hdf_fenced.pandas['Embarked'].isin(values=['C', 'Q'])<br/>some_ports</span><span id="e3c7" class="nw ke iq nq b gy ov or l os ot">Column&lt;b'udf(Embarked) AS `&lt;lambda&gt;(Embarked,)`'&gt;</span></pre><p id="afe1" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">但是，记住 Spark 有<strong class="ld ir">懒评</strong>，所以结果是一个<strong class="ld ir">列表达式</strong>，它利用了<strong class="ld ir">熊猫 UDF</strong>的力量。剩下唯一要做的就是实际上<strong class="ld ir">将</strong>结果分配到一个新列，对吗？</p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="e7b6" class="nw ke iq nq b gy oq or l os ot">hdf_fenced = hdf_fenced.assign(is_c_or_q=some_ports)<br/># What's in there?<br/>hdf_fenced.cols['is_c_or_q'][:5]</span><span id="55ce" class="nw ke iq nq b gy ov or l os ot">0     True<br/>1    False<br/>2    False<br/>3     True<br/>4     True<br/>Name: is_c_or_q, dtype: bool</span></pre><p id="e5c7" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">你说得对！<strong class="ld ir"> HandyFrame </strong>有一个非常方便的<strong class="ld ir">赋值</strong>方法，就像在<em class="lz">熊猫</em>里一样！</p><p id="99d2" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir">而且这还不是全部</strong>！来自<em class="lz">熊猫</em>的特殊化<code class="fe nn no np nq b"><strong class="ld ir">str</strong></code>和<code class="fe nn no np nq b"><strong class="ld ir">dt</strong></code>物品同样可用！例如，如果一个给定的字符串包含另一个子字符串，您希望<strong class="ld ir">查找</strong>该怎么办？</p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="80b1" class="nw ke iq nq b gy oq or l os ot">col_mrs = hdf_fenced.pandas['Name'].str.find(sub='Mrs.')<br/>hdf_fenced = hdf_fenced.assign(is_mrs=col_mrs &gt; 0)</span></pre><figure class="oi oj ok ol gt jr gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/5d629b8cc1826008c23c54028cc2dd4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/0*kNLVV27R3ZJsmj0i.png"/></div></figure><p id="b66d" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">有关所有支持功能的完整列表，请查看<a class="ae kc" href="https://github.com/dvgodoy/handyspark" rel="noopener ugc nofollow" target="_blank">库</a>。</p><h2 id="98a2" class="nw ke iq bd kf nx ny dn kj nz oa dp kn lm ob oc kr lq od oe kv lu of og kz oh bi translated">7.您自己的 UDF</h2><p id="93e5" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">天空才是极限！您可以创建<strong class="ld ir">常规 Python 函数</strong>并使用<strong class="ld ir">赋值</strong>创建新列:-)，它们将为您变成<strong class="ld ir">熊猫 UDF</strong>！</p><p id="9d83" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">您的函数(或<code class="fe nn no np nq b">lambda</code>)的参数应该有您想要使用的列的名称。比如说<code class="fe nn no np nq b">Fare</code>的<code class="fe nn no np nq b">log</code>:</p><pre class="oi oj ok ol gt om nq on oo aw op bi"><span id="3c3e" class="nw ke iq nq b gy oq or l os ot">import numpy as np<br/>hdf_fenced = hdf_fenced.assign(logFare=lambda Fare: np.log(Fare + 1))</span></pre><p id="30fe" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">您也可以使用接受多列作为参数的函数。请记住，默认的<strong class="ld ir">返回类型</strong>，即新列的数据类型，将与使用的第一列相同(在示例中为<code class="fe nn no np nq b">Fare</code>)。</p><p id="bada" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">也可以指定<strong class="ld ir">不同的返回类型</strong>——请查看<a class="ae kc" href="https://github.com/dvgodoy/handyspark" rel="noopener ugc nofollow" target="_blank">库</a>中的示例。</p><h2 id="3414" class="nw ke iq bd kf nx ny dn kj nz oa dp kn lm ob oc kr lq od oe kv lu of og kz oh bi translated">8.更好的例外</h2><p id="7873" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">Spark 异常是<strong class="ld ir"> loooong </strong> …无论什么时候出现故障，错误<em class="lz">都会通过看似无限的层冒出</em>！</p><p id="fd48" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">我总是建议我的学生将<em class="lz">一直向下滚动到</em>，然后向上滚动，试图找出问题的根源……但是，<strong class="ld ir">不再是了</strong>！</p><p id="edf1" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir"> HandySpark </strong>将解析错误，并在<strong class="ld ir">顶部</strong> :-)向您显示一个<strong class="ld ir">漂亮而醒目的红色摘要</strong>，它可能不完美，但肯定会有所帮助！</p><figure class="oi oj ok ol gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oy"><img src="../Images/898f59c7f39c1b89628cd482f3314878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*l9PuFx83cKmrkyMv.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Handy Exception</figcaption></figure><h2 id="8b4a" class="nw ke iq bd kf nx ny dn kj nz oa dp kn lm ob oc kr lq od oe kv lu of og kz oh bi translated">9.安全第一</h2><p id="50d6" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">一些数据帧操作，如<code class="fe nn no np nq b">collect</code>或<code class="fe nn no np nq b">toPandas</code>将<em class="lz">触发</em>检索数据帧的所有<em class="lz">行！</em></p><p id="887a" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">为了防止这些动作的不良副作用，<strong class="ld ir"> HandySpark </strong>实现了一个<strong class="ld ir">安全</strong>机制！它将自动<strong class="ld ir">限制输出</strong>为 1000 行:</p><figure class="oi oj ok ol gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oz"><img src="../Images/1e75cd8c37dd1ba2fee53eb7e31ea12f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_fnADkC4uahs-vFR.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Safety mechanism in action!</figcaption></figure><p id="6133" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">当然，你可以用<code class="fe nn no np nq b"><strong class="ld ir">set_safety_limit</strong></code>指定一个<strong class="ld ir">不同的极限</strong>或者豁出去告诉你的<strong class="ld ir">手柄</strong>到<strong class="ld ir">忽略</strong>安全使用<code class="fe nn no np nq b"><strong class="ld ir">safety_off</strong></code>。关闭安全机制对<strong class="ld ir">单次动作</strong>有好处，因为它会在返回请求的无限结果后<strong class="ld ir">返回</strong>。</p><h1 id="ce9b" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">最后的想法</h1><p id="c352" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我的目标是<strong class="ld ir">改善 PySpark 用户体验</strong>并允许从<em class="lz">熊猫</em>到<em class="lz"> Spark 数据帧</em>的<strong class="ld ir">更平滑的</strong> <strong class="ld ir">过渡</strong>，使得执行<strong class="ld ir">探索性数据分析</strong>和<strong class="ld ir">可视化</strong>数据更加容易。不用说，这是一项正在进行的工作，我已经计划了更多的改进。</p><p id="88c0" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated">如果你是使用<em class="lz"> PySpark </em>的数据科学家，我希望你尝试一下<strong class="ld ir"> HandySpark </strong>并<strong class="ld ir">让我知道你对它的想法:-) </strong></p><div class="ml mm gp gr mn mo"><a href="https://github.com/dvgodoy/handyspark" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd ir gy z fp mt fr fs mu fu fw ip bi translated">dvgodoy/handyspark</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">HandySpark -带来熊猫般的能力来激发数据帧</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">github.com</p></div></div><div class="mx l"><div class="pa l mz na nb mx nc jw mo"/></div></div></a></div><p id="8a51" class="pw-post-body-paragraph lb lc iq ld b le md lg lh li me lk ll lm mg lo lp lq mi ls lt lu mk lw lx ly ij bi translated"><em class="lz">如果你有什么想法、评论或者问题，请在下方留言或者联系我</em> <a class="ae kc" href="https://twitter.com/dvgodoy" rel="noopener ugc nofollow" target="_blank"> <em class="lz">推特</em> </a> <em class="lz">。</em></p></div></div>    
</body>
</html>