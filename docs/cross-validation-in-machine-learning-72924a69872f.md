# 机器学习中的交叉验证

> 原文：<https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f?source=collection_archive---------0----------------------->

![](img/4b5832b00294a83530a1029825040485.png)

总是需要验证你的机器学习模型的稳定性。我的意思是，你不能让模型适合你的训练数据，并希望它能准确地处理它从未见过的真实数据。你需要某种保证，你的模型已经从数据中得到了大多数正确的模式，并且它没有拾取太多的噪声，或者换句话说，它的偏差和方差很低。

## 确认

*这个决定量化变量间假设关系的数值结果是否可接受作为数据描述的过程，被称为* ***验证*** 。通常，模型的误差估计是在训练之后进行的，更好的说法是残差评估。在此过程中，对预测响应和原始响应之间的差异进行数值估计，也称为训练误差。然而，*这只能让我们了解我们的模型在用于训练它的数据上表现如何*。现在，模型可能对数据拟合不足或拟合过度。因此，这种评估技术的 ***问题在于，它没有给出学习者对独立/不可见数据集*** 的概括能力。得到关于我们模型的这个想法被称为交叉验证。

## 保持方法

现在，对此的一个*基本补救措施是删除一部分训练数据，并使用它从根据其余数据训练的模型中获得预测。*误差估计然后告诉我们的模型在看不见的数据或验证集上表现如何。**这是一种简单的交叉验证技术，也称为维持法**。尽管这种方法不需要任何计算开销，并且优于传统的 ***验证，但它仍然存在高方差*** 的问题。 ***这是因为不确定哪些数据点会出现在验证集中，不同的数据集结果可能完全不同。***

## ***K 倍交叉验证***

由于永远没有足够的数据来训练你的模型，为了验证而删除一部分数据会带来拟合不足的问题。 ***通过减少训练数据*** ， ***我们冒着丢失数据集中重要模式/趋势的风险，这反过来增加了由偏差引起的误差。*** 所以，我们需要的是一种既能为训练模型提供充足数据，又能为验证留下充足数据的方法。k 折叠交叉验证正是这样做的。

在 **K 折交叉验证**中，数据被分成 K 个子集。现在，保持方法被重复 k 次，使得每次 ***，k 个子集之一被用作测试集/验证集，而其他 k-1 个子集被放在一起形成训练集*** 。在所有 k 次试验中平均*误差估计，以获得我们的模型*的总有效性。可以看出，每个数据点恰好出现在验证集中一次，出现在训练集中 *k-1* 次。 ***这极大地减少了偏差，因为我们使用了大部分数据进行拟合，也极大地减少了方差，因为大部分数据也用于验证集中。交换训练集和测试集也增加了这种方法的有效性。**作为一般规则和经验证据，K = 5 或 10 通常是首选的**，但没有什么是固定的，它可以取任何值。***

## 分层 K 折交叉验证

*在某些情况下，响应变量中可能存在较大的不平衡*。例如，在关于房屋价格的数据集中，可能有大量价格高的房屋。或者在分类的情况下，阴性样本可能比阳性样本多几倍。对于这样的问题， ***在 K 倍交叉验证技术中做了一个微小的变化，使得每个倍包含与完整集近似相同百分比的每个目标类的样本，或者在预测问题的情况下，平均响应值在所有倍中近似相等。*** 这种变异也被称为**层状 K 褶**。

> **上述验证技术也被称为非穷举交叉验证方法。** *这些并不计算分割原始样本的所有方式，也就是说，你只需决定需要制作多少个子集。此外，这些是下面解释的* **方法的近似，也称为穷举方法，它计算所有可能的方式将数据分成训练集和测试集。**

## 遗漏交叉验证

这种方法将 p 个数据点排除在训练数据之外，即如果原始样本中有 n 个数据点，则 n-p 个样本用于训练模型，p 个点用作验证集。对原始样本可以这样分离的所有组合重复这一过程，然后对所有试验的误差进行平均，以给出总体有效性。

*该方法是详尽的，因为它需要为所有可能的组合训练和验证模型，并且对于适度大的 p，它在计算上变得不可行。*

这种方法的一个特例是当 p = 1 时。这就是所谓的遗漏交叉验证。这种方法通常优于前一种方法，因为 ***它不会遭受密集的计算，因为可能组合的数目等于原始样本中数据点的数目或 n.***

交叉验证是 ***一种非常有用的评估模型有效性的技术，尤其是在你需要减轻过度拟合的情况下。******也可用于确定您的*** 型号的超参数，即哪些参数会导致最低的测试误差。这是你开始交叉验证所需要的所有基础。您可以使用 **Scikit-Learn** 开始使用各种验证技术，只需几行 python 代码就可以启动并运行。

如果你喜欢这篇文章，一定要点击下面的❤来推荐它，如果你有任何问题，**留下评论**，我会尽力回答。

为了更加了解机器学习的世界，**跟我来**。这是最好的办法，等我多写点这样的文章就知道了。

你也可以在 [**关注我【推特】**](https://twitter.com/Prashant_1722)[**直接发邮件给我**](mailto:pr.span24@gmail.com) 或者 [**在 linkedin**](https://www.linkedin.com/in/prashantgupta17/) 上找我。我很乐意收到你的来信。

乡亲们，祝你们有美好的一天:)