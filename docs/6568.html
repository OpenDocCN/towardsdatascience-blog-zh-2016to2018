<html>
<head>
<title>Andrew Ng’s Machine Learning Course in Python (Regularized Logistic Regression) + Lasso Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">吴恩达的机器学习教程 Python(正则化 Logistic 回归)+ Lasso 回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-regularized-logistic-regression-lasso-regression-721f311130fb?source=collection_archive---------8-----------------------#2018-12-19">https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-regularized-logistic-regression-lasso-regression-721f311130fb?source=collection_archive---------8-----------------------#2018-12-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/1183d0f6807f9341c621fae0ec52d282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oOAH2Zl1i7tu0rbIwHDnmg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Machine Learning — Andrew Ng</figcaption></figure><p id="692d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">从编程任务 2 <a class="ae ld" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-logistic-regression-c0ae25509feb" rel="noopener">(逻辑回归)</a>继续，我们现在将继续进行 python 中的正则化逻辑回归，以帮助我们处理过度拟合的问题。</p><p id="2e07" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">正则化是收缩方法，通过减少模型的方差，将系数收缩到零，以防止过度拟合。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><p id="d098" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi ll translated">直接进入任务，我们从导入所有相关的库和数据集开始。这一次，数据集包含了一个工厂中微芯片的两个测试结果，我们将使用测试结果来预测微芯片应该被接受还是被拒绝。</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="3b06" class="md me it lz b gy mf mg l mh mi">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span><span id="ee28" class="md me it lz b gy mj mg l mh mi">df=pd.read_csv("ex2data2.txt", header=None)<br/>df.head()<br/>df.describe()</span></pre><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mk"><img src="../Images/808374ca1552f73bca7ef9552c44febe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FY9TdJSDmhz3PWQEKQrDxQ.png"/></div></div></figure><p id="4fd8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如您所见，这是一个多变量、二元分类问题，我们可以使用逻辑回归来解决。</p><p id="794a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在来看数据。与之前的逻辑回归可视化一样，x1 和 x2 的每种组合导致接受微芯片，相对于导致拒绝的组合作图</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="1c32" class="md me it lz b gy mf mg l mh mi">X=df.iloc[:,:-1].values<br/>y=df.iloc[:,-1].values</span><span id="00a7" class="md me it lz b gy mj mg l mh mi">pos , neg = (y==1).reshape(118,1) , (y==0).reshape(118,1)<br/>plt.scatter(X[pos[:,0],0],X[pos[:,0],1],c="r",marker="+")<br/>plt.scatter(X[neg[:,0],0],X[neg[:,0],1],marker="o",s=10)<br/>plt.xlabel("Test 1")<br/>plt.ylabel("Test 2")<br/>plt.legend(["Accepted","Rejected"],loc=0)</span></pre><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ml"><img src="../Images/0fe3949142ffda4a3ff13edd0ee81350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*Ow1w7RBKaCG0fK0LvHOVug.png"/></div></div></figure><p id="c6c8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">绘制数据清楚地表明，区分不同类别的决策边界是非线性的。这导致下一步的特征映射，我们添加额外的多项式项来尝试和更好地拟合数据(正常的逻辑回归只能拟合线性决策边界，在这种情况下不会做得很好)。在作业中决定，我们将增加多项式的 6 次方项。</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="12fe" class="md me it lz b gy mf mg l mh mi">def mapFeature(x1,x2,degree):<br/>    """<br/>    take in numpy array of x1 and x2, return all polynomial terms up to the given degree<br/>    """<br/>    out = np.ones(len(x1)).reshape(len(x1),1)<br/>    for i in range(1,degree+1):<br/>        for j in range(i+1):<br/>            terms= (x1**(i-j) * x2**j).reshape(len(x1),1)<br/>            out= np.hstack((out,terms))<br/>    return out</span><span id="fca1" class="md me it lz b gy mj mg l mh mi">X = mapFeature(X[:,0], X[:,1],6)</span></pre><p id="09bf" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><code class="fe mm mn mo lz b">mapFeature</code>函数还将一列 1 加到 X 上，这样我们就不必在以后处理它了。这里，我决定使用<code class="fe mm mn mo lz b">np.hstack</code>而不是<code class="fe mm mn mo lz b">np.append</code>向 numpy 数组添加一个新列。我发现与我通常使用的<code class="fe mm mn mo lz b">np.append</code>相比，<code class="fe mm mn mo lz b">np.hstack</code>的代码更加简洁。在这里，我允许 degree 作为一个参数，而不是像在作业中那样将其固定为 6，可以随意使用不同的 degree 并比较结果。</p><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/1ca38c8013be3c8cde30d53a4a940e9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*hd8NMVFyPqhMs_zkrBdfHQ.png"/></div></figure><p id="6982" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这个图表有助于形象化我们正在做的事情和涉及的多项式项。</p><p id="b932" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来，我们继续定义计算正则化成本函数和梯度的函数。请记住，成本函数现在有一个由λ控制的附加收缩损失。</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="4844" class="md me it lz b gy mf mg l mh mi">def sigmoid(z):<br/>    """<br/>    return the sigmoid of z<br/>    """<br/>    <br/>    return 1/ (1 + np.exp(-z))</span><span id="0d3a" class="md me it lz b gy mj mg l mh mi">def costFunctionReg(theta, X, y ,Lambda):<br/>    """<br/>    Take in numpy array of theta, X, and y to return the regularize cost function and gradient<br/>    of a logistic regression<br/>    """<br/>    <br/>    m=len(y)<br/>    y=y[:,np.newaxis]<br/>    predictions = sigmoid(X @ theta)<br/>    error = (-y * np.log(predictions)) - ((1-y)*np.log(1-predictions))<br/>    cost = 1/m * sum(error)<br/>    regCost= cost + Lambda/(2*m) * sum(theta**2)<br/>    <br/>    # compute gradient<br/>    j_0= 1/m * (X.transpose() @ (predictions - y))[0]<br/>    j_1 = 1/m * (X.transpose() @ (predictions - y))[1:] + (Lambda/m)* theta[1:]<br/>    grad= np.vstack((j_0[:,np.newaxis],j_1))<br/>    return regCost[0], grad</span></pre><p id="5f73" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">您可以使用相同的<code class="fe mm mn mo lz b">costFunction</code>代码并添加一个λ项来计算正则化成本函数。根据官方 numpy <a class="ae ld" href="https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.dot.html" rel="noopener ugc nofollow" target="_blank">文档</a>，我发现用<code class="fe mm mn mo lz b">@</code>替换<code class="fe mm mn mo lz b">np.dot</code>是矩阵乘法的首选选项。写<code class="fe mm mn mo lz b">@</code>也容易多了，所以我觉得很酷。如果你不熟悉 numpy 广播，你可以在这里查看<a class="ae ld" href="https://docs.scipy.org/doc/numpy-1.15.0/user/basics.broadcasting.html" rel="noopener ugc nofollow" target="_blank"/>。广播是我需要使用<code class="fe mm mn mo lz b">y=y[:,np.newaxis]</code>给 y 添加一个新轴的原因，以确保线性代数操作如我所料。如果你已经注意到了，我在之前的实现中使用了<code class="fe mm mn mo lz b">reshape()</code>来处理广播(告诉你们我正在边做边学)。顺便说一下，<code class="fe mm mn mo lz b">np.vstack</code>在这里添加到新的一行而不是一列，用于<code class="fe mm mn mo lz b">np.hstack</code>。</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="1422" class="md me it lz b gy mf mg l mh mi"># Initialize fitting parameters<br/>initial_theta = np.zeros((X.shape[1], 1))</span><span id="5241" class="md me it lz b gy mj mg l mh mi"># Set regularization parameter lambda to 1<br/>Lambda = 1</span><span id="9483" class="md me it lz b gy mj mg l mh mi">#Compute and display initial cost and gradient for regularized logistic regression<br/>cost, grad=costFunctionReg(initial_theta, X, y, Lambda)</span><span id="c9d5" class="md me it lz b gy mj mg l mh mi">print("Cost at initial theta (zeros):",cost)</span></pre><p id="a411" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">打印报表 print:初始θ(零)处的成本:<code class="fe mm mn mo lz b">0.6931471805599461</code></p><p id="33b1" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">至于优化算法，我再次使用标准梯度下降，而不是<code class="fe mm mn mo lz b">fminunc</code>。python 做<code class="fe mm mn mo lz b">fminunc</code>的方式可以在<a class="ae ld" href="https://docs.scipy.org/doc/scipy-0.10.0/reference/tutorial/optimize.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="225b" class="md me it lz b gy mf mg l mh mi">def gradientDescent(X,y,theta,alpha,num_iters,Lambda):<br/>    """<br/>    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps<br/>    with learning rate of alpha<br/>    <br/>    return theta and the list of the cost of theta during each iteration<br/>    """<br/>    <br/>    m=len(y)<br/>    J_history =[]<br/>    <br/>    for i in range(num_iters):<br/>        cost, grad = costFunctionReg(theta,X,y,Lambda)<br/>        theta = theta - (alpha * grad)<br/>        J_history.append(cost)<br/>    <br/>    return theta , J_history</span><span id="c7fd" class="md me it lz b gy mj mg l mh mi">theta , J_history = gradientDescent(X,y,initial_theta,1,800,0.2)</span><span id="d79e" class="md me it lz b gy mj mg l mh mi">print("The regularized theta using ridge regression:\n",theta)</span></pre><p id="e3b7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">print 语句将打印:</p><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/9312230edf08d263c88f3c95cf1489cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*I-9tCYztJcpVMefIny0L4g.png"/></div></figure><p id="2e36" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">同样，没有给出 alpha、num_iters 和λ值，尝试一些值的组合，得出最佳值。</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="a5e7" class="md me it lz b gy mf mg l mh mi">plt.plot(J_history)<br/>plt.xlabel("Iteration")<br/>plt.ylabel("$J(\Theta)$")<br/>plt.title("Cost function using Gradient Descent")</span></pre><p id="bfed" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">使用我陈述的值，这是相对于迭代次数绘图的结果成本函数。</p><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/2f63b9b126fda3f562c82fe22a55c2e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*ISaHXBzC6OF8VNREFG_8EA.png"/></div></figure><p id="c1ca" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">降低成本函数—检查<br/>成本函数平稳—检查</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="b1f2" class="md me it lz b gy mf mg l mh mi">def mapFeaturePlot(x1,x2,degree):<br/>    """<br/>    take in numpy array of x1 and x2, return all polynomial terms up to the given degree<br/>    """<br/>    out = np.ones(1)<br/>    for i in range(1,degree+1):<br/>        for j in range(i+1):<br/>            terms= (x1**(i-j) * x2**j)<br/>            out= np.hstack((out,terms))<br/>    return out</span><span id="2c3a" class="md me it lz b gy mj mg l mh mi">plt.scatter(X[pos[:,0],1],X[pos[:,0],2],c="r",marker="+",label="Admitted")<br/>plt.scatter(X[neg[:,0],1],X[neg[:,0],2],c="b",marker="x",label="Not admitted")</span><span id="a9dd" class="md me it lz b gy mj mg l mh mi"># Plotting decision boundary</span><span id="fa20" class="md me it lz b gy mj mg l mh mi">u_vals = np.linspace(-1,1.5,50)<br/>v_vals= np.linspace(-1,1.5,50)<br/>z=np.zeros((len(u_vals),len(v_vals)))<br/>for i in range(len(u_vals)):<br/>    for j in range(len(v_vals)):<br/>        z[i,j] =mapFeaturePlot(u_vals[i],v_vals[j],6) @ theta</span><span id="81be" class="md me it lz b gy mj mg l mh mi">plt.contour(u_vals,v_vals,z.T,0)<br/>plt.xlabel("Exam 1 score")<br/>plt.ylabel("Exam 2 score")<br/>plt.legend(loc=0)</span></pre><p id="32a1" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">绘制非线性决策边界，包括绘制分隔不同类别的等高线。我做了一些谷歌搜索，这个<a class="ae ld" href="https://stackoverflow.com/questions/22294241/plotting-a-decision-boundary-separating-2-classes-using-matplotlibs-pyplot" rel="noopener ugc nofollow" target="_blank"> stackoverflow </a>答案可能会对在座的一些人有所帮助。代码只是简单的翻译了作业中给出的八度音阶代码，为了代码背后的数学和直觉，请查看上面给出的 stackoverflow 链接。</p><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/89108ecae16a449598b4380f7090e951.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*2nbCy67ynaHFnkm3j9jWrg.png"/></div></figure><p id="8c48" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我会说非常好！</p><p id="0f58" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了检查模型的准确性，我们再次利用训练数据的正确分类百分比。</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="5bc4" class="md me it lz b gy mf mg l mh mi">def classifierPredict(theta,X):<br/>    """<br/>    take in numpy array of theta and X and predict the class <br/>    """<br/>    predictions = X.dot(theta)<br/>    <br/>    return predictions&gt;0</span><span id="4704" class="md me it lz b gy mj mg l mh mi">p=classifierPredict(theta,X)<br/>print("Train Accuracy:", (sum(p==y[:,np.newaxis])/len(y) *100)[0],"%")</span></pre><p id="9847" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">打印报表打印:训练精度:<code class="fe mm mn mo lz b">83.05084745762711%</code>。接近，但没有使用<code class="fe mm mn mo lz b">fminunc</code>在赋值中获得的<code class="fe mm mn mo lz b">88.983051%</code>高。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><p id="949e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来，我想谈谈套索回归，另一种用于防止过度拟合的正则化方法。参考《统计学习导论》，Lasso 回归比岭回归(我们刚刚做的正则化)有明显的优势。也就是说，虽然岭回归将系数缩小到零，但它永远不会将其减小到零，因此，无论系数的值有多小，所有要素都将包含在模型中。另一方面，套索回归能够将系数缩小到恰好为零，减少特征的数量，同时充当特征选择工具。这使得套索回归在高维情况下很有用，并有助于模型的可解释性。</p><p id="3bc1" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于 Lasso 回归，要最小化的成本函数与岭回归非常相似。</p><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mt"><img src="../Images/fb85ed95dfa9d832380cb57f41e98884.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gnMQeibYKc_YCtKW6_2hbA.png"/></div></div></figure><p id="16b7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">它不是将θ平方之和相加，而是使用绝对值，由于它涉及绝对值，计算起来很困难，因为它是不可微的。各种算法可用于计算这一点，使用 sklearn 库可以找到一个这样的例子。</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="51a2" class="md me it lz b gy mf mg l mh mi">from sklearn.linear_model import LogisticRegression</span><span id="d0b6" class="md me it lz b gy mj mg l mh mi">clf = LogisticRegression(penalty="l1")<br/>clf.fit(X,y)</span><span id="9d22" class="md me it lz b gy mj mg l mh mi">thetaLasso=clf.coef_<br/>print("The regularized theta using lasso regression:\n",thetaLasso.reshape(28,1))</span></pre><p id="21e9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">θ值的并排比较。</p><figure class="lu lv lw lx gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mu"><img src="../Images/09543c9e1d3dd154b622c2dee57655ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sDrzQL_ua_hG7GwgalQz9A.png"/></div></div></figure><p id="159c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如您所见，lasso 回归将几个要素减少到 0，从而降低了问题的维度并提高了模型的可解释性。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><p id="0204" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这是我所有的正规化。Jupyter 笔记本会上传到我的 GitHub 上(<a class="ae ld" href="https://github.com/Benlau93/Machine-Learning-by-Andrew-Ng-in-Python" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/Ben lau 93/Machine-Learning-by-Andrew-Ng-in-Python</a>)。</p><p id="c545" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于本系列中的其他 python 实现，</p><ul class=""><li id="4ab9" class="mv mw it kh b ki kj km kn kq mx ku my ky mz lc na nb nc nd bi translated"><a class="ae ld" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-linear-regression-dd04fba8e137" rel="noopener">线性回归</a></li><li id="71e4" class="mv mw it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated"><a class="ae ld" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-logistic-regression-c0ae25509feb" rel="noopener">逻辑回归</a></li><li id="5589" class="mv mw it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated"><a class="ae ld" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-neural-networks-e526b41fdcd9" rel="noopener">神经网络</a></li><li id="4b58" class="mv mw it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated"><a class="ae ld" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-support-vector-machines-435fc34b7bf9" rel="noopener">支持向量机</a></li><li id="3245" class="mv mw it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated"><a class="ae ld" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-kmeans-clustering-pca-b7ba6fafa74" rel="noopener">无监督学习</a></li><li id="76bc" class="mv mw it kh b ki ne km nf kq ng ku nh ky ni lc na nb nc nd bi translated"><a class="ae ld" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-anomaly-detection-1233d23dba95" rel="noopener">异常检测</a></li></ul><p id="a72e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">感谢您的阅读。</p></div></div>    
</body>
</html>