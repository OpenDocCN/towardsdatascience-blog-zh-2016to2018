<html>
<head>
<title>Beyond Word Embeddings Part 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超越单词嵌入第 4 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beyond-word-embeddings-part-4-introducing-semantic-structure-to-neural-nlp-96cf8a2723fb?source=collection_archive---------9-----------------------#2018-11-21">https://towardsdatascience.com/beyond-word-embeddings-part-4-introducing-semantic-structure-to-neural-nlp-96cf8a2723fb?source=collection_archive---------9-----------------------#2018-11-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="d2d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将语义结构引入神经自然语言处理</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="ab gu cl kq"><img src="../Images/d5cd4e15b245d559bc753c983be319d9.png" data-original-src="https://miro.medium.com/v2/0*K8eg3bUVu4AG-4FB"/></div></figure><h1 id="308d" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">TLDR；</h1><p id="5bd9" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">自从<a class="ae lw" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec </a>的出现，神经单词嵌入已经成为在 NLP 应用中封装分布式语义的常用方法。本系列将回顾使用预训练单词嵌入的优点和缺点，并演示如何将更复杂的语义表示方案(如语义角色标记、抽象意义表示和语义依赖解析)整合到您的应用程序中。</p><h2 id="746d" class="lx ku iq bd kv ly lz dn kz ma mb dp ld jy mc md lh kc me mf ll kg mg mh lp mi bi translated">介绍</h2><p id="ecc8" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">本系列的最后几篇文章回顾了神经 NLP 中的一些<a class="ae lw" rel="noopener" target="_blank" href="/beyond-word-embeddings-part-1-an-overview-of-neural-nlp-milestones-82b97a47977f">最近的里程碑</a>，将单词表示为向量的<a class="ae lw" rel="noopener" target="_blank" href="/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec">方法和利用它们的架构进展</a>，以及现有神经 NLP 系统的常见<a class="ae lw" rel="noopener" target="_blank" href="/beyond-word-embeddings-part-3-four-common-flaws-in-state-of-the-art-neural-nlp-models-c1d35d3496d0">陷阱</a>。</p><p id="6a8d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们在之前的文章中所看到的，尽管诸如 attention 等先进的神经技术有助于建立更强大的 NLP 模型，但它们仍然无法捕捉对语言的牢固理解，从而导致了经常意想不到的结果。</p><p id="32e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章将关注于将语言结构整合到自然语言处理应用中的形式主义的发展。</p><p id="026d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们在上一篇文章中所看到的，自然语言有两个特性使得建模变得很困难，模糊性和可变性。我非常感谢<a class="ae lw" href="https://www.linkedin.com/in/ayal-klein-33298a61/" rel="noopener ugc nofollow" target="_blank">BIU NLP 实验室</a><a class="ae lw" href="http://u.cs.biu.ac.il/~nlp/" rel="noopener ugc nofollow" target="_blank">的 Ayal Klein </a>和<a class="ae lw" href="http://u.cs.biu.ac.il/~dagan/" rel="noopener ugc nofollow" target="_blank"> Ido Dagan </a>对这篇文章的帮助。</p><h2 id="3e3c" class="lx ku iq bd kv ly lz dn kz ma mb dp ld jy mc md lh kc me mf ll kg mg mh lp mi bi translated"><strong class="ak">歧义</strong></h2><p id="01cb" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">根据出现的上下文，某些单词或短语可以有多种不同的词义。例如，句子“像苹果一样的果蝇”有两种模糊的潜在含义。这个短语可能指的是一种喜欢吃 T21 苹果的飞行昆虫，也可能指的是如果一个人穿过一个水果，它会像苹果一样飞行。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/7a204b6224b002f1d253388bbdd7e078.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/0*YpeZUf3PXh6wHWx5.jpg"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">The 6th Century BCE linguist <a class="ae lw" href="https://en.wikipedia.org/wiki/Pāṇini" rel="noopener ugc nofollow" target="_blank">Pāṇini</a></figcaption></figure><p id="b70f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自公元前 6 世纪的耶斯卡和 Pāṇini 以来，语言学家们已经认识到某些自然单词表现出共同的句法模式和相关的语义属性。为了解决歧义，语言学家定义了某些语法语言属性，如<strong class="jp ir"> <em class="mj">词性、语态和时态</em> </strong>来帮助区分歧义短语。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/61310eebf720fb52aba67c49eb87169a.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/0*RF8VbSD-Xeeo_PMI.jpg"/></div></figure><p id="4dee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然句法属性如<strong class="jp ir">词性</strong>有助于区分单个单词的预期含义，但它们无法捕捉单词之间的关系。</p><h2 id="2c6e" class="lx ku iq bd kv ly lz dn kz ma mb dp ld jy mc md lh kc me mf ll kg mg mh lp mi bi translated"><strong class="ak">句法</strong> <strong class="ak">语法</strong></h2><p id="40f9" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">随着 20 世纪 50 年代早期计算机的发展，为了处理文本，对解析单词表示之间的关系的形式化技术重新产生了兴趣。</p><p id="9ace" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">借鉴前人的正式数学著作，语言学家诺姆·乔姆斯基创造了<strong class="jp ir">短语结构语法</strong>，其中单个单词用<a class="ae lw" href="https://en.wikipedia.org/wiki/Terminal_and_nonterminal_symbols" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">终结符</strong>表示，它们的词性为<strong class="jp ir">非</strong> <strong class="jp ir">终结符</strong> <strong class="jp ir">规则</strong> </a>，可以用更复杂的<strong class="jp ir">成分结构</strong>如名词短语来构建。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mq"><img src="../Images/178d0cabdd7d1564916496b6150326b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*chG51Iqw7ajqPu7eye-7Xw.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">With parts of speech a linguist can easily differentiate between the two potential ambiguous meaning of the above phrase. Constituent structures help formalize part of speech identification by</figcaption></figure><p id="fd92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在实践中，为这些系统创建规则是一项艰苦的工作，而且非常脆弱，因为我们对语言的理解不是决定性的。虽然这些系统取得了一些早期的成功，如 SHRDLU，但这些系统并不比玩具程序多多少。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/df5400e5e37e75fd60469b42a693d42c.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*5bzb-y9yExuy71mh.jpg"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><a class="ae lw" href="https://www.youtube.com/watch?v=bo4RvYJYOzI" rel="noopener ugc nofollow" target="_blank">SHRDLU in action</a></figcaption></figure><h2 id="bb5d" class="lx ku iq bd kv ly lz dn kz ma mb dp ld jy mc md lh kc me mf ll kg mg mh lp mi bi translated"><strong class="ak">可变性</strong></h2><p id="1d9e" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">虽然近年来神经系统的<strong class="jp ir">出现有助于关于词性标注和成分分析和</strong>的<strong class="jp ir">和<em class="mj">的最新技术成果，但是它们仍然不能有效地概括共享语义的不同句法短语。</em></strong></p><p id="aabb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了更好地理解这一点，认识到自然语言可以以多种形式表达是有帮助的。</p><p id="d9d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们回到<a class="ae lw" href="http://tbd" rel="noopener ugc nofollow" target="_blank">之前帖子</a>中的例子，其中两个句子，如“<em class="mj">看着孩子受苦没有乐趣</em>”和“<em class="mj">看着孩子受苦，没有乐趣</em>”具有不同的句法结构，但仍然嵌入了相同的语义关系，如<em class="mj">孩子、受苦、观看、乐趣</em>和<em class="mj">没有</em>。</p><p id="8735" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下章节将涵盖一些不同的语义结构，这些语义结构有助于统一出现在不同句法上下文中但保持相同语义的单词之间的关系，不仅有助于更好地理解上下文，还有助于更好地理解词汇推理。</p><h1 id="45ab" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">语义结构</h1><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mw"><img src="../Images/30bae07a1429e04b08f40fab6aecc11f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DBYkvnWmPWqQeYJD"/></div></div></figure><h2 id="46b2" class="lx ku iq bd kv ly lz dn kz ma mb dp ld jy mc md lh kc me mf ll kg mg mh lp mi bi translated"><strong class="ak">语义角色标注(SRL) </strong></h2><p id="f753" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">SRL 旨在恢复一个句子的<strong class="jp ir">动词</strong> <strong class="jp ir">述元结构</strong>，如谁对谁做了什么，何时，为何，何地以及如何。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/f567e16a6f70e65677407c16c0f0f470.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/0*aS4NwRE53ZPg3gyZ.gif"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">For a relatively enjoyable introduction to <a class="ae lw" href="https://www.youtube.com/watch?v=Erc4S60eeTI" rel="noopener ugc nofollow" target="_blank">predicate argument structure</a> see this classic video from school house rock</figcaption></figure><p id="c56c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将<strong class="jp ir"> <em class="mj">谓词视为函数</em> </strong>，将<strong class="jp ir"> <em class="mj">语义角色视为类类型化参数</em> </strong>。AllenNLP 提供了一个最新的 SRL 标记器，可以用来映射动词谓词和论元之间的语义关系。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi my"><img src="../Images/8245c1b9e48fc1cf094c52f05dbd4b01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*_BTbYSeAEjcQ2nlUMBDd2Q.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Allen NLP SRL model</figcaption></figure><p id="46be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">语义角色标注的一个挑战是，虽然更容易解析，但它只映射给定句子的动词谓语论元信息，因此这种表示本质上无法捕捉副词和形容词之间的重要上下文关系。此外，谓词\处理复杂事件共指需要消除歧义。</p><h2 id="7b6e" class="lx ku iq bd kv ly lz dn kz ma mb dp ld jy mc md lh kc me mf ll kg mg mh lp mi bi translated"><strong class="ak">抽象意义表征</strong></h2><p id="b293" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">AMR 是从句子结构中抽象出关系的语义表示，例如在下图中，三个句子在同一个 AMR 图中表示。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/12aa2e160afb7ad3a0b5afb477210d0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*hCMI2wsIbsNu930dRqkGOg.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><a class="ae lw" href="https://www.slideshare.net/convcomp2016/verso-la-chat-intelligente-la-ricerca-in-natural-language-processing-e-machine-learning" rel="noopener ugc nofollow" target="_blank">Convcomp2016: Verso la “chat intelligente”: la ricerca in Natural Language Processing e Machine Learning</a></figcaption></figure><p id="1901" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">AMR 图是有根的、有标签的、有向的、无环的图(<a class="ae lw" href="https://en.wikipedia.org/wiki/Directed_acyclic_graph" rel="noopener ugc nofollow" target="_blank"> DAGs </a>)，包括整个句子。AMR 代表偏向于英语——它并不意味着作为一种国际辅助语言。虽然 AMR 到文本生成很有前途，但是准确解析 JAMR 仍然是一个公开的问题。JAMR 解析器是一个既能解析又能生成 AMR 语句表示的解析器。</p><h2 id="f6df" class="lx ku iq bd kv ly lz dn kz ma mb dp ld jy mc md lh kc me mf ll kg mg mh lp mi bi translated">语义依赖分析(SDP)</h2><p id="3c9f" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">SDP 任务类似于上面的 SRL 任务，除了目标是捕获一个句子中所有实词的<strong class="jp ir">谓词-论元</strong>关系。艾尔。, 2014).这些关系是由不同的语言学派生的语义语法定义的。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi na"><img src="../Images/dd46db45c5c501ce84d971e2b8b179a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dLBQ9whZ0m2ttjA3o88ipQ.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Example Semantic Dependency Grammars</figcaption></figure><p id="e3b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与 SRL 不同，SDP 解析考虑了所有实词之间的所有语义关系，而不仅仅是动词和名词谓词。因此，它们不需要谓词意义歧义消除，并且能够表示更广泛的语义现象。</p><p id="0633" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，与 AMR 语义依赖解析不同，SDP 与句子标记对齐，这意味着它们更容易与神经 NLP 序列模型一起解析，同时仍然保持语义泛化。</p><p id="8f3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">语义依赖解析任务已经有了稳步的改进，对于这个任务，最好的开源工具之一是 call NeurboParser。然而，由于 NeurboParser 是用 C++编写的，安装和使用起来很棘手，所以我冒昧地为这个工具编写了一个 python 包装器和 docker 环境，可以在这里找到。</p><div class="nb nc gp gr nd ne"><a href="https://github.com/aribornstein/pyNeurboParser" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ir gy z fp nj fr fs nk fu fw ip bi translated">aribornstein/pyNeurboParser</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">NeurboParser 的 python 包装器。通过在…上创建帐户，为 aribornstein/pyNeurboParser 的开发做出贡献</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">github.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns kr ne"/></div></div></a></div><p id="5581" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">未来的帖子将记录如何开始使用 pyNeurboParser。</p><h2 id="d3c6" class="lx ku iq bd kv ly lz dn kz ma mb dp ld jy mc md lh kc me mf ll kg mg mh lp mi bi translated">开放知识表示 OKR</h2><p id="7dd3" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">上述语义表示只映射到句子级标记。在现实世界的实际应用中，表示跨多个句子、段落和文档的数据之间的关系是很重要的。</p><p id="be1c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个早期的尝试是提供一个统一的知识表示来链接语义分析与事件和实体相互引用，这就是 T2 OKR:一个统一的多文本开放知识表示。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/9f2c38d7b36b2aff88aea591f8de6849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*zDs4KyBVl9gA7cHPwG6ZYA.png"/></div></figure><p id="2724" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Ori Shapira 的一个演示应用于<a class="ae lw" href="https://orishapira.files.wordpress.com/2017/08/2017emnlp_ias2.pdf" rel="noopener ugc nofollow" target="_blank">事件新闻推文的交互式抽象摘要</a>任务，OKR 将多条推文映射为语义摘要<a class="ae lw" href="http://u.cs.biu.ac.il/~shapiro1/okr/" rel="noopener ugc nofollow" target="_blank">在此</a>可以找到。</p><h1 id="e0af" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">总结和未来工作</h1><p id="1d4c" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">上述语义表示的一个关键挑战是它们是由语言学家在特定领域语料库上开发的，并且它们可能是复杂的和难以理解的。</p><p id="fe99" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">目前有许多 NLP 实验室，如华盛顿大学、巴尔-伊兰大学、脸书人工智能研究所和艾伦人工智能研究所，它们正在致力于生成新的语义自然语言语法，这些语法是由解析它们的文档驱动的。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nu"><img src="../Images/5b529782c53517e952e58f6e89895a4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*86OvKl87ec088Va-Ifm0dQ.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">QA- SRL demo learned relations from natural text <a class="ae lw" href="http://qasrl.org/" rel="noopener ugc nofollow" target="_blank">http://qasrl.org/</a></figcaption></figure><p id="8bfe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这项工作的一个例子是 QA-SRL，它试图对自然语言符号之间的关系提供更容易理解和动态的解析。</p><div class="nb nc gp gr nd ne"><a href="http://demo.qasrl.org/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ir gy z fp nj fr fs nk fu fw ip bi translated">质量保证-SRL |浏览数据</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">编辑描述</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">demo.qasrl.org</p></div></div></div></a></div><h1 id="1897" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">行动呼吁</h1><p id="d0c4" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">下面是一些资源，可以更好地理解上面概述的语义解析工具。</p><p id="f12e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">工具</strong></p><ul class=""><li id="ef7f" class="nv nw iq jp b jq jr ju jv jy nx kc ny kg nz kk oa ob oc od bi translated"><a class="ae lw" href="https://github.com/aribornstein/pyNeurboParser" rel="noopener ugc nofollow" target="_blank"> pyNeurboParser </a> SDP 解析器</li><li id="3e50" class="nv nw iq jp b jq oe ju of jy og kc oh kg oi kk oa ob oc od bi translated"><a class="ae lw" href="https://docs.microsoft.com/en-us/learn/modules/interactive-deep-learning/?WT.mc_id=blog-medium-abornst" rel="noopener ugc nofollow" target="_blank">在 Azure DLVM 上开始使用 pyTorch 和 Docker</a></li><li id="d642" class="nv nw iq jp b jq oe ju of jy og kc oh kg oi kk oa ob oc od bi translated"><a class="ae lw" href="http://demo.allennlp.org/semantic-role-labeling" rel="noopener ugc nofollow" target="_blank">艾伦 NLP SRL 解析器</a></li><li id="7cbf" class="nv nw iq jp b jq oe ju of jy og kc oh kg oi kk oa ob oc od bi translated"><a class="ae lw" href="https://github.com/jflanigan/jamr" rel="noopener ugc nofollow" target="_blank"> JAMR 解析器和文本生成器</a></li><li id="6e3c" class="nv nw iq jp b jq oe ju of jy og kc oh kg oi kk oa ob oc od bi translated"><a class="ae lw" href="https://github.com/vered1986/OKR" rel="noopener ugc nofollow" target="_blank"> OKR </a></li><li id="c3ed" class="nv nw iq jp b jq oe ju of jy og kc oh kg oi kk oa ob oc od bi translated"><a class="ae lw" href="http://qasrl.org/" rel="noopener ugc nofollow" target="_blank">质量保证-SRL </a></li><li id="fd81" class="nv nw iq jp b jq oe ju of jy og kc oh kg oi kk oa ob oc od bi translated"><a class="ae lw" href="http://nlp_architect.nervanasys.com/cross_doc_coref.html" rel="noopener ugc nofollow" target="_blank">英特尔 Nervana 交叉文档共同参考</a></li></ul><p id="3ba3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">资源</strong></p><ul class=""><li id="75fd" class="nv nw iq jp b jq jr ju jv jy nx kc ny kg nz kk oa ob oc od bi translated"><a class="ae lw" href="https://docs.google.com/presentation/d/192DBohgA078dcupoz10-DJA0TQEvwG_JcfY_et33e90/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://docs . Google . com/presentation/d/192 dbohga 078 dcupoz 10-dja 0 tqevwg _ JcfY _ et 33e 90/edit？usp =共享</a></li><li id="a018" class="nv nw iq jp b jq oe ju of jy og kc oh kg oi kk oa ob oc od bi translated"><a class="ae lw" href="https://medium.com/huggingface/learning-meaning-in-natural-language-processing-the-semantics-mega-thread-9c0332dfe28e" rel="noopener">https://medium . com/hugging face/learning-meaning-in-natural-language-processing-the-semantics-mega-thread-9c 0332 dfe 28 e</a></li><li id="7084" class="nv nw iq jp b jq oe ju of jy og kc oh kg oi kk oa ob oc od bi translated"><a class="ae lw" href="http://blog.jacobandreas.net/meaning-belief.html" rel="noopener ugc nofollow" target="_blank">http://blog.jacobandreas.net/meaning-belief.html</a></li><li id="d109" class="nv nw iq jp b jq oe ju of jy og kc oh kg oi kk oa ob oc od bi translated"><a class="ae lw" href="https://www.mn.uio.no/ifi/studier/masteroppgaver/ltg/semantic-dependency-parsing.html" rel="noopener ugc nofollow" target="_blank">https://www . Mn . uio . no/ifi/studier/masteroppgaver/ltg/semantic-dependency-parsing . html</a></li></ul><h1 id="6250" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">下一篇帖子</h1><p id="e662" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">在这个系列中，我们已经讨论了自然语言处理中除嵌入之外的一些最新发展和趋势。未来的帖子将涵盖相关的进步和代码示例，说明如何随着领域的发展使用这些工具。我还对与计算机视觉、时间序列处理和机器学习操作化相关的主题感兴趣，并将尝试涵盖这些主题。</p><p id="50e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你有任何问题、意见或话题想要我讨论，请随时在推特上关注我。</p><p id="9137" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mj">关于作者<br/> </em>亚伦(阿里)博恩施泰因是一个狂热的人工智能爱好者，对历史充满热情，致力于新技术和计算医学。作为微软云开发倡导团队的开源工程师，他与以色列高科技社区合作，用改变游戏规则的技术解决现实世界的问题，然后将这些技术记录在案、开源并与世界其他地方共享。</p></div></div>    
</body>
</html>