<html>
<head>
<title>[ Google ] Continuously Differentiable Exponential Linear Units with Interactive Code [ Manual Back Prop with TF ]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[谷歌]连续可微分指数线性单位与互动代码[手动反推与 TF ]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/google-continuously-differentiable-exponential-linear-units-with-interactive-code-manual-back-fcbe7f84e79?source=collection_archive---------10-----------------------#2018-05-09">https://towardsdatascience.com/google-continuously-differentiable-exponential-linear-units-with-interactive-code-manual-back-fcbe7f84e79?source=collection_archive---------10-----------------------#2018-05-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/4ff04e9bb2ed45e082ade454c8be11a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kzcmkpsbqgb9ggq_izo1Xw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Image from this <a class="ae kc" href="https://pixabay.com/en/fractal-math-geometry-science-1943527/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="ee68" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Jonathan T. Barron 是谷歌的一名研究人员，他提出了 CELU()，意为“<a class="ae kc" href="https://arxiv.org/abs/1704.07483" rel="noopener ugc nofollow" target="_blank"> <em class="lb">”连续可微的指数线性单位</em> </a> <em class="lb">”。简言之，这个新的激活函数在任何地方都是可微的。(对于一般的 ELU，当α值不为 1 时，它不是处处可微的。)</em></p><p id="8123" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于我最近报道了“<a class="ae kc" href="https://arxiv.org/abs/1511.07289" rel="noopener ugc nofollow" target="_blank"> <em class="lb">【通过指数线性单元(ELUs) </em> </a>”快速准确的深度网络学习”(请<a class="ae kc" rel="noopener" target="_blank" href="/iclr-2016-fast-and-accurate-deep-networks-learning-by-exponential-linear-units-elus-with-c0cdbb71bb02">单击此处阅读博文</a>)，接下来报道这篇文章自然是有意义的。最后，为了好玩，让我们用不同的优化方法来训练我们的网络。</p><p id="ed08" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lb">情况 a)用 ADAM 优化器自动微分(</em></strong><a class="ae kc" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"><em class="lb">【MNIST】</em></strong></a><strong class="kf ir"><em class="lb">数据集)<br/>情况 b)用 ADAM 优化器自动微分(</em></strong><a class="ae kc" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"><em class="lb">cifar 10</em></strong></a><strong class="kf ir"><em class="lb">数据集)<br/>情况 c)用</em></strong><a class="ae kc" href="https://medium.com/@SeoJaeDuk/implementation-of-optimization-for-deep-learning-highlights-in-2017-feat-sebastian-ruder-61e2cbe9b7cb" rel="noopener"><strong class="kf ir"><em class="lb">AMSGrad 优化器<em class="lb"> </em> </em></strong> </a><a class="ae kc" rel="noopener" target="_blank" href="/outperforming-tensorflows-default-auto-differentiation-optimizers-with-interactive-code-manual-e587a82d340e"> <strong class="kf ir"> <em class="lb">膨胀背 Pro </em> </strong> </a> <strong class="kf ir"> <em class="lb"> p 带</em></strong><a class="ae kc" href="https://medium.com/@SeoJaeDuk/implementation-of-optimization-for-deep-learning-highlights-in-2017-feat-sebastian-ruder-61e2cbe9b7cb" rel="noopener"><strong class="kf ir"><em class="lb">AMSGrad 优化器</em></strong></a><strong class="kf ir"><em class="lb">(</em></strong><a class="ae kc" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"><em class="lb">cifar 10</em></strong></a><strong class="kf ir"><em class="lb">数据集</em></strong></p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><figure class="lj lk ll lm gt jr"><div class="bz fp l di"><div class="ln lo l"/></div></figure></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="e675" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">连续可微的指数线性单位</strong></p><div class="lj lk ll lm gt ab cb"><figure class="lp jr lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/5693a9459efcfea437d06ff25f67cb65.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*VluuAtKIGEGbKnLKqVaLnQ.png"/></div></figure><figure class="lp jr lv lr ls lt lu paragraph-image"><img src="../Images/f9a0cbb87a59d016c40c3cbdb8590378.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*pvfmd3oTr9mxgmHrzM-5xw.png"/></figure></div><p id="3bce" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">左图</strong>→CELU 方程式()<br/> <strong class="kf ir">右图</strong>→ELU 方程式()</p><p id="0c17" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以上是 CELU()的方程式，我们已经可以看出它与 ELU()的原始方程式并无多大不同。只有一个区别，当 x 小于零时，用α除 x 值。现在让我们看看这个激活函数是怎样的。</p><div class="lj lk ll lm gt ab cb"><figure class="lp jr lw lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/e6f1fd17a00308d219a76202fd5f61c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*ku9HtTVBOBgVHvT7xUtNOQ.png"/></div></figure><figure class="lp jr lx lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/81bc1b24f4ebd81287bf77cd5b02f60d.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*Hy8N7t8JKNSadeah39RuRA.png"/></div></figure></div><p id="632d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">左图</strong>→CELU()和它的导数绘制时的样子<br/> <strong class="kf ir">右图</strong>→ELU()和它的导数绘制时的样子</p><p id="902c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在最右边的图像(ELU()的导数)上，我们可以观察到函数不是连续的，然而对于 CELU()的导数，我们可以观察到函数在任何地方都是连续的。现在让我们看看如何实现 CELU()及其衍生物。</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/487c8b9e09f0853c8192348fefa5ce27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*PTLyTm36BsP7gfo8aGq-Bw.png"/></div></figure><p id="73d7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先让我们看看 CELU()对输入 x 的导数</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/0b14e2a70a11eee95225adfd8a4a1a19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*K2MWYMbbFEX2pBVWLn11cw.png"/></div></figure><p id="200c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当用 python ( <a class="ae kc" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>)实现时，它看起来像上面的东西，请注意我已经将 alpha 值设置为 2。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="b5c6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">网络架构</strong></p><div class="lj lk ll lm gt ab cb"><figure class="lp jr ma lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/00389f1e3d37cb9a52dc6832e28e5b48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*UiEwFIl_TYuu1uRDPW7w9Q.png"/></div></figure><figure class="lp jr mb lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/35ed30b3981479f1da06367a69f50a0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*2NEsDpBJC3Q4L1vUmTZSVA.png"/></div></figure></div><p id="df71" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">红色矩形</strong> →输入图像(32*32*3) <br/> <strong class="kf ir">黑色矩形</strong> →与 CELU 卷积()有/无均值合并<br/> <strong class="kf ir">橙色矩形</strong> → Softmax 进行分类</p><p id="a50f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我要使用的网络是七层网络，采用平均池操作。由于我们将使用 MNIST 数据集以及 CIFAR 10 数据集，因此对平均池的数量进行了相应的调整，以适应图像尺寸。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="37eb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">案例 1)结果:用 ADAM 优化器自动微分(MNIST 数据集)</strong></p><div class="lj lk ll lm gt ab cb"><figure class="lp jr mc lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/1b297539f02a234cd552fabc0e1321af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*zI1ikAHX67KWdBN9MaHvGQ.png"/></div></figure><figure class="lp jr mc lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/8b7c8595a65963c8f34fe469e785d14e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*2SkW3-03iQeayxRfjnU09w.png"/></div></figure></div><p id="83e0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">左图</strong> →一段时间内的训练精度/成本<br/> <strong class="kf ir">右图</strong> →一段时间内的测试精度/成本</p><p id="d327" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于 MNIST 数据集，测试图像和训练图像的准确率都达到了 95%以上。我对训练/测试图像的超时成本进行了标准化，以使图形看起来更漂亮。</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi md"><img src="../Images/847356c07ac2ac9288acf8d54f53611a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GtPJud-GXvJKzr4-uVfoQQ.png"/></div></div></figure><p id="de12" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">测试和训练图像的最终准确度都是 98+%。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="15ab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">情况 2)结果:用 ADAM 优化器自动微分(</strong><a class="ae kc" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">cifar 10</strong></a><strong class="kf ir">数据集)</strong></p><div class="lj lk ll lm gt ab cb"><figure class="lp jr mc lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/39a360ae7ad3e986fc135e2e853428f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*CIga-PdNRucDt_giE-p9iQ.png"/></div></figure><figure class="lp jr mc lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/46a99ba838b29c54b1f0898d57507341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*xwtR5e-HUulSAx927gbGlA.png"/></div></figure></div><p id="897f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">左图</strong> →一段时间内的训练精度/成本<br/> <strong class="kf ir">右图</strong> →一段时间内的测试精度/成本</p><p id="0491" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们可以观察到模型开始遭受过度拟合。特别是使用 Adam optimizer，测试图像的准确率停滞在 74%。</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/07d464c813996afe0da500416d1e2b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*geZHfM2IrdXjkDWQ8k03Ag.png"/></div></div></figure><p id="16d8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">测试图像的最终准确率为 74%，而训练图像的最终准确率为 99%，这表明我们的模型过度拟合。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="2f54" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">案例 3)结果:手动回柱带</strong> <a class="ae kc" href="https://medium.com/@SeoJaeDuk/implementation-of-optimization-for-deep-learning-highlights-in-2017-feat-sebastian-ruder-61e2cbe9b7cb" rel="noopener"> <strong class="kf ir"> AMSGrad 优化器</strong></a><strong class="kf ir">(</strong><a class="ae kc" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">cifar 10</strong></a><strong class="kf ir">数据集)</strong></p><div class="lj lk ll lm gt ab cb"><figure class="lp jr mc lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/9a4d3325340244bbe1ba76e26201c5ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Z_IOHB3NICDB1LWgTV79WQ.png"/></div></figure><figure class="lp jr mc lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/ece64989a2eee6edf180304508a19723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Kpx2UyL_LTz_PrvT5i906A.png"/></div></figure></div><p id="82a4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">左图</strong> →一段时间内的训练精度/成本<br/> <strong class="kf ir">右图</strong> →一段时间内的测试精度/成本</p><p id="a1da" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于这个实验，AMSGrad 比常规的 Adam 优化器做得更好。尽管测试图像的准确性不能超过 80%，并且模型仍然存在过度拟合的问题，但给出的结果比 Adam 好 4%。</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mf"><img src="../Images/0c2ce0f8ddf1ec68f02759b6b5e59dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NMt0VoN0Gt6QE-nYujRaEA.png"/></div></div></figure><p id="7809" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">测试图像的最终精度为 78%,表明该模型仍然过拟合，但比使用没有任何正则化技术的常规 Adam 的结果好 4%,还不错。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="f7ea" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">案例 4)结果:</strong> <a class="ae kc" rel="noopener" target="_blank" href="/outperforming-tensorflows-default-auto-differentiation-optimizers-with-interactive-code-manual-e587a82d340e"> <strong class="kf ir">扩背 Pro </strong> </a> <strong class="kf ir"> p 带</strong> <a class="ae kc" href="https://medium.com/@SeoJaeDuk/implementation-of-optimization-for-deep-learning-highlights-in-2017-feat-sebastian-ruder-61e2cbe9b7cb" rel="noopener"> <strong class="kf ir"> AMSGrad 优化器</strong></a><strong class="kf ir">(</strong><a class="ae kc" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">cifar 10</strong></a><strong class="kf ir">数据集)</strong></p><div class="lj lk ll lm gt ab cb"><figure class="lp jr mg lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/c571cb7941db95031f3a7a969e0c9aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*IlpLIx6u1PZtut8S3DkoqA.png"/></div></figure><figure class="lp jr mh lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/602e28a42bce97af7bb1c200f621319e.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*vnui93NlhjDh-bJrcMnvOA.png"/></div></figure></div><p id="8e04" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">紫色箭头</strong> →反向传播的常规梯度流<br/> <strong class="kf ir">黑色弯箭头</strong> →增大梯度流的扩张后支柱</p><p id="d496" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">记住这个架构，让我们看看我们的网络在测试图像上的表现。</p><div class="lj lk ll lm gt ab cb"><figure class="lp jr mc lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/cd32fff4fb76d72a8ff63d6eee788882.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*A_0lmaxWExMcGvzxa6HFuQ.png"/></div></figure><figure class="lp jr mc lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/1592a27f3872ab3669fe6c85449093eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*_FXmWDt25EwMxqQc7i-5LQ.png"/></div></figure></div><p id="a204" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">左图</strong> →一段时间内的训练精度/成本<br/> <strong class="kf ir">右图</strong> →一段时间内的测试精度/成本</p><p id="cf9b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，它比常规的 Adam 优化器做得更好，但是它不能在测试图像上达到 80%的准确率。(仍然)</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mi"><img src="../Images/895e4bb1d81d4761c31ec285d88bbb8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EhLp0n4vTx9ELwcVAk45qA.png"/></div></div></figure><p id="f9eb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">测试图像的最终准确率为 77%，比 AMSGrad 的常规反向传播低 1%，但比常规 Adam 好 3%。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="c6d1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">交互代码</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/331dda377d74e139762b5fd36f4bdca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vEs_1LLTuu-jJrEJF0uprg.png"/></div></div></figure><p id="7d24" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于 Google Colab，你需要一个 Google 帐户来查看代码，而且你不能在 Google Colab 中运行只读脚本，所以在你的操场上做一个副本。最后，我永远不会请求允许访问你在 Google Drive 上的文件，仅供参考。编码快乐！同样为了透明，我在训练期间上传了所有的日志。</p><p id="8f14" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要访问案例 a 的<a class="ae kc" href="https://colab.research.google.com/drive/1L5Q82L7wiyotZdqWyGCrTp8lg75LBvDf" rel="noopener ugc nofollow" target="_blank">代码，请点击这里</a>，要访问<a class="ae kc" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/ContinuouslyELU/a/casea.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击这里</a>。<br/>访问<a class="ae kc" href="https://colab.research.google.com/drive/1TDIqohEHD3clDaBAp_DGZU-fyBZRoqvR" rel="noopener ugc nofollow" target="_blank">案例 b 的代码请点击此处，</a>访问<a class="ae kc" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/ContinuouslyELU/b/caseb.txt" rel="noopener ugc nofollow" target="_blank">日志请点击此处。</a> <br/>访问<a class="ae kc" href="https://colab.research.google.com/drive/1t6UptCq9jFgL3aHdLqmZNwywpU-z4Vi1" rel="noopener ugc nofollow" target="_blank">案例 c 的代码请点击此处，</a>访问<a class="ae kc" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/ContinuouslyELU/c/casec.txt" rel="noopener ugc nofollow" target="_blank">日志请点击此处。</a> <br/>要访问<a class="ae kc" href="https://colab.research.google.com/drive/1GeOpa_BjGOyMdy5Xv5vbpKFJt-Nby12T" rel="noopener ugc nofollow" target="_blank">案例 d 的代码，请点击此处</a>，要访问<a class="ae kc" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/ContinuouslyELU/d/cased.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处。</a></p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="64a4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">最后的话</strong></p><p id="f27b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有趣的是，根据训练方式的不同，每个网络的表现也各不相同。CELU()似乎比 ELU()的激活函数表现得更好(至少对我来说是这样)。</p><p id="3ff5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你希望看到我所有写作的列表，请<a class="ae kc" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">在这里查看我的网站</a>。</p><p id="ccc9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与此同时，在我的 twitter 上关注我<a class="ae kc" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae kc" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae kc" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae kc" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文 pos </a> t。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="940c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">参考</strong></p><ol class=""><li id="7d8b" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated">乔恩·巴伦。(2018).Jon Barron . info . 2018 年 5 月 8 日检索，来自<a class="ae kc" href="https://jonbarron.info/" rel="noopener ugc nofollow" target="_blank">https://jonbarron.info/</a></li><li id="a7b0" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">j . t . Barron(2017 年)。连续可微指数线性单位。arXiv 预印本 arXiv:1704.07483。</li><li id="ec31" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">[ICLR 2016]通过指数线性单元(ELUs)进行快速准确的深度网络学习，具有…(2018).走向数据科学。2018 年 5 月 8 日检索，来自<a class="ae kc" rel="noopener" target="_blank" href="/iclr-2016-fast-and-accurate-deep-networks-learning-by-exponential-linear-units-elus-with-c0cdbb71bb02">https://towards data science . com/iclr-2016-fast-and-accurate-deep-networks-learning-by-index-linear-units-elus-with-c 0 cdbb 71 bb 02</a></li><li id="a677" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">克利夫特博士、安特辛纳、t .和霍克雷特博士(2015 年)。通过指数线性单元(ELUs)进行快速准确的深度网络学习。Arxiv.org。检索于 2018 年 5 月 8 日，来自 https://arxiv.org/abs/1511.07289<a class="ae kc" href="https://arxiv.org/abs/1511.07289" rel="noopener ugc nofollow" target="_blank"/></li><li id="39dc" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">张量流。(2018).张量流。检索于 2018 年 5 月 9 日，来自 https://www.tensorflow.org/<a class="ae kc" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"/></li><li id="ad8b" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">CIFAR-10 和 CIFAR-100 数据集。(2018).Cs.toronto.edu。检索于 2018 年 5 月 9 日，来自<a class="ae kc" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">https://www.cs.toronto.edu/~kriz/cifar.html</a></li><li id="6ad5" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">MNIST 手写数字数据库，Yann LeCun，Corinna Cortes 和 Chris Burges。(2018).Yann.lecun.com。检索于 2018 年 5 月 9 日，来自<a class="ae kc" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">http://yann.lecun.com/exdb/mnist/</a></li></ol></div></div>    
</body>
</html>