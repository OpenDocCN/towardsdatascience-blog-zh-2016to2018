<html>
<head>
<title>Creating the Snapchat Filter System using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习创建 Snapchat 过滤器系统</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-snapchat-like-filters-using-deep-learning-13551940b174?source=collection_archive---------6-----------------------#2018-12-25">https://towardsdatascience.com/implementing-snapchat-like-filters-using-deep-learning-13551940b174?source=collection_archive---------6-----------------------#2018-12-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="9fda" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你们都不喜欢阅读</p><p id="c4ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">欢迎，所有可能在看到‘Snapchat’和‘深度学习’这两个词后打开这篇文章的千禧一代程序员。我向上帝发誓，这两个字像飞蛾扑火一样吸引着你们。我在骗谁，我也是它的受害者，这就是为什么我花了几个小时做这个项目。</p><p id="0f05" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，如果你愿意这样称呼它，我将回顾标题中的过程和项目背后的一点理论。坦白地说，即使我在标题中使用术语“Snapchat”也可能有点 clickbaity，因为尽管这个项目的工作原理相同(使用面部关键点将对象映射到面部)，但就复杂性和准确性而言，它甚至没有接近 Snapchat 的实现。说完这些，让我从介绍我使用的数据集开始。</p><h1 id="4044" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">数据集</h1><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/9a803029a8a4de97a52a7606d336226d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/0*n202LIqLzGfj0BKH.png"/></div></figure><p id="2337" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我使用的数据集如下:<a class="ae lr" href="https://www.kaggle.com/c/facial-keypoints-detection" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/facial-keypoints-detection</a>由<a class="ae lr" href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html" rel="noopener ugc nofollow" target="_blank">蒙特娄大学的 Yoshua Bengio </a>博士提供。</p><p id="3a9e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每个预测的关键点由像素索引空间中的(x，y)实值对指定。共有 15 个关键点，代表了面部的不同元素。输入图像在数据文件的最后一个字段中给出，由像素列表(按行排序)组成，整数为(0，255)。这些图像是 96x96 像素。</p><p id="995e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">既然我们对正在处理的数据类型有了很好的了解，我们需要对它进行预处理，以便我们可以将它用作模型的输入。</p><h2 id="3ab1" class="ls km iq bd kn lt lu dn kr lv lw dp kv jy lx ly kz kc lz ma ld kg mb mc lh md bi translated">步骤 1:数据预处理和其他诡计</h2><p id="f97b" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">上面的数据集有两个我们需要关注的文件——<strong class="jp ir">training . CSV</strong>和<strong class="jp ir"> test.csv. </strong>训练文件有 31 列:30 列为关键点坐标，最后一列包含字符串格式的图像数据。它包含 7049 个样本，<strong class="jp ir">然而</strong>，这些例子中的许多在一些关键点上有“NaN”值，这让我们很难处理。因此我们将只考虑没有任何 NaN 值的样本。下面的代码就是这样做的:<em class="mj">(下面的代码也对图像和关键点数据进行规范化，这是一个非常常见的数据预处理步骤)</em></p><p id="52c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一切都好吗？不完全是，没有。似乎只有<strong class="jp ir"> 2140 </strong>个样本不包含任何 NaN 值。训练一个通用和精确的模型，这些样本要少得多。因此，为了创建更多的数据，我们需要<strong class="jp ir">扩充我们当前的数据</strong>。</p><p id="b616" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据扩充是一种通过使用缩放、平移、旋转等技术从现有数据中生成更多数据的技术。在这种情况下，<strong class="jp ir">我镜像了每张图像及其对应的关键点</strong>，因为像缩放和旋转这样的技术可能会扭曲面部图像，从而破坏模型。最后，我将原始数据与新的扩充数据结合起来，得到总共<strong class="jp ir"> 4280 </strong>个样本。</p><h2 id="ee10" class="ls km iq bd kn lt lu dn kr lv lw dp kv jy lx ly kz kc lz ma ld kg mb mc lh md bi translated">步骤 2:模型架构和培训</h2><p id="a284" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">现在让我们进入项目的深度学习部分。我们的目标是预测一张看不见的脸的每个关键点的坐标值，<strong class="jp ir">，因此这是一个回归问题</strong>。由于我们正在处理图像，卷积神经网络是特征提取的一个非常明显的选择。这些提取的特征然后被传递到一个完全连接的神经网络，该网络输出坐标。最终的密集层需要 30 个神经元，因为我们需要 30 个值(15 对(x，y)坐标)。</p><ul class=""><li id="64ad" class="mk ml iq jp b jq jr ju jv jy mm kc mn kg mo kk mp mq mr ms bi translated">“ReLu”激活在每个卷积和密集层之后使用，除了最后一个密集层，因为这些是我们需要作为输出的坐标值</li><li id="bf40" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">剔除调整用于防止过度拟合</li><li id="9fc6" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">添加最大池是为了降维</li></ul><p id="8325" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型能够达到最小损耗<strong class="jp ir"> ~0.0113 </strong>，精度<strong class="jp ir"> ~80% </strong>，我觉得已经足够体面了。以下是测试集上模型性能的一些结果:</p><div class="lk ll lm ln gt ab cb"><figure class="my lo mz na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><img src="../Images/e31e6db0322a539ba16435c749bf6a7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*-ZCVll0LveDDzlnqvfeH4g.png"/></div></figure><figure class="my lo mz na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><img src="../Images/1b44d8926bafb894c0478beed00c262f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*MDQOCZxIoXhym74MweFYXw.png"/></div></figure><figure class="my lo mz na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><img src="../Images/f951efc38a2d0152eb78b2ce5d88562e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*4qr36rSlkN8HHd3Az8Vmbg.png"/></div></figure></div><div class="ab cb"><figure class="my lo mz na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><img src="../Images/99f4fe190986ec04f8ea67dc01efa2f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*yu3OoCrP8MYhEdNEsCZ6Fw.png"/></div></figure><figure class="my lo mz na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><img src="../Images/5a30ec6b67c1905a94fdc5e3a34c808b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*as4MzgltRSNHnhjuc2iInA.png"/></div></figure><figure class="my lo mz na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><img src="../Images/b14372e792d19043fb969a8b480edf91.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Je5PMHfG9rv6SFrk780fwA.png"/></div></figure></div><div class="ab cb"><figure class="my lo mz na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><img src="../Images/99a3d71026c93e0e6f91803b900609a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*HGQ8gy0eNWxRxod4Th3-sQ.png"/></div></figure><figure class="my lo mz na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><img src="../Images/b0f0a47ecb26e6b31bf6e5ecab389c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*4EELD1C1RjlNwuBhr7M2sg.png"/></div></figure><figure class="my lo mz na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><img src="../Images/aedc3d61ad52364a01b0721fec7b34dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*C5N4oLbiGNw6nWRNdPCVmQ.png"/></div></figure></div><div class="ab cb"><figure class="my lo ni na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><img src="../Images/94d5bb8646d344f9cf5c93c80b20112e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*JdF6wyz9Abw5bAuOAyQgEg.png"/></div></figure><figure class="my lo ni na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><img src="../Images/4e0635791f820a6b8ff41ad8845f5fdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*zyswpDozWS9Ew_ITiMW_bg.png"/></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk nn di no np">Model performance on the Test set</figcaption></figure></div><p id="6ca4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我还需要检查模型在来自我的网络摄像头的图像上的表现，因为这是模型在过滤器实施期间将接收到的，下面是模型在我美丽的脸的图像上的表现:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/34ac6338cfe687a112948bdae801283c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*CY5R9MbXVvWtvencXEYIjQ.png"/></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">Don’t be intimidated by this scary face. I don’t bite.</figcaption></figure><h2 id="d142" class="ls km iq bd kn lt lu dn kr lv lw dp kv jy lx ly kz kc lz ma ld kg mb mc lh md bi translated">第三步:将模型付诸实施</h2><p id="401c" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">我们已经让我们的模型工作了，所以我们现在要做的就是使用 OpenCV 来做以下事情:</p><ol class=""><li id="22d7" class="mk ml iq jp b jq jr ju jv jy mm kc mn kg mo kk nr mq mr ms bi translated">从网络摄像头获取图像帧</li><li id="fff5" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk nr mq mr ms bi translated">检测每个图像帧中的面部区域，因为图像的其他部分对模型没有用(我使用了<strong class="jp ir">正面面部哈尔级联</strong>来裁剪面部区域)</li><li id="7247" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk nr mq mr ms bi translated">通过转换为灰度、规格化和整形来预处理该裁剪区域</li><li id="7ad1" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk nr mq mr ms bi translated">将预处理后的图像作为输入传递给模型</li><li id="d778" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk nr mq mr ms bi translated">获得对关键点的预测，并使用它们在面部定位不同的过滤器</li></ol><p id="1982" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我开始测试的时候，我脑子里没有任何特定的过滤器。我在 2018 年 12 月 22 日<strong class="jp ir">左右产生了这个项目的想法</strong>，作为一个像其他普通人一样的超级圣诞粉丝，我决定采用以下过滤器:</p><div class="lk ll lm ln gt ab cb"><figure class="my lo ns na nb nc nd paragraph-image"><img src="../Images/96fd838ecc8a2ff40d83a27107389aa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*lo9BIlUiIHAS3TeHBYMpOQ.png"/></figure><figure class="my lo nt na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><img src="../Images/1fe1d1c602da3bbb0bc6534fbee3dc7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*90ZbIqL8-6rngo7a5xWzkA.png"/></div></figure><figure class="my lo nu na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><img src="../Images/45ca915adcdeb0d93229edb98f3c049a.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*KuqY5K6WXYs9s2qqqA2AFw.png"/></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk nv di nw np">Filters</figcaption></figure></div><p id="6ade" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我使用了特定的关键点来缩放和定位上述每个过滤器:</p><ul class=""><li id="6618" class="mk ml iq jp b jq jr ju jv jy mm kc mn kg mo kk mp mq mr ms bi translated"><strong class="jp ir">眼镜滤镜:</strong>左眼左关键点和右眼右关键点之间的距离用于缩放。眉毛关键点和左眼左关键点用于眼镜的定位</li><li id="6211" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated"><strong class="jp ir">胡须过滤器:</strong>左嘴唇关键点和右嘴唇关键点之间的距离用于缩放。上唇关键点和左唇关键点用于胡须的定位</li><li id="d252" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated"><strong class="jp ir">帽子滤镜:</strong>脸部的宽度用于缩放。眉毛关键点和左眼左关键点用于帽子的定位</li></ul><p id="f67e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">执行上述所有操作的代码如下:</p><h1 id="2020" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">结果</h1><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nx"><img src="../Images/1bf03c80171563c2c21dcce9607eaf9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5sCOruXqIzvZEk2NKh1Ztg.png"/></div></div></figure><p id="4862" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面，你可以看到项目的最终输出，其中包含一个在我脸上使用滤镜的实时视频和另一个标绘了关键点的实时视频。</p><h2 id="526d" class="ls km iq bd kn lt lu dn kr lv lw dp kv jy lx ly kz kc lz ma ld kg mb mc lh md bi translated">项目的局限性</h2><p id="96e1" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">虽然这个项目运行得很好，但我确实发现了一些不足之处，使它有点不完美:</p><ul class=""><li id="fbe4" class="mk ml iq jp b jq jr ju jv jy mm kc mn kg mo kk mp mq mr ms bi translated">不是最准确的模型。虽然在我看来 80%已经相当不错了，但还是有很大的提升空间。</li><li id="a6d0" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">这个当前的实现只适用于选定的一组过滤器，因为我不得不做一些手动调整，以获得更精确的定位和缩放。</li><li id="c7b7" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">将滤镜应用到图像的过程在计算上是相当低效的，因为要覆盖。png 过滤器图像到基于 alpha 通道的网络摄像头图像上，我必须在 alpha 不等于 0 的地方逐个像素地应用过滤器。这有时会导致程序在检测到图像中不止一张人脸时崩溃。</li></ul><p id="7824" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该项目的完整代码在我的 Github 上:<a class="ae lr" href="https://github.com/agrawal-rohit/Santa-filter-facial-keypoint-regression" rel="noopener ugc nofollow" target="_blank">https://Github . com/agr awal-rohit/Santa-filter-face-key point-regression</a></p><p id="3f35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你想改进这个项目，或者你对我解决上述问题有任何建议，请务必在下面留下回复，并在 Github repo 上生成一个 pull 请求。谢谢你的来访，希望你喜欢这本书。</p><p id="234d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">再见。</p></div></div>    
</body>
</html>