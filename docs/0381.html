<html>
<head>
<title>Regression prediction intervals with XGBOOST</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBOOST回归预测区间</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regression-prediction-intervals-with-xgboost-428e0a018b?source=collection_archive---------1-----------------------#2017-04-25">https://towardsdatascience.com/regression-prediction-intervals-with-xgboost-428e0a018b?source=collection_archive---------1-----------------------#2017-04-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/ff303c96be096211789decb275b5ab26.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*9AgW0fV9cXa4Vh3YYXuiJw.jpeg"/></div></figure><p id="9675" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><em class="ks">(更新2019-04-12:真不敢相信已经两年了。由于我一直收到各种更新代码的请求，我花了一些时间来重构、更新gists，甚至创建了一个合作笔记本</em><a class="ae kt" href="https://colab.research.google.com/drive/1KlRkrLi7JmVpprL94vN96lZU-HyFNkTq" rel="noopener ugc nofollow" target="_blank"><em class="ks"/></a><em class="ks">。我为那些在运行过去版本时遇到困难的人道歉。如果您有任何问题，请提问，我很乐意抽出时间来回答！)</em></p><p id="c218" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">(更新2020–08–19:感谢您<a class="ku kv ep" href="https://medium.com/u/231b8b24164f?source=post_page-----428e0a018b--------------------------------" rel="noopener" target="_blank"> Hassen Miri </a>帮助我完善分割收益论点)</p><p id="63ea" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于任何希望对自己的业务进行认真预测分析的人来说，了解算法预测的不确定性是至关重要的。预测从来都不是绝对的，了解潜在的变化是非常必要的。如果一个人希望知道每个航班的乘客量，他还需要知道有多少乘客的预测可能不同。另一个可以决定预测上岸时间。当然，在几个小时的预测和半个小时内95%的正确率和10个小时的潜在误差之间是有区别的！</p><p id="5a69" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在这里，我提出了一个定制的成本函数，用于将著名的xgboost回归器应用于分位数回归。Xgboost或极端梯度提升是一个非常成功和强大的基于树的算法。由于分位数回归成本函数的梯度和Hessian性质，xgboost的表现明显不佳。我表明，通过添加一个随机成分到平滑梯度，分位数回归可以成功应用。我展示了这种方法可以胜过流行的scikit-learn包中的<code class="fe kw kx ky kz b">GradientBoostingRegressor</code>算法。</p><h2 id="5ca4" class="la lb iq bd lc ld le dn lf lg lh dp li kf lj lk ll kj lm ln lo kn lp lq lr ls bi translated">为什么是预测区间？</h2><p id="003f" class="pw-post-body-paragraph ju jv iq jw b jx lt jz ka kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ij bi translated">虽然模型输出的是准确的预测，但它们本身是随机变量，也就是说，它们有分布。为了了解我们的结果正确的可能性，预测区间是必要的。这种可能性决定了可能值的区间。</p><p id="7744" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">比方说，我们希望知道90%确定概率的可能预测的范围，那么我们需要提供两个值<code class="fe kw kx ky kz b">Q_alpha, Q_{1-alpha}</code>，这样，真实值的概率在区间内</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/ec91680042259ba91420218bd8e9ed76.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/0*X7zvALXJdy3rsjqq.png"/></div></figure><p id="91d4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">是90%。</p><p id="95cc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">线性回归的基本陈述是，</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi md"><img src="../Images/f4667346bb87c47514198c6150e07e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y3dkeNnu3O-TLyl6dllHvA.png"/></div></div></figure><p id="c9f8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">假设观测值<code class="fe kw kx ky kz b">x_i</code>和<code class="fe kw kx ky kz b">ε_i</code>是独立的并且是正态分布的。</p><p id="42d8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">假设误差项正态分布，系数<code class="fe kw kx ky kz b">β_j</code>的最大似然估计使这些误差项的方差最小。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mi"><img src="../Images/f8866b8d61dce2548ab69429977623bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgZNBcNTS-Oa97CbB_JlYw.png"/></div></div></figure><p id="24aa" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这个方差当然是预测方差的最大似然(ML)估计。在我们假设特征和误差都是正态分布的情况下，预测也是正态分布的。</p><p id="f49c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">正态分布的预测区间很容易从期望和方差的最大似然估计中计算出来:</p><p id="333e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">68%的预测区间介于</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/f2ae22293833ce50ac5cc15ab09ff912.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/0*AMWvunRvWCb9H4Ia.png"/></div></figure><p id="1d02" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">，95%的预测区间介于</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mk"><img src="../Images/a44f5ba606461aa901970ef4213de9be.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/0*OOhICdpUILrujSbW.png"/></div></div></figure><p id="cc76" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">99.7%的预测区间在</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mk"><img src="../Images/2671644c380661efa76b3d05360bf5c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/0*rFyeF4a3ZdwARDgK.png"/></div></div></figure><p id="9c89" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这种方法有一些限制。首先，我们非常依赖正态性的基本假设，当然这并不总是正确的。最后，线性回归的简单性将我们限制在一个非常宽的预测区间。理想情况下，我们希望我们的预测边界也依赖于更多的特征。在下一节课中，我们将讨论使用树模型的实现。</p><h2 id="af35" class="la lb iq bd lc ld le dn lf lg lh dp li kf lj lk ll kj lm ln lo kn lp lq lr ls bi translated">树模型</h2><p id="600d" class="pw-post-body-paragraph ju jv iq jw b jx lt jz ka kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ij bi translated">回归树是一个非常强大的预测工具。树的一般思想是对要素进行分区，并为每个分区分配一个值。该值以这样的方式分配，使得分区内的成本函数最小化。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ml"><img src="../Images/e3025659b42599d1151f3264bd785bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nF0nNpl5nF7Lav8b."/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 1: Illustration of a tree model. The tree defines a partition of the features-space and assigns a values to each partition.</figcaption></figure><p id="687b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">分位数可以通过下面的优化问题找到，</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mq"><img src="../Images/eca6acdf4827c52c9af25dfe7fa71a83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cxoa70XJHRuxRp87mAk6Og.png"/></div></div></figure><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/97541b7734ab62977d323429f5f9b992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/0*9Wzmx7OIBCfhst0N."/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 2: Cost function of Quantile Regression</figcaption></figure><h2 id="67b9" class="la lb iq bd lc ld le dn lf lg lh dp li kf lj lk ll kj lm ln lo kn lp lq lr ls bi translated">xgboost的分位数回归</h2><p id="2c9c" class="pw-post-body-paragraph ju jv iq jw b jx lt jz ka kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ij bi translated">我还没有解释如何给每个分区赋值。</p><p id="9c36" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">Xgboost是一个强大的算法，它为我们做到了这一点。为了进一步参考，读者应该检查原始论文，</p><p id="447e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">陈，田琦和卡洛斯.盖斯特林。" Xgboost:一个可扩展的树增强系统."<em class="ks">第22届ACM SIGKDD知识发现和数据挖掘国际会议论文集</em>。ACM，2016。</p><p id="6949" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">将体积<code class="fe kw kx ky kz b">I</code>进一步划分成两个更小的子体积<code class="fe kw kx ky kz b">I_L</code>、<code class="fe kw kx ky kz b">I_R</code>的主要因素是得分函数，</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/c5f6e38423e7c0f54b6c1ee9339ea81c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*SomWlmGh6b0AVdRbGS1x7w.jpeg"/></div></figure><p id="9cb5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果发生分裂，那么通过将下面的值加到较大体积<code class="fe kw kx ky kz b">q_old</code>的分位数的旧值上，来更新每个子体积<code class="fe kw kx ky kz b">I_α</code>内的分位数<code class="fe kw kx ky kz b">q_new</code>的新值。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/e29117a8f43a51ae34279174d6267029.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/0*uAC6wEVtC7cYpA57.png"/></div></figure><p id="12f0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">参数λ和η分别是正则化和收缩参数，我在这里不做进一步讨论。</p><p id="32e5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这里重要的是梯度和Hessian的值:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/7d06207c3daca5efde7c6c4c2f2e878b.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/0*gGR50VauQG2rXP0H.png"/></div></figure><p id="d182" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在下图中，我们看到了梯度和黑森曲线。</p><p id="388c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">正如我们所见，梯度是一个阶跃函数，而海森函数处处为零，在原点处无穷大。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mv"><img src="../Images/560acf8801cb8f59bdb856dc97e8ae9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FdGj5rZKuBz7Xv45."/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 3: Gradient and Hessian of the cost-function of quantile regression with respect to the estimate</figcaption></figure><p id="3164" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">回头看看分裂的得分函数<code class="fe kw kx ky kz b">L_split</code>，我们看到几个问题。</p><p id="9ad6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们看看分位数值<code class="fe kw kx ky kz b">q_old</code>与分区内的观察值相距相对较远的情况。除非，我们运气好，在升压开始的时候是这种情况。梯度和Hessian对于大的差异都是常数<code class="fe kw kx ky kz b">x_i — q</code>。这意味着分离增益与任何特性无关。更糟糕的是，最佳分割增益告诉我们，只要我们将数据量分成两半，任何分割都是足够的！当寻找分割时，xgboost将尝试寻找具有最佳分割的特征。因为所有特征产生相同的分离增益，即节点中一半的数据向左，另一半向右，所以不会发生分离。</p><p id="fe37" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果我们使用上面显示的梯度和Hessian，甚至使用平滑的变体，很少会出现分裂，模型将表现不佳。</p><p id="1b24" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">一个有趣的解决方案是通过向渐变添加随机化来强制拆分。当分区内的观察值<code class="fe kw kx ky kz b">x_i</code>和旧的分位数估计值<code class="fe kw kx ky kz b">q_old</code>之间的差异很大时，这种随机化将强制随机分割该体积。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mw"><img src="../Images/55bda9583cf94628ecaa6f31636c3936.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jlOdCm_vUDZM6-m_."/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 4: Plots of the new Gradient and Hessian used to estimate the quantile. Close to the origin, we use a smoothed version of the Gradient and Hessian. Far from the origin, randomization has been added to force a random split of the partition.</figcaption></figure><p id="2ef2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">也就是说，我提出梯度和Hessian的以下平滑变型，</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/c6a9f8e101d3fd5e04091152a3dcbb8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/0*HnzjHi4HB0eRBiHo.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Gradient</figcaption></figure><p id="be4f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">随机变量ρ。对于这个实验，我简单地选择ρ，对于某个参数<code class="fe kw kx ky kz b">s</code>，以相等的概率取两个值<code class="fe kw kx ky kz b">s</code>或<code class="fe kw kx ky kz b">-s</code>中的一个。</p><p id="a082" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">黑森:</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/0e29ef3e0802368bc29cf98a49fbc5b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/0*GxRR1wPXlzKgaGq_.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Hessian</figcaption></figure><p id="3557" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在代码中:</p><figure class="lz ma mb mc gt jr"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="8d52" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们将此代码应用于由以下代码生成的数据，这些数据也在sklearn示例中使用:<a class="ae kt" href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/auto _ examples/ensemble/plot _ gradient _ boosting _ quantile . html</a></p><figure class="lz ma mb mc gt jr"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="116f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">下图显示了sklearn的<code class="fe kw kx ky kz b">GradientBoostingRegressor</code>和我们定制的<code class="fe kw kx ky kz b">XGBRegressor</code>的90%预测区间的比较结果，该区间是根据95%和5%分位数计算的。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nb"><img src="../Images/908738dcee04a8994267de166faf7154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*h_F2yLpH1POFlrgb."/></div></div></figure><p id="ab99" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于95%的分位数，我使用了参数值</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/a62448237268f9941de7e0ce9712b538.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*Vg98qEOKA1ihDIZt.png"/></div></figure><p id="85ec" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于5%的分位数，我用了</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/44a01c705e1354eaaa9ede0986463bca.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*KiW5NVJ0Dse3MTk8.png"/></div></figure><p id="a64f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">是通过网格搜索找到的。当使用成本函数对测试集进行评分时，发现定制xgboost的95%和5%分位数都是优越的。</p><p id="159d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><a class="ae kt" href="https://colab.research.google.com/drive/1KlRkrLi7JmVpprL94vN96lZU-HyFNkTq?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="jw ir">点击此处查看合作笔记本！！！</strong> </a></p></div></div>    
</body>
</html>