<html>
<head>
<title>Solving internal co-variate shift in deep learning with linked Neurons with Interactive Code [ Manual Back Prop in TF ]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用交互代码解决深度学习中链接神经元的内部同变移位[TF中的手动反推]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/solving-internal-co-variate-shift-in-deep-learning-with-linked-neurons-with-interactive-code-61859388af76?source=collection_archive---------9-----------------------#2018-04-20">https://towardsdatascience.com/solving-internal-co-variate-shift-in-deep-learning-with-linked-neurons-with-interactive-code-61859388af76?source=collection_archive---------9-----------------------#2018-04-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/245e9251671edebd85b4517aa9965558.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*XYYD42Tkt5hyOrCORzMyNQ.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://imgur.com/gallery/dhAQp6B" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="11a6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">所以我仍然在写我的期末论文，但是我真的很想写这篇论文。“<a class="ae jy" href="https://arxiv.org/abs/1712.02609" rel="noopener ugc nofollow" target="_blank"> <em class="kx">用链接的神经元解决深度学习中的内部同变量移位</em> </a>”和往常一样，让我们执行手动反向传播，看看我们是否能胜过自动微分。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><figure class="lf lg lh li gt jr"><div class="bz fp l di"><div class="lj lk l"/></div></figure></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="dbb4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">激活功能</strong></p><div class="lf lg lh li gt ab cb"><figure class="ll jr lm ln lo lp lq paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/123a2aaeffa2b280cde660f8578a209e.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*cxAS6fm8nPqLj9Q1lpygHw.png"/></div></figure><figure class="ll jr lv ln lo lp lq paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/d129c2629d7814f19d86b0afd01e7028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*aREF8cC6QB69gzS1yOtnyw.png"/></div></figure></div><p id="c75f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我喜欢这篇论文的原因之一是因为它在介绍各种激活函数方面做了一件令人惊奇的工作。我从来不知道<a class="ae jy" href="https://arxiv.org/pdf/1710.05941.pdf" rel="noopener ugc nofollow" target="_blank">唰激活功能</a>的存在，所以这是一个很好的发现。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="a40d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">链接神经元/实现</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lw"><img src="../Images/e5d49aa3d72967a5e38ee3e22dc577da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NfD7tPfqQQBYmT4vgQ0kVw.png"/></div></div></figure><p id="6704" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红框→ </strong>链接神经元的数学公式。</p><p id="3780" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">本文的主要贡献是链接神经元，这意味着梯度的导数中至少有一个非零，这将解决神经元死亡的问题。这是一个非常有趣的提议。此外，这种“链接神经元”的实现也非常简单。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lx"><img src="../Images/ec072687b97f80a14833a8d159d4857d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OGN5qC4B0fagg1VMhCP1uQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Implementatio<a class="ae jy" href="https://github.com/blauigris/linked_neurons" rel="noopener ugc nofollow" target="_blank">n from here</a></figcaption></figure><p id="0b32" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">所以基本上，我们连接每个激活函数的输出，并传递到下一层，很简单！现在让我们来看看实际情况。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ly"><img src="../Images/d3e9b242ccb8b1aa931501c9585e482b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QuVUUpvOha-LzWIxDtbUsg.png"/></div></div></figure><p id="df07" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当我们给这一层一个形状张量[？，28，28，32]输出为[？28，28，64]，因为它在信道维度中被连接。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="9024" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">网络架构(图形/ OOP形式)</strong></p><div class="lf lg lh li gt ab cb"><figure class="ll jr lz ln lo lp lq paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/4e1731a92c69a2efce9ba19dc6a8a55a.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*zcqzoMgwwHD8xRhPPZ0j4w.png"/></div></figure><figure class="ll jr ma ln lo lp lq paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/8bf5b20870be679c7c8ea3e07b6082ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*_D6raR_MubUVotduOXMJSQ.png"/></div></figure></div><p id="2047" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红圈</strong> →卷积运算<br/> <strong class="kb ir">蓝/白圈</strong> →不同输入的激活功能<br/> <strong class="kb ir">黑圈</strong> →串接输出层</p><p id="7df5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">所以以上是我们网络的基本积木，每一层都是由卷积，2激活，最后是均值池运算组成。现在让我们看看OOP表单。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mb"><img src="../Images/6cb0e6a161f0d5d6081cc86386c843eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-NiVnpwRlJtC9QTD_87Ig.png"/></div></div></figure><p id="7e45" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">使用上述架构，我们将创建5个完全卷积网络，并在MNIST数据集上进行测试。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/bd40d00a7db03ae1979cbc3c022ae170.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*VhKdT8vOhaT57QR6UIOOtA.png"/></div></figure></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="bfeb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果(自动微分)</strong></p><div class="lf lg lh li gt ab cb"><figure class="ll jr md ln lo lp lq paragraph-image"><img src="../Images/4a89c7a1e9cf086d054d5463a6cd04e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*SDOfu9ql-U3R9-jTusDLzg.png"/></figure><figure class="ll jr me ln lo lp lq paragraph-image"><img src="../Images/0d8357f126e383dfa327e5c9375f90cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*G90hPQie1DShsGcGMSm7fw.png"/></figure></div><div class="ab cb"><figure class="ll jr me ln lo lp lq paragraph-image"><img src="../Images/f59cbea0a89836502963f8ed4b5cb84d.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*43uABpjwSA7LJZHsN1Zs5Q.png"/></figure><figure class="ll jr md ln lo lp lq paragraph-image"><img src="../Images/87bd49d4a32470dd4507bb7a561d54c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*xBtcEHmnoShVUo1v5kb3kQ.png"/></figure></div><p id="5fc0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">右上</strong> →训练图像的平均时间成本<br/> <strong class="kb ir">左上</strong> →训练图像的平均时间精度<br/> <strong class="kb ir">左下</strong> →测试图像的平均时间成本<br/> <strong class="kb ir">右下</strong> →测试图像的平均时间精度</p><p id="7528" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">使用自动微分，我能够在100个历元内获得85%的准确度。学习率是0.0003，但是我做了额外的实验，将学习率设置为0.003，并且能够在相同的时间内达到+92%。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="6bbb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果(扩张反向传播)</strong></p><div class="lf lg lh li gt ab cb"><figure class="ll jr mf ln lo lp lq paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/6fe2db9343f53d9c483d367510b7a9fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*kpj-XZ3mKFWdBO5AusEFlA.png"/></div></figure><figure class="ll jr mf ln lo lp lq paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/08d33685b380dcd8ba3aa4ea467d244a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*jR4E8_UIeJN3V73gfsfwrQ.png"/></div></figure></div><div class="ab cb"><figure class="ll jr mf ln lo lp lq paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/c04404943b36d4a52ba09e0374679fb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*x9LpJbvCr9SHGrY77UBpxA.png"/></div></figure><figure class="ll jr mf ln lo lp lq paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/f8fe806fa8ae1ac9e53b3162e040a0dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*dBLi8FTMjpOEQpaD2qpj_A.png"/></div></figure></div><p id="62f6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">右上</strong> →训练图像的平均时间成本<br/> <strong class="kb ir">左上</strong> →训练图像的平均时间精度<br/> <strong class="kb ir">左下</strong> →测试图像的平均时间成本<br/> <strong class="kb ir">右下</strong> →测试图像的平均时间精度</p><p id="288f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于这个案例，我得到了一个非常令人失望的结果。该模型甚至无法达到20%的准确率，而且随着时间的推移，成本实际上还在增加！我再三检查了我是否正确地进行了求导，结果似乎是正确的。这绝对是我想要深入研究的案例。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="3910" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">互动代码\透明度</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mg"><img src="../Images/a8fe09fcc85d588582b0b8a205f28f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DJEsv0zgfs7FgUIgzlZxVA.png"/></div></div></figure><p id="7017" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="kx">对于Google Colab，您需要一个Google帐户来查看代码，而且您不能在Google Colab中运行只读脚本，因此请在您的操场上创建一个副本。最后，我永远不会请求允许访问你在Google Drive上的文件，仅供参考。编码快乐！</em></p><p id="c140" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要访问<a class="ae jy" href="https://colab.research.google.com/drive/1OGSB6AroeeA6MSEPTl3pNyiltfab0474" rel="noopener ugc nofollow" target="_blank">自动微分的代码，请点击此处。</a></p><p id="eba8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要访问<a class="ae jy" href="https://colab.research.google.com/drive/1GIs92XHrGwDoO_hcvpKclj_ZzKwDt0CZ" rel="noopener ugc nofollow" target="_blank">扩张反向传播的代码，请点击此处。</a></p><p id="ae26" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">为了使这个实验更加透明，我已经将我的命令窗口的所有输出上传到我的github，要访问自动微分<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/linked_neuron/auto_train.txt" rel="noopener ugc nofollow" target="_blank">模型的输出，请单击此处</a>，要访问手动反向传播<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/linked_neuron/man_train.txt" rel="noopener ugc nofollow" target="_blank">，请单击此处</a>。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="c7e7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="c198" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我不确定这种方法是否能完全取代批量标准化，但是这种方法确实很有趣。</p><p id="402f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到jae.duk.seo@gmail.com给我，如果你想看我所有写作的列表，请在这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的twitter上关注我<a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>了解更多内容。如果你感兴趣的话，我还做了解耦神经网络<a class="ae jy" href="https://becominghuman.ai/only-numpy-implementing-and-comparing-combination-of-google-brains-decoupled-neural-interfaces-6712e758c1af" rel="noopener ugc nofollow" target="_blank">的比较。</a></p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="5f70" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="d50b" class="mh mi iq kb b kc kd kg kh kk mj ko mk ks ml kw mm mn mo mp bi translated">Molina，C. R. R .，&amp; Vila，O. P. (2017)。用链接神经元解决深度学习中的内部协变量移位。<em class="kx"> arXiv预印本arXiv:1712.02609 </em>。</li><li id="1886" class="mh mi iq kb b kc mq kg mr kk ms ko mt ks mu kw mm mn mo mp bi translated">适合ML初学者的MNIST。(2018).张量流。检索于2018年4月18日，来自<a class="ae jy" href="https://www.tensorflow.org/versions/r1.1/get_started/mnist/beginners" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/versions/r 1.1/get _ started/mnist/初学者</a></li><li id="77e5" class="mh mi iq kb b kc mq kg mr kk ms ko mt ks mu kw mm mn mo mp bi translated">梯度下降优化算法综述。(2016).塞巴斯蒂安·鲁德。检索于2018年4月18日，来自<a class="ae jy" href="http://ruder.io/optimizing-gradient-descent/index.html#adam" rel="noopener ugc nofollow" target="_blank">http://ruder.io/optimizing-gradient-descent/index.html#adam</a></li><li id="bf08" class="mh mi iq kb b kc mq kg mr kk ms ko mt ks mu kw mm mn mo mp bi translated">用交互式代码实现英伟达用于自动驾驶汽车的神经网络。(2018).走向数据科学。检索于2018年4月18日，来自<a class="ae jy" rel="noopener" target="_blank" href="/implementing-neural-network-used-for-self-driving-cars-from-nvidia-with-interactive-code-manual-aa6780bc70f4">https://towards data science . com/implementing-neural-network-used-for-driving-cars-from-NVIDIA-with-interactive-code-manual-aa 6780 BC 70 f 4</a></li><li id="ea34" class="mh mi iq kb b kc mq kg mr kk ms ko mt ks mu kw mm mn mo mp bi translated">CIFAR-10和CIFAR-100数据集。(2018).Cs.toronto.edu。检索于2018年4月18日，来自<a class="ae jy" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">https://www.cs.toronto.edu/~kriz/cifar.html</a></li><li id="1940" class="mh mi iq kb b kc mq kg mr kk ms ko mt ks mu kw mm mn mo mp bi translated">JaeDukSeo/个人_日常_神经网络_实践。(2018).GitHub。检索于2018年4月18日，来自<a class="ae jy" href="https://github.com/JaeDukSeo/Personal_Daily_NeuralNetwork_Practice/blob/master/y_NDIVIA/d_upload.py" rel="noopener ugc nofollow" target="_blank">https://github . com/JaeDukSeo/Personal _ Daily _ neural network _ Practice/blob/master/y _ ndi via/d _ upload . py</a></li><li id="30b2" class="mh mi iq kb b kc mq kg mr kk ms ko mt ks mu kw mm mn mo mp bi translated">欢迎阅读链接神经元的文档！—链接神经元0.1.0文档。(2018).linked-neurons . readthedocs . io . 2018年4月19日检索，来自<a class="ae jy" href="https://linked-neurons.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">https://linked-neurons.readthedocs.io/en/latest/</a></li><li id="e82f" class="mh mi iq kb b kc mq kg mr kk ms ko mt ks mu kw mm mn mo mp bi translated">JaeDukSeo/个人_日常_神经网络_实践。(2018).GitHub。检索于2018年4月19日，来自<a class="ae jy" href="https://github.com/JaeDukSeo/Personal_Daily_NeuralNetwork_Practice/blob/dd6f52b24f45cdb73b46de5eeca07b3be1b7a195/3_tensorflow/archieve/7_auto_cnn.py" rel="noopener ugc nofollow" target="_blank">https://github . com/JaeDukSeo/Personal _ Daily _ neural network _ Practice/blob/dd6f 52 b 24 f 45 CDB 73 b 46 de 5 eeca 07 B3 be 1b 7 a 195/3 _ tensor flow/archieve/7 _ auto _ CNN . py</a></li><li id="5853" class="mh mi iq kb b kc mq kg mr kk ms ko mt ks mu kw mm mn mo mp bi translated">TensorFlow等同于numpy .重复问题#8246 tensorflow/tensorflow。(2018).GitHub。检索于2018年4月19日，来自https://github.com/tensorflow/tensorflow/issues/8246<a class="ae jy" href="https://github.com/tensorflow/tensorflow/issues/8246" rel="noopener ugc nofollow" target="_blank"/></li><li id="6a92" class="mh mi iq kb b kc mq kg mr kk ms ko mt ks mu kw mm mn mo mp bi translated">Ramachandran，p .，Zoph，b .，&amp; Le，Q. V. (2018年)。搜索激活功能。</li></ol></div></div>    
</body>
</html>