<html>
<head>
<title>Anchor your Model Interpretation by Anchors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">锚定你的模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/anchor-your-model-interpretation-by-anchors-aa4ed7104032?source=collection_archive---------8-----------------------#2018-09-08">https://towardsdatascience.com/anchor-your-model-interpretation-by-anchors-aa4ed7104032?source=collection_archive---------8-----------------------#2018-09-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/176d27a35af23b11b069d2561817b3c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fofLtZPvyaqHtYlv"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">“top view of two white yachts” by <a class="ae jg" href="https://unsplash.com/@tom_grimbert?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tom Grimbert</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="d816" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">模型解释意味着提供原因和背后的逻辑，以实现模型的可问责性和透明性。正如之前的博客(<a class="ae jg" rel="noopener" target="_blank" href="/3-ways-to-interpretate-your-nlp-model-to-management-and-customer-5428bc07ce15">模型解释简介</a>和<a class="ae jg" rel="noopener" target="_blank" href="/interpreting-your-deep-learning-model-by-shap-e69be2b47893"> SHAP </a>所提到的，模型解释对于数据科学获得管理层和客户的信任非常重要。</p><p id="324c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LIME 的作者提到，LIME 在某些情况下无法正确解释模型。因此，他们提出了一种新的模型解释方法，即<strong class="ki jk">锚。</strong></p><p id="f830" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看完这篇文章，你会明白:</p><ul class=""><li id="d616" class="le lf jj ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">石灰解释的局限性</li><li id="fe60" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">新方法:锚</li></ul><h1 id="c1e6" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">石灰解释的局限性</h1><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/413f79f0a0557ab323cbe56733243ef0.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*R05mGuGehKlZcIX50w4cbA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">From Ribeiro, Singh, and Guestrin 2018</figcaption></figure><p id="a424" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然我们可以用简单函数来局部解释复杂性，但它只能解释特定的情况。这意味着它可能不适合看不见的情况。</p><p id="7771" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从以上对情绪预测的解释来看，“不”在左手边提供了积极的影响，而在右手边提供了严重的消极影响。如果我们只看其中一种解释，这是没问题的，但是如果我们把两种解释放在一起看，我们就会感到困惑。</p><h1 id="175e" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">新方法:锚</h1><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/bb52df6bd50b112f184135ede99c6e33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*PaVg9BHdvqB6TYHGxqFrDA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">From Ribeiro, Singh, and Guestrin 2018</figcaption></figure><p id="b5c9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过学习直线(或斜率)，LIME 解释了预测结果。与 LIME 不同，Anchors 使用“局部区域”来学习如何解释模型。为了解释,“局部区域”是指生成数据集的更好构造。</p><p id="51e1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有两种方法可以找到锚点。第一种是自下而上的方法，这种方法简单，但需要更多的时间来计算结果。第二种是“锚”库中采用的波束搜索。</p><p id="b631" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mw">自下而上的方法</em></p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/2448719de6b080997c5a21d9d17b4cef.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*gh4cyEDCTuKSY5ZgEo-4qw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">From Ribeiro, Singh, and Guestrin 2018</figcaption></figure><p id="a658" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该方法从空集开始。在每次迭代中，它将生成一组候选项，并且每次将添加一个新的特征谓词(特征输入)。如果规则达到精度并满足等式 3 的标准，它将停止寻找下一个特征谓词。因为它的目标是找到<strong class="ki jk">最短的锚点</strong>，因为他们注意到短锚点可能具有更高的覆盖率。</p><p id="5867" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">直觉上，有两个主要问题。一次只能添加一个功能。同样，贪婪搜索的主要目的是识别最短路径。</p><p id="5930" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mw">光束搜索方法</em></p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi my"><img src="../Images/762af8c51e515b0c81865884c431e016.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*fvxZsFb5i0DQRENWNOEIUg.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">From Ribeiro, Singh, and Guestrin 2018</figcaption></figure><p id="314c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种方法解决了贪婪搜索的局限性。它使用 KL-LUCB 算法来选择那些最佳候选。预期的结果是，这种方法比自底向上搜索更有可能识别具有更高覆盖率的锚点。</p><h1 id="5493" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">履行</h1><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="85d3" class="ne lt jj na b gy nf ng l nh ni">explainer = anchor_text.AnchorText(spacy_nlp, labels, use_unk_distribution=True)<br/>    exp = explainer.explain_instance(x_test[idx], estimator, threshold=0.8, use_proba=True, batch_size=30)</span><span id="f1bf" class="ne lt jj na b gy nj ng l nh ni">max_pred = 2<br/>    print('Key Singal from Anchors: %s' % (' AND '.join(exp.names())))<br/>    print('Precision: %.2f' % exp.precision())<br/>    print()</span><span id="b2de" class="ne lt jj na b gy nj ng l nh ni">exp.show_in_notebook()<br/></span></pre><p id="a81b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当调用“解释实例”时，我们需要提供</p><ol class=""><li id="7456" class="le lf jj ki b kj kk kn ko kr lg kv lh kz li ld nk lk ll lm bi translated">x _ test[idx]:x 的目标</li><li id="88a5" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld nk lk ll lm bi translated">评估者:你的模型</li><li id="ae36" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld nk lk ll lm bi translated">阈值:锚点使用此阈值来查找包含锚点(特征)的最小精度</li><li id="3bef" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld nk lk ll lm bi translated">batch_size:要生成的批的数量。更多的批处理意味着生成更多可能的数据集，但也需要更长的时间</li></ol><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/5a7125f546a5997811546fb4388217ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jxNRxwTKIduP7spW6l-TsA.png"/></div></div></figure><p id="4836" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据上面的解释，Anchors 规定“印度”是将输入分类为“b”类别的输入锚点。</p><h1 id="35f7" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">外卖食品</h1><p id="c5d8" class="pw-post-body-paragraph kg kh jj ki b kj nm kl km kn nn kp kq kr no kt ku kv np kx ky kz nq lb lc ld im bi translated">要访问所有代码，你可以访问我的 github repo。</p><ul class=""><li id="d4c4" class="le lf jj ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">与 SHAP 相比，<strong class="ki jk">计算时间更少。</strong></li><li id="aaeb" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">在我之前的文章中，我用 SHAP 和锚来解释这个预测。你也可以<strong class="ki jk">考虑使用多模型解释器</strong>。</li><li id="8568" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki jk">标签只能接受整数</strong>。意味着不能传递准确的分类名称，只能传递编码的类别。</li></ul><h1 id="b93e" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kg kh jj ki b kj nm kl km kn nn kp kq kr no kt ku kv np kx ky kz nq lb lc ld im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以通过<a class="ae jg" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae jg" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae jg" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="d04d" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">参考</h1><p id="fd21" class="pw-post-body-paragraph kg kh jj ki b kj nm kl km kn nn kp kq kr no kt ku kv np kx ky kz nq lb lc ld im bi translated">里贝罗·m·t·辛格·s·格斯特林·C..“我为什么要相信你？”解释任何分类器的预测。2016.<a class="ae jg" href="https://arxiv.org/pdf/1602.04938.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1602.04938.pdf</a></p><p id="c162" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">里贝罗·m·t·辛格·s·格斯特林·C..锚:高精度模型不可知的解释。2018.<a class="ae jg" href="https://homes.cs.washington.edu/~marcotcr/aaai18.pdf" rel="noopener ugc nofollow" target="_blank">https://homes.cs.washington.edu/~marcotcr/aaai18.pdf</a></p><p id="549f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Kaufmann E .，Kalyanakrishnan s .“Bandit 子集选择中的信息复杂性”，2013 年。<a class="ae jg" href="http://proceedings.mlr.press/v30/Kaufmann13.pdf" rel="noopener ugc nofollow" target="_blank">http://proceedings.mlr.press/v30/Kaufmann13.pdf</a></p></div></div>    
</body>
</html>