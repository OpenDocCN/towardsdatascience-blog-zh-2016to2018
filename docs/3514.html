<html>
<head>
<title>Optimizers be TensorFlow’s Appetizers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化器是 TensorFlow 的开胃菜</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimizers-be-deeps-appetizers-511f3706aa67?source=collection_archive---------6-----------------------#2018-05-19">https://towardsdatascience.com/optimizers-be-deeps-appetizers-511f3706aa67?source=collection_archive---------6-----------------------#2018-05-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/c6ee925af2863d3c7b6520d23ff12056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*d6DJhBtdxR3uBlg0."/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/@patmcmanaman?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Patrick McManaman</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="022b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着大量深度学习框架的兴起，训练深度学习模型变得日益容易。TensorFlow 就是这样一个框架。</p><p id="c423" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管最初只在谷歌内部使用，但它在 2015 年被开源，此后一直是 ml 爱好者和研究人员使用最多的框架之一。</p><p id="54f1" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是不管这个框架对我们有多大帮助，还是有一些超参数会对我们训练的模型产生重大影响。改变一个超参数，也就是说，tensorflow 中的一行代码可以让你从 70%的准确率达到最先进的水平。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="576c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 TensorFlow 中，为了调整我们的模型，我们必须做出一些优化选择。正如该领域的许多领导者所说，机器学习是一个高度迭代的过程。选择超参数没有硬性规定。你必须遵循这个过程</p><p id="b425" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> <em class="li">试→调→提高→试→调→提高→试→ </em> </strong></p><p id="6a32" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在使用 TF 时，您会遇到两个重要的超参数。他们是<strong class="kf jh">初始化器</strong>和<strong class="kf jh">优化器</strong>。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="f8fd" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">想象一下“<strong class="kf jh"> <em class="li">成本山</em> </strong>”。</p><p id="9228" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的目标是到达成本山的底部。</p><figure class="lk ll lm ln gt is gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/2d94478018a50b35d4f46b365ef63661.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*RobpQEQ2NM-KnVjcrPX9fw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">The “cost hill”</figcaption></figure><blockquote class="lo lp lq"><p id="a6ff" class="kd ke li kf b kg kh ki kj kk kl km kn lr kp kq kr ls kt ku kv lt kx ky kz la ij bi translated">把初始化器看作是你开始下坡路的起点。和优化器作为你走下坡路的方法。</p></blockquote></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="7a43" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">初始化器</strong>用于初始化我们神经网络各层之间的权重。虽然我们可能会认为它们只是我们探索“成本山”<em class="li"/>底部之旅的随机起点，但它们远不止看上去那么简单。</p><p id="2982" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">阅读这篇关于为什么初始化如此重要的博客。</p><div class="ip iq gp gr ir lu"><a rel="noopener follow" target="_blank" href="/random-initialization-for-neural-networks-a-thing-of-the-past-bfcdd806bf9e"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jh gy z fp lz fr fs ma fu fw jf bi translated">神经网络的随机初始化:过去的事情</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">最近，神经网络已经成为几乎所有机器学习相关问题的解决方案。仅仅是因为…</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="me l mf mg mh md mi ix lu"/></div></div></a></div></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="6b6e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们有<strong class="kf jh">优化器。它们被用来决定我们下坡的步伐。我们可以通过三种主要方式实现这些跨越。</strong></p><ol class=""><li id="69a7" class="mj mk jg kf b kg kh kk kl ko ml ks mm kw mn la mo mp mq mr bi translated">批量梯度下降</li></ol><p id="ba8f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.小批量梯度下降</p><p id="cdef" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.随机梯度下降</p><p id="5907" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">我们培训流程中的一个步骤包括以下内容</strong>:</p><p id="a9ba" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正向传播→寻找成本→反向传播→寻找梯度→更新参数</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="eb99" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">批量梯度下降</strong>是指我们的整个训练集(即:我们的“m”个训练示例中的每一个)用于上述步骤。对“m”个训练样本进行前向传播，计算成本，对所有“m”个训练样本进行后向传播。对所有“m”个训练样本计算梯度。并且参数被更新。</p><p id="64bb" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之，在向成本山的底部迈出一小步之前，计算机必须经历所有的“m”训练示例。当训练集示例的数量很小时，比如说在 1000 到 5000 之间，这是理想的。但在通常包含数百万训练样本的现代数据集上，这种方法注定会失败。原因是缺乏计算能力和磁盘空间来一次性计算所有的训练集示例。毕竟，我们只能朝着“成本山”向下迈出一小步。</p><figure class="lk ll lm ln gt is gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/411af9302301180e678f8d5b46cae713.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*aBC6toAIQUCHXHFIJM6Zbw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">batch gradient descent as shown on a slide from Andrew N G’s machine learning course on Coursera</figcaption></figure><p id="8821" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意成本是如何均匀地直线下降到成本山的底部的。这是在梯度下降的每一步中使用我们所有的‘m’训练示例的优势。这是因为我们使用所有梯度的平均值来更新我们的参数，这将最有可能直接朝着最小成本的方向。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="4e22" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">随机梯度下降</strong>是一种方法，在这种方法中，我们一完成对每个训练集示例的计算，就开始采取向下的步骤。这有助于我们通过使用来自每个训练集示例的梯度来更新参数，从而在向下的旅程中尽快取得进展。这样，我们将在整个训练集的一次通过中向底部迈出“m”步，而不是在批量梯度下降中仅迈出一步。</p><p id="a14f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种方法可以快速得出结果，但有两个主要缺点:</p><p id="2765" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，由于我们的每个训练集示例都是不同的，对每个示例计算的梯度都指向不同的方向，因此我们不会直接朝着最小成本的方向前进，而是不一致地、有效地朝着最小成本的方向前进。因此，参数实际上永远不会达到最小成本点，而只会一直在它的附近盘旋。</p><p id="b7ce" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随机梯度下降的第二个缺点是，我们将失去向量化对代码的影响。矢量化是深度学习的核心，这是我们的计算机能够以如此快的速度执行如此复杂的计算的原因。在随机梯度下降中，由于计算是在每个训练集示例上一个接一个地进行的，因此必须使用 for 循环来遍历训练集，与矢量化计算相比，这可能需要花费一生的时间。</p><figure class="lk ll lm ln gt is gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/066b72346d8ede7e1bb522f5e45f8b35.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*GgCnDE_0YL8q7bnClxEW9A.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">this image shows stochastic gradient descent</figcaption></figure><p id="734c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意成本如何有效地朝着成本最小点前进，但不是直接到达那里。它也永远不会真正达到最低成本，它只能在自己的区域内循环。</p><figure class="lk ll lm ln gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mt"><img src="../Images/0411e66d63319b166d2593a183b424f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*DtH11u3I6YsarJeiMVZ1uQ.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><a class="ae jd" href="https://www.facebook.com/convolutionalmemes/" rel="noopener ugc nofollow" target="_blank">https://www.facebook.com/convolutionalmemes/</a></figcaption></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="fb82" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是<strong class="kf jh">小批量梯度下降</strong>发挥作用的地方。它处于批量梯度下降和随机梯度下降之间最佳点。我们不想通过一次获取所有的训练样本来浪费计算能力和内存。我们也不想因为一次只取一个训练样本而失去矢量化的能力。所以我们一次取多个训练样本，通常是 2 的幂。通过这种方式，我们可以在遍历整个训练集之前开始采取向下的步骤，而且还可以利用矢量化的能力来实现快速计算和更少的内存需求。</p><p id="62a8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是通过将我们的训练集分成固定数量的样本来完成的。这个数字被称为批量。它通常是 2 的幂。32 和 64 是常见的批量大小。训练集首先被混洗，因为我们不希望在每个时期计算相同的批次(一个时期是一次通过整个训练集，一个迷你批次接一个迷你批次)。下面给出了将训练集分割成小批次的代码。</p><figure class="lk ll lm ln gt is"><div class="bz fp l di"><div class="mu mv l"/></div></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="bce2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我在 TensorFlow 中训练了一个模型，使用时尚-MNIST 数据集识别 10 种不同类型的衣服。该数据集由 70，000 幅灰度图像组成。每张图片属于十类服装中的一类。</p><p id="5cd2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是我在<a class="ae jd" href="https://github.com/aditya9898/tensor-fashion" rel="noopener ugc nofollow" target="_blank"> <strong class="kf jh"> GitHub </strong> </a>上训练过的模型的代码。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h2 id="65db" class="mw mx jg bd my mz na dn nb nc nd dp ne ko nf ng nh ks ni nj nk kw nl nm nn no bi translated">一如既往的快乐学习。</h2><p id="d584" class="pw-post-body-paragraph kd ke jg kf b kg np ki kj kk nq km kn ko nr kq kr ks ns ku kv kw nt ky kz la ij bi translated"><em class="li"> ps:忽略俗气的标题</em></p><figure class="lk ll lm ln gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/6468ab68b857d17cc68873aed3c5806e.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*NkLSqJ3IuH0TLo3vMqhHFw.png"/></div></figure></div></div>    
</body>
</html>