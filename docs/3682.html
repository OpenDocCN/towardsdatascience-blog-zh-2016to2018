<html>
<head>
<title>Under The Hood of Neural Networks. Part 2: Recurrent.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在神经网络的保护下。第二部分:经常性。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/under-the-hood-of-neural-networks-part-2-recurrent-af091247ba78?source=collection_archive---------5-----------------------#2018-06-07">https://towardsdatascience.com/under-the-hood-of-neural-networks-part-2-recurrent-af091247ba78?source=collection_archive---------5-----------------------#2018-06-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/862e63cc547254c8b0de35b5da2c930f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LNCCiShcmOHjqqWrbreW0A.png"/></div></div></figure><div class=""/><p id="3078" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本系列的第 1 部分中，我们已经研究了前馈全连接网络的前向和后向通路。尽管前馈网络很普遍，并且在现实世界中有很多应用，但是它们有一个主要的局限性。前馈网络不能处理顺序数据。这意味着它们不能处理不同大小的输入，也不能存储以前状态的信息(内存)。因此，在本文中，我们将讨论允许克服命名限制的递归神经网络(RNNs)。</p><h1 id="977a" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">前进传球</h1><p id="b6d4" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">从技术上讲，<em class="ma">循环网络</em>可以表示为用状态变量和循环回路扩展的前馈网络。因此，在数学上，RNNs 的输入序列的每个元素被处理如下:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mb"><img src="../Images/b9d6589421eef2cca4b8c4676501c6a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3mDe6V5DRXqpHYKDfxN4Rg.png"/></div></div></figure><p id="a194" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<strong class="ka jc"> It </strong>为时步<strong class="ka jc"> t </strong>(输入序列的第<strong class="ka jc"> t </strong>个元素)的输入，<strong class="ka jc"> st </strong>为时步<strong class="ka jc"> t </strong>的隐藏状态，<strong class="ka jc"> yt </strong>为时步 t 的输出，<strong class="ka jc"> fs </strong>和<strong class="ka jc"> fo </strong>为各层的激活函数。从等式中我们可以看出，递归网络的状态基本上是隐含层的输出。网络的 r <em class="ma"> ecurrence </em>通过在时间步长<strong class="ka jc"> t </strong>的计算中出现<strong class="ka jc"> t-1 </strong>项来解释。因此递归网络的每次计算都依赖于先前的状态和输入。</p><p id="ebf6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">随着递归网络在各种现实世界问题中的应用，我们可以看到 RNN 架构的不同变体，这些变体不会显著改变背后的数学。这种应用和相应模型的例子可以是:(a)语言翻译，(b)视频分类，(c)图像字幕，(d)自动完成系统。</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mg"><img src="../Images/33a2596060bc3a2d460c4e315119c347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6yyJhTE-j3nzGnkjXX21qg.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">(a) RNN for translation</figcaption></figure><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/4d59bc3e6f3264f6b89a636ec435591f.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*6tOfFBZhZ2ERKuDFiYTThA.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">(b) RNN for video classification</figcaption></figure><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/120be624aaf4ab58950b205d49d7e114.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*TMniCGQ3v0B_3G5Cyf4wCQ.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">(c ) RNN for image captioning</figcaption></figure><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/d65ad1e19c05bbb6f5b03f9c3d3509e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*SrCrqZqCTpHLcFL-LF_H4Q.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">(d) RNN for the autocomplete system</figcaption></figure><p id="7ef6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于情况(a)的训练过程的数学推导可以容易地应用于其余情况，我们将考虑一个序列遍历网络的简单例子。假设我们有一个数字序列(用二进制表示)1，3，5，在将它们逐个输入我们的网络后，我们希望得到 5，3，1 作为输出。首先让我们随机初始化变量:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mo"><img src="../Images/30c203a2ff1576c1c6192c1b9e376990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_gGwrl2oEhsi6u4XbWFOzg.png"/></div></div></figure><p id="28ee" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">注:</strong>这里为了计算简单，我们将状态大小设为 2，然而状态变量通常是高维变量。</p><p id="cdbf" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于输出端可以有多个 1，我们将使用<a class="ae kw" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid </a>激活函数(<strong class="ka jc"> fo </strong>)，而<strong class="ka jc"> fs </strong>将使用<strong class="ka jc"> tanh </strong>。</p><p id="70db" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，已经初始化了变量，我们可以使用上面介绍的等式来计算网络的正向传递。结果我们会得到:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mp"><img src="../Images/03028fe12b29c9f48e7d4fbf5ff6c583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*08RSCpVfC5D7So-CcFBfqg.png"/></div></div></figure><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mq"><img src="../Images/1c256e518c9950543d95cade25efef47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SXkTYFGPIJF0mXPY9BnOKw.png"/></div></div></figure><p id="3dbb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">示意性地，我们的网络看起来像:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mg"><img src="../Images/0a7199f9af6cdb4569be824704605efa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ORht1uxQ9cmtM_lccAAslw.png"/></div></div></figure><h1 id="f658" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">偶数道次</h1><p id="31e2" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">与前馈相比，递归网络的第一个不同之处在于，我们也可以将序列作为输出。因此，损耗表示为每个输出端的损耗之和(或它们的平均值)。因此:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mr"><img src="../Images/d0be411b699f9722205f38c2bdb6bce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pM9e8pGgQ462XVGvFqDDlQ.png"/></div></div></figure><p id="965e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<strong class="ka jc">θ</strong>可以由任何可训练变量表示。</p><p id="6ccb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里我们将再次使用交叉衰减(在<a class="ae kw" rel="noopener" target="_blank" href="/under-the-hood-of-neural-networks-part-1-fully-connected-5223b7f78528">第 1 部分</a>中介绍)。在这种情况下，我们向前传球的损失是<strong class="ka jc"> 3.30 </strong>。在激活功能之前，损失相对于<strong class="ka jc"> o </strong>的导数为:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/cf82d0595f7cd086d4eb0ba4bee7e6ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*L1r7-afTMl-wFRVhf_9XcA.png"/></div></figure><p id="b1bf" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于<strong class="ka jc"> Wo </strong>和<strong class="ka jc"> Bo </strong>不依赖于之前的时间步，没有循环回路，我们将从它们开始推导:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mt"><img src="../Images/afdfcf3938f08dd854210e29c8e18311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*71zNtLdvm7xdrC0ONHyZ2g.png"/></div></div></figure><p id="dde2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">继续处理其余变量:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mu"><img src="../Images/9d2fa0437fff09be2e5ede913450eda2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DTuFnQRfDnfh6wdm9qJ1og.png"/></div></div></figure><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mv"><img src="../Images/2b0fde932a49814b9a8e84e7a1283d31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zp8vMmxqApjHsVSq88krjA.png"/></div></div></figure><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mw"><img src="../Images/dfce4c885e7fe559f96ee25a70193545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CnKdqjDAMnHM52-OCzg6Sw.png"/></div></div></figure><p id="da74" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">注:<strong class="ka jc"> tanh </strong>激活函数的</strong>衍生物:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mx"><img src="../Images/ee2f5a5499563823239b8e515f40df17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cwrKvk8_5SaFO7Emtb7LNA.png"/></div></div></figure><p id="b3e0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们有了计算梯度和应用梯度下降算法的所有方程(在<a class="ae kw" rel="noopener" target="_blank" href="/under-the-hood-of-neural-networks-part-1-fully-connected-5223b7f78528">第 1 部分</a>中讨论)。让我们从分别计算每个输出的梯度开始，我们将从时间步骤 4 的输出开始:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi my"><img src="../Images/f424a0cf635610b0bace365d26c78337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Ig_ktFSB3_9ctbpq5R4EA.png"/></div></div></figure><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mz"><img src="../Images/3c2f8753d796c623997c235f9eb9395e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mVt2YbF24a0YGdstY8IRPQ.png"/></div></div></figure><p id="a781" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对时间步骤 5 和 6 的输出执行相同的计算，我们将得到:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi na"><img src="../Images/d66b56c655e098fbed75535dca980701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7huK8zcycBGUcp8cgvVcKg.png"/></div></div></figure><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/91ae9ba806003d8f6f2862beaa312673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*1B_XNcI1L7ICi9CSCYY7Qg.png"/></div></figure><p id="9caa" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有了所有需要的梯度，我们可以将总损失的最终梯度计算为每个单独损失的梯度之和:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nc"><img src="../Images/657422214db378691fbbad647f329bb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sGcu9zQZzJRYMxgVBRy5jA.png"/></div></div></figure><p id="d3bd" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在应用学习率为 0.5 的梯度下降算法后，我们将得到新的变量:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nd"><img src="../Images/954be92169973d248d4730ba923a996b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*reghX7UbTEofyx3j65C9mw.png"/></div></div></figure><p id="8a96" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，更新的网络将导致以下正向传递:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mg"><img src="../Images/cfa86ebb02ac624bba2cb371ebab6a2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ovnCjzYkywqUghOhB25Kg.png"/></div></div></figure><p id="9f78" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">新的总损失等于<strong class="ka jc"> 1.36 </strong>。</p><h1 id="89f5" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">总结</strong></h1><p id="4cd1" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">在本文中，我介绍了<em class="ma">递归神经网络</em> (RNNs)，并推导了训练过程所需的基本方程。这种类型的网络对于序列处理任务特别有用。</p><p id="5f86" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">尽管反向传递后的训练损失显著减少，但这种网络的训练过程仍然是棘手的。首先，应该增加状态的维度。更重要的是，经典(或香草)rnn 很难记住旧的状态。为了解决这个问题，存在许多解决方案。其中最著名的可能是:</p><ol class=""><li id="eb92" class="ne nf jb ka b kb kc kf kg kj ng kn nh kr ni kv nj nk nl nm bi translated">在状态之间添加剩余连接(使得每个状态不仅依赖于前一个状态，也依赖于 t-2 或 t-3 状态)</li><li id="9432" class="ne nf jb ka b kb nn kf no kj np kn nq kr nr kv nj nk nl nm bi translated">使用长短期记忆(<a class="ae kw" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank"> LSTM </a>)细胞，这需要单独一篇文章来描述。</li></ol><p id="8f1d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在下一部分中，我将描述卷积神经网络，它成为了现代深度学习和计算机视觉领域的重要组成部分。</p><p id="1121" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你觉得这篇文章有用，别忘了鼓掌，关注我，看更多这样的帖子！</p><p id="249d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请在下面的评论中分享你对未来帖子的反馈和想法！</p><p id="de82" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">感谢阅读！</strong></p></div></div>    
</body>
</html>