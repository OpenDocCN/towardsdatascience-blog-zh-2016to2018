<html>
<head>
<title>Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">点评:RES next——ils vrc 2016(图像分类)亚军</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac?source=collection_archive---------2-----------------------#2018-12-09">https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac?source=collection_archive---------2-----------------------#2018-12-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4c9a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">神经元网络，一种新的维度:基数</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/96b2cbbd7e1008c4d63ccc1a564a8cfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MvA7JX1Z1rAG5byo"/></div></div></figure><p id="ecb1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi ln translated"><span class="l lo lp lq bm lr ls lt lu lv di">在</span>这个故事里，<strong class="kt ir"> ResNeXt，</strong>由<strong class="kt ir">加州大学圣地亚哥分校</strong>和<strong class="kt ir">脸书艾研究(FAIR)</strong>进行回顾。模型名称 ResNeXt 包含 NeXt。这意味着<em class="lw">下一个</em>尺寸，在<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>之上。下一个维度称为“<strong class="kt ir"> <em class="lw">基数</em></strong>”<strong class="kt ir">维度</strong>。而 ResNeXt 成为 ILSVRC 分类任务的<strong class="kt ir">亚军。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ly"><img src="../Images/accd679d2f97c28d954641203a3c2559.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RHpn70qFNCcqyVjkPdFtGA.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk"><strong class="bd md">ILSVRC 2016 Classification Ranking </strong><a class="ae lx" href="http://image-net.org/challenges/LSVRC/2016/results#loc" rel="noopener ugc nofollow" target="_blank">http://image-net.org/challenges/LSVRC/2016/results#loc</a></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi me"><img src="../Images/6f200ccd80801648290c35d9b965d226.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOoc11tkDoqv0pC6OH7mwA.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk"><strong class="bd md">Residual Block in ResNet (Left), A Block of ResNeXt with Cardinality = 32 (Right)</strong></figcaption></figure><p id="f37a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">与<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">ResNet</a>(ils vrc 2015 中的冠军，3.57%)和<a class="ae lx" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea"> PolyNet </a>(亚军，3.04%，队名 CU-DeepLink)相比，ResNeXt 得到了 3.03%的 Top-5 错误率，相对提高了 15%左右的较大幅度！！</p><p id="a174" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这篇文章发表在<strong class="kt ir"> 2017 CVPR </strong>上，在我写这篇文章的时候已经被引用了<strong class="kt ir"> 500 次</strong>。(<a class="mf mg ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----15d7f17b42ac--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="f006" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">涵盖哪些内容</h1><ol class=""><li id="1929" class="ng nh iq kt b ku ni kx nj la nk le nl li nm lm nn no np nq bi translated"><strong class="kt ir">聚合转换</strong></li><li id="801d" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm nn no np nq bi translated"><strong class="kt ir">与</strong> <a class="ae lx" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> <strong class="kt ir">的关系 Inception-ResNet </strong> </a> <strong class="kt ir">，以及</strong> <a class="ae lx" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener"> <strong class="kt ir">中的分组卷积 AlexNet </strong> </a></li><li id="48b6" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm nn no np nq bi translated"><strong class="kt ir">完整的架构和消融研究</strong></li><li id="0375" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm nn no np nq bi translated"><strong class="kt ir">结果</strong></li></ol></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="ad50" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">1.聚集变换</h1><h2 id="f984" class="nw mp iq bd mq nx ny dn mu nz oa dp my la ob oc na le od oe nc li of og ne oh bi translated">1.1.重温简单神经元</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/82c3cf303e32ff75ea9a614a7f50cfc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*lm62iiybACCoz4KR9w5Azg.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk"><strong class="bd md">A Simple Neuron (Left), and the corresponding equation (Right)</strong></figcaption></figure><p id="0a28" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们应该知道，<strong class="kt ir">一个简单的神经元</strong>如上，其输出是 wi 乘以 xi 的总和。上面的操作可以被改写为<strong class="kt ir">拆分、转换和聚合的组合。</strong></p><ul class=""><li id="beaf" class="ng nh iq kt b ku kv kx ky la oj le ok li ol lm om no np nq bi translated"><strong class="kt ir">分裂</strong>:向量 x 被切片为低维嵌入，在上图中，它是一个一维子空间 xi。</li><li id="28ea" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated"><strong class="kt ir">变换</strong>:对低维表示进行变换，在上面，简单缩放:wixi。</li><li id="6164" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated"><strong class="kt ir">聚合</strong>:所有嵌入中的变换通过求和来聚合。</li></ul><h2 id="e151" class="nw mp iq bd mq nx ny dn mu nz oa dp my la ob oc na le od oe nc li of og ne oh bi translated">1.2.聚合转换</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/29511c8ba99439b37caa4b0ed8255a3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r_63luxLeVstJMzd0o-8Iw.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk"><strong class="bd md">A Block of ResNeXt with Cardinality = 32 (Left), and Its Generic Equation (Right)</strong></figcaption></figure><p id="0733" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">与“网络中的网络”形成对比的是，“<strong class="kt ir">神经元中的网络</strong>”沿着一个新的维度展开。代替在每个路径中乘以 xi 的简单神经元中的线性函数，<strong class="kt ir">为每个路径执行非线性函数</strong>。</p><p id="b715" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">引入了一个新的维度<strong class="kt ir"> <em class="lw"> C </em> </strong>，称为“<strong class="kt ir">基数</strong>”。基数的维度<strong class="kt ir">控制着更复杂转换的数量</strong>。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="0a3f" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated"><strong class="ak"> 2。</strong> <a class="ae lx" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> <strong class="ak">与【Inception-ResNet】</strong></a><strong class="ak">的关系以及</strong> <a class="ae lx" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener"> <strong class="ak">与【AlexNet】</strong></a></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/c7323b61a938b270446604a35f2fd4fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cIm3uy7eNvEchxRbBeBScQ.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk"><strong class="bd md">(a) ResNeXt Block, (b) Inception-ResNet Block, (c) Grouped Convolution</strong></figcaption></figure><p id="4bea" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了比较，<strong class="kt ir">上述 3 个块在每个块内具有相同的内部尺寸。</strong></p><p id="f444" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> (a) ResNeXt 块(左)</strong></p><ul class=""><li id="2537" class="ng nh iq kt b ku kv kx ky la oj le ok li ol lm om no np nq bi translated">对于每个路径，<strong class="kt ir">con v1×1–con v3×3–con v1×1</strong>在每个卷积路径上完成。这是<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>块中的瓶颈设计。每个路径的内部尺寸表示为<strong class="kt ir"><em class="lw">d</em>(<em class="lw">d</em>= 4)</strong>。路径数是<strong class="kt ir">基数<em class="lw"> C </em> ( <em class="lw"> C </em> =32) </strong>。如果我们把每个 Conv3×3 的尺寸加起来(即<em class="lw"> d </em> × <em class="lw"> C </em> =4×32)，也是 128 的尺寸。</li><li id="f45b" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">维度直接从 4 增加到 256，然后加在一起，还加了跳过连接路径。</li><li id="ac99" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">与<a class="ae lx" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-ResNet </a>需要将维度从 4 增加到 128 再增加到 256 相比，<strong class="kt ir"> ResNeXt 需要最小的额外工作来设计每条路径。</strong></li><li id="ea06" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">与<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>不同，在 ResNeXt 中，一条路径上的神经元不会连接到其他路径上的神经元。</li></ul><p id="c543" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> (b) </strong> <a class="ae lx" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> <strong class="kt ir">【盗梦空间】</strong> </a> <strong class="kt ir">街区(中间)</strong></p><ul class=""><li id="a564" class="ng nh iq kt b ku kv kx ky la oj le ok li ol lm om no np nq bi translated">这在<a class="ae lx" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-v4 </a>中建议将<a class="ae lx" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener"> Inception </a>模块和<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>模块合并。由于遗留问题，对于每个卷积路径，首先执行<strong class="kt ir">con v1×1–con v3×3</strong>。当加在一起时(即 4×32)，Conv3×3 的尺寸为 128。</li><li id="1309" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">然后输出以 128 的维数连接在一起。<strong class="kt ir"> Conv1×1 </strong>用于将尺寸从 128 恢复到 256。</li><li id="f428" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">最后，使用跳过连接路径添加输出。</li><li id="8480" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">主要区别在于它们有一个<strong class="kt ir">早期连接</strong>。</li></ul><p id="2f32" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> (c)分组卷积</strong><a class="ae lx" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener"><strong class="kt ir">AlexNet</strong></a><strong class="kt ir">(右)</strong></p><ul class=""><li id="b344" class="ng nh iq kt b ku kv kx ky la oj le ok li ol lm om no np nq bi translated"><strong class="kt ir">con v1×1–con v3×3–con v1×1</strong>在卷积路径上完成，实际上是<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>中提出的一个<strong class="kt ir">瓶颈设计</strong>。Conv3×3 的尺寸为 128。</li><li id="9c7b" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">然而，这里使用了<a class="ae lx" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener"> AlexNet </a>中建议的<strong class="kt ir">分组卷积</strong>。因此，Conv3×3 是<strong class="kt ir">更宽但连接稀疏的模块。</strong>(因为一条路径上的神经元不会连接到其他路径上的神经元，所以连接稀疏。)</li><li id="5883" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">因此<strong class="kt ir">有 32 组回旋</strong>。(仅在<a class="ae lx" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener"> AlexNet </a>中有 2 组)</li><li id="eae5" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">则跳过连接是并行的，并添加了卷积路径。因此，卷积路径正在学习残差表示。</li></ul><p id="e9f0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">尽管(b)和(c)中的结构并不总是与 1.2 中所示方程的一般形式相同，实际上作者已经尝试了上述三种结构，他们发现结果是相同的。</p><p id="9345" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最后，作者选择实现(c)中的结构，因为它比其他两种形式更简洁和更快。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="e7a5" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">3.<strong class="ak">完整的架构和消融研究</strong></h1><h2 id="e30a" class="nw mp iq bd mq nx ny dn mu nz oa dp my la ob oc na le od oe nc li of og ne oh bi translated">3.1.相似复杂度下<em class="op"> C </em>和<em class="op"> d </em>的烧蚀研究</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/4cddfa0a0a59415c9dcd3fcf3cd2319a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UNkmAg1JVfprEfBBmqM44w.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk"><strong class="bd md">Detailed Architecture (Left), Number of Parameters for Each Block (Top Right), Different Settings to Maintain Similar Complexity (Middle Right), Ablation Study for Different Settings Under Similar Complexity (Bottom Right)</strong></figcaption></figure><ul class=""><li id="6013" class="ng nh iq kt b ku kv kx ky la oj le ok li ol lm om no np nq bi translated"><a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-50 </a>是 ResNeXt-50 的特例，其中<em class="lw"> C </em> =1，<em class="lw"> d </em> =64。</li><li id="eb48" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">为了公平比较，尝试了具有不同<em class="lw"> C </em>的不同 ResNeXt 和具有与<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>相似复杂度的<em class="lw"> d </em>。这在上图的右中部显示。</li><li id="a697" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">发现 ResNeXt-50 (32×4d)对 ImageNet-1K (1K 表示 1K 类)数据集获得 22.2%的 top-1 错误，而<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-50 </a>仅获得 23.9%的 top-1 错误。</li><li id="6475" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">ResNeXt-101 (32×4d)对 ImageNet 数据集获得 21.2%的 top-1 误差，而<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-101 </a>仅获得 22.0%的 top-1 误差。</li></ul><h2 id="f880" class="nw mp iq bd mq nx ny dn mu nz oa dp my la ob oc na le od oe nc li of og ne oh bi translated">3.2.基数的重要性</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/b07a682f6714a0eae4c393ad4116ce9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*xxm28sWSo6i-If09kwAqPQ.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk"><strong class="bd md">Ablation Study for Different Settings of 2× Complexity Models</strong></figcaption></figure><ul class=""><li id="014c" class="ng nh iq kt b ku kv kx ky la oj le ok li ol lm om no np nq bi translated"><a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kt ir">ResNet-200</strong></a>:21.7%的前 1 名和 5.8%的前 5 名错误率。</li><li id="4fd4" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated"><a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="kt ir"> ResNet-101，更宽</strong> </a>:仅获得 21.3%的 top-1 和 5.7%的 top-5 错误率，也就是说<strong class="kt ir">只做得更宽并没有太大帮助</strong>。</li><li id="4b39" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated"><strong class="kt ir"> ResNeXt-101 (2×64d) </strong>:仅通过使<strong class="kt ir"><em class="lw"/>= 2</strong>(即 ResNeXt 块内的两条卷积路径)<strong class="kt ir">已经获得了明显的改善</strong>，top-1 错误率为 20.7%，top-5 错误率为 5.5%。</li><li id="0a63" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated"><strong class="kt ir"> ResNeXt-101 (64×4d) </strong>:通过使<strong class="kt ir"><em class="lw"/>= 64</strong>(即 ResNeXt 块内的 64 个卷积路径)<strong class="kt ir">已经获得了更好的改进</strong>，具有 20.4%的 top-1 和 5.3%的 top-5 错误率。这意味着<strong class="kt ir">基数对于提高分类精度</strong>至关重要。</li></ul><h2 id="57d7" class="nw mp iq bd mq nx ny dn mu nz oa dp my la ob oc na le od oe nc li of og ne oh bi translated">3.2.剩余连接的重要性</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/b8b18ffa9c1d61dd9c09283d4f8bb4d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*EwrkhL-fqVii9MEjCHMZBA.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk"><strong class="bd md">Importance of Residual Connections</strong></figcaption></figure><p id="39d7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果没有剩余连接，ResNet-50 和 ResNeXt-50 的错误率都会大大增加。残留连接很重要。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="558f" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">4.结果</h1><h2 id="a94e" class="nw mp iq bd mq nx ny dn mu nz oa dp my la ob oc na le od oe nc li of og ne oh bi translated">4.1.ImageNet-1K</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/a7a7aae361a2e19cdb59005a3471022b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oLRaAqY2cnw2E5eJ_D8jmA.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk"><strong class="bd md">Single Crop Testing: ResNet/ResNeXt is 224×224 and 320×320, Inception models: 299×299</strong></figcaption></figure><p id="d9ed" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">ImageNet-1K 是 22K 类 ImageNet 数据集的子集，包含 1000 个类。它也是 ILSVRC 分类任务的数据集。</p><ul class=""><li id="752e" class="ng nh iq kt b ku kv kx ky la oj le ok li ol lm om no np nq bi translated">使用标准尺寸图像进行单作物测试，ResNeXt-101 获得 20.4%前 1 名和 5.3%前 5 名错误率，</li><li id="b264" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">由于使用更大尺寸的图像进行单作物测试，ResNeXt-101 获得了 19.1%的前 1 名和 4.4%的前 5 名错误率，这比所有最先进的方法、<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>、<a class="ae lx" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e">预激活 ResNet </a>、<a class="ae lx" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener"> Inception-v3 </a>、<a class="ae lx" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-v4 </a>和<a class="ae lx" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-ResNet-v2 </a>都有更好的结果。</li></ul><h2 id="321e" class="nw mp iq bd mq nx ny dn mu nz oa dp my la ob oc na le od oe nc li of og ne oh bi translated">4.2.ImageNet-5K</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/a0d775d2c141a37ae1e64982965500e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V99JmLNTxlXxIPYAIFiGwA.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk"><strong class="bd md">ImageNet-5K Results (All trained from scratch)</strong></figcaption></figure><p id="97a7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">ImageNet-1K 经过这么多年的发展，已经有些饱和了。</p><p id="9240" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">ImageNet-5K 是 22K 类 ImageNet 数据集的子集，包含 5000 个类，其中也包含 ImageNet-1K 类。</p><ul class=""><li id="b179" class="ng nh iq kt b ku kv kx ky la oj le ok li ol lm om no np nq bi translated">680 万张图片，是 ImageNet-1K 数据集的 5 倍。</li><li id="73ed" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">由于没有正式的训练/验证集，因此使用原始的 ImageNet-1K 验证集进行评估。</li><li id="c2cd" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated"><strong class="kt ir"> 5K 级别</strong>是超过 5K 级别的<strong class="kt ir">soft max</strong>。因此，当网络在 ImageNet-1K 验证数据集上预测其他 4K 类的标签时，将会出现自动错误。</li><li id="fccb" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated"><strong class="kt ir"> 1K 路分类</strong>就是超过 1K 级的<strong class="kt ir">soft max</strong>。</li></ul><p id="34a9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">ResNeXt 当然得到了比上面显示的<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>更好的结果。</p><h2 id="f627" class="nw mp iq bd mq nx ny dn mu nz oa dp my la ob oc na le od oe nc li of og ne oh bi translated">4.3.西法尔-10 和西法尔-100</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/095cb1cfa5dd52fc6b08a2649de2b9b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FqDUkl5vrLZufkQIG6xk7A.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk"><strong class="bd md">CIFAR-10 and CIFAR-100 Results</strong></figcaption></figure><p id="4572" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">CIFAR-10 &amp; CIFAR-100，两个非常著名的 10 类和 100 类数据集。</p><ul class=""><li id="f629" class="ng nh iq kt b ku kv kx ky la oj le ok li ol lm om no np nq bi translated">左图:与<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>相比，ResNeXt 在 CIFAR-10 中总是获得更好的成绩。</li><li id="e839" class="ng nh iq kt b ku nr kx ns la nt le nu li nv lm om no np nq bi translated">右图:与<a class="ae lx" rel="noopener" target="_blank" href="/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004">宽 ResNet (WRN) </a>相比，ResNeXt-29 (16×64d)对于 CIFAR-10 和 CIFAR-100 分别获得 3.58%和 17.31%的误差。这些是当时所有最先进方法中最好的结果。</li></ul><h2 id="39c2" class="nw mp iq bd mq nx ny dn mu nz oa dp my la ob oc na le od oe nc li of og ne oh bi translated">4.4.MS COCO 对象检测</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/bea6d845455155dbb089db1ffdb2d368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*_QhPxRX7B0jROHwhIuljCQ.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk"><strong class="bd md">MS COCO Objection Detection Results</strong></figcaption></figure><ul class=""><li id="7c48" class="ng nh iq kt b ku kv kx ky la oj le ok li ol lm om no np nq bi translated">通过将<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a> /ResNeXt 插入<a class="ae lx" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>，在相似的模型复杂度下，ResNeXt 在所有 IoU 级别的 AP@0.5 (IoU &gt; 0.5)和 mean AP(平均预测)方面始终优于<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>。</li></ul></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="8543" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">随着 ResNeXt 的成功，它也被 Mask R-CNN 用于实例分割。希望我以后也能报道面具 R-CNN。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h2 id="2ad8" class="nw mp iq bd mq nx ny dn mu nz oa dp my la ob oc na le od oe nc li of og ne oh bi translated">参考</h2><p id="0b9a" class="pw-post-body-paragraph kr ks iq kt b ku ni jr kw kx nj ju kz la ox lc ld le oy lg lh li oz lk ll lm ij bi translated">【2017 CVPR】【ResNeXt】<br/><a class="ae lx" href="https://arxiv.org/abs/1611.05431" rel="noopener ugc nofollow" target="_blank">深度神经网络的聚合残差变换</a></p><h2 id="b06a" class="nw mp iq bd mq nx ny dn mu nz oa dp my la ob oc na le od oe nc li of og ne oh bi translated">我对图像分类的相关综述</h2><p id="79f2" class="pw-post-body-paragraph kr ks iq kt b ku ni jr kw kx nj ju kz la ox lc ld le oy lg lh li oz lk ll lm ij bi translated">[<a class="ae lx" href="https://medium.com/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17" rel="noopener">LeNet</a>][<a class="ae lx" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener">AlexNet</a>][<a class="ae lx" href="https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103" rel="noopener">ZFNet</a>][<a class="ae lx" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener">VGGNet</a>][<a class="ae lx" href="https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679" rel="noopener">SPPNet</a>][<a class="ae lx" href="https://medium.com/coinmonks/review-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617" rel="noopener">PReLU-Net</a>][<a class="ae lx" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener">Google Net/Inception-v1</a>][<a class="ae lx" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">BN-Inception/Inception-v2</a>][<a class="ae lx" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener">Inception-v3</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-v4</a><a class="ae lx" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> [ </a><a class="ae lx" rel="noopener" target="_blank" href="/review-ror-resnet-of-resnet-multilevel-resnet-image-classification-cd3b0fcc19bb"> RoR </a> ] [ <a class="ae lx" rel="noopener" target="_blank" href="/review-stochastic-depth-image-classification-a4e225807f4a">随机深度</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004">WRN</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea">PolyNet</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803">dense net</a>]</p><h2 id="030e" class="nw mp iq bd mq nx ny dn mu nz oa dp my la ob oc na le od oe nc li of og ne oh bi translated">我对物体检测的相关评论</h2><p id="2a8b" class="pw-post-body-paragraph kr ks iq kt b ku ni jr kw kx nj ju kz la ox lc ld le oy lg lh li oz lk ll lm ij bi translated">[ <a class="ae lx" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a></p></div></div>    
</body>
</html>