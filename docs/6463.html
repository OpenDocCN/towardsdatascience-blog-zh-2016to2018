<html>
<head>
<title>Want to Cluster Text? Try Custom Word-Embeddings!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">想要对文本进行聚类？尝试自定义单词嵌入！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/want-to-cluster-text-try-custom-word-embeddings-615526cbef7a?source=collection_archive---------23-----------------------#2018-12-14">https://towardsdatascience.com/want-to-cluster-text-try-custom-word-embeddings-615526cbef7a?source=collection_archive---------23-----------------------#2018-12-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bdfb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">分析具有单词嵌入的 Tf-idf 向量的聚类有效性。本文考虑的文本语料库实例表明，自定义单词嵌入可以帮助提高语料库的聚类能力</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5d79e991910378cd23ed2c2beca6f7a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*705lJYazsQ3an9a5.jpg"/></div></div></figure><p id="236e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我们使用单词嵌入的文本分类结果之后，这是一个受欢迎的消息。在分类的上下文中，我们得出结论，用朴素贝叶斯和 tf-idf 向量保持简单是一个很好的起点。虽然我们不能一概而论，但是在构建文档+单词向量时所做的额外工作并没有为分类质量带来相应的收益。在这篇文章中，我们继续前面为文本聚类打下的基础，特别是两篇文章——<a class="ae ln" href="http://xplordat.com/2018/11/05/want-clusters-how-many-will-you-have/" rel="noopener ugc nofollow" target="_blank">想要聚类？你要几个？</a>和<a class="ae ln" href="http://xplordat.com/2018/11/26/clustering-text-with-transformed-document-vectors/" rel="noopener ugc nofollow" target="_blank">用转换后的文档向量聚类文本。</a>我们从<a class="ae ln" href="http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/" rel="noopener ugc nofollow" target="_blank">分类练习</a>中选择我们熟悉的<a class="ae ln" href="http://scikit-learn.org/stable/datasets/twenty_newsgroups.html" rel="noopener ugc nofollow" target="_blank"> 20 新闻</a>和<a class="ae ln" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">影评</a>文本语料库。我们评估了不同的降阶变换的聚类效果。重现这些结果的代码可以从<a class="ae ln" href="https://github.com/ashokc/Evaluating-Document-Transformations-for-Clustering-Text" rel="noopener ugc nofollow" target="_blank"> github </a>下载。</p><p id="94f2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">为什么降阶转换有助于聚类？</strong></p><p id="5457" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我们深入细节之前，让我们停下来考虑一下，是否有任何理由期待应用单词嵌入会带来一些好处。除了计算性能的明显优势之外，如果我们得不到高质量的集群，这当然是没有用的！</p><p id="150f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">从<a class="ae ln" href="https://pdfs.semanticscholar.org/4008/d78a584102086f2641bcb0dab51aff0d353b.pdf" rel="noopener ugc nofollow" target="_blank"> VSM </a>模型产生的文档向量是长且稀疏的。不幸的是，每个文档簇的质心都接近原点。也就是说，所有的文档簇几乎都有一个共同的质心！另外，由于我们将文档向量归一化为具有单位长度，所以一个簇中的每个文档离这个公共质心的距离大约相同。我们的情况是，集群间距离接近于零，集群内距离接近于 1！这不是划分集群的好时机。</p><p id="91d8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="lo">如果</em>我们可以在不破坏文档向量的内容/意义的情况下减少文档向量的顺序，我们当然可以期待更高质量的结果。这正是这篇文章的重点，下面是我的计划。</p><ul class=""><li id="a72a" class="lp lq iq kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated">将文档归入已知的组/群，并将它们混合在一起</li><li id="a6ab" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">应用降阶变换并构建文档向量</li><li id="9178" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">聚集文档向量，看看我们是否已经<em class="lo">完整地恢复了</em>原始组</li><li id="1b62" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">评估不同转换在恢复纯净完整的文档群集方面的能力</li></ul><h2 id="75e4" class="md me iq bd mf mg mh dn mi mj mk dp ml la mm mn mo le mp mq mr li ms mt mu mv bi translated">1.用于聚类的文档向量</h2><p id="3240" class="pw-post-body-paragraph kr ks iq kt b ku mw jr kw kx mx ju kz la my lc ld le mz lg lh li na lk ll lm ij bi translated">从有/没有单词嵌入的文本语料库中构建文档向量的准备工作已经在前面的文章中完成了— <a class="ae ln" href="http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/" rel="noopener ugc nofollow" target="_blank">单词嵌入和文档向量:第 2 部分。分类</a>。我们在弹性搜索索引中有 20 个新闻和电影评论文本语料库。从不同算法(FastText、Word2Vec 和 Glove)构建的预训练和定制单词向量也在索引中。将 K-Means 应用于降阶的文档向量是简单的。</p><ul class=""><li id="b438" class="lp lq iq kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated">从 elasticsearch 索引中获取(停止的令牌)稀疏的<em class="lo"> n </em>长文档向量。<em class="lo"> n </em>是文本语料库的词汇量大小。通过应用单词嵌入，将这些<em class="lo"> n </em>长的稀疏向量转换成密集的<em class="lo"> p </em>长的向量。</li><li id="444e" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">应用 K-Means 聚类(对于 20 条新闻，K=3，对于电影评论，K = 2 ),并找出所获得的聚类的纯度。</li></ul><p id="1377" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下图说明了其中的机制。设置与我们的分类类似，只是我们在这里对向量进行聚类。关于<a class="ae ln" href="http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/" rel="noopener ugc nofollow" target="_blank">分类的早期文章</a>对图中的每个步骤都有详细的描述，代码也在它的<a class="ae ln" href="https://github.com/ashokc/Word-Embeddings-and-Document-Vectors" rel="noopener ugc nofollow" target="_blank"> github </a>中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/ae0221a92e178a4b5ec55a7683b71b82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/0*mMcTIsbom6oDxQF7.jpg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 1. The mechanics of the text clustering exercise. The document (row) vectors X and pre-trained/custom (column) word-vectors W are fetched from the elasticsearch index to build dense (row) vectors Z that are then normalized to have unit length before clustering with K-Means</figcaption></figure><h2 id="e72c" class="md me iq bd mf mg mh dn mi mj mk dp ml la mm mn mo le mp mq mr li ms mt mu mv bi translated">2.变换空间中的距离</h2><p id="6b51" class="pw-post-body-paragraph kr ks iq kt b ku mw jr kw kx mx ju kz la my lc ld le mz lg lh li na lk ll lm ij bi translated">为了说明降阶变换对集群内和集群间距离的典型影响，让我们从 20 个新闻语料库中挑选“alt .无神论”组，并计算如下。</p><ul class=""><li id="a41a" class="lp lq iq kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated"><em class="lo">簇间距离</em>:20 个组中每个组的质心，以及从“alt.athiesm”的质心到其他 19 个组的质心的 19 个距离</li><li id="3a97" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated"><em class="lo">簇内距离</em>:备选无神论组中 799 个文档的每一个到其质心的距离。</li></ul><p id="95ed" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">图 2 中的盒须图(5%和 95%的须)显示了转换前后的距离分布。对于稀疏和长(<em class="lo"> n </em> = 44870，20 个新闻的停止词汇的大小)原始文档向量，簇内的距离都在预期的 1.0 左右，并且簇间距离接近 0.1。使用定制的 fasttext 单词嵌入(使用<em class="lo"> p </em> = 300，即转换后的向量长度为 300)，我们得到了一个有利的距离分布，其中聚类本身被处理(聚类内中值距离<em class="lo">从 1.0 降低</em>到 0.27)，而不同的聚类被推开(与其他聚类的中值距离<em class="lo">从 0.1 左右增加</em>到 0.28)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/272e855a171f82578a55e27c0e984fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/0*a3-yCZUXumnlHeak.jpg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 2. The order reduced document vectors in a known cluster show larger compaction among themselves and greater separation from other clusters. Such transformations are better for the clustering task.</figcaption></figure><h2 id="8e20" class="md me iq bd mf mg mh dn mi mj mk dp ml la mm mn mo le mp mq mr li ms mt mu mv bi translated">3.评估转换</h2><p id="dd79" class="pw-post-body-paragraph kr ks iq kt b ku mw jr kw kx mx ju kz la my lc ld le mz lg lh li na lk ll lm ij bi translated">我们从上一篇文章<a class="ae ln" href="http://xplordat.com/2018/11/26/clustering-text-with-transformed-document-vectors/" rel="noopener ugc nofollow" target="_blank">用转换后的文档向量聚类文本</a>中知道，</p><ul class=""><li id="646c" class="lp lq iq kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated">我们需要将文档向量标准化为单位长度，并且</li><li id="938e" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">不同变换之间的变换向量之间的距离不能直接比较，并且</li><li id="a5a9" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">可以比较每个空间中簇间距离与簇内距离的比率，以获得关于变换的有用性的一些线索。</li></ul><p id="01e8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，我们还概述了计算该比率的以下程序。</p><ol class=""><li id="57f3" class="lp lq iq kt b ku kv kx ky la lr le ls li lt lm nh lv lw lx bi translated">计算每个簇的质心，以及簇中每个文档与该质心之间的距离。找出这些距离的中间值。</li><li id="8bde" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm nh lv lw lx bi translated">取 1 中获得的所有聚类的中间值的平均值。这是该空间的“代表性”距离 A。</li><li id="4288" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm nh lv lw lx bi translated">计算质心之间的平均距离 B。</li><li id="a012" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm nh lv lw lx bi translated">比率 B/A 是我们可以跨转换比较的</li></ol><p id="2af6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了获得更好的集群能力，我们希望 B/A 更大。也就是说，最大化 B/A 的转换应该在聚类方面做得更好。下面的图 3 让我们看到了 at 的前景。所有情况下的文档都使用停止标记(但没有词干)通过 Scikit 的<a class="ae ln" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> tf-idf 矢量器</a>进行了矢量化。然后对这些向量应用不同的降阶变换。我们选择<em class="lo"> p </em>为 300，即转换后的文档向量在所有情况下都具有 300 的长度。尝试的转换包括以下内容。</p><ul class=""><li id="e773" class="lp lq iq kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated"><a class="ae ln" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html" rel="noopener ugc nofollow" target="_blank">奇异值分解</a></li><li id="49b6" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated"><a class="ae ln" href="https://github.com/stanfordnlp/GloVe" rel="noopener ugc nofollow" target="_blank">手套</a>(带<a class="ae ln" href="http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip" rel="noopener ugc nofollow" target="_blank">手套. 840B.300d </a>)</li><li id="b5e2" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated"><a class="ae ln" href="https://fasttext.cc/" rel="noopener ugc nofollow" target="_blank"> FastText </a>(带<a class="ae ln" href="https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M-subword.zip" rel="noopener ugc nofollow" target="_blank">crawl-300d-2M-subword . vec</a></li><li id="d8da" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated"><a class="ae ln" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>(带<a class="ae ln" href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">Google news-vectors-negative 300 . bin</a>)</li><li id="2a3d" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">使用 FastText 定制矢量(使用<a class="ae ln" href="https://radimrehurek.com/gensim/index.html" rel="noopener ugc nofollow" target="_blank"> Gensim </a>)</li><li id="68d2" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">使用 Word2Vec 定制矢量(使用<a class="ae ln" href="https://radimrehurek.com/gensim/index.html" rel="noopener ugc nofollow" target="_blank"> Gensim </a></li></ul><p id="9223" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">根据步骤 1-4 处理每次转换中获得的文档向量，以计算簇间/簇内距离比率 B/A。图 3A 显示了考虑所有 20 个组/簇的<a class="ae ln" href="http://scikit-learn.org/stable/datasets/twenty_newsgroups.html" rel="noopener ugc nofollow" target="_blank"> 20-news </a>文档向量的比率。图 3B 用于<a class="ae ln" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">电影评论</a>数据集。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/12456ef337532bcedd993d59ba227735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*j9E0GOhbHuBXLXz_.jpg"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 3. Order reducing transformations seem to improve the intercluster/intracluster distance ratio, boding well for the clustering task.</figcaption></figure><p id="c06d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">从图 3 中可以看出，降阶转换，尤其是自定义单词嵌入，似乎可以为 B/A 产生较大的值，因此有助于提高聚类能力。是时候验证事实是否如此了。</p><h2 id="b016" class="md me iq bd mf mg mh dn mi mj mk dp ml la mm mn mo le mp mq mr li ms mt mu mv bi translated">4.K-均值聚类</h2><p id="3fe9" class="pw-post-body-paragraph kr ks iq kt b ku mw jr kw kx mx ju kz la my lc ld le mz lg lh li na lk ll lm ij bi translated">现在我们开始实际的集群任务，如图 1 所示，以验证图 3 所显示的内容。我们将使用相同的变换列表，并检查所获得的簇有多纯。也就是说，在任何获得的聚类中，我们希望找到单个组/类的文章。我们的立场是，那些具有较大 B/A 值的转换应该产生更纯的集群。</p><p id="e4db" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在每种情况下，我们给 K-Means 模拟已知的聚类数。假设我们在这里通过给出精确的 K 来帮助 K-Means，我们希望它至少能够将文章按组分开，并将它们放在不同的簇中。但不幸的是，这种情况并不总是发生，在某些情况下，文章会一边倒地聚集在一两个集群中。从我们的角度来看，这是一次失败的集群行动。</p><blockquote class="nj nk nl"><p id="7b38" class="kr ks lo kt b ku kv jr kw kx ky ju kz nm lb lc ld nn lf lg lh no lj lk ll lm ij bi translated">如果聚类任务是成功的，我们应该最终得到这样的聚类，每个聚类都有一个主要部分的文章来自其中一个组，这样我们就可以用那个组来识别那个聚类。</p></blockquote><p id="f247" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然后，通过不属于该聚类/组的文章的数量来测量所获得的聚类的“纯度”。</p><h2 id="c08d" class="md me iq bd mf mg mh dn mi mj mk dp ml la mm mn mo le mp mq mr li ms mt mu mv bi translated">4.1 二十篇新闻文章</h2><p id="3a6a" class="pw-post-body-paragraph kr ks iq kt b ku mw jr kw kx mx ju kz la my lc ld le mz lg lh li na lk ll lm ij bi translated">在 20 个新闻数据集的情况下，我们从 3 个组中挑选文章进行这个练习——这样我们就可以很容易地绘制结果。然而，请注意，图 3A 中的结果是通过考虑所有 20 个组/簇而获得的。词云图提供了我们正在处理的内容的鸟瞰图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2485a16d20e784af08616ce4df83f658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*A9iZiVLMl8KOIx2P.jpg"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 4. The dominant words in these three groups are mostly different. This enhances dissimilarity and so we expect to be successful at clustering</figcaption></figure><p id="9fb1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面的图 5A 显示，使用自定义单词嵌入时，放错位置的文章数量最少，而原始文档向量的情况最差。具有定制 word2vec 嵌入的文档向量产生最纯的集群，每个集群标识一个特定的组。不同变换的性能与图 3A 所示的很好地匹配。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/302077bf3ea18d8cd21f9c0db3d1a5fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AOztj56p8CQgpWQu.jpg"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 5. (A) Six of the 7 attempted transformations were successful at clustering. Order reducing transformations seem to have helped with clistering (B) Several articles about hockey seem to have found their way into hardware &amp; religion based content when using the raw document vectors.</figcaption></figure><h2 id="8fc4" class="md me iq bd mf mg mh dn mi mj mk dp ml la mm mn mo le mp mq mr li ms mt mu mv bi translated">4.2 电影评论</h2><p id="421a" class="pw-post-body-paragraph kr ks iq kt b ku mw jr kw kx mx ju kz la my lc ld le mz lg lh li na lk ll lm ij bi translated">电影评论数据集只有两个类，所以很容易设计，我们考虑所有的文档。这里再一次出现了两个组的单词云。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/15cd2afc402f84abc437fa76cab49b96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wqeQTQEOG1_5Z9jO.jpg"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 6. The positive &amp; negative reviews share a lot of common words that are dominant. We can expect to have some difficulty with clustering.</figcaption></figure><p id="edb9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">图 7A 仅标识了两个成功的转换，并且它们与图 3B 中具有最大 B/A 比率的两个相一致。显然，正如图 6 中的单词云所表明的，电影评论更难聚类。总的来说，与 20 条新闻相比，我们可以做出以下观察。</p><ul class=""><li id="3c77" class="lp lq iq kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated">在群集任务中，7 次尝试转换中只有 2 次成功(相比之下，7 次尝试转换中有 6 次成功)。其他的则很不平衡，只有一个集群拥有大部分文档。</li><li id="601a" class="lp lq iq kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">即使在这两个成功的转化中，获得的簇的纯度/质量较差，有超过 30%的错簇。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/55ae68dc0bed68ced6907c4bc338f7a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JDoz9xc2oCu_-YVF.jpg"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 7. Of the 7 attempted transformations, only 2 make the grade in reasonably segregating the positive and negative reviews as separate clusters. And they both of them involve custom word-embeddings.</figcaption></figure><h2 id="cea5" class="md me iq bd mf mg mh dn mi mj mk dp ml la mm mn mo le mp mq mr li ms mt mu mv bi translated">5.结论</h2><p id="9e7b" class="pw-post-body-paragraph kr ks iq kt b ku mw jr kw kx mx ju kz la my lc ld le mz lg lh li na lk ll lm ij bi translated">至此，我们结束了另一篇文章。我们已经在这里展示了降阶转换可以帮助文档聚类。在这些转换中，自定义单词嵌入似乎有优势——因此它进入了标题。至于为什么自定义单词嵌入比其他降阶转换做得更好，以及这个结论是否超出了本文研究的文本存储库，还有待进一步研究。</p><p id="1682" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi">…</p><p id="6d2e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="lo">原载于 2018 年 12 月 14 日</em><a class="ae ln" href="http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/" rel="noopener ugc nofollow" target="_blank"><em class="lo">【xplordat.com】</em></a><em class="lo">。</em></p></div></div>    
</body>
</html>