# 什么时候不用深度学习？

> 原文：<https://towardsdatascience.com/when-not-to-use-dl-ml-ai-24103f147dd0?source=collection_archive---------5----------------------->

或者**一种更自由的软件设计方法**

这个趋势问题引发了许多争论。与其站在(DL 和非 DL)一边，我更喜欢软件设计的统一(和自由)观点。

让我们首先区分通过深度学习(或 ML)和传统软件工程编写的程序。

*   DL/ML/AI 都是关于编写程序(就像我们从计算时代开始就一直在编写的那样)来解决问题和执行任务。编写传统软件=用选定的*编程语言*的语法编写一系列**规则**(作为命令性动作或指示性约束)，大部分是手工编写的。
*   编写(有监督的)ML/DL 程序是*示例编程*，即需要以正确的*格式*对程序的*输入输出数据*进行指定和编码，并描述程序的*模板*(如神经网络的形状)。模板中的*未知参数*根据手动指定的*目标*自动*学习*(计算模型输出*错*多少)，从而最终确定程序。我之前的[帖子](https://medium.com/@ekshakhs/synthesizing-programs-with-deep-learning-21c1980a85f5)对此进行了更详细的讨论。
*   传统的离散程序通过转换数据结构来工作。输入数据结构→转换序列→输出数据结构实例。你把大部分精力花在编写转换序列的规则上，单元测试通过对输入/输出数据进行采样来确保正确性。
*   ML/DL 程序转换多维空间中的连续值向量或张量。变换的空间限于指令集，例如神经网络中的不同算子/层。最重要的是，为了能够学习，这些运算符应该是*可微的*。这些转换不是手动指定的——它们是通过使用非常普通的(和蛮力！)随机梯度下降算法。
*   也许，分界线(在 30k 英尺处)是使用离散数学还是连续数学概念、表示和程序合成工具。
*   回想起来，传统的基于规则的编程在许多以人为中心的程序(AI！)我们想创造，例如，建模感知(看，听，语言)。我们(甚至是集体)无法找出正确的规则。这在某种意义上，激发了热情(+炒作+恐惧！)围绕 AI 深度学习后的*在那些问题上成功了*。通过让神经网络找出转换，我们现在能够为许多感知任务(加上其他几个任务)编写*好程序*。
*   旁白:注意*推理*和*抽象*(人工智能的本质)也许做得还行(逻辑编程！)在基于规则的世界中，假设你*正确地*了符号和关系(主要警告！).

鉴于关于基于强大的深度学习的程序合成的引人注目的讨论，我们现在想知道是否所有可能的程序都能被深度学习者学会。例如，我们能用 DL 同样容易地编写典型的基于图形/树的程序吗？让我们在这里深入研究一下。

*   DL 程序接受输入的张量嵌入，对其进行几何变换(线性和非线性),并产生概率分布形式的输出(主要是)。如果有合适的架构和外部存储器，这种设置可能是图灵完全的。也许我们可以大致了解一切。
*   然而，就*编程成本*而言，这条路线可行吗？对于我们想写的每一个程序(或者说，我们想解决的问题)，有没有一个*低成本*架构(容易找到，需要训练的参数)？在某种程度上，我们需要多长时间才能为一个问题找到正确的可微算子集以及组合它们的最佳方式？
*   我们能确保为每个问题建立低成本的输入/输出数据吗？这是*注解*头痛的一部分。此外，正确的编码。
*   最后，要让数学*目标*约束真正反映你的直觉(或离散)目标，你会努力多长时间？

上述问题凸显了为什么基于范例的学习并不总是美好的:也许，这种方法也不是万能的。

我认为，更明智的方法是围绕我们所做的编程拓宽我们的思维，以解决任何问题。我们将编写两种程序——基于规则的和基于例子的。将这些模块看作解决方案代码的不同模块——您将挑选哪些模块为其编写规则，哪些模块提供数据以支持规则学习。

所以，我们正在谈论一个统一的编程风格的范围。你可能以基于规则的方式开始编写一个模块，因为规则集变得庞大而放弃，转而准备数据来指导学习模式。相反，您可能从一个基于深度网络的模块实现开始，对它的性能/数据准备感到厌倦，或者可能对它有足够的了解，然后切换到一个更可解释的基于规则/可学习的框架。完整的解决方案包括将问题智能分解成不同的编程风格*。*

具体示例:我尝试过的这种方法的一个实例是从图像中提取表格结构，例如申请表、收据等。该解决方案通常依次由两个模块组成。低级的东西(线条、字符边缘、文本边界框检测)在学习模式下做得更好。更高级的布局发现可以通过边界框上的一组解析规则来更直接地解决。当然，您可以使用端到端的学习公式，它以适当编码的格式输出表格布局。然而，我认为数据设置和训练比编写第二部分的规则更麻烦。

还有一部分争论(这篇文章没有涉及)是关于只有在大量数据或感知任务中应用 DL，还是在低数据或结构化设置中应用非神经 ML。我想这个问题还没有定论，但是越来越多的[证据表明，这种明显的分歧也是虚假的。](http://beamandrew.github.io/deeplearning/2017/06/04/deep_learning_works.html)

我发现我的想法与 Francois Chollet 的这篇主题更加广泛的文章[有所重叠。我认为他设想了一个更具凝聚力的神经程序合成模型，结合了基于规则和基于示例的学习模块，以及它们如何被重用。我主要谈的是界限明确的组合。](https://blog.keras.io/the-future-of-deep-learning.html)

很想听听其他从业者对这个话题的看法！