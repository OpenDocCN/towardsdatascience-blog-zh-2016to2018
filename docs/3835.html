<html>
<head>
<title>Realtime prediction using Spark Structured Streaming, XGBoost and Scala</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Spark 结构化流、XGBoost 和 Scala 进行实时预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/realtime-prediction-using-spark-structured-streaming-xgboost-and-scala-d4869a9a4c66?source=collection_archive---------4-----------------------#2018-06-24">https://towardsdatascience.com/realtime-prediction-using-spark-structured-streaming-xgboost-and-scala-d4869a9a4c66?source=collection_archive---------4-----------------------#2018-06-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/ec660ac8ff28db5a39b5d41fbfb5c85f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0-iESNRYWto9HuP_hCGaKg.jpeg"/></div></div></figure><p id="69af" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在本文中，我们将讨论如何构建一个完整的机器学习管道。第一部分将集中在以标准批处理模式训练二元分类器，第二部分我们将做一些实时预测。</p><p id="1273" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们将使用来自众多 Kaggle 竞赛之一的<a class="ae kz" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank">泰坦尼克号:从灾难中学习机器</a>的数据。</p><p id="082a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在开始之前，请了解您应该熟悉<a class="ae kz" href="https://www.tutorialspoint.com/scala/index.htm" rel="noopener ugc nofollow" target="_blank"> Scala </a>、<a class="ae kz" href="https://spark.apache.org" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>和<a class="ae kz" href="http://xgboost.readthedocs.io/en/latest/get_started/" rel="noopener ugc nofollow" target="_blank"> Xgboost </a>。</p><p id="fbc1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所有的源代码也可以在 Github 上找到。酷，现在让我们开始吧！</p><h2 id="d8e5" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">培养</h2><p id="c659" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">我们将使用 Spark 中的 ML 管道训练一个 XGBoost 分类器。分类器将被保存为输出，并将在 Spark 结构化流实时应用程序中使用，以预测新的测试数据。</p><p id="a657" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">步骤 1:开始 spark 会话</strong></p><p id="e712" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们正在创建一个 spark 应用程序，它将在本地运行，并将使用与使用<code class="fe ly lz ma mb b">local[*]</code>的内核一样多的线程:</p><pre class="mc md me mf gt mg mb mh mi aw mj bi"><span id="8b9d" class="la lb it mb b gy mk ml l mm mn"><strong class="mb iu">val </strong>spark  = SparkSession.<em class="mo">builder</em>()<br/>  .appName("Spark XGBOOST Titanic Training")<br/>  .master("local[*]")<br/>  .getOrCreate()</span></pre><p id="242e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">步骤 2:定义模式</strong></p><p id="be32" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">接下来，我们定义从 csv 读取的数据的模式。这通常是一个比让 spark 推断模式更好的实践，因为它消耗更少的资源，并且我们完全控制字段。</p><pre class="mc md me mf gt mg mb mh mi aw mj bi"><span id="b6fe" class="la lb it mb b gy mk ml l mm mn"><strong class="mb iu">val </strong>schema = StructType(<br/>  <em class="mo">Array</em>(<em class="mo">StructField</em>("PassengerId", DoubleType),<br/>    <em class="mo">StructField</em>("Survival", DoubleType),<br/>    <em class="mo">StructField</em>("Pclass", DoubleType),<br/>    <em class="mo">StructField</em>("Name", StringType),<br/>    <em class="mo">StructField</em>("Sex", StringType),<br/>    <em class="mo">StructField</em>("Age", DoubleType),<br/>    <em class="mo">StructField</em>("SibSp", DoubleType),<br/>    <em class="mo">StructField</em>("Parch", DoubleType),<br/>    <em class="mo">StructField</em>("Ticket", StringType),<br/>    <em class="mo">StructField</em>("Fare", DoubleType),<br/>    <em class="mo">StructField</em>("Cabin", StringType),<br/>    <em class="mo">StructField</em>("Embarked", StringType)<br/>  ))</span></pre><p id="cd6a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">步骤 3:读取数据</strong></p><p id="2b38" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们将 csv 读入一个<code class="fe ly lz ma mb b">DataFrame</code>，确保我们提到我们有一个头。</p><pre class="mc md me mf gt mg mb mh mi aw mj bi"><span id="fd53" class="la lb it mb b gy mk ml l mm mn"><strong class="mb iu">val </strong>df_raw = spark<br/>  .read<br/>  .option("header", "true")<br/>  .schema(schema)<br/>  .csv(filePath)</span></pre><p id="c332" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">步骤 4:删除空值</strong></p><p id="9bf1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所有空值都被替换为 0。这并不理想，但对于本教程的目的来说，这是可以的。</p><pre class="mc md me mf gt mg mb mh mi aw mj bi"><span id="5a6f" class="la lb it mb b gy mk ml l mm mn"><strong class="mb iu">val </strong>df = df_raw.na.fill(0)</span></pre><p id="0128" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">第五步:将标称值转换为数值</strong></p><p id="bc78" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在浏览这一步的代码之前，让我们简单地浏览一下 Spark ML 的一些概念。他们引入了 ML 管道的概念，这是一组建立在<code class="fe ly lz ma mb b">DataFrames</code>之上的高级 API，可以更容易地将多个算法合并到一个进程中。管道的主要元素是<code class="fe ly lz ma mb b">Transformer</code>和<code class="fe ly lz ma mb b">Estimator</code>。第一个可以表示一种算法，可以将一个<code class="fe ly lz ma mb b">DataFrame</code>转换成另一个<code class="fe ly lz ma mb b">DataFrame</code>，而后者是一种算法，可以适合一个<code class="fe ly lz ma mb b">DataFrame</code>来产生一个<code class="fe ly lz ma mb b">Transformer</code>。</p><p id="8484" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了将名义值转换成数值，我们需要为每一列定义一个<code class="fe ly lz ma mb b">Transformer</code>:</p><pre class="mc md me mf gt mg mb mh mi aw mj bi"><span id="7a94" class="la lb it mb b gy mk ml l mm mn"><strong class="mb iu">val </strong>sexIndexer = <strong class="mb iu">new </strong>StringIndexer()<br/>  .setInputCol("Sex")<br/>  .setOutputCol("SexIndex")<br/>  .setHandleInvalid("keep")<br/><br/><strong class="mb iu">val </strong>cabinIndexer = <strong class="mb iu">new </strong>StringIndexer()<br/>  .setInputCol("Cabin")<br/>  .setOutputCol("CabinIndex")<br/>  .setHandleInvalid("keep")<br/><br/><strong class="mb iu">val </strong>embarkedIndexer = <strong class="mb iu">new </strong>StringIndexer()<br/>  .setInputCol("Embarked")<br/>  .setOutputCol("EmbarkedIndex")<br/>  .setHandleInvalid("keep")</span></pre><p id="fe32" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们使用<code class="fe ly lz ma mb b">StringIndexer</code>来转换值。对于每个<code class="fe ly lz ma mb b">Transformer</code>,我们将定义包含修改值的输入列和输出列。</p><p id="d48e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">步骤 6:将列组合成特征向量</strong></p><p id="045f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们将使用另一个<code class="fe ly lz ma mb b">Transformer</code>将 XGBoost <code class="fe ly lz ma mb b">Estimator</code>分类中使用的列组装成一个向量:</p><pre class="mc md me mf gt mg mb mh mi aw mj bi"><span id="9504" class="la lb it mb b gy mk ml l mm mn"><strong class="mb iu">val </strong>vectorAssembler = <strong class="mb iu">new </strong>VectorAssembler()<br/>  .setInputCols(<em class="mo">Array</em>("Pclass", "SexIndex", "Age", "SibSp", "Parch", "Fare", "CabinIndex", "EmbarkedIndex"))<br/>  .setOutputCol("features")</span></pre><p id="dc0b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">步骤 7:添加 XGBoost 估计器</strong></p><p id="95a7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">定义将生成模型的<code class="fe ly lz ma mb b">Estimator</code>。估计器的设置可以在图中定义。我们还可以设置特征和标签列:</p><pre class="mc md me mf gt mg mb mh mi aw mj bi"><span id="7175" class="la lb it mb b gy mk ml l mm mn"><strong class="mb iu">val </strong>xgbEstimator = <strong class="mb iu">new </strong>XGBoostEstimator(<em class="mo">Map</em>[String, Any]("num_rounds" -&gt; 100))<br/>  .setFeaturesCol("features")<br/>  .setLabelCol("Survival")</span></pre><p id="e827" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">步骤 8:构建管道和分类器</strong></p><p id="243d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在我们创建了所有单独的步骤之后，我们可以定义实际的管道和操作顺序:</p><pre class="mc md me mf gt mg mb mh mi aw mj bi"><span id="6a57" class="la lb it mb b gy mk ml l mm mn"><strong class="mb iu">val </strong>pipeline = <strong class="mb iu">new </strong>Pipeline().setStages(<em class="mo">Array</em>(sexIndexer, cabinIndexer, embarkedIndexer, vectorAssembler, xgbEstimator))</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mp"><img src="../Images/21a2c235005bdf743d40b92841b8689b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5GXouKuoLYmdTCgu1onrzw.png"/></div></div></figure><p id="9935" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">输入<code class="fe ly lz ma mb b">DataFrame</code>将被转换多次，最终将产生用我们的数据训练的模型。我们将保存输出，以便在第二个实时应用程序中使用。</p><pre class="mc md me mf gt mg mb mh mi aw mj bi"><span id="8fc9" class="la lb it mb b gy mk ml l mm mn"><strong class="mb iu">val </strong>cvModel = pipeline.fit(df)<br/>cvModel.write.overwrite.save(modelPath)</span></pre><h2 id="283f" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">预言；预测；预告</h2><p id="f0de" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">我们将使用 Spark 结构化流从一个文件中传输数据。在现实世界的应用程序中，我们从 Apache Kafka 或 AWS Kinesis 等专用分布式队列中读取数据，但对于这个演示，我们将只使用一个简单的文件。</p><p id="798f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">简要描述<a class="ae kz" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources" rel="noopener ugc nofollow" target="_blank"> Spark 结构化流</a>是一个基于 Spark SQL 构建的流处理引擎。它使用了与<code class="fe ly lz ma mb b">DataFrames</code>相同的概念，数据存储在一个无界的表中，该表随着数据的流入而增加新的行。</p><p id="5069" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">步骤 1:创建输入读取流</strong></p><p id="6d87" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们再次创建一个 spark 会话，并为数据定义一个模式。请注意，测试 csv 不包含标签<code class="fe ly lz ma mb b">Survival</code>。最后我们可以创建输入流<code class="fe ly lz ma mb b">DataFrame,</code> <code class="fe ly lz ma mb b">df</code>。输入路径必须是我们存储 csv 文件的目录。它可以包含一个或多个具有相同模式的文件。</p><pre class="mc md me mf gt mg mb mh mi aw mj bi"><span id="beb4" class="la lb it mb b gy mk ml l mm mn"><strong class="mb iu">val </strong>spark: SparkSession = SparkSession.<em class="mo">builder</em>()<br/>  .appName("Spark Structured Streaming XGBOOST")<br/>  .master("local[*]")<br/>  .getOrCreate()<br/><br/><strong class="mb iu">val </strong>schema = StructType(<br/>  <em class="mo">Array</em>(<em class="mo">StructField</em>("PassengerId", DoubleType),<br/>    <em class="mo">StructField</em>("Pclass", DoubleType),<br/>    <em class="mo">StructField</em>("Name", StringType),<br/>    <em class="mo">StructField</em>("Sex", StringType),<br/>    <em class="mo">StructField</em>("Age", DoubleType),<br/>    <em class="mo">StructField</em>("SibSp", DoubleType),<br/>    <em class="mo">StructField</em>("Parch", DoubleType),<br/>    <em class="mo">StructField</em>("Ticket", StringType),<br/>    <em class="mo">StructField</em>("Fare", DoubleType),<br/>    <em class="mo">StructField</em>("Cabin", StringType),<br/>    <em class="mo">StructField</em>("Embarked", StringType)<br/>  ))<br/><br/>  <strong class="mb iu">val </strong>df = spark<br/>    .readStream<br/>    .option("header", "true")<br/>    .schema(schema)<br/>    .csv(fileDir)</span></pre><p id="a1fe" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">第二步:加载 XGBoost 模型</strong></p><p id="8627" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在对象<code class="fe ly lz ma mb b">XGBoostModel</code>中，我们加载预训练模型，该模型将应用于我们在流中读取的每一批新行。</p><pre class="mc md me mf gt mg mb mh mi aw mj bi"><span id="5460" class="la lb it mb b gy mk ml l mm mn"><strong class="mb iu">object </strong>XGBoostModel {<br/><br/>  <strong class="mb iu">private val </strong><em class="mo">modelPath </em>= "your_path"<br/><br/>  <strong class="mb iu">private val </strong><em class="mo">model </em>= PipelineModel.<em class="mo">read</em>.load(<em class="mo">modelPath</em>)<br/><br/>  <strong class="mb iu">def </strong>transform(df: DataFrame) = {<br/>    // replace nan values with 0<br/>    <strong class="mb iu">val </strong>df_clean = df.na.fill(0)<br/><br/>    // run the model on new data<br/>    <strong class="mb iu">val </strong>result = <em class="mo">model</em>.transform(df_clean)<br/><br/>    // display the results<br/>    result.show()<br/>  }<br/><br/>}</span></pre><p id="387c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">步骤 3:定义自定义 ML 接收器</strong></p><p id="4a0f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了能够将我们的分类器应用于新数据，我们需要创建一个新的接收器(流和输出之间的接口，在我们的例子中是 XGBoost 模型)。为此，我们需要一个自定义接收器(<code class="fe ly lz ma mb b">MLSink</code>)、一个抽象接收器提供者(<code class="fe ly lz ma mb b">MLSinkProvider</code>)和一个提供者实现(<code class="fe ly lz ma mb b">XGBoostMLSinkProvider</code>)。</p><pre class="mc md me mf gt mg mb mh mi aw mj bi"><span id="8af0" class="la lb it mb b gy mk ml l mm mn"><strong class="mb iu">abstract class </strong>MLSinkProvider <strong class="mb iu">extends </strong>StreamSinkProvider {<br/>  <strong class="mb iu">def </strong>process(df: DataFrame): Unit<br/><br/>  <strong class="mb iu">def </strong>createSink(<br/>                  sqlContext: SQLContext,<br/>                  parameters: Map[String, String],<br/>                  partitionColumns: Seq[String],<br/>                  outputMode: OutputMode): MLSink = {<br/>    <strong class="mb iu">new </strong>MLSink(process)<br/>  }<br/>}<br/><br/><strong class="mb iu">case class </strong>MLSink(process: DataFrame =&gt; Unit) <strong class="mb iu">extends </strong>Sink {<br/>  <strong class="mb iu">override def </strong>addBatch(batchId: Long, data: DataFrame): Unit = {<br/>    process(data)<br/>  }<br/>}</span><span id="bd1b" class="la lb it mb b gy mq ml l mm mn"><strong class="mb iu">class </strong>XGBoostMLSinkProvider <strong class="mb iu">extends </strong>MLSinkProvider {<br/>  <strong class="mb iu">override def </strong>process(df: DataFrame) {<br/>    XGBoostModel.<em class="mo">transform</em>(df)<br/>  }<br/>}</span></pre><p id="9850" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">步骤 4:将数据写入我们的自定义接收器</strong></p><p id="d275" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最后一步是定义一个将数据写入自定义接收器的查询。还必须定义一个检查点位置，以便应用程序在失败时“记住”流中读取的最新行。如果我们运行该程序，每一批新的数据将显示在控制台上，其中也包含预测的标签。</p><pre class="mc md me mf gt mg mb mh mi aw mj bi"><span id="fdbe" class="la lb it mb b gy mk ml l mm mn">df.writeStream<br/>  .format("titanic.XGBoostMLSinkProvider")<br/>  .queryName("XGBoostQuery")<br/>  .option("checkpointLocation", checkpoint_location)<br/>  .start()</span></pre></div></div>    
</body>
</html>