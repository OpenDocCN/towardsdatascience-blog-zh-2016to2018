<html>
<head>
<title>Facebook Research just published an awesome paper on learning hierarchical representations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">脸书研究中心刚刚发表了一篇关于学习层级表征的精彩论文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/facebook-research-just-published-an-awesome-paper-on-learning-hierarchical-representations-34e3d829ede7?source=collection_archive---------3-----------------------#2017-06-14">https://towardsdatascience.com/facebook-research-just-published-an-awesome-paper-on-learning-hierarchical-representations-34e3d829ede7?source=collection_archive---------3-----------------------#2017-06-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="fa3f" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">学习分层表示的庞加莱嵌入</h1><p id="9185" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">Arxiv 链接:<a class="ae lj" href="https://arxiv.org/abs/1705.08039" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1705.08039</a></p><p id="60f7" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">在初步阅读后，我的一位教授说:</p><blockquote class="lp lq lr"><p id="5f6c" class="kl km ls kn b ko lk kq kr ks ll ku kv lt lm ky kz lu ln lc ld lv lo lg lh li ij bi translated">这个想法很有趣，结果好得令人难以置信。</p></blockquote><p id="a215" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">这是我给很多人看结果时他们的反应。</p><p id="d3fd" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">首先，我将向你展示我在 WordNet 的一部分上得到的结果，这样上面的不可行性图片就淡化了一点:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/081a87b1952215418a574068dd5ba73e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*jR2lvsmy9qJnQz5M_RHeeA.png"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">WordNet embeddings for Mammal (only three levels)</figcaption></figure><p id="53c4" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">我只用过三级:</p><ul class=""><li id="5cdf" class="mi mj iq kn b ko lk ks ll kw mk la ml le mm li mn mo mp mq bi translated">黑色一号在 0 级</li><li id="181c" class="mi mj iq kn b ko mr ks ms kw mt la mu le mv li mn mo mp mq bi translated">红色的在第一层</li><li id="ec95" class="mi mj iq kn b ko mr ks ms kw mt la mu le mv li mn mo mp mq bi translated">绿色的在第二层。</li></ul><p id="4393" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">我觉得结果不错。</p><p id="e57d" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">在我以前的一篇文章中，<a class="ae lj" href="https://medium.com/towards-data-science/cross-lingual-word-embeddings-what-they-are-af7987df6670" rel="noopener">跨语言单词嵌入是什么？</a>，我解释了单词嵌入。它们可以用于不同的任务，如信息检索、情感分析和无数其他任务。</p><p id="e4e3" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">类似地，我们可以嵌入图，并有像 node2vec 这样的方法，潜在空间嵌入可以帮助我们表示图，并随后进行社区检测和链接预测。<br/>让我们从最初的 node2vec 论文开始研究这个问题→</p><blockquote class="lp lq lr"><p id="c2a6" class="kl km ls kn b ko lk kq kr ks ll ku kv lt lm ky kz lu ln lc ld lv lo lg lh li ij bi translated">在 node2vec 中，我们学习了节点到低维特征空间的映射，该映射最大化了保存节点的网络邻域的可能性。</p></blockquote><p id="c0b7" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">像 word2vec 嵌入一样，我们可以使用 node2vec 嵌入来预测新的链接。假设我们有一个社交网络的数据，现在当我们嵌入节点时，通过一些距离度量，我们看到两个节点(用户)有一个小的距离，然后我们可以建议用户成为社交网络上的朋友。可能发生这种情况的一种情况是，当用户 2 是社交网络的新用户，并且获得与用户 1 相似的朋友时，给定重叠的朋友(图中的边)，用户 2 和用户 1 彼此认识的机会增加。</p><p id="a452" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">node2vec 嵌入有两个主要方面:</p><blockquote class="lp lq lr"><p id="b88c" class="kl km ls kn b ko lk kq kr ks ll ku kv lt lm ky kz lu ln lc ld lv lo lg lh li ij bi translated">能够学习将来自相同网络社区的节点紧密嵌入在一起的表示，以及学习共享相似角色的节点具有相似嵌入的表示</p></blockquote><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mw"><img src="../Images/4f257c64dd9773ab82081b1137677dd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/0*WwAO3XESFlRuJb9S.png"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Source: Google Images</figcaption></figure><p id="73dc" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">现在考虑表示分层数据的情况，分层数据的结构像树一样</p><p id="e020" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">基于直觉，我们可以把它比作双曲线。<br/>根居中，叶向外展开。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/ae669717176ed27cb99dc5bb8dfacd55.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/0*9kZgZpLe2inbB0EB.jpg"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Source: Google Images</figcaption></figure><p id="da95" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">这一事实被报纸所利用:</p><blockquote class="lp lq lr"><p id="60b9" class="kl km ls kn b ko lk kq kr ks ll ku kv lt lm ky kz lu ln lc ld lv lo lg lh li ij bi translated">为了利用这种结构特性来学习更有效的表示，我们建议不在欧几里得空间中而是在双曲空间中计算嵌入，即具有恒定负曲率的空间。非正式地，双曲空间可以被认为是树的连续版本，因此它自然地被装备来建模分层结构。</p></blockquote><p id="5cad" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">他们使用<a class="ae lj" href="https://en.wikipedia.org/wiki/Poincar%C3%A9_disk_model" rel="noopener ugc nofollow" target="_blank">庞加莱球模型</a>作为双曲空间的模型，因为它非常适合基于梯度的优化，因此也适合反向传播。</p><p id="48f8" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">当然，当你谈论一个不同的空间时，距离的度量是不同的。在庞加莱球模型的情况下，两点<code class="fe nc nd ne nf b">u</code>和<code class="fe nc nd ne nf b">v</code>之间的双曲线距离由下式给出:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/6d659b125b9d85b40ff436e59c835ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*6roMMWBQgs-DiXR4A_PQGA.png"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Source: Original paper</figcaption></figure><p id="4cb4" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">其中<code class="fe nc nd ne nf b">||x||</code>是欧几里德范数。如果你看到维基百科给出的公式，你可能会注意到<code class="fe nc nd ne nf b">1</code>已经取代了<code class="fe nc nd ne nf b">|r|</code>，因为作者只取了一个半径为<code class="fe nc nd ne nf b">1</code>的球。</p><p id="5b43" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">现在，为了训练模型，我们基于这个距离创建一个优化函数，类似于 Mikolov 等人使用的负采样。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/de564bc02d6d0a94aa12e67dfc344fb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*JZ5-Lcqh14jabc2OeCu4xw.png"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Source: Original paper</figcaption></figure><p id="f22c" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">然后反向传播我们的方法来更新嵌入。</p><p id="9dc0" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated"><strong class="kn ir">∇e =(∂l(θ)/∂d(θ,x))*(∂d(θ,x)/∂d(θ)</strong></p><p id="ec16" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">第一部分已经知道了，他们已经给出了第二部分的公式:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/bd885cbbdf8a829d2381b06e471a5452.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*vP7pmpEw-PKP_7ihtvOMUA.png"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Source: Original Paper</figcaption></figure><p id="60ad" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">他们使用了一个小技巧，这样在更新后嵌入的内容不会比模型大。为此他们部署了以下投影:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/5267de248aed42620cd933c41e409161.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*qvGXVj0U4NPsX9TbLiXJTQ.png"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Source: Original Paper</figcaption></figure><p id="6e01" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">然后，它们通过以下规则更新嵌入:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3ba529b3bca473da67ad4eb343564d86.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*WBZK0KugVA1XN8a1L1LgGg.png"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Source: Original Paper</figcaption></figure><h1 id="1955" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">结果</h1><p id="17c1" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这是最有趣的部分</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nl"><img src="../Images/6e2f27fa255b3aab1e3485f81615d607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ynzfjexDICHQTH24Gg804w.png"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Source: Original Paper</figcaption></figure><p id="1f4f" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">我们可以看到，该模型在 5 维空间中实现的效果优于欧几里德模型在 200 维空间中实现的效果。</p><p id="fdf4" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">我希望你对结果感到敬畏。</p><p id="cc77" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated"><strong class="kn ir">TL；博士</strong></p><p id="28c4" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">他们使用双曲空间来嵌入分层数据的节点，并取得了一些超级棒的结果。</p><p id="7453" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">你可以在 twitter 上关注我，地址是<a class="ae lj" href="https://twitter.com/nishantiam" rel="noopener ugc nofollow" target="_blank"> @nishantiam </a>，我在 github 上的地址是<a class="ae lj" href="https://github.com/nishnik" rel="noopener ugc nofollow" target="_blank"> @nishnik </a>。</p></div></div>    
</body>
</html>