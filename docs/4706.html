<html>
<head>
<title>How machines understand our language: an introduction to Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器如何理解我们的语言:自然语言处理导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-machines-understand-our-language-an-introduction-to-natural-language-processing-4ab4bcd47d05?source=collection_archive---------5-----------------------#2018-09-01">https://towardsdatascience.com/how-machines-understand-our-language-an-introduction-to-natural-language-processing-4ab4bcd47d05?source=collection_archive---------5-----------------------#2018-09-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/8db87b3a950a8d01d879359897789500.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VDoOkt5Oj8iyXzz4tfs7aA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/photos/U7RdV0fKIsc?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Tatyana Dobreva</a> on <a class="ae kc" href="https://unsplash.com/search/photos/rome?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="9775" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对我来说，自然语言处理是数据科学中最迷人的领域之一。机器能够以一定的准确度理解文本内容的事实令人着迷，有时甚至令人害怕。</p><p id="ee01" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自然语言处理的应用是无穷无尽的。这是机器如何分类一封电子邮件是否是垃圾邮件，如果评论是正面或负面的，以及搜索引擎如何根据您的查询内容识别您是什么类型的人，以相应地定制响应。</p><p id="ca54" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是这在实践中是如何运作的呢？这篇文章介绍了自然语言处理的基础概念，并重点介绍了在<em class="lf"> Python </em>中使用的<code class="fe lb lc ld le b">nltk</code>包。</p><blockquote class="lg lh li"><p id="3a50" class="kd ke lf kf b kg kh ki kj kk kl km kn lj kp kq kr lk kt ku kv ll kx ky kz la ij bi translated">注意:要运行下面的例子，您需要安装<code class="fe lb lc ld le b">nltk</code>库。如果没有的话，开机前在你的 shell 中运行<code class="fe lb lc ld le b">pip install nltk</code>，在你的笔记本中运行<em class="iq"> </em> <code class="fe lb lc ld le b">nltk.download()</code>即可。</p></blockquote></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><p id="f830" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">无论输入机器的是什么文本或句子，都需要首先进行简化，这可以通过<em class="lf">标记化</em>和<em class="lf">词条化</em>来完成。这些复杂的单词意味着一些非常简单的事情:标记化意味着我们将文本分解成<em class="lf">标记</em>，根据具体情况分解成单个或成组的单词。词汇化意味着我们将一些单词转换成它们的词根，即复数变成单数，共轭动词变成基本动词等等。在这些操作之间，我们还从文本中清除所有不携带实际信息的单词，即所谓的<em class="lf">停用词</em>。</p><p id="6741" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看下面的句子，用一个例子来理解这一切意味着什么。</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lt"><img src="../Images/43b97ac30fab14faaaaf8042e373395e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jfZ4uK1Tko0TFugEk9oXDw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Example of tokenization and lemmatization for ngrams = 1.</figcaption></figure><p id="27f4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对文本进行分词时，相应地选择<em class="lf"> ngram </em>很重要。它是指定我们希望每个令牌包含多少单词的数字，在大多数情况下(就像上面的例子)，这个数字等于 1。但是，如果你在一个商业评论网站上进行情绪分析，你的文本可能会包含“不高兴”或“不喜欢”这样的语句，你不希望这些词相互抵消，以传达评论背后的负面情绪。在这种情况下，您可能需要考虑增加 ngram，看看它对您的分析有何影响。</p><p id="085c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在进行标记化时，还需要考虑其他因素，例如标点符号。大多数时候你想去掉任何标点符号，因为它不包含任何信息，除非文本中有有意义的数字。在这种情况下，您可能需要考虑保留标点符号，否则文本中包含的数字将在出现<code class="fe lb lc ld le b">.</code>或<code class="fe lb lc ld le b">,</code>的地方被拆分。</p><p id="b0a2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下面的代码中，我使用了<code class="fe lb lc ld le b">RegexpTokenizer</code>，一个正则表达式标记器。对于那些不熟悉正则表达式的人来说，在形式语言理论中，它是一个定义模式的字符序列，根据您在<code class="fe lb lc ld le b">RegexpTokenizer</code>函数中传递的参数，它将根据该参数分割文本。在一个正则表达式中，<code class="fe lb lc ld le b">\w+</code>字面意思是将所有长度大于或等于 1 的单词字符分组，丢弃空格(从而标记单个单词)和所有非单词字符，即标点符号。</p><figure class="lu lv lw lx gt jr"><div class="bz fp l di"><div class="ly lz l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Tokenizing with RegexpTokenizer.</figcaption></figure><p id="ab7a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这段代码生成的令牌列表如下:</p><pre class="lu lv lw lx gt ma le mb mc aw md bi"><span id="e4e5" class="me mf iq le b gy mg mh l mi mj">tokens = ['Rome', 'was', 'founded', 'in', '753BC', 'by', 'its', 'first', 'king', 'Romulus']</span></pre><p id="750c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个不错的开始，我们有了由单个单词组成的令牌，标点符号不见了！现在我们必须从令牌中删除停用词:幸运的是，对于许多不同的语言，<code class="fe lb lc ld le b">nltk</code>中包含了停用词列表。但是当然，根据具体情况，您可能需要定制这个单词列表。例如，文章<em class="lf"/>默认包含在此列表中，但是如果您正在分析一个电影或音乐数据库，您可能希望保留它，因为在这种情况下，它确实有所不同(有趣的事实:<em class="lf">帮助</em>和<em class="lf">帮助！</em>是两部不同的电影！).</p><figure class="lu lv lw lx gt jr"><div class="bz fp l di"><div class="ly lz l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Discarding the stop words from a text.</figcaption></figure><p id="1d79" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">新令牌列表是:</p><pre class="lu lv lw lx gt ma le mb mc aw md bi"><span id="dd0d" class="me mf iq le b gy mg mh l mi mj">clean_tokens = ['Rome', 'founded', '753BC', 'first', 'king', 'Romulus']</span></pre><p id="3e66" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们从 10 个单词增加到 6 个单词，现在终于到了词汇化的时候了！到目前为止，我已经测试了两个具有相同目的的物体:<code class="fe lb lc ld le b">WordNetLemmatizer</code>和<code class="fe lb lc ld le b">PorterStemmer</code>，后者肯定比前者更残忍，如下例所示。</p><figure class="lu lv lw lx gt jr"><div class="bz fp l di"><div class="ly lz l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Lemmatization example with WordNetLemmatizer.</figcaption></figure><p id="2d92" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后一个列表理解的输出是:</p><pre class="lu lv lw lx gt ma le mb mc aw md bi"><span id="2216" class="me mf iq le b gy mg mh l mi mj">['Rome', 'founded', '753BC', 'first', 'king', 'Romulus']</span></pre><p id="5e1b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">什么都没变！这是因为<code class="fe lb lc ld le b">WordNetLemmatizer</code>只作用于复数单词和一些其他的东西，在这个特殊的例子中，没有单词真正被词条化。另一方面,<code class="fe lb lc ld le b">PorterStemmer</code>转换复数和衍生词、动词，并使所有术语小写，如下所示:</p><figure class="lu lv lw lx gt jr"><div class="bz fp l di"><div class="ly lz l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Lemmatization example with PorterStemmer.</figcaption></figure><p id="d0b3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">列表理解的输出是:</p><pre class="lu lv lw lx gt ma le mb mc aw md bi"><span id="cec3" class="me mf iq le b gy mg mh l mi mj">['rome', 'found', '753bc', 'first', 'king', 'romulu']</span></pre><p id="7c02" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，没有大写字母的单词了，这对我们来说是没问题的，因为仅仅因为一个是小写字母而另一个不是，区分相同的单词是没有意义的，它们有相同的意思！动词<em class="lf"> founded </em>已经改成了<em class="lf"> found </em>甚至<em class="lf"> Romulus </em>都把自己名字的最后一个字母弄丢了，可能是因为<em class="lf"> </em> <code class="fe lb lc ld le b">PorterStemmer</code>以为是复数词。</p><p id="b47e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些引理化函数非常不同，根据具体情况，一个会比另一个更合适。</p><p id="b0b4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在建模之前，有许多不同的方法来收集和组织文本中的单词，这些只是可用选项的一小部分。在将文本输入机器学习模型以尽可能简化它之前，所有这些清理都是必要的。当你在预测模型中分析大量词汇时，在完成上述步骤后，你将很可能依靠<code class="fe lb lc ld le b"><a class="ae kc" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">CountVectorizer</a></code>、<code class="fe lb lc ld le b"><a class="ae kc" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" rel="noopener ugc nofollow" target="_blank">TfidfVectorizer</a></code>或<code class="fe lb lc ld le b"><a class="ae kc" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" rel="noopener ugc nofollow" target="_blank">HashingVectorizer</a></code>等<code class="fe lb lc ld le b">sklearn</code>方法，将原始文本转换成一个令牌计数的矩阵来训练你的预测模型。</p></div></div>    
</body>
</html>