<html>
<head>
<title>Finding Similar Quora Questions with BOW, TFIDF and Xgboost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 BOW，TFIDF 和 Xgboost 寻找相似的 Quora 问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/finding-similar-quora-questions-with-bow-tfidf-and-random-forest-c54ad88d1370?source=collection_archive---------4-----------------------#2018-10-19">https://towardsdatascience.com/finding-similar-quora-questions-with-bow-tfidf-and-random-forest-c54ad88d1370?source=collection_archive---------4-----------------------#2018-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/aea6aefa3822492d2085990e9a2a2f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OsCJR432J9GZ5eya7jnYLw.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">photo credit: Pixabay</figcaption></figure><div class=""/><div class=""><h2 id="be0a" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">Google 长期以来在索引和信息检索中使用 TFIDF 来计算给定关键字对给定页面的重要性</h2></div><p id="270b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Quora 是一个问答网站，它的用户社区以意见的形式提问、回答、编辑和组织问题。</p><p id="0509" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">2018 年 9 月，Quora 报告月度用户达到 3 亿。每个月有超过 3 亿人访问 Quora，所以很多人问重复的问题也就不足为奇了，也就是说，这些问题有着相同的意图。比如像“我怎样才能成为一个好的地质学家？”以及“我该怎么做才能成为一名伟大的地质学家？”是重复的问题，因为它们都有相同的意图，应该只回答一次。</p><p id="be6b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Quora 非常努力地消除重复问题，但是<a class="ae lq" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"> NLP </a>是一个非常困难的问题。有这么多方法来描述同一个意思，看看上面的例子就知道了。Quora 用户提供了很大的帮助，合并了类似的问题，比如:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lr"><img src="../Images/d3b257cfcf25fe781db67d6ce708bda4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X5OdFCnkoNFxwVr0B7zE_w.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">source: Quora</figcaption></figure><p id="26c9" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在这篇文章中，我们将开发一个机器学习和 NLP 系统来分类问题对是否重复，我们从一个带有 Xgboost 的模型<a class="ae lq" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank"> BOW </a>或<a class="ae lq" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>开始。</p><p id="49d0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">BOW 和 TF-IDF 是人们在信息检索中最常用的两种方法。一般来说，支持向量机和朴素贝叶斯更常用于分类问题，但是，因为它们的精度依赖于训练数据，所以 Xgboost 在这个特定的数据集中提供了最好的精度。XGBoost 是一个梯度增强框架，已经变得非常流行，尤其是在<a class="ae lq" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>社区中。因此，我决定使用这个模型作为基线模型，因为它设置简单，易于理解，并且有合理的机会提供体面的结果。我们的基线模型将允许我们快速获得性能基准。<a class="ae lq" href="https://blog.insightdatascience.com/always-start-with-a-stupid-model-no-exceptions-3a22314b9aaa" rel="noopener ugc nofollow" target="_blank">如果我们发现它提供的性能不够充分，那么检查简单模型的问题可以帮助我们选择下一个方法</a>。</p><h1 id="384d" class="lw lx jf bd ly lz ma mb mc md me mf mg kl mh km mi ko mj kp mk kr ml ks mm mn bi translated">数据预处理</h1><p id="e9f2" class="pw-post-body-paragraph ku kv jf kw b kx mo kg kz la mp kj lc ld mq lf lg lh mr lj lk ll ms ln lo lp ij bi translated"><a class="ae lq" href="https://www.kaggle.com/c/quora-question-pairs/data" rel="noopener ugc nofollow" target="_blank">Quora 重复问题公共数据集</a>包含超过 40 万对 Quora 问题。在我们的实验中，我们将数据随机分为 70%的训练样本和 30%的测试样本。</p><pre class="ls lt lu lv gt mt mu mv mw aw mx bi"><span id="b290" class="my lx jf mu b gy mz na l nb nc">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="7f21" class="my lx jf mu b gy nd na l nb nc">df = pd.read_csv('quora_train.csv')<br/>df.dropna(axis=0, inplace=True)<br/>df.groupby("is_duplicate")['id'].count().plot.bar()</span></pre><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/e22d475321c7c819cc7f434ee7fe636b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*rMlrXwJyi8hTQ429OkXl1A.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 1</figcaption></figure><p id="4b3a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这些职业并没有完全平衡，但这并不坏，我们不会去平衡它们。</p><p id="b092" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在清理文本之前，我们预览几个问题对，以确定如何清理它们。</p><pre class="ls lt lu lv gt mt mu mv mw aw mx bi"><span id="edf2" class="my lx jf mu b gy mz na l nb nc">df.drop(['id', 'qid1', 'qid2'], axis=1, inplace=True)<br/>a = 0 <br/>for i in range(a,a+10):<br/>    print(df.question1[i])<br/>    print(df.question2[i])<br/>    print()</span></pre><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/f45a0710a805b73564e63a3d2515bdb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aPaqjxYMnM2OUGUjBVjEGA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 2</figcaption></figure><p id="596a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">你可能已经注意到我们在文本清理方面有很多工作要做。经过一些检查，一些尝试和来自<a class="ae lq" href="https://www.kaggle.com/currie32/the-importance-of-cleaning-text" rel="noopener ugc nofollow" target="_blank">的想法 https://www . ka ggle . com/Currie 32/the-importance-of-cleaning-text</a>，我决定对文本进行如下清理:</p><ul class=""><li id="8e0f" class="ng nh jf kw b kx ky la lb ld ni lh nj ll nk lp nl nm nn no bi translated">不要删除停用词，因为像“什么”、“哪个”和“如何”这样的词可能有强烈的信号。</li><li id="cee8" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">不要词干。</li><li id="2dcd" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">去掉标点符号。</li><li id="eb94" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">纠正错别字。</li><li id="7d90" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">将缩写改为其原始术语。</li><li id="a037" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">删除数字之间的逗号。</li><li id="9103" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">将特殊字符转换为单词。诸如此类。</li></ul><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">clean</figcaption></figure><p id="f66d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">清理文本后，我们再次预览这些问题对。</p><pre class="ls lt lu lv gt mt mu mv mw aw mx bi"><span id="c31f" class="my lx jf mu b gy mz na l nb nc">a = 0 <br/>for i in range(a,a+10):<br/>    print(df.question1[i])<br/>    print(df.question2[i])<br/>    print()</span></pre><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/eb262c91192f98b06fcf045960acfbcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HDUqPcz-2GXTZIC3HY9M8Q.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 3</figcaption></figure><p id="eb58" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">好多了！</p><h2 id="db7e" class="my lx jf bd ly nx ny dn mc nz oa dp mg ld ob oc mi lh od oe mk ll of og mm oh bi translated">特征工程</h2><p id="dca6" class="pw-post-body-paragraph ku kv jf kw b kx mo kg kz la mp kj lc ld mq lf lg lh mr lj lk ll ms ln lo lp ij bi translated">一段时间以来，我们在对单个文本进行分类方面很有经验——但是对文本之间的关系进行精确建模的能力相对较新。这里我使用 Panda 的<code class="fe oi oj ok mu b">concat</code>函数将来自问题 1 和问题 2 的两个文本对象连接成一个。<code class="fe oi oj ok mu b">scipy.sparse.hstack</code>用于水平堆叠稀疏矩阵(按列)。</p><h1 id="38a0" class="lw lx jf bd ly lz ma mb mc md me mf mg kl mh km mi ko mj kp mk kr ml ks mm mn bi translated">词袋+ Xgboost 模型</h1><p id="8610" class="pw-post-body-paragraph ku kv jf kw b kx mo kg kz la mp kj lc ld mq lf lg lh mr lj lk ll ms ln lo lp ij bi translated">这种后续策略(计数矢量器)被称为<strong class="kw jg">单词包。</strong>通过单词出现次数来描述文档，而完全忽略单词在文档中的相对位置信息。它对文档进行标记，计算标记的出现次数，并以稀疏矩阵的形式返回它们。</p><p id="7724" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Xgboost 与以下参数一起使用，这些参数是根据验证数据中的性能选择的:</p><pre class="ls lt lu lv gt mt mu mv mw aw mx bi"><span id="84c7" class="my lx jf mu b gy mz na l nb nc">max_depth=50<br/>n_estimators=80<br/>objective='binary:logistic'<br/>eta=0.3</span></pre><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">bow_xgboost</figcaption></figure><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ol"><img src="../Images/d07fb2b7e2b5e4675b68d50d109559af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EAU1rw2Fkx_IG9LT9iBX0w.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 4</figcaption></figure><h1 id="e34d" class="lw lx jf bd ly lz ma mb mc md me mf mg kl mh km mi ko mj kp mk kr ml ks mm mn bi translated">字级 TF-IDF + Xgboost</h1><p id="8912" class="pw-post-body-paragraph ku kv jf kw b kx mo kg kz la mp kj lc ld mq lf lg lh mr lj lk ll ms ln lo lp ij bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">TF–IDF</a>是两个统计量的乘积，术语频率和逆文档频率，它是当今最流行的术语加权方案之一。</p><p id="363a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们将 tf-idf 归一化应用于出现计数的稀疏矩阵，从单词级、n-gram 级和字符级。</p><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">word_tfidf_Xgboost</figcaption></figure><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/241d139e3f1ee7be4e8b3aebd91216a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iqma2MWkzpPw_joPFdmECA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 5</figcaption></figure><p id="0341" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">关于特征工程的一个微妙之处是，它需要知道我们在实践中很可能不知道的特征统计。为了计算 tf-idf 表示，我们必须基于训练数据计算逆文档频率，并使用这些统计来缩放训练和测试数据。在 scikit-learn 中，在训练数据上安装特征转换器相当于收集相关的统计数据。然后，可以将安装好的变压器应用于测试数据。</p><h1 id="1690" class="lw lx jf bd ly lz ma mb mc md me mf mg kl mh km mi ko mj kp mk kr ml ks mm mn bi translated"><strong class="ak"> N-gram 级 TF-IDF + Xgboost </strong></h1><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">N_gram_tfidf_Xgboost</figcaption></figure><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/5af72b50ed273e6c162a232981ed5f3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T5e2k-kP_bMM8amhpsacXw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 6</figcaption></figure><h1 id="cce7" class="lw lx jf bd ly lz ma mb mc md me mf mg kl mh km mi ko mj kp mk kr ml ks mm mn bi translated"><strong class="ak">人物等级 TF-IDF + Xgboost </strong></h1><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">character_tfidf_Xgboost</figcaption></figure><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oo"><img src="../Images/69e1071714de4b2c176af832291cf2b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X_0v7zwANpw848uQX3bHcw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 7</figcaption></figure><p id="57f1" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们的最高验证分数是 0.80，对初学者来说一点也不差！</p><p id="2879" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">到目前为止，我们最好的 Xgboost 模型是字符级 TF-IDF+Xgboost，重复问题的召回率，即我们的模型能够检测的重复问题占重复问题总数的比例是 0.67。这对于手头的问题至关重要，我们希望检测并尽可能多地消除重复问题。</p><p id="3e53" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">考虑到这一点，我们将开发一个<a class="ae lq" href="https://medium.com/@actsusanli/finding-similar-quora-questions-with-word2vec-and-xgboost-1a19ad272c0d" rel="noopener"> word2vec 和 Xgboost 模型</a>，看看这个结果是否可以改进，这将是下一篇文章的<a class="ae lq" href="https://medium.com/@actsusanli/finding-similar-quora-questions-with-word2vec-and-xgboost-1a19ad272c0d" rel="noopener">。</a></p><p id="1514" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><a class="ae lq" href="https://github.com/susanli2016/NLP-with-Python/blob/master/BOW_TFIDF_Xgboost_update.ipynb" rel="noopener ugc nofollow" target="_blank">这篇文章的 Jupyter 笔记本</a>可以在<a class="ae lq" href="https://github.com/susanli2016/NLP-with-Python/blob/master/BOW_TFIDF_Xgboost_update.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。周末快乐！</p></div></div>    
</body>
</html>