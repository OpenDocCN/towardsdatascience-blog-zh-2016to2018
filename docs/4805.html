<html>
<head>
<title>Latent Semantic Analysis &amp; Sentiment Classification with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 Python 的潜在语义分析和情感分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/latent-semantic-analysis-sentiment-classification-with-python-5f657346f6a3?source=collection_archive---------2-----------------------#2018-09-08">https://towardsdatascience.com/latent-semantic-analysis-sentiment-classification-with-python-5f657346f6a3?source=collection_archive---------2-----------------------#2018-09-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/0ec1081f4f3974a13623be313a86721e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GCyzqs8aKcZ2WjKlmPXIdQ.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">photo credit: Pexels</figcaption></figure><div class=""/><div class=""><h2 id="30f5" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">自然语言处理，LSA，情感分析</h2></div><p id="3bce" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" rel="noopener ugc nofollow" target="_blank">潜在语义分析</a> (LSA)是一种通过应用于大型文本语料库的统计计算来提取和表示单词的上下文使用意义的理论和方法。</p><p id="72d7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">LSA 是一种信息检索技术，它分析和识别非结构化文本集合中的模式以及它们之间的关系。</p><p id="cf85" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">LSA 本身是一种在文档集合中发现同义词的不受监督的方式。</p><p id="2a13" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">首先，我们来看看潜在语义分析是如何在自然语言处理中用于分析一组文档和它们所包含的术语之间的关系的。然后我们进一步对情感进行分析和分类。我们将在此过程中回顾特征选择的<a class="ae lq" href="https://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html" rel="noopener ugc nofollow" target="_blank">卡方。我们开始吧！</a></p><h1 id="1914" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">数据</h1><p id="7858" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated">数据集包括超过 50 万条亚马逊美食评论，可以从<a class="ae lq" href="https://www.kaggle.com/snap/amazon-fine-food-reviews" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="79ef" class="mx ls jf mt b gy my mz l na nb">import pandas as pd</span><span id="8899" class="mx ls jf mt b gy nc mz l na nb">df = pd.read_csv('Reviews.csv')<br/>df.head()</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nd"><img src="../Images/e27e5c3d34a5f9a2dabf8aa4847bb833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tDc-PDjwzyNYr4V4zBpB8w.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 1</figcaption></figure><h1 id="7348" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated"><strong class="ak"> TFIDF </strong></h1><p id="d877" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>是一种信息检索技术，它衡量术语的频率(TF)及其逆文档频率(IDF)。每个单词都有各自的 TF 和 IDF 得分。一个单词的 TF 和 IDF 得分的乘积被称为该单词的 TFIDF 权重。</p><p id="e677" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">简单来说，TFIDF 得分(权重)越高的词越稀有，反之亦然。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="f42e" class="mx ls jf mt b gy my mz l na nb">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="82fb" class="mx ls jf mt b gy nc mz l na nb">tfidf = TfidfVectorizer()<br/>tfidf.fit(df['Text'])</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ne"><img src="../Images/19cafc97f02b913e343d6d0c185924d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FQn7liGJwcfQ0f9KucAM-A.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 2</figcaption></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="0f2e" class="mx ls jf mt b gy my mz l na nb">X = tfidf.transform(df['Text'])<br/>df['Text'][1]</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/09509172148df70d77d4b4d509f1af9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1BHjG_r2hrxPipgFdoixPQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 3</figcaption></figure><p id="b7ab" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">参考上面的句子，我们可以检查这个句子中几个词的 tf-idf 得分。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="8df2" class="mx ls jf mt b gy my mz l na nb">print([X[1, tfidf.vocabulary_['peanuts']]])</span></pre><p id="3e1a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">T11【0.37995462060339136】T13】</strong></p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="610c" class="mx ls jf mt b gy my mz l na nb">print([X[1, tfidf.vocabulary_['jumbo']]])</span></pre><p id="741c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"><em class="ng">【0.530965343023095】</em></strong></p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="46cc" class="mx ls jf mt b gy my mz l na nb">print([X[1, tfidf.vocabulary_['error']]])</span></pre><p id="db7a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"><em class="ng">【0.2302711360436964】</em>T21】</strong></p><p id="7e83" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在“花生”、“jumbo”、“错误”三个词中，tf-idf 给“jumbo”的权重最高。为什么？这表明“jumbo”是一个比“peanut”和“error”更罕见的词。这就是如何使用 tf-idf 来表明文档集合中单词或术语的重要性。</p><h1 id="c66a" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">情感分类</h1><p id="1a80" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated">为了对情绪进行分类，我们删除了中性分数 3，然后将分数 4 和 5 分组为正(1)，将分数 1 和 2 分组为负(0)。经过简单的清理，这就是我们将要处理的数据。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="eb47" class="mx ls jf mt b gy my mz l na nb">import numpy as np</span><span id="b484" class="mx ls jf mt b gy nc mz l na nb">df.dropna(inplace=True)<br/>df[df['Score'] != 3]<br/>df['Positivity'] = np.where(df['Score'] &gt; 3, 1, 0)<br/>cols = ['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary']<br/>df.drop(cols, axis=1, inplace=True)<br/>df.head()</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/a6c2aa1f3789807fa8fb2a5cc36b1e3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*M9lnhaL-OIVqL8qgSg-yWg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 4</figcaption></figure><h2 id="d520" class="mx ls jf bd lt ni nj dn lx nk nl dp mb ld nm nn md lh no np mf ll nq nr mh ns bi translated">列车测试分离</h2><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="4b08" class="mx ls jf mt b gy my mz l na nb">from sklearn.model_selection import train_test_split</span><span id="5319" class="mx ls jf mt b gy nc mz l na nb">X = df.Text<br/>y = df.Positivity<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)</span><span id="6737" class="mx ls jf mt b gy nc mz l na nb">print("Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive".format(len(X_train),<br/>                                                                             (len(X_train[y_train == 0]) / (len(X_train)*1.))*100,<br/>                                                                            (len(X_train[y_train == 1]) / (len(X_train)*1.))*100))</span></pre><p id="6445" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="ng">训练集共有 426308 个条目，其中 21.91%为负数，78.09%为正数</em> </strong></p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="fc41" class="mx ls jf mt b gy my mz l na nb">print("Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive".format(len(X_test),<br/>                                                                             (len(X_test[y_test == 0]) / (len(X_test)*1.))*100,<br/>                                                                            (len(X_test[y_test == 1]) / (len(X_test)*1.))*100))</span></pre><p id="fac5" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="ng">测试集共有 142103 个条目，其中 21.99%为阴性，78.01%为阳性</em> </strong></p><p id="a368" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">你可能已经注意到我们的班级是不平衡的，负面和正面的比例是 22:78。</p><p id="5f0f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">对抗不平衡类的策略之一是使用决策树算法，因此，我们使用随机森林分类器来学习不平衡数据并设置<code class="fe nt nu nv mt b">class_weight=balanced</code>。</p><p id="9723" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">首先，定义一个函数来打印出准确度分数。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="e16c" class="mx ls jf mt b gy my mz l na nb">from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.metrics import accuracy_score</span><span id="b0ce" class="mx ls jf mt b gy nc mz l na nb">def accuracy_summary(pipeline, X_train, y_train, X_test, y_test):<br/>    sentiment_fit = pipeline.fit(X_train, y_train)<br/>    y_pred = sentiment_fit.predict(X_test)<br/>    accuracy = accuracy_score(y_test, y_pred)<br/>    print("accuracy score: {0:.2f}%".format(accuracy*100))<br/>    return accuracy</span></pre><p id="6ffd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为了进行有效的情感分析或解决任何自然语言处理问题，我们需要很多特性。很难计算出所需功能的确切数量。所以我们准备试试，一万到三万。并打印出与特征数量相关的准确度分数。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="5146" class="mx ls jf mt b gy my mz l na nb">cv = CountVectorizer()<br/>rf = RandomForestClassifier(class_weight="balanced")<br/>n_features = np.arange(10000,30001,10000)</span><span id="8aba" class="mx ls jf mt b gy nc mz l na nb">def nfeature_accuracy_checker(vectorizer=cv, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=rf):<br/>    result = []<br/>    print(classifier)<br/>    print("\n")<br/>    for n in n_features:<br/>        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)<br/>        checker_pipeline = Pipeline([<br/>            ('vectorizer', vectorizer),<br/>            ('classifier', classifier)<br/>        ])<br/>        print("Test result for {} features".format(n))<br/>        nfeature_accuracy = accuracy_summary(checker_pipeline, X_train, y_train, X_test, y_test)<br/>        result.append((n,nfeature_accuracy))<br/>    return result</span><span id="0356" class="mx ls jf mt b gy nc mz l na nb">tfidf = TfidfVectorizer()<br/>print("Result for trigram with stop words (Tfidf)\n")<br/>feature_result_tgt = nfeature_accuracy_checker(vectorizer=tfidf,ngram_range=(1, 3))</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ne"><img src="../Images/6dcfaada5d04d4c06660b3e386984257.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Drt28J7OAGHe2gNK9Erdzw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 5</figcaption></figure><p id="c25e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">不错！</p><p id="b179" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在我们完成这里之前，我们应该检查分类报告。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="06dc" class="mx ls jf mt b gy my mz l na nb">from sklearn.metrics import classification_report</span><span id="7817" class="mx ls jf mt b gy nc mz l na nb">cv = CountVectorizer(max_features=30000,ngram_range=(1, 3))<br/>pipeline = Pipeline([<br/>        ('vectorizer', cv),<br/>        ('classifier', rf)<br/>    ])<br/>sentiment_fit = pipeline.fit(X_train, y_train)<br/>y_pred = sentiment_fit.predict(X_test)</span><span id="21f7" class="mx ls jf mt b gy nc mz l na nb">print(classification_report(y_test, y_pred, target_names=['negative','positive']))</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/861fae3febc217944e22dc6006681a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jMFCaBif8vZ9wWYz_Ld8aw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 6</figcaption></figure><h2 id="0ea5" class="mx ls jf bd lt ni nj dn lx nk nl dp mb ld nm nn md lh no np mf ll nq nr mh ns bi translated">特征选择的卡方检验</h2><p id="0bbb" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated">特征选择是机器学习中的一个重要问题。我将向您展示在我们的大规模数据集上进行基于卡方检验的特征选择是多么简单。</p><p id="26e2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们将计算所有特征的卡方得分，并可视化前 20 名，这里术语或单词或 N-grams 是特征，正面和负面是两个类别。给定一个特征 X，我们可以用卡方检验来评估它区分类的重要性。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="8a50" class="mx ls jf mt b gy my mz l na nb">from sklearn.feature_selection import chi2<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="48e5" class="mx ls jf mt b gy nc mz l na nb">plt.figure(figsize=(12,8))<br/>scores = list(zip(tfidf.get_feature_names(), chi2score))<br/>chi2 = sorted(scores, key=lambda x:x[1])<br/>topchi2 = list(zip(*chi2[-20:]))<br/>x = range(len(topchi2[1]))<br/>labels = topchi2[0]<br/>plt.barh(x,topchi2[1], align='center', alpha=0.5)<br/>plt.plot(topchi2[1], x, '-o', markersize=5, alpha=0.8)<br/>plt.yticks(x, labels)<br/>plt.xlabel('$\chi^2$')<br/>plt.show();</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nx"><img src="../Images/bed96cc30ec5ee29555351c708a49f1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NihpeTMvh6hewuJJR7EdGQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 7</figcaption></figure><p id="e55b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们可以观察到，具有高χ2 的特征可以被认为与我们正在分析的情感类别相关。</p><p id="5028" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">例如，卡方检验选出的前 5 个最有用的特征是“不”、“失望”、“非常失望”、“不买”和“最差”。我想它们大多来自负面评论。卡方检验选择的下一个最有用的特征是“棒极了”，我假设它主要来自正面评论。</p><p id="3c12" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">今天到此为止。源代码可以在<a class="ae lq" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Latent%20Semantic%20Analysis_reviews.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。我很高兴听到任何问题或反馈。</p></div></div>    
</body>
</html>