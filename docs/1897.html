<html>
<head>
<title>Estimating an Optimal Learning Rate For a Deep Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">估计深度神经网络的最佳学习速率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0?source=collection_archive---------1-----------------------#2017-11-13">https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0?source=collection_archive---------1-----------------------#2017-11-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="bdb6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">学习率是用于训练深度神经网络的最重要的超参数之一。</p><p id="dcc0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我正在描述一个简单而强大的方法来找到一个合理的学习率，这是我从<a class="ae kl" href="http://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai 深度学习课程</a>中学到的。我正在三藩市<a class="ae kl" href="https://www.usfca.edu/data-institute/certificates/deep-learning-part-one" rel="noopener ugc nofollow" target="_blank">大学</a>亲自参加新版本的课程。它还没有对公众开放，但将于今年年底在<a class="ae kl" href="http://course.fast.ai/" rel="noopener ugc nofollow" target="_blank"> course.fast.ai </a>(目前有去年的版本)上发布。</p><h1 id="acb0" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">学习率如何影响培训？</h1><p id="cd7d" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">深度学习模型通常由随机梯度下降优化器训练。随机梯度下降有很多变种:Adam，RMSProp，Adagrad 等。它们都可以让你设定学习速度。该参数告诉优化器在与小批量梯度相反的方向上移动权重的距离。</p><p id="9382" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果学习率低，那么训练更可靠，但是优化将花费大量时间，因为朝向损失函数的最小值的步骤很小。</p><p id="5942" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果学习率很高，那么训练可能不收敛，甚至发散。重量变化可能如此之大，以至于优化器超过了最小值，使损失更严重。</p><p id="b421" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练应该从相对较大的学习率开始，因为在开始时，随机权重远非最佳，然后学习率可以在训练期间降低，以允许更细粒度的权重更新。</p><p id="6bbc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有多种方法可以为学习速度选择一个好的起点。一个天真的方法是尝试几个不同的值，看看哪个给你最好的损失，而不牺牲训练速度。我们可以从一个像 0.1 这样的大值开始，然后尝试指数级的低值:0.01、0.001 等等。当我们以较大的学习率开始训练时，当我们运行训练的最初几次迭代时，损失不会改善，甚至可能增加。当以较小的学习速率训练时，在某个点，损失函数值在最初几次迭代中开始下降。这个学习率是我们可以使用的最大值，任何更高的值都不会让训练收敛。甚至这个值也太高了:它不足以训练多个时期，因为随着时间的推移，网络将需要更细粒度的权重更新。因此，开始训练的合理学习率可能会低 1-2 个数量级。</p><h1 id="021e" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">一定有更聪明的方法</h1><p id="4304" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">Leslie N. Smith 在 2015 年的论文“<a class="ae kl" href="https://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank">训练神经网络的循环学习率</a>”的第 3.3 节中描述了一种为神经网络选择学习率范围的强大技术。</p><p id="1124" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">诀窍是从低学习率开始训练网络，并为每一批以指数方式增加学习率。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/b75a1f50d63c93b95e7436f729db0ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*zgm3iy7aD4ZsXLiva0xtFg.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Learning rate increases after each mini-batch</figcaption></figure><p id="b96b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">记录每批的学习率和培训损失。然后，画出损失和学习率。通常情况下，它看起来像这样:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/d50eda44f18a46633a0d551f77fcaea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*HVj_4LWemjvOWv-cQO9y9g.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">The loss decreases in the beginning, then the training process starts diverging</figcaption></figure><p id="9559" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，在低学习率的情况下，损失改善缓慢，然后训练加速，直到学习率变得太大，损失增加:训练过程出现偏差。</p><p id="9216" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们需要在图上选择一个损失减少最快的点。在这个例子中，当学习率在 0.001 和 0.01 之间时，损失函数快速减小。</p><p id="4122" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">查看这些数字的另一种方法是计算损失的变化率(损失函数相对于迭代次数的导数)，然后在 y 轴上绘制变化率，在 x 轴上绘制学习率。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/a7ea63b89e61afdd17b17d715163e416.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*eYewkhRqRyGg7UsNNaX0Hg.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Rate of change of the loss</figcaption></figure><p id="9abd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它看起来太吵了，让我们使用简单移动平均来平滑它。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/c41ef31062f1fd7d0950f0ab0a3651c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*87mKq_XomYyJE29l91K0dw.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Rate of change of the loss, simple moving average</figcaption></figure><p id="c34f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个看起来更好。在这张图上，我们需要找到最小值。接近 lr=0.01。</p><h1 id="795e" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">履行</h1><p id="bcfd" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">杰瑞米·霍华德和他在<a class="ae kl" href="https://www.usfca.edu/data-institute" rel="noopener ugc nofollow" target="_blank"> USF 数据研究所</a>的团队开发了<a class="ae kl" href="https://github.com/fastai/fastai" rel="noopener ugc nofollow" target="_blank"> fast.ai </a>，这是一个深度学习库，是 PyTorch 之上的高级抽象。这是一个易于使用但功能强大的工具集，用于训练最先进的深度学习模型。杰里米在深度学习课程的最新版本(<a class="ae kl" href="http://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai </a>)中使用该库。</p><p id="c7fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该库提供了学习率查找器的实现。您只需要两行代码来绘制您的模型在学习率上的损失:</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="mc md l"/></div></figure><p id="33f0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该库没有绘制损失函数变化率的代码，但计算起来很简单:</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="mc md l"/></div></figure><p id="3ba1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，在训练之前选择一次学习率是不够的。最佳学习率在训练时下降。您可以定期重新运行相同的学习率搜索过程，以便在培训过程的稍后时间找到学习率。</p><h2 id="c9be" class="me kn iq bd ko mf mg dn ks mh mi dp kw jy mj mk la kc ml mm le kg mn mo li mp bi translated">使用其他库实现方法</h2><p id="0073" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">我还没有看到像 Keras 这样的其他库准备使用这种学习率搜索方法的实现，但是写起来应该很简单。只需多次运行培训，一次一个小批量。每次小批量后，通过乘以一个小常数来提高学习率。当损耗大大高于先前观察到的最佳值时(例如，当当前损耗&gt;最佳损耗* 4 时)，停止该程序。</p><h1 id="6b10" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">事情还不止这些</h1><p id="ee53" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">选择学习率的起始值只是问题的一部分。另一个需要优化的是学习进度:如何在训练中改变学习率。传统观点认为，学习率应该随着时间的推移而降低，并且有多种方式来设置这一点:当损失停止改善时的逐步学习率退火、指数学习率衰减、余弦退火等。</p><p id="168a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我上面提到的论文描述了一种循环改变学习率的新方法。该方法提高了卷积神经网络在各种图像分类任务中的性能。</p><p id="5002" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你知道训练深度神经网络的其他有趣的技巧和诀窍，请给我发消息。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><p id="96a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另请参见:</p><div class="mx my gp gr mz na"><a href="https://hackernoon.com/fast-ai-what-i-learned-from-lessons-1-3-b10f9958e3ff" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab fo"><div class="nc ab nd cl cj ne"><h2 class="bd ir gy z fp nf fr fs ng fu fw ip bi translated">Fast.ai:我从第 1-3 课中学到了什么</h2><div class="nh l"><h3 class="bd b gy z fp nf fr fs ng fu fw dk translated">Fast.ai 是一个非常棒的深度学习课程，适合那些喜欢通过做来学习的人。与其他课程不同，在这里您将…</h3></div><div class="ni l"><p class="bd b dl z fp nf fr fs ng fu fw dk translated">hackernoon.com</p></div></div><div class="nj l"><div class="nk l nl nm nn nj no lv na"/></div></div></a></div><div class="mx my gp gr mz na"><a href="https://medium.com/@surmenok/best-sources-of-deep-learning-news-fbc98815bad3" rel="noopener follow" target="_blank"><div class="nb ab fo"><div class="nc ab nd cl cj ne"><h2 class="bd ir gy z fp nf fr fs ng fu fw ip bi translated">深度学习新闻的最佳来源</h2><div class="nh l"><h3 class="bd b gy z fp nf fr fs ng fu fw dk translated">深度学习领域非常活跃，可以说每周都有一两个突破。研究论文…</h3></div><div class="ni l"><p class="bd b dl z fp nf fr fs ng fu fw dk translated">medium.com</p></div></div></div></a></div><div class="mx my gp gr mz na"><a href="https://becominghuman.ai/jeff-deans-talk-on-large-scale-deep-learning-171fb8c8ac57" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab fo"><div class="nc ab nd cl cj ne"><h2 class="bd ir gy z fp nf fr fs ng fu fw ip bi translated">杰夫·迪恩关于大规模深度学习的演讲</h2><div class="nh l"><h3 class="bd b gy z fp nf fr fs ng fu fw dk translated">杰夫·迪恩是谷歌高级研究员。他领导谷歌大脑项目。2017 年 8 月在 Y Combinator 演讲。的…</h3></div><div class="ni l"><p class="bd b dl z fp nf fr fs ng fu fw dk translated">becominghuman.ai</p></div></div><div class="nj l"><div class="np l nl nm nn nj no lv na"/></div></div></a></div></div></div>    
</body>
</html>