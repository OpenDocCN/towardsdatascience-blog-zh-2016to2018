<html>
<head>
<title>What’s the fuss about Regularization?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正规化有什么好大惊小怪的？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/whats-the-fuss-about-regularization-24a4a1eadb1?source=collection_archive---------17-----------------------#2018-12-16">https://towardsdatascience.com/whats-the-fuss-about-regularization-24a4a1eadb1?source=collection_archive---------17-----------------------#2018-12-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="da79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作为机器学习的新手，当他们的训练错误开始减少时，大多数人都会感到兴奋。他们进一步努力，开始进一步减少，他们的兴奋没有止境。他们向乌龟大师展示了他们的结果，乌龟大师平静地说，这不是一个好模型，你需要调整模型，并检查验证集的性能。如果你想了解什么是“正规化”以及它如何有所帮助，那么请继续阅读。</p><p id="c6a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我先打个比方，机器学习模型就像父母一样，他们对自己的孩子有一种亲和力，他们与孩子相处的时间越多，这种亲和力就越强，孩子就成了他们的世界。ML 模型也是如此，一旦你开始用数据训练它们，它们就开始对训练数据产生兴趣。你越是调整你的模型，它们就越符合训练数据，从而减少误差。然而，我们需要记住的是，训练数据只是总体的样本，因此它代表总体的一些趋势，而不是所有趋势。如果我们将模型与样本数据紧密拟合，那么就不能保证与测试数据拟合得那么好。这意味着测试数据中的趋势不会精确地复制训练数据，但是会是训练数据的概括。因此，我们的模型也需要很好地概括，任何完全适合训练数据的模型都不会很好地概括。这就是正则化有所帮助的地方，它就像一个明智的大师，帮助降低我们的模型与训练数据的亲和力，并帮助它进行归纳。</p><p id="9504" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">唷！很多理论让我们用一个例子来理解它。只是提醒一下，你会在文章前面遇到很多术语，但是不要担心，我会试着解释这些术语。</p><p id="f750" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个简单的例子是，考虑一个样本训练数据，其中 Y 与 x 呈现多项式关系。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/2de7b60d725feecf5270f7cca28da386.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*8fTLEFf2uXEu1bHR6DBx8w.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Training Data</figcaption></figure><p id="4a53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你拟合一个简单的线性回归模型，它看起来像这样，带有一些训练误差。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kx"><img src="../Images/3691ae1e4ba0adb1f3b11c5f97b79378.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*0TNyy0HGr6D_v47brCu9gA.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Fig 1-Linear Regression Fit</figcaption></figure><p id="0bcb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您对此不太满意，并希望进一步减少训练误差，因此您将二次多项式项添加到线性回归(X 平方)中，结果如下所示。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kx"><img src="../Images/8b0ce18a3431817bf179543891769bdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*zLlTC4mWT7aw5h3p82Fl1Q.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Fig 2-Second degree polynomial fit</figcaption></figure><p id="1173" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，看到训练误差已经减少，你忘乎所以，你用 50 次多项式拟合模型，它看起来像这样。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kx"><img src="../Images/eeadcaff28a5ed1239a87e21082b7b78.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*vQS4vagLgMrJ2CCBUs8OSg.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Fig 3–50 Degree polynomial fit</figcaption></figure><p id="5a66" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看看这个模型有多完美，它现在开始紧密地拟合每个点。这就是事情开始变得模糊的地方，这个模型仅仅对于训练数据来说是一个优秀的模型。然而，对于测试数据或该训练样本之外的任何数据来说，这都是废话。在机器学习的说法中，这被称为“高方差”模型。高方差意味着模型是过度拟合的，而过度拟合的模型对于预测来说是不好的。而图 1 中的模型被称为“高偏差”模型或欠拟合模型。我们需要一个平衡偏差和方差的模型。</p><p id="1a1b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在具有多个维度的真实世界中，在对模型进行参数调整之后，您真的不能一直查看图表。这就是你需要依靠正则化来减少方差的地方。但请记住，我们还需要衡量我们的模型在列车数据以外的数据上的表现，这是交叉验证有所帮助的地方。简而言之，从训练数据中保留模型在训练时看不到的一些数据，并根据这些数据(验证数据)评估模型性能。</p><p id="ec9f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么正则化到底是如何降低模型的方差的呢？</p><p id="8673" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为此，我们需要举一个最小二乘回归的例子。最小二乘回归成本函数为:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi ky"><img src="../Images/a76e58e4d9d17718db05ee79b4190356.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*p6oESbJkRM1hkriM15eCdg.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Fig -4 Cost function for Linear Regression</figcaption></figure><p id="99f8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正则化的工作方式是在成本函数中增加惩罚</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ld"><img src="../Images/9dea8af872a0634b8cedb2f6c7a447f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*lMoKQ-QtUdeR6xqSQ0IgsA.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Fig -5 Regularized Cost function</figcaption></figure><p id="6f2e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">记住这个惩罚是针对除了θ0(截距)之外的所有θ的。这如何有助于减少模型的过度拟合？我们来了解一下。</p><p id="8d5d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了减少模型的方差，重要的是我们有一个简约的模型，这意味着我们有一个简单的模型，它可以很好地概括。为了有一个简洁的模型，我们需要减少模型中的特征或者减少除截距之外的特征的 thetas 的权重。对这两种方法有一点了解。</p><ol class=""><li id="207f" class="le lf iq jp b jq jr ju jv jy lg kc lh kg li kk lj lk ll lm bi translated"><strong class="jp ir">减少特征的数量(模型选择/子集选择)</strong></li></ol><p id="56b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是通过模型选择或子集选择技术实现的。这是一个统计过程，其中使用特征组合构建各种模型，并使用 AIC (Akaike 信息标准)、SBC( Shwarz 贝叶斯标准)或 Mallows (Cp)或任何其他标准选择最佳模型(具有特征子集)。然而，我们不打算在本文中讨论这一点。</p><p id="04c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 2。</strong> <strong class="jp ir">降低特征的权重(正则化/收缩法)</strong></p><p id="2b8c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在正则化中，我们保留所有特征，但是减少特征的θ或权重的大小。该技术减少并消除了不重要特征的 thetas(权重)。因此，我们得到了一个简约的模型。</p><p id="13e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有多种方法可以正则化一个模型。对于线性回归，您可能听说过以下内容:</p><ul class=""><li id="c3f6" class="le lf iq jp b jq jr ju jv jy lg kc lh kg li kk ln lk ll lm bi translated">山脊(减少特征的权重)</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/8faee7fb95878ab55327c9353691bf3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*euM7xPIPPKlQCld65EaKMg.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Fig 6.2 — Ridge Regression</figcaption></figure><ul class=""><li id="0498" class="le lf iq jp b jq jr ju jv jy lg kc lh kg li kk ln lk ll lm bi translated">套索(消除一些特征)</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/f3a1694e66477ffd72207ae59b676a8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*l7CTYuNa9lQQw-lcRlsenA.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Fig 6.1 — Lasso Regression</figcaption></figure><ul class=""><li id="1398" class="le lf iq jp b jq jr ju jv jy lg kc lh kg li kk ln lk ll lm bi translated">ElasticNet(混合方法)</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/2e183efdadef3074dcc08b494f59ea8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*JdnM3JS_EWAEFmPBESZrTg.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Fig 6.3 Elastic Net Regression</figcaption></figure><p id="ebe1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lr">其中:</em></p><p id="ba2b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lr">λ=正则化的惩罚参数</em></p><h2 id="2f38" class="ls lt iq bd lu lv lw dn lx ly lz dp ma jy mb mc md kc me mf mg kg mh mi mj mk bi translated">山脉</h2><p id="e950" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">在脊正则化的情况下，罚值被称为<em class="lr"> l2 范数。</em>那么为什么岭回归会减少过拟合呢？随着λ的增加，岭的灵活性开始降低，当<em class="lr">λ= 0</em>时，岭回归与 OLS 相同。然而，特征权重可以非常接近于零，但是在脊的情况下很难得到零权重。</p><h2 id="434e" class="ls lt iq bd lu lv lw dn lx ly lz dp ma jy mb mc md kc me mf mg kg mh mi mj mk bi translated">套索</h2><p id="f2b6" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">在套索的情况下，惩罚被称为<em class="lr"> l1 范数</em>。<em class="lr"> </em>山脊和套索的区别在于，在套索的情况下，随着 lambda 的增加，灵活性开始迅速降低，很快一些特征被关闭，这意味着一些特征的特征权重降低到零。因此，套索在减少特征和具有更简洁的模型方面非常有用。</p><p id="c8c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们用一个例子来理解这一点。我正在使用一个名为 Credit 的数据集，它可以从著名的 ISLR 图书网站上获得。</p><p id="8fbe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">问题是使用一组特征来预测“平衡”</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/62b33c5afd2df286aa7c1522de6d1198.png" data-original-src="https://miro.medium.com/v2/resize:fit:192/format:webp/1*Du6oRnDuRLQQY1EkAGvxQA.png"/></div></figure><p id="0014" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个问题不是很大，但它将帮助我们了解山脊和套索在同一组要素上的表现。</p><p id="f6e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为简单起见，我们只取四个特征收入、评级、限制和学生。如果λ从 0.001 到 1000，我们将取 50 个值，然后检查收入、评级、限制和学生的权重。这将有助于深入了解山脊和套索的行为。</p><p id="c233" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在使用四个特征和 50 个不同的λ值运行脊之后，这里是这四个特征的系数如何变化。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/75817ea71adc3c9c136dc10af6d8a6ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*DYfw-uEs0UGER94F6Tt0_g.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Fig 7 — Ridge coefficients as a function of regularization</figcaption></figure><p id="13c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，当λ为零时，所有四个特征的权重都很高，并且对应于 OLS。随着λ开始增加，权重值开始下降。对于λ= 1000，收入和学生的权重接近于零但不等于零，其他两个特征的权重也显著降低。</p><p id="9542" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对这些的解释超出了本文的范围。这篇文章只是想给出一个正规化的概述，但是我会在下一篇文章中用一个具体的例子来解释套索和山脊。观察正则化如何工作以及它如何影响模型将是非常有趣的。</p><p id="75de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们对套索重复同样的过程时，我们看到的是:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/417860c1babe491593a17c41f665ceb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*IgrtbB-YOC1en5whAu_KjA.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Fig 8- Lasso Coefficients as function of regularization</figcaption></figure><p id="8305" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你能看出区别吗？对于大约 150 的λ，三个特征的权重下降到零。因此对于一个小的正则化套索完全消除了特征。因此，Lasso 在特征选择中是有用的，其中它通过将不必要的特征的权重减少到零来消除不必要的特征，并且实现更简约的模型。</p><p id="5a05" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们来看看 RSS(MSE)如何随着训练数据的这两个正则化而变化。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/075eb1bcbaece4ef50f39a2e8285e944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*ARTPkIM60IrXph5juE4qOQ.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Fig 9- MSE(train) for Lasso &amp; Ridge as function of regularization</figcaption></figure><p id="4c7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看看 Lasso 的给定 lambdas 在训练数据上的 MSE 是如何高于 Ridge 的。这是因为套索比山脊更能规范模型。</p><p id="53b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">值得一提的是，在三重交叉验证的情况下，MSE 是如何随交叉验证数据而变化的。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/0e0dd2d8ae178424f3890ccc65f1b252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*QaKt63ilE6hVVcjZip_MUg.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Fig 10- Cross Validation score for Lasso &amp; Ridge as function of regularization</figcaption></figure><p id="99d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看看 Lasso 的交叉验证分数如何比 Ridge 下降得更快，表明 Lasso 更适合验证数据。</p><p id="0756" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们需要记住正则化的一个重要方面，即数据必须标准化。</p><p id="31cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与 ridge 类似，lasso 也将系数估计缩小到零，但是在 lasso 的情况下，当 lambda 较大时，l1 罚分具有将一些系数强制为零的效果。因此，套索导致可变选择，套索模型有时也被称为稀疏模型。然而，当响应是所有预测值的函数时，岭是有效的，因为它不消除特征。</p><p id="874b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正则化不仅用于回归，还用于决策树，在决策树中称为修剪，在神经网络中称为丢弃。但是关于这些的讨论超出了本文的范围。</p><p id="e8db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我希望上面的文章能帮助你理解正规化。</p><p id="49dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你想看一个正在运行的例子，请在 google colab 上查看<a class="ae mv" href="https://colab.research.google.com/drive/1GxU2XyQtKxbklQrZVhS1M_M1eoKgHK5H" rel="noopener ugc nofollow" target="_blank">这里</a></p><div class="mw mx gp gr my mz"><a href="https://colab.research.google.com/drive/1GxU2XyQtKxbklQrZVhS1M_M1eoKgHK5H" rel="noopener  ugc nofollow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd ir gy z fp ne fr fs nf fu fw ip bi translated">谷歌联合实验室</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">编辑描述</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">colab.research.google.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn kr mz"/></div></div></a></div><p id="caf8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">github 代码可以从<a class="ae mv" href="https://gist.github.com/sagarmainkar/1159c818c5555fae189882118245d44e.js" rel="noopener ugc nofollow" target="_blank">这里</a>拉出来</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="no np l"/></div></figure></div></div>    
</body>
</html>