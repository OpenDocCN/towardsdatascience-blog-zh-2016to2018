<html>
<head>
<title>Lossless Triplet loss</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无损三重损耗</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lossless-triplet-loss-7e932f990b24?source=collection_archive---------1-----------------------#2018-02-15">https://towardsdatascience.com/lossless-triplet-loss-7e932f990b24?source=collection_archive---------1-----------------------#2018-02-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/06294997e482cee514090e98ee1d5ea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*023gxVgV9CH-4_qj7IZbiQ.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">pexels.com</figcaption></figure><div class=""/><div class=""><h2 id="9d6d" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">一种更有效的暹罗神经网络损失函数</h2></div><p id="926f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在工作中，我们与暹罗神经网络(NN)合作，对电信数据进行一次性训练。我们的目标是创建一个可以轻松检测电信运营商网络故障的神经网络。为此，我们构建了 N 维编码来描述网络的实际状态。通过这种编码，我们可以评估网络的状态并检测故障。这种编码与单词编码(<a class="ae lq" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>或其他)的目标相同。为了训练这种编码，我们使用一个<a class="ae lq" href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" rel="noopener ugc nofollow" target="_blank">暹罗网络</a>【科赫等人】来创建一个一次性编码，这样它就可以在任何网络上工作。暹罗网络的简单描述可以在<a class="ae lq" href="https://hackernoon.com/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e" rel="noopener ugc nofollow" target="_blank">这里</a>找到。关于我们实验的更多细节，你可以阅读我的同事的博客<a class="ae lq" href="https://thelonenutblog.wordpress.com/2017/12/14/do-telecom-networks-dreams-of-siamese-memories/" rel="noopener ugc nofollow" target="_blank">，他是这个想法背后的主脑。</a></p><p id="1413" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在<a class="ae lq" href="http://coffeeanddata.ca" rel="noopener ugc nofollow" target="_blank"> coffeeanddata.ca </a>上的原始帖子</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><p id="61f0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">目前的实验，到目前为止效果很好。这个网络可以分割不同的交通场景。正如您在这张图片上看到的，良好的流量(绿色)很容易从错误类型 1(红色)和错误类型 2(橙色)中分离出来</p><figure class="lz ma mb mc gt is gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/01c8e5d8606b1c13f2a8c0b08b56dad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*9gwamk7iZnmLPrghW0PtPw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Current Model</figcaption></figure><h1 id="2109" class="md me jf bd mf mg mh mi mj mk ml mm mn kl mo km mp ko mq kp mr kr ms ks mt mu bi translated">问题是</h1><p id="9603" class="pw-post-body-paragraph ku kv jf kw b kx mv kg kz la mw kj lc ld mx lf lg lh my lj lk ll mz ln lo lp ij bi translated">那么问题是什么，它似乎工作得很好，不是吗？经过一番思考，我意识到损失函数有一个很大的缺陷。</p><p id="8a19" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">首先是我们模型的代码。(不必阅读所有代码，我会指出问题。)</p><figure class="lz ma mb mc gt is"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="e14a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我的问题是这条损失函数线。</p><blockquote class="nc"><p id="525a" class="nd ne jf bd nf ng nh ni nj nk nl lp dk translated"><em class="nm"> loss = K.maximum(basic_loss，0.0) </em></p></blockquote><p id="00fd" class="pw-post-body-paragraph ku kv jf kw b kx nn kg kz la no kj lc ld np lf lg lh nq lj lk ll nr ln lo lp ij bi translated">这里有一个主要问题，每次你的损失低于 0，你就失去了信息，大量的信息。首先让我们看看这个函数。</p><p id="3d8c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">它基本上是这样的:</p><figure class="lz ma mb mc gt is gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/ad2598096f621bcf69a71f226e3ef87d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*_WNBFcRVEOz6QM7R."/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Schroff et al.</figcaption></figure><p id="98a3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">它试图使锚(当前记录)与正(理论上与锚相似的记录)尽可能接近负(与锚不同的记录)。</p><p id="ba76" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这一损失的实际公式为:</p><figure class="lz ma mb mc gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/32d9cf9814b981846ab317490a3195db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AX2TSZNk19_gDgTN.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Schroff et al.</figcaption></figure><p id="cc5a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这个过程在论文<a class="ae lq" href="https://arxiv.org/abs/1503.03832" rel="noopener ugc nofollow" target="_blank"> <em class="nu"> FaceNet 中有详细介绍:一个统一嵌入用于人脸识别和聚类的</em></a><em class="nu">by</em><a class="ae lq" href="https://arxiv.org/find/cs/1/au:+Schroff_F/0/1/0/all/0/1" rel="noopener ugc nofollow" target="_blank"><em class="nu">Florian Schroff</em></a><em class="nu">，</em><a class="ae lq" href="https://arxiv.org/find/cs/1/au:+Kalenichenko_D/0/1/0/all/0/1" rel="noopener ugc nofollow" target="_blank"><em class="nu">Dmitry Kalenichenko</em></a><em class="nu">和</em><a class="ae lq" href="https://arxiv.org/find/cs/1/au:+Philbin_J/0/1/0/all/0/1" rel="noopener ugc nofollow" target="_blank"><em class="nu">James Philbin</em></a><em class="nu">。</em></p><p id="7120" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">因此，只要负值比正值+α更大，算法就不会有增益来压缩正值和锚点。我的意思是:</p><figure class="lz ma mb mc gt is gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/8c698a9a4fafa2ec0c03089354dcb0b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*sGRGkYaWj-JJVhQanV6-pQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Distance costs</figcaption></figure><p id="c229" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我们假设:</p><ul class=""><li id="fefb" class="nw nx jf kw b kx ky la lb ld ny lh nz ll oa lp ob oc od oe bi translated">阿尔法是 0.2</li><li id="e90e" class="nw nx jf kw b kx of la og ld oh lh oi ll oj lp ob oc od oe bi translated">负距离是 2.4</li><li id="8525" class="nw nx jf kw b kx of la og ld oh lh oi ll oj lp ob oc od oe bi translated">正距离是 1.2</li></ul><p id="1281" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">损失函数结果将是 1.2–2.4+0.2 =<strong class="kw jg">-1</strong>。然后，当我们查看 Max(-1，0)时，我们最终得到 0 作为损失。正距离可以是大于 1 的任何值，损失也是一样的。在这种情况下，算法将很难减少锚和正值之间的距离。</p><p id="884f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">作为一个更直观的例子，这里有两个场景 A 和 b。它们都代表了损失函数为我们测量的内容。</p><figure class="lz ma mb mc gt is gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/dd85581a4d5292ab85c9cd2a141663ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*PB6FbGGnj00SLFGEbxlrvA.png"/></div></figure><figure class="lz ma mb mc gt is gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/35aaf261eea0c205e8c2f6ef7a28c671.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*XUaxmqt0eYhPH1InzGCqCQ.png"/></div></figure><p id="ea59" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在 Max 函数之后，A 和 B 现在都返回 0 作为它们的损失，这是明显的信息损失。单纯看，可以说 B 比 a 好。</p><p id="1abe" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">换句话说，你不能相信损失函数的结果，例如 50 年前后的结果。损失(训练和开发)为 0，但显然结果并不完美。</p><figure class="lz ma mb mc gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/e1552c7996fe9eb050a483b252b7afbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cAjl4cd6SjhdFexmqXanLw.png"/></div></div></figure><h1 id="09ff" class="md me jf bd mf mg mh mi mj mk ml mm mn kl mo km mp ko mq kp mr kr ms ks mt mu bi translated">其他损失</h1><p id="669b" class="pw-post-body-paragraph ku kv jf kw b kx mv kg kz la mw kj lc ld mx lf lg lh my lj lk ll mz ln lo lp ij bi translated">另一个著名的损失函数是严乐存和他的团队在他们的论文<a class="ae lq" href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf" rel="noopener ugc nofollow" target="_blank">中描述的对比损失，通过学习不变映射</a>进行降维，也最大化了负面结果，这产生了相同的问题。</p><figure class="lz ma mb mc gt is gh gi paragraph-image"><div class="gh gi on"><img src="../Images/6a0a059ec65cb8f7c1307e53cf7e3adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*Kk7ZTncyAGIpQbYqQ5MCxw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">The Contrastive Loss Function, (LeCun)</figcaption></figure><h1 id="bf9e" class="md me jf bd mf mg mh mi mj mk ml mm mn kl mo km mp ko mq kp mr kr ms ks mt mu bi translated">解决办法</h1><p id="c74c" class="pw-post-body-paragraph ku kv jf kw b kx mv kg kz la mw kj lc ld mx lf lg lh my lj lk ll mz ln lo lp ij bi translated">有了标题，你很容易猜到我的计划是什么…做一个损失函数，它将捕捉 0 以下的“丢失”信息。在一些基本的几何学之后，我意识到如果你包含了计算损失的 N 维空间，你可以更有效地控制它。所以第一步是修改模型。最后一层(嵌入层)需要控制大小。通过使用 Sigmoï de 激活函数而不是线性函数，我们可以保证每个维度都在 0 和 1 之间。</p><figure class="lz ma mb mc gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/d86f7a03619a61bd0e289adf6169b423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*EeTNLtU7ReDW00M3IYvCJQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Sigmoïde activation</figcaption></figure><p id="db00" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">那么我们可以假设距离的最大值是 N，N 是维数。举个例子，如果我的锚点在 0，0，0，我的负点在 1，1，1。基于 Schroff 公式的距离将是 1 +1 +1 = <strong class="kw jg"> 3 </strong>。所以如果我们考虑维度的数量，我们可以推导出最大距离。这是我提出的公式。</p><figure class="lz ma mb mc gt is gh gi paragraph-image"><div class="gh gi op"><img src="../Images/9d3beccd06e435aae823d87e17f3f601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*KoOPaO18mSAshqrjrBzttA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Linear loss function</figcaption></figure><p id="dc1c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">其中 N 是嵌入向量的维数。这看起来非常相似，但通过使用 sigmode 和适当的 N 设置，我们可以保证该值保持在 0 以上。</p><h1 id="3f06" class="md me jf bd mf mg mh mi mj mk ml mm mn kl mo km mp ko mq kp mr kr ms ks mt mu bi translated">初步结果</h1><p id="c5d4" class="pw-post-body-paragraph ku kv jf kw b kx mv kg kz la mw kj lc ld mx lf lg lh my lj lk ll mz ln lo lp ij bi translated">经过一些初步测试，我们最终有了这个模型。</p><figure class="lz ma mb mc gt is gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/c17560a56a6394c0577a3819173353e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*Vhau8DGXJxok3ELZ-l9PPw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Merge results</figcaption></figure><p id="df3c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">好的一面是，我们可以看到来自同一个集群的所有点都变得非常紧密，甚至到了它们变得相同的程度。但不利的一面是，这两种错误情况(橙色和红色)重叠了。</p><p id="e640" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这样做的原因是，当红色和橙色分开时，这样的损失较小。所以我们需要找到打破成本线性的方法；换句话说，随着错误越来越多，成本越来越高。</p><h1 id="0226" class="md me jf bd mf mg mh mi mj mk ml mm mn kl mo km mp ko mq kp mr kr ms ks mt mu bi translated">非线性</h1><p id="3aac" class="pw-post-body-paragraph ku kv jf kw b kx mv kg kz la mw kj lc ld mx lf lg lh my lj lk ll mz ln lo lp ij bi translated">代替线性成本，我们提出了非线性成本函数:</p><figure class="lz ma mb mc gt is gh gi paragraph-image"><div class="gh gi or"><img src="../Images/46d89152bcf51d067ee1428e6ee3d1cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*zBmxKeKoPCTylxYVlPSnFw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">From Google Graph</figcaption></figure><figure class="lz ma mb mc gt is gh gi paragraph-image"><div class="gh gi os"><img src="../Images/7912b592132d2250b6b533988c2fdda6.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*ZSttFrrQfT5Qj1ecFRiDXA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">curve function</figcaption></figure><p id="13d7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">其中当 N = 3 时，曲线由 ln 函数表示</p><p id="1352" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">有了这个新的非线性，我们的成本函数现在看起来像:</p><figure class="lz ma mb mc gt is gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/14982b62949a46b35a66a84516510798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*ImLyAlX_cky6AXjDzHdEjQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Lossless triplet loss</figcaption></figure><p id="6e6b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">其中 N 是维数(网络的输出数；用于嵌入的特征数量),β是比例因子。我们建议将其设置为 N，但也可以使用其他值来修改非线性成本。</p><p id="498b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">如您所见，结果不言自明:</p><figure class="lz ma mb mc gt is gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/f379b4d911a7a2cafb3ad5796da0bdd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*UPJaUl814AWzq1cGuYO9eg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Result of classification with lossless triplet loss</figcaption></figure><p id="0504" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们现在有非常浓缩的点群，比标准的三元组函数结果要多得多。</p><p id="6842" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">作为参考，损失函数的代码如下:</p><figure class="lz ma mb mc gt is"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="a7ce" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">记住，为了使它工作，你需要你的 NN 最后一层使用 Sigmoï de 激活函数。</p><figure class="lz ma mb mc gt is gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/f0c037ed82ead1dbb0da315732d21389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*GOxfIAEBpemer9B8oQvSDQ.png"/></div></figure><p id="c43f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">即使在 1000 个历元之后，无损三重损耗也不会像标准三重损耗那样产生 0 损耗。</p><h1 id="cee6" class="md me jf bd mf mg mh mi mj mk ml mm mn kl mo km mp ko mq kp mr kr ms ks mt mu bi translated">差异</h1><p id="4382" class="pw-post-body-paragraph ku kv jf kw b kx mv kg kz la mw kj lc ld mx lf lg lh my lj lk ll mz ln lo lp ij bi translated">基于我的同事<a class="ae lq" href="https://thelonenutblog.wordpress.com/" rel="noopener ugc nofollow" target="_blank">做的他的模型的很酷的动画，我决定做同样的事情，但是用两个损失函数的现场比较。这是现场结果，左边是标准三重损耗(来自 Schroff paper ),右边是无损三重损耗:</a></p><figure class="lz ma mb mc gt is"><div class="bz fp l di"><div class="ow nb l"/></div></figure><h1 id="571a" class="md me jf bd mf mg mh mi mj mk ml mm mn kl mo km mp ko mq kp mr kr ms ks mt mu bi translated">结论</h1><p id="7e03" class="pw-post-body-paragraph ku kv jf kw b kx mv kg kz la mw kj lc ld mx lf lg lh my lj lk ll mz ln lo lp ij bi translated">这个损失函数看起来不错，现在我需要在更多的数据、不同的用例上测试它，看看它是否真的是一个稳健的损失函数。让我知道你对这个损失函数的看法。</p><h1 id="9348" class="md me jf bd mf mg mh mi mj mk ml mm mn kl mo km mp ko mq kp mr kr ms ks mt mu bi translated">想要阅读更多内容</h1><p id="6a45" class="pw-post-body-paragraph ku kv jf kw b kx mv kg kz la mw kj lc ld mx lf lg lh my lj lk ll mz ln lo lp ij bi translated">关注我的博客:<a class="ae lq" href="http://coffeeanddata.ca" rel="noopener ugc nofollow" target="_blank"> coffeeanddata.ca </a></p><h1 id="3c8d" class="md me jf bd mf mg mh mi mj mk ml mm mn kl mo km mp ko mq kp mr kr ms ks mt mu bi translated">参考</h1><ul class=""><li id="cd9d" class="nw nx jf kw b kx mv la mw ld ox lh oy ll oz lp ob oc od oe bi translated">科赫、格雷戈里、理查德·泽梅尔和鲁斯兰·萨拉胡季诺夫。“用于一次性图像识别的连体神经网络。”<em class="nu"> ICML 深度学习工场</em>。第二卷。2015.</li><li id="2f4c" class="nw nx jf kw b kx of la og ld oh lh oi ll oj lp ob oc od oe bi translated">《向量空间中单词表征的有效估计》<em class="nu"> arXiv 预印本 arXiv:1301.3781 </em> (2013)。</li><li id="4496" class="nw nx jf kw b kx of la og ld oh lh oi ll oj lp ob oc od oe bi translated">施洛夫、弗洛里安、德米特里·卡列尼琴科和詹姆斯·菲尔宾。" Facenet:人脸识别和聚类的统一嵌入."IEEE 计算机视觉和模式识别会议论文集。2015.</li><li id="03e6" class="nw nx jf kw b kx of la og ld oh lh oi ll oj lp ob oc od oe bi translated">哈德塞尔、拉亚、苏米特·乔普拉和扬·勒昆。"通过学习不变映射来降低维数."<em class="nu">计算机视觉与模式识别，2006 年 IEEE 计算机学会会议于</em>。第二卷。IEEE，2006 年。</li><li id="ab0e" class="nw nx jf kw b kx of la og ld oh lh oi ll oj lp ob oc od oe bi translated">【https://thelonenutblog.wordpress.com/ T4】</li><li id="a6ac" class="nw nx jf kw b kx of la og ld oh lh oi ll oj lp ob oc od oe bi translated">https://hacker noon . com/one-shot-learning-with-siamese-networks-in-py torch-8d daab 10340 e</li></ul></div></div>    
</body>
</html>