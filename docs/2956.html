<html>
<head>
<title>How to build a simple artificial neural network with Go</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用Go构建一个简单的人工神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37?source=collection_archive---------11-----------------------#2018-03-23">https://towardsdatascience.com/how-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37?source=collection_archive---------11-----------------------#2018-03-23</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="dc16" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">从基础数学到用它识别笔迹的一步一步</h2></div><p id="e928" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我在职业生涯中写过很多计算机程序，大部分时间是为了解决各种问题或者执行一些任务(或者有时候只是为了好玩)。在大多数情况下，除了bug之外，只要我非常清楚地告诉计算机该做什么(无论我使用哪种编程语言)，它就会乖乖地听从我的指令。</p><p id="be24" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这是因为计算机程序非常擅长执行算法——遵循精确且经常重复的既定步骤和模式的指令。在大多数情况下，它们对我们处理数字运算或重复枯燥的工作很有帮助。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj lf"><img src="../Images/457eecbf6cbabd0e73f56f0719012207.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nzfsdObSmODQMJhy.jpg"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">The ENIAC was one of the first general-purpose, programmable computers ever made (public domain from <a class="ae lv" href="https://commons.wikimedia.org/wiki/File:Eniac.jpg" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/wiki/File:Eniac.jpg</a>)</figcaption></figure><p id="6502" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">然而，计算机程序不擅长做的事情是那些没有很好定义的任务，并且不遵循精确的模式。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj lw"><img src="../Images/03f2d560690c4802e49ef64293da9645.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/0*3117NCTgZSUkHDZs.png"/></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">In the 60s, Marvin Minsky assigned a couple of undergrads to spend the summer programming a computer to use a camera to identify objects in a scene. He figured they’d have the problem solved by the end of the summer. Half a century later, we’re still working on it. (from <a class="ae lv" href="https://www.explainxkcd.com/wiki/index.php/1425:_Tasks" rel="noopener ugc nofollow" target="_blank">Explain XKCD — 1425: Tasks</a>)</figcaption></figure><p id="f3eb" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">那么我们如何使用计算机来完成这样的任务呢？想想你如何完成这项任务。你可能在年轻的时候了解过鸟类，你被告知某些动物是鸟类，而某些动物不是，大部分是通过在现实生活中或通过图画书看到的。当你做错的时候，你会被告知并且记住。久而久之，你就有了一个<em class="lx">心理模型</em>，知道什么是鸟，什么不是。每次你看到一只鸟的某些部分(有爪的脚，有羽毛的翅膀，锋利的喙)你甚至不需要再看到整个动物，你会通过与你的心理模型进行比较来自动正确地识别它。</p><p id="cd82" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">那么我们如何用计算机程序做到这一点呢？基本上我们做同样的事情。我们试图创建一个<em class="lx">模型</em>，通过试错过程，我们可以用它来比较输入。由于计算机程序都是数学，你可以猜到这将是我们将要谈论的一个<em class="lx">数学模型</em>。</p><h1 id="ae72" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">猜谜游戏</h1><p id="85e7" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">让我们举一个简单的例子，创建一个接受输入并试图预测输出的黑盒。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj mv"><img src="../Images/9b46d7f667ca1628dc13f6caaed509bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*p_Nj1xUV90rg_Icf.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">A simple predictor</figcaption></figure><p id="dadf" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们给它一个输入，然后从这个预测器得到输出。既然我们知道实际输出应该是什么，我们就可以知道预测输出与实际输出有多大的不同。实际输出和预测输出之间的差异成为<em class="lx">误差</em>。</p><p id="0613" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">当然，如果预测器是静态的，不能改变，那就没什么意义了。当我们向预测器提供输入时，会产生一个带有错误的输出，这就是故事的结尾。不是很有用。</p><p id="890c" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">为了让我们的预测器更有用，让我们给它一个可配置的参数，我们可以用它来影响输出。因为它只有在没有错误的情况下才能正确预测，所以我们希望更改参数，以便随着我们不断向预测器提供数据，错误会缩小。目的是得到一个预测器，它在大多数情况下预测正确的输出，而实际上不需要给预测器明确的指令。</p><p id="4390" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">换句话说，这很像一个数字猜谜游戏。</p><p id="5061" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">让我们从更实际的角度来看这个问题。假设我们有一个带有简单数学公式<code class="fe mw mx my mz b">o = i x c</code>的预测器，其中<code class="fe mw mx my mz b">o</code>是输出，<code class="fe mw mx my mz b">i</code>是输入，<code class="fe mw mx my mz b">c</code>是可配置参数。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj mv"><img src="../Images/d9a90978e9a080040b6ac61c9d27af30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hHNkQOGxQYKodbxd.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">A simple predictor with a configurable parameter</figcaption></figure><p id="4684" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们还得到了一个给定输入的确认有效输出，即我们知道<code class="fe mw mx my mz b">i</code>是否为10，<code class="fe mw mx my mz b">o</code>是否为26。我们如何使用预测器找到<code class="fe mw mx my mz b">c</code>？</p><p id="d43f" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">首先，我们需要进行随机预测，假设<code class="fe mw mx my mz b">c</code>是2。让我们输入10，启动预测器。输出<code class="fe mw mx my mz b">o</code>为20。由于误差<code class="fe mw mx my mz b">e = t - o</code>其中<code class="fe mw mx my mz b">t</code>是真值(或目标)，这意味着<code class="fe mw mx my mz b">e = 26 - 20 = 6</code>。我们的误差<code class="fe mw mx my mz b">e</code>是6，我们想达到0，所以我们再试一次。</p><p id="8b93" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">让我们把<code class="fe mw mx my mz b">c</code>设为3。然后输出为<code class="fe mw mx my mz b">30</code>并且<code class="fe mw mx my mz b">e</code>现在为<code class="fe mw mx my mz b">-4</code>。哎呀，我们超过了！我们倒回去一点，让<code class="fe mw mx my mz b">c</code>为2.5。这使得<code class="fe mw mx my mz b">o</code>为25，而<code class="fe mw mx my mz b">e</code>为1。最后，我们尝试将<code class="fe mw mx my mz b">c</code>设为2.6，我们得到的误差<code class="fe mw mx my mz b">e</code>为0！</p><p id="d2e3" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">一旦我们知道<code class="fe mw mx my mz b">c</code>是什么，我们就可以使用预测器来预测其他输入的输出。假设输入<code class="fe mw mx my mz b">i</code>现在是20，那么我们可以预测<code class="fe mw mx my mz b">o</code>是52。</p><p id="2897" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">正如你所看到的，这种方法试图迭代地寻找答案，并不断改进自己，直到我们找到最佳答案。这本质上就是<a class="ae lv" href="https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[A%20Beginner%E2%80%99s%20Guide%20to%20AI/ML%20%F0%9F%A4%96%F0%9F%91%B6%20%E2%80%93%20Machine%20Learning%20for%20Humans%20%E2%80%93%20Medium](https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12)" rel="noopener ugc nofollow">机器学习</a>是什么。计算机程序试图迭代地寻找答案，并通过它的错误“学习”,直到它获得一个可以产生最佳答案的模型。一旦它有了正确的模型，我们就可以使用该模型来正确地猜测答案。这非常类似于我们人类所做的(从过去的错误中学习并纠正自己)，但我们具体是如何做的呢？</p><h1 id="b3c9" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">人类是如何做到的</h1><p id="3652" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">让我们出去一点。我们谈了一点机器如何利用数学函数学习。人类如何做同样的事情(正如多年来的研究表明的那样)是使用一种叫做<a class="ae lv" href="https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Understanding%20Neurons%E2%80%99%20Role%20in%20the%20Nervous%20System](https://www.verywellmind.com/what-is-a-neuron-2794890)" rel="noopener ugc nofollow" target="_blank"> <em class="lx">神经元</em> </a>的东西。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj na"><img src="../Images/4dfd467aab37c8802056ec4a45fcabeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/0*YfHYAaCYR6JKJsAC.jpg"/></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">Drawing of neurons in the pigeon cerebellum, by Spanish neuroscientist Santiago Ramón y Cajal in 1899 (public domain from <a class="ae lv" href="https://commons.wikimedia.org/wiki/File:PurkinjeCell.jpg" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/wiki/File:PurkinjeCell.jpg</a>)</figcaption></figure><p id="69b0" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">神经元或神经细胞是一种接收信息、处理信息并通过电信号和化学信号进行传输的细胞。我们的大脑和脊髓(我们的中枢神经系统的一部分)由神经元组成。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj nb"><img src="../Images/8b615054bb82eb35f1571916ce7b27e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YHuoAjHQSws_2aXz.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">A neuron with dendrites, a cell body and an axon</figcaption></figure><p id="e552" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">神经元由细胞体、树突和轴突组成，可以相互连接形成神经网络。在神经网络中，神经元的轴突连接到下一个神经元的树突，突触信号从一个神经元通过其轴突传输，并由下一个神经元通过其树突接收。轴突和树突之间的连接是突触。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj nc"><img src="../Images/d46dafcd591ff2b05fec4009f0ba95b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pq7CXpW4VZlqEIo9.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">Synapses are the connections between neurons</figcaption></figure><p id="3bda" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">通过树突传入的信号根据突触连接的使用频率而加强或减弱，这些加强或减弱的信号在细胞体中汇集在一起。</p><p id="5723" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">如果接收到的汇集信号足够强，它将触发一个新的信号，通过轴突发送到其他神经元。</p><p id="e34e" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">如你所见，神经元工作方式有点类似于我们之前的预测器。它通过树突处理大量输入，通过轴突输出。每个输入都与突触连接的强度(或权重)配对，而不是一个可配置的参数。</p><p id="8412" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">有了这些信息，让我们回到我们的预测器并做一些改变。</p><h1 id="afc4" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">人工神经元</h1><p id="e81a" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">我们从建立一个模拟真实生物神经元的人工神经元开始。这个人工神经元是我们升级的预测器。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj nd"><img src="../Images/a96928913ae9f5f04185f0e2e0555840.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PykgFnPoWS18gZqg.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">An artificial neuron mimicking a biological one</figcaption></figure><p id="b088" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们有一堆输入，而不是单个输入，每个输入都有一个权重(代替一个可配置的参数)。这些修改后的输入被累加并通过触发或<a class="ae lv" href="https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Understanding%20Activation%20Functions%20in%20Neural%20Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)" rel="noopener ugc nofollow">激活功能</a>传递，该功能确定是否应该发送输出。</p><p id="768f" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">那么，为什么会有激活功能呢(除了生物神经元的行为方式相同这一事实之外)？有几个很好的理由，但最重要的一个是激活函数将非线性引入网络。没有激活函数(或线性激活函数)的神经网络基本上只是一个<a class="ae lv" href="https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Introduction%20to%20Linear%20Regression](http://onlinestatbook.com/2/regression/intro.html)" rel="noopener ugc nofollow" target="_blank">线性回归</a>模型，不能完成更复杂的任务，如语言翻译和图像分类。稍后您将看到非线性激活函数是如何实现反向传播的。</p><p id="624f" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">现在，我们将假设使用一个普通的激活函数，<a class="ae lv" href="https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Sigmoid%20function](https://ipfs.io/ipfs/QmXoypizjW3WknFiJnKLwHCnL72vedxjQkDDP1mXWo6uco/wiki/Sigmoid_function.html)" rel="noopener ugc nofollow" target="_blank"> sigmoid函数</a>。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj ne"><img src="../Images/79de1cb17b12100c7b4baab0f24504c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b9t7WYT1SRiPzyip.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">Sigmoid function</figcaption></figure><p id="4bae" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">关于这个函数值得注意的有趣的事情是，输出总是在0和1之间的范围内，但从来没有达到任何一个。</p><h1 id="6a43" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">人工神经网络</h1><p id="13b8" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">就像我们有神经元形成神经网络一样，我们也可以将我们的人工神经元连接起来，形成人工神经网络。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj nf"><img src="../Images/78b03711174af79ae992cfe8d7506ada.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DYzvywpT7f3Jgc6L.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">Artificial neural network with 3 layers</figcaption></figure><p id="9880" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">现在看起来有点复杂了！</p><p id="4caf" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">然而，我们只是将神经元堆叠在不同的层中。所有输入都通过输入层进入，输入层将其输出发送到隐藏层，隐藏层又将其输出发送到最终输出层。虽然来自每个节点的输出是相同的(只有一个输出)，但是到下一层中神经元的连接被不同地加权。例如，隐藏层中第一个节点的输入将是<code class="fe mw mx my mz b">(w11 x i1) + (w21 x i2)</code>。</p><h1 id="366c" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">用矩阵简化</h1><p id="8f12" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">如果我们必须一次计算一个，计算这个网络中的最终输出可能会有点乏味，特别是如果我们有很多神经元。幸运的是，有一个更简单的方法。如果我们将输入和权重表示为矩阵，我们可以使用矩阵运算来简化计算。事实上，我们不再需要做单个神经元的输入求和和输出激活，我们只需一层一层地做。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj ng"><img src="../Images/500c7a1225883209f72f2aef81c517e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yiOFSQBXwmqL_0yx.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">Using matrices</figcaption></figure><p id="fc72" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">正如您将看到的，这将对代码的后续部分有很大帮助。</p><p id="4222" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们使用了<a class="ae lv" href="https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[How%20to%20Multiply%20Matrices](https://www.mathsisfun.com/algebra/matrix-multiplying.html)" rel="noopener ugc nofollow" target="_blank">矩阵点积</a>来处理输入和权重的乘法和求和，但是对于激活函数，我们需要对每个矩阵元素应用sigmoid函数。我们将不得不对每个隐藏层和输出层做同样的操作。</p><h1 id="7292" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">调整重量</h1><p id="9f0d" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">在这个时候，你可能会意识到，我们的神经网络(在概念上)只是神经元的一个更大的版本，因此非常像我们之前的预测器。就像我们的预测器一样，我们希望训练我们的神经网络，通过向它传递输入和已知输出来从它的错误中学习。然后利用已知和实际输出之间的差异(误差),我们改变权重以最小化误差。</p><p id="b04b" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">然而，你可能会意识到，神经网络比我们的预测器要复杂得多。首先，我们有多层排列的多个神经元。因此，虽然我们知道最终目标输出，但我们不知道中间不同层的中间目标输出。第二，虽然我们的预测器是线性的，但是我们的神经元通过一个非线性的激活函数，所以输出是非线性的。那么我们如何改变不同连接的权重呢？</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj nh"><img src="../Images/681afc97a1d0d6817376fb5b124abfa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*22F3tMZKOSlycWr2.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">Weights and outputs in artificial neuron</figcaption></figure><p id="e17e" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们从前面的预测器中知道，我们希望通过改变连接隐藏层和输出层之间的各种输出权重来最小化最终输出误差<code class="fe mw mx my mz b">Ek</code>。</p><p id="25fb" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这很好，但是我们如何通过改变输入变量来最小化一个函数的值呢？</p><p id="1a2f" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">让我们从不同的角度来看这个问题。我们知道最后的输出误差<code class="fe mw mx my mz b">Ek</code>是:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj ni"><img src="../Images/9b850c96abea70d1fbc69250040c5e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/0*WmFJW4bky2W8Jznu.png"/></div></figure><p id="c1f0" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">然而，仅仅从<code class="fe mw mx my mz b">tk</code>中减去<code class="fe mw mx my mz b">ok</code>并不是一个好主意，因为这通常会导致负数。如果我们试图找出网络的最终输出误差，我们实际上是将所有误差相加，因此如果其中一些误差是负数，就会导致错误的最终输出误差。一种常见的解决方案是使用<em class="lx">平方误差</em>，顾名思义就是:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj nj"><img src="../Images/f6ca0eb93d0419de939651f860097c04.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/0*wvJ23YyvrvOTz_6b.png"/></div></figure><p id="16b5" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">同时我们知道:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj nk"><img src="../Images/0bcbe62a3a2e37a454379e7cb0cb1831.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/0*1izFenDfBt5GuPuN.png"/></div></figure><p id="fc62" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">因此，我们知道(粗略地说)，如果我们将<code class="fe mw mx my mz b">Ek</code>与<code class="fe mw mx my mz b">wjk</code>对应起来，我们将得到一系列数值(蓝线)绘制在图表上(实际上这是一个多维图表，但为了保持我们的集体理智，我将使用一个二维图表):</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj nl"><img src="../Images/bd1c5f8035149b13099c1d1e5313b8b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3aUHe15u_oB6v0cF.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">Charting final output error to weights</figcaption></figure><p id="5593" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">如你所见，为了达到最小值<code class="fe mw mx my mz b">Ek</code>,我们沿着梯度或负梯度向下。换句话说，我们试图找到负梯度，相应地改变权重，然后再次找到负梯度，直到我们到达最小点<code class="fe mw mx my mz b">Ek</code>。这个算法叫做<a class="ae lv" href="https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[An%20Introduction%20to%20Gradient%20Descent%20and%20Linear%20Regression](https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/)" rel="noopener ugc nofollow" target="_blank"> <em class="lx">梯度下降</em> </a>。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj nl"><img src="../Images/159907557972cec23b2ceda718117163.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iomP7Ri530lFBgyB.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">Gradient descent</figcaption></figure><p id="6d5b" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">你可能还记得中学微积分，为了找到函数中点的梯度，我们使用<a class="ae lv" href="https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Introduction%20to%20Derivatives](https://www.mathsisfun.com/calculus/derivatives-introduction.html)" rel="noopener ugc nofollow" target="_blank">微分</a>来得到函数的导数。这让我们能够发现我们需要调整<code class="fe mw mx my mz b">wjk</code>到什么程度。为了找到<code class="fe mw mx my mz b">Ek</code>的最小值，我们从<code class="fe mw mx my mz b">wjk</code>中减去这个量，并重复这样做。</p><p id="befd" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">让我们做数学。</p><p id="1682" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">为了计算输出权重<code class="fe mw mx my mz b">wjk</code>所需的变化，我们应该计算最终输出误差<code class="fe mw mx my mz b">Ek</code>相对于输出权重<code class="fe mw mx my mz b">wjk</code>的导数。这意味着:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj nm"><img src="../Images/12abdd1e5a1e647fe961eca4feeade9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/0*hF2GTcduWkv9dOcU.png"/></div></figure><p id="c532" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这很好，但是我们如何使用其他变量得到我们的结果呢？为此我们需要使用<a class="ae lv" href="https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Chain%20rule%20-%20Wikipedia](https://en.wikipedia.org/wiki/Chain_rule)" rel="noopener ugc nofollow" target="_blank"> <em class="lx">链式法则</em> </a>:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj nn"><img src="../Images/c8bb39894ed95b189f8f8eedf15ba864.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/0*jdxvOULt5LaW387v.png"/></div></div></figure><p id="e160" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这看起来稍微好一点，但是我们可以更进一步:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj no"><img src="../Images/c0d242134685246bbfc3e7658d86dbaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/0*qWueHv6vacPS-hpc.png"/></div></div></figure><p id="b33f" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们开始工作吧。首先，我们需要找到<code class="fe mw mx my mz b">Ek</code>相对于最终输出<code class="fe mw mx my mz b">ok</code>的导数。</p><p id="17a6" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">从前面，我们知道<code class="fe mw mx my mz b">Ek</code>是平方误差:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj nj"><img src="../Images/e54e4a653278cf4aa56616fcbe1aa8d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/0*_6rx1yrJUVrqEqKn.png"/></div></figure><p id="a4c0" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">但是为了更好的区分，我们把它缩小了一半(我知道这有点像作弊，但这让我们的生活更轻松):</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj np"><img src="../Images/a2a1c48bacd581415051df08381503b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*WIEz1zuaq-elkucd.png"/></div></figure><p id="e58c" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">它的导数是:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj nq"><img src="../Images/a565e18b46ae166a17202d915da344fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/0*QnlaL_roUQkvv9fk.png"/></div></figure><p id="94fe" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这很简单！让我们看看最终输出<code class="fe mw mx my mz b">ok</code>相对于中间输出和权重<code class="fe mw mx my mz b">sumk</code>的乘积总和的导数。我们知道求和是通过一个sigmoid函数<code class="fe mw mx my mz b">sig</code>来得到最终输出<code class="fe mw mx my mz b">ok</code>:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj nr"><img src="../Images/c461ad5027c1996e93cef29155207350.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/0*CTCgrS4ngSHwVbu1.png"/></div></figure><p id="f9e7" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">因此，最终输出<code class="fe mw mx my mz b">ok</code>相对于总和<code class="fe mw mx my mz b">sumk</code>的导数为:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj ns"><img src="../Images/753310dfb117aa440caa53e7f9883640.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/0*FucrJ-VwEJY9Q_Fq.png"/></div></figure><p id="5e1c" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这是因为我们知道sigmoid的导数是:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj nt"><img src="../Images/84a8d8f5dc4412703bf2f2c176d9da50.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/0*3apvGF-CDHSB0PfE.png"/></div></figure><p id="82b9" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我前面提到过，我们使用sigmoid函数是有充分理由的——简单微分就是其中之一！这一点的证明可以在<a class="ae lv" href="https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[How%20to%20Compute%20the%20Derivative%20of%20a%20Sigmoid%20Function%20(fully%20worked%20example)%20-%20kawahara.ca](http://kawahara.ca/how-to-compute-the-derivative-of-a-sigmoid-function-fully-worked-example/)" rel="noopener ugc nofollow" target="_blank">这里</a>找到。现在既然:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj nr"><img src="../Images/6146a3e14d6742d63d2e815f7453d1a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/0*YkHRbw941hPJXjva.png"/></div></figure><p id="8fc1" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们可以将等式进一步简化为:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj nu"><img src="../Images/ced1a4486d594ecfb6e1c2b0500c1253.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/0*SG_MkraIiAk_owy0.png"/></div></figure><p id="81d6" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">最后，我们想找到总和<code class="fe mw mx my mz b">sumk</code>相对于输出权重<code class="fe mw mx my mz b">wjk</code>的导数。我们知道总和是输出权重<code class="fe mw mx my mz b">wjk</code>和先前输出<code class="fe mw mx my mz b">oj</code>的乘积之和:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj nv"><img src="../Images/acf220b2b79608c4bfa5683868b9cdf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/0*jtY2PLs0eqL3nyol.png"/></div></figure><p id="99c8" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">因此，总和<code class="fe mw mx my mz b">sumk</code>相对于输出权重<code class="fe mw mx my mz b">wjk</code>的导数为:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj nw"><img src="../Images/93b00c9feee55670757a6eb732ccdb88.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/0*88Ff0dFT8sFDBzwa.png"/></div></figure><p id="b863" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">现在我们有了所有的3个导数，让我们把它们放在一起。之前，我们说过:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj no"><img src="../Images/ee00b1ebffabffca3263630d2f477007.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/0*0hoeWA97SlCjE5Kk.png"/></div></div></figure><p id="5fea" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">因此:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj nx"><img src="../Images/3376572ac2b429eed523c24396614c64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/0*JRI6IJ8JGf33Bkvo.png"/></div></figure><p id="5e7b" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这样我们就有了改变输出层权重的公式。隐藏层的权重是多少？我们简单地使用相同的等式，但是后退一层。该算法被称为<a class="ae lv" href="https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Neural%20networks%20and%20deep%20learning](http://neuralnetworksanddeeplearning.com/chap2.html)" rel="noopener ugc nofollow" target="_blank"> <em class="lx">反向传播</em> </a>，因为它从最终输出反向计算权重。</p><p id="9b00" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">但是等等。我们没有隐藏层的目标输出。那么我们如何得到隐藏层的误差呢？我们必须找到另一种方法。</p><h1 id="58fa" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">反向传播误差</h1><p id="1b42" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">仔细想想，输出层的误差是由隐藏层的误差根据前一个隐藏层的连接造成的。换句话说，隐藏层的误差组合形成了输出层的误差。由于权重代表输入的重要性，它也代表误差的贡献。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj nh"><img src="../Images/0bf663a9c56410af726cbf8ec3367baa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xRtvrPQ7IwBSXFff.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">Contribution of errors</figcaption></figure><p id="bbbc" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">因此，我们可以使用重量的比率来计算每个重量的变化。因为分母是常数，我们可以通过去掉分母来进一步简化。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj ny"><img src="../Images/f069b5b81e96404c725b304846ba7b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8ZQaapdHnoZOkygn.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">Back propagating errors</figcaption></figure><p id="2930" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">现在，让我们看看如何使用矩阵从输出层反向传播误差。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj nz"><img src="../Images/51eb03b251e58bcb982ade58d9f93d2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wUKWTApQri0QVdR8.png"/></div></div></figure><p id="32f0" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">一旦我们有了隐藏层的误差，我们就可以使用和以前一样的等式，但是用隐藏的输出误差代替最终的输出误差。</p><h1 id="5821" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">学习率</h1><p id="d7bb" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">因此，人工神经网络通过使用梯度下降的反向传播进行学习。在梯度下降迭代过程中，经常很容易超调，这导致移动太快并跨过最小值<code class="fe mw mx my mz b">wjk</code>。为了防止这种情况，我们使用一个<em class="lx">学习率</em> <code class="fe mw mx my mz b">l</code>来缩小我们想要为权重改变的量。这导致我们之前等式的改变:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj oa"><img src="../Images/56eb671901a558f9c6a33ff166589549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/0*SW4Aq_724cGt7WpY.png"/></div></figure><p id="5b19" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated"><code class="fe mw mx my mz b">l</code>通常是一个很小的值，因此我们在超过最小值时会更加小心，但也不能太小，否则训练时间会很长。有很多关于设定最佳学习率的研究文献。</p><h1 id="f2d3" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">偏见</h1><p id="3d81" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">对于我们当前的神经网络，激活函数是在0.5处穿过<code class="fe mw mx my mz b">y</code>的s形曲线。对权重的任何改变仅仅改变了s形的陡度。因此，神经元的触发方式是有限制的。例如，当<code class="fe mw mx my mz b">x</code>为2时，让sigmoid返回0.1的低值是不可能的。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj ob"><img src="../Images/35677921d0dce781a2e668cf3c45497c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SV2ksPYGzDO1ZUB_.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">Sigmoid functions without bias</figcaption></figure><p id="5876" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">然而，如果我们给<code class="fe mw mx my mz b">x</code>加上一个<a class="ae lv" href="https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Make%20Your%20Own%20Neural%20Network:%20Bias%20Nodes%20in%20Neural%20Networks](http://makeyourownneuralnetwork.blogspot.sg/2016/06/bias-nodes-in-neural-networks.html)" rel="noopener ugc nofollow" target="_blank"> <em class="lx">偏置</em> </a>值，事情就完全变了。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj oc"><img src="../Images/2c93e247b0bd2d57314c8cc5cdfc8a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qMA9InPhDNfFhyI0.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">Sigmoid functions with bias</figcaption></figure><p id="2074" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们如何做到这一点是通过在神经网络中添加一个叫做<em class="lx">偏置神经元</em>的东西。这个偏置神经元总是输出1.0，并且被添加到一个层，但是没有任何输入。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj od"><img src="../Images/8a87fba71c3ff5e478b470d8fa76ac11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MKQUoDy14k2ER1vS.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">Artificial neural network with bias</figcaption></figure><p id="6151" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">不是所有的神经网络都需要偏向神经元。在我们稍后要写的简单神经网络中，我们不会使用任何偏向神经元(并且它工作得相当好)。</p><h1 id="a153" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">终于有代码了！</h1><p id="e91a" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">所以我们终于来了！在所有的概念和数学之后，我们现在要开始一些实现！</p><p id="2c5b" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这篇文章中的代码片段并不完整，所以不要简单地从这里剪切和粘贴来运行它。这里的所有代码都可以在这个Github资源库中找到:</p><p id="3386" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated"><a class="ae lv" href="https://github.com/sausheong/gonn" rel="noopener ugc nofollow" target="_blank">https://github.com/sausheong/gonn</a></p><p id="9d25" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">与Python不同，Go目前在机器学习的库方面没有太多支持。然而有一个非常有用的库叫做<a class="ae lv" href="https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Gonum](https://www.gonum.org/)" rel="noopener ugc nofollow" target="_blank"> Gonum </a>，它提供了我们最需要的东西——矩阵操作。</p><p id="3407" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">此外，虽然Gonum有非常好的包，但我认为Gonum中的一些怪癖使它变得不必要的冗长，所以我创建了自己的助手函数来克服它。</p><h1 id="d452" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">矩阵助手</h1><p id="1f0b" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">我们将首先从助手函数开始。Gonum用于矩阵操作的主包叫做<code class="fe mw mx my mz b">mat</code>。我们将使用的主要是<code class="fe mw mx my mz b">mat.Matrix</code>接口及其实现<code class="fe mw mx my mz b">mat.Dense</code>。</p><p id="c629" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated"><code class="fe mw mx my mz b">mat</code>包有一个怪癖，它要求我们在对矩阵执行操作之前，先创建一个包含正确行和列的新矩阵。对多个操作这样做相当烦人，所以我用自己的函数包装了每个函数。</p><p id="ae16" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">例如，Gonum <code class="fe mw mx my mz b">Product</code>函数允许我们对两个矩阵执行点积运算，我创建了一个助手函数，它找出矩阵的大小，创建它并在返回结果矩阵之前执行运算。</p><p id="a72e" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这有助于节省大约1-3行代码，具体取决于操作。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="21e3" class="oi lz iu mz b gz oj ok l ol om">func dot(m, n mat.Matrix) mat.Matrix {<br/>	r, _ := m.Dims()<br/>	_, c := n.Dims()<br/>	o := mat.NewDense(r, c, nil)<br/>	o.Product(m, n)<br/>	return o<br/>}</span></pre><p id="fbb9" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated"><code class="fe mw mx my mz b">apply</code>功能允许我们对矩阵应用函数。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="1958" class="oi lz iu mz b gz oj ok l ol om">func apply(fn func(i, j int, v float64) float64, m mat.Matrix) mat.Matrix {<br/>	r, c := m.Dims()<br/>	o := mat.NewDense(r, c, nil)<br/>	o.Apply(fn, m)<br/>	return o<br/>}</span></pre><p id="4742" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated"><code class="fe mw mx my mz b">scale</code>功能允许我们缩放矩阵，即将矩阵乘以标量。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="d5a8" class="oi lz iu mz b gz oj ok l ol om">func scale(s float64, m mat.Matrix) mat.Matrix {<br/>	r, c := m.Dims()<br/>	o := mat.NewDense(r, c, nil)<br/>	o.Scale(s, m)<br/>	return o<br/>}</span></pre><p id="9b83" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated"><code class="fe mw mx my mz b">multiply</code>函数将2个函数相乘(这不同于点积`)。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="437e" class="oi lz iu mz b gz oj ok l ol om">func multiply(m, n mat.Matrix) mat.Matrix {<br/>	r, c := m.Dims()<br/>	o := mat.NewDense(r, c, nil)<br/>	o.MulElem(m, n)<br/>	return o<br/>}</span></pre><p id="30b6" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated"><code class="fe mw mx my mz b">add</code>和<code class="fe mw mx my mz b">subtract</code>功能允许增加或减少一个功能。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="297d" class="oi lz iu mz b gz oj ok l ol om">func add(m, n mat.Matrix) mat.Matrix {<br/>	r, c := m.Dims()<br/>	o := mat.NewDense(r, c, nil)<br/>	o.Add(m, n)<br/>	return o<br/>}</span><span id="e4a4" class="oi lz iu mz b gz on ok l ol om">func subtract(m, n mat.Matrix) mat.Matrix {<br/>	r, c := m.Dims()<br/>	o := mat.NewDense(r, c, nil)<br/>	o.Sub(m, n)<br/>	return o<br/>}</span></pre><p id="13ad" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">最后，<code class="fe mw mx my mz b">addScalar</code>函数允许我们向矩阵中的每个元素添加一个标量值。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="8a0c" class="oi lz iu mz b gz oj ok l ol om">func addScalar(i float64, m mat.Matrix) mat.Matrix {<br/>	r, c := m.Dims()<br/>	a := make([]float64, r*c)<br/>	for x := 0; x &lt; r*c; x++ {<br/>		a[x] = i<br/>	}<br/>	n := mat.NewDense(r, c, a)<br/>	return add(m, n)<br/>}</span></pre><h1 id="1967" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">神经网络</h1><p id="d3c6" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">开始了。</p><p id="0ddf" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们将创建一个非常简单的3层前馈神经网络(也称为多层感知器)。我们从定义网络开始:</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="31cc" class="oi lz iu mz b gz oj ok l ol om">type Network struct {<br/>	inputs        int<br/>	hiddens       int<br/>	outputs       int<br/>	hiddenWeights *mat.Dense<br/>	outputWeights *mat.Dense<br/>	learningRate  float64<br/>}</span></pre><p id="6a58" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">字段<code class="fe mw mx my mz b">inputs</code>、<code class="fe mw mx my mz b">hiddens</code>和<code class="fe mw mx my mz b">output</code>定义了每个输入、隐藏和输出层中的神经元数量(记住，这是一个3层网络)。<code class="fe mw mx my mz b">hiddenWeights</code>和<code class="fe mw mx my mz b">outputWeights</code>字段是矩阵，分别表示从输入层到隐藏层以及从隐藏层到输出层的权重。最后，学习率是网络的学习率。</p><p id="b7b7" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">接下来，我们有一个简单的方法来创建神经网络。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="70bb" class="oi lz iu mz b gz oj ok l ol om">func CreateNetwork(input, hidden, output int, rate float64) (net Network) {<br/>	net = Network{<br/>		inputs:       input,<br/>		hiddens:      hidden,<br/>		outputs:      output,<br/>		learningRate: rate,<br/>	}<br/>	net.hiddenWeights = mat.NewDense(net.hiddens, net.inputs, randomArray(net.inputs*net.hiddens, float64(net.inputs)))<br/>	net.outputWeights = mat.NewDense(net.outputs, net.hiddens, randomArray(net.hiddens*net.outputs, float64(net.hiddens)))<br/>	return<br/>}</span></pre><p id="6718" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">输入、隐藏和输出神经元的数量以及学习速率从调用者传入以创建网络。然而，隐藏和输出权重是随机创建的。</p><p id="ddd4" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">如果您还记得上面的内容，我们创建的权重是一个矩阵，其列数由来自层的<em class="lx">表示，行数由<em class="lx">到</em>层表示。这是因为权重中的行数必须与<em class="lx">到</em>层中的神经元数量相同，列数必须与</em>层中的<em class="lx">的神经元数量相同(以便与</em>层中的<em class="lx">的输出相乘)。花点时间再看看下面的图表——它会更有意义。</em></p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj oo"><img src="../Images/c813d8dc9b5503763f5fa3bbfb7a325d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Jqs5kGvqtVfOLRxS.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">Neural network and matrices</figcaption></figure><p id="3700" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">用一组随机数字初始化权重是一个重要的参数。为此，我们将使用函数<code class="fe mw mx my mz b">randomArray</code>来创建float64的随机数组。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="cb8c" class="oi lz iu mz b gz oj ok l ol om">func randomArray(size int, v float64) (data []float64) {<br/>	dist := distuv.Uniform{<br/>		Min: -1 / math.Sqrt(v),<br/>		Max: 1 / math.Sqrt(v),<br/>	}</span><span id="3340" class="oi lz iu mz b gz on ok l ol om">	data = make([]float64, size)<br/>	for i := 0; i &lt; size; i++ {<br/>		data[i] = dist.Rand()<br/>	}<br/>	return<br/>}</span></pre><p id="e31d" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated"><code class="fe mw mx my mz b">randomArray</code>函数使用Gonum中的<code class="fe mw mx my mz b">distuv</code>包在<code class="fe mw mx my mz b">-1/sqrt(v)</code>和<code class="fe mw mx my mz b">1/sqrt(v)</code>之间创建一组均匀分布的值，其中<code class="fe mw mx my mz b">v</code>是来自层的<em class="lx">的大小。这是一个相当常用的分布。</em></p><p id="c48b" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">现在我们有了神经网络，我们可以要求它做的两个主要功能是用一组训练数据训练自己，或者根据一组测试数据预测值。</p><p id="6a82" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">从我们之前的努力工作中，我们知道预测意味着通过网络的前向传播，而训练意味着首先是前向传播，然后是后向传播，以使用一些训练数据来改变权重。</p><p id="266f" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">因为训练和预测都需要前向传播，所以让我们先从它开始。我们定义了一个名为<code class="fe mw mx my mz b">Predict</code>的函数，使用训练好的神经网络来预测这些值。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="2b31" class="oi lz iu mz b gz oj ok l ol om">func (net Network) Predict(inputData []float64) mat.Matrix {<br/>	// forward propagation<br/>	inputs := mat.NewDense(len(inputData), 1, inputData)<br/>	hiddenInputs := dot(net.hiddenWeights, inputs)<br/>	hiddenOutputs := apply(sigmoid, hiddenInputs)<br/>	finalInputs := dot(net.outputWeights, hiddenOutputs)<br/>	finalOutputs := apply(sigmoid, finalInputs)<br/>	return finalOutputs<br/>}</span></pre><p id="0264" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们首先从输入开始，通过创建一个名为<code class="fe mw mx my mz b">inputs</code>的矩阵来表示输入值。接下来，我们通过应用隐藏权重和输入之间的点积来找到隐藏层的输入，创建一个名为<code class="fe mw mx my mz b">hiddenInputs</code>的矩阵。换句话说，给定一个2神经元输入层和一个3神经元隐藏层，我们得到的是:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj ng"><img src="../Images/68a3620ae7ccdb74ee1cc97707b682e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*94bxio1r477zXmOk.png"/></div></div></figure><p id="7a95" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">接下来，我们将激活函数<code class="fe mw mx my mz b">sigmoid</code>应用于隐藏输入，以产生<code class="fe mw mx my mz b">hiddenOutputs</code>。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="f985" class="oi lz iu mz b gz oj ok l ol om">func sigmoid(r, c int, z float64) float64 {<br/>	return 1.0 / (1 + math.Exp(-1*z))<br/>}</span></pre><p id="0808" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们对最终输入和最终输出重复这两个动作，分别产生<code class="fe mw mx my mz b">finalInputs</code>和<code class="fe mw mx my mz b">finalOutputs</code>，预测就是最终输出。</p><p id="ee48" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这就是我们如何使用正向传播算法进行预测。让我们看看我们在训练中是如何进行前向和后向传播的。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="da8b" class="oi lz iu mz b gz oj ok l ol om">func (net *Network) Train(inputData []float64, targetData []float64) {<br/>	// forward propagation<br/>	inputs := mat.NewDense(len(inputData), 1, inputData)<br/>	hiddenInputs := dot(net.hiddenWeights, inputs)<br/>	hiddenOutputs := apply(sigmoid, hiddenInputs)<br/>	finalInputs := dot(net.outputWeights, hiddenOutputs)<br/>	finalOutputs := apply(sigmoid, finalInputs)</span><span id="b26d" class="oi lz iu mz b gz on ok l ol om">	// find errors<br/>	targets := mat.NewDense(len(targetData), 1, targetData)<br/>	outputErrors := subtract(targets, finalOutputs)<br/>	hiddenErrors := dot(net.outputWeights.T(), outputErrors)</span><span id="370a" class="oi lz iu mz b gz on ok l ol om">	// backpropagate<br/>	net.outputWeights = add(net.outputWeights,<br/>		scale(net.learningRate,<br/>			dot(multiply(outputErrors, sigmoidPrime(finalOutputs)),<br/>				hiddenOutputs.T()))).(*mat.Dense)<br/>	<br/>	net.hiddenWeights = add(net.hiddenWeights,<br/>		scale(net.learningRate,<br/>			dot(multiply(hiddenErrors, sigmoidPrime(hiddenOutputs)),<br/>				inputs.T()))).(*mat.Dense)<br/>}</span></pre><p id="19c7" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">正向传播部分与<code class="fe mw mx my mz b">Predict</code>功能完全相同。我们在这里没有调用<code class="fe mw mx my mz b">Predict</code>,因为我们仍然需要其他的中间值。</p><p id="b8f4" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">获得最终输出后，我们需要做的第一件事是确定输出误差。这相对简单，我们简单地从最终输出中减去我们的目标数据，得到<code class="fe mw mx my mz b">outputErrors</code>:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj ni"><img src="../Images/3194dbfafca0ca04a23512d254fe0b75.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/0*GXcej3jHJUWUJNz1.png"/></div></figure><p id="125c" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">隐藏层隐藏的错误有点棘手。还记得这个吗？</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj nz"><img src="../Images/f39055f769a4c7565b09fb724be06763.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7m89W4-fGReXrHKr.png"/></div></div></figure><p id="22f3" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们使用反向传播通过在输出权重和输出误差的转置上应用点积来计算隐藏误差。这会给我们<code class="fe mw mx my mz b">hiddenErrors</code>。</p><p id="834a" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">现在我们有了误差，我们简单地使用我们之前推导的公式(包括学习率)来改变我们需要做的权重:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj oa"><img src="../Images/d939a29b1fef02d9bd9c3e9b231e6e77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/0*dBGOU2EB2dg5sql2.png"/></div></figure><p id="0ec5" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">请记住，我们是从重量中减去这个数字。因为这是一个负数，我们最后把它加到权重上，这就是我们所做的。</p><p id="4888" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">为了简化计算，我们使用了一个<code class="fe mw mx my mz b">sigmoidPrime</code>函数，它无非是做<code class="fe mw mx my mz b">sigP = sig(1 - sig)</code>:</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="1034" class="oi lz iu mz b gz oj ok l ol om">func sigmoidPrime(m mat.Matrix) mat.Matrix {<br/>	rows, _ := m.Dims()<br/>	o := make([]float64, rows)<br/>	for i := range o {<br/>		o[i] = 1<br/>	}<br/>	ones := mat.NewDense(rows, 1, o)<br/>	return multiply(m, subtract(ones, m)) // m * (1 - m)<br/>}</span></pre><p id="d3a9" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">你可能还会看到，我们正在做前一个输出转置的点积——这是因为我们在跨层相乘。</p><p id="ce69" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">最后，我们这样做两次，为我们的神经网络获得新的隐藏和输出权重。</p><p id="1d90" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这就是对<code class="fe mw mx my mz b">Train</code>函数的总结。</p><h1 id="c411" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">保存培训结果</h1><p id="4f04" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">在我们继续使用神经网络之前，我们将看看如何保存我们的训练结果并加载它以供以后使用。我们当然不希望每次想做预测时都要从头开始训练——训练网络通常需要很长时间。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="f06d" class="oi lz iu mz b gz oj ok l ol om">func save(net Network) {<br/>	h, err := os.Create("data/hweights.model")<br/>	defer h.Close()<br/>	if err == nil {<br/>		net.hiddenWeights.MarshalBinaryTo(h)<br/>	}<br/>	o, err := os.Create("data/oweights.model")<br/>	defer o.Close()<br/>	if err == nil {<br/>		net.outputWeights.MarshalBinaryTo(o)<br/>	}<br/>}</span><span id="e9f4" class="oi lz iu mz b gz on ok l ol om">// load a neural network from file<br/>func load(net *Network) {<br/>	h, err := os.Open("data/hweights.model")<br/>	defer h.Close()<br/>	if err == nil {<br/>		net.hiddenWeights.Reset()<br/>		net.hiddenWeights.UnmarshalBinaryFrom(h)<br/>	}<br/>	o, err := os.Open("data/oweights.model")<br/>	defer o.Close()<br/>	if err == nil {<br/>		net.outputWeights.Reset()<br/>		net.outputWeights.UnmarshalBinaryFrom(o)<br/>	}<br/>	return<br/>}</span></pre><p id="5951" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated"><code class="fe mw mx my mz b">save</code>和<code class="fe mw mx my mz b">load</code>函数是彼此的镜像，我们使用Gonum <code class="fe mw mx my mz b">mat</code>包中的一个方便的函数将权重矩阵编组为二进制形式，并将相同的形式解组回矩阵。这很普通——唯一值得注意的是，当我们从二进制数据解组回权重矩阵时，我们需要首先将矩阵重置为零值，以便可以重用。</p><h1 id="42f8" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">使用我们的神经网络</h1><p id="ab11" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">我们终于来了——使用神经网络！</p><h1 id="cc50" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">MNIST手写识别</h1><p id="bd48" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">让我们从机器学习的“hello world”开始——使用MNIST数据集来识别手写数字。MNIST数据集是一组用于训练的60，000个扫描的手写数字图像和用于测试的10，000个类似图像。它是NIST(国家标准与技术研究所)的一个更大的集合的子集，已经过大小标准化和居中。这些图像是黑白的，28 x 28像素。原始数据集以一种更难处理格式存储，所以人们想出了更简单的CSV格式的数据集，这就是我们正在使用的。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj op"><img src="../Images/3cf22f17b42e357cb88951f7d1aed961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-6NYGeFpQsHtS8XH.png"/></div></div><figcaption class="lr ls gk gi gj lt lu bd b be z dk">MNIST dataset</figcaption></figure><p id="c3cc" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">在CSV格式中，每一行都是一幅图像，除了第一列以外的每一列都代表一个像素。第一列是标签，这是图像应该表示的实际数字。换句话说，这就是目标输出。由于有28 x 28个像素，这意味着每行有785列。</p><p id="6c2a" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">让我们从培训开始。我们创建一个名为<code class="fe mw mx my mz b">mnistTrain</code>的函数，它接收一个神经网络，并使用它来训练MNIST数据集:</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="c80d" class="oi lz iu mz b gz oj ok l ol om">func mnistTrain(net *Network) {<br/>	rand.Seed(time.Now().UTC().UnixNano())<br/>	t1 := time.Now()</span><span id="d968" class="oi lz iu mz b gz on ok l ol om">	for epochs := 0; epochs &lt; 5; epochs++ {<br/>		testFile, _ := os.Open("mnist_dataset/mnist_train.csv")<br/>		r := csv.NewReader(bufio.NewReader(testFile))<br/>		for {<br/>			record, err := r.Read()<br/>			if err == io.EOF {<br/>				break<br/>			}</span><span id="fee0" class="oi lz iu mz b gz on ok l ol om">			inputs := make([]float64, net.inputs)<br/>			for i := range inputs {<br/>				x, _ := strconv.ParseFloat(record[i], 64)<br/>				inputs[i] = (x / 255.0 * 0.99) + 0.01<br/>			}</span><span id="fff6" class="oi lz iu mz b gz on ok l ol om">			targets := make([]float64, 10)<br/>			for i := range targets {<br/>				targets[i] = 0.01<br/>			}<br/>			x, _ := strconv.Atoi(record[0])<br/>			targets[x] = 0.99</span><span id="e91e" class="oi lz iu mz b gz on ok l ol om">			net.Train(inputs, targets)<br/>		}<br/>		testFile.Close()<br/>	}<br/>	elapsed := time.Since(t1)<br/>	fmt.Printf("\nTime taken to train: %s\n", elapsed)<br/>}</span></pre><p id="1de5" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们打开CSV文件并读取每条记录，然后处理每条记录。对于我们读入的每条记录，我们创建一个表示输入的数组和一个表示目标的数组。</p><p id="3f29" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">对于<code class="fe mw mx my mz b">inputs</code>数组，我们从记录中取出每个像素，并将其转换为0.0到1.0之间的值，0.0表示没有值的像素，1.0表示完整的像素。</p><p id="3f80" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">对于<code class="fe mw mx my mz b">targets</code>数组，数组的每个元素代表索引成为目标数字的概率。例如，如果目标数字是3，那么第4个元素<code class="fe mw mx my mz b">targets[3]</code>将具有0.99的概率，而其余的将具有0.01的概率。</p><p id="950e" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">一旦我们有了输入和目标，我们就调用网络的<code class="fe mw mx my mz b">Train</code>函数，并将输入和目标传递给它。</p><p id="45e2" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">你可能会注意到我们在“时代”中运行这个。基本上，我们所做的是运行多次，因为我们运行训练的次数越多，神经网络的训练就越好。然而，如果我们过度训练它，网络将<em class="lx">过度适应</em>，这意味着它将很好地适应训练数据，最终将在处理它以前从未见过的数据时表现不佳。</p><p id="9148" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">预测手写图像基本上是同样的事情，除了我们只使用输入来调用<code class="fe mw mx my mz b">Predict</code>函数。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="7c0d" class="oi lz iu mz b gz oj ok l ol om">func mnistPredict(net *Network) {<br/>	t1 := time.Now()<br/>	checkFile, _ := os.Open("mnist_dataset/mnist_test.csv")<br/>	defer checkFile.Close()</span><span id="d6c8" class="oi lz iu mz b gz on ok l ol om">	score := 0<br/>	r := csv.NewReader(bufio.NewReader(checkFile))<br/>	for {<br/>		record, err := r.Read()<br/>		if err == io.EOF {<br/>			break<br/>		}<br/>		inputs := make([]float64, net.inputs)<br/>		for i := range inputs {<br/>			if i == 0 {<br/>				inputs[i] = 1.0<br/>			}<br/>			x, _ := strconv.ParseFloat(record[i], 64)<br/>			inputs[i] = (x / 255.0 * 0.99) + 0.01<br/>		}<br/>		outputs := net.Predict(inputs)<br/>		best := 0<br/>		highest := 0.0<br/>		for i := 0; i &lt; net.outputs; i++ {<br/>			if outputs.At(i, 0) &gt; highest {<br/>				best = i<br/>				highest = outputs.At(i, 0)<br/>			}<br/>		}<br/>		target, _ := strconv.Atoi(record[0])<br/>		if best == target {<br/>			score++<br/>		}<br/>	}</span><span id="10f2" class="oi lz iu mz b gz on ok l ol om">	elapsed := time.Since(t1)<br/>	fmt.Printf("Time taken to check: %s\n", elapsed)<br/>	fmt.Println("score:", score)<br/>}</span></pre><p id="04e1" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们得到的结果是一组概率。我们找出概率最高的元素，该数字应该是该元素的索引。如果是的话，我们就认为这是一场胜利。胜利的最终计数是我们的最终得分。</p><p id="9f62" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">因为我们有10，000张测试图像，如果我们能够准确地检测到所有这些图像，那么我们将拥有100%的准确性。我们来看一下<code class="fe mw mx my mz b">main</code>函数:</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="afa8" class="oi lz iu mz b gz oj ok l ol om">func main() {<br/>	// 784 inputs - 28 x 28 pixels, each pixel is an input<br/>	// 200 hidden neurons - an arbitrary number<br/>	// 10 outputs - digits 0 to 9<br/>	// 0.1 is the learning rate<br/>	net := CreateNetwork(784, 200, 10, 0.1)</span><span id="b4b6" class="oi lz iu mz b gz on ok l ol om">	mnist := flag.String("mnist", "", "Either train or predict to evaluate neural network")<br/>	flag.Parse()</span><span id="7ddc" class="oi lz iu mz b gz on ok l ol om">	// train or mass predict to determine the effectiveness of the trained network<br/>	switch *mnist {<br/>	case "train":<br/>		mnistTrain(&amp;net)<br/>		save(net)<br/>	case "predict":<br/>		load(&amp;net)<br/>		mnistPredict(&amp;net)<br/>	default:<br/>		// don't do anything<br/>	}<br/>}</span></pre><p id="0b8b" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">这非常简单，我们首先创建一个神经网络，在输入层有784个神经元(每个像素是一个输入)，在隐藏层有200个神经元，在输出层有10个神经元，每个神经元对应一个数字。</p><p id="8f80" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">然后用MNIST训练集训练网络，用测试集预测图像。这是我测试时得到的结果:</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div class="gi gj oq"><img src="../Images/b6e22a5a0492ed6344015879a1a31add.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/0*PSl5UM0woI3VluZS.png"/></div></figure><p id="1612" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">用60，000幅图像和5个时期训练网络需要8分钟，用10，000幅图像测试需要4.4秒。结果是9772幅图像被正确预测，准确率为97.72%！</p><h1 id="edd2" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">预测单个文件</h1><p id="1417" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">现在，我们已经测试了我们的网络，让我们看看如何使用它对个别图像。</p><p id="290c" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">首先，我们从PNG文件中获取数据。为此，我们创建了一个<code class="fe mw mx my mz b">dataFromImage</code>函数。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="37ca" class="oi lz iu mz b gz oj ok l ol om">func dataFromImage(filePath string) (pixels []float64) {<br/>	// read the file<br/>	imgFile, err := os.Open(filePath)<br/>	defer imgFile.Close()<br/>	if err != nil {<br/>		fmt.Println("Cannot read file:", err)<br/>	}<br/>	img, err := png.Decode(imgFile)<br/>	if err != nil {<br/>		fmt.Println("Cannot decode file:", err)<br/>	}</span><span id="d847" class="oi lz iu mz b gz on ok l ol om">	// create a grayscale image<br/>	bounds := img.Bounds()<br/>	gray := image.NewGray(bounds)</span><span id="255d" class="oi lz iu mz b gz on ok l ol om">	for x := 0; x &lt; bounds.Max.X; x++ {<br/>		for y := 0; y &lt; bounds.Max.Y; y++ {<br/>			var rgba = img.At(x, y)<br/>			gray.Set(x, y, rgba)<br/>		}<br/>	}<br/>	// make a pixel array<br/>	pixels = make([]float64, len(gray.Pix))<br/>	// populate the pixel array subtract Pix from 255 because <br/>	// that's how the MNIST database was trained (in reverse)<br/>	for i := 0; i &lt; len(gray.Pix); i++ {<br/>		pixels[i] = (float64(255-gray.Pix[i]) / 255.0 * 0.99) + 0.01<br/>	}<br/>	return<br/>}</span></pre><p id="9078" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">图像中的每个像素代表一个值，但我们不能使用普通的RGBA，而是需要一个<code class="fe mw mx my mz b">image.Gray</code>。从<code class="fe mw mx my mz b">image.Gray</code>结构中，我们得到了<code class="fe mw mx my mz b">Pix</code>值，并将其转换为<code class="fe mw mx my mz b">float64</code>值。MNIST图像是黑底白字，所以我们需要从255减去每个像素值。</p><p id="efd3" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">一旦我们有了像素阵列，事情就很简单了。我们使用一个<code class="fe mw mx my mz b">predictFromImage</code>函数，它接收神经网络并从图像文件中预测数字。结果是一个概率数组，其中索引是数字。我们需要做的是找到索引并返回它。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="97d9" class="oi lz iu mz b gz oj ok l ol om">func predictFromImage(net Network, path string) int {<br/>	input := dataFromImage(path)<br/>	output := net.Predict(input)<br/>	matrixPrint(output)<br/>	best := 0<br/>	highest := 0.0<br/>	for i := 0; i &lt; net.outputs; i++ {<br/>		if output.At(i, 0) &gt; highest {<br/>			best = i<br/>			highest = output.At(i, 0)<br/>		}<br/>	}<br/>	return best<br/>}</span></pre><p id="2cdd" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">最后，通过<code class="fe mw mx my mz b">main</code>函数，我们打印图像并从图像中预测数字。</p><pre class="lg lh li lj gu oe mz of og aw oh bi"><span id="1b6d" class="oi lz iu mz b gz oj ok l ol om">func main() {<br/>	// 784 inputs - 28 x 28 pixels, each pixel is an input<br/>	// 100 hidden nodes - an arbitrary number<br/>	// 10 outputs - digits 0 to 9<br/>	// 0.1 is the learning rate<br/>	net := CreateNetwork(784, 200, 10, 0.1)</span><span id="6e5f" class="oi lz iu mz b gz on ok l ol om">	mnist := flag.String("mnist", "", "Either train or predict to evaluate neural network")<br/>	file := flag.String("file", "", "File name of 28 x 28 PNG file to evaluate")<br/>	flag.Parse()</span><span id="948b" class="oi lz iu mz b gz on ok l ol om">	// train or mass predict to determine the effectiveness of the trained network<br/>	switch *mnist {<br/>	case "train":<br/>		mnistTrain(&amp;net)<br/>		save(net)<br/>	case "predict":<br/>		load(&amp;net)<br/>		mnistPredict(&amp;net)<br/>	default:<br/>		// don't do anything<br/>	}</span><span id="9f2b" class="oi lz iu mz b gz on ok l ol om">	// predict individual digit images<br/>	if *file != "" {<br/>		// print the image out nicely on the terminal<br/>		printImage(getImage(*file))<br/>		// load the neural network from file<br/>		load(&amp;net)<br/>		// predict which number it is<br/>		fmt.Println("prediction:", predictFromImage(net, *file))<br/>	}<br/>}</span></pre><p id="1bab" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">假设网络已经被训练过，这就是我们得到的。</p><figure class="lg lh li lj gu lk gi gj paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gi gj or"><img src="../Images/6c8875f3f1d6764b2fe93e786bf823ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*S-n_f86W_0MDNtHK.png"/></div></div></figure><p id="a4a1" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">就这样，我们用Go从头开始编写了一个简单的3层前馈神经网络！</p><h1 id="64e4" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">参考</h1><p id="9eb1" class="pw-post-body-paragraph kj kk iu kl b km mq jv ko kp mr jy kr ks ms ku kv kw mt ky kz la mu lc ld le in bi translated">这里是我写这篇文章和代码时参考的一些资料。</p><ul class=""><li id="ac3f" class="os ot iu kl b km kn kp kq ks ou kw ov la ow le ox oy oz pa bi translated">塔里克·拉希德的<a class="ae lv" href="https://www.amazon.com/Make-Your-Own-Neural-Network-ebook/dp/B01EER4Z4G" rel="noopener ugc nofollow" target="_blank">制作你自己的神经网络</a>是一本学习神经网络基础知识的好书，其简单的解释风格</li><li id="e32b" class="os ot iu kl b km pb kp pc ks pd kw pe la pf le ox oy oz pa bi translated">迈克尔·尼尔森的<a class="ae lv" href="http://neuralnetworksanddeeplearning.com/" rel="noopener ugc nofollow" target="_blank">神经网络和深度学习</a>免费在线书籍是学习构建神经网络复杂性的另一个惊人资源</li><li id="5027" class="os ot iu kl b km pb kp pc ks pd kw pe la pf le ox oy oz pa bi translated">丹尼尔·怀特纳克写了一本关于用围棋进行机器学习的书，他关于T2在围棋中从头开始构建神经网络的帖子很有教育意义</li><li id="7a44" class="os ot iu kl b km pb kp pc ks pd kw pe la pf le ox oy oz pa bi translated">Ujjwal Karn的数据科学博客有一篇关于神经网络的很好的<a class="ae lv" href="https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/" rel="noopener ugc nofollow" target="_blank">介绍文章</a></li></ul></div></div>    
</body>
</html>