<html>
<head>
<title>Single-layer Perceptron in Pharo</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pharo中的单层感知器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/single-layer-perceptron-in-pharo-5b13246a041d?source=collection_archive---------2-----------------------#2017-04-04">https://towardsdatascience.com/single-layer-perceptron-in-pharo-5b13246a041d?source=collection_archive---------2-----------------------#2017-04-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="233a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">面向对象的神经网络方法</h2></div><p id="188a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我将描述我在Pharo中实现的单层感知器。它将支持多类分类(一个或多个神经元)。每个神经元将被实现为一个对象。这个项目的代码可以从Smalltalkhub使用这个Metacello脚本获得(在你的Pharo图像的操场上做):</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="b439" class="lk ll iq lg b gy lm ln l lo lp">Metacello new <br/>repository: '<a class="ae lq" href="http://smalltalkhub.com/mc/Oleks/NeuralNetwork/main'" rel="noopener ugc nofollow" target="_blank">http://smalltalkhub.com/mc/Oleks/NeuralNetwork/main'</a>;<br/>configuration: 'MLNeuralNetwork';<br/>version: #development;<br/>load.</span></pre><p id="b382" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我将从说明设计问题和实现这个项目每个部分的不同方法开始。这将是相当长的，所以这是我的最终设计:</p><figure class="lb lc ld le gt ls gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/05036c7a77a8edbe39899a25c6cff6de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*um8DWFwMH0sejMGNpmEVjA.png"/></div></figure><h1 id="0299" class="lv ll iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">什么是感知器？</h1><p id="7ee7" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">首先，我们需要定义一个感知器。它是人工神经网络的最基本形式，然而，大多数人无法清楚地定义它实际上是什么。</p><p id="b5a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我将感知器称为遵循感知器学习过程的人工神经网络。</p><p id="e175" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个定义暗示了对感知机是什么以及它们能做什么的一些限制。</p><h2 id="42eb" class="lk ll iq bd lw mr ms dn ma mt mu dp me ko mv mw mg ks mx my mi kw mz na mk nb bi translated">感知器的限制</h2><ul class=""><li id="1c65" class="nc nd iq kh b ki mm kl mn ko ne ks nf kw ng la nh ni nj nk bi translated">它们只能收敛于线性可分的输入(参见XOR问题，Minski &amp; Pappet)。但是，如果输入是线性可分的，那么无论初始权重和学习速率如何，感知器都保证收敛于此(参见1962年由Block和Novikoff证明的感知器收敛定理)</li><li id="ca26" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nh ni nj nk bi translated">感知器(如上定义)只能有一层神经元。事情是这样的(Geoffrey Hinton的机器学习神经网络课程的第3周)，感知器学习程序只能应用于单层神经元。隐藏层中的神经元需要某种反馈来计算它们的误差并更新权重。这就是为什么我们需要一个不同的学习算法(例如，反向传播，将在下一阶段实现)。</li><li id="47ec" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nh ni nj nk bi translated">感知器只能在线学习(一次一个例子)。这是因为感知器学习是基于将输入向量与基于二进制分类器误差(该误差可以是-1、0或1)的权重相加(或相减)。</li></ul><h1 id="e1df" class="lv ll iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">设计问题</h1><h2 id="36e3" class="lk ll iq bd lw mr ms dn ma mt mu dp me ko mv mw mg ks mx my mi kw mz na mk nb bi translated">如何表示权重</h2><p id="81d0" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">当谈到神经网络的面向对象实现时，这可能是必须回答的最重要的问题。权重应该属于神经元吗？如果是，应该是发送还是接收神经元？或者也许他们应该属于一层？或者也许是整个网络？也许我们甚至应该将它们作为单独的对象来实现？</p><p id="29f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为只有一层的前馈网络，因此没有连接两个神经元的权重，单层感知器简化了这个问题。基本上，我们有三个选择:</p><ol class=""><li id="6cb5" class="nc nd iq kh b ki kj kl km ko nq ks nr kw ns la nt ni nj nk bi translated">每个神经元的输入权重作为向量存储在该神经元内。</li><li id="18e0" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nt ni nj nk bi translated">所有输入权重的矩阵存储在网络中。</li><li id="bb5a" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nt ni nj nk bi translated">权重被实现为对象并连接到神经元。</li></ol><p id="4a15" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二种选择是最有效的(向量矩阵乘法)，但不是非常面向对象。这个实现中的神经元是什么？显然，网络只是一个权重矩阵+一些学习规则。神经元应该是具有学习率的激活函数吗？但是话说回来，将它们存储在网络中会更有效率。所以基本上我们不需要<code class="fe nu nv nw lg b">Neuron</code>类。我们只需要一个矩阵和几个操作它的函数。对我来说这听起来不像面向对象。</p><p id="8f22" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，第三种选择是过度设计。这只会让整个事情变得更复杂。在多层神经网络中，将权重实现为对象可能会有一些意义，其中每个权重是两个神经元之间的连接(我们可以将输入视为假神经元)。它连接两个神经元，在它们之间发送信号，并且具有可以更新的“强度”。结果，神经元不知道其他神经元。它们只是接收、处理和发射信号。我假设这样的实现不会很快，但是它可以用于建模目的。我将在一篇关于多层网络的文章中详细阐述这个想法。</p><p id="b021" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一个选项看起来最适合单层感知器。而且非常容易实现，所以我会坚持下去。</p><h2 id="46c0" class="lk ll iq bd lw mr ms dn ma mt mu dp me ko mv mw mg ks mx my mi kw mz na mk nb bi translated">激活功能</h2><p id="7233" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">在这个项目中有两种表示激活函数的方式:</p><ol class=""><li id="15ff" class="nc nd iq kh b ki kj kl km ko nq ks nr kw ns la nt ni nj nk bi translated">将它们实现为方法</li><li id="9d16" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nt ni nj nk bi translated">将它们实现为类</li></ol><p id="a53e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一种方法速度更快，占用的内存更少。我们用抽象方法<code class="fe nu nv nw lg b">activation</code>和<code class="fe nu nv nw lg b">activationDerivative</code>创建了一个基类神经元。每个子类将是一种特殊类型的神经元，如<code class="fe nu nv nw lg b">BinaryThresholdNeuron</code>、<code class="fe nu nv nw lg b">SigmoidNeuron</code>，实现相应的激活功能。</p><figure class="lb lc ld le gt ls gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/5f16bc0219312937ba851c2d24d7f9b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*R-i6P3jhrT8W_cs_e6WzCg.png"/></div></figure><p id="644d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实现激活的另一种方式是用两个抽象方法<code class="fe nu nv nw lg b">value:</code>和<code class="fe nu nv nw lg b">derivative:</code>创建一个基类<code class="fe nu nv nw lg b">ActivationFunction</code>。这种方法更加灵活，因为如果有人想使用一个新的激活函数，他将能够将其实现为一个子类，只需定义它是什么以及它的派生函数是什么。然后，他将能够将这类对象传递给现有的神经元。每当我们需要创建一个函数时，重新实现整个神经元似乎是不符合逻辑的。</p><figure class="lb lc ld le gt ls gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/05b6e0414e7d9fc58fcc26747691d0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*oHb_VDm1TCqRDz-F10slnQ.png"/></div></figure><p id="9333" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以真正的问题可以听起来像这样(当然，它可能听起来更好):<br/> <em class="nz">神经元是由它们的激活来定义的吗？具有不同的激活是否意味着是完全不同类型的神经元？</em></p><h2 id="6b5e" class="lk ll iq bd lw mr ms dn ma mt mu dp me ko mv mw mg ks mx my mi kw mz na mk nb bi translated">共享或单独激活和学习率？</h2><p id="1def" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">激活率和学习率既可以由感知器的所有神经元共享，也可以分别存储在每个神经元中。问题是:<em class="nz">我们需要有不同激活和不同学习率的神经元吗？</em></p><p id="d3b5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们假设我们没有。事实上，在大多数情况下，一个网络(或一层)的所有神经元都具有相同的学习速率和相同的激活。如果网络有许多神经元(大多数网络都有)，那么我们将存储同样多的次数。如果激活函数被实现为一个类，那么我们将为每个神经元创建一个单独的类实例。</p><p id="d5b6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，如果我们想要并行化由神经元完成的计算，那么对每个神经元(或每个神经元块)具有单独的学习速率和单独的激活会更好。否则，它们会在每一步都互相阻止对方访问共享内存。此外，这个“重”神经元占据的总内存仍然很小。我认为，这样的神经元(或一组神经元)将很容易放入GPU单核的本地存储器中。</p><p id="0503" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是单层感知器通常不会执行繁重的计算。它们对于建模更有用。这就是为什么我们可能应该采取“分离”的方法，允许用户用完全不同的神经元(像积木一样)构建一个网络。</p><figure class="lb lc ld le gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oa"><img src="../Images/8352203e8db470a9b00cb35859c106a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gvutPTAhn1VjQXzU4leHBg.png"/></div></div></figure><p id="c2f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">顺便说一下，对于多层网络来说，一个好主意是在一层中共享相同的激活和学习速率，但是允许用户拥有完全不同的层。最后，他应该可以建立一些复杂的网络，比如图片上的卷积网络。但这不是这篇文章的主题。</p><h2 id="63d5" class="lk ll iq bd lw mr ms dn ma mt mu dp me ko mv mw mg ks mx my mi kw mz na mk nb bi translated">数据洗牌</h2><p id="7283" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">在线感知器对接收训练样本的顺序很敏感。在每个训练示例之后进行权重更新，这就是为什么训练向量<code class="fe nu nv nw lg b">#(#(0 1) #(1 1))</code>和<code class="fe nu nv nw lg b">#(#(1 1) #(0 1))</code>会产生不同的权重向量。根据示例的顺序，感知器可能需要不同次数的迭代才能收敛。</p><p id="bfdd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是为什么，为了测试这种学习的复杂性，感知机必须通过从训练集中随机选择的例子来训练。</p><h1 id="2ee0" class="lv ll iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">履行</h1><p id="b0f3" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">综上所述，这是我设计的单层peceptron:</p><figure class="lb lc ld le gt ls gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/05036c7a77a8edbe39899a25c6cff6de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*um8DWFwMH0sejMGNpmEVjA.png"/></div></figure><h2 id="c5cd" class="lk ll iq bd lw mr ms dn ma mt mu dp me ko mv mw mg ks mx my mi kw mz na mk nb bi translated">神经元类</h2><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="89cb" class="lk ll iq lg b gy lm ln l lo lp">Object subclass: #Neuron<br/>   instanceVariableNames: 'weights activation learningRate'<br/>   classVariableNames: ''<br/>   package: 'NeuralNetwork'</span></pre><p id="a89a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">权重用范围[0，1]内的随机数初始化。我不确定这是否是一个好的范围，但在简单的例子中，它工作得很好。</p><p id="ec33" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe nu nv nw lg b">BinaryThreshold</code>是默认激活函数，默认学习率为0.1。这些参数可以使用访问器<code class="fe nu nv nw lg b">activation:</code>和<code class="fe nu nv nw lg b">learningRate:</code>进行更改。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="34cb" class="lk ll iq lg b gy lm ln l lo lp">initialize: inputSize<br/>   "Creates a weight vector and initializes it with random values. Assigns default values to activation and learning rate"</span><span id="4859" class="lk ll iq lg b gy of ln l lo lp">   activation := BinaryThreshold new.<br/>   learningRate := 0.1.<br/> <br/>   weights := DhbVector new: (inputSize + 1).<br/> <br/>   1 to: (inputSize + 1) do: [ :i |<br/>      weights at: i put: (1 / (10 atRandom))].<br/>   ^ self</span></pre><p id="16ef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还需要在每个输入向量前加上1作为偏差单位。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="ef01" class="lk ll iq lg b gy lm ln l lo lp">prependBiasToInput: inputVector<br/>   “this method prepends 1 to input vector for a bias unit”<br/> <br/>   ^ (#(1), inputVector) asDhbVector.</span></pre><p id="cc37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据《数值方法》一书，每个函数都应该实现<code class="fe nu nv nw lg b">value:</code>方法。我想强调的是，从数学的角度来看，神经元是一种功能。</p><p id="d264" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然内部表示使用了DhbVector，但我希望用户编写类似于<code class="fe nu nv nw lg b">perceptron value: #(1 0).</code>的内容，而不是<code class="fe nu nv nw lg b">perceptron value: #(1 0) asDhbVector.</code></p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="52fc" class="lk ll iq lg b gy lm ln l lo lp">value: inputVector<br/>   "Takes a vector of inputs and returns the output value"<br/> <br/>   | inputDhbVector |<br/>   inputDhbVector := self prependBiasToInput: inputVector.<br/>   ^ activation value: (weights * inputDhbVector).</span></pre><p id="55d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们需要访问器来设置激活的学习速率。出于调试目的，我还为权重添加了一个简单的访问器。所有这些访问器都是琐碎的，所以我不会把代码放在这里。</p><p id="8184" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，还有感知机学习规则。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="eb0e" class="lk ll iq lg b gy lm ln l lo lp">learn: inputVector target: target<br/>   "Applies the perceptron learning rule after looking at one training example"<br/> <br/>   | input output error delta |<br/>   output := self value: inputVector.<br/>   error := target - output.<br/> <br/>   input := self prependBiasToInput: inputVector.<br/>  <br/>   delta := learningRate * error * input * <br/>      (activation derivative: weights * input).</span></pre><h2 id="3c10" class="lk ll iq bd lw mr ms dn ma mt mu dp me ko mv mw mg ks mx my mi kw mz na mk nb bi translated">感知器类</h2><p id="5507" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">单层感知器(根据我的设计)是神经元的容器。它唯一的实例变量是<code class="fe nu nv nw lg b">neurons</code>数组。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="24d5" class="lk ll iq lg b gy lm ln l lo lp">Object subclass: #SLPerceptron<br/>   instanceVariableNames: ‘neurons’<br/>   classVariableNames: ‘’<br/>   package: ‘NeuralNetwork’</span></pre><p id="26cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了创建一个<code class="fe nu nv nw lg b">SLPerceptron</code>的实例，我们需要指定输入向量的大小和类的数量，这等于我们的感知器(多类分类)中神经元的数量。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="f2b8" class="lk ll iq lg b gy lm ln l lo lp">initialize: inputSize classes: outputSize<br/>   “Creates an array of neurons”<br/>   neurons := Array new: outputSize.<br/> <br/>   1 to: outputSize do: [ :i |<br/>      neurons at: i put: (Neuron new initialize: inputSize). ]</span></pre><p id="7f6a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">单层感知器的输出是该层中每个神经元的标量输出向量。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="92d8" class="lk ll iq lg b gy lm ln l lo lp">value: input<br/>   “Returns the vector of outputs from each neuron”<br/>   | outputVector |<br/> <br/>   outputVector := Array new: (neurons size).<br/> <br/>   1 to: (neurons size) do: [ :i |<br/>      outputVector at: i put: ((neurons at: i) value: input) ].<br/> <br/>   ^ outputVector</span></pre><p id="caa2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们要求SLPerceptron学习，他会将那个请求传递给他所有的神经元(基本上，SLPerceptron只是一个神经元的容器，提供了操纵它们的接口)。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="f409" class="lk ll iq lg b gy lm ln l lo lp">learn: input target: output<br/>   "Trains the network (perceptron) on one (in case of online learning) or multiple (in case of batch learning) input/output pairs"</span><span id="51d0" class="lk ll iq lg b gy of ln l lo lp">  1 to: (neurons size) do: [ :i |<br/>     (neurons at: i) learn: input target: (output at: i) ].</span></pre><h1 id="ec52" class="lv ll iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">测试</h1><p id="35c3" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">我用BinaryThreshold激活函数在4个线性可分的逻辑函数上测试了我的SLPerceptron、OR、NAND和NOR，它收敛于所有这些函数。</p><p id="fd87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是对AND函数的测试。其他3个看起来完全一样(只有预期输出值不同)。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="462d" class="lk ll iq lg b gy lm ln l lo lp">testANDConvergence<br/>   "tests if perceptron is able to classify linearly-separable data"<br/>   "AND function"</span><span id="c811" class="lk ll iq lg b gy of ln l lo lp">   | perceptron inputs outputs k |<br/>   perceptron := SLPerceptron new initialize: 2 classes: 1.<br/>   perceptron activation: (BinaryThreshold new).<br/> <br/>   "logical AND function"<br/>   inputs := #(#(0 0) #(0 1) #(1 0) #(1 1)).<br/>   outputs := #(#(0) #(0) #(0) #(1)).<br/> <br/>   1 to: 100 do: [ :i |<br/>      k := 4 atRandom.<br/>      perceptron learn: (inputs at: k) target: (outputs at: k) ].<br/> <br/>   1 to: 4 do: [ :i |<br/>      self assert: (perceptron value: (inputs at: i)) equals: (outputs at: i) ].</span></pre><p id="f935" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而这个测试(或者更确切地说是一个演示)表明单层感知器无法学习XOR函数(不是线性可分的)。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="efbe" class="lk ll iq lg b gy lm ln l lo lp">testXORDivergence<br/>   "single-layer perceptron should not be uneble to classify data that is not linearly-separable"<br/>   "XOR function"<br/>   <br/>   | perceptron inputs outputs k notEqual |<br/>   perceptron := SLPerceptron new initialize: 2 classes: 1.<br/>   perceptron activation: (BinaryThreshold new).<br/> <br/>   "logical XOR function"<br/>   inputs := #(#(0 0) #(0 1) #(1 0) #(1 1)).<br/>   outputs := #(#(0) #(1) #(1) #(0)).<br/> <br/>   1 to: 100 do: [ :i |<br/>      k := 4 atRandom.<br/>      perceptron learn: (inputs at: k) target: (outputs at: k) ].<br/> <br/>   notEqual := false.<br/> <br/>   1 to: 4 do: [ :i |<br/>      notEqual := notEqual or:<br/>         ((perceptron value: (inputs at: i)) ~= (outputs at: i)) ].<br/>  <br/>   self assert: notEqual.</span></pre><p id="d01b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我也试图测试<code class="fe nu nv nw lg b">Sigmoid</code>功能，但是测试失败了。这意味着要么感知器(如本文开头所定义的)不能将sigmoid作为其激活，要么我对如何用sigmoid实现感知器没有足够好的理解。</p><h1 id="2dd5" class="lv ll iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">下一步是什么？</h1><ul class=""><li id="765d" class="nc nd iq kh b ki mm kl mn ko ne ks nf kw ng la nh ni nj nk bi translated">具有批量学习和不同学习规则的单层神经网络的实现</li><li id="0886" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nh ni nj nk bi translated">具有反向传播的多层神经网络的实现</li></ul></div></div>    
</body>
</html>