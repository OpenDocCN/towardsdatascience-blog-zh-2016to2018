<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47?source=collection_archive---------1-----------------------#2018-02-19">https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47?source=collection_archive---------1-----------------------#2018-02-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/fac29cc51ae992b0c2e3340021c2974b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*voNageVB1gI8Dfrmr-U3Ew.png"/></div></div></figure><p id="5549" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">Hochreiter 和 Schmiduber 引入了长短期记忆网络，通常称为“LSTMs”。这些已经广泛用于语音识别、语言建模、情感分析和文本预测。在深入 LSTM 之前，我们应该首先了解 LSTM 的需要，这可以用递归神经网络(RNN)实际使用的缺点来解释。所以，让我们从 RNN 开始。</p><h1 id="dbf5" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">递归神经网络(RNN)</h1><p id="6b56" class="pw-post-body-paragraph iz ja jb jc b jd kw jf jg jh kx jj jk jl ky jn jo jp kz jr js jt la jv jw jx ij bi translated">作为人类，当我们看一部电影时，我们不会每次都在理解任何事件的同时从零开始思考。我们依靠电影中最近发生的经历，并从中学习。但是，传统的神经网络不能从先前的事件中学习，因为信息不能从一个步骤传递到下一个步骤。相反，RNN 从紧接的前一步学习信息。</p><p id="11d3" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">比如电影里有一个场景，一个人在篮球场上。我们将在未来的画面中即兴创作篮球活动:一个人跑着跳着的图像可能被标记为<em class="lb">打篮球</em>，一个人坐着看的图像可能是<em class="lb">一个观看比赛的观众。</em></p><figure class="ld le lf lg gt is gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/ce4ce49277c0bfd0f679e430c6964a45.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*DvlB9rtndUHwtri4E2P-bg.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">A typical RNN (Source: <a class="ae ll" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>)</figcaption></figure><figure class="ld le lf lg gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lm"><img src="../Images/ab646a9b0ec8e05de2fe6ebce6238eee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xTKE0g6XNMLM8IQ4aFdP0w.png"/></div></div></figure><p id="d7b1" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">典型的 RNN 如上图所示——其中 X(t)是输入，h(t)是输出，A 是从循环中的前一步获得信息的神经网络。一个单元的输出进入下一个单元，信息被传递。</p><p id="745a" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">但是，有时我们不需要我们的网络只从最近的信息中学习。假设我们想预测文本中的空白单词“大卫，一个住在旧金山的 36 岁男子”。他有一个女性朋友玛丽亚。玛丽亚在纽约一家著名的餐馆当厨师，他是最近在一次校友聚会上认识她的。玛丽亚告诉他，她一直对 _________ 充满热情。在这里，我们希望我们的网络从依赖“烹饪”中学习，以预测“烹饪”。在我们想要预测的信息和我们想要预测的信息之间存在着差距。这叫长期依赖。我们可以说任何大于三元模型的都是长期依赖。不幸的是，RNN 在这种情况下实际上不起作用。</p><h1 id="8961" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">为什么 RNN 不实际工作</h1><p id="44ff" class="pw-post-body-paragraph iz ja jb jc b jd kw jf jg jh kx jj jk jl ky jn jo jp kz jr js jt la jv jw jx ij bi translated">在 RNN 的训练期间，由于信息一次又一次地循环，这导致对神经网络模型权重的非常大的更新。这是由于更新期间误差梯度的累积，因此导致网络不稳定。在极端情况下，权重值可能变得过大，以至于溢出并导致 NaN 值。通过在值大于 1 的网络图层中重复乘以梯度，爆炸会以指数增长的方式发生，如果值小于 1，爆炸会消失。</p><h1 id="e7b9" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">长短期记忆</h1><p id="f028" class="pw-post-body-paragraph iz ja jb jc b jd kw jf jg jh kx jj jk jl ky jn jo jp kz jr js jt la jv jw jx ij bi translated">RNN 的上述缺点促使科学家们开发并发明了一种新的 RNN 模型，称为长短期记忆。LSTM 可以解决这个问题，因为它用门来控制记忆过程。</p><p id="af95" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">让我们了解一下 LSTM 的建筑，并将其与 RNN 的建筑进行比较:</p><figure class="ld le lf lg gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ln"><img src="../Images/cfec4079737fb6aad0f8343b23e71c31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Niu_c_FhGtLuHjrStkB_4Q.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">A LSTM unit (Source : <a class="ae ll" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>)</figcaption></figure><p id="7259" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">此处使用的符号具有以下含义:</p><p id="d62d" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">a) X:信息的缩放</p><p id="bf3e" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">b)+:添加信息</p><p id="1a96" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">c) σ:乙状结肠层</p><p id="fe17" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">d) tanh: tanh 层</p><p id="295d" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">e) h(t-1):最后一个 LSTM 单位的产量</p><p id="a14f" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">f) c(t-1):来自最后一个 LSTM 单元的内存</p><p id="e417" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">g) X(t):电流输入</p><p id="e8d3" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">h) c(t):新更新的内存</p><p id="de81" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">i) h(t):电流输出</p><h1 id="7616" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">为什么是 tanh？</h1><p id="4a16" class="pw-post-body-paragraph iz ja jb jc b jd kw jf jg jh kx jj jk jl ky jn jo jp kz jr js jt la jv jw jx ij bi translated">为了克服梯度消失的问题，我们需要一个函数，它的二阶导数在趋于零之前可以维持很长一段时间。<em class="lb"> tanh </em>是具有上述性质的合适函数。</p><h1 id="5dbf" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">为什么是乙状结肠？</h1><p id="4f80" class="pw-post-body-paragraph iz ja jb jc b jd kw jf jg jh kx jj jk jl ky jn jo jp kz jr js jt la jv jw jx ij bi translated">由于 Sigmoid 可以输出 0 或 1，所以可以用来忘记或记住信息。</p><p id="73c7" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">信息通过许多这样的 LSTM 单元传递。图中标出了 LSTM 装置的三个主要组件:</p><ol class=""><li id="d2c0" class="lo lp jb jc b jd je jh ji jl lq jp lr jt ls jx lt lu lv lw bi translated">LSTM 有一个特殊的建筑，使它能够忘记不必要的信息。sigmoid 层获取输入 X(t)和 h(t-1 ),并决定旧输出中的哪些部分应该被移除(通过输出 0)。在我们的例子中，当输入是‘他有一个女性朋友玛丽亚’时，可以忘记‘大卫’的性别，因为主语已经变成了‘玛丽亚’。这个门叫做忘记门 f(t)。这个门的输出是 f(t)*c(t-1)。</li><li id="15ec" class="lo lp jb jc b jd lx jh ly jl lz jp ma jt mb jx lt lu lv lw bi translated">下一步是在单元状态中决定和存储来自新输入 X(t)的信息。Sigmoid 层决定应该更新或忽略哪些新信息。一个<em class="lb"> tanh </em>层从新输入创建一个所有可能值的向量。这两者相乘以更新新的单元状态。然后将这个新的记忆与旧的记忆 c(t-1)相加，得到 c(t)。在我们的例子中，对于新输入‘他有一个女性朋友 Maria’，Maria 的性别将被更新。当输入是“Maria 在纽约的一家著名餐馆当厨师，他最近在一次校友会上认识了她”，像“著名”、“校友会”这样的词可以被忽略，而像“厨师”、“餐馆”和“纽约”这样的词将被更新。</li><li id="dddb" class="lo lp jb jc b jd lx jh ly jl lz jp ma jt mb jx lt lu lv lw bi translated">最后，我们需要决定我们要输出什么。sigmoid 层决定了我们要输出细胞状态的哪一部分。然后，我们将单元状态通过一个产生所有可能值的<em class="lb"> tanh </em>并乘以 sigmoid 门的输出，这样我们只输出我们决定的部分。在我们的例子中，我们想要预测空白单词，我们的模型从它的记忆中知道它是一个与“烹饪”相关的名词，它可以很容易地回答它为“烹饪”。我们的模型不是从直接依赖中得到这个答案，而是从长期依赖中得到这个答案。</li></ol><p id="efac" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">我们刚刚看到，典型的 RNN 和 LSTM 的建筑有很大的不同。在 LSTM，我们的模型学会了在长期记忆中储存什么信息和去掉什么。</p><h1 id="89fa" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">情感分析 LSTM 的快速实现</h1><p id="d356" class="pw-post-body-paragraph iz ja jb jc b jd kw jf jg jh kx jj jk jl ky jn jo jp kz jr js jt la jv jw jx ij bi translated">在这里，我使用 LSTM 对来自 Yelp 公开数据集的评论数据进行了情感分析。</p><p id="9080" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">这是我的数据的样子。</p><figure class="ld le lf lg gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mc"><img src="../Images/26871b068f29d7b48c6ad4dfab9543e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pOZ3Dcc27oPlbt6dE-FbSg.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Dataset</figcaption></figure><p id="e504" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">我使用 tokenizer 将文本矢量化，并在限制 Tokenizer 仅使用最常见的 2500 个单词后将其转换为整数序列。我使用 pad_sequences 将序列转换成二维 numpy 数组。</p><figure class="ld le lf lg gt is"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="56b8" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">然后，我建立了我的 LSTM 网络。有几个超参数:</p><ol class=""><li id="09ab" class="lo lp jb jc b jd je jh ji jl lq jp lr jt ls jx lt lu lv lw bi translated">embed_dim:嵌入层将输入序列<br/>编码成一个维数为 embed_dim 的密集向量序列。</li><li id="659f" class="lo lp jb jc b jd lx jh ly jl lz jp ma jt mb jx lt lu lv lw bi translated">lstm_out:LSTM 将向量序列转换成大小为 lstm _ out 的单个向量，其中包含整个序列的信息。</li></ol><p id="9213" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">其他超级参数如 dropout、batch_size 与 CNN 相似。</p><p id="944b" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">我用 softmax 作为激活函数。</p><figure class="ld le lf lg gt is gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/4943e7a93c969a508d7b7fd8b11458d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*GsT2fwE_39fgmtC735924Q.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">LSTM network</figcaption></figure><figure class="ld le lf lg gt is"><div class="bz fp l di"><div class="md me l"/></div></figure><figure class="ld le lf lg gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mg"><img src="../Images/ccbc46f2a88e0c626a7e792343083c13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PCpvPF7eBi07rRc0ayFqXw.png"/></div></div></figure><p id="5baf" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">现在，我在训练集上拟合我的模型，并在验证集上检查准确性。</p><figure class="ld le lf lg gt is"><div class="bz fp l di"><div class="md me l"/></div></figure><figure class="ld le lf lg gt is gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/ae6518b6f507323e011607ce77d45bca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*XfPXSNqVb3vc5_jTRl-Q3w.png"/></div></figure><figure class="ld le lf lg gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mi"><img src="../Images/5b1f8021813b3a681df72f81c73c59b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dmZz0dp17eqlcrAWFtk2AA.png"/></div></div></figure><p id="185d" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">我在一个包含所有企业的小数据集上运行时，仅在一个时期内就获得了 86%的验证准确率。</p><h1 id="9b0b" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">未来工作:</h1><ol class=""><li id="0910" class="lo lp jb jc b jd kw jh kx jl mj jp mk jt ml jx lt lu lv lw bi translated">我们可以过滤像餐馆这样的特定行业，然后使用 LSTM 进行情感分析。</li><li id="e7d2" class="lo lp jb jc b jd lx jh ly jl lz jp ma jt mb jx lt lu lv lw bi translated">我们可以使用具有更多时期的大得多的数据集来提高精确度。</li><li id="7296" class="lo lp jb jc b jd lx jh ly jl lz jp ma jt mb jx lt lu lv lw bi translated">可以使用更多隐藏的密集层来提高精度。我们也可以调整其他超参数。</li></ol><h1 id="98f2" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">结论</h1><p id="98da" class="pw-post-body-paragraph iz ja jb jc b jd kw jf jg jh kx jj jk jl ky jn jo jp kz jr js jt la jv jw jx ij bi translated">当我们希望我们的模型从长期依赖关系中学习时，LSTM 优于其他模型。LSTM 的遗忘、记忆和更新信息的能力使其领先于 RNNs 一步。</p><h1 id="a335" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">参考资料和其他有用资源:</h1><ol class=""><li id="8f7c" class="lo lp jb jc b jd kw jh kx jl mj jp mk jt ml jx lt lu lv lw bi translated"><a class="ae ll" href="https://github.com/nsinha280/lstm" rel="noopener ugc nofollow" target="_blank">我的 Github 回购</a></li><li id="7ce0" class="lo lp jb jc b jd lx jh ly jl lz jp ma jt mb jx lt lu lv lw bi translated"><a class="ae ll" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">了解 LSTM </a></li><li id="0af7" class="lo lp jb jc b jd lx jh ly jl lz jp ma jt mb jx lt lu lv lw bi translated"><a class="ae ll" href="https://deeplearning4j.org/lstm.html" rel="noopener ugc nofollow" target="_blank">RNN 和 LSTM 入门指南</a></li></ol><p id="2a60" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">4.<a class="ae ll" href="http://blog.echen.me/2017/05/30/exploring-lstms/" rel="noopener ugc nofollow" target="_blank">探索 lstm</a></p><p id="b9cb" class="pw-post-body-paragraph iz ja jb jc b jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ij bi translated">5.<a class="ae ll" href="http://www.bioinf.jku.at/publications/older/2604.pdf" rel="noopener ugc nofollow" target="_blank">关于 LSTM 的研究论文</a></p></div></div>    
</body>
</html>