<html>
<head>
<title>Use Unsupervised Machine Learning To Find Potential Buyers of Your Products</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用无监督的机器学习来寻找你的产品的潜在买家</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/use-unsupervised-machine-learning-to-find-potential-buyers-of-your-products-cd75edaeefc7?source=collection_archive---------18-----------------------#2018-12-31">https://towardsdatascience.com/use-unsupervised-machine-learning-to-find-potential-buyers-of-your-products-cd75edaeefc7?source=collection_archive---------18-----------------------#2018-12-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8b52" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">让算法自己学习…</h2></div><h2 id="e09f" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">什么是无监督机器学习</h2><p id="3442" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">欢迎来到我关于数据科学的第三篇文章！在我之前的帖子中，我讨论了我如何使用监督机器学习来为一个慈善机构寻找捐助者。回想一下，在监督机器学习中，您有输入变量(X)和输出变量(Y ),并且您使用算法来学习从输入到输出的映射函数。<strong class="ld ir">相比之下，在无监督的机器学习中你只有输入数据(X)而没有相应的输出变量。</strong>无监督学习的目标是对数据中的底层结构或分布进行建模，以便了解更多关于数据的信息。这些被称为非监督学习，因为不像上面的监督学习，没有正确的答案，也没有老师。算法被留给它们自己的装置去发现和呈现数据中有趣的结构。</p><blockquote class="lu lv lw"><p id="f8f6" class="lb lc lx ld b le ly jr lg lh lz ju lj ma mb ll lm mc md lo lp me mf lr ls lt ij bi translated">无监督学习问题可以进一步分为聚类和关联问题。<em class="iq">聚类</em>:聚类问题是您想要发现数据中的内在分组，比如按照购买行为对客户进行分组。关联:关联规则学习问题是你想要发现描述大部分数据的规则，比如购买 X 的人也倾向于购买 y。</p></blockquote><h2 id="6242" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">项目亮点</h2><p id="b61a" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">在我将在这里描述的项目中，我使用了来自德国一家邮购销售公司的 Bertelsmann Arvato Analytics 的真实财务数据。我的任务是识别构成该公司用户基础核心的人群。换句话说，我处理了一个集群问题。</p><p id="37d2" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">在接下来的段落中，我将详细阐述我实现目标的步骤。就像在大部分机器学习问题中，无论你的算法有多复杂，都必须先清理数据。我们开始吧:)</p><h2 id="aad6" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">数据处理</h2><p id="3f93" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">有四个数据集与此案例相关:</p><ul class=""><li id="596c" class="mg mh iq ld b le ly lh lz ko mi ks mj kw mk lt ml mm mn mo bi translated"><code class="fe mp mq mr ms b">Udacity_AZDIAS_Subset.csv</code>:德国一般人口的人口统计数据；891211 人(行)x 85 个特征(列)。</li><li id="b1bf" class="mg mh iq ld b le mt lh mu ko mv ks mw kw mx lt ml mm mn mo bi translated"><code class="fe mp mq mr ms b">Udacity_CUSTOMERS_Subset.csv</code>:邮购公司客户的人口统计数据；191652 人(行)x 85 特征(列)。</li><li id="bbf6" class="mg mh iq ld b le mt lh mu ko mv ks mw kw mx lt ml mm mn mo bi translated"><code class="fe mp mq mr ms b">Data_Dictionary.md</code>:所提供数据集中要素的详细信息文件。</li><li id="38c9" class="mg mh iq ld b le mt lh mu ko mv ks mw kw mx lt ml mm mn mo bi translated"><code class="fe mp mq mr ms b">AZDIAS_Feature_Summary.csv</code>:人口统计数据特征属性汇总；85 个特征(行)x 4 列</li></ul><p id="b43d" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">人口统计文件的每一行代表一个人，但也包括个人以外的信息，包括他们的家庭、建筑和邻居的信息。我使用这些数据将普通人群分成具有相似人口统计特征的群体。目的是了解客户数据集中的人如何适应这些创建的分类。</p><p id="8d1e" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">让我们看看我们将使用哪些功能。</p><pre class="my mz na nb gt nc ms nd ne aw nf bi"><span id="39cd" class="kf kg iq ms b gy ng nh l ni nj"><em class="lx"># Load in the general demographics data.</em><br/>azdias = pd.read_csv('Udacity_AZDIAS_Subset.csv', sep = ';')<br/><br/><em class="lx"># Load in the feature summary file.</em><br/>feat_info = pd.read_csv('AZDIAS_Feature_Summary.csv', sep = ';')</span><span id="788a" class="kf kg iq ms b gy nk nh l ni nj">print(feat_info)</span></pre><figure class="my mz na nb gt nm gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/7f8f2dc6c805bfc24ab7493a93ac437c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*7sdZcDX5sDzzlt3zOmtLbA.jpeg"/></div></figure><p id="e390" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">在字典文件中，所有这些特性都有更详细的描述。即，提供了它们在英语中的含义，以及这些特征的不同级别的含义。</p><p id="71ee" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">请注意，特征属性摘要的第四列(在上面作为<code class="fe mp mq mr ms b">feat_info</code>载入)记录了数据字典中的代码，这些代码表示缺失或未知的数据。虽然文件将它编码为一个列表(例如<code class="fe mp mq mr ms b">[-1,0]</code>)，但它将作为一个字符串对象读入。我将匹配“缺失”或“未知”值代码的数据转换为 numpy NaN 值。</p><pre class="my mz na nb gt nc ms nd ne aw nf bi"><span id="9796" class="kf kg iq ms b gy ng nh l ni nj"><em class="lx"># turn missing_or_unknown to list </em><br/>feat_info['missing_or_unknown'] = feat_info['missing_or_unknown'].apply(<strong class="ms ir">lambda</strong> x: x[1:-1].split(','))<br/><br/><em class="lx"># Identify missing or unknown data values and convert them to NaNs.</em><br/><strong class="ms ir">for</strong> attrib, missing_values <strong class="ms ir">in</strong> zip(feat_info['attribute'], feat_info['missing_or_unknown']):<br/>    <strong class="ms ir">if</strong> missing_values[0] != '':<br/>        <strong class="ms ir">for</strong> value <strong class="ms ir">in</strong> missing_values:<br/>            <strong class="ms ir">if</strong> value.isnumeric() <strong class="ms ir">or</strong> value.lstrip('-').isnumeric():<br/>                value = int(value)<br/>            azdias.loc[azdias[attrib] == value, attrib] = np.nan</span></pre><p id="25ac" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated"><strong class="ld ir">评估各栏缺失数据</strong></p><pre class="my mz na nb gt nc ms nd ne aw nf bi"><span id="1115" class="kf kg iq ms b gy ng nh l ni nj"><em class="lx"># Perform an assessment of how much missing data there is in each column of the</em><br/><em class="lx"># dataset.</em><br/>missing_data = pd.Series(azdias.isnull().sum() / len(azdias))<br/>missing_data.plot(kind='barh', figsize=(10, 20))</span><span id="0a79" class="kf kg iq ms b gy nk nh l ni nj">missing_data[missing_data &gt; 0.2].index.tolist()</span><span id="f29a" class="kf kg iq ms b gy nk nh l ni nj">['AGER_TYP',<br/> 'GEBURTSJAHR',<br/> 'TITEL_KZ',<br/> 'ALTER_HH',<br/> 'KK_KUNDENTYP',<br/> 'KBA05_BAUMAX']</span><span id="65eb" class="kf kg iq ms b gy nk nh l ni nj"><em class="lx"># Remove the outlier columns from the dataset. (You'll perform other data engineering tasks such as re-encoding and imputation later.)</em></span><span id="944e" class="kf kg iq ms b gy nk nh l ni nj">azdias = azdias.drop(['AGER_TYP','GEBURTSJAHR','TITEL_KZ','ALTER_HH','KK_KUNDENTYP','KBA05_BAUMAX'], axis = 1)</span></pre><p id="b149" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">我已经用柱状图评估了缺失的数据。我已经将丢失值超过 20%的列归类为异常值。结果，我找到了 6 个这样的列，我放弃了。其他突出的异常值是有 13%缺失值的 PLZ8 特性；以及具有 15%缺失值的 KBA05 功能。</p><p id="e66e" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated"><strong class="ld ir">评估每行的缺失数据</strong></p><p id="b65a" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">就像评估列中缺失数据的情况一样，我寻找含有缺失数据的异常行。我将阈值设置为每行 10 个缺失值，并将数据分为两组:缺失值数量多的行(即高于 10)，缺失值数量少的行(即低于 10)。我比较了这两组之间几个非缺失特征的分布。目的是了解我是否应该以特定的方式对待这些数据点。</p><pre class="my mz na nb gt nc ms nd ne aw nf bi"><span id="48f8" class="kf kg iq ms b gy ng nh l ni nj"><em class="lx"># How much data is missing in each row of the dataset?</em><br/>missing_data_rows = azdias.isnull().sum(axis = 1)</span><span id="7378" class="kf kg iq ms b gy nk nh l ni nj">missing_data_rows_low = azdias[azdias.isnull().sum(axis=1) &lt; 10].reset_index(drop=<strong class="ms ir">True</strong>)<br/><br/>missing_data_rows_high = azdias[azdias.isnull().sum(axis = 1) &gt;= 10].reset_index(drop=<strong class="ms ir">True</strong>)</span><span id="81c2" class="kf kg iq ms b gy nk nh l ni nj"><strong class="ms ir">def</strong> countplot(columns, num):<br/>    fig, axs = plt.subplots(num, 2, figsize=(15, 15))<br/>    fig.subplots_adjust(hspace =2 , wspace=.2)<br/>    axs = axs.ravel()<br/>    <br/>    <strong class="ms ir">for</strong> i <strong class="ms ir">in</strong> range(num):<br/>        <br/>        sns.countplot(missing_data_rows_low[columns[i]], ax=axs[i*2])<br/>        axs[i*2].set_title('missing_data_rows_low')<br/>        sns.countplot(missing_data_rows_high[columns[i]], ax=axs[i*2+1])<br/>        axs[i*2+1].set_title('missing_data_rows_high')<br/>        <br/>countplot(missing_data_rows_high.columns, 3)</span></pre><figure class="my mz na nb gt nm gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi np"><img src="../Images/f43cba4d0afd87e879a2cbdf9743a855.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pHyzmygEf5K9UE6KTjSI2Q.png"/></div></div></figure><p id="1c16" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">对于我检查过的一些特性，缺失值多的行和缺失值少的行之间似乎有不同的分布。这意味着缺失值数量多的行和缺失值数量少的行之间的数据在性质上是不同的。缺失数据数量少的行组将被考虑用于进一步分析，而缺失数据数量多的行组最终将被视为附加聚类。</p><p id="3529" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated"><strong class="ld ir">数据角力</strong></p><pre class="my mz na nb gt nc ms nd ne aw nf bi"><span id="4b24" class="kf kg iq ms b gy ng nh l ni nj"><strong class="ms ir">def</strong> clean_data(df):<br/>    <em class="lx">"""</em><br/><em class="lx">    Perform feature trimming, re-encoding, and engineering for demographics</em><br/><em class="lx">    data</em><br/><em class="lx">    </em><br/><em class="lx">    INPUT: Demographics DataFrame</em><br/><em class="lx">    OUTPUT: Trimmed and cleaned demographics DataFrame</em><br/><em class="lx">    """</em><br/>    <br/>    <em class="lx"># Put in code here to execute all main cleaning steps:</em><br/>    <em class="lx"># convert missing value codes into NaNs, ...</em><br/>    <br/>    df_copy = df.copy()<br/><br/><em class="lx"># Identify missing or unknown data values and convert them to NaNs.</em><br/>    <strong class="ms ir">for</strong> col_name <strong class="ms ir">in</strong> df.columns:<br/>        df_copy[col_name] = df_copy[col_name].map(<strong class="ms ir">lambda</strong> x: np.nan <strong class="ms ir">if</strong> str(x) <strong class="ms ir">in</strong> feat_info.loc[col_name].missing_or_unknown <strong class="ms ir">else</strong> x)<br/>        <br/>    <em class="lx"># remove selected columns and rows, ...</em><br/>    c_removed =['AGER_TYP','GEBURTSJAHR','TITEL_KZ','ALTER_HH','KK_KUNDENTYP','KBA05_BAUMAX']<br/>    <br/>    <strong class="ms ir">for</strong> c <strong class="ms ir">in</strong> c_removed:<br/>        df_copy.drop(c, axis=1, inplace=<strong class="ms ir">True</strong>)<br/>        <br/>    df_copy = df_copy[df_copy.isnull().sum(axis=1) &lt; 10].reset_index(drop=<strong class="ms ir">True</strong>)<br/>    <br/>    <strong class="ms ir">for</strong> col <strong class="ms ir">in</strong> df_copy.columns:<br/>        df_copy[col] = df_copy[col].fillna(df_copy[col].mode()[0])<br/>        <br/>    <br/>    <em class="lx"># select, re-encode, and engineer column values.</em><br/>    multi_level = []<br/>    <strong class="ms ir">for</strong> column <strong class="ms ir">in</strong> df_copy.columns:<br/>        <strong class="ms ir">if</strong> feat_info.loc[column].type == 'categorical' <strong class="ms ir">and</strong> len(df_copy[column].unique()) &gt; 2:<br/>            multi_level.append(column)<br/>            <br/>    <strong class="ms ir">for</strong> col <strong class="ms ir">in</strong> multi_level:<br/>        df_copy.drop(col, axis=1, inplace=<strong class="ms ir">True</strong>)<br/>    <br/>    df_copy['decade'] = df_copy['PRAEGENDE_JUGENDJAHRE'].apply(create_interval_decade)<br/>    df_copy['movement'] = df_copy['PRAEGENDE_JUGENDJAHRE'].apply(create_binary_movement)<br/>    df_copy.drop('PRAEGENDE_JUGENDJAHRE', axis=1, inplace=<strong class="ms ir">True</strong>)<br/>    <br/>    df_copy['wealth'] = df_copy['CAMEO_INTL_2015'].apply(wealth)<br/>    df_copy['life_stage'] = df_copy['CAMEO_INTL_2015'].apply(life_stage)<br/>    df_copy.drop('CAMEO_INTL_2015', axis=1, inplace=<strong class="ms ir">True</strong>)<br/>    <br/>    df_copy = pd.get_dummies(data=df_copy, columns=['OST_WEST_KZ'])<br/>    <br/>    mixed = ['LP_LEBENSPHASE_FEIN','LP_LEBENSPHASE_GROB','WOHNLAGE','PLZ8_BAUMAX']<br/>    <br/>    <strong class="ms ir">for</strong> c <strong class="ms ir">in</strong> mixed:<br/>        df_copy.drop(c, axis=1, inplace=<strong class="ms ir">True</strong>)<br/>    <br/>     <em class="lx"># Return the cleaned dataframe.</em><br/>    <strong class="ms ir">return</strong> df_copy</span></pre><p id="95b2" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">我创建了一个清理数据函数，可以应用于一般人口统计数据和客户统计数据。</p><h2 id="e8ac" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">主成分分析</h2><p id="9bd8" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">PCA 是最常用的无监督机器学习工具之一。<strong class="ld ir">主成分</strong>是数据集中原始特征的线性组合，旨在保留原始数据中的大部分信息。主成分分析是一种基于现有特征从数据集中提取新的“潜在特征”的常用方法。想象一个<em class="lx">主成分</em>，就像你想象一个<em class="lx">潜在特征</em>一样。</p><p id="615b" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">当我们拥有包含成百上千个特征的数据集时，为了有效地构建模型，我们必须减少维数。有两种方法可以做到这一点:</p><p id="4d0c" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">a) <em class="lx">特征选择:</em>特征选择包括从您确定最相关和最有用的原始数据特征中寻找一个<strong class="ld ir">子集</strong>。</p><p id="b779" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">b) <em class="lx">特征提取:</em>特征提取包括提取或构建称为<strong class="ld ir">潜在特征</strong>的新特征。</p><h2 id="8e07" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">特征转换</h2><p id="b7f6" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">我执行了<em class="lx">特征缩放</em>，这样主成分向量就不会受到特征缩放的自然差异的影响。</p><pre class="my mz na nb gt nc ms nd ne aw nf bi"><span id="634c" class="kf kg iq ms b gy ng nh l ni nj"><em class="lx"># Fill the Nan values with the mode of that respective column.</em></span><span id="90ab" class="kf kg iq ms b gy nk nh l ni nj"><strong class="ms ir">for</strong> col <strong class="ms ir">in</strong> missing_data_rows_low.columns:<br/>        missing_data_rows_low[col] = missing_data_rows_low[col].fillna(missing_data_rows_low[col].mode()[0])</span><span id="cb72" class="kf kg iq ms b gy nk nh l ni nj"><em class="lx"># Apply feature scaling to the general population demographics data.</em></span><span id="a28a" class="kf kg iq ms b gy nk nh l ni nj">normalizer = StandardScaler()<br/>missing_data_rows_low[missing_data_rows_low.columns] = normalizer.fit_transform(missing_data_rows_low[missing_data_rows_low.columns])</span><span id="358e" class="kf kg iq ms b gy nk nh l ni nj">missing_data_rows_low.head()</span></pre><p id="a11e" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated"><strong class="ld ir">降维</strong></p><pre class="my mz na nb gt nc ms nd ne aw nf bi"><span id="61df" class="kf kg iq ms b gy ng nh l ni nj"><em class="lx"># Apply PCA to the data.</em><br/><br/>pca = PCA()<br/>missing_data_rows_low_pca = pca.fit_transform(missing_data_rows_low)</span><span id="4b51" class="kf kg iq ms b gy nk nh l ni nj"><em class="lx"># Investigate the variance accounted for by each principal component.</em></span><span id="ee0b" class="kf kg iq ms b gy nk nh l ni nj"><strong class="ms ir">def</strong> scree_plot(pca):<br/>    <em class="lx">'''</em><br/><em class="lx">    Creates a scree plot associated with the principal components </em><br/><em class="lx">    </em><br/><em class="lx">    INPUT: pca - the result of instantian of PCA in scikit learn</em><br/><em class="lx">            </em><br/><em class="lx">    OUTPUT:</em><br/><em class="lx">            None</em><br/><em class="lx">    '''</em><br/>    num_components = len(pca.explained_variance_ratio_)<br/>    ind = np.arange(num_components)<br/>    vals = pca.explained_variance_ratio_<br/> <br/>    plt.figure(figsize=(10, 6))<br/>    ax = plt.subplot(111)<br/>    cumvals = np.cumsum(vals)<br/>    ax.bar(ind, vals)<br/>    ax.plot(ind, cumvals)<br/> <br/>    ax.xaxis.set_tick_params(width=0)<br/>    ax.yaxis.set_tick_params(width=2, length=12)<br/> <br/>    ax.set_xlabel("Principal Component")<br/>    ax.set_ylabel("Variance Explained (%)")<br/>    plt.title('Explained Variance Per Principal Component')<br/>    <br/><br/>scree_plot(pca)</span></pre><figure class="my mz na nb gt nm gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/4999fa1a88432bec079e79919a5de1a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*47jmvUvjoqiW6SxQOElgxQ.jpeg"/></div></figure><pre class="my mz na nb gt nc ms nd ne aw nf bi"><span id="06e0" class="kf kg iq ms b gy ng nh l ni nj"><em class="lx"># Re-apply PCA to the data while selecting for number of components to retain.</em><br/><br/>pca = PCA(n_components=41)<br/>missing_data_rows_low_pca = pca.fit_transform(missing_data_rows_low)</span></pre><p id="3b1d" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">基于 PCA 图，解释的方差在 41 个成分之后变得极低，并且之后不变。所以我又用 41 个成分做了主成分分析。</p><p id="f98b" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">每个主成分是一个指向最高方差方向的单位向量(在考虑了早期主成分捕获的方差之后)。权重离零越远，主分量在相应特征的方向上就越多。如果两个特征具有相同符号的较大权重(都是正的或都是负的)，那么一个特征的增加会与另一个特征的增加相关联。相比之下，具有不同符号的特征可能会表现出负相关性:一个变量的增加会导致另一个变量的减少。</p><h2 id="6015" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">对普通人群应用聚类分析</h2><pre class="my mz na nb gt nc ms nd ne aw nf bi"><span id="d652" class="kf kg iq ms b gy ng nh l ni nj"><strong class="ms ir">def</strong> get_kmeans_score(data, center):<br/>    <em class="lx">'''</em><br/><em class="lx">    returns the kmeans score regarding SSE for points to centers</em><br/><em class="lx">    INPUT:</em><br/><em class="lx">        data - the dataset you want to fit kmeans to</em><br/><em class="lx">        center - the number of centers you want (the k value)</em><br/><em class="lx">    OUTPUT:</em><br/><em class="lx">        score - the SSE score for the kmeans model fit to the data</em><br/><em class="lx">    '''</em><br/>    <em class="lx">#instantiate kmeans</em><br/>    kmeans = KMeans(n_clusters=center)<br/><br/>    <em class="lx"># Then fit the model to your data using the fit method</em><br/>    model = kmeans.fit(data)<br/>    <br/>    <em class="lx"># Obtain a score related to the model fit</em><br/>    score = np.abs(model.score(data))<br/>    <br/>    <strong class="ms ir">return</strong> score</span><span id="0b7f" class="kf kg iq ms b gy nk nh l ni nj"><em class="lx"># Over a number of different cluster counts...</em><br/><em class="lx"># run k-means clustering on the data and...</em><br/><em class="lx"># compute the average within-cluster distances.</em></span><span id="124f" class="kf kg iq ms b gy nk nh l ni nj">scores = []<br/>centers = list(range(1,30,3))<br/><br/><strong class="ms ir">for</strong> center <strong class="ms ir">in</strong> centers:<br/>    scores.append(get_kmeans_score(missing_data_rows_low_pca, center))</span><span id="ad00" class="kf kg iq ms b gy nk nh l ni nj"><em class="lx"># Investigate the change in within-cluster distance across number of clusters.</em><br/><em class="lx"># HINT: Use matplotlib's plot function to visualize this relationship.</em></span><span id="e3f8" class="kf kg iq ms b gy nk nh l ni nj">plt.plot(centers, scores, linestyle='--', marker='o', color='b');<br/>plt.xlabel('K');<br/>plt.ylabel('SSE');<br/>plt.title('SSE vs. K')</span></pre><figure class="my mz na nb gt nm gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/da36985489578e7b9d72bfc2fa02a925.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*Awub7m1KptJstkMr-cxYhA.png"/></div></figure><pre class="my mz na nb gt nc ms nd ne aw nf bi"><span id="ac5b" class="kf kg iq ms b gy ng nh l ni nj"><em class="lx"># Re-fit the k-means model with the selected number of clusters and obtain</em><br/><em class="lx"># cluster predictions for the general population demographics data.</em><br/><br/><em class="lx"># Re-fit the k-means model with the selected number of clusters and obtain</em><br/><em class="lx"># cluster predictions for the general population demographics data.</em><br/><br/>kmeans = KMeans(n_clusters=22)<br/>model_general = kmeans.fit(missing_data_rows_low_pca)</span><span id="0ba6" class="kf kg iq ms b gy nk nh l ni nj">predict_general = model_general.predict(missing_data_rows_low_pca)</span></pre><p id="5c08" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">根据该图，我们可以看到 22 似乎是一个足够的集群数量。后来的变化率是 SSE 极低。</p><h2 id="3ab0" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">将客户数据与人口统计数据进行比较</h2><p id="b8f9" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">在对人口统计数据进行聚类之后，我们对客户统计数据应用相同的数据清理步骤和聚类。目的是看哪个对公司来说是强大的客户群。</p><p id="3a84" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">如果与一般人群相比，客户数据的聚类中有更高比例的人(例如，5%的人被分配到一般人群的聚类，但是 15%的客户数据最接近该聚类的质心)，则这表明该聚类中的人是公司的目标受众。另一方面，聚类中的数据在一般人群中的比例大于客户数据(例如，只有 2%的客户最接近捕获 6%数据的人群质心)表明该人群在目标人口统计之外。</p><figure class="my mz na nb gt nm gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nw"><img src="../Images/7071c44ee9a4df94b20ee6a2be7d205d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jti_BMOsej_ABb8zIKbQvA.png"/></div></div></figure><p id="5536" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated"><strong class="ld ir">分析客户数据过多的聚类</strong></p><pre class="my mz na nb gt nc ms nd ne aw nf bi"><span id="aa75" class="kf kg iq ms b gy ng nh l ni nj"><em class="lx"># What kinds of people are part of a cluster that is overrepresented in the</em><br/><em class="lx"># customer data compared to the general population?</em><br/>over = normalizer.inverse_transform(pca.inverse_transform(customers_clean_pca[np.where(predict_customers==11)])).round()<br/>df_over = pd.DataFrame(data = over, columns = customers_clean.columns)<br/>df_over.head(10)</span></pre><figure class="my mz na nb gt nm gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nx"><img src="../Images/63f8feb30f2c0715400dec7b258f68e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I6z9JHX-twhOVAKJed93Dg.png"/></div></div></figure><p id="640a" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">这个细分市场由年龄在 46 岁到 60 岁之间的个人组成，他们不是财务上的最低要求者。也就是说，他们可能是退休或接近退休的人，渴望消费商品和服务。</p><p id="87bb" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated"><strong class="ld ir">分析客户数据代表性不足的聚类</strong></p><pre class="my mz na nb gt nc ms nd ne aw nf bi"><span id="c006" class="kf kg iq ms b gy ng nh l ni nj"><em class="lx"># What kinds of people are part of a cluster that is underrepresented in the</em><br/><em class="lx"># customer data compared to the general population?</em><br/>under = normalizer.inverse_transform(pca.inverse_transform(customers_clean_pca[np.where(predict_customers==16)])).round()<br/>df_under = pd.DataFrame(data=under, columns=customers_clean.columns)<br/>df_under.head(10)</span></pre><figure class="my mz na nb gt nm gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi ny"><img src="../Images/ed176044f839eae9c41a828fe2d55ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VL3ClWngntHhry6A0qKs1A.png"/></div></div></figure><p id="d838" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">这部分人由较年轻年龄组(45 岁以下)的人组成，失业人口比例较大。</p><h2 id="9d9f" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">项目结束</h2><p id="33ca" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">回想一下，在数据处理步骤中，我们确定丢失数据数量较多的行组最终将被视为一个额外的聚类。上述组中的簇 22 是最后添加的簇。我们可以看到，在大多数集群中，人口百分比和客户百分比之间存在相当大的差异。在大多数集群中，一般人口所占比例过大。例如，在聚类 11 中，客户部分被过度表示；它由年龄在 46 岁到 60 岁之间的人组成，他们都不是财务上的极简主义者。也就是说，他们可能是退休或接近退休的人，渴望消费商品和服务。所以把它当成一个客户群是有道理的。另一方面，在聚类 16 中，客户细分未被充分代表。这部分人由较年轻年龄组(45 岁以下)的人组成，失业人口比例较大。由于他们大多失业，他们很可能依靠社会保障生活。因此，他们没有大量的可支配资金来购买销售给他们的产品。</p><h2 id="37ed" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结束语</h2><p id="eef7" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">如果你坚持到了最后，非常感谢你的阅读:)我知道这些帖子很长，并且包括许多代码片段，但我真的认为忽略机器学习的技术方面对于展示算法如何工作是不可行的。这个项目对我来说是有益的，因为它包含了真实世界的财务数据和真实世界的商业目标。</p><p id="784d" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">我希望这能让你理解无监督机器学习如何应用于商业和营销。如果你想浏览我写的所有代码，你可以在我的<a class="ae nz" href="https://github.com/andreigalanciuc/UNSUPERVISED-MACHINE-LEARNING/blob/master/Identify_Customer_Segments%20(1).ipynb" rel="noopener ugc nofollow" target="_blank"> github </a>上访问我的项目。</p><p id="d4d2" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">在 LinkedIn 上关注我:【https://www.linkedin.com/in/andreigalanchuk/ T2】</p><p id="4ae8" class="pw-post-body-paragraph lb lc iq ld b le ly jr lg lh lz ju lj ko mb ll lm ks md lo lp kw mf lr ls lt ij bi translated">上帝保佑你！</p></div></div>    
</body>
</html>