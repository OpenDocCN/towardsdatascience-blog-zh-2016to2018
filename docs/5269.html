<html>
<head>
<title>Polynomial Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多项式回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/polynomial-regression-bbe8b9d97491?source=collection_archive---------0-----------------------#2018-10-08">https://towardsdatascience.com/polynomial-regression-bbe8b9d97491?source=collection_archive---------0-----------------------#2018-10-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="a0c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是我在机器学习系列的第三篇博客。这个博客需要线性回归的先验知识。如果您不了解线性回归或者需要复习一下，请浏览本系列以前的文章。</p><ul class=""><li id="c437" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/linear-regression-using-python-b136c91bf0a2">使用 Python 进行线性回归</a></li><li id="e147" class="kl km iq jp b jq kv ju kw jy kx kc ky kg kz kk kq kr ks kt bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/linear-regression-on-boston-housing-dataset-f409b7e4a155">波士顿住房数据集的线性回归</a></li></ul><blockquote class="la lb lc"><p id="82e1" class="jn jo ld jp b jq jr js jt ju jv jw jx le jz ka kb lf kd ke kf lg kh ki kj kk ij bi translated">线性回归要求因变量和自变量之间的关系是线性的。如果数据的分布更复杂，如下图所示，会怎么样？线性模型可以用来拟合非线性数据吗？我们怎样才能生成一条最好地捕捉数据的曲线，如下图所示？那么，我们将在这篇博客中回答这些问题。</p></blockquote><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/50b39b8fc9eef240c10550bbf94008cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ELey2wytlZvKYFLbLbhCoA.png"/></div></figure><h2 id="7826" class="lp lq iq bd lr ls lt dn lu lv lw dp lx jy ly lz ma kc mb mc md kg me mf mg mh bi translated"><strong class="ak">目录</strong></h2><ul class=""><li id="9ea8" class="kl km iq jp b jq mi ju mj jy mk kc ml kg mm kk kq kr ks kt bi translated">为什么是多项式回归</li><li id="a498" class="kl km iq jp b jq kv ju kw jy kx kc ky kg kz kk kq kr ks kt bi translated">过度拟合与欠拟合</li><li id="58bf" class="kl km iq jp b jq kv ju kw jy kx kc ky kg kz kk kq kr ks kt bi translated">偏差与方差的权衡</li><li id="7ee2" class="kl km iq jp b jq kv ju kw jy kx kc ky kg kz kk kq kr ks kt bi translated">将多项式回归应用于波士顿住房数据集。</li></ul><h2 id="d4a7" class="lp lq iq bd lr ls lt dn lu lv lw dp lx jy ly lz ma kc mb mc md kg me mf mg mh bi translated">为什么要多项式回归？</h2><p id="ac17" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">为了理解多项式回归的需要，让我们首先生成一些随机数据集。</p><figure class="li lj lk ll gt lm"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="ccee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">生成的数据如下所示</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/2c5f2c4a296da13220851b708e8150c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*dJhMB97nyUB6_OgSECKxEQ.png"/></div></figure><p id="e874" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们对这个数据集应用一个线性回归模型。</p><figure class="li lj lk ll gt lm"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="e84f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最佳拟合线的曲线为</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/ff3f4335a99a2e61674a86b037d1d3c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*yim5OMiku3dNMXEv3GiItg.png"/></div></figure><p id="a5be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，直线无法捕捉数据中的模式。这是<strong class="jp ir">欠装配</strong>的一个例子。计算线性线的 RMSE 和 R 分数得出:</p><pre class="li lj lk ll gt ms mt mu mv aw mw bi"><span id="fb3b" class="lp lq iq mt b gy mx my l mz na">RMSE of linear regression is <strong class="mt ir">15.908242501429998</strong>.<br/>R2 score of linear regression is <strong class="mt ir">0.6386750054827146</strong></span></pre><blockquote class="la lb lc"><p id="ec51" class="jn jo ld jp b jq jr js jt ju jv jw jx le jz ka kb lf kd ke kf lg kh ki kj kk ij bi translated"><strong class="jp ir">为了克服欠拟合，我们需要增加模型的复杂度。</strong></p></blockquote><p id="15a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了生成高阶方程，我们可以添加原始特征的幂作为新特征。线性模型，</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/e93df5a9ca0c036c164caa40b11fc912.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*adrhNj5POluyuFCa9WfBIg.png"/></div></figure><p id="1e88" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以转化为</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/85eef6c5afcd75e6cf6c5b074f2d74eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*rL76rQ1hhrvPjAQFwvpN4w.png"/></div></figure><blockquote class="la lb lc"><p id="5051" class="jn jo ld jp b jq jr js jt ju jv jw jx le jz ka kb lf kd ke kf lg kh ki kj kk ij bi translated">这仍然被认为是<strong class="jp ir">线性模型</strong>，因为与特征相关的系数/权重仍然是线性的。x 只是一个特征。然而，我们正在拟合的曲线实际上是二次曲线。</p></blockquote><p id="3761" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了将原始特征转换成它们的高阶项，我们将使用由<code class="fe nd ne nf mt b">scikit-learn</code>提供的<code class="fe nd ne nf mt b">PolynomialFeatures</code>类。接下来，我们使用线性回归来训练模型。</p><figure class="li lj lk ll gt lm"><div class="bz fp l di"><div class="mq mr l"/></div></figure><pre class="li lj lk ll gt ms mt mu mv aw mw bi"><span id="2aed" class="lp lq iq mt b gy mx my l mz na">To generate polynomial features (here 2nd degree polynomial)<br/>------------------------------------------------------------</span><span id="9850" class="lp lq iq mt b gy ng my l mz na">polynomial_features = PolynomialFeatures(degree=2)<br/>x_poly = polynomial_features.fit_transform(x)</span><span id="e543" class="lp lq iq mt b gy ng my l mz na">Explaination<br/>------------</span><span id="2870" class="lp lq iq mt b gy ng my l mz na">Let's take the first three rows of X: <br/>[[-3.29215704]<br/> [ 0.79952837]<br/> [-0.93621395]]</span><span id="6edc" class="lp lq iq mt b gy ng my l mz na">If we apply polynomial transformation of degree 2, the feature vectors become</span><span id="5a8c" class="lp lq iq mt b gy ng my l mz na">[[-3.29215704 10.83829796]<br/> [ 0.79952837  0.63924562]<br/> [-0.93621395  0.87649656]]</span></pre><p id="29d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对变换后的要素拟合线性回归模型，给出了下图。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/9e231158922e11266de5610a127e77a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*uJtlIlaT-o3DDh5VaGsy4A.png"/></div></figure><p id="45ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从图中可以清楚地看出，二次曲线比线性曲线更能拟合数据。计算二次图的 RMSE 和 R 分数给出:</p><pre class="li lj lk ll gt ms mt mu mv aw mw bi"><span id="a505" class="lp lq iq mt b gy mx my l mz na">RMSE of polynomial regression is <strong class="mt ir">10.120437473614711</strong>.<br/>R2 of polynomial regression is <strong class="mt ir">0.8537647164420812</strong>.</span></pre><blockquote class="la lb lc"><p id="f229" class="jn jo ld jp b jq jr js jt ju jv jw jx le jz ka kb lf kd ke kf lg kh ki kj kk ij bi translated"><strong class="jp ir">我们可以看到，与直线相比，RMSE 降低了，R 值增加了。</strong></p></blockquote><p id="253a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们尝试将三次曲线(次数=3)拟合到数据集，我们可以看到它通过的数据点比二次曲线和线性曲线多。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/cf9ea3445f705eb89dbaf7f96a601dff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Pa_Ma_X-IQNMn7ZbtT5lPA.png"/></div></figure><p id="f9f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">三次曲线的度量标准是</p><pre class="li lj lk ll gt ms mt mu mv aw mw bi"><span id="0575" class="lp lq iq mt b gy mx my l mz na">RMSE is <strong class="mt ir">3.449895507408725</strong><br/>R2 score is <strong class="mt ir">0.9830071790386679</strong></span></pre><p id="c06a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是在数据集上拟合线性、二次和三次曲线的比较。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/33f4dec4ffff92d106928129556746b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*eXe8BOlfP-yTbHZtdnGSMg.png"/></div></figure><p id="0797" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们进一步增加次数到 20，我们可以看到曲线通过更多的数据点。下面是 3 度和 20 度的曲线比较。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/b940e47dc937f4b15df63d35f6220e50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*5UNAfcSSf7okQLQSKj0HVA.png"/></div></figure><p id="3a2c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于 degree=20，模型也捕捉数据中的噪声。这是<strong class="jp ir">过度拟合</strong>的一个例子。即使这个模型通过了大部分数据，它也无法对看不见的数据进行归纳。</p><blockquote class="la lb lc"><p id="47fd" class="jn jo ld jp b jq jr js jt ju jv jw jx le jz ka kb lf kd ke kf lg kh ki kj kk ij bi translated"><strong class="jp ir">为了防止过度拟合，我们可以添加更多的训练样本，这样算法就不会学习系统中的噪声，可以变得更一般化。</strong>(注意:如果数据本身就是噪声，添加更多数据可能会有问题)。</p></blockquote><p id="301d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们如何选择一个最优模型？要回答这个问题，我们需要理解偏差与方差的权衡。</p><h2 id="9c16" class="lp lq iq bd lr ls lt dn lu lv lw dp lx jy ly lz ma kc mb mc md kg me mf mg mh bi translated"><strong class="ak">偏差与方差的权衡</strong></h2><p id="fe10" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated"><strong class="jp ir">偏差</strong>是指由于模型在拟合数据时过于简单的假设而产生的误差。高偏差意味着模型无法捕捉数据中的模式，这导致<strong class="jp ir">欠拟合</strong>。</p><p id="9c99" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">方差</strong>是指由于复杂模型试图拟合数据而产生的误差。高方差意味着模型通过了大多数数据点，导致<strong class="jp ir">过度拟合</strong>数据。</p><p id="42b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下图总结了我们的学习。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/fe94223c9c2a79c78baeadee632aed84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zOl_ztYqnzyWRkBffeOsRQ.png"/></div></div></figure><p id="6d5e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从下图中我们可以观察到，随着模型复杂性的增加，偏差减少，方差增加，反之亦然。理想情况下，机器学习模型应该具有<strong class="jp ir">低方差和低偏差</strong>。但实际上不可能两者兼得。因此，为了实现一个在训练和看不见的数据上都表现良好的好模型，进行了<strong class="jp ir">折衷</strong>。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/699d2654a150233f961db4d332152efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*kADA5Q4al9DRLoXck6_6Xw.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Source: <a class="ae ku" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank">http://scott.fortmann-roe.com/docs/BiasVariance.html</a></figcaption></figure><p id="691d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">到目前为止，我们已经讨论了多项式回归背后的大部分理论。现在，让我们在我们在<a class="ae ku" rel="noopener" target="_blank" href="/linear-regression-on-boston-housing-dataset-f409b7e4a155">之前的</a>博客中分析的波士顿住房数据集上实现这些概念。</p><h2 id="3d72" class="lp lq iq bd lr ls lt dn lu lv lw dp lx jy ly lz ma kc mb mc md kg me mf mg mh bi translated"><strong class="ak">对住房数据集应用多项式回归</strong></h2><p id="561f" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">从下图可以看出，<code class="fe nd ne nf mt b">LSTAT</code>与目标变量<code class="fe nd ne nf mt b">MEDV</code>有轻微的非线性变化。在训练模型之前，我们将把原始特征转换成高次多项式。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nr"><img src="../Images/d80a53acbed5013703ae4ff93679b56a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vtUji1IIN5U-xrgC46yDuw.png"/></div></div></figure><p id="f441" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们定义一个函数，它将原始特征转换为给定次数的多项式特征，然后对其应用线性回归。</p><figure class="li lj lk ll gt lm"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="4767" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们调用上面的次数为 2 的函数。</p><figure class="li lj lk ll gt lm"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="95d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用多项式回归的模型性能:</p><pre class="li lj lk ll gt ms mt mu mv aw mw bi"><span id="800f" class="lp lq iq mt b gy mx my l mz na"><strong class="mt ir">The model performance for the training set</strong> <br/>------------------------------------------- <br/>RMSE of training set is 4.703071027847756 <br/>R2 score of training set is 0.7425094297364765 </span><span id="7ecb" class="lp lq iq mt b gy ng my l mz na"><strong class="mt ir">The model performance for the test set</strong><br/>------------------------------------------- <br/>RMSE of test set is 3.784819884545044 <br/>R2 score of test set is 0.8170372495892174</span></pre><p id="4154" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这比我们在<a class="ae ku" rel="noopener" target="_blank" href="/linear-regression-on-boston-housing-dataset-f409b7e4a155">之前的</a>博客中使用线性回归获得的结果要好。</p><p id="8696" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个故事到此为止。这个 Github <a class="ae ku" href="https://github.com/animesh-agarwal/Machine-Learning/tree/master/PolynomialRegression" rel="noopener ugc nofollow" target="_blank"> repo </a>包含了这个博客的所有代码，用于 Boston housing 数据集的完整 Jupyter 笔记本可以在<a class="ae ku" href="https://github.com/animesh-agarwal/Machine-Learning-Datasets/blob/master/boston-housing/Polynomial_Regression.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="5e1a" class="lp lq iq bd lr ls lt dn lu lv lw dp lx jy ly lz ma kc mb mc md kg me mf mg mh bi translated"><strong class="ak">结论</strong></h2><p id="8b05" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">在这个机器学习系列中，我们介绍了线性回归和多项式回归，并在波士顿住房数据集上实现了这两个模型。</p><p id="4a13" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将在下一篇博客中讨论逻辑回归。</p><p id="4edf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读！！</p></div></div>    
</body>
</html>