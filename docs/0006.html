<html>
<head>
<title>Big Data Processing using Spark and HBase</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Spark 和 HBase 进行大数据处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tagging-26-million-publications-within-26-minutes-adding-spark-to-code-536e567eb021?source=collection_archive---------1-----------------------#2017-01-07">https://towardsdatascience.com/tagging-26-million-publications-within-26-minutes-adding-spark-to-code-536e567eb021?source=collection_archive---------1-----------------------#2017-01-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f13b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在 26 分钟内标记 2600 万份出版物—为代码添加“火花”</h2></div><p id="282a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在<a class="ae lb" href="https://www.innoplexus.com/" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir"/></a>的数据工程团队，在过去的两个月里一直在不知疲倦地工作，以减少标记我们整个出版物数据库所需的时间，我非常兴奋地与大家分享这一成果。该数据库拥有多达 2600 万份出版物。我们完成这项工作的最初方法是每天只标记一百万份出版物，这意味着整个数据需要 26 天，这意味着我们的实时产品在更新期间会有这么多天处于停机状态。我们不得不把这个时间缩短到几个小时，甚至缩短到几分钟。我将深入探讨这些方法以及我们是如何解决的。</p><h2 id="ddf7" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated"><strong class="ak">为什么需要标签？</strong></h2><p id="c224" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">我们的产品 iPlexus.ai 为用户提供了一个可以找到出版物的界面。专利、临床试验、论文等。与生命科学研究相关。比方说，一位研究“结肠直肠癌”的院士希望看到该领域的最新出版物、专利或临床试验。为了筛选所有关于“结肠直肠癌”的资源，我们必须对我们的出版物数据库进行分类。所以我们想根据内容给我们的出版物添加标签。一个类似的例子是网飞根据类型对电影进行分类。</p><h2 id="fdec" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated"><strong class="ak">我们的标记方法</strong></h2><ul class=""><li id="d190" class="ma mb iq kh b ki lv kl lw ko mc ks md kw me la mf mg mh mi bi translated">创建标记术语的数据库:</li></ul><p id="e15d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们确定我们必须标记整个数据库，我们称为“领域专家”的优秀团队在数据科学家团队的帮助下开始工作。他们必须找出不同研究领域的所有相关标签，以及我们需要寻找哪些术语来添加这些标签。此外，我们意识到在出版物的内容中可以找到开源的术语数据库，以及与这些术语相对应的标签。结合在一起，他们提出了一个本体论。这个本体充当我们的知识库。在许多其他事情中，它有 620 万个要查找的标签术语和与之对应的标签。</p><ul class=""><li id="4b98" class="ma mb iq kh b ki kj kl km ko mj ks mk kw ml la mf mg mh mi bi translated">标记算法:</li></ul><p id="372d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们必须想出一个有效的标记算法。我们必须找到一种方法来利用这个本体并标记原始数据。在我们的出版物数据库中，我们有“文章标题”和“文章摘要”以及其他条目，如“作者”、“出版日期”、“期刊标题”。决定在“文章标题”和“文章摘要”中搜索标记术语的存在，然后相应地进行标记。标记术语可以是单词或短语，因此需要在“原样”的基础上匹配它们，字母的情况除外。标签术语中不同单词的顺序必须与文章标题和摘要中的顺序相同。此外，要检查整个短语，而不仅仅是短语的一部分。</p><p id="0135" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于出版物和标签术语这两个数据库非常庞大，我们不能选择遍历这两个数据库，因为复杂度是 O(n*n)。因此，我们选择创建一个术语标记图，其中关键字是我们将在出版物中寻找的术语，值是我们将添加到出版物中的标记。Redis 是一个显而易见的选择，因为它创建了一个内存中的数据库用于更快的查找，复杂度为 O(1)。现在，文章标题和摘要必须分解成与这些关键字相匹配的结构，以便我们可以查找它们的标签。因此，我们决定从标题和摘要中创建<a class="ae lb" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"> ngrams </a>，并在 Redis 服务器中寻找这些 ngrams，如果存在的话，获取相应的标签，并相应地标记出版物。一个标签术语中的最大字数是 7，所以我们创建了 7 或更少的 ngrams。这样，复杂度将是 O(n)。</p><h2 id="5f37" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">完成它</h2><ul class=""><li id="2e05" class="ma mb iq kh b ki lv kl lw ko mc ks md kw me la mf mg mh mi bi translated">初始方法:Python 代码</li></ul><p id="b26c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们的工作是实现这个算法。出版物数据存储在 mongoDB 中。标记术语存储在 Redis 服务器中。我们从编写一个 python 脚本开始，从 mongoDB 中读取发布数据，从这些数据中创建 n gram，从 Redis(如果存在)中提取这些 n gram 的值，并添加一个标签键和标签。当运行在 6 核服务器上，每个内存 8 GB 时，它每天标记大约 100 万个出版物。另一次尝试使用 60 个内核，却发现用了 6 天。</p><p id="e683" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们必须在最多一小时内完成。因此，一次头脑风暴会议被安排来彻底改变方法以减少时间。然后决定在代码中加入“Spark”。</p><ul class=""><li id="8708" class="ma mb iq kh b ki kj kl km ko mj ks mk kw ml la mf mg mh mi bi translated">最终方法:Spark/Scala 代码</li></ul><p id="4ab5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">出版物数据库转移到 HBase 进行分布式存储。实现了将数据从 mongo 写入 HBase 的 python 代码。事实上，spark 本身是用 scala 编写的，我们可以导入 java 的所有库以及它自己的库，所以我们选择用 scala 编写我们的标记算法。Spark 在内存中完成所有处理，比 mapreduce 编码模式快 100 倍。</p><p id="d994" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">出版物数据作为 RDD(弹性分布式数据集)从 HBase 提取到 Spark。类似地，从 Redis 服务器获取标记术语。为了更快地查找，我们创建了一个映射，其中关键字是这些标记术语，值是我们要标记的术语。对于出版物，创建少于或等于 7 个单词的 ngrams。这些 ngrams 在我们创建的映射中查找，如果存在，相应地标记值。</p><h2 id="2bac" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">结果</h2><p id="81da" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">怀着热切的希望，代码在一个 16 核服务器上运行，每个服务器有 64 GB RAM，标记整个出版物数据集只需要 26 分钟。我们对结果非常满意。更重要的是，我们的首席执行官在我们通知他的时候宣布了对我们的现场奖励。</p></div></div>    
</body>
</html>