<html>
<head>
<title>Finding the Cost Function of Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">寻找神经网络的成本函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9?source=collection_archive---------2-----------------------#2018-09-01">https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9?source=collection_archive---------2-----------------------#2018-09-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4ac8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一步一步:神经网络背后的数学</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e7b0042f0ff1ed6fb5fa50685f8e97dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j5iI07ojAm27P5zKcCY8Hw.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Title image: <a class="ae kv" href="https://pixabay.com/en/math-symbols-blackboard-classroom-1500720/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="9eb5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特伦斯·帕尔和杰瑞米·霍华德的论文，<a class="ae kv" href="https://arxiv.org/abs/1802.01528" rel="noopener ugc nofollow" target="_blank"> <em class="ls">深度学习所需的矩阵演算</em> </a> <em class="ls">，</em>提供了很多关于深度学习库(如 Tensorflow 或 PyTorch)如何真正工作的见解。如果不理解深度学习背后的数学，我们所做的只是编写几行抽象代码——建立模型，编译它，训练它，评估它——而没有真正学会欣赏支持所有这些功能的所有复杂的错综复杂之处。</p><p id="aa3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇(以及接下来的几篇)文章将是我从 Terence 和 Jeremy 的论文中学到的思考。本文将介绍这个问题，我们将在接下来的文章中解决它。我将解释大部分的数学知识，并加入一些我的见解，但要了解更多信息，请查看原始论文。</p><p id="563a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些文章(和论文)都假设了高中水平的微积分基础知识(即导数规则和如何应用它们)。如果你想重温的话，可以看看可汗学院的视频。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="6404" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是我们的问题。我们有一个只有一层(为了简单起见)和一个损失函数的神经网络。这一层是一个简单的全连接层，只有一个神经元，许多权重<em class="ls"> w₁，w₂，w₃ </em> …，一个偏置<em class="ls"> b </em>，以及一个 ReLU 激活。我们的损失函数是常用的均方误差(MSE)。知道了我们的网络和损失函数，我们如何调整权重和偏差来最小化损失？</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="d8ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了减少损失，我们使用梯度下降的概念。正如这里的<a class="ae kv" rel="noopener" target="_blank" href="/the-beginners-guide-to-gradient-descent-c23534f808fd">所解释的</a>(如果你不熟悉梯度下降的工作原理，请阅读此处)，梯度下降计算损失函数的斜率，然后根据该斜率将权重和偏差转移到更低的损失。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi gj"><img src="../Images/ff34a5bc63c6362adbde96635d518281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f4CewARPRQvSUftfaf_FOA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 1: Shifting the weights and biases of to minimize loss</figcaption></figure><p id="6d39" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要找到损失函数的斜率。然而，在此之前，让我们定义一下损失函数。MSE 简单地平方每个网络输出和真实标签之间的差，并取平均值。下面是 MSE 方程，其中<em class="ls"> C </em>是我们的损失函数(也称为<em class="ls">代价函数</em>)，<em class="ls"> N </em>是训练图像的数量，<strong class="ky ir"> y </strong>是真实标签的向量(<strong class="ky ir">y</strong>=【<em class="ls">目标(</em> <strong class="ky ir"> x </strong> ₁ <em class="ls">)，目标(</em><strong class="ky ir">x</strong>₂<em class="ls">)…目标(</em>【t28)(如果你还没有注意到，粗体字<strong class="ky ir">中的变量是向量。)</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ma"><img src="../Images/f317bac54158626390a9e3074ecc0680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LSYIwG58Ik_BLcEN1U4_Yg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 2: Loss function</figcaption></figure></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="ded6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以进一步扩展这个等式。网络输出是什么？我们将一个向量输入——姑且称之为<strong class="ky ir">x</strong>——给我们的全连接层。该层的激活是我们的网络输出。我们的全连接层的激活是什么？向量输入中的每一项都乘以一定的权重。然后，将所有这些产品加在一起，并在此基础上添加一个偏差。最后，该值通过 ReLu 传递，形成全连接层中一个神经元的激活。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mb"><img src="../Images/35fdb622e875da1ae2c2470a318a2e4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TBR9KCDb7IUljaFqjJh7YA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 3: Fully-connected layer, expanded. The orange circles are the input values, the blue circle is the output value (the prediction, since our network only has 1 layer), and the gray circles are just intermediate values used in the calculation. Each red line represents multiplication by a certain weight, the pink lines represent summing up all the values, and the green line represents addition with a certain bias.</figcaption></figure><p id="d24b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对每个输入值和每个权重的乘积求和，本质上是输入<strong class="ky ir"> x </strong>和一个权重向量(姑且称之为<strong class="ky ir"> w </strong>)之间的向量点积。ReLU 只是一个将任何负值转换为 0 的函数。让我们将其重命名为<em class="ls"> max(0，z) </em>函数，如果 z 为正，则返回 z，如果 z 为负，则返回 0。</p><p id="6bb8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">综上所述，我们得到了神经元激活的方程式:</p><p id="6c59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">激活(</em><strong class="ky ir"><em class="ls">x</em></strong><em class="ls">)= max(</em>0，<strong class="ky ir">w</strong>∙<strong class="ky ir">x</strong>+<em class="ls">b)</em></p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="d767" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们把它代入损失函数。因为我们使用多个输入进行训练，所以让我们将<strong class="ky ir"> X </strong>定义为所有输入的集合:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mc"><img src="../Images/83c9ae6e43731674dbfe5e4f0747c4cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1yk9R7aMFVJMbWmtgKb6jQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 4: X // <a class="ae kv" href="https://arxiv.org/pdf/1802.01528.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="8c59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们只有一层(有一个神经元)，这个神经元的激活就是我们模型的预测。因此，我们可以用我们的激活 in 代替损失函数中的<em class="ls"> o </em>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi md"><img src="../Images/e4b06482fb7769d1b123fd1c4cab09e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ME6U8e-3bPSgiRtRyCLcfg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 5: Loss function with activation substituted in</figcaption></figure><p id="f7cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后替换激活函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi me"><img src="../Images/187621d8d6992b6835ff8872b8776854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WGnJteHoZFMRLjF82Nibqg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 6: Loss function</figcaption></figure><p id="04c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我们要求斜率的损失函数。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="84d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了找到斜率，我们必须找到损失函数的导数。然而，不是任何导数，它必须是相对于权重和偏差的偏导数(因为这些是我们正在调整的值)。</p><p id="9eaf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查看<a class="ae kv" href="https://medium.com/@reina.wang/step-by-step-the-math-behind-neural-networks-ac15e178bbd" rel="noopener">第 2 部分</a>了解如何计算偏导数！</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="f322" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">跳到其他文章:</p><ul class=""><li id="f397" class="mf mg iq ky b kz la lc ld lf mh lj mi ln mj lr mk ml mm mn bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-ac15e178bbd">第二部分:偏导数</a></li><li id="b9af" class="mf mg iq ky b kz mo lc mp lf mq lj mr ln ms lr mk ml mm mn bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-d002440227fb">第三部分:向量微积分</a></li><li id="b459" class="mf mg iq ky b kz mo lc mp lf mq lj mr ln ms lr mk ml mm mn bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/calculating-gradient-descent-manually-6d9bee09aa0b">第 4 部分:把所有的东西放在一起</a></li></ul><p id="6ace" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里下载原论文<a class="ae kv" href="https://arxiv.org/abs/1802.01528" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="606c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你喜欢这篇文章，别忘了留下一些掌声！如果您有任何问题或建议，请在下面留下您的评论:)</p></div></div>    
</body>
</html>