<html>
<head>
<title>Linear Regression using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Python 进行线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2?source=collection_archive---------3-----------------------#2018-10-05">https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2?source=collection_archive---------3-----------------------#2018-10-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="8a15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">线性回归通常是每个数据科学家遇到的第一个机器学习算法。这是一个简单的模型，但每个人都需要掌握它，因为它为其他机器学习算法奠定了基础。</p><p id="1879" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">线性回归可以用在哪里？<br/> </strong>这是一种非常强大的技术，可以用来了解影响盈利能力的因素。它可以通过分析前几个月的销售数据来预测未来几个月的销售。它还可以用来获得关于客户行为的各种见解。在博客结束时，我们将建立一个类似下图的模型，即确定一条最符合数据的线。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/0e8566c92006f36d1a506e67e8fb958f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*LEmBCYAttxS6uI6rEyPLMQ.png"/></div></figure><p id="1346" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是我将要讲述的机器学习系列的第一篇博客。人们可能会被网络上大量关于机器学习算法的文章淹没。我写这篇博客的目的有两个。它可以作为进入机器学习领域的人的指南，也可以作为我的参考。</p><h2 id="5900" class="kt ku iq bd kv kw kx dn ky kz la dp lb jy lc ld le kc lf lg lh kg li lj lk ll bi translated">目录</h2><ol class=""><li id="717e" class="lm ln iq jp b jq lo ju lp jy lq kc lr kg ls kk lt lu lv lw bi translated">什么是线性回归</li><li id="da39" class="lm ln iq jp b jq lx ju ly jy lz kc ma kg mb kk lt lu lv lw bi translated">线性回归假设</li><li id="5f48" class="lm ln iq jp b jq lx ju ly jy lz kc ma kg mb kk lt lu lv lw bi translated">训练线性回归模型</li><li id="70e3" class="lm ln iq jp b jq lx ju ly jy lz kc ma kg mb kk lt lu lv lw bi translated">评估模型</li><li id="7480" class="lm ln iq jp b jq lx ju ly jy lz kc ma kg mb kk lt lu lv lw bi translated">scikit-learn 实现</li></ol><h2 id="cd55" class="kt ku iq bd kv kw kx dn ky kz la dp lb jy lc ld le kc lf lg lh kg li lj lk ll bi translated">什么是线性回归</h2><p id="c1bf" class="pw-post-body-paragraph jn jo iq jp b jq lo js jt ju lp jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">线性回归模型的目标是找到一个或多个特征(自变量)和一个连续目标变量(因变量)之间的关系。当只有一个特征时，称为<em class="mf">单变量</em>线性回归，如果有多个特征，称为<em class="mf">多元</em>线性回归。</p><h2 id="ccfb" class="kt ku iq bd kv kw kx dn ky kz la dp lb jy lc ld le kc lf lg lh kg li lj lk ll bi translated"><strong class="ak">线性回归假设</strong></h2><p id="ff9f" class="pw-post-body-paragraph jn jo iq jp b jq lo js jt ju lp jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">线性回归模型可以由下面的等式表示</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/2166ded9802a34dba586bfe78b4048c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*A32yDrkQKIU0Z-Nf7JD9Pg.png"/></div></figure><ul class=""><li id="270e" class="lm ln iq jp b jq jr ju jv jy mh kc mi kg mj kk mk lu lv lw bi translated"><em class="mf"> Y </em>是预测值</li><li id="491e" class="lm ln iq jp b jq lx ju ly jy lz kc ma kg mb kk mk lu lv lw bi translated"><em class="mf"> θ </em> ₀是偏置项。</li><li id="7b37" class="lm ln iq jp b jq lx ju ly jy lz kc ma kg mb kk mk lu lv lw bi translated"><em class="mf"> θ </em> ₁,…，<em class="mf"> θ </em> ₙ为模型参数</li><li id="d8ba" class="lm ln iq jp b jq lx ju ly jy lz kc ma kg mb kk mk lu lv lw bi translated"><em class="mf"> x </em> ₁、<em class="mf"> x </em> ₂,…、<em class="mf"> x </em> ₙ为特征值。</li></ul><p id="5abd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上述假设也可以表示为</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/79e7fae15462dec0e2ee71427bfcf773.png" data-original-src="https://miro.medium.com/v2/resize:fit:162/format:webp/1*7pjcby7gESbxk54njwCUZg.png"/></div></figure><p id="7e1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在哪里</p><ul class=""><li id="38c1" class="lm ln iq jp b jq jr ju jv jy mh kc mi kg mj kk mk lu lv lw bi translated"><em class="mf"> θ </em>是模型的参数向量，包括偏差项<em class="mf"> θ </em> ₀</li><li id="c49c" class="lm ln iq jp b jq lx ju ly jy lz kc ma kg mb kk mk lu lv lw bi translated"><em class="mf"> x </em>是<em class="mf"> x </em> ₀ =1 的特征向量</li></ul><h2 id="b305" class="kt ku iq bd kv kw kx dn ky kz la dp lb jy lc ld le kc lf lg lh kg li lj lk ll bi translated"><strong class="ak">数据集</strong></h2><p id="fd8d" class="pw-post-body-paragraph jn jo iq jp b jq lo js jt ju lp jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">让我们创建一些随机数据集来训练我们的模型。</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="dea8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用上述代码生成的数据集的曲线图如下所示:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/0857532e09b78030620b4366cb763ac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*w2w9hzrSYR-bSlmr0vEdhg.png"/></div></figure><h2 id="5a71" class="kt ku iq bd kv kw kx dn ky kz la dp lb jy lc ld le kc lf lg lh kg li lj lk ll bi translated"><strong class="ak">训练一个线性回归模型</strong></h2><p id="863a" class="pw-post-body-paragraph jn jo iq jp b jq lo js jt ju lp jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">在这里，模型的训练意味着找到参数，以使模型最适合数据。</p><p id="9daf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="mf">我们如何确定最佳拟合线？</em> </strong> <br/>预测值与观测值之间的<em class="mf">误差</em>最小的线称为最佳拟合线或<strong class="jp ir">回归</strong>线。这些误差也被称为<strong class="jp ir"> <em class="mf">残差</em> </strong>。残差可以通过从观察数据值到回归线的垂直线来可视化。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/ca0c93d20a6909bc3cb02ecaa25ce461.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*6lzTBeDt_J39XPvhI4NAMg.png"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk">Image Credits: <a class="ae mt" href="http://wiki.engageeducation.org.au/further-maths/data-analysis/residuals/" rel="noopener ugc nofollow" target="_blank">http://wiki.engageeducation.org.au/further-maths/data-analysis/residuals/</a></figcaption></figure><p id="4764" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了定义和测量我们模型的误差，我们将成本函数定义为残差平方和。成本函数表示为</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/c8339a0280813d3677d299dddc88a780.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*cUnU05zgo8BEzjm22oszLw.png"/></div></figure><p id="7fec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中假设函数<em class="mf"> h(x) </em>表示为</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/a152d31a3be13509c4cc9446409237d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*U918Cc_opofphkgbP9Kkpg.png"/></div></figure><p id="33de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">并且<em class="mf"> m </em>是我们的数据集中训练样本的总数。</p><blockquote class="mw mx my"><p id="668f" class="jn jo mf jp b jq jr js jt ju jv jw jx mz jz ka kb na kd ke kf nb kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="iq">为什么我们取残差的平方而不是残差的绝对值？</em> </strong>我们更希望惩罚远离回归线的点，而不是靠近回归线的点。</p></blockquote><p id="f946" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的目标是找到模型参数，使成本函数最小。我们将使用梯度下降来找到这个。</p><p id="3c27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">梯度下降</strong></p><p id="6e1d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度下降是许多机器学习算法中使用的通用优化算法。它反复调整模型的参数，以最小化成本函数。梯度下降的步骤概述如下。</p><ol class=""><li id="21e9" class="lm ln iq jp b jq jr ju jv jy mh kc mi kg mj kk lt lu lv lw bi translated">我们首先用一些随机值初始化模型参数。这也被称为<strong class="jp ir"> <em class="mf">随机初始化</em> </strong>。</li><li id="c744" class="lm ln iq jp b jq lx ju ly jy lz kc ma kg mb kk lt lu lv lw bi translated">现在我们需要测量成本函数如何随着参数的变化而变化。因此，我们计算成本函数 w.r.t 对参数<em class="mf"> θ </em> ₀、<em class="mf"> θ </em> ₁、…、<em class="mf"> θ </em> ₙ的偏导数</li></ol><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/a710ce5b078c542d538670889aefc5e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*1DbFTHQGx1yW_bygxQjDCw.png"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/085c96b66179f01a89cc5fc9a151ab3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*CuDapkX9GEW7kTWd5g6uoA.png"/></div></figure><p id="87a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">类似地，成本函数 w.r.t 对任何参数的偏导数可以表示为</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/3dc90e279aab9c94d6007fb106c5d90e.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*lM1cKDM6kxfIhPWVRf2n_g.png"/></div></figure><p id="e945" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以立刻计算所有参数的偏导数，使用</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/56586a66d54598c0eb1a220b41476352.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*PhHcLbn6vlzVYbi9xPJakQ.png"/></div></figure><p id="48c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<em class="mf"> h(x) </em>为</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/a152d31a3be13509c4cc9446409237d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*U918Cc_opofphkgbP9Kkpg.png"/></div></figure><p id="d27f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.计算导数后，我们更新参数如下</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/8bdb98e2f35b353d8dba7e83845b3e65.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*eCIJ2D7iEzr16XjNIIAX-Q.png"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/4089c4ce36e09d3e32e8b42cf7262d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*Gv_g5WHAGZUIeZLBmYncLw.png"/></div></figure><p id="94a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<strong class="jp ir"> <em class="mf"> α </em> </strong> <em class="mf"> </em>是<strong class="jp ir"> <em class="mf">学习参数</em> </strong>。</p><p id="8ecc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以一次更新所有参数，</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/4feee93fb01d64b2cace6fe7a5cba699.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*RtIwNf5fIewyRy_gVR5h1w.png"/></div></figure><p id="0455" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们重复步骤 2、3，直到成本函数收敛到最小值。<em class="mf"> </em>如果<em class="mf"> α </em>的值太小，代价函数需要更大的时间收敛。如果<em class="mf"> α </em>过大，梯度下降可能超过最小值，最终可能无法收敛。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/9a50c127b97313ac6bee60c458d2a478.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n79s9gvd0E8ALe9dLUEKAw.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk">Source: Andrew Ng’s course on Coursera</figcaption></figure><p id="3542" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了演示梯度下降算法，我们用 0 初始化模型参数。方程变成<em class="mf"> Y = 0。</em>梯度下降算法现在尝试更新参数<em class="mf"> </em>的值，以便我们达到最佳拟合线。</p><p id="ba4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当学习速度非常慢时，梯度下降需要更长的时间来找到最佳拟合线。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/30f94071b19fb338e58866eb3f6d9f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*W00VUhSMPC5BQ_F8VJJy4Q.gif"/></div></figure><p id="aeee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当学习率正常时</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/e0a7dbfda75c244627d89a0546ff7118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*58WjBE3gy7oU_WHd142jZA.gif"/></div></figure><p id="1ece" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当学习率任意高时，梯度下降算法会超过最佳拟合线，甚至可能找不到最佳拟合线。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/e7974ea1219cc1ebc2da75aad13cb24c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*qAlHti2Sw2Ln4R_ZP5Qjow.gif"/></div></figure><h2 id="bbf1" class="kt ku iq bd kv kw kx dn ky kz la dp lb jy lc ld le kc lf lg lh kg li lj lk ll bi translated"><strong class="ak">从头开始实施线性回归</strong></h2><p id="cb97" class="pw-post-body-paragraph jn jo iq jp b jq lo js jt ju lp jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">下面给出了具有梯度下降的线性回归的完整实现。</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="7287" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型参数如下所示</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="22a4" class="kt ku iq no b gy ns nt l nu nv">The coefficient is [2.89114079]<br/>The intercept is [2.58109277]</span></pre><p id="3338" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最佳拟合线的绘图</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/1eb075b360c8a27d0e13770909eb9f84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*7XWNzoEn6fzwzzLmsYUWlQ.png"/></div></figure><p id="8035" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面给出了成本函数与迭代次数的关系图。我们可以观察到，成本函数最初随着每次迭代而降低，并且在近 100 次迭代之后最终收敛。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/d0327c1af51ab09337ce3e9be8166c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*-sj4ufUNJuep1ePZloSgvg.png"/></div></figure><p id="2116" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">到目前为止，我们已经从头开始实施线性回归，并使用梯度下降来寻找模型参数。但是我们的模型有多好呢？我们需要一些方法来计算我们模型的准确性。让我们看看各种指标来评估我们上面构建的模型。</p><h2 id="bcd4" class="kt ku iq bd kv kw kx dn ky kz la dp lb jy lc ld le kc lf lg lh kg li lj lk ll bi translated"><strong class="ak">评估模型的性能</strong></h2><p id="03b7" class="pw-post-body-paragraph jn jo iq jp b jq lo js jt ju lp jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">我们将使用均方根误差(<strong class="jp ir"> RMSE </strong>)和决定系数(<strong class="jp ir"> R </strong>得分)来评估我们的模型。</p><p id="6196" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> RMSE </strong>是残差平方和的平均值的平方根。</p><p id="e7fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">RMSE 的定义是</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/5be069279af3d35cf95b128a06d4072f.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*-j7StdUGMAFvLGx7hD-Nzg.png"/></div></figure><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="00fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">RMSE 的分数是 2.764182038967211。</p><p id="c7f0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> R </strong>得分或<strong class="jp ir">决定系数</strong>解释了通过使用最小二乘回归可以将因变量的总方差减少多少。</p><p id="d950" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="mf"> R </em> </strong>由</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e812611e2d9638a8ce1baa6b778226f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/format:webp/1*dUAJL0vVJw7gfb5cq3eluA.png"/></div></figure><p id="4a82" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="mf"> SSₜ </em> </strong>是我们把观测值的平均值作为预测值的误差总和。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/6e8c0055d918706f01baf9e7b9975415.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*GAqVi6BDifcmeQeIXDaeiA.png"/></div></figure><p id="2edc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"><em class="mf"/></strong><em class="mf"/>是残差的平方和</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nz"><img src="../Images/d01533120bfe9daf2cd7005ceb613af4.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*YES7smJ6HquuEfYEZOxoWg.png"/></div></div></figure><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="mm mn l"/></div></figure><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="049b" class="kt ku iq no b gy ns nt l nu nv"><strong class="no ir"><em class="mf">SSₜ - </em></strong>69.47588572871659<br/><strong class="no ir"><em class="mf">SSᵣ - </em></strong>7.64070234454893<br/><strong class="no ir">R²</strong> score - 0.8900236785122296</span></pre><blockquote class="mw mx my"><p id="0463" class="jn jo mf jp b jq jr js jt ju jv jw jx mz jz ka kb na kd ke kf nb kh ki kj kk ij bi translated">如果我们使用观察值的平均值作为预测值，方差是 69.47588572871659，如果我们使用回归，总方差是 7.6407023454893。通过回归分析，我们将预测误差降低了 89%。</p></blockquote><p id="5a9d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们尝试使用流行的 scikit-learn 库来实现线性回归。</p><h2 id="2624" class="kt ku iq bd kv kw kx dn ky kz la dp lb jy lc ld le kc lf lg lh kg li lj lk ll bi translated"><strong class="ak"> Scikit-learn 实现</strong></h2><p id="6807" class="pw-post-body-paragraph jn jo iq jp b jq lo js jt ju lp jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">sckit-learn 是一个非常强大的数据科学库。完整的代码如下所示</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="df46" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型参数和模型的性能指标如下所示:</p><pre class="km kn ko kp gt nn no np nq aw nr bi"><span id="2e12" class="kt ku iq no b gy ns nt l nu nv">The coefficient is [[2.93655106]]<br/>The intercept is [2.55808002]<br/>Root mean squared error of the model is 0.07623324582875013.<br/>R-squared score is 0.9038655568672764.</span></pre><p id="32d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这几乎类似于我们从零开始实现线性回归时所获得的结果。</p><p id="dba1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个博客到此为止。完整的代码可以在这个 GitHub <a class="ae mt" href="https://github.com/animesh-agarwal/Machine-Learning/tree/master/LinearRegression" rel="noopener ugc nofollow" target="_blank"> repo </a>中找到。</p><h2 id="6fe8" class="kt ku iq bd kv kw kx dn ky kz la dp lb jy lc ld le kc lf lg lh kg li lj lk ll bi translated"><strong class="ak">结论</strong></h2><p id="18cc" class="pw-post-body-paragraph jn jo iq jp b jq lo js jt ju lp jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">我们已经学习了线性回归和梯度下降的概念。我们还使用 scikit-learn 库实现了该模型。</p><p id="7dd4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本系列的<a class="ae mt" href="https://medium.com/@animeshblog/linear-regression-on-boston-housing-dataset-f409b7e4a155" rel="noopener">下一篇</a>博客中，我们将获取一些原始数据集并构建一个线性回归模型。</p></div></div>    
</body>
</html>