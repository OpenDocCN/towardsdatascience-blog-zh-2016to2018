<html>
<head>
<title>Why random forests outperform decision trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么随机森林优于决策树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-random-forests-outperform-decision-trees-1b0f175a0b5?source=collection_archive---------2-----------------------#2018-10-28">https://towardsdatascience.com/why-random-forests-outperform-decision-trees-1b0f175a0b5?source=collection_archive---------2-----------------------#2018-10-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c92d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">说明了两个直观的原因</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f30d4777ff8cd1631f04d3def00cedbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TCX9EI7YX0InzAkn"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Pine forests at Rampart Lakes. Photo by @sakulich</figcaption></figure><p id="2a24" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">随机森林由多个单独的树组成，每个树都基于训练数据的随机样本。它们通常比单一决策树更准确。下图显示了随着更多树的添加，决策边界变得更加准确和稳定。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/f7dea6738f5a69e0e72f83d4a31cc774.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/1*EFBVZvHEIoMdYHjvAZg8Zg.gif"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Decision boundary from random forests (as more trees are added)</figcaption></figure><p id="5b36" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里我们将提供随机森林优于单决策树的两个原因。</p><p id="2509" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">特征空间中更高的分辨率</strong></p><p id="c458" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">树木未修剪</strong>。虽然像 CART 这样的单个决策树经常被修剪，但随机森林树是完全生长和未修剪的，因此，自然地，特征空间被分成更多和更小的区域。</p><p id="4763" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">树木多种多样</strong>。每个随机森林树是在随机样本上学习的，并且在每个节点上，考虑一组随机特征用于分裂。这两种机制都创造了树木的多样性。</p><p id="af7a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下图显示了两个随机树，每个都有一个分割。对于每棵树，可以给两个区域分配不同的标签。通过组合这两棵树，有四个区域可以被不同地标记。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/6bac73d81b37b2394eaf1d9995c259d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ciGKOLMqfTrPKZy5eoocFQ.png"/></div></div></figure><p id="a9bb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">未修剪的和多样的树导致特征空间中的高分辨率。对于连续要素，这意味着更平滑的决策边界，如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lt"><img src="../Images/2e185425e8e3e8d56b91bf30b1167c36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EyowUWk9fliAvDASU2HRwg.png"/></div></div></figure><p id="bee4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">处理过拟合</strong></p><p id="d0a5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">单个决策树需要修剪以避免过度拟合。下面显示了来自未修剪树的判定边界。边界更平滑，但会出现明显的错误(过度拟合)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lu"><img src="../Images/63eb819d8471c6b1802186f3b280fd33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DxKwnYi9NECMAF3StZ_qYQ.png"/></div></div></figure><p id="27f0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">那么随机森林如何在不过度拟合的情况下建造未修剪的树呢？</strong></p><p id="94bd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于下面的两类(蓝色和红色)问题，两个 splits x1 = 3 和 x2=3 都可以完全分开两类。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lv"><img src="../Images/b9409d0fb72689454624adad8abe79b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SM8WjmviTDhRn4NNFm8cPg.png"/></div></div></figure><p id="5cd4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，这两种分裂导致了非常不同的决策界限。决策树通常使用第一个变量进行拆分，因此训练数据中变量的顺序决定了决策边界。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lw"><img src="../Images/dc78f9307f39eb7204237ffaa0626d31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TJAAMjlVQDBDWvaSD_4U3w.png"/></div></div></figure><p id="b1a5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在考虑随机森林。对于用于训练树的每个随机样本，样本中丢失红点的概率为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lx"><img src="../Images/dcc717ba2527e1e65ade55de077cd311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pti9hU_qrMTY73IT_4E3zw.png"/></div></div></figure><p id="8e8a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，大约三分之一的树是用所有蓝色数据构建的，并且总是预测蓝色类。另外 2/3 的树在训练数据中有红点。由于在每个节点都考虑了随机的特征子集，我们预计大约 1/3 的树使用 x1，其余 1/3 使用 x2。这两种树的分割如下图所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ly"><img src="../Images/6d5ee7e84a5c11ea0f817a2502fd4de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s-EFDGJK1SHwgEZRafk8XA.png"/></div></div></figure><p id="2c3a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过聚合这三种类型的树，下面显示的决策边界现在对于 x1 和 x2 是对称的。只要有足够多的树，边界应该是稳定的，不依赖于变量的排序等无关信息。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lz"><img src="../Images/d3a65ab4507fa8a2c0382e7d4074739e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CnE7u1StRFIqcVTY_LTAzQ.png"/></div></div></figure><p id="37c1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">随机森林中的随机性和投票机制很好地解决了过度适应问题。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><p id="07ad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然随机森林是准确的，但它们被认为是黑盒模型(很难解释)。这篇<a class="ae mh" rel="noopener" target="_blank" href="/random-forest-3a55c3aca46d">文章</a>阐述了如何解读它们。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><p id="f54c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在<a class="ae mh" href="https://dataanalyticsbook.info/" rel="noopener ugc nofollow" target="_blank"> dataanalyticsbook.info </a>可以找到我的书的更多内容。</p></div></div>    
</body>
</html>