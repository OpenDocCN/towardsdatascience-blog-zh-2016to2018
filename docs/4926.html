<html>
<head>
<title>Batch normalization: theory and how to use it with Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">批量规范化:理论和如何与 Tensorflow 一起使用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad?source=collection_archive---------2-----------------------#2018-09-16">https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad?source=collection_archive---------2-----------------------#2018-09-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/afe45a5846771bfd989e11873d50e74d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8mSbe4y1nczVWR64"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">“time-lapse photography of highway road” by <a class="ae kc" href="https://unsplash.com/@cmreflections?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Clément M.</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="caa8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不久前，深度神经网络还很难训练，在合理的时间内让复杂的模型收敛是不可能的。如今，我们有许多技巧来帮助他们收敛，实现更快的训练，并解决当我们想要训练深度学习模型时出现的任何类型的麻烦。本文将探讨其中的一个技巧:<strong class="kf ir">批量规范化。</strong></p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h2 id="39c2" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">为什么你应该使用它</h2><p id="0d36" class="pw-post-body-paragraph kd ke iq kf b kg mb ki kj kk mc km kn ko md kq kr ks me ku kv kw mf ky kz la ij bi translated">为了理解什么是批处理规范化，首先我们需要解决它试图解决的问题。</p><p id="299d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通常，为了训练一个神经网络，我们对输入数据做一些预处理。例如，我们可以将所有数据标准化，使其类似于正态分布(也就是说，零均值和一元方差)。我们为什么要做这个预处理？原因有很多，其中一些是:防止像 sigmoid 函数这样的非线性激活函数过早饱和，确保所有输入数据都在相同的值范围内，等等。</p><p id="28c4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是问题出现在中间层，因为激活的分布在训练期间不断变化。这减慢了训练过程，因为每一层都必须在每个训练步骤中学会适应新的分布。这个问题被称为<strong class="kf ir">内部协变移位</strong>。</p><p id="77d7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么…如果我们<em class="mg">强制</em>每一层的输入在每一个训练步骤中具有近似相同的分布，会发生什么？</p><h2 id="cf46" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">事实真相</h2><p id="b320" class="pw-post-body-paragraph kd ke iq kf b kg mb ki kj kk mc km kn ko md kq kr ks me ku kv kw mf ky kz la ij bi translated">批量标准化是一种我们可以用来标准化每一层输入的方法，以解决内部协变量偏移问题。</p><p id="0970" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在训练期间，批处理规范化层执行以下操作:</p><ol class=""><li id="1bad" class="mh mi iq kf b kg kh kk kl ko mj ks mk kw ml la mm mn mo mp bi translated">计算层输入的平均值和方差。</li></ol><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/bb3c78b7e6ffe275af1aafdafd1708b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*_6xWFQC0jb9_T1yz9iqOWA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Batch statistics for step 1</figcaption></figure><p id="e5f7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.使用之前计算的批次统计数据对图层输入进行归一化。</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/6fb848d29d8f0309a927f9e716dfccd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*I7YluVpp6-mfMoj4AZZI5g.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Normalization of the layers input in step 2</figcaption></figure><p id="cb5a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.缩放和移动以获得层的输出。</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/d5325e626bd18ccfcad193555119b55d.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*G8-bO54pVT5eJCJ7MBabdA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Scaling and shifting the normalized input for step 3</figcaption></figure><p id="5569" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，<em class="mg"> γ </em>和<em class="mg"> β </em>是在训练期间学习的<strong class="kf ir">以及网络的原始参数。</strong></p><p id="4d21" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，如果每批有<em class="mg"> m </em>个样品，那么<em class="mg"> j </em>批:</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/4746170d6172dfe5f9ba50ef3887dd6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*j9KW8tVE8XTEu6lcP1dFoQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Inference formulas</figcaption></figure><p id="1bca" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">编辑:</strong>在测试(或推断)时，均值和方差是固定的。使用之前计算的每个训练批次的平均值和方差来估计它们。</p><h2 id="65e8" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">我们如何在 Tensorflow 中使用它</h2><p id="d9a6" class="pw-post-body-paragraph kd ke iq kf b kg mb ki kj kk mc km kn ko md kq kr ks me ku kv kw mf ky kz la ij bi translated">幸运的是，Tensorflow API 已经在<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization" rel="noopener ugc nofollow" target="_blank">TF . layers . batch _ normalization</a>层中实现了所有这些数学运算。</p><p id="4685" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了在您的模型中添加批处理规范化层，您只需使用以下代码:</p><figure class="mr ms mt mu gt jr"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="2deb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">获得 Tensorflow 文档中所述的更新 ops 非常重要，因为在训练时间内，必须更新层的移动方差和移动平均值。如果不这样做，批量标准化<strong class="kf ir">将无法工作</strong>，网络<strong class="kf ir">将无法按预期训练</strong>。</p><p id="b90e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">声明一个占位符来告诉网络它是处于训练时间还是推理时间也是有用的(我们已经讨论过训练和测试时间的区别)。</p><p id="5b5a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意这一层有<strong class="kf ir">很多</strong> <strong class="kf ir">更多的</strong>参数(你可以在文档中查看它们)，但是这些是你应该使用的基本工作代码。</p><h2 id="c97d" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">参考</h2><p id="e6fc" class="pw-post-body-paragraph kd ke iq kf b kg mb ki kj kk mc km kn ko md kq kr ks me ku kv kw mf ky kz la ij bi translated">如果您对批量标准化感到好奇，我建议您看看这篇论文和视频:</p><ul class=""><li id="242c" class="mh mi iq kf b kg kh kk kl ko mj ks mk kw ml la na mn mo mp bi translated"><a class="ae kc" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank"> Sergey Ioffe，Christian Szegedy，批量标准化:通过减少内部协变量转移加速深度网络训练，2015 年</a></li><li id="370c" class="mh mi iq kf b kg nb kk nc ko nd ks ne kw nf la na mn mo mp bi translated"><a class="ae kc" href="https://arxiv.org/abs/1603.09025" rel="noopener ugc nofollow" target="_blank">蒂姆·库伊曼斯，尼古拉斯·巴拉斯，塞萨尔·洛朗，阿格拉尔·居勒赫尔，亚伦·库维尔，循环批次标准化，2016 年</a></li><li id="87c2" class="mh mi iq kf b kg nb kk nc ko nd ks ne kw nf la na mn mo mp bi translated"><a class="ae kc" href="https://arxiv.org/abs/1806.02892" rel="noopener ugc nofollow" target="_blank"> Mahdi M. Kalayeh，Mubarak Shah，通过分离批量标准化模型中的变异模式加快训练，2018 年</a></li><li id="e018" class="mh mi iq kf b kg nb kk nc ko nd ks ne kw nf la na mn mo mp bi translated"><a class="ae kc" href="https://www.youtube.com/watch?v=wEoyxE0GP2M&amp;index=6&amp;list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk" rel="noopener ugc nofollow" target="_blank">斯坦福大学工程学院 CS231n 第六讲|训练神经网络 I，2017 </a></li><li id="eaf2" class="mh mi iq kf b kg nb kk nc ko nd ks ne kw nf la na mn mo mp bi translated"><a class="ae kc" href="https://arxiv.org/abs/1807.01702" rel="noopener ugc nofollow" target="_blank"> Daejin Jung，Wonkyung Jung，Byeongho Kim，Sunjung Lee，Wonjong Rhee，Jung Ho Ahn，重组批次正常化以加速 CNN 培训，2018 年</a></li></ul></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="55b1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请随时关注我的 Twitter 或 LinkedIn T21，让我知道你对这篇文章的看法。感谢阅读！</p></div></div>    
</body>
</html>