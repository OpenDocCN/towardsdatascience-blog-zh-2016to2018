<html>
<head>
<title>End-to-end Distributed ML using AWS EMR, Apache Spark (Pyspark) and MongoDB Tutorial with MillionSongs Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用AWS EMR、Apache Spark (Pyspark)和MongoDB Tutorial(包含百万首歌曲数据)的端到端分布式ML</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/end-to-end-distributed-ml-using-aws-emr-apache-spark-pyspark-and-mongodb-tutorial-with-4d1077f68381?source=collection_archive---------2-----------------------#2018-01-19">https://towardsdatascience.com/end-to-end-distributed-ml-using-aws-emr-apache-spark-pyspark-and-mongodb-tutorial-with-4d1077f68381?source=collection_archive---------2-----------------------#2018-01-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/804e83adb1366cc0552030519efba032.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J28sEr2RzBwMgvQYuMZl2g.jpeg"/></div></div></figure><p id="c7e1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇文章中，我将提到如何使用Python Spark API pyspark以分布式方式运行ML算法。我们还将了解如何设置AWS EMR实例以在云上运行我们的应用程序，如何设置MongoDB服务器作为NoSQL数据库以存储非结构化数据(如JSON、XML ),以及如何通过使用pyspark功能快速进行数据处理/分析。</p><p id="afd1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">要求:</strong></p><ul class=""><li id="f120" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">AWS帐户(或者您可以在本地运行)</li><li id="85ba" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">Python 2.7(用于运行将要提供的脚本)</li></ul><h1 id="8967" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated"><strong class="ak"> 1。设置EMR实例</strong></h1><p id="3d06" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">什么是亚马逊EMR？(网址:【https://aws.amazon.com/emr/ T4】)</p><blockquote class="mo mp mq"><p id="6219" class="jy jz mr ka b kb kc kd ke kf kg kh ki ms kk kl km mt ko kp kq mu ks kt ku kv ij bi translated">Amazon EMR提供了一个托管的Hadoop框架，可以轻松、快速且经济高效地跨可动态扩展的Amazon EC2实例处理大量数据。还可以在亚马逊EMR中运行其他流行的分布式框架如<a class="ae mn" href="https://aws.amazon.com/emr/details/spark/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>、<a class="ae mn" href="https://aws.amazon.com/emr/details/hbase/" rel="noopener ugc nofollow" target="_blank"> HBase </a>、<a class="ae mn" href="https://aws.amazon.com/emr/details/presto/" rel="noopener ugc nofollow" target="_blank"> Presto、</a>和<a class="ae mn" href="https://aws.amazon.com/blogs/big-data/use-apache-flink-on-amazon-emr/" rel="noopener ugc nofollow" target="_blank"> Flink </a>，与亚马逊S3、亚马逊DynamoDB等其他AWS数据仓库中的数据进行交互。</p></blockquote><p id="1751" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">尽管这一步是可选的，以便运行脚本并自上而下地了解正在发生的事情，但我们可能需要分布式计算的主要原因通常是我们的本地机器规格无法进行我们想要的处理、分析或建模。</p><p id="3e92" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们还可以启动多个EC2实例并配置主节点和工作节点，但所有这些步骤实际上都由EMR负责。简而言之，EMR将允许我们在主实例上运行我们想要运行的作业后，自动分发这些作业。否则，我们应该在所有节点上配置和安装Spark。</p><p id="b229" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面你可以看到Spark框架是如何分配工作的。驱动程序运行主进程，将其转换为任务，并为执行者安排任务。然后工人(执行者)运行这些任务。</p><figure class="mw mx my mz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mv"><img src="../Images/fc0dba3404fad5ae23895266e7386f29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*94sLpx-6ScSxcm5dTAuO_w.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Driver and Executor Architecture in Spark</figcaption></figure><p id="d52d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了启动我们的第一个电子病历实例，我们需要登录到aws.amazon.com，然后控制台会出现。</p><figure class="mw mx my mz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/ddc5b93c8f2f406a7fbb771031b2f61c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f6ZDvUsLfXdh-q97-D3aig.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">select “EMR” here</figcaption></figure><figure class="mw mx my mz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nf"><img src="../Images/4ccdc440c83f2af26146fb64b44eb8f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TNdhep2SLP-Cx7onECGKPA.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">click “Create Cluster”</figcaption></figure><figure class="mw mx my mz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/60175a5faf8cb9926da41692fa345e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yx8cBIjT7jqnSrTOISoMYg.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Here we should select “Spark” under Applications and define how many instances we want for our application. For example 1 for master and 2 for core nodes (executors which will run tasks). So here we will be distributing the job into 2 worker nodes.</figcaption></figure><p id="b990" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您还应该给出一个密钥对，以便以后ssh进入节点并运行您的应用程序。一旦你点击创建你的集群，它将开始引导，一旦一切准备就绪，主和核心将等待。关于电子病历的一个关键点是，一旦开始就无法停止。这意味着您不能像常规EC2实例一样停止和启动EMR实例。因此，运行所有应用程序，然后终止EMR是一个好的做法。如果你试图在工作没有完成的情况下停止，你将需要重新开始。数据处理或分析工作的另一个常见做法是使用亚马逊S3。EMR、S3、Spark在一起相处得很好。您可以将数据存储在S3，然后读取和处理数据，而无需实际存储在您的节点中，通过spark处理后，您可以将数据写回S3并终止EMR。</p><p id="f793" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">EMR实例准备就绪后，您可以使用您的pem密钥和主服务器的公共DNS进入您的终端并进行ssh。</p><pre class="mw mx my mz gt nh ni nj nk aw nl bi"><span id="c6f5" class="nm ll iq ni b gy nn no l np nq">$ ssh -i .ssh/mykey.pem hadoop@ec2–xx–xxx–xxx-xx.us-west-2.compute.amazonaws.com</span></pre><figure class="mw mx my mz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/c4195979ba856e8c47ba2e424b9f000f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KKMUtETa-qLUHxJBuN3l2w.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">EMR terminal</figcaption></figure><h1 id="ae8f" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">2.获取数据</h1><p id="f161" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">在本教程中，我将使用百万首歌曲数据集的子集，但是任何超过几个GB的数据集都会支持我们的观点。下面是多种方式获取数据的链接:<a class="ae mn" href="https://labrosa.ee.columbia.edu/millionsong/pages/getting-dataset" rel="noopener ugc nofollow" target="_blank">https://lab Rosa . ee . Columbia . edu/million song/pages/getting-dataset</a>。尽管他们在EC2映像中提供了这个数据集，但为了完整起见，我们将假设这是我们自己的数据，我们试图将其导入AWS。我们将在这里探索的数据称为MillionSongSubset，它随机选择了百万首歌曲的子集，即10，000首歌曲的集合。你可以参考“百万歌曲子集”的链接，并从那里下载tar.gz的文件。您可以将数据下载到您的本地，然后再下载到主节点和工作节点的scp，但这将花费scp很长时间。所以我在这里推荐使用一个通过Chrome的curlwget扩展，将tar.gz文件直接下载到主节点和工作节点。我在Chrome上使用的扩展叫做CurlWget。尝试在你的节点中存储数据/mnt/</p><p id="9ea1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下载完数据后，您会看到有两个主文件夹data/A/和data/B/这些文件夹有子目录，最终在每个. h5层次数据文件下有单首歌曲数据，关于数据的更多信息在获取数据集url中提供。</p><h1 id="9cee" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">3.用派斯帕克把H5变成RDD</h1><p id="9448" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">到目前为止，我们已经启动了我们的EMR实例，并将数据放入所有节点的相同路径中，现在我们将数据转换为Spark RDD，以便使用pyspark及其分布式计算功能。RDD(弹性分布式数据集)是spark表示数据并将其存储在分区中的方式。数据可以来自文本、csv、json格式或数据库。</p><p id="b40f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我找不到将H5文件转换成RDD的直接方法，但这个链接<a class="ae mn" href="https://www.hdfgroup.org/2015/03/from-hdf5-datasets-to-apache-spark-rdds/." rel="noopener ugc nofollow" target="_blank">https://www . hdf group . org/2015/03/from-HD F5-datasets-to-Apache-spark-rdds/</a>启发我编写自己的脚本，使用pyspark并行进行这种转换。基本思想是在一个python列表中收集所有h5文件，并根据这个列表创建一个RDD。然后，我们可以很容易地做我们想要的转换使用地图功能在这个RDD。</p><pre class="mw mx my mz gt nh ni nj nk aw nl bi"><span id="8713" class="nm ll iq ni b gy nn no l np nq"># 8 is number of partitions to distribute the data<br/># For local applications it will be distributed to 8 threads</span><span id="7743" class="nm ll iq ni b gy ns no l np nq">file_paths = sc.parallelize(h5_file_paths, 8)</span><span id="4f41" class="nm ll iq ni b gy ns no l np nq"># Read and convert to python dict in parallel</span><span id="96cd" class="nm ll iq ni b gy ns no l np nq">songs_rdd = file_paths.map(lambda x: h5todict(x))<br/>songs = songs_rdd.collect()</span></pre><p id="3009" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这里，我们也收集歌曲作为python列表，以便创建我们的MongoDB数据库，因为教程的一部分也要显示MongoDB，但我们可能只坚持使用包含python字典的RDD(在这种情况下，每个元素都是一个python字典，它在层次关系中有一个歌曲数据)。拥有RDD就足以通过pyspark继续处理、分析和建模。</p><p id="95c1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">边注</strong>:这里提供了转换的完整脚本:<a class="ae mn" href="https://github.com/KeremTurgutlu/distcomputing/blob/master/MillionSongs-Project/h5toMongo.py" rel="noopener ugc nofollow" target="_blank">https://github . com/KeremTurgutlu/distcomputing/blob/master/million songs-Project/H5 to mongo . py</a>。所有节点都应该安装了所需的python包，比如h5py、json、numpy…</p><p id="b21f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为此，您需要ssh到您的工作节点和主节点。要ssh到worker节点，您应该在master中拥有您的pem密钥，并通过master终端使用该密钥进行ssh。之后，您需要做的就是pip安装尚未提供的所需软件包。</p><h1 id="7f40" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">4.设置MongoDB</h1><p id="6da7" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">在这一步中，我们将在主节点中安装和设置mongodb。可以按照这里的步骤<a class="ae mn" href="https://docs.mongodb.com/manual/tutorial/install-mongodb-on-red-hat/" rel="noopener ugc nofollow" target="_blank">https://docs . MongoDB . com/manual/tutorial/install-MongoDB-on-red-hat/</a>进行安装。安装后，您应该创建一个目录，用于存储您创建的数据库。同样，在/mnt下将它们创建为/mnt/mongo/db更有利于磁盘的使用。您还需要通过运行以下命令来设置对此目录的权限:</p><pre class="mw mx my mz gt nh ni nj nk aw nl bi"><span id="74b8" class="nm ll iq ni b gy nn no l np nq">$ <!-- -->sudo chown -R `id -un` /mnt/mongo/db/</span></pre><p id="eb9a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">运行此步骤后:</p><pre class="mw mx my mz gt nh ni nj nk aw nl bi"><span id="00de" class="nm ll iq ni b gy nn no l np nq">$ mongod --dbpath /mnt/mongo/db</span></pre><p id="06d7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在MongoDB服务器应该在监听连接。在一个单独的终端上，您可以运行<strong class="ka ir"> mongo </strong>并启动mongo shell。在mongo shell中，您可以使用<strong class="ka ir"> show dbs查看当前数据库。</strong></p><p id="dd28" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，与MongoDB建立有效连接。接下来，我们需要在所有节点上安装pymongo包，它是MongoDB的python API。之后，我们可以在我们的主机上运行通过github repo提供的脚本。为了运行该脚本，我们还应该在同一路径中有<a class="ae mn" href="https://github.com/KeremTurgutlu/distcomputing/blob/master/MillionSongs-Project/user_definition.py" rel="noopener ugc nofollow" target="_blank">https://github . com/KeremTurgutlu/dist computing/blob/master/million songs-Project/user _ definition . py</a>。在用户定义中，我们定义了执行器驱动内存、最大结果和开销内存，以便在运行应用程序时不会遇到内存问题。</p><p id="d467" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个脚本将读取h5文件，并在默认情况下在名为“MillionSongs”的数据库下创建一个名为“songs”的mongoDB集合(当然，您可以在user_definition.py中为数据库和集合指定其他名称)。你可能认为数据库是一个普通的SQL数据库，集合是一个SQL表，文档(单曲的dict)是表中的一行。</p><h1 id="98d6" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">5.分布式分析和ML</h1><p id="e9b1" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">此时，我们有一个名为“MillionSongs”的MongoDB数据库，并位于一个名为“Songs”的集合下(或者您指定的数据库和集合)。对于您自己的数据，为了有一个数据库和集合，例如来自位于您当前目录下的json文件，您可以从另一个终端运行这个命令，而mongod在另一个终端上运行:</p><pre class="mw mx my mz gt nh ni nj nk aw nl bi"><span id="0f24" class="nm ll iq ni b gy nn no l np nq">$ mongoimport --db dbName --collection collectionName --file fileName.json</span></pre><p id="20d1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用mongodb，可以像在SQL中一样通过查询数据来非常容易地过滤、选择、聚集和执行许多其他操作。在我们的歌曲集合中，每首歌曲都被表示为一个文档(一个python字典),这是由于我们通过h5tomongo.py脚本创建和插入它们的方式。</p><p id="6dd9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的第一个任务是预测某首歌是在哪一个年代发行的。有关数据字段的更多详细说明，您可能会发现此链接很有帮助:<a class="ae mn" href="https://labrosa.ee.columbia.edu/millionsong/pages/field-list" rel="noopener ugc nofollow" target="_blank">https://lab Rosa . ee . Columbia . edu/million song/pages/field-list</a>。这项任务在Kaggle和UCI网站上都有推广。我们将使用segments_timbre来完成我们的任务，segments _ timbre是一种类似MFCC主成分分析的2d特征，涉及歌曲的纹理、歌曲的能量、歌曲的可跳性和歌曲的年份。</p><figure class="mw mx my mz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/9f5c0c14348e33777f2eff4c11b64d32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*312K1gj2rACkifoos05B3A.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">2d features for 12 segments</figcaption></figure><p id="ba11" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">首先，让我们从mongodb中收集我们想要的数据:</strong></p><p id="11e7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们通过定义具有分层字段名的字典来定义我们想要返回的字段(例如在我们的文档中year具有analysis -&gt; songs -&gt; year层次结构)，1表示我们想要返回的字段，0表示我们不想要的字段。默认情况下，还会返回文档的_id，这就是我们明确取消选择的原因。您可以看到，我们在find函数中定义了一个过滤器，其中创建了train_year_data和test_year_data。测试数据将是缺少年份字段的数据(在原始数据中被估算为0)。我们的任务可能是用训练数据建立一个好的预测模型，以便进一步估算这个测试集中缺失的年份。</p><pre class="mw mx my mz gt nh ni nj nk aw nl bi"><span id="5da2" class="nm ll iq ni b gy nn no l np nq">from pymongo import MongoClient</span><span id="e3ef" class="nm ll iq ni b gy ns no l np nq">client = MongoClient()<br/>db = client[dbname]<br/>collection = db[collection_name]</span><span id="ae92" class="nm ll iq ni b gy ns no l np nq">fields = {'musicbrainz.songs.year':1,                    'analysis.segments_timbre':1,                    'analysis.songs.energy':1,                    'analysis.songs.danceability':1,                    'metadata.songs.title':1,                    'metadata.songs.artist_name': 1,<br/>'_id':0}</span><span id="e7be" class="nm ll iq ni b gy ns no l np nq">train_year_data = collection.find({'musicbrainz.songs.year':{'$gte':1920}}, fields)<br/>test_year_data = collection.find({'musicbrainz.songs.year':{'$eq':0}}, fields)</span></pre><p id="c356" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">其次，让我们创建rdd和数据帧:</strong></p><p id="5af0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Spark SQL支持我们将用于输入机器学习算法的数据帧，它也非常方便地支持pandas数据帧。从MongoDB collection过渡到Spark RDD非常简单:</p><pre class="mw mx my mz gt nh ni nj nk aw nl bi"><span id="5866" class="nm ll iq ni b gy nn no l np nq"># create train and test rdds<br/>train_year_rdd = sc.parallelize(list(train_year_data))<br/>test_year_rdd = sc.parallelize(list(test_year_data))</span></pre><p id="fa1a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将通过取所有段的协方差和每个段的平均值，从timbre _ segments中创建一个1d特征向量。总计12个分段的90个特征。我们将通过map应用两个自定义函数，以获得创建Spark SQL数据框架所需的特性。因为train_year_rdd和test_year_rdd仍然是层次化的python dict格式，我们不能用它来创建数据帧以输入到ML算法中。你可以在这里找到这些步骤的源代码:<a class="ae mn" href="https://github.com/KeremTurgutlu/distcomputing/blob/master/MillionSongs-Project/predict_year.py" rel="noopener ugc nofollow" target="_blank">https://github . com/KeremTurgutlu/distcomputing/blob/master/million songs-Project/predict _ year . py</a></p><pre class="mw mx my mz gt nh ni nj nk aw nl bi"><span id="96a2" class="nm ll iq ni b gy nn no l np nq"># create train and test dataframes</span><span id="3f36" class="nm ll iq ni b gy ns no l np nq">train_full_df = train_year_rdd.map(lambda d: getYearFeatures(d)).\<br/>                               map(lambda d: getDFeatures(d)).toDF()</span><span id="8039" class="nm ll iq ni b gy ns no l np nq">test_df = test_year_rdd.map(lambda d: getYearFeatures(d)).\<br/>                        map(lambda d: getDFeatures(d, True)).toDF()</span></pre><p id="c1a2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">创建培训和验证集:</p><pre class="mw mx my mz gt nh ni nj nk aw nl bi"><span id="f6d6" class="nm ll iq ni b gy nn no l np nq"># split to train and validation dataframes<br/>train_df, val_df = train_full_df.randomSplit([0.7, 0.3])</span></pre><p id="4779" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">创建矢量汇编程序:</p><p id="74df" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Vector assembler获取所需列名的列表，这些列名将被输入ML算法，组合它们并为优化的spark ML做好准备。</p><pre class="mw mx my mz gt nh ni nj nk aw nl bi"><span id="6f78" class="nm ll iq ni b gy nn no l np nq"># define vector assembler and transform dataframes into correct format<br/># cache train and validation dataframes for fast iterations</span><span id="1dc6" class="nm ll iq ni b gy ns no l np nq">va = VectorAssembler(inputCols=x_cols, outputCol='features')<br/>train_va=va.transform(train_df).select('features','decade').cache()<br/>val_va=va.transform(val_df).select('features', 'decade').cache()<br/>test_va=va.transform(test_df).select('features', 'decade').cache()</span></pre><p id="fe1b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Pyspark ML包提供了多种型号、变压器和许多选项。查阅文档就好:<a class="ae mn" href="http://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html" rel="noopener ugc nofollow" target="_blank">http://spark . Apache . org/docs/2 . 2 . 0/API/python/py spark . ml . html</a>个人应用。几乎所有的分布式ML算法都是通过这个API提供的，而不是深度学习。对于深度学习，你可以检查H20.ai的苏打水:<a class="ae mn" href="https://www.h2o.ai/sparkling-water/" rel="noopener ugc nofollow" target="_blank">https://www.h2o.ai/sparkling-water/</a>或者简单地使用现代的GPU，甚至使用Pytorch的nn.DataParallel进行多次处理</p><p id="b368" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">既然我们有了向量组装器，现在我们可以进行实际的分布式训练。我将展示运行逻辑回归模型的代码，但是repo中提供的脚本同时运行逻辑回归和随机森林。</p><pre class="mw mx my mz gt nh ni nj nk aw nl bi"><span id="29af" class="nm ll iq ni b gy nn no l np nq"># define model and fit </span><span id="12b9" class="nm ll iq ni b gy ns no l np nq">lr = LogisticRegression(featuresCol='features', labelCol='decade',regParam=0.01, maxIter=1000, fitIntercept=True)<br/>lr_model = lr.fit(train_va)</span></pre><p id="a75c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">或者我们可以简单地进行交叉验证的网格搜索</p><pre class="mw mx my mz gt nh ni nj nk aw nl bi"><span id="14d0" class="nm ll iq ni b gy nn no l np nq"># create cross validation<br/>cv = CrossValidator().setEstimator(lr).setEvaluator(evaluator).setNumFolds(5)<br/>#ParamGridBuilder() – combinations of parameters and their values.<br/>paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [1000]).addGrid(lr.regParam, [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]).build()<br/>#setEstimatorParamMaps() takes ParamGridBuilder().<br/>cv.setEstimatorParamMaps(paramGrid)<br/>cvmodel = cv.fit(train_va)</span></pre><p id="bef2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于准确性不够好，我们通过使用多级F1分数来评估模型。</p><p id="f7b3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">运行提供的脚本:</strong></p><p id="7bad" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了通过终端运行spark作业，您需要使用spark-submit命令。在将您想要的脚本克隆或获取到一个文件夹中，并在h5toMongo.py中定义了正确的file _ roots以及在user_definition中定义了所需的内存分配之后，您可以运行:</p><pre class="mw mx my mz gt nh ni nj nk aw nl bi"><span id="80ed" class="nm ll iq ni b gy nn no l np nq">$ spark-submit h5toMongo.py</span></pre><p id="bad4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">和</p><pre class="mw mx my mz gt nh ni nj nk aw nl bi"><span id="79ed" class="nm ll iq ni b gy nn no l np nq">$ spark-submit predict_year.py</span></pre><p id="6a66" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我希望这个简短的教程对你有所帮助，有很多内容需要你通过文档和经验来学习。因此，如果您有任何建议、推荐或问题，请告诉我。</p><p id="ff2a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>