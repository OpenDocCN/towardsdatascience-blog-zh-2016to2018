<html>
<head>
<title>Estimating Probabilities with Bayesian Modeling in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 中的贝叶斯模型估计概率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/estimating-probabilities-with-bayesian-modeling-in-python-7144be007815?source=collection_archive---------0-----------------------#2018-11-28">https://towardsdatascience.com/estimating-probabilities-with-bayesian-modeling-in-python-7144be007815?source=collection_archive---------0-----------------------#2018-11-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/d84a18dda5aad64ad483c99f2263c32c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WXd6UTHKPpjm_nkMUXuqlQ.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">(<a class="ae jd" href="https://www.adventure-journal.com/2016/03/where-bears-have-the-right-of-way/" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><div class=""/><div class=""><h2 id="512f" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">概率编程在 Python 中的一个简单应用</h2></div><p id="c281" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">像最好的项目一样，它从几条推特开始:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="lv lw l"/></div></figure><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="lv lw l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Twitter is a<a class="ae jd" href="https://www.becomingadatascientist.com/2015/10/04/how-to-use-twitter-to-learn-data-science-or-anything/" rel="noopener ugc nofollow" target="_blank"> great resource for data science</a>!</figcaption></figure><p id="b814" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这似乎是一个简单的问题——患病率与观察到的数据完全相同(50%的狮子，33%的老虎和 17%的熊),对吗？如果你相信我们所做的观察是对潜在真理的完美再现，那么是的，这个问题再简单不过了。然而，<a class="ae jd" href="https://rationalwiki.org/wiki/Bayesian" rel="noopener ugc nofollow" target="_blank">作为一个贝叶斯</a>，这种世界观以及随后的推理却令人深感不满。</p><p id="5841" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，我们如何确定这一次去保护区的旅行代表了所有的旅行？如果我们在冬天熊冬眠的时候去呢？考虑到有限的数据，我们需要将<em class="lx">不确定性</em>纳入我们的估算中。第二，我们如何将<em class="lx">先前</em>对形势的信念整合到这个估计中？如果我们从朋友那里听说保护区里每种动物的数量相等，那么这肯定会在我们的估计中起一些作用。</p><blockquote class="ly lz ma"><p id="7846" class="kv kw lx kx b ky kz kh la lb lc kk ld mb lf lg lh mc lj lk ll md ln lo lp lq ij bi translated">幸运的是，有一个解决方案可以表达不确定性<em class="jg">并且</em>将先验信息整合到我们的估计中:贝叶斯推理。</p></blockquote><p id="b73e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我们将在学习概率分布、<a class="ae jd" href="https://en.wikipedia.org/wiki/Bayesian_inference" rel="noopener ugc nofollow" target="_blank">贝叶斯推理</a>和<a class="ae jd" href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers" rel="noopener ugc nofollow" target="_blank">基本概率规划</a>以及<a class="ae jd" href="https://docs.pymc.io/api.html" rel="noopener ugc nofollow" target="_blank"> PyMC3 </a>的过程中，探索在贝叶斯框架中从数据中估计概率的问题。完整的代码可以从 GitHub 上的<a class="ae jd" href="https://github.com/WillKoehrsen/probabilistic-programming/blob/master/Estimating%20Probabilities%20with%20Bayesian%20Inference.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本中获得。</a></p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi me"><img src="../Images/972a209f7b501e8a45123891f7511eca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ywAUf0aLxWGbqlglWEhlRA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">PDF and trace values from PyMC3</figcaption></figure></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="2d0d" class="mm mn jg bd mo mp mq mr ms mt mu mv mw km mx kn my kp mz kq na ks nb kt nc nd bi translated">背景:概念</h1><p id="3063" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">通常，特别是在统计学中，我发现解决方案背后的理论比实际解决问题更令人困惑。(我相信统计学家为了证明统计数据的存在而将其复杂化。)对我来说，编码一个答案并可视化解决方案通常比阅读无尽的方程更有用。因此，当我着手解决这个问题时，我研究了足够的想法来编码一个解决方案，只有在之后的<em class="lx">我才重新挖掘概念。</em></p><p id="cad6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这反映了我一般的<a class="ae jd" href="https://course.fast.ai/about.html" rel="noopener ugc nofollow" target="_blank"> <em class="lx">自上而下</em>学习新话题</a>的方法。不要从基础开始——这通常是乏味和难以理解的——找出如何实现一个想法，这样你就知道<em class="lx">为什么它是有用的</em>,然后回到形式主义。所以，如果你觉得自己对这个理论感到沮丧，继续寻找解决方案(从下面的推理部分开始)，如果你仍然感兴趣，再回到概念上来。</p><p id="f7bf" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">(这种自上而下的哲学在关于深度学习的优秀 fast.ai 课程中得到了体现。除了有效地教授神经网络之外，这些课程还对我学习新技术的方法产生了影响。)</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="0b76" class="mm mn jg bd mo mp mq mr ms mt mu mv mw km mx kn my kp mz kq na ks nb kt nc nd bi translated">贝叶斯模型</h1><p id="0426" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">因为我们想用贝叶斯方法解决这个问题，所以我们需要构建一个情境模型。基本的设置是我们有一系列的观察结果:3 只老虎，2 只狮子和 1 只熊，从这些数据中，我们想估计野生动物保护区中每个物种的流行程度。也就是说，我们正在寻找在给定数据的情况下看到每个物种的后验概率。</p><p id="d87a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们开始之前，我们想建立我们的假设:</p><ul class=""><li id="340a" class="nj nk jg kx b ky kz lb lc le nl li nm lm nn lq no np nq nr bi translated">把对一个物种的每一次观察当作一次独立的试验。</li><li id="007b" class="nj nk jg kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated">我们最初(先前)的信念是每个物种都有平等的代表。</li></ul><p id="9c07" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">整个系统是一个多项分布<strong class="kx jh">，其中我们有 3 个<strong class="kx jh">离散的</strong>选择(物种),每个选择具有未知的概率和 6 个总观察值。多项式分布是二项式分布在有两个以上结果的情况下的扩展。多项式的一个简单应用是掷骰子 5 次，每次有 6 种可能的结果。</strong></p><p id="4a64" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">具有 3 个离散结果的多项式的<a class="ae jd" href="https://en.wikipedia.org/wiki/Probability_mass_function" rel="noopener ugc nofollow" target="_blank">概率质量函数</a>如下所示:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/3bb4a0d87dba2a5801670d39e9306deb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*xS-4djPRfOrQvc2uCReqCg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Probability Mass Function (PMF) of a multinomial with 3 outcomes</figcaption></figure><p id="7143" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">多项式分布的特征是 k，即结果的数量，n，即试验的数量，以及<strong class="kx jh"> p </strong>，即每个结果的概率向量。对于这个问题，<strong class="kx jh"> p </strong>是我们的终极目标:我们想从观测数据中算出看到每个物种的概率。在贝叶斯统计中，多项式的参数向量从形成参数先验分布的<strong class="kx jh">狄利克雷分布</strong>中提取。</p><p id="6b5e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">反过来，狄利克雷分布的特征在于，k(结果的数量)和<strong class="kx jh">α</strong>(称为浓度参数的正实值向量)。这被称为<strong class="kx jh">超参数</strong>，因为它是先前 <strong class="kx jh">的<em class="lx">参数。(</em></strong>这个链可以继续下去:如果<strong class="kx jh"> alpha </strong>来自另一个发行版，那么这就是一个<em class="lx">超优先级</em>，它可以有<em class="lx">自己的参数</em>称为<em class="lx">超超参数</em>！).我们将通过显式设置 alpha 的值来停止我们的模型，每个结果都有一个条目。</p><h2 id="8b0f" class="ny mn jg bd mo nz oa dn ms ob oc dp mw le od oe my li of og na lm oh oi nc oj bi translated">超参数和先验信念</h2><p id="80aa" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">将 Dirichlet 参数向量视为伪计数的最佳方式是，在收集实际数据之前对每个结果进行观察。这些<em class="lx"> </em>假计数<em class="lx">捕捉了我们对情况</em>的先验信念。例如，因为我们认为在进入保护区之前，每种动物的流行程度是相同的，所以我们将所有的阿尔法值设置为相等，比如说<strong class="kx jh">阿尔法</strong>=【1，1，1】。</p><p id="aae6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">相反，如果我们希望看到更多的熊，我们可以使用像[1，1，2]这样的超参数向量(顺序是[狮子，老虎，熊])。伪计数的确切值反映了我们对先前信念的信心水平。较大的伪计数将对后验估计值产生较大的影响，而较小的值将产生较小的影响，并将让数据支配后验估计值。当我们进入推理时，我们会看到这一点，但现在，记住超参数向量是伪计数，反过来，它代表我们先前的信念。</p><p id="ae44" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">具有 3 个结果的狄利克雷分布如下所示，具有不同的超参数向量值。颜色表示浓度权重。</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/137265a629254746bac66120b4fda43c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QzQHMcE5x-J9Tq0pkmDBHQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Effect of the hyperparameter vector alpha on the Dirichlet Distribution (<a class="ae jd" href="https://frnsys.com/ai_notes/machine_learning/bayesian_learning.html" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><p id="2b87" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">还有很多细节我们不需要在这里深入讨论，但是如果你仍然好奇，可以看看下面列出的一些资源。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><p id="3cd7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的最终目标是根据数据和超参数，估计观察到每个物种的概率的后验分布，<strong class="kx jh"> p </strong>:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/8f5855f3b705bde2f8f588796f71777f.png" data-original-src="https://miro.medium.com/v2/resize:fit:260/format:webp/1*TYqK3poh4SR1nqrj9KTpfQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">The posterior distribution of the parameter is our objective. X is observations and alpha is hyperparameters.</figcaption></figure><p id="24b8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的最终模型由具有狄利克雷先验的多项式分布组成，称为狄利克雷多项式，如下所示:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/7147f8c1717d793ca3cf43c257b2c5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PDEh1jwvlcQMOfcYipiTkg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Model of problem</figcaption></figure><p id="50e1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面是问题细节的摘要:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/89f6e530d773c8746ee71b25202853ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n7IC4bYeW0IMJDm4M3KiWg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Model specifics</figcaption></figure><p id="f5b0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你还想要更多的背景细节，这里是我所依赖的一些来源(第一个可能是最有价值的):</p><p id="2968" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">来源:</strong></p><ol class=""><li id="c018" class="nj nk jg kx b ky kz lb lc le nl li nm lm nn lq oo np nq nr bi translated"><a class="ae jd" href="http://users.cecs.anu.edu.au/~ssanner/MLSS2010/Johnson1.pdf" rel="noopener ugc nofollow" target="_blank">狄利克雷多项式的贝叶斯推断</a></li><li id="92a1" class="nj nk jg kx b ky ns lb nt le nu li nv lm nw lq oo np nq nr bi translated"><a class="ae jd" href="http://christianherta.de/lehre/dataScience/bayesian/Multinomial-Dirichlet.slides.php" rel="noopener ugc nofollow" target="_blank">分类数据/多项式分布</a></li><li id="bdf7" class="nj nk jg kx b ky ns lb nt le nu li nv lm nw lq oo np nq nr bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution#Dirichlet-multinomial_as_a_compound_distribution" rel="noopener ugc nofollow" target="_blank">狄利克雷-多项式维基百科文章</a></li><li id="5e30" class="nj nk jg kx b ky ns lb nt le nu li nv lm nw lq oo np nq nr bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Multinomial_distribution" rel="noopener ugc nofollow" target="_blank">多项分布维基百科文章</a></li><li id="d831" class="nj nk jg kx b ky ns lb nt le nu li nv lm nw lq oo np nq nr bi translated"><a class="ae jd" href="https://stats.stackexchange.com/questions/244917/what-exactly-is-the-alpha-in-the-dirichlet-distribution/244946" rel="noopener ugc nofollow" target="_blank">狄利克雷分布中的阿尔法</a></li><li id="fe53" class="nj nk jg kx b ky ns lb nt le nu li nv lm nw lq oo np nq nr bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Dirichlet_distribution" rel="noopener ugc nofollow" target="_blank">狄利克雷分布维基百科文章</a></li><li id="d735" class="nj nk jg kx b ky ns lb nt le nu li nv lm nw lq oo np nq nr bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Hyperparameter" rel="noopener ugc nofollow" target="_blank">超参数维基百科文章</a></li><li id="acf2" class="nj nk jg kx b ky ns lb nt le nu li nv lm nw lq oo np nq nr bi translated"><a class="ae jd" href="https://stats.stackexchange.com/questions/304148/deriving-the-map-estimate-for-multinomial-dirichlet" rel="noopener ugc nofollow" target="_blank">推导狄利克雷多项式的 MAP 估计</a></li></ol><p id="dd0b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">还有其他方法来解决这个问题；艾伦·唐尼的解决方案产生了类似的结果。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="e483" class="mm mn jg bd mo mp mq mr ms mt mu mv mw km mx kn my kp mz kq na ks nb kt nc nd bi translated">推论:从数据中做出估计</h1><p id="6f34" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">现在我们有了问题的模型，我们可以使用贝叶斯方法来求解后验概率。<a class="ae jd" href="https://en.wikipedia.org/wiki/Statistical_inference" rel="noopener ugc nofollow" target="_blank">统计学中的推断</a>是从数据中估计(推断)一个概率分布的未知参数的过程。我们的未知参数是每个物种的流行程度，而数据是我们从野生动物保护区观察到的一组数据。我们的目标是找到看到每个物种的概率的后验分布。</p><p id="8d4a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们推导后验概率的方法将使用贝叶斯推理。这意味着我们建立模型，然后用它从后验数据中<em class="lx">取样，用<a class="ae jd" rel="noopener" target="_blank" href="/markov-chain-monte-carlo-in-python-44f7e609be98">马尔可夫链蒙特卡罗</a> (MCMC)方法近似后验数据</em>。当难以进行精确推断时，我们使用 MCMC，并且随着样本数量的增加，估计的后验收敛于真实的后验。</p><p id="376f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">MCMC 的结果不仅仅是我们答案的一个数字，而是一系列样本，让我们量化我们的不确定性，尤其是在数据有限的情况下。我们将很快看到如何在 Python 中执行贝叶斯推理，但是如果我们想要一个单一的估计，我们可以使用分布的<em class="lx">期望值</em>。</p><h2 id="8659" class="ny mn jg bd mo nz oa dn ms ob oc dp mw le od oe my li of og na lm oh oi nc oj bi translated">预期值</h2><p id="5514" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">期望值是后验分布的平均值。对于狄利克雷多项式，它可以用<a class="ae jd" href="http://users.cecs.anu.edu.au/~ssanner/MLSS2010/Johnson1.pdf" rel="noopener ugc nofollow" target="_blank">解析地表示为</a>:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi op"><img src="../Images/2a3ab16aed4cf46c24a7592d29d1431f.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*9JhNfML4bO36UxvDwtu5tA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Expected value of a Multinomial with Dirichlet priors.</figcaption></figure><p id="2672" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦我们开始输入数字，这个问题就很容易解决了。n 是试验次数，6，c_i 是每个类别的<em class="lx">观测计数</em>，alpha_i 是每个类别的<em class="lx">伪计数</em>(超参数)。设置所有α等于 1，可以计算预期物种概率:</p><pre class="lr ls lt lu gt oq or os ot aw ou bi"><span id="35cc" class="ny mn jg or b gy ov ow l ox oy">species = ['lions', 'tigers', 'bears']<br/># Observations<br/>c = np.array([3, 2, 1])<br/>#Pseudocounts<br/>alphas = np.array([1, 1, 1])</span><span id="91ba" class="ny mn jg or b gy oz ow l ox oy">expected = (alphas + c) / (c.sum() + alphas.sum())</span><span id="c00a" class="ny mn jg or b gy oz ow l ox oy"><strong class="or jh">Species: lions    Prevalence: 44.44%.<br/>Species: tigers   Prevalence: 33.33%.<br/>Species: bears    Prevalence: 22.22%.</strong></span></pre><p id="df3d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这代表了考虑了伪计数的预期值，伪计数代表了我们对情况的初始信念。</p><p id="9343" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以通过增加伪计数的大小来调整我们对这个先验信念的置信度。这迫使期望值更接近我们最初的信念，即每个物种的流行程度是相等的。几个不同超参数的预期值如下所示:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pa"><img src="../Images/3d2c9f3e3d63af471a32eacf7fe68ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*wQl95tMjsXPuW3Nbp9nG0A.png"/></div></div></figure><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/1f0c9ad411808e73920e42da9ff7a0e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*vbGlIgZIMVI8692CZCendw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Expected values for different pseudocounts.</figcaption></figure><p id="8c29" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们对超参数的选择有很大的影响。如果我们对自己的信念更有信心，那么我们就增加超参数的权重。另一方面，如果我们希望数据有更大的权重，我们就减少伪计数。</p><p id="94cc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然这个结果提供了一个点估计，但它具有误导性，因为它没有表达任何不确定性。我们只去过一次野生动物保护区，所以在这些估计中应该有很大的不确定性。通过贝叶斯推理，我们可以得到点估计和不确定性。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="9c8f" class="mm mn jg bd mo mp mq mr ms mt mu mv mw km mx kn my kp mz kq na ks nb kt nc nd bi translated">用 PyMC3 实现 Python 中的贝叶斯推理</h1><p id="a100" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">为了得到一个估计范围，我们使用贝叶斯推理，构建一个情境模型，然后<em class="lx">从后验样本中取样以近似后验。</em>这是通过 PyMC3 中的马尔可夫链蒙特卡罗(或一种更有效的变体，称为<a class="ae jd" href="https://arxiv.org/abs/1111.4246" rel="noopener ugc nofollow" target="_blank">不掉头采样器</a>)实现的。与模型背后的理论相比，在代码中设置它很简单:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="pc lw l"/></div></figure><p id="aff2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，我们可以从后面取样:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="pc lw l"/></div></figure><p id="971d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">此代码在 2 个不同的链中从后部抽取 1000 个样本(丢弃 500 个用于调整的样本)。我们剩下一个<code class="fe pd pe pf or b">trace</code>，它包含了运行过程中抽取的所有样本。我们用这个轨迹来估计后验分布。</p><p id="c4f6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">PyMC3 有许多方法来检查轨迹，如<code class="fe pd pe pf or b">pm.traceplot</code>:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi me"><img src="../Images/972a209f7b501e8a45123891f7511eca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ywAUf0aLxWGbqlglWEhlRA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">PDF and trace of samples.</figcaption></figure><p id="f6ce" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">左边是采样参数的核密度估计——事件概率的 PDF。在右边，我们为模型中的每个自由参数绘制了完整的样本。我们可以从 KDE 看到，正如预期的那样，熊 pm.posterior_plot:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pg"><img src="../Images/956c39b2028fb2f97ac4633871818528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*77PgNW6RkXRc01FP1jKz4Q.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Posterior plots from PyMC3</figcaption></figure><p id="eb02" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面是直方图，显示了从后验概率中采样每个概率的次数。我们有概率的点估计值——平均值——以及置信区间的贝叶斯等价值——95%的最高概率密度(也称为<a class="ae jd" href="https://en.wikipedia.org/wiki/Credible_interval" rel="noopener ugc nofollow" target="_blank">可信区间</a>)。我们在这些估计中看到了极端程度的不确定性，这与有限的数据相符。</p><p id="780b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了量化不确定性水平，我们可以得到结果的数据框架:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/aed4cfe7e61996dc11ede5f8c6ea58e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*b4hhl6J8IBZTR1TtBmVQew.png"/></div></figure><p id="7b45" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这显示了患病率的最佳估计值(平均值),而且 95%可信区间非常大。根据我们对保护区的一次考察，我们只能确定狮子的患病率在 16.3%到 73.6%之间。</p><p id="6918" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">贝叶斯推理之所以如此强大，就是因为这种内置的不确定性。在现实世界中，数据总是嘈杂的，我们通常比我们想要的少。因此，任何时候我们根据数据进行估计，我们都必须显示这种不确定性。对于这个问题，如果我们得到的野生动物保护区的熊的百分比不正确，没有人会受到伤害，但是如果我们用医学数据做一个类似的方法来推断疾病概率，会怎么样呢？</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h2 id="5743" class="ny mn jg bd mo nz oa dn ms ob oc dp mw le od oe my li of og na lm oh oi nc oj bi translated">从后面取样</h2><p id="730c" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">一旦我们有了踪迹，我们可以从后面取样来模拟去保护区的额外旅程。比如我们再考虑去 1000 次。每次旅行我们能看到多少个物种？</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="pc lw l"/></div></figure><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pi"><img src="../Images/8e33d3bed33c1223e8136560dbcba4f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8b8_jompSM71zIFFc4Ef9Q.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">1000 samples drawn from the estimated posterior.</figcaption></figure><p id="1367" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">根据证据，有时候我们去保护区看到 5 只熊和 1 只老虎！当然，这不太可能，像这样的图表显示了可能结果的整个<em class="lx">范围</em>，而不是只有一个。我们去保护区的一次旅行只是一个结果:1000 次模拟表明，我们不能指望每次去保护区都得到准确的观察结果。</p><p id="cd8e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们想在采样后看到新的狄利克雷分布，它看起来像:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/ef64a8bf21a3932d1c3094700c6a34b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*NziNR13hcGzi4RCOfEsKng.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Dirichlet distribution after sampling.</figcaption></figure><h2 id="5fae" class="ny mn jg bd mo nz oa dn ms ob oc dp mw le od oe my li of og na lm oh oi nc oj bi translated">纳入附加信息</h2><p id="7a58" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">当我们去了 4 次保护区，并想在我们的模型中加入额外的观察结果，会发生什么？在 PyMC3 中，这很简单:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="pc lw l"/></div></figure><p id="f16c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过更多的观察，后验概率的不确定性将会减少，事实上，这就是我们在数量和视觉上所看到的。直觉上，这又是有意义的:随着我们收集更多的数据，我们对世界的状态变得更加确定。在无限数据的情况下，我们的估计将收敛于真实值，先验将不起作用。</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/cadb0ffd88aee4795069ee459e9dcbb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*8NPaQi7k2Z3mZo_-zDc5ug.png"/></div></figure><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pl"><img src="../Images/9612a1aa101b16e53dd48f69992245d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NoUUeNxqWO2nhGIr_LLXkw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Posteriors with more data</figcaption></figure><h2 id="981d" class="ny mn jg bd mo nz oa dn ms ob oc dp mw le od oe my li of og na lm oh oi nc oj bi translated">增加和减少对先前信念的信心</h2><p id="6b16" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">前面我们讨论了超参数如何被认为是代表我们先前信念的伪计数。如果我们将 alpha 的所有值都设置为 1，我们将得到目前为止看到的结果。如果我们降低或增加我们对患病率相等的最初理论的信心呢？要做到这一点，我们所要做的就是改变阿尔法矢量。然后，我们再次从后验样本(使用原始观察)并检查结果。</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pg"><img src="../Images/54f9bd5b3c1021b2a77104d424eefab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VTxcPTEggUh4ADF-YmxT_A.png"/></div></div></figure><p id="4ef5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">超参数对结果有很大的影响！较低的值意味着数据本身在后验概率中具有较大的权重，而较高的值导致伪计数具有较大的权重。随着该值的增加，分布相互收敛。在最后一种情况下，我们需要大量的数据来克服我们强大的超参数。</p><p id="b6fa" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以比较α= 0.1 和α= 15 时的后验曲线:</p><div class="lr ls lt lu gt ab cb"><figure class="pm is pn po pp pq pr paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/9341199ec4ed7adb2e0a4910a40a83de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Q2fr1R-duC6nKQoLE78QtQ.png"/></div></figure><figure class="pm is ps po pp pq pr paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/7dcac46996f281703b0fa9ada5618c1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*WDKtS_K6S_GyxDzEHriddg.png"/></div></figure></div><p id="1468" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最终，我们对超参数的选择取决于我们对信念的信心。如果我们有充分的理由认为物种的流行是相等的，那么我们应该使超参数具有更大的权重。如果我们想让数据说话，那么我们可以降低超参数的影响。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="cc0f" class="mm mn jg bd mo mp mq mr ms mt mu mv mw km mx kn my kp mz kq na ks nb kt nc nd bi translated">结论</h1><p id="1070" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">那么，对于流行率这个问题，我们的最终答案应该是什么呢？如果我们是好的贝叶斯主义者，那么我们可以提出一个点估计，但只能附带不确定性(95%可信区间):</p><ul class=""><li id="7e32" class="nj nk jg kx b ky kz lb lc le nl li nm lm nn lq no np nq nr bi translated"><strong class="kx jh">狮子:44.5% (16.9% — 75.8%) </strong></li><li id="dfc6" class="nj nk jg kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated"><strong class="kx jh">老虎:32.7% (6.7% — 60.5%) </strong></li><li id="96e6" class="nj nk jg kx b ky ns lb nt le nu li nv lm nw lq no np nq nr bi translated"><strong class="kx jh">空头:22.7% (1.7% — 50.0%) </strong></li></ul><p id="8fd8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们估计下一个观察对象是一头熊？基于后验抽样，大约 23%。虽然这些结果可能不会让想要简单答案的人满意，但他们应该记住现实世界是不确定的。</p><p id="8b65" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">贝叶斯推理的<a class="ae jd" href="https://andrewgelman.com/2015/05/19/bayesian-inference-the-advantages-and-the-risks/" rel="noopener ugc nofollow" target="_blank">好处</a>是我们可以整合我们先前的信念，并通过我们的答案得到不确定性估计。世界是不确定的，作为负责任的数据科学家，贝叶斯方法为我们提供了处理不确定性的框架。</p><p id="11ce" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">此外，随着我们获得更多的数据，我们的答案变得更加准确。与贝叶斯推理的许多方面一样，这符合我们的直觉和我们自然地看待世界的方式，随着额外信息的增加，错误变得更少。最终，贝叶斯统计是令人愉快和有用的，因为它是最终有意义的统计。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><p id="1ee7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一如既往，我欢迎反馈和建设性的批评。可以通过 Twitter <a class="ae jd" href="http://twitter.com/@koehrsen_will" rel="noopener ugc nofollow" target="_blank"> @koehrsen_will </a>或者通过我的个人网站<a class="ae jd" href="https://willk.online" rel="noopener ugc nofollow" target="_blank"> willk.online </a>找到我。</p></div></div>    
</body>
</html>