<html>
<head>
<title>Linear Regression Simplified - Ordinary Least Square vs Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简化线性回归-普通最小二乘法与梯度下降法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76?source=collection_archive---------2-----------------------#2018-05-15">https://towardsdatascience.com/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76?source=collection_archive---------2-----------------------#2018-05-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/f7cbdbe79ce53207bd441ce11ba4a906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tw-_dMVB7ZTD-407mCj87g.jpeg"/></div></div></figure><div class=""/><p id="4dcb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">什么是线性回归？<br/> </strong>线性回归是一种寻找自变量和因变量之间关系的统计方法。让我们用一个简单的数据集来解释线性回归模型。</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi kw"><img src="../Images/04b58f3f5c3fe1b2c224ef9579a0ef19.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*cFq7XW-Z69fDBil9wjyEBQ.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Table 1 : Sample Salary Data</figcaption></figure><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/e1e1bbcad7a12687e633fcec21f3337f.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*g1pQ4iHs9MpiJ8mu22mQgQ.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Figure 1: Scatter Plot Diagram</figcaption></figure><p id="233d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">为什么我们称它们为自变量和因变量？</strong></p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/5297cb2e139d848a2bde86f8662fdb47.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*MUYXO8-4jVJ2VnW4Hy6QmA.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Table 2: Sample dataset with independent &amp; dependent variables hints</figcaption></figure><p id="98ee" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果以我们的示例数据集为例,“工作经验年数”列是自变量,“年薪 1000 美元”列的值是因变量。我们的自变量是独立的，因为我们不能用数学方法确定经验的年限。但是，我们可以根据多年的经验来确定/预测工资列的值(因变量)。如果你看数据，依赖列值(薪水在 1000 美元)是根据多年的经验增加/减少的。</p><p id="53a9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">总平方和(SST):</strong>SST 是一个样本的平均值与该样本中各个值之间所有平方差的总和。它在数学上用公式表示。</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/fe65e92ee572a1029fced2a83391d157.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*Kved6pDxJGXwamDFQX1PCQ.png"/></div></figure><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi li"><img src="../Images/bef754f5f2187c1462deb0219ba83691.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*dVZTerZb8D-c1GPdWmJQJg.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Table: SST Calculation</figcaption></figure><p id="45a2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">SST 输出的平方和为<em class="lj"> 5226.19 </em>。为了对线截距进行最佳拟合，我们需要应用线性回归模型来尽可能减小 SSE 值。为了确定斜率截距，我们使用以下等式</p><p id="14c7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">y = m<em class="lj">x</em>b，</strong></p><ul class=""><li id="c023" class="lk ll jb ka b kb kc kf kg kj lm kn ln kr lo kv lp lq lr ls bi translated">“m”是斜率</li><li id="5c46" class="lk ll jb ka b kb lt kf lu kj lv kn lw kr lx kv lp lq lr ls bi translated">x’→独立变量</li><li id="27ee" class="lk ll jb ka b kb lt kf lu kj lv kn lw kr lx kv lp lq lr ls bi translated">b '是截距</li></ul><p id="9730" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将使用普通的最小二乘法来寻找最佳线截距(b)斜率(m)</p><p id="9c5d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">普通最小二乘法(OLS)方法</strong></p><p id="18a4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了使用 OLS 方法，我们应用下面的公式来找到方程</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/2c434b83196cb91175edaac030dc0b32.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*ypggCCmwRs4tjXqscH8N4Q.png"/></div></figure><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/8e47bd6e4c131cbf204a0033c24ebeaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*_wFBlqEDEzOp6Tx3suNj2g.png"/></div></figure><p id="d7c7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们需要计算斜率‘m’和线截距‘b’。下面是计算这些值的简单表格。</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/eabe6a0fbd8d94069e3e3900cfe4ca9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*v5HiHkMt18z3VzIpHcIZ-g.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Table 4: OLS method calculations</figcaption></figure><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/2c434b83196cb91175edaac030dc0b32.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*ypggCCmwRs4tjXqscH8N4Q.png"/></div></figure><p id="70de" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">m = 1037.8/216.19<br/>m = 4.80<br/>b = 45.44-4.80 * 7.56 = 9.15<br/>因此，<strong class="ka jc">y = m<em class="lj">x+b→4.80 x+9.15<br/>y = 4.80 x+9.15</em></strong></p><p id="e84c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们将 OLS 方法的结果与 MS-Excel 进行比较。是的，我们可以在 Microsoft Excel 中测试我们的线性回归最佳直线拟合。</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/d7385e92d4469979e015834e19aaafe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*57Zg30D_UWCmueiYwHFkWA.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk"><strong class="bd mc"><em class="md">Figure 2: Linear Regression using MS-Excel</em></strong></figcaption></figure><p id="5c00" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">精彩！我们的 OLS 方法与 MS-Excel 输出的“y”非常相似。</p><ul class=""><li id="96c6" class="lk ll jb ka b kb kc kf kg kj lm kn ln kr lo kv lp lq lr ls bi translated">我们的 OLS 方法输出→<strong class="ka jc"><em class="lj">y = 4.80 x+9.15</em></strong></li><li id="6b90" class="lk ll jb ka b kb lt kf lu kj lv kn lw kr lx kv lp lq lr ls bi translated">MS-Excel 线性调节器。输出<strong class="ka jc"><em class="lj">→y = 4.79 x+9.18</em>T30】</strong></li></ul><p id="e605" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们通过使用我们的输出方程再次计算 SSE。</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi me"><img src="../Images/c1cb4d699fbb841d4024600ba9d70b3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*yvjiFI-2cMbyZ18YLVaRLQ.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Table 5: SSE calculations again after OLS method implementation</figcaption></figure><p id="6505" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，误差平方和从 5226.19 显著降低到 245.38。</p><p id="5e86" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">普通的最小二乘法看起来简单，计算容易。但是，这种 OLS 方法既适用于单自变量和单因变量的单变量数据集，也适用于多变量数据集。多变量数据集包含单个自变量集和多个因变量集，需要我们使用一种叫做“梯度下降”的机器学习算法。</p><p id="1fe2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">梯度下降算法</strong></p><p id="0da6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">梯度下降算法的主要目标是最小化成本函数。这是最大限度减少误差(实际值和预测值的差异)的最佳优化算法之一。</p><p id="2af0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们来表示假设 h，它是函数或者是一个学习算法。</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/6a4ad7e5457e57185f282b5ca1e85b99.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*K1-0bnoMqxSv6tZXpOki1Q.png"/></div></figure><p id="27d4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">目标类似于上面的操作，我们在斜率“m”中找出截距线“y”的最佳拟合。同样使用梯度下降算法，我们将通过应用θ0 和θ1 的各种参数来计算出最小成本函数，并查看斜率截距，直到它达到收敛。</p><p id="002c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在一个真实世界的例子中，找到一个最佳方向走下坡路是相似的。</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mg"><img src="../Images/0da52b80dbac39abcc1931a276854407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*09kq2L23D9XM_9Xtr8gc8Q.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Figure 3: Gradient Descent 3D diagram. Source: <a class="ae mh" href="https://www.coursera.org/learn/machine-learning/lecture/8SpIM/gradient-descent" rel="noopener ugc nofollow" target="_blank">Coursera</a> — Andrew Ng</figcaption></figure><p id="1045" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们向趴下的方向迈一步。从每一步开始，你都要再次观察方向，以便更快地下山和更快地下山。在该算法中使用类似的方法来最小化成本函数。</p><p id="fe68" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以使用成本函数来衡量假设函数的准确性，公式如下</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/ae6fd0d927e08e5ad361975287ec2b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*O0VceUdYADRY3FoTbBrqMA.png"/></div></figure><p id="83d5" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc"> <em class="lj">梯度下降为线性回归</em> </strong></p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/0ad522691cd5a8cb1ec5ab7d0b49697d.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*S6sc4ohsd5ckBDY42QhivA.png"/></div></figure><p id="9a45" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为什么我们在方程中使用偏导数？偏导数表示作为变量变化的函数的变化率。在我们的例子中，我们改变θ0 和θ1 的值，并确定变化率。为了将变化率值应用于θ0 和θ1，下面是用于θ0 和θ1 的等式，以将其应用于每个时期。</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/731223ac5e168f76015966a5fee3b839.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*TrFUjJR65rWBe3wN9Snacg.png"/></div></figure><p id="e313" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要找到最佳最小值，请重复上述步骤，对θ0 和θ1 应用不同的值。换句话说，重复步骤直到收敛。</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/daa6de7cb3be6bffb9c3c2ba85e0cb7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*G3evFxIAlDchOx5Wl7bV5g.png"/></div></figure><figure class="kx ky kz la gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ml"><img src="../Images/ee9111798be5960033417bf7eb940f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HrFZV7pKPcc5dzLaWvngtQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Figure 4: Gradient Descent for linear regression. Source: <a class="ae mh" href="http://sebastianraschka.com/" rel="noopener ugc nofollow" target="_blank">http://sebastianraschka.com/</a> — Python Machine Learning, 2nd Edition</figcaption></figure><ul class=""><li id="7c6e" class="lk ll jb ka b kb kc kf kg kj lm kn ln kr lo kv lp lq lr ls bi translated">其中，α(a)是学习速率/下坡需要多大的步长。</li></ul><p id="4b89" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">梯度下降算法的类型</strong></p><p id="e23c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有三种梯度下降算法:</p><p id="7cde" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">1.批次梯度下降<br/> 2。随机梯度下降<br/> 3。小批量梯度下降</p><p id="cdb0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">批量梯度下降</strong></p><ul class=""><li id="9abf" class="lk ll jb ka b kb kc kf kg kj lm kn ln kr lo kv lp lq lr ls bi translated">在批量梯度下降中，为了计算成本函数的梯度，我们需要对每个步骤的所有训练样本求和</li><li id="3e15" class="lk ll jb ka b kb lt kf lu kj lv kn lw kr lx kv lp lq lr ls bi translated">如果我们有 3 百万个样本(m 个训练样本)，那么梯度下降算法应该对每个时期的 3 百万个样本求和。要移动一步，我们要用 300 万次来计算每一步！</li><li id="b99e" class="lk ll jb ka b kb lt kf lu kj lv kn lw kr lx kv lp lq lr ls bi translated">批量梯度下降不太适合大型数据集</li><li id="f965" class="lk ll jb ka b kb lt kf lu kj lv kn lw kr lx kv lp lq lr ls bi translated">下面是批量梯度下降算法的 python 代码实现。</li></ul><figure class="kx ky kz la gt is"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="8f25" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">随机梯度下降(SGD) </strong></p><ul class=""><li id="2d2c" class="lk ll jb ka b kb kc kf kg kj lm kn ln kr lo kv lp lq lr ls bi translated">在随机梯度下降中，我们在每次迭代中使用一个示例或一个训练样本，而不是使用整个数据集来对每个步骤求和</li><li id="1995" class="lk ll jb ka b kb lt kf lu kj lv kn lw kr lx kv lp lq lr ls bi translated">SGD 广泛用于大型数据集训练，计算速度更快，并且可以并行训练</li><li id="c765" class="lk ll jb ka b kb lt kf lu kj lv kn lw kr lx kv lp lq lr ls bi translated">在计算之前，需要随机打乱训练样本</li><li id="7870" class="lk ll jb ka b kb lt kf lu kj lv kn lw kr lx kv lp lq lr ls bi translated">下面是 SGD 的 Python 代码实现</li></ul><figure class="kx ky kz la gt is"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="7051" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">小批量梯度下降</strong></p><ul class=""><li id="cd34" class="lk ll jb ka b kb kc kf kg kj lm kn ln kr lo kv lp lq lr ls bi translated">它类似于 SGD，它在每次迭代中使用<strong class="ka jc"> <em class="lj"> n </em> </strong>个样本，而不是 1 个。</li></ul><p id="7089" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">概要:</strong></p><p id="9002" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在总结中，对以下主题进行了详细解释。</p><ul class=""><li id="c490" class="lk ll jb ka b kb kc kf kg kj lm kn ln kr lo kv lp lq lr ls bi translated">线性回归的自变量和因变量</li><li id="5d32" class="lk ll jb ka b kb lt kf lu kj lv kn lw kr lx kv lp lq lr ls bi translated">普通最小二乘法(OLS)和误差平方和(SSE)详细信息</li><li id="8362" class="lk ll jb ka b kb lt kf lu kj lv kn lw kr lx kv lp lq lr ls bi translated">线性回归模型的梯度下降及梯度下降算法的类型。</li></ul><p id="6c10" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">参考:</p><ul class=""><li id="6267" class="lk ll jb ka b kb kc kf kg kj lm kn ln kr lo kv lp lq lr ls bi translated"><a class="ae mh" href="https://www.coursera.org/learn/machine-learning/lecture/rkTp3/cost-function" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/machine-learning/lecture/rktp 3/cost-function</a></li><li id="26a9" class="lk ll jb ka b kb lt kf lu kj lv kn lw kr lx kv lp lq lr ls bi translated"><a class="ae mh" href="https://github.com/rasbt/python-machine-learning-book-2nd-edition" rel="noopener ugc nofollow" target="_blank">https://github . com/rasbt/python-machine-learning-book-2nd-edition</a></li></ul></div></div>    
</body>
</html>