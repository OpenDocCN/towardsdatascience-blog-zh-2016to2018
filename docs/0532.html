<html>
<head>
<title>Topic Modeling for The New York Times News Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">纽约时报新闻数据集的主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/topic-modeling-for-the-new-york-times-news-dataset-1f643e15caac?source=collection_archive---------0-----------------------#2017-05-16">https://towardsdatascience.com/topic-modeling-for-the-new-york-times-news-dataset-1f643e15caac?source=collection_archive---------0-----------------------#2017-05-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/4bbbfc8d3e1d18104d55fd9e7b8494f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*toWf7lAVf_5GIb9IMfS8Bw.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><em class="jg">Credits: Mario Tama, News Getty Images</em></figcaption></figure><div class=""/><div class=""><h2 id="4038" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">一种用于新闻主题分类的非负矩阵分解方法</h2></div><h1 id="01da" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">为什么要主题建模？</h1><p id="a729" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们生活在一个不断收集数据流的世界。因此，从收集的信息中寻找见解会变得非常乏味和耗时。主题建模是作为组织、搜索和理解大量文本信息的工具而设计的。</p><h1 id="7ded" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">什么是主题建模？</h1><p id="f2e5" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在<a class="ae mm" href="https://medium.com/towards-data-science/becoming-a-machine-learning-geek-a956a98a7498" rel="noopener">机器学习</a>中，主题模型被具体定义为一种自然语言处理技术，用于发现文档集合中文本的隐藏语义结构，通常称为<em class="mn">语料库</em>。一般来说，每个文档指的是一组连续的单词，就像一个段落或一篇文章，其中每篇文章包含一组单词。</p><p id="9290" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz mq mb mc md mr mf mg mh ms mj mk ml im bi translated">让我们以<em class="mn">纽约时报</em>数据集为例，其中每篇文章代表一个文档。我们想解决的问题是这些单词在一个文档中出现的频率。这将允许我们将每个文档分类到一个特定的主题。</p><h1 id="ecc8" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">NMF 是如何工作的？</h1><p id="50d3" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">通常使用两种主要的主题建模技术，<em class="mn">潜在狄利克雷分布</em> (LDA)和<em class="mn">非负矩阵分解</em> (NMF)。让我们把重点放在主题建模的后一个实现上，对纽约时报新闻数据集中的相似词进行聚类。</p><p id="b8b6" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz mq mb mc md mr mf mg mh ms mj mk ml im bi translated">NMF 技术通过概率分布检查文档并在数学框架中发现主题。作为一个例子，我们首先用矩阵<strong class="ls jk"> X </strong>作为我们的数据。这个矩阵由两个更小的矩阵<strong class="ls jk"> W </strong>和<strong class="ls jk"> H </strong>表示，当它们相乘时，近似重构<strong class="ls jk"> X </strong>。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mt"><img src="../Images/32d670bf690d9139dd474a45a4cb567e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XPFI1LSFlLzfB27-TdosHg.png"/></div></div></figure><p id="d9bc" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz mq mb mc md mr mf mg mh ms mj mk ml im bi translated">在实施 NMF 之前有一些先决条件。首先，数据<strong class="ls jk"> X </strong>必须有非负的条目。没有丢失，但可能有许多零。第二，学习因式分解<strong class="ls jk"> W </strong>和<strong class="ls jk"> H </strong>必须有非负项。</p><h1 id="e9f2" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">如何实现 NMF？</h1><p id="9674" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这个问题中使用的数据由来自纽约时报的 8447 份文件组成。词汇量 3012 字<em class="mn"> </em>(此处下载<a class="ae mm" href="https://github.com/moorissa/nmf_nyt/blob/master/nyt_vocab.dat" rel="noopener ugc nofollow" target="_blank">)。我们首先使用该数据构建矩阵<strong class="ls jk"> X </strong>，其中 Xij 是单词 I 在文档 j 中出现的次数。因此，<strong class="ls jk"> X </strong>是 30，128，447，并且<strong class="ls jk"> X </strong>中的大多数值将等于零。</a></p><p id="0124" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz mq mb mc md mr mf mg mh ms mj mk ml im bi translated">这里，我们将一个<strong class="ls jk"> N </strong> × <strong class="ls jk"> M </strong>矩阵<strong class="ls jk"> X </strong>分解成一个秩- <strong class="ls jk"> K </strong>近似值<strong class="ls jk"> W H </strong>，其中<strong class="ls jk"> W </strong>是<strong class="ls jk"> N </strong> × <strong class="ls jk"> K </strong>，<strong class="ls jk"> H </strong>是<strong class="ls jk"> K </strong> × <strong class="ls jk"> M </strong>，矩阵中的所有值为<strong class="ls jk"> W </strong>和<strong class="ls jk"> H </strong>中的每个值都可以随机初始化为一个正数，这里我用的是均匀(0，1)分布。</p><p id="0067" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz mq mb mc md mr mf mg mh ms mj mk ml im bi translated">我们通过最小化以下散度损失对该数据实施并运行 NMF 算法，其中<strong class="ls jk"> W </strong>和<strong class="ls jk"> H </strong>包含非负值:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/4e61857b31fa6892d49bdaf674abdc83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V6KMZnLDTccFrfWqPnaBdQ.png"/></div></div></figure><blockquote class="mz na nb"><p id="5d9a" class="lq lr mn ls b lt mo kk lv lw mp kn ly nc mq mb mc nd mr mf mg ne ms mj mk ml im bi translated">对于数据预处理和矩阵计算，您可以在本文底部的编码参考中找到我的原始代码。</p></blockquote><p id="4d63" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz mq mb mc md mr mf mg mh ms mj mk ml im bi translated">现在让我们挑选一些我们想要排序的主题，以及我们预期目标函数会偏离的迭代次数。假设我们将等级设置为 25，并运行 100 次迭代，这也对应于学习 25 个主题。</p><p id="dd9b" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz mq mb mc md mr mf mg mh ms mj mk ml im bi translated">我们可以看到目标图是下面迭代的函数。通过观察目标函数的散度，我们确保了用于聚类相似单词的模型是稳定的。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nf"><img src="../Images/4d0d610a12a758659cf85a5b41c90195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CQDWdAbrVi_BmfmgmD-uOw.png"/></div></div></figure><p id="9e44" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz mq mb mc md mr mf mg mh ms mj mk ml im bi translated">运行该算法后，我们希望规范化<strong class="ls jk"> W </strong>的列，使它们的总和为 1。这也是为了确保我们得到的概率分布没有大于零的值。</p><p id="de3d" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz mq mb mc md mr mf mg mh ms mj mk ml im bi translated">现在，假设我们为每个话题选择 10 个单词。对于<strong class="ls jk"> W </strong>的每一列，我们列出了具有最大权重的 10 个单词，并显示权重以显示我们预期的概率分布。<strong class="ls jk"> W </strong>的第<em class="mn"> i- </em>行对应于提供数据的“字典”中的第<em class="mn"> i- </em>个单词。</p><h1 id="9f40" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">主题建模结果</h1><p id="550d" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">下表捕获了一组包含 25 个主题的文档，这是我们预期的结果。仔细查看结果，<em class="mn">话题 7 </em>指的是<strong class="ls jk">财经</strong>话题，<strong class="ls jk">医疗</strong>领域<em class="mn">话题 13 </em>，<strong class="ls jk">娱乐</strong>领域<em class="mn">话题 14 </em>，以及<strong class="ls jk">商业</strong>领域<em class="mn">话题 24 </em>。这些只是数百个话题中的 25 个，如果不是数百万的话，还有其他的可能性！</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nf"><img src="../Images/138919642a5f66f910bb683e3228cbe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*clWOoXK0ePTog04gjMfFZA.png"/></div></div></figure><p id="1da2" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz mq mb mc md mr mf mg mh ms mj mk ml im bi translated"><strong class="ls jk"> <em class="mn">挑战:</em> </strong>你能猜出上面每个题目代表什么吗？下面评论下来！</p><h1 id="a34c" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">编码参考</h1><p id="007a" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls jk">注意:</strong>代码是基于 NMF 背后的原始数学概念和直观计算实现的，但更方便的替代方法是使用 Python 库，如<em class="mn"> scikit-learn </em>。</p><p id="3772" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz mq mb mc md mr mf mg mh ms mj mk ml im bi translated">希望你喜欢！</p><figure class="mu mv mw mx gt iv"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><em class="jg">Source: </em><a class="ae mm" href="https://github.com/moorissa/nmf_nyt" rel="noopener ugc nofollow" target="_blank"><em class="jg">https://github.com/moorissa/nmf_nyt</em></a></figcaption></figure></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="8714" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz mq mb mc md mr mf mg mh ms mj mk ml im bi translated">Moorissa 是一名研究生，目前在哥伦比亚大学学习机器学习，希望有一天她可以利用这些技能让世界变得更好，一次一天。</p></div></div>    
</body>
</html>