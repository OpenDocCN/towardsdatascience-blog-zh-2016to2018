<html>
<head>
<title>How to use Dataset in TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在TensorFlow中使用数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428?source=collection_archive---------0-----------------------#2018-02-06">https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428?source=collection_archive---------0-----------------------#2018-02-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/c6828cd52a765a6e50e819756da7b7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1vYxIkzAjm82cROBYEwPHw.png"/></div></div></figure><div class=""/><p id="9a1d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf"> <em class="kz">我在</em></strong><a class="ae la" href="https://www.linkedin.com/in/francesco-saverio-zuppichini-94659a150/?originalSubdomain=ch" rel="noopener ugc nofollow" target="_blank"><strong class="kd jf"><em class="kz">LinkedIn</em></strong></a><strong class="kd jf"><em class="kz">，快来打个招呼</em> </strong>👋</p><p id="3b13" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">内置的输入管道。再也不用“feed-dict”了</p><p id="aa73" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf"> <em class="kz"> 16/02/2020:我已经换成PyTorch </em> </strong>😍</p><p id="9378" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz"> 29/05/2019:我会把教程更新到tf 2.0😎(我正在完成我的硕士论文)</em></p><p id="c805" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">2018年2月6日更新:添加了第二个完整示例，将csv直接读入数据集</em></p><p id="bac6" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">2018年5月25日更新:添加了第二个完整示例，带有一个</em> <strong class="kd jf"> <em class="kz">可重新初始化的迭代器</em> </strong></p><p id="b2c5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">更新至TensorFlow 1.8 </em></p><p id="c637" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">您应该知道，<code class="fe lb lc ld le b">feed-dict</code>是向TensorFlow传递信息的最慢方式，必须避免使用。将数据输入模型的正确方法是使用输入管道来确保GPU永远不会等待新的东西进来。</p><p id="eb59" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">幸运的是，TensorFlow有一个内置的API，名为<a class="ae la" href="https://www.tensorflow.org/programmers_guide/datasets" rel="noopener ugc nofollow" target="_blank"> Dataset </a>，可以更容易地完成这项任务。在本教程中，我们将了解如何创建输入管道，以及如何高效地将数据输入模型。</p><p id="ab9c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">本文将解释数据集的基本机制，涵盖最常见的用例。</p><p id="a258" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你可以在这里找到jupyter笔记本的所有代码:</p><p id="1e68" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae la" href="https://github.com/FrancescoSaverioZuppichini/Tensorflow-Dataset-Tutorial/blob/master/dataset_tutorial.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/FrancescoSaverioZuppichini/tensor flow-Dataset-Tutorial/blob/master/Dataset _ Tutorial . ipynb</a></p><h1 id="2ba7" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">一般概述</h1><p id="e98a" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">为了使用数据集，我们需要三个步骤:</p><ul class=""><li id="6411" class="mi mj je kd b ke kf ki kj km mk kq ml ku mm ky mn mo mp mq bi translated"><strong class="kd jf">导入数据</strong>。从一些数据创建数据集实例</li><li id="7614" class="mi mj je kd b ke mr ki ms km mt kq mu ku mv ky mn mo mp mq bi translated">创建一个迭代器。通过使用创建的数据集来制作迭代器实例来遍历数据集</li><li id="a8e1" class="mi mj je kd b ke mr ki ms km mt kq mu ku mv ky mn mo mp mq bi translated"><strong class="kd jf">消费数据</strong>。通过使用创建的迭代器，我们可以从数据集中获取元素，以提供给模型</li></ul><h1 id="72a5" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">导入数据</h1><p id="3d41" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">我们首先需要一些数据放入我们的数据集</p><h2 id="c87f" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">来自numpy</h2><p id="2001" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">这是常见的情况，我们有一个numpy数组，我们想把它传递给tensorflow。</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="94c1" class="mw lg je le b gy nq nr l ns nt"># create a random vector of shape (100,2)<br/>x = np.random.sample((100,2))<br/># make a dataset from a numpy array<br/>dataset = tf.data.Dataset.from_tensor_slices(x)</span></pre><p id="ea94" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们也可以传递不止一个numpy数组，一个经典的例子是当我们将一些数据分成特征和标签时</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="92da" class="mw lg je le b gy nq nr l ns nt">features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>dataset = tf.data.Dataset.from_tensor_slices((features,labels))</span></pre><h2 id="7209" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">来自张量</h2><p id="64a2" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">当然，我们可以用一些张量初始化我们的数据集</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="778f" class="mw lg je le b gy nq nr l ns nt"># using a tensor<br/>dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([100, 2]))</span></pre><h2 id="c832" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">从占位符</h2><p id="f0e4" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">当我们想要动态地改变数据集中的数据时，这是很有用的，我们将在后面看到如何改变。</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="862e" class="mw lg je le b gy nq nr l ns nt">x = tf.placeholder(tf.float32, shape=[None,2])<br/>dataset = tf.data.Dataset.from_tensor_slices(x)</span></pre><h2 id="1d27" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">来自发电机</h2><p id="fefe" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">我们也可以从生成器初始化一个数据集，当我们有一个不同元素长度的数组(例如一个序列)时，这很有用:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="dadf" class="mw lg je le b gy nq nr l ns nt"># from generator<br/>sequence = np.array([[[1]],[[2],[3]],[[3],[4],[5]]])</span><span id="c310" class="mw lg je le b gy nu nr l ns nt">def generator():<br/>    for el in sequence:<br/>        yield el</span><span id="6fe7" class="mw lg je le b gy nu nr l ns nt">dataset = tf.data.Dataset().batch(1).from_generator(generator,<br/>                                           output_types= tf.int64, <br/>                                           output_shapes=(tf.TensorShape([None, 1])))</span><span id="70f9" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_initializable_iterator()<br/>el = iter.get_next()</span><span id="ad45" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    sess.run(iter.initializer)<br/>    print(sess.run(el))<br/>    print(sess.run(el))<br/>    print(sess.run(el))</span></pre><p id="758d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">输出:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="fa24" class="mw lg je le b gy nq nr l ns nt">[[1]]<br/>[[2]<br/> [3]]<br/>[[3]<br/> [4]<br/> [5]]</span></pre><p id="4f2d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这种情况下，您还需要指定将用于创建正确张量的数据的类型和形状。</p><h2 id="5e3f" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">从csv文件</h2><p id="c1d9" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">您可以直接将csv文件读入数据集。例如，我有一个csv文件，里面有推文和他们的情绪。</p><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c6b42c4856e7250e61a751073c6db564.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*LE5N8IQTcgahmZt_odeo8Q.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">tweets.csv</figcaption></figure><p id="b807" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我现在可以通过调用<code class="fe lb lc ld le b">tf.contrib.data.make_csv_dataset</code>轻松地从它创建一个<code class="fe lb lc ld le b">Dataset</code>。请注意，迭代器将创建一个字典，以key作为列名，以张量的形式使用正确的行值。</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="ab21" class="mw lg je le b gy nq nr l ns nt"># load a csv<br/>CSV_PATH = './tweets.csv'<br/>dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=32)<br/>iter = dataset.make_one_shot_iterator()<br/>next = iter.get_next()<br/>print(next) # next is a dict with key=columns names and value=column data<br/>inputs, labels = next['text'], next['sentiment']</span><span id="34dd" class="mw lg je le b gy nu nr l ns nt">with  tf.Session() as sess:<br/>    sess.run([inputs, labels])</span></pre><p id="ff66" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><code class="fe lb lc ld le b">next</code>在哪里</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="8295" class="mw lg je le b gy nq nr l ns nt">{'sentiment': &lt;tf.Tensor 'IteratorGetNext_15:0' shape=(?,) dtype=int32&gt;, 'text': &lt;tf.Tensor 'IteratorGetNext_15:1' shape=(?,) dtype=string&gt;}</span></pre><h1 id="e56f" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">创建迭代器</h1><p id="8aa5" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">我们已经看到了如何创建数据集，但是如何取回我们的数据呢？我们必须使用一个<code class="fe lb lc ld le b">Iterator</code>，这将使我们能够遍历数据集并检索数据的真实值。有四种类型的迭代器。</p><ul class=""><li id="5f45" class="mi mj je kd b ke kf ki kj km mk kq ml ku mm ky mn mo mp mq bi translated"><strong class="kd jf">一针。</strong>它可以遍历一次数据集，你<strong class="kd jf">不能给它</strong>任何值。</li><li id="cdcd" class="mi mj je kd b ke mr ki ms km mt kq mu ku mv ky mn mo mp mq bi translated"><strong class="kd jf">可初始化</strong>:可以动态改变调用它的<code class="fe lb lc ld le b">initializer</code>操作，用<code class="fe lb lc ld le b">feed_dict</code>传递新数据。它基本上是一个可以装满东西的桶。</li><li id="1fb7" class="mi mj je kd b ke mr ki ms km mt kq mu ku mv ky mn mo mp mq bi translated"><strong class="kd jf">可重新初始化</strong>:它可以从不同的<code class="fe lb lc ld le b">Dataset.</code>初始化，当你有一个训练数据集需要一些额外的变换，例如shuffle，和一个测试数据集时非常有用。这就像使用塔式起重机来选择不同的容器。</li><li id="d202" class="mi mj je kd b ke mr ki ms km mt kq mu ku mv ky mn mo mp mq bi translated"><strong class="kd jf"> Feedable </strong> : <strong class="kd jf"> </strong>可以用迭代器选择使用。按照前面的例子，就像一个塔吊选择用哪个塔吊来选择拿哪个集装箱。在我看来是没用的。</li></ul><h2 id="419b" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">一次性迭代器</h2><p id="1bf5" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">这是最简单的迭代器。使用第一个例子</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="1d3a" class="mw lg je le b gy nq nr l ns nt">x = np.random.sample((100,2))<br/># make a dataset from a numpy array<br/>dataset = tf.data.Dataset.from_tensor_slices(x)</span><span id="da8c" class="mw lg je le b gy nu nr l ns nt"># create the iterator<br/>iter = dataset.make_one_shot_iterator()</span></pre><p id="8e08" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后你需要调用<code class="fe lb lc ld le b">get_next()</code>来获取包含你的数据的张量</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="5f65" class="mw lg je le b gy nq nr l ns nt">...<br/># create the iterator<br/>iter = dataset.make_one_shot_iterator()<br/>el = iter.get_next()</span></pre><p id="f142" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们可以运行<code class="fe lb lc ld le b">el</code>来查看它的值</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="496f" class="mw lg je le b gy nq nr l ns nt">with tf.Session() as sess:<br/>    print(sess.run(el)) # output: [ 0.42116176  0.40666069]</span></pre><h2 id="4fcf" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">可初始化迭代器</h2><p id="ebe2" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">如果我们想要构建一个动态数据集，可以在运行时改变数据源，我们可以创建一个带有占位符的数据集。然后我们可以使用通用的<code class="fe lb lc ld le b">feed-dict</code>机制初始化占位符。这是通过一个<em class="kz">可初始化的迭代器</em>完成的。使用上一节中的示例三</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="d1b6" class="mw lg je le b gy nq nr l ns nt"># using a placeholder<br/>x = tf.placeholder(tf.float32, shape=[None,2])<br/>dataset = tf.data.Dataset.from_tensor_slices(x)</span><span id="13bd" class="mw lg je le b gy nu nr l ns nt">data = np.random.sample((100,2))</span><span id="0187" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_initializable_iterator() # create the iterator<br/>el = iter.get_next()</span><span id="aa1d" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    # feed the placeholder with data<br/>    sess.run(iter.initializer, feed_dict={ x: data }) <br/>    print(sess.run(el)) # output [ 0.52374458  0.71968478]</span></pre><p id="3fa6" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这次我们叫<code class="fe lb lc ld le b">make_initializable_iterator</code>。然后，在<code class="fe lb lc ld le b">sess</code>范围内，我们运行<code class="fe lb lc ld le b">initializer</code>操作来传递我们的数据，在本例中是一个随机的numpy数组。。</p><p id="3640" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">想象一下，现在我们有一个训练集和一个测试集，这是一个真实的常见场景:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="581f" class="mw lg je le b gy nq nr l ns nt">train_data = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>test_data = (np.array([[1,2]]), np.array([[0]]))</span></pre><p id="c347" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，我们将训练模型，然后在测试数据集上评估它，这可以通过在训练后再次初始化迭代器来完成</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="ddc5" class="mw lg je le b gy nq nr l ns nt"># initializable iterator to switch between dataset<br/>EPOCHS = 10</span><span id="e6c1" class="mw lg je le b gy nu nr l ns nt">x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])<br/>dataset = tf.data.Dataset.from_tensor_slices((x, y))</span><span id="a8f4" class="mw lg je le b gy nu nr l ns nt">train_data = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>test_data = (np.array([[1,2]]), np.array([[0]]))</span><span id="b529" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_initializable_iterator()<br/>features, labels = iter.get_next()</span><span id="1963" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>#     initialise iterator with train data<br/>    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})<br/>    for _ in range(EPOCHS):<br/>        sess.run([features, labels])<br/>#     switch to test data<br/>    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})<br/>    print(sess.run([features, labels]))</span></pre><h2 id="f9f6" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated"><strong class="ak">可重新初始化的迭代器</strong></h2><p id="dcc3" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">这个概念类似于以前，我们要在数据之间动态切换。但是，我们不是向同一个数据集提供新数据，而是切换数据集。和以前一样，我们希望有一个训练数据集和一个测试数据集</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="ad4e" class="mw lg je le b gy nq nr l ns nt"># making fake data using numpy<br/>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>test_data = (np.random.sample((10,2)), np.random.sample((10,1)))</span></pre><p id="117a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们可以创建两个数据集</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="5807" class="mw lg je le b gy nq nr l ns nt"># create two datasets, one for training and one for test<br/>train_dataset = tf.data.Dataset.from_tensor_slices(train_data)<br/>test_dataset = tf.data.Dataset.from_tensor_slices(test_data)</span></pre><p id="9c7a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这就是诀窍，我们创建一个泛型迭代器</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="5034" class="mw lg je le b gy nq nr l ns nt"># create a iterator of the correct shape and type<br/>iter = tf.data.Iterator.from_structure(train_dataset.output_types,<br/>                                           train_dataset.output_shapes)</span></pre><p id="0001" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后是两个初始化操作:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="01dc" class="mw lg je le b gy nq nr l ns nt"># create the initialisation operations<br/>train_init_op = iter.make_initializer(train_dataset)<br/>test_init_op = iter.make_initializer(test_dataset)</span></pre><p id="dbd0" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们像以前一样得到下一个元素</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="73b2" class="mw lg je le b gy nq nr l ns nt">features, labels = iter.get_next()</span></pre><p id="c288" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，我们可以使用我们的会话直接运行两个初始化操作。综上所述，我们得到:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="a1e9" class="mw lg je le b gy nq nr l ns nt"># Reinitializable iterator to switch between Datasets<br/>EPOCHS = 10<br/># making fake data using numpy<br/>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>test_data = (np.random.sample((10,2)), np.random.sample((10,1)))<br/># create two datasets, one for training and one for test<br/>train_dataset = tf.data.Dataset.from_tensor_slices(train_data)<br/>test_dataset = tf.data.Dataset.from_tensor_slices(test_data)<br/># create a iterator of the correct shape and type<br/>iter = tf.data.Iterator.from_structure(train_dataset.output_types,<br/>                                           train_dataset.output_shapes)<br/>features, labels = iter.get_next()<br/># create the initialisation operations<br/>train_init_op = iter.make_initializer(train_dataset)<br/>test_init_op = iter.make_initializer(test_dataset)</span><span id="62df" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    sess.run(train_init_op) # switch to train dataset<br/>    for _ in range(EPOCHS):<br/>        sess.run([features, labels])<br/>    sess.run(test_init_op) # switch to val dataset<br/>    print(sess.run([features, labels]))</span></pre><h2 id="6090" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">可馈送迭代器</h2><p id="32ad" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">这非常类似于<code class="fe lb lc ld le b">reinitializable</code>迭代器，但是它不是在数据集之间切换，而是在迭代器之间切换。在我们创建了两个数据集之后</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="fea0" class="mw lg je le b gy nq nr l ns nt">train_dataset = tf.data.Dataset.from_tensor_slices((x,y))<br/>test_dataset = tf.data.Dataset.from_tensor_slices((x,y))</span></pre><p id="0a6d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一个用于培训，一个用于测试。然后，我们可以创建我们的迭代器，在这种情况下我们使用<code class="fe lb lc ld le b">initializable</code>迭代器，但是你也可以使用<code class="fe lb lc ld le b">one shot</code>迭代器</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="82a0" class="mw lg je le b gy nq nr l ns nt">train_iterator = train_dataset.make_initializable_iterator()<br/>test_iterator = test_dataset.make_initializable_iterator()</span></pre><p id="add3" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，我们需要定义和<code class="fe lb lc ld le b">handle</code>，这将是一个可以动态改变的占位符。</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="3e79" class="mw lg je le b gy nq nr l ns nt">handle = tf.placeholder(tf.string, shape=[])</span></pre><p id="59dc" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，与之前类似，我们使用数据集的形状定义一个泛型迭代器</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="10a3" class="mw lg je le b gy nq nr l ns nt">iter = tf.data.Iterator.from_string_handle(<br/>    handle, train_dataset.output_types, train_dataset.output_shapes)</span></pre><p id="b239" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，我们得到下一个元素</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="b224" class="mw lg je le b gy nq nr l ns nt">next_elements = iter.get_next()</span></pre><p id="cc53" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了在迭代器之间切换，我们只需调用<code class="fe lb lc ld le b">next_elemenents</code>操作，并在feed_dict中传递正确的<code class="fe lb lc ld le b">handle</code>。例如，要从训练集中获取一个元素:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="7032" class="mw lg je le b gy nq nr l ns nt">sess.run(next_elements, feed_dict = {handle: train_handle})</span></pre><p id="df57" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你正在使用<code class="fe lb lc ld le b">initializable</code>迭代器，就像我们正在做的那样，记得在开始之前初始化它们</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="05c0" class="mw lg je le b gy nq nr l ns nt">sess.run(train_iterator.initializer, feed_dict={ x: train_data[0], y: train_data[1]})<br/>    sess.run(test_iterator.initializer, feed_dict={ x: test_data[0], y: test_data[1]})</span></pre><p id="125f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">综上所述，我们得到:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="978d" class="mw lg je le b gy nq nr l ns nt"># feedable iterator to switch between iterators<br/>EPOCHS = 10<br/># making fake data using numpy<br/>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>test_data = (np.random.sample((10,2)), np.random.sample((10,1)))<br/># create placeholder<br/>x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])<br/># create two datasets, one for training and one for test<br/>train_dataset = tf.data.Dataset.from_tensor_slices((x,y))<br/>test_dataset = tf.data.Dataset.from_tensor_slices((x,y))<br/># create the iterators from the dataset<br/>train_iterator = train_dataset.make_initializable_iterator()<br/>test_iterator = test_dataset.make_initializable_iterator()<br/># same as in the doc <a class="ae la" href="https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator</a><br/>handle = tf.placeholder(tf.string, shape=[])<br/>iter = tf.data.Iterator.from_string_handle(<br/>    handle, train_dataset.output_types, train_dataset.output_shapes)<br/>next_elements = iter.get_next()</span><span id="5d0f" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    train_handle = sess.run(train_iterator.string_handle())<br/>    test_handle = sess.run(test_iterator.string_handle())<br/>    <br/>    # initialise iterators. <br/>    sess.run(train_iterator.initializer, feed_dict={ x: train_data[0], y: train_data[1]})<br/>    sess.run(test_iterator.initializer, feed_dict={ x: test_data[0], y: test_data[1]})<br/>    <br/>    for _ in range(EPOCHS):<br/>        x,y = sess.run(next_elements, feed_dict = {handle: train_handle})<br/>        print(x, y)<br/>        <br/>    x,y = sess.run(next_elements, feed_dict = {handle: test_handle})<br/>    print(x,y)</span></pre><h1 id="85f9" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">消费数据</h1><p id="8426" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">在前面的例子中，我们使用了会话来打印数据集中<code class="fe lb lc ld le b">next</code>元素的值。</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="28ae" class="mw lg je le b gy nq nr l ns nt">...<br/>next_el = iter.get_next()<br/>...<br/>print(sess.run(next_el)) # will output the current element</span></pre><p id="55e4" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了将数据传递给模型，我们必须传递从<code class="fe lb lc ld le b">get_next()</code>生成的张量</p><p id="340f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在下面的代码片段中，我们有一个包含两个numpy数组的数据集，使用了第一部分中的相同示例。注意，我们需要将<code class="fe lb lc ld le b">.random.sample</code>包装在另一个numpy数组中，以添加一个维度，我们需要这个维度来批量处理数据</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="1fdf" class="mw lg je le b gy nq nr l ns nt"># using two numpy arrays<br/>features, labels = (np.array([np.random.sample((100,2))]), <br/>                    np.array([np.random.sample((100,1))]))</span><span id="f3cb" class="mw lg je le b gy nu nr l ns nt">dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)</span></pre><p id="f5f5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后像往常一样，我们创建一个迭代器</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="0643" class="mw lg je le b gy nq nr l ns nt">iter = dataset.make_one_shot_iterator()<br/>x, y = iter.get_next()</span></pre><p id="2b4f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们制作一个模型，一个简单的神经网络</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="d506" class="mw lg je le b gy nq nr l ns nt"># make a simple model<br/>net = tf.layers.dense(x, 8) # pass the first value from iter.get_next() as input<br/>net = tf.layers.dense(net, 8)<br/>prediction = tf.layers.dense(net, 1)</span><span id="eb7b" class="mw lg je le b gy nu nr l ns nt">loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label<br/>train_op = tf.train.AdamOptimizer().minimize(loss)</span></pre><p id="14aa" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们<strong class="kd jf">直接</strong>使用来自<code class="fe lb lc ld le b">iter.get_next()</code>的张量作为第一层的输入，并作为损失函数的标签。包装在一起:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="5e8a" class="mw lg je le b gy nq nr l ns nt">EPOCHS = 10<br/>BATCH_SIZE = 16<br/># using two numpy arrays<br/>features, labels = (np.array([np.random.sample((100,2))]), <br/>                    np.array([np.random.sample((100,1))]))</span><span id="7bca" class="mw lg je le b gy nu nr l ns nt">dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)</span><span id="7fb5" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_one_shot_iterator()<br/>x, y = iter.get_next()</span><span id="489a" class="mw lg je le b gy nu nr l ns nt"># make a simple model<br/>net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input<br/>net = tf.layers.dense(net, 8, activation=tf.tanh)<br/>prediction = tf.layers.dense(net, 1, activation=tf.tanh)</span><span id="4d52" class="mw lg je le b gy nu nr l ns nt">loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label<br/>train_op = tf.train.AdamOptimizer().minimize(loss)</span><span id="b8cd" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    sess.run(tf.global_variables_initializer())<br/>    for i in range(EPOCHS):<br/>        _, loss_value = sess.run([train_op, loss])<br/>        print("Iter: {}, Loss: {:.4f}".format(i, loss_value))</span></pre><p id="2d5f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">输出:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="6cb0" class="mw lg je le b gy nq nr l ns nt">Iter: 0, Loss: 0.1328 <br/>Iter: 1, Loss: 0.1312 <br/>Iter: 2, Loss: 0.1296 <br/>Iter: 3, Loss: 0.1281 <br/>Iter: 4, Loss: 0.1267 <br/>Iter: 5, Loss: 0.1254 <br/>Iter: 6, Loss: 0.1242 <br/>Iter: 7, Loss: 0.1231 <br/>Iter: 8, Loss: 0.1220 <br/>Iter: 9, Loss: 0.1210</span></pre><h1 id="ce25" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">有用的东西</h1><h2 id="2f21" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">一批</h2><p id="8843" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">通常批处理数据是一件痛苦的事情，有了<code class="fe lb lc ld le b">Dataset </code> API，我们可以使用方法<code class="fe lb lc ld le b">batch(BATCH_SIZE)</code>以提供的大小自动批处理数据集。默认值为1。在下面的例子中，我们使用的批量大小为4</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="770a" class="mw lg je le b gy nq nr l ns nt"># BATCHING<br/>BATCH_SIZE = 4<br/>x = np.random.sample((100,2))<br/># make a dataset from a numpy array<br/>dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)</span><span id="fc7e" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_one_shot_iterator()<br/>el = iter.get_next()</span><span id="a9c4" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    print(sess.run(el)) </span></pre><p id="2023" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">输出:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="9f17" class="mw lg je le b gy nq nr l ns nt">[[ 0.65686128  0.99373963]<br/> [ 0.69690451  0.32446826]<br/> [ 0.57148422  0.68688242]<br/> [ 0.20335116  0.82473219]]</span></pre><h2 id="cfbd" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">重复</h2><p id="5fdb" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">使用<code class="fe lb lc ld le b">.repeat()</code>,我们可以指定数据集迭代的次数。如果没有传递参数，它将永远循环下去，通常最好是永远循环下去，用标准循环直接控制历元数。</p><h2 id="9598" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">洗牌</h2><p id="7450" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">我们可以通过使用方法<code class="fe lb lc ld le b">shuffle()</code>来打乱数据集，默认情况下，每个时期都会打乱数据集。</p><p id="7de7" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">记住:打乱数据集对于避免过度适应非常重要。</em></p><p id="8af5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们还可以设置参数<code class="fe lb lc ld le b">buffer_size</code>，一个固定大小的缓冲区，下一个元素将从其中统一选择。示例:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="0dc4" class="mw lg je le b gy nq nr l ns nt"># BATCHING<br/>BATCH_SIZE = 4<br/>x = np.array([[1],[2],[3],[4]])<br/># make a dataset from a numpy array<br/>dataset = tf.data.Dataset.from_tensor_slices(x)<br/>dataset = dataset.shuffle(buffer_size=100)<br/>dataset = dataset.batch(BATCH_SIZE)</span><span id="f7d7" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_one_shot_iterator()<br/>el = iter.get_next()</span><span id="5882" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    print(sess.run(el))</span></pre><p id="8621" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首次运行输出:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="7448" class="mw lg je le b gy nq nr l ns nt">[[4]<br/> [2]<br/> [3]<br/> [1]]</span></pre><p id="fd3a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">第二轮输出:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="096e" class="mw lg je le b gy nq nr l ns nt">[[3]<br/> [1]<br/> [2]<br/> [4]]</span></pre><p id="f06d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">没错。它被洗牌了。如果您愿意，也可以设置<code class="fe lb lc ld le b">seed</code>参数。</p><h1 id="b01e" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">地图</h1><p id="c92c" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">您可以使用<code class="fe lb lc ld le b">map</code>方法将自定义函数应用于数据集的每个成员。在下面的例子中，我们将每个元素乘以2:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="655f" class="mw lg je le b gy nq nr l ns nt"># MAP<br/>x = np.array([[1],[2],[3],[4]])<br/># make a dataset from a numpy array<br/>dataset = tf.data.Dataset.from_tensor_slices(x)<br/>dataset = dataset.map(lambda x: x*2)</span><span id="7c19" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_one_shot_iterator()<br/>el = iter.get_next()</span><span id="36e4" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>#     this will run forever<br/>        for _ in range(len(x)):<br/>            print(sess.run(el))</span></pre><p id="55c3" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">输出:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="bf55" class="mw lg je le b gy nq nr l ns nt">[2]<br/>[4]<br/>[6]<br/>[8]</span></pre><h1 id="02e2" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">完整示例</h1><h2 id="ac00" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated"><strong class="ak">可初始化的</strong>迭代器</h2><p id="1a9b" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">在下面的例子中，我们使用批处理来训练一个简单的模型，并且我们使用一个<strong class="kd jf">可初始化的迭代器</strong>在训练和测试数据集之间切换</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="0c0f" class="mw lg je le b gy nq nr l ns nt"># Wrapping all together -&gt; Switch between train and test set using Initializable iterator<br/>EPOCHS = 10<br/># create a placeholder to dynamically switch between batch sizes<br/>batch_size = tf.placeholder(tf.int64)</span><span id="aba9" class="mw lg je le b gy nu nr l ns nt">x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])<br/>dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()</span><span id="c369" class="mw lg je le b gy nu nr l ns nt"># using two numpy arrays<br/>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>test_data = (np.random.sample((20,2)), np.random.sample((20,1)))</span><span id="c6bd" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_initializable_iterator()<br/>features, labels = iter.get_next()<br/># make a simple model<br/>net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input<br/>net = tf.layers.dense(net, 8, activation=tf.tanh)<br/>prediction = tf.layers.dense(net, 1, activation=tf.tanh)</span><span id="99eb" class="mw lg je le b gy nu nr l ns nt">loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label<br/>train_op = tf.train.AdamOptimizer().minimize(loss)</span><span id="6a05" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    sess.run(tf.global_variables_initializer())<br/>    # initialise iterator with train data<br/>    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})<br/>    print('Training...')<br/>    for i in range(EPOCHS):<br/>        tot_loss = 0<br/>        for _ in range(n_batches):<br/>            _, loss_value = sess.run([train_op, loss])<br/>            tot_loss += loss_value<br/>        print("Iter: {}, Loss: {:.4f}".format(i, tot_loss / n_batches))<br/>    # initialise iterator with test data<br/>    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1], batch_size: test_data[0].shape[0]})<br/>    print('Test Loss: {:4f}'.format(sess.run(loss)))</span></pre><p id="a4b0" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf">注意，我们使用了一个批量大小的占位符，以便在训练后动态切换它</strong></p><p id="6e20" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">输出</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="abe0" class="mw lg je le b gy nq nr l ns nt">Training...<br/>Iter: 0, Loss: 0.2977<br/>Iter: 1, Loss: 0.2152<br/>Iter: 2, Loss: 0.1787<br/>Iter: 3, Loss: 0.1597<br/>Iter: 4, Loss: 0.1277<br/>Iter: 5, Loss: 0.1334<br/>Iter: 6, Loss: 0.1000<br/>Iter: 7, Loss: 0.1154<br/>Iter: 8, Loss: 0.0989<br/>Iter: 9, Loss: 0.0948<br/>Test Loss: 0.082150</span></pre><h2 id="d1a5" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">可重新初始化的迭代器</h2><p id="da06" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">在下面的例子中，我们使用批处理训练一个简单的模型，并且我们使用<strong class="kd jf">可重新初始化的迭代器</strong>在训练和测试数据集之间切换</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="ef22" class="mw lg je le b gy nq nr l ns nt"># Wrapping all together -&gt; Switch between train and test set using Reinitializable iterator<br/>EPOCHS = 10<br/># create a placeholder to dynamically switch between batch sizes<br/>batch_size = tf.placeholder(tf.int64)</span><span id="9b9e" class="mw lg je le b gy nu nr l ns nt">x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])<br/>train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size).repeat()<br/>test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size) # always batch even if you want to one shot it<br/># using two numpy arrays<br/>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>test_data = (np.random.sample((20,2)), np.random.sample((20,1)))</span><span id="96b5" class="mw lg je le b gy nu nr l ns nt"># create a iterator of the correct shape and type<br/>iter = tf.data.Iterator.from_structure(train_dataset.output_types,<br/>                                           train_dataset.output_shapes)<br/>features, labels = iter.get_next()<br/># create the initialisation operations<br/>train_init_op = iter.make_initializer(train_dataset)<br/>test_init_op = iter.make_initializer(test_dataset)</span><span id="3a30" class="mw lg je le b gy nu nr l ns nt"># make a simple model<br/>net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input<br/>net = tf.layers.dense(net, 8, activation=tf.tanh)<br/>prediction = tf.layers.dense(net, 1, activation=tf.tanh)</span><span id="e0a3" class="mw lg je le b gy nu nr l ns nt">loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label<br/>train_op = tf.train.AdamOptimizer().minimize(loss)</span><span id="7b18" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    sess.run(tf.global_variables_initializer())<br/>    # initialise iterator with train data<br/>    sess.run(train_init_op, feed_dict = {x : train_data[0], y: train_data[1], batch_size: 16})<br/>    print('Training...')<br/>    for i in range(EPOCHS):<br/>        tot_loss = 0<br/>        for _ in range(n_batches):<br/>            _, loss_value = sess.run([train_op, loss])<br/>            tot_loss += loss_value<br/>        print("Iter: {}, Loss: {:.4f}".format(i, tot_loss / n_batches))<br/>    # initialise iterator with test data<br/>    sess.run(test_init_op, feed_dict = {x : test_data[0], y: test_data[1], batch_size:len(test_data[0])})<br/>    print('Test Loss: {:4f}'.format(sess.run(loss)))</span></pre><h1 id="b6d7" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">其他资源</h1><p id="e071" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">张量流数据集教程:<a class="ae la" href="https://www.tensorflow.org/programmers_guide/datasets" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/programmers_guide/datasets</a></p><p id="78e0" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">数据集文档:</p><p id="21c9" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae la" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/data/Dataset</a></p><h1 id="efb9" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">结论</h1><p id="fbfa" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated"><code class="fe lb lc ld le b">Dataset</code> API为我们提供了一种快速而健壮的方式来创建优化的输入管道，以训练、评估和测试我们的模型。在本文中，我们已经看到了可以用它们进行的大多数常见操作。</p><p id="9559" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你可以用我为这篇文章做的笔记本作为参考。</p><p id="b64a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">感谢您的阅读，</p><p id="7658" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">弗朗西斯科·萨维里奥</p></div></div>    
</body>
</html>