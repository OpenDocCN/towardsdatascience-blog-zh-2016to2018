<html>
<head>
<title>Evolution of Object Detection and Localization Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">目标检测和定位算法的发展</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evolution-of-object-detection-and-localization-algorithms-e241021d8bad?source=collection_archive---------0-----------------------#2018-02-15">https://towardsdatascience.com/evolution-of-object-detection-and-localization-algorithms-e241021d8bad?source=collection_archive---------0-----------------------#2018-02-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="ae7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过对基本概念的直观解释，理解对象检测和定位的最新进展。</p><p id="9e92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">目标检测是计算机视觉中发展非常迅速的领域之一。感谢深度学习！每年都有新的算法/模型不断超越以前的算法/模型。事实上，脸书人工智能团队上周刚刚发布了一个最新的物体探测软件系统。这款软件名为<a class="ae kl" href="https://github.com/facebookresearch/Detectron" rel="noopener ugc nofollow" target="_blank"> Detectron </a>，整合了众多针对物体检测的研究项目，由<a class="ae kl" href="https://github.com/caffe2/caffe2" rel="noopener ugc nofollow" target="_blank"> Caffe2 </a>深度学习框架提供支持。</p><p id="cd81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">今天，有太多的预训练模型用于物体检测(YOLO，RCNN，快速RCNN，掩模RCNN，多框等)。).因此，只需要很少的努力就可以检测出视频或图像中的大多数对象。但是我博客的目的不是谈论这些模型的实现。相反，我试图以一种清晰和简洁的方式解释基本概念。</p><p id="567e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我最近完成了<strong class="jp ir">吴恩达的</strong> <strong class="jp ir">卷积神经网络课程</strong>的第三周，他在课程中讲述了物体检测算法。这个博客的大部分内容都是受那门课的启发。</p><p id="32dc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">编辑:</strong>我目前在做<strong class="jp ir"> Fast.ai的《程序员前沿深度学习》课程，老师是杰瑞米·霍华德</strong>。现在，我已经使用PyTorch和fast.ai库实现了下面讨论的算法。这里是<a class="ae kl" href="https://github.com/groverpr/deep-learning/tree/master/computer_vision" rel="noopener ugc nofollow" target="_blank">链接</a>到代码。如果你想了解下面讨论的算法的实现部分，可以看看这个。实现已借用fast.ai <a class="ae kl" href="https://github.com/fastai/fastai/tree/master/courses" rel="noopener ugc nofollow" target="_blank">课程笔记本</a>，有注释和笔记。</p><h1 id="680e" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">CNN简介</h1><p id="e368" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在我解释对象检测算法的工作原理之前，我想花一些时间介绍卷积神经网络，也称为CNN或ConvNets。在深度学习时代，细胞神经网络是大多数计算机视觉任务的基本构建模块。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/acc150f2f45d9c1ec494acd6eb59dec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xm2wPFYD_4vbDnoxxMK5bw.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="bd mf">Fig. 1. Convolution demo in Excel</strong></figcaption></figure><p id="dc0a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">我们想要什么？</strong>我们想要某种算法，它查看图像，看到图像中的模式，并告诉图像中有什么类型的对象。例如，是猫或狗的图像。</p><p id="cea6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">什么是计算机的图像？只是数字矩阵。例如，参见上面的图1。左边的图像只是手写数字2的28*28像素图像(取自MNIST数据)，在Excel电子表格中表示为数字矩阵。</p><p id="94cc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">我们怎样才能教会计算机学会识别图像中的物体？</strong>通过让计算机学习像垂直边缘、水平边缘、圆形以及许多其他人类未知的模式。</p><p id="7468" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">计算机如何学习模式？</strong>回旋！<br/>(看这个的时候看上图)卷积是两个矩阵之间给出第三个矩阵的数学运算。较小的矩阵，我们称之为过滤器或内核(图1中的3x3 ),对图像像素矩阵进行操作。根据滤波器矩阵中的数字，输出矩阵可以识别输入图像中存在的特定模式。在上面的例子中，滤波器是垂直边缘检测器，它学习输入图像中的垂直边缘。在深度学习的背景下，输入图像及其随后的输出从许多这样的过滤器中通过。滤波器中的数字由神经网络学习，而模式则自行产生。</p><p id="9b8c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">为什么卷积有效？</strong>因为在大多数图像中，对象在相对像素密度(数量级)上具有一致性，这可以被卷积利用。</p><p id="d8de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mg">我知道对于一个不了解CNN的读者来说，CNN上只有几行字是不够的。但是CNN并不是这篇博客的主要话题，我已经提供了基本的介绍，所以读者在继续阅读之前不必再打开10个链接来理解CNN。</em></p><p id="7f3a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看完这篇博客后，如果你还想了解更多关于CNN的信息，我强烈建议你阅读亚当·盖特吉的博客。</p><h1 id="520a" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated"><strong class="ak">计算机视觉任务的分类</strong></h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mj"><img src="../Images/ed3b776f990ab5c0f15eb0a6577afb7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cpe4z05DgTJvm0SG0MsrjA.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="bd mf">Fig. 2: Common computer vision tasks</strong></figcaption></figure><p id="4d94" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以图2中的猫和狗图像为例，以下是计算机视觉建模算法完成的最常见任务:</p><ol class=""><li id="4c49" class="mk ml iq jp b jq jr ju jv jy mm kc mn kg mo kk mp mq mr ms bi translated"><strong class="jp ir">图像分类:</strong>这是最常见的计算机视觉问题，算法查看图像并对其中的对象进行分类。图像分类具有广泛的应用，从社交网络上的人脸检测到医学中的癌症检测。这种问题通常使用卷积神经网络(CNN)来建模。</li><li id="1669" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated"><strong class="jp ir">物体分类与定位:</strong>假设我们不仅想知道图像中是否有猫，还想知道猫的确切位置。目标定位算法不仅标记目标的类别，而且在图像中围绕目标的位置画出一个包围盒。</li><li id="18d6" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated"><strong class="jp ir">多目标检测和定位:</strong>如果图像中有多个目标(如上图中的3只狗和2只猫)并且我们想检测它们，该怎么办？这将是一个目标检测和定位问题。这一点的一个众所周知的应用是在自动驾驶汽车中，其中该算法不仅需要检测汽车，还需要检测行人、摩托车、树木和帧中的其他物体。这类问题需要利用从图像分类和目标定位中学到的思想或概念。</li></ol><p id="51ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在回到计算机视觉任务。在深度学习的背景下，上述3种类型的任务之间的基本算法差异只是选择相关的输入和输出。让我用一个信息图来详细解释一下这条线。</p><h2 id="11b1" class="my kn iq bd ko mz na dn ks nb nc dp kw jy nd ne la kc nf ng le kg nh ni li nj bi translated"><strong class="ak"> 1。图像分类</strong></h2><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nk"><img src="../Images/013dfc0752a787d196f36c2266f3df21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Z2fuEg8j4IG8ThpyjdSHg.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="bd mf">Fig. 3: Steps for image classification using CNN</strong></figcaption></figure><p id="6001" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图3中的信息图显示了用于图像分类的典型CNN的样子。</p><blockquote class="nl nm nn"><p id="fa78" class="jn jo mg jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated"><strong class="jp ir"> 1。</strong>通过n个滤波器(图3中n = 4)对具有一定高度、宽度和通道深度(上例中为940、550、3)的输入图像进行卷积【如果您仍然不清楚卷积的确切含义，请查看<a class="ae kl" href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721" rel="noopener">此链接</a>以了解深度神经网络中的卷积】。<br/> <strong class="jp ir"> 2。</strong>卷积的输出用非线性变换处理，通常是最大池和RELU。<br/> <strong class="jp ir"> 3。</strong>卷积、最大池和RELU的上述3个操作被执行多次。<br/> <strong class="jp ir"> 4。</strong>最终层的输出被发送到Softmax层，soft max层将数字在0和1之间转换，给出图像属于特定类别的概率。我们使损失最小化，以便使最后一层的预测尽可能接近实际值。</p></blockquote><h2 id="016e" class="my kn iq bd ko mz na dn ks nb nc dp kw jy nd ne la kc nf ng le kg nh ni li nj bi translated"><strong class="ak"> 2。物体分类和定位</strong></h2><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nk"><img src="../Images/4385f3214be8de6ca649e6037ebab073.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aSfM0hKa4JNf3_YdQsx2vg.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="bd mf">Fig. 4: Input and output for object localization problems</strong></figcaption></figure><p id="8222" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，为了让我们的模型绘制对象的边界框，我们只需改变前面算法的输出标签，以便让我们的模型学习对象的类别以及对象在图像中的位置。我们在输出层增加了4个数字，包括物体的质心位置和图像中边界框的宽度和高度的比例。</p><p id="ebf1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简单吧？只需要加上一堆输出单位就可以吐槽出你要识别的不同位置的x，y坐标。这些不同的位置或标志对于我们所有的图像中的特定对象来说是一致的。例如，对于汽车，与图像中的其他点相比，高度将小于宽度，并且质心将具有特定的像素密度。</p><p id="6da0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mg">同样的逻辑，如果图像中有多个物体，我们想对它们进行分类和定位，你认为会发生什么变化？我建议你此刻停下来思考一下，你可能会自己找到答案。</em></p><h2 id="cb2e" class="my kn iq bd ko mz na dn ks nb nc dp kw jy nd ne la kc nf ng le kg nh ni li nj bi translated"><strong class="ak"> 3。多目标检测和定位</strong></h2><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nr"><img src="../Images/6c8a74b1022b76b5d9b0918b0df6b7fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GU-nroHGjzP-mqeFbidxdA.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="bd mf">Fig. 5: Input and output for object detection and localization problems</strong></figcaption></figure><p id="ab18" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了检测图像中的各种对象，我们可以直接使用我们迄今为止从对象定位中学到的知识。不同之处在于，我们希望我们的算法能够对图像中的所有对象进行分类和定位，而不仅仅是一个对象。因此，这个想法是，只需将图像裁剪成多个图像，并对所有裁剪的图像运行CNN来检测对象。</p><p id="e499" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">算法的工作方式如下:</p><blockquote class="nl nm nn"><p id="4d3f" class="jn jo mg jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated"><strong class="jp ir"> 1。</strong>制作一个尺寸远小于实际图像尺寸的窗口。裁剪它并将其传递给ConvNet (CNN)并让ConvNet进行预测。<br/> <strong class="jp ir"> 2。</strong>继续滑动窗口，将裁剪后的图像传入ConvNet。<br/> <strong class="jp ir"> 3。</strong>在用该窗口尺寸裁剪完图像的所有部分后，再次重复所有步骤以获得更大的窗口尺寸。再次将裁剪后的图像传入ConvNet，让它进行预测。<br/> <strong class="jp ir"> 4。</strong>最后，你将得到一组裁剪区域，其中包含一些对象，以及对象的类和边界框。</p></blockquote><p id="463d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种解决方案被称为滑动窗口对象检测。这是一个非常基本的解决方案，有如下许多注意事项:</p><p id="e35e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">答:计算量很大:</strong>裁剪多幅图像并通过ConvNet传输，计算量非常大。</p><p id="c51d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">解决方案:</strong>有一个简单的hack来提高滑动窗口法的计算能力。它是用1x1卷积层替换ConvNet中的全连接层，并且对于给定的窗口大小，仅传递输入图像一次。因此，在实际实现中，我们并不是一次传递一个裁剪后的图像，而是一次传递完整的图像。</p><p id="5487" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> B .不准确的边界框:</strong>我们在整个图像上滑动正方形的窗口，可能对象是矩形的，也可能没有一个正方形与对象的实际大小完全匹配。虽然这种算法能够在一幅图像中找到并定位多个目标，但是包围盒的精度仍然很差。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/5dc4db1017959f715db750f9c744c175.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/1*wbuckxPMCs4BEemAuqd94g.gif"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="bd mf">Fig. 6. Bounding boxes from sliding window CNN</strong></figcaption></figure><p id="4292" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我已经谈到了对象检测问题的最基本的解决方案。但是它有许多注意事项，不是最准确的，并且实现起来计算量很大。那么，如何才能让我们的算法更好更快呢？</p><h2 id="371c" class="my kn iq bd ko mz na dn ks nb nc dp kw jy nd ne la kc nf ng le kg nh ni li nj bi translated"><strong class="ak">更好的解决方案？YOLO </strong></h2><p id="0050" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">原来我们有YOLO(你只看一次)，它比滑动窗口算法更准确，更快。它仅仅是基于我们已经知道的算法之上的一个小调整。这个想法是将图像分成多个网格。然后，我们更改数据的标签，以便为每个网格单元实现定位和分类算法。让我再用一个信息图来解释一下。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nt"><img src="../Images/2e776224ba270bc693db0a5ebe865973.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lNZL03057Q4QH-zhzBD6tQ.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="bd mf">Fig. 7. Bounding boxes, input and output for YOLO</strong></figcaption></figure><p id="d405" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">YOLO轻松地说道:</p><blockquote class="nl nm nn"><p id="43e9" class="jn jo mg jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated"><strong class="jp ir"> 1。</strong>将图像分成多个网格。为了便于说明，我在上图中画了4x4的网格，但是YOLO的实际实现有不同数量的网格。(7x7用于在PASCAL VOC数据集上训练YOLO)</p><p id="3897" class="jn jo mg jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated"><strong class="jp ir"> 2。</strong>标注训练数据，如上图所示。如果C是数据中唯一对象的数量，S*S是我们将图像分割成的网格数量，那么我们的输出向量的长度将是S*S*(C+5)。例如，在上述情况下，我们的目标向量是4*4*(3+5)，因为我们将图像划分为4*4的网格，并针对3个独特的对象进行训练:汽车、灯光和行人。<br/> <br/> <strong class="jp ir"> 3。制作一个深度卷积神经网络，损失函数作为输出激活和标签向量之间的误差。基本上，该模型只需将输入图像通过ConvNet向前传递一次，即可预测所有网格的输出。</strong></p><p id="b692" class="jn jo mg jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated"><strong class="jp ir"> 4。</strong>请记住，网格单元中存在的对象标签(P.Object)是由该网格中存在的对象质心决定的。这对于不允许一个对象在不同的网格中被多次计数很重要。</p></blockquote><p id="7a35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">YOLO的注意事项及其解决方案:</strong></p><p id="b4de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> A .无法检测同一网格中的多个对象。</strong>这个问题可以通过选择较小的网格尺寸来解决。但是，即使选择较小的网格尺寸，该算法在物体彼此非常接近的情况下仍然会失败，例如一群鸟的图像。</p><p id="1bc1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">解决方案:锚箱。</strong>除了每个网格单元有5+C个标签(其中C是不同对象的数量)之外，锚定框的想法是每个网格单元有(5+C)*A个标签，其中A是必需的锚定框。如果一个对象被分配给一个网格中的一个锚定框，其他对象可以被分配给同一网格的另一个锚定框。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nu"><img src="../Images/63ecb2ee9221fe7751eafe70d7bf8aa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z1IAmRfbYQgtxqeY2b8JYQ.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="bd mf">Fig. 8. YOLO with anchor boxes</strong></figcaption></figure><p id="3de7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> B .多次检测一个物体的可能性。</strong></p><p id="f21f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">解:</strong> <strong class="jp ir">非max抑。</strong>非最大值抑制移除与高概率边界框非常接近的低概率边界框。</p><h2 id="989f" class="my kn iq bd ko mz na dn ks nb nc dp kw jy nd ne la kc nf ng le kg nh ni li nj bi translated"><strong class="ak">结论:</strong></h2><p id="0160" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">截至今天，在不同的深度学习框架中有多个版本的预训练YOLO模型可用，包括<a class="ae kl" href="https://github.com/thtrieu/darkflow" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>。最新的YOLO论文是:<a class="ae kl" href="https://arxiv.org/pdf/1612.08242.pdf" rel="noopener ugc nofollow" target="_blank">《yolo 9000:更好更快更强》</a>。该模型在9000个类上进行训练。还有一些基于选择性区域提议的区域CNN (R-CNN)算法，我没有讨论过。由脸书AI开发的软件系统Detectron也实现了R-CNN的变体，屏蔽R-CNN。</p><h2 id="4c1f" class="my kn iq bd ko mz na dn ks nb nc dp kw jy nd ne la kc nf ng le kg nh ni li nj bi translated">参考资料:</h2><ol class=""><li id="cffc" class="mk ml iq jp b jq lk ju ll jy nv kc nw kg nx kk mp mq mr ms bi translated">【https://arxiv.org/pdf/1506.02640.pdf】你只看一次:统一的、实时的物体检测<br/>  <a class="ae kl" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank"/></li><li id="97a8" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">更好、更快、更强<br/><a class="ae kl" href="https://arxiv.org/pdf/1612.08242.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.08242.pdf</a></li><li id="0b34" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated"><strong class="jp ir">吴恩达的卷积神经网络(deep learning . ai)<br/></strong><a class="ae kl" href="https://www.coursera.org/learn/convolutional-neural-networks" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/卷积神经网络</a></li></ol></div></div>    
</body>
</html>