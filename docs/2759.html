<html>
<head>
<title>Decision Trees — Understanding Explainable AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树——理解可解释的人工智能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-trees-understanding-explainable-ai-620fc37e598d?source=collection_archive---------5-----------------------#2018-03-01">https://towardsdatascience.com/decision-trees-understanding-explainable-ai-620fc37e598d?source=collection_archive---------5-----------------------#2018-03-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/4422177059837e1a3bf1053a83bfb05c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*14cEm2ezAJs_nPgjsVJHZg.jpeg"/></div></div></figure><p id="fe24" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">可解释的人工智能或 XAI 是人工智能的一个子类，与“黑盒”模型相反，模型做出的决定可以被人类解释。随着人工智能从纠正我们的拼写和定向广告转向驾驶我们的汽车和诊断病人，验证和证明得出的结论的需求开始得到优先考虑。</p><p id="6a03" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了开始深入这个领域，让我们看一个简单的 XAI 模型:决策树。决策树很容易阅读，甚至可以通过将选择分成许多小的子选择来模仿人类的决策方法。一个简单的例子是当一个人离开高中时，如何评价当地的大学。假设学生心中有一门课程，一个简单的决策过程可以是:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi kw"><img src="../Images/32d12a14faddb6a609e106b4e7cb996b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*1_XPfN_NNIb31ds0euY_qg.png"/></div></figure><p id="35c1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果第三方可以访问“模型”和所需的变量，学生如何得出他们的结论可以很容易地得到证明。</p><p id="e894" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这种相同的结构可以应用于监督学习，目标是创建一个最好地描述训练数据的决策树。然后，该模型可用于理解变量之间的关系或用于预测应用。</p><h1 id="b367" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">该算法</h1><p id="248e" class="pw-post-body-paragraph jy jz iq ka b kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv ij bi translated">决策树的构建是作为一个推理过程来完成的。</p><ol class=""><li id="c62a" class="me mf iq ka b kb kc kf kg kj mg kn mh kr mi kv mj mk ml mm bi translated">估计哪个变量给出最大的<em class="mn">信息增益</em>。信息增益是自变量状态已知时因变量熵的<em class="mn">减少。<br/>那里有很多大词。<br/>本质上，这衡量了当我们根据因变量的值将自变量分成组时，自变量的组织程度。</em></li><li id="894f" class="me mf iq ka b kb mo kf mp kj mq kn mr kr ms kv mj mk ml mm bi translated">选择在组织中提供最大增加的因变量，并根据该变量分割数据集。</li><li id="7376" class="me mf iq ka b kb mo kf mp kj mq kn mr kr ms kv mj mk ml mm bi translated">此时，三个条件之一必须为真:<br/> -因变量现在只取一个值。在这种情况下，树的这一分支是完整的，我们已经达到了我们的“决定”。<br/> -因变量取&gt; 1 值。在这里，我们简单地回到第一步，并尝试进一步缩小范围。<br/>-因变量取值&gt; 1，但我们没有更多的自变量来分割数据。在这里，我们简单地说一下决策可能采取的值，并根据每个选项的相对比例估计每个值的概率。</li></ol><h1 id="0e20" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">计算信息增益</h1><p id="332d" class="pw-post-body-paragraph jy jz iq ka b kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv ij bi translated">首先，我们需要一个组织或熵的公式。为了计算因变量的熵，我们使用:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/a5921fb4000ace704dc1be296e9b4434.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hlLK3rogTf9L56THBA-UwA.png"/></div></div></figure><p id="91f8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下图显示了 Y(其中 Y 有两种状态)的熵是如何随着每种状态的概率而变化的。当一个状态的概率为 0 时，熵也为 0，因为这是 Y 最有组织的时候，而当 Y 在两个状态之间平均分配时，熵最大。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/ee65a331bb890e7bcd369848de38bb7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*7uoib5BMb7Wzneoqda1oMw.png"/></div></figure><p id="15c0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">扩展这一点以增加已知独立变量 X 对熵的影响:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mv"><img src="../Images/b7cbd77e84ff60e97f1d8294286f5eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lKX0Iqy370cEut0Tw_bLsA.png"/></div></div></figure><p id="ca7a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">信息增益现在被定义为我们知道 X 和不知道 X 时的熵之差。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/9362159b9118ec18758358530c2bbd66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WCsnKIk_nZdTAX8d_qXKHg.png"/></div></div></figure><h1 id="7dbb" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">把密码给我！</h1><p id="8b3f" class="pw-post-body-paragraph jy jz iq ka b kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv ij bi translated">这里没有包括一些计算熵和创建图形的函数。</p><pre class="kx ky kz la gt mx my mz na aw nb bi"><span id="2b65" class="nc lc iq my b gy nd ne l nf ng">def decide(Y, X_dict, previous_node):<br/>    #Calc info gain for each X<br/>    max_IG = 0<br/>    var_to_split = None</span><span id="49dd" class="nc lc iq my b gy nh ne l nf ng">#Calculate information gain to find out which variable to split on<br/>    for x in X_dict.keys():<br/>        IG = InfoGain(Y, X_dict[x])<br/>        if IG &gt; max_IG:<br/>            max_IG = IG<br/>            var_to_split = x</span><span id="c216" class="nc lc iq my b gy nh ne l nf ng">#See if all variables have been used and none are left.<br/>    if var_to_split == None:<br/>        Y_options = list(set(Y))<br/>        tot = float(len(Y))<br/>        count = [0 for _ in range(len(Y_options))]</span><span id="7e4b" class="nc lc iq my b gy nh ne l nf ng">for op in range(len(Y_options)):<br/>            for i in range(len(Y)):<br/>                if Y[i] == op:<br/>                    count[op] += 1<br/>        #Format Node label<br/>        Prob = ""<br/>        for op in range(len(Y_options) - 1):<br/>            Prob += "P("<br/>            Prob += str(Y_options[op]) + ")-&gt; "<br/>            P = float(count[op]) / tot<br/>            Prob += "{0:.2f}".format(P)<br/>        #Make a new node<br/>        nodename = node(Prob, color = "orange")<br/>        edge(previous_node, nodename)<br/>    else:<br/>        print("Splitting on {0}".format(var_to_split))<br/>        X_options = list(set(X_dict[var_to_split]))<br/>        #Make decision variable node<br/>        Var_nodename = node(var_to_split, color = "red")<br/>        edge(previous_node, Var_nodename)<br/>        #Init new data for each new branch of the tree<br/>        for X_option in X_options:<br/>            X_nodename = node(str(X_option))<br/>            edge(Var_nodename, X_nodename)<br/>            New_X_dict = {}<br/>            #get remaining variables<br/>            for key in X_dict.keys():<br/>                if key != var_to_split:<br/>                    New_X_dict[key] = []<br/>            New_Y = []<br/>            #Populate<br/>            for i in range(len(Y)):<br/>                if X_dict[var_to_split][i] == X_option:<br/>                    New_Y.append(Y[i])<br/>                    for key in New_X_dict.keys():<br/>                        New_X_dict[key].append(X_dict[key][i])</span><span id="c13d" class="nc lc iq my b gy nh ne l nf ng">#Check if this is a terminal node:<br/>            if len(set(New_Y)) == 1:<br/>                nodename = node(str(New_Y[0]), color = "green")<br/>                edge(X_nodename, nodename)<br/>            else:<br/>                #No terminal node, so try again<br/>                decide(New_Y, New_X_dict, X_nodename)</span><span id="0209" class="nc lc iq my b gy nh ne l nf ng">Y, X_dict =  import_golf('golf.csv') #import data<br/>root_node = node("root", color = "blue") #Create the first node<br/>decide(Y, X_dict, root_node) #start the tree</span></pre><p id="4a0a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于<a class="ae ni" href="https://gerardnico.com/data_mining/weather" rel="noopener ugc nofollow" target="_blank">高尔夫数据集</a>，输出以下树，这是解释决策过程的简单方法。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/56212d5d695d3002c287774527c2f97f.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*2rrBOduhsiAPiUFcriLd5g.png"/></div></figure></div></div>    
</body>
</html>