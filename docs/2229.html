<html>
<head>
<title>Your badass Machine-Learning algorithm is not working as expected?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你那糟糕的机器学习算法没有按预期工作？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/your-badass-machine-learning-algorithm-is-not-working-as-expected-b9dd725d0b6?source=collection_archive---------5-----------------------#2016-09-30">https://towardsdatascience.com/your-badass-machine-learning-algorithm-is-not-working-as-expected-b9dd725d0b6?source=collection_archive---------5-----------------------#2016-09-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/c66ed8c90ca8008c3b28f597f481ab5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GNKLPay8K5My1U4RHWaJ2w.jpeg"/></div></div></figure><div class=""/><p id="a61c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">机器学习以及数据科学和大数据是一个热门词汇，任何与技术相关的人都在工作、大学或至少在互联网上读到过。不仅如此，机器学习在日常生活中如此普遍，以至于许多人在没有意识到的情况下使用它:电子邮件服务中的自动垃圾邮件分类、语音到文本转换、信用卡交易中的欺诈检测、基于偏好和以往购买行为的电子商务推荐。</p><p id="a0e4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在过去的几十年里，许多性感的算法，如人工神经网络，随机森林和支持向量机，已经被提出来承担像线性回归这样更无聊的技术留下的任务。任何机器学习领域之外的人，如果见过深度神经网络在图像识别方面的能力，可能会错误地认为它可以轻松解决简单得多的任务。甚至相当多的人可能已经实现了这些复杂算法中的一个，但结果令人失望。这是因为<em class="kw">在统计学中没有免费的午餐</em>:没有一种方法在所有可能的数据集上支配所有其他方法。为了理解为什么一个算法没有像预期的那样工作，我们必须考虑一些因素，这些因素将在监督学习的回归设置中讨论。</p><p id="ca15" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，任何机器学习问题都是从观察 p 个不同的特征 X 和一个定量的反应 y 开始的，期望它们之间有某种关系。这种关系可以写成非常一般的形式 y=f(X)+e，其中函数 f(X)指定系统信息，量 e 包含也与 y 相关的未考虑的特征或不可测量的变化。也就是说，f(X)是特征的某个固定但未知的函数，E 是独立于所述特征的随机误差项，期望值 E[e]=0。在这种情况下，人们必须求助于一组基于 n 次观测来估计 f(X)的方法。现在，由于误差项平均为零，因此可以使用 y*=f*(X)来预测 y，其中 f*(X)表示 f(X)的估计值。</p><p id="f88a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通常，均方误差 MSE=E[(y-y*) ]用于评估算法在给定数据集上的适当性。一般而言，如果预测值为 y 提供了准确的预测，则 MSE 将会很小，如果其中一些预测与 y 有显著差异，则 MSE 将会很大。由于大多数算法专门估计参数以最小化训练集 MSE，因此将此测量报告为模型对未知数据的预期性能并不是一个好主意。这就是为什么需要测试集的原因，测试集通常是通过分离一些观察直接获得的，或者是通过实现交叉验证等技术间接获得的。</p><p id="3044" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">可以证明，检验 MSE 总是可以分解为 f*(X)的方差、f*(X)的偏差平方和误差项 E 的方差之和，用数学术语来说，MSE = E[(y-y *)]= Var[f(X)]+Bias[f(X)-f *(X)]+Var[E]。在接下来的章节中，我们将从两个角度来深入了解这一重要指标的分解。</p><p id="d226" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">可约与不可约误差</strong></p><p id="cabc" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">y*作为 y 的预测值的准确性取决于两个分量:由 Var[f(X)]+Bias[f(X)-f*(X)]给出的可约误差和由 Var[e]表示的不可约误差。出现可减少的误差是因为 y*的精度可以通过实施更好的算法来估计 f(X)而提高。另一方面，即使有可能得到 f(X)的完美估计，y*仍然是一个近似值，因为 y 也是 e 的函数，根据定义，它不能用 X 来建模。</p><p id="f407" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">偏差-方差权衡</strong></p><p id="5692" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">学习算法通常可以基于它们的灵活性进行比较，即它们产生不同形状来估计 f(X)的能力。这种灵活性在某种程度上直接受模型中参数数量的影响，或者更正式地说，受其自由度的影响。结果表明，随着模型灵活性的增加，训练 MSE 单调下降。相比之下，当允许更大的灵活性时，测试 MSE 最初下降，但在某一点上稳定下来，然后开始增加，显示出典型的 U 形。因此，如果模型比需要的更灵活，预测可能会产生较小的训练 MSE，但会产生较大的测试 MSE，从而为称为过度拟合的现象提供了空间。</p><p id="8e39" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个 U 形是任何算法的两个竞争性属性的结果，称为偏差和方差。当 f*(X)的形状实质上不同于 f(X)的形状时，偏差被引入到预测中，并且因此受到学习算法的选择的影响。同时，当训练观测值的数量与参数的数量相比较低时，会导致方差，使得对这些参数的估计会在来自相同生成过程的训练数据集之间显著变化。一般来说，不灵活的模型往往具有高偏差和低方差，反之亦然，从而导致所谓的偏差-方差权衡。</p><p id="8dbc" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最终，永远记住:<em class="kw">垃圾进来，垃圾出去。</em>重要的是要熟悉手头问题的背景，包括相关特征，通过尝试假设不同形状的不同算法来选择最合适的技术(例如线性与非线性)，并获得足够的观察数据来训练它们。</p><p id="9853" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">参考文献:</strong></p><p id="6e15" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">James，g .，Witten D .，Hastie T .和 Tibshirani，R. (2013 年)。<em class="kw">统计学习入门</em>。纽约:斯普林格。</p></div></div>    
</body>
</html>