<html>
<head>
<title>Normalization in Gradient`s Point of View [ Manual Back Prop in TF ]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">渐变视点的规范化[TF 中的手动背景道具]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/normalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e?source=collection_archive---------5-----------------------#2018-12-22">https://towardsdatascience.com/normalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e?source=collection_archive---------5-----------------------#2018-12-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4cf8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">归一化图层对渐变有什么影响？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/6a5531522c462c6dbd47d370245daba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*gfHAG0LDpAE98Fhm_5q4UA.gif"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">GIF from this <a class="ae kr" href="https://giphy.com/gifs/animation-cats-transformation-3oEjHIOJjuDBQkuwVy" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="9bb4" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">归一化是一种预处理技术，它改变了给定分布的属性。尽管顾名思义，批量标准化执行标准化，并且已经表明标准化层加速了深度神经网络的训练。</p><p id="9535" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">但这是为什么呢？它是如何改变每个重量的梯度的？其他标准化方案有什么影响？例如层规范化以及实例规范化。</p><p id="d34d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">最后，如果归一化的目的是使分布更加对称(或正态)，如果我们执行一个更简单的变换，如 box-cox 变换，会发生什么？</p><p id="5206" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">下面是我们将要比较的方法列表。</p><p id="30ee" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir"> <em class="lo"> a)正常 CNN(基线)<br/> b) </em> </strong> <a class="ae kr" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ku ir"> <em class="lo">批量归一化</em></strong></a><strong class="ku ir"><em class="lo"><br/>c)</em></strong><a class="ae kr" href="https://arxiv.org/abs/1607.06450" rel="noopener ugc nofollow" target="_blank"><strong class="ku ir"><em class="lo">图层归一化</em></strong></a><strong class="ku ir"><em class="lo"><br/>d)</em></strong><a class="ae kr" href="https://arxiv.org/pdf/1607.08022.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ku ir"><em class="lo">实例归一化</em> </strong> </a> <strong class="ku ir"> <em class="lo"/></strong></p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="d07d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">简介</strong></p><p id="19de" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">总的来说，我把规范化理解为一种改变给定分布的属性的技术/过程。例如，标准化的最基本用法是将分布范围限制在某个范围内。在计算机视觉中，我们经常看到很多研究人员在 0 到 1 的范围内归一化像素强度。</p><p id="e36c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">然而，在深度学习中，这个术语的确切用法有所不同，具体来说，批处理规范化执行标准化，其中将分布居中为零，并将标准偏差更改为一。这个过程也被称为将不同的数据放入同一尺度的过程。</p><p id="b71d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">尽管它们被广泛使用，但我无法发现这些标准化对梯度有什么影响，所以我想修复它。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="f0b7" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">实验设置</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lw"><img src="../Images/eb82bcd0a28dc9e8ea5836622749fc69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QY9bgj3Nac-ioP0_iGh3rw.png"/></div></div></figure><p id="49cf" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">蓝色矩形</strong> →带 ReLU 激活的卷积层<br/> <strong class="ku ir">黄色矩形</strong> →归一化层，根据方案该层可以在批处理/层/实例和 box-cox 变换之间变化。在我们没有任何规范化方案的情况下，这一层不存在。<br/> <strong class="ku ir">红色矩形</strong> →最终输出矢量，应用 softmax 操作。<br/> <strong class="ku ir">灰色球体</strong> →输入图像尺寸(20，96，96，3)。</p><p id="4795" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">每个单个网络被训练 150 个历元，学习速率被设置为 0.0008，最小批量大小为 20。最后，我将使用<a class="ae kr" href="https://cs.stanford.edu/~acoates/stl10/" rel="noopener ugc nofollow" target="_blank"> STL 10 图像数据集</a>，没有任何数据扩充，请注意，这意味着我们有 5000 张训练图像，而测试精度是在 8000 张测试图像上测量的。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="05c7" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">结果位移</strong></p><p id="ba64" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在展示结果之前，我想介绍一下我是如何放置每个实验的结果的。每个图像包含六个图，并且每个图以连续的方式表示每个层。(按如下所示的顺序。).</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/9a24cc24b1446d22c02a952fa47e7f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*4rei9IKBl9X1V7ck29k_wg.png"/></div></figure><p id="dc6b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">所以从顶部开始，每个方框代表该层的值如何随时间变化。我将按照下面的顺序展示其中的四张图片。</p><p id="dc96" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">左上</strong> →表示相对于每一层权重的梯度<br/> <strong class="ku ir">右上</strong> →表示传递到前一层的梯度<br/> <strong class="ku ir">左下</strong> →表示每一层的权重<br/> <strong class="ku ir">右下</strong> →表示当前权重和计算的梯度之间的差</p><p id="06e6" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">最后，在显示 gif 之后，我将显示最终迭代结果的静态图像。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="d496" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">无任何归一化</strong></p><div class="kg kh ki kj gt ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/7c2d2829a2ea72957b14c23ecba2f595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*I0X-4_R0JFDUbXNd2hMYnQ.gif"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/454a56a7d9ed7df5b7d16cd2f529912e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*oA_ZwlCX3ZfP9imUyolqwA.gif"/></div></figure></div><div class="ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/914ea91c72f0e89d72bdc1ac2e8fc02e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*e7iEvkV0mVNBtd_AXfQyQg.gif"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/4059175718d6efa6b966daad24f7141d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*KQQMC9jjOtdmyiO8Md-eEA.gif"/></div></figure></div><p id="2572" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">左上</strong> →相对于每层权重的梯度<br/> <strong class="ku ir">右上</strong> →传递到上一层的梯度<br/> <strong class="ku ir">左下</strong> →每层权重<br/> <strong class="ku ir">右下</strong> →当前权重与计算的梯度相减</p><p id="4083" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">当我们把注意力集中到左上角的 gif 时，我们可以看到大部分的渐变都集中在零上。这表明大多数权重没有被更新。有很多零梯度的效果可以在位于左下方的 gif 中看到。简而言之，在几次迭代之后，权重变化不大，尤其是第二层、第三层和第四层。</p><div class="kg kh ki kj gt ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/e4b14740a32780c040fdc0aaaf857687.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*9b3x7HBowXXkxtfZIeIRyQ.png"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/aa7e82a7891ca310fc4ebb9a778d3206.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ef2fV0h4qDrWwXRIMMVTIA.png"/></div></figure></div><div class="ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/c2635754bb3c054bd86f1850f9b07501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Ji8n_WAhmVf9mbntiNu62A.png"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/fb258655382248188431327680945ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*QvZX2ecCyYR0PLytgCpowA.png"/></div></figure></div></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="b21f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">批量归一化</strong></p><div class="kg kh ki kj gt ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/86cf147b31ff461100bb7dbe8e80f099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*1HNT2c2bAu37RgxNCCxFZw.gif"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/71b5b2d6df14f4c2ef1df9d7f5adca7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*Qqh0AanP6sii2AsS_a_uIQ.gif"/></div></figure></div><div class="ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/4df4ba8069b614898b1269471fc78cef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*0l-Xd0IaFnV2Q4tcBh60eg.gif"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/ed7509b33c7460f3ab698ce901a790fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*IcgLoWeTgXJm_oQc7-oXyQ.gif"/></div></figure></div><p id="6dd0" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">左上</strong> →相对于各层权重的梯度<br/> <strong class="ku ir">右上</strong> →传递到前几层的梯度<br/> <strong class="ku ir">左下</strong> →各层权重<br/> <strong class="ku ir">右下</strong> →当前权重与计算的梯度相减</p><p id="9931" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们可以立即看到没有任何归一化方案的网络之间的一个显著差异，每层的非零梯度数量<strong class="ku ir">。</strong></p><p id="c47a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这种增加非零梯度数量的批量标准化属性是深度神经网络训练加速的原因。当我们观察权重如何随时间变化时，我们可以看到直方图有更多的整体移动。</p><div class="kg kh ki kj gt ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/65adb3555550fbeca8b793237796a5da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Ihz4u1H3mwToT0lil9ErVQ.png"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/43350b7b6476c33d38e908fd63875f5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*67edrt1vVTFPxXFKc33yAw.png"/></div></figure></div><div class="ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/4deefd4daf15212bcf8b5e19796d0691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*de6RyXAbmcn4d74ZZSTysw.png"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/b7cd885fe3267225482ce0df73be39fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*bOtAlJvKP9z0qF9Ts62hkg.png"/></div></figure></div></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="a0c3" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">图层归一化</strong></p><div class="kg kh ki kj gt ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/b3e6b6bb68ddf2213734e3da1a6ae618.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*HYj8XEdmXbwJqldr9JhZUA.gif"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/0e7ad029362eb0b0a18c7c0f6f288b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*6tQuSfybAbaMI7GsOspLfA.gif"/></div></figure></div><div class="ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/7550a611bb0c98f5f6287edf3da8160b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*bxBar8LwkywVhdW6oH3OqQ.gif"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/4bbe149f94320853767a54be9f6af448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*0DGTADjgOHz3Cy8vnDi37w.gif"/></div></figure></div><p id="ce0a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">左上</strong> →相对于每层权重的梯度<br/> <strong class="ku ir">右上</strong> →传递到上一层的梯度<br/> <strong class="ku ir">左下</strong> →每层权重<br/> <strong class="ku ir">右下</strong> →当前权重与计算的梯度相减</p><p id="0692" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">当我们在每层之间使用层标准化时，我们可以观察到类似的现象。随着非零梯度数量的增加，每层权重的更新变得更加频繁。</p><div class="kg kh ki kj gt ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/d24ff816d926c89929a7bd543b164b6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*DHeTFtbxkr2FMdr3ELNRqw.png"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/2d91a62b7c7078db1233c7ddf90aa6d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*rde4QSG1_nzuD7gYIXb5iw.png"/></div></figure></div><div class="ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/3497a05dc554a76c8bfedd483f198f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*I8ZiN5ZMhOk9CCPFPUa-HQ.png"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/cb60448e91b61c348d319fa0cc5c79cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*2SclUxK0ERHgu6DYrNqazA.png"/></div></figure></div></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="58a6" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">实例规范化</strong></p><div class="kg kh ki kj gt ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/145ad5cf202e9f235b74ba214039e126.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*XIvfVn-9Lb21VVJtSSEIcw.gif"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/998abaeebe430e7b27d07f3a791358b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*iZqXkLi3M_s_FoT9gJtMtw.gif"/></div></figure></div><div class="ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/4f1ceee83b2ab67fead48a95cc8c59b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*oYh0bqiJE10nd-Ghdk4ibQ.gif"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/bcb139010276f2c9ec8b95bd9d867393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*aWR-3uyGSuxSbMkVs3TBUg.gif"/></div></figure></div><p id="3ec8" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">左上</strong> →相对于每层权重的梯度<br/> <strong class="ku ir">右上</strong> →传递到上一层的梯度<br/> <strong class="ku ir">左下</strong> →每层权重<br/> <strong class="ku ir">右下</strong> →当前权重与计算的梯度相减</p><p id="9325" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">实例归一化标准化了每个图像或特征图，因此，我个人认为与其他归一化方案相比，非零梯度的数量是最大的。</p><div class="kg kh ki kj gt ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/2d7138dd697848ad610ab269434e0d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*hgtpdGybGVxIk0K0WbfaQw.png"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/2fae3320d3c2550c342047b6634be307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*45KItApyDSlHvpNCNSaqiA.png"/></div></figure></div><div class="ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/611f3511f3433f961c1fc952f4760878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*J4hiu5d0DhRNNLMg5TYQ5Q.png"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/30cd738aa7906f5639a931bfe41ca91b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*qFbMn5nCb9v4edbJA3mS3g.png"/></div></figure></div></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="e058" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir"> Box-Cox 变换</strong></p><div class="kg kh ki kj gt ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/d7b7f01b3031cb3bf70d5e13cdef9708.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*3dqHT6SVBs9nPdWU_YOMVg.gif"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/42df42828eee9acbdb848cafe2db211b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*qtfCJZAs04ALKVfeb9g2vw.gif"/></div></figure></div><div class="ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/3a45969a9339ead880023a5645c42d18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*g7NeDOA-mp37Dre-yGj17g.gif"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/3680ebd2fb8728e6303c59bb60a35d02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*MRyBwelSKKRa2QhJtmD-IA.gif"/></div></figure></div><p id="f661" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">左上</strong> →相对于每层权重的梯度<br/> <strong class="ku ir">右上</strong> →传递到上一层的梯度<br/> <strong class="ku ir">左下</strong> →每层权重<br/> <strong class="ku ir">右下</strong> →当前权重与计算的梯度相减</p><p id="7ff3" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">当与不具有任何归一化方案的网络相比时，我们可以看到在关于每个权重的梯度中有更多的非零元素。然而，与任何其他归一化方案相比，我们可以看到我们的梯度中仍然有许多零。</p><div class="kg kh ki kj gt ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/a3e4b227de9aeb9d55a0adb19a71b213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ClGKIBB7s6CKXCB8vVxSGA.png"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/e42503aa2b3e2d5b5d204189860699f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*kvEDvPG1wRxctQS1Gl5rVw.png"/></div></figure></div><div class="ab cb"><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/8fb7b0784a429d638d1b6bc8c1984e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*TJnAg6kQafzCTQvgKMaVfA.png"/></div></figure><figure class="mc kk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/4d5cdfba7578f2e92f64d0661bac9fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Tj-tl4MJV5RouSmcTkwnnw.png"/></div></figure></div></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="9f92" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">讨论</strong></p><p id="5275" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">需要记住的一件非常重要的事情是，这些网络(有/没有)归一化方案中的每一个都有完全相同数量的参数。</p><blockquote class="mi mj mk"><p id="380e" class="ks kt lo ku b kv kw jr kx ky kz ju la ml lc ld le mm lg lh li mn lk ll lm ln ij bi translated">这意味着他们的学习能力完全相同。</p></blockquote><p id="a874" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这样做的原因是因为我没有向批处理/层/实例标准化添加任何 alpha 或 beta 参数，所以通过每一层传递的所有数据都必须标准化。了解了这一点，我们就可以看到精度图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/9a8d62e6df9af3bcfe5c4674410ec0c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*5J-fAtGYJkl4fGWHj79HCA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Accuracy for training images</figcaption></figure><p id="c43a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">橙色</strong> →批量归一化<br/> <strong class="ku ir">红色</strong> →实例归一化<br/> <strong class="ku ir">绿色</strong> →图层归一化<br/> <strong class="ku ir">紫色</strong> → Box-Cox 变换<br/> <strong class="ku ir">蓝色</strong> →无归一化</p><p id="f02a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">当我们使用诸如批次/层/实例归一化的归一化方案时，我们可以在第 130 个时期之前在训练图像上实现+95%的准确度。同时，具有 box-cox 变换的网络以及没有任何归一化方案的网络甚至难以通过+60%的准确度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/78c3c50a3d35546e7411f6800efcfe11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*5RUCjmYO-Tf406JVvBDQFw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Accuracy for Testing Images</figcaption></figure><p id="1588" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">橙色</strong> →批量归一化<br/> <strong class="ku ir">红色</strong> →实例归一化<br/> <strong class="ku ir">绿色</strong> →图层归一化<br/> <strong class="ku ir">紫色</strong> → Box-Cox 变换<br/> <strong class="ku ir">蓝色</strong> →无归一化</p><p id="9dd8" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">从上面的图中我们可以得出结论，令人惊讶的是，没有任何归一化方案的网络做得最好。当考虑到我们有比训练图像多得多的测试图像时，这多少是一个令人印象深刻的结果。STL 10 数据集有 5000 个训练图像和 8000 个测试图像，因此 8000 个图像的 55%将意味着 4400 个图像。</p><p id="da87" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">此外，我们可以看到一种模式的出现，随着我们计算平均值和标准偏差的参数数量的减少，测试精度增加。更准确地说，当我们有一个维数为(20，96，96，16)的 4D 张量时，其中每个轴代表(批次大小、宽度、高度、通道)，当我们执行批次归一化时，我们计算通道维数的平均值，即 16。同时，层和实例标准化分别计算批次大小以及批次大小和通道尺寸的平均值。</p><p id="89c0" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">知道了上面所有的信息，我们可以看到相反的模式明显存在。随着参数数量的增加，我们计算的平均值和标准偏差增加，模型趋于过度拟合。</p><p id="c2fc" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">为什么？</p><p id="9e80" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">好吧，我不是机器学习方面的专家，但清晰的推测是梯度。更准确地说，非零梯度。事后看来，这非常有意义，如果关于权重的梯度由许多零元素组成，那么权重在更新操作之后不会改变。</p><p id="310d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">但我认为，还有另一个原因，更数学的原因。(注意我也不是数学专家 lol。).</p><blockquote class="mi mj mk"><p id="4088" class="ks kt lo ku b kv kw jr kx ky kz ju la ml lc ld le mm lg lh li mn lk ll lm ln ij bi translated">限制分布特性的归一化方案通过减少问题维度空间来简化问题。</p></blockquote><p id="f61d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我的意思是，减少内部协变量变化本质上是一样的。在批次/层/实例标准化中，我们将平均值和标准偏差限制为 0 和 1。并且这些操作有一个硬结果，意味着平均值<strong class="ku ir">是零，</strong>和标准差<strong class="ku ir">是一</strong>。</p><p id="fa89" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这种变换的一个软化版本是 box-cox 变换，其运算目的是使某个分布更加“正常”或“对称”。</p><p id="c2ec" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这两种操作都对特征图的分布施加了一定的限制，使得网络更容易在训练图像上做得很好。然而，这并不意味着它会在测试图像上表现良好。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="b455" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">这篇博文的结论和代码</strong></p><p id="0e53" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">归一化方案通过增加非零梯度的频率来加速训练，然而，在某些情况下，分布变化的这种限制导致网络失去泛化能力，从而导致过拟合。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mq"><img src="../Images/85268e703395b67ad5a026c832016fe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KIbZ94EzpylC_WLL-tgdVg.png"/></div></div></figure><p id="197d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">要访问 google collab 中这篇博客文章的代码，请<a class="ae kr" href="https://colab.research.google.com/drive/1gDx0kwnbBFcdghA34h-sKUiPFZnXQGpA" rel="noopener ugc nofollow" target="_blank">点击此处</a>，要访问 GitHub 版本，请<a class="ae kr" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/Class%20Stuff/Normalization%20in%20Gradient%20Point%20of%20View/blog/a%20blog.ipynb" rel="noopener ugc nofollow" target="_blank">点击此处。</a></p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="38f0" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">最后的话</strong></p><p id="dac8" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">一般来说，随着可训练参数数量的增加，模型越有可能过度拟合。(没有任何正规化，技巧。).该研究的未来方向可以进一步分为两个方向，1)当我们添加正则化参数时，网络将如何表现。2)诸如 box-cox 之类的变换方法对于对手攻击是否更健壮。</p><p id="4952" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">更多文章请访问我的<a class="ae kr" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">网站</a>。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="01ef" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">参考</strong></p><ol class=""><li id="624c" class="mr ms iq ku b kv kw ky kz lb mt lf mu lj mv ln mw mx my mz bi translated">瓦尔马河(2018 年)。罗汉·瓦尔马—📋我的博客；通过机器学习教程、操作指南、论文评论等等。Rohanvarma.me 于 2018 年 12 月 16 日检索，来自<a class="ae kr" href="http://rohanvarma.me/" rel="noopener ugc nofollow" target="_blank">http://rohanvarma.me/</a></li><li id="c3a9" class="mr ms iq ku b kv na ky nb lb nc lf nd lj ne ln mw mx my mz bi translated">rohan-varma/nn-init-demo。(2018).GitHub。检索于 2018 年 12 月 16 日，来自<a class="ae kr" href="https://github.com/rohan-varma/nn-init-demo/blob/master/batchnorm-demo.py" rel="noopener ugc nofollow" target="_blank">https://github . com/rohan-varma/nn-init-demo/blob/master/batch norm-demo . py</a></li><li id="665e" class="mr ms iq ku b kv na ky nb lb nc lf nd lj ne ln mw mx my mz bi translated">在神经网络中实现 BatchNorm。(2018).Wiseodd.github.io .检索 2018 年 12 月 16 日，来自<a class="ae kr" href="https://wiseodd.github.io/techblog/2016/07/04/batchnorm/" rel="noopener ugc nofollow" target="_blank">https://wiseodd.github.io/techblog/2016/07/04/batchnorm/</a></li><li id="ec3f" class="mr ms iq ku b kv na ky nb lb nc lf nd lj ne ln mw mx my mz bi translated">(2018).Arxiv.org。检索于 2018 年 12 月 21 日，来自 https://arxiv.org/pdf/1502.03167.pdf<a class="ae kr" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank"/></li><li id="c6b2" class="mr ms iq ku b kv na ky nb lb nc lf nd lj ne ln mw mx my mz bi translated">Ba，j .，Kiros，j .，&amp; Hinton，G. (2016)。图层规范化。arXiv.org。检索于 2018 年 12 月 21 日，来自 https://arxiv.org/abs/1607.06450<a class="ae kr" href="https://arxiv.org/abs/1607.06450" rel="noopener ugc nofollow" target="_blank"/></li><li id="7682" class="mr ms iq ku b kv na ky nb lb nc lf nd lj ne ln mw mx my mz bi translated">(2018).Arxiv.org。检索于 2018 年 12 月 21 日，来自<a class="ae kr" href="https://arxiv.org/pdf/1607.08022.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1607.08022.pdf</a></li><li id="fad7" class="mr ms iq ku b kv na ky nb lb nc lf nd lj ne ln mw mx my mz bi translated">SciPy . special . boxcox 1p—SciPy v 1 . 2 . 0 参考指南。(2018).Docs.scipy.org。检索于 2018 年 12 月 21 日，来自<a class="ae kr" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.boxcox1p.html#scipy.special.boxcox1p" rel="noopener ugc nofollow" target="_blank">https://docs . scipy . org/doc/scipy/reference/generated/scipy . special . boxcox 1p . html # scipy . special . boxcox 1p</a></li><li id="87b9" class="mr ms iq ku b kv na ky nb lb nc lf nd lj ne ln mw mx my mz bi translated">STL-10 数据集。(2018).Cs.stanford.edu。检索于 2018 年 12 月 22 日，来自<a class="ae kr" href="https://cs.stanford.edu/~acoates/stl10/" rel="noopener ugc nofollow" target="_blank">https://cs.stanford.edu/~acoates/stl10/</a></li></ol></div></div>    
</body>
</html>