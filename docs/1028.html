<html>
<head>
<title>An Introduction to different Types of Convolutions in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中不同类型卷积的介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d?source=collection_archive---------0-----------------------#2017-07-22">https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d?source=collection_archive---------0-----------------------#2017-07-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/74992078e3d4a7422585158e4c5e5631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ewkjy5JF7pBJpb0gdhRQlw.jpeg"/></div></div></figure><p id="2901" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我给你一个快速概述不同类型的卷积和他们的好处。为了简单起见，我只关注2D卷积。</p><h1 id="4582" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">回旋</h1><p id="58ed" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">首先，我们需要就定义卷积层的几个参数达成一致。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/17328b5f27b712d981768d9482c127ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/1*1okwhewf5KCtIPaFib4XaA.gif"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">2D convolution using a kernel size of 3, stride of 1 and padding</figcaption></figure><ul class=""><li id="4b11" class="mi mj iq ka b kb kc kf kg kj mk kn ml kr mm kv mn mo mp mq bi translated"><strong class="ka ir">内核大小</strong>:内核大小定义卷积的视野。2D的常见选择是3，即3x3像素。</li><li id="6688" class="mi mj iq ka b kb mr kf ms kj mt kn mu kr mv kv mn mo mp mq bi translated"><strong class="ka ir">步幅</strong>:步幅定义了内核遍历图像时的步长。虽然其默认值通常为1，但我们可以使用步幅2对图像进行缩减采样，类似于MaxPooling。</li><li id="9db4" class="mi mj iq ka b kb mr kf ms kj mt kn mu kr mv kv mn mo mp mq bi translated"><strong class="ka ir">填充</strong>:填充定义如何处理样本的边框。(一半)填充卷积将保持空间输出维度等于输入维度，而如果内核大于1，未填充卷积将裁剪掉一些边界。</li><li id="011f" class="mi mj iq ka b kb mr kf ms kj mt kn mu kr mv kv mn mo mp mq bi translated"><strong class="ka ir">输入&amp;输出通道</strong>:卷积层取一定数量的输入通道(I)，计算出特定数量的输出通道(O)。这种层所需的参数可以通过I*O*K来计算，其中K等于内核中值的数量。</li></ul><h1 id="b250" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">扩张的回旋</h1><p id="2a64" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">(又名atrous卷积)</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/4018c56491b3570946f2d70e4c487da7.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/1*SVkgHoFoiMZkjy54zM_SUw.gif"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">2D convolution using a 3 kernel with a dilation rate of 2 and no padding</figcaption></figure><p id="fa98" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">膨胀卷积为卷积层引入了另一个参数，称为<strong class="ka ir">膨胀率</strong>。这定义了内核中值之间的间距。膨胀率为2的3×3内核将具有与5×5内核相同的视野，同时仅使用9个参数。想象一下，取一个5x5的内核，每隔一行删除一列。</p><p id="49bf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这以相同的计算成本提供了更宽的视野。膨胀卷积在实时分割领域特别流行。如果你需要一个宽广的视野并且不能负担多重卷积或者更大的内核，使用它们。</p><h1 id="5b9f" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">转置卷积</h1><p id="06ac" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">(又名去卷积或分数步长卷积)</p><p id="45f9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有些资料来源使用反卷积这个名称，这是不恰当的，因为它不是反卷积。更糟糕的是，反进化确实存在，但它们在深度学习领域并不常见。实际的反卷积反转了卷积的过程。设想将一幅图像输入到一个卷积层中。现在拿出输出，把它扔进一个黑盒子里，你的原始图像又出来了。这个黑盒做反卷积。这是卷积层的数学逆运算。</p><p id="b72a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">转置卷积有些类似，因为它产生的空间分辨率与假设的反卷积图层相同。然而，对这些值执行的实际数学运算是不同的。转置卷积层执行常规卷积，但恢复其空间变换。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/736539d77018b425e62c49305912fd11.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/1*BMngs93_rm2_BpJFH2mS0Q.gif"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">2D convolution with no padding, stride of 2 and kernel of 3</figcaption></figure><p id="95fc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这一点上你应该很困惑，所以让我们看一个具体的例子。一个5x5的图像被送入卷积层。步幅被设置为2，填充被停用，并且内核是3x3。这会产生一个2x2的图像。</p><p id="aa88" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果我们想逆转这个过程，我们需要逆向数学运算，这样我们输入的每个像素就产生9个值。之后，我们以步长2遍历输出图像。这将是一个反卷积。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/ba33442d132d573646619622d31bb939.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/1*Lpn4nag_KRMfGkx1k6bV-g.gif"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Transposed 2D convolution with no padding, stride of 2 and kernel of 3</figcaption></figure><p id="37cf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">转置卷积不会这样做。唯一的共同点是，它保证输出也将是5x5图像，同时仍执行正常的卷积运算。为了实现这一点，我们需要对输入进行一些花哨的填充。</p><p id="679d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">正如你现在所能想象的，这一步不会逆转上面的过程。至少不考虑数值。</p><p id="2aac" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">它仅仅从以前重建空间分辨率并执行卷积。这可能不是数学上的逆运算，但对于编码器-解码器架构来说，它仍然非常有用。这样，我们可以将图像的放大与卷积结合起来，而不是进行两个独立的过程。</p><h1 id="6f51" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">可分卷积</h1><p id="b4d1" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">在可分离卷积中，我们可以将核操作分成多个步骤。让我们将一个卷积表示为<strong class="ka ir"> y = conv(x，k) </strong>其中<strong class="ka ir"> y </strong>为输出图像，<strong class="ka ir"> x </strong>为输入图像，<strong class="ka ir"> k </strong>为内核。简单。接下来，我们假设k可以通过:<strong class="ka ir"> k = k1.dot(k2) </strong>来计算。这将使它成为一个可分离的卷积，因为我们可以通过对k1和k2进行2次1D卷积来得到相同的结果，而不是对k进行2D卷积。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/26ca718656ac3d5264329867031f93d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*owXMr9DonUUWP1c2Thg_Dw.png"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Sobel X and Y filters</figcaption></figure><p id="d8d4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以图像处理中常用的Sobel核为例。通过将vector [1，0，-1]和[1，2，1].T相乘，可以得到相同的内核。在执行相同的操作时，这将需要6个而不是9个参数。上面的例子显示了所谓的<strong class="ka ir">空间可分卷积</strong>，据我所知，它没有用于深度学习。</p><p id="5509" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="my">编辑:实际上，人们可以通过堆叠1xN和Nx1内核层来创建非常类似于空间可分离卷积的东西。这是最近在一个叫做</em><a class="ae mz" href="https://arxiv.org/abs/1801.06434v1" rel="noopener ugc nofollow" target="_blank"><em class="my">EffNet</em></a><em class="my">的架构中使用的，显示出有希望的结果。</em></p><p id="5182" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在神经网络中，我们通常使用一种叫做<strong class="ka ir">深度方向可分离卷积的东西。</strong>这将在保持通道分离的同时执行空间卷积，然后执行深度卷积。在我看来，用一个例子最能理解。</p><p id="9600" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">假设我们在16个输入通道和32个输出通道上有一个3×3卷积层。具体发生的情况是，32个3×3内核遍历16个通道中的每一个，从而产生512(16×32)个特征图。接下来，我们通过将每个输入通道相加来合并1个特征图。因为我们可以这样做32次，我们得到了我们想要的32个输出通道。</p><p id="e144" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于同一个例子中的深度方向可分离卷积，我们遍历16个通道，每个通道具有1个3×3内核，给我们16个特征图。现在，在合并任何东西之前，我们用32个1x1卷积遍历这16个特征图，然后开始将它们加在一起。这导致656 (16x3x3 + 16x32x1x1)个参数，而不是上面的4608 (16x32x3x3)个参数。</p><p id="f046" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该示例是深度方向可分离卷积的具体实现，其中所谓的<strong class="ka ir">深度乘数</strong>是1。这是迄今为止此类层最常见的设置。</p><p id="3ebc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们这样做是因为假设空间和深度信息可以分离。从例外模型的表现来看，这个理论似乎是可行的。深度方向可分离卷积也用于移动设备，因为它们有效地使用参数。</p><h1 id="9d88" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">有问题吗？</h1><p id="05df" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">这就结束了我们通过不同类型的回旋的小旅行。我希望这有助于对这件事有一个简要的了解。如果你还有任何问题，请发表评论，并查看<a class="ae mz" href="https://github.com/vdumoulin/conv_arithmetic" rel="noopener ugc nofollow" target="_blank">GitHub页面获取更多卷积动画。</a></p></div></div>    
</body>
</html>