<html>
<head>
<title>Machine Learning: Ridge Regression in Detail</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:岭回归详解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-ridge-regression-in-detail-76787a2f8e2d?source=collection_archive---------9-----------------------#2018-11-09">https://towardsdatascience.com/machine-learning-ridge-regression-in-detail-76787a2f8e2d?source=collection_archive---------9-----------------------#2018-11-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/012d89009eb77fc40ba14401fbaffdf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YnZoKt0bnAo1JxPH3GOPIw.png"/></div></div></figure><div class=""/><p id="3a89" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因为这里已经有足够多的关于线性回归的文章了，我就不再写了。相反，我将写一种标准化的回归类型——岭回归——它解决了数据过度拟合的问题。</p><h2 id="3a0b" class="kw kx jb bd ky kz la dn lb lc ld dp le kj lf lg lh kn li lj lk kr ll lm ln lo bi translated"><strong class="ak">岭回归的动机</strong></h2><p id="8fa9" class="pw-post-body-paragraph jy jz jb ka b kb lp kd ke kf lq kh ki kj lr kl km kn ls kp kq kr lt kt ku kv ij bi translated">线性回归模型由以下等式给出:</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="4628" class="kw kx jb lz b gy md me l mf mg"><strong class="lz jc">Y = Σ WⱼHⱼ(Xᵢ)</strong></span><span id="3961" class="kw kx jb lz b gy mh me l mf mg">Here, <br/><strong class="lz jc">Σ</strong> runs from <em class="mi">j = 0 </em>to <em class="mi">j = D </em>where D is the total number of features.<br/><strong class="lz jc">Wⱼ </strong>is the <em class="mi">jᵗʰ</em> coefficient <br/><strong class="lz jc">Hⱼ </strong>is the <em class="mi">jᵗʰ </em>feature function which takes <strong class="lz jc">Xᵢ </strong>observation<br/><strong class="lz jc">Xᵢ </strong>is the <em class="mi">iᵗʰ </em>observation</span></pre><p id="1d10" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">假设我们知道<strong class="ka jc"> W </strong>系数的值，上面的等式给出了预测值。为了简化，让我们用𝔽( <strong class="ka jc"> X </strong>来表示上面的方程，其中 x 是观测值。</p><p id="064a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">线性回归模型的成本函数由以下等式给出:</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="4556" class="kw kx jb lz b gy md me l mf mg">Cost Function = <strong class="lz jc">RSS(W) = Σ [Yᵢ — 𝔽(Xᵢ)]²</strong><br/>Here,<br/><strong class="lz jc">Σ </strong>runs from <em class="mi">i=0 </em>to <em class="mi">i = N </em>where <em class="mi">N </em>is the total number of observations.<br/><strong class="lz jc">Yᵢ </strong>is the known value of <em class="mi">iᵗʰ </em>observation.<br/><strong class="lz jc">𝔽(Xᵢ) </strong>gives the predicted value of <em class="mi">iᵗʰ </em>observation.</span><span id="1ee0" class="kw kx jb lz b gy mh me l mf mg">RSS stands for Residual Sum of Squares</span></pre><p id="d3bb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">代价函数总是作用于训练数据集。</p><p id="c4fe" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">线性回归模型的整个思想围绕着最小化上述成本函数值。成本函数值越低，线性回归模型越好。</p><p id="1572" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通常，为了降低成本函数，我们增加模型中的特征数量。随着我们不断增加模型中的特征，模型开始很好地拟合训练数据集，并且成本函数值开始降低。</p><p id="1756" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是，随着特征数量的增加；我们的方程变成了一个高阶多项式方程；这导致了数据的过度拟合。</p><p id="70e5" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">为什么数据过度拟合不好？<br/> </strong>在过度拟合的模型中，训练误差几乎为零，这意味着模型在训练数据集上运行良好。但是，除了像真实的外部世界数据那样的训练数据集之外，这个模型在其他数据集上也能很好地工作吗？<br/>通常可以看出，过度拟合的模型在测试数据集上表现较差，并且还观察到过度拟合的模型在额外的新测试数据集上表现较差。</p><figure class="lu lv lw lx gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mj"><img src="../Images/341a7e83d64d6f21deff3f24f27b1f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IlQA-r3evI2M5BlHU0TaMg.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">Overfitted data &amp; performing worse on test data set. <a class="ae mo" href="https://www.researchgate.net/figure/A-sample-model-of-over-fitting_fig2_267840187" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="6534" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从上面的图表中，我们可以看到过拟合模型在训练数据集上表现良好，并且训练数据集的成本函数为零。</p><p id="8e5a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是当我们用上图中的测试数据集测试这个模型时，模型的表现一点也不好。对于测试数据，模型预测错误值与实际正确值相差甚远。这足以说明这种型号不适合在工业上使用。</p><p id="30cb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">如何抓过拟合？<br/> </strong>通过可视化模型(如上)，可以很容易地看到模型中的过度拟合(观察模型如何很好地拟合训练数据集)。但是，随着我们的模型的复杂性增加，它进入了更高维度，这使得它很难在图表(或其他工具)上可视化。</p><p id="c034" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">除了总是试图可视化模型，我们还可以通过查看系数的值(<strong class="ka jc"> W </strong>)来查看过拟合。通常当过拟合发生时，这些系数的值变得非常大。</p><p id="de10" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">岭回归用于通过测量系数的大小来量化数据的过度拟合。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><p id="4171" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要修复过度拟合的问题，我们需要平衡两件事:<br/> 1。函数/模型符合数据的程度。<br/> 2。系数的大小。</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="1b71" class="kw kx jb lz b gy md me l mf mg">So,<br/>Total Cost Function = Measure of fit of model + Measure of magnitude                    of coefficient</span><span id="b406" class="kw kx jb lz b gy mh me l mf mg">Here,<br/>Measure of fit of model = RSS(W)<br/>Measure of magnitude of coefficient = ||W||²</span><span id="dad2" class="kw kx jb lz b gy mh me l mf mg">If Measure of fit of the model is a small value that means model is well fit to the data.<br/>If Measure of magnitude of coefficient is a small value that means model is not overfit.</span><span id="cd09" class="kw kx jb lz b gy mh me l mf mg"><strong class="lz jc">Total Cost Function = RSS(W) + λ*||W||²</strong></span><span id="477e" class="kw kx jb lz b gy mh me l mf mg">We have added <strong class="lz jc">λ </strong>in total cost function as a tuning parameter to balance the fit of data and magnitude of coefficients.</span></pre><h2 id="8643" class="kw kx jb bd ky kz la dn lb lc ld dp le kj lf lg lh kn li lj lk kr ll lm ln lo bi translated">计算岭回归的梯度下降</h2><p id="8bfa" class="pw-post-body-paragraph jy jz jb ka b kb lp kd ke kf lq kh ki kj lr kl km kn ls kp kq kr lt kt ku kv ij bi translated">岭回归成本= RSS(W)+λ* | | W | | =(Y-WH)*(Y-WH)+WW</p><p id="0f0c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在矩阵符号中，它将被写成:<br/>岭回归成本= (Y - HW)ᵗ (Y - HW) + WᵗW</p><p id="79d9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">取上述方程的梯度(微分):</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="3f8f" class="kw kx jb lz b gy md me l mf mg">Δ[RSS(W) + λ||W||]²<br/>= Δ{(Y - HW)ᵗ(Y - HW)} + λ Δ{WᵗW}</span><span id="48d5" class="kw kx jb lz b gy mh me l mf mg">= -2Hᵗ(Y - HW)+2λW</span></pre><p id="de19" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">设置上面的梯度为 0，我们得到</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="7635" class="kw kx jb lz b gy md me l mf mg"><strong class="lz jc">W = (HᵗH + λI)-¹HᵗY</strong></span></pre><p id="9c98" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，我们知道了<strong class="ka jc"> W </strong>系数的值。</p><h2 id="be96" class="kw kx jb bd ky kz la dn lb lc ld dp le kj lf lg lh kn li lj lk kr ll lm ln lo bi translated">λ值怎么选？</h2><p id="f6bd" class="pw-post-body-paragraph jy jz jb ka b kb lp kd ke kf lq kh ki kj lr kl km kn ls kp kq kr lt kt ku kv ij bi translated">给定的数据集分为三组:<br/> 1。训练装置<br/> 2。验证集<br/> 3。测试装置</p><figure class="lu lv lw lx gt is gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/3bcb440dd2f025802ded3518180a812e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*nE5cCxhevkMTCPusJoU-jw.png"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk"><em class="mx">Division of data into three different sets</em></figcaption></figure><p id="7682" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">训练集<br/> </strong>该数据集将用于获取λ的每个值的<strong class="ka jc"> W </strong>系数的值。让我们假设λ值的每个值的<strong class="ka jc"> W </strong>系数的值为<strong class="ka jc"> W </strong> λ。</p><p id="5559" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">验证集<br/> </strong>将在验证集上评估<strong class="ka jc"> W </strong> λ的不同值。具有较低误差值的那个将被选择。</p><p id="b122" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">测试集<br/> </strong>测试数据集将再次评估<strong class="ka jc"> W </strong>系数的选定值。</p><p id="c9d3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">上述方法仅在存在足够数量的数据时使用。</p><p id="77c5" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这就是最终选择λ值的方法。这个过程有点暴力。但是利用聪明的猜测和经验，可以减少猜测λ值的迭代。</p><h2 id="c90a" class="kw kx jb bd ky kz la dn lb lc ld dp le kj lf lg lh kn li lj lk kr ll lm ln lo bi translated">结论</h2><p id="f870" class="pw-post-body-paragraph jy jz jb ka b kb lp kd ke kf lq kh ki kj lr kl km kn ls kp kq kr lt kt ku kv ij bi translated">我们已经看到为什么过拟合在机器学习中是不好的，以及如何通过查看模型的<strong class="ka jc"> W </strong>系数的值在模型中识别它。然后，我们看到了线性回归的新成本函数，它考虑了带有调整参数λ的数据的过度拟合。</p><p id="71f3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，我们看到了关于新成本函数的计算公式<strong class="ka jc"> W </strong>以及如何选择λ的值。</p></div></div>    
</body>
</html>