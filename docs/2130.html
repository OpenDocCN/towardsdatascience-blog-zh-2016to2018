<html>
<head>
<title>Gradient Descent Algorithm and Its Variants</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降算法及其变体</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3?source=collection_archive---------0-----------------------#2017-12-21">https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3?source=collection_archive---------0-----------------------#2017-12-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/a7832ce4b49e0b61d703f24471f1e0e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*70f9PB-RwFaakqD6lfp4iw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">Figure 1:</strong> Trajectory towards local minimum</figcaption></figure><p id="fe26" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">优化</strong>是指最小化/最大化一个由<em class="lb"> x </em>参数化的目标函数<em class="lb"> f(x) </em>的任务。在机器/深度学习术语中，它的任务是最小化由模型参数<em class="lb"> w </em> ∈ R^d.优化算法(在最小化的情况下)参数化的成本/损失函数<em class="lb"> J(w) </em>具有以下目标之一:</p><ul class=""><li id="63c2" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">找到目标函数的全局最小值。如果目标函数是凸的，即任何局部最小值都是全局最小值，这是可行的。</li><li id="da81" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">找到目标函数在其邻域内的最低可能值。如果目标函数不像大多数深度学习问题那样是凸的，通常就是这种情况。</li></ul><p id="a2bb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有三种优化算法:</p><ul class=""><li id="0310" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">非迭代优化算法，仅求解一个点。</li><li id="d8bf" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">本质上是迭代的优化算法，并且收敛到可接受的解决方案，而不管参数初始化，例如应用于逻辑回归的梯度下降。</li><li id="1220" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">本质上是迭代的优化算法，应用于一组具有非凸成本函数的问题，如神经网络。因此，参数的初始化对于加快收敛速度和降低错误率起着至关重要的作用。</li></ul><p id="2190" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">梯度下降</strong>是<em class="lb">机器学习</em>和<em class="lb">深度学习</em>中最常见的优化算法。它是一种一阶优化算法。这意味着在对参数进行更新时，它只考虑一阶导数。在每次迭代中，我们在目标函数<em class="lb"> J(w) </em> w.r.t的梯度的相反方向上更新参数，其中梯度给出最陡上升的方向。我们在每次迭代中达到局部最小值的步长由学习速率α决定。因此，我们沿着斜坡的方向下山，直到我们到达一个局部最小值。</p><p id="97fd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将介绍梯度下降算法及其变体:<em class="lb">批量梯度下降、小批量梯度下降和随机梯度下降</em>。</p><p id="0c40" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们先看看梯度下降是如何对逻辑回归起作用的，然后再详细讨论它的变体。为了简单起见，我们假设logistic回归模型只有两个参数:权重<em class="lb"> w </em>和偏差<em class="lb"> b </em>。</p><p id="baa6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1.将重量<em class="lb"> w </em>和偏差<em class="lb"> b </em>初始化为任意随机数。</p><p id="25c3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.为学习率α选择一个值。学习率决定了每次迭代的步长。</p><ul class=""><li id="516b" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">如果α非常小，则需要很长时间才能收敛，并且计算量很大。</li><li id="dfcb" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">如果α很大，它可能无法收敛并超过最小值。</li></ul><p id="5609" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，针对不同的α值绘制成本函数，并选择恰好在第一个不收敛的值之前的α值，这样我们就有了一个收敛的非常快速的学习算法(见图2)。</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lq"><img src="../Images/f91f58f465c936b4559967cf8f6f2b12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rcmvCjQvsxrJi8Y4HpGcCw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">Figure 2:</strong> Gradient descent with different learning rates. <a class="ae lv" href="http://cs231n.github.io/neural-networks-3/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><ul class=""><li id="368b" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">最常用的费率有:<em class="lb"> 0.001，0.003，0.01，0.03，0.1，0.3 </em>。</li></ul><p id="722b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.如果数据的比例非常不同，请确保对数据进行缩放。如果我们不缩放数据，水平曲线(等高线)会更窄更高，这意味着需要更长的时间来收敛(见图3)。</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lw"><img src="../Images/7f18e03d3984eda8f95d545b5c6e049c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vXpodxSx-nslMSpOELhovg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">Figure 3:</strong> Gradient descent: normalized versus unnormalized level curves.</figcaption></figure><p id="b37f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">调整数据，使μ = 0，σ = 1。下面是每个示例的缩放公式:</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/5a5bd08693ae95ce5c59684b0b90f32a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*2g6dhidPigWEuAFyNHL8iw.png"/></div></figure><p id="7f5e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">4.在每次迭代中，取成本函数<em class="lb"> J(w) </em> w.r.t每个参数(梯度)的偏导数:</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/74ce30f2a83f309c3ba6d6a564adfeb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*WmuFVQbceFdNKO2Usl_O7A.png"/></div></figure><p id="cd07" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更新方程是:</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lz"><img src="../Images/f96ca507287e3b427580606347f6ef19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VDTl0P6ongCcM0AgDPUR_g.png"/></div></div></figure><ul class=""><li id="b728" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">为了便于说明，我们假设我们没有偏见。如果<em class="lb"> w的当前值的斜率为&gt; 0 </em>，这意味着我们处于最优<em class="lb"> w* </em>的右边。因此，更新将是负的，并将开始接近最佳值<em class="lb"> w </em> *。然而，如果它是负的，更新将是正的，并将增加当前值<em class="lb"> w </em>以收敛到最优值<em class="lb"> w </em> *(见图4):</li></ul><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ma"><img src="../Images/c54cc9b90ec821dd06bdaf579917d0b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jNyE54fTVOH1203IwYeNEg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">Figure 4:</strong> Gradient descent. An illustration of how gradient descent algorithm uses the first derivative of the loss function to follow downhill it’s minimum.</figcaption></figure><ul class=""><li id="c0d0" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">继续该过程，直到成本函数收敛。也就是说，直到误差曲线变得平坦不变。</li><li id="3453" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">此外，在每次迭代中，步长将位于产生最大变化的方向，因为它垂直于每步的高程曲线。</li></ul><p id="ba06" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们讨论梯度下降算法的三种变体。它们之间的主要区别是我们在计算每个学习步骤的梯度时使用的数据量。它们之间的权衡是梯度的精度与执行每个参数更新(学习步骤)的时间复杂度。</p><h1 id="14cb" class="mb mc iq bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">批量梯度下降</h1><p id="4056" class="pw-post-body-paragraph kd ke iq kf b kg mz ki kj kk na km kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">批量梯度下降是指在对参数进行更新时，我们对每次迭代的所有示例进行求和。因此，对于每次更新，我们必须总结所有示例:</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/ca81ed445ea9f69c5961dc1d40e52765.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*nu5id-pd3BNCl1KktBxP4g.png"/></div></figure><pre class="lr ls lt lu gt nf ng nh ni aw nj bi"><span id="82cc" class="nk mc iq ng b gy nl nm l nn no">for i in range(num_epochs):<br/>    grad = compute_gradient(data, params)<br/>    params = params — learning_rate * grad</span></pre><p id="9213" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">主要优势:</p><ul class=""><li id="a5a5" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">我们可以在训练时使用固定的学习率，而不用担心学习率衰减。</li><li id="49e7" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">它具有朝向最小值的直线轨迹，并且如果损失函数是凸的，它保证在理论上收敛到全局最小值，并且如果损失函数不是凸的，它保证收敛到局部最小值。</li><li id="2dbc" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">它对梯度有无偏估计。例子越多，标准误差越低。</li></ul><p id="8a31" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">主要缺点:</p><ul class=""><li id="eee8" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">即使我们可以使用矢量化实现，浏览所有示例仍然会很慢，尤其是当我们有大型数据集时。</li><li id="7559" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">学习的每一步都是在看完所有的例子之后进行的，有些例子可能是多余的，对更新没有太大帮助。</li></ul><h1 id="0585" class="mb mc iq bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">小批量梯度下降</h1><p id="d3ba" class="pw-post-body-paragraph kd ke iq kf b kg mz ki kj kk na km kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">小批量梯度下降法不是遍历所有样品，而是根据批量大小对少量样品进行汇总。因此，学习发生在每个小批量的<em class="lb"> b </em>示例上:</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/a21bf9b6e0310e4f1928c57433c67365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9AlSFxKa6iyqR7qqlOQSQQ.png"/></div></div></figure><ul class=""><li id="960f" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">打乱训练数据集以避免预先存在的示例顺序。</li><li id="2531" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">根据批量大小将训练数据集划分为<em class="lb"> b </em>个小批量。如果训练集大小不能被批大小整除，则剩余的将是它自己的批。</li></ul><pre class="lr ls lt lu gt nf ng nh ni aw nj bi"><span id="de7f" class="nk mc iq ng b gy nl nm l nn no">for i in range(num_epochs):<br/>    np.random.shuffle(data)<br/>    for batch in radom_minibatches(data, batch_size=32):<br/>        grad = compute_gradient(batch, params)<br/>        params = params — learning_rate * grad</span></pre><p id="86d5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">批量大小是我们可以调整的。通常选择2的幂，如32、64、128、256、512等。其背后的原因是因为一些硬件，如GPU，在普通批量大小如2的幂的情况下，可以实现更好的运行时间。</p><p id="dedd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">主要优势:</p><ul class=""><li id="1a0d" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">比批处理版本快，因为它经历的例子比批处理(所有例子)少得多。</li><li id="8b01" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">随机选择例子将有助于避免多余的例子或非常相似的例子，对学习没有太大帮助。</li><li id="502f" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">利用批量大小&lt; size of training set, it adds noise to the learning process that helps improving generalization error.</li><li id="1428" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">Even though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur.</li></ul><p id="2a85" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">The main disadvantages:</p><ul class=""><li id="f5b1" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">It won’t converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges.</li><li id="3c1d" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">Due to the noise, the learning steps have more oscillations (see figure 4) and requires adding learning-decay to decrease the learning rate as we become closer to the minimum.</li></ul><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/808f11a8ebf97aede566fafe38cb83ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5mHkZw3FpuR2hBNFlRxZ-A.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">Figure 5:</strong> Gradient descent: batch versus mini-batch loss function</figcaption></figure><p id="4182" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">With large training datasets, we don’t usually need more than 2–10 passes over all training examples (epochs). Note: with batch size <em class="lb"> b = m </em>(训练样本数)，我们得到批量梯度下降。</p><h1 id="5e04" class="mb mc iq bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">随机梯度下降</h1><p id="3177" class="pw-post-body-paragraph kd ke iq kf b kg mz ki kj kk na km kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">随机梯度下降(SGD)不是遍历所有示例，而是对每个示例(x^i,y^i).)执行参数更新因此，学习发生在每一个例子上:</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/b532b05e4df273477752ba3c3148e05f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*ecOF_YWCDJE9GFcEzKzlPw.png"/></div></figure><ul class=""><li id="e73e" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">打乱训练数据集以避免预先存在的示例顺序。</li><li id="43ac" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">将训练数据集划分为m个<em class="lb">示例。</em></li></ul><pre class="lr ls lt lu gt nf ng nh ni aw nj bi"><span id="391c" class="nk mc iq ng b gy nl nm l nn no">for i in range(num_epochs):<br/>    np.random.shuffle(data)<br/>    for example in data:<br/>        grad = compute_gradient(example, params)<br/>        params = params — learning_rate * grad</span></pre><p id="95e2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它分享了小批量版本的大部分优点和缺点。以下是特定于SGD的选项:</p><ul class=""><li id="91af" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">与有助于改善泛化误差的小批量相比，它给学习过程添加了更多的噪声。然而，这将增加运行时间。</li><li id="653c" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">我们不能在1个例子上使用矢量化，速度变得非常慢。此外，由于我们在每个学习步骤中仅使用一个示例，因此差异变得很大。</li></ul><p id="f4ad" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图显示了梯度下降的变量及其朝向最小值的方向:</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/ef3150537ecac2f8bc90c0e755376fc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PV-fcUsNlD9EgTIc61h-Ig.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">Figure 6:</strong> Gradient descent variants’ trajectory towards minimum</figcaption></figure><p id="7b1c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上图所示，与小批量相比，SGD方向噪音很大。</p><h1 id="15e2" class="mb mc iq bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">挑战</h1><p id="5f60" class="pw-post-body-paragraph kd ke iq kf b kg mz ki kj kk na km kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">以下是关于梯度下降算法及其变体(主要是批处理和小批处理)的一些挑战:</p><ul class=""><li id="8f5c" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">梯度下降是一种一阶优化算法，这意味着它不考虑成本函数的二阶导数。然而，函数的曲率影响每个学习步骤的大小。梯度测量曲线的陡度，而二阶导数测量曲线的曲率。因此，如果:</li></ul><ol class=""><li id="67b6" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la nt li lj lk bi translated">二阶导数= 0→曲率呈线性。因此，步长=学习率α。</li><li id="efed" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la nt li lj lk bi translated">二阶导数&gt; 0 →曲率向上。因此，步长&lt; the learning rate α and may lead to divergence.</li><li id="98d5" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la nt li lj lk bi translated">Second derivative &lt; 0 → the curvature is going downward. Therefore, the step size &gt;即学习速率α。</li></ol><p id="9caf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果，看起来对梯度有希望的方向可能不是这样，并且可能导致学习过程变慢或者甚至发散。</p><ul class=""><li id="3666" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">如果Hessian矩阵具有较差的条件数，即最大曲率的方向比最小曲率的方向具有更大的曲率。这将导致代价函数在某些方向上非常敏感，而在另一些方向上不敏感。因此，它会使梯度变得更难，因为看起来对梯度有希望的方向可能不会导致成本函数的大变化(见图7)。</li></ul><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/16821d17ef60b7302eb459c8049406e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xoAZP424rr74lzYxPXxk0Q.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">Figure 7:</strong> Gradient descent fails to exploit the curvature information contained in the Hessian matrix. <a class="ae lv" href="http://www.deeplearningbook.org/contents/numerical.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><ul class=""><li id="dc1a" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">梯度gTg的范数应该随着每个学习步骤缓慢降低，因为曲线变得越来越平坦，并且曲线的陡度将降低。然而，我们看到，由于曲线的曲率，梯度的范数在增加。尽管如此，即使梯度的标准在增加，我们也能够实现非常低的错误率(见图8)。</li></ul><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/ab0a2585f7acacbad015ca9e183d12cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8_u3K1hsxLveHuOVKBPG4A.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">Figure 8:</strong> Gradient norm. <a class="ae lv" href="http://www.deeplearningbook.org/contents/optimization.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><ul class=""><li id="5025" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">在小维度中，局部极小是常见的；然而，在大尺寸中，鞍点更常见。鞍点是指函数在某些方向上向上弯曲，在其他方向上向下弯曲。换句话说，鞍点从一个方向看起来像最小值，从另一个方向看起来像最大值(见图9)。当hessian矩阵的至少一个特征值为负，而其余特征值为正时，会发生这种情况。</li></ul><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nw"><img src="../Images/2c2f6672757d883eaddf16427ea303d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6h_Q_BC_epUm6Fhoudpgew.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">Figure 9:</strong> Saddle point</figcaption></figure><ul class=""><li id="963b" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">如前所述，选择一个合适的学习速度是很难的。此外，对于小批量梯度下降，我们必须在训练过程中调整学习速率，以确保它收敛到局部最小值，而不是在它周围徘徊。计算学习率的衰减率也很困难，并且随着不同的数据集而变化。</li><li id="3dd0" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">所有参数更新具有相同的学习率；然而，我们可能希望对一些参数进行更大的更新，这些参数的方向导数比其他参数更符合朝向最小值的轨迹。</li></ul><p id="7e21" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">原载于2017年12月21日</em><a class="ae lv" href="https://imaddabbura.github.io/posts/optimization-algorithms/gradient-descent.html" rel="noopener ugc nofollow" target="_blank"><em class="lb">imaddabbura . github . io</em></a><em class="lb">。</em></p></div></div>    
</body>
</html>