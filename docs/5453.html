<html>
<head>
<title>Why Random Forest is My Favorite Machine Learning Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么随机森林是我最喜欢的机器学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-random-forest-is-my-favorite-machine-learning-model-b97651fa3706?source=collection_archive---------0-----------------------#2018-10-19">https://towardsdatascience.com/why-random-forest-is-my-favorite-machine-learning-model-b97651fa3706?source=collection_archive---------0-----------------------#2018-10-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f066" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">发现现实世界中随机森林的优点和缺点</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cd6dd7e7205859132bcc248faf72ecb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wDfweQrKYCvDXWk7"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">“green trees on foggy forest” by <a class="ae ky" href="https://unsplash.com/@djulien?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Julien R</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1d3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“模特就像一副护目镜。它让某些事情成为焦点。”—我的数据科学导师。</p><p id="5957" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在选择机器学习模型时，没有单一的算法占主导地位。有些在大型数据集上表现更好，有些在高维数据上表现更好。因此，评估模型对特定数据集的有效性非常重要。在本文中，我将对随机森林的工作原理做一个高层次的概述，并讨论这种模型在现实世界中的优缺点。</p><blockquote class="lv"><p id="36a5" class="lw lx it bd ly lz ma mb mc md me lu dk translated">从本质上讲，如果您想要高性能且较少需要解释，随机森林是一个很好的模型。</p></blockquote><p id="15e7" class="pw-post-body-paragraph kz la it lb b lc mf ju le lf mg jx lh li mh lk ll lm mi lo lp lq mj ls lt lu im bi translated">在回归模型之后，随机森林总是我的首选模型。让我告诉你为什么。</p><h1 id="f96a" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">什么是随机森林？</h1><p id="eac0" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">随机森林是<strong class="lb iu">打包的决策树</strong>模型，每次分裂时在<strong class="lb iu">特征子集</strong>上分裂。这是一个很长的话题，所以让我们先看一个单独的决策树，然后讨论袋装决策树，最后介绍对一个<strong class="lb iu"> </strong>随机特征子集的分割。</p><p id="1700" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">决策树</strong></p><p id="7d95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本质上，决策树根据数据的特征将数据分割成更小的数据组，直到我们得到一个足够小的数据集，其中只有一个标签下的数据点。让我们看一个例子。下面是一个人是否应该打网球的决策树。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/fc0ce643a13594995d5c4aa438a7f703.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/0*9W0mx4ffV2qhNnm0.jpg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image courtesy of <a class="ae ky" href="http://science.slc.edu/~jmarshall/courses/2005/fall/cs151/lectures/decision-trees/" rel="noopener ugc nofollow" target="_blank">http://science.slc.edu/~jmarshall/courses/2005/fall/cs151/lectures/decision-trees/</a></figcaption></figure><p id="e514" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中，决策树在多个特征上分裂，直到我们得出结论“是”，我们应该打网球，或者“否”，我们不应该打网球。沿着树的线条来确定决策。例如，如果天气阴沉，那么“是的”我们应该打网球。如果天气晴朗，湿度高，那么“不”我们不应该打网球。</p><p id="d1fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在决策树模型中，这些分裂是根据纯度度量来选择的。也就是说，在每个节点，我们希望信息增益最大化。对于回归问题，我们考虑残差平方和(RSS ),对于分类问题，我们考虑基尼指数或熵。我不会在这方面讲太多细节，但是如果你有兴趣了解更多，可以看看这个<a class="ae ky" href="https://www.slideshare.net/marinasantini1/lecture-4-decision-trees-2-entropy-information-gain-gain-ratio-55241087" rel="noopener ugc nofollow" target="_blank">讲座</a>。</p><p id="9870" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">袋装树</strong></p><p id="2aae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在采用决策树概念，让我们应用自举原理来创建袋装树。</p><p id="e53e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Bootstrapping </strong>是一种抽样技术，我们从数据集中随机抽样替换。</p><blockquote class="ni nj nk"><p id="51bc" class="kz la nl lb b lc ld ju le lf lg jx lh nm lj lk ll nn ln lo lp no lr ls lt lu im bi translated">旁注:自举时，我们只使用了大约 2/3 的数据。大约 1/3 的数据(“袋外”数据)没有在模型中使用，可以方便地用作测试集。</p></blockquote><p id="4568" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Bagging </strong>或 bootstrap aggregating，是我们通过创建<em class="nl"> X </em>数量的决策树来创建袋装树，这些决策树在<em class="nl">X</em>bootstrap 训练集上进行训练。最终的预测值是我们所有<em class="nl"> X </em>决策树的平均值。一个单独的决策树具有很高的方差(倾向于过度拟合)，因此通过<strong class="lb iu"> </strong>将许多弱学习者打包或组合成强学习者，我们将方差平均掉。这是多数票！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/8fa86ae21c25a13b2db1afb219796970.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/0*2z-o2YgW5gQHuzIu.jpg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image courtesy of <a class="ae ky" href="https://www.kdnuggets.com/2016/11/data-science-basics-intro-ensemble-learners.html" rel="noopener ugc nofollow" target="_blank">https://www.kdnuggets.com/2016/11/data-science-basics-intro-ensemble-learners.html</a></figcaption></figure><p id="0d58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">随机森林</strong></p><p id="d6d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林对 bagging 进行了改进，因为它<strong class="lb iu">通过引入对特征</strong>的<strong class="lb iu">随机子集的分割来使</strong>树去相关。这意味着在树的每次分裂时，模型只考虑特征的一个小的子集，而不是模型的所有特征。也就是说，从可用特征集合 n 中，随机选择 m 个特征的子集(m =的平方根)。这一点很重要，这样方差就可以被平均掉。考虑一下如果数据集包含一些强预测因素会发生什么。这些预测器将始终被选在树的顶层，所以我们将有非常相似的结构树。换句话说，这些树是高度相关的。</p><p id="e3fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">综上所述，随机森林是袋装决策树模型，在每次分裂时分裂特征子集。</p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h1 id="6f13" class="mk ml it bd mm mn nx mp mq mr ny mt mu jz nz ka mw kc oa kd my kf ob kg na nb bi translated"><strong class="ak">为什么随机森林这么酷？</strong></h1><p id="09b2" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated"><strong class="lb iu">令人印象深刻的多功能性</strong></p><p id="0243" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无论您有回归或分类任务，随机森林都是满足您需求的适用模型。它可以处理二元特征、分类特征和数字特征。需要做的预处理非常少。数据不需要重新缩放或转换。</p><p id="03b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">可并行化</strong></p><p id="9b21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它们是可并行化的，这意味着我们可以将流程拆分到多台机器上运行。这导致更快的计算时间。相比之下，增强模型是连续的，需要更长的时间来计算。</p><blockquote class="ni nj nk"><p id="e9ed" class="kz la nl lb b lc ld ju le lf lg jx lh nm lj lk ll nn ln lo lp no lr ls lt lu im bi translated">附注:具体来说，在 Python 中，要在多台机器上运行它，需要提供参数“n _ jobs =-1”。-1 表示使用所有可用的机器。更多详情见<a class="ae ky" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>文档。</p></blockquote><p id="ec27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">高维度大</strong></p><p id="0dda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林非常适合高维数据，因为我们处理的是数据子集。</p><p id="535d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">快速预测/训练速度</strong></p><p id="0b76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它比决策树训练更快，因为我们只处理这个模型中的一个特征子集，所以我们可以轻松地处理数百个特征。预测速度明显快于训练速度，因为我们可以保存生成的森林供将来使用。</p><p id="efb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">对异常值和非线性数据稳健</strong></p><p id="1d87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林通过本质上的宁滨处理离群值。它对非线性特征也漠不关心。</p><p id="6b41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">处理不平衡数据</strong></p><p id="1494" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它有平衡类总体不平衡数据集中的误差的方法。随机森林试图最小化整体错误率，因此当我们有一个不平衡的数据集时，较大的类将获得较低的错误率，而较小的类将具有较大的错误率。</p><p id="ebc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">低偏差，中等方差</strong></p><p id="ef52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个决策树都有很高的方差，但偏差很小。但是因为我们平均随机森林中的所有树，我们也平均方差，所以我们有一个低偏差和中等方差模型。</p><h1 id="2a06" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated"><strong class="ak">弊端</strong></h1><ol class=""><li id="950c" class="oc od it lb b lc nc lf nd li oe lm of lq og lu oh oi oj ok bi translated">模型的可解释性:随机森林模型并不都是可解释的；它们就像黑匣子。</li><li id="3f6e" class="oc od it lb b lc ol lf om li on lm oo lq op lu oh oi oj ok bi translated">对于非常大的数据集，树的大小会占用大量内存。</li><li id="f392" class="oc od it lb b lc ol lf om li on lm oo lq op lu oh oi oj ok bi translated">它可能会过度拟合，所以您应该调整超参数。</li></ol></div></div>    
</body>
</html>