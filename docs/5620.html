<html>
<head>
<title>ELMo helps to further improve your sentence embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ELMo 有助于进一步提高你的句子嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f?source=collection_archive---------4-----------------------#2018-10-30">https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f?source=collection_archive---------4-----------------------#2018-10-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/a3183e29a11822110d580456a0fee8af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JTrMvvOr-OewWqkn.jpg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">by <a class="ae kf" href="https://pixabay.com/en/universal-studios-singapore-2413365/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/en/universal-studios-singapore-2413365/</a></figcaption></figure><p id="54be" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在最后一个故事中，<a class="ae kf" rel="noopener" target="_blank" href="/replacing-your-word-embeddings-by-contextualized-word-vectors-9508877ad65d">上下文化的单词向量</a> (CoVe)被引入，这是单词嵌入的增强版本。Peters 等人提出了深度语境化单词表征，旨在为不同语境下的自然语言处理任务提供更好的单词表征。他们称之为 ELMo(来自语言模型的嵌入)。</p><p id="6431" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看完这篇帖子，你会明白:</p><ul class=""><li id="a628" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">语言模型设计中的嵌入</li><li id="373e" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">体系结构</li><li id="7f06" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">履行</li><li id="3f92" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">拿走</li></ul><h1 id="541d" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">语言模型设计中的嵌入</h1><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mq"><img src="../Images/4ceff6a8643199ab7bbbe3d75f26e6a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8HAQm_Kw2eX73T7o"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@madebyluddy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Conor Luddy</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="984d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ELMo 使用双向语言模型(biLM)来学习单词(例如，句法和语义)和语言上下文(即，对多义性建模)。在预训练之后，向量的内部状态可以被转移到下游的 NLP 任务。Peters 等人使用 6 个 NLP 任务来评估 biLM 的结果。</p><p id="6a2f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些任务分别是<strong class="ki iu">问答</strong>、<strong class="ki iu">文本蕴涵</strong>、<strong class="ki iu">语义角色标注</strong>、<strong class="ki iu">指代消解</strong>、<strong class="ki iu">命名实体抽取</strong>和<strong class="ki iu">情感分析</strong>。他们都取得了优异的成绩。</p><h1 id="0d9c" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">体系结构</h1><p id="abb7" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">与传统的单词嵌入不同，ELMo 针对不同的场景，为每个单词生成多个单词嵌入。较高层捕获单词嵌入的上下文相关方面，而较低层捕获语法的模型方面。在最简单的情况下，我们只使用 ELMo 的顶层(只有一层),同时我们也可以将所有层合并成一个矢量。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/6c29eb6145c0cc41c10734ed833505fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*_HsSVBam0IZc2LqbbkzC-A.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Peters et al. (2018)</figcaption></figure><p id="9e27" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以连接 ELMo 向量和令牌嵌入(<a class="ae kf" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a">单词嵌入</a>和/或<a class="ae kf" rel="noopener" target="_blank" href="/besides-word-embedding-why-you-need-to-know-character-embedding-6096a34a3b10">字符嵌入</a>)以形成如下新嵌入:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/39869ceaaca16da9b4624b979b7f68c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*S2e7Xcu_Gl-Vcv3VgZ9TVQ.png"/></div></figure><p id="e47c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在实验中，Peters 等人使用 L=2 (2 个 biLSTM 层),其中 4096 个单元和 512 个输出维度用于上下文相关部分，而 2048 个字符 n-gram 构成过滤器和 512 个输出维度用于上下文不敏感部分，以构建上下文化的单词嵌入。</p><h1 id="6faa" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">履行</h1><p id="21e4" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">原作者是 McCann 等人，他们用 python 3.6 通过 Pytorch 实现了 ELMo。有 Tensorflow，chainer 和 Keras 版本可供选择。我将使用 Keras 版本来演示我们如何将文本转换为矢量。对于其他人，您可以查看参考资料部分提到的 githubs。</p><p id="95b2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的例子展示了我们如何使用 keras 来实现它。<em class="nc"> Tensorflow Hub </em>是一个模型 Hub，存储了大量不同的模型。如果您只需要一个预训练的嵌入，您可以使用下面的代码从 Tensorflow Hub 中检索它并传输到 Keras。</p><p id="5435" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以从 Tensorflow Hub 预训练模型中选择 2 个嵌入层(总共有 5 层)。第一个“elmo”是其他 3 层的加权和。第二个是“默认”，它是所有层的固定均值轮询。你可以从<a class="ae kf" href="https://alpha.tfhub.dev/google/elmo/2" rel="noopener ugc nofollow" target="_blank">这里</a>找到更多信息。</p><p id="4326" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我的演示中，有 3 种方法可以使用 ELMo 预训练模型。它们是:</p><ol class=""><li id="e5ec" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld nd lk ll lm bi translated">具有单词嵌入的 3 层的加权和</li><li id="bdb7" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nd lk ll lm bi translated">没有单词嵌入的 3 层的加权和</li><li id="6131" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld nd lk ll lm bi translated">不含单词嵌入的固定均值池</li></ol><p id="b91d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nc"> 1。具有单词嵌入的 3 层的加权和</em> </strong></p><pre class="mr ms mt mu gt ne nf ng nh aw ni bi"><span id="3b4d" class="nj lt it nf b gy nk nl l nm nn"># Input Layers<br/>word_input_layer = Input(shape=(None, ), dtype='int32')<br/>elmo_input_layer = Input(shape=(None, ), dtype=tf.string)</span><span id="707d" class="nj lt it nf b gy no nl l nm nn"># Output Layers<br/>word_output_layer = Embedding(<br/>    input_dim=vocab_size, output_dim=256)(word_input_layer)<br/>elmo_output_layer = Lambda(<br/>    elmo_embs.to_keras_layer, <br/>    output_shape=(None, 1024))(elmo_input_layer)<br/>output_layer = Concatenate()(<br/>    [word_output_layer, elmo_output_layer])<br/>output_layer = BatchNormalization()(output_layer)<br/>output_layer = LSTM(<br/>    256, dropout=0.2, recurrent_dropout=0.2)(output_layer)<br/>output_layer = Dense(4, activation='sigmoid')(output_layer)</span><span id="b347" class="nj lt it nf b gy no nl l nm nn"># Build Model<br/>model = Model(<br/>    inputs=[word_input_layer, elmo_input_layer], <br/>    outputs=output_layer)<br/>model.compile(<br/>    loss='sparse_categorical_crossentropy', <br/>    optimizer='adam', metrics=['accuracy'])<br/>model.summary()<br/>model.fit(<br/>    [x_train_words, x_train_sentences], y_train,<br/>#     validation_data=([x_test_words, x_test_sentences], y_test), <br/>    epochs=10, batch_size=32)</span></pre><p id="3509" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果</p><pre class="mr ms mt mu gt ne nf ng nh aw ni bi"><span id="de60" class="nj lt it nf b gy nk nl l nm nn">Accuracy:77.10%<br/>Average Precision: 0.78<br/>Average Recall: 0.77<br/>Average f1: 0.76</span></pre><p id="ce01" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nc"> 2。没有单词嵌入的 3 层的加权和</em> </strong></p><pre class="mr ms mt mu gt ne nf ng nh aw ni bi"><span id="7a8e" class="nj lt it nf b gy nk nl l nm nn"># Input Layers<br/>elmo_input_layer = Input(shape=(None, ), dtype=tf.string)</span><span id="7903" class="nj lt it nf b gy no nl l nm nn"># Output Layers<br/>output_layer = Lambda(<br/>    elmo_embs.to_keras_layer, <br/>    output_shape=(None, 1024))(elmo_input_layer)<br/>output_layer = BatchNormalization()(output_layer)<br/>output_layer = LSTM(<br/>    256, dropout=0.2, recurrent_dropout=0.2)(output_layer)<br/>output_layer = Dense(4, activation='sigmoid')(output_layer)</span><span id="52f7" class="nj lt it nf b gy no nl l nm nn"># Build Model<br/>model = Model(<br/>    inputs=elmo_input_layer, outputs=output_layer)<br/>model.compile(<br/>    loss='sparse_categorical_crossentropy', <br/>    optimizer='adam', metrics=['accuracy'])<br/>model.summary()<br/>model.fit(<br/>    x_train_sentences, y_train,<br/>#     validation_data=([x_test_words, x_test_sentences], y_test), <br/>    epochs=10, batch_size=32)</span></pre><p id="dfdd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果</p><pre class="mr ms mt mu gt ne nf ng nh aw ni bi"><span id="49d0" class="nj lt it nf b gy nk nl l nm nn">Accuracy:76.83%<br/>Average Precision: 0.78<br/>Average Recall: 0.77<br/>Average f1: 0.77</span></pre><p id="9a77" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nc"> 3。不含单词嵌入的固定均值池</em> </strong></p><pre class="mr ms mt mu gt ne nf ng nh aw ni bi"><span id="75e8" class="nj lt it nf b gy nk nl l nm nn"># Input Layers<br/>input_layer = Input(shape=(None,), dtype=tf.string)</span><span id="1978" class="nj lt it nf b gy no nl l nm nn"># Output Layers<br/>output_layer = Lambda(<br/>    elmo_embs.to_keras_layer, <br/>    output_shape=(1024,))(input_layer)<br/>output_layer = Dense(<br/>    256, activation='relu')(output_layer)<br/>output_layer = Dense(4, activation='sigmoid')(output_layer)</span><span id="e372" class="nj lt it nf b gy no nl l nm nn">model = Model(inputs=[input_layer], outputs=output_layer)<br/>model.compile(<br/>    loss='sparse_categorical_crossentropy', <br/>    optimizer='adam', metrics=['accuracy'])<br/>model.summary()<br/>model.fit(<br/>    x_train_sentences, y_train,<br/>#     validation_data=([x_test_words, x_test_sentences], y_test), <br/>    epochs=10, batch_size=32)</span></pre><p id="e8cd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果</p><pre class="mr ms mt mu gt ne nf ng nh aw ni bi"><span id="2a10" class="nj lt it nf b gy nk nl l nm nn">Accuracy:74.37%<br/>Average Precision: 0.79<br/>Average Recall: 0.74<br/>Average f1: 0.75</span></pre><h1 id="a5e0" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">拿走</h1><p id="c0f7" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">要访问所有代码，您可以访问这个<a class="ae kf" href="https://github.com/makcedward/nlp/blob/master/sample/nlp-embeddings-sentence-elmo.ipynb" rel="noopener ugc nofollow" target="_blank"> github </a> repo</p><ul class=""><li id="35b9" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">CoVe 需要标签数据来获得上下文单词向量，而 ELMo 则采用无监督的方式。</li><li id="71cf" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">CoVe 只使用最后一层，而 ELMo 使用多层来表示上下文单词。</li><li id="4299" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">无法解决 OOV 问题。它建议使用零向量来表示未知单词。ELMO 可以处理 OOV 问题，因为它使用<strong class="ki iu">字符嵌入</strong>来构建单词嵌入。</li><li id="85c3" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">ELMo 计算矢量很费时间。根据作者的建议，您可以离线预计算令牌，并在在线预测期间查找它，以减少时间开销。</li></ul><h1 id="1190" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以通过<a class="ae kf" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae kf" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae kf" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="6f9f" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">参考</h1><p id="e468" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">彼得斯 M. E .、诺依曼 m .、乌耶 m .、加德纳 m .、克拉克 c ...深层语境化的词语表达。2018.<a class="ae kf" href="https://arxiv.org/pdf/1802.05365.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1802.05365.pdf</a></p><p id="f93b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://github.com/allenai/allennlp/" rel="noopener ugc nofollow" target="_blank">py torch 中的 ELMo</a>(原创)</p><p id="4cd8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://github.com/strongio/keras-elmo/blob/master/Elmo%20Keras.ipynb" rel="noopener ugc nofollow" target="_blank"> ELMo in Keras </a></p><p id="bd3d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://github.com/allenai/bilm-tf" rel="noopener ugc nofollow" target="_blank"> ELMo in Tensorflow </a></p><p id="9dd1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://github.com/chainer/models/tree/master/elmo-chainer" rel="noopener ugc nofollow" target="_blank"> ELMo in chainer </a></p></div></div>    
</body>
</html>