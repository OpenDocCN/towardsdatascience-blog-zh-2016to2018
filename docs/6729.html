<html>
<head>
<title>ART + AI — Generating African Masks using (Tensorflow and TPUs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ART + AI —使用(Tensorflow 和 TPUs)生成非洲面具</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/african-masks-gans-tpu-9a6b0cf3105c?source=collection_archive---------11-----------------------#2018-12-28">https://towardsdatascience.com/african-masks-gans-tpu-9a6b0cf3105c?source=collection_archive---------11-----------------------#2018-12-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5854" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">容易吗？需要多少数据？数据质量会影响结果吗？甘人学到了什么？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/76265f78b281ffd88efefc17aaad49a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rg7cVQIxT8zPPWFa4MVO5A.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">African Masks generated using a DCGAN model.</figcaption></figure><p id="dcd2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2018 年 8 月，我有机会参加了<a class="ae lr" href="https://medium.com/@victor.dibia/6-reasons-i-loved-the-2018-deep-learning-indaba-e01d0ff5009" rel="noopener"> 2018 深度学习 Indaba </a>，在这里 Google 慷慨地向所有参与者提供了 TPUs (v2)的访问权限！我最终花了一些时间来熟悉在 TPUs 上运行 ML 实验，并开始训练 GAN 基于我一直在管理的自定义数据集生成图像——非洲掩模数据集。TPUs 提供了一些强大的计算能力(与<a class="ae lr" href="https://blog.riseml.com/benchmarking-googles-new-tpuv2-121c03b71384" rel="noopener ugc nofollow" target="_blank">高端 GPU</a>不相上下)，能够快速完成训练实验。访问 TPU 使我能够探索一些与训练 gan 相关的问题——例如，多少数据是足够的数据？图像大小如何影响生成的输出？数据质量有多重要(例如，整理数据集的努力？)生成的样本有多新颖？等等。在这篇文章中，我提供了我训练 GAN 的步骤、结果和有趣的观察。</p><p id="5a6f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">想尝试使用生成的图像进行交互式演示，并探索它们与训练数据集中的图像的相似性吗？<a class="ae lr" href="https://victordibia.github.io/cocoafrica/#masks" rel="noopener ugc nofollow" target="_blank">试玩在这里</a>。有机会使用 TPU，并希望根据您的客户数据培训 GAN？本项目使用的代码在<a class="ae lr" href="https://github.com/victordibia/tpuDCGAN" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上，包括<a class="ae lr" href="https://github.com/victordibia/tpuDCGAN/tree/master/models/" rel="noopener ugc nofollow" target="_blank">训练好的模型</a>用于生成遮罩。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/58527960e2a141822288207605045938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Bz1a_USqW8LGlM79xyS3rQ.gif"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Web <a class="ae lr" href="https://victordibia.github.io/cocoafrica/#masks" rel="noopener ugc nofollow" target="_blank">demo interface</a> allows you view generated images and most similar from the dataset based on features extracted from a VGG network (block1_pool and block5_pool).</figcaption></figure><h1 id="c793" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">TPUs 上的张量流</h1><p id="6753" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">要在 TPUs 上运行<a class="mq mr ep" href="https://medium.com/u/b1d410cb9700?source=post_page-----9a6b0cf3105c--------------------------------" rel="noopener" target="_blank"> TensorFlow </a>代码，最重要的部分是使用 TPU 版本的设计相当好的<a class="ae lr" href="https://www.tensorflow.org/guide/estimators" rel="noopener ugc nofollow" target="_blank">估算器</a> API。估计器封装了训练循环的以下方面:训练、评估、预测、服务输出。在很大程度上，使用该 API 编写的 Tensorflow 代码(顺便说一句，keras 模型可以很容易地<a class="ae lr" href="https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator" rel="noopener ugc nofollow" target="_blank">转换为估算器 API</a>)可以通过用<a class="ae lr" href="https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimator" rel="noopener ugc nofollow" target="_blank"> TPU 估算器</a>替换常规估算器在 TPU 上运行。更好的是，tpuEstimator 将在 CPU 和 GPU 上工作(您应该设置<em class="ms"> use_tpu </em>标志)，允许在部署到 TPU 集群之前在本地机器上进行简单的测试。</p><blockquote class="mt mu mv"><p id="6c3e" class="kv kw ms kx b ky kz jr la lb lc ju ld mw lf lg lh mx lj lk ll my ln lo lp lq ij bi translated">TPUEstimator 处理在 TPU 设备上运行的许多细节，比如为每个内核复制输入和模型，以及定期返回主机运行钩子。</p></blockquote><p id="4d48" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">幸运的是，我不必从头开始实现 DCGANs，因为 Tensorflow 确实提供了一些<a class="ae lr" href="https://github.com/tensorflow/tpu/tree/master/models/experimental/dcgan" rel="noopener ugc nofollow" target="_blank">样本代码</a>。然而，该示例支持 32px 图像(cifar 数据集)的生成，我的主要任务是扩展该模型以支持大图像尺寸(64px，128px)，类似于使用<a class="ae lr" href="https://github.com/robbiebarrat/art-DCGAN" rel="noopener ugc nofollow" target="_blank"> artDCGAN </a>所做的工作。这主要是通过扩展(增加层)DCGAN 模型中的生成器和鉴别器来实现的。这里最棘手的部分是确保大小的一致性，即确保每一层或功能的输入/输出与预期的形状和大小相匹配。</p><h1 id="c79e" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">生成对抗网络</h1><p id="381f" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">GANs 是深度神经网络的一种形式，对于密度估计特别有用。gan 是<em class="ms">生成性的</em>，因为它们学习一个分布(训练图像数据集)并且能够<em class="ms">生成位于该分布内的</em>“新颖”样本。它们是<em class="ms">对抗性的</em>，因为它们被构造为一个游戏，其中两个神经网络(生成器<strong class="kx ir"> <em class="ms"> G </em> </strong>和鉴别器<strong class="kx ir"> <em class="ms"> D </em> </strong>)竞争——<strong class="kx ir"><em class="ms">G</em></strong>学习生成假图像(通过估计图像数据分布)，而<strong class="kx ir"> <em class="ms"> D </em> </strong>学习区分真实和假图像(图像来自分布或来自<strong class="kx ir">的概率</strong></p><p id="89ff" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">随着游戏的进行，<strong class="kx ir"> <em class="ms"> G </em> </strong>学会创造<strong class="kx ir"> <em class="ms"> D </em> </strong> <em class="ms"> </em>无法从真实图像中分辨出来的“令人信服”的图像。期望<strong class="kx ir"> <em class="ms"> G </em> </strong>学习数据中的所有显著“模式”,并在生成新图像时涵盖所有这些模式。例如，在 Cifar10 数据集上训练的 GAN 应生成所有 10 个类别(汽车、卡车、飞机等)的图像。该研究领域的进展集中在为两个网络寻找良好的自我评估标准(损失函数)、解决几个已知问题的更好的架构(例如，一个网络压倒另一个网络、<a class="ae lr" href="http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/" rel="noopener ugc nofollow" target="_blank">无法学习数据分布的所有“模式】</a>)、生成逼真/高分辨率图像等。</p><p id="52d9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于这个项目，使用无条件 DCGAN[1]或深度卷积 GAN 架构，其架构选择使其在训练期间保持稳定。Tensorflow 提供了使用 TPUs 训练 DCGAN  (CIFAR，MNIST)的<a class="ae lr" href="https://github.com/tensorflow/tpu/tree/master/models/experimental/dcgan" rel="noopener ugc nofollow" target="_blank">示例代码，该代码针对本实验进行了扩展。</a></p><h1 id="6131" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">数据准备和培训</h1><p id="3a02" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">对于这个项目，我使用了非洲面具数据集——一组手动管理的描绘非洲面具的<strong class="kx ir"> ~9300 </strong>图像(我仍在管理这个数据集，但计划很快发布)。</p><blockquote class="mt mu mv"><p id="e4a8" class="kv kw ms kx b ky kz jr la lb lc ju ld mw lf lg lh mx lj lk ll my ln lo lp lq ij bi translated">这个数据集的目标是为探索人工智能和非洲艺术的交集提供额外的资源。</p></blockquote><p id="b29e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">需要注意的一点是，该数据集包含不同“模式”的图像(即许多不同形状、纹理等的遮罩)，而不是像<a class="ae lr" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" rel="noopener ugc nofollow" target="_blank"> CelebA </a>这样定义良好的数据集，它只包含有限变化的人脸。看看 GAN 如何尝试从这个数据集学习模式以及它对这些模式的<em class="ms">解释</em>将会很有趣。我们的目标不是生成一个完美逼真的面具(这是最近 GAN 作品的一个共同目标)，而是更多地观察编码在生成的 GAN 中的任何创造性或艺术性的元素。</p><h2 id="62b6" class="mz lu iq bd lv na nb dn lz nc nd dp md le ne nf mf li ng nh mh lm ni nj mj nk bi translated">TPU 培训</h2><p id="c943" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">对于训练，第一步是<em class="ms">调整</em>、<em class="ms">裁剪</em>每张图像，然后转换为<a class="ae lr" href="https://www.tensorflow.org/tutorials/load_data/tf-records" rel="noopener ugc nofollow" target="_blank"> TFrecords </a>。用于此的脚本在<a class="ae lr" href="https://github.com/victordibia/tpuDCGAN/blob/master/convert_to_tfrecords.py" rel="noopener ugc nofollow" target="_blank">项目的 Github repo </a>中提供。虽然我们不关心无条件 gan 的标签，但是脚本使用目录名作为标签(类似于 torch vision<a class="ae lr" href="https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder" rel="noopener ugc nofollow" target="_blank">image folder</a>)。</p><p id="daf3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">两个版本的 DCGAN 模型被训练以分别生成 64px 和 128px 图像。这包括更改输入函数以反映新的输入大小，然后扩展模型(向 D 和 G 添加层)以匹配<a class="ae lr" href="https://github.com/victordibia/tpuDCGAN/blob/master/dcgan64_model.py" rel="noopener ugc nofollow" target="_blank"> 64px </a>和<a class="ae lr" href="https://github.com/victordibia/tpuDCGAN/blob/master/dcgan64_model.py" rel="noopener ugc nofollow" target="_blank"> 128px </a>代。两个模型的其他参数基本相似——训练步数:15k，批量:1024，固定噪声维数:100，学习率:0.0002(对于 D 和 G)。</p><h1 id="2d5d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">一些有趣的观察</h1><h2 id="274a" class="mz lu iq bd lv na nb dn lz nc nd dp md le ne nf mf li ng nh mh lm ni nj mj nk bi translated">64px 与 128px 一代</h2><p id="7702" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">与 128 像素模式相比，在 64px 模式下生成的图像提供了更好的多样性。虽然 128px 图像的质量明显更高，但它们似乎存在部分模型崩溃的问题。这可能是由于数据集不足以训练如此大的模型(我也为 128px 模型试验了更长的训练时间，但这没有帮助)。</p><div class="kg kh ki kj gt ab cb"><figure class="nl kk nm nn no np nq paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/7a8954cb3d2d0e050e74b2dc284e7261.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*YjU20kPPal9Bw250260fqg.jpeg"/></div></figure><figure class="nl kk nr nn no np nq paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/ee391fbf3e622c24996df2d766934a8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*SJIQy7luv3OLzuFHOz5tBQ.jpeg"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk ns di nt nu"><strong class="bd nv">LEFT:</strong> 128px images generated using a DCGAN, fixed noise vector (dim=100), 15k steps, batch size 1024. Notice how several “modes” appear to emerge within the generated masks (highlighted with same color). While this may be as a result of the sample size. However this does not occur to the same degree when generating 64px images using similar training parameters. <strong class="bd nv">RIGHT</strong>: 64px images generated using a fixed noise vector (dim=100) 15k steps, batch size 1024. These results show <strong class="bd nv">more</strong> diversity compared to the 128px images.</figcaption></figure></div><h2 id="3f31" class="mz lu iq bd lv na nb dn lz nc nd dp md le ne nf mf li ng nh mh lm ni nj mj nk bi translated">多少数据才算足够的数据？</h2><p id="31b6" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">我发现在实践中，一个低至 2k -3k 图像的数据集会生成某种形式的“结果”。为了进一步探索数据集大小如何影响生成的结果，我提取了掩膜数据集(3k 图像，64px)的随机样本，并训练了一个模型。这里的主要观察结果是，局部“模式崩溃”(这个问题在<a class="ae lr" href="http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/" rel="noopener ugc nofollow" target="_blank">这里</a>有很好的解释)的发生率更高，并且生成的图像的多样性降低。下面的屏幕截图包含从使用相同参数训练的两个模型生成的图像，但是使用 3k 数据集和完整的 9k 数据集。其他研究项目(见<a class="ae lr" href="https://ajolicoeur.wordpress.com/cats/" rel="noopener ugc nofollow" target="_blank">这里</a>)似乎训练 GANs 在 6k 和 9k 图像之间有不错的结果。到目前为止，我的直觉是，对于具有某种程度相关性的数据集，GAN 能够从大约 6k-9k 图像开始学习有趣的模式。随着我进行更多的实验，我将更新这一部分。</p><div class="kg kh ki kj gt ab cb"><figure class="nl kk nw nn no np nq paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/7803e6cfc85af60885367a1688367ccd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*HbkhsL8nsE0oBea5JQz5tw.jpeg"/></div></figure><figure class="nl kk nw nn no np nq paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/4fef3ddde92e19c0e70d4124dcaf68cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*DbWkLQgcisGQXNEf1zjfXQ.jpeg"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk ns di nt nu">64px images generated with similar models but left uses a 3k dataset and right uses the full 9k images in the dataset. Images generated using the 3k image dataset shows lower diversity with several similar/repeated images (highlighted).</figcaption></figure></div><h2 id="8bbd" class="mz lu iq bd lv na nb dn lz nc nd dp md le ne nf mf li ng nh mh lm ni nj mj nk bi translated">有噪声的数据集重要吗？</h2><p id="090e" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">管理数据集可能很乏味。初始图像搜索结果可能包含不可用的数据(例如，图片的图片、草图、2D 图像、标记为遮罩的无关图像等)。这就提出了一些相关问题——噪声图像的存在如何影响 GAN 产生的图像质量。为了探索这一点，我使用 13k 图像(来自网络搜索的未整理结果)训练了一个模型，并将结果与我手动整理的 9k 图像进行了视觉比较。</p><div class="kg kh ki kj gt ab cb"><figure class="nl kk nm nn no np nq paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/318be9594c16aa1927db27d914b765ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*i--j2oS2kZv1EpGmfv03IQ.png"/></div></figure><figure class="nl kk nr nn no np nq paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/a0ca1a8f7d6973a853a2b138ba541dae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*RFOp4qsI2DOuoFrcF98izw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk ns di nt nu">64px images generated with similar models but left uses a 3k dataset and right uses the full 9k images in the dataset. Images generated using the 3k image dataset shows lower diversity with several similar/repeated images (highlighted).</figcaption></figure></div><p id="c092" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我的例子中，似乎有额外的图像(大约 3k，整个数据集的 20%)没有被策划，并没有完全阻止生成器探索数据中有趣的模式(左上图)。然而，有一些随机的图像(用红色突出显示)明显没有非洲面具的预期纹理或图案。为了进一步探索这个问题，我收集了 29k 张“茶杯”图片的数据集，并训练了一个模型。下图显示了结果(训练至 25k 步和 32k 步)。虽然有一些看起来特别有趣的图像，但仍然不清楚我们是否会通过管理数据集做得更好。</p><blockquote class="mt mu mv"><p id="3abf" class="kv kw ms kx b ky kz jr la lb lc ju ld mw lf lg lh mx lj lk ll my ln lo lp lq ij bi translated">思考:如果你的目标不是“完美”地表示数据集中的模式，那么手工处理图像不会给你带来更多的价值。</p></blockquote><div class="kg kh ki kj gt ab cb"><figure class="nl kk nw nn no np nq paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/4cd3390f678b34fecadab0bf8598e367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*RPblov_ECXo2ZznN6htEWg.png"/></div></figure><figure class="nl kk nw nn no np nq paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/5212b5eccfb7b0706ea767ec1f818c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*RGYExUy2l3sBIFM6COtbnw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk ns di nt nu">Teacups generated from an “uncurated” dataset (29k) of teacups. <strong class="bd nv">LEFT</strong>: Trained to 25k steps. <strong class="bd nv">RIGHT</strong>: Trained to 32k steps.</figcaption></figure></div><h2 id="6b35" class="mz lu iq bd lv na nb dn lz nc nd dp md le ne nf mf li ng nh mh lm ni nj mj nk bi translated">甘人学到了什么？</h2><p id="cee4" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">这可能是这个实验中最有趣的方面。它聚焦于一个经过充分研究的问题“GAN 实际上学习什么？”，但更多的是从定性的角度。一种方法是观察生成的图像和数据集中的图像之间的相似性。在他们的论文中，Karras 等人[2]使用这些图像之间的像素的 L1 距离来计算这一点。这种方法被批评为过于简单(没有利用任何图像内容的知识)。为了计算相似性，使用预训练的 CNN ( <a class="ae lr" href="http://&lt;strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;VGG16&lt;/a&gt;" rel="noopener ugc nofollow" target="_blank"> VGG16 </a>，Simonyan 和 Zisserman 2016)作为特征提取器。然后计算相似性，作为生成的图像和训练数据集中的图像之间的余弦距离的度量。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="93e7" class="mz lu iq ny b gy oc od l oe of">def compute_cosine_distance_matrix(feat, feat_matrix):</span><span id="9427" class="mz lu iq ny b gy og od l oe of">    cosine_dist_matrix = scipy.spatial.distance.cdist(</span><span id="d279" class="mz lu iq ny b gy og od l oe of">    feat_matrix, feat.reshape(1, -1), 'cosine').reshape(-1, 1)</span><span id="b20a" class="mz lu iq ny b gy og od l oe of">    return 1 - cosine_dist_matrix</span></pre><p id="5888" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我尝试使用 VGG 的各种<code class="fe oh oi oj ny b">maxpool</code>图层提取特征进行相似性计算，发现早期和后期图层在识别视觉上看起来相似的图像方面表现最佳。我创建了一个<a class="ae lr" href="https://victordibia.github.io/cocoafrica/#masks" rel="noopener ugc nofollow" target="_blank">演示</a>，它根据 VGG 网络中两个层(早期层和后期层(block5_pool))的特征，显示了每个生成图像的数据集中前 24 个最相似的图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/58527960e2a141822288207605045938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Bz1a_USqW8LGlM79xyS3rQ.gif"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">An interface that displays images similar to the generated images from the GAN. Similarity is computed as cosine distance of features extracted from layers of a pretrained CNN (VGG). Qualitative inspection suggests using an features from an early layer (block1_pool) finds images with similar low level features (color, pixels, texture), while later layers find images with similar high level features (shapes and and objects) etc. Check out the demo on this here — <a class="ae lr" href="https://victordibia.github.io/cocoafrica/#masks" rel="noopener ugc nofollow" target="_blank">https://victordibia.github.io/cocoafrica/#masks</a></figcaption></figure><p id="21b9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">初步结果显示，GAN 生成的图像与数据集中它们最近的亲戚非常不同。没有明显的副本。这种解释是主观的，受限于生成图像的模糊性质，鼓励读者<a class="ae lr" href="https://victordibia.github.io/cocoafrica/#masks" rel="noopener ugc nofollow" target="_blank">观看演示</a>以形成更好的意见。</p><p id="9a06" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对生成图像的交互式检查还提供了查看数据集中一些有趣模式的机会。一些包括具有特定侧向取向的面具、具有毛发或毛发状突起的面具、长方形面具。相似性信息也可以是一种(弱监督的)技术，用于管理图像数据集，启用数据集的特定“模式”并发现<em class="ms">异常值</em>即坏图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/385f5e91db03e7ffb5b94277b11b33a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SH5Tk6Q8gkW9arvw-Nk0Ag.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Some of the images generated by the GAN are similar to some observable “modes” within the mask dataset. Some of these modes might reflect artistic style and origins of the masks themselves (yet to be explored).</figcaption></figure><h1 id="c923" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">摘要</h1><ul class=""><li id="c520" class="ol om iq kx b ky ml lb mm le on li oo lm op lq oq or os ot bi translated">与 TPU 一起工作比我最初想象的要容易。Estimator API 设计得非常好(封装了训练循环)；已阅读并遵循现有评估程序 API 代码。对于已经熟悉估计器(或其他支持标准化训练 NN 训练循环的框架)的个人来说，完成工作将会很容易。</li><li id="4f0d" class="ol om iq kx b ky ou lb ov le ow li ox lm oy lq oq or os ot bi translated">在这个项目中，一个麻烦的部分是对我的数据转换脚本(从原始图像到训练中摄取的 TFrecords)进行故障排除。我扩展了示例 DCGAN 代码，它碰巧期望一个形状为<strong class="kx ir">【通道，高度，宽度】</strong>的图像张量，而不是我更习惯的<strong class="kx ir">【高度，宽度，通道】</strong>。这并没有抛出任何错误——只是意味着我运行的前 30 个左右的实验产生了绝对嘈杂的废话。TFrecords 非常优秀有很多原因——然而，如果 Tensorflow 能够更好地标准化这一过程，它将会非常有用(不确定 TF 是否已经有了类似 torchvision <a class="ae lr" href="https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder" rel="noopener ugc nofollow" target="_blank"> imageFolder </a>的东西)。</li></ul><blockquote class="mt mu mv"><p id="c62e" class="kv kw ms kx b ky kz jr la lb lc ju ld mw lf lg lh mx lj lk ll my ln lo lp lq ij bi translated">提示:为了确保万无一失，要经常检查你的 TFrecords 中进出的东西。</p></blockquote><ul class=""><li id="2d4b" class="ol om iq kx b ky kz lb lc le oz li pa lm pb lq oq or os ot bi translated">组装一个干净的数据集是实际使用 GANs 进行有意义的艺术探索的一个非常重要的部分。这为支持数据集管理的工具提供了一个案例——可能以半监督的方式，我们标记一个小样本，在这个样本上训练一个模型，并使用它来“管理”其余的数据。</li><li id="0947" class="ol om iq kx b ky ou lb ov le ow li ox lm oy lq oq or os ot bi translated">甘斯可以用于艺术探索。在这种情况下，虽然一些生成的图像不是完整的蒙版，但它们擅长捕捉非洲艺术的<em class="ms">纹理</em>或<em class="ms">感觉</em>。例如，我给一位同事看，他们提到生成的图像具有“部落感觉”。</li><li id="5769" class="ol om iq kx b ky ou lb ov le ow li ox lm oy lq oq or os ot bi translated">GANs 领域的许多现有工作集中于生成“真实”图像。虽然这非常有用(事实上，这是 GANs 等应用程序所希望的，因为<a class="ae lr" href="https://arxiv.org/abs/1809.00219" rel="noopener ugc nofollow" target="_blank">超分辨率</a>)，但了解<strong class="kx ir"> <em class="ms">有多少来自训练数据的</em> </strong>信息被合并，<strong class="kx ir"> <em class="ms">它们如何被组合</em> </strong>以生成新图像，以及这些图像是否真的新颖也很重要。定性<em class="ms">检验</em>实验(真实世界数据集)可以提供这方面的见解。</li><li id="f553" class="ol om iq kx b ky ou lb ov le ow li ox lm oy lq oq or os ot bi translated">预训练模型作为这些检查/相似性实验的特征提取器工作良好(与像素中的简单 L1 距离相反 Karras 等人 2017 年[2])。早期的图层允许我们根据线条和纹理等低级特征来检查相似性，而后期的图层则侧重于形状等高级特征。</li></ul><h1 id="7a8e" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">后续步骤</h1><p id="cab0" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">在做这个项目的时候，我确实对 GANs 有所了解；非常感谢谷歌使 TPU 可用。一如既往，未来工作的机会数不胜数。我正在考虑的一些方法包括——扩展非洲面具数据集，用条件化的 GANs 做实验(学到的特征如何与艺术属性相对应？)和其他用于真实/更高分辨率图像生成的架构。</p><p id="69d0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">知道下一步该做什么吗？你可以随时联系 Twitter、T2、Github 或 T4 的 Linkedin。</p><h1 id="9d98" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">参考</h1><p id="d899" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">[1] A .拉德福德，l .梅斯，s .钦塔拉，“深度卷积生成对抗网络的无监督表示学习”，<em class="ms">arxiv 1511.06434【cs】</em>，第 1–16 页，2016。</p><p id="2919" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] T. Karras、T. Aila、S. Laine 和 J. Lehtinen，“为提高质量、稳定性和变化性而逐步种植 gan”，<em class="ms"> CoRR </em>，第 abs/1710.1 卷，2017 年 10 月。</p></div></div>    
</body>
</html>