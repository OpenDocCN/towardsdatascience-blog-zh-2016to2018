<html>
<head>
<title>Is Innovation in Advanced AI the Apocalypse?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高级人工智能的创新是天启吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/is-innovation-in-advanced-ai-the-apocalypse-adcada6a5518?source=collection_archive---------7-----------------------#2017-04-25">https://towardsdatascience.com/is-innovation-in-advanced-ai-the-apocalypse-adcada6a5518?source=collection_archive---------7-----------------------#2017-04-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c3fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你设想未来，它是暗淡的。世界正面临巨大的政治、经济和环境压力。真的很难知道最怕什么。甚至人类的存在都是不确定的。威胁来自许多潜在的方向:<strong class="jp ir">全球变暖，小行星撞击，一种新的疾病，或者机器把一切都变成灰尘。</strong></p><p id="d7a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">人工智能是另一个巨大的威胁。<em class="kl">“全人工智能的发展可能会导致人类的终结……它会自己起飞，并以越来越快的速度重新设计自己。人类受到缓慢的生物进化的限制，无法竞争，并将被取代”</em>告诉<a class="ae km" href="http://www.bbc.com/news/technology-37713629" rel="noopener ugc nofollow" target="_blank">斯蒂芬·霍金接受BBC </a>采访。去年，他补充道<em class="kl">“人工智能可能是发生在人类身上最好或最糟糕的事情”。</em></p><p id="8c04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，包括比尔盖茨、埃隆马斯克和史蒂夫沃兹尼亚克在内的科技巨头的高管也对人工智能做出了类似的预测。然而，数十亿美元投资于人工智能研究。并且取得了巨大的进步。2016年3月，“有史以来最不可思议的比赛之一”，<a class="ae km" href="https://www.scientificamerican.com/article/how-the-computer-beat-the-go-master/" rel="noopener ugc nofollow" target="_blank"> AlphaGo </a>程序在最后一场比赛中战胜韩国围棋大师李·塞多尔，以4比1赢得系列赛。在许多其他领域，从在地面上驾驶汽车到赢得空中格斗，计算机开始取代人类。</p><p id="0e06" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">霍金的恐惧围绕着技术奇点的概念。这是机器智能开始起飞的时间点，一个新的更聪明的物种开始居住在地球上。我们可以将技术奇点的概念追溯到许多不同的思想家，包括计算机的创始人之一约翰·冯·诺依曼和科幻小说作家弗诺·文奇。这个想法与人工智能本身的研究年代大致相同。1958年，数学家斯塔尼斯瓦夫·乌拉姆写了一篇赞颂冯·诺依曼的文章，其中他回忆道:“一次谈话集中在技术的不断加速进步和人类生活方式的变化上，这似乎接近了某种本质上的奇点……超过这一点，我们所知道的人类事务就无法继续下去了”(《美国数学学会公报》,第64卷，第1页)。</p><p id="c060" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最近，雷·库兹韦尔(Ray Kurzweil)和尼克·博斯特罗姆(Nick Bostrom )提出了技术奇点的概念，他们预测奇点将在2045年左右发生，并就其后果写了一本畅销书。有几个理由担心机器在智力上超越我们。人类之所以成为这个星球上的主导物种，很大程度上是因为我们非常聪明。许多动物比我们更大、更快或更强壮。但是我们用我们的智慧发明了工具、农业和令人惊叹的技术，如蒸汽机、电动机和智能手机。这些改变了我们的生活，让我们能够主宰这个星球。</p><p id="8e39" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，毫不奇怪，会思考的机器——甚至可能比我们思考得更好——可能会篡夺我们的权利。正如大象、海豚和熊猫依靠我们的善意才能继续生存；反过来，我们的命运可能取决于这些高级思维机器的决定。</p><p id="2354" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">智能爆炸的想法，即机器递归地提高它们的智能，从而迅速超过人类的智能，并不是一个特别疯狂的想法。计算领域从许多类似的指数趋势中获益匪浅。摩尔定律预测，集成电路上的晶体管数量每两年就会翻一番，几十年来几乎都是如此。因此，假设人工智能也将经历指数增长并不是不合理的。</p><p id="94e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">就像我在人工智能领域的许多熟人一样，我预测，人工智能距离实现超人智能只有30或40年的时间。但有几个强有力的理由说明技术奇点不可能出现。</p><h2 id="74bb" class="kn ko iq bd kp kq kr dn ks kt ku dp kv jy kw kx ky kc kz la lb kg lc ld le lf bi translated"><strong class="ak">“智力极限”的争论</strong></h2><p id="1863" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy li ka kb kc lj ke kf kg lk ki kj kk ij bi translated">宇宙中有许多基本的限制。有些是物理上的:你无法加速超过光速，无法完全准确地知道位置和动量，也无法知道放射性原子何时会衰变。我们建造的任何思维机器都会受到这些物理定律的限制。当然，如果那台机器本质上是电子的，甚至是量子的，那么这些极限很可能会超出我们人类大脑的生物和化学极限。然而，人工智能很可能会遇到一些根本性的限制。其中一些可能是由于自然界固有的不确定性。无论我们多么努力地思考一个问题，我们决策的质量都是有限的。在预测下一次欧洲百万乐透的结果方面，即使是超人的智慧也不会比你强多少。</p><h2 id="7b82" class="kn ko iq bd kp kq kr dn ks kt ku dp kv jy kw kx ky kc kz la lb kg lc ld le lf bi translated"><strong class="ak">关于“思维敏捷的狗”的争论</strong></h2><p id="8545" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy li ka kb kc lj ke kf kg lk ki kj kk ij bi translated">硅比我们大脑的湿件具有显著的速度优势，根据摩尔定律，这种优势大约每两年翻一倍。但是速度本身并不能增加智力。即使我能让我的狗思考得更快，它仍然不太可能下棋。它没有必要的心理结构、语言和抽象概念。史蒂芬·平克雄辩地阐述了这一观点:“纯粹的处理能力并不能神奇地解决你所有的问题。”</p><p id="5fd1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">智慧不仅仅是比别人思考问题更快或更久。当然，摩尔定律帮助了AI。我们现在学得更快，提供了更大的数据集。更快的计算机肯定会帮助我们建立人工智能。但是，至少对人类来说，智力取决于许多其他因素，包括多年的经验和训练。我们还不清楚是否可以简单地通过提高时钟速度或增加更多的内存来缩短这一过程。</p><h2 id="97be" class="kn ko iq bd kp kq kr dn ks kt ku dp kv jy kw kx ky kc kz la lb kg lc ld le lf bi translated"><strong class="ak">“计算复杂性”的争论</strong></h2><p id="dd80" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy li ka kb kc lj ke kf kg lk ki kj kk ij bi translated">最后，计算机科学已经有了解决不同问题的难度的成熟理论。有许多计算问题，即使是指数级的改进也不足以帮助我们实际解决它们。计算机无法分析某些代码并确定它是否会停止——这就是“停止问题”。计算和人工智能之父艾伦·图灵(Alan Turing)著名地证明了这样一个问题通常是不可计算的，无论我们让计算机分析代码有多快或多聪明。转而使用量子计算机等其他类型的设备会有所帮助。但是这些只能提供超越经典计算机的指数级改进，不足以解决像图灵停顿问题这样的问题。有假想的超级计算机可能会突破这种计算障碍。然而，这种装置是否存在仍有争议。</p><h2 id="67e0" class="kn ko iq bd kp kq kr dn ks kt ku dp kv jy kw kx ky kc kz la lb kg lc ld le lf bi translated"><strong class="ak">未来</strong></h2><p id="992b" class="pw-post-body-paragraph jn jo iq jp b jq lg js jt ju lh jw jx jy li ka kb kc lj ke kf kg lk ki kj kk ij bi translated">因此，有很多原因可以解释为什么我们可能永远不会见证一个技术奇点。但是，即使没有智能爆炸，我们也可能最终拥有表现出超人智能的机器。我们可能不得不自己痛苦地编写大部分程序。如果是这样的话，人工智能对我们的经济和社会的影响可能不会像霍金等人担心的那样迅速发生。然而，我们应该开始为这种影响做准备。</p><p id="9d9c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">即使没有技术奇点，人工智能也可能对工作性质产生巨大影响。许多工作，像出租车和卡车司机，可能会在未来十年或二十年消失。这将进一步增加我们今天在社会上看到的不平等。即使相当有限的人工智能也可能对战争的性质产生很大的影响。机器人将使战争工业化，降低战争壁垒，破坏当前的世界秩序。他们将被恐怖分子和流氓国家用来对付我们。如果我们不想和终结者同归于尽，我们最好尽快在战场上禁止机器人。如果我们做对了，人工智能将帮助我们变得更健康、更富有、更快乐。如果我们弄错了，人工智能很可能是我们犯过的最严重的错误之一。</p><p id="708e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">跟着我:</em> </strong></p><p id="a09c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">推特</strong> : <a class="ae km" href="http://twitter.com/Deepakv_raj" rel="noopener ugc nofollow" target="_blank"> @Deepakv_raj </a></p><p id="18a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">脸书</strong> : <a class="ae km" href="http://www.facebook.com/deepakamirraj" rel="noopener ugc nofollow" target="_blank"> deepakamirraj </a></p><p id="6385" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">LinkedIn</strong>:<a class="ae km" href="http://linkedin.com/in/deepakaraj" rel="noopener ugc nofollow" target="_blank">deepakaraj</a></p></div></div>    
</body>
</html>