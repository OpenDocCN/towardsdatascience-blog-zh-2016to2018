<html>
<head>
<title>Summary of Tabular Methods in Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习中的表格方法综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/summary-of-tabular-methods-in-reinforcement-learning-39d653e904af?source=collection_archive---------7-----------------------#2018-12-16">https://towardsdatascience.com/summary-of-tabular-methods-in-reinforcement-learning-39d653e904af?source=collection_archive---------7-----------------------#2018-12-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2c1c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">强化学习中不同列表方法的比较</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/146c3b782069a138174177721b61d9e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*798dyniH_71ATTSAGx7img.jpeg"/></div></div></figure><p id="d7ec" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">更新</strong>:学习和练习强化学习的最好方式是去 http://rl-lab.com<a class="ae lq" href="http://rl-lab.com" rel="noopener ugc nofollow" target="_blank"/></p><h1 id="b0b1" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">介绍</h1><p id="f31e" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">表格方法指的是这样的问题，其中状态和动作空间足够小，以至于近似值函数可以表示为数组和表格。</p><h1 id="4c41" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">问题是</h1><p id="bd1d" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">强化学习的目的是找到以下方程的解，称为贝尔曼方程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/b96de081d64ae8db931d1c08977ddc3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V8nDsacwXuatks4alA4_zw.png"/></div></div></figure><p id="6e46" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们解贝尔曼方程的意思是找到使状态价值函数最大化的最优策略。</p><p id="9d5b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于很难得到解析解，我们使用迭代方法来计算最优策略。最佳状态和动作值函数表示如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/1863720500a3fc4204e5244e78501526.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*fhkjiUDB9VcPiNHomp9Xrw.png"/></div></figure><h1 id="9ff1" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">动态规划</h1><p id="6c36" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">动态规划是一种方法，其中通过将周围状态的值作为输入来计算状态下的每个值(不管这些值是否准确)。一旦计算了一个状态的值，我们就转移到另一个状态，并重复相同的过程(考虑在先前状态中计算的任何新值)。</p><p id="b5c8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个过程重复足够多次，直到每个状态的变化小于我们定义的某个极限。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mq"><img src="../Images/b10fa1cfbb2a678e884af4139003336e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i0HUJ5lD7IyrizPrcuZpdw.png"/></div></div></figure><p id="f113" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">更多关于 DP 学习的内容可以在文章《<a class="ae lq" href="https://medium.com/@zsalloum/dynamic-programming-in-reinforcement-learning-the-easy-way-359c7791d0ac" rel="noopener">强化学习中的动态编程，简易方法</a>》中找到</p><h2 id="d4db" class="mr ls it bd lt ms mt dn lx mu mv dp mb ld mw mx md lh my mz mf ll na nb mh nc bi translated">赞成的意见</h2><p id="6a7b" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">DP 是有效的，在大多数情况下，它能在多项式时间内找到最优策略。</p><p id="80f7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">DP 保证找到最优策略。</p><h2 id="d433" class="mr ls it bd lt ms mt dn lx mu mv dp mb ld mw mx md lh my mz mf ll na nb mh nc bi translated">骗局</h2><p id="8e64" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">DP 不适合处理有数百万或更多状态的大问题。</p><p id="ec3a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">DP 需要转移概率矩阵的知识，然而这对于许多问题来说是不现实的要求。</p><h1 id="6da8" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">蒙特卡洛</h1><p id="542d" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">在蒙特卡罗(MC)中，我们玩游戏的一集，在各种状态中逐一移动，直到结束，记录我们遇到的状态、动作和奖励，然后计算我们经过的每个状态的 V(s)和 Q(s)。我们通过播放更多的剧集来重复这个过程，在每一集之后，我们得到状态、动作和奖励，并且我们对发现的 V(s)和 Q(s)的值进行平均。</p><p id="1548" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">MC 预测算法包括播放尽可能多的剧集，并在每集之后计算我们已经经过的状态的值，然后将这些结果与这些状态的当前值进行平均。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/5adfcb92c1e6a4f6fd267d2bd1ead12f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z-o_hdZuGaTqZozBdbZzVQ.png"/></div></div></figure><p id="cbd5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">保单首次就诊 MC 控制旨在找到最佳保单。它会播放剧集，但会跟踪每个状态下使用的每个动作。这样，就有可能知道什么动作产生了最佳 Q 值。<br/>最后，将具有最大 Q 值的行动添加到最佳策略中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/da2eea4e33b793a95a7ead301b016556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N0UxEewiXV01qF2SVSjL2g.png"/></div></div></figure><p id="47d1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">蒙特卡罗的详细解释可以在文章《<a class="ae lq" href="https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511" rel="noopener">强化学习中的蒙特卡罗，简易方法</a>》中找到</p><h2 id="ea97" class="mr ls it bd lt ms mt dn lx mu mv dp mb ld mw mx md lh my mz mf ll na nb mh nc bi translated">赞成的意见</h2><p id="98ff" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">MC 可以用来直接从与环境的交互中学习最优行为。它不需要环境动力学的模型。</p><p id="4a2b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">MC 可用于模拟或样本模型。</p><p id="c462" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">MC 可用于聚焦于一个特别感兴趣的区域，并且被准确地评估，而不必评估状态集的其余部分。</p><h2 id="10ca" class="mr ls it bd lt ms mt dn lx mu mv dp mb ld mw mx md lh my mz mf ll na nb mh nc bi translated">骗局</h2><p id="56c3" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">MC 只适用于偶发(终止)环境。它不适用于没有终止状态的环境。</p><p id="9173" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">MC 必须有一个完整的情节，它没有引导，这意味着它没有给出其他状态的估计。</p><p id="08f6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">MC 必须等到一集结束才知道回归。对于长时间发作的问题，这将变得太慢。</p><h1 id="9d47" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">时间差异</h1><p id="ce7d" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">TD 可以看作是 DP 和 MC 方法的融合。它播放剧集但不必等到结尾才知道回报。它根据对其他状态的估计来计算当前状态的值。<br/> TD(0)是指只向前看一步的事实，然后计算当前状态值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/50e4100b9a4dc622406fab237f362407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZQOYa442n4FzLVv5233zzw.png"/></div></div></figure><p id="6e56" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了找到最优策略，TD 提供了不同的方法，其中之一是 SARSA。<br/> SARSA 包括对状态 S 采取行动 A，注意奖励和下一个状态 S’，然后从状态 S’中选择行动 A’，然后使用所有这些信息更新 Q(S，A)，然后移动到 S’并执行之前选择的行动 A’。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/38e4517f7e2426b6f70c3bed72e798c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SUbdS2KYOTKUU35Hebpi7g.png"/></div></div></figure><p id="deaf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Q-learning 是寻找最优策略的另一种方法。<br/>像 SARSA 一样，它对状态 S 采取行动 A，注意奖励和下一个状态 S’，然后与 SARSA 不同，它选择状态 S’中的最大 Q 值，然后使用所有这些信息来更新 Q(S，A)，然后移动到 S’并执行ε贪婪行动，这不一定导致采取在状态 S’中具有最大 Q 值的行动。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/95204a9a9335ebdd498b43a81a09699f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d203n7cIso731qhJ7eOAkg.png"/></div></div></figure><p id="d6cc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">双 Q 学习是一种解决 Q 学习中特定问题的算法，特别是当 Q 学习可以基于一些积极的奖励被诱骗采取坏的行动，而这个行动的预期奖励肯定是负的。<br/>它通过维护两个 Q 值列表来做到这一点，每个列表从另一个列表更新自身。简而言之，它在一个列表中找到最大化 Q 值的动作，但是它不是使用这个 Q 值，而是使用这个动作从另一个列表中获得 Q 值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/f11bce3b81d631c58a044d383da97fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jyNCFhGu6gjTHnVVW7KOSw.png"/></div></div></figure><p id="38e3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">要了解更多关于 TD 学习的细节，请查看文章“<a class="ae lq" rel="noopener" target="_blank" href="/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce"> TD 在强化学习中，简单的方法</a>”。</p><p id="a04f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">关于双 Q 学习的更多细节可以在“<a class="ae lq" rel="noopener" target="_blank" href="/double-q-learning-the-easy-way-a924c4085ec3">双 Q-学习简单方法</a>”中找到</p><h2 id="fbbd" class="mr ls it bd lt ms mt dn lx mu mv dp mb ld mw mx md lh my mz mf ll na nb mh nc bi translated">赞成的意见</h2><p id="85ff" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">TD 不需要像 DP 中那样知道转移概率矩阵。</p><p id="e9c8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">TD 不需要等到剧集结束才知道回报，它增量更新状态值和动作值。</p><h2 id="ed6a" class="mr ls it bd lt ms mt dn lx mu mv dp mb ld mw mx md lh my mz mf ll na nb mh nc bi translated">骗局</h2><p id="c26f" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">SARSA 可能会陷入局部最小值。</p><p id="d185" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Q-Learning 在一些随机环境中表现不佳。</p><h1 id="38d8" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">参考</h1><p id="c46c" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">算法取自萨顿和巴尔托。强化学习:导论。</p></div></div>    
</body>
</html>