<html>
<head>
<title>[ Paper Summary ] An Introduction to Independent Component Analysis: InfoMax and FastICA algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">【论文摘要】独立分量分析导论:InfoMax 和 FastICA 算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-summary-an-introduction-to-independent-component-analysis-infomax-and-fastica-algorithms-7b44d18ab393?source=collection_archive---------9-----------------------#2018-09-05">https://towardsdatascience.com/paper-summary-an-introduction-to-independent-component-analysis-infomax-and-fastica-algorithms-7b44d18ab393?source=collection_archive---------9-----------------------#2018-09-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/164b566a4698f16a6fbbca1811bd08ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/1*3TRUUfPjhAQ1btjcOAnpkA.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/imadeit-qKltgF7Aw515K" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><blockquote class="jz ka kb"><p id="6326" class="kc kd ke kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">请注意，这篇帖子是给未来的自己看的，用来回顾和复习这篇论文上的材料。</strong></p></blockquote></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><figure class="li lj lk ll gt jr"><div class="bz fp l di"><div class="lm ln l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="http://mail.tqmp.org/RegularArticles/vol06-1/p031/p031.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="b9f3" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kf ir">摘要</strong></p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/3cfce7ade4c8d4c54b10e2cfc907234e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d0C3NmImsDyDt4-hh0GsIw.png"/></div></div></figure><p id="6257" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">本文介绍了不同于主成分分析的独立成分分析。其中优化了给定数据统计独立性。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="00c1" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kf ir">简介</strong></p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/e2e0c5acd791bbfda1af7b967f8dfaa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cgn5dcU5UmOlosd7cxTdGA.png"/></div></div></figure><p id="9bef" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">现在，进行某种数据分析是非常容易的，并且有各种各样的方法，如 pca 或因子分析。与此相关的一个重要概念是给定数据的分布，特别是分布的正态性。原因是因为这决定了某些方法在分解某些数据时是否会成功。如上所述，取决于方法和假设的分布，一些方法将成功地正确识别独立信号，而其他方法则失败。(注意 ICA 也有其自身的局限性，与排列或符号有关，但我们也有一种称为独立向量分析的方法。)</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="d8e4" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kf ir">独立成分分析的理论基础</strong></p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lx"><img src="../Images/8f9ca9be56bb0f676ea1fd944c3eb6f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhEQJMFmU_MO9weVmFL6fQ.png"/></div></div></figure><p id="5aa7" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">在这一节中，作者简要讨论了 ICA 的基本原理，例如寻找混合矩阵的逆的非混合矩阵。作者提出了关于 ICA 的 5 个假设。<br/> 1)来源在统计上是独立的。<br/> 2)混合矩阵为正方形，满秩。<br/> 3)不应有外部噪声<br/> 4)数据为零均值<br/> 5)源信号不得具有高斯概率密度函数。(至少其中一个。)</p><p id="fb6e" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><em class="ke">统计独立性</em></p><p id="e2c2" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">当我们有两个随机变量 x1 和 x2 时，我们可以通过下面的等式定义这两个变量之间的不相关性。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ly"><img src="../Images/914940fdd9f738af2caa8b99d4e8bbba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7hysrlM7ysapu_fXAlRZyA.png"/></div></div></figure><p id="8359" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">另一方面，我们可以将统计独立性定义为下面的等式。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lz"><img src="../Images/9dbfe9a4f1d40518df9244067dde24bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uReqsUrU_Ojf293KCssdPQ.png"/></div></div></figure><p id="6ce9" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">并且在特定情况下，当联合 pdf 是高斯不相关时，不相关等同于独立。作者介绍了两种度量独立性的方法，即最小化互信息法和最大化非高斯性法。(这是相同的解决方案。)</p><p id="da2f" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><em class="ke">互信息最小化</em></p><p id="ab0e" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">当我们有两个变量 X 和 Y 时，互信息可以被视为在观察到 Y 之后关于变量 X 的不确定性的减少。因此，通过具有寻求最小化互信息的算法，我们正在搜索最大程度独立的分量(潜在变量)。(InfoMax 是算法的名字)。</p><p id="bca3" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><em class="ke">非高斯性最大化</em></p><p id="8466" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">当我们有两个变量 X 和 Y 时，我们可以通过迫使它们尽可能远离正态分布来实现独立性。为了做到这一点，我们用负熵来度量非高斯性。(这是高斯性的一个积极量度。).并且我们计算近似的负熵而不是直接计算。(FastICA 是算法的名字。)</p><p id="1919" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">我不打算写任何关于“如何使用 ICA 包”的东西，因为它只是使用高级 api。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="3c13" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kf ir">示例/讨论</strong></p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ma"><img src="../Images/003015e4b4c372f4458356b33bb7762b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OsAGk2isyEXwYqjMf6wynQ.png"/></div></div></figure><p id="8843" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">本文作者将三幅图像混合，采用不同的独立分量分析方法提取原始信号。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mb"><img src="../Images/d3da389d1d8485243eeeb052c1d30925.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZR2CL2pDw7Fm5zxiQU36xw.png"/></div></div></figure><p id="21d2" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">如上所述，我们可以观察到 ICA 方法能够比 PCA 更清晰地提取信号(原始图像)。此外，在执行 ICA 时，必须考虑多个概念，比如白化数据。FastICA 和 InfoMax 都很健壮，但是必须事先提供正确的分布类型。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="d65b" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kf ir">参考</strong></p><ol class=""><li id="1f7c" class="mc md iq kf b kg kh kk kl lo me lp mf lq mg la mh mi mj mk bi translated">(2018).Mail.tqmp.org。检索于 2018 年 9 月 4 日，来自 http://mail.tqmp.org/RegularArticles/vol06-1/p031/p031.pdf<a class="ae jy" href="http://mail.tqmp.org/RegularArticles/vol06-1/p031/p031.pdf" rel="noopener ugc nofollow" target="_blank"/></li></ol></div></div>    
</body>
</html>