<html>
<head>
<title>Machine Learning, NLP: Text Classification using scikit-learn, python and NLTK.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习，NLP:使用 scikit-learn，python 和 NLTK 的文本分类。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a?source=collection_archive---------0-----------------------#2017-07-23">https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a?source=collection_archive---------0-----------------------#2017-07-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/283fe77769dcbf7f0efc95f3cbf577f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ljCBykAJUnvaZcuPYwm4_A.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Text Classification</figcaption></figure><p id="a995" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">最新更新:</strong> <br/>我已经把完整的代码(Python 和 Jupyter 笔记本)上传到 GitHub 上:<a class="ae kw" href="https://github.com/javedsha/text-classification" rel="noopener ugc nofollow" target="_blank">https://github.com/javedsha/text-classification</a></p><p id="cba3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">文档/文本分类</strong>是<em class="kx">监督的</em>机器学习(ML)中重要而典型的任务之一。为文档分配类别，文档可以是网页、图书馆书籍、媒体文章、图库等。有许多应用，例如垃圾邮件过滤、电子邮件路由、情感分析等。在这篇文章中，我想演示我们如何使用 python、scikit-learn 和一点 NLTK 来进行文本分类。</p><p id="1355" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="kx">免责声明</em> : </strong> <em class="kx">我是机器学习新手，也是博客新手(第一次)。因此，如果有任何错误，请让我知道。感谢所有反馈。</em></p><p id="9739" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们将分类问题分成以下步骤:</p><ol class=""><li id="6b01" class="ky kz iq ka b kb kc kf kg kj la kn lb kr lc kv ld le lf lg bi translated">先决条件和设置环境。</li><li id="cd4e" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">在 jupyter 中加载数据集。</li><li id="d09c" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">从文本文件中提取特征。</li><li id="d470" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">运行 ML 算法。</li><li id="9ce4" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">参数调整的网格搜索。</li><li id="61ec" class="ky kz iq ka b kb lh kf li kj lj kn lk kr ll kv ld le lf lg bi translated">有用的提示和一点 NLTK。</li></ol><h2 id="2871" class="lm ln iq bd lo lp lq dn lr ls lt dp lu kj lv lw lx kn ly lz ma kr mb mc md me bi translated"><strong class="ak">步骤 1:先决条件和设置环境</strong></h2><p id="74c8" class="pw-post-body-paragraph jy jz iq ka b kb mf kd ke kf mg kh ki kj mh kl km kn mi kp kq kr mj kt ku kv ij bi translated">遵循这个例子的先决条件是 python 版本<strong class="ka ir"> 2.7.3 </strong>和 jupyter notebook。你可以安装<a class="ae kw" href="https://www.continuum.io/downloads" rel="noopener ugc nofollow" target="_blank"><strong class="ka ir">anaconda</strong></a><strong class="ka ir">它会为你得到一切。此外，还需要一点 python 和 ML 基础知识，包括文本分类。在我们的例子中，我们将使用 scikit-learn (python)库。</strong></p><h2 id="6519" class="lm ln iq bd lo lp lq dn lr ls lt dp lu kj lv lw lx kn ly lz ma kr mb mc md me bi translated"><strong class="ak">第二步:</strong>在 jupyter 中加载数据集。</h2><p id="9319" class="pw-post-body-paragraph jy jz iq ka b kb mf kd ke kf mg kh ki kj mh kl km kn mi kp kq kr mj kt ku kv ij bi translated">本例中使用的数据集是著名的“20 新闻组”数据集。关于来自原<a class="ae kw" href="http://qwone.com/~jason/20Newsgroups/" rel="noopener ugc nofollow" target="_blank">网站</a>的数据:</p><blockquote class="mk ml mm"><p id="2916" class="jy jz kx ka b kb kc kd ke kf kg kh ki mn kk kl km mo ko kp kq mp ks kt ku kv ij bi translated">20 个新闻组数据集是大约 20，000 个新闻组文档的集合，平均分布在 20 个不同的新闻组中。据我所知，它最初是由 Ken Lang 收集的，可能是为了他的<a class="ae kw" href="http://qwone.com/~jason/20Newsgroups/lang95.bib" rel="noopener ugc nofollow" target="_blank">news weaver:Learning to filter net news</a>paper，尽管他没有明确提到这个收集。20 个新闻组集合已经成为机器学习技术的文本应用实验的流行数据集，例如文本分类和文本聚类。</p></blockquote><p id="75ef" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个数据集内置在 scikit 中，所以我们不需要显式下载它。</p><p id="cf3f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">I .在 windows 中打开命令提示符，键入“jupyter notebook”。这将在浏览器中打开笔记本，并为您启动一个会话。</p><p id="94e8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">二。选择新建&gt; Python 2。你可以给笔记本起个名字- <em class="kx">文本分类演示 1 </em></p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mq"><img src="../Images/a0da64046b12cb853ec73b4e8c443775.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*21FHNsyMi_HCZv25rkeemA.png"/></div></div></figure><p id="7132" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">三。加载数据集:(这可能需要几分钟，请耐心等待)</p><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="14e7" class="lm ln iq na b gy ne nf l ng nh">from sklearn.datasets import fetch_20newsgroups<br/>twenty_train = fetch_20newsgroups(subset='train', shuffle=True)</span></pre><p id="f394" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kx">注:以上，我们只是加载了</em> <strong class="ka ir"> <em class="kx">训练</em> </strong> <em class="kx">数据。我们将在后面的例子中单独加载测试数据。</em></p><p id="d185" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">四。您可以通过以下命令检查目标名称(类别)和一些数据文件。</p><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="a741" class="lm ln iq na b gy ne nf l ng nh">twenty_train.target_names #prints all the categories<br/>print("\n".join(twenty_train.data[0].split("\n")[:3])) #prints first line of the first data file</span></pre><h2 id="ae22" class="lm ln iq bd lo lp lq dn lr ls lt dp lu kj lv lw lx kn ly lz ma kr mb mc md me bi translated"><strong class="ak">第三步:从文本文件中提取特征。</strong></h2><p id="38f1" class="pw-post-body-paragraph jy jz iq ka b kb mf kd ke kf mg kh ki kj mh kl km kn mi kp kq kr mj kt ku kv ij bi translated">文本文件实际上是一系列单词(有序的)。为了运行机器学习算法，我们需要将文本文件转换成数字特征向量。我们将以使用<a class="ae kw" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank"> <strong class="ka ir">袋字</strong> </a>模型为例。简而言之，我们将每个文本文件分割成单词(英文按空格分割)，并计算每个单词在每个文档中出现的次数，最后给每个单词分配一个整数 id。<strong class="ka ir">我们字典中的每个唯一的单词都会对应一个特征(描述性特征)。</strong></p><p id="c01b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Scikit-learn 有一个高级组件，它将为用户“计数矢量器”创建特征矢量。更多关于它的<a class="ae kw" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="15ed" class="lm ln iq na b gy ne nf l ng nh">from sklearn.feature_extraction.text import CountVectorizer<br/>count_vect = CountVectorizer()<br/>X_train_counts = count_vect.fit_transform(twenty_train.data)<br/>X_train_counts.shape</span></pre><p id="279b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里通过做'<em class="kx">count _ vect . fit _ transform(twenty _ train . data)</em>'，我们正在学习词汇词典，它返回一个文档-术语矩阵。[n 个样本，n 个特征]。</p><p id="ab16" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">仅仅统计每个文档的字数有一个问题:它会给较长的文档比较短的文档更多的权重。为了避免这种情况，我们可以在每个文档中使用频率(<strong class="ka ir">TF-Term frequency</strong>)，即#count(word) / #Total words。</p><p id="f46c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> TF-IDF: </strong>最后，我们甚至可以降低更常见单词的权重，如(the，is，an 等。)出现在所有文档中。这被称为<strong class="ka ir"> TF-IDF，即术语频率乘以逆文档频率。</strong></p><p id="c202" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以使用下面一行代码实现这两个目标:</p><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="2196" class="lm ln iq na b gy ne nf l ng nh">from sklearn.feature_extraction.text import TfidfTransformer<br/>tfidf_transformer = TfidfTransformer()<br/>X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)<br/>X_train_tfidf.shape</span></pre><p id="6e48" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后一行将输出文档-术语矩阵的维度-&gt; (11314，130107)。</p><h2 id="fcf7" class="lm ln iq bd lo lp lq dn lr ls lt dp lu kj lv lw lx kn ly lz ma kr mb mc md me bi translated"><strong class="ak">步骤四。运行 ML 算法。</strong></h2><p id="bb19" class="pw-post-body-paragraph jy jz iq ka b kb mf kd ke kf mg kh ki kj mh kl km kn mi kp kq kr mj kt ku kv ij bi translated">有各种算法可用于文本分类。我们就从最简单的一个'<a class="ae kw" href="http://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯</a> (NB)' ( <em class="kx">不要觉得太幼稚！</em>😃)</p><p id="30ee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可以使用下面两行代码在 scikit 中轻松构建一个 NBclassifier:(注意——NB 有许多变体，但关于它们的讨论超出了范围)</p><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="8368" class="lm ln iq na b gy ne nf l ng nh">from sklearn.naive_bayes import MultinomialNB<br/>clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)</span></pre><p id="e520" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这将在我们提供的训练数据上训练 NB 分类器。</p><p id="37ef" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">构建管道:</strong>我们可以编写更少的代码，通过如下方式构建管道来完成上述所有工作:</p><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="9e73" class="lm ln iq na b gy ne nf l ng nh"><strong class="na ir">&gt;&gt;&gt; from</strong> <strong class="na ir">sklearn.pipeline</strong> <strong class="na ir">import</strong> Pipeline<br/><strong class="na ir">&gt;&gt;&gt; </strong>text_clf = Pipeline([('vect', CountVectorizer()),<br/><strong class="na ir">... </strong>                     ('tfidf', TfidfTransformer()),<br/><strong class="na ir">... </strong>                     ('clf', MultinomialNB()),<br/><strong class="na ir">... </strong>])</span><span id="2b91" class="lm ln iq na b gy ni nf l ng nh">text_clf = text_clf.fit(twenty_train.data, twenty_train.target)</span></pre><p id="3034" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kx">‘vect’、‘tfi df’和‘clf’这几个名字是随意取的，但以后会用到。</em></p><p id="e934" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">NB 分类器的性能:</strong>现在我们将在<strong class="ka ir">测试集</strong>上测试 NB 分类器的性能。</p><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="6984" class="lm ln iq na b gy ne nf l ng nh">import numpy as np<br/>twenty_test = fetch_20newsgroups(subset='test', shuffle=True)<br/>predicted = text_clf.predict(twenty_test.data)<br/>np.mean(predicted == twenty_test.target)</span></pre><p id="ff89" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们得到的准确率是<strong class="ka ir"> ~77.38% </strong>，对于 start 和一个幼稚的分类器来说已经不错了。还有，恭喜你！！！您现在已经成功地编写了一个文本分类算法👍</p><p id="ccd7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">支持向量机(SVM): </strong>让我们尝试使用不同的算法 SVM，看看我们是否能获得更好的性能。更多关于它的<a class="ae kw" href="http://scikit-learn.org/stable/modules/svm.html" rel="noopener ugc nofollow" target="_blank">在这里</a>。</p><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="6a55" class="lm ln iq na b gy ne nf l ng nh">&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier</span><span id="09b7" class="lm ln iq na b gy ni nf l ng nh">&gt;&gt;&gt; text_clf_svm = Pipeline([('vect', CountVectorizer()),<br/>...                      ('tfidf', TfidfTransformer()),<br/>...                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',<br/>...                                            alpha=1e-3, n_iter=5, random_state=42)),<br/>... ])</span><span id="c9dc" class="lm ln iq na b gy ni nf l ng nh">&gt;&gt;&gt; _ = text_clf_svm.fit(twenty_train.data, twenty_train.target)</span><span id="51e9" class="lm ln iq na b gy ni nf l ng nh">&gt;&gt;&gt; predicted_svm = text_clf_svm.predict(twenty_test.data)<br/>&gt;&gt;&gt; np.mean(predicted_svm == twenty_test.target)</span></pre><p id="297c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们得到的准确率是<strong class="ka ir"> ~82.38%。</strong>咦，好一点了👌</p><h2 id="4268" class="lm ln iq bd lo lp lq dn lr ls lt dp lu kj lv lw lx kn ly lz ma kr mb mc md me bi translated"><strong class="ak">第五步。网格搜索</strong></h2><p id="b317" class="pw-post-body-paragraph jy jz iq ka b kb mf kd ke kf mg kh ki kj mh kl km kn mi kp kq kr mj kt ku kv ij bi translated">几乎所有的分类器都有各种参数，可以调整这些参数以获得最佳性能。Scikit 提供了一个非常有用的工具‘GridSearchCV’。</p><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="83d9" class="lm ln iq na b gy ne nf l ng nh">&gt;&gt;&gt; from sklearn.model_selection import GridSearchCV<br/>&gt;&gt;&gt; parameters = {'vect__ngram_range': [(1, 1), (1, 2)],<br/>...               'tfidf__use_idf': (True, False),<br/>...               'clf__alpha': (1e-2, 1e-3),<br/>... }</span></pre><p id="b707" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里，我们创建了一个参数列表，我们希望对这些参数进行性能调优。所有参数名称都以分类器名称开始(记住我们给出的任意名称)。如 vect _ _ ngram _ range 在这里，我们告诉使用一元和二元，并选择一个是最佳的。</p><p id="37e6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我们通过传递分类器、参数和 n_jobs=-1 来创建网格搜索的实例，n _ jobs =-1 告诉使用来自用户机器的多个内核。</p><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="6f3d" class="lm ln iq na b gy ne nf l ng nh">gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)<br/>gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)</span></pre><p id="c3b4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这可能需要几分钟的时间，具体取决于机器配置。</p><p id="23f2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，要查看最佳平均分数和参数，请运行以下代码:</p><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="92cc" class="lm ln iq na b gy ne nf l ng nh">gs_clf.best_score_<br/>gs_clf.best_params_</span></pre><p id="b124" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">NB 分类器的准确率现在已经提高到<strong class="ka ir"> ~90.6% </strong> <strong class="ka ir">(不再那么幼稚了！😄)和对应的参数是{'clf__alpha': 0.01，' tfidf__use_idf': True，' vect__ngram_range': (1，2)}。</strong></p><p id="fbda" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">类似地，对于 SVM 分类器，我们使用下面的代码获得了提高的准确率<strong class="ka ir"> ~89.79% </strong>。<em class="kx">注意:您可以通过调整其他参数来进一步优化 SVM 分类器。这是留给你去探索更多。</em></p><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="7ca7" class="lm ln iq na b gy ne nf l ng nh">&gt;&gt;&gt; from sklearn.model_selection import GridSearchCV<br/>&gt;&gt;&gt; parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],<br/>...               'tfidf__use_idf': (True, False),<br/>...               'clf-svm__alpha': (1e-2, 1e-3),<br/>... }<br/>gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)<br/>gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)<br/>gs_clf_svm.best_score_<br/>gs_clf_svm.best_params_</span></pre><h2 id="1dd2" class="lm ln iq bd lo lp lq dn lr ls lt dp lu kj lv lw lx kn ly lz ma kr mb mc md me bi translated"><strong class="ak">第六步:</strong>有用的小技巧和一点 NLTK。</h2><ol class=""><li id="6331" class="ky kz iq ka b kb mf kf mg kj nj kn nk kr nl kv ld le lf lg bi translated"><strong class="ka ir">从数据中删除</strong> <a class="ae kw" href="https://en.wikipedia.org/wiki/Stop_words" rel="noopener ugc nofollow" target="_blank"> <strong class="ka ir">停止字</strong> </a> <strong class="ka ir"> : </strong> (the，then etc)。只有当停用词对潜在问题没有用时，才应该这样做。在大多数的文本分类问题中，这确实是没有用的。让我们看看删除停用词是否会提高准确性。按如下方式更新用于创建 CountVectorizer 对象的代码:</li></ol><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="1cc1" class="lm ln iq na b gy ne nf l ng nh">&gt;&gt;&gt; from sklearn.pipeline import Pipeline<br/>&gt;&gt;&gt; text_clf = Pipeline([('vect', CountVectorizer(<strong class="na ir">stop_words='english</strong>')),<br/>...                      ('tfidf', TfidfTransformer()),<br/>...                      ('clf', MultinomialNB()),<br/>... ])</span></pre><p id="2725" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是我们为 NB 分类器构建的管道。像以前一样运行剩余的步骤。这就把准确率从<strong class="ka ir"> 77.38%提高到了 81.69% </strong>(那太好了)。<em class="kx">你可以对 SVM 做同样的尝试，同时进行网格搜索。</em></p><p id="0108" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2.<strong class="ka ir"> FitPrior=False: </strong>当<a class="ae kw" href="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html" rel="noopener ugc nofollow" target="_blank">多项式 B </a>设置为 False 时，将使用统一的先验。这并没有多大帮助，但将准确率从 81.69%提高到了 82.14%(没有太大的提高)。试着看看这是否适用于你的数据集。</p><p id="3725" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 3。词干化:词干化是将词形变化(有时是派生的)的单词简化为词干、词根或词根形式的过程。例如，词干算法将单词“fishing”、“fished”和“fisher”简化为词根单词“fish”。</strong></p><p id="b57d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们需要 NLTK，它可以从<a class="ae kw" href="http://www.nltk.org/" rel="noopener ugc nofollow" target="_blank">安装到这里</a>。NLTK 附带了各种词干分析器(<em class="kx">关于词干分析器如何工作的细节超出了本文的范围</em>)，它们可以帮助将单词简化为它们的根形式。再次使用这个，如果它对你问题有意义的话。</p><p id="c8c8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面我用了雪球词干分析器，它对英语非常有效。</p><pre class="mr ms mt mu gt mz na nb nc aw nd bi"><span id="0dbf" class="lm ln iq na b gy ne nf l ng nh">import nltk<br/>nltk.download()</span><span id="1e58" class="lm ln iq na b gy ni nf l ng nh">from nltk.stem.snowball import SnowballStemmer<br/>stemmer = SnowballStemmer("english", ignore_stopwords=True)</span><span id="40d2" class="lm ln iq na b gy ni nf l ng nh">class StemmedCountVectorizer(CountVectorizer):<br/>    def build_analyzer(self):<br/>        analyzer = super(StemmedCountVectorizer, self).build_analyzer()<br/>        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])</span><span id="ac06" class="lm ln iq na b gy ni nf l ng nh">stemmed_count_vect = StemmedCountVectorizer(stop_words='english')</span><span id="0fba" class="lm ln iq na b gy ni nf l ng nh">text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),<br/>...                      ('tfidf', TfidfTransformer()),<br/>...                      ('mnb', MultinomialNB(fit_prior=False)),<br/>... ])</span><span id="441a" class="lm ln iq na b gy ni nf l ng nh">text_mnb_stemmed = text_mnb_stemmed.fit(twenty_train.data, twenty_train.target)</span><span id="873e" class="lm ln iq na b gy ni nf l ng nh">predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)</span><span id="a740" class="lm ln iq na b gy ni nf l ng nh">np.mean(predicted_mnb_stemmed == twenty_test.target)</span></pre><p id="7481" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们得到的词干准确率约为 81.67%。在我们使用 NB 分类器的情况下，边际改进。你也可以尝试 SVM 和其他算法。</p><p id="08ce" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">结论:</strong>我们已经学习了 NLP 中的经典问题，文本分类。我们学习了一些重要的概念，如单词袋、TF-IDF 和两个重要的算法 NB 和 SVM。我们看到，对于我们的数据集，两种算法在优化时几乎相等。有时，如果我们有足够的数据集，算法的选择几乎不会有什么不同。我们还了解了如何执行网格搜索以进行性能调优，并使用了 NLTK 词干法。您可以在数据集上使用这些代码，看看哪些算法最适合您。</p><p id="ea41" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">更新:</strong>如果有人尝试了不同的算法，请在评论区分享结果，对大家都会有用。</p><p id="148d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请让我知道是否有任何错误和反馈是受欢迎的✌️</p><p id="8752" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">推荐，评论，分享如果你喜欢这篇文章。</p><h2 id="1049" class="lm ln iq bd lo lp lq dn lr ls lt dp lu kj lv lw lx kn ly lz ma kr mb mc md me bi translated"><strong class="ak">参考文献:</strong></h2><p id="ced3" class="pw-post-body-paragraph jy jz iq ka b kb mf kd ke kf mg kh ki kj mh kl km kn mi kp kq kr mj kt ku kv ij bi translated">http://scikit-learn.org/(代码<a class="ae kw" href="http://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"/></p><p id="ca17" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">【http://qwone.com/~jason/20Newsgroups/ T4】(数据集)</p></div></div>    
</body>
</html>