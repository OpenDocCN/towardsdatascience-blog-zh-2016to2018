<html>
<head>
<title>How to do Deep Learning on Graphs with Graph Convolutional Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用图卷积网络在图上做深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780?source=collection_archive---------0-----------------------#2018-09-18">https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780?source=collection_archive---------0-----------------------#2018-09-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="66c6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">第 1 部分:图卷积网络的高级介绍</h2></div><p id="fd45" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图的机器学习是一项困难的任务，因为图的结构非常复杂，但也能提供丰富的信息。本文是关于如何使用图形卷积网络(GCNs)对图形进行深度学习的系列文章中的第一篇，图形卷积网络是一种强大的神经网络，旨在直接对图形进行处理并利用其结构信息。系列文章包括:</p><ol class=""><li id="81c9" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">图卷积网络的高级介绍(this)</li><li id="5c91" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae lp" rel="noopener" target="_blank" href="/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-62acf5b143d0">利用谱图卷积的半监督学习</a></li></ol><p id="e6ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本帖中，我将介绍 GCNs，并使用编码示例说明信息如何通过 GCN 的隐藏层传播。我们将了解 GCN 如何聚合来自之前图层的信息，以及这种机制如何在图表中生成有用的节点要素表示。</p><h1 id="a007" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">什么是图卷积网络？</h1><p id="13d3" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">GCNs 是一种非常强大的神经网络架构，用于在图上进行机器学习。事实上，它们是如此强大，甚至<em class="mn">一个随机启动的 2 层 GCN </em>也能产生网络中节点的有用特征表示。下图显示了由这种 GCN 产生的网络中每个节点的二维表示。注意，即使没有任何训练，网络中节点的相对接近度也保留在二维表示中。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mo"><img src="../Images/f8446e08f6b61f9c34f51dca2c5055aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pCeWhIrEFXoEgsB5eEB6sw.png"/></div></div></figure><p id="29fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更正式地说，<em class="mn">图卷积网络(GCN) </em>是一种对图进行操作的神经网络。给定一个图<em class="mn"> G = (V，E) </em>，一个 GCN 取为输入</p><ul class=""><li id="a16f" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la na lh li lj bi translated">输入特征矩阵<em class="mn"> N × F⁰ </em>特征矩阵，<strong class="kh ir"> <em class="mn"> X，</em> </strong>其中<em class="mn"> N </em>是节点的数量，<em class="mn"> F⁰ </em>是每个节点的输入特征的数量，并且</li><li id="7ad1" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la na lh li lj bi translated">图结构的一个<em class="mn"> N </em> × <em class="mn"> N </em>矩阵表示如 g[1]的邻接矩阵<strong class="kh ir"> <em class="mn"> A </em> </strong></li></ul><p id="53a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">GCN 中的一个隐藏层因此可以写成<strong class="kh ir"><em class="mn"/></strong><em class="mn">= f(</em><strong class="kh ir"><em class="mn">h</em></strong><em class="mn">【ⁱ⁻】，</em><strong class="kh ir"><em class="mn">a</em></strong><em class="mn">)</em>其中<strong class="kh ir"><em class="mn">h</em></strong><em class="mn">⁰=</em><strong class="kh ir"><em class="mn">x</em></strong>和<em class="mn"> f </em>为每层<strong class="kh ir"><em class="mn"/></strong>对应一个<em class="mn">n</em>×<em class="mn">f</em><strong class="kh ir"><em class="mn">ⁱ</em></strong>特征矩阵，其中每一行都是一个节点的特征表示。在每一层，使用传播规则<em class="mn"> f </em>聚集这些特征以形成下一层的特征。通过这种方式，每个连续层的特征变得越来越抽象。在这个框架中，GCN 的变体仅仅在传播规则的选择上有所不同。</p><h1 id="5683" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">一个简单的传播规则</h1><p id="bd2d" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">一个最简单的可能传播规则是[1]:</p><p id="5b6e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mn">f(</em><strong class="kh ir"><em class="mn">hⁱ</em></strong><em class="mn">，</em><strong class="kh ir"><em class="mn">a</em></strong><em class="mn">)=σ(</em><strong class="kh ir"><em class="mn">ahⁱwⁱ</em></strong><em class="mn">)</em></p><p id="1d20" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<strong class="kh ir"> <em class="mn"> Wⁱ </em> </strong>是层<em class="mn"> i </em>的权重矩阵，而<em class="mn"> σ </em>是非线性激活函数，例如<a class="ae lp" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU 函数</a>。权重矩阵有维度<em class="mn">f</em><strong class="kh ir"><em class="mn">ⁱ</em></strong><em class="mn">×fⁱ</em>⁺<em class="mn"/>；换句话说，权重矩阵的第二维的大小决定了下一层的特征数量。如果你熟悉<a class="ae lp" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a>，这个操作类似于过滤操作，因为这些权重在图中的节点间共享。</p><h2 id="9056" class="nb lr iq bd ls nc nd dn lw ne nf dp ma ko ng nh mc ks ni nj me kw nk nl mg nm bi translated">简单化</h2><p id="dc46" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">让我们从最简单的层面来研究传播规则。让</p><ul class=""><li id="a6a8" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la na lh li lj bi translated"><em class="mn"> i = 1 </em>，s.t. <em class="mn"> f </em>是输入特征矩阵的函数，</li><li id="dc54" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la na lh li lj bi translated"><em class="mn"> σ </em>是<a class="ae lp" href="https://en.wikipedia.org/wiki/Identity_function" rel="noopener ugc nofollow" target="_blank">恒等函数</a>，并且</li><li id="6888" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la na lh li lj bi translated">选择权重 s . t .<strong class="kh ir"><em class="mn">ah</em></strong>⁰<strong class="kh ir"><em class="mn">w</em></strong>⁰=<strong class="kh ir"><em class="mn">axw</em></strong>⁰=<strong class="kh ir"><em class="mn">ax</em></strong>。</li></ul><p id="35b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">换句话说，<em class="mn">f(</em><strong class="kh ir"><em class="mn">X</em></strong><em class="mn">，</em><strong class="kh ir"><em class="mn">A</em></strong><em class="mn">)=</em><strong class="kh ir"><em class="mn">AX</em></strong>。这个传播规则可能有点太简单了，但是我们稍后会添加缺少的部分。作为旁注，<strong class="kh ir"> AX </strong>现在相当于一个多层感知器的输入层。</p><h2 id="15e1" class="nb lr iq bd ls nc nd dn lw ne nf dp ma ko ng nh mc ks ni nj me kw nk nl mg nm bi translated">一个简单的图表示例</h2><p id="f64c" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">作为一个简单的例子，我们将使用下图:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/7639f57d77b4b151da0b4bff4eef74b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*jTW7doI_cqC_p9XQrmuu9A.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk">A simple directed graph.</figcaption></figure><p id="d11f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而下面是它的<code class="fe ns nt nu nv b">numpy</code>邻接矩阵表示。</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="a72a" class="nb lr iq nv b gy oa ob l oc od">A = np.matrix([<br/>    [0, 1, 0, 0],<br/>    [0, 0, 1, 1], <br/>    [0, 1, 0, 0],<br/>    [1, 0, 1, 0]],<br/>    dtype=float<br/>)</span></pre><p id="bce9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们需要特性！我们基于其索引为每个节点生成 2 个整数特征。这使得稍后手动确认矩阵计算变得容易。</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="f608" class="nb lr iq nv b gy oa ob l oc od">In [3]: X = np.matrix([<br/>            [i, -i]<br/>            <strong class="nv ir">for</strong> i <strong class="nv ir">in</strong> range(A.shape[0])<br/>        ], dtype=float)<br/>        X</span><span id="5007" class="nb lr iq nv b gy oe ob l oc od">Out[3]: matrix([<br/>           [ 0.,  0.],<br/>           [ 1., -1.],<br/>           [ 2., -2.],<br/>           [ 3., -3.]<br/>        ])</span></pre><h2 id="2eed" class="nb lr iq bd ls nc nd dn lw ne nf dp ma ko ng nh mc ks ni nj me kw nk nl mg nm bi translated">应用传播规则</h2><p id="e88b" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">好吧！我们现在有一个图，它的邻接矩阵<code class="fe ns nt nu nv b">A</code>和一组输入特征<code class="fe ns nt nu nv b">X</code>。让我们看看应用传播规则时会发生什么:</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="6ee2" class="nb lr iq nv b gy oa ob l oc od">In [6]: A * X<br/>Out[6]: matrix([<br/>            [ 1., -1.],<br/>            [ 5., -5.],<br/>            [ 1., -1.],<br/>            [ 2., -2.]]</span></pre><p id="6f4c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">发生了什么事？每个节点(每行)的表示现在是其相邻要素的总和！换句话说，图形卷积层将每个节点表示为其邻域的集合。我鼓励你自己检查计算。注意，在这种情况下，如果存在从<em class="mn"> v </em>到<em class="mn"> n </em>的边，则节点<em class="mn"> n </em>是节点<em class="mn"> v </em>的邻居。</p><h1 id="b67c" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">啊哦！即将出现的问题！</h1><p id="7847" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">您可能已经发现了问题:</p><ul class=""><li id="25e7" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la na lh li lj bi translated">一个节点的聚合表示不包括它自己的特性！该表示是相邻节点的特征的集合，因此只有具有自环的节点才会将它们自己的特征包括在集合中。[1]</li><li id="fff4" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la na lh li lj bi translated">度数较大的结点在其要素制图表达中将具有较大的值，而度数较小的结点将具有较小的值。这可能导致梯度消失或爆炸[1，2]，但对于通常用于训练此类网络的随机梯度下降算法来说也是一个问题，该算法对每个输入要素的比例(或取值范围)非常敏感。</li></ul><p id="9f8b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面，我将分别讨论这些问题。</p><h2 id="c34f" class="nb lr iq bd ls nc nd dn lw ne nf dp ma ko ng nh mc ks ni nj me kw nk nl mg nm bi translated">添加自循环</h2><p id="c8af" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">为了解决第一个问题，可以简单地给每个节点添加一个自环[1，2]。实际上，这是通过在应用传播规则之前将单位矩阵<code class="fe ns nt nu nv b">I</code>添加到邻接矩阵<code class="fe ns nt nu nv b">A</code>来完成的。</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="ee29" class="nb lr iq nv b gy oa ob l oc od">In [4]: I = np.matrix(np.eye(A.shape[0]))<br/>        I</span><span id="e5fb" class="nb lr iq nv b gy oe ob l oc od">Out[4]: matrix([<br/>            [1., 0., 0., 0.],<br/>            [0., 1., 0., 0.],<br/>            [0., 0., 1., 0.],<br/>            [0., 0., 0., 1.]<br/>        ])</span><span id="327d" class="nb lr iq nv b gy oe ob l oc od">In [8]: A_hat = A + I<br/>        A_hat * X<br/>Out[8]: matrix([<br/>            [ 1., -1.],<br/>            [ 6., -6.],<br/>            [ 3., -3.],<br/>            [ 5., -5.]])</span></pre><p id="5578" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于该节点现在是其自身的邻居，所以在对其邻居的特征求和时包括了该节点自身的特征！</p><h2 id="fb07" class="nb lr iq bd ls nc nd dn lw ne nf dp ma ko ng nh mc ks ni nj me kw nk nl mg nm bi translated">标准化要素制图表达</h2><p id="cd46" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">通过将邻接矩阵<code class="fe ns nt nu nv b">A</code>乘以逆矩阵<code class="fe ns nt nu nv b">D</code>【1】来转换邻接矩阵<code class="fe ns nt nu nv b">A</code>，可以通过节点度来归一化特征表示。因此，我们简化的传播规则如下所示[1]:</p><p id="30e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mn"> f( </em> <strong class="kh ir"> <em class="mn"> X </em> </strong> <em class="mn">，</em><strong class="kh ir"><em class="mn">a</em></strong><em class="mn">)=</em><strong class="kh ir"><em class="mn">d</em></strong><em class="mn">⁻</em><strong class="kh ir"><em class="mn">ax</em></strong></p><p id="8bae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看会发生什么。我们首先计算度矩阵。</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="7e45" class="nb lr iq nv b gy oa ob l oc od">In [9]: D = np.array(np.sum(A, axis=0))[0]<br/>        D = np.matrix(np.diag(D))<br/>        D<br/>Out[9]: matrix([<br/>            [1., 0., 0., 0.],<br/>            [0., 2., 0., 0.],<br/>            [0., 0., 2., 0.],<br/>            [0., 0., 0., 1.]<br/>        ])</span></pre><p id="84f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在应用规则之前，让我们看看变换后邻接矩阵会发生什么变化。</p></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><p id="a7c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">在</strong>之前</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="0a4a" class="nb lr iq nv b gy oa ob l oc od">A = np.matrix([<br/>    [0, 1, 0, 0],<br/>    [0, 0, 1, 1], <br/>    [0, 1, 0, 0],<br/>    [1, 0, 1, 0]],<br/>    dtype=float<br/>)</span></pre><p id="7d09" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">在</strong>之后</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="3d5d" class="nb lr iq nv b gy oa ob l oc od">In [10]: D**-1 * A<br/>Out[10]: matrix([<br/>             [0. , 1. , 0. , 0. ],<br/>             [0. , 0. , 0.5, 0.5],<br/>             [0. , 0.5, 0. , 0. ],<br/>             [0.5, 0. , 0.5, 0. ]<br/>])</span></pre></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><p id="8f09" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，邻接矩阵的每一行中的权重(值)已经除以了对应于该行的节点的度数。我们用转换后的邻接矩阵应用传播规则</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="8c9c" class="nb lr iq nv b gy oa ob l oc od">In [11]: D**-1 * A * X<br/>Out[11]: matrix([<br/>             [ 1. , -1. ],<br/>             [ 2.5, -2.5],<br/>             [ 0.5, -0.5],<br/>             [ 2. , -2. ]<br/>         ])</span></pre><p id="49c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并且获得对应于相邻节点的特征的平均值的节点表示。这是因为(经变换的)邻接矩阵中的权重对应于相邻节点特征的加权和中的权重。我再次鼓励你亲自验证这个观察结果。</p><h1 id="02d8" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">把所有的放在一起</h1><p id="d529" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">我们现在将自循环和规范化技巧结合起来。此外，我们将重新引入之前为了简化讨论而放弃的权重和激活函数。</p><p id="a1f4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">加回重量</strong></p><p id="7328" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先要做的是应用权重。注意，这里的<code class="fe ns nt nu nv b">D_hat</code>是<code class="fe ns nt nu nv b">A_hat = A + I</code>的度矩阵，即带有强制自循环的<code class="fe ns nt nu nv b">A</code>的度矩阵。</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="cd93" class="nb lr iq nv b gy oa ob l oc od">In [45]: W = np.matrix([<br/>             [1, -1],<br/>             [-1, 1]<br/>         ])<br/>         D_hat**-1 * A_hat * X * W<br/>Out[45]: matrix([<br/>            [ 1., -1.],<br/>            [ 4., -4.],<br/>            [ 2., -2.],<br/>            [ 5., -5.]<br/>        ])</span></pre><p id="0ea2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们想要减少输出特征表示的维数，我们可以减少权重矩阵的大小<code class="fe ns nt nu nv b">W</code>:</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="a976" class="nb lr iq nv b gy oa ob l oc od">In [46]: W = np.matrix([<br/>             [1],<br/>             [-1]<br/>         ])<br/>         D_hat**-1 * A_hat * X * W<br/>Out[46]: matrix([[1.],<br/>        [4.],<br/>        [2.],<br/>        [5.]]<br/>)</span></pre><h2 id="2b8b" class="nb lr iq bd ls nc nd dn lw ne nf dp ma ko ng nh mc ks ni nj me kw nk nl mg nm bi translated">添加激活功能</h2><p id="7a09" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">我们选择保留特征表示的维度，并应用<a class="ae lp" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU </a>激活函数。</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="c50d" class="nb lr iq nv b gy oa ob l oc od">In [51]: W = np.matrix([<br/>             [1, -1],<br/>             [-1, 1]<br/>         ])<br/>         relu(D_hat**-1 * A_hat * X * W)<br/>Out[51]: matrix([[1., 0.],<br/>        [4., 0.],<br/>        [2., 0.],<br/>        [5., 0.]])</span></pre><p id="9f8e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">瞧啊。一个完整的隐藏层，有邻接矩阵，输入特征，权重和激活函数！</p><h1 id="c8f6" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">回到现实</h1><p id="d611" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">现在，最后，我们可以将一个图卷积网络应用到一个真实的图上。我将向您展示如何生成我们在本文前面看到的要素制图表达。</p><h2 id="a930" class="nb lr iq bd ls nc nd dn lw ne nf dp ma ko ng nh mc ks ni nj me kw nk nl mg nm bi translated">扎卡里空手道俱乐部</h2><p id="6c2f" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">扎卡里的空手道俱乐部是一个常用的社交网络，其中节点代表空手道俱乐部的成员，边代表他们的相互关系。当扎卡里在学习空手道俱乐部时，管理员和教练之间发生了冲突，导致俱乐部一分为二。下图显示了网络的图形表示，节点根据俱乐部的位置进行标记。管理员和讲师分别标有“A”和“I”。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi om"><img src="../Images/64f6cf7a4af3540dd93551d14fb01a30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d62WDGX4uf6bwlu0KyfRsQ.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk">Zachary’s Karate Club</figcaption></figure><h2 id="215d" class="nb lr iq bd ls nc nd dn lw ne nf dp ma ko ng nh mc ks ni nj me kw nk nl mg nm bi translated">建造 GCN</h2><p id="431e" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">现在让我们建立图形卷积网络。我们实际上不会训练网络，只是简单地随机初始化它，以生成我们在本文开头看到的要素制图表达。我们将使用<code class="fe ns nt nu nv b">networkx</code>，它有一个很容易得到的俱乐部的图形表示，并计算<code class="fe ns nt nu nv b">A_hat</code>和<code class="fe ns nt nu nv b">D_hat</code>矩阵。</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="b06c" class="nb lr iq nv b gy oa ob l oc od">from networkx import karate_club_graph, to_numpy_matrix</span><span id="349b" class="nb lr iq nv b gy oe ob l oc od">zkc = karate_club_graph()<br/>order = sorted(list(zkc.nodes()))</span><span id="ed86" class="nb lr iq nv b gy oe ob l oc od">A = to_numpy_matrix(zkc, nodelist=order)<br/>I = np.eye(zkc.number_of_nodes())</span><span id="8e4b" class="nb lr iq nv b gy oe ob l oc od">A_hat = A + I<br/>D_hat = np.array(np.sum(A_hat, axis=0))[0]<br/>D_hat = np.matrix(np.diag(D_hat))</span></pre><p id="a5bd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们将随机初始化权重。</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="b50a" class="nb lr iq nv b gy oa ob l oc od">W_1 = np.random.normal(<br/>    loc=0, scale=1, size=(zkc.number_of_nodes(), 4))<br/>W_2 = np.random.normal(<br/>    loc=0, size=(W_1.shape[1], 2))</span></pre><p id="e9e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">堆叠 GCN 层。我们这里只使用单位矩阵作为特征表示，也就是说，每个节点被表示为一个独热编码的分类变量。</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="07c1" class="nb lr iq nv b gy oa ob l oc od">def gcn_layer(A_hat, D_hat, X, W):<br/>    return relu(D_hat**-1 * A_hat * X * W)</span><span id="fc79" class="nb lr iq nv b gy oe ob l oc od">H_1 = gcn_layer(A_hat, D_hat, I, W_1)<br/>H_2 = gcn_layer(A_hat, D_hat, H_1, W_2)</span><span id="ba2f" class="nb lr iq nv b gy oe ob l oc od">output = H_2</span></pre><p id="e0ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们提取特征表示。</p><pre class="mp mq mr ms gt nw nv nx ny aw nz bi"><span id="7160" class="nb lr iq nv b gy oa ob l oc od">feature_representations = {<br/>    node: np.array(output)[node] <br/>    for node in zkc.nodes()}</span></pre><p id="c573" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">瞧啊。将扎卡里空手道俱乐部的社区很好地分开的特征表示。我们甚至还没有开始训练！</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi om"><img src="../Images/9eae121b10121e28991584d9e2a850a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Voir16IcvOvmWyO3nX4WZA.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk">Feature Representations of the Nodes in Zachary’s Karate Club</figcaption></figure><p id="70bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我应该注意到，对于这个例子，随机初始化的权重很可能在 x 轴或 y 轴上给出 0 值，作为 ReLU 函数的结果，所以需要一些随机初始化来产生上面的图。</p><h1 id="286e" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">结论</h1><p id="de0c" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">在这篇文章中，我对图卷积网络进行了高度的介绍，并举例说明了 GCN 中每一层节点的特征表示是如何基于其邻域的聚合的。我们看到了如何使用 numpy 构建这些网络，以及它们有多么强大:即使是随机初始化的 gcn 也可以在 Zachary 的空手道俱乐部中分离社区。</p><p id="2844" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下一篇文章中，我将更深入地介绍技术细节，并展示如何使用半监督学习来实现和训练最近发布的 GCN。<strong class="kh ir">你在这里</strong>  <strong class="kh ir">找到系列</strong> <a class="ae lp" rel="noopener" target="_blank" href="/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-62acf5b143d0"> <strong class="kh ir">的下一篇帖子。</strong></a></p></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><p id="39de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mn">喜欢你读的书吗？考虑在</em><a class="ae lp" href="https://twitter.com/TobiasSJepsen" rel="noopener ugc nofollow" target="_blank"><em class="mn">Twitter</em></a><em class="mn">上关注我，在那里，除了我自己的帖子之外，我还会分享与数据科学和机器学习的实践、理论和伦理相关的论文、视频和文章。</em></p><p id="524c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mn">如需专业咨询，请在</em><a class="ae lp" href="https://www.linkedin.com/in/tobias-skovgaard-jepsen/" rel="noopener ugc nofollow" target="_blank"><em class="mn">LinkedIn</em></a><em class="mn">上联系我，或在</em><a class="ae lp" href="https://twitter.com/TobiasSJepsen" rel="noopener ugc nofollow" target="_blank"><em class="mn">Twitter</em></a><em class="mn">上直接留言。</em></p><h1 id="30b8" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">参考</h1><p id="1c32" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">[1]<a class="ae lp" href="https://tkipf.github.io/graph-convolutional-networks/" rel="noopener ugc nofollow" target="_blank">Thomas Kipf 关于图卷积网络的博文</a>。</p><p id="53f4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]<a class="ae lp" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank">Thomas Kipf 和 Max Welling 的论文</a>称为<em class="mn">图卷积网络半监督分类</em>。</p></div></div>    
</body>
</html>