<html>
<head>
<title>How to Get Started with PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何开始使用 PySpark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-get-started-with-pyspark-1adc142456ec?source=collection_archive---------1-----------------------#2018-06-11">https://towardsdatascience.com/how-to-get-started-with-pyspark-1adc142456ec?source=collection_archive---------1-----------------------#2018-06-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="8191" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="http://spark.apache.org/docs/2.2.0/api/python/pyspark.html" rel="noopener ugc nofollow" target="_blank"> PySpark </a>是使用 Spark 的 Python API，Spark 是运行大数据应用的并行分布式引擎。开始使用 PySpark 花了我几个小时——这是不应该的——因为我必须阅读大量的博客/文档来调试一些设置问题。这个博客旨在帮助您尽快开始使用 PySpark！</p><p id="30ac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">更新</strong>:我写了一篇关于 PySpark 以及如何使用一些托管服务(如 Databricks 和 EMR)和一些常见架构开始使用 Spark 的新博文。片名为<a class="ae ko" rel="noopener" target="_blank" href="/moving-from-pandas-to-spark-7b0b7d956adb"> <strong class="js iu">从熊猫到星火</strong>。</a>有兴趣了解更多就去看看吧！</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/8d46049d538a21259be7acfc001743bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RWAmxUsenPRcXDtFVeRaYg.jpeg"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">How do we analyze all the “big data” around us?</figcaption></figure><p id="1857" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些步骤适用于 Mac OS X(我运行的是 OS X 10.13 High Sierra)和 Python 3.6。</p><h2 id="1b56" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">1.开始新的康达环境</h2><p id="291b" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">你可以<a class="ae ko" href="https://www.anaconda.com/download/#macos" rel="noopener ugc nofollow" target="_blank">安装 Anaconda </a>，如果你已经有了它，使用<code class="fe md me mf mg b">conda create -n pyspark_env python=3</code>启动一个新的<code class="fe md me mf mg b">conda</code>环境。这将创建一个新的 conda 环境，使用最新版本的 Python 3，供我们尝试我们的 mini-PySpark 项目。<br/>用<code class="fe md me mf mg b">source activate pyspark_env</code>激活环境</p><h2 id="3d90" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">2.安装 PySpark 包</h2><p id="0dae" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">你可以尝试使用<code class="fe md me mf mg b">pip</code>来安装<code class="fe md me mf mg b">pyspark</code>，但是我无法让<code class="fe md me mf mg b">pyspark</code>集群正常启动。看了几个关于堆栈溢出的答案和官方文档，我发现了这个:</p><blockquote class="mh mi mj"><p id="b929" class="jq jr mk js b jt ju jv jw jx jy jz ka ml kc kd ke mm kg kh ki mn kk kl km kn im bi translated">Spark 的 Python 打包并不打算取代所有其他用例。Spark 的 Python 打包版本适合与现有集群(无论是 Spark standalone、YARN 还是 Mesos)进行交互，但不包含设置您自己的独立 Spark 集群所需的工具。你可以从<a class="ae ko" href="http://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank"> Apache Spark 下载页面</a>下载 Spark 的完整版本。</p></blockquote><p id="eadb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用上面的链接，我继续下载了<code class="fe md me mf mg b"><a class="ae ko" href="https://www.apache.org/dyn/closer.lua/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">spark-2.3.0-bin-hadoop2.7.tgz</a></code>,并将解压后的版本存储在我的主目录中。</p><h2 id="ea2f" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">3.安装 Java 8</h2><p id="2f37" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">一些说明建议使用 Java 8 或更高版本，我继续安装了 Java 10。当我试图在我的 Spark 集群中运行<code class="fe md me mf mg b">collect()</code>或<code class="fe md me mf mg b">count()</code>时，这实际上导致了如下几个错误:</p><pre class="kq kr ks kt gt mo mg mp mq aw mr bi"><span id="8ee8" class="lf lg it mg b gy ms mt l mu mv">py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.</span><span id="fde7" class="lf lg it mg b gy mw mt l mu mv">: java.lang.IllegalArgumentException</span></pre><p id="eef7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我最初的猜测是，它必须与<code class="fe md me mf mg b">Py4J</code>安装做一些事情，我试图重新安装了几次，没有任何帮助。没有多少人谈论这个错误，在阅读了几篇堆栈溢出的帖子后，我看到了这篇帖子，它讨论了 Spark 2.2.1 如何与 Java 9 及更高版本发生问题。推荐的解决方案是安装 Java 8。</p><p id="f822" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以，<a class="ae ko" href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" rel="noopener ugc nofollow" target="_blank">安装 Java 8 </a> JDK，进入下一步。</p><h2 id="654e" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">4.“改变”。“bash_profile”变量设置</h2><p id="c26f" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">为了告诉 bash 如何找到 Spark 包和 Java SDK，<strong class="js iu">在您的<code class="fe md me mf mg b">.bash_profile</code>中添加</strong>以下几行(如果您使用 vim，您可以做<code class="fe md me mf mg b">vim ~/.bash_profile</code>来编辑这个文件)</p><pre class="kq kr ks kt gt mo mg mp mq aw mr bi"><span id="d0f1" class="lf lg it mg b gy ms mt l mu mv">export JAVA_HOME=$(/usr/libexec/java_home)<br/>export SPARK_HOME=~/spark-2.3.0-bin-hadoop2.7<br/>export PATH=$SPARK_HOME/bin:$PATH<br/>export PYSPARK_PYTHON=python3</span></pre><p id="e295" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些命令告诉 bash 如何使用最近安装的 Java 和 Spark 包。运行<code class="fe md me mf mg b">source ~/.bash_profile</code>获取该文件或打开新终端自动获取该文件。</p><h2 id="9ddf" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">5.启动 PySpark</h2><p id="e11c" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">运行<code class="fe md me mf mg b">pyspark</code>命令，您将会看到:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/eda01679b551a0b846d8f539314b0a8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*1Myt7VruXmQ-qBm0qhloAg.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">PySpark welcome message on running `pyspark`</figcaption></figure><p id="f602" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您可以使用命令行来运行 Spark 命令，但是这不是很方便。可以使用<code class="fe md me mf mg b">pip install jupyter notebook</code>安装 jupyter 笔记本，运行<code class="fe md me mf mg b">jupyter notebook</code>时可以访问笔记本中的 Spark 集群。您也可以使用<code class="fe md me mf mg b">vim</code>或<code class="fe md me mf mg b">nano</code>或您选择的任何其他代码编辑器将代码写入 python 文件，您可以从命令行运行这些文件。</p><h2 id="33f9" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">6.用 PySpark 计算圆周率！</h2><p id="0c1e" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">运行一个小而快速的程序来估算<code class="fe md me mf mg b">pi</code>的值，看看你的火花簇在起作用！</p><pre class="kq kr ks kt gt mo mg mp mq aw mr bi"><span id="05cc" class="lf lg it mg b gy ms mt l mu mv">import random<br/>NUM_SAMPLES = 100000000<br/>def inside(p):<br/> x, y = random.random(), random.random()<br/> return x*x + y*y &lt; 1</span><span id="4061" class="lf lg it mg b gy mw mt l mu mv">count = sc.parallelize(range(0, NUM_SAMPLES)).filter(inside).count()<br/>pi = 4 * count / NUM_SAMPLES<br/>print(“Pi is roughly”, pi)</span></pre><h2 id="cdb4" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated"><strong class="ak"> 7。接下来的步骤</strong></h2><p id="3ffd" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">PySpark 中有很多东西可以探索，例如弹性分布式数据集或 rdd(更新:现在 DataFrame API 是使用 Spark 的最佳方式，rdd 谈论“如何”完成任务，而数据帧谈论“什么”——这使得数据帧更快和优化)和<a class="ae ko" href="https://spark.apache.org/docs/2.2.0/ml-guide.html" rel="noopener ugc nofollow" target="_blank"> MLlib </a>。</p><p id="98b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2021 年 6 月更新:我写了一篇关于 PySpark 以及如何开始使用 Spark 的一些托管服务(如 Databricks 和 EMR)和一些常见架构的新博客。它的名字叫<a class="ae ko" rel="noopener" target="_blank" href="/moving-from-pandas-to-spark-7b0b7d956adb"> <strong class="js iu">从熊猫到星火</strong>。</a>有兴趣了解更多就去看看吧！感谢您的阅读。</p></div></div>    
</body>
</html>