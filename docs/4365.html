<html>
<head>
<title>2 latent methods for dimension reduction and topic modeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2 种潜在的降维和主题建模方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/2-latent-methods-for-dimension-reduction-and-topic-modeling-20ff6d7d547?source=collection_archive---------3-----------------------#2018-08-11">https://towardsdatascience.com/2-latent-methods-for-dimension-reduction-and-topic-modeling-20ff6d7d547?source=collection_archive---------3-----------------------#2018-08-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/6de0ed71b574fc5a607b838e745c1268.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*doqjI4Nw8RVWbDGy.jpg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo: <a class="ae kf" href="https://pixabay.com/en/golden-gate-bridge-women-back-1030999/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/en/golden-gate-bridge-women-back-1030999/</a></figcaption></figure><p id="50b8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在最先进的<a class="ae kf" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a">单词嵌入</a>技术之前，潜在语义分析(LSA)和潜在狄利克雷分配(LDA)区域是处理自然语言处理问题的好方法。LSA 和 LDA 都有相同的输入，即矩阵格式的<strong class="ki iu">单词包</strong>。LSA 专注于降低矩阵维数，而 LDA 解决<strong class="ki iu">主题建模</strong>问题。</p><p id="8144" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将不会讨论数学细节，因为这方面有很多很好的材料。你可以参考一下。为了便于理解，我没有做停用词去除等预处理。当你使用 LSA、LSI 和 LDA 时，这是很关键的部分。看完这篇文章，你会知道:</p><ul class=""><li id="8f7d" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">潜在语义分析(LSA)</li><li id="ee30" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">潜在狄利克雷分配</li><li id="b2ff" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">拿走</li></ul><h1 id="d1ec" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">潜在语义分析(LSA)</h1><p id="19fa" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">用于自然语言处理任务的 LSA 是由 Jerome Bellegarda 在 2005 年提出的。LSA 的目标是降低分类维数。这个想法是，如果单词有相似的意思，它们会出现在相似的文本中。在自然语言处理领域，我们通常使用潜在语义索引(LSI)作为替代名称。</p><p id="f17e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们有 m 个文档和 n 个单词作为输入。当列和行分别是文档和单词时，可以构造 m * n 矩阵。您可以使用计数出现或 TF-IDF 分数。然而，在大多数情况下，TF-IDF 优于计数出现，因为高频率并不能说明更好的分类。</p><figure class="mv mw mx my gt ju gh gi paragraph-image"><div class="ab gu cl mz"><img src="../Images/23fe08ef25361d12c7f3ed244a87e02a.png" data-original-src="https://miro.medium.com/v2/format:webp/0*7r2GKRepjh5Fl41r.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo: <a class="ae kf" href="http://mropengate.blogspot.com/2016/04/tf-idf-in-r-language.html" rel="noopener ugc nofollow" target="_blank">http://mropengate.blogspot.com/2016/04/tf-idf-in-r-language.html</a></figcaption></figure><p id="ef1f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TF-IDF 的思想是高频可能不能提供太多的信息增益。换句话说，生僻字为模型贡献了更多的权重。如果在同一文档(即培训记录)中出现的次数增加，单词的重要性将增加。另一方面，如果它出现在语料库(即其他训练记录)中，则会减少。详情你可以查看这个<a class="ae kf" rel="noopener" target="_blank" href="/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016">博客</a>。</p><p id="2469" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">挑战在于矩阵非常稀疏(或高维)且有噪声(或包含大量低频词)。因此采用截断奇异值分解来降低维数。</p><figure class="mv mw mx my gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi na"><img src="../Images/e884ca7e35e4d53b10d8ac47c70f8fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z0EUVs7QElEqRqXtqut_FQ.png"/></div></div></figure><p id="21b7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">奇异值分解的思想是寻找最有价值的信息，用低维 t 来表示同一事物。</p><pre class="mv mw mx my gt nb nc nd ne aw nf bi"><span id="61be" class="ng lt it nc b gy nh ni l nj nk">tfidf_vec = TfidfVectorizer(use_idf=True, norm='l2')<br/>svd = TruncatedSVD(n_components=dim)</span><span id="7370" class="ng lt it nc b gy nl ni l nj nk">transformed_x_train = tfidf_vec.fit_transform(x_train)<br/>transformed_x_test = tfidf_vec.transform(x_test)</span><span id="bcff" class="ng lt it nc b gy nl ni l nj nk">print('TF-IDF output shape:', transformed_x_train.shape)</span><span id="3504" class="ng lt it nc b gy nl ni l nj nk">x_train_svd = svd.fit_transform(transformed_x_train)<br/>x_test_svd = svd.transform(transformed_x_test)</span><span id="2ff0" class="ng lt it nc b gy nl ni l nj nk">print('LSA output shape:', x_train_svd.shape)</span><span id="7c67" class="ng lt it nc b gy nl ni l nj nk">explained_variance = svd.explained_variance_ratio_.sum()<br/>print("Sum of explained variance ratio: %d%%" % (int(explained_variance * 100)))</span></pre><p id="555f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出</p><pre class="mv mw mx my gt nb nc nd ne aw nf bi"><span id="5b29" class="ng lt it nc b gy nh ni l nj nk">TF-IDF output shape: (11314, 130107)<br/>LSA output shape: (11314, 50)<br/>Sum of explained variance ratio: 8%</span></pre><p id="7397" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到维数从 130k 减少到 50。</p><pre class="mv mw mx my gt nb nc nd ne aw nf bi"><span id="8ea5" class="ng lt it nc b gy nh ni l nj nk">from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_val_score, KFold</span><span id="69ac" class="ng lt it nc b gy nl ni l nj nk">lr_model = LogisticRegression(solver='newton-cg',n_jobs=-1)<br/>lr_model.fit(x_train_svd, y_train)</span><span id="6f3e" class="ng lt it nc b gy nl ni l nj nk">cv = KFold(n_splits=5, shuffle=True)<br/>    <br/>scores = cross_val_score(lr_model, x_test_svd, y_test, cv=cv, scoring='accuracy')<br/>print("Accuracy: %0.4f (+/- %0.4f)" % (scores.mean(), scores.std() * 2))</span></pre><p id="b92f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出</p><pre class="mv mw mx my gt nb nc nd ne aw nf bi"><span id="8b8d" class="ng lt it nc b gy nh ni l nj nk">Accuracy: 0.6511 (+/- 0.0201)</span></pre><h1 id="5cee" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">潜在狄利克雷分配</h1><p id="1a99" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">2003 年，大卫·布莱、吴恩达和迈克尔·乔丹推出了《LDA》。它是无监督学习，主题模型是典型的例子。假设每个文档混合了各种主题，每个主题混合了各种单词。</p><figure class="mv mw mx my gt ju gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/c1aa05b3124ceffab8e6b1b592aaf8af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*SHahRtoGw3JP48e806DsIw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Various words under various topics</figcaption></figure><p id="3adb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">直观地说，您可以想象我们有两层聚合。第一层是类别分布。例如，我们有财经新闻、天气新闻和政治新闻。第二层是词在类别中的分布。例如，我们可以在天气新闻中找到“晴天”和“云”，而在财经新闻中则有“钱”和“股票”。</p><p id="bdff" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，“a”、“with”和“can”对主题建模问题没有贡献。这些单词存在于文档中，并且在类别之间具有大致相同的概率。因此，停用词的去除是获得更好结果的关键步骤。</p><figure class="mv mw mx my gt ju gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/d0987ce6430fa39d6752868d8f5e4dbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*qwA4jyRFBB6Htn3X4aftSw.png"/></div></figure><p id="008c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于特定的文档 d，我们得到主题分布θ。从这个分布(θ)中，题目 t 将被选择，并从ϕ.中选择相应单词</p><pre class="mv mw mx my gt nb nc nd ne aw nf bi"><span id="25f2" class="ng lt it nc b gy nh ni l nj nk">from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.decomposition import LatentDirichletAllocation</span><span id="4510" class="ng lt it nc b gy nl ni l nj nk">def build_lda(x_train, num_of_topic=10):<br/>    vec = CountVectorizer()<br/>    transformed_x_train = vec.fit_transform(x_train)<br/>    feature_names = vec.get_feature_names()</span><span id="d098" class="ng lt it nc b gy nl ni l nj nk">lda = LatentDirichletAllocation(<br/>        n_components=num_of_topic, max_iter=5, <br/>        learning_method='online', random_state=0)<br/>    lda.fit(transformed_x_train)</span><span id="59c5" class="ng lt it nc b gy nl ni l nj nk">return lda, vec, feature_names</span><span id="ead9" class="ng lt it nc b gy nl ni l nj nk">def display_word_distribution(model, feature_names, n_word):<br/>    for topic_idx, topic in enumerate(model.components_):<br/>        print("Topic %d:" % (topic_idx))<br/>        words = []<br/>        for i in topic.argsort()[:-n_word - 1:-1]:<br/>            words.append(feature_names[i])<br/>        print(words)</span><span id="ade3" class="ng lt it nc b gy nl ni l nj nk">lda_model, vec, feature_names = build_lda(x_train)<br/>display_word_distribution(<br/>    model=lda_model, feature_names=feature_names, <br/>    n_word=5)</span></pre><p id="6ba8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出</p><pre class="mv mw mx my gt nb nc nd ne aw nf bi"><span id="acd5" class="ng lt it nc b gy nh ni l nj nk">Topic 0:<br/>['the', 'for', 'and', 'to', 'edu']<br/>Topic 1:<br/>['c_', 'w7', 'hz', 'mv', 'ck']<br/>Topic 2:<br/>['space', 'nasa', 'cmu', 'science', 'edu']<br/>Topic 3:<br/>['the', 'to', 'of', 'for', 'and']<br/>Topic 4:<br/>['the', 'to', 'of', 'and', 'in']<br/>Topic 5:<br/>['the', 'of', 'and', 'in', 'were']<br/>Topic 6:<br/>['edu', 'team', 'he', 'game', '10']<br/>Topic 7:<br/>['ax', 'max', 'g9v', 'b8f', 'a86']<br/>Topic 8:<br/>['db', 'bike', 'ac', 'image', 'dod']<br/>Topic 9:<br/>['nec', 'mil', 'navy', 'sg', 'behanna']</span></pre><h1 id="e9fc" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">拿走</h1><p id="3412" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">要访问所有代码，你可以访问我的 github repo。</p><ul class=""><li id="efbb" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">两者都使用<strong class="ki iu">词袋作为输入矩阵</strong></li><li id="e525" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">奇异值分解的挑战在于我们很难<strong class="ki iu">确定最优维数</strong>。一般来说，低维消耗较少的资源，但我们可能无法区分相反意义的词，而高维克服它，但消耗更多的资源。</li></ul><h1 id="c1b1" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">我是湾区的数据科学家。专注于数据科学和人工智能领域的最新发展，尤其是 NLP 和平台相关领域。你可以通过<a class="ae kf" href="http://medium.com/@makcedward/" rel="noopener"> Medium </a>、<a class="ae kf" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或者<a class="ae kf" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系到我。</p><h1 id="ce80" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">参考</h1><p id="1642" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">[1] SVD 教程:<a class="ae kf" href="https://cs.fit.edu/~dmitra/SciComp/Resources/singular-value-decomposition-fast-track-tutorial.pdf" rel="noopener ugc nofollow" target="_blank">https://cs . fit . edu/~ dmitra/sci comp/Resources/singular-value-decomposition-fast-track-Tutorial . pdf</a></p><p id="4dce" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]http://www1.se.cuhk.edu.hk/~seem5680/lecture/LSI-Eg.pdf CUHK LSI 教程:<a class="ae kf" href="http://www1.se.cuhk.edu.hk/~seem5680/lecture/LSI-Eg.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="69e6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]斯坦福 LSI 教程:<a class="ae kf" href="https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf</a></p><p id="a7c2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[4]https://cs.stanford.edu/~ppasupat/a9online/1140.html 和 LSA 解释:<a class="ae kf" href="https://cs.stanford.edu/~ppasupat/a9online/1140.html" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>