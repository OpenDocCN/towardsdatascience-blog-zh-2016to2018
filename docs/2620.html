<html>
<head>
<title>How to potty train a Siamese Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何如厕训练暹罗网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-potty-train-a-siamese-network-3df6ca5e44da?source=collection_archive---------4-----------------------#2018-02-13">https://towardsdatascience.com/how-to-potty-train-a-siamese-network-3df6ca5e44da?source=collection_archive---------4-----------------------#2018-02-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/133895842c1ad88de9f581e07b2931e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mcElUy9GrqfxCCRJIXkH7A.jpeg"/></div></div></figure><div class=""/><p id="eff3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">是时候对我的一次性学习方法进行更新了，我使用了一个基于LSTM的深度神经网络，我们开发该网络是为了通过流量分析来识别电信网络故障。当我们将机器升级到最新的TensorFlow和Keras时，许多小细节都必须更改。仅此一项就引入了一些新的行为……同时我们获得了新例子的新数据，并发现了我们模型的一些问题。我不打算介绍所有的变化，但一些主要的变化以及一些有趣的发现。这感觉很像potty训练一只猫…如果你是这个系列的新手，你可以参考我以前的帖子:“<a class="ae kw" href="https://thelonenutblog.wordpress.com/2017/12/14/do-telecom-networks-dreams-of-siamese-memories/" rel="noopener ugc nofollow" target="_blank">电信网络会梦到暹罗记忆吗？</a>、<a class="ae kw" href="https://thelonenutblog.wordpress.com/2017/12/18/what-siamese-dreams-are-made-of/" rel="noopener ugc nofollow" target="_blank">暹罗梦是什么做成的… </a></p><p id="b13a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，Keras中的批量规范化现在在我的黑魔法列表中😊。我将不得不更多地挖掘它是如何实现的，尤其是火车时间和预测时间之间的差异。很长一段时间，我一直在想为什么我得到了极好的训练损失和差的验证损失，直到我删除了输入层上的批处理规范化。所以，有些东西需要研究。</p><p id="20fc" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其次，我介绍了用于训练和验证数据的数据生成器。对于必须提供大量相似和不相似线对的连体网络方法，使用生成器在某些时候是必须掌握的！一旦掌握了要领，就相当方便了。我发现Shervine Amidi的博客:“<a class="ae kw" href="https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly.html" rel="noopener ugc nofollow" target="_blank">一个关于如何使用Keras </a>的数据生成器的详细示例”是一个很好的解释示例。我将把它推荐给任何学习Keras数据生成器的人。</p><p id="958d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这个过程中，我发现我的triplet_loss函数(如前一篇文章所示)是有缺陷的…因为我用Keras concatenate打包基本神经网络输出的方式，我必须显式地指定范围。此外，我痛苦地了解到，Keras中的损失函数传递的是一小批y_true/y_pred值，而不是单个值。嗯，这一点乍一看对我来说并不清楚…我还利用这个机会修改了逻辑，使用了更多的Keras方法而不是TensorFlow(微妙的变化)。下面是新的损失函数。</p><figure class="kx ky kz la gt is"><div class="bz fp l di"><div class="lb lc l"/></div></figure><p id="50d2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第四个有趣的事情是，当我调试所有这些问题时，我觉得需要更好地可视化结果，而不是简单地查看预测值。我将输出向量空间从10维减少到3维，因为我现在没有太多不同的例子，所以3D应该足以将它们分开。此外，我更改了输出图层，使用sigmoid激活函数将输出空间限制在[0，1]范围内。这些变化反过来使我能够在变换的空间中查看预测点的位置，例如，交通模式现在对应于该输出空间中的3D位置。</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ld"><img src="../Images/7c4f9f282765ba54736cbcf2963a0141.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GRpjxThOjbmpuknfyt1PrA.png"/></div></div></figure><p id="e5df" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面我制作了一个视频，展示了这种投射是如何通过训练而演变的。最初，当神经网络用随机值初始化时，输出点在中心混杂在一起。但是很快我们看到它们被分开了，各自占据了空间的一角。当然，当神经网络试图找到一个更好的解决方案时，会有很多来回跳动，但我们可以看到，我们可以找到一个最佳点，在那里不同的流量模式可以很好地分开。顺便提一下，我们在这里看到了三种不同的流量模式。绿色表示正常流量，两种不同的错误情况，一种是红色表示所有流量都被阻塞，另一种是橙色表示我们达到了通信链路的容量极限。</p><figure class="kx ky kz la gt is"><div class="bz fp l di"><div class="le lc l"/></div></figure><p id="8080" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，在从我们的测试平台获取更多数据的同时，我们正在尝试使用不同的损失函数来分离流量。我的一个同事刚刚发布了一个不同损失函数的比较:“<a class="ae kw" rel="noopener" target="_blank" href="/lossless-triplet-loss-7e932f990b24">无损三重损失</a>”。我也可能尝试一些不同的损失函数，并展示我的发现。</p><p id="ca51" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我希望这表明，使用暹罗网络的一次性学习可以用于人脸识别以外的其他目的。在这种情况下，我们成功地将其用于信令流量分类和故障检测。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><p id="0ccb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="lm">原载于2018年2月13日</em><a class="ae kw" href="https://thelonenutblog.wordpress.com/2018/02/13/how-to-potty-train-a-siamese-network/" rel="noopener ugc nofollow" target="_blank"><em class="lm">【thelonenutblog.wordpress.com</em></a><em class="lm">。</em></p><p id="8b62" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="lm">封面照片由</em><a class="ae kw" href="https://pixabay.com/en/users/Jan-Mallander-615621/" rel="noopener ugc nofollow" target="_blank"><em class="lm">Jan-Mallander</em></a><em class="lm">at</em><a class="ae kw" href="https://pixabay.com" rel="noopener ugc nofollow" target="_blank"><em class="lm">Pixabay</em></a><em class="lm">。</em></p></div></div>    
</body>
</html>