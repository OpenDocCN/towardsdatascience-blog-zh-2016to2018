<html>
<head>
<title>Another Twitter sentiment analysis with Python — Part 6 (Doc2Vec)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python的另一个Twitter情感分析—第6部分(Doc2Vec)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-6-doc2vec-603f11832504?source=collection_archive---------1-----------------------#2018-01-18">https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-6-doc2vec-603f11832504?source=collection_archive---------1-----------------------#2018-01-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/33cec6c3f257dab3153c04a3a759dd9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TSc4AGpvGDzInutN"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@markuswinkler?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Markus Winkler</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="034c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是我正在进行的推特情感分析项目的第六部分。你可以从下面的链接找到以前的帖子。</p><ul class=""><li id="172e" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/another-twitter-sentiment-analysis-bb5b01ebad90">第一部分:数据清理</a></li><li id="3ac1" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/another-twitter-sentiment-analysis-with-python-part-2-333514854913">第二部分:EDA，数据可视化</a></li><li id="2352" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/another-twitter-sentiment-analysis-with-python-part-3-zipfs-law-data-visualisation-fc9eadda71e7">第三部分:齐夫定律，数据可视化</a></li><li id="501a" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/another-twitter-sentiment-analysis-with-python-part-4-count-vectorizer-b3f4944e51b5">第四部分:特征提取(计数矢量器)、N-gram、混淆矩阵</a></li><li id="8973" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/another-twitter-sentiment-analysis-with-python-part-5-50b4e87d9bdd">第5部分:特征提取(Tfidf矢量器)、机器学习模型比较、词法方法</a></li></ul><p id="bdf3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">*除了我将附上的简短代码块，你可以在这篇文章的末尾找到整个Jupyter笔记本的链接。</p><p id="518c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们跳到doc2vec之前，先提一下word2vec会比较好。Word2vec是一组用于产生单词嵌入的相关模型。这些模型是浅层的两层神经网络，经过训练可以重建单词的语言上下文。”</p><p id="7d9a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Word2vec不是一个单一的算法，而是由两种技术组成——CBOW(连续单词包)和Skip-gram模型。这两种技术都学习作为单词向量表示的权重。对于语料库，CBOW模型从周围上下文单词的窗口中预测当前单词，而Skip-gram模型在给定当前单词的情况下预测周围上下文单词。在Gensim包中，在实现Word2Vec时，可以通过传递参数“sg”来指定是使用CBOW还是Skip-gram。默认情况下(sg=0)，使用CBOW。否则(sg=1)，使用skip-gram。</p><p id="06d8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，假设我们有下面的句子:“我喜欢狗”。当给定“我”、“狗”作为输入时，CBOW模型试图预测单词“爱”，另一方面，当给定单词“爱”作为输入时，Skip-gram模型试图预测“我”、“狗”。</p><p id="c905" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图更正式地展示了这两个模型是如何工作的。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/04f19ceeac2867c32d57594051eca1db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*6YmcrrGj1_wAmv0BQBarhw.png"/></div></figure><p id="7a92" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，用作单词向量的实际上不是这些模型的预测结果，而是经过训练的模型的权重。通过提取权重，这样的向量以某种抽象的方式来表示单词的“含义”。如果想更详细的了解word2vec模型是如何工作的，有一篇<a class="ae kc" href="https://arxiv.org/pdf/1411.2738.pdf" rel="noopener ugc nofollow" target="_blank">欣荣(2016) </a>的很棒的论文，详细的讲解了模型的每一步。</p><p id="faf8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那doc2vec是什么？Doc2vec使用与word2vec相同的逻辑，但是将其应用于文档级。根据<a class="ae kc" href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" rel="noopener ugc nofollow" target="_blank"> Le和Mikolov(2014) </a>，“每个段落被映射到一个唯一的向量，由矩阵D中的一列表示，每个单词也被映射到一个唯一的向量，由矩阵w中的一列表示。段落向量和单词向量被平均或连接以预测上下文中的下一个单词……段落标记可以被认为是另一个单词。它起到了记忆的作用，记住了当前上下文或者段落主题中缺少的内容。”</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lu"><img src="../Images/c39fb16788044062c6d75bec70d838ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9tVCGDm-ytPydhtJWVx3Zw.png"/></div></div></figure><p id="94ac" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DM:这是类似于Word2vec中CBOW模型的Doc2Vec模型。段落向量是通过在基于上下文单词和上下文段落推断中心单词的任务上训练神经网络而获得的。</p><p id="b51a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DBOW:这是Doc2Vec模型对Word2Vec中的Skip-gram模型的模拟。段落向量是通过训练神经网络来获得的，该神经网络的任务是在给定从段落中随机采样的单词的情况下，预测段落中单词的概率分布。</p><p id="642c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我使用Python库Gensim实现了Doc2Vec模型。在DM模型的情况下，我实现了平均方法和连接方法。这是受<a class="ae kc" href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" rel="noopener ugc nofollow" target="_blank">乐和Mikolov (2014) </a>的研究论文的启发。在他们的论文中，他们用两种不同的方法实现了DM模型，一种是平均计算过程，另一种是串联计算方法。这在<a class="ae kc" href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb" rel="noopener ugc nofollow" target="_blank"> Gensim的教程</a>中也有展示。</p><p id="88e1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是我用来获取每条推文向量的方法。</p><ol class=""><li id="034f" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lv lh li lj bi translated">分布式单词包</li><li id="44dc" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lv lh li lj bi translated">分布式存储器级联</li><li id="0577" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lv lh li lj bi translated">分布式存储装置</li><li id="4bef" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lv lh li lj bi translated">DBOW + DMC</li><li id="57db" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lv lh li lj bi translated">DBOW + DMM</li></ol><p id="f7ad" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">利用从上述模型中获得的向量，我拟合了一个简单的逻辑回归模型，并在验证集上评估了结果。</p><p id="a489" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为准备，除了加载所需的依赖项，我们还需要使用Gensim的LabeledSentence函数用唯一的id标记每条tweet。</p><figure class="lq lr ls lt gt jr"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="0986" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了训练Doc2Vec，我使用了整个数据集。这背后的基本原理是doc2vec训练是完全无人监督的，因此没有必要保留任何数据，因为它是未标记的。这一基本原理的灵感来自于<a class="ae kc" href="https://arxiv.org/pdf/1607.05368.pdf" rel="noopener ugc nofollow" target="_blank"> Lau和Baldwin (2016) </a>在他们的研究论文《doc2vec的实证评估与文档嵌入生成的实际见解》中的基本原理</p><p id="72b2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样的基本原理也应用于<a class="ae kc" href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb" rel="noopener ugc nofollow" target="_blank"> Gensim的Doc2Vec教程</a>。在IMDB教程中，向量训练发生在数据集的所有文档上，包括所有训练/测试/开发集。</p><h2 id="0081" class="ly lz iq bd ma mb mc dn md me mf dp mg ko mh mi mj ks mk ml mm kw mn mo mp mq bi translated">分布式单词包</h2><p id="38d1" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">我们先训练纯DBOW模型。请注意，由于其训练方式，DBOW模型不会在单词级别产生任何有意义的向量。但是当我们谈到分布式内存模型时，我们也要看看单词向量。</p><p id="6542" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">据Gensim的开发者拉迪姆·řehůřek说，“这种算法运行方式的一个警告是，由于学习率在数据迭代过程中下降，在训练期间只在单个带标签的句子中看到的标签将以固定的学习率进行训练。这经常产生不太理想的结果。”</p><p id="9984" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面的迭代实现了带有附加混洗的显式多遍alpha缩减方法。这已经在Gensim的IMDB教程中介绍过了。</p><figure class="lq lr ls lt gt jr"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="0f63" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的代码块中，我还定义了一个函数“get_vectors”来从训练好的doc2vec模型中提取文档向量，这个函数也将在其他doc2vec模型中重用。</p><p id="a5b3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用逻辑回归在验证集上测试的准确度是73.89%。尽管DBOW模型不学习单个单词的含义，但作为输入到分类器的特征，它似乎正在完成它的工作。</p><p id="6cf5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但它似乎没有胜过计数矢量器或Tfidf矢量器(Tfidf矢量器具有100，000个特征，使用逻辑回归的验证集准确率为82.92%)。</p><h2 id="e000" class="ly lz iq bd ma mb mc dn md me mf dp mg ko mh mi mj ks mk ml mm kw mn mo mp mq bi translated">分布式存储器串联</h2><p id="6a49" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">现在让我们转到分布式内存模型，看看它是如何执行的。我将首先尝试用串联法进行训练。</p><figure class="lq lr ls lt gt jr"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="7543" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，验证集上的准确率是66.47%，这有点令人失望。但这并不意味着该模型未能在单词级别学习到有效的向量表示。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/78b464a9ca0c66e6be50eb37cc831a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Al_uoTiXm5svhJlPuDs_Hw.png"/></div></div></figure><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mx"><img src="../Images/9655ec1059a6daf8f683d9b5ed887167.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-HLZ82kBf_0K_zzfOzVT8A.png"/></div></div></figure><p id="285c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过在训练后查看与“facebook”相似的单词，看起来该模型恰当地抓住了SNS，web服务的含义。此外，该模式还成功地捕捉到了“小”的比较级形式，即“大”和“更大”。上面的代码行就像要求模型将与单词“bigger”和“small”相关联的向量相加，同时减去“big”等于顶部的结果，“small”。</p><h2 id="f7f1" class="ly lz iq bd ma mb mc dn md me mf dp mg ko mh mi mj ks mk ml mm kw mn mo mp mq bi translated">分布式存储装置</h2><p id="f318" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">我们试试另一种训练DM模型的方法。</p><figure class="lq lr ls lt gt jr"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="40cf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">验证集准确率为72.56%，远优于DMC模型，略低于DBOW模型。我们也来看看它学到了什么。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi my"><img src="../Images/32208c5e2e3f95f402d344ee7fdf8733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w9AFjfsvM_dpHh2XDWLmwA.png"/></div></div></figure><p id="ec89" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看起来它已经很好地理解了“好”的含义，模型为“好”选择的最相似的词是“太好了”。太好了！</p><h2 id="17c5" class="ly lz iq bd ma mb mc dn md me mf dp mg ko mh mi mj ks mk ml mm kw mn mo mp mq bi translated">组合模型</h2><p id="dcdf" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">由于我有来自三个不同模型的文档向量，现在我可以将它们连接起来，看看它是如何影响性能的。下面我定义了一个简单的函数来连接来自不同模型的文档向量。</p><figure class="lq lr ls lt gt jr"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="def2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DBOW + DMC模型的验证集准确率为74.58%，比纯DBOW模型(73.89%)有所提高。让我们试试另一种组合。</p><figure class="lq lr ls lt gt jr"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="a058" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这次通过将DBOW和DMM结合在一起，我得到了75.51%的验证集准确率。</p><p id="3d62" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于unigram，我了解到以不同的组合连接文档向量可以提高模型性能。我从单一模型得到的最好的验证准确率是DBOW的73.89%。通过连接向量，我用DBOW+DMM模型得到了75.51%的最高验证准确率。</p><p id="893e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下一篇文章中，我将看看使用Gensim的短语建模，并将其应用于Doc2Vec，看看这是否会影响性能。</p><p id="cb71" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢您的阅读，您可以通过下面的链接找到Jupyter笔记本。</p><p id="5ba9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://github.com/tthustla/twitter_sentiment_analysis_part6/blob/master/Capstone_part4-Copy4.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/tthustle a/Twitter _情操_分析_ part 6/blob/master/Capstone _ part 4-copy 4 . ipynb</a></p></div></div>    
</body>
</html>