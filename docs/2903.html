<html>
<head>
<title>Reinforcement Learning 101</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习 101</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292?source=collection_archive---------0-----------------------#2018-03-19">https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292?source=collection_archive---------0-----------------------#2018-03-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5dfd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">学习强化学习的要领！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3721470cbf00fb485f6ef98a25dfd154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vpfBGnwxjlmr9XRP3lw3XQ.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@danielkcheung" rel="noopener ugc nofollow" target="_blank">Daniel Cheung</a> on<a class="ae kv" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash</a></figcaption></figure><p id="53e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">强化学习(RL)是现代人工智能领域最热门的研究课题之一，其受欢迎程度还在不断增长。让我们看看开始学习 RL 需要知道的 5 件有用的事情。</p><h1 id="5ac9" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak"> 1。什么是强化学习？</strong> <strong class="ak">与其他 ML 技术相比如何？</strong></h1><p id="2058" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">强化学习(RL)是一种机器学习技术，它使代理能够在交互式环境中使用来自其自身行为和经验的反馈通过试错来学习。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/01f8188c3b0d653fdb173e038f3854e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*8OSHpISmR1l79yX4I234wg.jpeg"/></div></figure><p id="2888" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然监督学习和强化学习都使用输入和输出之间的映射，但与监督学习不同，监督学习向代理提供的反馈是执行任务的<strong class="ky ir">正确的动作集</strong>，强化学习使用<strong class="ky ir">奖励和惩罚</strong>作为积极和消极行为的信号。</p><p id="ebe0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与无监督学习相比，强化学习在目标方面是不同的。虽然无监督学习的目标是找到数据点之间的相似性和差异，但在强化学习的情况下，目标是找到一个合适的动作模型，使代理的总累积报酬最大化。下图显示了通用 RL 模型的<strong class="ky ir">行动奖励反馈回路</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/09f613d9fa9198745119ca7d1388d02d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7cuAqjQ97x1H_sBIeAVVZg.png"/></div></div></figure><h1 id="b0b8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak"> 2。如何公式化一个基本的强化学习问题？</strong></h1><p id="8204" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">描述 RL 问题基本要素的一些关键术语是:</p><ol class=""><li id="cc23" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated"><strong class="ky ir">环境— </strong>代理运行的物理世界</li><li id="a37b" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">状态— </strong>代理人的现状</li><li id="d5be" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">奖励— </strong>环境的反馈</li><li id="0a13" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">策略— </strong>将代理的状态映射到动作的方法</li><li id="d590" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">价值— </strong>代理在特定状态下采取行动将获得的未来回报</li></ol><p id="2860" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个 RL 的问题可以通过游戏得到最好的解释。就拿<a class="ae kv" href="https://en.wikipedia.org/wiki/Pac-Man" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">吃豆人</strong> </a>这个游戏来说，代理人(吃豆人)的目标是吃掉格子里的食物，同时避开途中的鬼魂。在这种情况下，网格世界是代理的交互环境。代理人会因为吃了食物而得到奖励，如果被鬼魂杀死(输掉游戏)则会受到惩罚。状态是代理在网格世界中的位置，总的累积奖励是代理赢得游戏。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/809bb66c1e4fe8337adb5fc612cd6a68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/1*D7JNcbvhP5UOR6_Ul-WJaw.gif"/></div></figure><p id="3eb0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了建立一个最优策略，代理人面临着探索新状态，同时最大化其整体报酬的困境。这就是所谓的<strong class="ky ir">勘探与开发</strong>权衡。为了平衡这两者，最好的整体策略可能需要短期的牺牲。因此，代理应该收集足够的信息，以便在未来做出最佳的整体决策。</p><p id="1581" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">马尔可夫决策过程</strong></a><strong class="ky ir">【MDPs】</strong>是描述 RL 中环境的数学框架，几乎所有的 RL 问题都可以用 MDPs 来公式化。一个 MDP 由一组有限的环境状态 S，一组在每个状态下的可能动作 A(s)，一个实值奖励函数 R(s)和一个转移模型 P(s '，s | a)组成。然而，真实世界的环境更可能缺乏任何关于环境动态的先验知识。在这种情况下，无模型 RL 方法就派上了用场。</p><p id="c93f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Q-learning" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">Q-learning</strong></a><strong class="ky ir"/>是一种常用的无模型方法，可用于构建自玩 PacMan 代理。它围绕着更新 Q 值的概念，Q 值表示在状态<em class="ng"> s </em>中执行动作<em class="ng"> a </em>的值。下面的值更新规则是 Q 学习算法的核心。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/f81ac7f0c582b3557383b76225b7bfe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VItpGaVoIUnh0RUEArqSGQ.png"/></div></div></figure><p id="082b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里有一个使用深度强化学习的 PacMan 代理的视频演示。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ni nj l"/></div></figure><h1 id="b47e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak"> 3。最常用的强化学习算法有哪些？</strong></h1><p id="c0de" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><strong class="ky ir"> Q-learning </strong>和<strong class="ky ir"> SARSA </strong>(状态-动作-奖励-状态-动作)是两种常用的无模型 RL 算法。它们的勘探战略不同，但开发战略相似。Q-learning 是一种非策略方法，其中代理基于从另一个策略导出的动作 a*来学习值，而 SARSA 是一种策略方法，其中它基于从其当前策略导出的当前动作<em class="ng"> a </em>来学习值。这两种方法实现起来很简单，但是缺乏通用性，因为它们不具备估计未知状态值的能力。</p><p id="7eda" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这可以通过更高级的算法来克服，例如使用神经网络来估计 Q 值的<a class="ae kv" href="https://deepmind.com/research/dqn/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">深度 Q 网络(DQNs) </strong> </a>。但是 dqn 只能处理离散的、低维的动作空间。</p><p id="ced4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">【DDPG】</strong></a>深度确定性策略梯度是一种无模型、非策略、行动者-批评家算法，通过在高维、连续的动作空间中学习策略来解决这个问题。下图是<strong class="ky ir">演员兼评论家</strong>建筑的代表。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/1ba3aeca3c6cc2857c4026a0d056c2f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*azzV78wFkRq9ePrzGnvf5Q.png"/></div></div></figure><h1 id="dd69" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak"> 4。强化学习有哪些实际应用？</strong></h1><p id="189d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">由于 RL 需要大量数据，因此它最适用于模拟数据容易获得的领域，如游戏、机器人。</p><ol class=""><li id="c2a1" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">RL 在构建用于玩电脑游戏的 AI 方面应用相当广泛。<a class="ae kv" href="https://deepmind.com/blog/alphago-zero-learning-scratch/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> AlphaGo Zero </strong> </a>是第一个在中国古代围棋比赛中击败世界冠军的计算机程序。其他包括雅达利游戏，西洋双陆棋等</li><li id="80a5" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">在机器人和工业自动化中，RL 用于使机器人能够为自己创建一个高效的自适应控制系统，该系统从自己的经验和行为中学习。<a class="ae kv" href="https://deepmind.com/research/publications/deep-reinforcement-learning-robotic-manipulation/" rel="noopener ugc nofollow" target="_blank"> DeepMind 的工作</a>关于<strong class="ky ir">利用异步策略</strong> <strong class="ky ir">更新</strong>进行机器人操作的深度强化学习就是一个很好的例子。观看这个有趣的演示视频。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="b490" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">RL 的其他应用包括抽象文本摘要引擎、可以从用户交互中学习并随时间改进的对话代理(文本、语音)、学习医疗保健中的最佳治疗策略以及用于在线股票交易的基于 RL 的代理。</p><h1 id="5ece" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak"> 5。强化学习怎么入门？</strong></h1><p id="d800" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">为了理解 RL 的基本概念，可以参考以下资源。</p><ol class=""><li id="d6ba" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated"><strong class="ky ir">强化学习——一本介绍</strong>的书，作者是强化学习之父——<a class="ae kv" href="https://en.wikipedia.org/wiki/Richard_S._Sutton" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">理查德·萨顿</strong> </a>和他的博士生导师<a class="ae kv" href="https://en.wikipedia.org/wiki/Andrew_Barto" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">安德鲁·巴尔托</strong> </a>。这本书的在线草稿可以在<a class="ae kv" href="http://incompleteideas.net/book/the-book-2nd.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</li><li id="9452" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><a class="ae kv" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">教材</strong> </a> <strong class="ky ir"> </strong>来自<strong class="ky ir">大卫·西尔弗</strong>包括视频讲座是一门很棒的 RL 入门课程。</li><li id="1b38" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">下面是另一个关于 RL 的<a class="ae kv" href="http://people.eecs.berkeley.edu/~pabbeel/nips-tutorial-policy-optimization-Schulman-Abbeel.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">技术教程</strong> </a>，作者是<strong class="ky ir">彼得·阿比尔</strong>和<strong class="ky ir">约翰·舒尔曼</strong>(开放人工智能/伯克利人工智能研究实验室)。</li></ol><p id="257c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于开始构建和测试 RL 代理，以下资源可能会有所帮助。</p><ol class=""><li id="6a08" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated"><a class="ae kv" href="http://karpathy.github.io/2016/05/31/rl/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">这篇博客</strong> </a>讲述了 Andrej Karpathy<strong class="ky ir">如何利用原始像素的策略梯度来训练神经网络 ATARI Pong 智能体</strong>将帮助您在仅 130 行 Python 代码中建立并运行您的第一个深度强化学习智能体。</li><li id="6cc7" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><a class="ae kv" href="https://deepmind.com/blog/open-sourcing-deepmind-lab/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> DeepMind Lab </strong> </a>是一个开源的 3D 类游戏平台，为基于智能体的人工智能研究创建了丰富的模拟环境。</li><li id="d862" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">项目马尔默 是另一个支持人工智能基础研究的人工智能实验平台。</li><li id="7353" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><a class="ae kv" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> OpenAI gym </strong> </a>是一个构建和比较强化学习算法的工具包。</li></ol></div></div>    
</body>
</html>