<html>
<head>
<title>Power of a Single Neuron</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单个神经元的功率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/power-of-a-single-neuron-perceptron-c418ba445095?source=collection_archive---------6-----------------------#2018-06-29">https://towardsdatascience.com/power-of-a-single-neuron-perceptron-c418ba445095?source=collection_archive---------6-----------------------#2018-06-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/1f9ec8dde87856a72e6b0de5f3d6ba85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNJFCsxJycOUCAjQ3n-CKg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Neuron (Image Source: <a class="ae kc" href="https://pixabay.com/illustrations/nerve-cell-neuron-brain-neurons-2213009/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/illustrations/nerve-cell-neuron-brain-neurons-2213009/</a>)</figcaption></figure><figure class="ke kf kg kh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi kd"><img src="../Images/bd56264ad683660d766a04eeb6a48394.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L9xLcwKhuZ2cuS8fF0ZjwA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Basic Unit of a Artificial Neural Network — Artificial Neuron</figcaption></figure><p id="98f7" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">神经网络是由<strong class="kk ir">个基本神经元</strong> —也叫<strong class="kk ir">个感知器</strong>(上图所示的一个基本单元——<em class="lg">中间的绿色圆圈</em>)排列成多层网络的组合(下图中的<em class="lg">)。要了解大型网络的工作原理和功率，首先我们需要了解单个单元的工作原理和功率。这就是我们在这篇文章中要关注的！</em></p><figure class="ke kf kg kh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lh"><img src="../Images/8f9c22180b4610028814ab874d4b8763.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BMgYbkYpMuCV_lH_TaNu7w.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">A Network of Neurons</figcaption></figure><p id="80ae" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">理解任何算法工作的最好方法是尝试自己编写代码。如果您可以编写一个简单的代码，通过计算每次迭代中的质心来创建数据聚类，那么您就知道 k-means。如果您可以编写一个简单的代码，在一个数据子样本上创建多个决策树，并从每个树上获取最大投票来分类一个数据点，您就知道随机森林。同样，如果您可以编写一个简单的代码，通过使用梯度下降来求解一个简单的线性方程、两个线性方程和多个线性方程，您就理解了神经网络和梯度下降。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><h1 id="9351" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">梯度下降</h1><p id="6aab" class="pw-post-body-paragraph ki kj iq kk b kl mn kn ko kp mo kr ks kt mp kv kw kx mq kz la lb mr ld le lf ij bi translated">神经网络的主干是梯度下降。要编写梯度下降的代码，我们首先需要理解它。</p><p id="7d39" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">假设我们有一个简单的线性方程来求解</p><figure class="ke kf kg kh gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/56ef0e1075e3a7fe245ef013a04f5551.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*8l5OJMHr4_4J_e0KIlrC1w.png"/></div></figure><p id="9be8" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">这里，我们需要找到 w1 和 w2 的值，这使得等式对于给定的 y、x1 和 x2 的值为真。如果我们简单地猜测 w1 和 w2 的值，我们将得到<strong class="kk ir"> y_hat </strong></p><figure class="ke kf kg kh gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/95e32d227a302efe985574e061330ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*aTIsXHtzW1dcP35bOXpOrA.png"/></div></figure><p id="4e3f" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">我们可以通过下式计算由于猜测 w1 和 w2 的值而产生的误差</p><figure class="ke kf kg kh gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/c2eb24dc9aa0bb9efb4da0bff529d5c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*TK6xpQGYyH7taW8v2sOQnA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Cost Function</figcaption></figure><p id="261b" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">也就是我们一般所说的<strong class="kk ir">成本</strong>。现在，我们的目标是找出 w1 和 w2 的值，使得<strong class="kk ir">成本 C 最小</strong>。成本 C 是相对于 w1 和 w2 的可微分函数。根据一些微积分复习资料，如果我们对 w1 和 w2 的函数进行微分(求导),并使其等于 0，我们将得到 w1 和 w2 的值，此时成本将达到最小值。</p><figure class="ke kf kg kh gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/5e7b6d26c866021a92a7b67ba8ba9e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*DVTRZtkGILJDtswFsf7SrA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Gradients</figcaption></figure><p id="b1f4" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">这些导数称为<strong class="kk ir">梯度</strong>。基本上，函数在某点的导数是该函数在该点的切线或梯度。看上面的等式，我们不能得到 w1 和 w2 的值，其中成本导数将是 0，因为它们依赖于 w1 和 w2。</p><p id="7b20" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">现在，为了达到最小值，我们将通过每次少量更新权重，开始朝着最小值的方向(这意味着与梯度方向相反)迈出小步。简而言之，我们是在切线或梯度的相反方向下降——这就是名字<strong class="kk ir">“梯度下降”的原因。</strong></p><figure class="ke kf kg kh gt jr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/636119e29a35df0e16fa9fbffe74cc05.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*OT0w9tcMnWyJuFSbsaxxgA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Gradient Descent</figcaption></figure><figure class="ke kf kg kh gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/3d7793bfb8fe23d939f50305ba2a2c82.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*_mVSRAxb7lfBOe5QuemTmQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Pictorial Representation of Gradient Descent (Image Source: <a class="ae kc" href="https://scipython.com/blog/visualizing-the-gradient-descent-method/" rel="noopener ugc nofollow" target="_blank">https://scipython.com/blog/visualizing-the-gradient-descent-method/</a>)</figcaption></figure></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><h2 id="59f0" class="my lq iq bd lr mz na dn lv nb nc dp lz kt nd ne md kx nf ng mh lb nh ni ml nj bi translated"><strong class="ak">手动编码单神经元解简单线性方程</strong></h2><p id="80f7" class="pw-post-body-paragraph ki kj iq kk b kl mn kn ko kp mo kr ks kt mp kv kw kx mq kz la lb mr ld le lf ij bi translated">使用实现梯度下降的 numpy</p><figure class="ke kf kg kh gt jr"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="ece8" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">纪元是我们朝着成本最小化迈出一小步的迭代。学习速度告诉你要迈出多小的一步。一大步永远不会让你达到最小值，而非常小的一步要花太多时间才能达到成本最小值。</p><p id="a87d" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">通过让 x1 = 3、x2 = 8 和 y = 41 来测试函数</p><figure class="ke kf kg kh gt jr"><div class="bz fp l di"><div class="nk nl l"/></div></figure><div class="ke kf kg kh gt ab cb"><figure class="nm jr nn no np nq nr paragraph-image"><img src="../Images/b3c3959aa7bc2ce73b351de40053badb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*R2V8cst2OpYaAtouIFn1oA.png"/></figure><figure class="nm jr ns no np nq nr paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/d9043c311b0628a5ac9aabfeb7b61a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*ZtoirKF9niDIzf5Sx1VZkw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk nt di nu nv">Results</figcaption></figure></div><p id="c37c" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">人们可以争辩说，一个方程可以有多个解——神经元将找到最接近开始猜测的解。</p><p id="0677" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated"><strong class="kk ir">单神经元求解两个线性方程</strong></p><p id="489f" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">同一个函数可以修改为求解两个方程。这个时间成本函数将是</p><figure class="ke kf kg kh gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e4de305e62d2f83814eceaa600a1ae84.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*llyE5GSwb53DRBaHZ8ifgQ.png"/></div></figure><figure class="ke kf kg kh gt jr"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ke kf kg kh gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/fe0b9e212e8f30c3cce2c77287a15ef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*taEVMovk8jFbLoyy5knujQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Cost Vs Epochs</figcaption></figure><p id="4c26" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated"><strong class="kk ir">单个神经元求解多个线性方程组</strong></p><p id="7987" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">同一个函数可以修改成解多个方程。这个时间成本函数将是</p><figure class="ke kf kg kh gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/0bd24a494174d1e1c1a573f98b73e943.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*8ejpTj8HZiSKB4Vh7K3jYQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Cost function for Multiple Equations</figcaption></figure><figure class="ke kf kg kh gt jr"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="f462" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">如果误差随着每个 epoc 的增加而增加，则降低学习率。如果误差减小，但没有低于阈值，则增加周期数(<strong class="kk ir">n _ 周期</strong>)。</p><p id="70b5" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">本文简单介绍了梯度下降算法，以及如何使用 numpy 求解线性方程来创建基本神经元。</p><p id="51c3" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">以上所有代码都可以在<a class="ae kc" href="https://github.com/sahuvaibhav/AI_Basics/blob/master/Neural_Netork_Handcoded/Neural%20Networks%20Linear%20Equations.ipynb" rel="noopener ugc nofollow" target="_blank">我的 git repo </a>找到。</p><p id="fd80" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">欢迎评论和反馈！</p></div></div>    
</body>
</html>