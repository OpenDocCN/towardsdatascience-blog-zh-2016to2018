# 1001 个黑盒的故事

> 原文：<https://towardsdatascience.com/the-tale-of-1001-black-boxes-62d12b5886aa?source=collection_archive---------18----------------------->

**或者为什么我认为亚马逊“种族主义”人工智能的故事实际上留下了乐观的空间**

如果你能上网，并且对人工智能感兴趣，你会偶然发现亚马逊最近的一个故事，它试图建立一个机器学习算法(或花哨的术语“人工智能”)来使[的招聘过程](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)自动化，结果发现它会惩罚包含“女性”一词的简历(例如，所有女子大学的候选人)。

正如任何季节性的“AI 变坏”新闻故事一样，首先必须打破一些神话。通常的反对意见通常有以下几种形式:*“如果女子学院的毕业生真的不够格，为什么惩罚她们就应该算作歧视呢？”*

记住上下文在这里是至关重要的。通过使用自己的招聘记录作为训练数据，亚马逊**没有**建立一个人工智能来学习如何聘用*成功的*候选人；根据定义，它所能做的就是建立一个人工智能，模仿它当前的招聘实践。这看似琐碎，但这一点被很多人忽略了。这就是语境的作用:我们有任何理由相信亚马逊的招聘行为没有偏见吗？

很久以前就已经确定，作为人类，我们的决策充满了各种各样的偏见。具体来说，在科技行业的招聘中，最常见的一种被称为“[确认偏差](https://en.wikipedia.org/wiki/Confirmation_bias)”。在早期，我们建立了一些成功的原型，我们倾向于寻找其他符合这些原型的人。当每一次“招聘”都是一项昂贵的投资时，**科技公司倾向于“利用”他们对成功候选人的现有理解，而不是“探索”**(给其他人机会，他们看起来不像典型的成功候选人，以证明他们的技能)。

好的，我们已经确定了 ML 是一个黑箱，它会使我们自己的偏见永久化。这能带来什么好处呢？

![](img/1864c1402941540b0a21fdc9bc09b60f.png)

Picture from [Pexels](https://www.pexels.com/photo/pile-of-black-shoe-box-next-to-wall-220685/)

我的观点是，现实世界中的人类决策过程可能比单个黑盒更糟糕。我们称之为**“亚马逊的招聘流程”**的实体由如此多的移动部分组成:它是一个由招聘人员、招聘经理和面试官组成的层级——每个人都有自己可能的偏见和怪癖——他们不一定遵循一个规定的议程。形象地说，**是一千个黑箱**。

这种结构意味着试图梳理出实际歧视的证据可能具有挑战性；亚马逊做出的每一个招聘决定都是可以否认的，因为它被归因于一个“有问题的”盒子(你读过多少次“这只是 X，Y，Z；这不代表我们公司的政策吗”？).

这就是为什么戴上一副合适的眼镜，ML 的进入可以被视为塞翁失马，焉知非福。当人类决策变得“算法化”时，它*就变成了*从输入(个人)到结果(决策)的明确映射。是的，这种映射对我们人类来说似乎极其复杂——就我们所知，是一个“黑盒”——但至少它是**的一个**这样的盒子。

通过对其历史招聘决策训练 ML 算法，亚马逊捕捉到了其决策过程的*系统*方面，即超越单一决策的方面。**亚马逊实际上将其 1000 个黑匣子减少到只有一个**。现在，仔细检查这个盒子(例如，通过检查通过将个人“喂入”黑盒获得的输入输出对，并观察预测的结果)可以为关于原始的、复杂的实体(即亚马逊的招聘实践)的假设提供更有力的基础。在这种情况下，它确实揭示了(或实现了推测)它的某些方面构成了明显的歧视候选人。

这是*而不是*说我们应该在敏感领域不经意地调用机器学习。亚马逊的故事是众多证明这显然是一个坏主意的例子之一，而且有越来越多的研究人员(我非常高兴成为其中的一员)正在仔细研究这些问题。这是试图说，尽管有太多的负面结果，关于 ML 可能促成的所有“坏事”，我们应该记住有一个潜在的积极前景:*如果使用得当，算法化可以作为一种工具，更好地理解和改进我们自己的实践。*