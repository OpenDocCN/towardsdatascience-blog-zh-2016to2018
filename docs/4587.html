<html>
<head>
<title>How to Use the Keras Tokenizer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用 Keras 记号赋予器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-classification-in-keras-part-2-how-to-use-the-keras-tokenizer-word-representations-fd571674df23?source=collection_archive---------7-----------------------#2018-08-24">https://towardsdatascience.com/text-classification-in-keras-part-2-how-to-use-the-keras-tokenizer-word-representations-fd571674df23?source=collection_archive---------7-----------------------#2018-08-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d002" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Keras 中教授 NLP 和文本分类系列的第 2 部分</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/39511c3e2424e3e485942912882657a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_WxD8_PMyHr66vkwtgh2lA.jpeg"/></div></div></figure></div><div class="ab cl kr ks hu kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ij ik il im in"><p id="552d" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">如果你还没看过第一部  <em class="lu">的话，别忘了看看</em> <a class="ae lv" rel="noopener" target="_blank" href="/text-classification-in-keras-part-1-a-simple-reuters-news-classifier-9558d34d01d3"> <em class="lu">！</em></a></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="72dc" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">如果你喜欢这个视频或觉得它有任何帮助，如果你给我一两美元来资助我的机器学习教育和研究，我会永远爱你！每一美元都让我离成功更近一步，我永远心存感激。</p><h1 id="623e" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">《恋恋笔记本》</h1><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="9f1b" class="mv lz iq mr b gy mw mx l my mz">import keras<br/>import numpy as np<br/>from keras.datasets import reuters<br/><br/>(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)<br/>word_index = reuters.get_word_index(path="reuters_word_index.json")<br/><br/>print('# of Training Samples: {}'.format(len(x_train)))<br/>print('# of Test Samples: {}'.format(len(x_test)))<br/><br/>num_classes = max(y_train) + 1<br/>print('# of Classes: {}'.format(num_classes))<br/><br/>from keras.preprocessing.text import Tokenizer<br/><br/>max_words = 10000<br/><br/>tokenizer = Tokenizer(num_words=max_words)<br/>x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')<br/>x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')<br/><br/>y_train = keras.utils.to_categorical(y_train, num_classes)<br/>y_test = keras.utils.to_categorical(y_test, num_classes)<br/><br/>print(x_train[0])<br/>print(len(x_train[0]))<br/>print(max(x_train[0]))<br/><br/>print(y_train[0])<br/>print(len(y_train[0]))<br/><br/>from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, Activation<br/><br/>model = Sequential()<br/>model.add(Dense(512, input_shape=(max_words,)))<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(num_classes))<br/>model.add(Activation('softmax'))<br/><br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>print(model.metrics_names)<br/><br/>batch_size = 32<br/>epochs = 2<br/><br/>history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)<br/>score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)<br/>print('Test loss:', score[0])<br/>print('Test accuracy:', score[1])<br/><br/>(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)<br/><br/>x_train = tokenizer.sequences_to_matrix(x_train, mode='count')<br/>x_test = tokenizer.sequences_to_matrix(x_test, mode='count')<br/><br/>y_train = keras.utils.to_categorical(y_train, num_classes)<br/>y_test = keras.utils.to_categorical(y_test, num_classes)<br/><br/>print(x_train[0])<br/>print(len(x_train[0]))<br/>print(max(x_train[0]))<br/>print(np.argmax(x_train[0]))<br/><br/>model = Sequential()<br/>model.add(Dense(512, input_shape=(max_words,)))<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(num_classes))<br/>model.add(Activation('softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/><br/>history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)<br/>score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)<br/>print('Test loss:', score[0])<br/>print('Test accuracy:', score[1])<br/><br/>(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)<br/><br/>x_train = tokenizer.sequences_to_matrix(x_train, mode='freq')<br/>x_test = tokenizer.sequences_to_matrix(x_test, mode='freq')<br/><br/>y_train = keras.utils.to_categorical(y_train, num_classes)<br/>y_test = keras.utils.to_categorical(y_test, num_classes)<br/><br/>print(x_train[0])<br/>print(len(x_train[0]))<br/>print(max(x_train[0]))<br/><br/>model = Sequential()<br/>model.add(Dense(512, input_shape=(max_words,)))<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(num_classes))<br/>model.add(Activation('softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/><br/>history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)<br/>score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)<br/>print('Test loss:', score[0])<br/>print('Test accuracy:', score[1])<br/><br/>(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)<br/><br/>tokenizer.fit_on_sequences(x_train)<br/><br/>x_train = tokenizer.sequences_to_matrix(x_train, mode='tfidf')<br/>x_test = tokenizer.sequences_to_matrix(x_test, mode='tfidf')<br/><br/>y_train = keras.utils.to_categorical(y_train, num_classes)<br/>y_test = keras.utils.to_categorical(y_test, num_classes)<br/><br/>print(x_train[0])<br/>print(len(x_train[0]))<br/>print(max(x_train[0]))<br/><br/>model = Sequential()<br/>model.add(Dense(512, input_shape=(max_words,)))<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(num_classes))<br/>model.add(Activation('softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/><br/>history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)<br/>score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)<br/>print('Test loss:', score[0])<br/>print('Test accuracy:', score[1])</span></pre></div></div>    
</body>
</html>