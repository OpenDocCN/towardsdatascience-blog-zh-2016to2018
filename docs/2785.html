<html>
<head>
<title>Machine Learning Workflow on Diabetes Data : Part 02</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">糖尿病数据的机器学习工作流程:第2部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-workflow-on-diabetes-data-part-02-11262b7f7a5c?source=collection_archive---------3-----------------------#2018-03-05">https://towardsdatascience.com/machine-learning-workflow-on-diabetes-data-part-02-11262b7f7a5c?source=collection_archive---------3-----------------------#2018-03-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="be2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本系列的上一篇文章中，我们讨论了关于糖尿病数据集的机器学习工作流。并讨论了诸如数据探索、数据清洗、特征工程基础和模型选择过程等主题。你可以在下面找到之前的文章。</p><div class="kl km gp gr kn ko"><a rel="noopener follow" target="_blank" href="/machine-learning-workflow-on-diabetes-data-part-01-573864fcc6b8"><div class="kp ab fo"><div class="kq ab kr cl cj ks"><h2 class="bd ir gy z fp kt fr fs ku fu fw ip bi translated">糖尿病数据的机器学习工作流程:第1部分</h2><div class="kv l"><h3 class="bd b gy z fp kt fr fs ku fu fw dk translated">“医疗环境中的机器学习可以帮助显著增强医疗诊断。”</h3></div><div class="kw l"><p class="bd b dl z fp kt fr fs ku fu fw dk translated">towardsdatascience.com</p></div></div><div class="kx l"><div class="ky l kz la lb kx lc ld ko"/></div></div></a></div><p id="8fe6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">选择模型后，我们能够确定<strong class="jp ir">逻辑回归</strong>比其他选择的分类模型表现更好。在本文中，我们将讨论机器学习工作流的下一阶段，高级特征工程和超参数调整。</p><h1 id="b8f5" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">第5阶段—特征工程(再次访问)</h1><p id="b8c7" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">并非所有的功能都必须是赢家。大多数时候，有些特性并不能改善模型。这种情况可以通过进一步分析与模型相关的特征来发现。"<strong class="jp ir">一个高度预测的特征可以弥补10个哑弹</strong>"。</p><p id="ee0b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如在阶段3中提到的，在模型选择之后，应该进一步讨论特征工程。因此，我们将分析选择的逻辑回归模型，以及特征重要性如何影响它。</p><p id="4835" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Scikit Learn提供了有用的方法，通过这些方法我们可以进行特征选择，并找出影响模型的特征的重要性。</p><ol class=""><li id="f0a4" class="mh mi iq jp b jq jr ju jv jy mj kc mk kg ml kk mm mn mo mp bi translated"><a class="ae mq" href="http://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">单变量特征选择</strong> </a>:统计检验可以用来选择那些与输出变量关系最强的特征。</li><li id="f271" class="mh mi iq jp b jq mr ju ms jy mt kc mu kg mv kk mm mn mo mp bi translated"><a class="ae mq" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">递归特征消除</strong> </a>:递归特征消除(或RFE)的工作原理是递归地删除属性，并在那些保留的属性上建立一个模型。它使用模型精度来确定哪些属性(和属性组合)对预测目标属性贡献最大。</li><li id="fa02" class="mh mi iq jp b jq mr ju ms jy mt kc mu kg mv kk mm mn mo mp bi translated"><a class="ae mq" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">主成分分析</strong> </a>:主成分分析(或称PCA)利用线性代数将数据集转换成压缩形式。通常这被称为数据简化技术。PCA的一个特性是可以选择变换结果中的维数或主分量。</li><li id="3c2e" class="mh mi iq jp b jq mr ju ms jy mt kc mu kg mv kk mm mn mo mp bi translated"><strong class="jp ir">特征重要性</strong>:随机森林和额外树等袋装决策树可以用来估计特征的重要性。</li></ol><p id="e6e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本教程中，我们将使用递归特征消除作为特征选择方法。</p><p id="304f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我们导入RFECV，它带有内置的交叉验证特性。与分类器模型相同，RFECV具有fit()方法，该方法接受特性和响应/目标。</p><h2 id="13f4" class="mw lf iq bd lg mx my dn lk mz na dp lo jy nb nc ls kc nd ne lw kg nf ng ma nh bi translated">逻辑回归—特征选择</h2><pre class="ni nj nk nl gt nm nn no np aw nq bi"><span id="712b" class="mw lf iq nn b gy nr ns l nt nu">from sklearn.feature_selection import RFECV</span><span id="53bc" class="mw lf iq nn b gy nv ns l nt nu">logreg_model = LogisticRegression()</span><span id="b86f" class="mw lf iq nn b gy nv ns l nt nu">rfecv = RFECV(estimator=logreg_model, step=1, cv=strat_k_fold, scoring='accuracy')<br/>rfecv.fit(X, y)</span></pre><p id="03f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">拟合后，它会显示一个属性<strong class="jp ir"> grid_scores_ </strong>，该属性会返回每个所选特征的精度分数列表。我们可以用它来绘制一个图表，以查看给定模型的最大精确度的特征数量。</p><pre class="ni nj nk nl gt nm nn no np aw nq bi"><span id="dfa1" class="mw lf iq nn b gy nr ns l nt nu">plt.figure()<br/>plt.title('Logistic Regression CV score vs No of Features')<br/>plt.xlabel("Number of features selected")<br/>plt.ylabel("Cross validation score (nb of correct classifications)")<br/>plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)<br/>plt.show()</span></pre><figure class="ni nj nk nl gt nx gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/fa042e9e65200f1c1a91abe1367789da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*bevOza6dbMNE7c2DN6jZOg.jpeg"/></div><figcaption class="nz oa gj gh gi ob oc bd b be z dk">Fig — Feature importance of Logistic Regression</figcaption></figure><p id="3e5d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过查看该图，我们可以看到，将4个特征输入到模型中会得到最佳的准确度分数。RFECV展示了<strong class="jp ir"> support_ </strong>，这是找出对预测贡献最大的特征的另一个属性。为了找出选择了哪些特性，我们可以使用下面的代码。</p><pre class="ni nj nk nl gt nm nn no np aw nq bi"><span id="ff15" class="mw lf iq nn b gy nr ns l nt nu">feature_importance = list(zip(feature_names, rfecv.support_))</span><span id="718e" class="mw lf iq nn b gy nv ns l nt nu">new_features = []</span><span id="abfb" class="mw lf iq nn b gy nv ns l nt nu">for key,value in enumerate(feature_importance):<br/>    if(value[1]) == True:<br/>        new_features.append(value[0])<br/>        <br/>print(new_features)</span></pre><blockquote class="od oe of"><p id="3faf" class="jn jo og jp b jq jr js jt ju jv jw jx oh jz ka kb oi kd ke kf oj kh ki kj kk ij bi translated">['妊娠'，'葡萄糖'，'身体质量指数'，'糖尿病血糖功能']</p></blockquote><p id="73f0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，给定的特征最适合预测响应类。我们可以对具有<strong class="jp ir">原始特征</strong>和<strong class="jp ir"> RFECV选定特征</strong>的模型进行比较，以查看准确性得分是否有所提高。</p><pre class="ni nj nk nl gt nm nn no np aw nq bi"><span id="bac3" class="mw lf iq nn b gy nr ns l nt nu"># Calculate accuracy scores <br/>X_new = diabetes_mod[new_features]</span><span id="8b29" class="mw lf iq nn b gy nv ns l nt nu">initial_score = cross_val_score(logreg_model, X, y, cv=strat_k_fold, scoring='accuracy').mean()<br/>print("Initial accuracy : {} ".format(initial_score))</span><span id="de7f" class="mw lf iq nn b gy nv ns l nt nu">fe_score = cross_val_score(logreg_model, X_new, y, cv=strat_k_fold, scoring='accuracy').mean()<br/>print("Accuracy after Feature Selection : {} ".format(fe_score))</span></pre><blockquote class="od oe of"><p id="ae7e" class="jn jo og jp b jq jr js jt ju jv jw jx oh jz ka kb oi kd ke kf oj kh ki kj kk ij bi translated">初始精度:0.7764400711728514 <br/>特征选择后的精度:0.2676486766</p></blockquote><p id="7971" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过观察精度，在将所选特征输入模型后，精度会略有提高。</p><h2 id="0d99" class="mw lf iq bd lg mx my dn lk mz na dp lo jy nb nc ls kc nd ne lw kg nf ng ma nh bi translated">梯度增强-要素选择</h2><p id="746c" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">我们还可以分析我们拥有的第二个最佳模型，即梯度增强分类器，以查看特征选择过程是否提高了模型准确性，以及在该过程之后它是否优于逻辑回归。</p><p id="3238" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们遵循与逻辑回归相同的程序</p><pre class="ni nj nk nl gt nm nn no np aw nq bi"><span id="e41b" class="mw lf iq nn b gy nr ns l nt nu">gb_model = GradientBoostingClassifier()</span><span id="1d84" class="mw lf iq nn b gy nv ns l nt nu">gb_rfecv = RFECV(estimator=gb_model, step=1, cv=strat_k_fold, scoring='accuracy')<br/>gb_rfecv.fit(X, y)</span><span id="720c" class="mw lf iq nn b gy nv ns l nt nu">plt.figure()<br/>plt.title('Gradient Boost CV score vs No of Features')<br/>plt.xlabel("Number of features selected")<br/>plt.ylabel("Cross validation score (nb of correct classifications)")<br/>plt.plot(range(1, len(gb_rfecv.grid_scores_) + 1), gb_rfecv.grid_scores_)<br/>plt.show()</span></pre><figure class="ni nj nk nl gt nx gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/cd4a48147e55271e321f28c92379b771.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*NBbp8JdSTZGBqfAk4q0sOA.jpeg"/></div><figcaption class="nz oa gj gh gi ob oc bd b be z dk">Fig — Feature importance of Gradient Boost</figcaption></figure><p id="de65" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，拥有4个输入特征可以产生最高的精度。</p><pre class="ni nj nk nl gt nm nn no np aw nq bi"><span id="22c7" class="mw lf iq nn b gy nr ns l nt nu">feature_importance = list(zip(feature_names, gb_rfecv.support_))</span><span id="15d5" class="mw lf iq nn b gy nv ns l nt nu">new_features = []</span><span id="78a2" class="mw lf iq nn b gy nv ns l nt nu">for key,value in enumerate(feature_importance):<br/>    if(value[1]) == True:<br/>        new_features.append(value[0])<br/>        <br/>print(new_features)</span></pre><blockquote class="od oe of"><p id="b32e" class="jn jo og jp b jq jr js jt ju jv jw jx oh jz ka kb oi kd ke kf oj kh ki kj kk ij bi translated">['葡萄糖'，'身体质量指数'，'糖尿病胰岛素功能'，'年龄']</p></blockquote><p id="9478" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以上4个特征最适合模型。我们可以比较特征选择前后的准确率。</p><pre class="ni nj nk nl gt nm nn no np aw nq bi"><span id="8d24" class="mw lf iq nn b gy nr ns l nt nu">X_new_gb = diabetes_mod[new_features]</span><span id="cd36" class="mw lf iq nn b gy nv ns l nt nu">initial_score = cross_val_score(gb_model, X, y, cv=strat_k_fold, scoring='accuracy').mean()<br/>print("Initial accuracy : {} ".format(initial_score))</span><span id="7e13" class="mw lf iq nn b gy nv ns l nt nu">fe_score = cross_val_score(gb_model, X_new_gb, y, cv=strat_k_fold, scoring='accuracy').mean()<br/>print("Accuracy after Feature Selection : {} ".format(fe_score))</span></pre><blockquote class="od oe of"><p id="a420" class="jn jo og jp b jq jr js jt ju jv jw jx oh jz ka kb oi kd ke kf oj kh ki kj kk ij bi translated">初始精度:0.764091206294081 <br/>特征选择后的精度:0.2666766667</p></blockquote><p id="e683" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，在特征选择之后，准确度有所提高。</p><p id="78e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而<strong class="jp ir">逻辑回归</strong>比梯度推进更准确。因此，我们将在参数调整阶段使用逻辑回归。</p><h1 id="22da" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">阶段6 —模型参数调整</h1><p id="a59d" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">Scikit Learn为模型提供了合理的默认参数，从而给出了不错的准确度分数。它还为用户提供了调整参数的选项，以进一步提高精度。</p><p id="939b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在分类器中，我们将选择逻辑回归进行微调，其中我们更改模型参数，以便可能提高特定数据集模型的准确性。</p><p id="ea89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以使用<a class="ae mq" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"> GridSearchCV </a>轻松执行穷举搜索，而不必手动搜索最佳参数，它会执行<em class="og">“对估计器的指定参数值进行穷举搜索”</em>。</p><p id="824a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这显然是一个非常方便的工具，但当要搜索的参数很高时，它会带来计算成本的代价。</p><blockquote class="od oe of"><p id="0c8a" class="jn jo og jp b jq jr js jt ju jv jw jx oh jz ka kb oi kd ke kf oj kh ki kj kk ij bi translated">重要提示:在使用GridSearchCV时，有些模型的参数相互之间不兼容。由于GridSearchCV使用所有给定参数的组合，如果两个参数不能相互配合，我们将无法运行GridSearchCV。</p><p id="796e" class="jn jo og jp b jq jr js jt ju jv jw jx oh jz ka kb oi kd ke kf oj kh ki kj kk ij bi translated">如果发生这种情况，可以提供一个参数网格列表来克服给定的问题。<strong class="jp ir">建议您阅读您正在尝试微调的的类文档，以了解参数之间的相互作用。</strong></p></blockquote><p id="5e8d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先我们导入GridSearchCV。</p><pre class="ni nj nk nl gt nm nn no np aw nq bi"><span id="e01d" class="mw lf iq nn b gy nr ns l nt nu">from sklearn.model_selection import GridSearchCV</span></pre><p id="f191" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">逻辑回归模型有一些超参数，这些超参数不能相互作用。因此，我们提供了一个具有兼容参数的网格列表来微调模型。通过反复试验，找到了以下兼容参数。</p><blockquote class="od oe of"><p id="2c54" class="jn jo og jp b jq jr js jt ju jv jw jx oh jz ka kb oi kd ke kf oj kh ki kj kk ij bi translated">逻辑回归类文档可以在这里找到<a class="ae mq" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote><pre class="ni nj nk nl gt nm nn no np aw nq bi"><span id="54c9" class="mw lf iq nn b gy nr ns l nt nu"># Specify parameters<br/>c_values = list(np.arange(1, 10))</span><span id="2202" class="mw lf iq nn b gy nv ns l nt nu">param_grid = [<br/>    {'C': c_values, 'penalty': ['l1'], 'solver' : ['liblinear'], 'multi_class' : ['ovr']},</span><span id="c6c4" class="mw lf iq nn b gy nv ns l nt nu">    {'C': c_values, 'penalty': ['l2'], 'solver' : ['liblinear', 'newton-cg', 'lbfgs'], 'multi_class' : ['ovr']}<br/>]</span></pre><p id="54db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我们将数据拟合到GridSearchCV，GridSearchCV针对给定的参数组合对数据执行K重交叉验证。这可能需要一段时间才能完成。</p><pre class="ni nj nk nl gt nm nn no np aw nq bi"><span id="6d0d" class="mw lf iq nn b gy nr ns l nt nu">grid = GridSearchCV(LogisticRegression(), param_grid, cv=strat_k_fold, scoring='accuracy')</span><span id="0a1e" class="mw lf iq nn b gy nv ns l nt nu">grid.fit(X_new, y)</span></pre><p id="fa53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在训练和评分完成之后，GridSearchCV提供了一些有用的属性来寻找最佳参数和最佳估计量。</p><pre class="ni nj nk nl gt nm nn no np aw nq bi"><span id="429c" class="mw lf iq nn b gy nr ns l nt nu">print(grid.best_params_)<br/>print(grid.best_estimator_)</span></pre><figure class="ni nj nk nl gt nx gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi ol"><img src="../Images/37fd9760b6ad7edf2701ae164a78a944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xSwaQy1k2ch0PhRdP12K6w.jpeg"/></div></div><figcaption class="nz oa gj gh gi ob oc bd b be z dk">Fig — Best attributes of the Logistic Regression</figcaption></figure><p id="5e67" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以观察到最佳超参数如下。</p><pre class="ni nj nk nl gt nm nn no np aw nq bi"><span id="f04a" class="mw lf iq nn b gy nr ns l nt nu">{'C': 1, 'multi_class': 'ovr', 'penalty': 'l2', 'solver': 'liblinear'}</span></pre><p id="d51e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以将最佳参数输入逻辑回归模型，并观察其准确性是否有所提高。</p><pre class="ni nj nk nl gt nm nn no np aw nq bi"><span id="3223" class="mw lf iq nn b gy nr ns l nt nu">logreg_new = LogisticRegression(C=1, multi_class='ovr', penalty='l2', solver='liblinear')</span><span id="bccc" class="mw lf iq nn b gy nv ns l nt nu">initial_score = cross_val_score(logreg_new, X_new, y, cv=strat_k_fold, scoring='accuracy').mean()<br/>print("Final accuracy : {} ".format(initial_score))</span><span id="65d5" class="mw lf iq nn b gy nv ns l nt nu">Final accuracy : 0.7805877119643279</span></pre><p id="9b0f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以得出结论，超参数调整并没有增加它的准确性。也许我们选择的超参数并不具有指示性。但是，欢迎您尝试添加更多的参数组合。</p><blockquote class="od oe of"><p id="0fdd" class="jn jo og jp b jq jr js jt ju jv jw jx oh jz ka kb oi kd ke kf oj kh ki kj kk ij bi translated">最重要的方面是进行超参数调整的过程，而不是结果本身。在大多数情况下，超参数调谐提高了精度。</p></blockquote><blockquote class="oq"><p id="8703" class="or os iq bd ot ou ov ow ox oy oz kk dk translated">我们设法实现了78.05%的分类准确率，可以说这是非常好的。</p></blockquote><h1 id="123c" class="le lf iq bd lg lh li lj lk ll lm ln lo lp pa lr ls lt pb lv lw lx pc lz ma mb bi translated">结论</h1><p id="c8a8" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">我们已经到了文章系列的结尾。在这个系列中，我们经历了整个机器学习工作流程。我们讨论了完成分类任务所需的理论和实践知识。</p><p id="8aa7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们讨论了机器学习的工作流程步骤，如数据探索、数据清理步骤、特征工程基础和高级特征选择、模型选择和使用Scikit Learn library的超参数调整。</p><p id="3cdf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在轮到你了！现在，您应该能够使用这些知识来尝试其他数据集。</p><p id="6ab3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">创建这篇文章的源代码可以在下面找到。</p><div class="kl km gp gr kn ko"><a href="https://github.com/LahiruTjay/Machine-Learning-With-Python/blob/master/Machine%20Learning%20Workflow%20on%20Diabetes%20Data.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="kp ab fo"><div class="kq ab kr cl cj ks"><h2 class="bd ir gy z fp kt fr fs ku fu fw ip bi translated">LahiruTjay/用Python学习机器</h2><div class="kv l"><h3 class="bd b gy z fp kt fr fs ku fu fw dk translated">这个库包含了各种用Python完成的机器学习的例子。</h3></div><div class="kw l"><p class="bd b dl z fp kt fr fs ku fu fw dk translated">github.com</p></div></div><div class="kx l"><div class="pd l kz la lb kx lc ld ko"/></div></div></a></div><p id="7f10" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你对这篇文章有任何问题，请不要犹豫，在下面留言或者给我发电子邮件:lahiru.tjay@gmail.com</p></div></div>    
</body>
</html>