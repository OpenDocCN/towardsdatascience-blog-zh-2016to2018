# 关于深度学习的矩阵演算的注记

> 原文：<https://towardsdatascience.com/notes-on-matrix-calculus-for-deep-learning-b9899effa7cf?source=collection_archive---------8----------------------->

基于此 [*论文*](http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html) 由[帕尔](http://parrt.cs.usfca.edu/)和[霍华德](https://www.kaggle.com/jhoward)。

*深度学习是一个令人兴奋的领域，正在产生巨大的现实世界影响。本文是基于特伦斯·帕尔和杰瑞米·霍华德的《深度学习所需的矩阵演算》的笔记集。*

![](img/735a7120f9467607136e1e42e2272a67.png)

深度学习都是线性代数。它是使用多层神经网络来解决复杂的问题。模型输入、多层中的神经元权重、激活函数等都可以被定义为向量。训练或利用神经网络所需的操作/转换本质上是非常并行的，同时应用于所有输入。向量/矩阵表示和可以在其上使用的线性代数运算非常适合于神经网络的流水线数据流模型。当输入、权重和函数被视为向量并且值流可以被视为矩阵上的操作时，数学变得非常简单。

深度学习也全是差异化！计算导数/用某种方法测量变化率在优化损失函数的训练阶段至关重要。从一组任意的网络模型权重 w 开始，目标是达到一组“最优”的权重，以便减少给定的损失函数。几乎所有的神经网络都使用反向传播方法来寻找这样一组权重。该过程包括确定权重值的变化如何影响输出。基于此，我们可以决定按比例增加或减少权重值。测量输出如何相对于权重的变化而变化与计算输出 w 相对于权重 w 的(偏)导数是相同的。对于所有层中的所有权重，对于所有训练示例，该过程重复多次。

矩阵微积分结合了数学的两个基本分支——线性代数和微积分。绝大多数人都是孤立地接触线性代数和微积分的。这两个话题本身就是重量级的。没有多少本科课程是以矩阵微积分为重点的。在研究反向传播等概念时，人们通常依靠直觉来弥合理解上的差距。大多数机器学习算法中的反向传播步骤都是关于计算导数和更新向量和矩阵中的值。大多数机器学习框架自己做繁重的工作，我们永远不会看到实际的导数被计算出来。然而，理解这在内部是如何工作的总是好的，如果你打算成为一个严肃的从业者或者从头开始开发一个 ML 库，这是必不可少的。

而 [*论文*](http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html) 则是面向 DL 从业者和编码者的，本质上是数学化的。关注符号以巩固你的理解是非常重要的。特别注意矢量的形状(长的或高的)，变量是标量还是矢量，矩阵的维数。向量用粗体字母表示。未经训练的眼睛可能不会注意到粗体 **f** 和斜体 *f* 字体之间的差异，但这在试图理解等式时会产生很大的差异。矢量的形状和方向也是如此。我走了很多兔子洞，试图理解一些东西，却发现我最初对这些术语的理解是错误的。

一件好事是函数概念的定义方式(以及计算其导数的方法)从简单到复杂。首先，我们从由 *f* ( *x* )表示的简单参数的函数开始。函数和参数 *x* 都是标量(用斜体表示)，我们可以用传统的求导规则来求 *f(x)* 的导数。第二，我们将会看到的这种函数通常有许多相关的变量；形式为 *f* (x，y，z)。为了计算这种函数的导数，我们使用相对于特定参数计算的偏导数。处理这种函数的微积分分支是多元微积分。

将输入变量 *x，y，z* 分组为粗体的向量 **x** ，我们可以将输入参数向量的标量函数表示为 *f* ( **x** )。这个域的演算将是向量演算，其中 *f* ( **x** )的偏导数被表示为向量本身，并且服从于各种向量运算。最后，对深度学习最有用的是同时表示多个这样的函数。我们用 **f** ( **x** )来表示一组形式为*f(****x****)的标量函数。*微积分对于这一领域是最普遍的，即矩阵微积分。

概括地说， *f(x)* 是标量变量的标量函数(使用简单的导数规则)，而 *f* ( **x** )是矢量变量 **x** 的标量函数(使用矢量演算规则)，而 **f(x)** 是许多标量值函数的矢量，每个函数依赖于一个输入矢量 **x** (使用矩阵演算规则)。摘要:本文演示了如何计算简单函数的导数，以及多元微积分中偏导数(∂/∂x)、向量微积分中梯度∇ *f* 函数、矩阵微积分中雅可比矩阵 *J* 之间的关系。不严谨地说，∇*f(***x***)*函数是 *f* 的偏导数以向量形式的集合。 **f(x)** 的雅可比基本上是一个个**∇*f(***x***)的*按行堆叠而成。**

**在计算偏导数的过程中，本文做了一些假设。重要的是要记住这些概念最终将应用到的最终产品，即计算输出函数(y = w.x +b)和损失函数的偏导数。这篇论文通过预示它们将在哪里被使用而提供了一瞥。第一个假设是向量 **x** 的基数等于 **f** 中标量函数的个数。这就产生了一个漂亮的正方形雅可比矩阵。如果你想知道为什么它们需要相等，考虑这样一种情况，即神经元 *xi* 的每个输入都与一个权重 *wi* 相关联(这里的标量函数类似于 *xi*wi* )，因此我们有多少个 *x* 就有多少个 *w***

**另一个重要的假设是关于元素的对角属性。基本上，该属性声明 **f(x)** 中的 *ith* 标量函数是(仅)向量 x 中的 *ith* 项的函数。同样，当您想到常见的神经元模式用例时，这更有意义。输入 *xi* 的贡献与单个参数 *wi* 成比例。假设基于元素的对角属性使雅可比矩阵(通过第一个假设变成正方形)成为对角矩阵，所有非对角项为零。**

**本文接下来的几个部分解释了计算更复杂函数的导数的过程。函数从简单到复杂有几种方式。首先，考虑通过对两个向量(当然，大小相同)应用元素式二元运算符而得到的函数。这些是形式为 **f(x，y)** = **x + y，**或max **(x，y)的函数。**注意，在这种情况下，x，y 是矢量。接下来，还有标量扩展函数，它是通过将标量乘/加到一个向量上得到的(可能会让我们中的一些人想起 Numpy 中的[广播](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html))。这种运算包括将标量“扩展”到与向量相同的维数，然后执行逐元素的乘法、加法运算。例如 **y** = **x** + b ( y，x 是向量)意味着 b 被扩展为向量 **b** 并逐元素添加到**x****

**第三，考虑将向量中的值折叠成单个值的函数。最常见的例子是计算神经网络的损失，通常形式为*y*=*sum*(**f(x)**)。这里 y 是通过将向量 **f(x)的元素相加得到的标量值。**文中计算了这三种情况的导数。有些函数可以更复杂，为此要使用导数的链式法则。本文描述了简单标量函数的链规则，并逐步将其扩展到最通用的向量链规则。**