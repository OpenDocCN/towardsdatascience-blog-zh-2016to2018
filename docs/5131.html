<html>
<head>
<title>Building a Simple Machine Learning Model on Breast Cancer Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于乳腺癌数据构建简单的机器学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-simple-machine-learning-model-on-breast-cancer-data-eca4b3b99fa3?source=collection_archive---------2-----------------------#2018-09-29">https://towardsdatascience.com/building-a-simple-machine-learning-model-on-breast-cancer-data-eca4b3b99fa3?source=collection_archive---------2-----------------------#2018-09-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/d31b278accc22cf1dcc2222914dc6313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*9AWKjTkBm-Tr6UKnQmzSsA.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Fig: Machine Learning Model</figcaption></figure></div><div class="ab cl jy jz hu ka" role="separator"><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd"/></div><div class="ij ik il im in"><h1 id="1f24" class="kf kg iq bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">简介</strong></h1><p id="d35c" class="pw-post-body-paragraph ld le iq lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">乳腺癌(BC)是全球女性中最常见的癌症之一，根据全球统计数据，其代表了大多数新癌症病例和癌症相关死亡，使其成为当今社会的重大公共健康问题。</p><p id="5dc4" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">BC 的早期诊断可以显著改善预后和生存机会，因为它可以促进对患者进行及时的临床治疗。良性肿瘤的进一步精确分类可以防止患者接受不必要的治疗。因此，BC 的正确诊断和患者的恶性或良性分类是许多研究的主题。由于其在从复杂 BC 数据集中检测关键特征方面的独特优势，机器学习(ML)被广泛认为是 BC 模式分类和预测建模的首选方法。</p><p id="d593" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">分类和数据挖掘方法是对数据进行分类的有效方法。尤其是在医学领域，这些方法被广泛用于诊断和分析以做出决策。</p><h2 id="0cd3" class="mg kg iq bd kh mh mi dn kl mj mk dp kp lo ml mm kt ls mn mo kx lw mp mq lb mr bi translated">推荐的筛查指南:</h2><p id="0bd9" class="pw-post-body-paragraph ld le iq lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lf ir">乳房 x 光检查。</strong>乳腺癌最重要的筛查测试是乳房 x 光检查。乳房 x 光片是乳房的 x 光片。它可以在您或您的医生感觉到肿瘤之前两年内检测出乳腺癌。</p><p id="9ac1" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated"><strong class="lf ir">年龄在 40-45 岁或以上的女性</strong>有患乳腺癌的平均风险，应该每年做一次乳房 x 光检查。</p><p id="134a" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">高危女性应该从 30 岁开始每年进行乳房 x 光检查和核磁共振检查。</p><h2 id="c2e3" class="mg kg iq bd kh mh mi dn kl mj mk dp kp lo ml mm kt ls mn mo kx lw mp mq lb mr bi translated">乳腺癌的一些风险因素</h2><p id="de1d" class="pw-post-body-paragraph ld le iq lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">以下是一些已知的乳腺癌风险因素。然而，大多数乳腺癌病例无法与特定原因联系起来。请向您的医生咨询您的具体风险。</p><p id="8d3e" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated"><strong class="lf ir">年龄。随着女性年龄的增长，患乳腺癌的几率也会增加。将近 80%的乳腺癌发生在 50 岁以上的女性身上。</strong></p><p id="7d94" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated"><strong class="lf ir">乳腺癌个人史。</strong>一个乳房患过乳腺癌的女性，另一个乳房患癌的风险更高。</p><p id="1358" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated"><strong class="lf ir">乳腺癌家族史。</strong>如果母亲、姐妹或女儿患有乳腺癌，女性患乳腺癌的风险更高，尤其是在年轻时(40 岁之前)。有其他亲属患有乳腺癌也可能会增加风险。</p><p id="e7a1" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated"><strong class="lf ir">遗传因素。</strong>具有某些基因突变(包括 BRCA1 和 BRCA2 基因的改变)的女性在其一生中患乳腺癌的风险更高。其他基因变化也可能增加患乳腺癌的风险。</p><p id="3f75" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated"><strong class="lf ir">生育和月经史。生第一个孩子的女性年龄越大，患乳腺癌的风险就越大。风险较高的还有:</strong></p><ul class=""><li id="69b7" class="ms mt iq lf b lg mb lk mc lo mu ls mv lw mw ma mx my mz na bi translated">年轻时(12 岁前)第一次来月经的妇女</li><li id="52f0" class="ms mt iq lf b lg nb lk nc lo nd ls ne lw nf ma mx my mz na bi translated">绝经较晚的女性(55 岁以后)</li><li id="77b9" class="ms mt iq lf b lg nb lk nc lo nd ls ne lw nf ma mx my mz na bi translated">从未生过孩子的女人</li></ul><h1 id="b888" class="kf kg iq bd kh ki ng kk kl km nh ko kp kq ni ks kt ku nj kw kx ky nk la lb lc bi translated">阶段 0 —数据准备</h1><p id="2da4" class="pw-post-body-paragraph ld le iq lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将使用 UCI 乳腺癌机器学习知识库<a class="ae nl" href="http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29" rel="noopener ugc nofollow" target="_blank">数据集</a>。</p><blockquote class="nm nn no"><p id="323b" class="ld le np lf b lg mb li lj lk mc lm ln nq md lq lr nr me lu lv ns mf ly lz ma ij bi translated"><a class="ae nl" href="http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29" rel="noopener ugc nofollow" target="_blank">http://archive . ics . UCI . edu/ml/datasets/breast+cancer+Wisconsin+% 28 diagnostic % 29</a></p></blockquote><p id="bbd5" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">这个故事中使用的数据集是公开可用的，由美国威斯康星州麦迪逊市威斯康星大学医院的内科医生 William H. Wolberg 博士创建。为了创建数据集，Wolberg 博士使用了从实性乳腺肿块患者身上采集的液体样本，以及一种易于使用的图形计算机程序 Xcyt，该程序能够基于数字扫描进行细胞学特征分析。该程序使用曲线拟合算法来计算样本中每个细胞的 10 个特征，然后计算图像每个特征的平均值、极值和标准误差，返回 30 个实值向量</p><p id="412c" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">属性信息:</p><ol class=""><li id="9f9f" class="ms mt iq lf b lg mb lk mc lo mu ls mv lw mw ma nt my mz na bi translated">ID 号 2)诊断(M =恶性，B =良性)3–32)</li></ol><p id="8cf6" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">为每个细胞核计算十个实值特征:</p><ol class=""><li id="0c6b" class="ms mt iq lf b lg mb lk mc lo mu ls mv lw mw ma nt my mz na bi translated">半径(从中心到周边各点的平均距离)</li><li id="bf59" class="ms mt iq lf b lg nb lk nc lo nd ls ne lw nf ma nt my mz na bi translated">纹理(灰度值的标准偏差)</li><li id="cdf8" class="ms mt iq lf b lg nb lk nc lo nd ls ne lw nf ma nt my mz na bi translated">周长</li><li id="5c11" class="ms mt iq lf b lg nb lk nc lo nd ls ne lw nf ma nt my mz na bi translated">区域</li><li id="5af7" class="ms mt iq lf b lg nb lk nc lo nd ls ne lw nf ma nt my mz na bi translated">平滑度(半径长度的局部变化)</li><li id="bfa6" class="ms mt iq lf b lg nb lk nc lo nd ls ne lw nf ma nt my mz na bi translated">紧凑性(周长/面积— 1.0)</li><li id="39f1" class="ms mt iq lf b lg nb lk nc lo nd ls ne lw nf ma nt my mz na bi translated">凹度(轮廓凹陷部分的严重程度)</li><li id="2266" class="ms mt iq lf b lg nb lk nc lo nd ls ne lw nf ma nt my mz na bi translated">凹点(轮廓凹陷部分的数量)</li><li id="1f10" class="ms mt iq lf b lg nb lk nc lo nd ls ne lw nf ma nt my mz na bi translated">对称</li><li id="e3cb" class="ms mt iq lf b lg nb lk nc lo nd ls ne lw nf ma nt my mz na bi translated">分形维数(“海岸线近似值”-1)</li></ol><p id="aecc" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">对每幅图像计算这些特征的平均值、标准误差和“最差”或最大值(三个最大值的平均值),得到 30 个特征。例如，字段 3 是平均半径，字段 13 是半径 SE，字段 23 是最差半径。</p><h2 id="fd7e" class="mg kg iq bd kh mh mi dn kl mj mk dp kp lo ml mm kt ls mn mo kx lw mp mq lb mr bi translated">目标</h2><p id="67a3" class="pw-post-body-paragraph ld le iq lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">该分析旨在观察哪些特征最有助于预测恶性或良性癌症，并了解可能有助于我们进行模型选择和超参数选择的总体趋势。目标是对乳腺癌是良性还是恶性进行分类。为了实现这一点，我使用了机器学习分类方法来拟合一个可以预测新输入的离散类的函数。</p><h1 id="da26" class="kf kg iq bd kh ki ng kk kl km nh ko kp kq ni ks kt ku nj kw kx ky nk la lb lc bi translated">第 1 阶段—数据探索</h1><p id="0844" class="pw-post-body-paragraph ld le iq lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将使用<strong class="lf ir"> <em class="np"> Spyder </em> </strong>来处理这个数据集。我们将首先导入必要的库，并将数据集导入 Spyder:</p></div><div class="ab cl jy jz hu ka" role="separator"><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd"/></div><div class="ij ik il im in"><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="16be" class="mg kg iq nz b gy od oe l of og">#importing the libraries<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span><span id="25d1" class="mg kg iq nz b gy oh oe l of og">#importing our cancer dataset<br/>dataset = pd.read_csv(‘cancer.csv')<br/>X = dataset.iloc[:, 1:31].values<br/>Y = dataset.iloc[:, 31].values</span></pre><figure class="nu nv nw nx gt jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/caee63543fb47c1c0d686dd74a7eb4ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*VLZNnUUZxPeRfg7evnjE1A.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Fig : Dataset and X set after importing the dataset</figcaption></figure><p id="5098" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">我们可以使用 pandas 的<strong class="lf ir"> head() </strong>方法来检查数据集。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="dcb9" class="mg kg iq nz b gy od oe l of og">dataset.head()</span></pre><figure class="nu nv nw nx gt jr gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/0dd674fb1bec287a019e41982e782adb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*FZ7g5LtbXCxGmOoXXQd2pA.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Fig : top 5 data of our dataset</figcaption></figure><p id="ce5d" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">我们可以使用 panda 数据集的“shape”属性找到数据集的维度。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="882e" class="mg kg iq nz b gy od oe l of og">print("Cancer data set dimensions : {}".format(dataset.shape))</span><span id="6a4a" class="mg kg iq nz b gy oh oe l of og">Cancer data set dimensions : (569, 32)</span></pre><p id="ed71" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">我们可以观察到数据集包含 569 行和 32 列。<em class="np">诊断</em>是我们将要预测的列，它表示癌症是 M =恶性还是 B =良性。1 表示癌症是恶性的，0 表示良性的。我们可以确定，在 569 人中，357 人被标记为 B(良性)，212 人被标记为 M(恶性)。</p><figure class="nu nv nw nx gt jr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/afce0d06567af35b4887b205f1d41086.png" data-original-src="https://miro.medium.com/v2/resize:fit:188/format:webp/1*-LaXrWS1f-L06FW3YaN-vQ.png"/></div></figure><p id="f2be" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">数据可视化是数据科学的一个重要方面。这有助于理解数据，也有助于向他人解释数据。Python 有几个有趣的可视化库，如 Matplotlib、Seaborn 等。</p><p id="5096" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">在本教程中，我们将使用构建在 matplotlib 之上的 pandas 可视化工具来查找要素的数据分布。</p><figure class="nu nv nw nx gt jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/dff918d27d65c89ed209a82cf86ea126.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*h6-K5mE6IPpnB6x6aMlutw.png"/></div></figure><figure class="nu nv nw nx gt jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/5dabc90ce0b0e774536e0e5e4f5b514c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*1dBULDsGQnOHJ8wjT-fGCA.png"/></div></figure><figure class="nu nv nw nx gt jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/5040348daba9a0339cb2ac6b34573067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*HSwnompSDg_xy9ZqMaLlOA.png"/></div></figure><figure class="nu nv nw nx gt jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/bf554c076fc15fad4cb3fb4f6fc1806c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*wcXX9XkBUIvxrNpGX0hxLw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Fig : Visualization of Dataset</figcaption></figure><p id="f1a7" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated"><strong class="lf ir">缺失或空数据点</strong></p><p id="25fa" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">我们可以使用下面的 pandas 函数找到数据集的任何缺失或空数据点(如果有的话)。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="b458" class="mg kg iq nz b gy od oe l of og">dataset.isnull().sum()<br/>dataset.isna().sum()</span></pre><figure class="nu nv nw nx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ol om di on bf oo"><div class="gh gi ok"><img src="../Images/ee4bb5e80185bf930965e78f3ab74db0.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*vBh84xDaMHTaEkHrtKDN9Q.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Fig : Observe missing data</figcaption></figure><h1 id="9863" class="kf kg iq bd kh ki ng kk kl km nh ko kp kq ni ks kt ku nj kw kx ky nk la lb lc bi translated">阶段 2 —分类数据</h1><p id="329f" class="pw-post-body-paragraph ld le iq lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">分类数据是包含标签值而非数值的变量。可能值的数量通常限于一个固定的集合。</p><p id="7f42" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">例如，通常按国家、性别、年龄组等来描述用户。</p><p id="0fdc" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">我们将使用标签编码器来标记分类数据。标签编码器是 Python 中 SciKit Learn 库的一部分，用于将分类数据或文本数据转换为数字，我们的预测模型可以更好地理解这些数字。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="8d8a" class="mg kg iq nz b gy od oe l of og">#Encoding categorical data values<br/>from sklearn.preprocessing import LabelEncoder<br/>labelencoder_Y = LabelEncoder()<br/>Y = labelencoder_Y.fit_transform(Y)</span></pre><figure class="nu nv nw nx gt jr gh gi paragraph-image"><div class="gh gi op"><img src="../Images/eb09cdf70865c225bae405e431f5b2d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*PY_Mer9Lim_q5wYMeePxvg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Fig: Diagnosis Data without Encoding</figcaption></figure><figure class="nu nv nw nx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ol om di on bf oo"><div class="gh gi oq"><img src="../Images/895af8d39347c35c7e62a5469cd48ba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*IQMruflI7LLL3A6TSAIVWQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Fig: Diagnosis Data after Encoding</figcaption></figure><h2 id="8abf" class="mg kg iq bd kh mh mi dn kl mj mk dp kp lo ml mm kt ls mn mo kx lw mp mq lb mr bi translated"><strong class="ak">分割数据集</strong></h2><p id="8109" class="pw-post-body-paragraph ld le iq lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们使用的数据通常分为训练数据和测试数据。训练集包含一个已知的输出，模型学习这个数据，以便以后推广到其他数据。我们有测试数据集(或子集)来测试我们的模型对这个子集的预测。</p><p id="e4de" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">我们将使用 Python 中的 SciKit-Learn 库通过 train_test_split 方法来实现这一点。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="b1c7" class="mg kg iq nz b gy od oe l of og"># Splitting the dataset into the Training set and Test set</span><span id="7169" class="mg kg iq nz b gy oh oe l of og">from sklearn.model_selection import train_test_split<br/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)</span></pre><figure class="nu nv nw nx gt jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/f43f46268392ae2856ba457e3044c6f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*D8lr4VapywwLzVYph97JVg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Fig: Training and test set</figcaption></figure><h1 id="7bb4" class="kf kg iq bd kh ki ng kk kl km nh ko kp kq ni ks kt ku nj kw kx ky nk la lb lc bi translated">阶段 3 —特征缩放</h1><p id="369e" class="pw-post-body-paragraph ld le iq lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">大多数情况下，您的数据集将包含在量级、单位和范围方面差异很大的要素。但是因为，大多数机器学习算法在它们的计算中使用两个数据点之间的欧几里德距离。我们需要将所有的特征提升到相同的数量级。这可以通过缩放来实现。这意味着您正在转换数据，使其符合特定的范围，如 0–100 或 0–1。</p><p id="c34a" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">我们将使用 SciKit-Learn 库中的 StandardScaler 方法。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="f1a1" class="mg kg iq nz b gy od oe l of og">#Feature Scaling</span><span id="7dde" class="mg kg iq nz b gy oh oe l of og">from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span></pre><h1 id="9f6f" class="kf kg iq bd kh ki ng kk kl km nh ko kp kq ni ks kt ku nj kw kx ky nk la lb lc bi translated">阶段 4 —型号选择</h1><p id="826a" class="pw-post-body-paragraph ld le iq lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是将机器学习应用于任何数据集的最令人兴奋的阶段。它也被称为预测最佳结果的算法选择。</p><p id="7807" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">通常数据科学家对大型数据集使用不同种类的机器学习算法。但是，在高层次上，所有这些不同的算法可以分为两组:监督学习和非监督学习。</p><p id="ec95" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">为了不浪费太多时间，我将简单介绍一下这两种学习方式。</p><p id="d007" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">监督学习:监督学习是一种既提供输入数据又提供期望输出数据的系统。输入和输出数据被标记以便分类，从而为将来的数据处理提供学习基础。监督学习问题可以进一步分为<strong class="lf ir">回归</strong>和<strong class="lf ir">分类</strong>问题。</p><p id="b99e" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">一个<strong class="lf ir">回归</strong>问题是当输出变量是一个实值或连续值时，比如“工资”或“体重”。</p><p id="34c2" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">一个分类问题是当输出变量是一个类别时，如过滤邮件“垃圾邮件”或“非垃圾邮件”</p><p id="25f0" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">无监督学习:无监督学习是<a class="ae nl" href="https://whatis.techtarget.com/definition/algorithm" rel="noopener ugc nofollow" target="_blank">算法</a>使用既未分类也未标记的信息，并允许算法在没有指导的情况下对该信息进行操作。</p><p id="f046" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">在我们的数据集中，我们有结果变量或因变量，即 Y 只有两组值，M(恶性)或 B(良性)。所以我们将使用监督学习的分类算法。</p><p id="8861" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">在机器学习中，我们有不同类型的分类算法</p><p id="c645" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">1.逻辑回归</p><p id="7935" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">2.最近邻</p><p id="81a9" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">3.支持向量机</p><p id="5089" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">4.内核 SVM</p><p id="6a2b" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">5.朴素贝叶斯</p><p id="e4d0" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">6.决策树算法</p><p id="3d42" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">7.随机森林分类</p><p id="8a55" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">让我们开始应用算法:</p><p id="108d" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">我们将使用 sklearn 库导入分类算法的所有方法。</p><p id="cdc2" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">我们将使用 Logistic 回归方法进行模型选择，使用 Logistic 回归算法，</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="53ee" class="mg kg iq nz b gy od oe l of og">#Using Logistic Regression Algorithm to the Training Set</span><span id="6482" class="mg kg iq nz b gy oh oe l of og">from sklearn.linear_model import LogisticRegression<br/>classifier = LogisticRegression(random_state = 0)<br/>classifier.fit(X_train, Y_train)</span><span id="98e0" class="mg kg iq nz b gy oh oe l of og">#Using KNeighborsClassifier Method of neighbors class to use Nearest Neighbor algorithm</span><span id="d811" class="mg kg iq nz b gy oh oe l of og"><em class="np">from sklearn.neighbors import KNeighborsClassifier<br/>classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)<br/>classifier.fit(X_train, Y_train)</em><br/></span><span id="fe06" class="mg kg iq nz b gy oh oe l of og">#Using SVC method of svm class to use Support Vector Machine Algorithm<br/></span><span id="612a" class="mg kg iq nz b gy oh oe l of og"><em class="np">from sklearn.svm import SVC<br/>classifier = SVC(kernel = 'linear', random_state = 0)<br/>classifier.fit(X_train, Y_train)</em><br/></span><span id="cb0e" class="mg kg iq nz b gy oh oe l of og">#Using SVC method of svm class to use Kernel SVM Algorithm<br/></span><span id="5a21" class="mg kg iq nz b gy oh oe l of og"><em class="np">from sklearn.svm import SVC<br/>classifier = SVC(kernel = 'rbf', random_state = 0)<br/>classifier.fit(X_train, Y_train)</em><br/></span><span id="15f2" class="mg kg iq nz b gy oh oe l of og">#Using GaussianNB method of naïve_bayes class to use Naïve Bayes Algorithm<br/></span><span id="0419" class="mg kg iq nz b gy oh oe l of og"><em class="np">from sklearn.naive_bayes import GaussianNB<br/>classifier = GaussianNB()<br/>classifier.fit(X_train, Y_train)</em><br/></span><span id="0a8b" class="mg kg iq nz b gy oh oe l of og">#Using DecisionTreeClassifier of tree class to use Decision Tree Algorithm</span><span id="d9f5" class="mg kg iq nz b gy oh oe l of og"><br/><em class="np">from sklearn.tree import DecisionTreeClassifier<br/>classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)<br/>classifier.fit(X_train, Y_train)</em></span><span id="53e9" class="mg kg iq nz b gy oh oe l of og"><br/>#Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm</span><span id="1646" class="mg kg iq nz b gy oh oe l of og"><br/><em class="np">from sklearn.ensemble import RandomForestClassifier<br/>classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)<br/>classifier.fit(X_train, Y_train)</em></span></pre><p id="52c3" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">现在，我们将预测测试集结果，并检查每个模型的准确性:</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="c71e" class="mg kg iq nz b gy od oe l of og">Y_pred = classifier.predict(X_test)</span></pre><p id="410c" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">为了检查准确性，我们需要导入度量类的混淆矩阵方法。混淆矩阵是一种将错误分类的数量制成表格的方式，即，基于真实类别在错误的分类箱中结束的预测类别的数量。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="b2fd" class="mg kg iq nz b gy od oe l of og">from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(Y_test, Y_pred)</span></pre><p id="9c31" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">我们将使用分类准确性方法来确定我们的模型的准确性。当我们使用术语“准确性”时，分类准确性就是我们通常所指的。它是正确预测数与输入样本总数的比率。</p><figure class="nu nv nw nx gt jr gh gi paragraph-image"><div class="gh gi or"><img src="../Images/9e60ab696edf08455c12b4c5e41e96e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*AQ3X4c8Ot2FDbyyCcOA7nA.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Fig: Accuracy</figcaption></figure><p id="efad" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">为了检查正确的预测，我们必须检查混淆矩阵对象并对角地添加预测结果，这将是正确预测的数量，然后除以预测的总数。</p><figure class="nu nv nw nx gt jr gh gi paragraph-image"><div class="gh gi os"><img src="../Images/64ae3e3385634aa20cb399033ec0f3d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*KhvhT9jW-TnhAKnpZY6mSw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Fig: Confusion Matrix</figcaption></figure><p id="61f7" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">在应用不同的分类模型后，我们得到了不同模型的以下准确度:</p><p id="fba9" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">1.逻辑回归— 95.8%</p><p id="e959" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">2.最近邻— 95.1%</p><p id="4503" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">3.支持向量机— 97.2%</p><p id="9408" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">4.内核 SVM — 96.5%</p><p id="7854" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">5.朴素贝叶斯——91.6%</p><p id="cf70" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">6.决策树算法— 95.8%</p><p id="9aaa" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">7.随机森林分类— 98.6%</p><p id="238c" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">最后，我们建立了分类模型，我们可以看到随机森林分类算法为我们的数据集提供了最佳结果。它并不总是适用于每一个数据集。为了选择我们的模型，我们总是需要分析我们的数据集，然后应用我们的机器学习模型。</p><p id="eae3" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">这是机器学习模型对任何数据集的基本应用。如果你有任何疑问，请随时提问。给 vishabh1010@gmail.com 发邮件或者通过<a class="ae nl" href="https://www.linkedin.com/in/vishabh-goel-27559aa6/" rel="noopener ugc nofollow" target="_blank">链接</a>联系我。</p><p id="3c59" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">你可以在<a class="ae nl" href="https://github.com/vishabh123/vishabh" rel="noopener ugc nofollow" target="_blank"> github </a>上找到代码，在 Ipython 控制台上试试。</p><p id="8ed0" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">切记始终牢记<a class="ae nl" href="https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76" rel="noopener">过度拟合和</a>欠拟合的问题。</p><p id="71e5" class="pw-post-body-paragraph ld le iq lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma ij bi translated">希望你喜欢这篇文章…..</p></div></div>    
</body>
</html>