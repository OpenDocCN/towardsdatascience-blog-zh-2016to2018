<html>
<head>
<title>[ICLR 2016] Fast and Accurate Deep Networks Learning By Exponential Linear Units (ELUs) with Interactive Code [ Manual Back prop with TF]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">【ICLR 2016】通过带交互代码的指数线性单元(ELUs)进行快速准确的深度网络学习【带 TF 的手动回推】</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/iclr-2016-fast-and-accurate-deep-networks-learning-by-exponential-linear-units-elus-with-c0cdbb71bb02?source=collection_archive---------11-----------------------#2018-05-07">https://towardsdatascience.com/iclr-2016-fast-and-accurate-deep-networks-learning-by-exponential-linear-units-elus-with-c0cdbb71bb02?source=collection_archive---------11-----------------------#2018-05-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/ce310bb37a31053e2e54ce563fca3c38.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*5XyG6N78msZJOe-6Nfma3w.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/yes-math-nodding-l0HlJIp1dIZzimEBq" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="56a5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我知道这份文件存在的时间最长，但是，直到现在我才有时间来实现这个网络。所以在这里，我终于实现了这个完全卷积神经网络与 ELU。非常简单但性能卓越的网络。</p><p id="8ba2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，为了好玩，让我们使用不同的方法来训练我们的网络，例如…… <br/>情况 a)使用标准反向传播的自动微分<br/>情况 b) <a class="ae jy" href="https://medium.com/@SeoJaeDuk/nips-2016-direct-feedback-alignment-provides-learning-in-deep-neural-networks-with-interactive-32d59045e8e" rel="noopener">反馈对准</a> <br/>情况 c) <a class="ae jy" rel="noopener" target="_blank" href="/outperforming-tensorflows-default-auto-differentiation-optimizers-with-interactive-code-manual-e587a82d340e">扩张反向传播</a>使用<a class="ae jy" href="https://msdn.microsoft.com/en-us/magazine/dn904675.aspx" rel="noopener ugc nofollow" target="_blank"> L2 正则化</a> <br/>情况 d)使用反向传播的自动微分、<a class="ae jy" href="https://msdn.microsoft.com/en-us/magazine/dn904675.aspx" rel="noopener ugc nofollow" target="_blank"> L2 正则化</a> <br/>情况 f) <a class="ae jy" rel="noopener" target="_blank" href="/outperforming-tensorflows-default-auto-differentiation-optimizers-with-interactive-code-manual-e587a82d340e">使用不同优化器的组合的扩张反向传播</a></p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><figure class="le lf lg lh gt jr"><div class="bz fp l di"><div class="li lj l"/></div></figure></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="d5ca" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">指数线性单位</strong></p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lk"><img src="../Images/4cf6e57f2e813ced530514d72728bbe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QXWKcQJhmJwsrWcPc9F-yQ.png"/></div></div></figure><p id="2974" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红框</strong> →激活功能的导数</p><p id="b026" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">本文的主要贡献是新颖的激活函数。它可能看起来很简单，但不要让表面水平愚弄你，作者实际上有一个完整的部分来解释为什么这种激活工作得这么好。总之，1)使用负梯度是个好主意，或者 2)激活函数的输入值以零为中心，以避免不必要的偏移。现在让我们看看这个函数在绘图时是什么样子的。</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lp"><img src="../Images/896bb2efe7f18fa924ade340ee9797f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oPbc4x9WSabbGS5gD_YYtQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Graph using this <a class="ae jy" href="https://www.desmos.com/calculator/v9hyi8qj0o" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="7ec9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红线</strong> → Alpha 值设为 1 <br/> <strong class="kb ir">蓝线</strong> → Alpha 值设为 2 <br/> <strong class="kb ir">绿线</strong> → Alpha 值设为 5 <br/> <strong class="kb ir">橙线</strong> → Alpha 值设为 0.5</p><p id="309e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">根据α值，我们可以观察到负值的陡度彼此不同。这个函数的求导非常简单，正如我们在上面已经看到的，现在让我们来看看实现。</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lq"><img src="../Images/714d2407303a2dea0e9abab663d04fa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tWMJ3GAapx9pk3jjveKTHw.png"/></div></div></figure><p id="46d0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红框</strong> →为小于 0 的输入增加 Alpha 值</p><p id="d323" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从添加的 alpha 值中，我们可以知道 tensor-flow 的 elu()实现将 alpha 值默认设置为 1。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="d540" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">数据集(CIFAR 10) </strong></p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lr"><img src="../Images/85d9e5e8856e4b6e0f88e914c64627cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*49VOL1tshAdhufSv68Sf-A.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Screen Shot from this <a class="ae jy" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="8f19" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们将使用 CIFAR 10 数据集来测试该网络的性能。然而，请注意，本文的作者已经执行了 1) <a class="ae jy" href="https://caglarift6266.wordpress.com/" rel="noopener ugc nofollow" target="_blank">全局对比度归一化</a> 2) <a class="ae jy" href="https://martin-thoma.com/zca-whitening/" rel="noopener ugc nofollow" target="_blank"> ZCA 白化</a>的预处理，以及进一步的数据增强(填充和翻转)，但我只是要归一化图像的每个通道。</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ls"><img src="../Images/0d8534e8a96457fcc58a7d8c807a0287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wxYxDNmm-YjkOz0htQ7-Kw.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Data Augmentation of the Paper</figcaption></figure><p id="89a4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后请注意，作者用 CIFAR 10 数据集做了两个实验，一个用 11 个卷积层，另一个用 18 个卷积层。我将实现 11 层网络。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="d900" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">网络架构</strong></p><div class="le lf lg lh gt ab cb"><figure class="lt jr lu lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/fb915a5ca23722cb5776e7f9e1b19b1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*3cme_OMAPdib9ztEkh5TqQ.png"/></div></figure><figure class="lt jr lz lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/33b5ccc0fd6dd59222bc688a0fd69ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*Dm3HdE9CA3JimBLchW57Kg.png"/></div></figure></div><p id="e85e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红色矩形</strong> →输入尺寸为 32 * 32 的图像<br/> <strong class="kb ir">黑色矩形</strong> →与 ELU 激活的卷积运算<br/> <strong class="kb ir">绿色矩形</strong> →均值/最大池运算<br/> <strong class="kb ir">橙色矩形</strong> →用于分类的 Softmax</p><p id="b64f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">网络本身并不复杂，它由 11 层组成，但原始论文的作者有特定的超参数设置，如下所示。</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ma"><img src="../Images/4caf1c7b315bf3473b5ea7b7e85198cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sdyAsQKA2KBDf2VlNgw6WQ.png"/></div></div></figure><p id="e5dc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">不幸的是，对我来说，为 165，000 次迭代训练一个模型将花费太多的时间。因此，我不打算这样做，但你可以检查交互代码部分，以准确了解我的超级参数是如何设置的。此外，还有一件事让我感到疑惑:2 * 2 max-在每个堆栈之后应用了跨距为 2 的池。如果我们从 32 *32 的图像开始，我们不能有 6 个池操作(这就是为什么我用了 5 个池层，如上所示。).如果有人知道作者的意思，请在下面评论。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="3259" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">案例 a)使用标准反向传播进行自动微分的结果<br/>(迭代:1K，优化器:动量)</strong></p><div class="le lf lg lh gt ab cb"><figure class="lt jr mc lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/15d9cf485ab82dc4143659b3d8b65c3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*yCq1ieWrrskbzVRTNGZw9g.png"/></div></figure><figure class="lt jr mc lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/ccbf196522b01e371bc3337a0526256f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*BA1TeJmT5kzFhR4UzxlFdQ.png"/></div></figure></div><p id="2b46" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →一段时间内的训练精度/成本<br/> <strong class="kb ir">右图</strong> →一段时间内的测试精度/成本</p><p id="88ab" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">使用动量优化器对该模型进行 1k 次迭代训练，如原始论文中所报告的那样设置退出率。即使在 1k 迭代之后，网络也做得不好，训练图像和测试图像的准确率都是 62%。虽然模型没有过度拟合是件好事，但这也表明收敛速度很慢。</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div class="gh gi md"><img src="../Images/83a686d7649144680e4212eff1a74e64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*cR62PzDvvWe6s_h6B1Vnmg.png"/></div></figure></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="d847" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">情况 b)结果为</strong><a class="ae jy" href="https://medium.com/@SeoJaeDuk/nips-2016-direct-feedback-alignment-provides-learning-in-deep-neural-networks-with-interactive-32d59045e8e" rel="noopener"><strong class="kb ir"/></a><strong class="kb ir"><br/>(迭代:85，优化器:Adam) </strong></p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div class="gh gi me"><img src="../Images/183157b9d6223cea3a807d3cfa77adf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*UixjL-c-cwEp3g8r7bIwcw.png"/></div></figure><p id="411a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我甚至没让模特完成训练。这是因为模型没有学到任何东西。在训练图像上达到 35%的准确率后，模型停止了改进。如果您希望查看培训的详细信息，请<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/ELU/elc%20b/caseb.txt" rel="noopener ugc nofollow" target="_blank">点击此处</a>查看日志。</p><blockquote class="mf mg mh"><p id="62fb" class="jz ka mb kb b kc kd ke kf kg kh ki kj mi kl km kn mj kp kq kr mk kt ku kv kw ij bi translated">**注意**我在这个案例中使用的网络架构与原始论文略有不同。</p></blockquote></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="9598" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">案例 c)结果为</strong> <a class="ae jy" rel="noopener" target="_blank" href="/outperforming-tensorflows-default-auto-differentiation-optimizers-with-interactive-code-manual-e587a82d340e"> <strong class="kb ir">扩张反向传播</strong> </a> <strong class="kb ir">与</strong> <a class="ae jy" href="https://msdn.microsoft.com/en-us/magazine/dn904675.aspx" rel="noopener ugc nofollow" target="_blank"> <strong class="kb ir"> L2 正则化</strong> </a> <strong class="kb ir"> <br/>(迭代:300，优化器:Adam) </strong></p><div class="le lf lg lh gt ab cb"><figure class="lt jr mc lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/ec218ae4a0c4c77dcd1caaeb31d98ac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Vu2TNByo1ziwr_ydPlqW6Q.png"/></div></figure><figure class="lt jr mc lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/c5d999ce92847b6a6633d98eebb56a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ioCQg2gnFTf34JmOkajWjA.png"/></div></figure></div><p id="4bf5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →一段时间内的训练精度/成本<br/> <strong class="kb ir">右图</strong> →一段时间内的测试精度/成本</p><p id="3913" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">请注意两件事，1)我忘记标准化成本，因此图表的 y 轴是如此倾斜 2)这个模型是使用<a class="ae jy" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank"> Adam 优化器</a>训练的。然而，再次，该模型并没有表现得像我想的那样好。由于 Adam Optimizer，它能够在训练图像上实现高精度，但它未能泛化，在测试图像上表现不佳。(这是一个严重的过度拟合案例。)</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/c2b6afaf5772aca0cc71def4771da0d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*SlstWlwipo5sBLZEbPT-kA.png"/></div></figure><blockquote class="mf mg mh"><p id="906f" class="jz ka mb kb b kc kd ke kf kg kh ki kj mi kl km kn mj kp kq kr mk kt ku kv kw ij bi translated">**注意**我在这个案例中使用的网络架构与原始论文略有不同。</p></blockquote></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="d339" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">情况 d)具有反向传播的自动微分的结果，</strong> <a class="ae jy" href="https://msdn.microsoft.com/en-us/magazine/dn904675.aspx" rel="noopener ugc nofollow" target="_blank"> <strong class="kb ir"> L2 正则化</strong> </a> <strong class="kb ir">(迭代:200，优化器:Adam) </strong></p><div class="le lf lg lh gt ab cb"><figure class="lt jr mc lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/a1c915aadf6cd039dc7a0a11ee7797f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*7Kb4Q3EZqsoQJFKwkPqt7w.png"/></div></figure><figure class="lt jr mc lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/fb5f075f4dcc199b21144d97624f2bc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*lZGVw5q88gJGW9AJ58jyQg.png"/></div></figure></div><p id="510f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →一段时间内的训练精度/成本<br/> <strong class="kb ir">右图</strong> →一段时间内的测试精度/成本</p><p id="558c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">即使有了 L2 正则化，当用 Adam optimizer 训练时，网络似乎过度适应。我们可以观察到，在第 25 次迭代之后，测试图像的成本开始增加。</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div class="gh gi me"><img src="../Images/0eb3d881b2a8edc901531c6e95ae4199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*xHlwB2olzm1FvwtBBjz9Ng.png"/></div></figure><p id="25d5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">尽管该网络在测试图像准确性上比其他网络稍好，但是它遭受过拟合。</p><blockquote class="mf mg mh"><p id="31c9" class="jz ka mb kb b kc kd ke kf kg kh ki kj mi kl km kn mj kp kq kr mk kt ku kv kw ij bi translated">**注意**我在这个案例中使用的网络架构与原始论文略有不同。</p></blockquote></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="dab4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">情况 d)使用不同优化器组合的</strong> <a class="ae jy" rel="noopener" target="_blank" href="/outperforming-tensorflows-default-auto-differentiation-optimizers-with-interactive-code-manual-e587a82d340e"> <strong class="kb ir">扩张反向传播</strong> </a> <strong class="kb ir">的结果(迭代:200，优化器:亚当/动量)</strong></p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mm"><img src="../Images/96a82c993137b53da5b0c78ae798f59c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RXMN9gxx7tqqT78tLDsosA.png"/></div></div></figure><p id="29cc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">紫色框</strong> →使用 Adam 优化器优化的图层</p><p id="37b2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于这个网络，我真的想尝试一些不同的东西，为每一层设置不同的优化器。如上所示，用 Adam optimizer 优化了周围有紫色矩形的层，并使用 momentum optimizer 训练了其他层。</p><div class="le lf lg lh gt ab cb"><figure class="lt jr mc lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/fbe747c33896defa33da846045ac3fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*P4N-znhq7Z6EmKxYQ-MnLA.png"/></div></figure><figure class="lt jr mc lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/86cf47b5bf3539b85f41999984e985ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*cNar8RLfNgxTxGZWi0t9xQ.png"/></div></figure></div><p id="63d3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →一段时间内的训练精度/成本<br/> <strong class="kb ir">右图</strong> →一段时间内的测试精度/成本</p><p id="b374" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">该网络的表现不如我所希望的那样好，尽管与情况 c)相比，该模型没有任何正则化技术方法，如 L2 正则化，但该模型能够在测试图像上实现 65%的准确性。</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/0914cb6bdf8e98c04a67e6f697de1e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*fYRq9GHcYXT5ITHGQJP7-Q.png"/></div></figure><blockquote class="mf mg mh"><p id="d949" class="jz ka mb kb b kc kd ke kf kg kh ki kj mi kl km kn mj kp kq kr mk kt ku kv kw ij bi translated">**注意**我在这个案例中使用的网络架构与原始论文略有不同。</p></blockquote></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="e152" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">互动代码/透明度</strong></p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/0071d0df20c402fdbfd6120722f8c08c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*XfsQCapqz8_SpyTTMODpKQ.png"/></div></figure><p id="fd83" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="mb">对于谷歌 Colab，你需要一个谷歌帐户来查看代码，而且你不能在谷歌 Colab 中运行只读脚本，所以在你的操场上做一个副本。最后，我永远不会请求允许访问你在 Google Drive 上的文件，仅供参考。编码快乐！</em></p><p id="8f14" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1zjjy8mE3q82UgiX7Ujj1nHfCx2p9Qv5x" rel="noopener ugc nofollow" target="_blank"> a 的代码，请点击此处</a>，要访问<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/ELU/elu%20a/dsadsa.txt" rel="noopener ugc nofollow" target="_blank">的日志，请点击此处。</a> <br/>访问案例<a class="ae jy" href="https://colab.research.google.com/drive/19tCArjCoAU_BJnM4Ae19vpr0x0P22zPI" rel="noopener ugc nofollow" target="_blank"> b 的代码请点击此处，</a>访问<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/ELU/elc%20b/caseb.txt" rel="noopener ugc nofollow" target="_blank">日志请点击此处。</a> <br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/17h07qoXvPE9EXjLMmEMHxLXOt9mH4t6Y" rel="noopener ugc nofollow" target="_blank"> c 的代码，请点击此处</a>，要访问<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/ELU/elu%20c/casec.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处。</a> <br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1NmQZmHUoRLu9w3UPFRNA9XaqNmCXKt8x" rel="noopener ugc nofollow" target="_blank"> d 的代码，请点击此处</a>，要访问<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/ELU/elu%20d/case%20d.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处。</a> <br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/13Nzp3z7PSpp5UkNQdqU36aigmRVbeUUc" rel="noopener ugc nofollow" target="_blank"> e 的代码请点击此处</a>，要访问<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/ELU/elu%20e/case%20e.txt" rel="noopener ugc nofollow" target="_blank">日志请点击此处。</a></p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="005f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="3681" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从这个实验中，我能够看到数据预处理和超参数调整对性能增益的重要性。因为最初的作者能够通过他们的超参数设置获得超过 90%的准确性。</p><figure class="le lf lg lh gt jr gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/858e85fb1f487a77f3f31dc747506527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*HSKkTiLzYAaFJeroxCMyzQ.png"/></div></figure><p id="7dba" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">蓝线</strong> →在原作者论文上测试 CIFAR 10 的错误率</p><p id="fa29" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有写作的列表，请在这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的 twitter 上关注我<a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文</a> t。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="6a03" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="b5e7" class="mq mr iq kb b kc kd kg kh kk ms ko mt ks mu kw mv mw mx my bi translated">Clevert，D. A .，Unterthiner，t .，&amp; Hochreiter，S. (2015 年)。通过指数线性单元(elus)进行快速准确的深度网络学习。<em class="mb"> arXiv 预印本 arXiv:1511.07289 </em>。</li><li id="748c" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">作为神经网络激活函数的 ELU。(2018).赛菲克·伊尔金·塞伦吉尔。2018 年 5 月 6 日检索，来自<a class="ae jy" href="https://sefiks.com/2018/01/02/elu-as-a-neural-networks-activation-function/" rel="noopener ugc nofollow" target="_blank">https://sefiks . com/2018/01/02/elu-as-a-neural-networks-activation-function/</a></li><li id="0a46" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">tf.nn.elu |张量流。(2018).张量流。检索于 2018 年 5 月 6 日，来自<a class="ae jy" href="https://www.tensorflow.org/api_docs/python/tf/nn/elu" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/nn/elu</a></li><li id="e43e" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">分段。(2018).德斯莫斯图形计算器。检索于 2018 年 5 月 6 日，来自<a class="ae jy" href="https://www.desmos.com/calculator/v9hyi8qj0o" rel="noopener ugc nofollow" target="_blank">https://www.desmos.com/calculator/v9hyi8qj0o</a></li><li id="1d30" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">CIFAR-10 和 CIFAR-100 数据集。(2018).Cs.toronto.edu。检索于 2018 年 5 月 6 日，来自<a class="ae jy" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">https://www.cs.toronto.edu/~kriz/cifar.html</a></li><li id="fce3" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">托马斯，M. (2017)。ZCA 美白。马丁·托马斯。检索于 2018 年 5 月 6 日，来自<a class="ae jy" href="https://martin-thoma.com/zca-whitening/" rel="noopener ugc nofollow" target="_blank">https://martin-thoma.com/zca-whitening/</a></li><li id="6888" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">aleju/papers。(2018).GitHub。2018 年 5 月 6 日检索，来自<a class="ae jy" href="https://github.com/aleju/papers/blob/master/neural-nets/ELUs.md" rel="noopener ugc nofollow" target="_blank">https://github . com/aleju/papers/blob/master/neural-nets/elus . MD</a></li><li id="e79b" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">IFT 6266 H13 博客。(2013).IFT 6266 H13 博客。2018 年 5 月 6 日检索，来自<a class="ae jy" href="https://caglarift6266.wordpress.com/" rel="noopener ugc nofollow" target="_blank">https://caglarift6266.wordpress.com/</a></li><li id="550c" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">aleju/papers。(2018).GitHub。2018 年 5 月 6 日检索，来自<a class="ae jy" href="https://github.com/aleju/papers" rel="noopener ugc nofollow" target="_blank">https://github.com/aleju/papers</a></li><li id="d7f4" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">Matplotlib . py plot . legend-Matplotlib 2 . 2 . 2 文档。(2018).Matplotlib.org。检索于 2018 年 5 月 6 日，来自<a class="ae jy" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.legend.html" rel="noopener ugc nofollow" target="_blank">https://matplotlib . org/API/_ as _ gen/matplotlib . py plot . legend . html</a></li><li id="9cb8" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">张量流正则化。(2018).ritchieng . github . io . 2018 年 5 月 6 日检索，来自<a class="ae jy" href="http://www.ritchieng.com/machine-learning/deep-learning/tensorflow/regularization/" rel="noopener ugc nofollow" target="_blank">http://www . ritchieng . com/machine-learning/deep-learning/tensor flow/regulation/</a></li><li id="248e" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">Only Numpy:对深度神经网络实施 L1 /L2 范数/正则化的不同组合…(2018).走向数据科学。2018 年 5 月 6 日检索，来自<a class="ae jy" rel="noopener" target="_blank" href="/only-numpy-implementing-different-combination-of-l1-norm-l2-norm-l1-regularization-and-14b01a9773b">https://towards data science . com/only-numpy-implementing-different-combination-of-L1-norm-L2-norm-L1-regularity-and-14b 01a 9773 b</a></li><li id="cafc" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">机器学习的 L1 和 L2 正则化。(2018).Msdn.microsoft.com。检索于 2018 年 5 月 7 日，来自<a class="ae jy" href="https://msdn.microsoft.com/en-us/magazine/dn904675.aspx" rel="noopener ugc nofollow" target="_blank">https://msdn.microsoft.com/en-us/magazine/dn904675.aspx</a></li><li id="7eb9" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">[ NIPS 2016 ]直接反馈对齐为深度神经网络中的学习提供了交互式学习。(2018).中等。检索于 2018 年 5 月 7 日，来自<a class="ae jy" href="https://medium.com/@SeoJaeDuk/nips-2016-direct-feedback-alignment-provides-learning-in-deep-neural-networks-with-interactive-32d59045e8e" rel="noopener">https://medium . com/@ SeoJaeDuk/nips-2016-direct-feedback-alignment-provides-learning-in-deep-neural-networks-with-interactive-32d 59045 e8e</a></li><li id="0d95" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">超越 Tensorflow 的默认自动微分优化器，具有交互式代码[手动…(2018).走向数据科学。检索于 2018 年 5 月 7 日，来自<a class="ae jy" rel="noopener" target="_blank" href="/outperforming-tensorflows-default-auto-differentiation-optimizers-with-interactive-code-manual-e587a82d340e">https://towards data science . com/outpering-tensor flows-default-auto-difference-optimizer-with-interactive-code-manual-e587 a82d 340 e</a></li><li id="4822" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">金马博士和巴律师(2014 年)。亚当:一种随机优化方法。Arxiv.org。于 2018 年 5 月 7 日检索，来自<a class="ae jy" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1412.6980</a></li></ol></div></div>    
</body>
</html>