<html>
<head>
<title>[ CVPR / Paper Summary ] Revisiting Salient Object Detection: Simultaneous Detection, Ranking, and Subitizing of Multiple Salient Objects</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[ CVPR /论文摘要]再谈显著目标检测:多个显著目标的同时检测、排序和分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cvpr-paper-summary-revisiting-salient-object-detection-simultaneous-detection-ranking-and-b759a9226d63?source=collection_archive---------13-----------------------#2018-09-12">https://towardsdatascience.com/cvpr-paper-summary-revisiting-salient-object-detection-simultaneous-detection-ranking-and-b759a9226d63?source=collection_archive---------13-----------------------#2018-09-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/6a501ae66a1f5f26ca080b418c508890.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*BsB81VR2w0UrDXZMywuQHg.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/montereybayaquarium-face-otter-sea-1xlGfZ07Dq62jqKT9t" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><blockquote class="jz ka kb"><p id="0422" class="kc kd ke kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">请注意，这篇帖子是为了我未来的自己复习这篇论文上的材料，而不是从头再看一遍。</strong></p></blockquote></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><figure class="li lj lk ll gt jr"><div class="bz fp l di"><div class="lm ln l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2523.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="4580" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kf ir">摘要</strong></p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/fbf329b26296f715d5d8528488f594f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QCR2i6sg-5QyTilXqIFNvw.png"/></div></div></figure><p id="b9df" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">许多研究人员提出了显著对象检测任务的解决方案，然而本文的作者认为，迄今为止大多数工作都解决了一个相对不适定的问题。具体来说，当有多个观察者时，对于什么构成显著物体没有普遍的一致意见。(这意味着一些对象比其他对象排名更突出，并且在对象之间存在排名。).本文的作者提出了一种方法/数据集，其性能优于传统的和新提出的工作。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="33e7" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kf ir">简介</strong></p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/1010867e02a49d9b530df3cdd5c9269d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tUJ6a9fp4q6UF4tn0rfR5w.png"/></div></div></figure><p id="7a7a" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">大多数显著目标检测任务考虑单个显著目标或多个显著目标，但是它们没有考虑不同的人可以将一些目标排列得比其他目标更显著。(此外，一些物体可以被认为更普遍地突出。因此，在本文中，作者更广泛地考虑了显著对象检测的问题，其包括检测图像中的所有显著区域，并通过向不同的显著区域分配置信度来解决观察者之间的可变性。(如上所述，模型不仅检测图像中的所有显著对象，还对它们进行排序。).</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="2367" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kf ir">显著物体检测/替换</strong></p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lx"><img src="../Images/11b5799912d1645b934fb18d168908ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GmTYWHJNxAM6Tl13i1Uigw.png"/></div></div></figure><p id="2c55" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">卷积神经网络提高了许多涉及图像的任务的性能标准。一些 CNN 使用超像素方法来实现显著对象检测。其他方法包括使用快捷连接来聚合多级特征。本文的作者通过应用新的机制来控制网络中的信息流，同时重要的是包括一个隐含携带确定相对显著性所必需的信息的堆叠策略，来使用逐级细化。除了分割之外，最近的工作还包括对图像中的显著对象进行子分类，这涉及对图像中的显著对象进行计数。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="ab50" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kf ir">提议的网络架构</strong></p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ly"><img src="../Images/60424950808bc8a8409cd745e89bded0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C9uHrbSxrouR2dErJ4M5SQ.png"/></div></div></figure><p id="d9c8" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><em class="ke">用于粗略预测的前馈网络</em></p><p id="983b" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">对于这一部分，作者使用 ResNet-101 作为编码网络，生成特征图。此外，为了用自上而下的细化网络增强编码器网络的主干，作者首先附加一个额外的卷积层以获得嵌套的相对显著性堆栈(NRSS)，然后附加堆栈卷积模块(SCM)以计算每个像素的粗略水平显著性分数。(作者对某些网络使用了 atrous 金字塔池。)</p><p id="675b" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><em class="ke">分阶段细化网络</em></p><p id="9e1f" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">大多数现有的工作共享一个公共的逐级解码结构来恢复每个像素的分类。这是因为尽管 CNN 中的最深层具有最丰富的可能特征表示，但是仅依靠解码阶段的卷积和解卷积来恢复丢失的信息可能会降低预测的质量。受此启发，作者提出了一种基于多级融合的细化网络，通过将初始粗略表示与在早期层表示的更精细特征相结合，来恢复解码阶段丢失的上下文信息。(该单元恢复丢失的空间信息，并对图像中呈现的对象的显著性进行分级。)</p><p id="a2ce" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><em class="ke">多级显著图融合/地面真实的堆叠表示</em></p><p id="91f8" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">为了组合不同 SCM 做出的不同预测，作者在网络末端添加了一个融合层，将不同阶段的预测显著图连接起来。此外，作者生成了一组对应于不同显著性级别(由观察者间协议定义)的堆叠地面实况图。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lz"><img src="../Images/4ff65fe3b6bfa6ff2ed003a49c9a461a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j3PijKIN6uALal-BOZ1vIg.png"/></div></div></figure><p id="fcd5" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><em class="ke">显著物体分群网络</em></p><p id="b42d" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">为了生成给定图像的置信度得分，作者再次使用 ResNet-101 作为基础模型，但去掉了最后一层。(并且已经附加了完全连接的网络以生成存在于输入图像中的 0、1、2、3 和 4+个显著对象的置信度得分，随后是另一个完全连接的层导致生成每个类别的最终置信度得分。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ma"><img src="../Images/0ad32f007de7724c0c67407b8af3ed02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o_GhGmhuFwXyhYIv9lvx1w.png"/></div></div></figure><p id="fa10" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">最后，作者为 Pascal-S 数据集创建了子分类基础事实，如上所述，可以看到图像中显著对象数量的分布。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="640c" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kf ir">实验</strong></p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mb"><img src="../Images/f26432a6e0976af2833e3d2d959b6a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cUd8VajrosXSx0zgJNzb8A.png"/></div></div></figure><p id="b370" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">作者添加了多种不同的网络体系结构，它们是…..<br/>RSDNet(expanded ResNet-101，NRSS，SCM)，RSDNet-A(同 RSDNet 但 gt 标签缩放 1000，使网络学习更多对比度。)、RSDNet-B (atrous 金字塔池模块)、RSDNet-C (RSDNet-B +地面实况缩放)、RSDNet-R(具有逐级秩感知细化单元的 RSDNet+多级显著图融合)。).</p><p id="44b9" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><em class="ke">数据集和评估指标</em></p><p id="1413" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">作者使用了 PASCAL 的数据集，并将图像分成两个子集，425 个用于训练，425 个用于测试。使用的指标包括精确召回(PR)曲线、F-measure(沿曲线的最大值)、ROC 曲线下面积(AUC)和平均绝对误差(MAE)。此外，作者还引入了显著对象排序(SOR)分数来评估显著对象的排序。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/555e628eca23b0150694c88eb030f469.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*hGACMycnfwhN2WU9ZOKc1g.png"/></div></figure><p id="d332" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">从上表可以明显看出，RSDNet-R 优于许多现有技术方法。值得注意的是，尽管作者的模型是在 Pascal-S 数据的子集上训练的，但它优于其他也利用大规模显著性数据集的算法。(即使没有 CRF)。最后一列(SOR)分数也是作者发明的新度量分数。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi md"><img src="../Images/d02edf77dc9b74ada1ea36ea7fdc5243.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LsKm8cTbKT-mKmdJwouo0Q.png"/></div></div></figure><p id="685c" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">如上所述，作者的方法能够生成最接近地面真实遮罩的显著图。</p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi me"><img src="../Images/7195f2610f325054afa0d681197ffebe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6dmw6xr7su5bqnOjgEMlUw.png"/></div></div></figure><p id="64dd" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">即使在其他任务上，如显著对象子分类，作者的方法也优于现有的方法。(需要注意的是，没有很多其他基线可以与这个分数进行比较。).</p><p id="67fc" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><em class="ke">检查嵌套的相对显著性堆栈</em></p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/a00ad9c31bf9018fe62c6bde2ea23c2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t8G8I1zfICD7s4NsHgjYVQ.png"/></div></div></figure><p id="a532" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">当作者对嵌套的相对显著性堆栈进行主成分分析，并以 RGB 图像的形式可视化时，我们可以看到这三个分量本身就足以看出显著对象的排序。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="8220" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kf ir">结论</strong></p><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mb"><img src="../Images/0fcd23fdb390e39ddd15ec083c30ba92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sVbyRPecqT3k1LV1JABtmQ.png"/></div></div></figure><p id="0869" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">作者提出了一种神经框架，不仅可以检测给定图像中的显著对象，还可以根据它们的显著性对它们进行排序。他们的方法不仅优于传统方法，也优于新提出的方法。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="c5dd" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kf ir">遗言</strong></p><p id="6bc7" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有写作的列表，请<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">在这里查看我的网站</a>。</p><p id="ccc9" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated">同时，在我的 twitter <a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>关注我，并访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我也实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文 pos </a> t。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="1939" class="pw-post-body-paragraph kc kd iq kf b kg kh ki kj kk kl km kn lo kp kq kr lp kt ku kv lq kx ky kz la ij bi translated"><strong class="kf ir">参考</strong></p><ol class=""><li id="84fc" class="mf mg iq kf b kg kh kk kl lo mh lp mi lq mj la mk ml mm mn bi translated">(2018).Openaccess.thecvf.com。检索于 2018 年 9 月 11 日，来自<a class="ae jy" href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2523.pdf" rel="noopener ugc nofollow" target="_blank">http://open access . the CVF . com/content _ cvpr _ 2018/camera ready/2523 . pdf</a></li></ol><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mo"><img src="../Images/31a3d6bef20910ab5f1322de4db97197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ivwJ7hmRbLMgBt2v5VztOA.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Photo by Scott Webb from Pexels</figcaption></figure></div></div>    
</body>
</html>