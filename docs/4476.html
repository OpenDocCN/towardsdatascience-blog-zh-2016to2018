<html>
<head>
<title>Neural Network Introduction for Software Engineers 1 — A Vanilla MLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">软件工程师神经网络导论 1——香草 MLP</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-network-introduction-for-software-engineers-1611d382c6aa?source=collection_archive---------7-----------------------#2018-08-18">https://towardsdatascience.com/neural-network-introduction-for-software-engineers-1611d382c6aa?source=collection_archive---------7-----------------------#2018-08-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="5259" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这第一篇博文将帮助你用 Python/Numpy 设计一个神经网络。它将展示普通多层感知器(MLPs)的衰落，提出一些简单的增强，并展示它们有多重要。最后，我们将展示这在一个组织良好的软件工程包中会是什么样子。</p><p id="63d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于非技术前言，请阅读<a class="ae kl" href="https://medium.com/@leetandata/machine-learning-preface-ba69bca4701d" rel="noopener"> ML 前言</a>以了解回归。</p><p id="d005" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了理解数学，可选地将最初的 numpy 版本与包含可视化的<a class="ae kl" href="https://medium.com/coinmonks/the-mathematics-of-neural-network-60a112dd3e05" rel="noopener">https://medium . com/coin monks/the-mathematics-of-neural-network-60a 112 dd3 e 05</a>进行比较。</p><h1 id="7cbc" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">软件工程师神经网络导论</h1><p id="a02d" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">首先，我们将建立一个简单的神经网络(NN 或多层感知器/MLP):</p><p id="89d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在数学上，我们将定义一个具有一个隐藏层的神经网络如下:</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="e8c6" class="ly kn iq lu b gy lz ma l mb mc">x: input matrix<br/>w1: 1st weight matrix for multiplication<br/>w2: 2nd weight matrix for multiplication<br/>Hidden = matrix_multiplication(x, w1)<br/>Hidden_rectified = Clip(Hidden, minimum_value = 0)<br/>output = matrix_multiplication(Hidden_rectified, w2</span></pre><p id="7f45" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这样，我们将输出计算为输入的函数。我们回顾微积分和线性代数，通过梯度下降最小化损失函数来优化(训练)我们的 MLP 以匹配数据集。</p><figure class="lp lq lr ls gt md"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="lp lq lr ls gt md gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/7dcd3a791cd4962456e5db143febe773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*mBDsdW8WngjJnUQzFGS9Yw.jpeg"/></div></figure><p id="21d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">哦不！我们的损失激增。一个基本的神经网络对它的学习速率非常敏感，因为它的梯度会爆炸。一旦梯度变大，它们可以不断地来回摆动它们相应的权重，并爆炸到无穷大。</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><p id="1bbb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们通过将应用的渐变剪切到[-1，1]来解决这个问题。</p><p id="fe19" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们也将学习率衰减到 0，以允许网络首先以大的学习率接近合理的解决方案，然后通过小的调整确定特定的解决方案。</p><p id="c021" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度削波也是通过将梯度削波到具有最大向量范数来完成的，并且学习率衰减有时被完全忽略，以利于其他微调策略，如 l2 正则化或批量增加。</p><figure class="lp lq lr ls gt md"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="lp lq lr ls gt md gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/76e6f97927ba6d70576748977c6828c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*6pkIVOrJDdOB99xzl95piQ.jpeg"/></div></figure><p id="ed66" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">太好了！我们的损失函数现在向零递减。</p><p id="4539" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你听说过动量优化器或者亚当优化器吗？看看上面曲线中波动的损失，你是否注意到它上下摆动，而通常向下移动？</p><p id="4f68" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">动量是在一个合理的恒定方向上保持速度的概念。当我们接收到一个梯度来调整我们的参数时，我们更新我们正在移动的速度，然后继续沿着我们认为在我们的参数空间中有效的方向行进。准确地说，</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="9857" class="ly kn iq lu b gy lz ma l mb mc">Value = Value-gradient</span></pre><p id="2f56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">变成了:</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="d52d" class="ly kn iq lu b gy lz ma l mb mc">velocity = velocity * momentum + gradient * (1-momentum)<br/>Value = Value-velocity</span></pre><p id="a723" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，通过动量和函数化我们的损失函数作为一个抽象层，为备选损失函数做准备，我们的代码变成:</p><figure class="lp lq lr ls gt md"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="lp lq lr ls gt md gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/18421e429cf7d25bfa494fdd72a355ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*fpgIY4oGS1-LhyUSZEgRtg.jpeg"/></div></figure><p id="ffa2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">太好了！我们的损失变小得更快，接近一个更小的值。让我们再添加一个机器学习工具来帮助模型更可靠地学习，称为“内斯特罗夫动量”</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><p id="77b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">内斯特罗夫的动量工程受到以下启发。在给定的有动力的一步，你大概知道你将采取的下一步。让我们看看我们期望移动的地方，计算那里的梯度，用这个梯度来修正我们的动量向量。然后用新的动量向量，我们像往常一样前进。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="7254" class="ly kn iq lu b gy lz ma l mb mc">Theoretical_next_state = Value - Velocity<br/>gradient = gradient at Theoretical_next_state<br/>Velocity = Velocity * momentum + gradient * (1-momentum)<br/>Value = Value-Velocity</span></pre><p id="1f09" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同时，我们注意到 ReLU 非线性的一个问题，即 ReLU 曲线的“关闭”部分的导数为零。让我们用更流行的泄漏 ReLU 来代替我们的 ReLU，固定这个零导数，平坦区域有一个缓坡。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="6a6e" class="ly kn iq lu b gy lz ma l mb mc">ReLU(x) = max(x, 0)<br/>becomes:<br/>lReLU(x) = max(x, .1*x)</span></pre><p id="27dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请耐心等待这一段，因为演示数学的代码有点疯狂。在下一部分中，我们将组织它，以便观众中的软件工程师感到舒适。</p><figure class="lp lq lr ls gt md"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="lp lq lr ls gt md gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/e33d34cf941d21d6bb0e989392cb664a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*NgGwjVGCIDLHP8BMQFExyg.jpeg"/></div></figure><p id="0f03" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">太好了！内斯特罗夫的势头奏效了。但是看看这段代码！太恐怖了。我们硬编码了整个系统来实现一个神经网络</p><ul class=""><li id="6fda" class="mq mr iq jp b jq jr ju jv jy ms kc mt kg mu kk mv mw mx my bi translated">恰好一个隐藏层</li><li id="4792" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated">泄漏的 relu 非线性</li><li id="4f0c" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated">整批优化</li></ul><p id="1fb4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们想要改变其中的任何一个，我们就必须改变多个系统来硬编码我们想要尝试的其他系统。我暂时不做解释，但是让我们来看看这个系统到底有多干净和可重用。我们暂时把它作为一个文件，这样我们就可以很容易地在博客中展示它。</p><figure class="lp lq lr ls gt md"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="lp lq lr ls gt md gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/8847100efabcba4783a1cf3faad43974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*NcTNHX5EeKgSOqMqQMUWKA.jpeg"/></div></figure><p id="3a2d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">万岁！我们有干净的软件来定义、训练和评估一个模型。从机器学习和软件工程的角度来看，我们的实现仍然存在一些巨大的问题，但它展示了一般的概念。</p><p id="e937" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在现实世界中，我们会(未来的连续博客文章):</p><ul class=""><li id="9ade" class="mq mr iq jp b jq jr ju jv jy ms kc mt kg mu kk mv mw mx my bi translated"><a class="ae kl" href="https://medium.com/@leetandata/neural-network-for-software-engineers-2-mini-batch-training-and-validation-46ee0a1269a0" rel="noopener">解决一个真实世界的问题，然后在单独的验证数据集上训练和评估我们的问题。</a></li><li id="1128" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated">思考我们的数据的意义，并构建我们的模型以一种深思熟虑的方式来表示它。(链接到卷积后)</li><li id="292f" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated">以一种有意义的方式思考我们的损失函数，建立我们的损失函数，给我们一个有意义的梯度。(房价亏损帖链接)</li><li id="fa99" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated">像上面那样思考我们想要什么，但是使用 Tensorflow 这样的神经网络 API 来实现。哇，太简单了！(链接 Tensorflow 房价贴)</li></ul></div></div>    
</body>
</html>