<html>
<head>
<title>A bunch of tips and tricks for training deep neural networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一堆训练深度神经网络的技巧和窍门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-bunch-of-tips-and-tricks-for-training-deep-neural-networks-3ca24c31ddc8?source=collection_archive---------3-----------------------#2018-08-31">https://towardsdatascience.com/a-bunch-of-tips-and-tricks-for-training-deep-neural-networks-3ca24c31ddc8?source=collection_archive---------3-----------------------#2018-08-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/de7c09fa2f83fbfbc9db0d7df6761745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ImYR0xOk23_nmGkh17S7WA.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">I took this nice photo in GüvenPark</figcaption></figure><div class=""/><p id="7589" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">训练深度神经网络是困难的。它需要知识和经验，以便正确地训练和获得最佳模型。在这篇文章中，我想分享我在训练深度神经网络中所学到的东西。以下提示和技巧可能对您的研究有益，可以帮助您加快网络架构或参数搜索的速度。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="a7c5" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi li translated">哦，让我们开始吧…</p><p id="90f5" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">①。</strong>在开始构建您的网络架构之前，您需要做的第一件事是验证您输入到网络中的数据是否与标签(y)相对应。在密集预测的情况下，请确保地面实况标签(y)被正确编码为标签索引(或一次性编码)。如果没有，培训就没有效果。</p><p id="147a" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 2)。</strong>决定是使用预先训练好的模型还是从头开始训练你的网络？</p><ul class=""><li id="37f1" class="lr ls jf ke b kf kg kj kk kn lt kr lu kv lv kz lw lx ly lz bi translated">如果您的问题域中的数据集与<a class="ae ma" href="http://www.image-net.org/challenges/LSVRC/2012/" rel="noopener ugc nofollow" target="_blank"> ImageNet 数据集</a>相似，请对该数据集使用预训练模型。使用最广泛的预训练模型有<a class="ae ma" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank"> VGG </a>网、<a class="ae ma" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">雷斯网</a>、<a class="ae ma" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank">丹森网</a>或<a class="ae ma" href="https://arxiv.org/abs/1610.02357" rel="noopener ugc nofollow" target="_blank">例外</a>等。有许多层结构，例如，VGG (19 和 16 层)，雷斯网(152，101，50 层或更少)，DenseNet (201，169 和 121 层)。<strong class="ke jg">注意</strong>:不要尝试使用更多的层网络来搜索超参数(例如<em class="la">VGG-19、ResNet-152 或 DenseNet-201 层网络，因为其计算成本很高</em>)，而是使用更少的层网络(<em class="la">例如 VGG-16、ResNet-50 或 DenseNet-121 层</em>)。选择一个预训练的模型，您认为它可以提供您的超参数的最佳性能(<em class="la">比如 ResNet-50 层</em>)。获得最佳超参数后，只需选择相同但更多的层网(<em class="la">如 ResNet-101 或 ResNet-152 层</em>)来提高精度。</li><li id="c0c7" class="lr ls jf ke b kf mb kj mc kn md kr me kv mf kz lw lx ly lz bi translated">微调几个层或只训练分类器，如果你有一个小数据集，你也可以尝试在你要微调的卷积层后插入<a class="ae ma" href="http://jmlr.org/papers/v15/srivastava14a.html" rel="noopener ugc nofollow" target="_blank"> Dropout </a>层，因为它可以帮助对抗网络中的过度拟合。</li><li id="e2b0" class="lr ls jf ke b kf mb kj mc kn md kr me kv mf kz lw lx ly lz bi translated">如果您的数据集与<a class="ae ma" href="http://www.image-net.org/challenges/LSVRC/2012/" rel="noopener ugc nofollow" target="_blank"> ImageNet 数据集</a>不相似，您可以考虑从头开始构建和训练您的网络。</li></ul><p id="0b27" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">③。</strong>在您的网络中始终使用标准化图层。如果你用一个大的<em class="la">批量</em>(比如 10 或更多)来训练网络，使用<a class="ae ma" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">批量标准化</a>层。否则，如果你用一个小的<em class="la">批量</em>(比方说 1)训练，用<a class="ae ma" href="https://arxiv.org/abs/1607.08022" rel="noopener ugc nofollow" target="_blank">实例规范化</a>层代替。请注意，主要作者发现，如果增加<em class="la">批处理大小</em>，批处理标准化会提高性能，而当<em class="la">批处理大小</em>小时，它会降低性能。然而，如果他们使用小的<em class="la">批处理大小</em>，实例规范化会稍微提高性能。或者你也可以尝试<a class="ae ma" href="https://arxiv.org/abs/1803.08494" rel="noopener ugc nofollow" target="_blank">分组规范化</a>。</p><p id="d840" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 4)。</strong>如果你有两个或更多的卷积层(<em class="la">比如李</em>)在同一个输入(<em class="la">比如 F </em>)上操作，那么在一个特征拼接之后使用<a class="ae ma" href="https://arxiv.org/abs/1411.4280" rel="noopener ugc nofollow" target="_blank">空间删除</a>。由于这些卷积层对相同的输入进行操作，因此输出特征很可能是相关的。因此，空间丢失会移除那些相关的特征，并防止网络中的过度拟合。<strong class="ke jg">注</strong>:多用于低层而非高层。</p><figure class="mh mi mj mk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mg"><img src="../Images/bd7f8021bf13972fa56437eed4d396f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ymn7DhMJzPSj4ucl9nhqlg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">SpatialDropout use-case</figcaption></figure><p id="42fd" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 5)。</strong>为了确定你的网络能力，试着用一小部分训练样本来充实你的网络。如果它没有超载，增加你的网络容量。过度拟合后，使用正则化技术，如<a class="ae ma" href="https://keras.io/regularizers/" rel="noopener ugc nofollow" target="_blank"> L1 </a>、<a class="ae ma" href="https://keras.io/regularizers/" rel="noopener ugc nofollow" target="_blank"> L2 </a>、<a class="ae ma" href="http://jmlr.org/papers/v15/srivastava14a.html" rel="noopener ugc nofollow" target="_blank">辍学</a>或其他技术来对抗过度拟合。</p><p id="9ba4" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 6)。</strong>另一个正则化技术是约束或限制你的网络权重。这也有助于防止网络中的梯度爆炸问题，因为权重总是有界的。与在损失函数中惩罚高权重的 L2 正则化相反，该约束直接正则化您的权重。可以在 Keras 中轻松设置权重约束:</p><figure class="mh mi mj mk gt is"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="13ce" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 7)。</strong>从数据中平均减法有时会给出非常差的性能，尤其是从灰度图像中减法(<em class="la">我个人在前景分割领域面临这个问题</em>)。</p><p id="b942" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">⑧。</strong>总是打乱你的训练数据，训练前的<em class="la">和训练中的</em>和<em class="la"/>，以防你没有从时间数据中获益。这可能有助于提高网络性能。</p><p id="575c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 9)。</strong>如果您的问题域与密集预测相关(<em class="la">例如语义分割</em>，我建议您使用<a class="ae ma" href="https://arxiv.org/abs/1705.09914" rel="noopener ugc nofollow" target="_blank">扩张残差网络</a>作为预训练模型，因为它针对密集预测进行了优化。</p><p id="6153" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 10) </strong>。要捕获对象周围的上下文信息，请使用多尺度要素池模块。这可以进一步帮助提高准确性，这一思想已成功用于<a class="ae ma" href="https://arxiv.org/abs/1802.02611" rel="noopener ugc nofollow" target="_blank">语义分割</a>或<a class="ae ma" href="https://arxiv.org/abs/1808.01477" rel="noopener ugc nofollow" target="_blank">前景分割</a>。</p><p id="2805" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 11)。</strong>从损失或准确性计算中剔除无效标签(或模糊区域)(如果有)。这可以帮助你的网络在预测时更加自信。</p><p id="6fd2" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 12)。</strong>如果您有高度<em class="la">不平衡数据问题</em>，请在训练期间应用<em class="la">类别权重</em>。换句话说，给稀有类更多的权重，给主要类更少的权重。使用<a class="ae ma" href="http://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html" rel="noopener ugc nofollow" target="_blank"> sklearn </a>可以轻松计算类别权重。或者尝试使用<a class="ae ma" href="https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis" rel="noopener ugc nofollow" target="_blank">过采样和欠采样技术</a>对训练集进行重新采样。这也有助于提高你预测的准确性。</p><p id="3019" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 13)。</strong>选择合适的优化器。有许多流行的自适应优化器，如<a class="ae ma" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank">亚当</a>、<a class="ae ma" href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" rel="noopener ugc nofollow" target="_blank">阿达格拉德</a>、<a class="ae ma" href="https://arxiv.org/abs/1212.5701" rel="noopener ugc nofollow" target="_blank">阿达德尔塔</a>或<a class="ae ma" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" rel="noopener ugc nofollow" target="_blank"> RMSprop </a>等。<a class="ae ma" href="http://cs231n.github.io/neural-networks-3/#sgd" rel="noopener ugc nofollow" target="_blank"> SGD+momentum </a>广泛应用于各种问题域。有两件事需要考虑:F <strong class="ke jg">首先</strong>，如果你关心快速收敛，使用 Adam 之类的自适应优化器，但它可能会以某种方式陷入局部最小值，并提供较差的泛化能力(如下图)。<strong class="ke jg">第二个</strong>，<em class="la"> SGD+momentum </em>可以实现找到一个全局最小值，但是它依赖于健壮的初始化，并且可能比其他自适应优化器需要更长的时间来收敛(下图)。我推荐你使用<em class="la"> SGD+momentum </em>，因为它会达到更好的效果。</p><figure class="mh mi mj mk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mn"><img src="../Images/7427ed614c7a97a8b28ab94a9b1a42dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GJjXOSwmn5Zl0yEu-7VOkA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">This image borrowed from <a class="ae ma" href="https://arxiv.org/abs/1705.08292" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1705.08292</a></figcaption></figure><p id="7a83" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 14)。</strong>有三个学习率起点(即 1e-1、1e-3 和 1e-6)。如果对预训练模型进行微调，可以考虑小于 1e-3 的低学习率(<em class="la">说 1e-4 </em>)。如果你从头开始训练你的网络，考虑学习率大于或等于 1e-3。你可以试试这些起点，调整一下，看看哪个效果最好，挑那个。还有一件事，你可以考虑通过使用<a class="ae ma" href="https://keras.io/callbacks/#learningratescheduler" rel="noopener ugc nofollow" target="_blank">学习率调度器</a>来降低学习率。这也有助于提高网络性能。</p><p id="2669" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 15)。</strong>除了随着时间降低学习率的<a class="ae ma" href="https://keras.io/callbacks/#learningratescheduler" rel="noopener ugc nofollow" target="_blank">学习率计划表</a>之外，还有另一种方法，如果<em class="la">验证损失</em>在某些时期(<em class="la">说 5 </em>)停止改善，并且如果<em class="la">验证损失</em>在某些时期(<em class="la">说 10 </em>)停止改善，我们可以通过某些因素降低学习率(<em class="la">说 10 </em>)。这可以通过在 Keras 中使用<a class="ae ma" href="https://keras.io/callbacks/#reducelronplateau" rel="noopener ugc nofollow" target="_blank">减速板</a>和<a class="ae ma" href="https://keras.io/callbacks/#earlystopping" rel="noopener ugc nofollow" target="_blank">提前停止</a>轻松完成。</p><figure class="mh mi mj mk gt is"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="dbc0" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 16)。</strong>如果您在密集预测领域工作，如<a class="ae ma" href="https://en.wikipedia.org/wiki/Foreground_detection" rel="noopener ugc nofollow" target="_blank">前景分割</a>或<a class="ae ma" href="https://en.wikipedia.org/wiki/Image_segmentation" rel="noopener ugc nofollow" target="_blank">语义分割</a>，您应该使用<a class="ae ma" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank">跳过连接</a>，因为最大池操作或步长卷积会丢失对象边界或有用信息。这也可以帮助你的网络容易地学习从特征空间到图像空间的特征映射，并且可以帮助缓解网络中的消失梯度问题。</p><p id="12a0" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 17)。</strong>更多的数据胜过巧妙的算法！总是使用数据增强，如水平翻转，旋转，缩放裁剪等。这有助于大幅提高精确度。</p><p id="07ec" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 18)。</strong>你必须有一个用于训练的高速图形处理器，但这有点昂贵。如果你希望使用免费的云 GPU，我推荐使用<a class="ae ma" href="https://colab.research.google.com/notebooks/welcome.ipynb#recent=true" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>。如果你不知道从哪里开始，可以看看我的<a class="ae ma" rel="noopener" target="_blank" href="/a-comprehensive-guide-on-how-to-fine-tune-deep-neural-networks-using-keras-on-google-colab-free-daaaa0aced8f">上一篇</a>或者尝试各种云 GPU 平台，比如<a class="ae ma" href="https://www.floydhub.com" rel="noopener ugc nofollow" target="_blank"> Floydhub </a>或者<a class="ae ma" href="https://www.paperspace.com" rel="noopener ugc nofollow" target="_blank"> Paperspace </a>等等。</p><p id="288f" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 19)。</strong>在<a class="ae ma" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU </a>之前使用 Max-pooling 以节省一些计算。因为 ReLU 阈值为零:<code class="fe mo mp mq mr b">f(x)=max(0,x)</code>和最大池仅最大激活:<code class="fe mo mp mq mr b">f(x)=max(x1,x2,...,xi)</code>，所以使用<code class="fe mo mp mq mr b">Conv &gt; MaxPool &gt; ReLU</code>而不是<code class="fe mo mp mq mr b">Conv &gt; ReLU &gt; MaxPool</code>。</p><p id="8d17" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">例如</strong>假设我们有两个来自<code class="fe mo mp mq mr b">Conv</code>的激活(即 0.5 和-0.5):</p><ul class=""><li id="dd7c" class="lr ls jf ke b kf kg kj kk kn lt kr lu kv lv kz lw lx ly lz bi translated">所以<code class="fe mo mp mq mr b">MaxPool &gt; ReLU = max(0, max(0.5,-0.5)) = 0.5</code></li><li id="befa" class="lr ls jf ke b kf mb kj mc kn md kr me kv mf kz lw lx ly lz bi translated">和<code class="fe mo mp mq mr b">ReLU &gt; MaxPool = max(max(0,0.5), max(0,-0.5)) = 0.5</code></li></ul><p id="a241" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">看到了吗？这两个操作的输出仍然是<code class="fe mo mp mq mr b">0.5</code>。在这种情况下，使用<code class="fe mo mp mq mr b">MaxPool &gt; ReLU</code>可以为我们省去一个<code class="fe mo mp mq mr b">max</code>操作。</p><p id="9e5d" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 20)。</strong>考虑使用<a class="ae ma" href="https://arxiv.org/abs/1610.02357" rel="noopener ugc nofollow" target="_blank">深度方向可分离卷积</a>运算，与普通卷积运算相比，该运算速度快，并大大减少了参数数量。</p><p id="2ee4" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> 21)。最后但同样重要的是，不要放弃💪。相信自己，你能行！如果你仍然没有达到你所期望的高精度，调整你的超参数、网络架构或训练数据，直到你达到你所期望的精度👏。</strong></p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="f5a6" class="ms mt jf bd mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated">最后的话…</h1><p id="4c91" class="pw-post-body-paragraph kc kd jf ke b kf nq kh ki kj nr kl km kn ns kp kq kr nt kt ku kv nu kx ky kz ij bi translated">如果你喜欢这个帖子，请随意鼓掌或分享给全世界。如果你有任何问题，请在下面留言。你可以在<a class="ae ma" href="http://www.linkedin.com/in/longang-snow" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我，或者在<a class="ae ma" href="https://twitter.com/LongAngLim1" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注我。过得愉快🎊。</p></div></div>    
</body>
</html>