<html>
<head>
<title>A Basic Introduction to Separable Convolutions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可分卷积的基本介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728?source=collection_archive---------0-----------------------#2018-08-14">https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728?source=collection_archive---------0-----------------------#2018-08-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="64fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">任何人只要看一看 MobileNet 的架构，就会毫无疑问地遇到可分卷积的概念。但那是什么，它和普通的卷积有什么不同？</p><p id="d2e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有两种主要类型的可分离卷积:空间可分离卷积和深度可分离卷积。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h1 id="e3ad" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">空间可分卷积</h1><p id="5892" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">从概念上来说，这是两个中较容易的一个，并且说明了将一个卷积分成两个的想法，所以我将从这个开始。不幸的是，空间可分卷积有一些明显的限制，这意味着它在深度学习中没有被大量使用。</p><p id="3512" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">空间可分卷积之所以如此命名，是因为它主要处理图像和内核的<strong class="jp ir">空间维度</strong>:宽度和高度。(另一个维度，即“深度”维度，是每个图像的通道数)。</p><p id="47ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">空间可分离卷积简单地将一个核分成两个更小的核。最常见的情况是将 3x3 内核分为 3x1 和 1x3 内核，如下所示:</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi lv"><img src="../Images/3998e61c3d45be8e89b7a688a3d1bd8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mL53fW0tJpNWEePp54y1Sg.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Image 1: Separating a 3x3 kernel spatially</figcaption></figure><p id="fa8d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们不是用 9 次乘法进行一次卷积，而是用两次卷积，每次 3 次乘法(总共 6 次)来达到相同的效果。随着乘法运算的减少，计算复杂度降低，网络能够运行得更快。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ml"><img src="../Images/fd28867f601638308f5a3592b521252a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o3mKhG3nHS-1dWa_plCeFw.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Image 2: Simple and spatial separable convolution</figcaption></figure><p id="7c5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以在空间上分离的最著名的卷积之一是 Sobel 核，用于检测边缘:</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi mm"><img src="../Images/9838b6de0d2eabed7bba90ebd0c280fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r4MjVvb2rehlIzpTKZdhcA.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Image 3: Separating the Sobel kernel</figcaption></figure><p id="aeca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">空间可分离卷积的主要问题是，不是所有的核都可以被“分离”成两个更小的核。这在训练期间变得特别麻烦，因为在网络可以采用的所有可能的核中，它最终只能使用可以分成两个更小的核的微小部分中的一个。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h1 id="0956" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">深度可分卷积</h1><p id="5d07" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">与空间可分离卷积不同，深度可分离卷积与不能被“分解”成两个更小内核的内核一起工作。因此，它更常用。这是在 keras.layers.SeparableConv2D 或 tf.layers.separable_conv2d 中看到的可分离卷积类型。</p><p id="8658" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">深度方向可分离卷积之所以如此命名，是因为它不仅处理空间维度，还处理深度维度——通道的数量。一个输入图像可能有 3 个通道:RGB。经过几次卷积后，一幅图像可能有多个通道。您可以将每个通道想象成该图像的特定解释；例如，“红色”通道解释每个像素的“红色”，“蓝色”通道解释每个像素的“蓝色”，“绿色”通道解释每个像素的“绿色”。具有 64 个通道的图像对该图像有 64 种不同的解释。</p><p id="9fe5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与空间可分离卷积类似，深度方向可分离卷积将一个核分割成两个独立的核，进行两种卷积:深度方向卷积和点方向卷积。但首先，让我们看看一个正常的卷积是如何工作的。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h2 id="b71a" class="mn kt iq bd ku mo mp dn ky mq mr dp lc jy ms mt lg kc mu mv lk kg mw mx lo my bi translated">正常卷积:</h2><p id="0d4e" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">如果你不知道从二维角度看卷积是如何工作的，请阅读<a class="ae mz" rel="noopener" target="_blank" href="/what-is-a-neural-network-6010edabde2b">这篇文章</a>或查看<a class="ae mz" href="http://setosa.io/ev/image-kernels/" rel="noopener ugc nofollow" target="_blank">这个网站</a>。</p><p id="a961" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，典型的图像不是二维的；它也有深度以及宽度和高度。假设我们有一个 12×12×3 像素的输入图像，一个大小为 12×12 的 RGB 图像。</p><p id="b137" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们对图像进行 5x5 卷积，没有填充，步幅为 1。如果只考虑图像的宽度和高度，卷积过程有点像这样:12x12 — (5x5) — &gt;8x8。5x5 内核每 25 个像素进行一次标量乘法，每次产生 1 个数字。我们最终得到一个 8×8 像素的图像，因为没有填充(12–5+1 = 8)。</p><p id="201a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，因为图像有 3 个通道，我们的卷积核也需要有 3 个通道。这意味着，每次内核移动时，我们实际上执行 5x5x3=75 次乘法，而不是执行 5x5=25 次乘法。</p><p id="5c44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">就像 2d 解释一样，我们对每 25 个像素进行标量矩阵乘法，输出 1 个数字。经过一个 5x5x3 的内核，12x12x3 的镜像会变成一个 8x8x1 的镜像。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi na"><img src="../Images/1d4636d5279be1f3cd22e6d95f0be49b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fgYepSWdgywsqorf3bdksg.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Image 4: A normal convolution with 8x8x1 output</figcaption></figure><p id="02d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们想增加输出图像中的通道数量，该怎么办？如果我们想要 8x8x256 大小的输出呢？</p><p id="871d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嗯，我们可以创建 256 个内核来创建 256 个 8x8x1 图像，然后将它们堆叠在一起，创建一个 8x8x256 图像输出。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi nb"><img src="../Images/d675413c83012db2742a6acb860a24fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XloAmCh5bwE4j1G7yk5THw.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Image 5: A normal convolution with 8x8x256 output</figcaption></figure><p id="bd12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这就是普通卷积的工作原理。我喜欢把它想象成一个函数:12x12x3 — (5x5x3x256) — &gt;12x12x256(其中 5x5x3x256 代表内核的高度、宽度、输入通道数和输出通道数)。不是说这不是矩阵乘法；我们不是将整个图像乘以内核，而是将内核移动通过图像的每个部分，并分别乘以图像的小部分。</p><p id="eb27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">深度方向可分离卷积将该过程分成两部分:深度方向卷积和点方向卷积。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h2 id="1cb3" class="mn kt iq bd ku mo mp dn ky mq mr dp lc jy ms mt lg kc mu mv lk kg mw mx lo my bi translated">第 1 部分—深度方向卷积:</h2><p id="c06e" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">在第一部分，深度方向卷积，我们给输入图像一个不改变深度的卷积。我们通过使用 3 个形状为 5x5x1 的内核来实现这一点。</p><figure class="lw lx ly lz gt ma"><div class="bz fp l di"><div class="nc nd l"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Video 1: Iterating 3 kernels through a 3 channel image</figcaption></figure><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ne"><img src="../Images/915cde827b53f65477a05531d9d8f565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yG6z6ESzsRW-9q5F_neOsg.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Image 6: Depthwise convolution, uses 3 kernels to transform a 12x12x3 image to a 8x8x3 image</figcaption></figure><p id="6a38" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每个 5x5x1 内核迭代图像的 1 个通道(注:<strong class="jp ir"> 1 个通道</strong>，不是所有通道)，得到每 25 个像素组的标量积，给出一个 8x8x1 的图像。将这些图像堆叠在一起会创建一个 8x8x3 的图像。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h2 id="c0ed" class="mn kt iq bd ku mo mp dn ky mq mr dp lc jy ms mt lg kc mu mv lk kg mw mx lo my bi translated">第 2 部分—逐点卷积:</h2><p id="4d6d" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">请记住，原始卷积将 12x12x3 图像转换为 8x8x256 图像。目前，深度方向卷积已将 12x12x3 图像转换为 8x8x3 图像。现在，我们需要增加每个图像的通道数。</p><p id="f206" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">逐点卷积之所以这样命名，是因为它使用了一个 1x1 内核，或者说是一个遍历每一个点的内核。无论输入图像有多少通道，该内核都具有深度；在我们的例子中，是 3。因此，我们通过 8x8x3 图像迭代 1x1x3 内核，得到 8x8x1 图像。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi nf"><img src="../Images/e637796e9fcbcab28f42583dacec403e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*37sVdBZZ9VK50pcAklh8AQ.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Image 7: Pointwise convolution, transforms an image of 3 channels to an image of 1 channel</figcaption></figure><p id="914f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以创建 256 个 1×1×3 的内核，每个内核输出一个 8×8×1 的图像，从而得到形状为 8×8×256 的最终图像。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ng"><img src="../Images/f26e82d065220c4fa1c6703a34335cc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7a20gyuunpJzXGnWayUDQ.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Image 8: Pointwise convolution with 256 kernels, outputting an image with 256 channels</figcaption></figure><p id="4cc7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">就是这样！我们已经将卷积分成两部分:深度方向卷积和点方向卷积。再抽象一点，如果原卷积函数是 12x12x3 — (5x5x3x256) →12x12x256，我们可以把这个新卷积图解为 12 x12 x3—(5x 5x 1 x1)—&gt;(1x1 x3 x256)—&gt; 12 x12 x256。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h2 id="a22b" class="mn kt iq bd ku mo mp dn ky mq mr dp lc jy ms mt lg kc mu mv lk kg mw mx lo my bi translated">好吧，但是创建深度方向可分离卷积有什么意义呢？</h2><p id="e228" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">让我们计算一下计算机在原始卷积中要做的乘法次数。有 256 个 5x5x3 内核移动 8x8 次。也就是 256x3x5x5x8x8=1，228，800 次乘法运算。</p><p id="75a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可分卷积呢？在深度方向卷积中，我们有 3 个移动 8x8 次的 5x5x1 内核。也就是 3x5x5x8x8 = 4，800 次乘法。在逐点卷积中，我们有 256 个移动 8×8 次的 1×1×3 内核。也就是 256x1x1x3x8x8=49，152 次乘法。把它们加在一起，就是 53，952 次乘法。</p><p id="e07e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">52952 比 1228800 少很多。随着计算量的减少，网络能够在更短的时间内处理更多的数据。</p><p id="1660" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是，这是怎么做到的呢？第一次碰到这种解释，直觉上对我来说真的没什么意义。这两个卷积不是在做同样的事情吗？在这两种情况下，我们通过 5x5 内核传递图像，将其缩小到一个通道，然后扩展到 256 个通道。为什么一个比另一个快两倍多？</p><p id="100d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">思考了一段时间后，我意识到主要的区别是这样的:在正常卷积中，我们对图像进行了 256 次的<strong class="jp ir">变换。并且每一次变换使用多达 5x5x3x8x8=4800 次乘法。在可分离卷积中，我们只真正地<strong class="jp ir">变换图像一次</strong>——在深度方向卷积中。然后，我们把变换后的图像<strong class="jp ir">简单地拉长到 256 通道</strong>。不必一次又一次地变换图像，我们可以节省计算能力。</strong></p><p id="d1cc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">值得注意的是，在 Keras 和 Tensorflow 中，都有一个被称为“深度乘数”的论点。默认设置为 1。通过更改此参数，我们可以更改深度方向卷积中的输出通道数量。例如，如果我们将深度乘数设置为 2，每个 5x5x1 内核将给出 8x8x2 的输出图像，使得深度方向卷积的总(堆叠)输出为 8x8x6，而不是 8x8x3。有些人可能会选择手动设置深度乘数，以增加其神经网络中的参数数量，从而更好地学习更多特征。</p><p id="f312" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">深度方向可分卷积的缺点是什么？肯定！因为它减少了卷积中的参数数量，所以如果您的网络已经很小了，您可能会因为参数太少而结束，并且您的网络可能无法在训练期间正确学习。然而，如果使用得当，它能够在不显著降低效率的情况下提高效率，这使得它成为一个非常受欢迎的选择。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h2 id="1d6f" class="mn kt iq bd ku mo mp dn ky mq mr dp lc jy ms mt lg kc mu mv lk kg mw mx lo my bi translated">1x1 内核:</h2><p id="33d9" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">最后，因为逐点卷积使用了这个概念，所以我想谈谈 1x1 内核的用法。</p><p id="9b49" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以在可分离卷积之外使用 1x1 内核，或者说 n 个 1x1xm 内核，其中 n 是输出通道的数量，m 是输入通道的数量。1x1 内核的一个明显目的是增加或减少图像的深度。如果你发现你的卷积有太多或太少的通道，1x1 内核可以帮助平衡它。</p><p id="3fd6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，对我来说，1x1 内核的主要目的是应用非线性。在神经网络的每一层之后，我们可以应用一个激活层。无论是 ReLU、PReLU、Softmax 还是其他，激活层都是非线性的，不像卷积层。"直线的线性组合仍然是一条直线."非线性层扩展了模型的可能性，这通常使得“深”网络比“宽”网络更好。为了在不显著增加参数和计算的数量的情况下增加非线性层的数量，我们可以应用 1x1 内核并在它之后添加激活层。这有助于给网络增加一层深度。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><p id="821a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您有任何进一步的问题，请在下面留下评论！别忘了给这个故事一些掌声！</p></div></div>    
</body>
</html>