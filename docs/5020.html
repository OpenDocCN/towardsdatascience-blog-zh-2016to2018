<html>
<head>
<title>Machine Learning Introduction 2: Our first Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习简介 2:我们的第一个例子</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ml-preface-2-355b1775723e?source=collection_archive---------6-----------------------#2018-09-22">https://towardsdatascience.com/ml-preface-2-355b1775723e?source=collection_archive---------6-----------------------#2018-09-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="35d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本帖跟随<a class="ae kl" href="https://medium.com/datadriveninvestor/machine-learning-preface-ba69bca4701d" rel="noopener">机器学习简介 1 </a>。我们将把机器学习应用于营销分析，作为一个简单的商业应用用例。我们将调查流行的机器学习方法，并将每种方法应用于市场分析。我们观察到，一些方法在衡量不同营销活动的投资回报率(ROI)时更具解释性，而其他方法提供了更准确的销售预测。参见第一堂课，了解本文中使用的每一种方法的解释。</p><p id="9b1d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个内容是深度学习第一讲的后半部分。</p><h1 id="c1b5" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">学习目标</h1><p id="3e8a" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">这篇文章给出了回归、特征工程和使用神经网络建模数据集的真实例子。它激励并展示了每种方法。</p><h1 id="bf32" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">问题设置</h1><p id="542d" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在这个问题中，我们是一个虚构的营销组织。我们查看我们的历史支出和销售额，并将每日营销支出映射到每日销售额。</p><p id="327c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的环境经过简化，适合作为入门机器学习的学习工具。在这种设定下，我们有很多天的品牌销售，但是每一天品牌的销售额只取决于公司在那一天花了多少钱。</p><p id="ff38" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个例子中，有两种类型的营销支出，品牌营销(brand)创造品牌形象，直接面向消费者营销(d2c)推动销售。</p><p id="3355" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们根据这两种类型的营销都会增加销售额的规则来模拟这些数据，但是如果两者之间存在平衡，它们的贡献最大。支出与销售的图表如下:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/4dc7aaf483ff4a04052323ca8cef6f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*adRyZx4qIX2Zp2FWjaS5_w.jpeg"/></div></div></figure><p id="125d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">单个结果可以像 excel 表格一样可视化，例如:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/c887c24a3b20cc5d9b3300bd90f572e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*sIUzNY6za4A5_qlye-ffdA.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Table Visualizing Sales</figcaption></figure><h2 id="670e" class="mg kn iq bd ko mh mi dn ks mj mk dp kw jy ml mm la kc mn mo le kg mp mq li mr bi translated">编码活动:数据可视化</h2><p id="6880" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">我们从导入一些我们需要的包开始</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="1c7a" class="mg kn iq mt b gy mx my l mz na"><strong class="mt ir">import</strong> <strong class="mt ir">numpy</strong> <strong class="mt ir">as</strong> <strong class="mt ir">np<br/>import pandas as pd<br/>import</strong> <strong class="mt ir">matplotlib.pyplot</strong> <strong class="mt ir">as</strong> <strong class="mt ir">plt</strong><br/><strong class="mt ir">from</strong> <strong class="mt ir">mpl_toolkits.mplot3d</strong> <strong class="mt ir">import</strong> Axes3D</span></pre><p id="e59c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们设置了一个随机种子，因此如果您在本地计算机上执行此操作，您将会看到与我们在这里看到的相同的随机结果。这对再现性很重要。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="c968" class="mg kn iq mt b gy mx my l mz na">np.random.seed(0)</span></pre><p id="1393" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们生成数据集</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="b8a6" class="mg kn iq mt b gy mx my l mz na">NUM_SAMPLES = int(2e3)<br/>brand_spend = np.random.rand(NUM_SAMPLES)<br/>d2c_spend = np.random.rand(NUM_SAMPLES)<br/>individual_contributions = brand_spend * .01 + d2c_spend * .02<br/>collaboration = np.square(brand_spend) * d2c_spend * 10<br/>sales = individual_contributions + collaboration + np.random.rand(NUM_SAMPLES)/10</span></pre><p id="2c90" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，对于线性回归部分，我们有一个执行线性回归的函数和另一个绘制结果的函数:</p><p id="8bf5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，我们将数据集包装到一个数据帧中。这让我们可以像在 python 中查看 excel 表格一样查看数据集。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="c9b9" class="mg kn iq mt b gy mx my l mz na">dataset = pd.DataFrame({<br/>    'brand': brand_spend,<br/>    'd2c': d2c_spend,<br/>    'sales': sales<br/>}).round(5)</span></pre><p id="2471" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以通过以下方式查看</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="9a80" class="mg kn iq mt b gy mx my l mz na">dataset.head()</span></pre><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/c887c24a3b20cc5d9b3300bd90f572e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*sIUzNY6za4A5_qlye-ffdA.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Table Visualizing Sales</figcaption></figure><p id="71ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们将数据集分成 X 和 Y，并可视化这些值。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="f27d" class="mg kn iq mt b gy mx my l mz na">X = dataset.drop('sales', 1)<br/>Y = dataset['sales']<br/>X1 = X['brand']<br/>X2 = X['d2c']<br/>fig = plt.figure(dpi=80, figsize = (10, 4))<br/>ax = fig.add_subplot(111, projection='3d')<br/>ax.scatter(X1, X2, Y, c='r', label='real sales', s = 1)<br/>ax.set_xlabel('Brand Marketing')<br/>ax.set_ylabel('Direct to Consumer')<br/>ax.set_zlabel('Sales')<br/>plt.legend()<br/>plt.savefig('images/data.jpg')</span></pre><p id="bfe3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">概念检查:用 Python 自己建立一个数据集:<br/> </strong>用营销活动和销售之间的线性和自定义数学关系建立并可视化一个类似的数据集。</p><p id="68b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你能看到你在营销活动之间建立的互动吗？</p><p id="ff75" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">试着在脚本的开始就参数化你的系统，以便于启用或禁用交互。</p><h2 id="eb8e" class="mg kn iq bd ko mh mi dn ks mj mk dp kw jy ml mm la kc mn mo le kg mp mq li mr bi translated">解决方案:</h2><p id="2109" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">这是一个备选数据集，其特征略有不同。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="e1b7" class="mg kn iq mt b gy mx my l mz na">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from mpl_toolkits.mplot3d import Axes3D<br/>np.random.seed(0)<br/>NUM_SAMPLES = int(2e3)<br/>noise_ratio= 1e-2<br/>brand_spend = np.random.rand(NUM_SAMPLES)<br/>d2c_spend = np.random.rand(NUM_SAMPLES)<br/>geo_mean_mult = 10<br/>exp_brand_spend_mult = 1<br/>contributions =  {'brand':.01, 'd2c':.02}</span><span id="a411" class="mg kn iq mt b gy nb my l mz na">brand_contributions = brand_spend * contributions['brand']<br/>d2c_contributions = d2c_spend * contributions['d2c']<br/>geo_contribution = np.sqrt(brand_spend* d2c_spend) * geo_mean_mult<br/>exp_brand_contribution = np.exp(brand_spend) * exp_brand_spend_mult</span><span id="624a" class="mg kn iq mt b gy nb my l mz na">sales = brand_contributions + d2c_contributions + exp_brand_spend_mult + np.random.rand(NUM_SAMPLES) * noise_ratio<br/>dataset = pd.DataFrame({<br/>    'brand': brand_spend,<br/>    'd2c': d2c_spend,<br/>    'sales': sales<br/>}).round(5)<br/>X = dataset.drop('sales', 1)<br/>Y = dataset['sales']<br/>X1 = X['brand']<br/>X2 = X['d2c']<br/>fig = plt.figure(dpi=80, figsize = (10, 4))<br/>ax = fig.add_subplot(111, projection='3d')<br/>ax.scatter(X1, X2, Y, c='r', label='real sales', s = 1)<br/>ax.set_xlabel('Brand Marketing')<br/>ax.set_ylabel('Direct to Consumer')<br/>ax.set_zlabel('Sales')<br/>plt.legend()<br/>plt.savefig('demo.jpg')</span></pre><h1 id="7f3a" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">方法概述</h1><p id="04de" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">为了了解营销支出的影响，我们将从使用线性回归将营销支出映射到销售额开始。然后，我们将查看我们的损失曲线，并认识到线性回归的不足，鼓励我们执行特征工程。</p><p id="f2aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在手动特征工程之后，为了帮助特征生成过程，我们使用神经网络的隐藏层来提取特征，并将其用于我们的最终回归问题。我们将比较和对比可解释性和性能。</p><h1 id="2392" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">建模线性回归</h1><p id="3357" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">为了了解销售和营销支出之间的关系，我们首先拟合一个线性回归模型。</p><p id="0e68" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是告诉模型填充以下公式中的空白:</p><p id="8dc1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">销售= __ *品牌营销+ __ *直接面向消费者+ __</p><p id="4655" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的线性回归通过学习参数来近似这条曲线:</p><p id="4c70" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">销售额= 5.11 *品牌营销+ 3.28 *直接面向消费者-2.45</p><p id="3ead" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是非常有见地的，因为我们可以直接读取每种支出的投资回报率，例如，我们可以将此解读为衡量品牌营销的投资回报率为 5，d2c 的投资回报率为 3。除了我们注意到一个问题！我们可以在下面直观地看到，该模型并没有准确地表示数据。模型学习到的<strong class="jp ir">预测</strong>显示在下面的<strong class="jp ir">蓝色</strong>中，显然不适合红色的销售:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/9220a147a007d569cf34bea776ed0fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vUR9hlTuF3MilUNKgykScQ.jpeg"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Linear Regression estimates vs Sales</figcaption></figure><p id="edd7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是一些用于训练线性回归模型和可视化预测的简单代码。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="db76" class="mg kn iq mt b gy mx my l mz na"><strong class="mt ir">from</strong> <strong class="mt ir">sklearn.neural_network</strong> <strong class="mt ir">import</strong> MLPRegressor<br/><strong class="mt ir">from</strong> <strong class="mt ir">sklearn.linear_model</strong> <strong class="mt ir">import</strong> Ridge<br/><strong class="mt ir">from</strong> <strong class="mt ir">mpl_toolkits.mplot3d</strong> <strong class="mt ir">import</strong> Axes3D<br/><strong class="mt ir">def</strong> model_sales_regression(dataset, model='Ridge'):<br/>    num_samples = dataset.shape[0]<br/>    cutoff = (num_samples * 3) // 4<br/>    Xtrn = dataset.drop('sales', 1).iloc[:cutoff,:]<br/>    Ytrn = dataset['sales'].iloc[:cutoff]<br/>    Xval = dataset.drop('sales', 1).iloc[cutoff:,:]<br/>    Yval = dataset['sales'].iloc[cutoff:]<br/>    model = Ridge().fit(Xtrn, Ytrn)<br/>    coefs = model.coef_.round(2)<br/>    yhat = model.predict(dataset.drop('sales', 1))<br/>    yhatval = model.predict(Xval)<br/>    loss = np.square(Yval - yhatval).mean()<br/>    <br/>    print('learned coefficients', list(zip(X.columns, coefs)))<br/>    print('loss:', loss)<br/>    print('intercept', model.intercept_.round(2))<br/>    <br/>    <strong class="mt ir">return</strong> model, yhat, coefs, loss</span><span id="ac59" class="mg kn iq mt b gy nb my l mz na"><strong class="mt ir">def</strong> graph_real_and_predicted(dataset, yhat, fname=<strong class="mt ir">None</strong>):<br/>    X = dataset.drop('sales', 1)<br/>    Y = dataset['sales']<br/>    X1 = X['brand']<br/>    X2 = X['d2c']<br/>    fig = plt.figure(dpi=80, figsize=(10, 4))<br/>    ax = fig.add_subplot(111, projection='3d')<br/>    ax.scatter(X1, X2, Y, c='r', label='real sales', s = 1)<br/>    ax.scatter(X1, X2, yhat, c='b', label='estimated sales', s = 1)<br/>    ax.set_xlabel('Brand Marketing')<br/>    ax.set_ylabel('Direct to Consumer')<br/>    ax.set_zlabel('Sales')<br/>    plt.legend()<br/>    <strong class="mt ir">if</strong> fname <strong class="mt ir">is</strong> <strong class="mt ir">not</strong> <strong class="mt ir">None</strong>:<br/>        plt.savefig('images/' + fname + '.jpg')</span></pre><p id="7763" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是训练线性回归的代码。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="9731" class="mg kn iq mt b gy mx my l mz na">model, yhat, coefs, loss = model_sales_regression(dataset)<br/>graph_real_and_predicted(dataset, yhat, 'linear')</span></pre><h1 id="8df2" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">误差量化</h1><p id="a3de" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">为了总结我们模型的准确性，我们来看看真实销售额和我们的预测之间的平方误差。我们用这些平方误差的平均值来表示我们的性能。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h2 id="dc62" class="mg kn iq bd ko mh mi dn ks mj mk dp kw jy ml mm la kc mn mo le kg mp mq li mr bi translated">为什么是平方误差？</h2><p id="c1ce" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">这是基于最小化最坏情况的常见默认选择。假设 1 的误差是可以接受的，如果我们经常在预测中误差 1，这并不完美，但我们可以处理它。但是如果我们有一次差了 3 分，那是不可接受的。我们从来不想偏离太多，但是我们可以接受经常偏离很少。误差平方会是 1 比 1 的平方，这没问题，它会是 3 比 9 的平方，这是一个更大的误差。</p><p id="1dc9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">平方误差是一种常见的误差度量，因为它可以最小化最坏情况下的误差(或者让我们这样想，并在后面列出其他数学原因)。</p><h2 id="59b6" class="mg kn iq bd ko mh mi dn ks mj mk dp kw jy ml mm la kc mn mo le kg mp mq li mr bi translated">概念检查:提出两个可供选择的有意义的误差度量。</h2><p id="b388" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">如果在课堂上阅读，分组讨论。</p><h2 id="d465" class="mg kn iq bd ko mh mi dn ks mj mk dp kw jy ml mm la kc mn mo le kg mp mq li mr bi translated">一些解决方案:</h2><p id="6365" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">1-平均绝对误差(我们预测的绝对误差的平均值)是有意义的，但是不会随着你犯大的误差而迅速增加。如果我们更关心百分比误差而不是实际误差，那么 2-平均绝对百分比误差(我们平均偏离的百分比)是有意义的。<br/> <code class="fe nj nk nl mt b">mean(abs(Y-yhat)/Y)</code></p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><p id="2789" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了确保我们计算出一个现实的性能衡量标准，我们在模型没有训练的数据点上衡量我们的损失。因此，我们从一些数据开始，用其中一些数据来拟合模型的参数，用其他数据点来测试我们的性能。这接近于衡量模型在未来对未知数据的表现。</p><p id="58de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用线性回归，我们的均方误差为 0.89。这还没有意义，但与其他模型相比，以后会有意义。</p><p id="bac1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是，我们可以清楚地看到，我们的模型没有准确地捕捉销售趋势，因为我们看到了预测和真实销售之间的可预测差异区域。</p><h1 id="1550" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">特征工程</h1><p id="840f" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">作为一个聪明的专业人士，我们得出结论，从花费的线性回归不能捕捉所有的数据结构。为了扩展我们的模型，我们注意到当两个输入都很大时，显著增加，所以我们可能需要一个特征来捕捉两种花费类型的组合。因此，我们产生了一个特征，即品牌营销的产品*直接面向消费者的营销支出。要让这个功能变大，需要品牌和 d2c 支出都很高。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h2 id="1b28" class="mg kn iq bd ko mh mi dn ks mj mk dp kw jy ml mm la kc mn mo le kg mp mq li mr bi translated">查看我们的新功能</h2><p id="d4d0" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">让我们快速检查一下这个新特性，brand * d2c。</p><p id="e520" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们没有钱花的时候会发生什么？该值为 0。如果 brand 是 1，d2c 是 1 呢？值是 1 * 1 = 1。所以它衡量营销支出？</p><p id="deb8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是如果我们有 10 美元可以花，我们在 d2c 上花了 1 美元，在品牌上花了 9 美元呢？我们有 1 * 9 = 9。好吧，但是如果我们拿同样的一美元，把它分成两类消费，每类 5 美元。我们有 5 * 5 = 25！该特征检测“两种花费类型都有投资的平衡”。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><p id="734b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们的模型学习使用函数更好地逼近曲线:</p><p id="8ddf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">销售额= 0.68 *品牌— 1.01 * d2c + 8.74 *品牌* d2c — 0.29</p><h2 id="0f88" class="mg kn iq bd ko mh mi dn ks mj mk dp kw jy ml mm la kc mn mo le kg mp mq li mr bi translated">概念检查:特征工程</h2><p id="1a51" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在运行线性回归和可视化预测之前，尝试在熊猫数据框架中构建特征品牌* d2c。在过去，这是一项非常有意义、深思熟虑的任务，是 ML 的一大部分。</p><h2 id="ffd6" class="mg kn iq bd ko mh mi dn ks mj mk dp kw jy ml mm la kc mn mo le kg mp mq li mr bi translated">解决方案:特征工程</h2><p id="5930" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">以下解决方案:</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="50fe" class="mg kn iq mt b gy mx my l mz na">dataset['brand * d2c'] = dataset['brand'] * dataset['d2c']<br/>model, yhat, coefs, loss = model_sales_regression(dataset)<br/>graph_real_and_predicted(dataset, yhat, 'feature_engineering')</span></pre><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/699217a83be3adcbc174d78d75ec84b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fkzR5aABI7R3PJ9sk1eN3w.jpeg"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Feature Engineering Approximation</figcaption></figure></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><p id="b167" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这看起来不错。我们的预测现在更好地跟踪销售，但我们仍然不完美。我们的误差现在是 0.2，低于纯线性回归的 0.9。</p><p id="a8d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看我们的参数，可以看到品牌和 d2c 营销的组合是最强的影响者，系数为 8.74。这只是真实销售额的近似值，但该参数可以这样解释:如果品牌支出为 0，那么增加 d2c 支出将无济于事，因为无论如何，品牌* d2c 都将为 0。</p><p id="b29f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="nm">如果品牌为 0，那么从 d2c 到组合期限的投资回报率为 0。</em>T3】</strong></p><p id="effb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是，如果品牌支出为 1(千美元)，那么随着我们增加 d2c 支出，我们每支出 1 美元将获得 8.74 美元的投资回报。</p><p id="f63b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="nm">如果品牌为 1，那么从 d2c 到组合期限的投资回报率为 8.74。</em>T3】</strong></p><p id="7374" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这听起来不太合理，但作为一个近似值，它告诉我们，增加销售的最佳方式是在品牌和 d2c 营销之间取得平衡，因为组合是最大的因素。</p><p id="e373" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此时，我们认识到我们仍然有 0.2 的显著损失，并且仍然有可预测误差的可见区域，并且认为我们仍然可以改进我们的模型。</p><h1 id="8f9e" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">用于特征提取的小型神经网络</h1><p id="cc80" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">为了添加更多生成的特征而不必定义它们应该是什么，我们考虑一个小型神经网络，它生成 6 个特征，然后使用这些特征来预测销售。</p><h2 id="6ea3" class="mg kn iq bd ko mh mi dn ks mj mk dp kw jy ml mm la kc mn mo le kg mp mq li mr bi translated">概念检查:编写一个 sklearn 神经网络代码</h2><p id="a617" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">使用 sklearn.neural_network 中的 MLPRegressor 生成特征，并用 6 个隐藏单元对销售进行建模，然后显示模型学习到的特征。</p><h2 id="cc03" class="mg kn iq bd ko mh mi dn ks mj mk dp kw jy ml mm la kc mn mo le kg mp mq li mr bi translated">解决方案:编写一个 sklearn 神经网络</h2><p id="e8f1" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">下面是像以前一样分割数据集的代码，但是使用了神经网络。然后，它将系数组合在一起，以报告模型表示。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="59ff" class="mg kn iq mt b gy mx my l mz na"><strong class="mt ir">def</strong> model_sales_MLP(dataset, hidden, print_coefs = <strong class="mt ir">True</strong>, max_iter= 10000):<br/>    num_samples = dataset.shape[0]<br/>    cutoff = (num_samples * 3) // 4<br/>    Xtrn = dataset.drop('sales', 1).iloc[:cutoff,:]<br/>    Ytrn = dataset['sales'].iloc[:cutoff]<br/>    Xval = dataset.drop('sales', 1).iloc[cutoff:,:]<br/>    Yval = dataset['sales'].iloc[cutoff:]<br/>    model = MLPRegressor(hidden, validation_fraction = 0, solver='lbfgs', max_iter= max_iter).fit(Xtrn, Ytrn)<br/>    coefs = model.coefs_<br/>    yhat = model.predict(X)<br/>    yhatval = model.predict(Xval)<br/>    loss = np.square(Yval - yhatval).mean()<br/>    hiddens = coefs[0].T<br/>    final_mlp = coefs[1].flatten()<br/>    <br/>    coefs = list(zip([dict(zip(X.columns, h)) <strong class="mt ir">for</strong> h <strong class="mt ir">in</strong> hiddens],<br/>                     [['output mult:', m] <strong class="mt ir">for</strong> m <strong class="mt ir">in</strong>  final_mlp.flatten()], <br/>                     [['intercept:', i] <strong class="mt ir">for</strong> i <strong class="mt ir">in</strong>  model.intercepts_[0]]))<br/>    print('loss:', loss)<br/>    <strong class="mt ir">if</strong> print_coefs:<br/>        <strong class="mt ir">for</strong> idx, c <strong class="mt ir">in</strong> enumerate(coefs):<br/>            f1, o, i = c<br/>            print('feature', idx, '=', f1['brand'].round(2), '* brand +', <br/>                  f1['d2c'].round(2), '* d2c', '+', i[1].round(2))<br/>        output = 'yhat = '<br/>        <strong class="mt ir">for</strong> fidx, v <strong class="mt ir">in</strong> enumerate(final_mlp):<br/>            output = output + str(v.round(2)) + ' * feat ' + str(fidx) + ' + '<br/>        output = output + str(model.intercepts_[1][0].round(2))<br/>        print(output)<br/>    <strong class="mt ir">return</strong> model, yhat, coefs, loss</span></pre><p id="90e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们从我们的数据集中创建模型并绘制它，用一个 6 个单元的隐藏层</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="6248" class="mg kn iq mt b gy mx my l mz na">model, yhat, coefs, loss = model_sales_MLP(dataset, [6])<br/>graph_real_and_predicted(dataset, yhat, 'neural_network')</span></pre></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><p id="6610" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在训练好我们的神经网络之后，我们考察模型的特征:<br/>特征 0 = 2.85 * brand + 1.83 * d2c -2.79 <br/>特征 1 = 1.62 * brand -2.15 * d2c -0.03 <br/>特征 2 = 0.08 * brand -0.54 * d2c -0.77 <br/>特征 3 = 0.55 * brand + 0.16 * d2c -0.81 <br/>特征 4 = 3.83 *</p><p id="2444" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">然后，在用于如下预测 yhat 之前，对特征进行剪裁以将任何负值设置为零:</strong></p><p id="ea9f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果特征[i]是正的:特征[i]=特征[I]</p><p id="90b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果特征[i]为负:feat[i] = 0</p><p id="c840" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Yhat = 3.0 *专长[0] -1.48 *专长[1] + 0.82 *专长[2] -0.2 *专长[3] + 1.6 *专长[4] -1.82 *专长[5]–0.56</p><p id="5ba8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">哇有那么难解读吗！在后面的讲座中，我们将开始解释神经网络，但我们肯定不会试图去解读每一个单独的特征，那会太乏味了。</p><p id="e3d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是看看我们的损失，我们下降到 0.12 的损失！此外，我们的图表现在看起来与真实销售额非常接近。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/781a345ba250110d9bf5d1359a214802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qHrT5pJPSNcEeuJkRH8CUQ.jpeg"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Neural Network Approximation</figcaption></figure><h1 id="93da" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">深度神经网络</h1><p id="e15f" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">最后，让我们假设我们不是试图通过衡量投资回报率来为营销决策提供信息，而只是试图预测每日销售额。在我们的神经网络中，不是 6 个隐藏单元(6 个特征)，而是使用 100 个隐藏单元，并使用该隐藏层来计算 100 个单元的新隐藏层，最终用于执行线性回归。</p><p id="b811" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这进一步减少了我们的损失，现在我们几乎完全符合数据集，损失为 0.001。现在想想，这甚至不是训练数据。这不是模型与训练数据的吻合程度，而是模型与训练过程中从未见过的数据的吻合程度。</p><p id="d165" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是如果我们想解释这个模型呢？我们有一个 100 单位的隐藏层，然后又有一个 100 单位的隐藏层。因此，为了计算第二隐藏层，我们从第一隐藏层为第二隐藏层中的 100 个单元中的每一个计算 100 次乘法。那是它学过的一万次乘法！我们永远无法概念化这么多的相互作用，但我们认识到神经网络已经开发出这些特征来精确地拟合数据。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/fd5e61778317f08cd4582bcdf6463218.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S7y2jJ64leUjCPa30NQKeA.jpeg"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Deep network appears to fit dataset perfectly</figcaption></figure><h1 id="ac14" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">包扎</h1><p id="5b00" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">让我们回顾一下。初始线性回归对数据集进行了建模，但它显然有不太适合的区域。我们执行了一些特征工程，因为我们从对问题的思考中猜出了一些有价值的特征。这帮助我们更好地拟合数据集，我们的模型更准确，但有点难以理解。</p><p id="6bf4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们在网络中引入了一个隐藏层来学习我们的特征，我们提出了六个特征，以某种方式为我们解决了问题。这些变得更加难以解释，但我们知道这些特征在某种程度上比我们最初的两三个特征更好地理解了模型。</p><p id="916b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们最后让一个深度神经网络来解决这个问题。我们观察到这个模型更加精确，但是解释成千上万的乘法运算是不可行的。</p><h1 id="a8e7" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">代码分析</h1><p id="58dc" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">下面我们将快速重新执行上面的所有分析，同时只报告代码和简要总结。首先，我们导入一些将在以后对我们有帮助的库</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="1b3a" class="mg kn iq mt b gy mx my l mz na"><strong class="mt ir">import</strong> <strong class="mt ir">numpy</strong> <strong class="mt ir">as</strong> <strong class="mt ir">np</strong><br/><strong class="mt ir">import</strong> <strong class="mt ir">pandas</strong> <strong class="mt ir">as</strong> <strong class="mt ir">pd</strong><br/><strong class="mt ir">import</strong> <strong class="mt ir">matplotlib</strong><br/>%matplotlib notebook<br/><strong class="mt ir">import</strong> <strong class="mt ir">matplotlib.pyplot</strong> <strong class="mt ir">as</strong> <strong class="mt ir">plt</strong><br/><strong class="mt ir">from</strong> <strong class="mt ir">sklearn.neural_network</strong> <strong class="mt ir">import</strong> MLPRegressor<br/><strong class="mt ir">from</strong> <strong class="mt ir">sklearn.linear_model</strong> <strong class="mt ir">import</strong> Ridge<br/><strong class="mt ir">from</strong> <strong class="mt ir">mpl_toolkits.mplot3d</strong> <strong class="mt ir">import</strong> Axes3D</span></pre><p id="2c4a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们设置了一个随机种子，因此如果您在本地计算机上执行此操作，您将会看到与我们在这里看到的相同的随机结果。这对再现性很重要。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="198e" class="mg kn iq mt b gy mx my l mz na">np.random.seed(0)</span></pre><p id="6139" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们生成数据集</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="4803" class="mg kn iq mt b gy mx my l mz na">NUM_SAMPLES = int(2e3)</span><span id="e34e" class="mg kn iq mt b gy nb my l mz na">brand_spend = np.random.rand(NUM_SAMPLES)<br/>d2c_spend = np.random.rand(NUM_SAMPLES)<br/>individual_contributions = brand_spend * .01 + d2c_spend * .02<br/>collaboration = np.square(brand_spend) * d2c_spend * 10<br/>sales = individual_contributions + collaboration + np.random.rand(NUM_SAMPLES)/10</span></pre><p id="70a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，对于线性回归部分，我们有一个执行线性回归的函数和另一个绘制结果的函数:</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="c4fe" class="mg kn iq mt b gy mx my l mz na"><strong class="mt ir">def</strong> model_sales_regression(dataset, model='Ridge'):<br/>    num_samples = dataset.shape[0]<br/>    cutoff = (num_samples * 3) // 4<br/>    Xtrn = dataset.drop('sales', 1).iloc[:cutoff,:]<br/>    Ytrn = dataset['sales'].iloc[:cutoff]<br/>    Xval = dataset.drop('sales', 1).iloc[cutoff:,:]<br/>    Yval = dataset['sales'].iloc[cutoff:]<br/>    model = Ridge().fit(Xtrn, Ytrn)<br/>    coefs = model.coef_.round(2)<br/>    yhat = model.predict(dataset.drop('sales', 1))<br/>    yhatval = model.predict(Xval)<br/>    loss = np.square(Yval - yhatval).mean()<br/>    <br/>    print('learned coefficients', list(zip(X.columns, coefs)))<br/>    print('loss:', loss)<br/>    print('intercept', model.intercept_.round(2))<br/>    <br/>    <strong class="mt ir">return</strong> model, yhat, coefs, loss</span><span id="f205" class="mg kn iq mt b gy nb my l mz na"><strong class="mt ir">def</strong> graph_real_and_predicted(dataset, yhat, fname=<strong class="mt ir">None</strong>):<br/>    X = dataset.drop('sales', 1)<br/>    Y = dataset['sales']<br/>    X1 = X['brand']<br/>    X2 = X['d2c']<br/>    fig = plt.figure(dpi=80, figsize=(10, 4))<br/>    ax = fig.add_subplot(111, projection='3d')<br/>    ax.scatter(X1, X2, Y, c='r', label='real sales', s = 1)<br/>    ax.scatter(X1, X2, yhat, c='b', label='estimated sales', s = 1)<br/>    ax.set_xlabel('Brand Marketing')<br/>    ax.set_ylabel('Direct to Consumer')<br/>    ax.set_zlabel('Sales')<br/>    plt.legend()<br/>    <strong class="mt ir">if</strong> fname <strong class="mt ir">is</strong> <strong class="mt ir">not</strong> <strong class="mt ir">None</strong>:<br/>        plt.savefig('images/' + fname + '.jpg')</span></pre><p id="f030" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，我们将数据集包装到一个数据帧中。这让我们可以像在 python 中查看 excel 表格一样查看数据集。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="697e" class="mg kn iq mt b gy mx my l mz na">dataset = pd.DataFrame({<br/>    'brand': brand_spend,<br/>    'd2c': d2c_spend,<br/>    'sales': sales<br/>}).round(5)</span></pre><p id="0347" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以通过以下方式查看</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="8c13" class="mg kn iq mt b gy mx my l mz na">dataset.head()</span></pre><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/c887c24a3b20cc5d9b3300bd90f572e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*sIUzNY6za4A5_qlye-ffdA.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Table Visualizing Sales</figcaption></figure><p id="b420" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们将数据集分成 X 和 Y，并可视化这些值。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="4887" class="mg kn iq mt b gy mx my l mz na">X = dataset.drop('sales', 1)<br/>Y = dataset['sales']<br/>X1 = X['brand']<br/>X2 = X['d2c']<br/>fig = plt.figure(dpi=80, figsize = (10, 4))<br/>ax = fig.add_subplot(111, projection='3d')<br/>ax.scatter(X1, X2, Y, c='r', label='real sales', s = 1)<br/>ax.set_xlabel('Brand Marketing')<br/>ax.set_ylabel('Direct to Consumer')<br/>ax.set_zlabel('Sales')<br/>plt.legend()<br/>plt.savefig('images/data.jpg')</span></pre><p id="6f95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是训练线性回归的代码。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="72ca" class="mg kn iq mt b gy mx my l mz na">model, yhat, coefs, loss = model_sales_regression(dataset)<br/>graph_real_and_predicted(dataset, yhat, 'linear')</span></pre><p id="c0f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们添加一个要素，并从扩充的数据集执行线性回归。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="5e40" class="mg kn iq mt b gy mx my l mz na">dataset['brand * d2c'] = dataset['brand'] * dataset['d2c']<br/>model, yhat, coefs, loss = model_sales_regression(dataset)<br/>graph_real_and_predicted(dataset, yhat, 'feature_engineering')</span></pre><p id="5a3c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">转到神经网络，我们定义我们的多层感知器(神经网络的另一个术语)功能。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="ac09" class="mg kn iq mt b gy mx my l mz na"><strong class="mt ir">def</strong> model_sales_MLP(dataset, hidden, print_coefs = <strong class="mt ir">True</strong>, max_iter= 10000):<br/>    num_samples = dataset.shape[0]<br/>    cutoff = (num_samples * 3) // 4<br/>    Xtrn = dataset.drop('sales', 1).iloc[:cutoff,:]<br/>    Ytrn = dataset['sales'].iloc[:cutoff]<br/>    Xval = dataset.drop('sales', 1).iloc[cutoff:,:]<br/>    Yval = dataset['sales'].iloc[cutoff:]<br/>    model = MLPRegressor(hidden, validation_fraction = 0, solver='lbfgs', max_iter= max_iter).fit(Xtrn, Ytrn)<br/>    coefs = model.coefs_<br/>    yhat = model.predict(X)<br/>    yhatval = model.predict(Xval)<br/>    loss = np.square(Yval - yhatval).mean()<br/>    hiddens = coefs[0].T<br/>    final_mlp = coefs[1].flatten()<br/>    <br/>    coefs = list(zip([dict(zip(X.columns, h)) <strong class="mt ir">for</strong> h <strong class="mt ir">in</strong> hiddens],<br/>                     [['output mult:', m] <strong class="mt ir">for</strong> m <strong class="mt ir">in</strong>  final_mlp.flatten()], <br/>                     [['intercept:', i] <strong class="mt ir">for</strong> i <strong class="mt ir">in</strong>  model.intercepts_[0]]))<br/>    print('loss:', loss)<br/>    <strong class="mt ir">if</strong> print_coefs:<br/>        <strong class="mt ir">for</strong> idx, c <strong class="mt ir">in</strong> enumerate(coefs):<br/>            f1, o, i = c<br/>            print('feature', idx, '=', f1['brand'].round(2), '* brand +', <br/>                  f1['d2c'].round(2), '* d2c', '+', i[1].round(2))<br/>        output = 'yhat = '<br/>        <strong class="mt ir">for</strong> fidx, v <strong class="mt ir">in</strong> enumerate(final_mlp):<br/>            output = output + str(v.round(2)) + ' * feat ' + str(fidx) + ' + '<br/>        output = output + str(model.intercepts_[1][0].round(2))<br/>        print(output)<br/>    <strong class="mt ir">return</strong> model, yhat, coefs, loss</span></pre><p id="2c45" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们对原始销售和支出数据使用我们的函数，没有增强功能。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="4100" class="mg kn iq mt b gy mx my l mz na">dataset = pd.DataFrame({<br/>    'brand': brand_spend,<br/>    'd2c': d2c_spend,<br/>    'sales': sales<br/>}).round(5)</span><span id="2c5e" class="mg kn iq mt b gy nb my l mz na">model, yhat, coefs, loss = model_sales_MLP(dataset, [6])<br/>graph_real_and_predicted(dataset, yhat, 'neural_network')</span></pre><p id="11b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们再次调用函数，但是使用更深的神经网络。</p><pre class="lq lr ls lt gt ms mt mu mv aw mw bi"><span id="292a" class="mg kn iq mt b gy mx my l mz na">model, yhat, coefs, loss = model_sales_MLP(dataset, [100, 100], max_iter = 1000, print_coefs=<strong class="mt ir">False</strong>)<br/>graph_real_and_predicted(dataset, yhat, fname = <strong class="mt ir">None</strong>)</span></pre><p id="5122" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是从市场归因和评估的实用角度对机器学习的介绍！</p><p id="10ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">上面的代号是:</strong> <a class="ae kl" href="https://github.com/leedtan/LeeTanData/blob/master/MachineLearningPrefaceCode/ML_Preface_2.ipynb" rel="noopener ugc nofollow" target="_blank">这里的</a></p><p id="313d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">此问题展开:</strong> <a class="ae kl" href="https://medium.com/datadriveninvestor/machine-learning-preface-ba69bca4701d" rel="noopener"> ML 前言</a>。</p><p id="8cca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">本系列预期跟进</strong>:<a class="ae kl" rel="noopener" target="_blank" href="/ml-intro-3-logistic-output-units-ec42cc576634">https://towards data science . com/ml-intro-3-logistic-output-units-EC 42 cc 576634</a></p><p id="b0d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">对于完全理解这一条的有意跟进帖子:</strong><a class="ae kl" href="https://medium.com/@leetandata/machine-learning-engineering-1-custom-loss-function-for-house-sales-estimation-95eec6b12457" rel="noopener">https://medium . com/@ leetandata/machine-learning-engineering-1-custom-loss-function-for-house-sales-estimating-95 eec6b 12457</a></p><p id="aeb7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">对于苦于代码或理解的人可选后续:<br/> </strong>本帖代码详细分析:<a class="ae kl" href="https://medium.com/@leetandata/machine-learning-python-programming-introduction-for-business-people-10588e13ce9d" rel="noopener">https://medium . com/@ leetandata/machine-learning-python-programming-introduction-for-business-people-10588 e 13 ce 9d</a></p><p id="0b37" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于 Python 编程的免费综合课程。确保注册，但选择免费的可选项目。所有课堂内容免费提供:<br/><a class="ae kl" href="https://www.coursera.org/learn/python-programming-introduction/" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/python-programming-introduction/</a></p><p id="913d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个比这个更复杂的机器学习教程，但比下面的(不是我写的)更容易<a class="ae kl" href="https://www.kaggle.com/rochellesilva/simple-tutorial-for-beginners" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/rochelle Silva/simple-tutorial-for-初学者</a></p><p id="da32" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">软件工程师可选后续岗位:<br/> </strong>重数学、重 CS 的详解(上):<a class="ae kl" href="https://medium.com/@leetandata/neural-network-introduction-for-software-engineers-1611d382c6aa" rel="noopener">https://medium . com/@ leetandata/neural-network-introduction-for-Software-Engineers-1611d 382 C6 aa</a></p><p id="7ba9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">重数学、重 CS 的详解(下):<a class="ae kl" href="https://medium.com/@leetandata/neural-network-for-software-engineers-2-mini-batch-training-and-validation-46ee0a1269a0" rel="noopener">https://medium . com/@ leetandata/neural-network-for-software-engineers-2-mini-batch-training-and-validation-46ee 0a 1269 a 0</a></p><p id="8d4a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">Cho 教授可选数学笔记:</strong><br/><a class="ae kl" href="https://github.com/nyu-dl/Intro_to_ML_Lecture_Note/raw/master/lecture_note.pdf" rel="noopener ugc nofollow" target="_blank">https://github . com/NYU-dl/Intro _ to _ ML _ Lecture _ Note/raw/master/Lecture _ Note . pdf</a></p></div></div>    
</body>
</html>