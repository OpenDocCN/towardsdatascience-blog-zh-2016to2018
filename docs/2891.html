<html>
<head>
<title>Vehicle Detection and Tracking using Machine Learning and HOG</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于机器学习和 HOG 的车辆检测和跟踪</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/vehicle-detection-and-tracking-using-machine-learning-and-hog-f4a8995fc30a?source=collection_archive---------5-----------------------#2018-03-17">https://towardsdatascience.com/vehicle-detection-and-tracking-using-machine-learning-and-hog-f4a8995fc30a?source=collection_archive---------5-----------------------#2018-03-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="7d83" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我进入了 Udacity 自动驾驶汽车 Nanodegree 的第一个学期，我想分享我关于第一学期最终项目的经验，即车辆检测和跟踪。完整的代码可以在<a class="ae kl" href="https://github.com/harveenchadha/Udacity-CarND-Vehicle-Detection-and-Tracking" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/e3d476a0ac716197ee4fdf6aa70b37ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*RCtm1RMBgNZozAVuM6TV3w.jpeg"/></div></figure><h1 id="9c71" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">介绍</h1><p id="75c4" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">这个项目的基本目标是应用 HOG 和机器学习的概念从仪表板视频中检测车辆。等一下？机器学习和 2018 年的物体检测？听起来过时了，不是吗？当然，利用卷积神经网络的深度学习实现，如 YOLO 和 SSD，在这方面表现突出，但如果你是这一领域的初学者，最好从经典方法开始。所以让我们开始吧！！</p><h1 id="8448" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">收集数据</h1><p id="f58b" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">对于任何机器学习问题来说，最重要的是带标签的数据集，这里我们需要两组数据:车辆和非车辆图像。这些图像取自一些已经可用的数据集，如<a class="ae kl" href="http://www.gti.ssr.upm.es/data/Vehicle_database.html" rel="noopener ugc nofollow" target="_blank"> GTI </a>和<a class="ae kl" href="http://www.cvlibs.net/datasets/kitti/" rel="noopener ugc nofollow" target="_blank"> KITTI Vision </a>。图像大小为 64x64，看起来有点像这样:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi lx"><img src="../Images/caaacf03ad74c177600fc9ef9e10f9de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tbw4_4zf75g6ElYZYn9Stg.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 1. Vehicle Images</figcaption></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mg"><img src="../Images/017674a7df19f5c48d80d1f07eed11cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xomTyjYSJpJCw96ZwM4YHA.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 2. Non Vehicle Images</figcaption></figure><h1 id="040b" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">提取特征</h1><p id="e10e" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">一旦我们得到了数据集，下一个明显的步骤就是从图像中提取特征。但是为什么呢？为什么不能把图像原样喂给机器学习分类器？我的朋友，如果我们这样做，将需要很长时间来处理图像，只是提醒一下，我们不是在这里向 CNN 提供图像，这毕竟不是一个深度学习问题！</p><p id="5496" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">明白了，但是如何提取特征呢？如果你想从图像中提取特征，有三种好方法。</p><ol class=""><li id="2c01" class="mh mi iq jp b jq jr ju jv jy mj kc mk kg ml kk mm mn mo mp bi translated"><strong class="jp ir">颜色直方图- </strong>最简单直观的方法就是从图像的各个颜色通道中提取特征。这可以通过绘制各种颜色通道的直方图，然后从直方图的箱中收集数据来完成。这些箱给了我们关于图像的有用信息，并且在提取好的特征方面非常有帮助。</li></ol><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mq"><img src="../Images/4f2e5f3bd1f6da5ac5acd5f9ae4a0828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rnl12KlaDrqvAiPuoL50GQ.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 3. Color Histograms for Vehicle Image</figcaption></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mr"><img src="../Images/42f9a311f91927e5f40a32996309055d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A0r3aHFKJ-HTSMQ01fw6OQ.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 4. Color Histograms for Non Vehicle Image</figcaption></figure><p id="b4ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.空间宁滨- 彩色直方图确实很酷，但是如果图像的特征如此重要，那么为什么我们不能使用某种 numpy 函数来提取所有的特征呢？在这一点上你肯定是正确的。我们可以通过使用 numpy.ravel()展平图像来提取图像中的所有信息。但是等一下，让我们做一些计算，图像大小是 64x64，它是一个 3 通道图像，所以提取的特征总数是 12，288！！单个图像中接近 12k 的特征不是一个好主意！所以这里空间宁滨来图片！如果我说，一个 64x64 的图像提供的信息和 16x16 的图像提供的信息是一样的呢？当然有一些信息丢失，但我们仍然能够从图像中提取出好的特征！因此，如果我将 numpy.ravel()应用于 16x16 的图像，我将只能获得 768 个特征！</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ms"><img src="../Images/fd763f88935dd8dfa3ca02947ecacd6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IChYqFYFWmVvS4l4W_5F7Q.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 5. Spatial Binning Intuition</figcaption></figure><p id="80e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.<strong class="jp ir"> HOG(梯度方向直方图)——</strong>上面讨论的特征提取技术很酷，但肯定比不上 HOG。HOG 实际上拍摄了一幅图像，把它分成不同的块，在块中我们有细胞，在细胞中我们观察像素并从中提取特征向量。单元内的像素被分类到不同的方向，并且块内特定单元的结果向量由最强向量的幅度决定。注意——这里我们不计算某个像素在某个特定方向上的出现次数，而是对该像素在该特定方向上的大小感兴趣。阅读更多关于猪这是一个很好的<a class="ae kl" href="https://www.learnopencv.com/histogram-of-oriented-gradients/" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mt"><img src="../Images/8d48fc4cc9c13c72bf0df228134b6867.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RsyVJpi9g6_gz7C2olGwvA.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 6. HOG with 9 orientations. CPB(Cells Per Block) PPC (Pixels Per Cell)</figcaption></figure><p id="b848" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里只需要注意一点。OpenCV HOG 返回 HOG 图像和特征向量，但是 image.ravel()的长度不等于特征向量长度。这是因为 HOG 在内部执行一些计算，减少数据中的冗余，并返回优化的特征向量。此外，您在图像中看到的行数越多，意味着它将返回更多的特征。</p><h1 id="555e" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">生成数据集和数据预处理</h1><p id="74c4" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">好了，酷，现在我们知道如何提取特征，所以我们将处理所有图像的这些步骤？是的，你是对的，但是没有必要使用上述所有方法的所有功能。让我们暂时只使用 HOG，忽略颜色直方图和空间宁滨。</p><p id="7b4e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们决定提取特征的 HOG 参数。经过多次尝试，我决定采用以下方法:</p><ul class=""><li id="7ae0" class="mh mi iq jp b jq jr ju jv jy mj kc mk kg ml kk mu mn mo mp bi translated">方向- 9</li><li id="2872" class="mh mi iq jp b jq mv ju mw jy mx kc my kg mz kk mu mn mo mp bi translated">每块电池- 2</li><li id="b642" class="mh mi iq jp b jq mv ju mw jy mx kc my kg mz kk mu mn mo mp bi translated">每个单元格的像素- 16</li><li id="30d9" class="mh mi iq jp b jq mv ju mw jy mx kc my kg mz kk mu mn mo mp bi translated">色彩空间- YUV</li></ul><p id="bb53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">酷，用这些参数通过 HOG 函数运行图像后，最终的参数大小是 972，非常酷！</p><p id="fcb7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">数据预处理<br/> </strong>现在我们的特性已经准备好了，下一步是预处理数据。不要担心，sklearn library 总是在那里帮助完成这些任务。我们可以执行以下预处理:</p><p id="241e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">I)混洗数据<br/> ii)将数据集分成训练集和测试集<br/> iii)数据的标准化和缩放(数据集的拟合和变换)</p><p id="ab12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里要注意的非常重要的一点是，在步骤(ii)之后，我们必须拟合和转换数据，但是我们不应该拟合测试集中的数据，因为我们不希望我们的分类器将峰值偷偷带入我们的数据中。</p><h1 id="be83" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">训练分类器</h1><p id="8dd7" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">嗯，特征提取，数据预处理！接下来呢？是的，现在轮到我们的分类器了。你可以选择分类器，但是有很多分类器可供选择:</p><ul class=""><li id="9c1c" class="mh mi iq jp b jq jr ju jv jy mj kc mk kg ml kk mu mn mo mp bi translated">支持向量机</li><li id="26c0" class="mh mi iq jp b jq mv ju mw jy mx kc my kg mz kk mu mn mo mp bi translated">朴素贝叶斯</li><li id="138f" class="mh mi iq jp b jq mv ju mw jy mx kc my kg mz kk mu mn mo mp bi translated">决策图表</li></ul><p id="5131" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我决定用支持向量机，因为它们和 HOG 有很好的兼容性。现在在 SVM，我们有 SVC(支持向量分类器),在这里，我们也可以选择不同的核和不同的 C 和γ值。</p><p id="3998" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我在线性核和 rbf 核上训练我的分类器。线性核花费大约 1.8 秒来训练，测试精度为 98.7%，而 rbf 核花费大约 25 分钟来训练，测试精度为 98.3%。我决定使用带有默认参数的 LinearSVC，仅仅是因为它运行时间更短，而且比 rbf 内核更精确。</p><h1 id="2bc7" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">推拉窗</strong></h1><p id="aa25" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">酷我们的分类器现在训练有素，它将有 99%的时间能够正确预测车辆和非车辆。那么下一步是什么？好吧，下一步是将分类器应用到你的图像中，以便找到汽车在图像中的确切位置！</p><p id="3dff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是首先你需要决定各种重要的参数。第一件事是从哪里开始搜索汽车，显然你不应该在天空中搜索汽车，因此你可以忽略图像的上半部分，所以基本上确定一个地平线，在这个地平线下你将搜索你的汽车。第二件重要的事情是你要找的窗口大小是多少，两个窗口应该重叠多少？这取决于你的输入图像长度，因为这里是 64x64，所以我们将只从 64x64 的基本窗口大小开始。下一件重要的事情，也是这里要注意的非常重要的一点是<strong class="jp ir">，你在地平线附近搜索较小的汽车，当你走向仪表板摄像头时，你搜索较大的汽车</strong>。这是因为如果汽车靠近地平线，它们就越小，因为它们远离你的汽车，而靠近的汽车则相反。</p><p id="dccc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是我应该增加多少窗口大小和多少重叠？那要看你的眼光了。我决定使用 4 种不同尺寸的窗户。在下面的图片中，我将尝试用相应的窗口大小来说明我的搜索区域。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi na"><img src="../Images/880d22c4fe11cd415980f3b1d14f4c7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*4yTgxzgU2m_dprjNVlODEg.jpeg"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 7. My Choice of Window Sizes with Overlap and Y coordinates</figcaption></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/91dca4773540feb69c3f55e18e4c9ca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*ceIMDLL0Bwf5Ilyf4tAFRA.jpeg"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 8. Window Size 64x64 Coverage</figcaption></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/f0adad7f2d866dbc5b8b7b49287346d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*dVObtUCDcciK2mT9CKmdHA.jpeg"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 9. Window Size 80x80 Coverage</figcaption></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/6d192a44004c4ca60819b49038a03026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*YaK_vrgaR6G49xTb_1rc7g.jpeg"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 10. Window Size 96x96 Coverage</figcaption></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/ed49fd7e4f16ca481c791f06ad544d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*YM4xsH56WUD6064TLoIVbA.jpeg"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 11. Window Size 128x128 coverage</figcaption></figure><p id="231b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我的总窗口大小为 470！因此，一旦我们定义了我们将搜索的所有滑动窗口，下一步就是逐个窗口提取所有补丁的特征，并运行我们的分类器来预测找到的窗口是否是汽车。请记住，我们在从 64x64 图像提取的特征上训练我们的模型，因此对于大小不同的窗口，我们需要首先将它们的大小调整为 64x64，以保持特征相同。让我们看看我们的分类器是如何工作的。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/7cdc38cd1b5f286a07365d3aec8bf1fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*iqut53Zj7OzK4TeKnigFpQ.jpeg"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 12. Refined Windows after running the classifier.</figcaption></figure><h1 id="3c38" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">热图</h1><p id="1262" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">因此，我们能够检测滑动窗口，但有一个问题。这么多窗口相互重叠，如何画出最终的包围盒？答案是热图。我们将创建一个与原始图像大小相同的空白黑色图像，对于所有已识别的优化窗口，我们将为优化窗口的整个区域增加一个像素值。这样，我们将得到具有不同强度的区域，其中公共区域是最强的。然后，我们可以应用一个阈值来裁剪最终的图像，并获得最终盒子的坐标。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/1ab2b2518b4b819cdf9f01c9fd6f09ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*_LHNVY22lvpnqBYcjoqk9w.jpeg"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 13. Heatmap drawn after increasing the pixel intensities of refined windows</figcaption></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ng"><img src="../Images/31602a487f00b4ce4a76341b54a60a18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5NVPIjwExeFL33FndZN6ag.jpeg"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 14. Final Image After applying the threshold</figcaption></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nh"><img src="../Images/fa400e27495ec2c4f984eb78254de5d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vPiJXEnhQe1RCJd8Lbh_Sg.jpeg"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 15. Testing Pipeline On a New Image</figcaption></figure><p id="fe95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">好吧，太酷了，就这样？嗯，是也不是！当您在更多的测试图像上运行代码时，还有一个问题。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ni"><img src="../Images/5b676fc7b649d8c87d8699c450420eba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ri3XRzhNoYR0-qxDaOR7Og.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 16. Applying Heatmap on some more Images</figcaption></figure><p id="3337" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如你所观察到的，在我们的图像中检测到了一些误报，从左车道驶来的车辆也被检测到，那么我们如何解决这个问题呢？首先我们需要观察这个问题是如何出现的？我们的分类器有 98.7%的准确率。我们总共有 470 扇窗户。因此，在生成的窗口中，我们将有大约 6 个窗口是误报的。如果最终热图图像中的阈值较低，这些窗口可以出现在任何地方。因此，要解决汽车驶入另一条车道的问题，一些误报可以通过提高阈值来解决。在我的例子中，我将阈值设置为值 4，但是阈值设置还是取决于许多因素，使用的色彩空间，SVM 精度等等。</p><h1 id="3698" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">求平均值</h1><p id="ff1c" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">我们现在差不多完成了！管道处理图像的效果非常好，但是如果你要对来自视频流的图像运行管道，还是有一个问题。最终检测到的框将变得非常不稳定，无法提供流畅的体验，在某些帧中框可能会消失。那么解决办法是什么呢？解决方案非常直观，存储所有从前 15 帧检测到的细化窗口，并平均当前帧中的矩形。此外，你需要调整阈值到一个更高的水平。通过这样做，最终的边界框看起来不那么抖动，并提供一个平滑的流程。我在项目视频上尝试了我的管道，结果有点像<a class="ae kl" href="https://github.com/harveenchadha/Udacity-CarND-Vehicle-Detection-and-Tracking/blob/master/project_video_output.mp4" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><h1 id="2fa7" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">一些提示和技巧</h1><ol class=""><li id="2225" class="mh mi iq jp b jq ls ju lt jy nj kc nk kg nl kk mm mn mo mp bi translated">如果您在视频上运行管道时处理时间过长，请先尝试缩小窗口。</li><li id="68af" class="mh mi iq jp b jq mv ju mw jy mx kc my kg mz kk mm mn mo mp bi translated">如果处理时间仍然很长，请尝试减少提取的要素数量。</li><li id="fa1a" class="mh mi iq jp b jq mv ju mw jy mx kc my kg mz kk mm mn mo mp bi translated">如果你有一个包含 10，000 张图片的测试集，你的特征大小是 8000，即使测试集的准确率超过 95%，SVM 也不会达到标准。使用具有“rbf”内核或 reduce 特征的 SVM。</li><li id="18e7" class="mh mi iq jp b jq mv ju mw jy mx kc my kg mz kk mm mn mo mp bi translated">如果管道运行仍然需要很长时间，请尝试跳过一半或三分之二的帧。它会加速。记住，我所说的跳过帧是指跳过对帧的处理，并将该帧的细化窗口设置为从存储在某个数据结构中的前 15 帧中收集的矩形。</li></ol><blockquote class="nm"><p id="223a" class="nn no iq bd np nq nr ns nt nu nv kk dk translated">你可以在这里观察最终的视频输出<a class="ae kl" href="https://github.com/harveenchadha/Udacity-CarND-Vehicle-Detection-and-Tracking/blob/master/project_video_output.mp4" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote><p id="ccb4" class="pw-post-body-paragraph jn jo iq jp b jq nw js jt ju nx jw jx jy ny ka kb kc nz ke kf kg oa ki kj kk ij bi translated">当然，视频并不完美，但我很高兴最终的输出。有较少的滞后，也没有检测到来自其他方向的汽车。还有。我正在研究 YOLO 和 SSD 方法，并将很快写下从中得到的经验。万岁！！第一学期结束了，是时候自我反省了，也为第二学期感到非常兴奋。</p></div></div>    
</body>
</html>