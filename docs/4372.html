<html>
<head>
<title>Perceptron: The Artificial Neuron (An Essential Upgrade To The McCulloch-Pitts Neuron)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">感知器:人工神经元(麦卡洛克-皮茨神经元的本质升级)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d?source=collection_archive---------0-----------------------#2018-08-12">https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d?source=collection_archive---------0-----------------------#2018-08-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="d427" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">深度神经网络的最基本单元被称为<em class="kl">人工神经元</em>，它接受输入，对其进行处理，将其传递给类似<a class="ae km" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> Sigmoid </a>的激活函数，并返回激活的输出。在这篇文章中，我们将只讨论在“激活”部分出现之前提出的<em class="kl">感知机</em>模型。</p><p id="7a02" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">美国心理学家弗兰克·罗森布拉特(Frank Rosenblatt)在 1958 年提出了<em class="kl">经典感知机</em>模型。由<a class="ae km" href="http://science.sciencemag.org/content/165/3895/780" rel="noopener ugc nofollow" target="_blank">明斯基和 Papert </a> (1969)进一步提炼和仔细分析——他们的模型被称为<em class="kl">感知器</em>模型。这是我上一篇关于<a class="ae km" rel="noopener" target="_blank" href="/mcculloch-pitts-model-5fdf65ac5dd1">麦卡洛克-皮茨神经元</a>的文章的后续，我建议你至少快速浏览一下，以便更好地欣赏明斯基-帕佩特的贡献。</p><p id="b414" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">引用注:本文的概念、内容和结构灵感来自于</em> <a class="ae km" href="https://www.cse.iitm.ac.in/~miteshk/" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> <em class="kl">米特什·m·卡普拉</em> </strong> </a> <em class="kl">教授关于</em><a class="ae km" href="http://nptel.ac.in" rel="noopener ugc nofollow" target="_blank"><em class="kl">NPTEL</em></a><em class="kl">的</em> <a class="ae km" href="https://onlinecourses.nptel.ac.in/noc18_cs41/preview" rel="noopener ugc nofollow" target="_blank"> <em class="kl">深度学习</em> </a> <em class="kl">课程的精彩讲座和材料。看看吧！</em></p><h1 id="3489" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">感知器</h1><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/bd3d854bf4d9a96eaba8d36180168953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*-JtN9TWuoZMz7z9QKbT85A.png"/></div></figure><p id="257d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Minsky-Papert 提出的<em class="kl">感知器</em>模型是一个比麦卡洛克-皮茨神经元更通用的计算模型。它通过引入输入的数字权重(重要性的度量)的概念和学习这些权重的机制，克服了 M-P 神经元的一些限制。输入不再像 M-P 神经元那样局限于布尔值，它还支持实输入，这使它更加有用和通用。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lt"><img src="../Images/6ffe2c19f204f83cb767347c8c6ac730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fyapb-JRFJ-VtnLYLLXCwg.png"/></div></div></figure><p id="fc86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，这非常类似于 M-P 神经元，但是我们取输入的加权和，并且仅当和大于任意阈值(<strong class="jp ir"> <em class="kl">、θ</em></strong>)时将输出设置为 1。然而，根据惯例，我们没有手动编码阈值参数<strong class="jp ir"> <em class="kl"> thetha </em> </strong>，而是将其添加为输入之一，权重为- <strong class="jp ir"> <em class="kl"> theta </em> </strong>，如下所示，这使得它可以学习(在我的下一篇文章中会详细介绍这一点— <em class="kl">感知器学习算法</em>)。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lt"><img src="../Images/79d9dd18774e77dee8646185ffa01ed7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gKFs7YU44vJFiS2rF3-bpg.png"/></div></div></figure><p id="b7ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑使用可用的行为数据预测我是否会在电视上观看随机的足球比赛的任务(与我的 M-P neuron 帖子中的例子相同)。让我们假设我的决定完全依赖于 3 个二进制输入(为简单起见是二进制)。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ly"><img src="../Images/b5dbfc9ed6ef2ad01f256a3d23906e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GJp3269Q0pY1Lg54DHbteQ.png"/></div></div></figure><p id="5f2b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，<strong class="jp ir"> <em class="kl"> w_0 </em> </strong>之所以被称为偏见，是因为它代表了先验(preference)。一个足球狂可能有一个非常低的门槛，可以观看任何足球比赛，而不考虑联赛、俱乐部或比赛的重要性[ <strong class="jp ir"> <em class="kl"> theta = 0 </em> </strong> ]。另一方面，像我这样的选择性观众可能只观看英超联赛的足球比赛，以曼联比赛为特色，并且不友好。重点是，<strong class="jp ir">权重</strong>和<strong class="jp ir">偏差</strong>将取决于数据(在这种情况下是我的观看历史)。</p><p id="22f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">根据数据，如果需要，模型可能必须赋予<em class="kl"> isManUnitedPlaying </em>输入很大的重要性(高权重),并惩罚其他输入的权重。</p><h1 id="13b2" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">感知器 vs 麦卡洛克-皮茨神经元</h1><p id="6e18" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">使用<em class="kl">感知器</em>可以实现什么样的功能？它与麦卡洛克-皮茨神经元有多大不同？</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi me"><img src="../Images/b4e0cab19789f1c9777c78fcb5383c2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SzzZhOB-xHyOchebT9Fv7w.png"/></div></div></figure><p id="f87c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从等式中可以清楚地看出，即使是一个<em class="kl">感知器</em>也会将输入空间分成两半，正的和负的。产生输出 1 的所有输入位于一侧(正半空间)，产生输出 0 的所有输入位于另一侧(负半空间)。</p><p id="836f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">换句话说，单个<em class="kl">感知器</em>只能用来实现<strong class="jp ir">线性可分</strong>功能，就像 M-P 神经元一样。那有什么区别呢？为什么我们声称<em class="kl">感知器</em>是 M-P 神经元的升级版？这里，包括阈值的权重可以是学习的<strong class="jp ir"/>和输入可以是真实的<strong class="jp ir">值。</strong></p><h1 id="4bbb" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">使用感知器的布尔函数</h1><h2 id="1a83" class="mf ko iq bd kp mg mh dn kt mi mj dp kx jy mk ml lb kc mm mn lf kg mo mp lj mq bi translated">还是功能——能做！</h2><p id="db84" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">只是重温过去的美好时光，或者用感知机的方式发挥作用。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mr"><img src="../Images/f978282f56c44bb5457e434570b16a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5LeL8JDfoGbkUg0cu1M-w.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Try solving the equations on your own.</figcaption></figure><p id="1719" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的‘可能解’是通过解左边的线性方程组得到的。很明显，解决方案将输入空间分成两个空间，负半空间和正半空间。我鼓励您尝试使用 AND 和其他布尔函数。</p><p id="40f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，如果你真的试着解上面的线性方程，你会发现可能有多个解。但是哪种解决方案是最好的呢？为了更正式地定义“最佳”解决方案，我们需要理解错误和错误表面，这将在我的下一篇关于<em class="kl">感知器学习算法的文章中进行。</em></p><h2 id="971f" class="mf ko iq bd kp mg mh dn kt mi mj dp kx jy mk ml lb kc mm mn lf kg mo mp lj mq bi translated">XOR 函数—做不到！</h2><p id="7725" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">现在让我们来看一个非线性布尔函数，也就是说，你不能画一条线来区分正输入和负输入。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mw"><img src="../Images/77653a87d3574db96fb7f752a372deaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E7YhKsJ2wb-VdeXpg89lvg.png"/></div></div></figure><p id="0023" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，第四个等式与第二个和第三个等式相矛盾。关键是，对于非线性分离的数据，没有<em class="kl">感知器</em>解决方案。因此，关键是一个<strong class="jp ir">单个</strong> <em class="kl">感知器</em>无法学会分离本质上是非线性的数据。</p><blockquote class="mx my mz"><p id="04d8" class="jn jo kl jp b jq jr js jt ju jv jw jx na jz ka kb nb kd ke kf nc kh ki kj kk ij bi translated"><strong class="jp ir">异或事件</strong></p><p id="80cf" class="jn jo kl jp b jq jr js jt ju jv jw jx na jz ka kb nb kd ke kf nc kh ki kj kk ij bi translated">在 Minsky 和 Papert 于 1969 年出版的书中，作者暗示，由于单个人工神经元无法实现某些功能，如逻辑功能的 XOR，因此更大的网络也有类似的局限性，因此应该放弃。后来对三层感知器的研究显示了如何实现这样的功能，从而避免了这项技术被抹杀。</p><p id="61d1" class="jn jo kl jp b jq jr js jt ju jv jw jx na jz ka kb nb kd ke kf nc kh ki kj kk ij bi translated">— <em class="iq">维基百科</em></p></blockquote><h1 id="9fb7" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">(可选)乙状结肠神经元的动机</h1><p id="f0d6" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">正如我前面提到的，我们今天使用的人工神经元与我们看到的<em class="kl">感知器</em>略有不同，区别在于激活功能。给你。有些人可能会说一个<em class="kl">感知器</em>使用的阈值逻辑非常苛刻。例如，如果你看一个决定我是否要看电影的问题，只基于一个实值输入(<strong class="jp ir"> <em class="kl"> x_1 </em> </strong> = <em class="kl">评论</em>)，如果我们设置的阈值是 0.5(<strong class="jp ir"><em class="kl">w _ 0</em></strong>=-0.5)和<strong class="jp ir"><em class="kl">【w _ 1</em></strong>= 1，那么我们的设置将是这样的:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/81f43d0ad62225f1e2ef17d4778f0718.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*oPtJQSy2JkluyWgU6f55MQ.png"/></div></figure><p id="9285" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于一部<em class="kl"/>= 0.51 的电影，会有什么样的决定？<em class="kl">是的！<br/> </em>对于一部<em class="kl"/>= 0.49 的电影，会有什么决定？<em class="kl">不！有些人可能会说，我们会看一部评分为 0.51 的电影，而不是一部评分为 0.49 的电影，这太苛刻了，这就是西格蒙德出现的原因。现在说服你自己，这个苛刻的阈值并不仅仅归因于我们在这里选择的一个特定问题，它可能发生在我们处理的任何或每一个问题上。这是<em class="kl">感知器</em>功能本身的一个特征，其行为类似于阶跃函数。</em></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ne"><img src="../Images/b5d0b5400aec1abbe74437d03e4d8a41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-qZhObF2Hp0Av68lDojaVA.png"/></div></div></figure><p id="a062" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">z 值越过阈值(- <strong class="jp ir"> <em class="kl"> w_0 </em> </strong>)时会有这种决策的突变(从 0 到 1)。对于大多数真实世界的应用，我们期望一个从 0 到 1 逐渐变化的更平滑的决策函数。</p><p id="2849" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">引入输出函数比阶跃函数平滑得多的 sigmoid 神经元似乎是一件合乎逻辑且显而易见的事情。请注意，sigmoid 函数是一个具有典型“S”形曲线的数学函数，也称为<strong class="jp ir"> sigmoid </strong>曲线。有许多功能可以帮您完成这项工作，下面列出了一些:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/226c488f4f759475cc42a65742e8f49b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XPjkd29mA6P9lexeDI6g0Q.png"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">- <a class="ae km" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">Wikipedia</a></figcaption></figure><p id="0dd0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最简单的一个例子是逻辑函数。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ng"><img src="../Images/419687c94c14d3fb159e3fa54b603e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QLtPF0YkwYRLVoSeh2w4gw.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Quick Question: What happens to <strong class="bd nh">y</strong> when <strong class="bd nh">z </strong>is infinite? Or when it is -infinite?</figcaption></figure><p id="7afa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们不再看到围绕<strong class="jp ir"> <em class="kl"> w_0 </em> </strong>的急剧转变。此外，输出不再是二进制的，而是介于 0 和 1 之间的真实值，可以解释为概率。因此，我们得到的不是“是/否”的决定，而是“是”的概率。这里的输出是<strong class="jp ir">平滑</strong>、<strong class="jp ir">连续</strong>和<strong class="jp ir">可微</strong>，这是任何学习算法都喜欢的。要自己验证这一点，请翻翻深度学习中的<strong class="jp ir">反向传播</strong>概念。</p><h1 id="1086" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">结论</h1><p id="b6b3" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">在这篇文章中，我们看了一个<em class="kl">感知器</em>，深度神经网络的基本单元。我们还通过例子展示了<em class="kl">感知器</em>与麦卡洛克-皮茨神经元相比，如何更加一般化，并克服了当时的一些相关限制。我们也简单地建立了乙状结肠神经元的动机。</p><p id="209d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我的下一篇文章中，我们将仔细观察著名的<a class="ae km" rel="noopener" target="_blank" href="/d5db0deab975"> <em class="kl">感知器学习算法</em> </a>，并尝试获得它为什么工作的直觉，而不进入任何复杂的证明，以及从头开始用 Python 实现该算法。</p><p id="c8b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢您阅读文章。自己活也让别人活！<br/>答</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ni"><img src="../Images/f020f3f5a6f5130ab5f78dad69c0309f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GF6ozztlIUgBB7wY8OtxLA.jpeg"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Photo by <a class="ae km" href="https://unsplash.com/photos/BW0vK-FA3eg?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Clint Adair</a> on <a class="ae km" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div>    
</body>
</html>