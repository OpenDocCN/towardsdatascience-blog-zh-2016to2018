<html>
<head>
<title>Understanding basic machine learning with Python — Perceptrons and Artificial Neurons</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 理解基本的机器学习——感知器和人工神经元</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700?source=collection_archive---------4-----------------------#2018-07-07">https://towardsdatascience.com/understanding-basic-machine-learning-with-python-perceptrons-and-artificial-neurons-dfae8fe61700?source=collection_archive---------4-----------------------#2018-07-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9143" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">基本功能的完整代码演练</h2></div><p id="e5b5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你正在学习机器学习，很可能你的目标是应用前沿算法和“深度学习”来创建强大的应用程序。随着直观的 API 的广泛使用，这是可以实现的，只需要对正在发生的事情或深层底层实际上是如何工作的理解很少。</p><p id="79ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，在做这件事之前，浅显的架构基础知识和对底层的洞察是有益的。如果没有这一点，构建偏离盲目跟随教程的模型将变得困难(猴子见猴子做综合症)。</p><p id="0f77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们将讨论人工神经元和分类的起点，包括感知器和 Adaline 模型。对于每个模型，我们将讨论主要组件，然后将它们组合成完整的 Python 类实现。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/93763dd39c243868e8ad6a875b0214ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qqszxCf2kyCRGsyVqby3Tw.jpeg"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Image 1.</strong> Neuron Cell, taken from Pixabay.</figcaption></figure><h1 id="272e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">感知器模型</h1><p id="d845" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">机器学习最根本的起点是<strong class="kh ir">人工神经元</strong>。第一个简化的脑细胞模型发表于 1943 年，被称为<strong class="kh ir">麦卡洛克-皮茨(MCP) </strong>神经元。本质上，这是一个具有二进制输出(“0”或“1”)的基本逻辑门。如果到达神经元的输入的总和超过给定的阈值，则产生“1 ”,其中每个输入乘以相应的权重系数以产生这个总和。因此，神经元的放电很大程度上取决于输入集和相应的权重系数值。</p><p id="9beb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">弗兰克·罗森布拉特在 MCP 神经元的基本概念发表后不久就采用了这一概念，并提出了<strong class="kh ir">感知器规则算法。</strong>该<strong class="kh ir"> </strong>允许自动学习神经元模型的最佳权重系数。通过这样的模型，我们可以预测给定的输入是属于一个类还是另一个类(二元分类)。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mp"><img src="../Images/575fef554d01af49b90fdee6d5888b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C787SPoo6-TvZVk1wUUbBw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 1.</strong> Simplified Perceptron Model. Diagram by Author.</figcaption></figure><p id="4d53" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感知器模型可能有任意数量的输入，也称为<strong class="kh ir">特征</strong>。因为我们通常有多个特征，所以用向量和矩阵来表示我们的输入和权重是有帮助的。不要混淆输入样本和输入特征；每个数据样本都有许多特征，<em class="mq"> n，</em>，这些特征通过我们的模型输入，一次处理一个(按顺序)。例如，如果我们有 100 个汽车数据样本，其中每个样本都有<em class="mq"> m </em>个特征(代表最高速度、bhp、成本等)。)，那么数据的每个样本 x 将具有一个<em class="mq"> m 个</em>特征的向量。每个输入特征都有一个相应的权重，经过训练后，权重将根据我们训练的数据进行优化。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/151eecd126ddaaa0df98985a9eddcd2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*67kl4E-nKrkanzAyS2oQ5w.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 2.</strong> Vectors for our input features and weights.</figcaption></figure><p id="158f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于每个数据样本，我们的输入特征和权重作为一个<strong class="kh ir">加权和</strong>在感知器中被接受，这就是通常所说的<strong class="kh ir">网络输入函数，<em class="mq"> z </em> </strong> <em class="mq">。</em>对上面的<em class="mq"> x </em>和<em class="mq"> w </em>使用向量的好处是，我们可以使用线性代数来简洁地表示它，就像这样:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/aae0d1fa4fa9f04b3eb4ba336f23fd6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*eFUZAsFlG1OC5aUQBGeU1Q.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 3. </strong>Perceptron Net Input Function.</figcaption></figure><p id="3559" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们转置权重向量<strong class="kh ir"> w </strong>，然后与我们的输入向量<strong class="kh ir">x</strong>相乘，这形成了我们需要的和。<strong class="kh ir"> </strong>还应注意，我们还使用了一个<strong class="kh ir">偏差项</strong>来允许我们的感知器模型中有一个阈值，它等于我们向量中的第一个权重项。为了方便起见，输入矩阵中的第一个 x 项总是设置为 1。这意味着我们需要在输入向量中加入偏差项，然后再将它放入模型中。</p><p id="c8c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你对这种数学符号完全陌生，我强烈推荐一些线性代数入门。几乎你在进一步的机器学习中所涉及的所有内容都将大量基于线性代数。</p><p id="25fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在净输入求和之后，我们的值被传递给感知器阈值，或<strong class="kh ir">决策函数</strong>。在这种情况下，该功能是一个<strong class="kh ir">单位步进功能</strong>。如果输入高于给定值，则输出“1”，否则输出“0”。</p><p id="1d23" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么我们的感知机实际上是如何从一组数据中学习的呢？整个感知器过程如下:</p><ul class=""><li id="76a7" class="mt mu iq kh b ki kj kl km ko mv ks mw kw mx la my mz na nb bi translated">将重量参数初始化为初始值(通常是小的随机值，而不是零)。</li><li id="848c" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la my mz na nb bi translated">从每个训练样本输入的给定特征预测二进制输出。</li><li id="0a2d" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la my mz na nb bi translated">通过<strong class="kh ir">感知器学习规则更新权重。</strong></li></ul><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nh"><img src="../Images/516ae9c64a09286b3cb5711ec336a59a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BoPvH_toJiDr7OcK8bKWXQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 4.</strong> Model for the Perceptron, including weight optimisation using the error. Diagram by Author.</figcaption></figure><p id="6ef2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感知器的成本函数非常简单，被称为<strong class="kh ir">感知器学习规则。</strong>每次训练迭代后，我们的权重向量中的每个权重都会有一个小的变化，如下所示:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ni"><img src="../Images/773ff54d78a56897925897d7d3c7a390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MM1h5hYr4SCSLaJC5hPChQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 5.</strong> Perceptron Learning Rule for updating of weights.</figcaption></figure><p id="91e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">:=符号意味着我们同时更新权重向量中的所有权重，而不是迭代地更新。因此，进行输出预测，然后更新所有权重参数，作为一个训练周期的一部分。上面的 alpha 项也等于<strong class="kh ir">学习率</strong>，它决定了我们的模型在每个训练周期中学习的速度。</p><p id="2655" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们用于训练的数据是线性可分的，如果经过足够的训练周期(时期)，感知器会将权重更新为最佳值。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="c5cb" class="ls lt iq bd lu lv nq lx ly lz nr mb mc jw ns jx me jz nt ka mg kc nu kd mi mj bi translated">Python 中的感知器</h1><p id="ab4b" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">我们将从用 Python 编写感知器的每个组件开始，然后在最后将它们组合成一个更大的感知器类。对于这个基本模型，我们唯一需要的 Python 包是<strong class="kh ir"> numpy </strong>和<strong class="kh ir"> matplotlib </strong>。</p><h2 id="a827" class="nv lt iq bd lu nw nx dn ly ny nz dp mc ko oa ob me ks oc od mg kw oe of mi og bi translated">抽样资料</h2><p id="2456" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">为此，我们将生成一个简单的数据集，其中包含两个不同的输出类，供我们的感知器进行训练。<strong class="kh ir">注意</strong>:该随机训练数据是通过随机函数生成的，并不完全可复制。</p><p id="2ca3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了帮助理解我们正在努力做的事情，<strong class="kh ir">可视化</strong>我们的数据是有帮助的。在这种情况下，我们的输入样本只有两个特征，因此使用散点图来可视化我们的数据很容易。下面显示了数据生成，然后绘制成散点图。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="oh oi l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="ak">Figure 6.</strong> Sample code for generating bird data and plotting on a scatter graph.</figcaption></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/ca1e373241dd982a6ef4a4ff5dcc85f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*b5hqMtlaTXxEu3Bsz57UTg.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 7.</strong> Visualisation of our generated data.</figcaption></figure><h2 id="556a" class="nv lt iq bd lu nw nx dn ly ny nz dp mc ko oa ob me ks oc od mg kw oe of mi og bi translated">初始权重的生成</h2><p id="c4c1" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">我们希望将我们的权重初始化为小的随机数字，而不是将它们初始化为零。我们需要这样做，否则我们的学习率将会降低训练过程中对分类结果的影响。这背后的理由将不被涵盖，然而，这是因为学习率只影响权重向量的规模，而不是方向，如果权重初始化为零。</p><p id="5a22" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些权重可以以不同的方式生成。使用 numpy.random 的一个例子是:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ok"><img src="../Images/57431e747e0ffa975d8fd25a273c1403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2tCDrfv9THnCQD5buMZ3YA.png"/></div></div></figure><h2 id="8488" class="nv lt iq bd lu nw nx dn ly ny nz dp mc ko oa ob me ks oc od mg kw oe of mi og bi translated"><strong class="ak">向 X 添加偏差项</strong></h2><p id="edaa" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">对于偏置项，我们只需在输入数据 x 中添加一列 1，在 numpy 中，这可以通过使用<em class="mq">one</em>和<em class="mq"> hstack </em> numpy 函数来实现。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ol"><img src="../Images/bc31530ba69374b2d8f1c57e97b04b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hbqRf4fy1j4XFVdbowuhXg.png"/></div></div></figure><h2 id="8e3d" class="nv lt iq bd lu nw nx dn ly ny nz dp mc ko oa ob me ks oc od mg kw oe of mi og bi translated"><strong class="ak">加权求和函数(净输入)</strong></h2><p id="87c8" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">如上所述，由于我们对每个样本的输入特征及其相关权重使用向量，因此我们可以非常容易地计算净输入和。在 Python 中使用带有 numpy 的矢量化，根据我们是否向训练数据添加了 1 的偏差列，网络输入函数变为:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi om"><img src="../Images/d18356cfde86cdd719cd448131480b2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wcz58ktSEzlYPzeJf8TW_Q.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 8.</strong> Implementing the net input sum in numpy.</figcaption></figure><h2 id="499d" class="nv lt iq bd lu nw nx dn ly ny nz dp mc ko oa ob me ks oc od mg kw oe of mi og bi translated"><strong class="ak">预测用单位阶跃函数</strong></h2><p id="7101" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">如果我们的净输入总和大于或等于零，我们希望返回 1，否则返回 0。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi on"><img src="../Images/bf821e5dcd7c4ca6f8740a1190ed14f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LjixITkh5hkBOJEs1dumdg.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 9.</strong> Implementing the step input function in numpy.</figcaption></figure><h2 id="0a6f" class="nv lt iq bd lu nw nx dn ly ny nz dp mc ko oa ob me ks oc od mg kw oe of mi og bi translated"><strong class="ak">感知器学习规则</strong></h2><p id="7c21" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">使用之前在图 6 中定义的学习规则，我们可以执行一个完整的训练周期。这包括遍历所有训练样本，并根据感知器学习规则更新权重，如下所示:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oo"><img src="../Images/b05f5161df187beca12003470444d6b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o-hH7chYLqmfxUfXLXHMFA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 10. </strong>Perceptron training cycle implementation.</figcaption></figure><h2 id="eeab" class="nv lt iq bd lu nw nx dn ly ny nz dp mc ko oa ob me ks oc od mg kw oe of mi og bi translated">把所有的放在一起</h2><p id="1bb5" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">有了使用 numpy 形成的感知器的基本组件，我们可以将它们拼凑在一起，实现一个完整的感知器二进制分类器。使用分类器，我们可以在训练周期中可视化我们的错误，并且(希望如此！)随着权重的优化，错误数量会随着时间的推移而减少。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="oh oi l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="ak">Figure 11.</strong> Code for a Perceptron class in Python for binary classification.</figcaption></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi op"><img src="../Images/fe594740703cdcf6458f0134d4d2dd9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kFCQ6G5hZQ64WorFITqmsA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 12. </strong>Visualisation of the errors and decision boundaries made by our Perceptron classifier.</figcaption></figure><p id="9d21" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从误差与训练周期(时期)的关系图中可以看出，我们得到的误差数量随着时间的推移而减少。大约 7 个周期后，我们的模型将错误数量减少到零。我们的感知器分类器训练后做出的决策边界如右图所示，有效地区分了金雕和角枭。我们可以通过使用一组新输入 x 调用分类器预测函数，使用我们训练的模型对新数据进行预测。</p><p id="deb5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感知器的一个问题是，如果所提供的数据是完全线性可分的，它们只会正确地优化权重。对于上面使用的数据，它是线性可分的。然而，如果我们使用更现实的数据，这些数据通常不能用直线分开，那么我们的感知器分类器将永远不会停止优化权重，并将不断出现错误。这是一个巨大的缺点，也是不使用感知机的原因之一。然而，它们确实是对基本神经元和更复杂模型的良好介绍。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="e057" class="ls lt iq bd lu lv nq lx ly lz nr mb mc jw ns jx me jz nt ka mg kc nu kd mi mj bi translated">Adaline——自适应线性神经元</h1><p id="38bd" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">对原始感知器模型的改进是 Adaline，它增加了一个用于优化权重的线性激活函数。通过这种添加，使用连续成本函数而不是单位步长。Adaline 很重要，因为它为更高级的机器学习模型奠定了基础。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oq"><img src="../Images/4ae6935b3174fe0ade889b8ea6ebe10d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1deCZXuJjBpVpm0vCuLfMg.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 13.</strong> Model for Adaline, which includes the additional activation function. Diagram by Author.</figcaption></figure><p id="1439" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如您在上面看到的，单位步骤仍然存在，但是它仅用于模型末尾的输出分类(“1”或“0”)。线性激活模型的连续输出值用于<strong class="kh ir">成本(或目标)函数</strong>。这表明我们的算法在我们的训练数据上做得有多差(或多好)，并且可以通过众所周知的算法(如梯度下降)将其最小化。</p><h2 id="43e4" class="nv lt iq bd lu nw nx dn ly ny nz dp mc ko oa ob me ks oc od mg kw oe of mi og bi translated">价值函数</h2><p id="79b6" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在这种情况下，我们将使用<strong class="kh ir">误差平方和</strong>作为我们的成本函数，可以这样实现:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi or"><img src="../Images/3adc420f1b4ab0f61e3cae8622b4ded8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*90yN7feNSTCjNuC6x1oujw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 14. </strong>Sum of Squared Errors implementation in Python.</figcaption></figure><h2 id="e03a" class="nv lt iq bd lu nw nx dn ly ny nz dp mc ko oa ob me ks oc od mg kw oe of mi og bi translated">梯度下降</h2><p id="60c6" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">为了应用<strong class="kh ir">梯度下降</strong>，我们需要确定成本函数相对于每个权重的偏导数。利用微分，我们发现这个偏导数等于:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi os"><img src="../Images/f544e59b4873c860b4f4693a1819271c.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*HxlPW9aoYzsyLyFE7kzlvQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 15.</strong> Partial derivative of our cost function with respect to each weight.</figcaption></figure><p id="cb2f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了优化我们的权重，我们希望以我们的学习速率参数所选择的速率，在梯度的相反方向上迈出一步。这背后的逻辑是最小化成本并最终达到全局最小值。因此，对于每次重量优化，我们需要应用:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ot"><img src="../Images/51f8b8862134cd01b3458ef8bbffbedb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bOyxRYJ8wMnZVVvSddImkQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 16. </strong>Optimisation updates for our weights in the Adaline model.</figcaption></figure><p id="9038" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以将权重优化步骤和成本计算步骤合并到一个训练周期代码片段中，如下所示:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ou"><img src="../Images/b84618c517b99609fbff35e8ae06dbe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DCIhlLCRFMVp5trhcnPaKw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 17.</strong> One training cycle for our Adaline model.</figcaption></figure><h2 id="c378" class="nv lt iq bd lu nw nx dn ly ny nz dp mc ko oa ob me ks oc od mg kw oe of mi og bi translated">特征标准化</h2><p id="4e23" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">除了激活功能和梯度下降优化，我们还将通过 F <strong class="kh ir">功能标准化</strong>增强我们模型的性能。这是机器学习中的一种常见做法，对于给定数量的时期，这种做法使优化过程更快、更有效。为此，我们将从训练数据中的每个相关特征值中减去每个特征的平均值，然后将特征值除以它们各自的标准偏差。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ov"><img src="../Images/0bc7d5ecd19247dae486b154eef68b88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e4RKeNomJGLDDhBi7DNEJQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 18.</strong> Feature standardisation in Python.</figcaption></figure><h2 id="4533" class="nv lt iq bd lu nw nx dn ly ny nz dp mc ko oa ob me ks oc od mg kw oe of mi og bi translated">把所有的放在一起</h2><p id="2998" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">有了 Adaline 模型的附加组件，我们就可以用 Python 生成一个最终的实现，就像我们用感知器一样。这个 Adaline 实现如下所示:</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="oh oi l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="ak">Figure 19.</strong> Adaline Classifier implemented in Python.</figcaption></figure><p id="1db6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从下图中的基本 Adaline 示例中，我们可以看到为我们的模型设置超参数是多么重要；如果我们选择 0.1 的学习率，成本趋于无穷大并且不收敛，而如果我们选择 0.001 的学习率，我们的模型成功地收敛到成本函数的全局最小值。迭代的次数也很重要，应该足够高以充分降低成本。机器学习模型的这些可调方面被称为<strong class="kh ir">超参数</strong>，这些优化对于我们创建的任何模型都是一个重要方面。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ow"><img src="../Images/00a341321f3a394e9f9ac40c78815db5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YowLR3CC8SvdDh2zGuvHBw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><strong class="bd lr">Figure 20.</strong> Comparison of different choices of learning rate for our Adaline classifier.</figcaption></figure><h2 id="03fc" class="nv lt iq bd lu nw nx dn ly ny nz dp mc ko oa ob me ks oc od mg kw oe of mi og bi translated">进一步改进和结束语</h2><p id="ada1" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">有很多方法可以进一步改进这个模型，包括随机梯度下降，它可以更频繁地优化，而不是像上述模型那样使用整批训练数据。我们还可以将它扩展到不仅仅是二元分类，使用一元对多元分类。</p><p id="342e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，还有比 Adaline 好得多的分类器可用。Adaline 是一个起点，可以用作理解更复杂的分类模型(如逻辑回归和神经网络)的基础。</p><p id="0d52" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们研究了基本神经元的特性，然后使用 Python 和 numpy 构建了一个基本的 Rosenblatt 感知器模型。然后，我们构建了一个工作 Adaline 分类器，它通过激活函数和优化成本函数，在许多方面改进了感知器。在此期间，我们还应用了一些有价值的特征处理，包括标准化以使梯度下降更快更有效。</p><p id="35df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然这些模型不能替代通过 Sci-kit Learn 和其他软件包获得的专业构建和优化的模型，但它们应该能够让人们了解基本分类器的工作原理。我希望我们遵循的过程有助于您理解基本的机器学习分类，并为您提供使用 Python 实现这些模型的洞察力。</p></div></div>    
</body>
</html>