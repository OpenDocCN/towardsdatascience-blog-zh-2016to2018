<html>
<head>
<title>Double Q-Learning, the Easy Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">双 Q 学习，最简单的方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/double-q-learning-the-easy-way-a924c4085ec3?source=collection_archive---------6-----------------------#2018-12-06">https://towardsdatascience.com/double-q-learning-the-easy-way-a924c4085ec3?source=collection_archive---------6-----------------------#2018-12-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2017" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解双 Q 学习的介绍</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/326bc8e521a3026befa43c54fb4b4557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ByrK6MDCI5KuhCJj-Pugfw.jpeg"/></div></div></figure><p id="036c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">更新</strong>:学习和练习强化学习的最好方式是去 http://rl-lab.com<a class="ae lq" href="http://rl-lab.com" rel="noopener ugc nofollow" target="_blank"/></p><p id="fc82" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Q-learning (Watkins，1989)被认为是 TD 控制强化学习算法的突破之一。</p><p id="b973" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然而，在他的论文<a class="ae lq" href="https://papers.nips.cc/paper/3964-double-q-learning.pdf" rel="noopener ugc nofollow" target="_blank">双 Q 学习</a>中，哈多·范·哈瑟尔特解释了 Q 学习在一些随机环境中表现不佳的原因。他指出，性能不佳是由于在 Q-learning 中使用 Max Q(s '，a)而导致的动作值被大大高估。</p><p id="34ca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了解决这个问题，他提出了双 Q 学习法。</p><h1 id="5768" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">问题是</h1><p id="bec2" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">考虑一个有四个状态的 MDP，其中两个是终态。<br/>状态 A 总是被认为是开始状态，并且有两个动作，或者右或者左。右边的动作给出零奖励，并到达终端状态 c。<br/>左边的动作将代理移动到状态 B，奖励为零。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/55573f0f5a66e4d107919b872f0a8834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HmGJAGiZG8coo-B4Q7m7-g.png"/></div></div></figure><p id="f6d9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">状态 B 有许多动作，它们将代理移动到终端状态 D。但是(这很重要)从 B 到 D 的每个动作的奖励 R 有一个<strong class="kw iu">随机值，该值遵循均值为-0.5、方差为 1.0 的正态分布</strong>。</p><p id="9c14" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">已知 R 的期望值为负(-0.5)。这意味着在大量的实验中，R 的平均值小于零。</p><p id="ed97" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">基于这个假设，很明显，从 A 向左移动总是一个坏主意。然而，因为 R 的一些值是正的，Q-Learning 会被欺骗，认为从 A 向左移动会使回报最大化。事实上，这是一个糟糕的决定，因为即使它对某些情节有效，但从长远来看，它肯定是一个负面的回报。</p><p id="1e3d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么为什么 Q-Learning 会高估呢？<br/>为了回答这个问题，我们考虑以下场景:<br/>设 X1 和 X2 是两个随机变量，它们代表状态 b 下两个行为的回报。由于它们是随机变量，我们将计算它们的期望值 E(X1)和 E(X2)。但有一个问题，我们不知道他们的期望值，所以我们能做的是通过计算增量平均𝝁1 和𝝁2.来使用这些期望值的估计值这些估计值是无偏的，因为随着样本数量的增加，整组值的平均值越来越接近 E(X1)和 E(X2)，如下表所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mp"><img src="../Images/e081cbbd53e643a897428a700077ba03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*47PA48y8jv8hPfIBO_aL4g.png"/></div></div></figure><p id="c8b9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然而，Q-Learning 使用 Max Q(s '，a)，在表中由 Max(𝝁).表示从表中可以清楚地看出(见红色单元格)，E(Max(𝝁)不同于 Max E(X)。这说明 Max(𝝁)不是 Max E(X)的一个好的估计量。是有失偏颇！<br/>换句话说，当用最大 Q(s’，a)更新 Q(s，a)时，Q(s，a)没有向状态 B 的动作的期望值移动，即-0.5。</p><p id="833f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个场景直观地解释了为什么 Q-Learning 高估了这些值。正式的数学证明可以在论文中找到。</p><h1 id="36d7" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">解决方案</h1><p id="82be" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">所提出的解决方案是维护两个 Q 值函数 QA 和 QB，其中一个从另一个获得下一个状态的更新。更新包括找到在下一个状态中使 QA 最大化的动作 a*(Q(s '，a*) = Max Q(s '，a))，然后使用 a*获得 QB(s '，a*)的值，以便更新 QA(s，a)。</p><p id="71b4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面的伪代码显示了算法的行为。请注意，本文末尾有一段 python 代码，对这两种方法进行了比较。<br/>算法的第 3 行显示了如何从两个 Q 值函数中选择动作。例如，可以合并两个 Q(平均每个动作的值)，然后应用ε-greedy。</p><p id="ced0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该算法以等概率的方式更新 QA 和 QB。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mq"><img src="../Images/d3809b95f5f577652830638e5fd5f889.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iaP4c7KMM5X-Xo8m3TYaMw.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Algorithm taken from Double Q-learning by Hado van Hasselt</figcaption></figure><p id="f12d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面的图表显示了当状态 B 的动作数量连续为 10 和 100 时，双 Q 学习和 Q 学习之间的比较。<br/>很明显，双 Q 学习比 Q 学习收敛得更快。<br/>注意，当 B 点的动作数量增加时，Q-learning 比双 Q-Learning 需要更多的训练。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/770dd8c301a90aa1dc8a76d1a459a99c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aqcsdggghmQ5wFtJU1BaNg.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/24b02fb4e017253ab66b943b35d34015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BY0K7zQ8OwJ37P-Ne2DDfg.png"/></div></div></figure><h2 id="db32" class="mx ls it bd lt my mz dn lx na nb dp mb ld nc nd md lh ne nf mf ll ng nh mh ni bi translated">为什么有效？</h2><p id="2bce" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">范·哈瑟尔特在他的论文中证明了 E(Q2(s '，a*)≤马克斯·Q1(s '，a*)。<br/>所以经过足够多的实验，Q2(s '，a*)的期望值小于或等于最大 Q1(s '，a*)，这意味着 Q1(s，a)没有用最大值更新。</p><p id="38a6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下表显示了随着发作次数的增加，状态 A 下左侧动作的 Q 值的演变。<br/>请注意，在 Q-Learning 中，Q(A，Left)是正的，因为它受到状态 b 中存在的正奖励的影响。由于这个正值，算法对采取左边的动作更感兴趣，希望最大化奖励。正如你所看到的，直到第 50 集，左手动作的比例一直在增加。<br/>在双 Q-Learning 中，Q1(左一)和 Q2(左一)开始时略显消极。<br/>因此，左侧动作的百分比很早就开始下降，从而节省了训练时间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/f91428edb29a3eba8cfa3a0c0e58dce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6-RT2htg-Xkns-jtZSyCTw.png"/></div></div></figure><h1 id="5f4a" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">结论</h1><p id="959a" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">本文表明双 Q 学习有时可能低估动作值，但避免了 Q 学习高估偏差的缺陷。它还表明，对于这种类型的问题，双 Q 学习更快地达到良好的性能水平。</p><h1 id="5387" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">Python 代码</h1><p id="a3af" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">以下 Python 代码模拟了双 Q 学习和 Q 学习，并输出了这两种方法的表格和对比图。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="acfd" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">相关文章</h1><ul class=""><li id="a14b" class="nm nn it kw b kx mj la mk ld no lh np ll nq lp nr ns nt nu bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce"> TD 在强化学习中，最简单的方法</a></li><li id="6320" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated"><a class="ae lq" href="https://medium.com/p/9350e1523031" rel="noopener"> Q vs V 在强化学习中，最简单的方法</a></li><li id="b9e3" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated"><a class="ae lq" href="https://medium.com/p/1b7ed0c030f4" rel="noopener">数学背后的强化学习，最简单的方法</a></li><li id="6318" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated"><a class="ae lq" href="https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511" rel="noopener">蒙特卡洛强化学习中最简单的方法</a></li><li id="e2ca" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated"><a class="ae lq" href="https://medium.com/@zsalloum/dynamic-programming-in-reinforcement-learning-the-easy-way-359c7791d0ac" rel="noopener">动态编程中的强化学习，简单易行的方法</a></li></ul></div></div>    
</body>
</html>