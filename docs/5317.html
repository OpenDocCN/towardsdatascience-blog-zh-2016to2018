<html>
<head>
<title>Perplexity Intuition (and its derivation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">困惑直觉(及其衍生)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3?source=collection_archive---------1-----------------------#2018-10-11">https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3?source=collection_archive---------1-----------------------#2018-10-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="42b1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">永远不要再被困惑所困扰。</h2></div><p id="2415" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可能在 NLP 类中看到过类似这样的内容:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/a2f6c49773d9560635d7244cb6dd817b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QsEk2VjesgHrXPZilDEv1A.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">A slide from <a class="ae lu" href="https://courses.cs.washington.edu/courses/csep517/18au/" rel="noopener ugc nofollow" target="_blank">Dr. Luke Zettlemoyer’s NLP class</a></figcaption></figure><p id="581b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lv"><img src="../Images/a622dccd31d5f5f616e45f5b53cb900f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CcNxMTWLOsJnpPa00QcErQ.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">A slide of <a class="ae lu" href="https://web.stanford.edu/class/cs124/" rel="noopener ugc nofollow" target="_blank">CS 124</a> at Stanford (Dr. Dan Jurafsky)</figcaption></figure><p id="a7e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在课堂上，我们真的没有花时间去推导困惑。也许困惑是一个你可能已经知道的基本概念？这篇文章是为那些不知道的人写的。</p><p id="c8ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一般来说，困惑是一个概率模型如何预测样本的度量。在自然语言处理的背景下，困惑是<strong class="kk iu">评估语言模型</strong>的一种方式。</p><h1 id="0f66" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">但是为什么 NLP 中的困惑是这样定义的呢？</h1><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mo"><img src="../Images/d79b662b376c74a19190bb806ea10e72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wkqCQ4E5RFmCaaz17YCN7Q.png"/></div></div></figure></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><p id="8707" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你在维基百科上查找<strong class="kk iu">离散概率分布的困惑</strong>:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mw"><img src="../Images/e5f6a62ddf84059696154f56fee3b28f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0fkJkmacKK79zG1L_yYFUA.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">from <a class="ae lu" href="https://en.wikipedia.org/wiki/Perplexity" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Perplexity</a></figcaption></figure><p id="df73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="kk iu"> H(p)是分布 p(x) </strong>的熵，而<em class="mx"> x </em>是所有可能事件的随机变量。</p><p id="36a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lu" href="https://medium.com/@aerinykim/the-intuition-behind-shannons-entropy-e74820fe9800" rel="noopener">在之前的帖子</a>中，我们从零开始推导了<strong class="kk iu"> H(p) </strong>并直观地展示了<strong class="kk iu">为什么熵是我们对信息进行编码所需的平均位数。</strong>如果你不明白<strong class="kk iu"> H(p) </strong>，请先阅读此<strong class="kk iu"> </strong> ⇩ <strong class="kk iu"> </strong>再进一步阅读。</p><div class="my mz gp gr na nb"><a href="https://medium.com/@aerinykim/the-intuition-behind-shannons-entropy-e74820fe9800" rel="noopener follow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd iu gy z fp ng fr fs nh fu fw is bi translated">香农熵背后的直觉</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">【警告:太容易了！]</h3></div></div><div class="nj l"><div class="nk l nl nm nn nj no lo nb"/></div></div></a></div><p id="9319" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们同意 H(p)=-σp(x)log p(x)。</p><h1 id="4b21" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">那么，困惑只是<strong class="ak">熵的一个<strong class="ak">幂运算</strong>！</strong></h1><p id="4fb6" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">是的。熵是对随机变量中包含的信息进行编码的平均位数，因此熵的幂应该是<strong class="kk iu">所有可能信息的总量，</strong>或者更准确地说，是随机变量具有的选择的加权平均数<strong class="kk iu">。</strong></p><p id="10cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，<strong class="kk iu">如果测试集中的平均句子可以用 100 比特编码，则模型困惑度是每个句子 2 ⁰⁰。</strong></p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><p id="73f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们确认维基百科中的定义与幻灯片中的定义相匹配。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nu"><img src="../Images/eec0b54337eeaf3cc3e83d04321ed4c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h4qCgiWk3CaWTQdWrjq0Bw.png"/></div></div></figure><p id="be39" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在哪里</p><p id="090b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> p </strong>:我们要建模的概率分布<em class="mx">。</em>从<strong class="kk iu"> p </strong>中抽取一个训练样本，其分布未知。</p><p id="605c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> q </strong>:提出的概率模型。我们的预测。</p><blockquote class="nv nw nx"><p id="d7c4" class="ki kj mx kk b kl km ju kn ko kp jx kq ny ks kt ku nz kw kx ky oa la lb lc ld im bi translated">我们可以通过测试从<strong class="kk iu"> p. </strong>抽取的样本来评估我们的预测<strong class="kk iu"> q </strong>，然后<strong class="kk iu">基本上就是计算交叉熵</strong>。在上面的推导中，我们假设所有单词在<strong class="kk iu"> p </strong>中具有相同的概率(1 /单词数)。</p></blockquote><h2 id="c483" class="ob lx it bd ly oc od dn mc oe of dp mg kr og oh mi kv oi oj mk kz ok ol mm om bi translated">评论</h2><ul class=""><li id="592d" class="on oo it kk b kl np ko nq kr op kv oq kz or ld os ot ou ov bi translated">当<strong class="kk iu"> q(x) = 0 </strong>时，困惑度将为<strong class="kk iu"> ∞ </strong>。事实上，这也是 NLP 中引入<a class="ae lu" href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf" rel="noopener ugc nofollow" target="_blank">平滑概念的原因之一。</a></li><li id="dfa0" class="on oo it kk b kl ow ko ox kr oy kv oz kz pa ld os ot ou ov bi translated">如果我们对<strong class="kk iu"> q </strong>(简单来说就是所有单词的 1/N)使用统一的概率模型，那么困惑度就等于词汇量。</li><li id="1bcf" class="on oo it kk b kl ow ko ox kr oy kv oz kz pa ld os ot ou ov bi translated">以上推导仅用于说明目的，以便得出 UW/斯坦福幻灯片中的公式。在这两张幻灯片中，它假设我们正在使用一个 unigram 模型计算整个语料库的困惑度，并且没有重复的单词。(它假设总单词数(N)与唯一单词数相同。)此外，它假设所有单词具有相同的概率 1/N。这些都不是现实的假设。</li></ul></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h2 id="b42b" class="ob lx it bd ly oc od dn mc oe of dp mg kr og oh mi kv oi oj mk kz ok ol mm om bi translated"><strong class="ak">外卖</strong></h2><ul class=""><li id="51f0" class="on oo it kk b kl np ko nq kr op kv oq kz or ld os ot ou ov bi translated">较小的熵(或较少无序的系统)比较大的熵更有利。因为可预测的结果优于随机性。这就是为什么人们说<strong class="kk iu">低困惑是好的，高困惑是坏的，因为困惑是熵</strong>的幂(你可以放心地认为困惑的概念是熵)。</li><li id="0e69" class="on oo it kk b kl ow ko ox kr oy kv oz kz pa ld os ot ou ov bi translated">语言模型是句子的概率分布。最好的语言模型是能够最好地预测一个未知测试集的语言模型。</li><li id="af89" class="on oo it kk b kl ow ko ox kr oy kv oz kz pa ld os ot ou ov bi translated"><strong class="kk iu">为什么我们要用困惑度而不是熵？<br/> </strong>如果我们把困惑想象成一个<strong class="kk iu">分支因子</strong>(一个随机变量拥有的选择的加权平均数)，<strong class="kk iu">那么这个数字比熵更容易理解。我觉得这很令人惊讶，因为我以为会有更深刻的原因。我问 Zettlemoyer 博士，除了容易理解之外，是否还有其他原因。他的回答是“我想就是这样！<strong class="kk iu">这在很大程度上是历史性的，因为许多其他指标也可以合理使用！”</strong></strong></li></ul></div></div>    
</body>
</html>