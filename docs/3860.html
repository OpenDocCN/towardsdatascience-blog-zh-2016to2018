<html>
<head>
<title>Pitfalls of Batch Norm in TensorFlow and Sanity Checks for Training Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流批量范数的缺陷和训练网络的健全性检验</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pitfalls-of-batch-norm-in-tensorflow-and-sanity-checks-for-training-networks-e86c207548c8?source=collection_archive---------2-----------------------#2018-06-26">https://towardsdatascience.com/pitfalls-of-batch-norm-in-tensorflow-and-sanity-checks-for-training-networks-e86c207548c8?source=collection_archive---------2-----------------------#2018-06-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/4d8872bfb0047b50bf460c8fe00c12bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uu5uU4EPi86Uzb37DzogmA.png"/></div></div></figure><div class=""/><div class=""><h2 id="dd9a" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">在这篇文章中，我将写下我们在编写深度学习模型和一些健全性检查时犯的错误，这将帮助你缩小小守护进程的范围，并挽救你的夜晚睡眠。</h2></div><h1 id="7d68" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">I .让我们一劳永逸地完成 TensorFlow 中的批量定额</h1><h2 id="ec33" class="li kr jb bd ks lj lk dn kw ll lm dp la ln lo lp lc lq lr ls le lt lu lv lg lw bi translated">批量标准化的简要理论</h2><p id="9812" class="pw-post-body-paragraph lx ly jb lz b ma mb kc mc md me kf mf ln mg mh mi lq mj mk ml lt mm mn mo mp ij bi translated">这项革命性的技术是由 Sergey Ioffe，Christian Szegedy 在<a class="ae mq" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> <a class="ae mq" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">，</a>中介绍的，到目前为止被引用了 4994 次。根据该论文，批量标准化减少了内部协方差移动，即，它使得网络中各层的学习更加相互独立。批量范数层的目标是向激活层输入，单位高斯，使得神经元在 sigmoid 和 tanh 的情况下不会饱和。它有助于网络的快速收敛，允许你不用关心权重的初始化，作为正则化来代替丢失和其他正则化技术。现在我不会讨论太多的理论细节，而是更多地讨论实现。我将带您了解 TensorFlow 及其数学中批处理规范的实现。<br/>在训练的时候-</p><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/14ef7f881d3b838617aa4d9b9b3ec521.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*U0ZFNnQZnuldcHFN14Dy-Q.png"/></div></figure><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/f8aa5e32b731fb27165261c49e850d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*Ki4E1TnC-0ZtkFhFuLGrOg.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk">At training time. Images taken from Batch Normalization paper.</figcaption></figure><p id="e0e7" class="pw-post-body-paragraph lx ly jb lz b ma nb kc mc md nc kf mf ln nd mh mi lq ne mk ml lt nf mn mo mp ij bi translated">现在假设卷积层和激活层之间使用批量范数层，那么<strong class="lz jc"> <em class="ng"> x </em> </strong>是卷积层的输出，<strong class="lz jc"> <em class="ng"> y </em> </strong>是批量范数层的输出，输入到激活层，可能是 ReLU，sigmoid 等。批量定额层学习参数<strong class="lz jc"> <em class="ng"> γ </em> </strong>和<strong class="lz jc"> <em class="ng"> β </em> </strong>，<strong class="lz jc"> <em class="ng"> y </em> </strong>取决于它们的值。因此，示出了使用<strong class="lz jc"> <em class="ng"> γ </em> </strong>和<strong class="lz jc"> <em class="ng"> β </em> </strong>来学习<strong class="lz jc"> <em class="ng"> x </em> </strong>的最佳表示。请注意另外两个统计量，即<strong class="lz jc"><em class="ng">【x】</em></strong>和<strong class="lz jc"><em class="ng">Var【x】</em></strong>、总体均值和方差。它们是通过训练期间的移动均值和移动方差来估计的。</p><p id="5b5a" class="pw-post-body-paragraph lx ly jb lz b ma nb kc mc md nc kf mf ln nd mh mi lq ne mk ml lt nf mn mo mp ij bi translated">在推断的时候-</p><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/7ae7560948b68a83255c9a40f95b3087.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*2wEyEM8-6XEXbmVWiK88Xw.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk">At inference time</figcaption></figure><p id="8e5f" class="pw-post-body-paragraph lx ly jb lz b ma nb kc mc md nc kf mf ln nd mh mi lq ne mk ml lt nf mn mo mp ij bi translated">推理时向前通过批范数层与训练时不同。在推断时，我们不用批均值(<strong class="lz jc"><em class="ng"/></strong>)和方差(<strong class="lz jc"><em class="ng">【σ2</em></strong>)而是用总体均值(<strong class="lz jc"><em class="ng">e【x】</em></strong>)和方差(<strong class="lz jc"><em class="ng">var【x】</em></strong>)来计算<strong class="lz jc"> <em class="ng"> x^ </em> </strong>。假设您在推断过程中给定批量大小为 1 的批，并使用批均值和批方差进行归一化，在这种情况下，<strong class="lz jc"><em class="ng"/></strong>为<strong class="lz jc"> <em class="ng"> μ=x </em> </strong>然后<strong class="lz jc"> <em class="ng"> y=β </em> </strong>，这表明层输出<em class="ng">(</em><strong class="lz jc"><em class="ng">y</em><em class="ng">)</em>对于任何输入<em class="ng"> ( </em> <strong class="lz jc">)因此，为了在推断时进行标准化，我们使用总体均值和方差，这是我们在训练期间使用移动均值和方差计算的。在下一节中，我们将介绍 TensorFlow 中批处理规范的实现。</strong></strong></p><h2 id="c8a1" class="li kr jb bd ks lj lk dn kw ll lm dp la ln lo lp lc lq lr ls le lt lu lv lg lw bi translated">TF 中批量定额的注意事项</h2><ul class=""><li id="d69a" class="ni nj jb lz b ma mb md me ln nk lq nl lt nm mp nn no np nq bi translated"><strong class="lz jc">移动平均值和方差不更新:<br/> </strong>张量流是基于图形的，计算按依赖关系的顺序进行。</li></ul><pre class="ms mt mu mv gt nr ns nt nu aw nv bi"><span id="fbeb" class="li kr jb ns b gy nw nx l ny nz">import tensorflow as tf</span><span id="cb17" class="li kr jb ns b gy oa nx l ny nz">a = tf.placeholder(tf.float32, shape=[1,3])<br/>b = <!-- -->tf.placeholder(tf.float32, shape=[1,3])<br/>c = tf.Variable(tf.zeros([1, 3]), dtype=tf.float32)</span><span id="c006" class="li kr jb ns b gy oa nx l ny nz"><br/>def train(a,b):<br/>    c = tf.add(c,a)<br/>    e = some_train_op(a,b)<br/>    return e</span><span id="6e43" class="li kr jb ns b gy oa nx l ny nz">output = train(a,b)<br/>loss = tf.log(output)</span><span id="35d9" class="li kr jb ns b gy oa nx l ny nz">sess = tf.Session()<br/>saver = tf.train.Saver()<br/>sess.run(tf.global_variables_initializer())</span><span id="e035" class="li kr jb ns b gy oa nx l ny nz">for itr in range(1000):<br/>    ls = sess.run(loss, {a:x, b:y})</span><span id="521b" class="li kr jb ns b gy oa nx l ny nz">saver.save(sess, some_path)</span><span id="0874" class="li kr jb ns b gy oa nx l ny nz">#Note that this code is for reference only. </span></pre><p id="ed06" class="pw-post-body-paragraph lx ly jb lz b ma nb kc mc md nc kf mf ln nd mh mi lq ne mk ml lt nf mn mo mp ij bi translated">现在，在此我们运行<strong class="lz jc"> <em class="ng">损失</em> </strong>张量在每次迭代中，对于每次迭代<strong class="lz jc"> <em class="ng">列车</em> </strong>函数都会被调用。<strong class="lz jc"> <em class="ng"> c </em> </strong>是一个变量，对所有迭代的<strong class="lz jc"> <em class="ng"> a </em> </strong>的值求和。因此，当我们保存模型时，我们期望变量<strong class="lz jc"> <em class="ng"> c </em> </strong>等于在每次迭代中传递给占位符<strong class="lz jc"><em class="ng"/></strong>的所有值的总和。</p><p id="adc2" class="pw-post-body-paragraph lx ly jb lz b ma nb kc mc md nc kf mf ln nd mh mi lq ne mk ml lt nf mn mo mp ij bi translated">让我们详细分析一下，这个模型的图中的依赖关系。当我们运行<strong class="lz jc"> <em class="ng">损失</em> </strong>张量，<strong class="lz jc"> <em class="ng">输出</em> </strong>将被计算然后<strong class="lz jc"> <em class="ng">输出</em> </strong>将调用<strong class="lz jc"> <em class="ng">列车</em> </strong>从<strong class="lz jc"> <em class="ng"> e </em> </strong>中需要哪个值，将使用<strong class="lz jc"><em class="ng"/></strong>和<strong class="lz jc"> <em class="ng"> b </em> </strong>来计算在所有这些计算中<strong class="lz jc"> <em class="ng"> c </em> </strong>根本不会被计算为<strong class="lz jc"> <em class="ng">损失</em> </strong>张量的值不依赖于<strong class="lz jc"> <em class="ng"> c </em> </strong>。解决方法是在计算<strong class="lz jc"><em class="ng"/></strong>e 之前加上<strong class="lz jc"> <em class="ng"> c </em> </strong>作为依赖。因此，解决方案</p><pre class="ms mt mu mv gt nr ns nt nu aw nv bi"><span id="2a73" class="li kr jb ns b gy nw nx l ny nz">def train(a,b):<br/>    c = tf.add(c,a)<br/>    with tf.control_dependencies([c]):<br/>        e = <!-- -->some_train_op(a,b)<br/>    return e</span></pre><p id="0fce" class="pw-post-body-paragraph lx ly jb lz b ma nb kc mc md nc kf mf ln nd mh mi lq ne mk ml lt nf mn mo mp ij bi translated">在训练网络时，我们调用优化器 op 来完成所有计算并更新所有权重，但是优化器 op 从不依赖于移动平均值或方差的计算。因此，移动平均值和方差永远不会更新，而是保持其初始值。</p><p id="9eaa" class="pw-post-body-paragraph lx ly jb lz b ma nb kc mc md nc kf mf ln nd mh mi lq ne mk ml lt nf mn mo mp ij bi translated">解决方案:TensorFlow 为批量规范化提供了三个主要函数</p><pre class="ms mt mu mv gt nr ns nt nu aw nv bi"><span id="c850" class="li kr jb ns b gy nw nx l ny nz"><strong class="ns jc">tf.nn.batch_normalization()</strong> - Never use this for training, it just compute the y = gamma*x^hat + beta where x^hat = (x-mean)/std_dev.<br/>It doesn't account for keeping moving mean and variance. </span><span id="a9be" class="li kr jb ns b gy oa nx l ny nz"><strong class="ns jc">tf.layers.batch_normalization() </strong>-<strong class="ns jc"> </strong>This function can be used for your model. Note: Always add batch norm dependency when using this, either on optimizer op or any other op which you sure that will execute when optimizer op is run. ex:-</span><span id="0497" class="li kr jb ns b gy oa nx l ny nz">update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)                            with tf.control_dependencies(update_ops):                                                            <br/>    self.solver = tf.train.AdamOptimizer(learning_rate=1e-04) <br/>                  .minimize(self.loss)</span><span id="e8cf" class="li kr jb ns b gy oa nx l ny nz">Here tf.GraphKeys.UPDATE_OPS is the collections which have moving mean and variance op. </span><span id="b3ce" class="li kr jb ns b gy oa nx l ny nz"><strong class="ns jc">tf.contrib.layers.batch_norm()</strong> - This function is in contrib module of TensorFlow. This function gives two ways to add dependencies as follows:<br/>1.It has '<em class="ng">update</em>' parameter, set <em class="ng">update=None</em> while calling the function and dependency will be added inside the function itself and you don't have to anything else.<br/><strong class="ns jc">Note:This is believed to be slow in comparison to the next method.</strong></span><span id="f960" class="li kr jb ns b gy oa nx l ny nz">2. Let the <em class="ng">update</em> parameter have it's default value which is "<em class="ng">tf.GraphKeys.UPDATE_OPS</em>". Now here also you have to manually add dependency as above with tf.layers.batch_normalization().</span></pre><p id="3964" class="pw-post-body-paragraph lx ly jb lz b ma nb kc mc md nc kf mf ln nd mh mi lq ne mk ml lt nf mn mo mp ij bi translated">因此，您可以使用<strong class="lz jc">TF . layers . batch _ normalization()或 TF . contrib . layers . batch _ norm()</strong>进行适当的更新依赖处理。</p><ul class=""><li id="620d" class="ni nj jb lz b ma nb md nc ln ob lq oc lt od mp nn no np nq bi translated"><strong class="lz jc">训练和测试时的不同用法:</strong> <br/>从训练到测试时，我们要在批均值和方差或者总体均值和方差之间切换。我们在 tf.contrib.layers.batch_norm()和 tf.nn.batch_normalization()中分别有参数<code class="fe oe of og ns b">is_training</code>和<code class="fe oe of og ns b">training </code>。当其值为<code class="fe oe of og ns b">True</code>时，使用批均值&amp;方差并更新移动均值和方差来估计总体均值和方差。在<code class="fe oe of og ns b">False</code>上，使用总体均值和方差。因此你应该为此使用一个占位符<code class="fe oe of og ns b">train_phase</code>。因此，解决方案如下所示</li></ul><pre class="ms mt mu mv gt nr ns nt nu aw nv bi"><span id="b84f" class="li kr jb ns b gy nw nx l ny nz">def Bn(x, <strong class="ns jc">is_train</strong>):<br/>        return tf.contrib.layers.batch_norm(x, decay= 0.99,                                                                                                               <br/>          <strong class="ns jc"><em class="ng">is_training=is_train</em></strong>,center= True, scale=True, <br/>          reuse= False)</span><span id="8d35" class="li kr jb ns b gy oa nx l ny nz"><em class="ng">train_phase = tf.placeholder(tf.bool, name="is_training")</em><br/>data = tf.placeholder(tf.float32, shape=[None,240,240,3],                                                                                                        name="Image")                                         </span><span id="b444" class="li kr jb ns b gy oa nx l ny nz">bn_layer = Bn(data, train_phase)</span><span id="b49d" class="li kr jb ns b gy oa nx l ny nz">loss = some_loss_op()</span><span id="1740" class="li kr jb ns b gy oa nx l ny nz">solver = tf.train.AdamOptimizer().minimize(loss)</span><span id="5b7c" class="li kr jb ns b gy oa nx l ny nz"># Training iteration<br/>sess.run(solver, {data: input_data, <em class="ng">train_phase:True</em>})</span><span id="8ca9" class="li kr jb ns b gy oa nx l ny nz">#Validation iteration<br/>sess.run(loss, {data: input_data, <em class="ng">train_phase:False</em>})</span></pre><ul class=""><li id="8ccc" class="ni nj jb lz b ma nb md nc ln ob lq oc lt od mp nn no np nq bi translated"><strong class="lz jc">共享批次范数参数:<br/> </strong>通常你在两个并行网络中共享权重，比如 siamese 或者在 GANs 的鉴别器中。当您共享您的重量时，批量定额参数也会被共享。有时候，跨两个网络共享批量定额参数也不能健康，具体什么时候我会告诉你。<br/>假设我们正在训练一个 GAN，它将有两个具有共享参数的鉴别器实例，当训练开始时，伪图像和真实图像的分布是不同的。现在我们用两种不同的分布来更新移动平均值和方差，一种是假图像，另一种是真实图像。但是随着训练的继续和生成器学习真实数据的分布，假数据和真实数据的分布最终变得近似相同。然后我们在更新同分布的均值和方差，所以我们的总体均值和方差应该是完整训练后真实数据的估计统计量。因此，理论上，在 GANs 的情况下，共享批量定额参数可能不是一个大问题。但是我会在这两种条件下做一些实验，并在新的博客上更新实验结果。<br/>现在<a class="ae mq" href="https://www.alexirpan.com/2017/04/26/perils-batch-norm.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lz jc">这篇博文</strong> </a>可以很好地解释共享批量定额参数会对模型的准确性产生不利影响的情况，并具有更好的可视化效果。<br/>简而言之，在两个共享批次范数层的输入分布相同之前，共享批次范数参数是可以的，如果不是这种情况，请尝试在上述批次范数函数中使用<code class="fe oe of og ns b">reuse=False</code>，这将允许您在想要共享权重的两个网络中拥有独立的批次范数参数。</li></ul></div><div class="ab cl oh oi hu oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ij ik il im in"><h2 id="0487" class="li kr jb bd ks lj lk dn kw ll lm dp la ln lo lp lc lq lr ls le lt lu lv lg lw bi translated">二。健全性检查和调试-</h2><ul class=""><li id="e310" class="ni nj jb lz b ma mb md me ln nk lq nl lt nm mp nn no np nq bi translated"><strong class="lz jc">端到端图像模型:</strong> <br/>在图像分割或深度预测等以图像为输入的任务中，我们希望输出大小相同的图像。训练您的模型以学习身份函数，即将输入图像作为目标图像。在这种情况下，网络将代表身份功能。如果这是完美的训练，但你仍然无法学习分割或深度，可能是你的模型没有足够的能力。你可以尝试添加更多的层或可能更好的损失功能为您的任务。</li><li id="18ab" class="ni nj jb lz b ma oo md op ln oq lq or lt os mp nn no np nq bi translated"><strong class="lz jc">目标和输出范围:</strong> <br/>如果您已经将目标图像归一化为 0 到 1，那么请确保在图像生成层中使用 sigmoid 作为激活函数，而不是 tanh，因为 tanh 将具有-1 到 1 的范围，这将使学习变得困难。如果归一化在-1 到 1 之间，则使用双曲正切。</li><li id="0a73" class="ni nj jb lz b ma oo md op ln oq lq or lt os mp nn no np nq bi translated"><strong class="lz jc">波动损失:</strong> <br/>最可能的原因可能是您的机型没有足够的容量。因此，权重快速改变以学习模型，但是由于容量有限，它们保持快速改变它们的权重，但是从未接近最优。你也应该试着降低学习速度。</li><li id="6be1" class="ni nj jb lz b ma oo md op ln oq lq or lt os mp nn no np nq bi translated"><strong class="lz jc">超参数:</strong> <br/>千万不要忽略超参数。一般来说，它们对于你的网络的准确性和性能可能不是非常关键，但是对于训练的收敛性和稳定性绝对是非常重要的。疏忽的权重初始化可能导致网络中的梯度或 NaNs 被利用/消失。如果你使用正态分布，检查你的权重初始化的 std_dev，范围[0.2，0.01]是可以的。如果损失表现异常，学习率和批量大小是首先要试验的。</li><li id="9ffd" class="ni nj jb lz b ma oo md op ln oq lq or lt os mp nn no np nq bi translated"><strong class="lz jc">顺序数据的混排:</strong> <br/>当您的输入数据是顺序数据时，这种情况在视频中最常见，如自动驾驶中的分段、深度预测等。你应该考虑改变数据，用连续数据训练会对测试中的模型准确性产生负面影响。<br/> <strong class="lz jc">注:每当任务需要时间一致性时，如光流、视觉里程计等，你应该有顺序数据。</strong></li><li id="fd28" class="ni nj jb lz b ma oo md op ln oq lq or lt os mp nn no np nq bi translated"><strong class="lz jc">缩放损耗:</strong> <br/>如果您在网络中使用一种以上的损耗类型，例如 mse、对抗性、L1、<a class="ae mq" href="https://arxiv.org/pdf/1603.08155.pdf" rel="noopener ugc nofollow" target="_blank">特征损耗</a>、<a class="ae mq" href="https://www.tensorflow.org/api_docs/python/tf/image/ssim" rel="noopener ugc nofollow" target="_blank"> SSIM </a>，那么请确保所有损耗都被适当缩放到相同的阶数，即 MSE 为 1e-01，特征损耗为 1e03，然后将特征损耗缩放到相同的阶数。</li><li id="e522" class="ni nj jb lz b ma oo md op ln oq lq or lt os mp nn no np nq bi translated"><strong class="lz jc">图形可视化，这场战争中最大的武器:</strong> <br/>如果你使用 TF 和其他有可视化工具的框架。您应该使用可视化来检查连接，它们将帮助您找出遗漏的连接、尺寸误差或重量分配误差。</li><li id="f5fd" class="ni nj jb lz b ma oo md op ln oq lq or lt os mp nn no np nq bi translated"><strong class="lz jc">共享批量定额参数:</strong> <br/>如上所述，如果您在两个网络中共享批量定额参数，那么请确保两个网络的输入数据分布相同。</li></ul><p id="0492" class="pw-post-body-paragraph lx ly jb lz b ma nb kc mc md nc kf mf ln nd mh mi lq ne mk ml lt nf mn mo mp ij bi translated">这篇<a class="ae mq" href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607" rel="noopener ugc nofollow" target="_blank">博文</a>还详尽列举了你的网络失败的其他原因。</p><p id="d9da" class="pw-post-body-paragraph lx ly jb lz b ma nb kc mc md nc kf mf ln nd mh mi lq ne mk ml lt nf mn mo mp ij bi translated">感谢大家阅读这篇博客，希望它能拯救你与批量定额和培训网络的斗争。我欢迎您的所有评论，如果您想添加任何内容或觉得任何内容有误导性，请告诉我。如有疑问，可发邮件至 dishank.bansal@iitkgp.ac.in 联系我。</p></div><div class="ab cl oh oi hu oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ij ik il im in"><h2 id="011f" class="li kr jb bd ks lj lk dn kw ll lm dp la ln lo lp lc lq lr ls le lt lu lv lg lw bi translated">参考资料:</h2><ul class=""><li id="adf6" class="ni nj jb lz b ma mb md me ln nk lq nl lt nm mp nn no np nq bi translated"><a class="ae mq" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1502.03167</a>—批量定额纸</li><li id="f372" class="ni nj jb lz b ma oo md op ln oq lq or lt os mp nn no np nq bi translated"><a class="ae mq" href="https://r2rt.com/implementing-batch-normalization-in-tensorflow.html" rel="noopener ugc nofollow" target="_blank">https://r2rt . com/implementing-batch-normalization-in-tensor flow . html</a></li><li id="c54e" class="ni nj jb lz b ma oo md op ln oq lq or lt os mp nn no np nq bi translated"><a class="ae mq" href="https://medium.com/machine-learning-world/how-to-debug-neural-networks-manual-dc2a200f10f2" rel="noopener">https://medium . com/machine-learning-world/how-to-debug-neural-networks-manual-dc2a 200 F10 f 2</a></li><li id="69c7" class="ni nj jb lz b ma oo md op ln oq lq or lt os mp nn no np nq bi translated"><a class="ae mq" href="https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/layers/normalization.py" rel="noopener ugc nofollow" target="_blank">https://github . com/tensor flow/tensor flow/blob/r 1.8/tensor flow/python/layers/normalization . py</a></li></ul></div></div>    
</body>
</html>