<html>
<head>
<title>Shallow Understanding on Bayesian Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对贝叶斯优化的浅见</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/shallow-understanding-on-bayesian-optimization-324b6c1f7083?source=collection_archive---------1-----------------------#2017-07-29">https://towardsdatascience.com/shallow-understanding-on-bayesian-optimization-324b6c1f7083?source=collection_archive---------1-----------------------#2017-07-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/67dde40523211c9f9ce46bb250332626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PhKGj_bZlND8IEfII426wA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Sample data increases &amp; Curve fitting gets improved.</figcaption></figure><h2 id="a6f1" class="kc kd iq bd ke kf kg dn kh ki kj dp kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">预先知道的硬性条款</h2><blockquote class="ky"><p id="d71a" class="kz la iq bd lb lc ld le lf lg lh li dk translated">高斯过程</p><p id="a453" class="kz la iq bd lb lc ld le lf lg lh li dk translated">无噪声全局优化</p><p id="2cf8" class="kz la iq bd lb lc ld le lf lg lh li dk translated">噪声全局优化</p><p id="004a" class="kz la iq bd lb lc ld le lf lg lh li dk translated">采集功能</p><p id="d48f" class="kz la iq bd lb lc ld le lf lg lh li dk translated">代理函数(/高斯过程)</p><p id="fae9" class="kz la iq bd lb lc ld le lf lg lh li dk translated">随机的</p><p id="917d" class="kz la iq bd lb lc ld le lf lg lh li dk translated">贝叶斯回归</p></blockquote><p id="c2b4" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt kl lu lv lw kp lx ly lz kt ma mb mc li ij bi translated">什么是高斯过程？？</p><p id="da04" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">这是一个贝叶斯回归</p><p id="41be" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">——这是一种<strong class="ll ir">精确</strong>插值<strong class="ll ir">回归</strong>的方法。</p><p id="3540" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">——是<strong class="ll ir">线性</strong>回归的<strong class="ll ir">自然推广</strong>。</p><ul class=""><li id="d742" class="mi mj iq ll b lm md lq me kl mk kp ml kt mm li mn mo mp mq bi translated">它指定了一个<strong class="ll ir">分配</strong> <strong class="ll ir">到</strong> <em class="mr">功能</em>。</li></ul><p id="8843" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi ms translated"><span class="l mt mu mv bm mw mx my mz na di"> B </span>想象一下，如果我们不知道一个函数，我们通常会做什么？当然，我们会尝试用一些已知的先验知识来猜测或近似它。同样的想法背后是<strong class="ll ir">后验概率</strong>。这里的标准是我们有<strong class="ll ir">观察</strong>，其中的数据是一个记录接一个记录而来(<strong class="ll ir">在线学习</strong>)，所以我们需要训练这个模型。训练好的模型显然会服从一个函数。我们不知道的那个函数将完全依赖于<strong class="ll ir">学习数据</strong>。所以我们的任务是找到<strong class="ll ir">最大化</strong>学习模式的<strong class="ll ir">超参数</strong>。</p><p id="8db9" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">我们这里有一些变量。一个是<strong class="ll ir">观察记录(特征+标签)</strong>，第二个是<strong class="ll ir">参数</strong>，定义模型。</p><h1 id="658d" class="nb kd iq bd ke nc nd ne kh nf ng nh kk ni nj nk ko nl nm nn ks no np nq kw nr bi translated">例如。</h1><blockquote class="ns nt nu"><p id="8869" class="lj lk mr ll b lm md lo lp lq me ls lt nv mf lv lw nw mg ly lz nx mh mb mc li ij bi translated">(比如说<strong class="ll ir">中的 like y = MX+c</strong>；m &amp; c 是参数，y，x 是变量-标签&amp;特征。我们有义务找到这些 m &amp; c 来学习我们最好的模型)</p></blockquote><p id="4bec" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">贝叶斯优化有助于在众多模型中找到一个最佳模型。我们正在进行交叉验证。但是我们要在一个预先列表上尝试多少个样本来从中选择一个最好的模型。这就是为什么贝叶斯方法<strong class="ll ir">通过减少计算任务来加速这个过程，并且不期望人们帮助猜测数值。这种优化技术基于<strong class="ll ir">随机性</strong>和<strong class="ll ir">概率分布。</strong></strong></p><p id="3ff2" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">用我的话来说，<em class="mr">第一个观测点</em>来到模型。我们划清界限。那么下一点就来了。我们把这两点连接起来，从之前的曲线画一条调整过的线。第三点来了，所以我们画一条非线性曲线。当观察点的数量增加时，可能曲线的组合数量增加。这类似于统计学中的抽样定理，我们从样本参数中估计总体参数。</p><p id="9843" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">我们绘制了另一条<strong class="ll ir">直方图曲线</strong>以及非线性目标函数曲线。这表明，对于给定的目标函数曲线，在所有过去的观察值中，<strong class="ll ir"> max </strong>值将出现在哪里(找到<strong class="ll ir"> arg </strong> max)。</p><figure class="nz oa ob oc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/a3eebf2d93262fe0bb7bd12eee71ffde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bp7dYIKiYGeQD7e1eW4zcg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd od">Histogram Curve</strong>, that is the <strong class="bd od">Acquisition Function</strong> (includes different acq functions).</figcaption></figure><p id="1c97" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">我们这里的目标是<strong class="ll ir">而不是</strong>用尽可能多的观察点将目标函数(未知)完全确定为一条曲线。相反，我们需要最大值的<strong class="ll ir">arg(max 函数的索引)。所以我们需要将注意力转向直方图函数。当目标函数曲线的可能组合增加时，直方图形成描述采集函数的分布。这是贝叶斯优化背后的思想。</strong></p><blockquote class="ky"><p id="a87d" class="kz la iq bd lb lc ld le lf lg lh li dk translated">记住，我们的目标，首先需要确定最大值的 arg，其次选择下一个最好的最大值，它可能在函数曲线上。</p></blockquote><figure class="of og oh oi oj jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/99bd265760560a8a48e946e934c54ffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tkZ9AEweXXJhwBWneV0HsA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd od">Many Random Ensemble Curves</strong> that we could draw from the <strong class="bd od">above observed black samples.</strong></figcaption></figure><p id="491c" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">我们在图中看到许多<strong class="ll ir">摇摆曲线</strong>。这告诉函数曲线的范围<strong class="ll ir">下一个点可以位于给定采样点的</strong>。方差给出了函数曲线上平均值的分布。</p><p id="a07a" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">当我们通过将观测值叠加在未知函数曲线上进行估计时，那么这就是<strong class="ll ir">无噪声优化</strong>。但是当我们需要考虑<strong class="ll ir">噪声优化</strong>时，我们的未知函数会将观察点略微除以噪声误差值。</p><p id="641f" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">我们在许多测量下定义了<strong class="ll ir">采集函数</strong>，其中一些在这里，</p><blockquote class="ns nt nu"><p id="69b3" class="lj lk mr ll b lm md lo lp lq me ls lt nv mf lv lw nw mg ly lz nx mh mb mc li ij bi translated"><strong class="ll ir"> 1。PMAX </strong></p><p id="b853" class="lj lk mr ll b lm md lo lp lq me ls lt nv mf lv lw nw mg ly lz nx mh mb mc li ij bi translated"><strong class="ll ir"> 2。IEMAX </strong></p><p id="1224" class="lj lk mr ll b lm md lo lp lq me ls lt nv mf lv lw nw mg ly lz nx mh mb mc li ij bi translated"><strong class="ll ir"> 3。MPI =最大改善概率</strong></p><p id="1213" class="lj lk mr ll b lm md lo lp lq me ls lt nv mf lv lw nw mg ly lz nx mh mb mc li ij bi translated"><strong class="ll ir"> 4。MEI =最大预期改善值</strong></p><p id="1358" class="lj lk mr ll b lm md lo lp lq me ls lt nv mf lv lw nw mg ly lz nx mh mb mc li ij bi translated"><strong class="ll ir"> 5。UCB =置信上限</strong></p><p id="367f" class="lj lk mr ll b lm md lo lp lq me ls lt nv mf lv lw nw mg ly lz nx mh mb mc li ij bi translated"><strong class="ll ir"> 6。GP-对冲</strong></p><p id="7cc1" class="lj lk mr ll b lm md lo lp lq me ls lt nv mf lv lw nw mg ly lz nx mh mb mc li ij bi translated"><strong class="ll ir"> MM =最大平均值</strong></p><p id="558d" class="lj lk mr ll b lm md lo lp lq me ls lt nv mf lv lw nw mg ly lz nx mh mb mc li ij bi translated"><strong class="ll ir"> MUI =最大上限区间</strong></p></blockquote><p id="6ad2" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated"><strong class="ll ir">采集标准</strong> :- <em class="mr">接下来应该选择哪个样本</em></p><p id="d6cf" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated"><strong class="ll ir">替代函数/响应面(模型)</strong></p><ul class=""><li id="48f9" class="mi mj iq ll b lm md lq me kl mk kp ml kt mm li mn mo mp mq bi translated">基于先前未观察到的点<strong class="ll ir">的后验超过<strong class="ll ir">。</strong></strong></li><li id="6cee" class="mi mj iq ll b lm ok lq ol kl om kp on kt oo li mn mo mp mq bi translated">其<strong class="ll ir">参数</strong>可能基于先验。</li></ul><h1 id="dc07" class="nb kd iq bd ke nc nd ne kh nf ng nh kk ni nj nk ko nl nm nn ks no np nq kw nr bi translated">采集功能:-</h1><p id="1bc7" class="pw-post-body-paragraph lj lk iq ll b lm op lo lp lq oq ls lt kl or lv lw kp os ly lz kt ot mb mc li ij bi translated">测量的是<strong class="ll ir"> <em class="mr">改善的概率(PI)。</em>T51】</strong></p><figure class="nz oa ob oc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ou"><img src="../Images/2dd388a5b6b338d4312440a33909c990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5FpIvJ-TVh_ZlAKmrv9nRg.png"/></div></div></figure><figure class="nz oa ob oc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ov"><img src="../Images/287b5a55d9ac29795150c693586af8ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*epvSotjfl110JKkZhZYtLg.png"/></div></div></figure><p id="5450" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated"><strong class="ll ir"> μ </strong> + =过去点中<strong class="ll ir">最佳</strong> <strong class="ll ir">观察到的</strong>值。</p><p id="93c6" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">我们正在寻找未知函数<strong class="ll ir">的最大值</strong>。但是在我们以往的观测当中，<strong class="ll ir">下一个</strong> <strong class="ll ir">可能的</strong>最大值应该大于<strong class="ll ir">当前的最大观测值。</strong></p><p id="44a5" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated"><strong class="ll ir"> ε </strong> = <strong class="ll ir">非常小的任意值表示高斯分布中的点。</strong></p><h1 id="8e1a" class="nb kd iq bd ke nc nd ne kh nf ng nh kk ni nj nk ko nl nm nn ks no np nq kw nr bi translated">我们的目标:-</h1><figure class="nz oa ob oc gt jr gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/9d22ee77f3dc225f9d6283765187a767.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*AcbaPDYbf0nSeTq154hRyA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Find the arg of the max of f(X)</figcaption></figure><p id="2863" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated"><strong class="ll ir"> PI(x) </strong> =发现<strong class="ll ir">未知函数</strong>的<strong class="ll ir">下一个最大值</strong>高于<strong class="ll ir">最大观测值</strong>加上<strong class="ll ir">一些偏差的概率。</strong></p><p id="06cb" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">这只是高斯分布与最大观测值的阈值边界的 1D 积分。</p><p id="9c6c" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">对于未知函数中的每个点，我们考虑在每个点样本上垂直放置一个高斯分布。这个分布在未知函数的值上有均值&amp;某个确定的方差。</p><p id="8885" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">我们把这个标准化在上面的<strong class="ll ir"> <em class="mr">库什纳 1964 年</em> </strong>方程中。</p><p id="a626" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated"><strong class="ll ir">探索&amp;开发</strong>是解释一种现象和优化算法的常用术语。这两者之间存在一种权衡。因此，我们<strong class="ll ir">使用“<strong class="ll ir">采集函数</strong>”来平衡</strong>这种折衷。</p><p id="fe90" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">我们的<strong class="ll ir">下一个选点(x ) </strong>应该有<strong class="ll ir">高均值</strong> ( <em class="mr">开采</em> ) &amp; <strong class="ll ir">高方差</strong> ( <em class="mr">勘探</em>)。</p><figure class="nz oa ob oc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ox"><img src="../Images/8fa9c7b968d2af4150b278943f9eb73b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PF8XTtgVm1UYTuRc3U2ePQ.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Choosing our next possible max point, with trade-off between <strong class="bd od">Exploration &amp; Exploitation.</strong></figcaption></figure><p id="602e" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">因为我们在寻找高方差分布中的下一个点。这意味着探索 x 点。</p><p id="467b" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">高均值意味着我们选择了具有高偏移/偏差的下一个点(x ),因此我们应该给出一些开发权重来降低下一个点的均值。</p><figure class="nz oa ob oc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oy"><img src="../Images/63b7f5a6f8267b22f3d5892a2cebf6e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6wykUbMfd2e1TSPr0gbagA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">The way we approach Bayesian Optimization.</figcaption></figure><p id="c354" class="pw-post-body-paragraph lj lk iq ll b lm md lo lp lq me ls lt kl mf lv lw kp mg ly lz kt mh mb mc li ij bi translated">上图给出了我们算法的一些直觉。红色曲线是我们的<strong class="ll ir">真实目标</strong>函数曲线。可惜我们不知道这个函数和它的方程。所以我们要用<strong class="ll ir">高斯过程来近似它。</strong>在我们的<strong class="ll ir">采样点</strong>(此处给出 4 个样本)中，我们绘制了一条与我们观察到的样本相符的直观/自信曲线。因此绿色区域显示了<strong class="ll ir">置信区域</strong>，在这里最有可能定位曲线点。根据上述<strong class="ll ir">先验知识</strong>，我们确定第二点(<strong class="ll ir"> f+ </strong>)为<strong class="ll ir">最大观测值</strong>。所以下一个最大值点应该在它上面或者等于它。我们在这里画一条蓝线。下一个最大值点应该在这条线以上。因此，从<strong class="ll ir"> f+ </strong>和<strong class="ll ir">置信区域的<strong class="ll ir">相交点</strong>，我们可以假设<strong class="ll ir"> f+ </strong>以下的曲线样本应该与我们寻找<strong class="ll ir"> arg max 的目标一起被丢弃。因此，现在我们已经缩小了调查范围。对下一个<strong class="ll ir">采样点继续同样的过程。</strong></strong></strong></p><figure class="nz oa ob oc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oz"><img src="../Images/412ee984832a72844f044b307c54275f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vh-KMqYVCotjmCrIW4OCwA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Data sample increases; comparison of different<strong class="bd od"> Acquisition functions</strong> with <strong class="bd od">Curve Fitting.</strong></figcaption></figure><h1 id="5fac" class="nb kd iq bd ke nc nd ne kh nf ng nh kk ni nj nk ko nl nm nn ks no np nq kw nr bi translated">参考:-</h1><figure class="nz oa ob oc gt jr"><div class="bz fp l di"><div class="pa pb l"/></div></figure></div></div>    
</body>
</html>