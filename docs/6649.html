<html>
<head>
<title>Discretisation Using Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用决策树的离散化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/discretisation-using-decision-trees-21910483fa4b?source=collection_archive---------3-----------------------#2018-12-24">https://towardsdatascience.com/discretisation-using-decision-trees-21910483fa4b?source=collection_archive---------3-----------------------#2018-12-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e96d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从连续到离散</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/cc0c46d5ba436742e334fae70438715e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yvT6c6_6sjLnZaOe"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@sherlockholmes26?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Alua Magzumova</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="9608" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">1.介绍</h1><p id="cb57" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">离散化是通过创建一组跨越变量值范围的连续区间，将连续变量转化为离散变量的过程。</p><h2 id="1706" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">1.1 离散化有助于处理异常值和高度偏斜的变量</h2><p id="6bff" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">离散化有助于处理异常值，方法是将这些值与分布的剩余内层值一起放入较低或较高的区间。因此，这些异常值不再与分布尾部的其余值不同，因为它们现在都在同一区间/时段中。此外，通过创建适当的箱或区间，离散化可以帮助将偏斜变量的值分散到一组具有相同数量观察值的箱中。</p><h2 id="33d4" class="mk kx iq bd ky ml mm dn lc mn mo dp lg lx mp mq li mb mr ms lk mf mt mu lm mv bi translated">1.2 离散化方法</h2><p id="99de" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">有几种方法可以将连续变量转化为离散变量。这个过程也被称为<strong class="lq ir">宁滨</strong>，每个面元就是每个区间。离散化方法分为两类:<strong class="lq ir">监督的和非监督的</strong>。</p><p id="23c7" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb">无监督方法</em> </strong> <em class="nb">除了变量分布</em>之外，不使用任何信息来创建将放置值的连续箱。</p><p id="c7da" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb">监督方法</em> </strong> <em class="nb">通常使用目标信息来创建仓或区间。</em></p><p id="08f0" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">在本文中，我们将只讨论使用决策树的监督离散化方法</p><p id="faaa" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">但是在进入下一步之前，让我们加载一个数据集，我们将在其上执行离散化。</p><h1 id="ee81" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">决策树离散化</h1><p id="2a98" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">决策树离散化包括使用决策树来确定最佳分裂点，这些分裂点将决定仓或连续区间:</p><p id="c076" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb">第一步</em> </strong>:首先它使用我们要离散化的变量训练一个有限深度(2、3 或 4)的决策树来预测目标。</p><p id="fa72" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb">第二步:</em> </strong>原来的变量值然后被树返回的概率代替。对于单个仓内的所有观察值，概率是相同的，因此用概率替换等同于在由决策树决定的截止范围内对观察值进行分组。</p><p id="76cc" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir">优点:</strong></p><ul class=""><li id="2ad0" class="nc nd iq lq b lr mw lu mx lx ne mb nf mf ng mj nh ni nj nk bi translated">决策树返回的概率预测与目标单调相关。</li><li id="12a4" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj nh ni nj nk bi translated">新的箱显示出降低的熵，这是因为每个桶/箱内的观察结果与其自身比与其他桶/箱的观察结果更相似。</li><li id="d19e" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj nh ni nj nk bi translated">该树会自动找到垃圾箱。</li></ul><p id="052e" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir">缺点:</strong></p><ul class=""><li id="2b95" class="nc nd iq lq b lr mw lu mx lx ne mb nf mf ng mj nh ni nj nk bi translated">这可能会导致过度拟合</li><li id="eb11" class="nc nd iq lq b lr nl lu nm lx nn mb no mf np mj nh ni nj nk bi translated">更重要的是，可能需要对树参数进行一些调整，以获得最佳分割(例如，深度、一个分区中样本的最小数量、分区的最大数量以及最小信息增益)。这很费时间。</li></ul><p id="3084" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">让我们看看如何<em class="nb">使用<a class="ae kv" href="https://www.kaggle.com/francksylla/titanic-machine-learning-from-disaster" rel="noopener ugc nofollow" target="_blank"> Titanic 数据集对决策树执行离散化</a></em>。</p><ol class=""><li id="db47" class="nc nd iq lq b lr mw lu mx lx ne mb nf mf ng mj nq ni nj nk bi translated"><strong class="lq ir"> <em class="nb">导入有用的库</em> </strong></li></ol><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="5809" class="mk kx iq ns b gy nw nx l ny nz">IN[1]:<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.model_selection import train_test_split</span></pre><p id="4624" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">2.<strong class="lq ir"> <em class="nb">加载数据集</em> </strong></p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="ea1b" class="mk kx iq ns b gy nw nx l ny nz">IN[2]:<br/>data = pd.read_csv('titanic.csv',usecols =['Age','Fare','Survived'])<br/>data.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/54de40da7104c735e99f65012ec7bfef.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*K61DbRHGnYWHHSSWTvesSQ.png"/></div></figure><p id="9755" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><em class="nb"> 3。</em> <strong class="lq ir"> <em class="nb">将数据分成训练集和测试集</em> </strong></p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="cc5c" class="mk kx iq ns b gy nw nx l ny nz">IN[3]:<br/>X_train, X_test, y_train, y_test = train_test_split(data[['Age', 'Fare', 'Survived']],data.Survived , test_size = 0.3)</span></pre><p id="60ed" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">因此，假设我们在数据集中没有缺失值(或者即使我们在数据集中有缺失数据，我们也对它们进行了估算)。我离开这一部分是因为我的主要目标是展示离散化是如何工作的。</p><p id="ebbf" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">那么，现在让我们将数据可视化，以便从中获得一些见解并理解变量</p><p id="1001" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb"> 4。让我们使用</em> </strong> <code class="fe ob oc od ns b"><strong class="lq ir"><em class="nb">age</em></strong></code> <strong class="lq ir"> <em class="nb">构建一个分类树来预测</em> </strong> <code class="fe ob oc od ns b"><strong class="lq ir"><em class="nb">Survived</em></strong></code> <strong class="lq ir"> <em class="nb">，以便离散化</em> </strong> <code class="fe ob oc od ns b"><strong class="lq ir"><em class="nb">age</em></strong></code> <strong class="lq ir"> <em class="nb">变量。</em>T41】</strong></p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="6cda" class="mk kx iq ns b gy nw nx l ny nz">IN[4]:<br/>tree_model = DecisionTreeClassifier(max_depth=2)</span><span id="29fa" class="mk kx iq ns b gy oe nx l ny nz">tree_model.fit(X_train.Age.to_frame(), X_train.Survived)</span><span id="b482" class="mk kx iq ns b gy oe nx l ny nz">X_train['Age_tree']=tree_model.predict_proba(X_train.Age.to_frame())[:,1] </span><span id="811a" class="mk kx iq ns b gy oe nx l ny nz">X_train.head(10)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/e0ee75eb4c778ed14b49c0ad117565ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*Gq3MN0cyMeREBRIM3fS7wQ.png"/></div></figure><p id="c76d" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">现在我们有了一个使用<code class="fe ob oc od ns b">age</code>变量预测<code class="fe ob oc od ns b">Survived</code>变量的分类模型。</p><p id="c18f" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">新创建的变量<code class="fe ob oc od ns b">Age_tree</code>包含数据点属于相应类别的概率</p><p id="e7f1" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb"> 5。检查</em> </strong> <code class="fe ob oc od ns b"><strong class="lq ir"><em class="nb">Age_tree</em></strong></code> <strong class="lq ir"> <em class="nb">变量</em> </strong>中唯一值的数量</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="b198" class="mk kx iq ns b gy nw nx l ny nz">IN[5]:<br/>X_train.Age_tree.unique()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/ea7f54fdd05784e8e48df87551a816c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*KaiN4NqBZ-f6bzLGcW5TLQ.png"/></div></figure><p id="4077" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb">为什么只有四种概率正确？</em>T53】</strong></p><p id="5758" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><em class="nb">在上面的输入 4 中，我们提到了</em> <code class="fe ob oc od ns b"><em class="nb">max_depth = 2.</em></code> <em class="nb">一个深度为 2 的树，进行了 2 次拆分，因此生成了 4 个桶，这就是为什么我们在上面的输出中看到 4 个不同的概率。</em></p><p id="d6e1" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir">6<em class="nb">6。检查离散变量</em> </strong> <code class="fe ob oc od ns b"><strong class="lq ir"><em class="nb">Age_tree</em></strong></code> <strong class="lq ir"> <em class="nb">和目标</em> </strong> <code class="fe ob oc od ns b"><strong class="lq ir"><em class="nb">Survived</em></strong></code> <strong class="lq ir"> <em class="nb">之间的关系。</em> </strong></p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="8d22" class="mk kx iq ns b gy nw nx l ny nz">IN[6]:<br/>fig = plt.figure()<br/>fig = X_train.groupby(['Age_tree'])['Survived'].mean().plot()<br/>fig.set_title('Monotonic relationship between discretised Age and target')<br/>fig.set_ylabel('Survived')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/78cc67930fe69821da88b53d2e0084c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*Vn6laLrSDoARZc1EidzwEg.png"/></div></figure><p id="527f" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">这里，我们可以看到离散变量<code class="fe ob oc od ns b">Age_tree</code>和目标变量<code class="fe ob oc od ns b">Survived</code>之间的单调关系。该图表明<code class="fe ob oc od ns b">Age_tree</code>似乎是目标变量<code class="fe ob oc od ns b">Survived</code>的一个良好预测器。</p><p id="b343" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb"> 7。检查每个概率桶/箱的乘客数量是否低于离散变量的分布。</em> </strong></p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="4c24" class="mk kx iq ns b gy nw nx l ny nz">IN[7]:<br/>X_train.groupby(['Age_tree'])['Survived'].count().plot.bar()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/23de72910ea038019697c5e9c55613cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*_DAoeQ4eID4cdVmOxi4HUA.png"/></div></figure><p id="3d1a" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">让我们通过捕获每个概率桶的最小和最大年龄来检查由树生成的年龄限制桶，以了解桶的界限。</p><p id="00e9" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb"> 8。检查树</em> </strong>生成的年龄限制桶</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="e811" class="mk kx iq ns b gy nw nx l ny nz">IN[7]:<br/>pd.concat( [X_train.groupby(['Age_tree'])['Age'].min(),<br/>            X_train.groupby(['Age_tree'])['Age'].max()], axis=1)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/7e8038cbe5c4b4661978c74a4c55640b.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*MKSmWmwTwgoURWEuQL6S7Q.png"/></div></div></figure><p id="fbc6" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">于是，决策树生成了桶:<code class="fe ob oc od ns b">0–11</code>、<code class="fe ob oc od ns b">12–15</code>、<code class="fe ob oc od ns b">16–63 </code>、<br/>、<code class="fe ob oc od ns b">46–80</code>，生存概率分别为<code class="fe ob oc od ns b">0.51</code>、<code class="fe ob oc od ns b">0.81</code>、<code class="fe ob oc od ns b">0.37</code>和<code class="fe ob oc od ns b">0.10</code>。</p><p id="4f45" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb"> 9。想象这棵树。</em> </strong></p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="28e5" class="mk kx iq ns b gy nw nx l ny nz">IN[8]:<br/>with open("tree_model.txt", "w") as f:<br/>    f = export_graphviz(tree_model, out_file=f)</span><span id="772c" class="mk kx iq ns b gy oe nx l ny nz">from IPython.display import Image<br/>from IPython.core.display import HTML <br/>PATH = "tree_visualisation.png"<br/>Image(filename = PATH , width=1000, height=1000)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/0f27e67452dae4f94796e8ef476089ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*62IfMbAl6tJR4crbWXFlTQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Tree Visualisation</figcaption></figure><p id="95d3" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">从图中可以看出，我们为<code class="fe ob oc od ns b">max_depth=2</code>获得了 4 个箱。</p><p id="2180" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">正如我前面提到的，我们可以使用决策树优化许多参数，以获得最佳的 bin 分裂。下面我将为演示优化树的深度。但是请记住，您还可以优化决策树的其余参数。访问<a class="ae kv" href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" rel="noopener ugc nofollow" target="_blank"> sklearn 网站</a>查看还有哪些参数可以优化。</p><p id="b75e" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb"> 10。选择树的最佳深度</em> </strong></p><p id="4c9b" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">我将构建不同深度的树，并将计算为每棵树的变量和目标确定的<em class="nb"> roc-auc </em>，然后我将选择产生最佳<em class="nb"> roc-auc </em>的深度</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="fa32" class="mk kx iq ns b gy nw nx l ny nz">IN[9]:<br/>score_ls = []     # here I will store the roc auc<br/>score_std_ls = [] # here I will store the standard deviation of the roc_auc</span><span id="a479" class="mk kx iq ns b gy oe nx l ny nz">for tree_depth in [1,2,3,4]:<br/>    tree_model = DecisionTreeClassifier(max_depth=tree_depth)<br/>    <br/>    scores = cross_val_score(tree_model, X_train.Age.to_frame(),       <br/>    y_train, cv=3, scoring='roc_auc')   <br/>    <br/>    score_ls.append(np.mean(scores))<br/>    <br/>    score_std_ls.append(np.std(scores))<br/>    <br/>temp = pd.concat([pd.Series([1,2,3,4]), pd.Series(score_ls), pd.Series(score_std_ls)], axis=1)</span><span id="fa8f" class="mk kx iq ns b gy oe nx l ny nz">temp.columns = ['depth', 'roc_auc_mean', 'roc_auc_std']</span><span id="5912" class="mk kx iq ns b gy oe nx l ny nz">print(temp)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/4d11f13a5ae5dc437c171d47c3b13000.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*sDIieW35SxP3zXmaBkekbg.png"/></div></figure><p id="3660" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">在这里，我们可以很容易地观察到，我们使用深度 1 或 2 获得了最佳的 roc-auc。我将选择 2 的<em class="nb">深度继续。</em></p><p id="5ea0" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb"> 11。使用</em> </strong>树变换  <code class="fe ob oc od ns b"><strong class="lq ir"><em class="nb">Age</em></strong></code> <strong class="lq ir"> <em class="nb">变量</em></strong></p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="2c44" class="mk kx iq ns b gy nw nx l ny nz">IN[10]:<br/>tree_model = DecisionTreeClassifier(max_depth=2)</span><span id="0755" class="mk kx iq ns b gy oe nx l ny nz">tree_model.fit(X_train.Age.to_frame(), X_train.Survived)</span><span id="2719" class="mk kx iq ns b gy oe nx l ny nz">X_train['Age_tree'] = tree_model.predict_proba(X_train.Age.to_frame())[:,1]</span><span id="a439" class="mk kx iq ns b gy oe nx l ny nz">X_test['Age_tree'] = tree_model.predict_proba(X_test.Age.to_frame())[:,1]</span></pre><p id="5801" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb"> 12。检查火车集合</em> </strong>中转换后的  <code class="fe ob oc od ns b"><strong class="lq ir"><em class="nb">age</em></strong></code> <strong class="lq ir"> <em class="nb">变量</em></strong></p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="8544" class="mk kx iq ns b gy nw nx l ny nz">IN[11]:<br/>X_train.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/fe7b6e7227c7a87e711eec169c2a7023.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*xQ9SIvxougypkgr6IGtvqw.png"/></div></figure><p id="f5b1" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb"> 13。检查列车组</em> </strong>中每个箱的唯一值</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="6f0b" class="mk kx iq ns b gy nw nx l ny nz">IN[12]:<br/>X_train.Age_tree.unique()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/bd20350d67bd603ad79d59d0bdd61931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*vyRxQdPI9TNcNuyCsr35Wg.png"/></div></figure><p id="ebff" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir"> <em class="nb"> 14。检查测试集</em> </strong>中转换后的  <code class="fe ob oc od ns b"><strong class="lq ir"><em class="nb">age</em></strong></code> <strong class="lq ir"> <em class="nb">变量</em></strong></p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="8a20" class="mk kx iq ns b gy nw nx l ny nz">IN[13]:<br/>X_test.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/00ada14cfa5565d2d7d3c074cd0838b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*4JdixN7ldKRHV4uzjy9S6g.png"/></div></figure><p id="b4ab" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated"><strong class="lq ir">15<em class="nb">。检查列车组</em> </strong>中每个箱的唯一值</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="ccbe" class="mk kx iq ns b gy nw nx l ny nz">IN[14]:<br/>X_test.Age_tree.unique()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/bd20350d67bd603ad79d59d0bdd61931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*vyRxQdPI9TNcNuyCsr35Wg.png"/></div></figure><p id="0e5a" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">现在，我们已经成功地将<code class="fe ob oc od ns b">Age</code>变量离散为四个离散值，这可能有助于我们的模型做出更好的预测。</p><p id="653d" class="pw-post-body-paragraph lo lp iq lq b lr mw jr lt lu mx ju lw lx my lz ma mb mz md me mf na mh mi mj ij bi translated">如果我们想要像<code class="fe ob oc od ns b">Fare</code>一样离散化剩余的变量，我们也可以执行相同的程序。</p></div></div>    
</body>
</html>