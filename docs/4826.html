<html>
<head>
<title>Machine Learning Basics with the K-Nearest Neighbors Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 K-最近邻算法的机器学习基础</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761?source=collection_archive---------0-----------------------#2018-09-10">https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761?source=collection_archive---------0-----------------------#2018-09-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="23e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">k-最近邻(KNN)算法是一种简单、易于实现的监督机器学习算法，可用于解决分类和回归问题。暂停！让我们打开它。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/e5baf574288b9e9104ebb7afee01f1c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zo9t7i-x7tyuzk3l0T5w6g.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">ABC. We are keeping it super simple!</figcaption></figure><h1 id="7f19" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">分解它</h1><p id="df28" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated"><strong class="jp ir">有监督的机器学习</strong>算法(与无监督的机器学习算法相反)是一种依靠有标签的输入数据来学习一个函数的算法，当给定新的无标签数据时，该函数产生适当的输出。</p><p id="632a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想象一台计算机是一个孩子，我们是它的监管人(例如父母、监护人或老师)，我们希望这个孩子(计算机)学习猪的样子。我们将向孩子展示几种不同的图片，其中一些是猪，其余的可以是任何东西的图片(猫、狗等)。</p><p id="a4b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们看到猪时，我们会喊“猪！”当它不是猪的时候，我们就喊“不，不是猪！”对孩子这样做几次后，我们给他们看一张图片，问“猪？”而且他们会正确地(大部分时间)说“猪！”或者“不，不是猪！”取决于图片是什么。那就是监督机器学习。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi me"><img src="../Images/cb80e6ec3d88456be665d6ebabae0a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IgWi454WdzlqKjKxTZvVNw.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">“Pig!”</figcaption></figure><p id="ad15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">监督机器学习算法用于解决分类或回归问题。</p><p id="0c33" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个<strong class="jp ir">分类问题</strong>有一个离散值作为它的输出。例如，“喜欢比萨饼上的菠萝”和“不喜欢比萨饼上的菠萝”是不连续的。没有中间地带。上面教孩子识别猪的类比是分类问题的另一个例子。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/7eb1e616b2a913e765d543690b904286.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*W3paYieyGIlD0uulkhn_Eg.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Image showing randomly generated data</figcaption></figure><p id="b90d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此图显示了分类数据的基本示例。我们有一个预测器(或一组预测器)和一个标签。在图中，我们可能试图根据某人的年龄(预测值)来预测他是否喜欢在披萨上放菠萝(1)或不喜欢(0)。</p><p id="fad7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">标准做法是将分类算法的输出(标签)表示为整数，如 1、-1 或 0。在这种情况下，这些数字纯粹是代表性的。不应该对它们进行数学运算，因为这样做没有意义。想一想。什么是“喜欢菠萝”+“不喜欢菠萝”？没错。我们不能将它们相加，所以我们不应该将它们的数值表示相加。</p><p id="66d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个<strong class="jp ir">回归问题</strong>的输出是一个实数(一个带小数点的数)。例如，我们可以使用下表中的数据，在给定身高的情况下估计某人的体重。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/9ab19e6a5b8c9ed1dbc567e3d2aaedb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*6P1K9Bd7-Fq9CujR0B6SQw.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Image showing a portion of the<a class="ae mh" href="http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights" rel="noopener ugc nofollow" target="_blank"> SOCR height and weights data set</a></figcaption></figure><p id="05f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回归分析中使用的数据看起来与上图中显示的数据相似。我们有一个独立变量(或一组独立变量)和一个因变量(给定我们的独立变量，我们试图猜测的东西)。例如，我们可以说身高是自变量，体重是因变量。</p><p id="2480" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，每行通常称为<strong class="jp ir">示例、观察值或数据点</strong>，而每列(不包括标签/因变量)通常称为<strong class="jp ir">预测值、维度、自变量或特征。</strong></p><p id="0308" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一种<strong class="jp ir">无监督机器学习</strong>算法利用没有任何标签的输入数据——换句话说，没有老师(标签)告诉孩子(计算机)什么时候是正确的，什么时候它犯了错误，以便它可以自我纠正。</p><p id="6b2e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">监督学习试图学习一个函数，该函数将允许我们在给定一些新的未标记数据的情况下进行预测，而非监督学习则不同，它试图学习数据的基本结构，以便让我们对数据有更多的了解。</p><h1 id="73c3" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">k-最近邻</h1><p id="734c" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">KNN 算法假设相似的事物存在于附近。换句话说，相似的事物彼此靠近。</p><blockquote class="mi"><p id="4308" class="mj mk iq bd ml mm mn mo mp mq mr kk dk translated">"物以类聚，人以群分."</p></blockquote><figure class="mt mu mv mw mx kq gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/1f51cf768cbd41bfcf428c8c2f59fe09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*wW8O-0xVQUFhBGexx2B6hg.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae mh" href="https://commons.wikimedia.org/wiki/File:Map1NNReducedDataSet.png" rel="noopener ugc nofollow" target="_blank">Image showing how similar data points typically exist close to each other</a></figcaption></figure><p id="a345" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意上图中，大多数时候，相似的数据点彼此接近。KNN 算法取决于这个假设是否足够真实，以使该算法有用。KNN 捕捉到了相似性(有时称为距离、接近度或紧密度)的概念，这与我们童年时可能学过的一些数学有关——计算图表上各点之间的距离。</p><p id="1f8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="my">注:</em> </strong> <em class="my">在继续之前，了解我们如何计算图上点之间的距离是必要的。如果您不熟悉或需要复习如何计算，请通读“</em> <a class="ae mh" href="https://www.mathsisfun.com/algebra/distance-2-points.html" rel="noopener ugc nofollow" target="_blank"> <em class="my">两点间距离</em> </a> <em class="my">”的全文，然后回来。</em></p><p id="0b76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还有其他计算距离的方法，根据我们要解决的问题，有一种方法可能更好。然而，直线距离(也称为欧几里德距离)是一个流行和熟悉的选择。</p><h2 id="2091" class="mz lc iq bd ld na nb dn lh nc nd dp ll jy ne nf lp kc ng nh lt kg ni nj lx nk bi translated">KNN 算法</h2><ol class=""><li id="45c1" class="nl nm iq jp b jq lz ju ma jy nn kc no kg np kk nq nr ns nt bi translated">加载数据</li><li id="3674" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">将 K 初始化为您选择的邻居数量</li></ol><p id="8b32" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.对于数据中的每个例子</p><p id="262a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.1 根据数据计算查询示例和当前示例之间的距离。</p><p id="fbad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.2 将示例的距离和索引添加到有序集合中</p><p id="3ebd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4.按距离从小到大(按升序)对距离和索引的有序集合进行排序</p><p id="18f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">5.从排序的集合中挑选前 K 个条目</p><p id="fea0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">6.获取所选 K 个条目的标签</p><p id="24cc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">7.如果是回归，返回 K 个标签的平均值</p><p id="2707" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">8.如果分类，返回 K 标签的模式</p><h2 id="ee3e" class="mz lc iq bd ld na nb dn lh nc nd dp ll jy ne nf lp kc ng nh lt kg ni nj lx nk bi translated">KNN 实现(从头开始)</h2><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nz oa l"/></div></figure><h2 id="43fc" class="mz lc iq bd ld na nb dn lh nc nd dp ll jy ne nf lp kc ng nh lt kg ni nj lx nk bi translated">选择正确的 K 值</h2><p id="76a8" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">为了选择最适合您的数据的 K，我们使用不同的 K 值运行 KNN 算法几次，并选择减少我们遇到的错误数量的 K，同时保持算法在给定以前从未见过的数据时准确做出预测的能力。</p><p id="ccdf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是一些需要记住的事情:</p><ol class=""><li id="cb88" class="nl nm iq jp b jq jr ju jv jy ob kc oc kg od kk nq nr ns nt bi translated">当我们把 K 值减小到 1 时，我们的预测变得不稳定。只需思考一分钟，想象 K=1，我们有一个由几个红色和一个绿色包围的查询点(我在想上面彩色图的左上角)，但绿色是单个最近邻。合理地说，我们会认为查询点最有可能是红色的，但是因为 K=1，KNN 错误地预测查询点是绿色的。</li><li id="7867" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">相反，当我们增加 K 的值时，由于多数投票/平均，我们的预测变得更加稳定，因此，更有可能做出更准确的预测(直到某一点)。最终，我们开始看到越来越多的错误。在这一点上，我们知道我们把 K 值推得太远了。</li><li id="a6aa" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">如果我们在标签中采取多数投票(例如，在分类问题中选择模式)，我们通常将 K 设为奇数，以进行平局决胜。</li></ol><h2 id="6d37" class="mz lc iq bd ld na nb dn lh nc nd dp ll jy ne nf lp kc ng nh lt kg ni nj lx nk bi translated">优势</h2><ol class=""><li id="b533" class="nl nm iq jp b jq lz ju ma jy nn kc no kg np kk nq nr ns nt bi translated">该算法简单，易于实现。</li><li id="2683" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">没有必要建立一个模型，调整几个参数，或作出额外的假设。</li><li id="604f" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">算法是通用的。它可以用于分类、回归和搜索(我们将在下一节中看到)。</li></ol><h2 id="1c86" class="mz lc iq bd ld na nb dn lh nc nd dp ll jy ne nf lp kc ng nh lt kg ni nj lx nk bi translated">不足之处</h2><ol class=""><li id="1d0a" class="nl nm iq jp b jq lz ju ma jy nn kc no kg np kk nq nr ns nt bi translated">随着示例和/或预测器/独立变量的数量增加，算法变得明显更慢。</li></ol><h1 id="1e62" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">实践中的 KNN</h1><p id="a3bb" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">随着数据量的增加，KNN 的主要缺点是速度明显变慢，这使得它在需要快速做出预测的环境中是不切实际的选择。此外，还有更快的算法可以产生更准确的分类和回归结果。</p><p id="1815" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，如果你有足够的计算资源来快速处理你用来做预测的数据，KNN 仍然可以用来解决那些依赖于识别相似物体的问题。这方面的一个例子是在推荐系统中使用 KNN 算法，这是 KNN 搜索的一个应用。</p><h2 id="44a5" class="mz lc iq bd ld na nb dn lh nc nd dp ll jy ne nf lp kc ng nh lt kg ni nj lx nk bi translated"><strong class="ak">推荐系统</strong></h2><p id="e167" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">从规模上看，这就像在亚马逊上推荐产品，在 Medium 上推荐文章，在网飞上推荐电影，或者在 YouTube 上推荐视频。不过，我们可以肯定的是，由于他们处理的数据量巨大，他们都使用了更有效的方式来提出建议。</p><p id="b8ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，我们可以使用我们在本文中学到的知识，在较小的规模上复制这些推荐系统中的一个。让我们来构建一个电影推荐系统的核心。</p><p id="1e14" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">我们试图回答什么问题？</strong></p><p id="9818" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">给定我们的电影数据集，与电影查询最相似的 5 部电影是什么？</p><p id="593d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">收集电影数据</strong></p><p id="633b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们在网飞、Hulu 或 IMDb 工作，我们可以从他们的数据仓库中获取数据。因为我们不在任何一家公司工作，所以我们必须通过其他途径获取数据。我们可以使用来自<a class="ae mh" href="https://archive.ics.uci.edu/ml/datasets/Movie" rel="noopener ugc nofollow" target="_blank"> UCI 机器学习库、</a> <a class="ae mh" href="https://www.imdb.com/interfaces/" rel="noopener ugc nofollow" target="_blank"> IMDb 数据集、</a>的一些电影数据，或者煞费苦心地创建我们自己的数据。</p><p id="1fff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">探索、清理和准备数据</strong></p><p id="7230" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">无论我们从哪里获得数据，都可能有一些问题需要我们去纠正，以便为 KNN 算法做准备。例如，数据可能不是算法预期的格式，或者可能有我们应该在将数据传送到算法之前填充或删除的缺失值。</p><p id="7e8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们上面的 KNN 实现依赖于结构化数据。它需要以表格的形式。此外，该实现假设所有列都包含数字数据，并且数据的最后一列有标签，我们可以对其执行一些功能。因此，无论我们从哪里获得数据，我们都需要使它符合这些约束。</p><p id="e4cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面的数据是我们清理后的数据的一个例子。该数据包含三十部电影，包括七个流派的每部电影的数据及其 IMDB 评级。labels 列全是零，因为我们没有使用这个数据集进行分类或回归。</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nz oa l"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Self-made movies recommendation data set</figcaption></figure><p id="eb9e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，当使用 KNN 算法时，电影之间存在不被考虑的关系(例如，演员、导演和主题)，这仅仅是因为捕获这些关系的数据从数据集中缺失。因此，当我们对我们的数据运行 KNN 算法时，相似性将只基于所包含的电影类型和 IMDB 评级。</p><p id="4830" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">使用算法</strong></p><p id="d8f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想象一下。我们正在浏览电影 Xb 网站，这是一个虚构的 IMDb 衍生网站，我们遇到了<a class="ae mh" href="https://www.imdb.com/title/tt6294822/?ref_=adv_li_tt" rel="noopener ugc nofollow" target="_blank"> <em class="my">《邮报》</em>。我们不确定是否想看，但它的类型吸引了我们；我们对其他类似的电影很好奇。我们向下滚动到“更像这样”部分，看看 MoviesXb 会做出什么推荐，算法齿轮开始转动。</a></p><p id="b097" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">MoviesXb 网站向其后端发送与帖子最相似的 5 部电影的请求。后端有一个和我们一模一样的推荐数据集。它首先为<em class="my">帖子</em>创建行表示(更好地称为<strong class="jp ir">特征向量</strong>),然后运行一个类似于下面的程序来搜索与<em class="my">帖子</em>最相似的 5 部电影，最后将结果发送回 MoviesXb 网站。</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="d5a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们运行这个程序时，我们看到 MoviesXb 推荐了<em class="my"> 12 年的奴隶</em>、<em class="my">钢锯岭</em>、<em class="my">卡特威女王</em>、<em class="my">风起</em>、<em class="my">美丽心灵</em>。既然我们完全理解了 KNN 算法是如何工作的，我们就能够准确地解释 KNN 算法是如何提出这些建议的。恭喜你！</p><h1 id="4494" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">摘要</h1><p id="2cf8" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">k-最近邻(KNN)算法是一种简单的监督机器学习算法，可用于解决分类和回归问题。它易于实现和理解，但有一个主要缺点，即随着所用数据的增长，速度会明显变慢。</p><p id="97e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">KNN 的工作方式是找出查询和数据中所有示例之间的距离，选择最接近查询的指定数量的示例(K)，然后投票选择最频繁的标签(在分类的情况下)或平均标签(在回归的情况下)。</p><p id="63b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在分类和回归的情况下，我们看到为我们的数据选择正确的 K 是通过尝试几个 K 并挑选一个最有效的来完成的。</p><p id="0dd2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们看了一个 KNN 算法如何用于推荐系统的例子，这是 KNN 搜索的一个应用。</p></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/3597a89a5e67b9dd73aff28a9c604b92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*AuXDgGrr0wbCoF6KDXXSZQ.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">KNN Be Like… ”Show me your friends, and I’ll tell you who you are.”</figcaption></figure><h1 id="24c8" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">补遗</strong></h1><p id="242b" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">[1]为了简单起见，本文中实现的 KNN 电影推荐器不处理电影查询可能是推荐数据集的一部分的情况。这在生产系统中可能是不合理的，应该适当地处理。</p><h2 id="785b" class="mz lc iq bd ld na nb dn lh nc nd dp ll jy ne nf lp kc ng nh lt kg ni nj lx nk bi translated">如果你学到了新的东西或者喜欢阅读这篇文章，请鼓掌👏并分享给其他人看。也可以随意发表评论。</h2></div></div>    
</body>
</html>