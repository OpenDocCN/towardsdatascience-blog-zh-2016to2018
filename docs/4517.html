<html>
<head>
<title>Everything you need to know about AutoML and Neural Architecture Search</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于 AutoML 和神经结构搜索你需要知道的一切</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/everything-you-need-to-know-about-automl-and-neural-architecture-search-8db1863682bf?source=collection_archive---------1-----------------------#2018-08-21">https://towardsdatascience.com/everything-you-need-to-know-about-automl-and-neural-architecture-search-8db1863682bf?source=collection_archive---------1-----------------------#2018-08-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><blockquote class="jq jr js"><p id="1249" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">想获得灵感？快来加入我的<a class="ae ks" href="https://www.superquotes.co/?utm_source=mediumtech&amp;utm_medium=web&amp;utm_campaign=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="jw iu">超级行情快讯</strong> </a>。😎</p></blockquote><p id="adc3" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">AutoML 和神经架构搜索(NAS)是深度学习城堡的新国王。它们是在不做太多工作的情况下，为您的机器学习任务获得巨大准确性的快速而肮脏的方法。简单有效；这就是我们想要的人工智能！</p><p id="12f3" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">那么它是如何工作的呢？你如何使用它？今天你有什么选择来驾驭这种力量呢？</p><p id="34ff" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">这里是您需要了解的关于 AutoML 和 NAS 的所有信息。</p><h1 id="cc6e" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">神经结构搜索(NAS)</h1><p id="6057" class="pw-post-body-paragraph jt ju it jw b jx lu jz ka kb lv kd ke kt lw kh ki ku lx kl km kv ly kp kq kr im bi translated">开发神经网络模型通常需要大量的架构工程。有时你可以通过<a class="ae ks" href="http://cs231n.github.io/transfer-learning/" rel="noopener ugc nofollow" target="_blank">转移学习</a>，但是如果你真的想要最好的性能，通常最好设计你自己的网络。这需要专业技能(从商业的角度来看，这很昂贵,),总体来说具有挑战性；我们甚至可能不知道当前最先进技术的极限！这需要反复试验，而且实验本身既耗时又昂贵。</p><p id="372e" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">这就是 NAS 的用武之地。NAS 是<em class="jv">搜索</em>寻找最佳<em class="jv">神经网络架构</em>的算法。大多数算法都是这样工作的。首先定义一组可能用于我们网络的“构建块”。例如，最先进的<a class="ae ks" href="https://arxiv.org/pdf/1707.07012.pdf" rel="noopener ugc nofollow" target="_blank"> NASNet 论文</a>提出了图像识别网络的这些常用模块:</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/c7cd835ae9aaea6bff1a5a9fb1e85550.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*smVaUym_m0MWjRgR2hO8Uw.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">NASNet blocks for image recognition network</figcaption></figure><p id="4b56" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">在 NAS 算法中，控制器递归神经网络(RNN)对这些构建块进行采样，将它们放在一起以创建某种端到端架构。这种架构通常体现了与最新网络相同的风格，如<a class="ae ks" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank"> ResNets </a>或<a class="ae ks" href="https://arxiv.org/pdf/1608.06993.pdf" rel="noopener ugc nofollow" target="_blank"> DenseNets </a>，但使用了非常不同的块组合和配置。</p><p id="bf97" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">然后，这个新的网络体系结构被训练收敛，以在保留的验证集上获得一定的准确性。由此产生的精度用于更新控制器，使得控制器将随着时间的推移产生更好的架构，可能通过选择更好的块或进行更好的连接。控制器权重用策略梯度更新。完整的端到端设置如下所示。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/a6e7187aa6c216a147babebbd1036257.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*iGQabhVn-pUJ089ZHQyd1g.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">The NAS algorithm</figcaption></figure><p id="b032" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">这是一种相当直观的方法！简而言之:让一个算法抓取不同的块，并将这些块放在一起组成一个网络。训练并测试网络。根据您的结果，调整您用来制作网络的块以及如何将它们放在一起！</p><p id="fc46" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">这个算法成功的部分原因是因为它的约束和假设。NAS 发现的体系结构是在比真实世界小得多的数据集上训练和测试的。这样做是因为在像 ImageNet 这样的大型系统上进行训练需要很长时间。但是，这个想法是，在较小但结构相似的数据集上表现更好的网络也应该在较大和更复杂的数据集上表现更好，这在深度学习时代通常是正确的。</p><p id="16e7" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">第二，是搜索空间本身相当有限。NAS 旨在构建在风格上与当前最先进的体系结构非常相似的体系结构。对于图像识别，这是指在网络中有一组重复的块，同时进行逐步下采样，如下左图所示。在当前的研究中，选择一组积木来建造重复积木也是非常普遍的。NAS 发现网络的主要部分是块如何连接在一起。</p><p id="e8c8" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">在右下方查看 ImageNet 网络的最佳发现块和结构。有趣的是注意到它们是如何包含看起来相当随机的混合运算，包括许多可分离的卷积。</p><div class="ma mb mc md gt ab cb"><figure class="mm me mn mo mp mq mr paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><img src="../Images/22493de47547d95d8ab30484a3f6e844.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*bj0RdUdJr92dpX1tJgxdcA.png"/></div></figure><figure class="mm me mw mo mp mq mr paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><img src="../Images/8bb48dbc08466eefc35430bbeb7ff5ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1426/format:webp/1*ChLWM5j4bUnC2HvpL24Pqw.png"/></div></figure></div><h1 id="d554" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">建筑搜索的进展</h1><p id="e7cc" class="pw-post-body-paragraph jt ju it jw b jx lu jz ka kb lv kd ke kt lw kh ki ku lx kl km kv ly kp kq kr im bi translated">NASNet 的论文具有惊人的进步性，因为它提供了深度学习研究的新方向。不幸的是，它的效率很低，而且谷歌之外的普通用户无法访问。使用<em class="jv">450 GPU</em>花了<em class="jv">3-4 天</em>的训练才找到那个伟大的架构。因此，NAS 的许多最新研究都集中在使这一过程更加高效。</p><p id="b56a" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated"><a class="ae ks" href="https://arxiv.org/pdf/1712.00559.pdf" rel="noopener ugc nofollow" target="_blank">渐进式神经架构搜索(PNAS) </a>提出使用所谓的基于序列模型的优化(SMBO)策略，而不是 NASNet 中使用的强化学习。使用 SMBO，我们不是随机地从外部集合中抓取和尝试块，而是测试块并按照复杂度增加的顺序搜索结构。这不会缩小搜索空间，但它确实使搜索以更智能的方式进行。SMBO 基本上是在说:<em class="jv">不要一次尝试所有的事情，让我们从简单开始，只在需要的时候变得复杂。</em>这种 PNAS 方法的效率是原始 NAS 的 5 到 8 倍(因此成本更低)。</p><p id="c3bc" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated"><a class="ae ks" href="https://arxiv.org/pdf/1802.03268.pdf" rel="noopener ugc nofollow" target="_blank">高效的神经架构搜索(ENAS) </a>是另一种尝试，试图使一般的架构搜索更有效，这一次普通的 GPU 从业者都可以使用。作者的假设是，NAS 的计算瓶颈是训练每个模型收敛，只测量其测试精度，然后<em class="jv">扔掉所有训练的权重</em>。</p><p id="12b5" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">研究和实践已经反复表明，迁移学习有助于在短时间内达到高精度，因为为有些相似的任务训练的网络发现相似的权重；迁移学习基本上只是网络权重的转移。ENAS 算法迫使所有模型共享权重，而不是从零开始训练到收敛。我们以前在以前的模型中尝试过的任何块都将使用那些以前学习的权重。因此，每次我们训练一个新的模型，我们本质上都在做一个<em class="jv">转移学习</em>，收敛得更快！</p><p id="1631" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">论文中的表格显示了 ENAS 使用单个 1080Ti GPU 半天的训练效率有多高。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/54ee82ab1a36cf4e7cabd9722e63558b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*N5U5tsi1cXWIssb8zxHfCA.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Performance and efficiency of ENAS</figcaption></figure><h1 id="bc37" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">深度学习的新方式:AutoML</h1><p id="65b7" class="pw-post-body-paragraph jt ju it jw b jx lu jz ka kb lv kd ke kt lw kh ki ku lx kl km kv ly kp kq kr im bi translated">许多人将 AutoML 称为进行深度学习的新方式，这是整个系统的一种变化。我们不用设计复杂的深度网络，只需运行预设的 NAS 算法。谷歌最近将这一点发挥到了极致，推出了<a class="ae ks" href="https://cloud.google.com/automl/" rel="noopener ugc nofollow" target="_blank">云自动化</a>。只需上传你的数据，谷歌的 NAS 算法就会为你找到一个架构，又快又简单！</p><p id="2ca4" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">AutoML 的想法是简单地抽象出深度学习的所有复杂部分。你需要的只是数据。让 AutoML 来做网络设计的难点就好了！深度学习就像其他工具一样变成了一个插件工具。获取一些数据，并自动创建一个由复杂的神经网络驱动的决策函数。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi my"><img src="../Images/85f3d9f96a709f4e97832ea8ce1642b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lMPBDUDldrjVJf6ZI3mZWA.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Google Cloud’s AutoML pipeline</figcaption></figure><p id="b830" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">Cloud AutoML 的价格高达 20 美元，不幸的是，一旦你的模型被训练好，你就不能导出它；你必须使用他们的 API 在云上运行你的网络。有几个完全免费的替代方案，但是需要多做一点工作。</p><p id="f5e8" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">AutoKeras 是一个使用 ENAS 算法的 GitHub 项目。它可以使用 pip 安装。因为它是用 Keras 编写的，所以很容易控制和使用，所以你甚至可以深入 ENAS 算法并尝试做一些修改。如果你更喜欢 TensorFlow 或 Pytorch，这里还有那些<a class="ae ks" href="https://github.com/melodyguan/enas" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae ks" href="https://github.com/carpedm20/ENAS-pytorch" rel="noopener ugc nofollow" target="_blank">这里</a>的公共代码项目！</p><p id="aa68" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">总的来说，现在有几种选择来使用 AutoML。这只是取决于你是否会玩弄你想要的算法，以及你愿意付出多少来获得更多的代码抽象出来。</p><h1 id="c36e" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">NAS 和 AutoML 的未来预测</h1><p id="1e87" class="pw-post-body-paragraph jt ju it jw b jx lu jz ka kb lv kd ke kt lw kh ki ku lx kl km kv ly kp kq kr im bi translated">很高兴看到过去几年在自动化深度学习方面取得的长足进步。它使用户和企业更容易接触到它；深度学习的力量变得越来越容易为大众所接受。但是，总有一些改进的空间。</p><p id="eca9" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">建筑搜索变得更加高效；像 ENAS 一样在一天的训练中找到一个只有一个 GPU 的网络是非常令人惊讶的。然而，我们的搜索空间仍然非常有限。当前的 NAS 算法仍然使用手工设计的结构和构建块，它们只是以不同的方式将它们放在一起！</p><p id="b276" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">一个强大的和潜在的突破性的未来方向将是更广泛的搜索，真正寻找新的架构。这种算法可能会揭示这些庞大而复杂的网络中隐藏的更多深度学习秘密。当然，这样的搜索空间需要高效的算法设计。</p><p id="8405" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">NAS 和 AutoML 的这一新方向为人工智能社区提供了令人兴奋的挑战，也为科学领域的另一次突破提供了机会。</p></div></div>    
</body>
</html>