<html>
<head>
<title>Road scene understating using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习的道路场景理解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/road-scene-understating-using-deep-learning-c3610f6b1c4?source=collection_archive---------2-----------------------#2017-09-03">https://towardsdatascience.com/road-scene-understating-using-deep-learning-c3610f6b1c4?source=collection_archive---------2-----------------------#2017-09-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jn jo jp jq"><div class="bz fp l di"><div class="jr js l"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">The final outcome of applying semantic segmentation on road scene</figcaption></figure><h1 id="06d6" class="jx jy iq bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">场景理解</h1><p id="f33d" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">语义分割代表了深度学习中的一种技术，其中我们通过将图像中的每个像素分配给预定义的类别集来为其分配意义。从上面的 GIF 中，我们可以看到我们在语义分割过程中有两个类(<strong class="kx ir"> road </strong>和<strong class="kx ir"> not road </strong>)被相应地着色。</p><p id="72c5" class="pw-post-body-paragraph kv kw iq kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">使用语义分割的方法，我们能够将场景分解成我们特别感兴趣的片段。这对于自动驾驶汽车尤其重要，因为它将能够检测场景的哪个部分是可驾驶区域。</p><h1 id="29ae" class="jx jy iq bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">这是如何工作的？</h1><p id="f28d" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">语义分割的一种方法是使用全卷积网络(FCN)。加州大学伯克利分校有一篇很棒的<a class="ae ly" href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>，详细解释了方法论和架构。我们的实现也将基于本文。</p><p id="826c" class="pw-post-body-paragraph kv kw iq kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">典型的卷积网络由几个卷积层和完全连接的层组成。这种类型的架构适用于分类问题(例如:<em class="lz">这是一条路吗？</em>)。但是为了在一幅图像上定位道路(<em class="lz">道路在哪里？</em>问题)我们需要网络能够保存空间信息。这就是 FCN 发挥作用并提供最先进的语义分割结果的地方。</p><h1 id="3d2f" class="jx jy iq bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">如何建设 FCN？</h1><p id="00d5" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">建造 FCN 有三种主要技术:</p><ol class=""><li id="668a" class="ma mb iq kx b ky lt lc lu lg mc lk md lo me ls mf mg mh mi bi translated">用 1x1 卷积层替换全连接层。</li><li id="79e5" class="ma mb iq kx b ky mj lc mk lg ml lk mm lo mn ls mf mg mh mi bi translated">通过使用转置卷积层引入上采样。</li><li id="247f" class="ma mb iq kx b ky mj lc mk lg ml lk mm lo mn ls mf mg mh mi bi translated">添加跳过连接。</li></ol><p id="4549" class="pw-post-body-paragraph kv kw iq kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">下面我们来详细看看如何做到这一点。</p><p id="e2d1" class="pw-post-body-paragraph kv kw iq kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">我们的 FCN 架构有两部分:<strong class="kx ir">编码器</strong>和<strong class="kx ir">解码器</strong>。</p><figure class="mp mq mr ms gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mo"><img src="../Images/8d56300c8a85b0dc31dd2e137d6042b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Li8osvpQE-s0AYO8cPumFQ.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">FCN architecture</figcaption></figure><p id="7317" class="pw-post-body-paragraph kv kw iq kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">对于<strong class="kx ir">编码器</strong>，我们将使用在 ImageNet 上预先训练的 VGG16 模型。VGG16 模型中完全连接的层将替换为卷积<strong class="kx ir"> (1) </strong>。编码器从图像中提取将被解码器使用的特征。</p><p id="2c1b" class="pw-post-body-paragraph kv kw iq kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">下一步是构建<strong class="kx ir">解码器</strong>，这是通过使用转置卷积层<strong class="kx ir"> (2) </strong>对最后一个编码器层进行上采样来完成的。转置卷积本质上是一种逆卷积。</p><figure class="mp mq mr ms gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mz"><img src="../Images/0db9b770fd6322e5eaa3332691a3dd3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FhkyU4-fusf4pJaMIm2HKw.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">FCN architecture based on pre-trained model, replaced fully connected layers and transposed convolutions</figcaption></figure><p id="3099" class="pw-post-body-paragraph kv kw iq kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">最后，我们将在特定层之间添加跳过层<strong class="kx ir"> (3) </strong>，使网络能够使用多种分辨率。这反过来提供了更好的分割结果。跳过连接可以减少信息丢失，使网络能够“看到全局”，因为典型的卷积网络只关注图像的一部分。</p><figure class="mp mq mr ms gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi na"><img src="../Images/42229320f9edb9ea82effff866f1b70f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-K3rRQuCfTkFcOuBLOfDPA.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Skip connections in FCN</figcaption></figure><p id="bd7f" class="pw-post-body-paragraph kv kw iq kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">我们在添加跳过连接时应该小心，因为它们会极大地影响模型的大小。</p><h1 id="41f0" class="jx jy iq bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">最后结局</h1><p id="f2a1" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">用于道路场景理解的 FCN 的实施是作为<a class="ae ly" href="http://udacity.com/drive" rel="noopener ugc nofollow" target="_blank"> Udacity 自动驾驶纳米学位项目</a>的一部分完成的。我已经使用 Python 和 TensorFlow 建立和训练了模型。该模型已经在 NVIDIA 1080Ti 显卡上进行了 30 次训练，耗时不到半小时。</p><p id="b7f8" class="pw-post-body-paragraph kv kw iq kx b ky lt la lb lc lu le lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">这是推理的输出:</p><figure class="mp mq mr ms gt jq gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/91045eede32972199fc050b560e23945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*Vsgt82oDuJlSPWGKPcHX1Q.png"/></div></figure><h1 id="e8ef" class="jx jy iq bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">结论</h1><p id="4da8" class="pw-post-body-paragraph kv kw iq kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">相关的源代码可以在我的 GitHub 个人资料中找到:</p><div class="nc nd gp gr ne nf"><a href="https://github.com/bdjukic/CarND-Semantic-Segmentation" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd ir gy z fp nk fr fs nl fu fw ip bi translated">bdjukic/CarND-语义分段</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">通过在 GitHub 上创建一个帐户，为 CarND-Semantic-Segmentation 开发做出贡献。</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">github.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt mx nf"/></div></div></a></div></div></div>    
</body>
</html>