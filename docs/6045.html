<html>
<head>
<title>Speedy Computer Vision Pipelines using Parallelism</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用并行技术的快速计算机视觉流水线</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speedy-cv-pipelines-using-parallelism-d7bebad2ff5f?source=collection_archive---------12-----------------------#2018-11-22">https://towardsdatascience.com/speedy-cv-pipelines-using-parallelism-d7bebad2ff5f?source=collection_archive---------12-----------------------#2018-11-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="bdaf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你曾经使用 OpenCV、MoviePy 或任何其他海量的库编写过自己的代码来处理视频，你可能会面临处理速度非常慢的问题。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/326c695fc046bebf465c1e19aad00c90.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*HUvcbOaGZ3Pnp-wnVGV1zQ.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Unfortunate is ze who has to wait 20 minutes to watermark a 5 minute 4k video</figcaption></figure><p id="b160" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是因为我们编写的代码基本上是为了在单核上运行而编写的。我们有一个庞大的任务，需要处理大量的数据，没有现成的解决方案可以简化我们的生活。然而，我们想要的是一个完全不同的故事。蠢朋克的一些歌词最好地概括了这一点</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kx"><img src="../Images/b673537de70f7692752f033dfe651361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*UPceRnPH_ZB2H7tM1Nv55A.gif"/></div></figure><blockquote class="ky"><p id="752b" class="kz la iq bd lb lc ld le lf lg lh kk dk translated">努力吧，努力吧…..更难、更好、更快、更强</p></blockquote><p id="1897" class="pw-post-body-paragraph jn jo iq jp b jq li js jt ju lj jw jx jy lk ka kb kc ll ke kf kg lm ki kj kk ij bi translated">看到<em class="ln">更快这个词，</em>你脑子里第一个想到的可能就是排比。当我们考虑并行视频处理时，这是必须的，对吗？</p><h1 id="1db2" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">方法 1</h1><p id="70ee" class="pw-post-body-paragraph jn jo iq jp b jq mm js jt ju mn jw jx jy mo ka kb kc mp ke kf kg mq ki kj kk ij bi translated">我们可以将帧一个接一个地分配给空闲的内核。似乎是一个简单而明显的解决方案，不是吗？</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/8c1d293ebe35f93a042ac96286db27cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*TRk5F-IOFWBRK5aZ7NgXvg.gif"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">First thoughts: How to NOT parallelize video processing across cores</figcaption></figure><p id="fd09" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而不幸的是，这并不十分奏效。事实上，在很多情况下，这最终会比我们简单的单核代码的性能更差。要了解原因，我们必须深入了解视频本身是如何存储的。由于视频编码是顺序的，当一个内核解码一帧时，其他内核必须处于空闲状态。他们不能开始处理下一帧，直到该核心解码了前一帧(<a class="ae ms" href="https://www.youtube.com/watch?v=qbGQBT2Vwvc" rel="noopener ugc nofollow" target="_blank">这里是</a>一个关于视频压缩如何工作的技术快速介绍)。</p><p id="4ac1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">像<code class="fe mt mu mv mw b">joblib</code>这样的库也试图以类似的方式并行化作业，因此没有提供我们希望看到的那种加速。这意味着我们必须寻找更好的替代方案</p><h1 id="9191" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">方法 2</h1><p id="ea10" class="pw-post-body-paragraph jn jo iq jp b jq mm js jt ju mn jw jx jy mo ka kb kc mp ke kf kg mq ki kj kk ij bi translated">聪明的阿德里安在他的博客中强调了另一种加速视频处理的方法。在这篇博文中，他讨论了将帧解码过程转移到另一个线程，并存储解码后的帧，直到主处理线程需要检索它。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/1bcf0c1fd93e2fb399780ff93a447609.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*KhJQqNL1_6KSeeQHsrLA9Q.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">New frames are decoded in a dedicated thread and enqueued to the back of a Queue. They are then dequeued from the front of the list as and when needed by the main processing thread (source: <a class="ae ms" href="https://www.pyimagesearch.com/2017/02/06/faster-video-file-fps-with-cv2-videocapture-and-opencv/" rel="noopener ugc nofollow" target="_blank">pyimagesearch</a>)</figcaption></figure><p id="f207" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尽管这本身是一个绝妙的想法，但是这种方法的一个很大的局限性是它只能同时利用机器的两个线程/内核。此外，出于某种原因，Adrian 博客中的代码似乎没有直接扩展到 Python 3(参见帖子上的评论。或许 Python 改变了他们的<code class="fe mt mu mv mw b">multithreading</code>库的内部工作方式？)不管怎样，如果我们朝这个方向努力，我们应该能够达到一个水平，至少比我们开始的时候稍微好一点。</p><p id="f633" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是仍然没有达到我们期望的速度。我们希望让 Threadripper 或 Xeon 在同一视频中大规模分配工作负载。为了能够做到这一点，让我们回头看看方法 1。我们第一种方法的主要问题是内核之间的相互依赖。这是不可避免的，因为视频编码从根本上来说是“分块”的。</p><p id="3f6e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，从逻辑上讲，通过强制内核在完全不同的预分配片段上工作，应该很容易解决这个问题。这是我们最后的方法</p><h1 id="dac0" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">方法 3</h1><p id="1296" class="pw-post-body-paragraph jn jo iq jp b jq mm js jt ju mn jw jx jy mo ka kb kc mp ke kf kg mq ki kj kk ij bi translated">为了做到这一点，我们所要做的就是让每个内核寻找视频的一个完全不同的片段，并对其进行操作</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/ea8ba8b98384417a775aaa5dd0be05b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*slNyV6WPLoZXd__KJZNVMg.gif"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">A better way to distribute video processing across your cores</figcaption></figure><p id="52e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设我们有 10，000 帧和 4 个内核，这意味着我们将每个内核专用于这 10，000 帧中固定的<strong class="jp ir">连续</strong>季度(=2500)。这样，内核就不必相互等待来处理下一帧。在此之后，剩下的唯一任务是以正确的顺序重新组装已处理的帧。遵循这种方法可以让我们轻松地并行处理我们的视频处理流水线</p><p id="4141" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 Python 中做到这一点非常简单。作为参考，这可能是您的普通视频处理代码的样子:</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="my mz l"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">A normal code to perform an operation on a video</figcaption></figure><p id="9347" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们必须并行化这段代码，我们必须使用 python <code class="fe mt mu mv mw b">multiprocessing</code>库</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="my mz l"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Parallelizing the video processing by distributing video fragments across cores</figcaption></figure><p id="8f8d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大部分已经完成了。然而，仍然遗漏了一个重要的部分。我们只剩下加工过的碎片。我们仍然需要合并这些碎片。这可以通过以下方式使用<code class="fe mt mu mv mw b">ffmpeg</code>轻松完成:</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="my mz l"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Merging the video fragments that are generated by our parallel processing code</figcaption></figure><p id="27f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们做到了！一种并行化视频处理管道的方法，可根据我们投入的内核数量进行扩展。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi na"><img src="../Images/96be77301a234030be4e62d05d9913e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*ctPDfnX9fPzpSFM98nPfeA.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">A speed-up of <strong class="bd nb">6x </strong>is observed with 6 cores and <strong class="bd nb">2x </strong>with 2 cores for the simple task of video blurring</figcaption></figure><p id="0c9c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你不必相信我的话，:D。自己试试吧，在下面的评论中告诉我。所有的代码都可以在 https://github.com/rsnk96/fast-cv 的 GitHub 中找到</p><p id="993b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">备注:</strong></p><ul class=""><li id="5b34" class="nc nd iq jp b jq jr ju jv jy ne kc nf kg ng kk nh ni nj nk bi translated">如果你想知道<code class="fe mt mu mv mw b">cv2.set(CAP_PROP_POS_FRAMES)</code>如何跳到一个特定的帧，而不必使用前面的帧解码，这是因为它跳到了最近的关键帧。查看视频链接，更好地理解这一点</li><li id="8710" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated"><code class="fe mt mu mv mw b">cv2.set(CAP_PROP_POS_FRAMES)</code> <a class="ae ms" href="https://github.com/opencv/opencv/issues/9053" rel="noopener ugc nofollow" target="_blank">已知</a>没有准确寻找指定帧。这可能是因为它寻找最近的关键帧。这意味着在视频片段的地方可能会有几帧重复。所以不建议对关键帧用例采用这种方法。</li><li id="6bea" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated">最佳性能实际上可能通过方法 2 和 3 的组合来获得。但是要把它编码起来需要更多的努力。如果有人设法拼凑出一个代码，请在评论中告诉我！:)</li><li id="e310" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated">**:包含对<code class="fe mt mu mv mw b">if ret==False: break</code>的检查是正常的做法，但是为了简单起见，我在这里避免了它</li><li id="2a8e" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated">这篇文章是我最近在 Pysangamam 和 Pycon India 发表的演讲的摘要。<a class="ae ms" href="https://www.youtube.com/watch?v=v29nBvfikcE" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=v29nBvfikcE</a></li></ul></div></div>    
</body>
</html>