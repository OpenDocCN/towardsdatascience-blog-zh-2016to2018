<html>
<head>
<title>LDA Topic Modeling: An Explanation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LDA 主题建模:一种解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lda-topic-modeling-an-explanation-e184c90aadcd?source=collection_archive---------2-----------------------#2018-06-24">https://towardsdatascience.com/lda-topic-modeling-an-explanation-e184c90aadcd?source=collection_archive---------2-----------------------#2018-06-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/f6c00ce037dc0a4298625ae118b9a60c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4AWyn46J0Ckua1do"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@impatrickt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Patrick Tomasso</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="709d" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">背景</h2><p id="b2c9" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">主题建模是在一组文档中识别主题的过程。这对于搜索引擎、客户服务自动化以及任何其他了解文档主题很重要的情况都很有用。有多种方法可以做到这一点，但在这里我将解释一种:<strong class="le iu">潜在狄利克雷分配(LDA)。</strong></p><h2 id="1559" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">该算法</h2><p id="577a" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">LDA 是一种无监督学习的形式，它将文档视为单词包(即顺序无关紧要)。LDA 的工作方式是首先做出一个关键的假设:生成文档的方式是挑选一组主题，然后为每个主题挑选一组单词。现在你可能会问“好吧，那么它是如何找到主题的？”答案很简单:它逆向工程这个过程。为此，它为每个文档<em class="lx"> m </em>执行以下操作:</p><ol class=""><li id="39f6" class="ly lz it le b lf ma lj mb kp mc kt md kx me lw mf mg mh mi bi translated">假设所有文档中都有<em class="lx"> k </em>个主题</li><li id="fbc2" class="ly lz it le b lf mj lj mk kp ml kt mm kx mn lw mf mg mh mi bi translated">通过给每个单词分配一个主题，在文档<em class="lx"> m </em>中分布这些<em class="lx"> k </em>主题(这种分布称为α，可以是对称的，也可以是非对称的，稍后会详细介绍)。</li><li id="a9d6" class="ly lz it le b lf mj lj mk kp ml kt mm kx mn lw mf mg mh mi bi translated">对于文档<em class="lx"> m </em>中的每个单词<em class="lx"> w </em>，假设其主题是错误的，但是每隔一个单词被分配正确的主题。</li><li id="2ba1" class="ly lz it le b lf mj lj mk kp ml kt mm kx mn lw mf mg mh mi bi translated">基于以下两点从概率上给单词<em class="lx"> w </em>分配一个主题:<br/> -文档<em class="lx"> m <br/> - </em>有多少次<em class="lx"> </em>单词<em class="lx"> w </em>在所有文档中被分配了一个特定的主题(这种分布被称为<em class="lx"> β </em>，稍后将详细介绍)</li><li id="57e8" class="ly lz it le b lf mj lj mk kp ml kt mm kx mn lw mf mg mh mi bi translated">对每个文档重复这个过程几次，你就完成了！</li></ol><h2 id="6f1b" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">模型</h2><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/10c9a13f74324ac5c7b6c500f007e38e.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*VTHd8nB_PBsDtd2hd87ybg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Smoothed LDA from <a class="ae kf" href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation</a></figcaption></figure><p id="0692" class="pw-post-body-paragraph lc ld it le b lf ma lh li lj mb ll lm kp mt lo lp kt mu lr ls kx mv lu lv lw im bi translated">上面是所谓的 LDA 模型的板块图，其中:<br/> α是每个文档的主题分布，<br/> β是每个主题的单词分布，<br/> θ是文档<em class="lx"> m、<br/> </em>的主题分布，φ是主题<em class="lx"> k、<br/> </em> z 是文档<em class="lx"> m </em>中第<em class="lx"> n </em>个单词的主题，以及<em class="lx"> <br/> </em>的主题</p><h2 id="55ab" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">调整模型</h2><p id="b72b" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">在上面的板模型图中，可以看到 w 是灰色的。这是因为它是系统中唯一可观察的变量，而其他变量是潜在的。正因为如此，要调整模型，有一些事情你可以弄乱，下面我重点介绍两个。</p><p id="3699" class="pw-post-body-paragraph lc ld it le b lf ma lh li lj mb ll lm kp mt lo lp kt mu lr ls kx mv lu lv lw im bi translated">α是一个矩阵，其中每行是一个文档，每列代表一个主题。行<em class="lx"> i </em>和列<em class="lx"> j </em>中的值表示文档<em class="lx"> i </em>包含主题<em class="lx"> j </em>的可能性。对称分布意味着每个主题均匀地分布在整个文档中，而非对称分布则倾向于某些主题。这将影响模型的起点，当您大致了解主题如何分布以改善结果时，可以使用它。</p><p id="3305" class="pw-post-body-paragraph lc ld it le b lf ma lh li lj mb ll lm kp mt lo lp kt mu lr ls kx mv lu lv lw im bi translated">β是一个矩阵，其中每行代表一个主题，每列代表一个单词。行<em class="lx"> i </em>和列<em class="lx"> j </em>中的值表示主题<em class="lx"> i </em>包含单词<em class="lx"> j </em>的可能性。通常每个单词在整个主题中均匀分布，这样就不会有主题偏向某些单词。这可以被利用，虽然为了偏向某些主题，以有利于某些词。例如，如果你知道你有一个关于苹果产品的主题，那么在其中一个主题中偏向“iphone”和“ipad”这样的词会有所帮助，以便推动模型找到那个特定的主题。</p><h2 id="a772" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">结论</h2><p id="076a" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">本文并不打算成为一个成熟的 LDA 教程，而是给出 LDA 模型如何工作以及如何使用它们的概述。有许多实现方式，例如<a class="ae kf" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank"> Gensim </a>易于使用且非常有效。关于使用 Gensim 库进行 LDA 建模的很好的教程可以在<a class="ae kf" rel="noopener" target="_blank" href="/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24">这里</a>找到。</p><p id="280e" class="pw-post-body-paragraph lc ld it le b lf ma lh li lj mb ll lm kp mt lo lp kt mu lr ls kx mv lu lv lw im bi translated">有什么想法或发现我错过了什么？让我知道！</p><p id="8751" class="pw-post-body-paragraph lc ld it le b lf ma lh li lj mb ll lm kp mt lo lp kt mu lr ls kx mv lu lv lw im bi translated">快乐话题造型！</p></div></div>    
</body>
</html>