<html>
<head>
<title>Linear Regression Sucks.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归烂透了。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-sucks-27a5215e50c0?source=collection_archive---------3-----------------------#2017-04-04">https://towardsdatascience.com/linear-regression-sucks-27a5215e50c0?source=collection_archive---------3-----------------------#2017-04-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="6d9b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">线性回归。这是有史以来第一种被深入研究的回归分析，是任何监督学习课程的基础，是…你明白了吧。很糟糕。</p><p id="039c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在现实世界中，线性回归(GLS)表现不佳有多种原因:</p><ul class=""><li id="da56" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated"><strong class="js iu">对异常值和质量差的数据很敏感</strong>—在现实世界中，数据经常受到异常值和质量差的数据的污染。如果异常值相对于非异常值数据点的数量多于几个，那么线性回归模型将偏离真正的基本关系。</li><li id="726b" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated"><strong class="js iu">它要求所有变量都是多元正态的</strong>(单变量高斯到更高维的推广)—使用非线性变换将非正态变量变换为正态变量(例如，通常对数变换可以解决这个问题)可能会引入多重共线性效应。</li><li id="4529" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated"><strong class="js iu">它假设变量之间没有(或很少)多重共线性</strong>(即变量相互独立)——这使得模型极其不稳定。虽然测试和删除很简单，但这可能会很痛苦。</li><li id="bbad" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated"><strong class="js iu">它假设了homoscedassity</strong>(希腊语:<em class="lc"> homo </em>“相同”<em class="lc"> skedasis </em>“分散”)——也就是说，误差项的标准差是恒定的。如果线性回归是蓝色的(最佳线性无偏估计量)，它们还必须是不相关的，并且具有零均值</li><li id="7784" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated"><strong class="js iu">它要求我们的因变量和预测变量之间的关系是线性的</strong>——嗯，这是它的名字(我仍然认为值得一提，尽管这篇文章是关于线性估计量的)。</li></ul><p id="8781" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为一个典型的经验法则，使用参数方法，如线性回归，当你的数据是高质量的，这样的方法会更有效。非参数方法(不假设数据的某种潜在分布)在交换中使用数据的效率较低，无法消除某种类型的假设。</p><p id="1e3f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是这些非参数线性方法是什么呢？这里有几个我最喜欢的。</p><p id="89cd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">泰尔森估计器</strong></p><p id="8dc3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Theil-Sen估计量可以有效地计算，并且对异常值不敏感。即使数据是正态分布的并且数据是高质量的(即不存在异常值)，它仍然比最小二乘回归有竞争力。低效率的问题(它以O(n)运行)，但是使用诸如随机采样或者甚至一些确定性方法的方法，这可以减少到O(n log n)。此外…这几乎不是一个深度卷积神经网络，你不会用这些简单的方法进行大量的计算。</p><p id="e1a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从概念上讲，这非常简单:计算因变量和自变量之间所有组合的所有斜率值的中值，由此斜率可以通过使用(<em class="lc">yj</em>-<em class="lc">易</em>)/(<em class="lc">XJ</em>-<em class="lc">Xi)</em>来确定。</p><p id="d75e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">最小修剪正方形</strong></p><p id="abc6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最小修整平方是一种稳健的回归方法(一种旨在规避某些回归模型的一些缺点的回归形式)，与Theil-Sen和即将推出的MM估计器一样，它对异常值的影响不敏感。</p><p id="4abd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">LTS不是像标准最小二乘法那样最小化所有n个点的残差平方和，而是最小化一个k <n smallest="" square="" residuals="" where="" k="" is="" greater="" than="" half="" the="" number="" of="" data="" points.=""/></p><p id="1b34" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Theil Sen, it’s a fairly inefficient method.</p><p id="1e62" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> MM估计值</strong></p><p id="a1bb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">1973年，Peter J. Huber引入了一种称为M-估计(M代表“最大似然型”)的回归方法，该方法对响应中的异常值稳健，但对解释变量不稳健，由于显而易见的原因，该方法并不理想。然而，它非常有效。</p><p id="dbcd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后是S-估计(S代表“比例”)技术，其思想是找到一个平面，使残差比例的稳健估计最小化。它对异常值和杠杆点有很强的抵抗力，但是对于实际应用来说效率太低。</p><p id="233b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，像一个无畏的骑士骑着种马投入战斗一样，MM估计出现了，它结合了S估计的稳健性和M估计的效率。它的工作原理是找到一个稳健的S估计，使残差标度的M估计最小化，它保持不变，同时找到参数的接近M估计。</p><p id="a430" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">少数几篇论文将MM估计量与最小修整平方和OLS进行了比较，MM估计量是其中的首选。</p><p id="2f81" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">贝叶斯稳健回归</strong></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ld"><img src="../Images/7106a4190684474b819ee811f35f6b74.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*aVZII0x6DoK8vE0_XgZaug.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Normal vs Student-T distributions</figcaption></figure><p id="e89b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将正态分布替换为具有大约5个自由度的重尾t分布(但要进行测试和比较)在各种实际情况下都能很好地工作。<strong class="js iu">贝叶斯稳健回归</strong>依赖于这样的分布。t分布的尾部有更多的“肉”,这意味着我们在那里采样的概率比在正态分布的相同位置采样的概率更高。异常值对我们的估计影响较小，因为似然函数假设异常值更有可能出现。</p></div><div class="ab cl lp lq hx lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="im in io ip iq"><p id="3b0c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其原因是因为高相关性将使设计矩阵(您的数据)与其自身一起转置(计算估计量所需的步骤)奇异，因此无法求逆，因为行列式为零，所以不存在估计量。即使它几乎是奇异的，也很可能是计算不稳定的。</p><p id="eff6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意，如果你考虑异方差，那么线性回归往往是最好的结果…我猜它实际上并不总是很糟糕。</p><p id="d02a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae lw" href="http://iacmc.zu.edu.jo/ar/images/stories/IACMC2016/39.pdf" rel="noopener ugc nofollow" target="_blank">http://iacmc.zu.edu.jo/ar/images/stories/IACMC2016/39.pdf</a></p></div></div>    
</body>
</html>