<html>
<head>
<title>Adaboost for Dummies: Breaking Down the Math (and its Equations) into Simple Terms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">傻瓜Adaboost:将数学(及其方程)分解成简单的术语</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adaboost-for-dummies-breaking-down-the-math-and-its-equations-into-simple-terms-87f439757dcf?source=collection_archive---------2-----------------------#2018-04-09">https://towardsdatascience.com/adaboost-for-dummies-breaking-down-the-math-and-its-equations-into-simple-terms-87f439757dcf?source=collection_archive---------2-----------------------#2018-04-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/19b2b14f28d46ed6f8c1916b945b014d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eSAesgb09NTkPf8Yi4oTfw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Learning machines is a long walk up the stairs. But we can shorten the path. (Photo by <a class="ae kc" href="https://unsplash.com/photos/cfQEO_1S0Rs?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Gabriel Izgi</a> on <a class="ae kc" href="https://unsplash.com/search/photos/ideas?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a>)</figcaption></figure><p id="bf8a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Adaboost是Adaptive Boosting的缩写，是一种机器学习方法，在概念上容易理解，但在数学上不太容易掌握。部分原因是由于方程和公式没有被分解成简单的术语，用基本的数学作为方程的演示。本文旨在通过Adaboost实现这一目标，将数据科学领域的新手作为主要目标受众。</p><p id="2fa7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有许多精彩的讲座、视频和论文有效而简洁地解释了Adaboost背后的概念。简而言之，这种想法是为分类器和数据点(样本)设置权重，迫使分类器专注于难以正确分类的观察值。该过程按顺序完成<em class="lb">，因为随着算法迭代的进行，在每一步都调整两个权重。这就是Adaboost被称为<em class="lb">顺序集成方法</em>的原因——集成是指一种结合几个模型以提高最终预测性能的学习类型。</em></p><p id="49d5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一概念很容易理解，但是一旦我们试图更深入地理解支持这一概念的数学，我们就会面对许多具有类似景象的文章和讲座:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/eb82c757b668eee2feff4e4ffebac7ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/0*oxAkJZ1WEe_-ugoe.png"/></div></figure><p id="71e4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于那些没有数学学术背景的人和/或非数学类型的人来说，随着最近的机器学习热潮，现在正在学习数据科学，光是符号就显得非常令人生畏。本文旨在通过分解符号和公式，并使用简单的数学来解释Adaboost的工作原理，从而消除混淆和恐惧。</p><h1 id="207e" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">基本术语</h1><p id="d890" class="pw-post-body-paragraph kd ke iq kf b kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">首先，让我们复习一些基本术语。</p><p id="a250" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">增强:结合许多弱(简单)学习者来创建高度准确的预测。</p><p id="329a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">弱学习者:产生比随机猜测略好的预测的分类器。随机猜测相当于50%，像抛硬币一样。熟悉信息论，尤其是香农熵概念的人对此会很熟悉。</p><p id="5ec5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设:我们的分类器，也就是我们的机器学习算法用来逼近未知函数的函数，目标(真实)函数，它模拟输入值x和输出值y之间的关系。</p><p id="7a15" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Adaboost:第一个实用的boosting算法，由Freund和Schapire (1995)发明。它基于Vapnik和Chervonekis的想法，即一个经过训练的分类器要在其预测中有效和准确，它应该满足以下三个条件:</p><p id="2c65" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1)应该在“足够”的训练样本上训练分类器</p><p id="6f48" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2)它应该通过产生低训练误差来提供对这些例子的良好拟合</p><p id="3b85" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3)它应该简单(因为简单的模型比过于复杂的模型更好)</p><h1 id="a635" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">解释Adaboost，一步一步来</h1><p id="384d" class="pw-post-body-paragraph kd ke iq kf b kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">让我们回顾一下迭代公式，将每一步的每一个符号分解成一个粒度级别，以便于理解。</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/670b1cc01f64225aebe4c642e4120361.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/0*lmjOP6qGkEPDM9-3.png"/></div></figure><p id="e2a1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> 1) </strong> <strong class="kf ir">给定(x1，y1)，…..，(x_m，y_m)其中x_i ∈ X，y_i ∈ {-1，+1} </strong></p><p id="7ab4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">有用的符号</em></p><p id="c5d2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">∑:"元素"</p><p id="a9b8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">{}:设置</p><p id="8a83" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb"> ex: </em>如果A = {1，2，3，7}，2 ∈ A</p><p id="cd9f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(x_1，y_1):第一个训练样本，(x_m，y_m) =第m个训练样本</p><p id="fba3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然我们已经记下了所有的符号，我们可以将公式的第一部分读作:</p><p id="aeac" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">“给定包含m个样本的训练集，其中所有X个输入是总集X的元素，y个输出是仅包含两个值的集的元素，-1(负类)和1(正类)…”</p><p id="1ba9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> 2)初始化:对于i = 1，…，m，D1(I)= 1/m</strong></p><p id="9419" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，D =样本的权重，i =第I个训练样本。在其他论文中，D将被写成w。因此，下一个语句是:</p><p id="a50b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">“…将样本的所有权重初始化为1除以训练样本数…”</p><p id="7e51" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> 3)对于t=1，…，T: </strong></p><p id="1357" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">*使用分布式Dt训练弱学习者。</p><p id="506a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">*得到弱假设h_t: X -&gt; {-1，+1}</p><p id="0a02" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">*目标:选择加权误差低的h_t:</p><p id="6c80" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ε = Pr_i~Dt [h_t(xi)不等于y_i]</p><p id="c40b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">*选择α_t = 1/2 * ln(1-ε / ε)</p><p id="5666" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">*更新，对于i = 1，…，m:</p><p id="5191" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Dt+1(I)= Dt(I)exp(-αt * y _ I * h _ t(x _ I)/Zt</p><p id="e352" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">有用的符号</em></p><p id="fca1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Pr =概率</p><p id="d997" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设/分类器</p><p id="90a5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ε =模型的最小错误分类误差</p><p id="da71" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">α =分类器的重量</p><p id="5327" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">exp =欧拉方程e: 2.71828</p><p id="4d38" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Zt =归一化因子，用于确保权重代表真实分布</p><p id="600f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了这些符号，我们可以阅读下一部分:</p><p id="0a61" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于t=1到T个分类器，使其适合训练数据(其中每个预测为-1或1)，并选择加权分类错误最低的分类器</p><p id="c6fb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正式计算<strong class="kf ir"> ε </strong>的公式如下:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/eab08fd74c5cccce71edd4e861b59e7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/0*g1eL291iveyZCR7E.png"/></div></figure><p id="0131" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们来分解这个特定的模型。</p><p id="5ff8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">有用的符号</em></p><p id="3c97" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">σ=总和</p><p id="f37b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果分类错误，y_i不等于h_j = 1，如果分类正确，y _ I不等于0</p><p id="701b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">重量指数=重量</p><p id="a903" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，公式为:“误差等于错误分类率的总和，其中训练样本I和y_i的权重不等于我们的预测h_j(如果错误分类，则等于1，如果正确分类，则等于0)。”</p><p id="2880" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们用简单的数学来理解这个公式。假设有4个不同的样本，权重分别为0.5、0.2、0.1和0.04。想象一下，我们的分类器h预测值为1，1，-1和-1，但实际输出值y为-1，1，-1，1。</p><blockquote class="ml"><p id="0ea1" class="mm mn iq bd mo mp mq mr ms mt mu la dk translated">预测时间:1 1月1日</p><p id="456a" class="mm mn iq bd mo mp mq mr ms mt mu la dk translated">实际值:-1 1 -1 1</p><p id="556b" class="mm mn iq bd mo mp mq mr ms mt mu la dk translated">重量:0.5 0.2 0.1 0.04</p><p id="d569" class="mm mn iq bd mo mp mq mr ms mt mu la dk translated">1或0: 1 0 0 1</p></blockquote><p id="f622" class="pw-post-body-paragraph kd ke iq kf b kg mv ki kj kk mw km kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">这导致错误分类率的以下计算:</p><blockquote class="ml"><p id="3e1e" class="mm mn iq bd mo mp mq mr ms mt mu la dk translated">误分类率/误差=(0.5 * 1+0.2 * 0+0.1 * 0+0.04 * 1)/(0.5+0.2+0.1+0.04)</p><p id="4752" class="mm mn iq bd mo mp mq mr ms mt mu la dk translated">误差= 0.64285714285</p></blockquote><p id="9141" class="pw-post-body-paragraph kd ke iq kf b kg mv ki kj kk mw km kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">接下来，通过公式1/2 * ln(1- error / error)为分类器选择权重α。</p><p id="9742" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，简单的数学可能比语言解释得更清楚。例如，假设我们有0.30，0.70，0.5的误差。</p><p id="c4d4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的分类器权重计算如下:</p><blockquote class="ml"><p id="67d2" class="mm mn iq bd mo mp mq mr ms mt mu la dk">ε = 0.3</p><p id="920f" class="mm mn iq bd mo mp mq mr ms mt mu la dk translated">α = 1/2 * ln(1- 0.3 / 0.3) = 0.42365</p><p id="0c57" class="mm mn iq bd mo mp mq mr ms mt mu la dk">ε = 0.7</p><p id="a9ba" class="mm mn iq bd mo mp mq mr ms mt mu la dk translated">α = 1/2 * ln(1- 0.7 / 0.7) = -0.42365</p><p id="e2e9" class="mm mn iq bd mo mp mq mr ms mt mu la dk">ε = 0.5</p><p id="fe37" class="mm mn iq bd mo mp mq mr ms mt mu la dk translated">α = 1/2 * ln(1- 0.5 / 0.5) = 0</p></blockquote><p id="d741" class="pw-post-body-paragraph kd ke iq kf b kg mv ki kj kk mw km kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">注意三个有趣的观察结果:1)准确度高于50%的分类器导致该分类器的正权重(换句话说，如果ε &lt; = 0.5) ，则<em class="lb"> α &gt;为0)，2)准确度为50%的分类器为0，因此对最终预测没有贡献，以及3)误差0.3和0.7导致符号相反的分类器权重。</em></p><p id="e3ec" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在是等式中非常重要的部分:更新每个样本的权重。我在上面提到过，更新的重点是迫使分类器专注于难以正确分类的观察值。这是通过在迭代后用增加的权重更新错误分类的案例来实现的。增加权重将使我们的学习算法在下一次迭代中更加关注这些观察结果。相反，在下一次迭代中，正确分类的案例将接收到降低的权重，并减少我们的分类器的注意力。</p><p id="53a9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">再一次，用简单的数字作为示范，信息吸收是没有痛苦的。让我们用上面0.3的误差率代入公式。请记住，我们正在寻找<em class="lb">低加权误差</em>。换句话说，我们不应该使用0.5及以上的错误率。在低错误率的情况下，让我们检查当案例被错误分类时和当案例被正确分类时会发生什么。</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/10c847635d2699b26c8196e91237b304.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/0*gEDCzjWkWun2vlot.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">misclassified</figcaption></figure><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/dc21e0e3259d1c8c3096ccf036bea2b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/0*fPBL5RIxOo87artR.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">correctly classified</figcaption></figure><p id="8410" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在你知道了！在不正确分类的情况下，exp项变得大于1，而在正确分类的情况下，exp项变得小于1。因此，不正确的分类将获得更高的权重，促使我们的分类器在下一次迭代中更加关注它们，而正确分类的相反情况将导致相反的结果。</p><p id="253d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们继续这种迭代，直到a)实现低训练误差，或者b)已经添加了预设数量的弱学习者(这是在我们控制之下的参数)。然后，我们通过将每个分类器的加权预测相加得到最终预测。</p><h2 id="2fba" class="na li iq bd lj nb nc dn ln nd ne dp lr ko nf ng lv ks nh ni lz kw nj nk md nl bi translated">摘要</h2><p id="542c" class="pw-post-body-paragraph kd ke iq kf b kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">我们已经看到了Adaboost是如何通过分解公式中的每一个符号在粒度级别上工作的。然后，我们应用简单的数学来理解公式的每个组成部分是如何工作的。这种通过分解的部分来处理公式的实践对于理解机器学习算法是一种有用的实践。</p></div></div>    
</body>
</html>