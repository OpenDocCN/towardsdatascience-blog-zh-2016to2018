<html>
<head>
<title>A line-by-line layman’s guide to Linear Regression using TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 TensorFlow 进行线性回归的逐行外行指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-line-by-line-laymans-guide-to-linear-regression-using-tensorflow-3c0392aa9e1f?source=collection_archive---------2-----------------------#2018-10-19">https://towardsdatascience.com/a-line-by-line-laymans-guide-to-linear-regression-using-tensorflow-3c0392aa9e1f?source=collection_archive---------2-----------------------#2018-10-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="e66b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本文是使用 TensorFlow 进行线性回归的逐行外行指南。如果你读这篇文章有困难，考虑在这里订阅<a class="ae ko" href="https://derekchia.medium.com/membership" rel="noopener">中级会员</a>！</p><p id="986e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">线性回归是机器学习之旅的一个良好开端，因为它是一个非常简单的问题，可以通过流行的模块来解决，如<a class="ae ko" href="http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn 包</a>。在本文中，我们将讨论使用 TensorFlow 实现线性回归的逐行方法。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/908d299a01ce25123392e01f8cf916d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/1*23OuN0pORumj_A9x-UOblg.gif"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">linear regression equation</figcaption></figure><p id="092b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">查看上面的线性回归方程，我们首先构建一个图形，通过多次迭代学习斜率(<strong class="js iu"> W </strong>)和偏差(<strong class="js iu"> b </strong>)的梯度。在每次迭代中，我们的目标是通过比较输入<strong class="js iu"> y </strong>和预测的<strong class="js iu">y</strong>来缩小差距(损失)。也就是说，我们想要修改<strong class="js iu"> W </strong>和<strong class="js iu"> b </strong>，这样<strong class="js iu"> x </strong>的输入就会给我们想要的<strong class="js iu"> y </strong>。求解线性回归也称为寻找最佳拟合线或趋势线。</p></div><div class="ab cl lb lc hx ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="im in io ip iq"><h1 id="6c90" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">生成数据集</h1><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="fbe8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[第 1、2、3 行]</p><pre class="kq kr ks kt gt mi mj mk ml aw mm bi"><span id="c57b" class="mn lj it mj b gy mo mp l mq mr">import numpy as np<br/>import tensorflow as tf<br/>import matplotlib.pyplot as plt</span></pre><p id="27c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本文中，我们将使用一些流行的模块，如<a class="ae ko" href="http://www.numpy.org/" rel="noopener ugc nofollow" target="_blank"> numpy </a>、<a class="ae ko" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> tensorflow </a>和<a class="ae ko" href="https://matplotlib.org/tutorials/introductory/pyplot.html" rel="noopener ugc nofollow" target="_blank"> matplotlib.pyplot </a>。让我们导入它们。</p><p id="5ad5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[第 6、7 行]</p><pre class="kq kr ks kt gt mi mj mk ml aw mm bi"><span id="3367" class="mn lj it mj b gy mo mp l mq mr">x_batch = <a class="ae ko" href="https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.linspace.html" rel="noopener ugc nofollow" target="_blank">np.linspace</a>(0, 2, 100)<br/>y_batch = 1.5 * x_batch + np.random.randn(*x_batch.shape) * 0.2 + 0.5</span></pre><p id="e2e0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，我们从生成数据集开始，即<strong class="js iu"> x </strong>和<strong class="js iu"> y </strong>。你可以把<strong class="js iu"> x </strong>和<strong class="js iu"> y </strong>中的每个值想象成图上的点。在第 6 行中，我们希望 numpy 生成 100 个值在 0 和 2 之间的点，均匀分布。结果是一个存储在<code class="fe ms mt mu mj b">x_batch</code>中的 numpy 数组。类似地，我们还想随机生成<strong class="js iu"> y </strong>，这样它的梯度为 1.5 ( <strong class="js iu"> W </strong>)并且使用<code class="fe ms mt mu mj b">np.random.randn()</code>具有某种形式的随机性。为了让事情变得有趣，我们将 y 轴截距<strong class="js iu"> b </strong>设置为 0.5。</p><p id="78a6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【第八行】<code class="fe ms mt mu mj b">return x_batch, y_batch</code></p><p id="68a4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们返回 numpy 数组<code class="fe ms mt mu mj b">x_batch</code>和<code class="fe ms mt mu mj b">y_batch</code>。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/f4c719b20046592d188b85fa3a65e5bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*18pmlyD7PXezI9Kh-iJ1Sg.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">plt.scatter(x_batch, y_batch) — this is our starting point</figcaption></figure><p id="8ad1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是<code class="fe ms mt mu mj b">generate_dataset()</code>的剧情样子。请注意，从视觉上看，这些点形成了一条从左下方到右上方的趋势线，但没有穿过原点(0，0)。</p></div><div class="ab cl lb lc hx ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="im in io ip iq"><h1 id="a42b" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">构建图表</h1><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="b73a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[第 2 行和第 3 行]</p><pre class="kq kr ks kt gt mi mj mk ml aw mm bi"><span id="5c19" class="mn lj it mj b gy mo mp l mq mr">x = tf.placeholder(tf.float32, shape=(None, ), name='x')  <br/>y = tf.placeholder(tf.float32, shape=(None, ), name='y')</span></pre><p id="9a05" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们构建张量流图，帮助我们计算<strong class="js iu"> W </strong>和<strong class="js iu"> b </strong>。这在函数<code class="fe ms mt mu mj b">linear_regression()</code>中完成。在我们的公式<code class="fe ms mt mu mj b">y = Wx + b</code>中，<strong class="js iu"> x </strong>和<strong class="js iu"> y </strong>是表示为 TensorFlow 的<a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/placeholder" rel="noopener ugc nofollow" target="_blank">占位符</a>的节点。将<strong class="js iu"> x </strong>和<strong class="js iu"> y </strong>声明为占位符意味着我们需要在以后传入值——我们将在下一节重新讨论这个问题。请注意，我们现在仅仅是构建图形，而不是运行它(TensorFlow 有<a class="ae ko" href="https://medium.com/coinmonks/tensorflow-graphs-and-sessions-c7fa116209db" rel="noopener">惰性评估</a>)。</p><p id="5d39" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<code class="fe ms mt mu mj b">tf.placeholder</code>的第一个参数中，我们将数据类型定义为 float 32——占位符中常见的数据类型。第二个参数是设置为<code class="fe ms mt mu mj b">None</code>的占位符的形状，因为我们希望它在训练期间确定。第三个参数让我们设置占位符的名称。</p><blockquote class="mw mx my"><p id="42f5" class="jq jr mz js b jt ju jv jw jx jy jz ka na kc kd ke nb kg kh ki nc kk kl km kn im bi translated">占位符仅仅是一个变量，我们将在以后的某一天把数据赋给它。它允许我们创建我们的操作和构建我们的计算图，而不需要数据。在 TensorFlow 术语中，我们通过这些占位符将数据输入图表。</p><p id="d06f" class="jq jr mz js b jt ju jv jw jx jy jz ka na kc kd ke nb kg kh ki nc kk kl km kn im bi translated">参考:<a class="ae ko" href="https://learningtensorflow.com/lesson4/" rel="noopener ugc nofollow" target="_blank">https://learningtensorflow.com/lesson4/</a></p></blockquote><p id="2293" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【第 5 行】<code class="fe ms mt mu mj b">with tf.variable_scope(‘lreg’) as scope:</code></p><p id="445e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这一行为第 6 行和第 7 行中的变量定义了变量范围。简而言之，<a class="ae ko" href="https://jasdeep06.github.io/posts/variable-sharing-in-tensorflow/" rel="noopener ugc nofollow" target="_blank">变量作用域</a>允许以层次方式命名变量，以避免名称冲突。详细地说，它是 TensorFlow 中的一种机制，允许在图形的不同部分共享变量，而无需四处传递对变量的引用。请注意，尽管我们在这里没有重用变量，但是恰当地命名它们是一个很好的做法。</p><pre class="kq kr ks kt gt mi mj mk ml aw mm bi"><span id="5475" class="mn lj it mj b gy mo mp l mq mr">with tf.name_scope("foo"):<br/>    with tf.variable_scope("var_scope"):<br/>        v = tf.get_variable("<strong class="mj iu">var</strong>", [1])<br/>with tf.name_scope("bar"):<br/>    with tf.variable_scope("var_scope", reuse=True):<br/>        v1 = tf.get_variable("<strong class="mj iu">var</strong>", [1])<br/>assert v1 == v<br/><strong class="mj iu">print(v.name)   # var_scope/var:0<br/>print(v1.name)  # var_scope/var:0</strong></span></pre><p id="fc62" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在上面的代码中，我们看到变量("<strong class="js iu"> var </strong>")被重用并被断言为真。要使用同一个变量，只需调用<code class="fe ms mt mu mj b"><a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/variable_scope" rel="noopener ugc nofollow" target="_blank">tf.get_variable</a>(“var”, [1])</code>。</p><p id="9b99" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【第六行】<code class="fe ms mt mu mj b">w = tf.Variable(np.random.normal(), name=’W’)</code></p><p id="a96b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与占位符不同，<strong class="js iu"> W </strong>被定义为一个<code class="fe ms mt mu mj b"><a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/Variable" rel="noopener ugc nofollow" target="_blank">tf.Variable</a></code>，其值随着我们训练模型而变化，每次都以较低的损失结束。在第 10 行，我们将解释“损失”是什么意思。现在，我们使用<code class="fe ms mt mu mj b"><a class="ae ko" href="https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.normal.html" rel="noopener ugc nofollow" target="_blank">np.random.normal()</a></code>设置变量，以便它从正态(高斯)分布中抽取一个样本。</p><blockquote class="mw mx my"><p id="6d19" class="jq jr mz js b jt ju jv jw jx jy jz ka na kc kd ke nb kg kh ki nc kk kl km kn im bi translated">tf。变量—一个变量在调用<code class="fe ms mt mu mj b"><em class="it">run()</em></code>时维护图中的状态。您通过构造类<code class="fe ms mt mu mj b"><em class="it">Variable</em></code>的实例向图中添加一个变量。</p><p id="cb88" class="jq jr mz js b jt ju jv jw jx jy jz ka na kc kd ke nb kg kh ki nc kk kl km kn im bi translated"><code class="fe ms mt mu mj b"><em class="it">Variable()</em></code>构造函数需要变量的初始值，可以是任何类型和形状的<code class="fe ms mt mu mj b"><em class="it">Tensor</em></code>。初始值定义了变量的类型和形状。构造后，变量的类型和形状是固定的。可以使用赋值方法之一来更改该值。</p><p id="dd67" class="jq jr mz js b jt ju jv jw jx jy jz ka na kc kd ke nb kg kh ki nc kk kl km kn im bi translated">参考:<a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/Variable" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/Variable</a></p></blockquote><p id="1c7d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请注意，即使现在定义了变量，也必须在使用该值运行操作之前显式初始化它。这是惰性求值的一个特性，我们将在后面进行实际的初始化。</p><p id="b17d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里<strong class="js iu"> W </strong>真正做的是找到我们最佳拟合线的梯度。之前，我们使用 1.5 的梯度生成数据集，因此我们应该期望经过训练的<strong class="js iu"> W </strong>接近这个数字。为<strong class="js iu"> W </strong>选择起始数字有些重要——想象一下，如果我们可以“随机”选择 1.5，工作就完成了，不是吗？差不多吧…</p><p id="0712" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为我们的主题是寻找线性回归中的最佳梯度，我需要指出的是，无论我们在哪里初始化<strong class="js iu"> W </strong>，我们的损失函数将总是产生一个最小损失值。这是由于我们的损失函数,<strong class="js iu"> W </strong>和<strong class="js iu"> b </strong>的凸性，当我们在这样的图表中绘制它们时。换句话说，这个碗形图形让我们可以确定最低点，不管我们从哪里开始。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/87498b5660fccb0764c97e48da1dc80f.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*k9Ovh3KXw6oZSW6k3b9MGQ.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">One global minimum</figcaption></figure><p id="a547" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，对于更复杂的问题，情况并非如此，在这些问题中存在多个局部最小值，如下所示。选择一个不好的数字来初始化你的变量会导致你的梯度搜索陷入局部极小值。这阻止了你达到具有较低损失的全局最小值。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/4e0318758443f59cb607b1ecd6d294df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*zAtCgH3GtFu6dDh7CbeoVw.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Multiple local minima with one global minimum</figcaption></figure><p id="a45d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">研究人员想出了替代的初始化方法，例如<a class="ae ko" href="https://www.quora.com/What-is-an-intuitive-explanation-of-the-Xavier-Initialization-for-Deep-Neural-Networks" rel="noopener ugc nofollow" target="_blank"> Xavier 初始化</a>，试图避免这个问题。如果您想使用它，请随意使用:</p><p id="a2b3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe ms mt mu mj b">tf.get_variable(…, initializer=<a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer" rel="noopener ugc nofollow" target="_blank">tf.contrib.layers.xavier_initializer()</a>)</code>。</p><p id="c1de" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【第 7 行】<code class="fe ms mt mu mj b">b = tf.Variable(np.random.normal(), name=’b’)</code></p><p id="cfdc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">除了<strong class="js iu"> W </strong>之外，我们还要训练我们的 bias <strong class="js iu"> b </strong>。如果没有<strong class="js iu"> b </strong>，我们的最佳拟合线将总是穿过原点，而不会学习 y 轴截距。还记得 0.5 吗？我们也需要了解这一点。</p><p id="e3a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【第九行】<code class="fe ms mt mu mj b">y_pred = tf.add(tf.multiply(w, x), b)</code></p><p id="fcbb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在分别定义了<strong class="js iu"> x </strong>、<strong class="js iu"> y </strong>和<strong class="js iu">W</strong><strong class="js iu">、</strong>之后，我们现在准备将它们放在一起。为了实现公式<code class="fe ms mt mu mj b">y = Wx + b</code>，我们首先使用<code class="fe ms mt mu mj b"><a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/multiply" rel="noopener ugc nofollow" target="_blank">tf.multipl</a>y</code>将<code class="fe ms mt mu mj b">w</code>和<code class="fe ms mt mu mj b">x</code>相乘，然后使用<code class="fe ms mt mu mj b"><a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/math/add" rel="noopener ugc nofollow" target="_blank">tf.add</a></code>将变量<code class="fe ms mt mu mj b">b</code>相加。这将执行逐元素的乘法和加法，从而产生张量<code class="fe ms mt mu mj b">y_pred</code>。<code class="fe ms mt mu mj b">y_pred</code>代表预测的<strong class="js iu"> y </strong>值，正如您可能会怀疑的那样，<strong class="js iu">预测的 y </strong>一开始会很糟糕，并且与<strong class="js iu">生成的 y</strong>相差甚远，类似于占位符或变量，您可以随意为其命名。</p><p id="b0db" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【第 11 行】<code class="fe ms mt mu mj b">loss = tf.reduce_mean(tf.square(y_pred — y))</code></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/ba52b78fcdbe1930a4cf50852aeb2ff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/1*4BS1hp2Xoy3GFNSO0lvQjw.gif"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Mean Squared Error (MSE)</figcaption></figure><p id="d5c1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">计算完<code class="fe ms mt mu mj b">y_pred</code>，我们想知道<strong class="js iu">预测的 y </strong>与我们<strong class="js iu">生成的 y </strong>有多远。为此，我们需要设计一种方法来计算“差距”。这种设计被称为<a class="ae ko" href="https://blog.algorithmia.com/introduction-to-loss-functions/" rel="noopener ugc nofollow" target="_blank">损失函数</a>。这里，我们选择了<a class="ae ko" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">均方误差(MSE) </a>又名 L2 损失函数作为我们的“评分机制”。还有其他流行的损失函数，但我们不包括它们。</p><p id="c4fd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了理解我们的 MSE 实现，我们首先使用<code class="fe ms mt mu mj b">y_pred — y</code>找到<code class="fe ms mt mu mj b">y_pred</code>和<code class="fe ms mt mu mj b">y</code>的 100 个点之间的差异。接下来，我们通过对它们求平方(<code class="fe ms mt mu mj b"><a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/square" rel="noopener ugc nofollow" target="_blank">tf.square</a></code>)来放大它们的差异，从而使差异变大(很多)。哎哟！😝</p><p id="95ce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">向量大小为 100，我们现在有一个问题—我们如何知道这 100 个值是否代表一个好的分数？通常分数是决定你表现如何的一个数字(就像你的考试一样)。因此，为了得到一个单一的值，我们利用<code class="fe ms mt mu mj b"><a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/reduce_mean" rel="noopener ugc nofollow" target="_blank">tf.reduce_mean</a></code>找到所有 100 个值的平均值，并将其设置为我们的<code class="fe ms mt mu mj b">loss</code>。</p><p id="fcc9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【第 13 行】<code class="fe ms mt mu mj b">return x, y, y_pred, loss</code></p><p id="f58a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后但同样重要的是，我们在构造它们之后返回所有的 4 个值。</p></div><div class="ab cl lb lc hx ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="im in io ip iq"><h1 id="b9e2" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">计算图表</h1><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="9c19" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有了<code class="fe ms mt mu mj b">generate_dataset()</code>和<code class="fe ms mt mu mj b">linear_regression()</code>，我们现在准备运行程序，并开始寻找我们的最佳梯度<strong class="js iu"> W </strong>和偏差<strong class="js iu"> b </strong>！</p><p id="a90b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[第 2、3 行]</p><pre class="kq kr ks kt gt mi mj mk ml aw mm bi"><span id="ea43" class="mn lj it mj b gy mo mp l mq mr">x_batch, y_batch = generate_dataset()<br/>x, y, y_pred, loss = linear_regression()</span></pre><p id="d7eb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个<code class="fe ms mt mu mj b">run()</code>函数中，我们首先调用<code class="fe ms mt mu mj b">generate_dataset()</code>和<code class="fe ms mt mu mj b">linear_regression()</code>来得到<code class="fe ms mt mu mj b">x_batch</code>、<code class="fe ms mt mu mj b">y_batch</code>、<code class="fe ms mt mu mj b">x</code>、<code class="fe ms mt mu mj b">y</code>、<code class="fe ms mt mu mj b">y_pred</code>和<code class="fe ms mt mu mj b">loss</code>。向上滚动查看这两个功能的解释。</p><p id="e736" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[第 5、6 行]</p><pre class="kq kr ks kt gt mi mj mk ml aw mm bi"><span id="2154" class="mn lj it mj b gy mo mp l mq mr">optimizer = tf.train.GradientDescentOptimizer(0.1)<br/>train_op = optimizer.minimize(loss)</span></pre><p id="bdb3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们定义<a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer" rel="noopener ugc nofollow" target="_blank">优化器</a>并要求它最小化图中的损失。有几个优化器可供选择，我们方便地选择了<a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer" rel="noopener ugc nofollow" target="_blank">梯度下降算法</a>并将<a class="ae ko" href="https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2" rel="noopener">学习率</a>设置为 0.1。</p><p id="77fd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们不会深入优化算法的世界，但简而言之，优化者的工作是最小化(或最大化)你的损失(目标)函数。它通过在每次运行时向最优解的方向更新可训练变量(<strong class="js iu"> W </strong>和<strong class="js iu"> b </strong>)来实现这一点。</p><p id="8da7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">调用<a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer#minimize" rel="noopener ugc nofollow" target="_blank">最小化</a>函数计算梯度并将它们应用于变量——这是默认行为，您可以使用参数<code class="fe ms mt mu mj b">var_list</code>随意更改。</p><p id="6356" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【第八行】<code class="fe ms mt mu mj b">with tf.Session() as session:</code></p><p id="5e21" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们构建图形的前面部分，我们说过张量流使用惰性求值。这实际上意味着只有在会话开始时才计算图形。这里，我们将会话对象命名为<code class="fe ms mt mu mj b">session</code>。</p><p id="85d9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【第 9 行】<code class="fe ms mt mu mj b">session.run(<a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/initializers/global_variables" rel="noopener ugc nofollow" target="_blank">tf.global_variables_initializer()</a>)</code></p><p id="947a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后我们通过初始化我们要求变量保存的所有值来启动我们的第一个会话。由于惰性求值，在第一次构建图形时，变量如<strong class="js iu"> W </strong> ( <code class="fe ms mt mu mj b">w = tf.Variable(np.random.normal(), name=’W’)</code>)没有初始化，直到我们运行这一行。更多解释见<a class="ae ko" href="https://stackoverflow.com/questions/44433438/understanding-tf-global-variables-initializer" rel="noopener ugc nofollow" target="_blank">本</a>。</p><p id="6e0d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【第 10 行】<code class="fe ms mt mu mj b">feed_dict = {x: x_batch, y: y_batch}</code></p><p id="4505" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们需要提出<a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/Session#run" rel="noopener ugc nofollow" target="_blank"> feed_dict </a>，它本质上是<code class="fe ms mt mu mj b">session.run()</code>的一个参数。<code class="fe ms mt mu mj b">feed_dict</code>是一个字典，其关键字为<code class="fe ms mt mu mj b">tf.Tensor</code>、<code class="fe ms mt mu mj b">tf.placeholder</code>或<code class="fe ms mt mu mj b">tf.SparseTensor</code>。<code class="fe ms mt mu mj b">feed_dict</code>参数允许调用者覆盖图中张量(标量、字符串、列表、numpy 数组或 tf.placeholder，例如<strong class="js iu"> x </strong>和<strong class="js iu"> y </strong>)的值。</p><p id="6986" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这一行中，<strong class="js iu"> x </strong>和<strong class="js iu"> y </strong>是占位符，<strong class="js iu"> x_batch </strong>和<strong class="js iu"> y_batch </strong>是生成的值，准备在<code class="fe ms mt mu mj b">session.run()</code>期间填充占位符。</p><p id="079d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【第 12 行】<code class="fe ms mt mu mj b">for i in range(30):</code></p><p id="b605" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在初始化变量并使用<code class="fe ms mt mu mj b">feed_dict</code>为占位符准备值之后，我们现在进入脚本的核心，定义我们想要“调整”/“训练”权重(<strong class="js iu"> W </strong>)和偏差(<strong class="js iu"> b </strong>)的次数。我们在一个完整周期中遍历训练数据的次数(<strong class="js iu"> x </strong>和<strong class="js iu"> y </strong>)也称为<strong class="js iu">时期/训练步骤</strong>。一个完整的循环也被定义为一个<a class="ae ko" href="https://en.wikipedia.org/wiki/Feedforward_neural_network" rel="noopener ugc nofollow" target="_blank">前馈</a>和一个<a class="ae ko" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">反向传播</a>。</p><p id="cd94" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">前馈时，我们传入<strong class="js iu"> x </strong>、<strong class="js iu"> w </strong>和<strong class="js iu"> b </strong>的值，得到<strong class="js iu">预测的 y </strong>。这将计算用数字表示的损失。由于该图的目标是最小化损耗，优化器将执行反向传播以“调整”可训练变量(<strong class="js iu"> W </strong>和<strong class="js iu"> b </strong>)，以便下一次我们执行前馈(在另一个时期)时，损耗将会降低。</p><p id="056a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们这样向前向后循环 30 次。请注意，30 是一个超参数，您可以自由更改它。还要注意，更多的纪元=更长的训练时间。</p><p id="d64f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[第 13 行]</p><pre class="kq kr ks kt gt mi mj mk ml aw mm bi"><span id="e34b" class="mn lj it mj b gy mo mp l mq mr">session.run(train_op, feed_dict)</span></pre><p id="9ad6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们准备通过调用获取的<code class="fe ms mt mu mj b">session.run()</code>和<code class="fe ms mt mu mj b">feed_dict</code>来运行我们的第一个纪元。在这里，<code class="fe ms mt mu mj b">session.run()</code>评估提取的每个张量(<code class="fe ms mt mu mj b">train_op</code>)并用<code class="fe ms mt mu mj b">feed_dict</code>中的值替换相应的输入值。</p><blockquote class="mw mx my"><p id="068b" class="jq jr mz js b jt ju jv jw jx jy jz ka na kc kd ke nb kg kh ki nc kk kl km kn im bi translated"><code class="fe ms mt mu mj b"><strong class="js iu">fetches</strong></code>:单个图形元素、图形元素列表或其值为图形元素或图形元素列表的字典(参见<code class="fe ms mt mu mj b">run</code>的文档)。</p></blockquote><p id="6127" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当<code class="fe ms mt mu mj b">session</code>对象调用<code class="fe ms mt mu mj b">run()</code>方法时，在幕后发生的事情是，您的代码将运行图的必要部分(节点),以计算提取中的每个张量。由于<code class="fe ms mt mu mj b">train_op</code>指的是<code class="fe ms mt mu mj b">optimizer</code>调用方法<code class="fe ms mt mu mj b">minimize(loss)</code>，那么将通过调用损失函数来计算<code class="fe ms mt mu mj b">loss</code>，损失函数又触发<code class="fe ms mt mu mj b">y_pred</code>、<code class="fe ms mt mu mj b">y</code>、<code class="fe ms mt mu mj b">W</code>、<code class="fe ms mt mu mj b">x</code>和<code class="fe ms mt mu mj b">b</code>进行计算。</p><p id="4e3b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面是来自<a class="ae ko" href="https://www.tensorflow.org/api_docs/python/tf/Session#run" rel="noopener ugc nofollow" target="_blank"> TensorFlow 文档</a>的代码。你可以看到提取可以是单例、列表、元组、命名元组或字典。在我们的例子中，我们使用 feed_dict 作为 dictionary 类型的参数。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="mg mh l"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">fetches in session.run()</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/0573812c015ad9b376763efad5c029f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*F6B86SXWafvT0sKgM4fqAw.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">print(i, “loss:”, loss.eval(feed_dict))</figcaption></figure><p id="ffb3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【第 14 行】<code class="fe ms mt mu mj b">print(i, “loss:”, loss.eval(feed_dict))</code></p><p id="c468" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这一行打印出每个时期的损失。在左侧，您可以看到每个时期的损失值都在减少。</p><p id="a224" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用<code class="fe ms mt mu mj b">loss.eval()</code>和<code class="fe ms mt mu mj b">feed_dict</code>作为参数计算损失值。</p><p id="28fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[第 16、17 行]</p><pre class="kq kr ks kt gt mi mj mk ml aw mm bi"><span id="8392" class="mn lj it mj b gy mo mp l mq mr">print('Predicting')<br/>y_pred_batch = session.run(y_pred, {x : x_batch})</span></pre><p id="0af9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">30 个纪元后，我们现在有一个训练有素的<strong class="js iu"> W </strong>和<strong class="js iu"> b </strong>供我们执行<a class="ae ko" href="https://blogs.nvidia.com/blog/2016/08/22/difference-deep-learning-training-inference-ai/" rel="noopener ugc nofollow" target="_blank">推理</a>。与训练类似，可以使用<code class="fe ms mt mu mj b">session.run()</code>对相同的图进行推理，但这一次，获取将是<strong class="js iu"> y_pred </strong>而不是<strong class="js iu"> train_op </strong>，我们只需要输入<strong class="js iu"> x </strong>。我们这样做是因为<strong class="js iu"> W </strong>和<strong class="js iu"> b </strong>已经被训练过，并且<strong class="js iu">预测的 y </strong>可以仅用<strong class="js iu"> x </strong>来计算。注意在<code class="fe ms mt mu mj b">tf.add(tf.multiply(w, x), b)</code>中，没有<strong class="js iu"> y </strong>。</p><p id="6dff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">到目前为止，我们已经声明了 3 个<code class="fe ms mt mu mj b">session.run()</code>，所以让我们回顾一下它们的用法，因为<code class="fe ms mt mu mj b">session.run()</code>是我们运行操作和评估图中凸榫的命令。第一次我们做的是初始化我们的变量，第二次在训练期间传递我们的 feed_dict，第三次运行预测。</p><p id="926c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[第 19–23 行]</p><pre class="kq kr ks kt gt mi mj mk ml aw mm bi"><span id="1bd1" class="mn lj it mj b gy mo mp l mq mr">plt.scatter(x_batch, y_batch)<br/>plt.plot(x_batch, y_pred_batch, color='red')<br/>plt.xlim(0, 2)<br/>plt.ylim(0, 2)<br/>plt.savefig('plot.png')</span></pre><p id="68b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们用生成的<code class="fe ms mt mu mj b">x_batch</code>和<code class="fe ms mt mu mj b">y_batch</code>以及我们预测的线(用<code class="fe ms mt mu mj b">x_batch</code>和<code class="fe ms mt mu mj b">y_pred_batch</code>)绘制图表。最后，我们有我们的预测线很好地画在下面。花点时间回顾一下我们的第一个神经网络如何计算出梯度和 y 截距，并欣赏机器学习的魔力！</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/72625fde8d500c5cadb0e9b9df795476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*wrZ8FVZ_nG9odoGm4AZriQ.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">plt.plot(x_batch, y_pred_batch) — we drew the line of best fit</figcaption></figure><p id="124d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[第 25、56 行]</p><pre class="kq kr ks kt gt mi mj mk ml aw mm bi"><span id="9fbc" class="mn lj it mj b gy mo mp l mq mr">if __name__ == "__main__":<br/> run()</span></pre><p id="8c8d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">不需要解释——你可以做得更好。😉</p><h1 id="b0ce" class="li lj it bd lk ll nh ln lo lp ni lr ls lt nj lv lw lx nk lz ma mb nl md me mf bi translated">最后</h1><p id="52e1" class="pw-post-body-paragraph jq jr it js b jt nm jv jw jx nn jz ka kb no kd ke kf np kh ki kj nq kl km kn im bi translated">投身机器学习并不容易。有人从理论开始，有人从代码开始。我写这篇文章是为了让自己理解基本概念，并帮助那些正在接触机器学习或 TensorFlow 的人入门。</p><p id="994b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以在这里找到最终代码<a class="ae ko" href="https://gist.github.com/DerekChia/f12a4d60b7afc9bcb509595d3b727d6f" rel="noopener ugc nofollow" target="_blank"/>。如果您发现任何错误，并希望提出建议或改进，请随时发表评论或发<a class="ae ko" href="https://twitter.com/derekchia" rel="noopener ugc nofollow" target="_blank"> tweet me </a>。🙏</p><p id="63ad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">特别感谢<a class="ae ko" href="https://twitter.com/remykarem" rel="noopener ugc nofollow" target="_blank">莱米</a>、<a class="ae ko" href="https://github.com/notha99y" rel="noopener ugc nofollow" target="_blank">任杰</a>和<a class="ae ko" href="http://seowyuxin.com/" rel="noopener ugc nofollow" target="_blank">羽心</a>阅读本文初稿。你是最棒的！💪</p></div></div>    
</body>
</html>