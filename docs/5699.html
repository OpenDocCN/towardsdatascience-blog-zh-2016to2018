<html>
<head>
<title>Decrypting your Machine Learning model using LIME</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用石灰解密你的机器学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decrypting-your-machine-learning-model-using-lime-5adc035109b5?source=collection_archive---------1-----------------------#2018-11-04">https://towardsdatascience.com/decrypting-your-machine-learning-model-using-lime-5adc035109b5?source=collection_archive---------1-----------------------#2018-11-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="525c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">你为什么要相信你的模型？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4d30b02f28fe9b1331b97fa570282368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yK5G9nHmOD-wrJSRSvPEpw.jpeg"/></div></div></figure><p id="612a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最近，人们开始重新关注模型的可解释性。ML 专家能够理解模型可解释性在随后的业务适应中的重要性。模型可解释性的问题在于，很难以人类可以理解的方式定义模型的决策边界。LIME 是一个 python 库，它试图通过产生局部忠实的解释来解决模型的可解释性。下面是一个解释文本分类问题的例子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ln"><img src="../Images/7c84e70bf90477095cdb74b47b2ded39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k4SeJ22RFWUWokh3JYnLtg.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Example of an explanation by LIME for a binary classification model(atheism/Christian). The words (features) highlighted in blue support atheism.</figcaption></figure><p id="bfb2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="ls">本帖将涵盖以下主题:</em></p><ol class=""><li id="0ddf" class="lt lu iq kt b ku kv kx ky la lv le lw li lx lm ly lz ma mb bi translated"><em class="ls">信任你的模型的重要性</em></li><li id="80f7" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated"><em class="ls">什么是石灰？</em></li><li id="af68" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">什么使 LIME 成为一个好的模型解释者？</li><li id="807b" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">【LIME 如何实现模型可解释性？</li><li id="2060" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated"><em class="ls">在分类问题上使用石灰的实例</em></li></ol><h2 id="0633" class="mh mi iq bd mj mk ml dn mm mn mo dp mp la mq mr ms le mt mu mv li mw mx my mz bi translated"><strong class="ak">信任你的模型的重要性</strong></h2><p id="a9ce" class="pw-post-body-paragraph kr ks iq kt b ku na jr kw kx nb ju kz la nc lc ld le nd lg lh li ne lk ll lm ij bi translated">为了在模型中建立信任，我们运行多个交叉验证并执行拒绝集验证。这些模拟给出了未知数据上模型性能的汇总视图。这无助于理解为什么我们的一些预测是正确的，而另一些是错误的，我们也无法追踪我们的模型的决策路径。换句话说，我们无法理解它的学习或找出它的虚假结论。但是，如果我告诉你，有一种工具可以用人类可以理解的方式解释你的模型的决策边界，那会怎么样呢？这个魔法图书馆的名字叫莱姆。</p><h2 id="2795" class="mh mi iq bd mj mk ml dn mm mn mo dp mp la mq mr ms le mt mu mv li mw mx my mz bi translated"><strong class="ak">什么是石灰？</strong></h2><p id="7f24" class="pw-post-body-paragraph kr ks iq kt b ku na jr kw kx nb ju kz la nc lc ld le nd lg lh li ne lk ll lm ij bi translated">LIME(局部可解释模型不可知解释)是一种新颖的解释技术，它通过学习预测周围的局部可解释模型，以可解释和忠实的方式解释任何分类器的预测。</p><p id="46aa" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">LIME 在模型可解释性方面提供了什么？<br/> </strong> 1。一致的模型不可知论解释者[ <strong class="kt ir"> LIME </strong> ]。<br/> 2。一种选择具有解释[ <strong class="kt ir"> SP-LIME </strong> ]的代表性集合的方法，以确保模型在复制人类逻辑时行为一致。这个代表性的集合将提供对模型的直观的全局理解。</p><p id="d824" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">LIME 解释了一个预测，因此即使是非专家也可以通过特征工程对一个不可信的模型进行比较和改进。一个理想的模型解释器应该包含以下理想的特性:</p><ol class=""><li id="025b" class="lt lu iq kt b ku kv kx ky la lv le lw li lx lm ly lz ma mb bi translated"><strong class="kt ir">可解释的</strong> <br/>它应该提供输入变量和响应之间的定性理解。应该很好理解。</li><li id="b8c0" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">一个解释不可能完全忠实，除非它是模型本身的完整描述。已经说过，它应该至少是局部忠实的，即它必须在被预测的实例附近复制模型的行为。</li><li id="6f65" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated"><strong class="kt ir">模型不可知论者<br/> </strong>解释者应该能够解释任何模型，在提供解释的同时不应该对模型做任何假设。</li><li id="8647" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated"><strong class="kt ir">全局视角</strong> <br/>解释者应向用户解释一个代表性集合，以便用户对模型有一个全局直觉。</li></ol><h2 id="73b8" class="mh mi iq bd mj mk ml dn mm mn mo dp mp la mq mr ms le mt mu mv li mw mx my mz bi translated">什么使莱姆成为一个好的模型解释者？</h2><p id="e294" class="pw-post-body-paragraph kr ks iq kt b ku na jr kw kx nb ju kz la nc lc ld le nd lg lh li ne lk ll lm ij bi translated">让我们看看石灰是如何衡量这些特征的:</p><p id="0c45" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> 1。可解释的数据表示法<br/> </strong>使用人类理解的表示法，而不管模型使用的实际特征。这被称为可解释的表现。一个可解释的表示会随着我们正在处理的数据类型而变化，例如:<br/> 1。对于文本:它表示有/没有单词。<br/> 2。对于图像:它表示超像素(相似像素的连续块)的存在/不存在。<br/> 3。对于表格数据:它是列的加权组合。</p><p id="ba4a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="ls">简而言之，LIME 的解释者即使是非专家也能理解。</em></p><p id="3f1d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> 2。保真度-可解释性权衡</strong></p><p id="d947" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们想要一个忠实的(本地复制我们模型的行为)和可解释的(第一点)解释器。为了达到这一时间，最大限度地减少以下内容:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/3ae0b12a6eb275747bba70ce1aa054cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C8RYYBcCoBusd80G-2QBsQ.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Explanation model equation</figcaption></figure><p id="f504" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="ls">方程变量</em> <strong class="kt ir"> <em class="ls"> <br/> f </em> </strong>:原始预测值<br/> <strong class="kt ir"> x </strong>:原始特征<br/> <strong class="kt ir"> <em class="ls"> g </em> </strong> <em class="ls">:解释模型，可以是线性模型、决策树、 或者下降规则列出了<br/></em><strong class="kt ir"><em class="ls">Pi</em></strong><em class="ls">:z 的一个实例到 x 之间的邻近性度量，以定义 x 周围的局部性，它根据它们到 x 的距离来加权 z’(扰动的实例)<br/> </em> <strong class="kt ir"> <em class="ls">第一项</em></strong><em class="ls">:g 在 Pi 定义的局部性中逼近 f 的不忠实性的度量。 这在原始论文<br/> </em> <strong class="kt ir"> <em class="ls">中被称为</em> <strong class="kt ir"> <em class="ls">位置感知损失</em> </strong> <em class="ls">最后一项</em> </strong> <em class="ls">:解释模型复杂度 g 的度量。例如，如果你的解释模型是决策树，它可以是树的深度，或者在线性解释模型的情况下，它可以是非零权重的数量</em></p><p id="acbe" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="ls">速记以备将来参考<br/> 1。</em><strong class="kt ir"><em class="ls">x’</em></strong><em class="ls">(可解释表示)</em>:这个二进制向量是原始模型使用的实际特征的人类可理解版本。<br/> 2。<strong class="kt ir">z’</strong>(扰动样本):x’的非零元素的分数。<br/> 3。<strong class="kt ir"> f(z) </strong>:等级标签<br/> 4。<strong class="kt ir"> g(z') </strong>:这是 LIME 学习的模型(解释模型)。</p><p id="003b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了确保可解释性和局部保真度<strong class="kt ir"> <em class="ls">最小化位置感知损失</em> </strong>,同时保持第二项足够低以便人类能够解释。这将被引用为<strong class="kt ir"><em class="ls">ω(g)</em></strong><em class="ls">用于后文<br/> </em>的其余部分，同时优化位置感知损失时间以实现局部保真度。</p><p id="4b2d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> 3。用于局部探测的采样<br/> </strong>只是重申<strong class="kt ir"> g </strong>是要学习的模型，<strong class="kt ir">z’</strong>是训练数据的一个实例，<strong class="kt ir"> f (z) </strong>是 y。为了创建完整的训练集，我们从<strong class="kt ir">x’执行随机均匀采样。</strong>换句话说，我们从一行<strong class="kt ir"> x </strong>中创建多个<strong class="kt ir">z’</strong>(原始训练示例)。<br/>然后这些由<strong class="kt ir"> Pi(x) </strong>加权，以更加关注更接近 x 的<strong class="kt ir">z’</strong>。<br/>给定该数据集和标签，等式 1 被优化以学习解释模型。总而言之，LIME 提供的解释不依赖于原始模型的类型(模型不可知)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/6a8ef7e6b3bd4f6b98b54110d785c38b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*vE3PUuhG6RRgK1J9oxg0nA.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk"><em class="nh">The black-box model’s complex decision function f (unknown to LIME) is represented by the blue/pink background, which cannot be approximated well by a linear model. The bold red cross is the instance being explained. LIME samples instances get predictions using f and weigh them by the proximity to the instance being explained (represented here by size). The dashed line is the learned explanation that is locally (but not globally) faithful.</em></figcaption></figure><p id="1192" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> 4。稀疏线性解释<br/>T52】我们假设<br/> 1。<strong class="kt ir"> g(z') = w . z' </strong>(使解释模型线性)<strong class="kt ir"> <br/> </strong> 2 <strong class="kt ir">。局部感知损耗</strong> =平方损耗<br/> 3。<strong class="kt ir"> Pi(z) : </strong> exp(-D(x，z)(2)/sigma(2))(样本的近似加权)<br/> 4。<strong class="kt ir"> D(x，z) </strong>:距离函数</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/707a37030163fa19a37a7ee8fe770594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Buma_NJQN94q3hTdtHsd4g.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Locally-aware square loss</figcaption></figure><h2 id="80e6" class="mh mi iq bd mj mk ml dn mm mn mo dp mp la mq mr ms le mt mu mv li mw mx my mz bi translated"><strong class="ak">了解石灰算法</strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/272c8914491ac557274cd55bffc548dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2em4WEHHWn4lYd_jMSBaJA.png"/></div></div></figure><p id="ad1e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> K </strong>是解释时要考虑的变量数量的限制。例如，对于文本，K 是要考虑的字数，对于图像，是超像素的数量，对于表格数据，是列的数量。为了实现这一点，我们使<strong class="kt ir"> Omega </strong>趋向于无穷大，如果<strong class="kt ir"> size(w) &gt; K. </strong>总而言之，使用线性解释器来近似原始模型的决策边界。</p><p id="1bf1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第 2 部分:解释模型的子模型选择(SP-LIME)</strong></p><p id="613b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">LIME 旨在将模型的预测归因于人类可以理解的特征。为了做到这一点，我们需要在一组不同但有代表性的实例上运行解释模型，以返回一个非冗余的解释集，它是模型的全局表示。在介绍算法之前，让我们先了解一下先决条件:<br/> 1。<strong class="kt ir"> B </strong>(预算):用户愿意检查的说明数量<br/> 2。<strong class="kt ir">挑选步骤:</strong>从所有实例中挑选 B 实例的任务<br/> 3 .<strong class="kt ir"> <em class="ls"> W </em> </strong>(解释矩阵):<strong class="kt ir"> <em class="ls"> n </em> </strong>(样本数)<strong class="kt ir"><em class="ls">* d’</em></strong>(人类可理解的特征)矩阵<br/> 4 .<strong class="kt ir"> <em class="ls"> I(j): </em> </strong>解释空间中分量 j 的全局重要性<br/> 5。<strong class="kt ir"> V </strong>:说明<br/> 6 所考虑的特征<strong class="kt ir"> </strong>。<strong class="kt ir"><em class="ls">【V，W，I】</em></strong>:<strong class="kt ir"><em class="ls"/></strong>计算在集合 V 中的至少一个实例中出现的特征的总重要性。3)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/104a99879b71cfa789afb0e74ed88c1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sazx8yt0rFs1YlcN3VXSTw.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Nonredundant coverage intuition</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/b3e4ee264ddc490367eafc898bccf7b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HxpZBV14LSWuaCszlPZZzg.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Maximizing the weighted coverage function</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/a6ce6c3321e90d68ced19a5650f40203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2xhvZLglUmY2o-eWu13OSQ.png"/></div></div></figure><p id="a623" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="ls">算法 2 步骤<br/> </em> </strong> 1。对所有实例(所有 x)运行解释模型<br/> 2。计算单个组件的全局重要性<br/> 3。通过迭代地添加具有最高最大覆盖增益的实例来最大化覆盖函数。<br/> 4。Return V(代表性非冗余解释集)</p><p id="993a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">由此，我们可以看到 LIME 拥有理想模型解释器的所有 4 个理想属性。</p><h2 id="ba4c" class="mh mi iq bd mj mk ml dn mm mn mo dp mp la mq mr ms le mt mu mv li mw mx my mz bi translated"><em class="nh">在一个分类问题上使用石灰的实际例子</em></h2><p id="66f7" class="pw-post-body-paragraph kr ks iq kt b ku na jr kw kx nb ju kz la nc lc ld le nd lg lh li ne lk ll lm ij bi translated">下面是运行泰坦尼克号经典分类案例的模型解释的代码。我用 LightGBM 来训练模型(点击了解<a class="ae nn" href="https://medium.com/@abhisheksharma_57055/what-makes-lightgbm-lightning-fast-a27cf0d9785e" rel="noopener"> LightGBM </a>库，这里了解其<a class="ae nn" rel="noopener" target="_blank" href="/algorithms-for-hyperparameter-optimisation-in-python-edda4bdb167">优化</a>)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="0c9a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是对训练数据中第 1 行的解释</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/c55cbdee25e8c04524f7cc67d03dc559.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZamjNSnOPgIIunLXk76X0Q.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">orange colored features support class 1 and blue colored features support class 0</figcaption></figure><p id="41bb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用不同的数据实例运行最后两行，以获得不同的特征图。</p><p id="9411" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这个解释有三个部分:</p><ol class=""><li id="f5bc" class="lt lu iq kt b ku kv kx ky la lv le lw li lx lm ly lz ma mb bi translated">最左边部分显示预测概率</li><li id="fbdf" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">中间部分返回 5 个最重要的特性。对于二进制分类任务，它将是橙色/蓝色两种颜色。橙色的属性支持类别 1，蓝色的属性支持类别 0。Sex_le ≤0 支持 1 类。水平条上的浮点数代表这些特性的相对重要性。</li><li id="b32b" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">各部分的颜色编码是一致的。它包含前 5 个变量的实际值。</li></ol><p id="1ea3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用下面的代码运行 SP-LIME</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/d32a2495e13a60f15dee93328007f610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qVVx31mU0qZ4wYVh-Bn9Ag.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Results of running SP-LIME</figcaption></figure><p id="05b9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了在我们的模型中建立信任，我们不仅需要向 ML 专家解释模型，还需要向领域专家解释，这需要人类可以理解的解释。这是通过创建一个模型不可知的本地忠实解释集来实现的，它甚至可以帮助非专家理解原始模型是如何做出决策的。通过创建代表性样本集，LIME 为用户提供了模型决策边界的全局视图。这个模型的可解释性对于人类与 ML 系统的有效交互是至关重要的。解释个人预测在评估信任度时很重要，称赞也支持模型选择中的集合验证。</p><h2 id="abae" class="mh mi iq bd mj mk ml dn mm mn mo dp mp la mq mr ms le mt mu mv li mw mx my mz bi translated"><strong class="ak">参考文献</strong></h2><ol class=""><li id="e648" class="lt lu iq kt b ku na kx nb la ns le nt li nu lm ly lz ma mb bi translated">原文链接:【https://arxiv.org/abs/1602.04938 T4】</li><li id="0145" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">Github 链接:<a class="ae nn" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank">https://github.com/marcotcr/lime</a></li><li id="2aa7" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">博客作者:论文作者:<a class="ae nn" href="https://homes.cs.washington.edu/~marcotcr/blog/lime/" rel="noopener ugc nofollow" target="_blank">https://homes.cs.washington.edu/~marcotcr/blog/lime/</a></li></ol><p id="da04" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">请在下面分享您的想法、反馈或建议。</p></div></div>    
</body>
</html>