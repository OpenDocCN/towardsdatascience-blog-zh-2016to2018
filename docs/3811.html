<html>
<head>
<title>Besides Word Embedding, why you need to know Character Embedding?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">除了单词嵌入，为什么你需要知道字符嵌入？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/besides-word-embedding-why-you-need-to-know-character-embedding-6096a34a3b10?source=collection_archive---------1-----------------------#2018-06-21">https://towardsdatascience.com/besides-word-embedding-why-you-need-to-know-character-embedding-6096a34a3b10?source=collection_archive---------1-----------------------#2018-06-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/1c61d73d47b0ebd95d1ee7988f7d7221.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EOcACx6LFWH7wf1y"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@dilucidus?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Kai D.</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="27e1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2013 年，Tomas Mikolov [1]引入了单词嵌入来学习更好的单词质量。那时，单词嵌入是处理文本的最新技术。后来，也引入了 doc2vec。如果我们从另一个角度思考呢？不是从一个单词到一个文档的聚合，而是从一个字符到一个单词的聚合。</p><p id="d696" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，你将经历什么，为什么，什么时候以及如何进行字符嵌入。</p><h1 id="2714" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">什么？</h1><p id="3da9" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">项和扬恩[2]介绍了人物 CNN。他们发现字符包含了改善模型性能的关键信号。在本文中，字符表定义了 70 个字符，其中包括 26 个英文字母、10 个数字、33 个特殊字符和换行符。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/e710c7c9c2dee9f08a71ef5d538dee52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*tMTUPcxnRrc452e5IrLKqA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Capture from Text Understanding from Scratch (Xiang and Yann, 2016)</figcaption></figure><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="cace" class="mr lf it mn b gy ms mt l mu mv"># Copy from Char CNN paper</span><span id="c37d" class="mr lf it mn b gy mw mt l mu mv">abcdefghijklmnopqrstuvwxyz0123456789 -,;.!?:’’’/\|_@#$%ˆ&amp;*˜‘+-=()[]{}</span></pre><p id="b0a4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一方面，Google Brain 团队引入了<a class="ae kf" href="https://arxiv.org/pdf/1602.02410.pdf" rel="noopener ugc nofollow" target="_blank">探索语言建模的极限</a>并发布了<a class="ae kf" href="https://github.com/tensorflow/models/tree/master/research/lm_1b" rel="noopener ugc nofollow" target="_blank"> lm_1b </a>模型，该模型包含 256 个向量(包括 52 个字符、特殊字符)，维数仅为 16。与单词嵌入相比，维数可以增加到 300，而向量的数量是巨大的。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/405a828e047c68425dfe594952d71bdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*VfJM5rAt7xLEr5RGv450qw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Capture from Exploring the Limits of Language Modeling ( Rafal et al., 2016)</figcaption></figure><h1 id="1e21" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">为什么？</h1><p id="76ec" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在英语中，所有的单词都是由 26 个字符组成的(如果同时包含大小写字符，则为 52 个字符，如果包含特殊字符，则为更多)。有了字符嵌入，每个单词的<strong class="ki iu">向量都可以形成，即使是未登录词</strong>(可选)。另一方面，单词嵌入只能处理那些看到的单词。</p><p id="667a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">Another benefit is that it good fits for <strong class="ki iu">misspelling words, emoticons, new words (e.g. In 2018, Oxford English Dictionary introduced new word which is boba tea 波霸奶茶. Before that we do not have any pre-trained word embedding for that)</strong>.</p><p id="67f3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它比 word2vec 嵌入更好地处理不常用的单词，因为后者缺乏足够的训练机会来训练这些不常用的单词。</p><p id="85d8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第三个原因是，由于只有少量的向量，它<strong class="ki iu">降低了模型的复杂性并提高了性能</strong>(就速度而言)</p><h1 id="8335" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">什么时候？</h1><p id="6009" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在 NLP 中，我们可以将字符嵌入应用于:</p><ul class=""><li id="d404" class="my mz it ki b kj kk kn ko kr na kv nb kz nc ld nd ne nf ng bi translated"><a class="ae kf" href="https://arxiv.org/pdf/1509.01626.pdf" rel="noopener ugc nofollow" target="_blank">文本分类</a></li><li id="2c81" class="my mz it ki b kj nh kn ni kr nj kv nk kz nl ld nd ne nf ng bi translated"><a class="ae kf" href="https://arxiv.org/pdf/1602.02410.pdf" rel="noopener ugc nofollow" target="_blank">语言模型</a></li><li id="1fea" class="my mz it ki b kj nh kn ni kr nj kv nk kz nl ld nd ne nf ng bi translated"><a class="ae kf" href="https://www.aclweb.org/anthology/Q16-1026" rel="noopener ugc nofollow" target="_blank">命名实体识别</a></li></ul><h1 id="ebb4" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">怎么会？</h1><p id="a203" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated"><strong class="ki iu"> <em class="nm">字符嵌入</em> </strong></p><ol class=""><li id="dc8f" class="my mz it ki b kj kk kn ko kr na kv nb kz nc ld nn ne nf ng bi translated">定义一个列表中的一个字符(即 m)。例如，可以使用字母数字和一些特殊字符。对于我的例子，英语字符(52)，数字(10)，特殊字符(20)和一个未知字符，UNK。总共 83 个字符。</li><li id="8a6c" class="my mz it ki b kj nh kn ni kr nj kv nk kz nl ld nn ne nf ng bi translated">将字符转换为 1-hot 编码，得到一个向量序列。对于未知字符和空白字符，用全零向量代替。如果超过预定义的最大字符长度(即 l)，则忽略它。每个字符的输出是 16 维向量。</li><li id="b0d1" class="my mz it ki b kj nh kn ni kr nj kv nk kz nl ld nn ne nf ng bi translated">使用 3 个 1D CNN 层(可配置)来学习序列</li></ol><p id="6c24" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nm">句子嵌入</em> </strong></p><ol class=""><li id="b7ae" class="my mz it ki b kj kk kn ko kr na kv nb kz nc ld nn ne nf ng bi translated">双向 LSTM 遵循 CNN 层</li><li id="fc12" class="my mz it ki b kj nh kn ni kr nj kv nk kz nl ld nn ne nf ng bi translated">LSTM 之后添加了一些辍学层。</li></ol><h1 id="4cf9" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">密码</h1><p id="3eac" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">这个示例代码将演示如何使用字符嵌入来进行分类。测试数据集可以在<a class="ae kf" href="http://archive.ics.uci.edu/ml/datasets/News+Aggregator" rel="noopener ugc nofollow" target="_blank"> UCI ML 知识库</a>中找到。</p><p id="377b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nm">预处理</em> </strong></p><p id="6a15" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们需要准备元信息，包括字符字典和将标签从文本转换为数字(因为 keras 只支持数字输入)。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="39f7" class="mr lf it mn b gy ms mt l mu mv">preporcess(labels=df['category'].unique())</span></pre><p id="f7f1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="aa16" class="mr lf it mn b gy ms mt l mu mv">-----&gt; Stage: preprocess<br/>Totoal number of chars: 83<br/>First 3 char_indices sample: {';': 0, '"': 1, 'A': 2}<br/>First 3 indices_char sample: {0: ';', 1: '"', 2: 'A'}<br/>Label to Index:  {'b': 0, 'e': 2, 'm': 3, 't': 1}<br/>Index to Label:  {0: 'b', 1: 't', 2: 'e', 3: 'm'}</span></pre><p id="3db5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nm">流程</em> </strong></p><p id="100c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们必须将原始输入训练数据和测试转换为 keras 输入的 numpy 格式</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="bf24" class="mr lf it mn b gy ms mt l mu mv">x_train, y_train = char_cnn.process(<br/>    df=train_df, x_col='headline', y_col='category')<br/>x_test, y_test = char_cnn.process(<br/>    df=test_df, x_col='headline', y_col='category')</span></pre><p id="1fd0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="51b9" class="mr lf it mn b gy ms mt l mu mv">-----&gt; Stage: process<br/>Number of news: 100<br/>Actual max sentence: 3<br/>Train Shape:  (100, 5, 256) (100,)<br/>-----&gt; Stage: process<br/>Number of news: 10<br/>Actual max sentence: 1<br/>Train Shape:  (10, 5, 256) (10,)</span></pre><p id="4040" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nm">建立模型</em> </strong></p><p id="22ec" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">利用 CNN 层和双向 LSTM 构建字符 CNN 文本分类器。首先，用一维 CNN 建立字符块</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="507f" class="mr lf it mn b gy ms mt l mu mv">def _build_character_block(<br/>    self, block, dropout=0.3, <br/>    filters=[64, 100], kernel_size=[3, 3], <br/>    pool_size=[2, 2], padding='valid', activation='relu', <br/>    kernel_initializer='glorot_normal'):<br/>        <br/>    for i in range(len(filters)):<br/>        block = Conv1D(<br/>            filters=filters[i], kernel_size=kernel_size[i],<br/>            padding=padding, activation=activation,<br/>            kernel_initializer=kernel_initializer)(block)</span><span id="8804" class="mr lf it mn b gy mw mt l mu mv">        block = Dropout(dropout)(block)<br/>        block = MaxPooling1D(pool_size=pool_size[i])(block)</span><span id="0b7e" class="mr lf it mn b gy mw mt l mu mv">    block = GlobalMaxPool1D()(block)<br/>    block = Dense(128, activation='relu')(block)<br/>    return block</span></pre><p id="2e9c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">句子级别块将使用 3 个字符块构建</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="26fb" class="mr lf it mn b gy ms mt l mu mv">def _build_sentence_block(<br/>    self, max_len_of_sentence, max_num_of_setnence, <br/>    filters=[[100, 200, 200], [200, 300, 300], [300, 400, 400]], <br/>    kernel_sizes=[[4, 3, 3], [5, 3, 3], [6, 3, 3]], <br/>    pool_sizes=[[2, 2, 2], [2, 2, 2], [2, 2, 2]],<br/>    dropout=0.4):<br/>        <br/>    sent_input = Input(shape=(max_len_of_sentence, ), dtype='int64')<br/>    embedded = Embedding(self.num_of_char, char_dimension,<br/>        input_length=max_len_of_sentence)(sent_input)<br/>        <br/>    blocks = []<br/>    for i, filter_layers in enumerate(filters):<br/>        blocks.append(<br/>            self._build_character_block(<br/>                    block=embedded, filters=filter_layers,<br/>                    kernel_size=kernel_sizes[i],<br/>                    pool_size=pool_sizes[i])<br/>            )</span><span id="f664" class="mr lf it mn b gy mw mt l mu mv">    sent_output = concatenate(blocks, axis=-1)<br/>    sent_output = Dropout(dropout)(sent_output)<br/>    sent_encoder = Model(inputs=sent_input, outputs=sent_output)</span><span id="19f7" class="mr lf it mn b gy mw mt l mu mv">    return sent_encoder</span></pre><p id="5cc5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们将根据句子建立文档块</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="bdd5" class="mr lf it mn b gy ms mt l mu mv">def _build_document_block(<br/>    self, sent_encoder, max_len_of_sentence, max_num_of_setnence, <br/>    num_of_label, dropout=0.3, <br/>    loss='sparse_categorical_crossentropy', optimizer='rmsprop',<br/>    metrics=['accuracy']):<br/>    <br/>    doc_input = Input(shape=(<br/>        max_num_of_setnence, max_len_of_sentence), dtype='int64')<br/>    doc_output = TimeDistributed(sent_encoder)(doc_input)<br/>    doc_output = Bidirectional(LSTM(<br/>        128, return_sequences=False, <br/>        dropout=dropout))(doc_output)</span><span id="10e8" class="mr lf it mn b gy mw mt l mu mv">    doc_output = Dropout(dropout)(doc_output)<br/>    doc_output = Dense(128, activation='relu')(doc_output)<br/>    doc_output = Dropout(dropout)(doc_output)<br/>    doc_output = Dense(<br/>        num_of_label, activation='sigmoid')(doc_output)</span><span id="f677" class="mr lf it mn b gy mw mt l mu mv">    doc_encoder = Model(inputs=doc_input, outputs=doc_output)<br/>    doc_encoder.compile(<br/>        loss=loss, optimizer=optimizer, metrics=metrics)<br/>    return doc_encoder</span></pre><p id="df0d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出</p><p id="d1db" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nm">训练</em> </strong></p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="5098" class="mr lf it mn b gy ms mt l mu mv">train(x_train, y_train, x_test, y_test, epochs=10)</span></pre><p id="4b81" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/9794d72c705bb757cb724f21784e6893.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yWWfPyyRKtlCVqr5zHUzjw.png"/></div></div></figure><h1 id="af30" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">结论</h1><p id="55b8" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">你可以从<a class="ae kf" href="https://github.com/makcedward/nlp/blob/master/sample/nlp-character_embedding.ipynb" rel="noopener ugc nofollow" target="_blank"> github </a>中找到所有代码。</p><p id="e1ef" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">字符嵌入是解决大量文本分类的绝妙设计。它解决了一些单词嵌入问题。字符嵌入和单词嵌入的区别在于，字符嵌入可以构建任何单词，只要包含这些字符。</p><p id="ea90" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">脸书艾研究(<a class="ae kf" href="https://research.fb.com/category/facebook-ai-research/" rel="noopener ugc nofollow" target="_blank">尚可</a>)做了更进一步。小组使用子词来训练模型。以“脸书”为例，他们用“F”、“Fa”、“Fac”等来训练单词。关于 fasttext 的详细信息，您可以访问他们的网站<a class="ae kf" href="https://fasttext.cc" rel="noopener ugc nofollow" target="_blank">以便更好地了解。</a></p><p id="ca3e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是对字符嵌入的一些评论，因为它不包括任何词义，而只是使用字符。我们可以同时包括字符嵌入和单词嵌入来解决我们的 NLP 问题。</p><h1 id="ce2d" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">额外材料</h1><p id="9dd5" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated"><a class="ae kf" href="http://www.iro.umontreal.ca/~lisa/pointeurs/ir0895-he-2.pdf" rel="noopener ugc nofollow" target="_blank">用于信息检索的卷积池结构潜在语义模型</a></p><p id="dd6d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="http://proceedings.mlr.press/v32/santos14.pdf" rel="noopener ugc nofollow" target="_blank">学习词性标注的字符级表示</a></p><p id="057b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://arxiv.org/pdf/1509.01626.pdf" rel="noopener ugc nofollow" target="_blank">用于文本分类的字符级卷积网络</a></p><p id="3ee1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://offbit.github.io/how-to-read/" rel="noopener ugc nofollow" target="_blank">情感分析中的人物级深度学习</a></p><h1 id="abcd" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">关于我</h1><p id="dce4" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以通过<a class="ae kf" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae kf" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae kf" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="af9f" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">参考</h1><p id="7912" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">[1]托马斯·米科洛夫、格雷戈·科拉多、程凯和杰弗里·迪恩。向量空间中单词表示的有效估计。2013 年 9 月。<a class="ae kf" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1301.3781.pdf</a></p><p id="da27" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]张翔，颜乐存。文本理解从零开始。2016.<a class="ae kf" href="https://arxiv.org/pdf/1502.01710v5.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1502.01710v5.pdf</a></p><p id="c81f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]金伊、杰尼特伊、桑塔格、拉什..字符感知神经语言模型。2015.<a class="ae kf" href="https://arxiv.org/pdf/1508.06615.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1508.06615.pdf</a></p></div></div>    
</body>
</html>