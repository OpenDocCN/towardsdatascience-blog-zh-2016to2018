<html>
<head>
<title>A Gentle Introduction to Maximum Likelihood Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最大似然估计简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-9fbff27ea12f?source=collection_archive---------0-----------------------#2018-02-20">https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-9fbff27ea12f?source=collection_archive---------0-----------------------#2018-02-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="3293" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我第一次听到有人使用术语<strong class="js iu">最大似然估计</strong>时，我去了谷歌，并找出了它的意思。然后我去<a class="ae ko" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">维基百科</a>了解它真正的意思。我得到了这个:</p><blockquote class="kp kq kr"><p id="51f1" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated">在统计学中，<strong class="js iu">最大似然估计</strong> ( <strong class="js iu"> MLE </strong>)是一种<a class="ae ko" href="https://en.wikipedia.org/wiki/Estimator" rel="noopener ugc nofollow" target="_blank">估计</a><a class="ae ko" href="https://en.wikipedia.org/wiki/Statistical_model" rel="noopener ugc nofollow" target="_blank">统计模型</a>给定观测值的<a class="ae ko" href="https://en.wikipedia.org/wiki/Statistical_parameter" rel="noopener ugc nofollow" target="_blank">参数</a>的方法，通过寻找使给定参数下进行观测的<a class="ae ko" href="https://en.wikipedia.org/wiki/Likelihood" rel="noopener ugc nofollow" target="_blank">似然</a>最大化的参数值。MLE可以被视为<a class="ae ko" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank">最大后验概率估计</a> (MAP)的特殊情况，其假设参数的<a class="ae ko" href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)" rel="noopener ugc nofollow" target="_blank">均匀</a> <a class="ae ko" href="https://en.wikipedia.org/wiki/Prior_probability" rel="noopener ugc nofollow" target="_blank">先验分布</a>，或者被视为忽略先验分布的MAP的变体，因此其<a class="ae ko" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" rel="noopener ugc nofollow" target="_blank">不规则</a>。</p></blockquote><p id="2c41" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ks">好吗？？</em></p><p id="2509" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了避免你为理解MLE并将其融入你的数据科学工作流程、理念和项目而绞尽脑汁，我编辑了这个指南。下面，我们将:</p><ul class=""><li id="bb01" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">将概率上下文设置为MLE</li><li id="df6a" class="kw kx it js b jt lf jx lg kb lh kf li kj lj kn lb lc ld le bi translated">钻研所需的数学</li><li id="206d" class="kw kx it js b jt lf jx lg kb lh kf li kj lj kn lb lc ld le bi translated">看看MLE在Python中是如何工作的</li><li id="f274" class="kw kx it js b jt lf jx lg kb lh kf li kj lj kn lb lc ld le bi translated">利用MLE探索数据科学的最佳实践</li></ul><p id="bc64" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">不过先来点<a class="ae ko" href="https://xkcd.com" rel="noopener ugc nofollow" target="_blank"> xkcd </a>:</p><h1 id="72b5" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated"><a class="ae ko" href="https://xkcd.com/1132/" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">常客vs .贝叶斯</strong> </a></h1><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mi"><img src="../Images/f799e444b6bff465db1b27a219dc4124.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kJHUF87cQNkxxPbeyEgHig.png"/></div></div></figure><p id="7b30" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这很有趣(如果你遵循这个奇怪的幽默领域)，而且这两个阵营之间的差异基本上是正确的。不要介意我们的太阳进入新星并不是一个真正可重复的实验——抱歉，常客们！—我们可以概括为，对于真实的观察和研究，两个阵营通常会得出类似的结论，但当研究设计或数据开始变得棘手时，就会有很大的不同。</p><p id="26c3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">简而言之，MLE帮助我们回答了这个问题:</p><blockquote class="mu"><p id="720d" class="mv mw it bd mx my mz na nb nc nd kn dk translated">哪些参数/系数最适合我的模型？</p></blockquote><p id="5248" class="pw-post-body-paragraph jq jr it js b jt ne jv jw jx nf jz ka kb ng kd ke kf nh kh ki kj ni kl km kn im bi translated">有趣的是，你可以用这两种观点中的任何一种来解释MLE为什么有效！因为，虽然最大似然估计给出了一个现场估计——这在frequentist输出中很常见——但它可以被视为<a class="ae ko" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank">最大后验概率</a> (MAP)估计的一个特例，在这里我们使用了一个天真的先验知识，并且从不费心去更新它。</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h1 id="a97f" class="lk ll it bd lm ln nq lp lq lr nr lt lu lv ns lx ly lz nt mb mc md nu mf mg mh bi translated">设置我们的问题</h1><p id="81b7" class="pw-post-body-paragraph jq jr it js b jt nv jv jw jx nw jz ka kb nx kd ke kf ny kh ki kj nz kl km kn im bi translated">今天为了接近MLE，让我们从贝叶斯的角度出发，使用<a class="ae ko" href="https://www.bayestheorem.net" rel="noopener ugc nofollow" target="_blank">贝叶斯定理</a>将我们的问题框定如下:</p><pre class="mj mk ml mm gt oa ob oc od aw oe bi"><span id="3654" class="of ll it ob b gy og oh l oi oj">P(β∣y) = P(y∣β) x P(β) / P(y)</span></pre><p id="36ea" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">或者，用英语说:</p><pre class="mj mk ml mm gt oa ob oc od aw oe bi"><span id="9de6" class="of ll it ob b gy og oh l oi oj">posterior = likelihood x prior / evidence</span></pre><p id="3037" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以有效地忽略<code class="fe ok ol om ob b">prior</code>和<code class="fe ok ol om ob b">evidence</code>,因为——给定均匀先验分布的Wiki定义——所有系数值都是同等可能的。而且所有数据值的概率(假设连续)都是相等的，而且基本上为零。</p><p id="47dd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以，在实际的英语中:给定一些特定系数的概率，我看到一些结果，涉及到以完全相反的方式提出问题。这很有帮助，因为这个问题更容易解决。</p><h2 id="48ee" class="of ll it bd lm on oo dn lq op oq dp lu kb or os ly kf ot ou mc kj ov ow mg ox bi translated">概率和可能性</h2><p id="1b87" class="pw-post-body-paragraph jq jr it js b jt nv jv jw jx nw jz ka kb nx kd ke kf ny kh ki kj nz kl km kn im bi translated">从今以后，我们将在代码中引入可能性的概念，或<code class="fe ok ol om ob b">L</code>。为了理解其中的区别，我将从Randy Gallistel的<a class="ae ko" href="https://www.psychologicalscience.org/observer/bayes-for-beginners-probability-and-likelihood" rel="noopener ugc nofollow" target="_blank">精彩文章</a>中摘录:</p><blockquote class="kp kq kr"><p id="b4f7" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated">概率和可能性之间的区别是非常重要的:概率与可能的结果相联系；可能性附属于假设。</p><p id="fb99" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated">可能的结果是互斥的和穷尽的。假设我们让一个受试者预测10次投掷硬币的结果。只有11个可能的结果(0到10个正确的预测)。实际的结果总是一个又一个可能的结果。因此，可能结果的概率总和必须为1。</p><p id="f20f" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated">与结果不同，假设既不是互斥的，也不是穷尽的。假设我们测试的第一个对象正确预测了10个结果中的7个。我可能会假设受试者只是猜测，你可能会假设受试者<em class="it">可能</em>有某种千里眼，你的意思是受试者可能被期望以稍高于概率的长期正确预测结果。这些是不同的假设，但并不互相排斥，因为你说“<em class="it">可能</em>是”的时候就对冲了。你因此允许你的假设包括我的。用专业术语来说，我的假设嵌套在你的假设中。其他人可能会假设受试者有很强的透视能力，观察到的结果低估了她下一次预测正确的可能性。另一个人可以假设一些完全不同的东西。人们可以接受的假设是无限的。</p><p id="6503" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated">我们赋予可能性的一系列假设受到我们虚构它们的能力的限制。实际上，我们很少能确信我们已经设想了所有可能的假设。我们关心的是估计实验结果在多大程度上影响我们和其他人目前所接受的假设的相对可能性。因为我们通常不考虑所有的替代假设，而且因为一些假设嵌套在其他假设中，我们附加到假设上的可能性本身没有任何意义；只有相对可能性——也就是两个可能性的比率——才有意义。</p></blockquote><p id="51bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">太神奇了！谢谢你兰迪。</p><p id="ecd5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">MLE是频率主义者，但可以从贝叶斯的角度出发:</p><ul class=""><li id="9903" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">常客可以声称MLE，因为它是一个<strong class="js iu">逐点估计</strong>(不是一个分布)，并且它假设<strong class="js iu">没有先验分布</strong>(技术上，不知情或一致)。</li><li id="b28b" class="kw kx it js b jt lf jx lg kb lh kf li kj lj kn lb lc ld le bi translated">此外，MLE没有给出真实参数值的95%概率区域。</li><li id="dccf" class="kw kx it js b jt lf jx lg kb lh kf li kj lj kn lb lc ld le bi translated">然而，最大似然法是一种特殊形式的映射，并且使用了似然的概念，这是贝叶斯哲学的核心。</li></ul><p id="11ad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当心天真或一致的先验假设！！您可能会将数据错误地归因于一个极不可能的模型。你可能会成为辛普森悖论的受害者，如下图。你很容易被小样本欺骗。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/eb94660648c85f0a0cd14807dbe7f3de.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*1GpPTLRm6rgaSqZ9jMrMIA.png"/></div><figcaption class="oz pa gj gh gi pb pc bd b be z dk">Simpson’s Paradox</figcaption></figure><p id="44a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以上都是常客和数据科学家必须处理或意识到的问题，所以MLE没有什么本质上更糟糕的。</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h2 id="68f9" class="of ll it bd lm on oo dn lq op oq dp lu kb or os ly kf ot ou mc kj ov ow mg ox bi translated">回到我们的问题</h2><p id="9abf" class="pw-post-body-paragraph jq jr it js b jt nv jv jw jx nw jz ka kb nx kd ke kf ny kh ki kj nz kl km kn im bi translated">所以如果<code class="fe ok ol om ob b">p(y|<em class="ks">β</em>)</code>相当于<code class="fe ok ol om ob b"> L(<em class="ks">β</em>|y)</code>，那么<code class="fe ok ol om ob b">p(y_1,y_2,...,y_n|<em class="ks">β</em>)</code>相当于<code class="fe ok ol om ob b">L(<em class="ks">β</em>|y_1,y_2,...,y_n)</code>。另外，请记住，我们可以将独立概率相乘，就像这样:</p><p id="efd3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe ok ol om ob b">p(A,B) = p(A)p(B)</code></p><p id="34de" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们越来越近了！这是我们当前的设置:</p><p id="bdcf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe ok ol om ob b">L(<em class="ks">β</em>|y1,y2,…,yn) = p(y1|<em class="ks">β</em>)p(y2|<em class="ks">β</em>),…,p(yn|<em class="ks">β</em>) = ∏p(yi|<em class="ks">β</em>)</code></p><p id="bfcb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">右边的部分看起来像是我们可以最大化的东西:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/44d4c85856f29ab05390ffdb94975d65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*RttVmcCwV5Xr4OXemx5txg.jpeg"/></div><figcaption class="oz pa gj gh gi pb pc bd b be z dk">Initial Cost Function</figcaption></figure><p id="ae3d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是我们可以做得更好！用自然对数把我们的积函数变成和函数怎么样？日志是<a class="ae ko" href="https://en.wikipedia.org/wiki/Monotonic_function" rel="noopener ugc nofollow" target="_blank">单调变换</a>，所以我们将简化我们的计算，但保持我们的最佳结果。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/48853c04a1b114a545b1c70a34afc25d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*mjde9NSmFlEi5HD6Y-9syw.jpeg"/></div><figcaption class="oz pa gj gh gi pb pc bd b be z dk">Halfway there!</figcaption></figure><p id="45e9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们最终的成本函数如下所示:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/365022f141b240bfdcfbe662373802e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*FH2_KIPaqW9u_KxmVN_nYA.jpeg"/></div><figcaption class="oz pa gj gh gi pb pc bd b be z dk">Ready to roll!</figcaption></figure><p id="0982" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了简单起见，让我们假设我们有一个回归问题，所以我们的结果是连续的。最大似然法对于离散结果的分类问题非常有效，但是我们必须使用不同的分布函数，这取决于我们有多少个类，等等。</p><p id="7533" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，记住普通最小二乘法(OLS)等模型的一个中心假设是残差正态分布在均值零附近，我们拟合的OLS模型实际上成为了最大期望值<code class="fe ok ol om ob b">y</code>的体现。而我们的概率分布是… <a class="ae ko" href="http://mathworld.wolfram.com/NormalDistribution.html" rel="noopener ugc nofollow" target="_blank">正态</a>！</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/629897bb48a96679bc04b899ea496382.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*vLUAZ4Hhg_Uh8T-5bvTlXg.jpeg"/></div><figcaption class="oz pa gj gh gi pb pc bd b be z dk">y is normally distributed around our ŷ</figcaption></figure><p id="b98e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为计算机在计算概率方面比我们好得多，所以我们将从这里转向Python！</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h1 id="d232" class="lk ll it bd lm ln nq lp lq lr nr lt lu lv ns lx ly lz nt mb mc md nu mf mg mh bi translated">Python中的MLE</h1><p id="5be6" class="pw-post-body-paragraph jq jr it js b jt nv jv jw jx nw jz ka kb nx kd ke kf ny kh ki kj nz kl km kn im bi translated">在您的数据科学建模管道中实现MLE可能非常简单，有多种方法。下面是一个你可以偷着开始的方法。</p><h2 id="f7ee" class="of ll it bd lm on oo dn lq op oq dp lu kb or os ly kf ot ou mc kj ov ow mg ox bi translated">设置</h2><p id="2d61" class="pw-post-body-paragraph jq jr it js b jt nv jv jw jx nw jz ka kb nx kd ke kf ny kh ki kj nz kl km kn im bi translated">如果导入正确的包，MLE很容易:</p><pre class="mj mk ml mm gt oa ob oc od aw oe bi"><span id="2565" class="of ll it ob b gy og oh l oi oj"># import libraries<br/>import numpy as np, pandas as pd<br/>from matplotlib import pyplot as plt<br/>import seaborn as sns<br/>from scipy.optimize import minimize<br/>import scipy.stats as stats</span><span id="8802" class="of ll it ob b gy ph oh l oi oj">import pymc3 as pm3<br/>import numdifftools as ndt<br/>import statsmodels.api as sm<br/>from statsmodels.base.model import GenericLikelihoodModel<br/>%matplotlib inline</span></pre><p id="0537" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在此基础上，我们将生成遵循围绕基本事实函数的正态分布误差的数据:</p><pre class="mj mk ml mm gt oa ob oc od aw oe bi"><span id="756a" class="of ll it ob b gy og oh l oi oj"># generate data<br/>N = 100<br/>x = np.linspace(0,20,N)<br/>ϵ = np.random.normal(loc = 0.0, scale = 5.0, size = N)<br/>y = 3*x + ϵ</span><span id="a296" class="of ll it ob b gy ph oh l oi oj">df = pd.DataFrame({‘y’:y, ‘x’:x})<br/>df[‘constant’] = 1</span></pre><p id="e56b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，让我们使用Seaborn的<a class="ae ko" href="https://seaborn.pydata.org/generated/seaborn.regplot.html" rel="noopener ugc nofollow" target="_blank"> regplot </a>进行可视化:</p><pre class="mj mk ml mm gt oa ob oc od aw oe bi"><span id="a3a3" class="of ll it ob b gy og oh l oi oj"># plot<br/>sns.regplot(df.x, df.y);</span></pre><p id="816f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我得到了下面的，你应该看到类似的东西。但是，请记住这里有随机性，我们没有使用种子:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi pi"><img src="../Images/5f4992faead0f8fae5df64e27663a1e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rhMwpP8VSZPnc6ebYztrVg.jpeg"/></div></div><figcaption class="oz pa gj gh gi pb pc bd b be z dk">Scatter plot with OLS line and confidence intervals</figcaption></figure></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h2 id="b221" class="of ll it bd lm on oo dn lq op oq dp lu kb or os ly kf ot ou mc kj ov ow mg ox bi translated">使用Statsmodels为OLS建模</h2><p id="31cc" class="pw-post-body-paragraph jq jr it js b jt nv jv jw jx nw jz ka kb nx kd ke kf ny kh ki kj nz kl km kn im bi translated">由于我们创建了类似回归的连续数据，我们将使用<code class="fe ok ol om ob b">sm.OLS</code>来计算最佳系数和对数似然(ll)作为基准。</p><pre class="mj mk ml mm gt oa ob oc od aw oe bi"><span id="d095" class="of ll it ob b gy og oh l oi oj"># split features and target<br/>X = df[[‘constant’, ‘x’]]</span><span id="d9db" class="of ll it ob b gy ph oh l oi oj"># fit model and summarize<br/>sm.OLS(y,X).fit().summary()</span></pre><p id="8ebf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我得到这个，并将记录拟合模型的系数:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi pj"><img src="../Images/6bf727fa07a015dcedf1fae2f7cc3d43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f1ReFJYOoOGfVm1mD515LA.jpeg"/></div></div></figure><p id="79fe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意<code class="fe ok ol om ob b">constant</code>接近于零，对于我们使用的地面真实发生器，特征<code class="fe ok ol om ob b">x</code>的<code class="fe ok ol om ob b">beta</code>接近于3。</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h2 id="2eb4" class="of ll it bd lm on oo dn lq op oq dp lu kb or os ly kf ot ou mc kj ov ow mg ox bi translated">最大化LL以求解最佳系数</h2><p id="3e76" class="pw-post-body-paragraph jq jr it js b jt nv jv jw jx nw jz ka kb nx kd ke kf ny kh ki kj nz kl km kn im bi translated">从这里开始，我们将使用软件包和自定义函数的组合，看看我们是否可以使用MLE方法计算相同的OLS结果。</p><p id="625e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为<code class="fe ok ol om ob b">scipy.optimize</code>只有一个<code class="fe ok ol om ob b">minimize</code>方法，我们将最小化对数似然的负值。这甚至是他们推荐的<a class="ae ko" href="https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html" rel="noopener ugc nofollow" target="_blank">！数学欺骗通常比重新发明轮子更快更容易！</a></p><p id="4afb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以构建一个简单的函数，一次完成回归输出的所有工作:</p><pre class="mj mk ml mm gt oa ob oc od aw oe bi"><span id="ef58" class="of ll it ob b gy og oh l oi oj"># define likelihood function<br/>def MLERegression(params):<br/> intercept, beta, sd = params[0], params[1], params[2] # inputs are guesses at our parameters<br/> yhat = intercept + beta*x # predictions</span><span id="84d4" class="of ll it ob b gy ph oh l oi oj"># next, we flip the Bayesian question<br/># compute PDF of observed values normally distributed around mean (yhat)<br/># with a standard deviation of sd<br/> negLL = -np.sum( stats.norm.logpdf(y, loc=yhat, scale=sd) )</span><span id="4849" class="of ll it ob b gy ph oh l oi oj"># return negative LL<br/> return(negLL)</span></pre><p id="170f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们有了一个成本函数，让我们初始化并最小化它:</p><pre class="mj mk ml mm gt oa ob oc od aw oe bi"><span id="b40d" class="of ll it ob b gy og oh l oi oj"># let’s start with some random coefficient guesses and optimize<br/>guess = np.array([5,5,2])</span><span id="a71f" class="of ll it ob b gy ph oh l oi oj">results = minimize(MLERegression, guess, method = ‘Nelder-Mead’, <br/> options={‘disp’: True})</span><span id="4156" class="of ll it ob b gy ph oh l oi oj">--------------------------------------------------------------------<br/>Optimization terminated successfully.<br/>         Current function value: 311.060386<br/>         Iterations: 111<br/>         Function evaluations: 195</span></pre><p id="60e3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们来看看结果:</p><pre class="mj mk ml mm gt oa ob oc od aw oe bi"><span id="fc2c" class="of ll it ob b gy og oh l oi oj">results # this gives us verbosity around our minimization<br/># notice our final key and associated values…</span><span id="84ea" class="of ll it ob b gy ph oh l oi oj">--------------------------------------------------------------------<br/>final_simplex: (array([[0.45115297, 3.03667376, 4.86925122],<br/>       [0.45123459, 3.03666955, 4.86924261],<br/>       [0.45116379, 3.03667852, 4.86921688],<br/>       [0.45119056, 3.03666796, 4.8692127 ]]), array([300.18758478, 300.18758478, 300.18758478, 300.18758479]))<br/>           fun: 300.18758477994425<br/>       message: 'Optimization terminated successfully.'<br/>          nfev: 148<br/>           nit: 80<br/>        status: 0<br/>       success: True<br/>             x: array([0.45115297, 3.03667376, 4.86925122])</span></pre><p id="2ddb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以进一步清理:</p><pre class="mj mk ml mm gt oa ob oc od aw oe bi"><span id="0c2e" class="of ll it ob b gy og oh l oi oj"># drop results into df and round to match statsmodels<br/>resultsdf = pd.DataFrame({'coef':results['x']})<br/>resultsdf.index=['constant','x','sigma']   <br/>np.round(resultsdf.head(2), 4)</span><span id="1452" class="of ll it ob b gy ph oh l oi oj"># do our numbers match the OLS model?<br/>--------------------------------------------------------------------</span></pre><p id="ee7b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你会注意到OLS和我很相配！你的结果会有所不同，因为我们没有使用随机种子。</p><h1 id="5634" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">MLE的最佳实践</h1><p id="09ea" class="pw-post-body-paragraph jq jr it js b jt nv jv jw jx nw jz ka kb nx kd ke kf ny kh ki kj nz kl km kn im bi translated">在我们更进一步之前，这可能是一个加强我们对MLE信任的好时机。作为我们的回归基线，我们知道，根据定义，普通最小二乘法是具有正态分布残差并满足线性回归的其他假设的连续结果的最佳线性无偏估计量。使用最大似然法来寻找我们的系数是否稳健？</p><h2 id="956a" class="of ll it bd lm on oo dn lq op oq dp lu kb or os ly kf ot ou mc kj ov ow mg ox bi translated"><em class="pk">是的！</em></h2><ul class=""><li id="fe15" class="kw kx it js b jt nv jx nw kb pl kf pm kj pn kn lb lc ld le bi translated">MLE与OLS一致。</li><li id="cbe7" class="kw kx it js b jt lf jx lg kb lh kf li kj lj kn lb lc ld le bi translated">对于无限的数据，它将估计最佳的<em class="ks"> β，并很好地近似它</em>用于小而健壮的数据集。</li><li id="cafd" class="kw kx it js b jt lf jx lg kb lh kf li kj lj kn lb lc ld le bi translated">MLE是高效的；没有一致的估计量具有比MLE更低的渐近均方误差。</li></ul><p id="067a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以看起来它完全复制了OLS的做法。那么…为什么用MLE而不是OLS？</p><h2 id="ee2d" class="of ll it bd lm on oo dn lq op oq dp lu kb or os ly kf ot ou mc kj ov ow mg ox bi translated">因为！</h2><ul class=""><li id="8d16" class="kw kx it js b jt nv jx nw kb pl kf pm kj pn kn lb lc ld le bi translated">MLE对于回归和分类是可推广的！</li><li id="406b" class="kw kx it js b jt lf jx lg kb lh kf li kj lj kn lb lc ld le bi translated">MLE是高效的；如果你使用正确的分布，没有一致的估计比MLE有更低的渐近误差。</li></ul><p id="7242" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以将MLE视为通过优化概率成本函数来拟合模型的模块化方式！</p><h2 id="845c" class="of ll it bd lm on oo dn lq op oq dp lu kb or os ly kf ot ou mc kj ov ow mg ox bi translated">应用MLE的四个主要步骤:</h2><ol class=""><li id="11a0" class="kw kx it js b jt nv jx nw kb pl kf pm kj pn kn po lc ld le bi translated">定义可能性，确保您对回归或分类问题使用正确的分布。</li><li id="b3e0" class="kw kx it js b jt lf jx lg kb lh kf li kj lj kn po lc ld le bi translated">取自然对数，将乘积函数简化为和函数。</li><li id="938a" class="kw kx it js b jt lf jx lg kb lh kf li kj lj kn po lc ld le bi translated">最大化——或最小化目标函数的负值。</li><li id="721c" class="kw kx it js b jt lf jx lg kb lh kf li kj lj kn po lc ld le bi translated">验证制服前科是一个安全的假设！否则，你可以将数据归因于一个生成函数或世界模型，它不符合<a class="ae ko" href="https://en.wikipedia.org/wiki/Occam%27s_razor" rel="noopener ugc nofollow" target="_blank">简约法则</a>。</li></ol><p id="2ca7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在最大似然估计空间中有更多的东西，包括分类分布，使用贝叶斯统计软件包如<code class="fe ok ol om ob b">PyMC3</code>等。但是今天我们就讲到这里。</p><p id="1f95" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您如何在数据科学工作流程中使用MLE？在下面评论，或者在<a class="ae ko" href="https://www.linkedin.com/in/jbalaban/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或者<a class="ae ko" href="https://twitter.com/ultimetis" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上联系我！</p><p id="e6d0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ks">特别感谢</em> <a class="ae ko" href="https://www.linkedin.com/in/chadscherrer/" rel="noopener ugc nofollow" target="_blank"> <em class="ks">查德·谢勒</em> </a> <em class="ks">的优秀同行点评。</em></p></div></div>    
</body>
</html>