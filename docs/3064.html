<html>
<head>
<title>Hyper-parameters in Action! Part I — Activation Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数在起作用！第一部分—激活功能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyper-parameters-in-action-a524bf5bf1c?source=collection_archive---------0-----------------------#2018-04-05">https://towardsdatascience.com/hyper-parameters-in-action-a524bf5bf1c?source=collection_archive---------0-----------------------#2018-04-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/24cb3a89644e3b98193781685106c407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Zk3gPVcNOKlU0XjzJh48g.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Sigmoid activation function in action!</figcaption></figure><h1 id="2cc4" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">介绍</h1><p id="b31b" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">这是一系列帖子的第一篇，旨在以清晰简洁的方式直观地展示训练神经网络的一些基本活动部分:超参数。</p><h1 id="7d91" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">动机</h1><p id="f51e" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">深度学习是关于<strong class="lf iu">超参数</strong>！也许这是一种夸张，但对不同超参数对训练深度神经网络的影响有一个良好的理解肯定会让你的生活更容易。</p><p id="888f" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">在研究深度学习时，你可能会发现大量关于正确设置网络超参数重要性的信息:<strong class="lf iu"> <em class="mg">激活函数、权重初始化器、优化器、学习速率、小批量</em> </strong>，以及网络架构本身，如<strong class="lf iu"> <em class="mg">隐藏层数</em> </strong>和<strong class="lf iu"> <em class="mg">每层中单元数</em> </strong>。</p><p id="b19b" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">所以，你学习所有的最佳实践，你建立你的网络，定义超参数(或者只是使用它的默认值)，开始训练并监控你的模型的<strong class="lf iu"> <em class="mg">损失</em> </strong>和<strong class="lf iu"> <em class="mg">指标</em> </strong>的进度。</p><p id="7cd8" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">也许实验没有你预期的那么顺利，所以你<strong class="lf iu">对它进行迭代</strong>，<strong class="lf iu">调整</strong>网络，直到你找到对你的特定问题有用的一组值。</p><h1 id="94c6" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">寻找更深层次的理解(无意双关！)</h1><p id="527d" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">你有没有想过引擎盖下到底发生了什么？我做了，事实证明一些简单的实验可能会对这个问题有所帮助。</p><p id="715d" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">以<strong class="lf iu"> <em class="mg">激活功能</em> </strong>为例，本帖题目。你我都知道，激活函数的作用是引入一个<strong class="lf iu">非线性</strong>，否则整个神经网络可以简单地被一个相应的<a class="ae mh" href="https://medium.com/hipster-color-science/computing-2d-affine-transformations-using-only-matrix-multiplication-2ccb31b52181" rel="noopener"> <strong class="lf iu">仿射变换</strong> </a> <strong class="lf iu"> <em class="mg"> </em> </strong>(即一个<strong class="lf iu">线性变换</strong>，比如<strong class="lf iu">旋转</strong>、<strong class="lf iu">缩放</strong>或<strong class="lf iu">倾斜，</strong>后跟一个<strong class="lf iu">平移</strong></p><p id="90ce" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">一个只有<strong class="lf iu"> <em class="mg">线性</em> </strong> <em class="mg"> </em> <strong class="lf iu"> <em class="mg">激活</em> </strong>(即<strong class="lf iu">不激活</strong>！)将很难处理像这样非常简单的分类问题(每条线有1，000个点，为在-1.0和1.0之间等距分布的<em class="mg"> x </em>值生成):</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/60529f7c1bd13aa95ba6df027a8f98b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*WgHkrR6rsJaiww5rkmqWGA.png"/></div></figure><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/f4993f25f2c6cc6db0a71ed42ea9b02b.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*3jGx6YoSYXsrSfgbxO2yzw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mo">Figure 1</strong>: in this two-dimensional feature space, the blue line represents the negative cases (y = 0), while the green line represents the positive cases (y= 1).</figcaption></figure><p id="844d" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">如果网络唯一能做的事情是执行仿射变换，这可能是它能够提出的解决方案:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/13d1ac5251c5a19adb019484628459ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*KaD6__suBEL1p9YcIhW3EQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mo">Figure 2</strong>: linear boundary — doesn’t look so good, right?</figcaption></figure><p id="819f" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">显然，这还远远不够！更好的解决方案的一些例子有:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mp"><img src="../Images/53dfdf961e58e50565168be6a7f6331a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BoWjJPEuXqtUJXFuOnu_gQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mo">Figure 3</strong>: Non-linearities to the rescue!</figcaption></figure><p id="a1eb" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">这是<strong class="lf iu"> <em class="mg">非线性激活函数</em> </strong>带来的三个很好的例子！你能猜出哪个图像对应于一个<strong class="lf iu"> <em class="mg"> ReLU </em> </strong>吗？</p><h1 id="bf30" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">非线性边界(或者它们是？)</h1><p id="a45c" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">这些非线性边界是如何形成的？嗯，非线性的实际作用是<strong class="lf iu">扭曲和旋转特征空间</strong>，以至于边界变成… <strong class="lf iu">线性</strong>！</p><p id="def6" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">好了，现在事情变得越来越有趣了(至少，我第一次看到这个<strong class="lf iu">很棒的</strong>克里斯·奥拉的博客<a class="ae mh" href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" rel="noopener ugc nofollow" target="_blank">帖子</a>时是这样认为的，我从这个博客中获得了写这篇文章的灵感)。所以，我们再深入调查一下吧！</p><p id="aa9f" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">下一步是建立最简单的神经网络来解决这个特殊的分类问题。在我们的<strong class="lf iu">特征空间</strong>(<strong class="lf iu"><em class="mg">x1</em></strong><em class="mg"/>和<strong class="lf iu"><em class="mg">x2</em></strong><em class="mg">)</em>中有<strong class="lf iu">两个维度</strong>，网络有一个单独的隐层，有<strong class="lf iu"><em class="mg"/></strong>两个单元，所以当涉及到隐层的输出时我们保留维度的数目(<strong class="lf iu"> <em class="mg"> z1 </em></strong></p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mq"><img src="../Images/6814ab87529bac39c919ac3d50d0f500.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Frni4L9WiHQCNvVFCIW2ZA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mo">Figure 4</strong>: diagram of a simple neural network with a single 2-unit hidden layer</figcaption></figure><p id="7982" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">到此为止，我们仍然在<strong class="lf iu">仿射变换</strong>的领域上……所以，是时候用一个<strong class="lf iu"> <em class="mg">非线性激活函数</em> </strong>了，用希腊字母<strong class="lf iu"> sigma </strong>表示，从而产生了<strong class="lf iu">激活值</strong> ( <strong class="lf iu"> <em class="mg"> a1 </em> </strong>和<strong class="lf iu"> <em class="mg"> a2 </em> </strong>)</p><p id="55cb" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">这些<strong class="lf iu">激活值</strong>代表了我在本节第一段提到的<strong class="lf iu">扭曲和旋转特征空间</strong>。这是使用【This形作为<strong class="lf iu"> <em class="mg">激活功能</em> </strong>时的预览图:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/3d12247e76363a70fc60d04724877041.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*ZNPrD0PmXz7I6rhnbPuh1A.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mo">Figure 5</strong>: two-dimensional feature space: twisted and turned!</figcaption></figure><p id="ed14" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">如承诺的那样，边界是线性的！顺便说一下，上面的图对应于原始特征空间上具有非线性边界的最左边的解决方案(<strong class="lf iu">图3 </strong>)。</p><h1 id="7e15" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">神经网络的基本数学概述</h1><p id="0046" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">为了确保你和我在同一页上，我在下面向你展示由神经网络执行的非常基本的矩阵算法的四种表示，直到隐藏层，<strong class="lf iu">在</strong>应用<strong class="lf iu"> <em class="mg">激活函数</em> </strong>之前(即，只是一个<strong class="lf iu">仿射变换</strong>如 <strong class="lf iu"> <em class="mg"> xW + b </em> </strong>)</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/40f3577a07863329204f3c572052324f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*bUNxrEy2KjKNrwMRo8eS9w.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Basic matrix arithmetic: 4 ways of representing the same thing in the network</figcaption></figure><p id="7b21" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">应用<strong class="lf iu"> <em class="mg">激活功能</em> </strong>的时间，在网络图上用希腊字母<strong class="lf iu"> sigma </strong>表示。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/df3d66b7183c2557a5117fdbb324b637.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/format:webp/1*9DfvAg_pENO5MX0ELeY_kg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Activation function: applied on the results of the affine transformations</figcaption></figure><p id="4442" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">瞧！我们从<strong class="lf iu">输入</strong>到隐藏层的<strong class="lf iu">激活值</strong>！</p><h1 id="49c5" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">在Keras实施网络</h1><p id="4ff0" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">对于这个简单网络的实现，我使用了<strong class="lf iu"> Keras顺序模型API </strong>。除了不同的<strong class="lf iu"> <em class="mg">激活功能</em> </strong>之外，每个被训练的模型都使用完全相同的<strong class="lf iu">超参数</strong>:</p><ul class=""><li id="25ac" class="mu mv it lf b lg mb lk mc lo mw ls mx lw my ma mz na nb nc bi translated"><strong class="lf iu"> <em class="mg">权重初始值</em></strong><em class="mg">:</em>Glorot(Xavier)法线(隐藏层)和随机法线(输出层)；</li><li id="dc3e" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu"> <em class="mg">优化器</em> </strong>:随机梯度下降(SGD)；</li><li id="e981" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu"> <em class="mg">学习率</em></strong>:0.05；</li><li id="d237" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu"> <em class="mg">小批量</em></strong>:16；</li><li id="4982" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu"> <em class="mg">隐藏层数</em></strong>:1；</li><li id="63dc" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu"> <em class="mg">单位数</em> </strong>(在隐藏层):2。</li></ul><p id="8c77" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">鉴于这是一个<strong class="lf iu">二进制分类任务</strong>,<strong class="lf iu">输出层</strong>有一个<strong class="lf iu"> <em class="mg">单单元</em> </strong>有一个<strong class="lf iu">s形</strong> <strong class="lf iu"> <em class="mg">激活函数</em> </strong>和<strong class="lf iu"> <em class="mg">损失</em> </strong>由<strong class="lf iu">二进制交叉熵</strong>给出。</p><figure class="mj mk ml mm gt ju"><div class="bz fp l di"><div class="ni nj l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Code: simple neural network with a single 2-unit hidden layer</figcaption></figure><h1 id="0c66" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">激活功能正在发挥作用！</h1><p id="3474" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">现在，对于有趣的部分— <strong class="lf iu">在网络训练时可视化</strong>扭曲的特征空间，每次使用不同的<strong class="lf iu"> <em class="mg">激活函数</em></strong>:<strong class="lf iu">sigmoid</strong>，<strong class="lf iu"> tanh </strong>和<strong class="lf iu"> ReLU </strong>。</p><p id="856b" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">除了显示特征空间中的变化，动画还包含:</p><ul class=""><li id="0596" class="mu mv it lf b lg mb lk mc lo mw ls mx lw my ma mz na nb nc bi translated"><strong class="lf iu">阴性(蓝线)和阳性病例(绿线)的预测概率直方图</strong>，错误分类的<strong class="lf iu">病例显示在<strong class="lf iu">红条</strong>中(使用阈值= 0.5)；</strong></li><li id="1baf" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu">精度</strong>和<strong class="lf iu">平均损耗</strong>的线图；</li><li id="196e" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu">数据集中每个元素的损失直方图</strong>。</li></ul><h1 id="de94" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">乙状结肠的</h1><p id="b46a" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">让我们从最传统的<strong class="lf iu"> <em class="mg">激活函数</em></strong><strong class="lf iu">sigmoid</strong>开始，尽管现在，它的使用仅限于分类任务中的输出层。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/f5636d70d15f3d3e1d2fe65ca275302a.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*iTaMkyoEbpPhgxQdcUlyhg.png"/></div></figure><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/5688578b88ddcf6d6754b54d864989cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*tXzS5GwC3BBqi7ppwcQWdw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mo">Figure 6</strong>: sigmoid activation function and its gradient</figcaption></figure><p id="9a58" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">从图6 的<strong class="lf iu">中可以看到，一个<strong class="lf iu">的sigmoid </strong> <strong class="lf iu"> <em class="mg">激活函数</em></strong><em class="mg">将</em>的输入值挤压到<strong class="lf iu">的范围(0，1) </strong>(同样的范围概率可以取，这也是它在输出层用于分类任务的原因)。此外，请记住，任何给定层的激活值都是下一层的输入，给定<strong class="lf iu"> sigmoid </strong>的范围，<strong class="lf iu">激活值</strong>将是以0.5 </strong>为中心的<strong class="lf iu">，而不是零(通常是归一化输入的情况)。</strong></p><p id="4398" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">也可以验证其<strong class="lf iu">梯度</strong>峰值为0.25(对于<em class="mg"> z </em> = 0)，并且当| <em class="mg"> z </em> |达到值5时已经接近零。</p><p id="2808" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">那么，使用一个<strong class="lf iu"> sigmoid </strong> <strong class="lf iu"> <em class="mg">激活函数</em> </strong> n如何为这个简单的网络工作呢？让我们来看看动画:</p><figure class="mj mk ml mm gt ju"><div class="bz fp l di"><div class="nm nj l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Sigmoid in action!</figcaption></figure><p id="e9ea" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">有几点需要注意:</p><ul class=""><li id="0b33" class="mu mv it lf b lg mb lk mc lo mw ls mx lw my ma mz na nb nc bi translated"><strong class="lf iu">时期15–40</strong>:可以注意到水平轴上发生典型的<strong class="lf iu">s形</strong><em class="mg">挤压</em>；</li><li id="83ea" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu">历元40–65</strong>:变换后的特征空间<strong class="lf iu"><em class="mg"/></strong>停留在一个平台上，在纵轴上有一个“<em class="mg">加宽</em>”；</li><li id="1abf" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu"> epoch 65 </strong>:此时，<strong class="lf iu">阴性病例</strong>(蓝线)全部被<strong class="lf iu">正确分类</strong>，尽管其关联概率仍然分布到0.5；而边缘上的<strong class="lf iu">阳性病例仍为<strong class="lf iu">误分类</strong>；</strong></li><li id="aa2d" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu">时期65–100</strong>:前述的<em class="mg">加宽</em>变得越来越强烈，几乎覆盖了所有的特征空间，而<strong class="lf iu">损失</strong>稳步下降；</li><li id="1fa2" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu">第103期</strong>:由于<em class="mg">扩大</em>，所有<strong class="lf iu">阳性病例</strong>现在都位于<strong class="lf iu">适当的边界</strong>内，尽管仍有一些概率勉强超过0.5阈值；</li><li id="77b9" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu">纪元100–150</strong>:现在垂直轴上也发生了一些<em class="mg">挤压</em>，损失<strong class="lf iu">下降到似乎是一个新的平稳状态，除了一些积极的边缘情况，网络对其预测相当有信心<strong class="lf iu"/>。</strong></li></ul><p id="2170" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">因此，<strong class="lf iu">s形<em class="mg">激活功能</em> </strong>成功分离两条线，但是<strong class="lf iu"> <em class="mg">损耗</em> </strong>缓慢下降<strong class="lf iu"/>，同时在<strong class="lf iu">停滞</strong>处停留相当一部分训练时间。</p><p id="0272" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">我们能否用一个<strong class="lf iu">不同的<em class="mg">激活功能</em> </strong>做得更好？</p><h1 id="2596" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">双曲正切</h1><p id="58fd" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lf iu"> tanh </strong></p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/22d07282c46ad88bebdd38cccb5b4ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*70aRcslbuW34QMHD7rYA9g.png"/></div></figure><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/fad105931738ac8b81647d2eedd8cfe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*SeCBB7lfA7KPJ-T1Mi7GRg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mo">Figure 7</strong>: tanh activation function and its gradient</figcaption></figure><p id="58dc" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">在<strong class="lf iu">图7 </strong>中可以看到，<strong class="lf iu"> tanh </strong> <strong class="lf iu"> <em class="mg">激活功能</em></strong><em class="mg">将</em>输入值挤压到<strong class="lf iu">范围(-1，1) </strong>。因此，由于<strong class="lf iu">以零</strong>为中心，激活值已经(在某种程度上)是下一层的归一化输入。</p><p id="a0c4" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">关于<strong class="lf iu">梯度</strong>，它有一个大得多的峰值1.0(同样，对于<em class="mg"> z </em> = 0)，但是它的下降甚至更快，接近于零到值<em class="mg"> |z </em> |低至3。这是所谓的<strong class="lf iu">消失梯度</strong>问题的根本原因，它导致网络的训练越来越慢。</p><p id="4866" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">现在，对于相应的动画，使用<strong class="lf iu"> tanh </strong>作为<strong class="lf iu"> <em class="mg">激活函数</em> </strong>:</p><figure class="mj mk ml mm gt ju"><div class="bz fp l di"><div class="nm nj l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Tanh in action!</figcaption></figure><p id="e260" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">有几点需要注意:</p><ul class=""><li id="f372" class="mu mv it lf b lg mb lk mc lo mw ls mx lw my ma mz na nb nc bi translated"><strong class="lf iu">时段10–40</strong>:横轴上有一个<strong class="lf iu">tanh</strong><em class="mg">squashing</em>发生，虽然不太明显，而<strong class="lf iu">损耗</strong>停留在一个平台上；</li><li id="101a" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu">时期40–55</strong>:在<strong class="lf iu"> <em class="mg">损失</em> </strong>上仍然没有改善，但是在纵轴上有一个变换后的特征空间的<em class="mg">加宽</em>；</li><li id="6a4d" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu">第55期</strong>:此时，<strong class="lf iu">阴性病例</strong>(蓝线)都被<strong class="lf iu">正确分类为</strong>，尽管其关联概率仍然分布到0.5；而边缘上的<strong class="lf iu">阳性病例</strong>仍为<strong class="lf iu">误分类</strong>；</li><li id="4c45" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu">时期55–65</strong>:前述的<em class="mg">加宽</em>很快到达几乎所有特征空间再次被覆盖的点，而<strong class="lf iu">损失</strong>突然下降；</li><li id="0936" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu">纪元69 </strong>:由于“<em class="mg">扩大</em>”，所有<strong class="lf iu">阳性病例</strong>现在都位于<strong class="lf iu">适当的边界</strong>内，尽管仍有一些概率勉强高于0.5阈值；</li><li id="305d" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu">时期65–90</strong>:现在垂直轴上也发生了一些<em class="mg">挤压</em>，损耗<strong class="lf iu">持续下降，直到达到新的平稳状态，网络对所有预测显示出<strong class="lf iu">高置信度</strong>；</strong></li><li id="5c58" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated">时期90–150:在这一点上，预测的概率只有很小的提高。</li></ul><p id="26a8" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">好了，好像好一点了……这个<strong class="lf iu"> tanh </strong> <strong class="lf iu"> <em class="mg">激活功能</em> </strong>达到了<strong class="lf iu">正确分类</strong>对于所有情况<strong class="lf iu">更快</strong>，随着<strong class="lf iu"> <em class="mg">损耗</em> </strong>也下降<strong class="lf iu">更快</strong>(就是下降的时候)，但是它也在<strong class="lf iu">停滞期</strong>花费了很多时间。</p><p id="61c6" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">如果我们<strong class="lf iu">摆脱</strong>所有的<em class="mg">挤压</em>会怎么样？</p><h1 id="652c" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">热卢</h1><p id="619e" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lf iu">Re</strong>activated<strong class="lf iu">L</strong>linear<strong class="lf iu">U</strong>nits，简称<strong class="lf iu"> ReLUs </strong>，是目前<strong class="lf iu"> <em class="mg">激活函数</em> </strong>的常见选择。一个<strong class="lf iu"> ReLU </strong>解决了<strong class="lf iu">消失渐变</strong>的问题，这在它的两个前辈中很常见，同时也是<strong class="lf iu">计算渐变最快的</strong>。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi no"><img src="../Images/5d9d005162e89192d10dcc9701ac5a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*TTVSx5g-godrk5JUo7WfjA.png"/></div></figure><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/b6816ae8239c8e7e293f5954921cc0cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*piSRCnAIA2paTMd8kVDfKg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mo">Figure 8</strong>: ReLU activation function and its gradient</figcaption></figure><p id="71b2" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">正如你在<strong class="lf iu">图8 </strong>中看到的，<strong class="lf iu"> ReLU </strong>是一个完全不同的怪兽:它不会将值“挤压”到一个范围内——它只是<strong class="lf iu">保留正值</strong>并将所有的负值<strong class="lf iu">变成零</strong>。</p><p id="64a4" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">使用<strong class="lf iu"> ReLU </strong>的好处是它的<strong class="lf iu">渐变</strong>要么是1(正值)，要么是0(负值)——<strong class="lf iu">不再有消失渐变</strong>！这种模式导致网络的<strong class="lf iu">更快收敛</strong>。</p><p id="a5cc" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">另一方面，这种行为会导致所谓的<strong class="lf iu">“死神经元”</strong>，也就是说，神经元的输入始终为负，因此，其激活值始终为零。</p><p id="b693" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">最后一个动画的时间，与前两个有很大的不同，由于<strong class="lf iu"> ReLU </strong> <strong class="lf iu"> <em class="mg">激活功能</em> </strong>中的<em class="mg">挤压</em>的<strong class="lf iu">缺席:</strong></p><figure class="mj mk ml mm gt ju"><div class="bz fp l di"><div class="nm nj l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">ReLU in action!</figcaption></figure><p id="f008" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">有几点需要注意:</p><ul class=""><li id="7632" class="mu mv it lf b lg mb lk mc lo mw ls mx lw my ma mz na nb nc bi translated"><strong class="lf iu">时段0–10</strong>:亏损从一开始就稳步下降</li><li id="5d28" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu">历元10 </strong>:此时，<strong class="lf iu">阴性案例</strong>(蓝线)全部被<strong class="lf iu">正确分类</strong>，尽管其关联概率仍然分布到0.5；而边缘上的<strong class="lf iu">阳性病例</strong>仍为<strong class="lf iu">误分类</strong>；</li><li id="d6a0" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated"><strong class="lf iu">时期10–60</strong>:<strong class="lf iu">损失</strong>下降直至达到稳定状态，<strong class="lf iu">自<strong class="lf iu">时期52 </strong>以来，所有情况</strong>已经被<strong class="lf iu">正确分类</strong>，并且网络已经对所有预测展现出<strong class="lf iu">高置信水平</strong>；</li><li id="5bae" class="mu mv it lf b lg nd lk ne lo nf ls ng lw nh ma mz na nb nc bi translated">时期60–150:在这一点上，预测的概率只有很小的提高。</li></ul><p id="179b" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">嗯，难怪<strong class="lf iu"> ReLUs </strong>是如今<strong class="lf iu">激活功能</strong>事实上的<em class="mg">标准。<strong class="lf iu"> <em class="mg">损失</em> </strong>使<strong class="lf iu">从一开始就稳步下降</strong>，只有<strong class="lf iu">稳定在</strong>接近零的水平，在<strong class="lf iu">中所有情况下达到<strong class="lf iu">正确分类</strong>的时间约为</strong>花费<strong class="lf iu"> tanh </strong>的75%。</em></p><h1 id="f225" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">摊牌</h1><p id="bad3" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">动画很酷(好吧，我有偏见，是我做的！)，但是不太方便在<strong class="lf iu">特征空间</strong>上比较每一个不同的<strong class="lf iu"> <em class="mg">激活函数</em> </strong>的<strong class="lf iu">总体效果</strong>。所以，为了便于你比较，它们并排在这里:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi np"><img src="../Images/d6bee19f33ac4bc44ca6e7959e725d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q6PeKr1g90y-pJ1nrj5_2g.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mo">Figure 9</strong>: linear boundaries on transformed feature space (top row), non-linear boundaries on original feature space (bottom row)</figcaption></figure><p id="bb05" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">那并排<strong class="lf iu"> <em class="mg">准确度</em> </strong>和<strong class="lf iu"> <em class="mg">损耗</em> </strong>曲线呢，这样我也可以对比一下训练速度？当然，我们开始吧:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nq"><img src="../Images/d533039694a2ccc6c2c516ac3af63df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TV30vh5INNcysPHjpSIgug.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mo">Figure 10</strong>: accuracy and loss curves for each activation function</figcaption></figure><h1 id="d3c4" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">最后的想法</h1><p id="9f34" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">我用来说明这篇文章的例子是尽可能简单的，动画中描述的<strong class="lf iu">模式仅仅是为了给你一个<strong class="lf iu">大概的概念</strong>关于每一个<strong class="lf iu"> <em class="mg">激活函数的<strong class="lf iu">底层机制</strong>。</em>T25】</strong></strong></p><p id="9a8e" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">况且我用我的<strong class="lf iu"> <em class="mg">初始化权重</em> </strong>得到了<em class="mg">幸运</em>(也许用<a class="ae mh" href="https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_(42)" rel="noopener ugc nofollow" target="_blank"> 42 </a>做种子是个好兆头？！)并且所有三个网络都学会了在150个训练时期内对所有情况进行正确分类。事实证明，训练对初始化非常敏感，但这是以后文章的主题。</p><p id="74b3" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated">尽管如此，我真心希望这篇文章和它的动画能给你一些<strong class="lf iu">见解</strong>甚至一些<strong class="lf iu">“啊哈！”</strong>了解这个令人着迷的话题的时刻，这个话题就是<strong class="lf iu">深度学习</strong>。</p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><blockquote class="ny nz oa"><p id="b41c" class="ld le mg lf b lg mb li lj lk mc lm ln ob md lq lr oc me lu lv od mf ly lz ma im bi translated">更新(2018年5月10日)</p><p id="29bf" class="ld le mg lf b lg mb li lj lk mc lm ln ob md lq lr oc me lu lv od mf ly lz ma im bi translated">现在你可以自己复制剧情和动画了:-)我已经发布了一个包——<strong class="lf iu">deep replay</strong>——在<a class="ae mh" href="https://github.com/dvgodoy/deepreplay" rel="noopener ugc nofollow" target="_blank"> GitHub </a>及其对应的<a class="ae mh" rel="noopener" target="_blank" href="/hyper-parameters-in-action-introducing-deepreplay-31132a7b9631">帖子</a>上查看一下。</p><p id="55c9" class="ld le mg lf b lg mb li lj lk mc lm ln ob md lq lr oc me lu lv od mf ly lz ma im bi translated">感谢<a class="ae mh" href="https://medium.com/@weakish" rel="noopener"> Jakukyo弗列尔，</a>你还可以查看这篇帖子的<a class="ae mh" href="https://www.jqr.com/article/000161" rel="noopener ugc nofollow" target="_blank">中文版</a>。</p><p id="4665" class="ld le mg lf b lg mb li lj lk mc lm ln ob md lq lr oc me lu lv od mf ly lz ma im bi translated">修正了<strong class="lf iu">图10 </strong>中的错别字:中间的图显示的是<strong class="lf iu"> tanh </strong>，而不是sigmoid。</p></blockquote></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><p id="1789" class="pw-post-body-paragraph ld le it lf b lg mb li lj lk mc lm ln lo md lq lr ls me lu lv lw mf ly lz ma im bi translated"><em class="mg">如有任何想法、意见或问题，请在下方留言或联系我</em> <a class="ae mh" href="https://twitter.com/dvgodoy" rel="noopener ugc nofollow" target="_blank"> <em class="mg">推特</em> </a> <em class="mg">。</em></p></div></div>    
</body>
</html>