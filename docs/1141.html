<html>
<head>
<title>Detecting bats by recognising their sound with Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过Tensorflow识别蝙蝠的声音来探测蝙蝠</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/detecting-bats-by-recognising-their-sound-with-tensorflow-cdd5e1c22b14?source=collection_archive---------13-----------------------#2017-08-02">https://towardsdatascience.com/detecting-bats-by-recognising-their-sound-with-tensorflow-cdd5e1c22b14?source=collection_archive---------13-----------------------#2017-08-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ad39d98b7164f1f39cfeb9e0031219c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FadK0gRTanK-IzMh62lNyA.png"/></div></div></figure><p id="1470" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">上周我发现我的公寓后面有蝙蝠。我立即抓起我的“蝙蝠探测器”:一种将蝙蝠用来回声定位的超声波信号从听不见的频率范围转换成听得见的频率范围的装置。因此，“蝙蝠探测器”这个名字是一个谎言:你可以用它来探测蝙蝠，但它本身并不能探测蝙蝠。在本教程中，我将向你展示如何使用Tensorflow建立一个真正的蝙蝠探测器。</p><p id="535e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="http://www.pinchofintelligence.com/detecting-bats-recognising-sound-tensorflow/" rel="noopener ugc nofollow" target="_blank"> <strong class="ka ir">不幸的是媒体不支持音频文件。去我的博客看看有声音的版本。</strong>T3】</a></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi kx"><img src="../Images/3fa3f6c894c7d8626ca5ea03107cb345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*q5ZIVVemyR-vioaL.jpg"/></div></figure><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/383c70c089aed26c1f8b0522d1fcc0c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/0*kLn9sv0En1eqNFno.png"/></div></figure><h2 id="5fff" class="ld le iq bd lf lg lh dn li lj lk dp ll kj lm ln lo kn lp lq lr kr ls lt lu lv bi translated">问题陈述</h2><p id="a97e" class="pw-post-body-paragraph jy jz iq ka b kb lw kd ke kf lx kh ki kj ly kl km kn lz kp kq kr ma kt ku kv ij bi translated">为了解决这个问题，我将蝙蝠探测器连接到我的笔记本电脑上，录下了几个片段。在一个<a class="ae kw" href="https://github.com/rmeertens/batdetection/blob/master/Label%20data.ipynb" rel="noopener ugc nofollow" target="_blank">单独的Jupyter笔记本上，我创建了一个标签程序</a>。这个程序创建了一秒钟的“声音片段”，我将其分为包含蝙蝠声音的片段和不包含蝙蝠声音的片段。我用数据和标签创建一个分类器来区分它们。</p><h2 id="cfac" class="ld le iq bd lf lg lh dn li lj lk dp ll kj lm ln lo kn lp lq lr kr ls lt lu lv bi translated">识别声音的图书馆</h2><p id="656f" class="pw-post-body-paragraph jy jz iq ka b kb lw kd ke kf lx kh ki kj ly kl km kn lz kp kq kr ma kt ku kv ij bi translated">我导入了一些非常有用的库来构建声音识别管道。我导入的明显的库是Tensorflow、Keras和scikit。我喜欢的一个声音专用库是<a class="ae kw" href="https://github.com/librosa/librosa" rel="noopener ugc nofollow" target="_blank"> librosa </a>，它帮助我加载和分析数据。</p><p id="2820" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在[1]中:</p><pre class="ky kz la lb gt mb mc md me aw mf bi"><span id="fb59" class="ld le iq mc b gy mg mh l mi mj">import random<br/>import sys<br/>import glob<br/>import os<br/>import time<br/><br/>import IPython<br/>import matplotlib.pyplot as plt<br/>from matplotlib.pyplot import specgram<br/><br/>import librosa<br/>import librosa.display<br/><br/>from sklearn.preprocessing import normalize<br/>import numpy as np<br/>import tensorflow as tf<br/>import keras<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten</span><span id="edc3" class="ld le iq mc b gy mk mh l mi mj">Using TensorFlow backend.</span></pre><h2 id="edad" class="ld le iq bd lf lg lh dn li lj lk dp ll kj lm ln lo kn lp lq lr kr ls lt lu lv bi translated">用Python加载声音数据</h2><p id="d458" class="pw-post-body-paragraph jy jz iq ka b kb lw kd ke kf lx kh ki kj ly kl km kn lz kp kq kr ma kt ku kv ij bi translated">在数据标签笔记本中，我们输入标签，并将声音字节保存到我们输入的文件夹中。通过从这些文件夹加载，我可以加载蝙蝠声音和非蝙蝠声音文件。取决于有多少声音文件加载此数据可能需要很长时间。我把一个压缩文件夹里的所有文件都上传到了谷歌云平台。</p><ul class=""><li id="0df2" class="ml mm iq ka b kb kc kf kg kj mn kn mo kr mp kv mq mr ms mt bi translated"><a class="ae kw" href="https://storage.googleapis.com/pinchofintelligencebucket/labeled.zip" rel="noopener ugc nofollow" target="_blank">标注声音</a></li><li id="742d" class="ml mm iq ka b kb mu kf mv kj mw kn mx kr my kv mq mr ms mt bi translated"><a class="ae kw" href="https://storage.googleapis.com/pinchofintelligencebucket/batsounds.zip" rel="noopener ugc nofollow" target="_blank">原始声音</a></li></ul><p id="f032" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">注意，这个笔记本本身也可以从Git库下载。显然，Jupyter笔记本中的声音比wordpress/medium中的声音要大得多。你可能得把声音调大一点！</p><p id="7bdc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在[2]中:</p><pre class="ky kz la lb gt mb mc md me aw mf bi"><span id="7946" class="ld le iq mc b gy mg mh l mi mj"># Note: SR stands for sampling rate, the rate at which my audio files were recorded and saved. <br/>SR = 22050 # All audio files are saved like this<br/><br/>def load_sounds_in_folder(foldername):<br/>    """ Loads all sounds in a folder"""<br/>    sounds = []<br/>    for filename in os.listdir(foldername):<br/>        X, sr = librosa.load(os.path.join(foldername,filename))<br/>        assert sr == SR<br/>        sounds.append(X)<br/>    return sounds<br/><br/>## Sounds in which you can hear a bat are in the folder called "1". Others are in a folder called "0". <br/>batsounds = load_sounds_in_folder('labeled/1')<br/>noisesounds = load_sounds_in_folder('labeled/0')<br/><br/>print("With bat: %d without: %d total: %d " % (len(batsounds), len(noisesounds), len(batsounds)+len(noisesounds)))<br/>print("Example of a sound with a bat:")<br/>IPython.display.display(IPython.display.Audio(random.choice(batsounds), rate=SR,autoplay=True))<br/>print("Example of a sound without a bat:")<br/>IPython.display.display(IPython.display.Audio(random.choice(noisesounds), rate=SR,autoplay=True))</span><span id="2841" class="ld le iq mc b gy mk mh l mi mj">With bat: 96 without: 1133 total: 1229 <br/>Example of a sound with a bat:</span></pre><p id="1541" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您的浏览器不支持音频元素。</p><pre class="ky kz la lb gt mb mc md me aw mf bi"><span id="0cc3" class="ld le iq mc b gy mg mh l mi mj">Example of a sound without a bat:</span></pre><p id="d9f6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您的浏览器不支持音频元素。</p><h2 id="225e" class="ld le iq bd lf lg lh dn li lj lk dp ll kj lm ln lo kn lp lq lr kr ls lt lu lv bi translated">用Librosa可视化声音</h2><p id="0f9a" class="pw-post-body-paragraph jy jz iq ka b kb lw kd ke kf lx kh ki kj ly kl km kn lz kp kq kr ma kt ku kv ij bi translated">当你用耳机听蝙蝠的声音时，当一只蝙蝠飞过时，你可以听到清晰的声音。Librosa库可以执行<a class="ae kw" href="https://en.wikipedia.org/wiki/Fourier_transform" rel="noopener ugc nofollow" target="_blank">傅立叶变换</a>来提取组成声音的频率。<br/>在构建任何机器学习算法之前，仔细检查你正在处理的数据非常重要。在这种情况下，我决定:</p><ul class=""><li id="d4cb" class="ml mm iq ka b kb kc kf kg kj mn kn mo kr mp kv mq mr ms mt bi translated">听听声音</li><li id="0177" class="ml mm iq ka b kb mu kf mv kj mw kn mx kr my kv mq mr ms mt bi translated">绘制声波图</li><li id="076c" class="ml mm iq ka b kb mu kf mv kj mw kn mx kr my kv mq mr ms mt bi translated">绘制<a class="ae kw" href="https://en.wikipedia.org/wiki/Spectrogram" rel="noopener ugc nofollow" target="_blank">频谱图(频率振幅随时间变化的可视化表示)</a>。</li></ul><p id="4c7f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在[3]中:</p><pre class="ky kz la lb gt mb mc md me aw mf bi"><span id="990e" class="ld le iq mc b gy mg mh l mi mj">def get_short_time_fourier_transform(soundwave):<br/>    return librosa.stft(soundwave, n_fft=256)<br/><br/>def short_time_fourier_transform_amplitude_to_db(stft):<br/>    return librosa.amplitude_to_db(stft)<br/><br/>def soundwave_to_np_spectogram(soundwave):<br/>    step1 = get_short_time_fourier_transform(soundwave)<br/>    step2 = short_time_fourier_transform_amplitude_to_db(step1)<br/>    step3 = step2/100<br/>    return step3<br/><br/>def inspect_data(sound):<br/>    plt.figure()<br/>    plt.plot(sound)<br/>    IPython.display.display(IPython.display.Audio(sound, rate=SR))<br/>    a = get_short_time_fourier_transform(sound)<br/>    Xdb = short_time_fourier_transform_amplitude_to_db(a)<br/>    plt.figure()<br/>    plt.imshow(Xdb)<br/>    plt.show()<br/>    print("Length per sample: %d, shape of spectogram: %s, max: %f min: %f" % (len(sound), str(Xdb.shape), Xdb.max(), Xdb.min()))<br/><br/>inspect_data(batsounds[0])<br/>inspect_data(noisesounds[0])</span></pre><p id="5c16" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您的浏览器不支持音频元素。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/fe3e819c5cb930c9a2576c65ff0b565f.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/0*qTWF5lM1xplrUc5b.png"/></div></figure><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/af3d36f2a1bb31da417ae97590a42f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/0*rMUJSCA3ilnPy6XI.png"/></div></figure><pre class="ky kz la lb gt mb mc md me aw mf bi"><span id="a4b7" class="ld le iq mc b gy mg mh l mi mj">Length per sample: 22050, shape of spectogram: (129, 345), max: -22.786959 min: -100.000000</span></pre><p id="bd5d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您的浏览器不支持音频元素。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/b4b3bac3a3f69c671be22493c45d6a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/0*Xa2DPQojbKU4RQtw.png"/></div></figure><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/6604b91cac6ddaeb70af0c1fee002266.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/0*h_YJBOXm1Ew7cUZ5.png"/></div></figure><pre class="ky kz la lb gt mb mc md me aw mf bi"><span id="9457" class="ld le iq mc b gy mg mh l mi mj">Length per sample: 22050, shape of spectogram: (129, 345), max: -58.154167 min: -100.000000</span></pre><h2 id="32d7" class="ld le iq bd lf lg lh dn li lj lk dp ll kj lm ln lo kn lp lq lr kr ls lt lu lv bi translated">数据分析</h2><p id="5cdb" class="pw-post-body-paragraph jy jz iq ka b kb lw kd ke kf lx kh ki kj ly kl km kn lz kp kq kr ma kt ku kv ij bi translated">首先，重要的是要注意，我们正在处理的数据并不完全是大数据…只有大约100个阳性样本，深度神经网络很可能在这个daa上过度拟合。我们正在处理的一个问题是，收集阴性样本很容易(只需记录一整天没有蝙蝠的情况)，而收集阳性样本很难(蝙蝠每天只在这里呆15-20分钟左右，我需要手动标记数据)。在确定如何对数据进行分类时，我们会考虑少量的阳性样本。</p><h2 id="4d9e" class="ld le iq bd lf lg lh dn li lj lk dp ll kj lm ln lo kn lp lq lr kr ls lt lu lv bi translated">可听声信号</h2><p id="cdbf" class="pw-post-body-paragraph jy jz iq ka b kb lw kd ke kf lx kh ki kj ly kl km kn lz kp kq kr ma kt ku kv ij bi translated">正如我们从上面可以看到的，信号的幅度随着噪声而降低，而信号的幅度却很高。然而，这并不意味着所有有声音的东西都是蝙蝠。在这个频率下，你还会接收到其他噪音，如手指摩擦声或电话信号。我决定把每一个负面信号放在一个大的“负面”堆上，把电话信号、手指引起的噪音和其他东西放在一个大堆里。</p><h2 id="5e16" class="ld le iq bd lf lg lh dn li lj lk dp ll kj lm ln lo kn lp lq lr kr ls lt lu lv bi translated">光谱图</h2><p id="e8fe" class="pw-post-body-paragraph jy jz iq ka b kb lw kd ke kf lx kh ki kj ly kl km kn lz kp kq kr ma kt ku kv ij bi translated">我希望能在我们的频谱图中看到蝙蝠产生的准确频率。不幸的是，看起来我的传感器在所有频率上都把它当成了噪音。看着声谱图，你仍然可以看到蝙蝠声音和噪音之间的明显区别。我的第一个尝试是使用这个频谱图作为卷积神经网络的输入。不幸的是，仅使用几个正样本，很难训练这个网络。因此，我放弃了这种方法。</p><p id="6a7d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，我决定采用“元数据方法”。我把每秒钟的声音分成22部分。对于每个部分，我确定样本的最大值、最小值、平均值、标准偏差和最大最小值。我采取这种方法的原因是因为“蝙蝠信号”在音频可视化中确实清楚地显示为非高振幅信号。通过分析音频信号的不同部分，我可以发现信号的多个部分是否具有某些特征(如高标准偏差)，从而检测到蝙蝠的叫声。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/4dc71f2dcf477f5a4bdd0aee6ab0d6d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/0*bYdSy-6ZybD0bXwf.png"/></div></figure><p id="9af6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在[4]中:</p><pre class="ky kz la lb gt mb mc md me aw mf bi"><span id="cb65" class="ld le iq mc b gy mg mh l mi mj">WINDOW_WIDTH = 10<br/>AUDIO_WINDOW_WIDTH = 1000 # With sampling rate of 22050 we get 22 samples for our second of audio<br/>def audio_to_metadata(audio):<br/>    """ Takes windows of audio data, per window it takes the max value, min value, mean and stdev values"""<br/>    features = []<br/>    for start in range(0,len(audio)-AUDIO_WINDOW_WIDTH,AUDIO_WINDOW_WIDTH):<br/>        subpart = audio[start:start+AUDIO_WINDOW_WIDTH]<br/>        maxval = max(subpart)<br/>        minval = min(subpart)<br/>        mean = np.mean(subpart)<br/>        stdev = np.std(subpart)<br/>        features.extend([maxval,minval,mean,stdev,maxval-minval])<br/>    return features<br/><br/>metadata = audio_to_metadata(batsounds[0])<br/>print(metadata)<br/>print(len(metadata))</span><span id="e861" class="ld le iq mc b gy mk mh l mi mj">[0.00088500977, -0.00076293945, 6.7962646e-05, 0.00010915515, 0.0016479492, 0.0002746582, 3.0517578e-05, 0.00017904663, 5.4772983e-05, 0.00024414062, 0.00057983398, -0.00057983398, -2.8137207e-05, 8.1624778e-05, 0.001159668, -9.1552734e-05, -0.0002746582, -0.00019345093, 3.922523e-05, 0.00018310547, 0.00048828125, -0.00076293945, -0.00036187744, 0.00015121402, 0.0012512207, -3.0517578e-05, -0.00057983398, -0.00027001952, 0.00015006117, 0.00054931641, 0.00045776367, -0.00036621094, 5.9234619e-05, 5.0381914e-05, 0.00082397461, 0.00015258789, 6.1035156e-05, 0.00011447143, 1.7610495e-05, 9.1552734e-05, 0.00015258789, 6.1035156e-05, 9.3963623e-05, 1.8880468e-05, 9.1552734e-05, 0.00082397461, -0.00048828125, 7.7423094e-05, 8.6975793e-05, 0.0013122559, 0.00021362305, 6.1035156e-05, 0.00014205933, 2.5201958e-05, 0.00015258789, 0.00054931641, -0.00061035156, 2.8991699e-05, 9.5112577e-05, 0.001159668, -3.0517578e-05, -0.00018310547, -0.00010638428, 2.9584806e-05, 0.00015258789, 3.0517578e-05, -9.1552734e-05, -2.7862548e-05, 2.323009e-05, 0.00012207031, 6.1035156e-05, -3.0517578e-05, 1.8341065e-05, 1.905331e-05, 9.1552734e-05, 0.00018310547, -0.00039672852, 4.9438477e-05, 4.7997077e-05, 0.00057983398, 0.00021362305, 9.1552734e-05, 0.00017184448, 2.1811828e-05, 0.00012207031, 0.00015258789, -6.1035156e-05, 5.0659179e-05, 4.6846228e-05, 0.00021362305, 0.0, -0.00015258789, -5.4656983e-05, 2.7488175e-05, 0.00015258789, -3.0517578e-05, -0.00012207031, -9.0820315e-05, 1.7085047e-05, 9.1552734e-05, 0.0, -0.00012207031, -7.2296141e-05, 1.917609e-05, 0.00012207031, 0.0, -9.1552734e-05, -4.4189452e-05, 1.8292634e-05, 9.1552734e-05]<br/>110</span></pre><h2 id="f6dd" class="ld le iq bd lf lg lh dn li lj lk dp ll kj lm ln lo kn lp lq lr kr ls lt lu lv bi translated">数据管理</h2><p id="6b8f" class="pw-post-body-paragraph jy jz iq ka b kb lw kd ke kf lx kh ki kj ly kl km kn lz kp kq kr ma kt ku kv ij bi translated">正如每个机器学习项目一样，建立输入输出管道非常重要。我们定义了从声音文件中获取“元数据”的函数:我们可以制作音频频谱图，并简单地从音频数据中提取多个元特征样本。下一步是将我们的预处理函数映射到我们的训练和测试数据。我首先对每个音频样本进行预处理，并将低音和非低音保存在两个不同的列表中。后来我加入了声音和标签。</p><p id="75a1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这种情况下，我们处理的是少量的“阳性”样本和大量的阴性样本。在这种情况下，将你所有的数据标准化是一个非常好的主意。我的阳性样本可能不同于正态分布，并且很容易被检测到。为此，我使用了<a class="ae kw" href="http://scikit-learn.org/stable/modules/preprocessing.html" rel="noopener ugc nofollow" target="_blank"> scikit learn sklearn .预处理函数“normalize”</a>。在培训期间，我发现我对标准化和规范化的想法与scikit的定义完全相反。在这种情况下，这可能不是一个问题，因为正常化蝙蝠声音可能仍然会产生不同于正常化噪音声音的结果。</p><p id="3283" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在[5]中:</p><pre class="ky kz la lb gt mb mc md me aw mf bi"><span id="2106" class="ld le iq mc b gy mg mh l mi mj"># Meta-feature based batsounds and their labels<br/>preprocessed_batsounds = list()<br/>preprocessed_noisesounds = list()<br/><br/>for sound in batsounds:<br/>    expandedsound = audio_to_metadata(sound)<br/>    preprocessed_batsounds.append(expandedsound)<br/>for sound in noisesounds:<br/>    expandedsound = audio_to_metadata(sound)<br/>    preprocessed_noisesounds.append(expandedsound)<br/><br/>labels = [0]*len(preprocessed_noisesounds) + [1]*len(preprocessed_batsounds)<br/>assert len(labels) == len(preprocessed_noisesounds) + len(preprocessed_batsounds)<br/>allsounds = preprocessed_noisesounds + preprocessed_batsounds<br/>allsounds_normalized = normalize(np.array(allsounds),axis=1)<br/>one_hot_labels = keras.utils.to_categorical(labels)<br/>print(allsounds_normalized.shape)<br/>print("Total noise: %d total bat: %d total: %d" % (len(allsounds_normalized), len(preprocessed_batsounds), len(allsounds)))<br/><br/>## Now zip the sounds and labels, shuffle them, and split into a train and testdataset<br/>zipped_data = zip(allsounds_normalized, one_hot_labels)<br/>np.random.shuffle(zipped_data)<br/>random_zipped_data = zipped_data<br/>VALIDATION_PERCENT = 0.8 # use X percent for training, the rest for validation<br/>traindata = random_zipped_data[0:int(VALIDATION_PERCENT*len(random_zipped_data))]<br/>valdata = random_zipped_data[int(VALIDATION_PERCENT*len(random_zipped_data))::]<br/>indata = [x[0] for x in traindata]<br/>outdata = [x[1] for x in traindata]<br/>valin = [x[0] for x in valdata]<br/>valout = [x[1] for x in valdata]</span><span id="7588" class="ld le iq mc b gy mk mh l mi mj">(1229, 110)<br/>Total noise: 1229 total bat: 96 total: 1229</span></pre><h2 id="b6f7" class="ld le iq bd lf lg lh dn li lj lk dp ll kj lm ln lo kn lp lq lr kr ls lt lu lv bi translated">机器学习模型</h2><p id="3733" class="pw-post-body-paragraph jy jz iq ka b kb lw kd ke kf lx kh ki kj ly kl km kn lz kp kq kr ma kt ku kv ij bi translated">为了检测蝙蝠，我决定尝试一个非常简单的具有三个隐藏层的神经网络。由于可训练参数太少，网络只能区分无声和有声。有了太多的可训练参数，网络将很容易在我们拥有的小数据集上过度适应。</p><p id="b653" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我决定在Keras中实现这个网络，这个库给了我最好的函数，让我可以很容易地在这个简单的问题上尝试不同的神经网络架构。</p><p id="e2ea" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在[6]中:</p><pre class="ky kz la lb gt mb mc md me aw mf bi"><span id="f39d" class="ld le iq mc b gy mg mh l mi mj">LEN_SOUND = len(preprocessed_batsounds[0])<br/>NUM_CLASSES = 2 # Bat or no bat<br/><br/>model = Sequential()<br/>model.add(Dense(128, activation='relu',input_shape=(LEN_SOUND,)))<br/>model.add(Dense(32, activation='relu'))<br/>model.add(Dense(32, activation='relu'))<br/>model.add(Dense(2))<br/>model.compile(loss="mean_squared_error", optimizer='adam', metrics=['mae','accuracy'])<br/>model.summary()<br/>model.fit(np.array(indata), np.array(outdata), batch_size=64, epochs=10,verbose=2, shuffle=True) <br/>valresults = model.evaluate(np.array(valin), np.array(valout), verbose=0)<br/>res_and_name = zip(valresults, model.metrics_names)<br/>for result,name in res_and_name: <br/>    print("Validation " + name + ": " + str(result))</span><span id="9f66" class="ld le iq mc b gy mk mh l mi mj">_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>dense_1 (Dense)              (None, 128)               14208     <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 32)                4128      <br/>_________________________________________________________________<br/>dense_3 (Dense)              (None, 32)                1056      <br/>_________________________________________________________________<br/>dense_4 (Dense)              (None, 2)                 66        <br/>=================================================================<br/>Total params: 19,458<br/>Trainable params: 19,458<br/>Non-trainable params: 0<br/>_________________________________________________________________<br/>Epoch 1/10<br/>0s - loss: 0.2835 - mean_absolute_error: 0.4101 - acc: 0.9237<br/>Epoch 2/10<br/>0s - loss: 0.0743 - mean_absolute_error: 0.1625 - acc: 0.9237<br/>Epoch 3/10<br/>0s - loss: 0.0599 - mean_absolute_error: 0.1270 - acc: 0.9237<br/>Epoch 4/10<br/>0s - loss: 0.0554 - mean_absolute_error: 0.1116 - acc: 0.9237<br/>Epoch 5/10<br/>0s - loss: 0.0524 - mean_absolute_error: 0.1071 - acc: 0.9237<br/>Epoch 6/10<br/>0s - loss: 0.0484 - mean_absolute_error: 0.1024 - acc: 0.9237<br/>Epoch 7/10<br/>0s - loss: 0.0436 - mean_absolute_error: 0.1036 - acc: 0.9329<br/>Epoch 8/10<br/>0s - loss: 0.0375 - mean_absolute_error: 0.0983 - acc: 0.9481<br/>Epoch 9/10<br/>0s - loss: 0.0327 - mean_absolute_error: 0.0923 - acc: 0.9624<br/>Epoch 10/10<br/>0s - loss: 0.0290 - mean_absolute_error: 0.0869 - acc: 0.9644<br/>Validation loss: 0.0440898474639<br/>Validation mean_absolute_error: 0.101937913192<br/>Validation acc: 0.930894308458</span></pre><h2 id="ad1d" class="ld le iq bd lf lg lh dn li lj lk dp ll kj lm ln lo kn lp lq lr kr ls lt lu lv bi translated">检测流水线的结果和实现</h2><p id="c34f" class="pw-post-body-paragraph jy jz iq ka b kb lw kd ke kf lx kh ki kj ly kl km kn lz kp kq kr ma kt ku kv ij bi translated">验证集的准确率达到了95 %,看起来我们做得非常好。下一步是检查我们是否能在一段我们从未处理过的更长的音频中找到蝙蝠。我在蝙蝠几乎消失后录了一段录音，让我们看看是否能找到什么:</p><p id="6005" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在[7]中:</p><pre class="ky kz la lb gt mb mc md me aw mf bi"><span id="d62e" class="ld le iq mc b gy mg mh l mi mj">soundarray, sr = librosa.load("batsounds/bats9.m4a")<br/>maxseconds = int(len(soundarray)/sr)<br/>for second in range(maxseconds-1):<br/>    audiosample = np.array(soundarray[second*sr:(second+1)*sr])<br/>    metadata = audio_to_metadata(audiosample)<br/>    testinput = normalize(np.array([metadata]),axis=1)<br/>    prediction = model.predict(testinput)<br/><br/>    if np.argmax(prediction) ==1:<br/>        IPython.display.display(IPython.display.Audio(audiosample, rate=sr,autoplay=True))<br/>        time.sleep(2)<br/>        print("Detected a bat at " + str(second) + " out of " + str(maxseconds) + " seconds")<br/>        print(prediction)</span></pre><p id="95ee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您的浏览器不支持音频元素。</p><pre class="ky kz la lb gt mb mc md me aw mf bi"><span id="b062" class="ld le iq mc b gy mg mh l mi mj">Detected a bat at 514 out of 669 seconds<br/>[[ 0.45205975  0.50231218]]</span></pre><h2 id="6e38" class="ld le iq bd lf lg lh dn li lj lk dp ll kj lm ln lo kn lp lq lr kr ls lt lu lv bi translated">结论，以及类似项目</h2><p id="21fb" class="pw-post-body-paragraph jy jz iq ka b kb lw kd ke kf lx kh ki kj ly kl km kn lz kp kq kr ma kt ku kv ij bi translated">最后，在26分钟的音频中，我的传感器每次检测到1只蝙蝠，而外面可能没有蝙蝠(但我无法证实这一点)。我会断定我的程序有效！现在，我们能够将这个程序集成到一个小管道中，以便每当外面有蝙蝠时向我发出警告，或者我们可以每天进行记录，并每天测量蝙蝠的活动。</p><p id="4f41" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这个项目中，自然智能城市项目创造了T2蝙蝠伦敦项目。通过传感器你可以<a class="ae kw" href="http://www.batslondon.com/sensors/7" rel="noopener ugc nofollow" target="_blank">看到蝙蝠的活动</a>。同样有趣的是，它们的传感器能够捕捉更有趣的声音，比如蝙蝠发出的社交叫声。很高兴看到其他人也对这一主题感兴趣，并且可以比较不同的方法。bats London项目建造了漂亮的盒子，里面有一台计算机，它根据光谱图进行所有的处理。他们使用基于3秒声音文件的卷积神经网络，每6秒记录一次。将来他们甚至想开始区分不同种类的蝙蝠！他们在一个非常有趣的项目上做得非常好！</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/0c54c7d1fc83bb2e5f151fe9cbfa5536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iMDT9-0EjE0cPR6T."/></div></div></figure></div></div>    
</body>
</html>