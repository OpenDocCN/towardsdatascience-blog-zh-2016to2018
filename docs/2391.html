<html>
<head>
<title>Only Numpy: Implementing Different combination of L1 /L2 norm/regularization to Deep Neural Network (regression) with interactive code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Only Numpy:使用交互式代码对深度神经网络(回归)实施 L1 /L2 范数/正则化的不同组合</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/only-numpy-implementing-different-combination-of-l1-norm-l2-norm-l1-regularization-and-14b01a9773b?source=collection_archive---------1-----------------------#2018-01-20">https://towardsdatascience.com/only-numpy-implementing-different-combination-of-l1-norm-l2-norm-l1-regularization-and-14b01a9773b?source=collection_archive---------1-----------------------#2018-01-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/fe7c1f14bc41c2df338957de26fa45b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*pIq2yKnH8ROD_Hy1aNQpMA.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Gif from <a class="ae jy" href="https://giphy.com/gifs/animation-transform-functions-B95LXCXM5LLfa/download" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><p id="4799" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我一直对不同类型的成本函数和正则化技术感兴趣，所以今天，我将实现损失函数和正则化的不同组合，以查看哪种性能最好。我们还将看看每个模型的重量的绝对总和，看看重量变得有多小。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi kx"><img src="../Images/f6d001d4e8ac65a4472ac6d53be5814e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o5V0P3EWIqJLu1wkwACctw.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Ignore the 7 case — we are going to compare 6 cases</figcaption></figure></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="b9c0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">L1-范数损失函数和 L2-范数损失函数</strong></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi ln"><img src="../Images/ff611d3385ada35563bccc0b77254be8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yN8adiw-dFxr8FvrEEKZmA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from <a class="ae jy" href="http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/" rel="noopener ugc nofollow" target="_blank">Chioka’s blog</a></figcaption></figure><p id="c2e8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我认为上述解释是对两种成本函数最简单而有效的解释。所以我就不多说了，让我们看看规则。</p><h1 id="7a7f" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">热门人工智能文章:</h1><blockquote class="mm"><p id="08ff" class="mn mo iq bd mp mq mr ms mt mu mv kw dk translated"><a class="ae jy" href="https://becominghuman.ai/making-a-simple-neural-network-2ea1de81ec20" rel="noopener ugc nofollow" target="_blank"> 1。制作一个简单的神经网络</a></p><p id="0d5f" class="mn mo iq bd mp mq mr ms mt mu mv kw dk translated"><a class="ae jy" href="https://becominghuman.ai/keras-cheat-sheet-neural-networks-in-python-738c0d170c2e" rel="noopener ugc nofollow" target="_blank"> 2。Keras 备忘单:Python 中的神经网络</a></p><p id="79fc" class="mn mo iq bd mp mq mr ms mt mu mv kw dk translated"><a class="ae jy" href="https://becominghuman.ai/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow-1907a5bbb1fa" rel="noopener ugc nofollow" target="_blank"> 3。使用 Tensorflow 实现 RNN-LSTM 的 noob 指南</a></p></blockquote><p id="c881" class="pw-post-body-paragraph jz ka iq kb b kc mw ke kf kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw ij bi translated"><strong class="kb ir"> L1 正则化和 L2 正则化</strong></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nb"><img src="../Images/7091d6299896c073ae6884a8e83f439e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zMLv7EHYtjfr94JOBzjqTA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from <a class="ae jy" href="http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/" rel="noopener ugc nofollow" target="_blank">Chioka’s blog</a></figcaption></figure><p id="1b68" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同样，从上到下的红框代表 L1 正则化和 L2 正则化。我没有什么要补充的了。然而，由于我必须驱动导数(反向传播)，我将触及一些东西。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="a7b1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">绝对函数的导数</strong></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/327ed49520385ae1fe407bf0d85d7b54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*Whx0XMi7Vjlqe5SJfXeCTQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from <a class="ae jy" href="https://math.stackexchange.com/questions/83861/finding-the-derivative-of-x-using-the-limit-definition" rel="noopener ugc nofollow" target="_blank">stack overflow here</a></figcaption></figure><p id="73c9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，绝对函数的导数有三种不同的情况，当 X &gt; 1 时，X &lt; 1 and X = 0.</p><p id="ee51" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">Option 1 → When X &gt; 1，导数= 1 <br/>选项 2 →当 X = 0 时，导数=未定义<br/>选项 3 →当 X &lt; 1 时，导数= -1</p><p id="ac29" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">因为我们不能让梯度“不确定”,所以我打破了这个规则。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/a2fbfb82cf34570cb032b319c50e888e.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*fGVXXHjDzd7_ST_Izq3X4Q.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">My broken rule of derivative</figcaption></figure><p id="22b6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">上面的，是我将用来计算 x 值的导数的函数，如上所述，我没有第二个选项，我们把它合并到第一个选项中。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="78e8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">初始化超参数和权重</strong></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi ne"><img src="../Images/0b8708bbcb722b60a3c09171ba88ae71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WVGtp_julI8Wf0xv8S6Kdw.png"/></div></div></figure><p id="8b60" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">上面是创建训练数据和声明一些噪声以及学习率和阿尔法值(这些是正则化)。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nf"><img src="../Images/2279163941e1ad97d9148a9b6c4924a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TNhvfdE91TMhXjaTZ0vFcQ.png"/></div></div></figure><p id="1092" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">初始化权重并将其复制到每个案例中。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="10ea" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">网络架构+前馈</strong></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi kx"><img src="../Images/b6332bc956d39870afa24600aeaf1f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*52xS-Ll_2YN1P2TC7wsiXw.jpeg"/></div></div></figure><p id="5cfa" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">网拱没什么特别的，简单来说。</p><p id="7478" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">由每层下面的红色标记表示。<br/>第 1 层→ 100 个神经元<br/>第 2 层→ 104 个神经元<br/>第 3 层→ 200 个神经元<br/>第 4 层→ 1 个神经元</p><p id="6b28" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">并且权重具有适当的维度来执行层之间的变换。然而，有两个盒子我想谈一下。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="4b92" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红框→身份激活功能</strong></p><pre class="ky kz la lb gt ng nh ni nj aw nk bi"><span id="41a0" class="nl lp iq nh b gy nm nn l no np">def IDEN(x):<br/>    return x<br/>def d_IDEN(x):<br/>    return 1</span></pre><p id="b7cf" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果用 python 实现，它看起来会像上面一样，非常简单的线性函数。我们需要这个，因为我们要对连续值进行回归。如果你想知道为什么我们需要激活函数，请阅读我的另一篇博文“<a class="ae jy" href="https://medium.com/@SeoJaeDuk/only-numpy-why-we-need-activation-function-non-linearity-in-deep-neural-network-with-529e928820bc" rel="noopener"> Only Numpy:为什么我们需要激活函数(非线性)，在深度神经网络中——带有交互代码</a>”</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="a37b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">蓝盒子→反正切 Sigmoid 函数</strong></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nq"><img src="../Images/4d150d3d02f9495b03d5da8a4a6010f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*Jxv46eNZ_UctYN0hLFvcDQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from <a class="ae jy" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">wiki</a></figcaption></figure><p id="4ffc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">可以把这个函数想象成 tanh()函数，但是范围更广。我个人认为，我们不必拘泥于逻辑的乙状结肠或 tanh。<strong class="kb ir"> <em class="nr">我认为我们需要探索的激活功能种类繁多</em> </strong>。无论如何，如果你想要可视化的 archtan()，请看下图。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi ns"><img src="../Images/600874fc78e8e70b84f20ff4686a3980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5VRqUJlXO-lucdjzuP8spg.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from <a class="ae jy" href="http://mathworld.wolfram.com/InverseTangent.html" rel="noopener ugc nofollow" target="_blank">wolfam Alpha</a></figcaption></figure></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="1b31" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">成本函数和正则化案例</strong></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi kx"><img src="../Images/f6d001d4e8ac65a4472ac6d53be5814e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o5V0P3EWIqJLu1wkwACctw.jpeg"/></div></div></figure><p id="c247" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所示，我们总共可以处理 6 个(忽略 7 个)案例。</p><p id="d58c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">情况 1 → L1 范数损失<br/>情况 2 → L2 范数损失<br/>情况 3 → L1 范数损失+ L1 正则化<br/>情况 4 → L2 范数损失+ L2 正则化<br/>情况 5 → L1 范数损失+ L2 正则化<br/>情况 6 → L2 范数损失+ L1 正则化</p><p id="476c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们将看到每个 case 函数是如何相互区别的！</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="c79f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">反向传播(在情况 1、3 和 4 下)</strong></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi kx"><img src="../Images/b034c5e12595fe130bdc59e3a35a9e21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XeHFbUP9Z3LUmiE7RVAkYw.jpeg"/></div></div></figure><p id="0f80" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">由于每隔一个案例都可以从这三个案例中派生出来，所以我不会做每一个反向传播过程。不过，有一件事我想让你知道。用红色马克笔写的部分是我们违反对绝对函数求导规则的地方！(注意这个！！)</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="698e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">常规结果</strong></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/50e852ea7043fe5374b4a9133cc94834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*wLGzI7aX1Q2kH93yfhWO4g.png"/></div></figure><p id="be23" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">正如预期的那样，正则化的网络对噪声最鲁棒。然而具有纯 L1 范数函数的模型变化最小，但是有一个问题！如果你看到绿色星星的位置，我们可以看到红色回归线的精确度急剧下降。</p><p id="1c7d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">还有，需要注意的一点是蓝星的位置，大多数模型都无法预测 X 开始时 Y 的正确值，这让我很感兴趣。我们来看看权重的绝对和。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nu"><img src="../Images/dedeb4640a7f5fb65bf610374d7875be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZuaAQkflS2acOvJVbq4x7Q.png"/></div></div></figure><p id="4346" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">总的来说，非常清楚的是，具有正则化的模型具有小得多的权重。其中 L2 正则化的 L1 代价函数具有最小的权值。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="769d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">常规之外的思考:结果</strong></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nv"><img src="../Images/1186c43743d44bb41660945a4b857db0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vsx6pEOgtkjrBb1cJpqLPw.png"/></div></div></figure><p id="0b31" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我是从哪里以及如何得到上述结果的？很简单，我没有遵循绝对函数的严格导数，而是稍微放宽了导数。具体区别见下文。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nw"><img src="../Images/774ef1090e3c21ed98d0db6d2302bad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bu8fGzYsLQQts_hfDq_tMQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Regular Back Propagation on Case 5</figcaption></figure><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nx"><img src="../Images/bd970111cc35754962a2b6f2d8681469.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LuSCpOcqAo9mcA3RZoMISg.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Special case for loosening up the derivative on Case 5</figcaption></figure><p id="a0c0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所示，我没有遵循严格的推导规则，而是将成本函数调整为(Layer_4_act — Y)/m。</p><p id="64a6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我认为当谈到深度学习时，有时创造力会产生更好的结果，我不确定，但辛顿博士在反向传播中随机降低权重，仍然取得了良好的结果。无论如何，让我们来看看权重的绝对值总和。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi ny"><img src="../Images/fd8a567b5ed5825a47728b948b91c3f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fHYoNOq4CcrfJSPGMMPQyQ.png"/></div></div></figure></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="b69d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">互动代码—常规结果</strong></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nz"><img src="../Images/b44521c86a410c4eda1c765bae956331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yfRqenQD-c7K0joi-fw8qg.png"/></div></div></figure><p id="2501" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">请点击这个<a class="ae jy" href="https://trinket.io/python3/a1c7b627de" rel="noopener ugc nofollow" target="_blank">链接查看结果和代码。</a></p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="8338" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">互动代码——公约之外的思考:结果</strong></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi oa"><img src="../Images/7c570e95a7d49c19e73ea6d4940ccd71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DbS4T_pZTLtFxj9UbzlifA.png"/></div></div></figure><p id="850c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">请点击此<a class="ae jy" href="https://trinket.io/python3/46f661f6b7" rel="noopener ugc nofollow" target="_blank">链接查看结果和代码。</a></p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><h2 id="15ce" class="nl lp iq bd lq ob oc dn lu od oe dp ly kk of og mc ko oh oi mg ks oj ok mk ol bi translated">最后的话</h2><p id="f061" class="pw-post-body-paragraph jz ka iq kb b kc om ke kf kg on ki kj kk oo km kn ko op kq kr ks oq ku kv kw ij bi translated">我从另一篇文章中看到总共有 7 种正则化技术，所以我只是触及了表面。迫不及待想知道更多。</p><p id="a43f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 找我。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的 twitter <a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>关注我，并访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。如果你感兴趣的话，我还在简单的 RNN <a class="ae jy" href="https://medium.com/@SeoJaeDuk/only-numpy-vanilla-recurrent-neural-network-with-activation-deriving-back-propagation-through-time-4110964a9316" rel="noopener">上做了反向传播。</a></p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="e3b3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考文献</strong></p><ol class=""><li id="f779" class="or os iq kb b kc kd kg kh kk ot ko ou ks ov kw ow ox oy oz bi translated">Seo，J. D. (2018 年 1 月 16 日)。Only Numpy:为什么我们需要激活函数(非线性)，在深度神经网络中—用…2018 年 1 月 20 日检索自<a class="ae jy" href="https://medium.com/@SeoJaeDuk/only-numpy-why-we-need-activation-function-non-linearity-in-deep-neural-network-with-529e928820bc" rel="noopener">https://medium . com/@ SeoJaeDuk/only-Numpy-Why-we-needle-Activation-Function-Non-Linearity-in-Deep-Neural-Network-With-529 e 928820 BC</a></li><li id="9265" class="or os iq kb b kc pa kg pb kk pc ko pd ks pe kw ow ox oy oz bi translated">反正切。(未注明)。检索于 2018 年 1 月 20 日，来自<a class="ae jy" href="http://mathworld.wolfram.com/InverseTangent.html" rel="noopener ugc nofollow" target="_blank">http://mathworld.wolfram.com/InverseTangent.html</a></li><li id="6bab" class="or os iq kb b kc pa kg pb kk pc ko pd ks pe kw ow ox oy oz bi translated">乙状结肠函数。(2018 年 1 月 18 日)。检索于 2018 年 1 月 20 日，来自<a class="ae jy" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Sigmoid_function</a></li><li id="6811" class="or os iq kb b kc pa kg pb kk pc ko pd ks pe kw ow ox oy oz bi translated">(未注明)。检索于 2018 年 1 月 20 日，来自<a class="ae jy" href="http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/" rel="noopener ugc nofollow" target="_blank">http://www . chioka . in/differences-between-L1-and-L2-as-loss-function-and-regulation/</a></li><li id="baad" class="or os iq kb b kc pa kg pb kk pc ko pd ks pe kw ow ox oy oz bi translated">用极限定义求|x|的导数。(未注明)。检索于 2018 年 1 月 20 日，来自<a class="ae jy" href="https://math.stackexchange.com/questions/83861/finding-the-derivative-of-x-using-the-limit-definition" rel="noopener ugc nofollow" target="_blank">https://math . stack exchange . com/questions/83861/finding-the-derivative-of-x-using-the-limit-definition</a></li><li id="6d3a" class="or os iq kb b kc pa kg pb kk pc ko pd ks pe kw ow ox oy oz bi translated">基于 RANSAC 的稳健线性模型估计。(未注明)。2018 年 1 月 20 日检索，来自<a class="ae jy" href="http://scikit-learn.org/stable/auto_examples/linear_model/plot_ransac.html#sphx-glr-auto-examples-linear-model-plot-ransac-py" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/auto _ examples/linear _ model/plot _ ran sac . html # sphx-glr-auto-examples-linear-model-plot-ran sac-py</a></li><li id="e0fc" class="or os iq kb b kc pa kg pb kk pc ko pd ks pe kw ow ox oy oz bi translated">Jain，a .，Shaikh，f .，Choudhary，a .，Singh，g .，&amp; Kaur，P. (2016 年 2 月 07 日)。Python 中脊和套索回归的完整教程。检索于 2018 年 1 月 20 日，来自<a class="ae jy" href="https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/</a></li><li id="4ddd" class="or os iq kb b kc pa kg pb kk pc ko pd ks pe kw ow ox oy oz bi translated">杰基奇，M. (2017 年 10 月 30 日)。建立一个多层神经网络与 L2 正则化使用张量流。检索于 2018 年 1 月 20 日，来自<a class="ae jy" href="https://markojerkic.com/build-a-multi-layer-neural-network-with-l2-regularization-with-tensorflow/" rel="noopener ugc nofollow" target="_blank">https://markojerkic . com/build-a-multi-layer-neural-network-with-L2-regulation-with-tensor flow/</a></li></ol><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="pf pg l"/></div></figure><div class="ky kz la lb gt ab cb"><figure class="ph jr pi pj pk pl pm paragraph-image"><a href="https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c"><img src="../Images/20880898f038333e31843bbd07b0e4df.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*2f7OqE2AJK1KSrhkmD9ZMw.png"/></a></figure><figure class="ph jr pi pj pk pl pm paragraph-image"><a href="https://upscri.be/8f5f8b"><img src="../Images/dd23357ef17960a7bfb82e7b277f50f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*v-PpfkSWHbvlWWamSVHHWg.png"/></a></figure><figure class="ph jr pi pj pk pl pm paragraph-image"><a href="https://becominghuman.ai/write-for-us-48270209de63"><img src="../Images/91ecfb22295488bc2c6af3d2ac34d857.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*Wt2auqISiEAOZxJ-I7brDQ.png"/></a></figure></div></div></div>    
</body>
</html>