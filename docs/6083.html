<html>
<head>
<title>Evaluation of Language Models through Perplexity and Shannon Visualization Method</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于困惑度和香农可视化方法的语言模型评估</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evaluation-of-language-models-through-perplexity-and-shannon-visualization-method-9148fbe10bd0?source=collection_archive---------10-----------------------#2018-11-25">https://towardsdatascience.com/evaluation-of-language-models-through-perplexity-and-shannon-visualization-method-9148fbe10bd0?source=collection_archive---------10-----------------------#2018-11-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bc6c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">你的自然语言处理模型有多好？</h2></div><h1 id="cbda" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">你的语言模型有多好？</h1><p id="54cc" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了回答语言模型的上述问题，我们首先需要回答下面的中间问题:与那些很少遇到或有一些语法错误的句子相比，我们的语言模型是否为语法正确和频繁的句子分配了更高的概率？为了训练任何模型的参数，我们需要一个训练数据集。在训练完模型之后，我们需要评估模型的参数被训练得有多好；为此，我们使用完全不同于训练数据集的测试数据集，因此模型看不到它。之后，我们定义一个评估指标来量化我们的模型在测试数据集上的表现。</p><h1 id="8466" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">语言模型的体内评估</h1><p id="69ef" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了比较两个语言模型 A 和 B，让这两个语言模型通过一个特定的自然语言处理任务并运行作业。之后，比较模型 A 和 B 的准确性，以评估相互比较的模型。自然语言处理任务可以是文本摘要、情感分析等。</p><p id="1861" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">限制</strong>:耗时的评估方式。</p><h1 id="720f" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">语言模型的内在评价:困惑</h1><p id="89ab" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">困惑是由语言模型分配给测试集的概率的乘法倒数，由测试集中的单词数归一化。如果语言模型可以从测试集中预测未见过的单词，即 P(测试集中的句子)最高；那么这样的语言模型更准确。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/86b150d58b77f1e3cab5f11e5ba0e017.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*vkk7LoxkmIRJgbfLs6Rddg.png"/></div></figure><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mg"><img src="../Images/be2c19327fcef11d68588e1f9afaf8c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6o6ftyQ3MmVWJni8YJnnUw.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Perplexity equations. Image credit: <em class="mp">Speech and Language Processing (3rd edition)</em>, Chapter 3 Language Modeling with N-grams, <strong class="bd mq">Slide 33</strong>, available at <a class="ae mr" href="http://web.stanford.edu/class/cs124/lec/languagemodeling.pdf" rel="noopener ugc nofollow" target="_blank">http://web.stanford.edu/~jurafsky/slp3/3.pdf</a></figcaption></figure><p id="6fe8" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">因此，对于测试集来说，更好的语言模型将具有更低的困惑值或更高的概率值。</p><p id="5a39" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">使用来自<em class="ms">华尔街日报数据集</em> </strong>的 3800 万个单词训练并使用 150 万个单词测试的不同 N 元语言模型的示例困惑值</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mt"><img src="../Images/a8b9838fdf9c1648168223be882c361d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*87oFvS2X71XSC28BCjY2JQ.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Image credit: <em class="mp">Speech and Language Processing (3rd edition)</em>, Chapter 3 Language Modeling with N-grams, <strong class="bd mq">Slide 36</strong>, available at <a class="ae mr" href="http://web.stanford.edu/class/cs124/lec/languagemodeling.pdf" rel="noopener ugc nofollow" target="_blank">http://web.stanford.edu/~jurafsky/slp3/3.pdf</a></figcaption></figure><h1 id="63d0" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">困惑背后的直觉作为一个评价指标和香农可视化方法</h1><p id="b7fa" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">下面这个例子可以解释困惑背后的直觉:假设给出一个句子如下:教授给我的任务是 ____。一个更好的语言模型将通过基于使用训练集分配的条件概率值放置单词来产生有意义的句子。因此，我们可以说，语言模型能够多好地预测下一个单词并因此生成有意义的句子是由基于测试集分配给语言模型的困惑值来断言的。</p><p id="122c" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">香农可视化方法</strong></p><p id="9229" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">这是一种从经过训练的语言模型生成句子的方法。假设经训练的语言模型是二元模型，则香农可视化方法创建如下句子:</p><blockquote class="mu mv mw"><p id="079d" class="kx ky ms kz b la lt jr lc ld lu ju lf mx lv li lj my lw lm ln mz lx lq lr ls ij bi translated">根据概率选择一个随机二元模型(<s>，w)现在根据概率选择一个随机二元模型(w，x)以此类推，直到我们选择</s>然后将单词串在一起</p></blockquote><p id="3ca5" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">这里的<s>和</s>分别表示句子的开始和结束。例如:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi na"><img src="../Images/33591adb79c557561dacf3e95dc71fc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WNyDJ177qNDAkTPBDCGCYw.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Image credit: <em class="mp">Speech and Language Processing (3rd edition)</em>, Chapter 3 Language Modeling with N-grams, <strong class="bd mq">Slide 39</strong>, available at <a class="ae mr" href="http://web.stanford.edu/class/cs124/lec/languagemodeling.pdf" rel="noopener ugc nofollow" target="_blank">http://web.stanford.edu/~jurafsky/slp3/3.pdf</a></figcaption></figure><p id="8038" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">近似于莎士比亚</strong></p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nb"><img src="../Images/ea8825f8793999567c839b19842311d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3mCSssdbYYgXNRZ-ufM1GA.png"/></div></div></figure><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nc"><img src="../Images/96286c4039485e4cf032198a8e793296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BLm-gZX04W4WvJbNlweIyQ.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Sentences generated from unigram, bigram, trigram and quadrigram language models trained using Shakespeare’s corpus. Image credit: <em class="mp">Speech and Language Processing (3rd edition)</em>, Chapter 3 Language Modeling with N-grams, <strong class="bd mq">Figure 3.3</strong>, available at <a class="ae mr" href="http://web.stanford.edu/~jurafsky/slp3/3.pdf" rel="noopener ugc nofollow" target="_blank">http://web.stanford.edu/~jurafsky/slp3/3.pdf</a></figcaption></figure><p id="0b6d" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">莎士比亚的语料库和句子生成使用香农可视化方法的局限性</strong></p><p id="a874" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">令牌数= 884，647，类型数= 29，066。然而，在 V * V = 844 亿个可能的二元模型中，莎士比亚的语料库包含了大约 300，000 个二元模型。因此，大约 99.96%的可能二元模型从未在莎士比亚的语料库中出现过。结果，那些看不见的二元模型的二元模型概率值将等于零，使得句子的总概率等于零，进而困惑度等于无穷大。这是一个限制，可以使用<strong class="kz ir">平滑</strong>技术解决。</p><p id="f24d" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">Quadrigram 更糟糕，因为它看起来像莎士比亚的语料库，因为它是莎士比亚的语料库，这是由于 Quadrigram 语言模型中的依赖性增加到等于 3 而导致的<strong class="kz ir">过度学习</strong>。</p><p id="a440" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">逼近华尔街日报</strong></p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nd"><img src="../Images/6d32341af44d5e46ab9e29b154caa70c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eZ6AkL2b7sksm8AM6oURcQ.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Sentences generated from unigram, bigram and trigram language models trained using The Wall Street Journal’s corpus. Image credit: <em class="mp">Speech and Language Processing (3rd edition)</em>, Chapter 3 Language Modeling with N-grams, <strong class="bd mq">Figure 3.4</strong>, available at <a class="ae mr" href="http://web.stanford.edu/~jurafsky/slp3/3.pdf" rel="noopener ugc nofollow" target="_blank">http://web.stanford.edu/~jurafsky/slp3/3.pdf</a></figcaption></figure></div></div>    
</body>
</html>