<html>
<head>
<title>Training a Naive Bayes model to identify the author of an email or document</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">训练朴素贝叶斯模型以识别电子邮件或文档的作者</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-a-naive-bayes-model-to-identify-the-author-of-an-email-or-document-17dc85fa630a?source=collection_archive---------4-----------------------#2018-09-30">https://towardsdatascience.com/training-a-naive-bayes-model-to-identify-the-author-of-an-email-or-document-17dc85fa630a?source=collection_archive---------4-----------------------#2018-09-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/20e75f77ccee34d325151d5284f51010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-mJnFYDiB1sVsoxc"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">“brown track and field” by <a class="ae kc" href="https://unsplash.com/@adigold1?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Adi Goldstein</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="b88d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个例子中，我们使用由两个不同的人写的一组电子邮件或文档。目的是训练一个朴素贝叶斯模型，使其能够根据文档/电子邮件中使用的单词来预测是谁写的文档/电子邮件</p><p id="cfb9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">包含本例中使用的文件的 Github 存储库可以在<a class="ae kc" href="https://github.com/duranivan/naivebayes-email-author" rel="noopener ugc nofollow" target="_blank">这里</a>找到。文件<code class="fe lb lc ld le b">nb_email_author.py</code>包含加载数据、训练模型以及查找训练和测试集预测得分的脚本。</p><p id="2132" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们解释脚本的主要部分和结果。</p><h1 id="354d" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated"><strong class="ak">加载数据</strong></h1><p id="71a4" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">脚本的第一部分加载数据。这里唯一的特殊性是‘pickle’用于反序列化原始数据文件(关于 pickle 和序列化/反序列化的更多信息，请看这个<a class="ae kc" href="https://docs.python.org/3/library/pickle.html" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><p id="ff76" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">加载两个文件，一个包含电子邮件或文档，另一个只包含电子邮件的作者。一旦我们反序列化并加载文件，我们就有两个数组(列表)，一个名为<code class="fe lb lc ld le b">words</code>，一个名为<code class="fe lb lc ld le b">authors</code>，大小都是 17，578。<code class="fe lb lc ld le b">words</code>中的每个元素都是包含电子邮件或文档的单个字符串。“authors”中的每个元素不是 0 就是 1。</p><p id="7bd1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">像往常一样，我们使用 Scikit-learn 方法<code class="fe lb lc ld le b">sklearn.model_selection.train_test_split</code>将数据分成训练集和测试集。</p><pre class="mi mj mk ml gt mm le mn mo aw mp bi"><span id="604f" class="mq lg iq le b gy mr ms l mt mu"><em class="mv">from</em> sklearn.model_selection <em class="mv">import</em> train_test_split</span><span id="b5d9" class="mq lg iq le b gy mw ms l mt mu">features_train, features_test, labels_train, labels_test = train_test_split(words, authors, <em class="mv">test_size</em>=0.1, <em class="mv">random_state</em>=10)</span></pre><h1 id="8300" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated"><strong class="ak">文本矢量化</strong></h1><p id="dd69" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">在机器学习中处理文本时，将文本转换成易于分析和量化的数据是很常见的。</p><p id="9208" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于这一点，最常用的技术是<strong class="kf ir"> tf-idf </strong>简称“词频-逆文档频”，基本反映了一个词在集合或语料库(我们的一组邮件或文档)中对一个文档(邮件)有多重要。</p><p id="6195" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> tf-idf </strong>是一个统计量，随着一个单词在文档中出现的次数而增加，由语料库中包含该单词的文档数量来惩罚(<a class="ae kc" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> Wikipedia </a>)。</p><p id="7927" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">幸运的是，Scikit-learn 有一个方法可以做到这一点(<code class="fe lb lc ld le b">sklearn.feature_extraction.text.TfidfVectorizer</code>)。见文档<a class="ae kc" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><p id="7e6d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们以如下方式将此方法应用于我们的数据:</p><pre class="mi mj mk ml gt mm le mn mo aw mp bi"><span id="9934" class="mq lg iq le b gy mr ms l mt mu"><em class="mv">from</em> sklearn.feature_extraction.text <em class="mv">import</em> TfidfVectorizer</span><span id="9572" class="mq lg iq le b gy mw ms l mt mu">vectorizer = TfidfVectorizer(<em class="mv">sublinear_tf</em>=True, <em class="mv">max_df</em>=0.5, <em class="mv">stop_words</em>='english')</span><span id="2dd5" class="mq lg iq le b gy mw ms l mt mu">features_train = vectorizer.fit_transform(features_train)<br/>features_test = vectorizer.transform(features_test)</span></pre><p id="67d4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lb lc ld le b">TfidfVectorizer</code>设置矢量器。这里我们将<code class="fe lb lc ld le b">sublinear_tf</code>改为 true，用<strong class="kf ir"> 1 + log(tf) </strong>代替<strong class="kf ir"> tf </strong>。这解决了“一个术语在一个文档中出现二十次”不代表“一次出现的重要性的二十倍”(<a class="ae kc" href="https://nlp.stanford.edu/IR-book/html/htmledition/sublinear-tf-scaling-1.html" rel="noopener ugc nofollow" target="_blank">链接</a>)的问题。因此降低了高频词的重要性(注意<strong class="kf ir"> 1+log(1) = 1 </strong>，<strong class="kf ir"> </strong>而<strong class="kf ir"> 1+log(20) = 2.3 </strong>)。</p><p id="3368" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，<code class="fe lb lc ld le b">stop_words</code>被设置为“英语”，因此在这种情况下，诸如“和”、“the”、“him”之类的停用词将被忽略，而<code class="fe lb lc ld le b">max_df=0.5</code>意味着我们将忽略文档频率高于 0.5 的术语(即，出现该术语的文档的比例)。</p><p id="4769" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们对训练集和测试集的特征(在我们的例子中是术语或单词)进行拟合和转换。注意，对于训练集，我们使用<code class="fe lb lc ld le b">fit_transform</code>，对于测试集，我们只使用<code class="fe lb lc ld le b">transform</code>。</p><p id="9a75" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是有意义的，因为我们希望模型通过训练集学习词汇和文档频率，然后将训练特征转换为术语-文档矩阵。对于测试集，我们只想使用学习到的文档频率(idf)和词汇将其转换为术语-文档矩阵。</p><p id="59c2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">让我们用一个简化的例子来看看这个是什么样子的:</strong></p><p id="3c88" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们有以下训练语料库，同样，每个条目代表一个文档/电子邮件:</p><pre class="mi mj mk ml gt mm le mn mo aw mp bi"><span id="12d9" class="mq lg iq le b gy mr ms l mt mu">corpus = [<br/>    "This is my first email.",<br/>    "I'm trying to learn machine learning.",<br/>    "This is the second email",<br/>    "Learning is fun"<br/>]</span></pre><p id="fb84" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们来适应和改造它:</p><pre class="mi mj mk ml gt mm le mn mo aw mp bi"><span id="0ef2" class="mq lg iq le b gy mr ms l mt mu">vectorizer = TfidfVectorizer()</span><span id="8914" class="mq lg iq le b gy mw ms l mt mu">X = vectorizer.fit_transform(corpus)<br/>print(X.__str__)</span><span id="a00d" class="mq lg iq le b gy mw ms l mt mu"><em class="mv"># &lt;4x13 sparse matrix of type ‘&lt;class ‘numpy.float64’&gt;’ with 18 stored elements in Compressed Sparse Row format&gt;</em></span></pre><p id="c066" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lb lc ld le b">fit_transform</code>返回一个稀疏矩阵:</p><pre class="mi mj mk ml gt mm le mn mo aw mp bi"><span id="ec00" class="mq lg iq le b gy mr ms l mt mu">print(X)</span><span id="5369" class="mq lg iq le b gy mw ms l mt mu"><em class="mv"># (0, 10)   0.41263976171812644<br/># (0, 3)    0.3340674500232949<br/># (0, 7)    0.5233812152405496<br/># (0, 1)    0.5233812152405496<br/># (0, 0)    0.41263976171812644<br/># (1, 12)   0.4651619335222394<br/># (1, 11)   0.4651619335222394<br/># (1, 4)    0.4651619335222394<br/># (1, 6)    0.4651619335222394<br/># (1, 5)    0.3667390112974172<br/># (2, 10)   0.41263976171812644<br/># (2, 3)    0.3340674500232949<br/># (2, 0)    0.41263976171812644<br/># (2, 9)    0.5233812152405496<br/># (2, 8)    0.5233812152405496<br/># (3, 3)    0.4480997313625986<br/># (3, 5)    0.5534923152870045<br/># (3, 2)    0.7020348194149619</em></span></pre><p id="c9f8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们将<code class="fe lb lc ld le b">X</code>转换成一个 2D 数组，它看起来像这样(总共有 13 列，每一列代表一个单词/术语，为了简洁起见，奇数列被省略):</p><pre class="mi mj mk ml gt mm le mn mo aw mp bi"><span id="367e" class="mq lg iq le b gy mr ms l mt mu">vocabulary = vectorizer.get_feature_names()</span><span id="371c" class="mq lg iq le b gy mw ms l mt mu">pd.DataFrame(<em class="mv">data</em>=X.toarray(), <em class="mv">columns</em>=vocabulary).iloc[:,0::2]</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/1396a966fb2d0573ea5e1c62a1e15b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*D3sQ_Tup9ZzRrYSyA5OHgA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">TFIDF Matrix (Training Set)</figcaption></figure><pre class="mi mj mk ml gt mm le mn mo aw mp bi"><span id="7583" class="mq lg iq le b gy mr ms l mt mu">print(vocabulary)</span><span id="3234" class="mq lg iq le b gy mw ms l mt mu"><em class="mv"># [‘email’, ‘first’, ‘fun’, ‘is’, ‘learn’, ‘learning’, ‘machine’, ‘my’, ‘second’, ‘the’, ‘this’, ‘to’, ‘trying’]</em></span></pre><p id="44dd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们假设我们有下面的“测试”文档:</p><pre class="mi mj mk ml gt mm le mn mo aw mp bi"><span id="1f61" class="mq lg iq le b gy mr ms l mt mu">test = [“I’m also trying to learn python”]</span></pre><p id="19a8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们转换一下，看看它:</p><pre class="mi mj mk ml gt mm le mn mo aw mp bi"><span id="0985" class="mq lg iq le b gy mr ms l mt mu">X_test = vectorizer.transform(test)</span><span id="4040" class="mq lg iq le b gy mw ms l mt mu">pd.DataFrame(<em class="mv">data</em>=X_test.toarray(), <em class="mv">columns</em>=vocabulary).iloc[:, 0::2]</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/686f689402a53041cb7cfd106d1606d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*gTkjO6ybBpow2feKn94A5g.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">TFIDF Matrix (Test Set)</figcaption></figure><p id="5b15" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就这样，这就是文本或文档如何被矢量化以供进一步分析的过程。</p><h1 id="6f4e" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated"><strong class="ak">选择一组较小的特征</strong></h1><p id="b26b" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">虽然选择一个较小的特征集并不是绝对必要的，但用太多的单词或特征来训练模型可能在计算上具有挑战性。</p><p id="8d85" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本例中，我们使用 Scikit-learn 的<code class="fe lb lc ld le b">SelectPercentile</code>来选择得分最高的特性(<a class="ae kc" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html" rel="noopener ugc nofollow" target="_blank">文档</a>):</p><pre class="mi mj mk ml gt mm le mn mo aw mp bi"><span id="2753" class="mq lg iq le b gy mr ms l mt mu"><em class="mv">from</em> sklearn.feature_selection <em class="mv">import</em> SelectPercentile, f_classif</span><span id="0a80" class="mq lg iq le b gy mw ms l mt mu">selector = SelectPercentile(f_classif, <em class="mv">percentile</em>=10)<br/>selector.fit(features_train, labels_train)</span><span id="be80" class="mq lg iq le b gy mw ms l mt mu">features_train = selector.transform(features_train).toarray()<br/>features_test = selector.transform(features_test).toarray()</span></pre><p id="c8e3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">选择器使用<code class="fe lb lc ld le b">f_classif</code>作为得分函数，计算样本的方差分析 F 值。基本上，我们选择具有最大 F 值的术语(即，在不同类别或作者中，频率均值最有可能不同的术语或单词)。这是常见的，以便选择跨类的最佳区别特征(在最初的 38，209 个单词中，我们最终得到 3，821 个)。</p><h1 id="020a" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated"><strong class="ak">训练一个朴素贝叶斯模型</strong></h1><p id="20b2" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">对于这个例子，我们使用高斯朴素贝叶斯(NB)实现(Scikit-learn 文档<a class="ae kc" href="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB" rel="noopener ugc nofollow" target="_blank">这里</a>)。在以后的文章中，我们将详细讨论朴素贝叶斯背后的理论。</p><p id="8052" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">目前，值得一提的是，NB 是基于应用贝叶斯规则来计算一组单词(文档/电子邮件)由某人或某个类编写的概率或可能性(例如<strong class="kf ir"> P("Chris"| "learn "、" machine "、" trying "，……)</strong>)。</p><p id="2cf0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，没有所谓的朴素贝叶斯法则。“天真”一词的出现是由于假设特征彼此独立(条件独立)，这意味着，对于我们的电子邮件分析，我们假设单词在句子中的位置是完全随机的(即，“am”或“robot”同样可能跟在单词“I”后面，这当然不是真的)。</p><h2 id="f86d" class="mq lg iq bd lh mz na dn ll nb nc dp lp ko nd ne lt ks nf ng lx kw nh ni mb nj bi translated"><strong class="ak"> NB 带 Scikit-learn </strong></h2><p id="f330" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">一般来说，用 Scikit-learn 训练机器学习模型很简单，通常遵循相同的模式:</p><ul class=""><li id="9697" class="nk nl iq kf b kg kh kk kl ko nm ks nn kw no la np nq nr ns bi translated">初始化类模型的实例，</li><li id="608f" class="nk nl iq kf b kg nt kk nu ko nv ks nw kw nx la np nq nr ns bi translated">拟合训练数据，</li><li id="fb85" class="nk nl iq kf b kg nt kk nu ko nv ks nw kw nx la np nq nr ns bi translated">预测测试数据(我们在这里省略了这一点)，</li><li id="56dd" class="nk nl iq kf b kg nt kk nu ko nv ks nw kw nx la np nq nr ns bi translated">计算训练集和测试集分数。</li></ul><pre class="mi mj mk ml gt mm le mn mo aw mp bi"><span id="ac62" class="mq lg iq le b gy mr ms l mt mu"><em class="mv">from</em> sklearn.naive_bayes <em class="mv">import</em> GaussianNB<br/><em class="mv">from</em> time <em class="mv">import</em> time</span><span id="b614" class="mq lg iq le b gy mw ms l mt mu">t0 = time()<br/>model = GaussianNB()<br/>model.fit(features_train, labels_train)</span><span id="3eaa" class="mq lg iq le b gy mw ms l mt mu">print(f”\nTraining time: {round(time()-t0, 3)}s”)</span><span id="e357" class="mq lg iq le b gy mw ms l mt mu">t0 = time()<br/>score_train = model.score(features_train, labels_train)<br/>print(f”Prediction time (train): {round(time()-t0, 3)}s”)</span><span id="9ad3" class="mq lg iq le b gy mw ms l mt mu">t0 = time()<br/>score_test = model.score(features_test, labels_test)<br/>print(f”Prediction time (test): {round(time()-t0, 3)}s”)</span><span id="39ec" class="mq lg iq le b gy mw ms l mt mu">print(“\nTrain set score:”, score_train)<br/>print(“Test set score:”, score_test)</span></pre><p id="920a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果如下:</p><pre class="mi mj mk ml gt mm le mn mo aw mp bi"><span id="65ab" class="mq lg iq le b gy mr ms l mt mu">&gt;&gt;&gt; Training time: 1.601s<br/>&gt;&gt;&gt; Prediction time (train): 1.787s<br/>&gt;&gt;&gt; Prediction time (test): 0.151s<br/>&gt;&gt;&gt; Train set score: 0.9785082174462706<br/>&gt;&gt;&gt; Test set score: 0.9783845278725825</span></pre><p id="5b4e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还不错！</p></div></div>    
</body>
</html>