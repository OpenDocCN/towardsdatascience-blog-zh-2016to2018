# 用 Python 构建预测 API(第 4 部分):解耦模型和 API

> 原文：<https://towardsdatascience.com/building-prediction-apis-in-python-part-4-decoupling-the-model-and-api-4b5eaf2ed125?source=collection_archive---------6----------------------->

![](img/3a652a09bd234e14c407771398c8adf9.png)

[在最后一部分](/building-prediction-apis-in-python-part-3-automated-testing-a7cfa1fa7e9d)中，我们用 pytest 查看了自动化测试，以验证我们的 API 是否正确地对模型评分。在本文中，我们将探讨如何将负责处理请求和准备响应的 API 功能与准备特性和模型评分所需的代码分离开来。

如果你是这个系列的新手，欢迎！此外，您可能希望从头开始，只是为了回顾我们已经完成的内容。

[](/building-prediction-apis-in-python-part-1-series-introduction-basic-example-fe89e12ffbd3) [## 用 Python 构建预测 API(第 1 部分):系列介绍&基本示例

### 好吧，你已经训练了一个模型，但是现在呢？如果没有人会使用，所有的工作都是没有意义的。在某些应用中…

towardsdatascience.com](/building-prediction-apis-in-python-part-1-series-introduction-basic-example-fe89e12ffbd3) 

在这篇文章中，我们将会看到一些代码片段，但是完整的文件可以在 GitHub 上找到。

附注:我非常感谢这个系列给我的反馈。在前三个部分，我花了很多时间修改草稿，以确保尽可能清晰地呈现内容。我最近刚开始面试，所以时间有点紧。我将尽我所能继续定期发帖，但可能会有更长的延迟。我也会试着花更少的时间复习。如果有令人困惑的部分，请告诉我，我会尽力澄清。谢谢！

# 紧密耦合代码

我们将从[第三部分](/building-prediction-apis-in-python-part-3-automated-testing-a7cfa1fa7e9d)中的虹膜预测模型的最后一个例子开始。我们将拒绝缺少`petal_width`的请求；对于所有其他缺失的特征，我们将使用均值插补进行评分。

正如我们所看到的，API 包含了大量与这个特定模型相关的代码。事实上，大部分代码都是特定于这个模型或 iris 模型的一个版本的:

1.  我们从请求中检索硬编码的查询字符串参数，将它们转换成`float`值，如果它们丢失或不能转换，就用默认值替换它们。
2.  我们正在检查`petal_width`是否丢失，并准备发送一个特定的错误消息给呼叫者。
3.  我们正在创建特征向量。API 需要知道将特性放入`features`列表的正确顺序。
4.  类别标签包含在一个全局`MODEL_LABELS`变量中。

我们可以修改这个代码来处理这个模型的变体。例如，对我们的特性使用硬编码默认值通常是糟糕的编码实践。我们可以将这些提取到一个配置文件中。这将允许我们用不同的平均插补值对模型的两个版本进行评分。

不幸的是，我们将很快发现其他的变化将很难适应。假设我们想要实现两个不同的模型意图:当前版本和一个如果缺少`petal_width`我们将估算，但是拒绝缺少`sepal_width`的请求。如果处理其中一个模型的缺失值的逻辑变得更加复杂怎么办？如果我们有一个完全不同的模型——一个使用 40 个特征来预测欺诈的模型——会怎么样？

正如在之前的一篇文章中提到的，我们可以为每种类型的模型创建单独的 API 或端点，但是引用 Raymond Hettinger 的话，“一定有更好的方法！”

# 拉开线球

在理想情况下，我们会将这种功能划分如下:

*   API:接受请求，找到合适的模型进行评分，将原始数据传递给模型进行评分，并根据模型输出准备响应。
*   Model:提取正确的字段，将原始数据准备成特性，预测是否可以对记录进行评分，如果不能，则引发错误，并将结果发送回 API。

## 包装我们的模型

有几种方法可以实现这一点，但是我们将从一种简单的方法开始。我们将在一些额外的代码中包装我们的模型，这些代码将处理特征检索和准备。

`ModelWrapper`类只有三种方法:`__init__`、`predict`和`_prepare_features`。当我们创建一个实例时，我们将传入带有`class_labels`(之前存储在`MODEL_LABELS`中)和`feature_defaults` ( `dict`包含插补值)的 Scikit-Learn 模型对象(`model_object`)。

API 将调用`predict`方法来获得预测。API 将直接传入`request.args`对象，模型将提取正确的字段。缺失值的提取和处理已转移到单独的方法`_prepare_features`。这些函数中的代码与我们最初在 API 中的代码几乎相同。

顺便提一句，如果您不熟悉 Python 中的单个前导下划线约定，这是一种将方法标记为“仅供内部使用”的方式——代码只打算由类(或其基类/子类)中定义的其他方法调用。然而，没有机制可以阻止任何人直接调用它。

最后要讨论的是我们如何将预测或错误返回给 API 代码。如果我们成功了，我们将返回一个带有`label`和`probabilities`的`dict`。在出现错误的情况下，我们将引发一个定制的异常，`ModelError`，它被定义在文件的顶部。一个奇怪的实现细节是我选择将这个异常作为`ModelWrapper`的一部分。稍后，我们将在 API 中使用它来引用`try/except`块中的这个异常。更干净的方法是在定义的地方有一个共享的工具模块/库。然后，模型包装器代码和 API 都可以引用这个定义。我认为共享库的方法可能更令人困惑(单独的文件),这就是为什么我选择了这个。

## 修改模型构建

既然我们已经定义了包装器代码，我们需要将它合并到我们的模型构建过程中。这样，我们可以使用`joblib`将模型、特征提取和评分代码一起打包/保存。

基本上，我们只是创建了一个`dict`，它有键的特性名称和特性的平均值作为相应的值。我们包装模型，然后保存这个包装的版本。为了清楚起见，我省略了生成我们的测试数据集的代码(参见[第 3 部分](/building-prediction-apis-in-python-part-3-automated-testing-a7cfa1fa7e9d)，但是你可以在这里找到[完整的文件。](https://github.com/cmoradi/prediction-apis/blob/master/part03-testing/code/04_test_reject_missing_petalwidth/build_model_v1.0.py)

## 新的 API

最后一步是修改我们的 API 来使用包装的模型。

大多数情况下，我们需要删除所有的特征提取和评分代码。`MODEL`以同样的方式加载(作为一个全局变量)，但是这个版本除了包含我们的 Scikit 模型之外，还包含了所有的`ModelWrapper`代码。在一个`try/except`街区，我们叫`MODEL.predict()`，但现在我们只是路过`request.args`。如果出现异常，我们可以捕捉异常，并将错误消息转换成正确的响应(带有正确的状态代码)。如果没有出现异常，我们将返回一个带有标签和类别概率的`dict`。剩下的唯一事情就是通过`jsonify()`准备响应。

## 运行我们的测试

我们可以复制我们在第 3 部分中构建的`test_predict_api.py`文件(这里也有)。将它与所有其他文件放在同一个目录中，并运行`pytest`。这些测试应该都能通过。

# 这种方法的局限性

我们已经成功地将我们的模型评分代码从我们的通用 API 代码中分离出来。现在，API 的`predict`端点中没有任何东西是特定于这个 model⁴的，所以我们可以快速扩展这个 API 来对多个模型进行评分。

虽然这是一个巨大的进步，但这种方法存在一些挑战。主要的一点是特性生成代码与一般的模型构建代码是分开的。可能很难看到这一点，因为我们的模型构建非常简单。我们的训练数据集甚至没有任何缺失值，所以我们不需要做均值插补。假设我们正在使用一个更现实的模型，该模型需要分类变量的虚拟/一次性编码、插补或复杂的特征计算，而不仅仅是转换到`float`。使用我们当前的方法，我们可能需要维护这个特征化代码的两个版本:一个操作训练数据，另一个版本是`ModelWrapper`的一部分。

创建一个可以应用于训练和评分环境的统一特征代码库有巨大的好处，但这可能具有挑战性，并不总是可能的。通过在两个上下文中使用单一版本的特征化代码，我们减少了错误，并且可以更快地部署模型。这不总是可能的主要原因是每个上下文有不同的约束。在模型构建过程中，必须准备好整个训练数据集，因此可以优化实现以一次处理许多记录。对于评分，我们只需要准备一个单一的记录，但这应该尽快完成。

我们不打算在这篇文章中讨论它，但是 Scikit-Learn 有一个管道特性，让我们定义在模型构建和评分上下文之间共享的预处理/特征化步骤。然而，这也不是一个完美的解决方案。对可以在管道中定义的步骤类型有一些约束，这可能是限制性的。⁵

# 脚注

1.  Raymond 从事 Python 核心开发已经超过 15 年，并且是`collections`和`itertools`模块的创建者。他还教授 Python 多年，是一位杰出的教育家。谷歌一下他，看看他所有的演讲，因为他们简直太棒了。
2.  你可以把这些看作是 Java 类中的`private`或`protected`方法。但是，Java 明确禁止从外部调用这些方法。Python 不会。
3.  状态不是结果`dict`的一部分，所以我们需要添加它。如果你不熟悉`**result`，这是执行关键字解包。快速举例:`d = {'a': 1, 'b': 2, 'c': 3}`。然后，调用:`myfuct(**d)`等价于调用:`myfunct(a=1, b=2, c=3)`
4.  我们仍然有硬编码的`MODEL`全局变量，但是我们会在将来修复它。
5.  可能有一个我们希望在 API 平台上运行的特定步骤，我们不需要在模型构建过程中运行。在以后的文章中，我们将关注日志/数据库存储。我们可能希望存储的一件事是准备好的特征向量，以便如果我们在 API 平台上遇到问题，我们可以验证特征是否被正确计算。为此，我们需要在模型评分发生之前在管道中插入一个“记录”步骤(管道的最后一步)。我不确定这是否可能。