<html>
<head>
<title>The General Ideas of Word Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入的一般概念</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-three-main-branches-of-word-embeddings-7b90fa36dfb9?source=collection_archive---------5-----------------------#2018-12-30">https://towardsdatascience.com/the-three-main-branches-of-word-embeddings-7b90fa36dfb9?source=collection_archive---------5-----------------------#2018-12-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/2fc571afaafda4f16595ad50fad13ad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nDl7ltgOg4dIgAWb"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@ratushny?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Dmitry Ratushny</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="2c7b" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">Word2Vec、GloVe 和 FastText 的主要概念概述</h2></div><p id="252f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">单词嵌入</strong>——或分布式表示——的概念是近年来自然语言处理(<strong class="la jk"> NLP </strong>)中最引人注目的发展。与所有快节奏的领域一样，人们很容易迷失方向，感觉被最新的突破和发展抛在后面。</p><p id="70de" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最好的解药是意识到更普遍的趋势和单词嵌入概念背后的主要思想。为了做到这一点，我在这篇文章中为你提供了一个简要的概述，并在底部提供了其他材料的链接。这篇文章关注三种“经典”风格的单词嵌入:<strong class="la jk"> Word2Vec </strong>、<strong class="la jk"> GloVe </strong>和<strong class="la jk"> FastText </strong>。他们举例说明了看待单词嵌入的三种不同方式。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mb"><img src="../Images/17c866f638c7477753366444f47ef0e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hdthiYAPL5dAphRH"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://unsplash.com/@evankirby2?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Evan Kirby</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="2187" class="mg mh jj bd mi mj mk ml mm mn mo mp mq kp mr kq ms ks mt kt mu kv mv kw mw mx bi translated">这一切是如何开始的— Word2Vec (2013)</h1><p id="65bf" class="pw-post-body-paragraph ky kz jj la b lb my kk ld le mz kn lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi translated">单词嵌入的雪崩始于 2013 年，当时托马斯·米科洛夫周围的谷歌研究人员发表了<a class="ae jg" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">这篇论文</strong> </a>。他们提出了一种方法，即众所周知的 Word2Vec。它使用小型神经网络来计算基于单词上下文的单词嵌入。有两种方法可以实现这种方法。</p><p id="90a7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先是<strong class="la jk">连续袋字</strong> <em class="nd"> </em>或<strong class="la jk"> CBOW </strong>。在这种方法中，网络试图根据上下文预测哪个单词最有可能出现。同样可能出现的单词可以被解释为具有共享维度。如果我们可以将一个句子中的"<em class="nd">猫"</em>替换为"<em class="nd">狗</em>"，这种方法预测两者的概率相似。所以我们推断这几个词的意思至少在一个层面上是相似的。</p><p id="a678" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第二种方法是<strong class="la jk">跳格</strong>。这个想法非常相似，但是网络的工作方式正好相反。也就是说，它使用目标单词来预测其上下文。更多细节请看本文末尾的链接。</p><p id="6dff" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当 2013 年 Word2Vec 问世时，结果是前所未有的，但也很难从理论角度解释。它工作了，但是有一些困惑为什么。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ne"><img src="../Images/67284b9bb79c5819aba502187a702a27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DplXqptnTPguorF3"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://unsplash.com/@kris_ricepees?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Gary Bendig</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="6263" class="mg mh jj bd mi mj mk ml mm mn mo mp mq kp mr kq ms ks mt kt mu kv mv kw mw mx bi translated">斯坦福大学的竞争方法——GloVe(2014 年)</h1><p id="bb6b" class="pw-post-body-paragraph ky kz jj la b lb my kk ld le mz kn lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi translated">一年后，斯坦福的研究人员发布了<strong class="la jk">手套</strong>。你可以在这里找到<em class="nd"> </em> <a class="ae jg" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">原文</strong> </a>。为了理解这个变体试图做什么，我们需要简单地讨论一下 Word2Vec 的一个不太明显的方面。</p><p id="9325" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Word2Vec 通过将目标单词与其上下文相关联来学习嵌入。但是，它忽略了某些上下文单词是否比其他单词出现得更频繁。对于<strong class="la jk"> Word2Vec </strong>，一个词的频繁同现创造了<strong class="la jk">更多的训练实例</strong>，但是它没有携带<strong class="la jk">额外的信息</strong>。</p><p id="80ba" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相比之下，<strong class="la jk"> GloVe </strong>强调<strong class="la jk">共现的频率是至关重要的信息</strong>，不应作为额外的训练示例而被“浪费”。相反，GloVe 构建单词嵌入的方式是，单词向量的组合与这些单词在语料库中共现的概率直接相关。</p><p id="9e0f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样，查看底部的链接了解更多详情。对于这篇文章来说，理解 GloVe 不是 Word2Vec 意义上的训练有素的模型就足够了。相反，它的嵌入可以被解释为反映共现的低维度训练语料库的总结。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nf"><img src="../Images/37d5ad08bf38259e85cb29cf64f3b2ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*q6m1Qgn2MwMR9uBz"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://unsplash.com/@loverna?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Loverna Journey</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="ceff" class="mg mh jj bd mi mj mk ml mm mn mo mp mq kp mr kq ms ks mt kt mu kv mv kw mw mx bi translated">轮到脸书了——快速文本(2016)</h1><p id="82bd" class="pw-post-body-paragraph ky kz jj la b lb my kk ld le mz kn lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi translated">2016 年，人工神经网络已经获得了相当大的吸引力，Word2Vec 已经证明了它在 NLP 的许多领域中的实用性。然而，还有一个问题没有解决:对未知单词的泛化。脸书于 2016 年发布的开发产品 FastText 承诺要克服这一障碍。</p><p id="3a61" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个想法非常类似于 Word2Vec，但是有一个主要的变化。与其使用单词来构建单词嵌入，<strong class="la jk"> FastText 更深入一层</strong>。这个更深的层次由部分单词和字符组成。从某种意义上说，一个词成为它的上下文。因此，基石是文字，而不是语言。</p><p id="b231" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">FastText 输出的单词嵌入看起来与 Word2Vec 提供的非常相似。但是，它们不是直接计算的。相反，它们是低层嵌入的组合。</p><p id="003c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种方法有两个主要优点。首先，<strong class="la jk">泛化是可能的</strong>只要新单词与已知单词具有相同的特征。第二，需要较少的训练数据，因为可以从每段文本中提取更多的信息。这就是为什么预训练的快速文本模型比其他任何嵌入算法都适用于更多的语言。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ng"><img src="../Images/49e511f6870af6c1d206a2d2fcf582ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3oFXwFwbWSnlDXsy"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@eugi1492?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Eugenio Mazzone</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="d7ae" class="mg mh jj bd mi mj mk ml mm mn mo mp mq kp mr kq ms ks mt kt mu kv mv kw mw mx bi translated">主要的收获</h1><p id="5cc3" class="pw-post-body-paragraph ky kz jj la b lb my kk ld le mz kn lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi translated">我选择这三种算法是因为它们代表了关于如何计算单词嵌入的三种一般思想:</p><ol class=""><li id="cabe" class="nh ni jj la b lb lc le lf lh nj ll nk lp nl lt nm nn no np bi translated">Word2Vec 将文本作为神经网络的训练数据。最终的嵌入捕获单词是否出现在相似的上下文中。</li><li id="b233" class="nh ni jj la b lb nq le nr lh ns ll nt lp nu lt nm nn no np bi translated"><strong class="la jk"> GloVe </strong>关注整个语料库中的同现词。它的嵌入与两个词一起出现的概率有关。</li><li id="7629" class="nh ni jj la b lb nq le nr lh ns ll nt lp nu lt nm nn no np bi translated">FastText 对 Word2Vec 进行了改进，也考虑了单词部分。这种技巧能够在较小的数据集上训练嵌入，并推广到未知单词。</li></ol><p id="8052" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果对你有帮助或者你想补充什么，请在评论中或者在<a class="ae jg" href="https://twitter.com/TimoBohm" rel="noopener ugc nofollow" target="_blank">推特</a>上告诉我。我也很乐意在<a class="ae jg" href="https://www.linkedin.com/in/timo-boehm-datascience/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系。<strong class="la jk">感谢阅读！</strong></p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="397b" class="mg mh jj bd mi mj nv ml mm mn nw mp mq kp nx kq ms ks ny kt mu kv nz kw mw mx bi translated">附加材料</h1><h2 id="3c43" class="oa mh jj bd mi ob oc dn mm od oe dp mq lh of og ms ll oh oi mu lp oj ok mw ol bi translated">Word2Vec</h2><p id="efbc" class="pw-post-body-paragraph ky kz jj la b lb my kk ld le mz kn lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi translated">如果你想深入了解，这里有两篇马尼什·查布拉尼的精彩博文:</p><div class="is it gp gr iu oo"><a rel="noopener follow" target="_blank" href="/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jk gy z fp ot fr fs ou fu fw ji bi translated">Word2Vec(跳格模型):第 1 部分——直觉。</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">这里的大部分内容来自克里斯的博客。我对它进行了压缩，并做了一些小的改动。</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc ja oo"/></div></div></a></div><p id="545f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你正在寻找 Word2Vec 如何进入机器学习的其他领域的例子，看看 Ramzi Karam 的这篇博文:</p><div class="is it gp gr iu oo"><a rel="noopener follow" target="_blank" href="/using-word2vec-for-music-recommendations-bb9649ac2484"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jk gy z fp ot fr fs ou fu fw ji bi translated">使用 Word2vec 进行音乐推荐</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">我们如何使用神经网络将数十亿条数据流转化为更好的推荐。</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pd l oz pa pb ox pc ja oo"/></div></div></a></div><h2 id="e8d0" class="oa mh jj bd mi ob oc dn mm od oe dp mq lh of og ms ll oh oi mu lp oj ok mw ol bi translated">手套</h2><p id="bd47" class="pw-post-body-paragraph ky kz jj la b lb my kk ld le mz kn lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi translated">Brendan Whitaker 写了一个关于手套的五集系列，我强烈推荐。这是第一部分，但很容易找到其他四个部分:</p><div class="is it gp gr iu oo"><a rel="noopener follow" target="_blank" href="/emnlp-what-is-glove-part-i-3b6ce6a7f970"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jk gy z fp ot fr fs ou fu fw ji bi translated">什么是手套？第一部分</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">从共现矩阵无监督学习单词嵌入的介绍。</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pe l oz pa pb ox pc ja oo"/></div></div></a></div><h2 id="5666" class="oa mh jj bd mi ob oc dn mm od oe dp mq lh of og ms ll oh oi mu lp oj ok mw ol bi translated">快速文本</h2><p id="c153" class="pw-post-body-paragraph ky kz jj la b lb my kk ld le mz kn lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi translated"><a class="om on ep" href="https://medium.com/u/ceeb00afdff0?source=post_page-----7b90fa36dfb9--------------------------------" rel="noopener" target="_blank"> Nishan Subedi </a>在此更详细地描述了 FastText:</p><div class="is it gp gr iu oo"><a rel="noopener follow" target="_blank" href="/fasttext-under-the-hood-11efc57b2b3"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jk gy z fp ot fr fs ou fu fw ji bi translated">快速文本:引擎盖下</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">在这里我们将看到一个性能最好的嵌入库是如何实现的。</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pf l oz pa pb ox pc ja oo"/></div></div></a></div><h2 id="16a9" class="oa mh jj bd mi ob oc dn mm od oe dp mq lh of og ms ll oh oi mu lp oj ok mw ol bi translated">履行</h2><p id="b49a" class="pw-post-body-paragraph ky kz jj la b lb my kk ld le mz kn lg lh na lj lk ll nb ln lo lp nc lr ls lt im bi">There are different ways to implement word embeddings. <a class="om on ep" href="https://medium.com/u/2fc7b9c3f02a?source=post_page-----7b90fa36dfb9--------------------------------" rel="noopener" target="_blank">黃功詳 Steeve Huang</a> describes ways to do this with the Gensim package:</p><div class="is it gp gr iu oo"><a rel="noopener follow" target="_blank" href="/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jk gy z fp ot fr fs ou fu fw ji bi translated">使用 Gensim 嵌入 Word2Vec 和 FastText 单词</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">在自然语言处理(NLP)中，我们经常将单词映射成包含数值的向量，以便机器能够识别</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pg l oz pa pb ox pc ja oo"/></div></div></a></div><p id="0a56" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一种方法是在 tensorflow 中重建结构。aneesh joshi 在这里描述了细节:</p><div class="is it gp gr iu oo"><a rel="noopener follow" target="_blank" href="/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jk gy z fp ot fr fs ou fu fw ji bi translated">通过在 tensorflow 中实现来学习 Word2Vec</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">通过 tensorflow 中的编码理解 word2vec</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="ph l oz pa pb ox pc ja oo"/></div></div></a></div></div></div>    
</body>
</html>