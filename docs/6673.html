<html>
<head>
<title>Intuitions on L1 and L2 Regularisation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对 L1 和 L2 正则化的直觉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261?source=collection_archive---------0-----------------------#2018-12-26">https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261?source=collection_archive---------0-----------------------#2018-12-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/e5dcde5f44369629a9da63dca6343ab5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7DYIUpuT5RSSc5uMCSrOTQ.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/photos/XFQvdjALGv4?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">rawpixel</a> on <a class="ae jd" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="373b" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">解释 L1 和 L2 如何使用梯度下降法</h2></div><p id="7224" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">(右跳</em> <a class="ae jd" href="#15c2" rel="noopener ugc nofollow"> <em class="lr">此处</em> </a> <em class="lr">跳过引见。)</em></p><p id="32b3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2020 年 3 月 27 日:在 2-norm 和 p-norm 中的术语中添加了 absolute。感谢 <a class="ls lt ep" href="https://medium.com/u/2bd22be5dd60?source=post_page-----235f2db4c261--------------------------------" rel="noopener" target="_blank"> <em class="lr">里卡多 N 桑托斯</em> </a> <em class="lr">指出这一点。</em></p><p id="a16c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> O </span> <strong class="kx jh">过度拟合</strong>是一种机器学习或统计模型针对特定数据集定制，无法推广到其他数据集时发生的现象。这通常发生在复杂的模型中，比如深度神经网络。</p><p id="ed40" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">规则化</strong>是引入额外信息以防止过度拟合的过程。本文的重点是 L1 和 L2 的正规化。</p><p id="2e3e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有很多解释，但老实说，它们有点太抽象了，我可能会忘记它们，并最终访问这些页面，结果又忘记了。在这篇文章中，我将与你分享一些直觉，为什么 L1 和 L2 使用<strong class="kx jh">梯度下降</strong>进行解释。梯度下降是一种简单的方法，通过使用梯度值的迭代更新来找到“正确的”系数。(这篇文章展示了如何在简单的线性回归中使用梯度下降。)</p><h2 id="1627" class="md me jg bd mf mg mh dn mi mj mk dp ml le mm mn mo li mp mq mr lm ms mt mu mv bi translated">内容</h2><p id="1535" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">0) <a class="ae jd" href="#f810" rel="noopener ugc nofollow">什么是 L1 和 L2？</a> <br/> 1) <a class="ae jd" href="#2a1f" rel="noopener ugc nofollow">模型</a> <br/> 2) <a class="ae jd" href="#1d17" rel="noopener ugc nofollow">损失函数</a> <br/> 3) <a class="ae jd" href="#461b" rel="noopener ugc nofollow">梯度下降</a> <br/> 4) <a class="ae jd" href="#15c2" rel="noopener ugc nofollow">如何防止过拟合？</a></p><p id="bcab" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们走吧！</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h2 id="f810" class="md me jg bd mf mg mh dn mi mj mk dp ml le mm mn mo li mp mq mr lm ms mt mu mv bi translated">0)什么是 L1 和 L2？</h2><p id="3053" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">L1 和 L2 的正规化分别归功于 L1 和 L2 规范的一个向量<strong class="kx jh">、T42、w</strong>。这里有一个关于规范的初级读本:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/df01df244547d374261c5381bbbad0d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*wPXsgTmOJ4GtLhc2vDWxDQ@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">1-norm (also known as L1 norm)</figcaption></figure><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/d14348a5e896f3326621324fc9bfa053.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*44D2AeENsOASBjxWA-XcFw@2x.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">2-norm (also known as L2 norm or Euclidean norm)</figcaption></figure><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/00dbd7c1001d4cc6dbdf24be173078c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P1RbxZbgSb51in3zhwwEGQ@2x.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><em class="np">p</em>-norm</figcaption></figure><p id="9c56" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><change log:="" missed="" out="" taking="" the="" absolutes="" for="" and="" p-norm=""/></p><p id="cf82" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">实现 L1 范数正则化的线性回归模型称为<strong class="kx jh">套索回归</strong>，实现(平方)L2 范数正则化的线性回归模型称为<strong class="kx jh">岭回归</strong>。要实现这两个，请注意线性回归模型保持不变:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nq"><img src="../Images/3ac2ec69aa4080e3169556437172406c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0yeLlLHof9GUiDjaGmGJwg@2x.png"/></div></div></figure><p id="5998" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是损失函数的计算包括这些正则化项:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/98c87f82c17cd8959d262696d4bc13fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*M5JUlsA7zg1eCJRuGUl3yg@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Loss function with no regularisation</figcaption></figure><figure class="nj nk nl nm gt is gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/4f9cb1cbd67800cbf987961d0e674e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*1zCcVuEOPi64mjkkF6Uj7w@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Loss function with L1 regularisation</figcaption></figure><figure class="nj nk nl nm gt is gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/2cb16faf6f2c0253e1278f0e10c93a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*FfBxNNuFoCnzq8eEzPrRcw@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Loss function with L2 regularisation</figcaption></figure><blockquote class="nt nu nv"><p id="464e" class="kv kw lr kx b ky kz kh la lb lc kk ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated"><em class="jg">注:严格来说，最后一个方程(岭回归)是一个权重为</em> <strong class="kx jh"> <em class="jg">平方</em> </strong> <em class="jg"> L2 范数的损失函数(注意没有平方根)。(感谢</em><a class="ls lt ep" href="https://medium.com/u/38025716cba8?source=post_page-----235f2db4c261--------------------------------" rel="noopener" target="_blank"><em class="jg">Max Pechyonkin</em></a><em class="jg">强调这一点！)</em></p></blockquote><p id="7415" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">除了必须最小化真实<em class="lr"> y </em>和预测<em class="lr"> ŷ </em>之间的误差之外，正则化项是优化算法在最小化损失函数时必须“遵守”的“约束”。</p><h2 id="2a1f" class="md me jg bd mf mg mh dn mi mj mk dp ml le mm mn mo li mp mq mr lm ms mt mu mv bi translated"><strong class="ak"> 1)型号</strong></h2><p id="ea74" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">让我们定义一个模型来看看 L1 和 L2 是如何工作的。为简单起见，我们定义一个简单的线性回归模型<em class="lr"> ŷ </em>与一个独立变量。</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/fd8c0cbcb5d95aae7e6657d71dd5562e.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*9NO4BAzUUNuEF8NL0iHz7w@2x.png"/></div></figure><p id="2075" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这里，我使用了深度学习约定<em class="lr"> w </em>(“权重”)和<em class="lr"> b </em>(“偏差”)。</p><blockquote class="nt nu nv"><p id="f2d5" class="kv kw lr kx b ky kz kh la lb lc kk ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">在实践中，简单的线性回归模型不容易过度拟合。正如引言中所提到的，深度学习模型由于其模型的复杂性，更容易受到此类问题的影响。</p><p id="d02b" class="kv kw lr kx b ky kz kh la lb lc kk ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">因此，请注意，本文中使用的表达式很容易扩展到更复杂的模型，而不仅限于线性回归。</p></blockquote><h2 id="1d17" class="md me jg bd mf mg mh dn mi mj mk dp ml le mm mn mo li mp mq mr lm ms mt mu mv bi translated"><strong class="ak"> 2)损失函数</strong></h2><p id="22da" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">为了证明 L1 和 L2 正则化的效果，让我们使用 3 种不同的损失函数/目标来拟合我们的线性回归模型:</p><ul class=""><li id="cf4f" class="oa ob jg kx b ky kz lb lc le oc li od lm oe lq of og oh oi bi translated">L</li><li id="d231" class="oa ob jg kx b ky oj lb ok le ol li om lm on lq of og oh oi bi translated">腰神经 2</li><li id="e87d" class="oa ob jg kx b ky oj lb ok le ol li om lm on lq of og oh oi bi translated">L2</li></ul><p id="8f92" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的目标是尽量减少这些不同的损失。</p><h2 id="2604" class="md me jg bd mf mg mh dn mi mj mk dp ml le mm mn mo li mp mq mr lm ms mt mu mv bi translated"><strong class="ak"> 2.1)无正则化的损失函数</strong></h2><p id="32f2" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">我们将损失函数 l 定义为平方误差，其中误差是<em class="lr"> y </em>(真实值)和<em class="lr"> ŷ </em>(预测值)之差。</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/11eb1291da9e1ad4224896c1d7df28fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*RV0od0ju0m60EoeMn0nMzQ@2x.png"/></div></figure><p id="5854" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们假设使用这个损失函数，我们的模型会过度拟合。</p><h2 id="8e05" class="md me jg bd mf mg mh dn mi mj mk dp ml le mm mn mo li mp mq mr lm ms mt mu mv bi translated"><strong class="ak">2.2)L1 正则化损失函数</strong></h2><p id="402c" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">基于上述损失函数，添加一个 L1 正则化项，如下所示:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div class="gh gi op"><img src="../Images/6659e81a3378bc2d29476e8d2bd7fa20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*qrK0afS1Av7K7YDJn1RpNw@2x.png"/></div></figure><p id="31a4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中调整参数<em class="lr">λ</em>T58】0 是手动调整的。让我们称这个损失函数为 L1。注意| <em class="lr"> w| </em>处处可微，除了当<em class="lr"> w </em> =0 时，如下图。我们以后会需要这个。</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oq"><img src="../Images/44fd0f2aba588508b34da92103fb7bd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*LpOmoffgS_uIx97r21kSgw@2x.png"/></div></div></figure><h2 id="922c" class="md me jg bd mf mg mh dn mi mj mk dp ml le mm mn mo li mp mq mr lm ms mt mu mv bi translated"><strong class="ak">2.3)L2 正则化损失函数</strong></h2><p id="5d3a" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">类似地，将 L2 正则化项添加到 L 看起来像这样:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div class="gh gi or"><img src="../Images/ca2f2abcd6bcd7f1cb6014473408620d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*rHQqvTzO0N78KgeNy4K6mw@2x.png"/></div></figure><p id="70bb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里又，<em class="lr">λ</em>T59】0。</p><h2 id="461b" class="md me jg bd mf mg mh dn mi mj mk dp ml le mm mn mo li mp mq mr lm ms mt mu mv bi translated">3)梯度下降</h2><p id="32b7" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">现在，让我们基于上述<em class="lr">定义的 3 个损失函数，使用梯度下降优化求解线性回归模型。</em>回想一下，在梯度下降中更新参数<em class="lr"> w </em>如下:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div class="gh gi os"><img src="../Images/ae5f9b780e62c9ba576cfbd3cce9540b.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*bEzqAEMLO_qnXBIWmS9jfg@2x.png"/></div></figure><p id="9db4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们用 L <em class="lr">、</em> L1 和 L2 w.r.t. <em class="lr"> w. </em>的梯度来代替上式中的最后一项</p><p id="8935" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">l:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/bf3e3e9faf0c44fd06b8552ec4152c6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*l6E7S7S36mPPwZ2yMlU_og@2x.png"/></div></figure><p id="a64f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">L1:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ou"><img src="../Images/9bad6384f089891e2cdc82b90d9e94b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hC68XigZjhYVCEJbwuVZrQ@2x.png"/></div></div></figure><p id="4f04" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">L2:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ov"><img src="../Images/bb1ee9ea0a7f0eca2ccb249ed0fe95eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bV49pcaBBMW79adaN5gp_w@2x.png"/></div></div></figure><h2 id="15c2" class="md me jg bd mf mg mh dn mi mj mk dp ml le mm mn mo li mp mq mr lm ms mt mu mv bi translated">4)如何防止过度拟合？</h2><p id="b523" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">从这里开始，让我们对上面的等式进行以下替换(为了更好的可读性):</p><ul class=""><li id="4b66" class="oa ob jg kx b ky kz lb lc le oc li od lm oe lq of og oh oi bi translated"><em class="lr"> η </em> = 1，</li><li id="dda9" class="oa ob jg kx b ky oj lb ok le ol li om lm on lq of og oh oi bi translated"><em class="lr"> H = 2x </em> ( <em class="lr"> wx+b-y </em>)</li></ul><p id="f544" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这给了我们</p><p id="1aaa" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">l:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/bfdded60c109c37bd821618cd0d01d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*k9fevtlt4GIrkIao9fli1g@2x.png"/></div></figure><p id="cdec" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">L1:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ox"><img src="../Images/345fbb4cec7af4b8c170bba86616006b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cX7OClIl2O-ZLTVqfB47-A@2x.png"/></div></div></figure><p id="a9d7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">L2:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oy"><img src="../Images/b6807c924bf2d46d99b1b796850dcbd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pU47ApyQYzt5Yj_jTpVNFw@2x.png"/></div></div></figure><h2 id="2a05" class="md me jg bd mf mg mh dn mi mj mk dp ml le mm mn mo li mp mq mr lm ms mt mu mv bi translated">4.1)有无正规化</h2><p id="4d73" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">观察有正则化参数<em class="lr"> λ </em>和没有正则化参数的重量更新之间的差异。以下是一些直觉。</p><p id="9305" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">直觉 A: </strong></p><p id="8354" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们用等式 0 来说，计算<em class="lr"> w-H </em>给出了导致过拟合的<em class="lr"> w </em>值。然后，直观地，等式{1.1、1.2 和 2}将减少过拟合的机会，因为引入<em class="lr"> λ </em>使我们将<em class="lr">从上一句中导致过拟合问题的<em class="lr"> w </em>处移开</em>。</p><p id="f03a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">直觉 B: </strong></p><p id="e6fb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设一个过度拟合的模型意味着我们有一个<em class="lr"> w </em>值，它对于我们的模型来说是<strong class="kx jh">完美的</strong>。“完美”意味着如果我们将数据(<em class="lr"> x </em>)代入模型，我们的预测<em class="lr"> ŷ </em>将非常非常接近真实的<em class="lr"> y </em>。当然，这很好，但我们不想要完美。为什么？因为这意味着我们的模型只适用于我们训练的数据集。这意味着我们的模型将产生与其他数据集的真实值相差甚远的预测。因此，我们满足于<strong class="kx jh">不太完美的</strong>，希望我们的模型也能得到与其他数据接近的预测。为此，我们用惩罚项<em class="lr"> λ“玷污”等式 0 中的这个完美的<em class="lr"> w </em>。这给了我们等式{1.1，1.2 和 2}。</em></p><p id="5926" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">直觉 C: </strong></p><p id="f213" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意，<em class="lr"> H </em>(此处定义为<a class="ae jd" href="#dda9" rel="noopener ugc nofollow"/>)是<strong class="kx jh">依赖于型号(<em class="lr"> w </em>和<em class="lr"> b </em>)和数据(<em class="lr"> x </em>和<em class="lr"> y </em>)。仅基于模型和等式 0 中的数据更新权重<em class="lr">会导致过度拟合，从而导致泛化能力差。另一方面，在等式{1.1、1.2 和 2}中，<em class="lr"> w </em>的最终值不仅受模型和数据的影响，而且<em class="lr">还受独立于模型和数据的<strong class="kx jh">的预定义参数<em class="lr"> λ </em>的影响。因此，如果我们设置一个合适的值<em class="lr"> λ </em>，我们可以防止过拟合，尽管太大的值会导致模型严重欠拟合。</strong></em></em></strong></p><p id="c87c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">直觉 D: </strong></p><p id="4bf6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ls lt ep" href="https://medium.com/u/a0c252a1bf6d?source=post_page-----235f2db4c261--------------------------------" rel="noopener" target="_blank"> Edden Gerber </a>(谢谢！)提供了一个关于我们的解决方案正在转向的<em class="lr">方向</em>的直觉。在评论里看看:<a class="ae jd" href="https://medium.com/@edden.gerber/thanks-for-the-article-1003ad7478b2" rel="noopener">https://medium . com/@ edden . gerber/thanks-for-the-article-1003 ad 7478 B2</a></p><h2 id="5439" class="md me jg bd mf mg mh dn mi mj mk dp ml le mm mn mo li mp mq mr lm ms mt mu mv bi translated"><strong class="ak"> 4.2) L1 对 L2 </strong></h2><p id="055f" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">我们现在将注意力集中到 L1 和 L2，并通过重新排列它们的<em class="lr"> λ </em>和<em class="lr"> H </em>项来重写方程{1.1、1.2 和 2},如下所示:</p><p id="05f9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">L1:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oz"><img src="../Images/fe50360ccbb98dd0b8147ebc2e0d8ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*abHVX1SUuzdiAWcpDq4TzA@2x.png"/></div></div></figure><p id="d1e1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">L2:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oy"><img src="../Images/ef2813c33fba74e96c73e183e63ca3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5etBmH3PZk7dR0e_H3zA8g@2x.png"/></div></div></figure><p id="3b44" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">比较上面每个等式的第二项。除了<em class="lr"> H </em>之外，<em class="lr"> w </em>的变化取决于<strong class="kx jh"> <em class="lr"> λ </em>项</strong>或<strong class="kx jh"> -2 <em class="lr"> λ </em> w 项</strong>，这突出了以下的影响:</p><ol class=""><li id="79ba" class="oa ob jg kx b ky kz lb lc le oc li od lm oe lq pa og oh oi bi translated">电流的符号<em class="lr"> w </em> (L1，L2)</li><li id="bbae" class="oa ob jg kx b ky oj lb ok le ol li om lm on lq pa og oh oi bi translated">电流大小<em class="lr"> w </em> (L2)</li><li id="064c" class="oa ob jg kx b ky oj lb ok le ol li om lm on lq pa og oh oi bi translated">正则化参数加倍(L2)</li></ol><p id="e52f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用 L1 的权重更新受第一点的影响，而来自 L2 的权重更新受所有三个点的影响。虽然我只是基于迭代方程更新进行了比较，但请注意，这并不意味着一个比另一个“更好”。</p><p id="4277" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，让我们看看下面如何通过当前的<em class="lr"> w. </em>符号获得来自 L1 的正则化效果</p><h2 id="86cf" class="md me jg bd mf mg mh dn mi mj mk dp ml le mm mn mo li mp mq mr lm ms mt mu mv bi translated"><strong class="ak"> 4.3) L1 效应向 0 推进(稀疏度)</strong></h2><p id="b0a0" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">看看方程 3.1 中的 L1。如果<em class="lr"> w </em>为正，正则化参数<em class="lr">λ</em>T54】0 将通过从<em class="lr"> w </em>中减去<em class="lr"> λ </em>来推动<em class="lr"> w </em>为负。相反地，在等式 3.2 中，如果<em class="lr"> w </em>为负，那么<em class="lr"> λ </em>将被加到<em class="lr"> w </em>上，使其变得更小。因此，这具有将<em class="lr"> w </em>推向 0 的效果。</p><p id="f2fa" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这在一元线性回归模型中当然是无意义的，但将证明它在多元回归模型中“去除”无用变量的能力。你也可以把 L1 想成是减少了模型中特性数量的 T52。这里有一个 L1 试图在多元线性回归模型中“推动”一些变量的任意例子:</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pb"><img src="../Images/5d339e0c21bf43c744ce820c7d267a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_hKrbkLm8f8YZmkNmgFurw@2x.png"/></div></div></figure><p id="51d4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么，在 L1 正则化中，将<em class="lr"> w </em>推向 0 如何帮助过度拟合呢？如上所述，当<em class="lr"> w </em>变为零时，我们通过降低变量重要性来减少特征的数量。在上式中，我们看到<em class="lr"> x_2 </em>、<em class="lr"> x_4 </em>和<em class="lr"> x_5 </em>几乎是“无用”的，因为它们的系数很小，因此我们可以将它们从等式中移除。这反过来<strong class="kx jh">降低了模型的复杂性</strong>，使得我们的模型更加简单。更简单的模型可以减少过度拟合的机会。</p><h2 id="3cc6" class="md me jg bd mf mg mh dn mi mj mk dp ml le mm mn mo li mp mq mr lm ms mt mu mv bi translated"><strong class="ak">注</strong></h2><p id="350e" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">虽然 L1 有将权重推向 0 的<em class="lr">影响力，而 L2 没有，但这并不意味着由于 L2，权重不能接近 0。</em></p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="8366" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你发现文章的任何部分令人困惑，请随意突出显示并留下回复。此外，如果有任何反馈或建议如何改进这篇文章，请在下面留下评论！</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h2 id="5847" class="md me jg bd mf mg mh dn mi mj mk dp ml le mm mn mo li mp mq mr lm ms mt mu mv bi translated">参考</h2><p id="b7ec" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Norm_(mathematics)" rel="noopener ugc nofollow" target="_blank">常模(数学)</a>(wikipedia.org)</p><p id="cc77" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Lasso_(statistics)" rel="noopener ugc nofollow" target="_blank">拉索(统计)</a>(wikipedia.org)</p><p id="55a7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" href="https://medium.com/@dk13093/lasso-and-ridge-regularization-7b7b847bce34" rel="noopener">套索和山脊正规化【medium.com T21】</a></p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="f3fd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">特别感谢</em><a class="ae jd" href="https://medium.com/@wyextay" rel="noopener"><em class="lr"/></a><em class="lr"/><a class="ae jd" href="https://medium.com/@renjietan" rel="noopener"><em class="lr">【任杰】</em> </a> <em class="lr">、丹尼尔</em> <a class="ae jd" href="https://medium.com/@derekchia" rel="noopener"> <em class="lr">德里克</em> </a> <em class="lr">对本文的观点、建议和修正。也感谢</em><a class="ls lt ep" href="https://medium.com/u/c3d5b340105?source=post_page-----235f2db4c261--------------------------------" rel="noopener" target="_blank"><em class="lr">C Gam</em></a><em class="lr">指出导数中的错误。</em></p><p id="4d00" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">关注我上</em><a class="ae jd" href="https://www.twitter.com/remykarem" rel="noopener ugc nofollow" target="_blank"><em class="lr">Twitter</em></a><em class="lr">@ remykarem 或者</em><a class="ae jd" href="http://www.linkedin.com/in/raimibkarim" rel="noopener ugc nofollow" target="_blank"><em class="lr">LinkedIn</em></a><em class="lr">。你也可以通过 raimi.bkarim@gmail.com 联系我。欢迎访问我的网站</em><a class="ae jd" href="https://remykarem.github.io/" rel="noopener ugc nofollow" target="_blank"><em class="lr">remykarem . github . io</em></a><em class="lr">。</em></p></div></div>    
</body>
</html>