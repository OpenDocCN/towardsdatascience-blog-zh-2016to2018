<html>
<head>
<title>Data Pre Processing Techniques You Should Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你应该知道的数据预处理技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-pre-processing-techniques-you-should-know-8954662716d6?source=collection_archive---------1-----------------------#2018-12-02">https://towardsdatascience.com/data-pre-processing-techniques-you-should-know-8954662716d6?source=collection_archive---------1-----------------------#2018-12-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/5b182509bb0d4d8a0b476656583b6070.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*xA8rhR_UM7cZCmZgMJQvsQ.jpeg"/></div></figure><p id="e7d0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">嗨伙计们！欢迎回来。今天，我们将讨论可以帮助您获得更高准确度的特征工程技术。</p><p id="e66f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如您所知，对于数据科学家来说，数据可能非常令人生畏。如果你手里有一个数据集，如果你是一个数据科学家，那么你会开始考虑对你手里的原始数据集做些什么。这是数据科学家的天性。所以我还在成为数据科学家的学习过程中。我试图用各种数据预处理技术来充实我的头脑，因为这些技术对于了解你是否想处理数据是非常必要的。</p><p id="fa7a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于这个分析，我将使用 Kaggle 提供的信用卡交易数据集。我已经写了一篇关于使用自动编码器检测信用卡欺诈的文章。链接在这里:<a class="ae ks" href="https://medium.com/@manisharajarathna/credit-card-fraud-detection-using-autoencoders-in-h2o-399cbb7ae4f1" rel="noopener">https://medium . com/@ manisharajarathna/credit-card-fraud-detection-using-auto 编码器-in-h2o-399cbb7ae4f1 </a></p><h1 id="1fbd" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">什么是数据预处理？</h1><p id="21af" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">它是一种将原始数据转换成可理解格式的数据挖掘技术。原始数据(真实世界的数据)总是不完整的，并且这些数据不能通过模型发送。这将导致某些错误。这就是为什么我们需要在通过模型发送之前预处理数据。</p><h2 id="475d" class="lw ku iq bd kv lx ly dn kz lz ma dp ld kf mb mc lh kj md me ll kn mf mg lp mh bi translated">数据预处理的步骤</h2><p id="0b1d" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">以下是我遵循的步骤；<br/> 1。导入库<br/> 2。读取数据<br/> 3。检查缺失值<br/> 4。检查分类数据<br/> 5。标准化数据<br/> 6。PCA 变换<br/> 7。数据分割</p><h1 id="a5bf" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">1.输入数据</h1><p id="495e" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">作为主库，我使用熊猫、Numpy 和 time<br/> <strong class="jw ir">熊猫</strong>:用于数据操作和数据分析。<br/> <strong class="jw ir"> Numpy </strong>:用 Python 进行科学计算的基础包。</p><p id="e52f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">至于可视化，我用的是 Matplotlib 和 Seaborn。<br/>对于数据预处理技术和算法，我<strong class="jw ir"> </strong>使用了<strong class="jw ir"> Scikit-learn </strong>库。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="c7c6" class="lw ku iq mn b gy mr ms l mt mu"># main libraries<br/>import pandas as pd<br/>import numpy as np<br/>import time</span><span id="00be" class="lw ku iq mn b gy mv ms l mt mu"># visual libraries<br/>from matplotlib import pyplot as plt<br/>import seaborn as sns<br/>from mpl_toolkits.mplot3d import Axes3D <br/>plt.style.use('ggplot')</span><span id="8fb3" class="lw ku iq mn b gy mv ms l mt mu"># sklearn libraries<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import normalize<br/>from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,matthews_corrcoef,classification_report,roc_curve<br/>from sklearn.externals import joblib<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.decomposition import PCA</span></pre><h1 id="a725" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">2.读出数据</h1><p id="f4b6" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">你可以在这里找到更多关于数据集的细节:<a class="ae ks" href="https://www.kaggle.com/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/mlg-ulb/creditcardfraud</a></p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="9d5d" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw"># Read the data in the CSV file using pandas</em><br/>df = pd.read_csv('../input/creditcard.csv')<br/>df.head()</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mx"><img src="../Images/6ea83a28b1327e8316b6885afffd6281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8dBW_k09uVpNRu2TnZMaBw.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Fig 1 : Dataset</figcaption></figure><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="19da" class="lw ku iq mn b gy mr ms l mt mu">df.shape<br/>&gt; (284807, 31)</span></pre><h1 id="1748" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">3.检查缺少的值</h1><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="6361" class="lw ku iq mn b gy mr ms l mt mu">df.isnull().any().sum()<br/>&gt; 0</span></pre><p id="066b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因为在数据集中没有发现缺失值，所以我没有使用任何缺失值处理技术。</p><h2 id="2661" class="lw ku iq bd kv lx ly dn kz lz ma dp ld kf mb mc lh kj md me ll kn mf mg lp mh bi translated">让我们看看数据</h2><p id="d39c" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">因此数据集被标记为 0 和 1。</p><ul class=""><li id="5fd8" class="ng nh iq jw b jx jy kb kc kf ni kj nj kn nk kr nl nm nn no bi translated">0 =非欺诈</li><li id="3f45" class="ng nh iq jw b jx np kb nq kf nr kj ns kn nt kr nl nm nn no bi translated">1 =欺诈</li></ul><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="a80f" class="lw ku iq mn b gy mr ms l mt mu">All = df.shape[0]<br/>fraud = df[df['Class'] == 1]<br/>nonFraud = df[df['Class'] == 0]<br/><br/>x = len(fraud)/All<br/>y = len(nonFraud)/All<br/><br/>print('frauds :',x*100,'%')<br/>print('non frauds :',y*100,'%')</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nu"><img src="../Images/6f7269af0b4a0aab484a53724fa77318.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TVdvzpGbxPv7wqAq4ORS4Q.png"/></div></div></figure><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="2735" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw"><br/># Let's plot the Transaction class against the Frequency<br/></em>labels = ['non frauds','fraud']<br/>classes = pd.value_counts(df['Class'], sort = True)<br/>classes.plot(kind = 'bar', rot=0)<br/>plt.title("Transaction class distribution")<br/>plt.xticks(range(2), labels)<br/>plt.xlabel("Class")<br/>plt.ylabel("Frequency")</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/e04b8e47dd55b805207afcff3b06988a.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*8B3NNQfyIod7H-372PBSrg.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Fig 2 : Class vs Frequency</figcaption></figure><h1 id="9c16" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">4.检查分类数据</h1><p id="91b5" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">我们在这个数据集中唯一的分类变量是目标变量。其他要素已经是数字格式，所以不需要转换成分类数据。</p><h2 id="f1c5" class="lw ku iq bd kv lx ly dn kz lz ma dp ld kf mb mc lh kj md me ll kn mf mg lp mh bi translated">让我们画出特征的分布</h2><p id="9f4d" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">我使用 seaborn distplot()来可视化数据集中的要素分布。数据集中有 30 个特征和目标变量。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="a078" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw"># distribution of Amount</em><br/>amount = [df['Amount'].values]<br/>sns.distplot(amount)</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/369bc4ac114d874bf3b80b033cb70e8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*GA-50sKs3iIsiKrethlYXQ.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Fig 3 : Distribution of Amount</figcaption></figure><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="4497" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw"># distribution of Time</em><br/>time = df['Time'].values<br/>sns.distplot(time)</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/edc42360905c2f4cc7efe1616187a3a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*aGuhKXr65KG_i9TsydyIMw.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Fig 4 : Distribution of Time</figcaption></figure><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="56f8" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw"># distribution of anomalous features</em><br/>anomalous_features = df.iloc[:,1:29].columns<br/><br/>plt.figure(figsize=(12,28*4))<br/>gs = gridspec.GridSpec(28, 1)<br/>for i, cn <strong class="mn ir">in</strong> enumerate(df[anomalous_features]):<br/>    ax = plt.subplot(gs[i])<br/>    sns.distplot(df[cn][df.Class == 1], bins=50)<br/>    sns.distplot(df[cn][df.Class == 0], bins=50)<br/>    ax.set_xlabel('')<br/>    ax.set_title('histogram of feature: ' + str(cn))<br/>plt.show()</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ny"><img src="../Images/8acedebb42a9fbc1261ccf7a42441e5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vgoRS86utwMhBKn72YOAAA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Fig 5 : Distribution of anomalous features</figcaption></figure><p id="97cf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在这个分析中，我不会放弃任何查看特征分布的特征，因为我仍处于以多种方式处理数据预处理的学习过程中。所以我想一步一步地实验数据。</p><p id="67b6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">相反，所有的特征将被转换成比例变量。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="e6b8" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw"># heat map of correlation of features</em><br/>correlation_matrix = df.corr()<br/>fig = plt.figure(figsize=(12,9))<br/>sns.heatmap(correlation_matrix,vmax=0.8,square = True)<br/>plt.show()</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/440d3082d56232e35401b3ad4385329b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*i25Zji8lu9-6xQhxhEzJ1A.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Fig 6 : Heatmap of features</figcaption></figure><h1 id="53b5" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">5.使数据标准化</h1><p id="dea0" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">数据集仅包含作为 PCA 变换结果的数字输入变量。V1、V2……v 28 是通过五氯苯甲醚获得的主要成分，唯一没有通过五氯苯甲醚转化的特征是“时间”和“数量”。因此，主成分分析受比例的影响，所以我们需要在应用主成分分析之前对数据中的特征进行比例缩放。对于缩放，我使用 Scikit-learn 的 StandardScaler()。为了适合定标器，数据应该在-1 和 1 之间整形。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="582b" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw"># Standardizing the features</em><br/>df['Vamount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))<br/>df['Vtime'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1,1))<br/><br/>df = df.drop(['Time','Amount'], axis = 1)<br/>df.head()</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi oa"><img src="../Images/90ff9b80e35759141c3482eb5ceb3649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2o9dlq10paFVnDHSEWGaDA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Fig 7 : Standardized dataset</figcaption></figure><p id="d5fe" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在，所有特征都被标准化为单位尺度(平均值= 0，方差= 1)</p><h1 id="9290" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">6.PCA 变换</h1><p id="aed0" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">PCA(主成分分析)主要用于减少特征空间的大小，同时保留尽可能多的信息。在这里，所有的特征使用 PCA 转换成 2 个特征。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="4a2f" class="lw ku iq mn b gy mr ms l mt mu">X = df.drop(['Class'], axis = 1)<br/>y = df['Class']<br/><br/>pca = PCA(n_components=2)<br/>principalComponents = pca.fit_transform(X.values)<br/>principalDf = pd.DataFrame(data = principalComponents<br/>             , columns = ['principal component 1', 'principal component 2'])</span><span id="1e94" class="lw ku iq mn b gy mv ms l mt mu">finalDf = pd.concat([principalDf, y], axis = 1)<br/>finalDf.head()</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/06f04aa840488f70d81e5513acf00464.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*tf1bsTjP6mUcEdS6vAlYbQ.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Fig 8 : Dimensional reduction</figcaption></figure><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="5994" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw"># 2D visualization</em><br/>fig = plt.figure(figsize = (8,8))<br/>ax = fig.add_subplot(1,1,1) <br/>ax.set_xlabel('Principal Component 1', fontsize = 15)<br/>ax.set_ylabel('Principal Component 2', fontsize = 15)<br/>ax.set_title('2 component PCA', fontsize = 20)<br/>targets = [0, 1]<br/>colors = ['r', 'g']<br/>for target, color <strong class="mn ir">in</strong> zip(targets,colors):<br/>    indicesToKeep = finalDf['Class'] == target<br/>    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']<br/>               , finalDf.loc[indicesToKeep, 'principal component 2']<br/>               , c = color<br/>               , s = 50)<br/>ax.legend(targets)<br/>ax.grid()</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/66ac414bcb1418e71db72ca0d885654d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*n942s6gkkJqrPWQX1P9t4A.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Fig 9 : Scatter plot of PCA transformation</figcaption></figure><p id="69c4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">由于数据非常不平衡，我只从非欺诈交易中提取了 492 行。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="e61c" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw"># Lets shuffle the data before creating the subsamples</em><br/>df = df.sample(frac=1)<br/><br/>frauds = df[df['Class'] == 1]<br/>non_frauds = df[df['Class'] == 0][:492]<br/><br/>new_df = pd.concat([non_frauds, frauds])<br/><em class="mw"># Shuffle dataframe rows</em><br/>new_df = new_df.sample(frac=1, random_state=42)<br/></span><span id="c122" class="lw ku iq mn b gy mv ms l mt mu"><em class="mw"># Let's plot the Transaction class against the Frequency</em><br/>labels = ['non frauds','fraud']<br/>classes = pd.value_counts(new_df['Class'], sort = True)<br/>classes.plot(kind = 'bar', rot=0)<br/>plt.title("Transaction class distribution")<br/>plt.xticks(range(2), labels)<br/>plt.xlabel("Class")<br/>plt.ylabel("Frequency")</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi od"><img src="../Images/815b78adf0e5fa6dcdfc741762fa6600.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*OUM9FO1vQFGJBO1tnpjCLQ.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Fig 10 : Distribution of classes</figcaption></figure><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="7a16" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw"># prepare the data</em><br/>features = new_df.drop(['Class'], axis = 1)<br/>labels = pd.DataFrame(new_df['Class'])<br/><br/>feature_array = features.values<br/>label_array = labels.values</span></pre><h1 id="ab6c" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">7.数据分割</h1><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="6002" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw"># splitting the faeture array and label array keeping 80% for the trainnig sets</em><br/>X_train,X_test,y_train,y_test = train_test_split(feature_array,label_array,test_size=0.20)<br/><br/><em class="mw"># normalize: Scale input vectors individually to unit norm (vector length).</em><br/>X_train = normalize(X_train)<br/>X_test=normalize(X_test)</span></pre><p id="f0f8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于模型建筑，我使用 K 个最近邻。因此，我们需要找到一个最佳的 K 来获得最佳效果。</p><h1 id="72f3" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">k-最近邻</h1><p id="22c0" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">来自<em class="mw">维基百科；</em></p><blockquote class="oe of og"><p id="98dc" class="ju jv mw jw b jx jy jz ka kb kc kd ke oh kg kh ki oi kk kl km oj ko kp kq kr ij bi translated">在模式识别中，<strong class="jw ir"><em class="iq">k</em>-最近邻算法</strong> ( <strong class="jw ir"> <em class="iq"> k </em> -NN </strong>)是一种用于分类和回归的非参数方法。在这两种情况下，输入由特征空间中的<strong class="jw ir"> <em class="iq"> k </em> </strong>个最接近的训练样本组成。输出取决于<em class="iq"> k </em> -NN 是用于分类还是回归。</p></blockquote><p id="5767" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">KNN 算法假设相似的事物存在于附近。换句话说，相似的事物彼此靠近。</p><blockquote class="ok"><p id="891e" class="ol om iq bd on oo op oq or os ot kr dk translated"><em class="ou">“物以类聚，人以群分。”</em></p></blockquote><p id="16a8" class="pw-post-body-paragraph ju jv iq jw b jx ov jz ka kb ow kd ke kf ox kh ki kj oy kl km kn oz kp kq kr ij bi translated"><em class="mw">训练算法</em>只存储数据，计算由<em class="mw">预测算法</em>完成。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="8a1e" class="lw ku iq mn b gy mr ms l mt mu">neighbours = np.arange(1,25)<br/>train_accuracy =np.empty(len(neighbours))<br/>test_accuracy = np.empty(len(neighbours))<br/><br/>for i,k <strong class="mn ir">in</strong> enumerate(neighbours):<br/>    <em class="mw">#Setup a knn classifier with k neighbors</em><br/>    knn=KNeighborsClassifier(n_neighbors=k,algorithm="kd_tree",n_jobs=-1)<br/>    <br/>    <em class="mw">#Fit the model</em><br/>    knn.fit(X_train,y_train.ravel())<br/>    <br/>    <em class="mw">#Compute accuracy on the training set</em><br/>    train_accuracy[i] = knn.score(X_train, y_train.ravel())<br/>    <br/>    <em class="mw">#Compute accuracy on the test set</em><br/>    test_accuracy[i] = knn.score(X_test, y_test.ravel())</span></pre><p id="7801" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们可以通过将测试集的准确性与训练集的准确性进行绘图来找到最佳 K 值。k 的最佳值是给出最大测试精度的点。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="411d" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw">#Generate plot</em><br/>plt.title('k-NN Varying number of neighbors')<br/>plt.plot(neighbours, test_accuracy, label='Testing Accuracy')<br/>plt.plot(neighbours, train_accuracy, label='Training accuracy')<br/>plt.legend()<br/>plt.xlabel('Number of neighbors')<br/>plt.ylabel('Accuracy')<br/>plt.show()</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/395c62be231cfaab55c23c505c9e8791.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*dUy6Vk80bbV0CzxMDeeuuQ.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Fig 11 : Number of Neighbors vs Accuracy</figcaption></figure><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="7340" class="lw ku iq mn b gy mr ms l mt mu">idx = np.where(test_accuracy == max(test_accuracy))<br/>x = neighbours[idx]</span></pre><p id="f20e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我使用 Scikit-learn<em class="mw">KNeighborsClassifier()</em>来构建模型<em class="mw">。</em></p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="52ae" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw">#k_nearest_neighbours_classification</em><br/>knn=KNeighborsClassifier(n_neighbors=x[0],algorithm="kd_tree",n_jobs=-1)<br/>knn.fit(X_train,y_train.ravel())</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pb"><img src="../Images/e4562b69532bfc1d67c35af7fe652376.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jX4yn20joVuFLXkgam56EA.png"/></div></div></figure><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="1514" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw"># save the model to disk</em><br/>filename = 'finalized_model.sav'<br/>joblib.dump(knn, filename)</span><span id="3cb0" class="lw ku iq mn b gy mv ms l mt mu"><em class="mw"># load the model from disk</em><br/>knn = joblib.load(filename)</span></pre><p id="b398" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在我们建立模型之后，我们可以使用 predict()来预测测试集的标签。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="6247" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw"># predicting labels for testing set</em><br/>knn_predicted_test_labels=knn.predict(X_test)</span></pre><h2 id="0e39" class="lw ku iq bd kv lx ly dn kz lz ma dp ld kf mb mc lh kj md me ll kn mf mg lp mh bi translated">模型评估</h2><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="92fd" class="lw ku iq mn b gy mr ms l mt mu">from pylab import rcParams<br/><em class="mw">#plt.figure(figsize=(12, 12))</em><br/>rcParams['figure.figsize'] = 14, 8<br/>plt.subplot(222)<br/>plt.scatter(X_test[:, 0], X_test[:, 1], c=knn_predicted_test_labels)<br/>plt.title(" Number of Blobs")</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/2508a26979a06b93e7390183b0557b7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*df5J0w3knnX820AKVzrCLw.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Fig 12 : Number of Blobs</figcaption></figure><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="538d" class="lw ku iq mn b gy mr ms l mt mu"><em class="mw">#scoring knn</em><br/>knn_accuracy_score  = accuracy_score(y_test,knn_predicted_test_labels)<br/>knn_precison_score  = precision_score(y_test,knn_predicted_test_labels)<br/>knn_recall_score    = recall_score(y_test,knn_predicted_test_labels)<br/>knn_f1_score        = f1_score(y_test,knn_predicted_test_labels)<br/>knn_MCC             =    matthews_corrcoef(y_test,knn_predicted_test_labels)</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pd"><img src="../Images/6eb927731658706707c4d9693ebf024e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xnrG3bjqGM4Kge_xY992cg.png"/></div></div></figure><h2 id="2038" class="lw ku iq bd kv lx ly dn kz lz ma dp ld kf mb mc lh kj md me ll kn mf mg lp mh bi translated"><strong class="ak">混淆矩阵</strong></h2><blockquote class="oe of og"><p id="2b6c" class="ju jv mw jw b jx jy jz ka kb kc kd ke oh kg kh ki oi kk kl km oj ko kp kq kr ij bi translated">混淆矩阵是一个表格，通常用于描述一个分类模型(或“分类器”)对一组真实值已知的测试数据的性能。Scikit-learn 提供了使用混淆矩阵方法计算混淆矩阵的工具。</p></blockquote><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="ccba" class="lw ku iq mn b gy mr ms l mt mu">import seaborn as sns<br/>LABELS = ['Normal', 'Fraud']<br/>conf_matrix = confusion_matrix(y_test, knn_predicted_test_labels)<br/>plt.figure(figsize=(12, 12))<br/>sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt="d");<br/>plt.title("Confusion matrix")<br/>plt.ylabel('True class')<br/>plt.xlabel('Predicted class')<br/>plt.show()</span></pre><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/8ced9e9894e4df872de6a8f05caf07cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*t0C4bC8IyAD0C39PocJWgg.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Fig 13 : Confusion Matrix</figcaption></figure><h1 id="7bfd" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">结论</h1><p id="4ef2" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">我试图不标准化数据，以获得更好的准确性。但是在我学会并应用这个方法后，它给出了一个有希望的结果。我还在做实验，还在学习数据预处理技术。对于这个数据集，我只使用了 KNN 算法。如果你觉得这个内核有帮助，请随意评论并投赞成票。</p><h1 id="4fea" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">参考</h1><p id="e93b" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">你可以在这里找到更多关于 PCA 转换的信息:<a class="ae ks" rel="noopener" target="_blank" href="/pca-using-python-scikit-learn-e653f8989e60">https://towardsdatascience . com/PCA-using-python-scikit-learn-e653 f 8989 e 60</a></p></div></div>    
</body>
</html>