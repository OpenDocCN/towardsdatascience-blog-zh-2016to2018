<html>
<head>
<title>K-Means Clustering — Introduction to Machine Learning Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-均值聚类—机器学习算法简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-clustering-introduction-to-machine-learning-algorithms-c96bf0d5d57a?source=collection_archive---------0-----------------------#2018-06-09">https://towardsdatascience.com/k-means-clustering-introduction-to-machine-learning-algorithms-c96bf0d5d57a?source=collection_archive---------0-----------------------#2018-06-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="24de" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">最简单的聚类算法—代码和解释</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ab71b7cbe4a8205c0d946f11fc94d2ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UTgPfbImdhoXSISPYgmWEg.jpeg"/></div></div></figure><h2 id="ab44" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">简介—无监督学习</h2><p id="5d20" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv la lw lx ly le lz ma mb li mc md me mf ij bi translated">在机器学习中，我们并不总是被提供一个目标来优化，我们也不总是被提供一个目标标签来对输入数据点进行分类。在人工智能领域中，没有目标或标签可供我们分类的问题被称为无监督学习问题。在无监督学习问题中，我们试图对数据中存在的潜在结构化信息进行建模。聚类是一种无监督学习问题，我们试图根据相似数据的底层结构将它们分组到群组/聚类中。K-means 算法是一种广泛使用的著名聚类算法。k 代表我们要将数据点分类到的聚类数。</p><h2 id="51a2" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">K-Means 伪代码</h2><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="8dcf" class="kr ks iq mh b gy ml mm l mn mo">## K-Means Clustering </span><span id="dd4e" class="kr ks iq mh b gy mp mm l mn mo">1. Choose the number of clusters(K) and obtain the data points <br/>2. Place the centroids c_1, c_2, ..... c_k randomly <br/>3. Repeat steps 4 and 5 until convergence or until the end of a fixed number of iterations<br/>4. for each data point x_i:<br/>       - find the nearest centroid(c_1, c_2 .. c_k) <br/>       - assign the point to that cluster <br/>5. for each cluster j = 1..k<br/>       - new centroid = mean of all points assigned to that cluster<br/>6. End </span></pre><p id="536c" class="pw-post-body-paragraph ln lo iq lp b lq mq jr ls lt mr ju lv la ms lx ly le mt ma mb li mu md me mf ij bi translated">下面的模拟将提供对 K-means 算法的更好理解。</p><div class="kg kh ki kj gt ab cb"><figure class="mv kk mw mx my mz na paragraph-image"><img src="../Images/d158a769c44d87684655344566260810.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*PQa0wLR3drFiJuoh.gif"/></figure><figure class="mv kk mw mx my mz na paragraph-image"><img src="../Images/1d05add7a2cf6113571e92c763e615f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*f9HcysjkU6XyM1hb.gif"/><figcaption class="nb nc gj gh gi nd ne bd b be z dk nf di ng nh">K — Means Algorithm</figcaption></figure></div><h2 id="743b" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">怎么选 K？？</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="7f7b" class="pw-post-body-paragraph ln lo iq lp b lq mq jr ls lt mr ju lv la ms lx ly le mt ma mb li mu md me mf ij bi translated">在某些情况下，我们不知道集群的数量。那么，我们如何选择 K 的值呢？？？有一种方法叫肘法。在这种方法中，选择不同数量的簇，并开始绘制簇内到质心的距离。图表如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/6ce33c4995046b35a5e7618b4fbebf9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dChOocbcsLLT1fcxTxj2Ng.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Elbow method</figcaption></figure><p id="5170" class="pw-post-body-paragraph ln lo iq lp b lq mq jr ls lt mr ju lv la ms lx ly le mt ma mb li mu md me mf ij bi translated">从上图中我们可以推断出，在 k=4 时，该图达到最佳最小值。即使类内距离在 4 之后减少，我们也会做更多的计算。这类似于收益递减定律。因此，我们选择值 4 作为聚类的最佳数量。之所以称之为肘关节法，是因为最佳聚类数代表一个肘关节！</p><h2 id="5626" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">K-均值聚类算法的应用</h2><ul class=""><li id="4368" class="nl nm iq lp b lq lr lt lu la nn le no li np mf nq nr ns nt bi translated">行为细分</li><li id="e9de" class="nl nm iq lp b lq nu lt nv la nw le nx li ny mf nq nr ns nt bi translated">异常检测</li><li id="9a4d" class="nl nm iq lp b lq nu lt nv la nw le nx li ny mf nq nr ns nt bi translated">社会网络分析</li><li id="fc68" class="nl nm iq lp b lq nu lt nv la nw le nx li ny mf nq nr ns nt bi translated">市场细分</li></ul><p id="c7e4" class="pw-post-body-paragraph ln lo iq lp b lq mq jr ls lt mr ju lv la ms lx ly le mt ma mb li mu md me mf ij bi translated">只有几个应用聚类算法(如 K-means)的例子。</p><h2 id="1c3e" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">让我们写一些代码</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz nj l"/></div></figure><p id="bad0" class="pw-post-body-paragraph ln lo iq lp b lq mq jr ls lt mr ju lv la ms lx ly le mt ma mb li mu md me mf ij bi translated">我们将使用<a class="ae oa" href="https://www.kaggle.com/jchen2186/machine-learning-with-iris-dataset/data" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a>来构建我们的算法。即使虹膜数据集有标签，我们也将丢弃它们，只使用特征点对数据进行聚类。我们知道有 3 个聚类(“鸢尾-海滨鸢尾”、“鸢尾-刚毛鸢尾”、“鸢尾-杂色”)。因此，在我们的例子中 k=3。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob nj l"/></div></figure><p id="5543" class="pw-post-body-paragraph ln lo iq lp b lq mq jr ls lt mr ju lv la ms lx ly le mt ma mb li mu md me mf ij bi translated">我们加载数据集并删除目标值。我们将特征点转换成一个 numpy 数组，并将其分成训练和测试数据。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob nj l"/></div></figure><p id="b781" class="pw-post-body-paragraph ln lo iq lp b lq mq jr ls lt mr ju lv la ms lx ly le mt ma mb li mu md me mf ij bi translated">我们实现了上面显示的伪代码，我们可以发现我们的算法在 6 次迭代后收敛。我们现在可以输入一个测试数据点，找到它最接近的质心，并将该点分配给相应的聚类。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob nj l"/></div></figure><p id="e974" class="pw-post-body-paragraph ln lo iq lp b lq mq jr ls lt mr ju lv la ms lx ly le mt ma mb li mu md me mf ij bi translated">Scikit-learn 库通过提供一个我们可以用来实现算法的抽象层次对象，再一次使我们免于编写这么多行代码。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob nj l"/></div></figure><h2 id="daac" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">结论</h2><p id="af64" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv la lw lx ly le lz ma mb li mc md me mf ij bi translated">K-means 是聚类技术的入门算法，也是最简单的算法。你可能已经注意到了，这里没有目标/损失函数。因此，不需要偏导数，并且消除了复杂的数学运算。K-means 是一种易于实现的简便算法。</p><h2 id="a2a0" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">参考</h2><div class="oc od gp gr oe of"><a href="https://www.datascience.com/blog/k-means-clustering" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd ir gy z fp ok fr fs ol fu fw ip bi translated">K-均值聚类简介</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">具体话题经验:新手职业经验:无行业经验机器知识…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">www.datascience.com</p></div></div><div class="oo l"><div class="op l oq or os oo ot kp of"/></div></div></a></div></div></div>    
</body>
</html>