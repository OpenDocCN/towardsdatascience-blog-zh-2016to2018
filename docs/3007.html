<html>
<head>
<title>Topic Modelling in Python with NLTK and Gensim</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用NLTK和Gensim在Python中进行主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21?source=collection_archive---------0-----------------------#2018-03-30">https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21?source=collection_archive---------0-----------------------#2018-03-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e4bb566a363e40b5e49f812187592220.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4LfElzVXP3uZgC5xa110zw.png"/></div></div></figure><p id="c0f0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇文章中，我们将学习如何识别文档中讨论的主题，称为主题建模。特别是，我们将讨论<a class="ae kw" href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="noopener ugc nofollow" target="_blank">潜在的狄利克雷分配</a> (LDA):一种广泛使用的主题建模技术。我们将应用LDA将一组研究论文转换成一组主题。</p><p id="27ce" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">研究论文主题建模是一种无监督的机器学习方法，它帮助我们发现论文中隐藏的语义结构，允许我们学习语料库中论文的主题表示。该模型可以应用于文档上的任何种类的标签，例如网站上帖子上的标签。</p><h1 id="1dcb" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">该过程</h1><ul class=""><li id="53a8" class="lv lw iq ka b kb lx kf ly kj lz kn ma kr mb kv mc md me mf bi translated">即使我们不确定主题是什么，我们也会提前选择主题的数量。</li><li id="8cf2" class="lv lw iq ka b kb mg kf mh kj mi kn mj kr mk kv mc md me mf bi translated">每个文档都表示为主题的分布。</li><li id="14fb" class="lv lw iq ka b kb mg kf mh kj mi kn mj kr mk kv mc md me mf bi translated">每个主题都表示为单词的分布。</li></ul><p id="d08d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">研究论文文本数据只是一堆未标记的文本，可以在<a class="ae kw" href="https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/dataset.csv" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="4f2d" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">文本清理</h1><p id="763f" class="pw-post-body-paragraph jy jz iq ka b kb lx kd ke kf ly kh ki kj ml kl km kn mm kp kq kr mn kt ku kv ij bi translated">我们使用下面的函数来清理我们的文本并返回一个令牌列表:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="91a6" class="mx ky iq mt b gy my mz l na nb">import spacy<br/>spacy.load('en')<br/>from spacy.lang.en import English<br/>parser = English()</span><span id="072c" class="mx ky iq mt b gy nc mz l na nb">def tokenize(text):<br/>    lda_tokens = []<br/>    tokens = parser(text)<br/>    for token in tokens:<br/>        if token.orth_.isspace():<br/>            continue<br/>        elif token.like_url:<br/>            lda_tokens.append('URL')<br/>        elif token.orth_.startswith('@'):<br/>            lda_tokens.append('SCREEN_NAME')<br/>        else:<br/>            lda_tokens.append(token.lower_)<br/>    return lda_tokens</span></pre><p id="fc36" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们使用<a class="ae kw" href="http://www.nltk.org/howto/wordnet.html" rel="noopener ugc nofollow" target="_blank"> NLTK的Wordnet </a>来寻找单词、同义词、反义词等的意思。另外，我们使用<a class="ae kw" href="http://www.nltk.org/_modules/nltk/stem/wordnet.html" rel="noopener ugc nofollow" target="_blank"> WordNetLemmatizer </a>来获取词根。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="149b" class="mx ky iq mt b gy my mz l na nb">import nltk<br/>nltk.download('wordnet')</span><span id="306d" class="mx ky iq mt b gy nc mz l na nb">from nltk.corpus import wordnet as wn<br/>def get_lemma(word):<br/>    lemma = wn.morphy(word)<br/>    if lemma is None:<br/>        return word<br/>    else:<br/>        return lemma<br/>    <br/>from nltk.stem.wordnet import WordNetLemmatizer<br/>def get_lemma2(word):<br/>    return WordNetLemmatizer().lemmatize(word)</span></pre><p id="15fb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">过滤掉停用词:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="e1b8" class="mx ky iq mt b gy my mz l na nb">nltk.download('stopwords')<br/>en_stop = set(nltk.corpus.stopwords.words('english'))</span></pre><p id="6ac8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们可以定义一个函数来为主题建模准备文本:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="27eb" class="mx ky iq mt b gy my mz l na nb">def prepare_text_for_lda(text):<br/>    tokens = tokenize(text)<br/>    tokens = [token for token in tokens if len(token) &gt; 4]<br/>    tokens = [token for token in tokens if token not in en_stop]<br/>    tokens = [get_lemma(token) for token in tokens]<br/>    return tokens</span></pre><p id="6b48" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">打开我们的数据，逐行阅读，每行，为LDA准备文本，然后添加到列表中。</p><p id="edd1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们可以看到我们的文本数据是如何转换的:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="6d3f" class="mx ky iq mt b gy my mz l na nb">import random<br/>text_data = []<br/>with open('dataset.csv') as f:<br/>    for line in f:<br/>        tokens = prepare_text_for_lda(line)<br/>        if random.random() &gt; .99:<br/>            print(tokens)<br/>            text_data.append(tokens)</span></pre><p id="5c18" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="nd">[' social crowd '，' social '，' network '，' base '，' framework '，' crowd '，' simulation'] <br/> ['detection '，' technique '，' clock '，' recovery '，' application'] <br/> ['voltage '，' syllabic '，' companding '，' domain '，' filter'] <br/> ['perceptual '，' base '，' coding '，' decision'] <br/> ['cognitive '，' mobile '，' virtual '，' network '，' operator '，' investment '，' pricing '，'f '，'神经元'] <br/> ['平铺'，'交错'，'多级'，'离散'，'小波'，'变换'] <br/> ['安全'，'交叉'，'层'，'协议'，'无线'，'传感器'，'网络'] <br/> ['客观性'，'工业'，'展示'] <br/> ['平衡'，'分组'，'丢弃'，'提高'，'性能'，'网络'] <br/> ['身体qos '，'自适应'，'无线电'，'不可知论者'，'传感器'，'网络'] <br/> ['设计'，'可靠性'，'方法论'] <br/> ['上下文'，'</em></p><h1 id="d639" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">带有Gensim的LDA</h1><p id="6c63" class="pw-post-body-paragraph jy jz iq ka b kb lx kd ke kf ly kh ki kj ml kl km kn mm kp kq kr mn kt ku kv ij bi translated">首先，我们从数据中创建一个字典，然后转换成<a class="ae kw" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">单词袋</a>语料库，并保存字典和语料库以备将来使用。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="2eb4" class="mx ky iq mt b gy my mz l na nb">from gensim import corpora<br/>dictionary = corpora.Dictionary(text_data)corpus = [dictionary.doc2bow(text) for text in text_data]</span><span id="9db8" class="mx ky iq mt b gy nc mz l na nb">import pickle<br/>pickle.dump(corpus, open('corpus.pkl', 'wb'))<br/>dictionary.save('dictionary.gensim')</span></pre><p id="afa6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们要求LDA在数据中找出5个主题:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="ecfa" class="mx ky iq mt b gy my mz l na nb">import gensim<br/>NUM_TOPICS = 5<br/>ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)<br/>ldamodel.save('model5.gensim')</span><span id="6033" class="mx ky iq mt b gy nc mz l na nb">topics = ldamodel.print_topics(num_words=4)<br/>for topic in topics:<br/>    print(topic)</span></pre><p id="46e4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="nd"> (0，' 0.034* "处理器"+ 0.019* "数据库"+ 0.019* "发布"+ 0.019* "概述" ')<br/> (1，' 0.051* "计算机"+ 0.028* "设计"+ 0.028* "图形"+ 0.028* "图库" ')<br/> (2，' 0.050* "管理"+ 0.027* "对象"+ 0.027</em></p><p id="ad2c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">主题0包括像“处理器”、“数据库”、“问题”和“概述”这样的词，听起来像是与数据库相关的主题。主题1包括像“计算机”、“设计”、“图形”和“画廊”这样的词，它肯定是一个平面设计相关的主题。主题2包括像“管理”、“对象”、“电路”和“高效”这样的词，听起来像是企业管理相关的主题。诸如此类。</p><p id="7fcc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用LDA，我们可以看到具有不同主题不同文档，且区别是明显的。</p><p id="1ab5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们尝试一个新文档:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="dbb0" class="mx ky iq mt b gy my mz l na nb">new_doc = 'Practical Bayesian Optimization of Machine Learning Algorithms'<br/>new_doc = prepare_text_for_lda(new_doc)<br/>new_doc_bow = dictionary.doc2bow(new_doc)<br/>print(new_doc_bow)<br/>print(ldamodel.get_document_topics(new_doc_bow))</span></pre><p id="fe91" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="nd"> [(38，1)，(117，1)] <br/> [(0，0.06669136)，(1，0.40170625)，(2，0.06670282)，(3，0.39819494)，(4，0.066704586)] </em></p><p id="a56f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我的新文档是关于机器学习算法的，LDA输出显示主题1具有最高的分配概率，主题3具有第二高的分配概率。我们同意了！</p><p id="d05a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">记住以上5个概率加起来是1。</p><p id="8973" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，我们要求LDA在数据中找出3个主题:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="3f15" class="mx ky iq mt b gy my mz l na nb">ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 3, id2word=dictionary, passes=15)<br/>ldamodel.save('model3.gensim')<br/>topics = ldamodel.print_topics(num_words=4)<br/>for topic in topics:<br/>    print(topic)</span></pre><p id="bb70" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="nd"> (0，' 0.029* "处理器"+ 0.016* "管理"+ 0.016* "辅助"+ 0.016* "算法" ')<br/> (1，' 0.026* "无线电"+ 0.026* "网络"+ 0.026* "认知"+ 0.026* "高效" ')<br/> (2，' 0.029* "电路"+ 0.029* "分发"+ 0.016*。</em></p><p id="9e82" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们还可以找到10个主题:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="eb8a" class="mx ky iq mt b gy my mz l na nb">ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=15)<br/>ldamodel.save('model10.gensim')<br/>topics = ldamodel.print_topics(num_words=4)<br/>for topic in topics:<br/>    print(topic)</span></pre><p id="7e08" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="nd"> (0，' 0.055* "数据库"+ 0.055* "系统"+ 0.029* "技术"+ 0.029* "递归" ')<br/> (1，' 0.038* "分发"+ 0.038* "图形"+ 0.038* "重新生成"+ 0.038* "精确" ')<br/> (2，' 0.055* "管理"+ 0.029* "多版本"+ 0.029</em></p><h1 id="5a63" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">皮尔戴维斯</h1><p id="ee19" class="pw-post-body-paragraph jy jz iq ka b kb lx kd ke kf ly kh ki kj ml kl km kn mm kp kq kr mn kt ku kv ij bi translated">pyLDAvis 旨在帮助用户解释符合文本数据语料库的主题模型中的主题。该软件包从拟合的LDA主题模型中提取信息，以通知基于web的交互式可视化。</p><p id="2a98" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">可视化5个主题:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="7d7f" class="mx ky iq mt b gy my mz l na nb">dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')<br/>corpus = pickle.load(open('corpus.pkl', 'rb'))<br/>lda = gensim.models.ldamodel.LdaModel.load('model5.gensim')</span><span id="3d71" class="mx ky iq mt b gy nc mz l na nb">import pyLDAvis.gensim<br/>lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)<br/>pyLDAvis.display(lda_display)</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/a5ad5c179a59be18606a7b4f019afaa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5UsDZltz8qxa-LRIiCKGGw.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">Figure 1</figcaption></figure><p id="8b0e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">显著性:衡量术语告诉你多少关于主题的信息。</p><p id="5b61" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">相关性:给定主题的单词和给定主题的单词的概率的加权平均值，由主题的概率归一化。</p><p id="1ebb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">气泡的大小衡量了主题相对于数据的重要性。</p><p id="2c42" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，我们得到了最显著的术语，意思是术语主要告诉我们与主题相关的事情。我们也可以看个别话题。</p><p id="e52f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">可视化3个主题:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="eff1" class="mx ky iq mt b gy my mz l na nb">lda3 = gensim.models.ldamodel.LdaModel.load('model3.gensim')<br/>lda_display3 = pyLDAvis.gensim.prepare(lda3, corpus, dictionary, sort_topics=False)<br/>pyLDAvis.display(lda_display3)</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/3f4fc5304e0fe38d7ecebab9033353dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RmeMaAWFoLkDu7Y5ba7uZA.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">Figure 2</figcaption></figure><p id="0d45" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">可视化10个主题:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="0c24" class="mx ky iq mt b gy my mz l na nb">lda10 = gensim.models.ldamodel.LdaModel.load('model10.gensim')<br/>lda_display10 = pyLDAvis.gensim.prepare(lda10, corpus, dictionary, sort_topics=False)<br/>pyLDAvis.display(lda_display10)</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/3724c45c088b8e8dc3243e2240bbc382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JpYUtXwUzXW7q11Hnt5aGQ.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">Figure 3</figcaption></figure><p id="4e51" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当我们有5或10个主题时，我们可以看到某些主题聚集在一起，这表明主题之间的相似性。这是一个多么好的方法来形象化我们到目前为止所做的事情！</p><p id="37dc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">试试看，找个文本数据集，有标签就去掉标签，自己建个主题模型！</p><p id="73c3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">源代码可以在<a class="ae kw" href="https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/topic_modeling_Gensim.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。我期待听到任何反馈或问题。</p></div></div>    
</body>
</html>