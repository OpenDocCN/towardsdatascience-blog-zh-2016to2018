<html>
<head>
<title>Sentiment analysis with a simple naive Bayes classifier in Go</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">围棋中基于简单朴素贝叶斯分类器的情感分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentiment-analysis-with-a-simple-naive-bayes-classifier-in-go-6c18a0134a1c?source=collection_archive---------12-----------------------#2018-05-29">https://towardsdatascience.com/sentiment-analysis-with-a-simple-naive-bayes-classifier-in-go-6c18a0134a1c?source=collection_archive---------12-----------------------#2018-05-29</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="2841" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">使用 Go 通过情感分析对文本进行分类</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/b7235a4b5c3983899c84a5c26b73a838.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*trUwY2s1tVPT7lx3.jpg"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">(credits: Chang Sau Sheong)</figcaption></figure><p id="c1c3" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我最近在读 Pedro Domingos 的<a class="ae lv" href="https://www.amazon.com/Master-Algorithm-Ultimate-Learning-Machine/dp/1501299387" rel="noopener ugc nofollow" target="_blank">大师算法</a>。这是一本引人入胜的读物，有一些有趣的想法。在书中，多明戈斯提出，机器学习算法可以归入 5 个部落之一——符号主义者、连接主义者、进化论者、贝叶斯主义者和类比主义者。每个部落都有自己的算法。符号主义者是逆向演绎(决策树)，连接主义者是反向传播(神经网络)，进化者是遗传编程(遗传算法)，贝叶斯主义者是贝叶斯定理，类比主义者是支持向量机(SVM)。</p><p id="075a" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">当我在读关于贝叶斯人的那一章时，我记得大约 10 年前我在我的旧博客中写过一篇关于朴素贝叶斯分类器的<a class="ae lv" href="https://blog.saush.com/2009/02/11/naive-bayesian-classifiers-and-ruby/" rel="noopener ugc nofollow" target="_blank">博文。所以我把它挖了出来(我的旧博客现在处于休眠状态，已经很多年没碰过了)，掸去灰尘，决定刷新和重温这个话题。</a></p><p id="d2fa" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我想做一些事情。首先，我将在 Go 中重写它。Ruby 代码工作得非常好，但是重温旧代码并看看如何改进它总是感觉很好。第二，我将把它用于一个非常流行的目的——情感分析(稍后我会谈到这一点)。最后，我将使用适当的训练和测试数据集对其进行训练和测试(我之前的博客文章中没有这一项)。</p><h1 id="f9e4" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">贝叶斯定理</h1><p id="f421" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">在我们开始学习贝叶斯定理之前，让我们回到基础知识，谈谈概率。概率是某件事情发生的可能性，在数学中我们将其表示为 0 到 1 之间的一个数字，其中 0 表示它永远不会发生，1 表示它永远会发生。</p><p id="3733" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated"><em class="mt">条件概率</em>是一种特殊的概率，它受某些条件或某些背景信息的影响。例如，你可能会也可能不会和你的朋友出去(一种可能性)，但这受天气影响——如果雨下得很大，你可能不想出去。所以你出去的概率是一个条件概率，基于天气。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj mu"><img src="../Images/9ac2db684aba8e5b863ddd72655897ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_V6El3Pb_sCNU_k3.jpg"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">Going out in the rain with your friends (credits: <a class="ae lv" href="https://commons.wikimedia.org/wiki/File:Singin%27_in_the_Rain_trailer.jpg" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/wiki/File:Singin%27_in_the_Rain_trailer.jpg</a>)</figcaption></figure><p id="2391" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">为了用数学来表示，我们假设不管发生什么，你出门的概率是<code class="fe mv mw mx my b">A</code>，坏天气的概率是<code class="fe mv mw mx my b">B</code>，你出门的条件概率，取决于天气是<code class="fe mv mw mx my b">p(A|B)</code>，或者大声读出是“给定 B，A 的概率”。</p><p id="2950" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">一个<em class="mt">联合概率</em>是两个事件都实现的概率。在我们上面的例子中，你在恶劣天气下外出的概率是<code class="fe mv mw mx my b">p(A and B)</code>。你可能已经学过(或者从中学数学中有一些模糊的回忆)如果两个概率彼此独立，那么<code class="fe mv mw mx my b">p(A and B) = p(A)p(B)</code>，即<code class="fe mv mw mx my b">A</code>的概率和<code class="fe mv mw mx my b">B</code>是<code class="fe mv mw mx my b">A</code>的独立概率和<code class="fe mv mw mx my b">B</code>的独立概率的倍数。</p><p id="87ec" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">然而我们刚刚了解到<code class="fe mv mw mx my b">A</code>实际上并不独立于<code class="fe mv mw mx my b">B</code>，所以<code class="fe mv mw mx my b">p(A)</code>实际上是<code class="fe mv mw mx my b">p(A|B)</code>的一个特例。如果下雨，会减少你外出的可能性。换句话说，更一般的数学描述是:</p><p id="d5e1" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated"><code class="fe mv mw mx my b">p(A and B) = p(A|B)p(B)</code></p><p id="7d08" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">由于<code class="fe mv mw mx my b">A</code>和<code class="fe mv mw mx my b">B</code>可以是一般化的任何事件，因此<code class="fe mv mw mx my b">A</code>和<code class="fe mv mw mx my b">B</code>的合取概率是可交换的:</p><p id="98f6" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated"><code class="fe mv mw mx my b">p(A and B) = p(B and A)</code></p><p id="a0ab" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">如果我们代入方程:</p><p id="20c9" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated"><code class="fe mv mw mx my b">p(A|B)p(B) = p(B|A)p(A)</code></p><p id="4d18" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">然后得到<code class="fe mv mw mx my b">p(A|B)</code>的条件概率:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj mz"><img src="../Images/2ba658a79928ddcdb5156f578888c666.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/0*nY4yO1CA0MPxdwWp.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">Bayes’ Theorem</figcaption></figure><p id="bcc6" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">这就是所谓的<a class="ae lv" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯定理</a>(或贝叶斯定律或贝叶斯法则)。</p><h1 id="2a41" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">情感分析</h1><p id="c3f5" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">现在让我们看看我们想要解决的问题，然后再回头看看贝叶斯定理是如何被用来解决它的。情感分析是一种根据说话者或作者所说或写下的内容来判断其心理状态的技术。它通常用于挖掘社交媒体(推文、评论、评论等)对品牌、产品或服务的情感，因为手动挖掘太困难、太昂贵或太慢。情感分析也被用于政治，在竞选活动中衡量公众对某些话题的看法。</p><p id="152d" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">这是一个相当复杂的问题，有许多不同类型的算法可以使用，其中一些可能非常复杂。在我们的例子中，我们希望分析来自亚马逊、IMDB 和 Yelp 的不同文本评论，并了解情绪是积极的还是消极的。换句话说，这是一个分类问题，我们将基于贝叶斯定理构建一个分类器。</p><h1 id="f038" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">基于贝叶斯定理的文档分类</h1><p id="6456" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">分类器就是对其他事物进行分类的东西。分类器是一个函数，它接收一组数据，并告诉我们数据属于哪个类别或分类。要对一个文本文档进行分类，我们要问——给定一个特定的文档，它属于这个类别的概率是多少？当我们找到给定文档在所有类别中的概率时，分类器挑选概率最高的类别，并宣布它为获胜者，也就是说，该文档最有可能属于该类别。</p><p id="2a1c" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">让我们将前面的数学公式转换成一个可用于文档分类的公式:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj na"><img src="../Images/ab2c8f4c0284b2411313f8d2d9ada6d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*evFZUdJXqoJBc4e9.png"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">Bayes Theorem for document classification</figcaption></figure><p id="0320" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">在上面的公式中，<code class="fe mv mw mx my b">p(category|document)</code>就是我们要找的东西——给定一个文档，它属于这个类别的概率是多少？</p><p id="d4c1" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">类似地，<code class="fe mv mw mx my b">p(document|category)</code>是该文档存在于该类别中的概率，<code class="fe mv mw mx my b">p(category)</code>是该类别与任何文档无关的概率，<code class="fe mv mw mx my b">p(document)</code>是该文档与任何类别无关的概率。</p><p id="808b" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我们真正需要的只是<code class="fe mv mw mx my b">p(document|category)</code>和<code class="fe mv mw mx my b">p(category)</code>。我们可以去掉<code class="fe mv mw mx my b">p(document)</code>，因为它对每个类别都是一样的。</p><p id="ac3f" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">那么我们如何找到<code class="fe mv mw mx my b">p(document|category)</code>？文档是由一串单词组成的，因此文档的概率是文档中所有单词的联合概率。给定一个类别的文档的概率是该文档中所有单词在一个类别中的联合概率。</p><p id="5008" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">一个词出现在一个类别中的概率很简单，那只是这个词在类别中出现的次数。连接部分非常棘手，因为单词不会随机出现在文档中，单词的顺序和外观取决于文档中的其他单词。那么我们如何解决这个问题呢？这就是<em class="mt">朴素贝叶斯分类器</em>的<em class="mt">朴素</em>部分的用武之地。我们简单地忽略单词的条件概率，并假设每个单词都是相互独立的。换句话说(双关语)，我们假设单词<em class="mt">随机出现在文档中。做出这样的假设似乎非常愚蠢，但是让我们看看它是如何实现的。</em></p><p id="1d1e" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">概率<code class="fe mv mw mx my b">p(category)</code>相对容易，它只是一个类别中文档的数量除以所有类别中文档的总数。</p><p id="14fa" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">这很简单。让我们来看看代码。</p><h1 id="7ce9" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">围棋中的朴素贝叶斯分类器</h1><h1 id="ba2a" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">创建分类器</h1><p id="421b" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">我们将开始在一个名为<code class="fe mv mw mx my b">classifier.go</code>的文件中创建一个通用的朴素贝叶斯文本分类器。</p><p id="1993" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">首先，我们创建了<code class="fe mv mw mx my b">Classifier</code>结构。</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="bbb9" class="nf lx iu my b gz ng nh l ni nj">// Classifier is what we use to classify documents<br/>type Classifier struct {<br/>	words               map[string]map[string]int<br/>	totalWords          int<br/>	categoriesDocuments map[string]int<br/>	totalDocuments      int<br/>	categoriesWords     map[string]int<br/>	threshold           float64<br/>}<br/><br/>// create and initialize the classifier<br/>func createClassifier(categories []string, threshold float64) (c Classifier) {<br/>	c = Classifier{<br/>		words:               make(map[string]map[string]int),<br/>		totalWords:          0,<br/>		categoriesDocuments: make(map[string]int),<br/>		totalDocuments:      0,<br/>		categoriesWords:     make(map[string]int),<br/>		threshold:           threshold,<br/>	}<br/><br/>	for _, category := range categories {<br/>		c.words[category] = make(map[string]int)<br/>		c.categoriesDocuments[category] = 0<br/>		c.categoriesWords[category] = 0<br/>	}<br/>	return<br/>}</span></pre><p id="1dd6" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">在该结构中，<code class="fe mv mw mx my b">words</code>是表示已经由分类器训练的单词的映射图。大概是这样的(不完全是):</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="8302" class="nf lx iu my b gz ng nh l ni nj">{<br/>    "1": {<br/>        "good": 10,<br/>        "wonderful": 5,<br/>        "amazing": 7,<br/>    },<br/>    "0": {<br/>        "awful": 6,<br/>        "loud": 4,<br/>    }<br/>}</span></pre><p id="7eaa" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated"><code class="fe mv mw mx my b">totalWords</code>字段是分类器中单词的总数，而<code class="fe mv mw mx my b">totalDocuments</code>是分类器中文档的总数。</p><p id="9dec" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated"><code class="fe mv mw mx my b">categoriesDocuments</code>字段是给出每个类别中文档数量的图:</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="263a" class="nf lx iu my b gz ng nh l ni nj">{<br/>    "1": 13,<br/>    "0": 16,<br/>}</span></pre><p id="73d6" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated"><code class="fe mv mw mx my b">categoriesWords</code>字段是给出每个类别中单词数量的图:</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="919f" class="nf lx iu my b gz ng nh l ni nj">{<br/>    "1": 35,<br/>    "0": 44,<br/>}</span></pre><p id="af9e" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">稍后我会描述<code class="fe mv mw mx my b">threshold</code>。</p><h1 id="22a0" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">计数单词</h1><p id="b1b6" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">分类器的核心实际上是对单词进行计数，所以接下来让我们来看看。我们有一个函数<code class="fe mv mw mx my b">countWords</code>可以做到这一点，它传入一个文档，并返回每个单词出现次数的地图。</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="765e" class="nf lx iu my b gz ng nh l ni nj">var cleaner = regexp.MustCompile(`[^\w\s]`)<br/>// truncated list<br/>var stopWords = map[string]bool{"a": true, "able": true, "about": true, ..., "you've": true, "z": true, "zero": true}<br/><br/>// clean up and split words in document, then stem each word and count the occurrence<br/>func countWords(document string) (wordCount map[string]int) {<br/>	cleaned := cleaner.ReplaceAllString(document, "")<br/>	words := strings.Split(cleaned, " ")<br/>	wordCount = make(map[string]int)<br/>	for _, word := range words {<br/>		if !stopWords[word] {<br/>			key := stem(strings.ToLower(word))<br/>			wordCount[key]++<br/>		}<br/>	}<br/>	return<br/>}<br/><br/>// stem a word using the Snowball algorithm<br/>func stem(word string) string {<br/>	stemmed, err := snowball.Stem(word, "english", true)<br/>	if err == nil {<br/>		return stemmed<br/>	}<br/>	fmt.Println("Cannot stem word:", word)<br/>	return word<br/>}</span></pre><p id="404b" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">首先，我们使用正则表达式清理文档，删除所有不是单词的内容(包括标点符号等)。然后我们将文档拆分成单词。</p><p id="6e6f" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我们不需要文档中的所有单词，我们只需要关键词，所以我们删除了文档中任何常见的单词，例如，我们将忽略冠词，如<em class="mt"> a </em>、<em class="mt"> the </em>，代词，如<em class="mt"> he </em>、<em class="mt"> she </em>等等。所以我们用一个停用词列表，过滤掉那些常用词。其余的将被放入小写，使关键一致。</p><p id="24d3" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">很多词都有不同的变体，比如名词可以是复数(<em class="mt">猫</em>和<em class="mt">猫</em>要一起算)，动词可以有时态(<em class="mt">吃</em>，<em class="mt">吃</em>和<em class="mt">吃</em>要一起算)等等。为了不重复计算单词变化，我们使用一种叫做<a class="ae lv" href="https://en.wikipedia.org/wiki/Stemming" rel="noopener ugc nofollow" target="_blank">词干</a>的技术。在我们的分类器中，我使用了一个词干分析器库，基于<a class="ae lv" href="http://snowballstem.org/" rel="noopener ugc nofollow" target="_blank">雪球算法</a>的<a class="ae lv" href="https://github.com/kljensen/snowball" rel="noopener ugc nofollow" target="_blank">雪球</a>。</p><p id="1502" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">最后，将单词计数相加并返回。</p><h1 id="7de7" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">训练分类器</h1><p id="641b" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">训练分类器实际上就是对训练数据集文档中的单词进行计数，繁重的工作由<code class="fe mv mw mx my b">countWords</code>函数完成。分类器中的<code class="fe mv mw mx my b">Train</code>方法简单地使用了<code class="fe mv mw mx my b">countWords</code>函数，并根据类别分配计数。</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="5cf3" class="nf lx iu my b gz ng nh l ni nj">// Train the classifier<br/>func (c *Classifier) Train(category string, document string) {<br/>	for word, count := range countWords(document) {<br/>		c.words[category][word] += count<br/>		c.categoriesWords[category] += count<br/>		c.totalWords += count<br/>	}<br/>	c.categoriesDocuments[category]++<br/>	c.totalDocuments++<br/>}</span></pre><h1 id="e22c" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">文档分类</h1><p id="7095" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">这是实际行动开始的地方。在进入<code class="fe mv mw mx my b">Classify</code>方法之前，让我们再看一下等式:</p><p id="0c56" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated"><code class="fe mv mw mx my b">p(category|document) = p(document|category)p(category)</code></p><p id="2b78" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我们将创建一个方法来计算每个概率。先说<code class="fe mv mw mx my b">p(category)</code>。</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="5ffd" class="nf lx iu my b gz ng nh l ni nj">// p (category)<br/>func (c *Classifier) pCategory(category string) float64 {<br/>	return float64(c.categoriesDocuments[category]) / float64(c.totalDocuments)<br/>}</span></pre><p id="e393" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">这是不言自明的——我们将类别中的文档数除以文档总数，得到该类别的概率。</p><p id="0f16" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">接下来我们来看<code class="fe mv mw mx my b">p(document|category)</code>。</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="0919" class="nf lx iu my b gz ng nh l ni nj">// p (document | category)<br/>func (c *Classifier) pDocumentCategory(category string, document string) (p float64) {<br/>	p = 1.0<br/>	for word := range countWords(document) {<br/>		p = p * c.pWordCategory(category, word)<br/>	}<br/>	return p<br/>}<br/><br/>func (c *Classifier) pWordCategory(category string, word string) float64 {<br/>	return float64(c.words[category][stem(word)]+1) / float64(c.categoriesWords[category])<br/>}</span></pre><p id="e181" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">首先，我们使用<code class="fe mv mw mx my b">countWords</code>给出文档中的字数。这里我们实际上并不关心字数，我们只是想要文档中的关键词列表。对于每一个关键词，我们在类别中找到它的概率。这就是该关键字在类别中出现的次数除以类别中的总字数。例如，在训练分类器之后，假设对于类别<code class="fe mv mw mx my b">1</code>(它是肯定的)，我们有 10 个单词“good”。而我们在类别<code class="fe mv mw mx my b">1</code>中总共有 100 个单词。这意味着该词在类别中的概率是<code class="fe mv mw mx my b">0.1</code>。</p><p id="36c6" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我们对文档中的每个关键词都这样做，然后将所有这些概率相乘，这将是<code class="fe mv mw mx my b">p(document|category)</code>。</p><p id="1e0f" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">最后，我们找到了<code class="fe mv mw mx my b">p(category|document)</code>，这是相当琐碎的。</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="97ab" class="nf lx iu my b gz ng nh l ni nj">// p (category | document)<br/>func (c *Classifier) pCategoryDocument(category string, document string) float64 {<br/>	return c.pDocumentCategory(category, document) * c.pCategory(category)<br/>}</span></pre><p id="c806" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">现在我们有了每个类别的条件概率，我们把它们放在一个单一的地图中。</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="c59e" class="nf lx iu my b gz ng nh l ni nj">// Probabilities of each category<br/>func (c *Classifier) Probabilities(document string) (p map[string]float64) {<br/>	p = make(map[string]float64)<br/>	for category := range c.words {<br/>		p[category] = c.pCategoryDocument(category, document)<br/>	}<br/>	return<br/>}</span></pre><p id="ef4c" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">这将由我们的<code class="fe mv mw mx my b">Classify</code>方法使用。</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="f5cc" class="nf lx iu my b gz ng nh l ni nj">// Classify a document<br/>func (c *Classifier) Classify(document string) (category string) {<br/>	// get all the probabilities of each category<br/>	prob := c.Probabilities(document)<br/><br/>	// sort the categories according to probabilities<br/>	var sp []sorted<br/>	for c, p := range prob {<br/>		sp = append(sp, sorted{c, p})<br/>	}<br/>	sort.Slice(sp, func(i, j int) bool {<br/>		return sp[i].probability &gt; sp[j].probability<br/>	})<br/><br/>	// if the highest probability is above threshold select that<br/>	if sp[0].probability/sp[1].probability &gt; c.threshold {<br/>		category = sp[0].category<br/>	} else {<br/>		category = "unknown"<br/>	}<br/><br/>	return<br/>}</span></pre><p id="9ac7" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我们的<code class="fe mv mw mx my b">Classify</code>方法根据概率对类别进行排序，并找出最重要的类别。但这还不是结束。顶级和二级之间的差别可能非常小。例如，让我们以将电子邮件分类为垃圾邮件和非垃圾邮件为例，假设概率是这样的-垃圾邮件是 51%，非垃圾邮件是 49%。该文档是否应归类为垃圾邮件？这取决于你希望分类器有多严格。</p><p id="6e51" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">这就是<code class="fe mv mw mx my b">threshold</code>字段的原因，它是用于分离类别的阈值比率。例如，如果<code class="fe mv mw mx my b">threshold</code>是<code class="fe mv mw mx my b">1.5</code>，这意味着具有最高概率的类别需要比第二高的概率高 1.5 倍。如果顶级类别没有达到阈值，我们将把它归类为<code class="fe mv mw mx my b">unknown</code>。</p><p id="1059" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我们已经完成了分类器，接下来让我们看看如何使用它。</p><h1 id="d2d3" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">使用朴素贝叶斯分类器的情感分析</h1><p id="7383" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">在这篇博文中，我使用了 Dimitrios Kotzias 等人为论文“使用深度特征从群体到个体标签”创建的<a class="ae lv" href="https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences" rel="noopener ugc nofollow" target="_blank">情感标签句子数据集</a>。艾尔。KDD 2015。该数据集包含来自亚马逊、IMDB 和 Yelp 网站的 1000 条评论，标签为正面的<code class="fe mv mw mx my b">1</code>或负面的<code class="fe mv mw mx my b">0</code>。这些评论分别摘自产品、电影和餐馆的评论。</p><p id="da28" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">首先，让我们看看数据是如何设置的。</p><h1 id="0ee4" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">设置数据</h1><p id="10e0" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">我将每个数据集分为训练数据集和测试数据集，并使用训练数据集来训练分类器，使用测试数据集来验证分类器。这是通过<code class="fe mv mw mx my b">setupData</code>功能完成的。</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="16f4" class="nf lx iu my b gz ng nh l ni nj">// datasets<br/>type document struct {<br/>	sentiment string<br/>	text      string<br/>}<br/><br/>var train []document<br/>var test []document<br/><br/>// set up data for training and testing<br/>func setupData(file string) {<br/>	rand.Seed(time.Now().UTC().UnixNano())<br/>	data, err := readLines(file)<br/>	if err != nil {<br/>		fmt.Println("Cannot read file", err)<br/>		os.Exit(1)<br/>	}<br/>	for _, line := range data {<br/>		s := strings.Split(line, "|")<br/>		doc, sentiment := s[0], s[1]<br/><br/>		if rand.Float64() &gt; testPercentage {<br/>			train = append(train, document{sentiment, doc})<br/>		} else {<br/>			test = append(test, document{sentiment, doc})<br/>		}<br/>	}<br/>}<br/><br/>// read the file line by line<br/>func readLines(path string) ([]string, error) {<br/>	file, err := os.Open(path)<br/>	if err != nil {<br/>		return nil, err<br/>	}<br/>	defer file.Close()<br/><br/>	var lines []string<br/>	scanner := bufio.NewScanner(file)<br/>	for scanner.Scan() {<br/>		lines = append(lines, scanner.Text())<br/>	}<br/>	return lines, scanner.Err()<br/>}</span></pre><p id="dba2" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我使用变量<code class="fe mv mw mx my b">testPercentage</code>来获取整个数据集作为测试数据集的百分比。这又被用来随机选择测试数据集中的一些记录。</p><h1 id="16d2" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">创建文档分类器</h1><p id="84a0" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">一旦我们建立了数据集，我们开始用我们的参数创建分类器。</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="850b" class="nf lx iu my b gz ng nh l ni nj">// parameters<br/>var (<br/>	testPercentage = 0.1<br/>	datafile       = "amazon.txt"<br/>	threshold      = 1.1<br/>)<br/><br/>var categories = []string{"1", "0"}<br/><br/>func main() {<br/>	setupData(datafile)<br/>	fmt.Println("Data file used:", datafile)<br/>	fmt.Println("no of docs in TRAIN dataset:", len(train))<br/>	fmt.Println("no of docs in TEST dataset:", len(test))<br/><br/>	c := createClassifier(categories, threshold)<br/><br/>...</span></pre><h1 id="2de5" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">用训练数据集训练分类器</h1><p id="9082" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">给定分类器，我们开始使用训练数据集来训练它。</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="f553" class="nf lx iu my b gz ng nh l ni nj">...<br/>	// train on train dataset<br/>	for _, doc := range train {<br/>		c.Train(doc.sentiment, doc.text)<br/>    }<br/>...</span></pre><h1 id="1b4b" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">测试数据集上的测试分类器</h1><p id="9b04" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">在训练分类器之后，我们使用它在测试数据集上进行测试。</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="e08e" class="nf lx iu my b gz ng nh l ni nj">...<br/>	// validate on test dataset<br/>	count, accurates, unknowns := 0, 0, 0<br/>	for _, doc := range test {<br/>		count++<br/>		sentiment := c.Classify(doc.text)<br/>		if sentiment == doc.sentiment {<br/>			accurates++<br/>		}<br/>		if sentiment == "unknown" {<br/>			unknowns++<br/>		}<br/>	}<br/>	fmt.Printf("Accuracy on TEST dataset is %2.1f%% with %2.1f%% unknowns", float64(accurates)*100/float64(count), float64(unknowns)*100/float64(count))<br/>...</span></pre><p id="85e3" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我们计算精确分类的数量，以及未知分类的数量。</p><h1 id="85de" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">在训练数据集上测试分类器</h1><p id="b31c" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">我们还对一些训练数据集进行了测试，以进行健全性检查。</p><pre class="kk kl km kn gu nb my nc nd aw ne bi"><span id="261b" class="nf lx iu my b gz ng nh l ni nj">..<br/>	// validate on the first 100 docs in the train dataset<br/>	count, accurates, unknowns = 0, 0, 0<br/>	for _, doc := range train[0:100] {<br/>		count++<br/>		sentiment := c.Classify(doc.text)<br/>		if sentiment == doc.sentiment {<br/>			accurates++<br/>		}<br/>		if sentiment == "unknown" {<br/>			unknowns++<br/>		}<br/>	}<br/>	fmt.Printf("\nAccuracy on TRAIN dataset is %2.1f%% with %2.1f%% unknowns", float64(accurates)*100/float64(count), float64(unknowns)*100/float64(count))<br/>...</span></pre><p id="135a" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我们来看看结果。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nk"><img src="../Images/249e2de94c50d48f64478da77c7d056c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PcwItJCRUY6V4YFY.png"/></div></div></figure><p id="d2dd" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">成绩还不算太差！你可能会意识到，如果你运行多次，你会得到不同的结果，这是因为在培训中使用的顺序和文件实际上很重要。您也可以尝试调整不同的参数，以查看分类器的行为。</p><h1 id="a32c" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">结论</h1><p id="03f7" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">你可能会很惊讶(如果你还不知道的话)这样一个简单的算法实际上是如何表现得相对较好的。然而，这种特殊的做法有一个警告。首先，对使用的数据集进行清洗，选择明确的阳性或阴性。在现实世界中，情感分析充满了许多其他问题，真的不是一个简单的问题。然而，这一点只是为了表明贝叶斯定理可以成为你的工具箱中一个非常强大的工具。</p><h1 id="21fe" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">源代码</h1><p id="2cd3" class="pw-post-body-paragraph kz la iu lb b lc mo jv le lf mp jy lh li mq lk ll lm mr lo lp lq ms ls lt lu in bi translated">你可以在 https://github.com/sausheong/gonb 的 Github 中找到这段代码</p><h1 id="19ab" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">参考</h1><ol class=""><li id="03e3" class="nl nm iu lb b lc mo lf mp li nn lm no lq np lu nq nr ns nt bi translated">我最初的博文大量引用了托比·塞格兰的《T2 编程集体智慧》一书</li><li id="ee0c" class="nl nm iu lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">佩德罗·多明戈斯引人入胜的<a class="ae lv" href="https://www.amazon.com/Master-Algorithm-Ultimate-Learning-Machine/dp/1501299387" rel="noopener ugc nofollow" target="_blank">大师算法</a>一书引发了这篇文章，值得再次提及</li><li id="a08a" class="nl nm iu lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">艾伦·唐尼的书<a class="ae lv" href="http://greenteapress.com/wp/think-bayes/" rel="noopener ugc nofollow" target="_blank">认为贝叶斯</a>很好地描述了贝叶斯定律，我在这篇博客文章中描述它时从书中得到了一些启示</li><li id="58d2" class="nl nm iu lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">我使用了 Kotzias 等人的论文“使用深度特征从组到个体标签”中的数据集。艾尔。KDD 2015</li></ol></div></div>    
</body>
</html>