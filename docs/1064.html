<html>
<head>
<title>AutoEncoders are Essential in Deep Neural Nets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动编码器在深度神经网络中至关重要</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/autoencoders-are-essential-in-deep-neural-nets-f0365b2d1d7c?source=collection_archive---------0-----------------------#2017-07-26">https://towardsdatascience.com/autoencoders-are-essential-in-deep-neural-nets-f0365b2d1d7c?source=collection_archive---------0-----------------------#2017-07-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/eb01dee4cf5f2ac86d78e624a774bde9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*lOZVC-aKzITERzC1HgNEMA.png"/></div></figure><p id="5edb" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi ks translated"><span class="l kt ku kv bm kw kx ky kz la di">一个</span> <strong class="jw ir">自动编码器</strong>是一个神经网络(NN)，也是一个<strong class="jw ir">无监督学习</strong>(特征学习)算法。</p><ul class=""><li id="81b7" class="lb lc iq jw b jx jy kb kc kf ld kj le kn lf kr lg lh li lj bi translated">通过将目标值设置为与输入相同，应用<strong class="jw ir">反向传播</strong>。</li><li id="7985" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">它试图从<strong class="jw ir"> <em class="lp"> x </em> </strong>预测<strong class="jw ir"> <em class="lp"> x </em> </strong>，而不需要<strong class="jw ir">标签。</strong></li></ul><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/6ce3cc5ea76d0b4eb5a4d55ffa65b9ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*RD7rWiS_9gYDN1RFoa7ShQ.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Output = f . f ‘ . Input</figcaption></figure><ul class=""><li id="c557" class="lb lc iq jw b jx jy kb kc kf ld kj le kn lf kr lg lh li lj bi translated">它试图学习一个“<strong class="jw ir">恒等式</strong>”函数的近似值。</li><li id="ec0a" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">但是我们在网络上放置<strong class="jw ir">约束</strong>就像<strong class="jw ir">在隐藏层中少了</strong> <em class="lp">个单元。</em></li><li id="723e" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">它代表来自<strong class="jw ir">压缩、噪声或损坏数据的原始输入。</strong></li><li id="9d39" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">它由<strong class="jw ir">编码器&amp;解码器之间的窄隐层</strong>组成。</li></ul><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lz"><img src="../Images/da7d1fbe84b58a5775c77ad6e4c86313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nmnm2rRJjaBD1TCUgL4dmA.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Network level representation</figcaption></figure><ul class=""><li id="d59c" class="lb lc iq jw b jx jy kb kc kf ld kj le kn lf kr lg lh li lj bi translated"><strong class="jw ir">中间层</strong> ( <strong class="jw ir">中间层</strong> [ <em class="lp">瓶颈</em>或<strong class="jw ir">潜在空间表示</strong>)是输入数据的<strong class="jw ir">压缩表示</strong>。</li><li id="cf2f" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">自动编码器与工程压缩的区别在于，</li></ul><p id="4a6c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">— —在自动编码器压缩和解压缩功能中<strong class="jw ir">从数据</strong>本身学习，而不像<strong class="jw ir"> jpg </strong>和<strong class="jw ir"> zip </strong>。</p><ul class=""><li id="bbf4" class="lb lc iq jw b jx jy kb kc kf ld kj le kn lf kr lg lh li lj bi translated"><strong class="jw ir">编码器</strong>和<strong class="jw ir">解码器</strong>为<strong class="jw ir">前馈NN。</strong></li><li id="b7d6" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">通过<strong class="jw ir">最小化</strong>输入和输出之间的<em class="lp">差异来训练网络。</em></li><li id="6c27" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">如果输入是<strong class="jw ir">完全随机</strong>(从<strong class="jw ir"> IID </strong> &amp;中选择<strong class="jw ir">独立于其他特征</strong>，那么<em class="lp">压缩任务</em>将会是<strong class="jw ir">困难</strong>。</li><li id="ff22" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">如果输入特征与<strong class="jw ir">相关</strong>，那么网络将<strong class="jw ir">发现那些</strong>。</li><li id="6b63" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">它的学习风格像PCA，所以和PCA(主成分分析)关系密切。</li><li id="17f8" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">我们可以从<strong class="jw ir">本征矢表象</strong>的<em class="lp">相似度</em>中观察到。</li><li id="7367" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated"><strong class="jw ir"> PCA </strong> &amp; <strong class="jw ir">自动编码器的区别</strong>类似</li></ul><blockquote class="me mf mg"><p id="85bf" class="ju jv lp jw b jx jy jz ka kb kc kd ke mh kg kh ki mi kk kl km mj ko kp kq kr ij bi translated">—自动编码器比PCA灵活得多。</p><p id="d00a" class="ju jv lp jw b jx jy jz ka kb kc kd ke mh kg kh ki mi kk kl km mj ko kp kq kr ij bi translated">— NN激活函数在编码中引入了“<strong class="jw ir">非线性</strong>”，但PCA <strong class="jw ir">仅引入了</strong>线性变换。</p><p id="7971" class="ju jv lp jw b jx jy jz ka kb kc kd ke mh kg kh ki mi kk kl km mj ko kp kq kr ij bi translated">—我们可以堆叠自动编码器，形成一个<strong class="jw ir">深度自动编码器网络。</strong></p></blockquote><ul class=""><li id="c60d" class="lb lc iq jw b jx jy kb kc kf ld kj le kn lf kr lg lh li lj bi translated">自动编码器的重要性，</li></ul><blockquote class="me mf mg"><p id="4502" class="ju jv lp jw b jx jy jz ka kb kc kd ke mh kg kh ki mi kk kl km mj ko kp kq kr ij bi translated">—它找到输入数据的低维表示<strong class="jw ir">。</strong></p><p id="df53" class="ju jv lp jw b jx jy jz ka kb kc kd ke mh kg kh ki mi kk kl km mj ko kp kq kr ij bi translated">—在我们的模型中，一些输入特征可能是<strong class="jw ir">冗余</strong> / <strong class="jw ir">相关</strong> →浪费处理时间&amp; " <strong class="jw ir">过拟合</strong>(参数太多)。</p></blockquote><ul class=""><li id="b303" class="lb lc iq jw b jx jy kb kc kf ld kj le kn lf kr lg lh li lj bi translated">如果<strong class="jw ir">隐藏单元</strong>的数量<strong class="jw ir">大</strong>，我们可以对隐藏层施加<strong class="jw ir">稀疏度</strong>约束。网络仍然能够<strong class="jw ir">发现</strong>特征。</li><li id="d6f9" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">网络的<strong class="jw ir">神经元</strong>可以是<strong class="jw ir">激活</strong>(作为<strong class="jw ir">触发</strong>)或者神经元可以是<strong class="jw ir">非激活</strong>。</li><li id="a57f" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated"><strong class="jw ir">稀疏度参数</strong>通常是<em class="lp">接近</em>到<strong class="jw ir">零</strong> = 0.05的小值。</li><li id="7797" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">网络将<strong class="jw ir">额外惩罚项</strong>引入我们的<strong class="jw ir">优化</strong>目标。</li><li id="871b" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">在正则化中使用KL(Kullback-Leibler)散度，并且考虑<strong class="jw ir">伯努利随机变量</strong>与来自<strong class="jw ir"> 2 </strong>分布的<strong class="jw ir">均值</strong>。</li><li id="a307" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">训练应该在“<strong class="jw ir">白化的</strong>”<strong class="jw ir">自然图像上进行。</strong></li><li id="ac62" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated"><strong class="jw ir">白化</strong>是一个<strong class="jw ir">预处理步骤</strong>，去除输入中的<em class="lp">冗余。</em></li><li id="b54e" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">NN的训练策略<strong class="jw ir">在这里也</strong>管用，</li></ul><blockquote class="me mf mg"><p id="b8c8" class="ju jv lp jw b jx jy jz ka kb kc kd ke mh kg kh ki mi kk kl km mj ko kp kq kr ij bi translated">1.反向传播</p><p id="2bea" class="ju jv lp jw b jx jy jz ka kb kc kd ke mh kg kh ki mi kk kl km mj ko kp kq kr ij bi translated">2.规范化</p><p id="8158" class="ju jv lp jw b jx jy jz ka kb kc kd ke mh kg kh ki mi kk kl km mj ko kp kq kr ij bi translated">3.拒绝传统社会的人</p><p id="f098" class="ju jv lp jw b jx jy jz ka kb kc kd ke mh kg kh ki mi kk kl km mj ko kp kq kr ij bi translated">4.RBM预培训</p></blockquote><h1 id="0d76" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated">其他类型的自动编码器</h1><h2 id="0388" class="ni ml iq bd mm nj nk dn mq nl nm dp mu kf nn no my kj np nq nc kn nr ns ng nt bi translated">1)卷积自动编码器(CAE)</h2><ul class=""><li id="e802" class="lb lc iq jw b jx nu kb nv kf nw kj nx kn ny kr lg lh li lj bi translated">这用“<strong class="jw ir">卷积层</strong>”替换了“<strong class="jw ir">全连接层</strong>”。</li><li id="29bd" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated"><strong class="jw ir">宽薄隐层</strong> — — —转换成—&gt;—<strong class="jw ir">窄厚隐层。</strong></li><li id="fa73" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">这有助于<em class="lp">提取视觉特征。</em></li><li id="13e0" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">重建质量:-</li></ul><blockquote class="nz"><p id="9581" class="oa ob iq bd oc od oe of og oh oi kr dk translated">-经常模糊不清</p><p id="0b22" class="oa ob iq bd oc od oj ok ol om on kr dk translated">-质量较低</p><p id="0b31" class="oa ob iq bd oc od oj ok ol om on kr dk translated">-丢失了一些信息</p></blockquote><ul class=""><li id="50a3" class="lb lc iq jw b jx oo kb op kf oq kj or kn os kr lg lh li lj bi translated">CAE的使用:-</li></ul><p id="5b5f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi ks translated"><span class="l kt ku kv bm kw kx ky kz la di"> 1。</span> <strong class="jw ir">超基础图像重建</strong></p><p id="a476" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">—学习<strong class="jw ir">从图片中去除噪声</strong>/<strong class="jw ir">重建</strong>缺失的部分。</p><p id="d8df" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">—输入(噪声版本)；所以输出(干净版)。</p><p id="34a2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">—网络会填充图像中的间隙。</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ot"><img src="../Images/387c8b4e3580fdbec5cb8043449df066.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K7ouHU0e8cAl7P8MN4wslQ.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Removes the dark cross-bar in the image</figcaption></figure><p id="4cf7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi ks translated"><span class="l kt ku kv bm kw kx ky kz la di"> 2。</span> <strong class="jw ir">超基础图像彩色化</strong></p><p id="cc32" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">— CAE将一幅图像中的<strong class="jw ir">圆</strong>和<strong class="jw ir">正方形</strong>映射到同一幅图像，但分别使用红色和蓝色(着色)。</p><p id="ef4a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">—紫色有时是因为颜色的<strong class="jw ir">混合</strong>而形成的，网络在圆形或方形之间徘徊。</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ou"><img src="../Images/dd99d6724f2337b997d33776d7cb25f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J4A4HuCltZC9UrDZbE41Ng.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Coloring</figcaption></figure><p id="b86c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi ks translated"><span class="l kt ku kv bm kw kx ky kz la di"> 3。</span> <strong class="jw ir">高级应用</strong></p><p id="d65d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">—完全图像彩色化</p><p id="dfdf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">—潜在空间聚类</p><p id="04c9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">—生成更高分辨率的图像</p><blockquote class="me mf mg"><p id="45d6" class="ju jv lp jw b jx jy jz ka kb kc kd ke mh kg kh ki mi kk kl km mj ko kp kq kr ij bi translated">在图像输入的情况下，</p></blockquote><p id="11a0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">— —自动编码器使用<strong class="jw ir"> Conv图层</strong></p><ul class=""><li id="6ac0" class="lb lc iq jw b jx jy kb kc kf ld kj le kn lf kr lg lh li lj bi translated">编码器=典型的conv <strong class="jw ir">金字塔</strong></li><li id="d7aa" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">最大轮询层跟随以减少维度</li><li id="a3f1" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">解码器= <strong class="jw ir">反进化</strong>过程</li></ul><p id="8ec7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">— —反进化过程，</p><blockquote class="me mf mg"><p id="a574" class="ju jv lp jw b jx jy jz ka kb kc kd ke mh kg kh ki mi kk kl km mj ko kp kq kr ij bi translated">-从之前学习的内容中添加新数据</p><p id="dc8e" class="ju jv lp jw b jx jy jz ka kb kc kd ke mh kg kh ki mi kk kl km mj ko kp kq kr ij bi translated">——容易出现<strong class="jw ir">视觉</strong> <strong class="jw ir">假象</strong>。</p><p id="6f89" class="ju jv lp jw b jx jy jz ka kb kc kd ke mh kg kh ki mi kk kl km mj ko kp kq kr ij bi translated">-使用“最近邻”或“双线性插值(上采样)”调整大小，这是比<strong class="jw ir">标准去卷积层更好的选择。</strong></p></blockquote><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ov"><img src="../Images/ded1065a1b6b2fc92ef4a3e78b0672cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HQ2LzQQaklCbOuGnVPlNUg.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Pushes more pixels to the image</figcaption></figure><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ow"><img src="../Images/f3be031ec83b9a9d4bfb0e8b3bb831b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IE3RHJm6OKZmzbIksDgS-g.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Fully Image Colorization</figcaption></figure><h2 id="ac75" class="ni ml iq bd mm nj nk dn mq nl nm dp mu kf nn no my kj np nq nc kn nr ns ng nt bi translated"><strong class="ak"> 2)变分自动编码器(VAE) </strong></h2><ul class=""><li id="549a" class="lb lc iq jw b jx nu kb nv kf nw kj nx kn ny kr lg lh li lj bi translated">这结合了<strong class="jw ir">贝叶斯推理。</strong></li><li id="852d" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated"><strong class="jw ir">压缩的</strong>表示是一个<strong class="jw ir">概率分布。</strong></li></ul><h2 id="ce05" class="ni ml iq bd mm nj nk dn mq nl nm dp mu kf nn no my kj np nq nc kn nr ns ng nt bi translated">3)稀疏自动编码器</h2><ul class=""><li id="10ef" class="lb lc iq jw b jx nu kb nv kf nw kj nx kn ny kr lg lh li lj bi translated">这用于<strong class="jw ir">特征提取。</strong></li><li id="bf0f" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">这比输入有更多的隐藏单元。</li><li id="e93e" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">这允许输入数据的稀疏表示。</li></ul><h2 id="5122" class="ni ml iq bd mm nj nk dn mq nl nm dp mu kf nn no my kj np nq nc kn nr ns ng nt bi translated"><strong class="ak"> 4)堆叠自动编码器</strong></h2><ul class=""><li id="9e73" class="lb lc iq jw b jx nu kb nv kf nw kj nx kn ny kr lg lh li lj bi translated">如果使用了一个以上的隐藏层，那么我们寻找这个自动编码器。</li><li id="56c2" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">每个连续层都</li></ul><blockquote class="nz"><p id="e610" class="oa ob iq bd oc od oe of og oh oi kr dk translated">-最佳加权</p><p id="bc37" class="oa ob iq bd oc od oj ok ol om on kr dk translated">-非线性</p><p id="1081" class="oa ob iq bd oc od oj ok ol om on kr dk translated">-训练数据的低维投影</p></blockquote><h2 id="3995" class="ni ml iq bd mm nj ox dn mq nl oy dp mu kf oz no my kj pa nq nc kn pb ns ng nt bi translated"><strong class="ak"> 4)深度自动编码器</strong></h2><ul class=""><li id="cdd8" class="lb lc iq jw b jx nu kb nv kf nw kj nx kn ny kr lg lh li lj bi translated">这有2个对称的“T32”深度信念网络“T33”，通常有4或5个“T34”浅层。</li><li id="db30" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">它的层是<strong class="jw ir">受限玻尔兹曼机(RBM)。</strong></li><li id="51de" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr lg lh li lj bi translated">用途，</li></ul><p id="7f9e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi ks translated"><span class="l kt ku kv bm kw kx ky kz la di"> 1。</span> <strong class="jw ir">图片搜索</strong></p><p id="ebbe" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">— —一张图片可以压缩成30个左右的向量(就像谷歌图片搜索一样)。</p><p id="7a65" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi ks translated"><span class="l kt ku kv bm kw kx ky kz la di"> 2。</span> <strong class="jw ir">数据压缩</strong></p><p id="03ad" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">— —深度自动编码器对“<strong class="jw ir">语义哈希”很有用。</strong></p><p id="a549" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi ks translated"><span class="l kt ku kv bm kw kx ky kz la di"> 3。</span> <strong class="jw ir">主题建模&amp;信息检索</strong></p><h1 id="84fd" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated"><strong class="ak">参考文献:- </strong></h1><ol class=""><li id="db53" class="lb lc iq jw b jx nu kb nv kf nw kj nx kn ny kr pc lh li lj bi translated"><a class="ae pd" href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/" rel="noopener ugc nofollow" target="_blank">http://ufldl . Stanford . edu/tutorial/unsupervised/auto encoders/</a></li><li id="739e" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr pc lh li lj bi translated"><a class="ae pd" href="https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694" rel="noopener ugc nofollow" target="_blank">https://hacker noon . com/auto encoders-deep-learning-bits-1-11731 e 200694</a></li><li id="c87f" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr pc lh li lj bi translated"><a class="ae pd" href="https://deeplearning4j.org/deepautoencoder" rel="noopener ugc nofollow" target="_blank">https://deeplearning4j.org/deepautoencoder</a></li><li id="0a06" class="lb lc iq jw b jx lk kb ll kf lm kj ln kn lo kr pc lh li lj bi translated"><a class="ae pd" href="https://lazyprogrammer.me/a-tutorial-on-autoencoders/" rel="noopener ugc nofollow" target="_blank">https://lazyprogrammer.me/a-tutorial-on-autoencoders/</a></li></ol></div></div>    
</body>
</html>