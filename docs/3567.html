<html>
<head>
<title>Implementation of Optimization for Deep Learning Highlights in 2017 (feat. Sebastian Ruder)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2017 年深度学习优化实现亮点(feat。塞巴斯蒂安·鲁德)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementation-of-optimization-for-deep-learning-highlights-in-2017-feat-sebastian-ruder-616766fe37f0?source=collection_archive---------12-----------------------#2018-05-24">https://towardsdatascience.com/implementation-of-optimization-for-deep-learning-highlights-in-2017-feat-sebastian-ruder-616766fe37f0?source=collection_archive---------12-----------------------#2018-05-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/658f785a08ce4b6f20778f273acc1490.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*2bJ6jVFdio4pNcRXqqoHnw.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/animation-news-3o6ZtbRh0xqwWzO8PC" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="10b3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">Sebastian Rude r 是自然语言处理专业的博士生，也是 T2 AYLIEN 的研究科学家。他有一个关于自然语言处理和机器学习的最有趣、信息量最大的博客。(我一直都在读，我强烈推荐任何人也去读！)</p><p id="4e4f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我在这篇文章中讲述了我从他的博客文章<a class="ae jy" href="http://ruder.io/optimizing-gradient-descent/index.html" rel="noopener ugc nofollow" target="_blank">中了解到的所有优化算法</a><a class="ae jy" rel="noopener" target="_blank" href="/only-numpy-implementing-and-comparing-gradient-descent-optimization-algorithms-google-brains-8870b133102b">。现在这篇文章是第二个版本，涵盖了更先进的优化技术。</a></p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="87e7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">网络架构/基准比较</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi le"><img src="../Images/c51fbabdeabd78586f8a55a158ab6736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*FK_Xn5MziKRgbDIPCbeTVg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Network Architecture of ELU Network, from this<a class="ae jy" rel="noopener" target="_blank" href="/iclr-2016-fast-and-accurate-deep-networks-learning-by-exponential-linear-units-elus-with-c0cdbb71bb02"> blog post</a></figcaption></figure><p id="6e00" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我最近报道了'<a class="ae jy" href="https://arxiv.org/abs/1511.07289" rel="noopener ugc nofollow" target="_blank"> <em class="lj">通过指数线性单元(ELUs)进行快速准确的深度网络学习'</em> </a>，<a class="ae jy" rel="noopener" target="_blank" href="/iclr-2016-fast-and-accurate-deep-networks-learning-by-exponential-linear-units-elus-with-c0cdbb71bb02">单击此处</a>阅读博文，不幸的是，我们在那里实现的模型遭受了对训练图像的过度拟合。所以让我们来看看这些方法是如何提高模型的性能的。(换句话说，它能概括得多好)我之所以这么说，是因为所有这些方法都在解决概括的问题。</p><blockquote class="lk ll lm"><p id="7209" class="jz ka lj kb b kc kd ke kf kg kh ki kj ln kl km kn lo kp kq kr lp kt ku kv kw ij bi translated">请注意，为了公平比较(我想知道这些方法如何改进网络),所以我没有添加任何额外的层，如批量标准化或任何数据预处理。此外，所有的网络都是使用 Adam Optimizer 的某种变体来训练的。</p></blockquote><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/1b9f942aaab56e61a0e594002304345f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*gw8PMrh2xOlg1yTRIVxnlQ.png"/></div></figure><p id="4ff7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">上图是同一个网络在使用 L2 正则化的自动微分(adam optimizer)进行训练时的表现。因此，总的来说，我们的目标是用新的优化方法在测试图像上达到 64%的准确率。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="9ba5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">修正 Adam 中的权重衰减正则化</strong></p><figure class="lf lg lh li gt jr"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="121d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">上述论文是塞巴斯蒂安在他的博客文章中使用的主要参考资料之一。所以我认为把它链接到这里也是一个好主意。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="de30" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">案例 1)解耦重量衰减/结果</strong></p><div class="lf lg lh li gt ab cb"><figure class="lt jr lu lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><img src="../Images/d0d3330337867a9eeab688b1f048c46f.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*0VEQ9VflF8V1XCOQx3fTtA.png"/></div></figure><figure class="lt jr md lv lw lx ly paragraph-image"><img src="../Images/85381ebf07b1fcc0b22041d7d9e682d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*-R3aPPHiB6QSfSYpho1D9Q.png"/></figure></div><p id="5987" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →常规 Adam 优化器<br/> <strong class="kb ir">右图</strong> →带解耦权重衰减的 Adam<br/><strong class="kb ir">红框</strong> →添加权重衰减项</p><p id="bd83" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">第一种方法非常简单，当更新新的权重时，我们将添加某种权重衰减项(小于 1 ),将其乘以权重以及学习速率。当用 python 实现时，它看起来像下面这样。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi me"><img src="../Images/8477a6bcc68d6ad6ee0506739a9a8b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KuNFnAgDpkyRrtBhiGyT8Q.png"/></div></div></figure><p id="e9b6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红线</strong> →添加重量衰减调整线</p><p id="ba62" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我已经将重量衰减率设置为 0.512，并且在每 10、50、100 和 150 次迭代中，我将重量衰减值减半。</p><div class="lf lg lh li gt ab cb"><figure class="lt jr mf lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><img src="../Images/b1a45df09ce79825c9165371b5a824cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*xk_eE7SCxXe3byoOPPGUlQ.png"/></div></figure><figure class="lt jr mf lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><img src="../Images/9dc0d383f791a76e1533bbcba5a12df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*0BZfGoL6klbGmRuP31Lk2A.png"/></div></figure></div><p id="e4c1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →一段时间内的训练精度/成本<br/>T5】右图 →一段时间内的测试精度/成本</p><p id="b1bf" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">由于成本上升到 80 英镑，右边的图表是倾斜的，但是不要担心，我已经附上了下面的图像，以显示模型的最终准确性。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/5cfbdfaf585ff544ad72325b30b92269.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*TzXZmSiZ0Dr6Q0zVNq8bSw.png"/></div></figure><p id="1715" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，使用这种方法，我们能够在测试图像上实现 56%的准确率。考虑到仅亚当和 L2 正则化就能达到 64%的准确率，这还不算太好。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="1a36" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">案例 2)固定指数移动平均值/结果</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mh"><img src="../Images/87c9e38e2524e6ff1c9d2917b7f03028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FfnsHRQIXPZ66_Bzkepzrw.png"/></div></div></figure><p id="4155" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红框</strong> →新旧 v 值之间的最大值</p><p id="0a15" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">上图显示了 Adam 的新更新规则(该算法的另一个名称是 AMSGrad)。另外，请注意，对于这条规则，我已经将 Beta 2 的值更改为 0.9，而不是使用默认值 0.999。(如下图所示)</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/f2ba5833dc85d8c4e018bda004813e3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*PnxFN94rtK_LWVItwsx0AQ.png"/></div></figure><p id="6d3a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，当在 Tensorflow 中实现时，它可能如下所示。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mj"><img src="../Images/ef98348b0f46c54de4cc1db138b62e18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8rbDEtKq1ujP1LYgzTEJAA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Followed <a class="ae jy" href="https://fdlm.github.io/post/amsgrad/" rel="noopener ugc nofollow" target="_blank">Filip KRZN</a>’s implementation</figcaption></figure><p id="ed1b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红框</strong> →新旧 v 值之间的最大值</p><p id="f70b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我个人最喜欢这种方法，因为它实现起来非常简单。但是我明白当比较张量时，减少和可能不是最好的方法，另一种方法可以是比较欧几里得范数。但是现在让我们看看这个方法是如何提高模型的性能的。</p><div class="lf lg lh li gt ab cb"><figure class="lt jr mf lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><img src="../Images/e22866291bb38aa14cb782af1e72037e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*S_REG2gBXyP_zkSyYftHkg.png"/></div></figure><figure class="lt jr mf lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><img src="../Images/65c194a92dfb154b3e5b635185d1c395.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ZnJfKF0n8C_88b3Ko1AF7A.png"/></div></figure></div><p id="9ff5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →一段时间内的训练精度/成本<br/> <strong class="kb ir">右图</strong> →一段时间内的测试精度/成本</p><p id="a76b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，这种方法并没有完全防止模型过度拟合，但是它确实比我们的基准测试获得了更好的性能。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mk"><img src="../Images/ffa3cf3d727d2cc3be2de0d03aabf635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qDo5b6Sbl9yKnmp9aW9Fqw.png"/></div></div></figure><p id="08cf" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在测试图像上有 66%的准确性，而在训练图像上有 99%的准确性，除了我们在反向传播中所做的改变之外，没有其他形式的正则化方法。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="1cc1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">情况 3)调整学习率/结果</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/ac9c90f5eb5d0d826ebef905e15a426e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*DnLVYUHGToinjixiEB11KA.png"/></div></figure><p id="5036" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红框</strong> →更改 Adam 的 Hyper 参数<br/> <strong class="kb ir">蓝框</strong> →模型参数的计算数量<br/> <strong class="kb ir">紫框</strong> →等式选择当前步骤的学习率</p><p id="d9e6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">为此，我们将计算我们网络中的参数数量。如果有人想知道怎么做，请点击这个链接。下面是 VGG 16 计算参数值的一个简单例子。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mm"><img src="../Images/13fe74df87255012f1e516b396306944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I2XTS8xrnh94ImVBQUjnNg.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from this <a class="ae jy" href="http://networks?, H. (2018). How to calculate the number of parameters of convolutional neural networks?. Stack Overflow. Retrieved 8 May 2018, from https://stackoverflow.com/questions/28232235/how-to-calculate-the-number-of-parameters-of-convolutional-neural-networks" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="67ba" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">下面是一个为我们的网络计算参数的例子。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/7d2673f2656f5ee3a74a68ac9b077eec.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*APOq98KX7IcPt0RayDA-Mg.png"/></div></figure><p id="a705" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红框</strong> →参数计算<br/>T3】蓝框 →网络架构参考</p><p id="6471" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在，既然我们已经准备好了参数的数量，让我们来看看计算学习率的等式。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mo"><img src="../Images/57c9fdbae9fa7d87e7f0e86938e5df32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jU66tnbW9sylsPxWHhmtGA.png"/></div></div></figure><p id="ca75" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">通过简单的 tf.cond()，我们能够在 step_num^(-0.5 和 step_num*warmup_steps^(-1.5).之间选择最小值最后让我们看看结果。</p><div class="lf lg lh li gt ab cb"><figure class="lt jr mf lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><img src="../Images/5c92638fe38359b409aa95b0b30121f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*LI4ezAA2_wGaK5cRBoezaQ.png"/></div></figure><figure class="lt jr mf lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><img src="../Images/69dc0d0cf21d272d808c560c7359a657.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*HFm-ZeFo-CqtVuzgsaw5iA.png"/></div></figure></div><p id="f561" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →一段时间内的训练精度/成本<br/> <strong class="kb ir">右图</strong> →一段时间内的测试精度/成本</p><p id="f40f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，使用这种方法，模型甚至不能在训练图像上表现良好。我怀疑这种方法的学习率会更高。</p><blockquote class="lk ll lm"><p id="885b" class="jz ka lj kb b kc kd ke kf kg kh ki kj ln kl km kn lo kp kq kr lp kt ku kv kw ij bi translated"><strong class="kb ir">请注意，我将步骤数解释为迭代(或当前迭代)数，如果这是错误的，请在下面评论。</strong></p></blockquote><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mp"><img src="../Images/49a20de9157c096ec840083202724fa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ON2fE_8kAzCfAjGfMnBGsw.png"/></div></div></figure><p id="6c1c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于训练图像和测试图像，该模型最终精度大约为 23%。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="dae4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">情况 4)带重启/结果的 Adam</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mq"><img src="../Images/687b7c037518ea0eee4c1b745f539910.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0N1fQbWxJ0i7pEFwWx4tZg.png"/></div></div></figure><p id="4782" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">上面是重启的随机梯度下降(SGD)方程。总之，对于每次迭代 Ti，学习率被重置。所以当我们绘制学习率随时间变化的曲线时，它看起来会像下面这样。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/d979ff5bca6e53f7f0353e0a6d37b7ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*Oi9tQZ0pwSAZDwEjSpnOMg.png"/></div></figure><p id="75a8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，学习率在迭代 1 时被设置为 1，再次设置为 300，700 等等。现在让我们来看看如何将这个方法应用到亚当身上。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ms"><img src="../Images/49868f69b0349958b8e32115c46a8003.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rHuVWo7bzt_aiGS_fASFCA.png"/></div></div></figure><p id="810d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红框</strong> →如何将此方法应用于 Adam 优化器</p><p id="3cb6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，我们首先需要修复重量衰减，我们已经看到了这一点。(这是我们在案例 1 中使用的方法)。现在让我们看一下实现。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mt"><img src="../Images/6fe4ff05b15116794db45340e5f57911.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aIB1Ki_fFeCn0IWYC6pNKA.png"/></div></div></figure><p id="c445" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红框</strong>→Ti 和 Tcur 的可变占位符<br/> <strong class="kb ir">蓝框</strong> →学习率计算公式</p><p id="168d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">实现重启的方法有很多种，但我选择最简单的方法 LOL。另外请注意，原始论文的作者建议将学习率的最大值设置为 1，我设置为 0.001。当我们绘制学习率如何随时间变化(200 次迭代)时，它看起来像下面这样。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi le"><img src="../Images/591476f82565dc4ebdb4f0efb6901c24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*lwsDg6AoQtKBnqTi__drSw.png"/></div></figure><p id="fc4b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">重启迭代次数设置为 10、30、70 和 150。(当我们将第一次重启迭代设置为 10 时)。现在让我们来看看模型的性能。</p><div class="lf lg lh li gt ab cb"><figure class="lt jr mf lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><img src="../Images/4fa826bfcc9785949be42379d7c6d604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*5076djA2iWZF1AUgCeSpgA.png"/></div></figure><figure class="lt jr mf lv lw lx ly paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><img src="../Images/a2370a12393e497794620d68580b8054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*FTGKV6ODdtgo5VeCsknqsA.png"/></div></figure></div><p id="6120" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →一段时间内的训练精度/成本<br/> <strong class="kb ir">右图</strong> →一段时间内的测试精度/成本</p><p id="a092" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">嗯……毫无疑问，这个模型做得很糟糕。我实际上花了很多时间试图让这个模型工作，但是这个模型似乎总是只在迭代的开始学习一些东西，并且在训练期间的某个地方，它似乎过了头，停止了所有的学习过程。</p><blockquote class="lk ll lm"><p id="85c3" class="jz ka lj kb b kc kd ke kf kg kh ki kj ln kl km kn lo kp kq kr lp kt ku kv kw ij bi translated">由于我很想知道我做错了什么，如果你对这款车型有什么推荐，请在下面评论。</p></blockquote><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mu"><img src="../Images/ecd3faffb956530ebfbca06bd09bb1e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eZFm2dQqh0r-F6DsHdp4Ag.png"/></div></div></figure><p id="3e5c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">训练图像和测试图像的最终准确度都是 9%。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="c875" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">互动代码/透明度</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mv"><img src="../Images/e7d61fe778b5a1805870c4e4ee5f6987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zZDjrp-idg_aUBFCAxv9tA.png"/></div></div></figure><p id="e1de" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="lj">对于谷歌实验室，你需要一个谷歌帐户来查看代码，你也不能在谷歌实验室运行只读脚本，所以在你的操场上做一个副本。最后，我永远不会请求允许访问你在 Google Drive 上的文件，仅供参考。编码快乐！同样为了透明，我在训练期间上传了所有的日志。</em></p><p id="8f14" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1xfKt1BmEx_-OAFlXz6o3xQjR3SO-dcxy" rel="noopener ugc nofollow" target="_blank"> 1 的代码请点击此处</a>，要访问<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/optimizer2017/opt1/case1.txt" rel="noopener ugc nofollow" target="_blank">日志请点击此处</a>。<br/>访问案例<a class="ae jy" href="https://colab.research.google.com/drive/143sXisWX2tl0LDO4zB0Qw8RNqir0X8OM" rel="noopener ugc nofollow" target="_blank"> 2 的代码请点击此处</a>，访问<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/optimizer2017/opt2/case2.txt" rel="noopener ugc nofollow" target="_blank">日志请点击此处</a>。<br/>访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1LuJC-G85nXF1J1qbUfrTZEYPnUv9X-bv" rel="noopener ugc nofollow" target="_blank"> 3 的代码请点击此处</a>，访问<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/optimizer2017/opt3/case3.txt" rel="noopener ugc nofollow" target="_blank">日志请点击此处</a>。<br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1Jt5uUmssdY4S_Pc0_6G3wqSXpflCueT-" rel="noopener ugc nofollow" target="_blank"> 4 的代码请点击此处</a>，要访问<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/optimizer2017/opt4/case4.txt" rel="noopener ugc nofollow" target="_blank">的日志请点击此处。</a></p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="ca17" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="605b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这些都是由许多聪明的研究人员完成的惊人的工作，然而，对于这个实验来说，没有一个看起来像是灵丹妙药。</p><p id="5dbb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有写作的列表，请在这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的 twitter 上关注我<a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文</a> t。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="ed5e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="c57d" class="mw mx iq kb b kc kd kg kh kk my ko mz ks na kw nb nc nd ne bi translated">2017 年深度学习优化亮点。(2017).塞巴斯蒂安·鲁德。检索于 2018 年 5 月 7 日，来自<a class="ae jy" href="http://ruder.io/deep-learning-optimization-2017/" rel="noopener ugc nofollow" target="_blank">http://ruder.io/deep-learning-optimization-2017/</a></li><li id="7493" class="mw mx iq kb b kc nf kg ng kk nh ko ni ks nj kw nb nc nd ne bi translated">只有 Numpy:实现和比较梯度下降优化算法+谷歌大脑的…(2018).走向数据科学。2018 年 5 月 7 日检索，来自<a class="ae jy" rel="noopener" target="_blank" href="/only-numpy-implementing-and-comparing-gradient-descent-optimization-algorithms-google-brains-8870b133102b">https://towards data science . com/only-numpy-implementing-and-comparison-gradient-descent-optimization-algorithms-Google-brains-8870 b 133102 b</a></li><li id="4333" class="mw mx iq kb b kc nf kg ng kk nh ko ni ks nj kw nb nc nd ne bi translated">克利夫特博士、安特辛纳、t .和霍克雷特博士(2015 年)。通过指数线性单元(ELUs)进行快速准确的深度网络学习。Arxiv.org。检索于 2018 年 5 月 7 日，来自<a class="ae jy" href="https://arxiv.org/abs/1511.07289" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1511.07289</a></li><li id="6f2b" class="mw mx iq kb b kc nf kg ng kk nh ko ni ks nj kw nb nc nd ne bi translated">[ICLR 2016]通过指数线性单元(ELUs)进行快速准确的深度网络学习，具有…(2018).走向数据科学。2018 年 5 月 7 日检索，来自<a class="ae jy" rel="noopener" target="_blank" href="/iclr-2016-fast-and-accurate-deep-networks-learning-by-exponential-linear-units-elus-with-c0cdbb71bb02">https://towards data science . com/iclr-2016-fast-and-accurate-deep-networks-learning-by-index-linear-units-elus-with-c 0 cdbb 71 bb 02</a></li><li id="eb5b" class="mw mx iq kb b kc nf kg ng kk nh ko ni ks nj kw nb nc nd ne bi translated">张量流正则化。(2018).ritchieng . github . io . 2018 年 5 月 7 日检索，来自<a class="ae jy" href="http://www.ritchieng.com/machine-learning/deep-learning/tensorflow/regularization/" rel="noopener ugc nofollow" target="_blank">http://www . ritchieng . com/machine-learning/deep-learning/tensor flow/regulation/</a></li><li id="7408" class="mw mx iq kb b kc nf kg ng kk nh ko ni ks nj kw nb nc nd ne bi translated">tf.cond |张量流。(2018).张量流。检索于 2018 年 5 月 7 日，来自<a class="ae jy" href="https://www.tensorflow.org/api_docs/python/tf/cond" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/cond</a></li><li id="6ae5" class="mw mx iq kb b kc nf kg ng kk nh ko ni ks nj kw nb nc nd ne bi translated">'开关')，V. (2018)。值错误:形状的等级必须为 0，但对于“cond _ 11/Switch”(op:“Switch”)的等级为 1。堆栈溢出。2018 年 5 月 8 日检索，来自<a class="ae jy" href="https://stackoverflow.com/questions/47739707/valueerror-shape-must-be-rank-0-but-is-rank-1-for-cond-11-switch-op-switch" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/47739707/value error-shape-must-be-rank-0-but-is-rank-1-for-cond-11-switch-op-switch</a></li><li id="4523" class="mw mx iq kb b kc nf kg ng kk nh ko ni ks nj kw nb nc nd ne bi translated">tensorflow？，H. (2018)。如何在 tensorflow 中求圆周率？。堆栈溢出。检索于 2018 年 5 月 8 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/45995471/how-to-get-pi-in-tensorflow" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/45995471/how-to-get-pi-in-tensor flow</a></li><li id="aa69" class="mw mx iq kb b kc nf kg ng kk nh ko ni ks nj kw nb nc nd ne bi translated">网络？，H. (2018)。如何计算卷积神经网络的参数个数？。堆栈溢出。检索于 2018 年 5 月 8 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/28232235/how-to-calculate-the-number-of-parameters-of-convolutional-neural-networks" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/28232235/how-to-calculation-the-number of-parameters-of-convolutionary-neural-networks</a></li><li id="c27b" class="mw mx iq kb b kc nf kg ng kk nh ko ni ks nj kw nb nc nd ne bi translated">Loshchilov，I .，&amp; Hutter，F. (2017)。修正 Adam 中的权重衰减正则化。<em class="lj"> arXiv 预印本 arXiv:1711.05101 </em>。</li><li id="2749" class="mw mx iq kb b kc nf kg ng kk nh ko ni ks nj kw nb nc nd ne bi translated">示例:基础— imgaug 0.2.5 文档。(2018).img aug . readthe docs . io . 2018 年 5 月 8 日检索，来自<a class="ae jy" href="http://imgaug.readthedocs.io/en/latest/source/examples_basics.html" rel="noopener ugc nofollow" target="_blank">http://img aug . readthe docs . io/en/latest/source/examples _ basics . html</a></li><li id="a7c5" class="mw mx iq kb b kc nf kg ng kk nh ko ni ks nj kw nb nc nd ne bi translated">[ICLR 2016]通过指数线性单元(ELUs)进行快速准确的深度网络学习，具有…(2018).走向数据科学。2018 年 5 月 8 日检索，来自<a class="ae jy" rel="noopener" target="_blank" href="/iclr-2016-fast-and-accurate-deep-networks-learning-by-exponential-linear-units-elus-with-c0cdbb71bb02">https://towards data science . com/iclr-2016-fast-and-accurate-deep-networks-learning-by-index-linear-units-elus-with-c 0 cdbb 71 bb 02</a></li><li id="e1e4" class="mw mx iq kb b kc nf kg ng kk nh ko ni ks nj kw nb nc nd ne bi translated">(2018).Arxiv.org。于 2018 年 5 月 8 日检索，来自<a class="ae jy" href="https://arxiv.org/pdf/1711.05101.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1711.05101.pdf</a></li></ol></div></div>    
</body>
</html>