<html>
<head>
<title>How to handle BigData Files on Low Memory?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在低内存上处理 BigData 文件？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-learn-from-bigdata-files-on-low-memory-incremental-learning-d377282d38ff?source=collection_archive---------6-----------------------#2018-12-05">https://towardsdatascience.com/how-to-learn-from-bigdata-files-on-low-memory-incremental-learning-d377282d38ff?source=collection_archive---------6-----------------------#2018-12-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="33fd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于如何使用 Pandas/Dask 在 Python 中处理大数据文件的演练</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/64d359589eb5eb0f8adfc5ad9de3a039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NjpPkrRIAMvONOQL"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@opak_02?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ruffa Jane Reyes</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="2415" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我在<code class="fe ls lt lu lv b">Tackle</code>类别的帖子中的一个，可以在我的 github repo <a class="ae kv" href="https://github.com/PuneetGrov3r/MediumPosts/tree/master/Tackle" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><p id="a02a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> (Edit-31/01/2019) </strong> — <a class="ae kv" href="#2d3d" rel="noopener ugc nofollow">为 BigData 添加了 dask.distributed.LocalCluster 的信息</a></p><p id="0186" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">(Edit-12/4/2019)——</strong>添加了关于数据集大小缩减和文件类型使用的新章节【尚未完成，但您仍可从中获得应用思路。]</p><h1 id="bd3e" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">索引</h1><ol class=""><li id="c8b7" class="mv mw iq ky b kz mx lc my lf mz lj na ln nb lr nc nd ne nf bi translated"><a class="ae kv" href="#dd6b" rel="noopener ugc nofollow">简介</a></li><li id="7fa5" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="#d920" rel="noopener ugc nofollow">列记忆还原(pd。Series.astype()) </a> [:未完成]</li><li id="c7b4" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="#36ea" rel="noopener ugc nofollow">文件类型(减少内存使用)</a> [:未完成]</li><li id="4ff3" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="#f270" rel="noopener ugc nofollow">数据探索</a></li><li id="95da" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="#d167" rel="noopener ugc nofollow">预处理</a></li><li id="aabf" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="#d418" rel="noopener ugc nofollow">增量学习(熊猫)</a></li><li id="d94a" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="#e0ba" rel="noopener ugc nofollow"> Dask(探索+准备+拟合预测)</a></li><li id="8493" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="#ca96" rel="noopener ugc nofollow">延伸阅读</a></li><li id="b302" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="#e618" rel="noopener ugc nofollow">参考文献</a></li></ol><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="4e6e" class="np me iq lv b gy nq nr l ns nt"><strong class="lv ir"><em class="nu">NOTE:<br/></em></strong>This post goes along with <strong class="lv ir"><em class="nu">Jupyter Notebook</em></strong> available in my Repo on Github:[<a class="ae kv" href="https://nbviewer.jupyter.org/github/PuneetGrov3r/MediumPosts/blob/master/Tackle/BigData-IncrementalLearningAndDask.ipynb" rel="noopener ugc nofollow" target="_blank">HowToHandleBigData</a>]  (with dummy data)<br/>and <br/>Kaggle:[<a class="ae kv" href="https://www.kaggle.com/puneetgrover/learn-from-bigdata-files-on-low-memory" rel="noopener ugc nofollow" target="_blank">HowToHandleBigData</a>] (with Kaggle <a class="ae kv" href="https://www.kaggle.com/c/ga-customer-revenue-prediction" rel="noopener ugc nofollow" target="_blank">competition</a> data)</span></pre><h1 id="dd6b" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">1.简介<a class="ae kv" href="#bd3e" rel="noopener ugc nofollow"> ^ </a></h1><p id="b87f" class="pw-post-body-paragraph kw kx iq ky b kz mx jr lb lc my ju le lf nv lh li lj nw ll lm ln nx lp lq lr ij bi translated">随着数据量呈指数级增长，我们无法在电脑内存中容纳我们的数据，甚至我们的模型。我们都买不起高端组装台式机。</p><p id="8e44" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，最近的 kaggle <a class="ae kv" href="https://www.kaggle.com/c/ga-customer-revenue-prediction" rel="noopener ugc nofollow" target="_blank">竞赛</a>的数据集无法容纳在 kaggle 内核或 Colab 的 17GB 内存中。它有将近 200 万行，最重要的是，一些列有非常大的 JSON 数据作为字符串。我们应该如何解决这个问题？我们可以向谁求助呢？</p><p id="0f29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">增量学习和/或 Dask 来拯救！</p><p id="b21d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可能已经知道神经网络本质上是增量学习器，所以我们可以在那里解决这个问题。许多<code class="fe ls lt lu lv b">sklearn</code>的模型提供了一种叫做<code class="fe ls lt lu lv b">partial_fit</code>的方法，使用它我们可以批量建模。一些像<code class="fe ls lt lu lv b">XGBoost</code>和<code class="fe ls lt lu lv b">LightGBM</code>这样的 Boosting 库提供了一种渐进学习的方式来处理大数据。</p><p id="acc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我们将研究一些 Boosting 算法提供的增量解决方案。然后我们将在同一个数据集上使用<code class="fe ls lt lu lv b">Dask</code>，并使用其模型进行预测。</p><h1 id="d920" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">2.列存储缩减</h1><p id="d528" class="pw-post-body-paragraph kw kx iq ky b kz mx jr lb lc my ju le lf nv lh li lj nw ll lm ln nx lp lq lr ij bi translated"><a class="ae kv" href="#bd3e" rel="noopener ugc nofollow"> ^ </a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/4c2aa5249ea4b031f1b6b829679e0a55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*S5h8IvxgkCJIiA6e"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@lh_photography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Lisa H</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="6cd6" class="np me iq lv b gy nq nr l ns nt"># I don't know who the original author of this function is,<br/># but you can use this function to <strong class="lv ir">reduce memory</strong><br/># <strong class="lv ir">consumption</strong> <strong class="lv ir">by</strong> <strong class="lv ir">60-70%!</strong></span><span id="c6c9" class="np me iq lv b gy nz nr l ns nt"><strong class="lv ir">def</strong> <em class="nu">reduce_mem_usage</em>(df):<br/>    """ <br/>    iterate through all the columns of a dataframe and <br/>    modify the data type to reduce memory usage.        <br/>    """<br/>    start_mem = df.memory_usage().sum() / 1024**2<br/>    <strong class="lv ir">print</strong>(('Memory usage of dataframe is {:.2f}' <br/>                     'MB').format(start_mem))<br/>    <br/>    <strong class="lv ir">for</strong> col in df.columns:<br/>        col_type = df[col].dtype<br/>        <br/>        <strong class="lv ir">if</strong> col_type != object:<br/>            c_min = df[col].min()<br/>            c_max = df[col].max()<br/>            <strong class="lv ir">if</strong> str(col_type)[:3] == 'int'<strong class="lv ir">:</strong><br/>                <strong class="lv ir">if</strong> c_min &gt; np.iinfo(np.int8).min and c_max &lt;\<br/>                  np.iinfo(np.int8).max:<br/>                    df[col] = df[col].astype(np.int8)<br/>                <strong class="lv ir">elif </strong>c_min &gt; np.iinfo(np.int16).min and c_max &lt;\<br/>                   np.iinfo(np.int16).max:<br/>                    df[col] = df[col].astype(np.int16)<br/>                <strong class="lv ir">elif </strong>c_min &gt; np.iinfo(np.int32).min and c_max &lt;\<br/>                   np.iinfo(np.int32).max:<br/>                    df[col] = df[col].astype(np.int32)<br/>                <strong class="lv ir">elif </strong>c_min &gt; np.iinfo(np.int64).min and c_max &lt;\<br/>                   np.iinfo(np.int64).max:<br/>                    df[col] = df[col].astype(np.int64)  <br/>            <strong class="lv ir">else</strong>:<br/>                <strong class="lv ir">if </strong>c_min &gt; np.finfo(np.float16).min and c_max &lt;\<br/>                   np.finfo(np.float16).max:<br/>                    df[col] = df[col].astype(np.float16)<br/>                <strong class="lv ir">elif </strong>c_min &gt; np.finfo(np.float32).min and c_max &lt;\<br/>                   np.finfo(np.float32).max:<br/>                    df[col] = df[col].astype(np.float32)<br/>                <strong class="lv ir">else</strong>:<br/>                    df[col] = df[col].astype(np.float64)<br/>        <strong class="lv ir">else</strong>:<br/>            df[col] = df[col].astype('category')</span><span id="f13f" class="np me iq lv b gy nz nr l ns nt">    end_mem = df.memory_usage().sum() / 1024**2<br/>    <strong class="lv ir">print</strong>(('Memory usage after optimization is: {:.2f}' <br/>                              'MB').format(end_mem))<br/>    <strong class="lv ir">print</strong>('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) <br/>                                             / start_mem))<br/>    <br/>    <strong class="lv ir">return </strong>df</span></pre><p id="d0d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">坦克</p><h1 id="36ea" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">3.文件类型</h1><p id="490f" class="pw-post-body-paragraph kw kx iq ky b kz mx jr lb lc my ju le lf nv lh li lj nw ll lm ln nx lp lq lr ij bi translated"><a class="ae kv" href="#bd3e" rel="noopener ugc nofollow"> ^ </a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/3f7ecbbae1394563cb7c1fbff4ca9402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w-MVGb0kIhZc6B8c"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@kellysikkema?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Kelly Sikkema</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ef6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">HDF5、拼花地板等。</p><blockquote class="ob oc od"><p id="750d" class="kw kx nu ky b kz la jr lb lc ld ju le oe lg lh li of lk ll lm og lo lp lq lr ij bi translated"><em class="iq">注:如果你不想落入这一节，你可以只看 HDF5。这应该足够了。转到下一部分。</em></p></blockquote><p id="bc18" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Hierarchical_Data_Format" rel="noopener ugc nofollow" target="_blank">分层数据格式—维基百科</a> (HDF5)</p><p id="bd01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://www.youtube.com/watch?v=rVC9F1y38oU&amp;list=PLbk_EDDIZpfa-nAodPK_fxwbPNLyixhGQ&amp;index=8&amp;t=0s" rel="noopener ugc nofollow" target="_blank">阿帕奇拼花地板</a></p><p id="aca7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://community.hitachivantara.com/community/products-and-solutions/pentaho/blog/2017/11/07/hadoop-file-formats-its-not-just-csv-anymore" rel="noopener ugc nofollow" target="_blank"> Hadoop 文件格式:它不再仅仅是 CSV——Kevin Haas</a>(不要在阅读 Hadoop 后被吓跑，您可以只使用 Parquet 格式并通过使用<code class="fe ls lt lu lv b">pd.DataFrame.to_parquet</code>和<code class="fe ls lt lu lv b">pd.DataFrame.read_parquet</code>方法。)</p><p id="5b41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">还有，如果你有兴趣:<a class="ae kv" href="https://www.youtube.com/watch?v=sLuHzdMGFNA" rel="noopener ugc nofollow" target="_blank"> Parquet vs Avro </a>【无论如何我都会推荐你去看。]</p><p id="562f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">坦克</p><h1 id="f270" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">4.数据探索<a class="ae kv" href="#bd3e" rel="noopener ugc nofollow"> ^ </a></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/bc39fb8e2b834318c4fda11866ead6d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bERjyTHarYEzMtZA"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@meymigrou?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Panos Sakalakis</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1841" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们将通过使用以下命令查看前几行来了解我们的数据是什么样子的:</p><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="f9a6" class="np me iq lv b gy nq nr l ns nt">part = pd.read_csv("train.csv.zip", nrows=10)<br/>part.head()</span></pre><p id="2857" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过这个，你将有关于不同的列是如何构成的，如何处理每个列等的基本信息。列出不同类型的栏目，如<code class="fe ls lt lu lv b">numerical_columns</code>、<code class="fe ls lt lu lv b">obj_columns</code>、<code class="fe ls lt lu lv b">dictionary_columns</code>等。它将保存所有相应的列。</p><p id="bed0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，为了研究数据，我们将像这样逐列进行:</p><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="e3a8" class="np me iq lv b gy nq nr l ns nt"><strong class="lv ir">#</strong> For dictionary columns you can do:</span><span id="977e" class="np me iq lv b gy nz nr l ns nt"># 'idx' is index of corresponding column in DataFrame.<br/># You can find it by using np.where(col==df.columns)</span><span id="41f1" class="np me iq lv b gy nz nr l ns nt">for col in dictionary_columns:<br/>    df = pd.read_csv("train.csv.zip", usecols = [idx], converters={col: json.loads})<br/>    column_as_df = json_normalize(df[col])<br/>    # ... plot each column ...<br/>    # ... check if you want to drop any column ...<br/></span></pre><p id="0ca9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以将所有列名作为键的字典和应用于它的方法保存在一个列表中，作为该列的管道。你也可以把你的字典保存起来，以备将来使用:</p><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="47c6" class="np me iq lv b gy nq nr l ns nt">with open("preprocessing_pipeline.pickle", "wb") as fle:<br/>  pickle.dump(preprocessing_pipeline, fle)</span></pre><p id="eac8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果其中一列对您的内存来说太大，实际上我上面提到的 kaggle 竞赛中的一行就是这种情况。你甚至可以逐步打开一列，并可以做一些基本的事情，如计算平均值，标准差等。手动。或者你也可以用<code class="fe ls lt lu lv b">Dask</code>来代替它，用和<code class="fe ls lt lu lv b">pandas</code>几乎一样的 API 来计算它们。参见最后一节<code class="fe ls lt lu lv b">Dask</code>。</p><h1 id="d167" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">5.预处理<a class="ae kv" href="#bd3e" rel="noopener ugc nofollow"> ^ </a></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/ea0733752a594d5dc78342bf5b09eae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JHimrfye4USCHfhd"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@rawpixel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">rawpixel</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="0c1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于预处理数据，我们将使用我们之前创建的字典，它包含关于我们希望保留哪些列(作为键)以及对每列应用什么方法(作为值)的信息，以创建一个方法。</p><p id="3e82" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在增量学习过程中，每一批数据都会调用这个方法。</p><p id="0291" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在这里要注意的一件事是我们安装了方法(比如<em class="nu"> LabelEncoder </em>的、<em class="nu">scalar</em>的等等。)在探索整个数据列的过程中，我们将在这里的每个增量步骤中使用它来转换数据。因为，在每一批中，可能会有一些数据丢失，如果我们使用了不同的<em class="nu">标签编码器</em>、<em class="nu">标量</em>等。对于每一批，这些方法不会给出相同类别的相同结果。这就是为什么我们在探索过程中已经安装了整个列。</p><p id="dd3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是预处理数据的方法:</p><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="daec" class="np me iq lv b gy nq nr l ns nt">def preprocess(df):<br/>  df.reset_index(drop=True, inplace=True)<br/>  <br/>  # For dict columns:<br/>  for col in dict_columns:<br/>    col_df = json_normalize(df[col])<br/>    # json.loads during pd.read_csv to convert string to dict.                                <br/>    col_df.columns = [f"{col}.{subcolumn}" for subcolumn in col_df.columns]<br/>    # Select all columns which we selected before.<br/>    selected_columns = [c for c in dictionary.keys() if c in col_df.columns()]<br/>    to_drop = [c for c in col_df.columns if not in selected_columns]<br/>    <br/>    # Drop all previously unselected columns.<br/>    col_df = col_df.drop(to_drop, axis=1)                                       <br/>    <br/>    df = df.drop(col, axis=1).merge(col_df, right_index=True, left_index=True)<br/>    <br/>  # And so on...<br/>  <br/>  # And then apply all Scalars, LabelEncoder's to all columns selected...<br/>  <br/>  return df</span></pre><h1 id="d418" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">6.增量学习<a class="ae kv" href="#bd3e" rel="noopener ugc nofollow"> ^ </a></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/69316806a29405a548bc0c99caefa83d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Kq8vZprLFNnPaLyL"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@bruno_nascimento?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Bruno Nascimento</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="301e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要使用<code class="fe ls lt lu lv b">pandas</code>增量读取数据文件，您必须使用一个参数<code class="fe ls lt lu lv b">chunksize</code>，该参数指定一次要读/写的行数。</p><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="fc19" class="np me iq lv b gy nq nr l ns nt">incremental_dataframe = pd.read_csv("train.csv",<br/>                        chunksize=100000) <strong class="lv ir">#</strong> Number of lines to read.</span><span id="c120" class="np me iq lv b gy nz nr l ns nt"># This method will return a sequential file reader (TextFileReader)<br/># reading 'chunksize' lines every time. To read file from <br/># starting again, you will have to call this method again.</span></pre><p id="e541" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后你可以使用<code class="fe ls lt lu lv b">XGBoost</code> <a class="ae kv" href="#8633" rel="noopener ugc nofollow"> </a>或<code class="fe ls lt lu lv b">LightGBM</code>对你的数据进行增量训练。对于<code class="fe ls lt lu lv b">LightGBM</code>,你必须向它的<code class="fe ls lt lu lv b">.train</code>方法传递一个参数<code class="fe ls lt lu lv b">keep_training_booster=True</code>,向<code class="fe ls lt lu lv b">XGBoost</code>的<code class="fe ls lt lu lv b">.train</code>方法传递三个参数。</p><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="b9fe" class="np me iq lv b gy nq nr l ns nt"><strong class="lv ir"># </strong>First one necessary for incremental learning:<br/>lgb_params = {<br/>  'keep_training_booster': True,<br/>  'objective': 'regression',<br/>  'verbosity': 100,<br/>}</span><span id="8095" class="np me iq lv b gy nz nr l ns nt"><strong class="lv ir">#</strong> First three are for incremental learning:<br/>xgb_params = {<br/>  'update':'refresh',<br/>  'process_type': 'update',<br/>  'refresh_leaf': True,<br/>  'silent': False,<br/>  }</span></pre><p id="6804" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在每一步中，我们将保存我们的估计值，然后在下一步中将其作为参数传递。</p><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="332a" class="np me iq lv b gy nq nr l ns nt"><strong class="lv ir">#</strong> For saving regressor for next use.<br/>lgb_estimator = None<br/>xgb_estimator = None</span><span id="c059" class="np me iq lv b gy nz nr l ns nt">for df in incremental_dataframe:<br/>  df = preprocess(df)<br/>  <br/>  xtrain, ytrain, xvalid, yvalid = # Split data as you like<br/>  <br/>  lgb_estimator = lgb.train(lgb_params,<br/>                         <strong class="lv ir">#</strong> Pass partially trained model:<br/>                         init_model=lgb_estimator,<br/>                         train_set=lgb.Dataset(xtrain, ytrain),<br/>                         valid_sets=lgb.Dataset(xvalid, yvalid),<br/>                         num_boost_round=10)<br/>  <br/>  xgb_model = xgb.train(xgb_params, <br/>                        dtrain=xgb.DMatrix(xtrain, ytrain),<br/>                        evals=(xgb.DMatrix(xvalid, yvalid),"Valid"),<br/>                        <strong class="lv ir">#</strong> Pass partially trained model:<br/>                        xgb_model = xgb_estimator)<br/>  <br/>  del df, xtrain, ytrain, xvalid, yvalid<br/>  gc.collect()</span></pre><p id="7e0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ls lt lu lv b">CatBoost</code>的增量学习法正在进行中。<a class="ae kv" href="#6942" rel="noopener ugc nofollow"> </a></p><p id="3591" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了加快速度，如果您的块仍然足够大，您可以使用<code class="fe ls lt lu lv b">Python</code>的<code class="fe ls lt lu lv b">multiprocessing</code>库函数来并行化您的预处理方法，如下所示:</p><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="eb56" class="np me iq lv b gy nq nr l ns nt">n_jobs = 4<br/>for df in incremental_dataframe:<br/>  <!-- -->p = Pool(n_jobs)<br/>  f_ = p.map(preprocess, np.array_split(df, n_jobs))<br/>  f_ = pd.concat(f_, axis=0, ignore_index=True)<br/>  p.close()<br/>  p.join()<br/>  <br/>  # And then your model training ...</span></pre><p id="bb66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于<code class="fe ls lt lu lv b">Python</code>中并行编程的介绍，请在这里阅读我的帖子<a class="ae kv" rel="noopener" target="_blank" href="/speed-up-your-algorithms-part-3-parallelization-4d95c0888748"/>。</p><h1 id="e0ba" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">7.达斯克<a class="ae kv" href="#bd3e" rel="noopener ugc nofollow"> ^ </a></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/d79afb98d3f16cda5ffb456632bf2b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eLZ5KrE36r0YM5hc"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@jannerboy62?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Nick Fewings</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="ob oc od"><p id="a6f4" class="kw kx nu ky b kz la jr lb lc ld ju le oe lg lh li of lk ll lm og lo lp lq lr ij bi translated">Dask 有助于以顺序并行的方式利用大数据资源。</p></blockquote><p id="4256" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于<code class="fe ls lt lu lv b">Dask</code>的介绍，请在这里阅读我的帖子<a class="ae kv" rel="noopener" target="_blank" href="/speeding-up-your-algorithms-part-4-dask-7c6ed79994ef"/>。</p><p id="8f7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以在这里以类似的方式应用来自<code class="fe ls lt lu lv b">pandas</code> API 的函数。您可以检查是否有这样的空值:</p><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="e300" class="np me iq lv b gy nq nr l ns nt">df.isnull().sum().compute()</span></pre><p id="6489" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要缩放列，可以将它们转换为数组，并以类似的方式使用其 Scaler 函数:</p><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="7780" class="np me iq lv b gy nq nr l ns nt">rsc = dask_ml.preprocessing.RobustScaler()<br/>result = rsc.fit_transform(X[:,i].reshape(-1, 1)) # for ith column</span></pre><p id="39bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">处理 JSON 或任何其他半结构化数据，如日志文件等。你可以使用<code class="fe ls lt lu lv b">Dask</code>的<code class="fe ls lt lu lv b">Bag</code>容器提供的函数。</p><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="2058" class="np me iq lv b gy nq nr l ns nt">df[key] = df[dict_col].to_bag().pluck(key).to_dataframe().iloc[:,0]</span></pre><p id="12e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在预处理之后，你可以使用<code class="fe ls lt lu lv b">Dask</code>的一个模型来训练你的数据。</p><p id="bb2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完整代码请阅读 Jupyter 笔记本<a class="ae kv" href="https://nbviewer.jupyter.org/github/PuneetGrov3r/MediumPosts/blob/master/Tackle/BigData-IncrementalLearningAndDask.ipynb#Method-2:-Using-Dask:" rel="noopener ugc nofollow" target="_blank">中的<code class="fe ls lt lu lv b">Dask</code>部分，点击这里</a>。</p><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="bae1" class="np me iq lv b gy nq nr l ns nt"><strong class="lv ir">Note:<br/></strong>You should only use Dask in case of Big Data, where it is not able to fit in your memory. Otherwise in-memory learning with pandas and sklearn will be lot faster.</span><span id="2d3d" class="np me iq lv b gy nz nr l ns nt"><strong class="lv ir">Note: (Local Cluster)</strong><br/>You can perform almost any BigData related query/tasks with the help of LocalCluster. You can, specifically, use 'memory_limit' parameter to constrict Dask's memory usage to a specific amount. <br/>Also, at times you might notice that <strong class="lv ir">Dask</strong> is exceeding memory use, even though it is dividing tasks. It could be happening to you because of the function you are trying to use on your <strong class="lv ir">dataset </strong>wants most of your data for processing, and multiprocessing can make things worse as all workers might try to copy <strong class="lv ir">dataset </strong>to memory. This can happen in aggregating cases.</span><span id="557d" class="np me iq lv b gy nz nr l ns nt">In these cases you can use <strong class="lv ir">Dask.distributed.LocalCluster</strong> parameters and pass them to <strong class="lv ir">Client</strong>() to make a <strong class="lv ir">LocalCluster</strong> using cores of your Local machines.</span><span id="709d" class="np me iq lv b gy nz nr l ns nt"><strong class="lv ir">from</strong> dask.distributed <strong class="lv ir">import</strong> Client, LocalCluster<br/>client = <strong class="lv ir">Client</strong>(n_workers=1, threads_per_worker=1, processes=False,<br/>                memory_limit='25GB', scheduler_port=0, <br/>                silence_logs=False, diagnostics_port=0)<br/>client</span><span id="09d7" class="np me iq lv b gy nz nr l ns nt">'scheduler_port=0' and 'diagnostics_port=0' will choose random port number for this particular client. With 'processes=False' <strong class="lv ir">dask</strong>'s client won't copy dataset, which would have happened for every process you might have made.<br/>You can tune your client as per your needs or limitations, and for more info you can look into parameters of <strong class="lv ir">LocalCluster.<br/></strong>You can also use multiple clients on same machine at different ports.</span></pre><h1 id="ca96" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">8.延伸阅读<a class="ae kv" href="#bd3e" rel="noopener ugc nofollow"> ^ </a></h1><ol class=""><li id="ee09" class="mv mw iq ky b kz mx lc my lf mz lj na ln nb lr nc nd ne nf bi translated"><a class="ae kv" href="https://www.kaggle.com/mlisovyi/bigdata-dask-pandas-flat-json-trim-data-upd" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/mlisovyi/bigdata-dask-pandas-flat-JSON-trim-data-upd</a></li><li id="8633" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="https://github.com/dmlc/xgboost/issues/3055#issuecomment-359505122" rel="noopener ugc nofollow" target="_blank"><em class="nu">https://github . com/dmlc/xgboost/issues/3055 # issue comment-359505122</em></a></li><li id="22c3" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="https://www.kaggle.com/ogrellier/create-extracted-json-fields-dataset" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/ogrellier/create-extracted-JSON-fields-dataset</a></li><li id="0722" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="https://github.com/Microsoft/LightGBM/blob/master/examples/python-guide/advanced_example.py" rel="noopener ugc nofollow" target="_blank">https://github . com/Microsoft/light GBM/blob/master/examples/python-guide/advanced _ example . py</a></li><li id="7004" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65" rel="noopener ugc nofollow" target="_blank">将数据帧内存大小减少约 65% | Kaggle </a></li><li id="beec" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="https://machinelearningmastery.com/large-data-files-machine-learning/" rel="noopener ugc nofollow" target="_blank">机器学习处理大数据文件的 7 种方法——机器学习掌握</a></li></ol><h1 id="e618" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">9.参考文献<a class="ae kv" href="#bd3e" rel="noopener ugc nofollow"> ^ </a></h1><ol class=""><li id="34db" class="mv mw iq ky b kz mx lc my lf mz lj na ln nb lr nc nd ne nf bi translated"><a class="ae kv" href="https://www.kaggle.com/mlisovyi/bigdata-dask-pandas-flat-json-trim-data-upd" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/mlisovyi/bigdata-dask-pandas-flat-JSON-trim-data-upd</a></li><li id="ea73" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="https://www.kaggle.com/ogrellier/create-extracted-json-fields-dataset" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/ogrellier/create-extracted-JSON-fields-dataset</a></li><li id="51df" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="https://gist.github.com/goraj/6df8f22a49534e042804a299d81bf2d6" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/goraj/6 df 8 f 22 a 49534 e 042804 a 299d 81 bf2d 6</a></li><li id="dcd5" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="https://github.com/dmlc/xgboost/issues/3055" rel="noopener ugc nofollow" target="_blank">https://github.com/dmlc/xgboost/issues/3055</a></li><li id="6942" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="https://github.com/catboost/catboost/issues/464" rel="noopener ugc nofollow" target="_blank">https://github.com/catboost/catboost/issues/464</a></li><li id="a941" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated">https://github.com/Microsoft/LightGBM/issues/987<a class="ae kv" href="https://github.com/Microsoft/LightGBM/issues/987" rel="noopener ugc nofollow" target="_blank"/></li><li id="509f" class="mv mw iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="https://gist.github.com/ylogx/53fef94cc61d6a3e9b3eb900482f41e0" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/ylogx/53 fef 94 cc 61d 6a 3 e 9 B3 EB 900482 f 41 e 0</a></li></ol><pre class="kg kh ki kj gt nl lv nm nn aw no bi"><span id="60f6" class="np me iq lv b gy nq nr l ns nt">Suggestions and reviews are welcome.<br/>Thank you for reading!</span></pre><p id="9935" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">签名:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/ca01c1d315400c09978fb5e62da01d87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*N7tbEUmEr0wEqsdlZNQ5iA.png"/></div></div></figure></div></div>    
</body>
</html>