<html>
<head>
<title>Multi-Class Classification in Text using R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 R 的文本多类分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-class-classification-in-text-using-r-e6cf72ef1da3?source=collection_archive---------5-----------------------#2018-12-20">https://towardsdatascience.com/multi-class-classification-in-text-using-r-e6cf72ef1da3?source=collection_archive---------5-----------------------#2018-12-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6696" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">预测 Ted 演讲的收视率</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/692cb942f54581e1b2cc0ffb315ecc68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Lt7kQ3CGgQwXEKcY"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@hermez777?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Hermes Rivera</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="631a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个博客是我的 NLP 博客系列的延续。在之前的博客中，我讨论了 R 中的<a class="ae kv" href="https://medium.com/@shubhanshugupta/data-preprocessing-in-r-2f0e25487bb" rel="noopener">数据预处理步骤</a>和<a class="ae kv" href="https://medium.com/datadriveninvestor/emotions-in-ted-talks-text-analytics-in-r-1b6016f3316e" rel="noopener">识别 ted 演讲中的情绪</a>。在这个博客中，我将预测观众对 ted 演讲的评价。这将需要多类分类和相当多的数据清理和预处理。我们将在下面详细讨论每个步骤。</p><p id="dd7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，让我们开始吧！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/068f2afd068a07b723cab195c7ebecc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*riIiqesfHxXnwTS7"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">A way to visualize text classification</figcaption></figure><h2 id="fbc7" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">数据清理和解析</h2><p id="af1f" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated"><a class="ae kv" href="https://www.kaggle.com/rounakbanik/ted-talks" rel="noopener ugc nofollow" target="_blank"> ted 演讲数据集</a>中的评级栏看起来像<code class="fe mr ms mt mu b">Figure 1</code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/15eb2193acf6e37d047d6dbc91e593c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*z352_WmgvFGp8sOC"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 1</figcaption></figure><p id="44f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面这个栏目的截图，<code class="fe mr ms mt mu b">Figure 1</code>表示有多少人认为一个特定的演讲是“鼓舞人心的”、“漂亮的”、“有创意的”、“有说服力的”等等。JSON 中的 count 键表示给予谈话的评分值。例如，这个演讲被 385 个人评为“鼓舞人心”，只有 2 个人认为这个演讲“有趣”。在这里，我们的目标是获得每次谈话的最高评分。</p><p id="4e42" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中一个主要问题是解析 ratings 列。我已经使用了<em class="mw"> gsub </em>函数来用双引号替换单引号。我在这里找到了关于<em class="mw">gsub</em>T13】的细节和解释。</p><pre class="kg kh ki kj gt mx mu my mz aw na bi"><span id="a750" class="lt lu iq mu b gy nb nc l nd ne">library(jsonlite)</span><span id="a5a4" class="lt lu iq mu b gy nf nc l nd ne">formatted_ted_ratings &lt;- gsub(“‘“,’”’,ted_talks$ratings)</span></pre><p id="d0f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下一步是解析 JSON 以获得 id、评级名称和评级计数的列表。R 中的<em class="mw"> jsonlite </em>库提供了流、验证和美化 JSON 数据的功能。<a class="ae kv" href="https://www.rdocumentation.org/packages/RJSONIO/versions/1.3-0/topics/fromJSON" rel="noopener ugc nofollow" target="_blank"> fromJSON 函数</a>用于将 JSON 对象反序列化为 R 对象。最后，<em class="mw"> purrr::map </em>函数将一个函数(在我们的例子中是 fromJSON)应用于列表中的每个元素。文档和实现可以在这里阅读<a class="ae kv" href="https://jennybc.github.io/purrr-tutorial/ls01_map-name-position-shortcuts.html" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="kg kh ki kj gt mx mu my mz aw na bi"><span id="fadc" class="lt lu iq mu b gy nb nc l nd ne">ted_ratings &lt;- purrr::map(formatted_ted_ratings, jsonlite::fromJSON)</span></pre><p id="7ebf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的代码块为我们提供了一个经过解析的评级列的简洁列表。看起来像<code class="fe mr ms mt mu b">Figure 2</code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/b6b076e5ef3dac718f216239f9481282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*75tY6IIe0SW18jSc"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 2: Parsed rating column</figcaption></figure><p id="61f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下一步中，我将创建一个新的列“highest_rating ”,在其中我将存储每次谈话的最高评分。发布后，我会将该列转换为因子，这将有效地给我们 13 个独特的因素(评级)来处理。我希望，到目前为止，您一定已经得到了一个提示，这个具有 13 个因子变量的最高评级列将用于多类分类。在这种意义上，二元分类问题有两个类别来分类数据点，例如真和假。然而，在这个问题中，我们必须将数据点分类到 13 个类别中的一个，因此，这是一个多类别分类问题。</p><pre class="kg kh ki kj gt mx mu my mz aw na bi"><span id="3f08" class="lt lu iq mu b gy nb nc l nd ne">for (i in (1:length(ted_ratings))) {<br/> ted_ratings_df &lt;- ted_ratings[[i]]<br/> highest_rating_count &lt;- ted_ratings_df[which(ted_ratings_df$count == max(ted_ratings_df$count)), ]<br/> ted_talks$highest_rating[i] &lt;- highest_rating_count$name<br/>}</span><span id="5729" class="lt lu iq mu b gy nf nc l nd ne">ted_talks$highest_rating = as.factor(ted_talks$highest_rating)</span></pre><p id="9be9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完成以上步骤后，我们的数据集准备工作就完成了。</p><h2 id="ef65" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">数据建模</h2><p id="3e20" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">我们现在将数据集分为训练和测试。我以 60:40 的比例划分了我的数据集。</p><pre class="kg kh ki kj gt mx mu my mz aw na bi"><span id="c6df" class="lt lu iq mu b gy nb nc l nd ne">trainObs &lt;- sample(nrow(ted_talks), .6 * nrow(ted_talks), replace = FALSE)<br/>testObs &lt;- sample(nrow(ted_talks), .4 * nrow(ted_talks), replace = FALSE)</span><span id="da7d" class="lt lu iq mu b gy nf nc l nd ne">train_dat &lt;- ted_talks[trainObs,]<br/>test_dat &lt;- ted_talks[testObs,]</span></pre><p id="0606" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我现在将所有预处理步骤应用于我的训练和测试数据(分别)。不知何故，我处于双重心态:是将 DTM 分成训练和测试，还是分割数据集，然后分别准备他们的 DTM。不知何故，我选择了后者。你可以试试前一种选择，如果它对你合适，请告诉我。</p><p id="d3ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我还处理了稀疏性，我在我的博客中详细讨论了这个问题。为了更直观，我还将目标变量重命名为“y ”,而不是 highest_rating。</p><pre class="kg kh ki kj gt mx mu my mz aw na bi"><span id="653f" class="lt lu iq mu b gy nb nc l nd ne">train_corpus &lt;- VCorpus(VectorSource(train_dat$transcript))</span><span id="033b" class="lt lu iq mu b gy nf nc l nd ne">##Removing Punctuation<br/>train_corpus &lt;- tm_map(train_corpus, content_transformer(removePunctuation))</span><span id="ee8a" class="lt lu iq mu b gy nf nc l nd ne">##Removing numbers<br/>train_corpus &lt;- tm_map(train_corpus, removeNumbers)</span><span id="fc34" class="lt lu iq mu b gy nf nc l nd ne">##Converting to lower case<br/>train_corpus &lt;- tm_map(train_corpus, content_transformer(tolower))</span><span id="92b4" class="lt lu iq mu b gy nf nc l nd ne">##Removing stop words<br/>train_corpus &lt;- tm_map(train_corpus, content_transformer(removeWords), stopwords(“english”))</span><span id="8624" class="lt lu iq mu b gy nf nc l nd ne">##Stemming<br/>train_corpus &lt;- tm_map(train_corpus, stemDocument)</span><span id="4a52" class="lt lu iq mu b gy nf nc l nd ne">##Whitespace<br/>train_corpus &lt;- tm_map(train_corpus, stripWhitespace)</span><span id="c712" class="lt lu iq mu b gy nf nc l nd ne"># Create Document Term Matrix<br/>dtm_train &lt;- DocumentTermMatrix(train_corpus)</span><span id="1419" class="lt lu iq mu b gy nf nc l nd ne">train_corpus &lt;- removeSparseTerms(dtm_train, 0.4)</span><span id="e42d" class="lt lu iq mu b gy nf nc l nd ne">dtm_train_matrix &lt;- as.matrix(train_corpus)<br/>dtm_train_matrix &lt;- cbind(dtm_train_matrix, train_dat$highest_rating)</span><span id="0584" class="lt lu iq mu b gy nf nc l nd ne">colnames(dtm_train_matrix)[ncol(dtm_train_matrix)] &lt;- “y”</span><span id="8970" class="lt lu iq mu b gy nf nc l nd ne">training_set_ted_talk &lt;- as.data.frame(dtm_train_matrix)<br/>training_set_ted_talk$y &lt;- as.factor(training_set_ted_talk$y)</span></pre><p id="5561" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经准备好了训练数据集，我们可以训练我们的模型了。我在 caret 中使用了<em class="mw"> caret </em>包和<em class="mw"> svmLinear3 </em>方法。<em class="mw"> svmLinear3 </em>为 SVM 的<em class="mw"> L2 正则化</em>提供<em class="mw">线性核</em>。同意，这是很多技术术语，我故意不在这里解释，因为这完全是另一个博客。同时，我会留下一些链接让你们了解<a class="ae kv" href="https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization" rel="noopener ugc nofollow" target="_blank"> <em class="mw"> L2 正则化</em> </a>，以及<a class="ae kv" href="https://www.svm-tutorial.com/2014/10/svm-linear-kernel-good-text-classification/" rel="noopener ugc nofollow" target="_blank"> <em class="mw">带线性核的 SVM</em></a>。</p><pre class="kg kh ki kj gt mx mu my mz aw na bi"><span id="fd04" class="lt lu iq mu b gy nb nc l nd ne">library(caret)</span><span id="6879" class="lt lu iq mu b gy nf nc l nd ne">review_ted_model &lt;- train(y ~., data = training_set_ted_talk, method = ‘svmLinear3’)</span><span id="063b" class="lt lu iq mu b gy nf nc l nd ne">Preparing our test data. It’s the same repetitive procedure.</span><span id="478e" class="lt lu iq mu b gy nf nc l nd ne">test_corpus &lt;- VCorpus(VectorSource(test_dat$transcript))</span><span id="3d78" class="lt lu iq mu b gy nf nc l nd ne">##Removing Punctuation<br/>test_corpus &lt;- tm_map(test_corpus, content_transformer(removePunctuation))</span><span id="ec7c" class="lt lu iq mu b gy nf nc l nd ne">##Removing numbers<br/>test_corpus &lt;- tm_map(test_corpus, removeNumbers)</span><span id="e4b3" class="lt lu iq mu b gy nf nc l nd ne">##Converting to lower case<br/>test_corpus &lt;- tm_map(test_corpus, content_transformer(tolower))</span><span id="3396" class="lt lu iq mu b gy nf nc l nd ne">##Removing stop words<br/>test_corpus &lt;- tm_map(test_corpus, content_transformer(removeWords), stopwords(“english”))</span><span id="52d2" class="lt lu iq mu b gy nf nc l nd ne">##Stemming<br/>test_corpus &lt;- tm_map(test_corpus, stemDocument)</span><span id="b5f2" class="lt lu iq mu b gy nf nc l nd ne">##Whitespace<br/>test_corpus &lt;- tm_map(test_corpus, stripWhitespace)</span><span id="f7ac" class="lt lu iq mu b gy nf nc l nd ne"># Create Document Term Matrix<br/>dtm_test &lt;- DocumentTermMatrix(test_corpus)</span><span id="3c10" class="lt lu iq mu b gy nf nc l nd ne">test_corpus &lt;- removeSparseTerms(dtm_test, 0.4)</span><span id="e8a8" class="lt lu iq mu b gy nf nc l nd ne">dtm_test_matrix &lt;- as.matrix(test_corpus)</span></pre><h2 id="d311" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">模型准确性和其他指标</h2><p id="2ff5" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">现在，我将根据测试数据检查我们的模型的准确性/性能。</p><pre class="kg kh ki kj gt mx mu my mz aw na bi"><span id="0684" class="lt lu iq mu b gy nb nc l nd ne">#Build the prediction <br/>model_ted_talk_result &lt;- predict(review_ted_model, newdata = dtm_test_matrix)</span><span id="386e" class="lt lu iq mu b gy nf nc l nd ne">check_accuracy &lt;- as.data.frame(cbind(prediction = model_ted_talk_result, rating = test_dat$highest_rating))</span><span id="7b17" class="lt lu iq mu b gy nf nc l nd ne">library(dplyr)<br/>check_accuracy &lt;- check_accuracy %&gt;% mutate(prediction = as.integer(prediction) — 1)</span><span id="eb12" class="lt lu iq mu b gy nf nc l nd ne">check_accuracy$accuracy &lt;- if_else(check_accuracy$prediction == check_accuracy$rating, 1, 0)<br/>round(prop.table(table(check_accuracy$accuracy)), 3)</span><span id="ee69" class="lt lu iq mu b gy nf nc l nd ne">library(performanceEstimation)<br/>classificationMetrics(as.integer(test_dat$highest_rating), model_ted_talk_result)</span><span id="8876" class="lt lu iq mu b gy nf nc l nd ne">most_common_misclassified_ratings = check_accuracy %&gt;% filter(check_accuracy$accuracy == 0) %&gt;%<br/> group_by(rating) %&gt;%<br/> summarise(Count = n()) %&gt;%<br/> arrange(desc(Count)) %&gt;%<br/> head(3)</span><span id="aa6f" class="lt lu iq mu b gy nf nc l nd ne">##Most commong missclassified rating<br/>levels(train_dat$highest_rating)[most_common_misclassified_ratings$rating]</span></pre><p id="1262" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型指标包括:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/8ff0953b458d1151b09d27d33845329f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*grXwT8Zu1qiZKcrxCv8AXw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Model metrics</figcaption></figure><p id="5b0c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">前三个最常见的<strong class="ky ir">错误分类</strong>评级是:“鼓舞人心的”、“信息丰富的”、“引人入胜的”。你可以从<a class="ae kv" href="https://blog.revolutionanalytics.com/2016/03/com_class_eval_metrics_r.html#macro" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae kv" href="https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001" rel="noopener ugc nofollow" target="_blank">这里</a>阅读更多关于<em class="mw">微观</em>和<em class="mw">宏观 F1 分数</em>。</p><h2 id="40e2" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">结束语</h2><p id="2445" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">在本文中，我们讨论了文本的多类分类。对我来说，解决这个数据问题相当具有挑战性，主要是因为:</p><ul class=""><li id="7e09" class="nh ni iq ky b kz la lc ld lf nj lj nk ln nl lr nm nn no np bi translated">我现在只处理二元分类问题</li><li id="e4a0" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr nm nn no np bi translated">数据操作步骤导致每个单独的评级列的 13 个类。单引号破坏了看起来完美的 JSON 专栏。在没有任何直觉的情况下，找出单引号引起的问题是非常困难的。我最终设法用 gsub 解析了它。</li></ul><p id="07ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，上面的模型分数严重依赖于我们在训练方法中使用的方法。我使用线性核 SVM 和 L2 正则化，这本身是计算量很大。您可以尝试其他方法，但是计算资源可能是一个问题。请让我知道你用的其他方法和你得到的分数(准确性和 F1)。</p><p id="ec0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">目前就这些。我将在下一篇博客中讨论一个与 NLP 相关的新任务。我也写过其他与软件工程相关的<a class="ae kv" href="https://medium.com/@shubhanshugupta/engineering-challenges-of-streaming-a-million-concurrent-json-data-streams-from-product-to-crm-360506b29aca" rel="noopener">帖子</a>。你可能想在这里查看它们<a class="ae kv" href="https://shubhanshugupta.com/blog/" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>