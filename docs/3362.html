<html>
<head>
<title>Gated Multimodal Units for Information Fusion</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于信息融合的门控多通道单元</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gated-multimodal-units-for-information-fusion-966a9a2e1c54?source=collection_archive---------13-----------------------#2018-05-05">https://towardsdatascience.com/gated-multimodal-units-for-information-fusion-966a9a2e1c54?source=collection_archive---------13-----------------------#2018-05-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/36c8586bc739ad4b30312523b2447665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ArH-yLJVuNzRB7WpJ81fJQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">The output of the GMU architecture</figcaption></figure><p id="4c48" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">深度学习已经在许多领域证明了它的优越性，在各种各样的任务中，例如图像分类和文本生成。处理涉及来自多个模态的输入的任务是一个有趣的研究领域。</p><p id="f7ea" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">门控多模式单元(GMU)是由最近的一篇论文提出的一种新的构建模块，该论文在 2017 年 ICLR 世博会上以研讨会的形式展示。这个构建模块的目标是以一种智能的方式融合来自多个不同模态的信息。</p><p id="af4c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在这篇文章中，我将描述 GMU，并说明它如何在玩具数据集上工作。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="4402" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">建筑</h1><p id="8973" class="pw-post-body-paragraph kf kg it kh b ki mj kk kl km mk ko kp kq ml ks kt ku mm kw kx ky mn la lb lc im bi translated">给定不同模态的两种表示，xᵛ和 xᵗ(例如视觉和文本模态)，GMU 块执行一种形式的自我注意</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/4c42c68036371201214d5309d626ff3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*xZ1eEC4F0gUMt5sXjPDYYQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">GMU architecture</figcaption></figure><p id="861f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">描述 GMU 的方程相对简单:</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/3a98b98fb03f66117cab7ea45a59b8ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*szQdMWct27XNAyMIvQGGiA.png"/></div></figure><p id="6ec7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">(1) + (2)将表示转换成不同的表示，然后根据在(3)中计算的<em class="mu"> z </em>在(4)中参与。因为 z 是 xᵛ和 xᵗ的函数，这意味着我们在处理自我关注机制。</p><p id="a993" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">GMU 背后的直觉是，它使用表示本身来理解哪个模态应该影响预测。考虑预测一个被拍照者的性别的任务，伴随着他的声音记录。如果给定示例的记录噪声太大，模型应该学会仅使用该示例中的图像。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="1f46" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">综合数据</h1><p id="b3b8" class="pw-post-body-paragraph kf kg it kh b ki mj kk kl km mk ko kp kq ml ks kt ku mm kw kx ky mn la lb lc im bi translated">在论文中，他们描述了一个很好的合成数据集，展示了 GMU 是如何工作的。</p><p id="6842" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在这里，我们将实现相同的数据集，并自己找出 GMU 是否真的有效(剧透:它有效)。</p><p id="1697" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">首先，让我们做导入:</p><figure class="mp mq mr ms gt ju"><div class="bz fp l di"><div class="mv mw l"/></div></figure><h1 id="58e9" class="ll lm it bd ln lo mx lq lr ls my lu lv lw mz ly lz ma na mc md me nb mg mh mi bi translated">生成数据</h1><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nc"><img src="../Images/9481a6ff803129fa0a64e70d15f2bccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdGkJPO3_kBOtheYfVLP0A.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Data generating process</figcaption></figure><p id="6d6a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">不要被这个图表吓到——稍后您会发现这个图表生成的数据的可视化。</p><p id="13f0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">基本上图中所说的是目标类<em class="mu"> C </em>描述了模态 yᵛ和 yᵗ的值——当然带有一些随机性。</p><p id="3bfc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在下一步中，随机变量<em class="mu"> M </em>决定忽略 yᵗyᵛ的哪个输入，而是使用 ŷᵗ.ŷᵛ的噪声源</p><p id="01aa" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">最终，xᵛ和 xᵗ要么包含了能够描述目标类<em class="mu"> C </em>的真实信息源，要么包含了随机噪声。</p><p id="3bbf" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">GMU 块的目标是成功地找出给定特定示例的哪个来源是信息来源，并对该来源给予全部关注。</p><figure class="mp mq mr ms gt ju"><div class="bz fp l di"><div class="mv mw l"/></div></figure><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nd"><img src="../Images/2331abeb3a14e145579429a325d8641c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TX61Ix8vQn4hDctwYVq2CA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Synthetic data. Color denotes the class C</figcaption></figure></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="5ebc" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">创建模型</h1><p id="5adb" class="pw-post-body-paragraph kf kg it kh b ki mj kk kl km mk ko kp kq ml ks kt ku mm kw kx ky mn la lb lc im bi translated">我将实现 GMU 的一个基本版本——只是为了让它更容易理解。</p><p id="bffc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">将代码一般化以处理两种以上的模态是很简单的。</p><figure class="mp mq mr ms gt ju"><div class="bz fp l di"><div class="mv mw l"/></div></figure><h1 id="20fc" class="ll lm it bd ln lo mx lq lr ls my lu lv lw mz ly lz ma na mc md me nb mg mh mi bi translated">训练模型</h1><figure class="mp mq mr ms gt ju"><div class="bz fp l di"><div class="mv mw l"/></div></figure><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ne"><img src="../Images/1585902d1762c2b19c7758aaa1bda6c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6F8kMTjREhSP-EGZRBMPcw.png"/></div></div></figure><h1 id="fc34" class="ll lm it bd ln lo mx lq lr ls my lu lv lw mz ly lz ma na mc md me nb mg mh mi bi translated">检查结果</h1><p id="01b1" class="pw-post-body-paragraph kf kg it kh b ki mj kk kl km mk ko kp kq ml ks kt ku mm kw kx ky mn la lb lc im bi translated">损失看起来不错。</p><p id="dcfc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们看看 z 和预测是什么样子的。论文中还出现了以下可视化内容。</p><figure class="mp mq mr ms gt ju"><div class="bz fp l di"><div class="mv mw l"/></div></figure><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nf"><img src="../Images/ea34cedf6b5c9947bb400f101a28a6a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*76dsLxEGtn5fSOmawcvc6A.png"/></div></div></figure><p id="5bd9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们可以看到<em class="mu"> z </em>的行为完全如我们所愿(左图)。它的好处在于，实际上只使用一种模态就可以预测远离边界线的点类。这意味着模型学会了何时忽略包含纯无预测噪声的模态。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="94be" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">为什么不用简单的 FF(前馈)？</h1><p id="c9a2" class="pw-post-body-paragraph kf kg it kh b ki mj kk kl km mk ko kp kq ml ks kt ku mm kw kx ky mn la lb lc im bi translated">如果我们忽略数据生成过程，只看数据点，显然有 4 个不同的集群。</p><p id="2793" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这些集群不是线性可分的。虽然 GMU 为模型提供了容量以解释这种非线性行为，但人们可以改为向混合物中添加另一层，从而用简单的前馈(FF)网络解决问题。</p><blockquote class="ng"><p id="4367" class="nh ni it bd nj nk nl nm nn no np lc dk translated"><em class="nq">普适逼近定理指出，一个包含有限个神经元的单隐层前馈网络，可以逼近连续函数……(</em><a class="ae ld" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank"><em class="nq">维基</em> </a> <em class="nq"> ) </em></p></blockquote><p id="7879" class="pw-post-body-paragraph kf kg it kh b ki nr kk kl km ns ko kp kq nt ks kt ku nu kw kx ky nv la lb lc im bi translated">因此，实际上，对于这个人为的例子，一个简单的 FF 就可以完成这项工作。然而，引入新架构(在这种情况下是 GMU)的目的是引入归纳偏差，允许训练过程利用我们对问题的先验知识。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="a75d" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">结论</h1><p id="8783" class="pw-post-body-paragraph kf kg it kh b ki mj kk kl km mk ko kp kq ml ks kt ku mm kw kx ky mn la lb lc im bi translated">对于涉及多模态的真实世界问题，作者声称 GMU 实现了优越的性能。他们使用基于情节和海报识别电影类型的任务来展示他们的方法。</p><p id="dcc4" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">GMU 很容易实现，如果您需要训练一个模型使用多种模态作为输入，将它放在您的工具箱中是值得的。为此，您可以为每个主机创建一个子网。子网络不必相同——例如，你可以用 CNN 做视觉模态，用 LSTM 做文本模态。重要的是每个子网络输出其模态的密集表示。然后，将这些表示送入 GMU 块，以便将信息融合成一个表示。融合的表示将被馈送到另一个子网络，其输出将是最终的预测。</p><p id="4b59" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="mu">本帖原帖</em><a class="ae ld" href="http://www.anotherdatum.com" rel="noopener ugc nofollow" target="_blank"><em class="mu">www.anotherdatum.com</em></a><em class="mu">。</em></p></div></div>    
</body>
</html>