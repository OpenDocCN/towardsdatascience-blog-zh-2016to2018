<html>
<head>
<title>Data Science Skills: Web scraping using python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学技能:使用 python 进行网络搜集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-science-skills-web-scraping-using-python-d1a85ef607ed?source=collection_archive---------0-----------------------#2018-09-06">https://towardsdatascience.com/data-science-skills-web-scraping-using-python-d1a85ef607ed?source=collection_archive---------0-----------------------#2018-09-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/e2999fd4adf340a88228f10930a4b599.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d7UE4B96x_fs1ezUx8YfBQ.png"/></div></div></figure><div class=""/><p id="49fc" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">作为一名数据科学家，我在工作中接受的第一批任务之一就是网络搜集。这在当时对我来说是一个完全陌生的概念，使用代码从网站收集数据，但这是最符合逻辑和最容易获得的数据来源之一。经过几次尝试，网页抓取已经成为我的第二天性，也是我几乎每天都要使用的技能之一。</p><p id="ef71" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本教程中，我将通过一个简单的例子来说明如何从<a class="ae kw" href="http://www.fasttrack.co.uk/" rel="noopener ugc nofollow" target="_blank">快速通道</a>中抓取一个网站来收集 2018 年前 100 家公司的数据。使用 web scraper 自动完成这一过程避免了手动收集数据，节省了时间，还允许您将所有公司数据保存在一个结构化文件中。</p><h2 id="27ec" class="kx ky jb bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated"><strong class="ak">TL；关于 python 中一个简单的 web scraper 的快速例子，你可以在 GitHub 上找到本教程中的完整代码。</strong></h2><h1 id="4a14" class="lq ky jb bd kz lr ls lt lc lu lv lw lf lx ly lz li ma mb mc ll md me mf lo mg bi translated">入门指南</h1><p id="d6ca" class="pw-post-body-paragraph jy jz jb ka b kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ml kt ku kv ij bi translated">在开始使用任何 python 应用程序之前，首先要问的问题是“我需要哪些库？”</p><p id="4429" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于 web 抓取，有几个不同的库可以考虑，包括:</p><ul class=""><li id="505d" class="mm mn jb ka b kb kc kf kg kj mo kn mp kr mq kv mr ms mt mu bi translated">美味的汤</li><li id="a61e" class="mm mn jb ka b kb mv kf mw kj mx kn my kr mz kv mr ms mt mu bi translated">要求</li><li id="5592" class="mm mn jb ka b kb mv kf mw kj mx kn my kr mz kv mr ms mt mu bi translated">Scrapy</li><li id="b00f" class="mm mn jb ka b kb mv kf mw kj mx kn my kr mz kv mr ms mt mu bi translated">硒</li></ul><p id="e97b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这个例子中，我们将使用美丽的汤。使用 Python 包管理器<code class="fe na nb nc nd b">pip</code>，您可以用以下代码安装 Beautiful Soup:</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="b531" class="kx ky jb nd b gy nm nn l no np">pip install <!-- -->BeautifulSoup4</span></pre><p id="2681" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">安装好这些库之后，让我们开始吧！</p><h1 id="06ca" class="lq ky jb bd kz lr ls lt lc lu lv lw lf lx ly lz li ma mb mc ll md me mf lo mg bi translated">检查网页</h1><p id="3a7d" class="pw-post-body-paragraph jy jz jb ka b kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ml kt ku kv ij bi translated">要知道 python 代码中需要哪些元素，首先需要检查网页。</p><p id="1ffa" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要从<a class="ae kw" href="http://www.fasttrack.co.uk/league-tables/tech-track-100/league-table/" rel="noopener ugc nofollow" target="_blank">技术跟踪 100 强公司</a>收集数据，您可以通过右键单击感兴趣的元素并选择 inspect 来检查页面。这将显示 HTML 代码，从中我们可以看到包含每个字段的元素。</p><div class="ne nf ng nh gt ab cb"><figure class="nq is nr ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/856fbe17e6eb336c4590708818dde0a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*bnAgwIK0qH4tE49im7mYHA.png"/></div></figure><figure class="nq is nr ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/e7000c303b1cbca48308f5204e463cc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*JAWO1QTCEzhgPI6njZFKLg.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk oa di ob oc">Right click on the element you are interested in and select ‘Inspect’, this brings up the html elements</figcaption></figure></div><p id="f475" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因为数据存储在一个表中，所以只需几行代码就可以轻松完成。如果你想熟悉抓取网站，这是一个很好的例子，也是一个很好的起点，但是要记住，事情不会总是这么简单！</p><p id="0181" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所有 100 个结果都包含在<code class="fe na nb nc nd b">&lt;tr&gt;</code>元素的行中，这些结果都可以在一个页面上看到。情况并非总是如此，当结果跨越多个页面时，您可能需要更改网页上显示的结果数量，或者循环所有页面以收集所有信息。</p><p id="4514" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在排行榜网页上，显示了包含 100 个结果的表格。当检查页面时，很容易看到 html 中的模式。结果包含在表的行中:</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="10fb" class="kx ky jb nd b gy nm nn l no np">&lt;table class="tableSorter"&gt;</span></pre><p id="23b3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">重复的行<code class="fe na nb nc nd b">&lt;tr&gt;</code>将通过使用 python 中的循环查找数据并写入文件来保持我们的代码最少！</p><p id="c1b4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="od">补充说明:另一个可以完成的检查是检查网站上是否发出了 HTTP GET 请求，该请求可能已经以结构化响应(如 JSON 或 XML 格式)的形式返回了结果。您可以在 inspect tools 的 network 选项卡中检查这一点，通常在 XHR 选项卡中。页面刷新后，将显示加载的请求，如果响应包含格式化的结构，使用 REST 客户端(如</em> <a class="ae kw" href="https://insomnia.rest/" rel="noopener ugc nofollow" target="_blank"> <em class="od">失眠症</em> </a> <em class="od">)返回输出通常会更容易。</em></p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oe"><img src="../Images/48a809c7b38d62c7f53be0822d448e88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ymjQ44VpEL_gyQfBfp7Tw.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">Network tab of the page inspect tool after refreshing the webpage</figcaption></figure><h1 id="422c" class="lq ky jb bd kz lr ls lt lc lu lv lw lf lx ly lz li ma mb mc ll md me mf lo mg bi translated">使用美汤解析网页 html</h1><p id="833c" class="pw-post-body-paragraph jy jz jb ka b kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ml kt ku kv ij bi translated">现在你已经看到了 html 的结构，并且熟悉了你所抓取的内容，是时候开始使用 python 了！</p><p id="74de" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第一步是导入将用于 web scraper 的库。我们已经在上面谈到了 BeautifulSoup，它帮助我们处理 html。我们要导入的下一个库是<code class="fe na nb nc nd b">urllib</code>,它连接到网页。最后，我们将把输出写到一个 csv 文件中，所以我们还需要导入<code class="fe na nb nc nd b">csv</code>库。作为替代，这里可以使用<code class="fe na nb nc nd b">json</code>库。</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="bb4b" class="kx ky jb nd b gy nm nn l no np"># import libraries<br/>from bs4 import BeautifulSoup<br/>import urllib.request<br/>import csv</span></pre><p id="20a0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下一步是定义您正在抓取的 url。如前所述，此网页在一页上显示所有结果，因此地址栏中的完整 url 在此给出。</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="5ee0" class="kx ky jb nd b gy nm nn l no np"># specify the url<br/>urlpage =  '<a class="ae kw" href="http://www.fasttrack.co.uk/league-tables/tech-track-100/league-table/'" rel="noopener ugc nofollow" target="_blank">http://www.fasttrack.co.uk/league-tables/tech-track-100/league-table/'</a></span></pre><p id="0394" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，我们连接到网页，我们可以使用 BeautifulSoup 解析 html，将对象存储在变量“Soup”中。</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="a3e8" class="kx ky jb nd b gy nm nn l no np"># query the website and return the html to the variable 'page'<br/>page = urllib.request.urlopen(urlpage)<br/># parse the html using beautiful soup and store in variable 'soup'<br/>soup = BeautifulSoup(page, 'html.parser')</span></pre><p id="9208" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以在这个阶段打印 soup 变量，它将返回我们所请求的网页的完整解析的 html。</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="c527" class="kx ky jb nd b gy nm nn l no np">print(soup)</span></pre><p id="e72a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果有错误或变量为空，则请求可能没有成功。此时，您可能希望使用<code class="fe na nb nc nd b"><a class="ae kw" href="https://docs.python.org/3/library/urllib.error.html" rel="noopener ugc nofollow" target="_blank">urllib.error</a></code>模块实现错误处理。</p><h1 id="4ab3" class="lq ky jb bd kz lr ls lt lc lu lv lw lf lx ly lz li ma mb mc ll md me mf lo mg bi translated">搜索 html 元素</h1><p id="5150" class="pw-post-body-paragraph jy jz jb ka b kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ml kt ku kv ij bi translated">因为所有的结果都包含在一个表中，所以我们可以使用<code class="fe na nb nc nd b">find</code>方法搜索表的 soup 对象。然后我们可以使用<code class="fe na nb nc nd b">find_all</code>方法找到表中的每一行。</p><p id="0385" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果我们打印行数，结果应该是 101，这 100 行加上标题。</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="fb05" class="kx ky jb nd b gy nm nn l no np"># find results within table<br/>table = soup.find('table', attrs={'class': 'tableSorter'})<br/>results = table.find_all('tr')<br/>print('Number of results', len(results))</span></pre><p id="769a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，我们可以循环结果来收集数据。</p><p id="5e68" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">打印 soup 对象中的前 2 行，我们可以看到每一行的结构是:</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="b20b" class="kx ky jb nd b gy nm nn l no np">&lt;tr&gt;<br/>&lt;th&gt;Rank&lt;/th&gt;<br/>&lt;th&gt;Company&lt;/th&gt;<br/>&lt;th class=""&gt;Location&lt;/th&gt;<br/>&lt;th class="no-word-wrap"&gt;Year end&lt;/th&gt;<br/>&lt;th class="" style="text-align:right;"&gt;Annual sales rise over 3 years&lt;/th&gt;<br/>&lt;th class="" style="text-align:right;"&gt;Latest sales £000s&lt;/th&gt;<br/>&lt;th class="" style="text-align:right;"&gt;Staff&lt;/th&gt;<br/>&lt;th class=""&gt;Comment&lt;/th&gt;<br/>&lt;!--                            &lt;th&gt;FYE&lt;/th&gt;--&gt;<br/>&lt;/tr&gt;<br/>&lt;tr&gt;<br/>&lt;td&gt;1&lt;/td&gt;<br/>&lt;td&gt;&lt;a href="<a class="ae kw" href="http://www.fasttrack.co.uk/company_profile/wonderbly-3/" rel="noopener ugc nofollow" target="_blank">http://www.fasttrack.co.uk/company_profile/wonderbly-3/</a>"&gt;&lt;span class="company-name"&gt;Wonderbly&lt;/span&gt;&lt;/a&gt;Personalised children's books&lt;/td&gt;<br/>&lt;td&gt;East London&lt;/td&gt;<br/>&lt;td&gt;Apr-17&lt;/td&gt;<br/>&lt;td style="text-align:right;"&gt;294.27%&lt;/td&gt;<br/>&lt;td style="text-align:right;"&gt;*25,860&lt;/td&gt;<br/>&lt;td style="text-align:right;"&gt;80&lt;/td&gt;<br/>&lt;td&gt;Has sold nearly 3m customisable children’s books in 200 countries&lt;/td&gt;<br/>&lt;!--                                            &lt;td&gt;Apr-17&lt;/td&gt;--&gt;<br/>&lt;/tr&gt;</span></pre><p id="115f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">表格中有 8 列，包括:排名、公司、地点、年末、年销售额上升、最新销售额、员工和评论，所有这些都是我们可以保存的有趣数据。</p><p id="18a5" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这种结构在网页的所有行中都是一致的(并非所有网站都是如此！)，因此我们可以再次使用<code class="fe na nb nc nd b">find_all</code>方法将每一列分配给一个变量，我们可以通过搜索<code class="fe na nb nc nd b">&lt;td&gt;</code>元素将该变量写入 csv 或 JSON。</p><h1 id="28e8" class="lq ky jb bd kz lr ls lt lc lu lv lw lf lx ly lz li ma mb mc ll md me mf lo mg bi translated">遍历元素并保存变量</h1><p id="a467" class="pw-post-body-paragraph jy jz jb ka b kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ml kt ku kv ij bi translated">在 python 中，将结果附加到列表中，然后将数据写入文件，这很有用。我们应该在循环之前声明列表并设置 csv 的头，如下所示:</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="3139" class="kx ky jb nd b gy nm nn l no np"># create and write headers to a list <br/>rows = []<br/>rows.append(['Rank', 'Company Name', 'Webpage', 'Description', 'Location', 'Year end', 'Annual sales rise over 3 years', 'Sales £000s', 'Staff', 'Comments'])<br/>print(rows)</span></pre><p id="f51b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这将打印出我们添加到包含标题的列表中的第一行。</p><p id="8723" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可能会注意到有几个额外的字段<code class="fe na nb nc nd b">Webpage</code>和<code class="fe na nb nc nd b">Description</code>，它们不是表中的列名，但是如果您仔细查看我们打印上面的 soup 变量时的 html，第二行包含的不仅仅是公司名称。我们可以使用一些进一步的提取来获得这些额外的信息。</p><p id="3b27" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下一步是遍历结果，处理数据并添加到可以写入 csv 的<code class="fe na nb nc nd b">rows </code>。</p><p id="ff5f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要在循环中查找结果:</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="11b1" class="kx ky jb nd b gy nm nn l no np"># loop over results<br/>for result in results:<br/>    # find all columns per result<br/>    data = result.find_all('td')<br/>    # check that columns have data <br/>    if len(data) == 0: <br/>        continue</span></pre><p id="0a26" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于表中的第一行只包含标题，我们可以跳过这个结果，如上所示。它也不包含任何<code class="fe na nb nc nd b">&lt;td&gt;</code>元素，因此当搜索该元素时，不会返回任何内容。然后，我们可以通过要求数据长度不为零来检查是否只处理包含数据的结果。</p><p id="f470" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后我们可以开始处理数据并保存到变量中。</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="7991" class="kx ky jb nd b gy nm nn l no np">    # write columns to variables<br/>    rank = data[0].getText()<br/>    company = data[1].getText()<br/>    location = data[2].getText()<br/>    yearend = data[3].getText()<br/>    salesrise = data[4].getText()<br/>    sales = data[5].getText()<br/>    staff = data[6].getText()<br/>    comments = data[7].getText()</span></pre><p id="6db6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">上面的代码只是从每一列中获取文本，并保存到变量中。然而，这些数据中的一些需要进一步清理，以移除不需要的字符或提取进一步的信息。</p><h1 id="7318" class="lq ky jb bd kz lr ls lt lc lu lv lw lf lx ly lz li ma mb mc ll md me mf lo mg bi translated">数据清理</h1><p id="dccd" class="pw-post-body-paragraph jy jz jb ka b kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ml kt ku kv ij bi translated">如果我们打印出变量<code class="fe na nb nc nd b">company</code>，文本不仅包含公司名称，还包含描述。如果我们打印出<code class="fe na nb nc nd b">sales</code>，它包含了不需要的字符，比如脚注符号，删除这些字符会很有用。</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="4c19" class="kx ky jb nd b gy nm nn l no np">    print('Company is', company)<br/>    # Company is WonderblyPersonalised children's books          <br/>    print('Sales', sales)<br/>    # Sales *25,860</span></pre><p id="5083" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们想把<code class="fe na nb nc nd b">company</code>分成公司名称和描述，这可以用几行代码来完成。再次查看 html，对于该列，有一个仅包含公司名称的<code class="fe na nb nc nd b">&lt;span&gt;</code>元素。本专栏中还有一个链接，链接到网站上的另一个页面，该页面有关于该公司的更多详细信息。我们稍后会用到它！</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="8fc7" class="kx ky jb nd b gy nm nn l no np">&lt;td&gt;&lt;a href="<a class="ae kw" href="http://www.fasttrack.co.uk/company_profile/wonderbly-3/" rel="noopener ugc nofollow" target="_blank">http://www.fasttrack.co.uk/company_profile/wonderbly-3/</a>"&gt;&lt;span class="company-name"&gt;Wonderbly&lt;/span&gt;&lt;/a&gt;Personalised children's books&lt;/td&gt;</span></pre><p id="729c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了将<code class="fe na nb nc nd b">company</code>分成两个字段，我们可以使用<code class="fe na nb nc nd b">find</code>方法保存<code class="fe na nb nc nd b">&lt;span&gt;</code>元素，然后使用<code class="fe na nb nc nd b">strip</code>或<code class="fe na nb nc nd b">replace</code>从<code class="fe na nb nc nd b">company</code>变量中删除公司名称，这样就只剩下描述了。<br/>为了从<code class="fe na nb nc nd b">sales</code>中删除不想要的字符，我们可以再次使用<code class="fe na nb nc nd b">strip</code>和<code class="fe na nb nc nd b">replace</code>方法！</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="0f3f" class="kx ky jb nd b gy nm nn l no np">    # extract description from the name<br/>    companyname = data[1].find('span', attrs={'class':'company-name'}).getText()    <br/>    description = company.replace(companyname, '')<br/>    <br/>    # remove unwanted characters<br/>    sales = sales.strip('*').strip('†').replace(',','')</span></pre><p id="9314" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们希望保存的最后一个变量是公司网站。如上所述，第二列包含到另一个页面的链接，该页面包含每个公司的概述。每个公司页面都有自己的表格，大部分时间都包含公司网站。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oe"><img src="../Images/aa311c827bc9eb3e8dada8b47fd0dcad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BDxmEE0Vka_Dqq_78i_X7w.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">Inspecting the element of the url on the company page</figcaption></figure><p id="c379" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要从每个表中抓取 url 并将其保存为变量，我们需要使用与上面相同的步骤:</p><ul class=""><li id="f186" class="mm mn jb ka b kb kc kf kg kj mo kn mp kr mq kv mr ms mt mu bi translated">在快速通道网站上查找包含公司页面 url 的元素</li><li id="9036" class="mm mn jb ka b kb mv kf mw kj mx kn my kr mz kv mr ms mt mu bi translated">向每个公司页面 url 发出请求</li><li id="5f7a" class="mm mn jb ka b kb mv kf mw kj mx kn my kr mz kv mr ms mt mu bi translated">使用 Beautifulsoup 解析 html</li><li id="3bba" class="mm mn jb ka b kb mv kf mw kj mx kn my kr mz kv mr ms mt mu bi translated">找到感兴趣的元素</li></ul><p id="f4ed" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">查看一些公司页面，如上面的截图所示，URL 位于表格的最后一行，因此我们可以在最后一行中搜索<code class="fe na nb nc nd b">&lt;a&gt;</code>元素。</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="4449" class="kx ky jb nd b gy nm nn l no np">    # go to link and extract company website<br/>    url = data[1].find('a').get('href')<br/>    page = urllib.request.urlopen(url)<br/>    # parse the html <br/>    soup = BeautifulSoup(page, 'html.parser')<br/>    # find the last result in the table and get the link<br/>    try:<br/>        tableRow = soup.find('table').find_all('tr')[-1]<br/>        webpage = tableRow.find('a').get('href')<br/>    except:<br/>        webpage = None</span></pre><p id="7d00" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">也可能有公司网站不显示的情况，因此我们可以使用<code class="fe na nb nc nd b">try</code> <code class="fe na nb nc nd b">except</code>条件，以防找不到 url。</p><p id="933c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦我们将所有数据保存到变量中，仍然在循环中，我们可以将每个结果添加到列表<code class="fe na nb nc nd b">rows</code>。</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="aba3" class="kx ky jb nd b gy nm nn l no np">    # write each result to rows<br/>    rows.append([rank, companyname, webpage, description, location, yearend, salesrise, sales, staff, comments])</span><span id="0bc0" class="kx ky jb nd b gy of nn l no np">print(rows)</span></pre><p id="58af" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后在循环之外打印变量是很有用的，在把它写到文件之前，检查它看起来是否像你期望的那样！</p><h1 id="484e" class="lq ky jb bd kz lr ls lt lc lu lv lw lf lx ly lz li ma mb mc ll md me mf lo mg bi translated">写入输出文件</h1><p id="e7f5" class="pw-post-body-paragraph jy jz jb ka b kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ml kt ku kv ij bi translated">您可能希望保存这些数据以供分析，这可以在 python 中从我们的列表中非常简单地完成。</p><pre class="ne nf ng nh gt ni nd nj nk aw nl bi"><span id="f206" class="kx ky jb nd b gy nm nn l no np"># Create csv and write rows to output file<br/>with open('techtrack100.csv','w', newline='') as f_output:<br/>    csv_output = csv.writer(f_output)<br/>    csv_output.writerows(rows)</span></pre><p id="92cf" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">运行 python 脚本时，将生成包含 100 行结果的输出文件，您可以进一步查看详细信息！</p><h1 id="60ef" class="lq ky jb bd kz lr ls lt lc lu lv lw lf lx ly lz li ma mb mc ll md me mf lo mg bi translated">摘要</h1><p id="10ca" class="pw-post-body-paragraph jy jz jb ka b kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ml kt ku kv ij bi translated">这篇关于使用 python 进行 web 抓取的简短教程概述了:</p><ul class=""><li id="7a7c" class="mm mn jb ka b kb kc kf kg kj mo kn mp kr mq kv mr ms mt mu bi translated">连接到网页</li><li id="3cdb" class="mm mn jb ka b kb mv kf mw kj mx kn my kr mz kv mr ms mt mu bi translated">使用 BeautifulSoup 解析 html</li><li id="4479" class="mm mn jb ka b kb mv kf mw kj mx kn my kr mz kv mr ms mt mu bi translated">遍历 soup 对象以查找元素</li><li id="34a4" class="mm mn jb ka b kb mv kf mw kj mx kn my kr mz kv mr ms mt mu bi translated">执行一些简单的数据清理</li><li id="f534" class="mm mn jb ka b kb mv kf mw kj mx kn my kr mz kv mr ms mt mu bi translated">将数据写入 csv</li></ul><p id="fb6f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是我的第一个教程，所以让我知道你是否有任何问题或意见，如果事情不清楚！</p></div><div class="ab cl og oh hu oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ij ik il im in"><p id="183e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">感谢您的阅读！如果你喜欢我的文章，那么<a class="ae kw" href="https://kaparker.substack.com/" rel="noopener ugc nofollow" target="_blank">订阅</a>我的每月简讯，在那里你可以将我的最新文章和顶级资源直接发送到你的收件箱！</p><p id="f270" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你可以在<a class="ae kw" href="https://medium.com/@_kaparker" rel="noopener"> Medium </a>上关注我以获取更多文章，在<a class="ae kw" href="https://twitter.com/_kaparker" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注我，或者在我的<a class="ae kw" href="http://kaparker.com/" rel="noopener ugc nofollow" target="_blank">网站</a>上了解更多我正在做的事情。</p></div></div>    
</body>
</html>