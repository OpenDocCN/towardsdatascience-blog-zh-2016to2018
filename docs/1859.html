<html>
<head>
<title>Understanding objective functions in neural networks.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解神经网络中的目标函数。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-objective-functions-in-neural-networks-d217cb068138?source=collection_archive---------1-----------------------#2017-11-04">https://towardsdatascience.com/understanding-objective-functions-in-neural-networks-d217cb068138?source=collection_archive---------1-----------------------#2017-11-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/3f4c003d1c4bd37b257c6931a2cfcf0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_PAQiO2ghxCeKKpR0dguKA.jpeg"/></div></div></figure><p id="99f7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这篇博文的目标读者是有机器学习经验的人，他们希望对用于训练神经网络的不同目标函数有更好的直觉。</p><h2 id="8868" class="kw kx iq bd ky kz la dn lb lc ld dp le kj lf lg lh kn li lj lk kr ll lm ln lo bi translated">介绍</h2><p id="697a" class="pw-post-body-paragraph jy jz iq ka b kb lp kd ke kf lq kh ki kj lr kl km kn ls kp kq kr lt kt ku kv ij bi translated">我决定写这篇博客的原因有三点:</p><ul class=""><li id="af38" class="lu lv iq ka b kb kc kf kg kj lw kn lx kr ly kv lz ma mb mc bi translated">博客帖子经常解释优化方法，如随机梯度下降或其<a class="ae md" href="http://ruder.io/optimizing-gradient-descent/" rel="noopener ugc nofollow" target="_blank">变体</a>，但很少花时间解释如何为神经网络构建目标函数。为什么均方误差(MSE)和交叉熵对数损失被用作resp的目标函数。回归分类？为什么添加一个正则化术语有意义？总的想法是，通过研究目标函数，人们可以了解为什么神经网络以它们的方式工作，或者为什么它们在其他情况下失败。</li></ul><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/cf3c4ea164b737298d68bafd29aa5106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gNuP7PN6sC42vAYWvoAMMA.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Cross entropy log loss between the ground truth p and network output q, used in classification problems.</figcaption></figure><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/88979c30a08ce090ab2dd3c820805a26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/0*CRZU7qETwW3bwBwK.gif"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Mean squared error between the ground truth y and network output y_tilde, used in regression problems.</figcaption></figure><ul class=""><li id="3d44" class="lu lv iq ka b kb kc kf kg kj lw kn lx kr ly kv lz ma mb mc bi translated">神经网络以提供糟糕的概率估计而闻名，并且它们受到<a class="ae md" href="https://blog.openai.com/adversarial-example-research/" rel="noopener ugc nofollow" target="_blank">反面例子</a>的困扰。简而言之:神经网络往往高度自信，即使它们是错误的。当它们部署在现实生活场景中时(例如自动驾驶汽车)，这可能是一个问题。自动驾驶汽车在90英里/小时的速度下做决定时应该是确定的。如果我们部署深度学习管道，我们应该知道它们的优势和劣势。</li><li id="1378" class="lu lv iq ka b kb mo kf mp kj mq kn mr kr ms kv lz ma mb mc bi translated">我一直想知道神经网络如何从概率的角度进行解释，以及它们如何适应更广泛的机器学习模型框架。人们倾向于用概率来谈论网络输出。神经网络的概率解释和它们的目标函数之间有联系吗？</li></ul><p id="e33c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这篇博文的主要灵感来自于我和我的朋友<a class="ae md" href="http://www.briantrippe.com" rel="noopener ugc nofollow" target="_blank">布莱恩·特里普</a>在剑桥大学<a class="ae md" href="http://learning.eng.cam.ac.uk/Public/" rel="noopener ugc nofollow" target="_blank">计算和生物学习实验室</a>所做的关于贝叶斯神经网络的工作。我强烈推荐任何人阅读布莱恩关于神经网络中变分推理的<a class="ae md" href="http://briantrippe.com/MPhilThesisSubmission.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><p id="d037" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">免责声明</strong>:在计算和生物学习实验室，贝叶斯机器学习技术被毫无歉意地作为未来的方向教授。因此，请注意这篇博文中的潜在偏见(😉).</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h2 id="6d77" class="kw kx iq bd ky kz la dn lb lc ld dp le kj lf lg lh kn li lj lk kr ll lm ln lo bi translated">监督机器学习</h2><p id="9052" class="pw-post-body-paragraph jy jz iq ka b kb lp kd ke kf lq kh ki kj lr kl km kn ls kp kq kr lt kt ku kv ij bi translated">在有监督的机器学习问题中，我们经常考虑观察对(<em class="na"> x </em>，<em class="na"> y </em>)的数据集<em class="na"> D </em>，并且我们尝试对以下分布建模:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/e9d12c47adf19c2eb125ff929046d729.png" data-original-src="https://miro.medium.com/v2/resize:fit:254/format:webp/1*-OZrvL5XSLTZFl-d42U48g.jpeg"/></div></figure><p id="2c2d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">例如，在图像分类中，x代表图像，y代表相应的图像标签。p(y|x，θ)表示给定图像x和由参数θ定义的模型的标签y的概率。</p><p id="cc7e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">遵循这种方法的模型被称为判别模型。在判别或条件模型中，定义条件概率分布函数p(y|x，θ)的参数是从训练数据中推断出来的。</p><p id="48cc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">基于观察数据x(输入数据或特征值)，模型输出概率分布，然后用于预测<em class="na"> y </em>(类别或真实值)。不同的机器学习模型需要估计不同的参数。线性模型(例如，由一组等于特征数量的权重定义的逻辑回归)和非线性模型(例如，由每层的一组权重定义的神经网络)都可以用来近似条件概率分布。</p><p id="c341" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于典型的分类问题，这组可学习参数θ用于定义不同标签上从<em class="na"> x </em>到<a class="ae md" href="https://en.wikipedia.org/wiki/Categorical_distribution" rel="noopener ugc nofollow" target="_blank">分类分布</a>的映射。判别分类模型产生N个概率作为输出，N等于类别的数量。每个<em class="na"> x </em>属于单个类别，但是模型不确定性通过输出类别上的分布来反映。通常，在做出决策时，会选择概率最大的类别。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/a6d0b6eec17ccba18af424b7bb5b41bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*9vyzX8YvtHRK9iq1."/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">In image classification, the network outputs a categorical distribution over image classes. The image above depicts the top 5 classes (classes with highest probability) for a test image.</figcaption></figure><p id="b3ca" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">注意，判别回归模型通常只输出一个预测值，而不是所有真实值的分布。这不同于鉴别分类模型，在鉴别分类模型中提供了所有可能类别的分布。这是否意味着歧视性模型在回归中会土崩瓦解？模型的输出不应该告诉我们哪些回归值比其他值更有可能吗？</p><p id="83b4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">虽然判别回归模型的单一输出是误导性的，但回归模型的输出实际上与众所周知的概率分布，即高斯分布有关。事实证明，判别回归模型的输出代表高斯分布的均值(高斯分布完全由均值和标准差定义)。有了这些信息，你就可以确定给定输入x的每个实值的可能性。</p><p id="55db" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通常只对该分布的平均值进行建模，高斯的标准偏差要么不进行建模，要么被选择为在所有<em class="na"> x </em>上保持不变。在判别回归模型中，θ因此定义了从x到高斯均值的映射，y从该高斯均值被采样。做决定时，几乎总是选择平均值。输出给定x的平均值和标准差的模型更能提供信息，因为该模型能够表达不确定的x(通过增加标准差)。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/5592e79331c0ab5cea333b32f9e35b32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/0*znk7XTNdPNlg2usv.jpg"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">A model needs to be uncertain in regions where there is no training data and certain in regions where it has training data. Such a model is displayed in the image above, from Yarin Gal’s blog post.</figcaption></figure><p id="ef75" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其他概率模型(如高斯过程)在建模回归问题中的不确定性方面做得更好，而判别回归模型在同时建模均值和标准差时往往过于自信。</p><p id="396d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">高斯过程能够通过明确地模拟标准偏差来量化不确定性。高斯过程的唯一缺点是它们不能很好地适应大型数据集。在下图中，您可以看到GP模型在包含大量数据的区域周围具有较小的置信区间(由标准差确定)。在数据点很少的区域，置信区间明显变大。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/cf98d8a82968f22bfe4206fd258a6ee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iXzjWnm8KR2wKDR-.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">A Gaussian Process model is certain at the data points, but uncertain at other places (image taken from <a class="ae md" href="https://www.google.be/url?sa=i&amp;rct=j&amp;q=&amp;esrc=s&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiQjpTIiqXXAhWLDBoKHRdaCDQQjhwIBQ&amp;url=http%3A%2F%2Fscikit-learn.org%2F0.17%2Fauto_examples%2Fgaussian_process%2Fplot_gp_regression.html&amp;psig=AOvVaw0Im8omKPY_J-HgrLFfrYi1&amp;ust=1509890495868115" rel="noopener ugc nofollow" target="_blank">Sklearn</a>)</figcaption></figure><p id="50e0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在训练数据集上训练判别模型，以便学习代表类或真实值的数据中的属性。如果模型能够将高概率分配给正确的样本类或接近测试数据集中真实值的平均值，则该模型表现良好。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h2 id="4803" class="kw kx iq bd ky kz la dn lb lc ld dp le kj lf lg lh kn li lj lk kr ll lm ln lo bi translated">与神经网络连接</h2><p id="55f2" class="pw-post-body-paragraph jy jz iq ka b kb lp kd ke kf lq kh ki kj lr kl km kn ls kp kq kr lt kt ku kv ij bi translated">当神经网络被训练用于分类或回归任务时，使用神经网络对前述分布(分类和高斯)的参数进行建模。</p><p id="a3b7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当我们试图确定神经网络<em class="na">的参数<em class="na"> θ </em>的最大似然估计(MLE)时，这变得很清楚。</em>MLE对应于找到使训练数据的似然性(或等效对数似然性)最大化的参数<em class="na"> θ </em>。更具体地，下面的表达式被最大化:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/c69cb092b2ed4d7ffa0f6c82db4d2d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*-gwNvMNu2GiZ4nSYA5Fw0Q.png"/></div></figure><p id="420b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">p(Y | X，θ)表示当用模型确定时，训练数据中真实标签的概率。如果p(Y | X，θ)更接近1，这意味着模型能够确定训练集中的正确标签/均值。给定训练数据(<em class="na"> X </em>，<em class="na"> Y </em>)由<em class="na"> N </em>个观察对组成，训练数据的可能性可以被重写为对数概率的和。</p><p id="1cf5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在分类和回归的情况下，p( <em class="na"> y </em> | <em class="na"> x，θ </em>)，单个对(x，y)的后验概率，可以重写为分类和高斯分布。在优化神经网络的情况下，目标是以这样的方式移动参数，即对于一组输入<em class="na"> X </em>，在输出(回归值或类)给出概率分布<em class="na"> Y </em>的正确参数。这通常通过梯度下降或其变体来实现。为了获得最大似然估计，目标是相对于真实输出优化模型输出:</p><ul class=""><li id="c21d" class="lu lv iq ka b kb kc kf kg kj lw kn lx kr ly kv lz ma mb mc bi translated">最大化分类分布的对数对应于最小化近似分布和真实分布之间的交叉熵。</li><li id="3655" class="lu lv iq ka b kb mo kf mp kj mq kn mr kr ms kv lz ma mb mc bi translated">最大化高斯分布的对数对应于最小化近似平均值和真实平均值之间的均方误差。</li></ul><p id="6f07" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，先前图像中的表达式可以被重写，并且分别导致交叉熵损失和均方误差，这是用于分类回归的神经网络的目标函数。</p><p id="0bd2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">与更传统的概率模型相比，神经网络学习从输入到概率或均值的非线性函数很难解释。虽然这是神经网络的一个显著缺点，但神经网络能够建模的复杂函数的广度也带来了显著的优势。基于本节中的推导，很明显，在确定参数的MLE时出现的神经网络的目标函数可以用概率来解释。</p><p id="23d8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">神经网络的一个有趣的解释是它们与广义线性模型(线性回归、逻辑回归等)的关系。神经网络不是采用特征的线性组合(如GLM的方法)，而是产生高度非线性的特征组合。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="e584" class="ng kx iq bd ky nh ni nj lb nk nl nm le nn no np lh nq nr ns lk nt nu nv ln nw bi translated">最大后验概率</h1><p id="585e" class="pw-post-body-paragraph jy jz iq ka b kb lp kd ke kf lq kh ki kj lr kl km kn ls kp kq kr lt kt ku kv ij bi translated">但是，如果神经网络可以被解释为概率模型，为什么它们提供糟糕的概率估计，并遭受对立的例子？为什么他们需要这么多数据？</p><p id="6cfd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我喜欢把不同的模型(逻辑回归、神经网络……)看作是在不同的搜索空间中寻找好的函数逼近器。虽然拥有一个非常大的搜索空间意味着在建模后验概率时有很大的灵活性，但这也是有代价的。例如，神经网络被证明是通用函数逼近器。这意味着只要有足够的参数，它们就可以逼近任何函数(太棒了！).然而，为了确保函数在整个数据空间中得到很好的校准，需要指数级的大数据集(昂贵！).</p><p id="ea17" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">重要的是要知道一个标准的神经网络通常使用最大似然优化。使用MLE的优化倾向于过度拟合训练数据，并且需要大量数据来获得适当的结果。机器学习的目标不是找到一个能很好地解释训练数据的模型。您宁愿尝试找到一个模型，该模型能够很好地概括看不见的数据，并且不确定数据是否与训练数据显著不同。</p><p id="6393" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用最大后验概率(MAP)方法是一种有效的替代方法，当概率模型遭受过拟合时，通常会探索这种方法。那么在神经网络的上下文中，MAP对应于什么呢？它对目标函数有什么影响？</p><p id="d5fd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">类似于MLE，MAP也可以被重写为神经网络环境中的目标函数。本质上，使用MAP，您可以在假定<em class="na"> θ : </em>的先验分布的同时，最大化给定数据的一组参数<em class="na"> θ </em>的概率</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e876f0eba7797a87bf1e35896692a709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*cmgZFNMOAOOiRuohXe6k0w.jpeg"/></div></figure><p id="4337" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用MLE，只考虑公式的第一个元素(模型解释训练数据的能力)。对于MAP，为了减少过拟合，模型满足先验假设(T4θ与先验的拟合程度)也很重要。</p><p id="c764" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">将平均值为0的高斯先验放在<em class="na"> θ </em>上对应于添加到目标的L2正则化(确保许多小权重)，而将拉普拉斯先验放在<em class="na"> θ </em>上对应于添加到目标的L1正则化(确保许多值为0的权重)。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/27b63ca2dbd55b27cbf98f21b1f2b622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ljY2Cnh_yNk1c3PiqMkhMg.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">L1 regularisation on the left and L2 regularisation on the right.</figcaption></figure></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="39b3" class="ng kx iq bd ky nh ni nj lb nk nl nm le nn no np lh nq nr ns lk nt nu nv ln nw bi translated">完全贝叶斯方法</h1><p id="b4e0" class="pw-post-body-paragraph jy jz iq ka b kb lp kd ke kf lq kh ki kj lr kl km kn ls kp kq kr lt kt ku kv ij bi translated">在MLE和MAP的情况下，使用单个模型(具有单组参数)。尤其是对于复杂的数据，例如图像，数据空间中的某些区域很可能没有被很好地覆盖。这些区域中模型的输出取决于模型和训练过程的随机初始化，导致对数据空间的未覆盖段中的点的概率估计较差。</p><p id="7b85" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">尽管MAP确保模型不会在这些区域过度拟合，但它仍然会导致模型过于自信。在完全贝叶斯方法中，这通过在多个模型上平均来解决，从而产生更好的不确定性估计。目标不是单一的一组参数，而是对参数的分布进行建模。如果所有模型(不同的参数设置)在未覆盖的区域提供不同的估计，这表明该区域有很大的不确定性。通过对这些模型进行平均，最终结果是一个在这些区域不确定的模型。这正是我们想要的！</p><p id="3ce3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在下一篇博文中，我将讨论贝叶斯神经网络，以及它们如何试图解决传统神经网络的上述问题。贝叶斯神经网络(BNN的)仍然是一项积极的研究工作，在训练它们时没有明确的赢家方法。</p><p id="39f6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我强烈推荐Yarin Gal关于深度学习中的不确定性的博文！</p></div></div>    
</body>
</html>