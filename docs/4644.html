<html>
<head>
<title>How to Scrape the Web using Python with ScraPy Spiders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用 Python 和 ScraPy 蜘蛛抓取网页</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-scrape-the-web-using-python-with-scrapy-spiders-e2328ac4526?source=collection_archive---------2-----------------------#2018-08-28">https://towardsdatascience.com/how-to-scrape-the-web-using-python-with-scrapy-spiders-e2328ac4526?source=collection_archive---------2-----------------------#2018-08-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/dbd515a26a229e8c43c29225641663bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KSV2MaVJv-T4z7SU_EwQJw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Each of those hairs scrapes. Source: <a class="ae kc" href="https://pixabay.com/en/spider-macro-zebra-spider-insect-arachni-564685/" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><p id="7dd3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有时候 Kaggle 还不够，还需要自己生成数据集。</p><p id="5ec6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也许你需要你正在训练的这个疯狂的卷积神经网络的蜘蛛图片，或者也许你为了，嗯，科学的目的想要刮 NSFW 子网格。不管你的理由是什么，浏览网页可以给你带来非常有趣的数据，并帮助你汇编令人惊叹的数据集。</p><p id="8e2f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将使用 ScraPy 来抓取 Reddit 子编辑并获取图片。<br/>有些人会告诉我<a class="ae kc" href="https://github.com/praw-dev/praw" rel="noopener ugc nofollow" target="_blank">使用 Reddit 的 API </a>是一种更实用的获取数据的方法，这绝对是真的。确实如此，我可能很快会写一篇关于它的文章。<br/>但是只要我们做得很少，并且不要让 Reddit 繁忙的服务器超负荷工作，应该没问题。所以请记住，本教程仅用于教育目的，如果你需要 Reddit 的数据，你应该使用官方渠道，比如他们的<a class="ae kc" href="https://github.com/praw-dev/praw" rel="noopener ugc nofollow" target="_blank">牛逼 API </a>。</p><p id="69e3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，我们如何着手清理网站呢？让我们从头开始。</p><h1 id="64d7" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">检查机器人. txt</h1><p id="c996" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">首先我们将进入 reddit.com/robots.txt。对于一个站点来说，习惯上是让他们的<em class="me"> robots.txt </em>文件可以从他们的主域访问。它遵循以下格式:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="d2a4" class="mo lc iq mk b gy mp mq l mr ms">User-agent: &lt;pattern&gt;<br/>Disallow: &lt;patterns&gt;</span></pre><p id="87e0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中 U <em class="me"> ser-agent </em>描述一种设备类型(我们属于*，通配符模式)，而<em class="me"> Disallow </em>指向一个我们无法抓取的 url 模式列表。<br/>我在那里没有看到/r/*，所以我<em class="me">觉得</em>刮一个 subreddit 的主页也可以。出于礼节，我仍然建议你在任何严肃的项目中使用 API。</p><p id="aeab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不尊重一个网站的<em class="me"> robots.txt </em>文件可能会有法律后果，但这主要只是让你看起来像一个卑鄙的人，我们不希望这样。</p><h1 id="5c47" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">建立我们的项目。</h1><p id="4772" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">为了用 Python 抓取网站，我们将使用 ScraPy，它是主要的抓取框架。有些人喜欢美丽的声音，但我觉得 ScraPy 更有活力。</p><p id="08f1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ScraPy 的基本抓取单元叫做<em class="me">蜘蛛，</em>，我们将通过创建一个空蜘蛛来开始这个程序。</p><p id="501d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，首先，我们将安装 ScraPy:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="ed33" class="mo lc iq mk b gy mp mq l mr ms">pip install --user scrapy</span></pre><p id="cc3a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们将开始一个零碎的项目:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="1803" class="mo lc iq mk b gy mp mq l mr ms">scrapy startproject project_name </span></pre><p id="eef4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以在此输入任何内容，而不是项目名称。该命令将创建一个目录，其中包含许多文件和 python 脚本。</p><p id="8eee" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，对于我们的最后一个初始化命令，我们将创建我们的第一个蜘蛛。为此，我们将运行 scrapy 的<em class="me"> genspider </em>命令，该命令将一个蜘蛛的<em class="me">名称</em>和一个<em class="me">域 url </em>作为其参数。我会给我的取名为<em class="me">小猫捕手</em>(小心:剧透)和爬行<em class="me">reddit.com/r/cats</em>。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="b206" class="mo lc iq mk b gy mp mq l mr ms">scrapy genspider kitten_getter reddit.com/r/cats</span></pre><p id="9423" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们只进入/<em class="me">spider</em>目录，不关注其余部分。和往常一样，我在<a class="ae kc" href="https://github.com/StrikingLoo/kitten-getter" rel="noopener ugc nofollow" target="_blank">GitHub 项目</a>中发布了我的代码。</p><h1 id="87dc" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">搭建我们的第一只蜘蛛</h1><p id="26ae" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">在<em class="me">蜘蛛</em>目录中，我们将打开名为<em class="me"> kitten_getter.py </em>的文件并粘贴以下代码:</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="43fc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里发生了什么事？嗯，每个蜘蛛需要三样东西:一个<em class="me">解析</em>方法，一个<em class="me">开始请求</em>方法，和一个<em class="me">名称</em>。</p><ul class=""><li id="b144" class="mv mw iq kf b kg kh kk kl ko mx ks my kw mz la na nb nc nd bi translated">每当我们从控制台启动蜘蛛时，都会使用蜘蛛的名称<em class="me">和</em>。</li><li id="52a5" class="mv mw iq kf b kg ne kk nf ko ng ks nh kw ni la na nb nc nd bi translated">从控制台运行 spider 将使其从<em class="me"> start_requests </em>例程启动。</li><li id="e819" class="mv mw iq kf b kg ne kk nf ko ng ks nh kw ni la na nb nc nd bi translated">我们让例程在 URL 列表上执行<em class="me"> http </em>请求，并在它们的<em class="me"> http </em>响应上调用我们的<em class="me">解析</em>方法。</li></ul><p id="b0ea" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了运行它，我们所要做的就是在项目的目录中打开我们的终端并运行:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="258a" class="mo lc iq mk b gy mp mq l mr ms">scrapy crawl kitten_getter</span></pre><p id="f8a2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">释放你的蜘蛛！让他们漫游网络，攫取宝贵的数据。</p><p id="0412" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您运行该命令，它将运行我们刚刚编写的蜘蛛，因此它将发出请求，获取我们提供的<em class="me"> url_list </em>中第一个<em class="me"> url </em>的 HTML，并按照我们要求的方式解析它。在这种情况下，我们所做的就是将整个响应直接写入一个名为“kitten_response0”的文件中(大小约为 140Kb)。</p><p id="6bb4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你打开它，你会看到它只是我们刮的网站的 HTML 代码。这对我们的下一个目标很有用。</p><h1 id="37e6" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">识别模式</h1><p id="09e9" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">如果你去链接<a class="ae kc" href="http://reddit.com/r/cats" rel="noopener ugc nofollow" target="_blank">reddit.com/r/cats</a>寻找小猫图片，你会注意到有两种帖子。</p><ul class=""><li id="b7c2" class="mv mw iq kf b kg kh kk kl ko mx ks my kw mz la na nb nc nd bi translated">点击链接到评论区的文章。</li><li id="3a2a" class="mv mw iq kf b kg ne kk nf ko ng ks nh kw ni la na nb nc nd bi translated">直接指向 pic 的帖子</li></ul><p id="1728" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还注意到，我们无法在不违反<em class="me"> robots.txt、</em>的情况下找到任何与<em class="me">reddit.com/r/*/comments/*</em>匹配的内容，因此从帖子中提取图片是错误的。然而，如果图片直接链接到 subreddit 的主页，我们可以获得图片的 URL。我们看到那些链接总是在一个<em class="me">&lt;&gt;</em>标签中的<em class="me"> href </em>属性，所以我们要做的是调用响应对象的<em class="me"> xpath </em>方法来获得它们。</p><p id="4add" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">xPath 是一种在网站的 HTML 树中移动并获取其中一些元素的方法。Scrapy 还为我们提供了<em class="me"> css </em>方法，它允许一种不同的索引和标记元素的方式。我个人发现在浏览器中右键单击一个元素，点击 inspect，然后<em class="me"> copy xpath </em>是一种快速的开始方式，然后我只是稍微摆弄一下输出。</p><p id="1892" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个特殊的例子中，因为我们需要的只是每个<em class="me">&lt;&gt;</em>元素的<em class="me"> href </em>值，我们将调用</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="b721" class="mo lc iq mk b gy mp mq l mr ms">response.xpath(‘//a/@href’)</span></pre><p id="3483" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这将为每个<em class="me"> href </em>值(ScraPy 库中的一个对象)返回一个迭代器。然后，我们通过调用 extract 方法提取该值的字符串形式，并通过查看它是否以'结尾来检查它是否实际上是一个到图像的链接。png '或'。jpg。<br/>下面是整个改进的解析方法，它现在还创建了一个 html 文件来显示所有图像，而无需下载它们:</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="9547" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们让我们的蜘蛛再次爬行，输出应该是这样的:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="b352" class="mo lc iq mk b gy mp mq l mr ms">Crawled (200) &lt;GET <a class="ae kc" href="https://www.reddit.com/r/cats/" rel="noopener ugc nofollow" target="_blank">https://www.reddit.com/r/cats/</a>&gt; (referer: None)<br/><a class="ae kc" href="https://i.imgur.com/Au0aqkj.jpg" rel="noopener ugc nofollow" target="_blank">https://i.imgur.com/Au0aqkj.jpg</a><br/><a class="ae kc" href="https://i.imgur.com/Xw90WFo.jpg" rel="noopener ugc nofollow" target="_blank">https://i.imgur.com/Xw90WFo.jpg</a><br/><a class="ae kc" href="https://i.imgur.com/fOINLvP.jpg" rel="noopener ugc nofollow" target="_blank">https://i.imgur.com/fOINLvP.jpg</a></span></pre><p id="6959" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中每个链接都是一只可爱小猫的图片。作为奖励，文件<em class="me">kittens.html</em>应该洋溢着可爱。</p><p id="5224" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就是这样！您已经成功抓取了您的第一个网站！</p><h2 id="ec37" class="mo lc iq bd ld nj nk dn lh nl nm dp ll ko nn no lp ks np nq lt kw nr ns lx nt bi translated">保存图像</h2><p id="9115" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">假设我们想下载图片，而不是制作一个 HTML 文件。然后我们要做的是导入 Python 的<em class="me">请求</em>库，以及<em class="me"> unicodedata </em>库。<em class="me">请求</em>将完成这项繁重的工作，但是我们需要<em class="me"> unicodedata </em>，因为提取的字符串默认为<em class="me"> unicode </em>，而<em class="me">请求</em>需要 ASCII 码。</p><p id="5afd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们将传递我们的<em class="me"> scrapy，而不是解析方法。请求</em>函数将下面的函数作为回调参数:</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="6b81" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它所做的只是下载一张图片，并将其保存为 JPG。它还自动增加存储在蜘蛛中的索引属性，为每张图片命名。</p><h2 id="2177" class="mo lc iq bd ld nj nk dn lh nl nm dp ll ko nn no lp ks np nq lt kw nr ns lx nt bi translated">到处玩:交互式 shell</h2><p id="2032" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">ScraPy 为我们提供了一个交互式的 shell，在这里我们可以尝试不同的命令、表达式和 xpaths。这是一种比用<em class="me"> crawl </em>命令一遍又一遍地运行整个程序更有效的迭代和调试蜘蛛的方式。要启动 shell，我们只需运行以下命令:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="4f9a" class="mo lc iq mk b gy mp mq l mr ms">scrapy shell ‘http://reddit.com/r/cats’</span></pre><p id="0017" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，这个网址可以用其他任何网址代替。</p><h2 id="2396" class="mo lc iq bd ld nj nk dn lh nl nm dp ll ko nn no lp ks np nq lt kw nr ns lx nt bi translated">伸出我们的蜘蛛</h2><p id="614d" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">如果我们想得到更多的图像，我们可以让<em class="me"> download_pictures </em>方法调用<em class="me"> scrapy。请求下一页的 URL 上的</em>，可以从“下一页”按钮的<em class="me"> href </em>属性中获得。我们还可以让蜘蛛将 subreddit 作为参数，或者更改下载的文件扩展名。</p><p id="9b10" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总而言之，最好的解决方案通常是最简单的，所以使用 Reddit 的 API 会让我们省去很多麻烦。</p><p id="11d9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望你现在能够制作自己的蜘蛛，并获得自己的数据。请告诉我您是否觉得这很有用，以及您认为使用此工具可以生成什么样的好数据集—越有创意越好。</p><p id="320c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，还有一本我喜欢的奥莱利的书。当我开始我的数据科学之旅时，我发现它非常有用，它让我接触到了一个不同的、更容易使用(尽管不太灵活)的 Web 抓取框架。用 Python 叫做<a class="ae kc" href="https://www.bookdepository.com/book/9781491901427/?a_aid=strikingloo&amp;chan=ws" rel="noopener ugc nofollow" target="_blank">从零开始的数据科学，大概也是我得到这份工作的一半原因。如果你读到这里，你可能会喜欢它！</a></p><p id="d795" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="me">关注我，获取更多 Python 教程、技巧和诀窍！</em></p><p id="fcd3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="me">你可以在我的</em> <a class="ae kc" href="http://strikingloo.github.io/wiki" rel="noopener ugc nofollow" target="_blank"> <em class="me">个人网站</em> </a> <em class="me">中看到我正在做的事情以及我最近的文章和笔记。</em></p></div></div>    
</body>
</html>