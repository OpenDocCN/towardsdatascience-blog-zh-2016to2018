<html>
<head>
<title>Topic modelling with PLSA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PLSA 的主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41?source=collection_archive---------3-----------------------#2018-10-06">https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41?source=collection_archive---------3-----------------------#2018-10-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/4fc76b983f741c1d95f619653a6431e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*kd0pAM3XiYYzlF9RyeXQAA.jpeg"/></div></figure><p id="a7a7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">假设你有数百篇文章/句子。你想知道每篇文章/句子谈论什么话题。一篇描述对一家制药公司的指控的文章可能会谈论像<em class="ks">政府、医药</em>或<em class="ks">商业这样的话题。</em>我们的目标是将这些主题分配给文档。</p><p id="5340" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">执行这项任务的方法之一是概率潜在语义分析(PLSA)。</p><p id="cb6b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">PLSA 或概率潜在语义分析是一种用于在概率框架下对信息建模的技术。<em class="ks">潜在</em>因为话题被当作潜在的或隐藏的变量。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><h1 id="c242" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">型号:</strong></h1><p id="24de" class="pw-post-body-paragraph ju jv iq jw b jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr ij bi translated">可以用两种不同的方式来理解 PLSA。</p><ol class=""><li id="2719" class="md me iq jw b jx jy kb kc kf mf kj mg kn mh kr mi mj mk ml bi translated">潜在变量模型</li><li id="54b3" class="md me iq jw b jx mm kb mn kf mo kj mp kn mq kr mi mj mk ml bi translated">矩阵分解</li></ol><p id="d870" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">第一个帮助你很好地理解 PLSA 的数学。而第二种方法很容易用 Python 实现。</p><p id="e366" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们正式定义出现在 PLSA 的变量。</p><p id="9ec9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们有三组变量</p><ol class=""><li id="4d67" class="md me iq jw b jx jy kb kc kf mf kj mg kn mh kr mi mj mk ml bi translated"><strong class="jw ir">文件数:</strong> <em class="ks"> D={d1，d2，d3，…dN} </em>，<em class="ks"> N </em>为文件数。<em class="ks"> di </em>表示<em class="ks"> D </em>中带有文件的<em class="ks">。<em class="ks">注意——此处的文档也可以指句子。这两个词可以互换使用。</em></em></li><li id="8458" class="md me iq jw b jx mm kb mn kf mo kj mp kn mq kr mi mj mk ml bi translated"><strong class="jw ir">单词:</strong> <em class="ks"> W={w1，w2，…wM} </em>，M 是我们词汇量的大小。<em class="ks"> wi </em>表示词汇表中的第 I 个单词<em class="ks"> W. </em> <br/> <em class="ks">注:集合 W 被视为单词包。意思是索引 I 的赋值没有特定的顺序</em></li><li id="3d16" class="md me iq jw b jx mm kb mn kf mo kj mp kn mq kr mi mj mk ml bi translated"><strong class="jw ir">题目:</strong> <em class="ks"> Z={z1，z2，…zk} — </em>潜变量或隐变量。数字<em class="ks"> k </em>是我们指定的参数。</li></ol><p id="42ca" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">指定了命名约定之后，让我们来看看第一种方法。</p><h2 id="ecde" class="mr lb iq bd lc ms mt dn lg mu mv dp lk kf mw mx lo kj my mz ls kn na nb lw nc bi translated">潜在变量模型:</h2><p id="88c7" class="pw-post-body-paragraph ju jv iq jw b jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr ij bi translated">如前所述，主题是隐藏变量。我们唯一看到的是文字和一套文件。在这个框架中，我们将隐藏变量与观察变量联系起来。</p><p id="8345" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们将<em class="ks"> z </em>与(<em class="ks"> d，w) </em>联系起来的方式是，我们描述一个生成过程，我们选择一个文档，然后是一个主题，然后是一个单词。形式上，</p><ol class=""><li id="5af9" class="md me iq jw b jx jy kb kc kf mf kj mg kn mh kr mi mj mk ml bi translated">我们选择一个概率为<em class="ks"> P(d) </em>的文档</li><li id="5afe" class="md me iq jw b jx mm kb mn kf mo kj mp kn mq kr mi mj mk ml bi translated">对于本文档中的每一个单词<em class="ks"> dn，wi</em>T47】——从概率为<em class="ks"> P(z|dn)的条件分布中选择一个话题<em class="ks"> zi </em>。<br/> - </em>选择一个有概率的单词<em class="ks"> P(w|zi) </em></li></ol><p id="be14" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果不是完全清楚，也不用担心。我在数学上把一切都放在前面了。在进入方程之前，让我们讨论一下模型的两个假设。</p><p id="fb35" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">假设 1——单词包:</strong>正如我们之前讨论的，单词在词汇表中的排序并不重要。更准确地说，联合变量(d，w)是独立采样的。</p><figure class="ne nf ng nh gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/6346480c05d874d975bea213caffd6ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*TaVn_YvvKd9sprJohLt2FA.jpeg"/></div></figure><p id="e669" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">假设—条件独立:我们做的一个关键假设是单词和文档是条件独立的。有条件地聚焦<strong class="jw ir">这个词</strong>。这意味着—</p><p id="ede6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> <em class="ks"> P(w，d | z)= P(w | z)* P(d | z)——</em></strong>(3)</p><p id="41cf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">上述讨论下的模型可指定如下—</p><figure class="ne nf ng nh gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/9770db0dd1339eb75171f4fc97655001.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*sjYatEoQW9O1-j7QzGlwSw.jpeg"/></div></figure><p id="8480" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> <em class="ks">现在，</em> </strong></p><figure class="ne nf ng nh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nj"><img src="../Images/7873833d6162b225cf61c9d24072bef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*HKmRyClbqX7tN0Z8E-js-w.jpeg"/></div></div></figure><figure class="ne nf ng nh gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/67a6e0e6870babf3050e9039ae7397bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*wrlH47rrBxwEzsmHiPbqaA.jpeg"/></div></figure><p id="80fd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">利用条件独立性，</p><figure class="ne nf ng nh gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/03dee04555bd502ee33a3c0ba4f82a71.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*Xw8OIr5bGCRgVUrPkMan5A.jpeg"/></div></figure><p id="62c3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">使用贝叶斯规则，</p><figure class="ne nf ng nh gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/42aa9982075ddbe9c1e2a541b70e7dd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*hTenLukkUKrqX1NMDpdzyA.jpeg"/></div></figure><p id="0353" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">模型中的参数是—</p><ol class=""><li id="2794" class="md me iq jw b jx jy kb kc kf mf kj mg kn mh kr mi mj mk ml bi translated"><strong class="jw ir"><em class="ks">P(w | z)——</em></strong>有(M-1)*K 个。怎么会？对于每一个<strong class="jw ir"> <em class="ks"> z </em> </strong>我们都有 M 个单词。但是因为这 M 个概率的和应该是 1，我们失去了一个自由度。</li><li id="fc30" class="md me iq jw b jx mm kb mn kf mo kj mp kn mq kr mi mj mk ml bi translated"><strong class="jw ir"><em class="ks">P(z | d)——</em></strong>有(K-1)*N 个参数来确定。</li></ol><p id="1fa3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">使用期望最大化或似然函数的 EM 算法来确定上述参数。</p><p id="56c4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">可能性函数—</p><figure class="ne nf ng nh gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/416b75c92857c8babaff7e774817c520.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*89mZeTmTY_DgC_HaEgIV-g.jpeg"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk">Likelihood function</figcaption></figure><p id="de46" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对数可能性—</p><figure class="ne nf ng nh gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/eaabcaa56176a3c67f348294516015ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*tj0dt08xF3DUG3gBla1kBQ.jpeg"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk">Log Likelihood</figcaption></figure><p id="97ab" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们不会深入 EM 算法是如何工作的。这次讨论的主要目的是让你熟悉主题如何与观察到的变量、单词和文档相关联。如果你想了解它的数学，你可以在这里找到相关的论文。</p><h2 id="4a32" class="mr lb iq bd lc ms mt dn lg mu mv dp lk kf mw mx lo kj my mz ls kn na nb lw nc bi translated">矩阵分解模型</h2><p id="c921" class="pw-post-body-paragraph ju jv iq jw b jx ly jz ka kb lz kd ke kf ma kh ki kj mb kl km kn mc kp kq kr ij bi translated">另一种表示 PLSA 的方法是矩阵分解模型。</p><p id="752c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">考虑一个维度为<em class="ks"> N*M </em>的文档-单词矩阵，其中<em class="ks"> N </em>是文档的数量，<em class="ks"> M </em>是词汇表的大小。矩阵的元素是一个单词在文档中出现的次数。如果单词<em class="ks"> wi </em>在文档<em class="ks"> dj </em>中出现一次，那么元素<em class="ks"> (j，i) = 1。</em></p><p id="40f3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果你想到这个矩阵，大部分元素都是 0。假设我们有一个 10 个单词的文档和 1000 个单词的词汇表。自然地，该行的 990 个元素将具有值 0。这样的矩阵称为<em class="ks">稀疏矩阵。</em></p><p id="73a6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">矩阵分解的作用是将这个矩阵(姑且称之为<em class="ks"> A </em>)分解成低维矩阵(奇异值分解)</p><figure class="ne nf ng nh gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/bc2e6dce1af28e0527cb892b680c5d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:196/format:webp/1*ldwEwRk-aJKqZ_caaO9VBg.jpeg"/></div></figure><figure class="ne nf ng nh gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/e30ed48fb31d1ca906090a9f1764cb26.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*9TCz0n3e-qADLlmJxVJ5aA.jpeg"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk">Pictorial representation of the above equation</figcaption></figure><p id="4310" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><em class="ks"> L、U </em>和<em class="ks"> R </em>的尺寸分别为<em class="ks"> N*K、K*K </em>和<em class="ks"> K*M </em>。</p><p id="36ad" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">矩阵 U 是对角矩阵，取值为<em class="ks"> AA* </em>的特征值的平方根，其中*表示转置。对于任意给定的<em class="ks"> k，</em>你选择<em class="ks"> L 的第 k <em class="ks">行，</em>的第<em class="ks"> k </em>元素，<em class="ks"> U </em>的第<em class="ks"> k </em>列，<em class="ks"> R. </em>的第<em class="ks">k</em>列，记住，<em class="ks"> k </em>是我们设置的题目数。</em></p><p id="e9e9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这个模型与潜变量模型没有太大的不同。这三个矩阵可以解释为—</p><ol class=""><li id="2eae" class="md me iq jw b jx jy kb kc kf mf kj mg kn mh kr mi mj mk ml bi translated"><em class="ks"> L </em>包含文档概率<strong class="jw ir"> <em class="ks"> P(d|z) </em> </strong></li><li id="5a15" class="md me iq jw b jx mm kb mn kf mo kj mp kn mq kr mi mj mk ml bi translated"><em class="ks"> U </em>是主题<strong class="jw ir"> <em class="ks"> P(z) </em> </strong>的先验概率的对角矩阵</li><li id="ea18" class="md me iq jw b jx mm kb mn kf mo kj mp kn mq kr mi mj mk ml bi translated"><em class="ks"> R </em>对应单词概率<strong class="jw ir"> <em class="ks"> P(w|z) </em> </strong></li></ol><p id="a479" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">所以如果你把三个矩阵相乘，你实际上做的是下面的等式——</p><figure class="ne nf ng nh gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/03dee04555bd502ee33a3c0ba4f82a71.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*Xw8OIr5bGCRgVUrPkMan5A.jpeg"/></div></figure><p id="3fd1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">请注意，这三个矩阵的元素不能为负，因为它们代表概率。因此，使用<a class="ae nx" href="https://www.cc.gatech.edu/~hpark/papers/nmf_book_chapter.pdf" rel="noopener ugc nofollow" target="_blank">非负矩阵分解</a>来分解<em class="ks"> A </em>矩阵。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><p id="fdc4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果你想在 Python 中应用这个算法，你可以在这里找到一个例子<a class="ae nx" href="https://github.com/DhruvilKarani/Non-Negative-Matrix-Factorization/blob/master/NNMF.ipynb" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="17d4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">感谢阅读！如果你喜欢这个博客，你可能想看看我的另一篇关于单词嵌入和 Word2Vec 的文章。</p><p id="f3e4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">欢迎分享你的观点。很乐意在 LinkedIn 上与您联系</p></div></div>    
</body>
</html>