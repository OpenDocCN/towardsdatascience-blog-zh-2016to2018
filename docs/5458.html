<html>
<head>
<title>Scraping Reddit with PRAW</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 PRAW 刮红迪网</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scraping-reddit-with-praw-76efc1d1e1d9?source=collection_archive---------5-----------------------#2018-10-19">https://towardsdatascience.com/scraping-reddit-with-praw-76efc1d1e1d9?source=collection_archive---------5-----------------------#2018-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/265e32b7ecc9fbf64408e59ac7f0061f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e3E0OQzfYCuWk0pket5dAA.png"/></div></div></figure><p id="5abe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最近，我试图开始一个项目，该项目将使用自然语言处理来分类一个给定的帖子来自哪个子编辑。例如，这个模型应该能够预测一个帖子是否来自于<a class="ae kw" href="https://www.reddit.com/r/python" rel="noopener ugc nofollow" target="_blank"> r/Python </a>子编辑或者<a class="ae kw" href="https://www.reddit.com/r/rlanguage" rel="noopener ugc nofollow" target="_blank"> r/Rlanguage </a>子编辑。这个过程的第一步是从每个子编辑中收集一些帖子。在过去，我使用过<a class="ae kw" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank"> BeautifulSoup </a>和<a class="ae kw" href="http://docs.python-requests.org/en/master/" rel="noopener ugc nofollow" target="_blank"> requests </a> python 库来完成这项工作。我的一些同事提到 API 包装器非常有用，可以简化大部分过程。这就是我如何偶然发现 Python Reddit API 包装器(<a class="ae kw" href="https://praw.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> PRAW </a>)。</p><p id="ceb4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我发现的最有帮助的文章之一是 Felippe Rodrigues 的“<a class="ae kw" href="http://www.storybench.org/how-to-scrape-reddit-with-python/" rel="noopener ugc nofollow" target="_blank">如何用 Python 刮 Reddit</a>”他在基本步骤和设置方面做得很好。如果你有兴趣做类似的事情，一定要去看看。我将这篇文章作为我自己项目的起点。下面我将带你了解如何使用 PRAW 进行设置，以及如何抓取文章标题、内容和其他元数据。你可以在我的个人<a class="ae kw" href="https://github.com/smidem/reddit_scraper" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到完整的代码。</p><p id="6140" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">开始使用 PRAW 的第一步是通过 reddit 创建一个应用程序。如果你转到这个<a class="ae kw" href="https://www.reddit.com/prefs/apps" rel="noopener ugc nofollow" target="_blank">链接</a>，你应该会在底部找到一个标有“创建应用”或“创建另一个应用”的按钮你必须给你的脚本一个名字，并填写一个描述。一旦完成，确保选择“脚本”选项，然后确保将以下内容放入重定向 uri 框:<a class="ae kw" href="http://localhost:8080" rel="noopener ugc nofollow" target="_blank"> http://localhost:8080 </a>。这是由<a class="ae kw" href="https://praw.readthedocs.io/en/latest/getting_started/authentication.html#script-application" rel="noopener ugc nofollow" target="_blank"> PRAW 文档</a>建议的，但显然是必要的，因为 Reddit 需要一个重定向 uri，即使我们的应用程序不使用它。如果成功的话，你会得到两串看似随机的字符。你可以在左上角找到第一个，你的“个人使用脚本”，就在“个人使用脚本”的正下方这个应该有 14 个字符长。接下来你需要的是“秘密”。虽然这听起来像是幻想小说中的东西，但它应该表示为一个 27 个字符的字符串，列在“个人使用脚本”的下面。有了这两样东西，我们终于可以开始从 Reddit 抓取帖子的旅程了！</p><p id="4a80" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，您需要打开您最喜欢的编辑器或 IDE。我用 PyCharm 做这个项目，但是 Atom 或者 Sublime 也可以。我们首先需要创建一个 praw.ini 文件。这将允许我们在未来利用 PRAW Reddit 实例，这意味着如果您决定发布它，我们可以将我们的个人使用脚本和秘密从我们的主要脚本中删除。</p><p id="0958" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你可以参考关于 praw.ini 文件的<a class="ae kw" href="https://praw.readthedocs.io/en/latest/getting_started/authentication.html#script-application" rel="noopener ugc nofollow" target="_blank">文档</a>，但是一定要确保这个文件确实叫做“praw.ini ”,并且它和你的抓取脚本位于同一个目录。可以在这个文件中定义多个凭证，但是今天我们将只使用一个凭证，所以我们将坚持使用[默认]站点。</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="47fb" class="lg lh iq lc b gy li lj l lk ll">[DEFAULT]<br/>; this is your 14 character personal use script<br/>client_id=51IfSlyxtyOUy3</span><span id="9029" class="lg lh iq lc b gy lm lj l lk ll">; this is your 27 character secret<br/>client_secret=NToU1zG5d0LJq9fo3ryUtOigM5h</span><span id="d4b2" class="lg lh iq lc b gy lm lj l lk ll">; this is the name you gave your application<br/>user_agent=subreddit scraper</span><span id="66ad" class="lg lh iq lc b gy lm lj l lk ll">; this is username for the reddit account the app was created with<br/>username=fake_username</span><span id="d94a" class="lg lh iq lc b gy lm lj l lk ll">; password for the account<br/>password=fake_password</span></pre><p id="171e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请确保在这些字段中填写您的信息。上面的字段是使用随机生成的假值填写的。如果您不使用自己的信息替换这些字段，它将不起作用。</p><p id="eb54" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们可以开始编写实际的抓取脚本了。第一步是导入必要的库，并使用我们在 praw.ini 文件中定义的凭证实例化 Reddit 实例。</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="ccb5" class="lg lh iq lc b gy li lj l lk ll">from os.path import isfile<br/>import praw<br/>import pandas as pd<br/>from time import sleep</span><span id="8a2c" class="lg lh iq lc b gy lm lj l lk ll"><em class="ln"># Get credentials from DEFAULT instance in praw.ini<br/></em>reddit = praw.Reddit()</span></pre><p id="6b10" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我决定创建一个类，允许我指定我感兴趣的特定子编辑、排序方法、文章数量以及结果是否应该写入文件。__init__ 方法如下所示:</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="750f" class="lg lh iq lc b gy li lj l lk ll">class SubredditScraper:<br/><br/>    def __init__(self, sub, sort=<strong class="lc ir">'new'</strong>, lim=900, mode=<strong class="lc ir">'w'</strong>):<br/>        self.sub = sub<br/>        self.sort = sort<br/>        self.lim = lim<br/>        self.mode = mode</span><span id="c5e9" class="lg lh iq lc b gy lm lj l lk ll">        print(<br/>            <strong class="lc ir">f'SubredditScraper instance created with values '<br/>            f'sub = {sub}, sort = {sort}, lim = {lim}, mode = {mode}'</strong>)</span></pre><p id="fc0f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我加入了一些打印语句来感受一下脚本的进度，因为在处理数百个帖子时可能需要一点时间来运行。接下来，我们将为 subreddit 实例设置排序方法。</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="ea8a" class="lg lh iq lc b gy li lj l lk ll">def set_sort(self):<br/>    if self.sort == <strong class="lc ir">'new'</strong>:<br/>        return self.sort, reddit.subreddit(self.sub).new(limit=self.lim)<br/>    elif self.sort == <strong class="lc ir">'top'</strong>:<br/>        return self.sort, reddit.subreddit(self.sub).top(limit=self.lim)<br/>    elif self.sort == <strong class="lc ir">'hot'</strong>:<br/>        return self.sort, reddit.subreddit(self.sub).hot(limit=self.lim)<br/>    else:<br/>        self.sort<em class="ln"> </em>= <strong class="lc ir">'hot'<br/>        </strong>print(<strong class="lc ir">'Sort method was not recognized, defaulting to hot.'</strong>)<br/>        return self.sort, reddit.subreddit(self.sub).hot(limit=self.lim)</span></pre><p id="47b2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个方法将把 reddit 实例的 subreddit 和排序参数设置为我们在实例化我们的类时指定的值。这将返回一个元组，我们将在下一个方法中解包它。如果排序方法不是“新”、“前”或“热”，则默认为“热”。</p><p id="3c76" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，我们可以开始从指定的子编辑中收集帖子和其他信息:</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="8179" class="lg lh iq lc b gy li lj l lk ll">def get_posts(self):<br/>    <em class="ln">"""Get unique posts from a specified subreddit."""<br/><br/>    </em>sub_dict = {<br/>        <strong class="lc ir">'selftext'</strong>: [], <strong class="lc ir">'title'</strong>: [], <strong class="lc ir">'id'</strong>: [], <strong class="lc ir">'sorted_by'</strong>: [],<br/>        <strong class="lc ir">'num_comments'</strong>: [], <strong class="lc ir">'score'</strong>: [], <strong class="lc ir">'ups'</strong>: [], <strong class="lc ir">'downs'</strong>: []}<br/>    csv = <strong class="lc ir">f'{self.sub}_posts.csv'<br/><br/>    </strong><em class="ln"># Attempt to specify a sorting method.<br/>    </em>sort, subreddit = self.set_sort()<br/><br/>    <em class="ln"># Set csv_loaded to True if csv exists since you can't <br/>    # evaluate the truth value of a DataFrame.<br/>    </em>df, csv_loaded = (pd.read_csv(csv), 1) if isfile(csv) else (<strong class="lc ir">''</strong>, 0)<br/><br/>    print(<strong class="lc ir">f'csv = {csv}'</strong>)<br/>    print(<strong class="lc ir">f'After set_sort(), sort = {sort} and sub = {self.sub}'</strong>)<br/>    print(<strong class="lc ir">f'csv_loaded = {csv_loaded}'</strong>)<br/><br/>    print(<strong class="lc ir">f'Collecting information from r/{self.sub}.'</strong>)</span></pre><p id="98b0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里我们创建了一个占位符字典，其中包含了我们将从这个子编辑的每个帖子中收集的每个属性。那我们就要建立。csv 文件以备将来使用，我们正在对 set_sort 方法返回的元组进行解包。最后一项设置是查看我们之前是否为此子编辑收集过帖子。如果是这样，我们将加载它。csv 文件放入数据帧，并将布尔变量 csv_loaded 设置为 1。否则，df 将是一个空字符串，csv_loaded 将被设置为 0。</p><p id="5f34" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这给我们带来了真正的刮肉。我们将使用 for 循环来查看每篇文章，并收集我们感兴趣的属性。</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="b2ec" class="lg lh iq lc b gy li lj l lk ll">for post in subreddit:<br/><br/>    <em class="ln"># Check if post.id is in df and set to True if df is empty.<br/>    # This way new posts are still added to dictionary when df = ''<br/>    </em>unique_id = post.id not in tuple(df.id) if csv_loaded else True<br/><br/>    <em class="ln"># Save any unique posts to sub_dict.<br/>    </em>if unique_id:<br/>        sub_dict[<strong class="lc ir">'selftext'</strong>].append(post.selftext)<br/>        sub_dict[<strong class="lc ir">'title'</strong>].append(post.title)<br/>        sub_dict[<strong class="lc ir">'id'</strong>].append(post.id)<br/>        sub_dict[<strong class="lc ir">'sorted_by'</strong>].append(sort)<br/>        sub_dict[<strong class="lc ir">'num_comments'</strong>].append(post.num_comments)<br/>        sub_dict[<strong class="lc ir">'score'</strong>].append(post.score)<br/>        sub_dict[<strong class="lc ir">'ups'</strong>].append(post.ups)<br/>        sub_dict[<strong class="lc ir">'downs'</strong>].append(post.downs)<br/>    sleep(0.1)</span></pre><p id="576c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您计划多次使用这个脚本来收集大量的帖子，我们需要检查每个帖子是否是唯一的，或者我们是否已经将它添加到我们的。csv 文件。这就是我们的布尔变量 csv_loaded 派上用场的地方。我们将检查文章的 id 属性是否在数据框(df)的“id”列中。如果这是我们第一次为此子编辑收集帖子，那么 unique_id 将为每个帖子设置为 True。然后，我们将把我们感兴趣的文章的每个属性添加到占位符字典中。最后，也可能是最重要的，我们会在每次发布后睡十分之一秒。这是一种人为的限速器。如果我们不这样做，我们最终会达到 API 的请求限制(1000)。有几种方法可以绕过这个限制，其中之一是<a class="ae kw" href="https://praw.readthedocs.io/en/latest/tutorials/refresh_token.html" rel="noopener ugc nofollow" target="_blank">请求一个刷新令牌</a>，但是这暂时应该可以正常工作。</p><p id="65df" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我们将把结果保存到. csv 文件中，或者根据我们之前是否抓取过这个子编辑来修改现有的文件:</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="0d49" class="lg lh iq lc b gy li lj l lk ll">new_df = pd.DataFrame(sub_dict)<br/><br/><em class="ln"># Add new_df to df if df exists then save it to a csv.<br/></em>if <strong class="lc ir">'DataFrame' </strong>in str(type(df)) and self.mode == <strong class="lc ir">'w'</strong>:<br/>    pd.concat([df, new_df], axis=0, sort=0).to_csv(csv, index=False)<br/>    print(<br/>        <strong class="lc ir">f'{len(new_df)} new posts collected and added to {csv}'</strong>)<br/>elif self.mode == <strong class="lc ir">'w'</strong>:<br/>    new_df.to_csv(csv, index=False)<br/>    print(<strong class="lc ir">f'{len(new_df)} posts collected and saved to {csv}'</strong>)<br/>else:<br/>    print(<br/>        <strong class="lc ir">f'{len(new_df)} posts were collected but they were not '<br/>        f'added to {csv} because mode was set to "{self.mode}"'</strong>)</span></pre><p id="b4fa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里我们使用 pandas concat 方法，以便将我们的新结果添加到现有结果中。如果存在 csv 文件，并且在实例化我们的类时 mode 被设置为“w ”,那么我们将它加载到 df 数据帧中。如果没有现存的。我们将把结果写入这个子编辑的 csv 文件。如果模式没有设置为“w ”,我们将打印出找到的帖子数量，而不是将它们写入文件。</p><p id="7d1e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这就是我们完善 SubredditScraper 类所需的全部内容。剩下要做的最后一件事是实例化我们的新类。</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="23d9" class="lg lh iq lc b gy li lj l lk ll">if __name__ == <strong class="lc ir">'__main__'</strong>:<br/>    SubredditScraper(<br/>        <strong class="lc ir">'python'</strong>,<br/>         lim=997,<br/>         mode=<strong class="lc ir">'w'</strong>,<br/>         sort=<strong class="lc ir">'new'</strong>).get_posts()</span></pre><p id="5355" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你不熟悉<code class="fe lo lp lq lc b">if __name__ == '__main__':</code>的用法，我建议看看<a class="ae kw" href="https://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g" rel="noopener ugc nofollow" target="_blank">科里·斯查费的</a>关于这个话题的视频<a class="ae kw" href="https://www.youtube.com/watch?v=sugvnHA7ElY" rel="noopener ugc nofollow" target="_blank">这里</a>。这样，您应该能够运行这个脚本，从 python subreddit 中收集近 1000 篇帖子，并将这些帖子及其一些元数据保存到一个. csv 文件中。我希望这有所帮助！别忘了你可以在这里找到这个项目的完整代码。</p></div></div>    
</body>
</html>