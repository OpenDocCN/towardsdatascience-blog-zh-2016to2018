<html>
<head>
<title>Machine Learning for Diabetes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">糖尿病的机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-for-diabetes-562dd7df4d42?source=collection_archive---------1-----------------------#2017-12-17">https://towardsdatascience.com/machine-learning-for-diabetes-562dd7df4d42?source=collection_archive---------1-----------------------#2017-12-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ca8d671e11b9e020b338f48808f5ce65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TJapPdGxjpOh5FxDwG18rQ.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo credit: Pixabay</figcaption></figure><p id="9041" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">根据疾病控制和预防中心的数据，现在大约七分之一的美国成年人患有糖尿病。但到2050年，这一比例可能会飙升至三分之一。考虑到这一点，这就是我们今天要做的事情:学习如何使用机器学习来帮助我们预测糖尿病。我们开始吧！</p><h1 id="5447" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据</h1><p id="a145" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">糖尿病数据集源自<a class="ae la" href="http://archive.ics.uci.edu/ml/index.php" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库</a>，可以从<a class="ae la" href="https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/diabetes.csv" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5940" class="mn lc iq mj b gy mo mp l mq mr">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="3ae1" class="mn lc iq mj b gy ms mp l mq mr">diabetes = pd.read_csv('diabetes.csv')<br/>print(diabetes.columns)</span></pre><p id="0162" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">指数(['妊娠'，'血糖'，'血压'，'皮肤厚度'，'胰岛素'，'身体质量指数'，'糖尿病血糖'，'年龄'，'结果']，dtype= '对象')</em> </strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="e9ad" class="mn lc iq mj b gy mo mp l mq mr">diabetes.head()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mu"><img src="../Images/4ab9974de8b84979be89b319222ee61c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nj8BZurnL38BJzVrcSryWQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1</figcaption></figure><p id="fc50" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">糖尿病数据集由768个数据点组成，每个数据点有9个特征:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4874" class="mn lc iq mj b gy mo mp l mq mr">print("dimension of diabetes data: {}".format(diabetes.shape))</span></pre><p id="bd9c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">糖尿病数据的维度:(768，9) </em> </strong></p><p id="b1d0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">“结局”是我们要预测的特征，0表示没有糖尿病，1表示有糖尿病。在这768个数据点中，500个被标记为0，268个被标记为1:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4376" class="mn lc iq mj b gy mo mp l mq mr">print(diabetes.groupby('Outcome').size())</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/5a5199e3a34846c24f43cbdc112bf20e.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*BvYjMIE0buV2srXr4T3jDw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2</figcaption></figure><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="b15d" class="mn lc iq mj b gy mo mp l mq mr">import seaborn as sns</span><span id="0a63" class="mn lc iq mj b gy ms mp l mq mr">sns.countplot(diabetes['Outcome'],label="Count")</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/f90d865a442f2c81975441511562f528.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*XnHW5W71ZYlp20pimC5RXw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3</figcaption></figure><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="3ecf" class="mn lc iq mj b gy mo mp l mq mr">diabetes.info()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/29b36c645b757a0201255a8920a4bd6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*HWBun7NOFurZde01yG2NXQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4</figcaption></figure><h1 id="6f6e" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">k-最近邻</h1><p id="22e8" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">k-NN算法可以说是最简单的机器学习算法。构建模型仅包括存储训练数据集。为了对新的数据点进行预测，该算法会在训练数据集中查找最近的数据点，即它的“最近邻居”</p><p id="8ca7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">首先，让我们调查一下我们是否能够确认模型复杂性和准确性之间的联系:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="3cc5" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.model_selection import train_test_split</span><span id="6189" class="mn lc iq mj b gy ms mp l mq mr">X_train, X_test, y_train, y_test = train_test_split(diabetes.loc[:, diabetes.columns != 'Outcome'], diabetes['Outcome'], stratify=diabetes['Outcome'], random_state=66)</span><span id="3a2c" class="mn lc iq mj b gy ms mp l mq mr">from sklearn.neighbors import KNeighborsClassifier</span><span id="b4a6" class="mn lc iq mj b gy ms mp l mq mr">training_accuracy = []<br/>test_accuracy = []<br/># try n_neighbors from 1 to 10<br/>neighbors_settings = range(1, 11)</span><span id="449c" class="mn lc iq mj b gy ms mp l mq mr">for n_neighbors in neighbors_settings:<br/>    # build the model<br/>    knn = KNeighborsClassifier(n_neighbors=n_neighbors)<br/>    knn.fit(X_train, y_train)<br/>    # record training set accuracy<br/>    training_accuracy.append(knn.score(X_train, y_train))<br/>    # record test set accuracy<br/>    test_accuracy.append(knn.score(X_test, y_test))</span><span id="814a" class="mn lc iq mj b gy ms mp l mq mr">plt.plot(neighbors_settings, training_accuracy, label="training accuracy")<br/>plt.plot(neighbors_settings, test_accuracy, label="test accuracy")<br/>plt.ylabel("Accuracy")<br/>plt.xlabel("n_neighbors")<br/>plt.legend()<br/>plt.savefig('knn_compare_model')</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/68b6ba0bc542e03f2304923e37aee7f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*UYHrGtzv4gnfrbzwrAuUyA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 5</figcaption></figure><p id="8aa8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">上图显示了y轴上的训练集和测试集精度与x轴上的n_neighbors设置的对比。考虑到如果我们选择一个单一的最近邻，对训练集的预测是完美的。但是当考虑更多的邻居时，训练精度下降，这表明使用单个最近邻居会导致模型过于复杂。最好的表现是大约9个邻居。</p><p id="a3a1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">该图建议我们应该选择n_neighbors=9。我们到了:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="2d48" class="mn lc iq mj b gy mo mp l mq mr">knn = KNeighborsClassifier(n_neighbors=9)<br/>knn.fit(X_train, y_train)</span><span id="99d3" class="mn lc iq mj b gy ms mp l mq mr">print('Accuracy of K-NN classifier on training set: {:.2f}'.format(knn.score(X_train, y_train)))<br/>print('Accuracy of K-NN classifier on test set: {:.2f}'.format(knn.score(X_test, y_test)))</span></pre><p id="a51b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mt">K-NN分类器在训练集上的准确率:0.79 </em> </strong></p><p id="cb3e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mt">K-NN分类器在测试集上的准确率:0.78 </em> </strong></p><h1 id="f293" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">逻辑回归</h1><p id="8df1" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">逻辑回归是最常见的分类算法之一。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="02be" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.linear_model import LogisticRegression</span><span id="5c0f" class="mn lc iq mj b gy ms mp l mq mr">logreg = LogisticRegression().fit(X_train, y_train)<br/>print("Training set score: {:.3f}".format(logreg.score(X_train, y_train)))<br/>print("Test set score: {:.3f}".format(logreg.score(X_test, y_test)))</span></pre><p id="04d8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集精度:0.781 </em> </strong></p><p id="9995" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试设定精度:0.771 </em> </strong></p><p id="90f7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">C=1的默认值提供了78%的训练准确率和77%的测试准确率。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="a1b2" class="mn lc iq mj b gy mo mp l mq mr">logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)<br/>print("Training set accuracy: {:.3f}".format(logreg001.score(X_train, y_train)))<br/>print("Test set accuracy: {:.3f}".format(logreg001.score(X_test, y_test)))</span></pre><p id="ab0b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练设定精度:0.700 </em> </strong></p><p id="caed" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试设定精度:0.703 </em> </strong></p><p id="0861" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">使用C=0.01会导致训练集和测试集的精确度都较低。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="a331" class="mn lc iq mj b gy mo mp l mq mr">logreg100 = LogisticRegression(C=100).fit(X_train, y_train)<br/>print("Training set accuracy: {:.3f}".format(logreg100.score(X_train, y_train)))<br/>print("Test set accuracy: {:.3f}".format(logreg100.score(X_test, y_test)))</span></pre><p id="76de" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集精度:0.785 </em> </strong></p><p id="ab2b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试设定精度:0.766 </em> </strong></p><p id="6cb1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">使用C=100会导致训练集的精度稍微高一点，而测试集的精度稍微低一点，这证实了正则化程度较低且更复杂的模型可能不会比默认设置概括得更好。</p><p id="fa62" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因此，我们应该选择默认值C=1。</p><p id="135c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们用正则化参数c的三种不同设置来可视化模型学习的系数。</p><p id="b99b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">更强的正则化(C=0.001)将系数越来越推向零。更仔细地观察该图，我们还可以看到特征“DiabetesPedigreeFunction”，对于C=100、C=1和C=0.001，系数为正。这表明高“糖尿病”特征与样本是“糖尿病”相关，不管我们看的是哪种模型。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5317" class="mn lc iq mj b gy mo mp l mq mr">diabetes_features = [x for i,x in enumerate(diabetes.columns) if i!=8]</span><span id="813f" class="mn lc iq mj b gy ms mp l mq mr">plt.figure(figsize=(8,6))<br/>plt.plot(logreg.coef_.T, 'o', label="C=1")<br/>plt.plot(logreg100.coef_.T, '^', label="C=100")<br/>plt.plot(logreg001.coef_.T, 'v', label="C=0.001")<br/>plt.xticks(range(diabetes.shape[1]), diabetes_features, rotation=90)<br/>plt.hlines(0, 0, diabetes.shape[1])<br/>plt.ylim(-5, 5)<br/>plt.xlabel("Feature")<br/>plt.ylabel("Coefficient magnitude")<br/>plt.legend()<br/>plt.savefig('log_coef')</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/a6c7492dddac2adea2b72eacfc37d2bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*DW7K-hn3jGv70ME8vrMfFw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 6</figcaption></figure><h1 id="f1eb" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">决策图表</h1><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4e7c" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.tree import DecisionTreeClassifier</span><span id="9f6b" class="mn lc iq mj b gy ms mp l mq mr">tree = DecisionTreeClassifier(random_state=0)<br/>tree.fit(X_train, y_train)<br/>print("Accuracy on training set: {:.3f}".format(tree.score(X_train, y_train)))<br/>print("Accuracy on test set: {:.3f}".format(tree.score(X_test, y_test)))</span></pre><p id="0154" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集上的精度:1.000 </em> </strong></p><p id="814c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集上的精度:0.714 </em> </strong></p><p id="444c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">训练集的准确率为100%，而测试集的准确率要差得多。这表明该树过拟合，不能很好地推广到新数据。因此，我们需要对树进行预修剪。</p><p id="536d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们设置max_depth=3，限制树的深度可以减少过度拟合。这导致训练集的精确度较低，但测试集的精确度有所提高。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1c2d" class="mn lc iq mj b gy mo mp l mq mr">tree = DecisionTreeClassifier(max_depth=3, random_state=0)<br/>tree.fit(X_train, y_train)</span><span id="836b" class="mn lc iq mj b gy ms mp l mq mr">print("Accuracy on training set: {:.3f}".format(tree.score(X_train, y_train)))<br/>print("Accuracy on test set: {:.3f}".format(tree.score(X_test, y_test)))</span></pre><p id="8d46" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集上的准确率:0.773 </em> </strong></p><p id="c611" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集精度:0.740 </em> </strong></p><h1 id="4b3d" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">决策树中的特征重要性</h1><p id="91c5" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">特征重要性评定每个特征对于树所做决策的重要性。对于每个特征，它是一个介于0和1之间的数字，其中0表示“完全没有使用”，1表示“完美地预测了目标”。特征重要性的总和总是1:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="cadb" class="mn lc iq mj b gy mo mp l mq mr">print("Feature importances:\n{}".format(tree.feature_importances_))</span></pre><p id="38c7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">特征重要度:[ 0.04554275 0.6830362 0。0.0.0.27142106 0.0.】</em> </strong></p><p id="5289" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">然后，我们可以将特性的重要性可视化:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="a016" class="mn lc iq mj b gy mo mp l mq mr">def plot_feature_importances_diabetes(model):<br/>    plt.figure(figsize=(8,6))<br/>    n_features = 8<br/>    plt.barh(range(n_features), model.feature_importances_, align='center')<br/>    plt.yticks(np.arange(n_features), diabetes_features)<br/>    plt.xlabel("Feature importance")<br/>    plt.ylabel("Feature")<br/>    plt.ylim(-1, n_features)</span><span id="6f9d" class="mn lc iq mj b gy ms mp l mq mr">plot_feature_importances_diabetes(tree)<br/>plt.savefig('feature_importance')</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/d5bd315bd088c910cf8bae818fe8e1fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*V_KascT8ngbztT4RKX6rHg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 7</figcaption></figure><p id="eb40" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">特征“葡萄糖”是迄今为止最重要的特征。</p><h1 id="8ced" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">随机森林</strong></h1><p id="c442" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">让我们对糖尿病数据集应用一个由100棵树组成的随机森林:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="08da" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.ensemble import RandomForestClassifier</span><span id="6868" class="mn lc iq mj b gy ms mp l mq mr">rf = RandomForestClassifier(n_estimators=100, random_state=0)<br/>rf.fit(X_train, y_train)<br/>print("Accuracy on training set: {:.3f}".format(rf.score(X_train, y_train)))<br/>print("Accuracy on test set: {:.3f}".format(rf.score(X_test, y_test)))</span></pre><p id="2df2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集上的精度:1.000 </em> </strong></p><p id="c102" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集上的精度:0.786 </em> </strong></p><p id="7d1c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在不调整任何参数的情况下，随机森林给我们提供了78.6%的准确性，优于逻辑回归模型或单个决策树。但是，我们可以调整max_features设置，看看结果是否可以改进。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5fcc" class="mn lc iq mj b gy mo mp l mq mr">rf1 = RandomForestClassifier(max_depth=3, n_estimators=100, random_state=0)<br/>rf1.fit(X_train, y_train)<br/>print("Accuracy on training set: {:.3f}".format(rf1.score(X_train, y_train)))<br/>print("Accuracy on test set: {:.3f}".format(rf1.score(X_test, y_test)))</span></pre><p id="11b0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集上的精度:0.800 </em> </strong></p><p id="113a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集精度:0.755 </em> </strong></p><p id="6e1b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">它没有，这表明随机森林的默认参数工作良好。</p><h1 id="3a26" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">随机森林中的特征重要性</strong></h1><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="0c53" class="mn lc iq mj b gy mo mp l mq mr">plot_feature_importances_diabetes(rf)</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/39e585987fbfef20d5bea1c3b251c62d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*byVvzSfmfNH7z7mnb9WFBQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 8</figcaption></figure><p id="e395" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">与单一决策树类似，随机森林也非常重视“葡萄糖”特征，但它也选择“身体质量指数”作为第二大信息特征。构建随机森林的随机性迫使算法考虑许多可能的解释，结果是随机森林比单棵树捕捉到更广泛的数据图像。</p><h1 id="77de" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">梯度推进</strong></h1><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="0aba" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.ensemble import GradientBoostingClassifier</span><span id="64e7" class="mn lc iq mj b gy ms mp l mq mr">gb = GradientBoostingClassifier(random_state=0)<br/>gb.fit(X_train, y_train)</span><span id="988e" class="mn lc iq mj b gy ms mp l mq mr">print("Accuracy on training set: {:.3f}".format(gb.score(X_train, y_train)))<br/>print("Accuracy on test set: {:.3f}".format(gb.score(X_test, y_test)))</span></pre><p id="7e25" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">在训练集上的精度:0.917 </em> </strong></p><p id="6b46" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集上的精度:0.792 </em> </strong></p><p id="3c7c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们可能会过度适应。为了减少过度拟合，我们可以通过限制最大深度来应用更强的预修剪，或者降低学习速率:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="7c5c" class="mn lc iq mj b gy mo mp l mq mr">gb1 = GradientBoostingClassifier(random_state=0, max_depth=1)<br/>gb1.fit(X_train, y_train)</span><span id="ab7d" class="mn lc iq mj b gy ms mp l mq mr">print("Accuracy on training set: {:.3f}".format(gb1.score(X_train, y_train)))<br/>print("Accuracy on test set: {:.3f}".format(gb1.score(X_test, y_test)))</span></pre><p id="1cd4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集上的精度:0.804 </em> </strong></p><p id="d1c9" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集上的精度:0.781 </em> </strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="3c9d" class="mn lc iq mj b gy mo mp l mq mr">gb2 = GradientBoostingClassifier(random_state=0, learning_rate=0.01)<br/>gb2.fit(X_train, y_train)</span><span id="4126" class="mn lc iq mj b gy ms mp l mq mr">print("Accuracy on training set: {:.3f}".format(gb2.score(X_train, y_train)))<br/>print("Accuracy on test set: {:.3f}".format(gb2.score(X_test, y_test)))</span></pre><p id="4cad" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集上的精度:0.802 </em> </strong></p><p id="095e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集上的精度:0.776 </em> </strong></p><p id="e04a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">正如所料，降低模型复杂性的两种方法都降低了训练集的准确性。然而，在这种情况下，这些方法都没有提高测试集的泛化性能。</p><p id="4d1d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">即使我们对模型并不满意，我们也可以将特性的重要性可视化，以便更深入地了解我们的模型:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="e662" class="mn lc iq mj b gy mo mp l mq mr">plot_feature_importances_diabetes(gb1)</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/c2cdde744f96561c5cc8c8954893ccdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*ZuWqC_WBmsHUUwgI5dH0oA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 9</figcaption></figure><p id="2df3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们可以看到，梯度增强树的特征重要性有点类似于随机森林的特征重要性，在这种情况下，它对所有特征进行加权。</p><h1 id="5a50" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">支持向量机</strong></h1><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="e1db" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.svm import SVC</span><span id="7695" class="mn lc iq mj b gy ms mp l mq mr">svc = SVC()<br/>svc.fit(X_train, y_train)</span><span id="9566" class="mn lc iq mj b gy ms mp l mq mr">print("Accuracy on training set: {:.2f}".format(svc.score(X_train, y_train)))<br/>print("Accuracy on test set: {:.2f}".format(svc.score(X_test, y_test)))</span></pre><p id="0e56" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集上的精度:1.00 </em> </strong></p><p id="e1c4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集精度:0.65 </em> </strong></p><p id="fd49" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">该模型在很大程度上过度拟合，在训练集上得到满分，而在测试集上只有65%的准确率。</p><p id="f52a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">SVM要求所有的特征在相似的尺度上变化。我们需要重新调整我们的数据，使所有要素的比例大致相同:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5dfb" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.preprocessing import MinMaxScaler</span><span id="6613" class="mn lc iq mj b gy ms mp l mq mr">scaler = MinMaxScaler()<br/>X_train_scaled = scaler.fit_transform(X_train)<br/>X_test_scaled = scaler.fit_transform(X_test)</span><span id="b6ee" class="mn lc iq mj b gy ms mp l mq mr">svc = SVC()<br/>svc.fit(X_train_scaled, y_train)</span><span id="3722" class="mn lc iq mj b gy ms mp l mq mr">print("Accuracy on training set: {:.2f}".format(svc.score(X_train_scaled, y_train)))<br/>print("Accuracy on test set: {:.2f}".format(svc.score(X_test_scaled, y_test)))</span></pre><p id="0caa" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集上的精度:0.77 </em> </strong></p><p id="757a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集上的精度:0.77 </em> </strong></p><p id="b9f7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">缩放数据产生了巨大的差异！现在，我们实际上是欠拟合的，训练集和测试集的性能非常相似，但不太接近100%的准确性。从这里开始，我们可以尝试增加C或gamma来适应更复杂的模型。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="53a9" class="mn lc iq mj b gy mo mp l mq mr">svc = SVC(C=1000)<br/>svc.fit(X_train_scaled, y_train)</span><span id="2f86" class="mn lc iq mj b gy ms mp l mq mr">print("Accuracy on training set: {:.3f}".format(<br/>    svc.score(X_train_scaled, y_train)))<br/>print("Accuracy on test set: {:.3f}".format(svc.score(X_test_scaled, y_test)))</span></pre><p id="652c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集上的精度:0.790 </em> </strong></p><p id="a39b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集精度:0.797 </em> </strong></p><p id="f426" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这里，增加C允许我们改进模型，导致79.7%的测试集准确性。</p><h1 id="f0fa" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">深度学习</h1><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="6ec5" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.neural_network import MLPClassifier</span><span id="5830" class="mn lc iq mj b gy ms mp l mq mr">mlp = MLPClassifier(random_state=42)<br/>mlp.fit(X_train, y_train)</span><span id="e39c" class="mn lc iq mj b gy ms mp l mq mr">print("Accuracy on training set: {:.2f}".format(mlp.score(X_train, y_train)))<br/>print("Accuracy on test set: {:.2f}".format(mlp.score(X_test, y_test)))</span></pre><p id="025c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集上的精度:0.71 </em> </strong></p><p id="0816" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集精度:0.67 </em> </strong></p><p id="7647" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">多层感知器(MLP)的准确性不如其他模型，这可能是由于数据的缩放。深度学习算法还期望所有输入特征以相似的方式变化，并且理想情况下具有0的均值和1的方差。我们必须重新调整我们的数据，以满足这些要求。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="679c" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.preprocessing import StandardScaler</span><span id="89ee" class="mn lc iq mj b gy ms mp l mq mr">scaler = StandardScaler()<br/>X_train_scaled = scaler.fit_transform(X_train)<br/>X_test_scaled = scaler.fit_transform(X_test)</span><span id="7385" class="mn lc iq mj b gy ms mp l mq mr">mlp = MLPClassifier(random_state=0)<br/>mlp.fit(X_train_scaled, y_train)</span><span id="c0d2" class="mn lc iq mj b gy ms mp l mq mr">print("Accuracy on training set: {:.3f}".format(<br/>    mlp.score(X_train_scaled, y_train)))<br/>print("Accuracy on test set: {:.3f}".format(mlp.score(X_test_scaled, y_test)))</span></pre><p id="b111" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集上的准确率:0.823 </em> </strong></p><p id="4c31" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集精度:0.802 </em> </strong></p><p id="e78c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们增加迭代次数:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5b87" class="mn lc iq mj b gy mo mp l mq mr">mlp = MLPClassifier(max_iter=1000, random_state=0)<br/>mlp.fit(X_train_scaled, y_train)</span><span id="109a" class="mn lc iq mj b gy ms mp l mq mr">print("Accuracy on training set: {:.3f}".format(<br/>    mlp.score(X_train_scaled, y_train)))<br/>print("Accuracy on test set: {:.3f}".format(mlp.score(X_test_scaled, y_test)))</span></pre><p id="b19b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集上的准确率:0.877 </em> </strong></p><p id="13cd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集上的精度:0.755 </em> </strong></p><p id="77d6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">增加迭代次数只会提高训练集的性能，而不会提高测试集的性能。</p><p id="4c2a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们增加alpha参数并增加权重的更强正则化:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="336b" class="mn lc iq mj b gy mo mp l mq mr">mlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)<br/>mlp.fit(X_train_scaled, y_train)</span><span id="bb86" class="mn lc iq mj b gy ms mp l mq mr">print("Accuracy on training set: {:.3f}".format(<br/>    mlp.score(X_train_scaled, y_train)))<br/>print("Accuracy on test set: {:.3f}".format(mlp.score(X_test_scaled, y_test)))</span></pre><p id="73ab" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集上的准确率:0.795 </em> </strong></p><p id="413d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集精度:0.792 </em> </strong></p><p id="7833" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">结果是好的，但是我们不能进一步提高测试精度。</p><p id="d456" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因此，我们目前为止最好的模型是缩放后的默认深度学习模型。</p><p id="32b0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">最后，我们绘制了在糖尿病数据集上学习的神经网络中第一层权重的热图。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4a0c" class="mn lc iq mj b gy mo mp l mq mr">plt.figure(figsize=(20, 5))<br/>plt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis')<br/>plt.yticks(range(8), diabetes_features)<br/>plt.xlabel("Columns in weight matrix")<br/>plt.ylabel("Input feature")<br/>plt.colorbar()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nd"><img src="../Images/17226dc88cbc3c4f81b954eb49edb904.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1YcjNStS0LtCI8f_zgtRpA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 10</figcaption></figure><p id="daac" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">从热点图中，很难快速指出哪些要素与其他要素相比权重相对较低。</p><h1 id="6e30" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">总结</strong></h1><p id="ed7b" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">我们练习了一系列用于分类和回归的机器学习模型，它们的优点和缺点是什么，以及如何控制每个模型的复杂性。我们看到，对于许多算法来说，设置正确的参数对于良好的性能非常重要。</p><p id="ac55" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们应该能够知道如何应用、调整和分析我们上面实践的模型。现在轮到你了！尝试将这些算法应用于scikit-learn中的内置数据集或您选择的任何数据集。机器学习快乐！</p><p id="9870" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">创建这篇文章的源代码可以在<a class="ae la" href="https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Machine%20Learning%20for%20Diabetes.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。我将很高兴收到关于上述任何反馈或问题。</p><p id="a3aa" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">参考:<a class="ae la" href="http://shop.oreilly.com/product/0636920030515.do" rel="noopener ugc nofollow" target="_blank">用Python进行机器学习的介绍</a></p></div></div>    
</body>
</html>