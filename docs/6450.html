<html>
<head>
<title>Predicting Stars, Galaxies &amp; Quasars with Random Forest Classifiers in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 中的随机森林分类器预测恒星、星系和类星体</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-stars-galaxies-quasars-with-random-forest-classifiers-in-python-edb127878e43?source=collection_archive---------10-----------------------#2018-12-14">https://towardsdatascience.com/predicting-stars-galaxies-quasars-with-random-forest-classifiers-in-python-edb127878e43?source=collection_archive---------10-----------------------#2018-12-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7a0b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">来自斯隆数字巡天数据集的见解</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/71c91cf975b5a9202bb37d22df7c5d30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1myEmnrSb422MwLbCWtNrA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The SDSS website banner!</figcaption></figure><p id="2296" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最近，在我寻找有趣(物理学相关)数据集的过程中，我偶然发现了 Kaggle 上的<a class="ae lr" href="https://www.kaggle.com/lucidlenn/sloan-digital-sky-survey" rel="noopener ugc nofollow" target="_blank">斯隆数字巡天(SDSS)数据集</a>，以及 Faraz Rahman 的<a class="ae lr" href="https://www.kaggle.com/farazrahman/predicting-star-galaxy-quasar-with-svm/" rel="noopener ugc nofollow" target="_blank">辉煌 Kaggle 内核</a>，它使用 R 中的支持向量机预测了不同类型的天文物体(恒星、星系和类星体)。然而，由于<a class="ae lr" href="https://www-teaching.physics.ox.ac.uk/practical_course/scripts/" rel="noopener ugc nofollow" target="_blank"> R 带回了一些可怕的记忆</a>和<a class="ae lr" href="https://stackoverflow.com/questions/16585465/training-complexity-of-linear-svm" rel="noopener ugc nofollow" target="_blank">训练 SVM 需要大量的计算工作</a>，我决定使用 sci 来尝试一下这个分类问题</p><h1 id="8579" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据概述</h1><h2 id="7365" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">标签</h2><p id="3ad1" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">那么恒星、星系和类星体到底是什么？如果你在开始这个项目之前问我，我可能无法回答(真为我感到羞耻)。幸运的是，法拉兹的笔记本简明扼要地总结了它们是什么:</p><ul class=""><li id="938d" class="nb nc iq kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated"><em class="nk">一个</em> <strong class="kx ir">星系</strong> <em class="nk">是一个由恒星、恒星残骸、星际气体、尘埃和暗物质组成的引力束缚系统。星系根据它们的视觉形态分为椭圆形、螺旋形和不规则形。许多星系被认为在其活动中心有超大质量黑洞。</em></li><li id="1cd5" class="nb nc iq kx b ky nl lb nm le nn li no lm np lq ng nh ni nj bi translated"><em class="nk"/><strong class="kx ir">恒星</strong> <em class="nk">是一种天文物体，由一个发光的等离子球体组成，通过自身重力聚集在一起。离地球最近的恒星是太阳。</em></li><li id="a06c" class="nb nc iq kx b ky nl lb nm le nn li no lm np lq ng nh ni nj bi translated"><em class="nk">一颗</em> <strong class="kx ir">类星体</strong> <em class="nk">又称准恒星天体，是一颗极其明亮的活动星系核(AGN)。类星体辐射的能量是巨大的。最强大的类星体的亮度超过 1041 瓦，是普通大星系如银河系的数千倍。</em></li></ul><p id="1a80" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">安德鲁·邦克教授关于<a class="ae lr" href="https://users.physics.ox.ac.uk/~Bunker/s26StarsGalaxies.htm" rel="noopener ugc nofollow" target="_blank">恒星和星系(S26) </a>的短选项课程的页面也是一个很好的资源。</p><h2 id="88a0" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">特征</h2><p id="2cbe" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">细节的细节可以在 Kaggle 数据集概述上找到。更重要的功能总结如下:</p><ul class=""><li id="0520" class="nb nc iq kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated">ra，dec —分别为赤经和赤纬</li><li id="d6b9" class="nb nc iq kx b ky nl lb nm le nn li no lm np lq ng nh ni nj bi translated">u，g，r，I，z-过滤波段(又称光度系统或天文星等)</li><li id="2882" class="nb nc iq kx b ky nl lb nm le nn li no lm np lq ng nh ni nj bi translated">运行、重新运行、camcol、字段—图像中字段的描述符(即 2048 x 1489 像素)</li><li id="08d9" class="nb nc iq kx b ky nl lb nm le nn li no lm np lq ng nh ni nj bi translated">红移——由于天文物体的运动，波长增加</li><li id="5a79" class="nb nc iq kx b ky nl lb nm le nn li no lm np lq ng nh ni nj bi translated">车牌——车牌号码</li><li id="4f1e" class="nb nc iq kx b ky nl lb nm le nn li no lm np lq ng nh ni nj bi translated">mjd —修正儒略历观测日期</li><li id="bf92" class="nb nc iq kx b ky nl lb nm le nn li no lm np lq ng nh ni nj bi translated">光纤 id —光纤 id</li></ul><h1 id="ae95" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">探索性分析</h1><p id="dd40" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">这里的分析遵循了法拉兹的分析。我会让观想自己说话。</p><h2 id="315a" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">加载库和数据</h2><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="bbf7" class="mk lt iq nr b gy nv nw l nx ny">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>%matplotlib inline</span><span id="27ca" class="mk lt iq nr b gy nz nw l nx ny">df = pd.read_csv("skyserver.csv")</span></pre><h2 id="1334" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">数据描述</h2><p id="52d1" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">自然，我从<code class="fe oa ob oc nr b">df.head()</code>、<code class="fe oa ob oc nr b">df.describe()</code>和<code class="fe oa ob oc nr b">df.info()</code>开始。<code class="fe oa ob oc nr b">df.info()</code>的输出如下所示:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="569a" class="mk lt iq nr b gy nv nw l nx ny">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 10000 entries, 0 to 9999<br/>Data columns (total 18 columns):<br/>objid        10000 non-null float64<br/>ra           10000 non-null float64<br/>dec          10000 non-null float64<br/>u            10000 non-null float64<br/>g            10000 non-null float64<br/>r            10000 non-null float64<br/>i            10000 non-null float64<br/>z            10000 non-null float64<br/>run          10000 non-null int64<br/>rerun        10000 non-null int64<br/>camcol       10000 non-null int64<br/>field        10000 non-null int64<br/>specobjid    10000 non-null float64<br/>class        10000 non-null object<br/>redshift     10000 non-null float64<br/>plate        10000 non-null int64<br/>mjd          10000 non-null int64<br/>fiberid      10000 non-null int64<br/>dtypes: float64(10), int64(7), object(1)<br/>memory usage: 1.4+ MB</span></pre><p id="c866" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">没有一个条目是 NaN，这是维护良好的数据集所期望的。清洁不是必须的。</p><h2 id="166b" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">唯一条目</h2><p id="4abe" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated"><code class="fe oa ob oc nr b">nunique()</code>方法返回包含每列唯一条目数量的 Series 对象。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="5cb5" class="mk lt iq nr b gy nv nw l nx ny">df.nunique().to_frame().transpose()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/f0b661e21a771478f3eb2cb07d97c7b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VnoaYib31MjoKHaC-ezQnA.png"/></div></div></figure><h2 id="4f70" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">每个天文实体的出现</h2><p id="bb35" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">然后我在班级栏上运行了<code class="fe oa ob oc nr b">value_counts()</code>。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="6212" class="mk lt iq nr b gy nv nw l nx ny">occurrences = df['class'].value_counts().to_frame().rename(index=str, columns={'class': 'Occurrences'})</span><span id="f822" class="mk lt iq nr b gy nz nw l nx ny">occurrences</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/a1b155c6f92e4b549df879371a9fcc6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*8ORW7mvCdeze5dtle64sxw.png"/></div></figure><p id="519b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们看到大多数条目不是星系就是恒星。只有 8.5%的条目被归类为类星体。</p><h2 id="cec9" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">密度分布图</h2><p id="f654" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">使用核密度估计(kde)，我绘制了各种特征的(平滑)密度分布。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="e0e3" class="mk lt iq nr b gy nv nw l nx ny">featuredf = df.drop(['class','objid'], axis=1)<br/>featurecols = list(featuredf)<br/>astrObjs = df['class'].unique()</span><span id="c8d8" class="mk lt iq nr b gy nz nw l nx ny">colours = ['indigo', '#FF69B4', 'cyan']</span><span id="4e00" class="mk lt iq nr b gy nz nw l nx ny">plt.figure(figsize=(15,10))<br/>for i in range(len(featurecols)):<br/>    plt.subplot(4, 4, i+1)<br/>    for j in range(len(astrObjs)):<br/>        sns.distplot(df[df['class']==astrObjs[j]][featurecols[i]], hist = False, kde = True, color = colours[j], kde_kws = {'shade': True, 'linewidth': 3}, label = astrObjs[j])<br/>    plt.legend()<br/>    plt.title('Density Plot')<br/>    plt.xlabel(featurecols[i])<br/>    plt.ylabel('Density')<br/>plt.tight_layout()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/03aba8ce969e14bfb10f689b0a0fdd88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aEus_oVldmse7fixnYwBJg.png"/></div></div></figure><p id="8de1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">还为每个类别绘制了滤波器频带密度。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="9e63" class="mk lt iq nr b gy nv nw l nx ny">filterbands = pd.concat([df.iloc[:,3:8], df['class']],axis=1)</span><span id="13aa" class="mk lt iq nr b gy nz nw l nx ny">plt.figure(figsize=(15,5))<br/>plt.suptitle('Density Plots')<br/>sns.set_style("ticks")<br/>for i in range(len(astrObjs)):<br/>    plt.subplot(1, 3, i+1)<br/>    for j in range(len(featurecols2)):<br/>        sns.distplot(df[df['class']==astrObjs[i]][featurecols2[j]], hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 3}, label = featurecols2[j])<br/>    plt.legend()<br/>    plt.xlabel(astrObjs[i])<br/>    plt.ylabel('Density')<br/>plt.tight_layout()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/312def1aedb3e23542f8262ff54f35a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3J8Y9hpUP22gTNgeW41cIg.png"/></div></div></figure><h2 id="7a78" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">附加可视化</h2><p id="9bca" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">为了完整起见，我包括一个 3D 图，与原始笔记本相同。最初的意图似乎是确定 SVM 的线性核是否有效(如果我错了，请纠正我)。底部有很多群集，我取了红移的对数(忽略误差)以使可视化更清晰。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="c804" class="mk lt iq nr b gy nv nw l nx ny">from mpl_toolkits.mplot3d import Axes3D</span><span id="f10d" class="mk lt iq nr b gy nz nw l nx ny">fig = plt.figure(figsize=(5,5))<br/>ax = Axes3D(fig)</span><span id="f7cf" class="mk lt iq nr b gy nz nw l nx ny">for obj in astrObjs:<br/>    luminous = df[df['class'] == obj]<br/>    ax.scatter(luminous['ra'], luminous['dec'], np.log10(luminous['redshift']))</span><span id="1744" class="mk lt iq nr b gy nz nw l nx ny">ax.set_xlabel('ra')<br/>ax.set_ylabel('dec')<br/>ax.set_zlabel('log redshift')</span><span id="ddfe" class="mk lt iq nr b gy nz nw l nx ny">ax.view_init(elev = 0, azim=45)</span><span id="5b5c" class="mk lt iq nr b gy nz nw l nx ny">plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/e7fc1aea4aa58de8a29b15204aa36325.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*odbdJhJPYv5eLHbm-OnKJA.png"/></div></figure><h1 id="9b61" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">构建随机森林分类器</h1><h2 id="0b7e" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">训练集和测试集分离</h2><p id="88f8" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">传统的列车测试分割可以通过以下方式完成:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="ff1d" class="mk lt iq nr b gy nv nw l nx ny">from sklearn.model_selection import train_test_split<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="30b7" class="mk lt iq nr b gy nz nw l nx ny">x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=123, stratify=labels)</span><span id="2c27" class="mk lt iq nr b gy nz nw l nx ny">clf = RandomForestClassifier()</span></pre><h2 id="c656" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">训练复杂性</h2><p id="1f2c" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">当我最初试图用<code class="fe oa ob oc nr b">sklearn.svm.linearSVC</code>训练我的数据时，我的笔记本电脑开始严重过热。训练时间复杂度一般在 O(mn)到 O(mn)之间，其中 m 是特征数，n 是观测数，如 Jessica Mick <a class="ae lr" href="https://datascience.stackexchange.com/questions/989/svm-using-scikit-learn-runs-endlessly-and-never-completes-execution" rel="noopener ugc nofollow" target="_blank">此处</a>所解释。另一方面，growing CART(分类和回归树)的训练复杂度为 O(mn logn)和 O(mn)，这里的<a class="ae lr" href="https://stats.stackexchange.com/questions/17616/literature-on-the-algorithm-for-optimal-splitting-in-the-growing-of-classificati" rel="noopener ugc nofollow" target="_blank">解释为</a>(随机森林是 CART 的系综)。由于时间、耐心和手头的咖啡有限，我决定换成随机森林模型。</p><p id="7e62" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">事后看来，我可以做的一件事是将我的数据缩放到[-1，1]来加速 SVM(甚至随机森林)，正如 Shelby Matlock <a class="ae lr" href="https://datascience.stackexchange.com/questions/989/svm-using-scikit-learn-runs-endlessly-and-never-completes-execution" rel="noopener ugc nofollow" target="_blank">在同一篇文章</a>中提到的。这样我也能得到更稳定的预测结果。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="361f" class="mk lt iq nr b gy nv nw l nx ny">from sklearn.preprocessing import MinMaxScaler<br/>scaling = MinMaxScaler(feature_range=(-1,1)).fit(x_train)<br/>x_train_scaled = scaling.transform(x_train)<br/>x_test_scaled = scaling.transform(x_test)</span></pre><h2 id="dbf5" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">超参数优化</h2><p id="4d58" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">对于超参数调谐，我发现<a class="ae lr" rel="noopener" target="_blank" href="/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74">这个</a>和<a class="ae lr" href="https://chrisalbon.com/machine_learning/model_selection/hyperparameter_tuning_using_random_search/" rel="noopener ugc nofollow" target="_blank">这个</a>相当方便。我们首先实例化一个随机森林，并查看可用超参数的默认值。美化打印<code class="fe oa ob oc nr b">get_params()</code>方法:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="a128" class="mk lt iq nr b gy nv nw l nx ny">from pprint import pprint<br/>pprint(clf.get_params())</span></pre><p id="2201" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这给出了:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="f81b" class="mk lt iq nr b gy nv nw l nx ny">{'bootstrap': True,<br/> 'class_weight': None,<br/> 'criterion': 'gini',<br/> 'max_depth': None,<br/> 'max_features': 'auto',<br/> 'max_leaf_nodes': None,<br/> 'min_impurity_decrease': 0.0,<br/> 'min_impurity_split': None,<br/> 'min_samples_leaf': 1,<br/> 'min_samples_split': 2,<br/> 'min_weight_fraction_leaf': 0.0,<br/> 'n_estimators': 10,<br/> 'n_jobs': None,<br/> 'oob_score': False,<br/> 'random_state': None,<br/> 'verbose': 0,<br/> 'warm_start': False}</span></pre><p id="0b6e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我决定关注的超参数是:</p><ul class=""><li id="7aa9" class="nb nc iq kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated"><code class="fe oa ob oc nr b">n_estimators</code>(森林中的树木数量)</li><li id="fecf" class="nb nc iq kx b ky nl lb nm le nn li no lm np lq ng nh ni nj bi translated"><code class="fe oa ob oc nr b">max_features</code> (max。节点分割中使用的特征数量，通常为。&lt;数据集中的特征数量)</li><li id="1cdc" class="nb nc iq kx b ky nl lb nm le nn li no lm np lq ng nh ni nj bi translated"><code class="fe oa ob oc nr b">max_depth</code>(最大。每个决策树中的层数)</li><li id="c398" class="nb nc iq kx b ky nl lb nm le nn li no lm np lq ng nh ni nj bi translated"><code class="fe oa ob oc nr b">min_samples_split</code>(敏。分割节点之前节点中的数据点数)</li><li id="50bb" class="nb nc iq kx b ky nl lb nm le nn li no lm np lq ng nh ni nj bi translated"><code class="fe oa ob oc nr b">min_samples_leaf</code>(最小。节点中允许的数据点数)</li><li id="45b7" class="nb nc iq kx b ky nl lb nm le nn li no lm np lq ng nh ni nj bi translated"><code class="fe oa ob oc nr b">criterion</code>(用于确定决策树停止标准的度量)</li></ul><h2 id="ef5d" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">使用随机搜索进行调整</h2><p id="3a21" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">为了缩小搜索范围，我首先进行了随机搜索交叉验证。在这里，我使用 k = 10 倍交叉验证(<code class="fe oa ob oc nr b">cv=10</code>)、跨越 100 个不同组合(<code class="fe oa ob oc nr b">n_iter=100</code>)并同时使用所有可用内核(<code class="fe oa ob oc nr b">n_jobs=-1</code>)来执行参数的随机搜索。随机搜索随机选择特征的组合，而不是遍历每个可能的组合。更高的<code class="fe oa ob oc nr b">n_iter</code>和<code class="fe oa ob oc nr b">cv</code>分别导致更多的组合和更小的过拟合可能性。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="5098" class="mk lt iq nr b gy nv nw l nx ny">from sklearn.model_selection import RandomizedSearchCV</span><span id="ad0e" class="mk lt iq nr b gy nz nw l nx ny">hyperparameters = {'max_features':[None, 'auto', 'sqrt', 'log2'],<br/>                   'max_depth':[None, 1, 5, 10, 15, 20],<br/>                   'min_samples_leaf': [1, 2, 4],<br/>                   'min_samples_split': [2, 5, 10],<br/>                   'n_estimators': [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)],<br/>                   'criterion': ['gini', 'entropy']}</span><span id="039e" class="mk lt iq nr b gy nz nw l nx ny">rf_random = RandomizedSearchCV(clf, hyperparameters, n_iter = 100, cv = 10, verbose=2, random_state=123, n_jobs = -1)</span><span id="ea2e" class="mk lt iq nr b gy nz nw l nx ny">rf_random.fit(x_train, y_train)</span></pre><p id="524d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一大堆东西出现了。为了获得最佳参数，我调用了:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="95c2" class="mk lt iq nr b gy nv nw l nx ny">rf_random.best_params_</span></pre><p id="45c8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这给出了:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="2206" class="mk lt iq nr b gy nv nw l nx ny">{'n_estimators': 100,<br/> 'min_samples_split': 5,<br/> 'min_samples_leaf': 2,<br/> 'max_features': None,<br/> 'max_depth': 15,<br/> 'criterion': 'entropy'}</span></pre><h2 id="7dee" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">使用网格搜索进行调整</h2><p id="3599" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">我现在可以指定更小范围的超参数来关注。GridSearchCV 非常适合超参数的微调。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="7917" class="mk lt iq nr b gy nv nw l nx ny">from sklearn.model_selection import GridSearchCV</span><span id="f296" class="mk lt iq nr b gy nz nw l nx ny">hyperparameters = {'max_features':[None],<br/>                   'max_depth':[14, 15, 16],<br/>                   'min_samples_leaf': [1, 2, 3],<br/>                   'min_samples_split': [4, 5, 6],<br/>                   'n_estimators': [90, 100, 110],<br/>                   'criterion': ['entropy']}</span><span id="a745" class="mk lt iq nr b gy nz nw l nx ny">rf_grid = GridSearchCV(clf, hyperparameters, cv = 10, n_jobs = -1, verbose = 2)<br/>rf_grid.fit(x_train, y_train)</span></pre><p id="101d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这花了我大约 50 分钟。我打了电话:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="ec56" class="mk lt iq nr b gy nv nw l nx ny">rf_grid.best_params_</span></pre><p id="7b22" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这返回了:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="d724" class="mk lt iq nr b gy nv nw l nx ny">{'criterion': 'entropy',<br/> 'max_depth': 14,<br/> 'max_features': None,<br/> 'min_samples_leaf': 2,<br/> 'min_samples_split': 5,<br/> 'n_estimators': 100}</span></pre><h2 id="c34d" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">训练分类器</h2><p id="c6d6" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">我最后用最佳超参数更新了分类器。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="db4b" class="mk lt iq nr b gy nv nw l nx ny">clf.set_params(criterion = 'entropy', max_features = None, max_depth = 14, min_samples_leaf = 2, min_samples_split = 5, n_estimators = 100)</span></pre><h1 id="dda6" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">测试和评估</h1><p id="83ca" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">然后，我在测试集上测试了更新后的分类器，并根据几个指标对其进行了评估。</p><h2 id="a32b" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">准确性得分和 F1 得分</h2><p id="3f30" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">注意，<code class="fe oa ob oc nr b">accuracy_score</code>是指校正预测的分数，<code class="fe oa ob oc nr b">f1_score</code>是精度(分类器不将阴性样本标记为阳性的能力)和召回率(分类器找到所有阳性样本的能力)的加权平均值。scikit-learn 文档最好地解释了这些概念:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/a7a604cfee5587a535c9ef051fe6bc55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5XNK0ZGD0dZgNVeGCfp1g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Formula for accuracy_score</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/bb3950ecfaad14358b21cc7f925b26dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BdQ-MJTd6P0QbNCblMGB3A.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/9d7d79117794d45dd35d9c9877132ca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4DSz4JZrje5SVB6spiuS9g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Formula for f1_score, where beta=1</figcaption></figure><p id="0ad8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><code class="fe oa ob oc nr b">sklearn.metrics</code>有这些现成的。列表<code class="fe oa ob oc nr b">f1_score</code>中分数的顺序对应于类的编码方式，这可以通过使用分类器的<code class="fe oa ob oc nr b">.classes_</code>属性来访问。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="1251" class="mk lt iq nr b gy nv nw l nx ny">from sklearn.metrics import accuracy_score, f1_score</span><span id="b18f" class="mk lt iq nr b gy nz nw l nx ny">sortedlabels = clf.classes_<br/>accscore = accuracy_score(y_test, y_pred)<br/>f1score = f1_score(y_test, y_pred, average = None)</span><span id="0a45" class="mk lt iq nr b gy nz nw l nx ny">print(accscore)<br/>for i in range:<br/>    print((sortedlabels[i],f1score[i]), end=" ")</span></pre><p id="e9ad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这将返回非常令人满意的分数。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="8635" class="mk lt iq nr b gy nv nw l nx ny">0.99<br/>('GALAXY', 0.9900265957446809) ('QSO', 0.9596774193548387) ('STAR', 0.9959935897435898)</span></pre><h2 id="9ed4" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">混淆矩阵</h2><p id="fefb" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">A <a class="ae lr" href="https://scikit-learn.org/dev/auto_examples/model_selection/plot_confusion_matrix.html" rel="noopener ugc nofollow" target="_blank">混淆矩阵</a> C 有矩阵元素 C_(i，j)对应于已知在 I 组但预测在 j 组的观测值的数量，换句话说，对角线元素代表正确的预测，而非对角线元素代表错误标记。我们的目标是得到混淆矩阵的大对角线值 C(I，I)。</p><p id="0d73" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们看一下分类报告和混淆矩阵。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="da96" class="mk lt iq nr b gy nv nw l nx ny">from sklearn.metrics import classification_report, confusion_matrix</span><span id="6ef0" class="mk lt iq nr b gy nz nw l nx ny">cm = confusion_matrix(y_test, y_pred, sortedlabels)</span><span id="6e19" class="mk lt iq nr b gy nz nw l nx ny">print(classification_report(y_test, y_pred))<br/>print(cm)</span></pre><p id="0f34" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这将返回:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="e6e3" class="mk lt iq nr b gy nv nw l nx ny">precision    recall  f1-score   support<br/><br/>      GALAXY       0.98      0.99      0.98      1499<br/>         QSO       0.95      0.89      0.92       255<br/>        STAR       0.99      1.00      1.00      1246<br/><br/>   micro avg       0.98      0.98      0.98      3000<br/>   macro avg       0.97      0.96      0.97      3000<br/>weighted avg       0.98      0.98      0.98      3000<br/><br/>[[1481   11    7]<br/> [  29  226    0]<br/> [   1    1 1244]]</span></pre><h2 id="0015" class="mk lt iq bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">可视化混淆矩阵</h2><p id="f4ee" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">我一般觉得有用(而且好看！)来绘制混淆矩阵。在我选择的 seaborn 调色板中，较深的颜色意味着更多的条目。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="efff" class="mk lt iq nr b gy nv nw l nx ny">cm = pd.DataFrame(cm, index=sortedlabels, columns=sortedlabels)</span><span id="c628" class="mk lt iq nr b gy nz nw l nx ny">sns.set(font_scale=1.2)<br/>sns.heatmap(cm, linewidths=0.5, cmap=sns.light_palette((1, 0.2, 0.6),n_colors=10000), annot=True)<br/>plt.xlabel('Predicted')<br/>plt.ylabel('True')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/d5f450886deb449f1e7046a3ee98b847.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G-9AJmNzfcpar2cJFxIM7w.png"/></div></div></figure><p id="d4f2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你有反馈/建设性的批评，请在下面随意评论！</p></div></div>    
</body>
</html>