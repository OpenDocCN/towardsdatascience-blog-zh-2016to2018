<html>
<head>
<title>Logistic Regression — Detailed Overview</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归—详细概述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc?source=collection_archive---------1-----------------------#2018-03-15">https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc?source=collection_archive---------1-----------------------#2018-03-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e03f665ff952a6f8c42a5a380db336b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UgYbimgPXf6XXxMy2yqRLw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1: Logistic Regression Model (Source:<a class="ae kc" href="http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/" rel="noopener ugc nofollow" target="_blank">http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/</a>)</figcaption></figure><p id="731f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">逻辑回归在二十世纪早期被用于生物科学。后来，它被用于许多社会科学应用中。当因变量(目标)是分类变量时，使用逻辑回归。</p><p id="87e6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">举个例子，</p><ul class=""><li id="395b" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">要预测电子邮件是垃圾邮件(1)还是(0)</li><li id="059b" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">肿瘤是恶性的(1)还是非恶性的(0)</li></ul><p id="065d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑一个场景，我们需要对一封电子邮件是否是垃圾邮件进行分类。如果我们对这个问题使用线性回归，就需要设置一个阈值，根据这个阈值可以进行分类。比方说，如果实际类别是恶性的，预测连续值为0.4，阈值为0.5，则数据点将被分类为非恶性的，这会导致实时的严重后果。</p><p id="b360" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从这个例子可以推断，线性回归不适合分类问题。线性回归是无限的，这就带来了逻辑回归。它们的值严格地在0到1之间。</p><p id="76e8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">简单逻辑回归</strong></p><p id="6bab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(完整源代码:<a class="ae kc" href="https://github.com/SSaishruthi/LogisticRegression_Vectorized_Implementation/blob/master/Logistic_Regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/SSaishruthi/LogisticRegression _ Vectorized _ Implementation/blob/master/Logistic _ regression . ipynb</a>)</p><p id="f8a5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lp">型号</em> </strong></p><p id="d8b4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出= 0或1</p><p id="00b3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设=&gt; Z = WX + B</p><p id="9348" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">hθ(x)= sigmoid(Z)</p><p id="e792" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lp">乙状结肠功能</em> </strong></p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lq"><img src="../Images/698cc061611efa530ea7fd9a56858818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RqXFpiNGwdiKBWyLJc_E7g.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2: Sigmoid Activation Function</figcaption></figure><p id="07ab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果‘Z’趋于无穷大，Y(预测)将变为1，如果‘Z’趋于负无穷大，Y(预测)将变为0。</p><p id="1eb5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lp">分析假设</em> </strong></p><p id="64e2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设的输出是估计的概率。这用于推断给定输入x时，预测值与实际值的可信度。考虑以下示例，</p><p id="2e5c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X = [x0 x1] = [1个IP地址]</p><p id="c3d8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基于x1值，假设我们获得的估计概率为0.8。这表明一封电子邮件有80%的可能是垃圾邮件。</p><p id="bfa0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从数学上讲，这可以写成:</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/6263feab47bad1993cea29e9e58f0dba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*i_QQvUzXCETJEelf4mLx8Q.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3: Mathematical Representation</figcaption></figure><p id="874b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这证明了“逻辑回归”这个名称的合理性。将数据拟合到线性回归模型中，然后由预测目标分类因变量的逻辑函数对其进行操作。</p><p id="4007" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lp">逻辑回归的类型</em> </strong></p><p id="df74" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1.二元逻辑回归</p><p id="4844" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">分类回答只有两种可能的结果。示例:垃圾邮件与否</p><p id="8c22" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.多项式逻辑回归</p><p id="d2a6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">三个或更多类别，无需排序。示例:预测哪种食物更受欢迎(素食、非素食、纯素食)</p><p id="0ce1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.有序逻辑回归</p><p id="d827" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">三个或三个以上的分类与排序。示例:电影等级从1到5</p><p id="5834" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lp">决定边界</em> </strong></p><p id="4303" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了预测数据属于哪一类，可以设置一个阈值。基于该阈值，将所获得的估计概率分类。</p><p id="daac" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">比如说，如果predicted_value ≥ 0.5，那么将邮件归类为垃圾邮件，否则归类为非垃圾邮件。</p><p id="d477" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">决策边界可以是线性或非线性的。多项式阶可以增加，以获得复杂的决策边界。</p><p id="30ed" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lp">成本函数</em> </strong></p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/bfb0b906c00f25351baebe8962fcdf67.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*TqZ9myxIdLuKNmt8orCeew.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4: Cost Function of Logistic Regression</figcaption></figure><p id="260d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为什么用于线性的成本函数不能用于逻辑？</p><p id="226d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">线性回归使用均方误差作为其成本函数。如果这用于逻辑回归，那么它将是参数(θ)的非凸函数。梯度下降只有在函数是凸的情况下才会收敛到全局极小值。</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/a8918c67da6465b87231b559904f7777.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*ZyjEj3A_QyR4WY7y5cwIWQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 5: Convex and non-convex cost function</figcaption></figure><p id="8f5a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lp">成本函数解释</em> </strong></p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ly"><img src="../Images/772e603063a690ddb3caad47b8e9479c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5AYaGPV-gjYUf37d2IhgTQ.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 6: Cost Function part 1</figcaption></figure><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lz"><img src="../Images/8e3e62ca61507ea290a4a1d1e9d4d675.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MFMIEUC_dobhJrRjGK7PBg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 7: Cost Function part 2</figcaption></figure><p id="532f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lp">简化成本函数</em> </strong></p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ma"><img src="../Images/09da1ca8b911384ad7620a92b299515e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ueEwU1dE0Yu-KpMJanf9AQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 8: Simplified Cost Function</figcaption></figure><p id="40cd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lp">为什么这个成本函数？</em>T15】</strong></p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mb"><img src="../Images/df68fd69bdd2645f1113489b18c88937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*heGae4aZ-dN-rLsfx2-P9g.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 9: Maximum Likelihood Explanation part-1</figcaption></figure><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mc"><img src="../Images/c677c1850a656591967e7c6f24c2358b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JIpaau-jFfvX2yR9L1YZ6A.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 10: Maximum Likelihood Explanation part-2</figcaption></figure><p id="6727" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个负函数是因为我们在训练的时候，需要通过最小化损失函数来最大化概率。假设样本是从完全独立的分布中抽取的，降低成本将增加最大可能性。</p><p id="776e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lp">推导梯度下降算法的公式</em> </strong></p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi md"><img src="../Images/4c4facbd957b49f02d96b6fcf083b46b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r7fhk417IOuq7meXIctGXg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 11: Gradient Descent Algorithm part 1</figcaption></figure><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/d44b999ac08fe4e87e906823f111d161.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pJEi5f4gdVGezYev9MChBw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 12: Gradient Descent part 2</figcaption></figure><p id="5f8f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lp"> Python实现</em> </strong></p><pre class="lr ls lt lu gt mf mg mh mi aw mj bi"><span id="e41e" class="mk ml iq mg b gy mm mn l mo mp">def weightInitialization(n_features):<br/>    w = np.zeros((1,n_features))<br/>    b = 0<br/>    return w,b</span><span id="52e5" class="mk ml iq mg b gy mq mn l mo mp">def sigmoid_activation(result):<br/>    final_result = 1/(1+np.exp(-result))<br/>    return final_result</span><span id="1700" class="mk ml iq mg b gy mq mn l mo mp"><br/>def model_optimize(w, b, X, Y):<br/>    m = X.shape[0]<br/>    <br/>    #Prediction<br/>    final_result = sigmoid_activation(np.dot(w,X.T)+b)<br/>    Y_T = Y.T<br/>    cost = (-1/m)*(np.sum((Y_T*np.log(final_result)) + ((1-Y_T)*(np.log(1-final_result)))))<br/>    #<br/>    <br/>    #Gradient calculation<br/>    dw = (1/m)*(np.dot(X.T, (final_result-Y.T).T))<br/>    db = (1/m)*(np.sum(final_result-Y.T))<br/>    <br/>    grads = {"dw": dw, "db": db}<br/>    <br/>    return grads, cost</span><span id="b507" class="mk ml iq mg b gy mq mn l mo mp">def model_predict(w, b, X, Y, learning_rate, no_iterations):<br/>    costs = []<br/>    for i in range(no_iterations):<br/>        #<br/>        grads, cost = model_optimize(w,b,X,Y)<br/>        #<br/>        dw = grads["dw"]<br/>        db = grads["db"]<br/>        #weight update<br/>        w = w - (learning_rate * (dw.T))<br/>        b = b - (learning_rate * db)<br/>        #<br/>        <br/>        if (i % 100 == 0):<br/>            costs.append(cost)<br/>            #print("Cost after %i iteration is %f" %(i, cost))<br/>    <br/>    #final parameters<br/>    coeff = {"w": w, "b": b}<br/>    gradient = {"dw": dw, "db": db}<br/>    <br/>    return coeff, gradient, costs</span><span id="a819" class="mk ml iq mg b gy mq mn l mo mp">def predict(final_pred, m):<br/>    y_pred = np.zeros((1,m))<br/>    for i in range(final_pred.shape[1]):<br/>        if final_pred[0][i] &gt; 0.5:<br/>            y_pred[0][i] = 1<br/>    return y_pred</span></pre><p id="73dc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">成本与迭代次数</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/325e4bad192c41f15104edf76f253c4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*uRaeTkF5Ig_DYZwR8HiJMQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 13: Cost Reduction</figcaption></figure><p id="bc19" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">系统的训练和测试准确率为100 %</p><p id="5ca6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该实现用于二元逻辑回归。对于超过2类的数据，必须使用softmax回归。</p><p id="d973" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个教育性的帖子，灵感来自吴恩达教授的深度学习课程。</p><p id="30f3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整代码:<a class="ae kc" href="https://github.com/SSaishruthi/LogisticRegression_Vectorized_Implementation/blob/master/Logistic_Regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/SSaishruthi/LogisticRegression _ Vectorized _ Implementation/blob/master/Logistic _ regression . ipynb</a></p></div></div>    
</body>
</html>