# 机器学习能为理论科学做些什么？

> 原文：<https://towardsdatascience.com/what-can-machine-learning-do-for-theoretical-science-ed16fbe8dc8e?source=collection_archive---------10----------------------->

![](img/df2b4291214b79dfb933cce610a3e9c9.png)

科学理论使这个世界变得可以理解，至少对我们大多数人来说是这样。但是后来我们听到了一个传言，说城里有一种新游戏:机器学习。连同它的兄弟，大数据，他们威胁要把科学理论赶出城镇。机器学习，尤其是深度学习，已经成为构建更加准确的预测模型的魔盒。利用它，人们可以根据以前观察到的模式进行预测。传统上，做预测是一件复杂的事情，除了其他事情之外，还包括发展理解事物如何运作的基本理论。但是现在你可以把足够多的数据扔给一个足够大的神经网络，你会从另一边得到预测。那么，为什么要为理论费心呢？

谣言很快就消散了，因为它是基于一个错误的前提，即科学的目标是做出预测。它不是。科学的目标是提供理解。理解来自解释，解释由理论提供。现代科学的整个大厦是建立在一张相互联系的理论网的基础上的。

谣言可能已经消失了，但它的幽灵仍在困扰着我们。老派理论家倾向于将这一新的经验主义浪潮视为平民对他们专业的攻击。而且，许多来自我们新民主化领域的分析能力较弱的新数据专家，似乎经常将理论与先入为主的偏见混为一谈。

对我个人来说，这种相当令人遗憾的事态……有些尴尬。我最初是一名理论物理学家。理论帮助我理解这个世界。然而，我现在靠修补机器学习算法谋生。我可以直接体会到这些算法的威力。是的，机器学习是一种工具，但它是独一无二的工具。它从根本上改变了我们与信息的关系。无论如何，我们对什么构成对现实的理解的概念将受到机器学习在科学中所扮演的角色的影响。

如果理性主义要在经验主义的洪流中生存下来，那么理论家需要找到一种方法，将机器学习有意义地融入他们的世界。不是作为一个处理挖掘数据的无脑苦差事的外国职员，而是作为一个完整的公民和建立科学理论艺术的向导。

这并不是一个奇怪的愿望。毕竟，我们如何存储、处理或传递信息的大多数重要进步，无论是新的数学技术还是电子计算机，都在科学理论的发展中得到了应用。没有理由认为机器学习应该是一个粗暴的例外。问题是，怎么做？

我们用来构建理论的模板很大程度上来源于物理学。理论本质上是一套规则，可用于推导现象不同方面的预测模型。理论的解释力来自于它们提供现实各方面的整体图景的能力，也就是说，能够表明不同的现象产生于一小组简单的规则。比如同样的[统计力学](https://en.wikipedia.org/wiki/Statistical_mechanics)的规则可以用来计算[平衡](https://en.wikipedia.org/wiki/Thermodynamic_equilibrium)中任何物质的[热力学性质](https://en.wikipedia.org/wiki/List_of_thermodynamic_properties)(如温度、压力、密度)。

从历史上看，我们相信能够在这样的理论框架的基础上解释宇宙，这在很大程度上是由物理学惊人的成功所推动的。然而，由于上个世纪最后 25 年肯尼斯·威尔逊和其他人的开创性工作所提供的见解，这一信念现在建立在健康的理解基础上。

考虑规则集的层次结构，初始(底层)规则集代表理论的数学结构，最终(顶层)规则集代表数据中观察到的稳定相关性的数学结构。我们现在可以考虑这样一种转换，即通过将这种转换应用于前一级别的规则集来获得每一级别的规则集。这个用来从低级规则集导出高级规则集的过程被称为重正化群流(我非常宽松地使用这个术语)。

对于某些类型的转换和规则集，会发生一些非常显著和意想不到的事情；从非常不同的初始规则集开始，最终得到相同的最终规则集。在这种情况下，最后一个规则集被称为固定点，导致相同固定点的一组初始规则集被称为构成了一个普遍性类。普遍性假设(或简称为普遍性)指出，自然界中实际存在的规则集和转换都属于上述类型。(见[此处](https://arxiv.org/pdf/1210.2262.pdf)介绍普适性和重整化群)。

如果普适性是真的，那么这将意味着复杂系统中观察到的稳定相关性将独立于基础理论的细节，即简单的理论可能就足够好了。此外，我们应该看到在各种不相关的领域中，相关性具有相同的数学结构。

普适性首先是在接近连续[相变](https://en.wikipedia.org/wiki/Phase_transition)的不同系统的热力学变量的行为中被观察和研究的。从那以后，人们在各种不同的和不相关的地方观察到了这种现象，例如[复杂网络的动力学](http://barabasi.com/f/432.pdf)、[多智能体系统](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.97.9529&rep=rep1&type=pdf)、[粉红噪声的出现](https://en.wikipedia.org/wiki/Pink_noise)和[墨西哥一个城镇的公交系统](https://www.quantamagazine.org/in-mysterious-pattern-math-and-nature-converge-20130205/)，仅举几个例子(见[这里](https://arxiv.org/pdf/math-ph/0603038.pdf)一些有趣的例子)。有足够的经验证据表明，自然(包括许多人造实体)确实偏爱普遍性。

虽然属于普遍性类别的理论可能有非常不同的起源(就它们试图解释的现实方面而言)和数学细节，但它们共享一些重要的数学性质，这些性质对它们的数学结构有严格的限制。对于物理学中发现的普适类，这些性质通常是对称性、维度和局部性。但是，一般来说，它们将取决于具体的普适类，并且可以通过执行该类成员的重整化群流来确定。

普遍性本身只能部分解释为什么物理学的理论框架如此成功。第二部分来自对物理系统中规则集的层次结构与我们的直觉非常吻合的观察。在物理学中，规则的等级是尺度或分辨率的等级。直觉上，我们期望大的事物(宏观物体)有规则，小的事物(微观实体)也必须有规则。我们也知道大的事物是由小的事物组成的，因此宏观模式应该遵循微观理论。这正是现实中发生的事情。这就是为什么(近乎天真的)还原论在物理学的大部分领域如此有效的原因。

这个谜题的最后一块与技术发展的时间线有关。我们从观察人类尺度的现象开始，直到那时才开始发展技术，显微镜和望远镜，来观察越来越小和越来越大尺度的现象。这个时间线与物理系统中规则集的层次非常吻合。因此，我们可以在理论和实验之间建立一个非常富有成效的反馈。但是，更重要的是，起点非常关键——对于许多物理系统来说，人类尺度是普遍性发生的尺度。这意味着即使只有少量数据和人工检查，稳定的相关性也是显而易见的。

为了理解为什么以上几点如此重要，考虑这样一种情况:我们从包含一箱气体中所有原子在不同时间的快照的图片开始，而不是测量热力学性质。从这些数据中推导出热力学或统计力学有多容易？

我们目前在生物学、经济学或社会科学等领域遇到的情况与上述情况没有太大不同。与物理学不同，在这些领域中，我们没有机会知道现实中规则集的层级对应着什么。我们也不知道普遍性应该在哪个阶段发生，我们应该期待看到稳定的相关性。

但是我们以前没有，现在有了，更多的数据和一个工具，机器学习，来提取这些数据并找到这些稳定的相关性。有充分的理由相信深度神经网络本质上执行一种重正化群流，并且它们如此有效的原因之一是因为在许多情况下，数据生成的生成过程(规则集)是[分层的](https://link.springer.com/article/10.1007/s10955-017-1836-5)。当通过普遍性的棱镜来看时，这意味着深度神经网络为我们提供了对包含正确基础理论的普遍性类中的重整化群流的访问，然后可以使用它来约束基础理论的数学结构。

考虑一个思想实验，其中向深度神经网络提供气体原子的快照以及热力学变量的一些复杂函数的值；我们训练网络的任务是从快照中预测价值。我们期望热力学出现在网络的最后一层吗？我们应该能够从网络的权重中约束统计力学的数学结构吗？原则上，没有理由不这样认为。

回到前面提出的问题上来；机器学习如何帮助理论科学？机器学习可以为科学理论提供数学支架，理论家将为其添加意义和通往现实的桥梁。然而，在我们到达那里之前，我们需要对机器学习有更好的理解。我们将需要从一般原理来理解机器学习算法。换句话说，机器学习中对称性、维数和局部性的类似物是什么？也许，是时候开始开发一个真正的机器学习理论了。