<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-recurrent-neural-networks-the-prefered-neural-network-for-time-series-data-7d856c21b759?source=collection_archive---------3-----------------------#2017-06-26">https://towardsdatascience.com/understanding-recurrent-neural-networks-the-prefered-neural-network-for-time-series-data-7d856c21b759?source=collection_archive---------3-----------------------#2017-06-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><p id="854d" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">了解递归神经网络:时序数据的首选神经网络</strong></p><p id="63e8" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">人工智能几十年来一直处于幕后，在远处扬起灰尘，但从未真正到来。那个时代已经结束了。2017 年，AI 已经冲破尘雾，<a class="ae jo" href="https://www.cbinsights.com/blog/artificial-intelligence-startup-funding/" rel="noopener ugc nofollow" target="_blank">大举到来</a>。但是为什么呢？突然有什么大不了的？而<em class="jp">递归神经网络</em>又有什么关系呢？嗯，实际上是一批<em class="jp"/>。由于一种在传统神经网络中闻所未闻的巧妙形式的短期记忆，今天的递归神经网络(rnn)已经证明自己是强大的预测引擎。当涉及到某些连续的机器学习任务时，如语音识别，rnn 正在一次又一次地达到预测准确性的水平，这是其他算法无法比拟的。然而，第一代 rnn 在当时并没有这么火。他们在修正错误的过程中遭受了严重的挫折，这阻碍了他们几十年的进步。最后，在 90 年代后期出现了一个重大突破，产生了新一代更加精确的 rnn。在这一突破的基础上，近二十年来，开发人员不断改进和完善他们的新 rnn，直到全明星应用程序，如谷歌语音搜索和苹果的 Siri 开始将其用于关键过程。现在，循环网络正在各地出现，并正在帮助点燃正在展开的人工智能复兴。</p><p id="f113" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">依附过去的神经网络</strong></p><figure class="jr js jt ju gt jv gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/249a88f2a660f6091a0e80d7fad6468e.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*chs1MCz2rCK4_dFRLnUEIg.png"/></div></figure><p id="0241" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">大多数人工神经网络，例如前馈神经网络，对它们刚刚接收到的输入没有记忆。例如，如果你给一个前馈神经网络提供字母序列“WISDOM”，当它到达“D”时，它已经忘记它刚刚读了“s”，这是一个大问题。无论你如何努力训练它，它总是很难猜出最有可能的下一个字符:“o”。这使它成为某些任务的一个相当糟糕的候选人，例如语音识别，这些任务极大地受益于预测接下来会发生什么的能力。另一方面，循环网络确实记得它们刚刚遇到的东西，而且是在非常复杂的水平上。</p><p id="2683" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">让我们再次以输入“智慧”为例，将其应用于一个递归网络。RNN 的单元或人工神经元在接收到“D”时，也将它刚才接收到的字符“s”作为其输入。换句话说，它将刚刚过去的时间添加到现在。这给了它有限的短期记忆的优势，随着它的训练，提供了足够的上下文来猜测下一个字符最有可能是什么:“o。”</p><p id="ba41" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">调整和重新调整</strong></p><p id="9653" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">如果你喜欢进入杂草，这是你兴奋的地方。否则，准备好迎接艰难的日子吧。但是坚持住，这是值得的。像所有人工神经网络一样，RNN 的单元为它们的多个输入分配一个权重矩阵，然后对这些权重应用一个函数来确定一个<em class="jp">单个</em>输出。然而，循环网络不仅将权重应用于它们当前的输入，还应用于它们之前的输入。然后，他们通过一个过程来调整分配给他们现在和过去输入的权重，这个过程涉及两个关键概念，如果你真的想进入人工智能，你肯定想知道:<em class="jp">梯度下降</em>和<em class="jp">通过时间的反向传播(BPTT) </em>。</p><p id="67e7" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><em class="jp">梯度下降</em></p><p id="2af5" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">机器学习中最著名的算法之一被称为<em class="jp">梯度下降</em>。它的主要优点是能够避开可怕的“维数灾难”。这个问题困扰着神经网络等系统，这些系统有太多的变量，以至于无法对它们的最优值进行暴力计算。然而，梯度下降通过放大多维误差或<em class="jp">成本函数</em>的局部低点，或<em class="jp">局部最小值</em>，打破了维数灾难。这有助于系统确定调整值或权重，分配给网络中的每个单元，使精度回到线上。</p><p id="4f13" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><em class="jp">通过时间反向传播</em></p><p id="2cbf" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">RNN 通过对称为<em class="jp">反向传播</em>的反馈过程稍加修改后调整其权重来训练其单元。好吧，这是个奇怪的概念。但是如果你喜欢人工智能，你会学着爱上它。反向传播过程从网络的最终输出一层一层地返回，根据单元的总输出误差的计算部分调整每个单元或人工神经元的权重。明白了吗？如果是这样，请准备好迎接更复杂的一层。递归神经网络使用这个过程的一个更重版本，称为<em class="jp">时间反向传播(BPTT)。</em>这个版本扩展了调整过程，包括 T-1 个输入值的权重，负责每个单元对先前时刻的记忆。</p><p id="fa75" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn"> Yikes:消失渐变问题</strong></p><figure class="jr js jt ju gt jv gh gi paragraph-image"><div class="gh gi jy"><img src="../Images/cef4f29ac8f82b16dfcd96a15ed67c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*SVG2AqQQCWoYcGFhdG5ULQ.jpeg"/></div></figure><p id="1f73" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">尽管在梯度下降和 BPTT 的帮助下取得了一些初步成功，但许多人工神经网络，包括第一代 RNNs，最终耗尽了气体。从技术上讲，他们遭遇了一个严重的挫折，被称为<em class="jp">消失梯度问题。</em>虽然细节超出了本文的范围，但基本思想非常简单。首先，让我们看看<em class="jp">梯度</em>的概念。就像它更简单的亲戚——导数一样，你可以把梯度看作斜率。在训练深度神经网络的情况下，梯度越大，坡度越陡，系统就能越快地下坡到达终点并完成其训练。但是这也是开发者遇到麻烦的地方——他们的斜坡对于快速训练来说太平坦了。这在他们深层网络的第一层尤其成问题，当涉及到记忆单元的适当调整时，这是最关键的。在这里，梯度值变得如此之小，它们对应的斜率如此之平，以至于人们可以将它们描述为“消失”，因此出现了<em class="jp">消失梯度问题</em>。随着坡度变得越来越小，越来越平坦，训练时间变得难以忍受的长。这是一场无休止的纠错噩梦。</p><p id="285f" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">大突破:<em class="jp">长</em>短时记忆</strong></p><p id="d52b" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">最后，在 90 年代后期，<a class="ae jo" href="https://www.researchgate.net/publication/13853244_Long_Short-term_Memory" rel="noopener ugc nofollow" target="_blank">的一个重大突破</a>解决了消失下降问题，给了递归网络开发第二次风。这种新方法的核心是长短期记忆单元(LSTM)。</p><figure class="jr js jt ju gt jv gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi jz"><img src="../Images/7920ec21e9b424351dd008a627891b69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KiIRm8LW171045Y6If5DSA.png"/></div></div></figure><p id="ac94" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">虽然听起来很奇怪，但总而言之，LSTM 在人工智能领域创造了一个与众不同的世界。这些新的单元，或人工神经元，像 RNNs 的标准短期记忆单元一样，记住他们片刻之前的输入。然而，与标准的 RNN 单元不同，LSTMs 可以挂在它们的存储器上，这些存储器具有类似于传统计算机中的存储器寄存器的读/写属性。然而 LSTMs 有模拟存储器，而不是数字存储器，这使得它们的功能不同。换句话说，它们的曲线是连续的，你可以找到它们斜率的陡度。因此它们非常适合于涉及反向传播和梯度下降的偏微分方程。</p><figure class="jr js jt ju gt jv gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ke"><img src="../Images/5812e80086679d4f45338b1b98253c04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OMKwhlkGfB8GhizsK1EvSg.png"/></div></div></figure><p id="071c" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">总之，LSTMs 不仅可以调整它们的权重，还可以根据它们训练的特点保留、删除、转换和控制它们存储数据的流入和流出。最重要的是，LSTMs 可以在足够长的时间内保持重要的误差信息，以保持梯度相对较陡，从而训练周期相对较短。这消除了消失梯度问题，并大大提高了今天的 LSTM 为基础的递归网络的准确性。由于 RNN 架构的这一显著改进，谷歌、苹果和许多其他领先公司，更不用说初创公司，现在都在使用 rnn 来支持其业务中心的应用程序。简而言之，rnn 突然成了一件大事。</p><p id="ff5f" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated"><strong class="ir jn">关于 RNNs 需要记住什么</strong></p><p id="0182" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">让我们回顾一下这些惊人的记忆机器的亮点。递归神经网络(RNNs)可以记住它们以前的输入，这使它们在处理语音识别等顺序的上下文敏感任务时，比其他人工神经网络具有很大的优势。然而，第一代 rnn 在通过非常重要的反向传播和梯度下降这两个过程来纠正错误的能力方面遇到了瓶颈。被称为可怕的<em class="jp">消失梯度问题</em>，这个绊脚石实际上停止了该领域的进展，直到 1997 年，一项重大突破将一种大大改进的 LSTM 架构引入该领域。这种新方法有效地将循环网络中的每个单元变成了模拟计算机，大大提高了准确性，并有助于导致我们今天在我们周围看到的人工智能的复兴。</p><p id="96cd" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">如果你喜欢这篇文章，你能给的最大的赞美就是<strong class="ir jn"> <em class="jp">和你认为会喜欢这篇文章的人分享吧！</em> </strong></p><p id="3e54" class="pw-post-body-paragraph io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ij bi translated">此外，如果你不想错过一篇文章，点击下面的绿色心形按钮订阅我的文章！感谢阅读，祝你有美好的一天，永远不要停止学习！</p></div></div>    
</body>
</html>