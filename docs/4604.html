<html>
<head>
<title>Unfolding Naïve Bayes from Scratch !</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始展开朴素贝叶斯！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unfolding-na%C3%AFve-bayes-from-scratch-2e86dcae4b01?source=collection_archive---------3-----------------------#2018-08-25">https://towardsdatascience.com/unfolding-na%C3%AFve-bayes-from-scratch-2e86dcae4b01?source=collection_archive---------3-----------------------#2018-08-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e988" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对于 ML 初学者</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e07cb2e0d6c087bb6886dc984ba81549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sjet9qSO4O8fX2-FXvxflw.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@tateisimikito?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Jukan Tateisi</a> on <a class="ae kv" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="73f9" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">从头开始展开朴素贝叶斯！Take-1🎬</h1><p id="1e51" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi mk translated"><span class="l ml mm mn bm mo mp mq mr ms di"> W </span> <strong class="lq ir">无论你是机器学习的初学者，还是你一直在努力理解<em class="mt">超级自然的</em> <em class="mt">机器学习算法，但你仍然觉得点与点之间没有某种联系，</em>这篇文章绝对适合你！</strong></p><h1 id="bd33" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">这篇博文的目的</strong></h1><p id="791f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我试图让事情简单明了。唯一的目的是深入和清楚地理解一个众所周知的文本分类 ML 算法(朴素贝叶斯)的工作原理，而不是陷入在 ML 算法的解释中经常使用的莫名其妙的数学术语！</p><h1 id="ebb9" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">谁是目标受众？</strong></h1><p id="7fd8" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">任何人从零开始寻找对 ML 算法的深入而易懂的解释</p><h1 id="59d8" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">本教程的成果</strong></h1><p id="0993" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">一个完整清晰的朴素贝叶斯 ML 算法的图片，所有神秘的数学都被揭开了神秘的面纱，在你的 ML 旅程中向前迈出了具体的一步！</p><h1 id="8ee8" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">定义路线图…..🚵</h1><p id="b3f7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><strong class="lq ir">里程碑# 1 : </strong> <a class="ae kv" href="#b3f7" rel="noopener ugc nofollow"> <strong class="lq ir">简单介绍一下朴素贝叶斯算法</strong> </a></p><p id="735e" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir">里程碑# 2 : </strong> <a class="ae kv" href="#f20f" rel="noopener ugc nofollow"> <strong class="lq ir">朴素贝叶斯模型的训练阶段</strong> </a></p><p id="afa7" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir">重大里程碑# 3 : </strong> <a class="ae kv" href="#6bd8" rel="noopener ugc nofollow"> <strong class="lq ir">测试阶段——预测开始发挥作用了！</strong> </a></p><p id="a787" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir">里程碑# 4 : </strong> <a class="ae kv" href="#5700" rel="noopener ugc nofollow"> <strong class="lq ir">深入挖掘概率的数学</strong> </a></p><p id="e55a" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir">里程碑# 5 : </strong> <a class="ae kv" href="#08ef" rel="noopener ugc nofollow"> <strong class="lq ir">避免下溢错误的常见陷阱！</strong>T45】</a></p><p id="c992" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir">里程碑# 6 : </strong> <a class="ae kv" href="#ff0b" rel="noopener ugc nofollow"> <strong class="lq ir">结束语……</strong>T51】</a></p><h1 id="7ed0" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">简单介绍一下朴素贝叶斯算法</h1><p id="b5aa" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">朴素贝叶斯是最常见的 ML 算法之一，常用于文本分类。如果你刚刚步入 ML，它是最容易开始的分类算法之一。朴素贝叶斯是一种概率分类算法，因为它使用概率来进行分类预测。</p><blockquote class="mz"><p id="f20f" class="na nb iq bd nc nd ne nf ng nh ni mj dk translated">因此，如果你期待着在机器学习的旅程中向前迈出一步，朴素贝叶斯分类器绝对是你的下一站！</p></blockquote><blockquote class="nj nk nl"><p id="515f" class="lo lp mt lq b lr nm jr lt lu nn ju lw no np lz ma nq nr md me ns nt mh mi mj ij bi translated"><strong class="lq ir"> <em class="iq">达到里程碑# 1👍</em>T55】</strong></p></blockquote></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><h1 id="8759" class="kw kx iq bd ky kz ob lb lc ld oc lf lg jw od jx li jz oe ka lk kc of kd lm ln bi translated"><strong class="ak">朴素贝叶斯模型的训练阶段</strong></h1><p id="0558" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">比方说，有一篇餐厅评论，“非常好的食物和服务！！!"，你想预测这个给定的评论是暗示积极的还是消极的情绪。要做到这一点，我们首先需要在相关的带标签的训练数据集上训练一个模型(这实质上意味着确定每个类别的字数)，然后这个模型本身将能够自动地将这样的评论分类到它被训练所针对的给定情感之一中。假设给你一个看起来像下面的训练数据集(一个评论及其相应的情绪):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/294228a5279392a7959966ec17aa0077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HbX0eWfzGx6sPk7mjetbqg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Labelled Training Dataset</figcaption></figure><blockquote class="nj nk nl"><p id="17cf" class="lo lp mt lq b lr mu jr lt lu mv ju lw no mw lz ma nq mx md me ns my mh mi mj ij bi translated"><strong class="lq ir">快速补充说明</strong>:朴素贝叶斯分类器是一种监督机器学习算法</p></blockquote><h1 id="9f86" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">那么我们如何开始呢？</h1><h2 id="5aa5" class="oh kx iq bd ky oi oj dn lc ok ol dp lg lx om on li mb oo op lk mf oq or lm os bi translated">步骤# 1:数据预处理</h2><p id="0f33" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">作为预处理阶段的一部分(这篇文章中没有详细介绍)，训练语料库/训练数据集中的所有单词都被<strong class="lq ir">转换成小写</strong>和<strong class="lq ir">，除了像标点符号这样的字母之外的一切都从训练示例中排除</strong>。</p><blockquote class="nj nk nl"><p id="a233" class="lo lp mt lq b lr mu jr lt lu mv ju lw no mw lz ma nq mx md me ns my mh mi mj ij bi translated"><strong class="lq ir">一个小提示:</strong><em class="iq"/><strong class="lq ir"><em class="iq">一个常见的缺陷是没有以与训练数据集预处理</em> </strong> <em class="iq">相同的方式预处理测试数据，而是将测试示例直接输入到训练好的模型中。结果，</em> <strong class="lq ir"> <em class="iq">训练好的模型在给定的测试示例</em> </strong> <em class="iq">上表现很差，而在该测试示例上它应该表现得很好！</em></p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/75912139ed75d0a5929f511e3a2a738c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kh6qwA5_jJEFbyNXHi7CVw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Preprocessed Training Dataset</figcaption></figure><h2 id="bb00" class="oh kx iq bd ky oi oj dn lc ok ol dp lg lx om on li mb oo op lk mf oq or lm os bi translated">第二步:训练你的朴素贝叶斯模型</h2><p id="4e05" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">只需简单地制作两个单词包(BoW)，每个类别一个，每个包将简单地包含单词及其对应的计数。所有属于“正面”情感/标签的单词将被归入一个 BoW，所有属于“负面”情感的单词将有它们自己的 BoW。训练集中的每个句子都被拆分成单词(基于作为标记符/分隔符的空间),这是如何简单地构造单词计数对的，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/dcd4b223ff8fdfd885dbf5ad1a05b54b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xd82fleqMdSdrfxxFvjDPg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">BoW for Both categories</figcaption></figure><blockquote class="mz"><p id="58d5" class="na nb iq bd nc nd ov ow ox oy oz mj dk translated">我们已经完成了朴素贝叶斯模型的训练！</p></blockquote><blockquote class="nj nk nl"><p id="cc6f" class="lo lp mt lq b lr nm jr lt lu nn ju lw no np lz ma nq nr md me ns nt mh mi mj ij bi translated"><strong class="lq ir"> <em class="iq">达到里程碑# 2👍</em>T25】</strong></p></blockquote></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><p id="c962" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">在进入第三个里程碑之前，喝杯咖啡或者舒展一下肌肉</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/00906a55c9ecd733e8e4634cf4f2cf49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bkhYERWS9x-GIyAUqFbVwA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@londonwoodco?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Rumman Amin</a> on <a class="ae kv" href="https://unsplash.com/search/photos/cup-of-tea?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="7c7d" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir">注意:我们即将开始朴素贝叶斯模型最重要的部分，即使用上述训练好的模型来预测餐馆评论。我现在觉得这有点长，但完全值得，因为我们将讨论每一个细节，最终结果是零歧义！</strong></p></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><h1 id="741f" class="kw kx iq bd ky kz ob lb lc ld oc lf lg jw od jx li jz oe ka lk kc of kd lm ln bi translated"><strong class="ak">测试阶段——预测开始发挥作用了！</strong></h1><p id="a16c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">考虑现在你的模型被给予一个餐馆评论，<strong class="lq ir"> <em class="mt">“非常好的食物和服务！！!"</em> </strong>，它需要归类到它所属的特定类别。正面评价还是负面评价？我们需要找到这个给定的评论属于每个类别的概率，然后我们会根据这个测试示例能够为哪个特定类别获得更多的概率，给它分配一个正面或负面的标签。</p><h1 id="7b86" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">找到给定测试示例的概率</h1><h2 id="21fd" class="oh kx iq bd ky oi oj dn lc ok ol dp lg lx om on li mb oo op lk mf oq or lm os bi translated">步骤# 1:测试示例的预处理</h2><p id="2743" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">以与预处理训练示例相同的方式预处理测试示例，即，将示例更改为小写，并排除除字母/字母表之外的所有内容。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/a06a189e1414c96dd50a379763b496f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hc4eldXzg8HyIishbBtO4w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Preprocessed Test Example</figcaption></figure><h2 id="c1da" class="oh kx iq bd ky oi oj dn lc ok ol dp lg lx om on li mb oo op lk mf oq or lm os bi translated">步骤 2:预处理测试示例的标记化</h2><p id="3008" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">将测试示例标记化，即将其拆分为单个单词。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/86f69ca270601ba3d25ed5313f7dfe9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*8FpO4caw-Q58N81BgJELBA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Tokenized Preprocessed Example</figcaption></figure><blockquote class="nj nk nl"><p id="c862" class="lo lp mt lq b lr mu jr lt lu mv ju lw no mw lz ma nq mx md me ns my mh mi mj ij bi translated"><strong class="lq ir">快速边注</strong> <em class="iq">:你一定已经熟悉机器学习中的术语</em> <strong class="lq ir"> <em class="iq">特征</em> </strong> <em class="iq">。在这里，在朴素贝叶斯中，</em><strong class="lq ir"><em class="iq"/></strong><em class="iq"/><strong class="lq ir"><em class="iq">的每一类训练数据集的词汇中的每个词构成了一个分类特征</em> </strong> <em class="iq">。这意味着</em><strong class="lq ir"><em class="iq"/></strong><em class="iq"/><strong class="lq ir"><em class="iq">每一类的所有唯一单词(即词汇/vocab)基本上是该特定类的一组特征。为什么我们需要“计数”？因为我们需要分类词特征的数字表示，因为朴素贝叶斯模型/算法需要数字特征来找出概率分数！</em> </strong></p></blockquote><h2 id="8a24" class="oh kx iq bd ky oi oj dn lc ok ol dp lg lx om on li mb oo op lk mf oq or lm os bi translated">步骤 3:使用概率来预测标记化测试示例的标签</h2><blockquote class="nj nk nl"><p id="b95f" class="lo lp mt lq b lr mu jr lt lu mv ju lw no mw lz ma nq mx md me ns my mh mi mj ij bi translated"><strong class="lq ir"><em class="iq"/></strong>寻找概率的不那么吓人的数学形式</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/f57cb88e2fd273ada81978e5507cfeea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IDdw5KNLTrfsL2hpqWCENA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Probability of a Given <strong class="bd pe">Test Example</strong> <strong class="bd pe">i </strong>of belonging to class <strong class="bd pe">c</strong></figcaption></figure><ul class=""><li id="6c75" class="pf pg iq lq b lr mu lu mv lx ph mb pi mf pj mj pk pl pm pn bi translated">让<strong class="lq ir"> <em class="mt">我</em> </strong> <em class="mt"> = </em>测试示例= <strong class="lq ir"> <em class="mt">“非常好的食物和服务！！!"</em>T49】</strong></li><li id="fd15" class="pf pg iq lq b lr po lu pp lx pq mb pr mf ps mj pk pl pm pn bi translated"><strong class="lq ir"> <em class="mt">中的总字数 i </em> </strong> = 5，所以<strong class="lq ir"><em class="mt">j</em></strong><em class="mt">(</em><strong class="lq ir"><em class="mt">表示特征号</em> </strong> <em class="mt"> ) </em> <strong class="lq ir">的值从 1 到 5 </strong>不等。就这么简单！</li></ul><p id="85c9" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">让我们将上面的场景映射到给定的测试示例中，这样会更清楚！</p><h2 id="e094" class="oh kx iq bd ky oi oj dn lc ok ol dp lg lx om on li mb oo op lk mf oq or lm os bi translated">让我们开始计算这些产品术语的值</h2><h2 id="d78b" class="oh kx iq bd ky oi oj dn lc ok ol dp lg lx om on li mb oo op lk mf oq or lm os bi translated">第一步:求 c 类 p 项的值</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pt"><img src="../Images/45f34bea6fc164f0ead64c88933de380.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*krSm7ixbHpcMDMqN4UgojA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Simply the <strong class="bd pe">Fraction of Each Category/Class in the Training Set</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/ca76453513fc349afd93c23b737e1df3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*BkNnQMX_3Bvb44bxLrVB4A.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd pe"><em class="pv">p </em></strong><em class="pv">of class</em><strong class="bd pe"><em class="pv"> c</em></strong> for<strong class="bd pe"> Positive</strong> &amp; <strong class="bd pe">Negative</strong> categories</figcaption></figure><h2 id="cb14" class="oh kx iq bd ky oi oj dn lc ok ol dp lg lx om on li mb oo op lk mf oq or lm os bi translated">步骤# 2:求项的值:乘积(c 类中测试词 j 的 p)</h2><p id="9356" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在我们开始推断特定类别中测试词<strong class="lq ir"><em class="mt">【j】</em></strong><strong class="lq ir"><em class="mt">c</em></strong>的概率之前，让我们快速熟悉一些简单的符号，这些符号在这篇博客文章的不太远的行中使用:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pw"><img src="../Images/491fb6908acd974910282e198cce62b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EXRpALNKI5ZRjAvIcE5VIw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Understanding Notations — e, i &amp; j</figcaption></figure><p id="40b0" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">由于目前我们的测试集中只有一个例子(为了便于理解)，所以<strong class="lq ir"> <em class="mt"> i </em> </strong> <em class="mt"> = </em> 1。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi px"><img src="../Images/dd8768ee144afe3f4798d4b8da90e5bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MHikauYVwGIZKBy8HTKaow.png"/></div></div></figure><blockquote class="nj nk nl"><p id="a704" class="lo lp mt lq b lr mu jr lt lu mv ju lw no mw lz ma nq mx md me ns my mh mi mj ij bi translated"><strong class="lq ir">快速补充说明</strong> <em class="iq">:在</em> <strong class="lq ir"> <em class="iq">测试时间/预测时间期间，我们将测试示例的每个单词映射到在训练阶段</em> </strong> <em class="iq">发现的计数。因此，在这种情况下，我们在这个给定的测试示例中总共寻找 5 个单词计数。</em></p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi py"><img src="../Images/e5287f371074f11039a6a258f0386ca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3J4tWsRN7aa1uen8oEAWzA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@jaredbrashier?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Jared Brashier</a> on <a class="ae kv" href="https://unsplash.com/search/photos/drones?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="nj nk nl"><p id="f084" class="lo lp mt lq b lr mu jr lt lu mv ju lw no mw lz ma nq mx md me ns my mh mi mj ij bi translated">只是一个让你无法入睡的随机事件！</p></blockquote><h2 id="cf62" class="oh kx iq bd ky oi oj dn lc ok ol dp lg lx om on li mb oo op lk mf oq or lm os bi translated">求 c 类中测试词“j”的概率</h2><p id="0d2e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在我们开始计算<strong class="lq ir">的乘积</strong> ( <strong class="lq ir"> <em class="mt"> p </em> </strong>中的一个测试字<strong class="lq ir"> <em class="mt"> j </em> </strong>中的类<strong class="lq ir"> <em class="mt"> c </em> </strong>)之前，我们显然首先需要确定<strong class="lq ir"> <em class="mt"> p </em> </strong>中的一个测试字<strong class="lq ir"><em class="mt">j</em><em class="mt">c</em></strong>有两种方法可以做到这一点，如下所述— <em class="mt">几分钟后，我们将发现哪一种方法应该被真正遵循，而不是被实际使用……</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pz"><img src="../Images/8d59ca14ef47063bdf299605f7ce6eb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RfVo4VJxxntRDlhErAeBng.png"/></div></div></figure><h2 id="9297" class="oh kx iq bd ky oi oj dn lc ok ol dp lg lx om on li mb oo op lk mf oq or lm os bi translated">让我们首先尝试使用方法 1 来寻找概率:</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qa"><img src="../Images/f318999309fe4e228b38bedb691967f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2SROWzl771cbywEmQYqOGw.png"/></div></div></figure><p id="b047" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">现在我们可以将单个单词的概率相乘(如上所述)以找到该项的数值:<br/> <strong class="lq ir">乘积</strong>(测试单词<strong class="lq ir"><em class="mt">【j】</em></strong><strong class="lq ir"><em class="mt">c</em></strong>)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qb"><img src="../Images/386481ba66a99f6b9e78f37dcb4f0d8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lTkCRVvVQLDQg9dar5XJLw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd pe">The Common Pitfall of Zero Probabilities!</strong></figcaption></figure><p id="9a1f" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">到目前为止，我们已经有了两个术语的数值，即类<strong class="lq ir"> <em class="mt"> c </em> </strong>和<strong class="lq ir"> <em class="mt"> </em>产品</strong>(测试字<strong class="lq ir"><em class="mt">【j】</em></strong>和类<strong class="lq ir"> <em class="mt"> c </em>中的<strong class="lq ir"> <em class="mt"> p </em> </strong>因此，我们可以将这两项相乘，以确定这两个类别的<strong class="lq ir"><em class="mt">p</em></strong>(<strong class="lq ir"><em class="mt">I</em></strong>属于类别<strong class="lq ir"> <em class="mt"> c </em> </strong>)。下面演示了这一点:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qc"><img src="../Images/4c757bb36fb558fa1f6d9f6f546cb6fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N-dqN0wbpUCR_ZPmiuNl-A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd pe">The Common Pitfall of Zero Probabilities!</strong></figcaption></figure><p id="db94" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir"><em class="mt">p</em></strong>(<strong class="lq ir"><em class="mt">I</em></strong>属于类<strong class="lq ir"> <em class="mt"> c </em> </strong>)对于两个类别来说都是<strong class="lq ir">零！！！</strong>但显然测试示例<strong class="lq ir">“非常好的食物和服务！！!"</strong>属于正班！很明显，这是因为类<strong class="lq ir"><em class="mt"/></strong><strong class="lq ir">中测试词<br/><strong class="lq ir"><em class="mt">【j】</em></strong>的<strong class="lq ir">乘积</strong> ( <strong class="lq ir"> <em class="mt"> p </em>)对于两个类别</strong>来说都是零，而这又是因为<strong class="lq ir">中的几个词(以橙色突出显示)从未在我们的训练数据集中出现过，因此它们的概率为零！很明显是他们造成了所有的破坏！</strong></strong></p><p id="43f2" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">那么这是否意味着无论何时出现在测试示例中但从未出现在训练数据集中的单词总是会导致这种破坏？在这种情况下，我们训练过的模型将永远无法预测正确的情绪？它只会随机选择积极或消极的类别，因为两者都有相同的零概率和预测错误？答案是否定的！这就是第二种方法(编号 2)发挥作用的地方，事实上这是一个数学公式，它实际上用于推导<strong class="lq ir"><em class="mt"/></strong>(<strong class="lq ir"><em class="mt">I</em></strong>属于类<strong class="lq ir"> <em class="mt"> c </em> </strong>)。但是在我们继续第二种方法之前，我们应该先熟悉一下它的数学原理！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/5936b799548aa97d8688df076d4b64d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*LNEGRr8UZInlbuQiA1gBQA.png"/></div></figure><p id="a1b1" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">所以现在在添加了为 1 的<strong class="lq ir">伪计数</strong> <strong class="lq ir">之后，从未出现在训练数据集中的测试字的概率<em class="mt"> p </em>将永远不会为零</strong>，因此<strong class="lq ir">， </strong>项<strong class="lq ir">乘积</strong>(<strong class="lq ir"><em class="mt">p</em><em class="mt">【j】</em></strong>在类<strong class="lq ir"><em class="mt">【c】</em></strong>)<strong class="lq ir"/>中的数值永远不会以零结束，这又意味着<br/> <strong class="lq ir"> <em class="mt"> p </em> 所以一切都很好，不会再有零概率的破坏了！</strong></p><p id="442d" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir">所以方法 2 的分子项将增加 1，因为我们已经为词汇表中的每个单词增加了 1，所以它变成了</strong> <strong class="lq ir"> : </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qe"><img src="../Images/3d5acefde8be05917dee865e26f33cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JW3fhqAPb6keA3aF4Ep8Cw.png"/></div></div></figure><p id="083f" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir">类似地，分母变成:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qf"><img src="../Images/9814eac6165d20000b880161078a9520.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U4LY1kGGUxH9iFhLbAgPSg.png"/></div></div></figure><p id="0bb5" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir">于是完整的公式:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qg"><img src="../Images/bb1062f542b3f380ee580c6afc63860f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OHJARMJwCJzoikdsQCWKWw.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/6fbbc72492cf0e15fbd325ead7107505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*TKFVpHFOEMzfRAdi9iXQRg.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qi"><img src="../Images/cbdfe6939b099e6fc65ab2337a19fcdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y8s-AhQn8zD1LUeOURRoKQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Probabilities of Positive &amp; Negative Class</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qj"><img src="../Images/d362294d63d884135a5c8e8f52026c8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0nLHtoXmx_IAggh7Iyo_uA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@grakozy?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Greg Rakozy</a> on <a class="ae kv" href="https://unsplash.com/search/photos/ocean-water-eye-level?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="nj nk nl"><p id="3b79" class="lo lp mt lq b lr mu jr lt lu mv ju lw no mw lz ma nq mx md me ns my mh mi mj ij bi translated"><strong class="lq ir"> <em class="iq">你快到了！</em> </strong></p></blockquote><h2 id="0873" class="oh kx iq bd ky oi oj dn lc ok ol dp lg lx om on li mb oo op lk mf oq or lm os bi translated">现在用第二种方法计算概率:</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qk"><img src="../Images/b3542da263431da75adb481bcc0aa2ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fSQvoFi5TBLpsFciHV2JLw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd pe">Handling of Zero Probabilities : These act like failsafe probabilities !</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ql"><img src="../Images/d8477628299eebd7a90ab559a300d7a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LUdR0KZ3czNJyyv33ZwMoA.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qm"><img src="../Images/3ad6fc03aee80b49796668251f8c2531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gHwyeL1zgY1G39UbcGhSxA.png"/></div></div></figure><blockquote class="mz"><p id="a4b1" class="na nb iq bd nc nd ov ow ox oy oz mj dk translated">现在作为测试例子的概率，“非常好的食物和服务！！!"与负面类别(即 7.74E-09)相比，正面类别(即 9.33E-09)更多，因此我们可以将其预测为正面情绪！这就是我们如何简单地预测一个测试/看不见的例子的标签</p></blockquote><blockquote class="nj nk nl"><p id="e87f" class="lo lp mt lq b lr nm jr lt lu nn ju lw no np lz ma nq nr md me ns nt mh mi mj ij bi translated"><strong class="lq ir"> <em class="iq">里程碑# 3 达成！！👍 👍 👍</em>T11】</strong></p></blockquote></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><blockquote class="nj nk nl"><p id="861d" class="lo lp mt lq b lr mu jr lt lu mv ju lw no mw lz ma nq mx md me ns my mh mi mj ij bi translated"><strong class="lq ir"> <em class="iq">只剩最后几笔润色了！</em>T15】</strong></p></blockquote></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><blockquote class="nj nk nl"><p id="d6c3" class="lo lp mt lq b lr mu jr lt lu mv ju lw no mw lz ma nq mx md me ns my mh mi mj ij bi translated"><strong class="lq ir">快速补充说明:</strong> <em class="iq">像所有其他机器学习算法一样，朴素贝叶斯也需要一个验证集来评估训练模型的有效性。但是，因为这篇文章旨在关注算法的见解，所以我故意避开它，直接跳到测试部分</em></p></blockquote><h1 id="f7e2" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">深入挖掘概率数学</h1><p id="30aa" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在，您已经对训练朴素贝叶斯模型所需的概率计算建立了基本的理解，然后使用它来预测给定测试句子的概率，我现在将更深入地挖掘概率细节。<br/>在计算上一节中给定测试句子的概率时，我们只执行了给定的概率公式来进行测试时的预测:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/580575bf1c29d597f947e408898ad075.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*0GYZ13HaXpjFdp7BZHGSmw.png"/></div></figure><p id="f078" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir">解码上面的数学方程式:</strong></p><p id="59d5" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">"<strong class="lq ir"> | </strong> " =指已经给定的状态和/或一些过滤标准</p><p id="e84a" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">=类/类别</p><p id="e30b" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">"<strong class="lq ir"> x </strong> " =测试示例/测试句子</p><p id="1be8" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir"><em class="mt">p(c | x)</em></strong><em class="mt"/>=<strong class="lq ir">给定测试示例<em class="mt"> x </em> </strong>，它属于类<em class="mt"> c </em>的概率是多少。这也被称为后验概率。这是<strong class="lq ir">为每个给定培训类的给定测试示例<em class="mt"> x </em>找到的条件概率。</strong></p><p id="7ed8" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir"> <em class="mt"> p(x|c) </em> </strong> = <strong class="lq ir">给定类<em class="mt"> c </em> </strong>，实例<em class="mt"> x </em>属于类<em class="mt"> c </em>的<strong class="lq ir">概率是多少。</strong>这也被称为可能性，因为它暗示了示例<strong class="lq ir"> <em class="mt"> x </em> </strong>属于类<strong class="lq ir"> <em class="mt"> c </em> </strong>的可能性有多大。这也是<strong class="lq ir">条件概率</strong>，因为我们正在从类<strong class="lq ir"><em class="mt"/></strong>的全部实例中寻找<strong class="lq ir"><em class="mt"/></strong>的概率，也就是说，在寻找<em class="mt"> x </em>的概率时，我们已经将<strong class="lq ir">的搜索空间限制/调节到类<em class="mt"> c </em>。我们使用在训练阶段确定的字数来计算这个概率。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/ee4ba26b86b8a2cc38ffec702e7463b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*ZO9HOBJWp0q2c2zn9g9u-g.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Here <strong class="bd pe">“ <em class="pv">j </em>”</strong> <strong class="bd pe"><em class="pv">represents a class</em></strong><em class="pv"> and </em><strong class="bd pe"><em class="pv">k</em></strong><em class="pv"> </em><strong class="bd pe"><em class="pv">represents a feature</em></strong></figcaption></figure><p id="106d" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">由于我们有两个类，我们在上面的计算部分隐式地使用了这个公式两次。还记得在类<strong class="lq ir"><em class="mt"><strong class="lq ir"><em class="mt">p</em></strong>中找到一个测试字<strong class="lq ir"><em class="mt">j</em></strong><em class="mt">c</em></em></strong>的数值吗？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qp"><img src="../Images/6da5c441c7c7f5f8d0806eb4106ed9e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l_oQzfN76efgMukcEIVb7Q.png"/></div></div></figure><p id="67ca" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir"> <em class="mt"> p </em> </strong> =这暗示了类<em class="mt"> c </em>的<strong class="lq ir">概率。</strong>这也被称为先验概率/无条件概率。这是无条件概率。我们在前面的概率计算部分已经计算过了(在步骤# 1 中，寻找类<strong class="lq ir"> <em class="mt"> c </em> </strong>的 term : <strong class="lq ir"> <em class="mt"> p </em> </strong>的值)</p><p id="da28" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir"><em class="mt">【p(x)</em></strong>=这也被称为<strong class="lq ir"> <em class="mt">归一化常数以便概率</em></strong><em class="mt"/><strong class="lq ir"><em class="mt">p(c | x)确实落在范围【0，1】</em></strong><em class="mt">内。所以如果去掉这个，概率</em><strong class="lq ir"><em class="mt">p(c | x)</em></strong><em class="mt">不一定落在[0，1]的范围内。</em>直观地说，这意味着在任何情况下或不管其类别标签是正还是负，实例<strong class="lq ir"> <em class="mt"> x </em> </strong>的概率。<br/>这也反映在全概率定理中，全概率定理用于<strong class="lq ir"><em class="mt"/></strong>计算 p(x)<strong class="lq ir"><em class="mt"/></strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qq"><img src="../Images/532ae2ec13a7d0f04c9b8289c46bf79b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SJ8K92XvcIy3uKN-heIXsg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Total Probability Theorem</figcaption></figure><p id="5e1a" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">这意味着，如果我们有两个阶级，那么我们将有两个术语，所以在我们的积极和消极情绪的特殊情况下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qr"><img src="../Images/4269164399b370cee3789619117a0d1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Lgd1YKDcFpGDcinMLGNwg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Total Probability Theorem for Two Classes</figcaption></figure><p id="f0ad" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">我们在上面的计算中使用它了吗？不，我们没有。为什么？？？因为我们正在比较正类和负类的概率，并且因为分母保持不变，所以在这种特殊情况下，省略相同的分母不会影响我们训练好的模型的预测。这两个类只是相互抵消。因此，虽然我们可以包括它，但没有这样做的逻辑理由。<strong class="lq ir">但是同样由于我们已经消除了<em class="mt">归一化常数，概率 p(c|x)不一定落在【0，1】</em>T5 的范围内</strong></p><blockquote class="nj nk nl"><p id="369d" class="lo lp mt lq b lr mu jr lt lu mv ju lw no mw lz ma nq mx md me ns my mh mi mj ij bi translated"><strong class="lq ir"> <em class="iq">达到里程碑# 4👍</em>T9】</strong></p></blockquote></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><h1 id="ecc0" class="kw kx iq bd ky kz ob lb lc ld oc lf lg jw od jx li jz oe ka lk kc of kd lm ln bi translated">避免下溢错误的常见陷阱！</h1><ul class=""><li id="7c64" class="pf pg iq lq b lr ls lu lv lx qs mb qt mf qu mj pk pl pm pn bi translated">如果你注意到的话，单词概率的数值(即类别<strong class="lq ir"><em class="mt">【c</em></strong>中的一个测试单词<strong class="lq ir"><em class="mt">【j】</em></strong>)非常小。因此，将所有这些微小的概率相乘以找到<strong class="lq ir">乘积</strong>(<strong class="lq ir"><em class="mt">p</em><em class="mt">【j】</em></strong>类<strong class="lq ir"><em class="mt">【c】</em></strong>)将产生甚至更小的数值，这经常导致下溢，这显然意味着对于给定的测试句子，训练的模型将无法预测其类别/情感。因此，为了避免这种下溢误差，我们采用如下数学对数的帮助:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qv"><img src="../Images/5d54b73d79a2da85a2c21ab6fd36ed91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*flR4CHgqTxn995JbRR2VvA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Avoiding the Underflow Error</figcaption></figure><ul class=""><li id="eb52" class="pf pg iq lq b lr mu lu mv lx ph mb pi mf pj mj pk pl pm pn bi translated">所以现在，我们将简单地把它们相加，而不是把微小的单个单词的概率相乘。为什么只有日志？为什么不是其他功能？因为 log 单调增加或减少，这意味着它不会影响概率的顺序。较小的概率在应用对数后仍然会保持较小，反之亦然。因此，假设测试单词“是”比测试单词“快乐”具有更小的概率，因此在通过 log 之后，虽然增加了它们的大小，但是“是”仍然具有比“快乐”更小的概率。因此，在不影响我们的训练模型的预测的情况下，我们可以有效地避免下溢误差的常见陷阱。</li></ul><blockquote class="nj nk nl"><p id="5a59" class="lo lp mt lq b lr mu jr lt lu mv ju lw no mw lz ma nq mx md me ns my mh mi mj ij bi translated"><strong class="lq ir"> <em class="iq">达到里程碑# 5👍</em> </strong></p></blockquote></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><h1 id="aa72" class="kw kx iq bd ky kz ob lb lc ld oc lf lg jw od jx li jz oe ka lk kc of kd lm ln bi translated">结束语……</h1><ul class=""><li id="75a7" class="pf pg iq lq b lr ls lu lv lx qs mb qt mf qu mj pk pl pm pn bi translated">尽管我们生活在一个 API 的时代，几乎很少从头开始编码。但是深入理解算法理论对于正确理解机器学习算法实际上是如何工作的极其重要。只有关键的理解才能区分真正的数据科学家和天真的数据科学家，以及在训练一个真正优秀的模型时真正重要的东西。因此，在转向 API 之前，我个人认为，一个真正的数据科学家应该从头开始编写代码，以真正看到数字背后以及某个特定算法优于其他算法的原因。</li><li id="ab6d" class="pf pg iq lq b lr po lu pp lx pq mb pr mf ps mj pk pl pm pn bi translated">朴素贝叶斯模型最好的特点之一是，你可以通过简单地用新词汇更新它来提高它的准确性，而不是总是重新训练它。你只需要添加单词到词汇表中，并相应地更新单词数。就是这样！</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qw"><img src="../Images/15151b0a3d70ef518cac4fc138c7fea5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rPiEOqV7Qnv2PGwXY3AwTg.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@bendavisual?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Benjamin Davies</a> on <a class="ae kv" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="nj nk nl"><p id="c7b9" class="lo lp mt lq b lr mu jr lt lu mv ju lw no mw lz ma nq mx md me ns my mh mi mj ij bi translated"><strong class="lq ir"> <em class="iq">终于来了！终于！达到里程碑# 6😤 😤 😤</em>T9】</strong></p></blockquote></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><p id="d6df" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">这篇博文到此结束&amp;你已经在你的 ML 之旅中向前迈进了一步！😄</p><p id="de76" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">即将发布的帖子将包括:</p><ul class=""><li id="684c" class="pf pg iq lq b lr mu lu mv lx ph mb pi mf pj mj pk pl pm pn bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/naïve-bayes-from-scratch-using-python-only-no-fancy-frameworks-a1904b37222d">从头开始展开朴素贝叶斯！Take-2🎬用 Python 从头开始实现朴素贝叶斯</a></li><li id="8709" class="pf pg iq lq b lr po lu pp lx pq mb pr mf ps mj pk pl pm pn bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/the-final-act-just-like-a-naïve-bayes-pro-5c440b511b8d">从头开始展开朴素贝叶斯！Take-3🎬使用 scikit-learn 实现朴素贝叶斯(<em class="mt"> Python 的</em> <em class="mt">机器学习的圣杯！</em> ) </a></li></ul><p id="15ac" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">敬请期待！📻</p><p id="8662" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">如果您有任何想法、意见或问题，欢迎在下面评论或联系📞跟我上<a class="ae kv" href="https://www.linkedin.com/in/aisha-javed/" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir"> LinkedIn </strong> </a></p></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><h1 id="a339" class="kw kx iq bd ky kz ob lb lc ld oc lf lg jw od jx li jz oe ka lk kc of kd lm ln bi translated">内容许可—一些注意事项…</h1><h2 id="4ff8" class="oh kx iq bd ky oi oj dn lc ok ol dp lg lx om on li mb oo op lk mf oq or lm os bi translated">再用</h2><p id="7416" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">图表和文本根据媒体的服务条款获得许可，即作者拥有其创作并在媒体上发布的内容的权利。"未经您的许可(或合理使用许可)，其他人不得复制、分发或执行您的作品."</p><p id="e620" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">这篇博文中包含的来自其他来源的图片不属于本许可证的范围，可以通过标题“图片作者…”中的注释来识别。所有此类图片均取自<a class="ae kv" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>，可免费使用，无需获得摄影师或 Unsplash 的许可或提供其信用。详细的许可条款可以参考<a class="ae kv" href="https://unsplash.com/license" rel="noopener ugc nofollow" target="_blank">这里的</a>。</p><p id="dc7b" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated"><strong class="lq ir">引文<br/> </strong>出于署名目的，本作品被引为</p><pre class="kg kh ki kj gt qx qy qz ra aw rb bi"><span id="f87c" class="oh kx iq qy b gy rc rd l re rf">Aisha Javed, “Unfolding Naïve Bayes from Scratch !”, Towards Data Science, 2018</span></pre><p id="dc14" class="pw-post-body-paragraph lo lp iq lq b lr mu jr lt lu mv ju lw lx mw lz ma mb mx md me mf my mh mi mj ij bi translated">BibTeX 引文</p><pre class="kg kh ki kj gt qx qy qz ra aw rb bi"><span id="05d9" class="oh kx iq qy b gy rc rd l re rf">@ARTICLE {javed2018a,<br/>    author  = "Javed, Aisha",<br/>    title   = "Unfolding Naïve Bayes from Scratch !",<br/>    journal = "Towards Data Science",<br/>    year    = "2018",<br/>    note    = "https://towardsdatascience.com/unfolding-na%C3%AFve-bayes-from-scratch-2e86dcae4b01"<br/>    <!-- -->doi     =  "2e86dcae4b01"<br/>}</span></pre></div></div>    
</body>
</html>