<html>
<head>
<title>Building a Toy Detector with Tensorflow Object Detection API</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Tensorflow 对象检测 API 构建玩具检测器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-toy-detector-with-tensorflow-object-detection-api-63c0fdf2ac95?source=collection_archive---------0-----------------------#2017-09-22">https://towardsdatascience.com/building-a-toy-detector-with-tensorflow-object-detection-api-63c0fdf2ac95?source=collection_archive---------0-----------------------#2017-09-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="e876" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">用数据做酷事！</em></p><p id="5be0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个项目是我的热门项目的第二阶段-<a class="ae km" href="https://medium.com/towards-data-science/is-google-tensorflow-object-detection-api-the-easiest-way-to-implement-image-recognition-a8bd1f500ea0" rel="noopener">Google tensor flow 物体检测 API 是实现图像识别最简单的方法吗</a>？在最初的文章中，我使用 Tensorflow 提供的<a class="ae km" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" rel="noopener ugc nofollow" target="_blank">模型</a>来检测 youtube 视频中的常见对象。这些模型在<a class="ae km" href="http://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank"> COCO </a>数据集上进行训练，并在该数据集中包含的 90 个常见对象上运行良好。</p><p id="8a86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里我扩展了 API 来训练一个不属于 COCO 数据集的新对象。在这种情况下，我选择了一个到处都是的玩具。见下图 gif。到目前为止，我对 API 的性能印象深刻。这里强调的步骤可以扩展到您想要构建的任何单个或多个对象检测器。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi kn"><img src="../Images/72c6dacff53a823d07b6bd23eef28947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*726b_wRuqYwUm_5N3Tqg8Q.gif"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Tensorflow Toy Detector~</figcaption></figure><p id="43dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以在我的<a class="ae km" href="https://github.com/priya-dwivedi/Deep-Learning/tree/master/tensorflow_toy_detector" rel="noopener ugc nofollow" target="_blank"> Github </a> repo 上找到代码</p><ol class=""><li id="f1b3" class="kz la iq jp b jq jr ju jv jy lb kc lc kg ld kk le lf lg lh bi translated"><strong class="jp ir">收集数据</strong></li></ol><p id="8612" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一步是为您的项目收集图像。你可以从谷歌上下载它们，确保你在角度，亮度，比例等方面有很大的变化。在我的例子中，我创建了一个小飞机玩具的视频，并使用<a class="ae km" href="https://stackoverflow.com/questions/33311153/python-extracting-and-saving-video-frames" rel="noopener ugc nofollow" target="_blank"> Opencv </a>从视频中提取图像。这节省了我很多时间。我确保图像是从多个角度拍摄的。您还可以随机改变某些图像的亮度，以便探测器可以在不同的闪电条件下工作。总共 100-150 张照片就足够了。请看下面的一些样本图片:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi li"><img src="../Images/656f986208b448359873c6fffba4e1a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*bjWsM_WAcmzFM3xLlfS2kQ.jpeg"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Sample images</figcaption></figure><p id="1ee4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PS:由于视频是从我的 Iphone 上拍摄的，原始图像相当大——1920 x 1090。这将需要大量的内存，所以使用<a class="ae km" href="http://pillow.readthedocs.io/en/3.1.x/reference/Image.html" rel="noopener ugc nofollow" target="_blank"> PIL 调整大小</a>来调整它们的大小到 500x300，保持纵横比。</p><p id="7e3d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.<strong class="jp ir">注释图像</strong></p><p id="f0ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我用<a class="ae km" href="https://github.com/tzutalin/labelImg" rel="noopener ugc nofollow" target="_blank">标签</a>给图像加了注释。这是一个非常方便的工具，注释是以 Pascal VOC 格式创建的，这对以后很有用。用 Python 写的，接口用 Qt。我用 Python3 + Qt5 都没问题。参见注释图像示例。本质上，我们识别对象的 xmin、ymin、xmax 和 ymax，并将其与用于训练的图像一起传递给模型</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/586fe93c6e44a144574cc9365f94d4b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*j1JUAOzx49An4Dg_xmPovw.png"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Annotating using labelimg</figcaption></figure><p id="f7aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 3。创建 TFR 数据集</strong></p><p id="0907" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Tensorflow API 希望数据集采用<a class="ae km" href="https://www.tensorflow.org/api_guides/python/python_io#tfrecords_format_details" rel="noopener ugc nofollow" target="_blank"> TFRecord 文件格式</a>。这可能是最棘手的部分。然而 tensorflow 已经提供了几个方便的脚本让你开始使用——<code class="fe lk ll lm ln b"><a class="ae km" href="https://github.com/tensorflow/models/blob/master/research/object_detection/create_pascal_tf_record.py" rel="noopener ugc nofollow" target="_blank">create_pascal_tf_record</a>.py</code>和<code class="fe lk ll lm ln b"><a class="ae km" href="https://github.com/tensorflow/models/blob/master/research/object_detection/create_pet_tf_record.py" rel="noopener ugc nofollow" target="_blank">create_pet_tf_record.py</a>. </code>我可以使用<code class="fe lk ll lm ln b"><a class="ae km" href="https://github.com/tensorflow/models/blob/master/research/object_detection/create_pet_tf_record.py" rel="noopener ugc nofollow" target="_blank">create_pet_tf_record.py</a></code>进行最小的编辑，因为 labelimg 已经以正确的格式创建了注释。我还喜欢这个脚本随机抽取 30%的数据并创建一个验证 TFR 文件。</p><p id="38f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您还需要创建一个 label.pbtxt 文件，用于将标签名称转换为数字 id。对我来说，这很简单</p><pre class="ko kp kq kr gt lo ln lp lq aw lr bi"><span id="8e9f" class="ls lt iq ln b gy lu lv l lw lx">item {<br/> id: 1<br/> name: ‘toy’<br/>}</span></pre><p id="b8e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我在 github 上包含了 label_map.pbtxt 文件和 create_pet_tf_records.py 文件。如果你被困在某个地方，我强烈推荐 Tensorflow 提供的<a class="ae km" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md" rel="noopener ugc nofollow" target="_blank"> Oxfort Pets 漫游</a>。</p><p id="45f8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 4。创建模型配置文件</strong></p><p id="6516" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">创建 TFR 数据集后，首先需要决定是使用现有模型并对其进行微调，还是从头构建。我强烈推荐使用现有模型，因为 CNN 学习的大多数特征通常是对象不可知的，并且微调现有模型通常是一个简单而准确的过程。请注意，如果你决定从头开始构建，你将需要远远超过 150 个图像和培训将需要几天时间。该 API 提供了 5 种不同的模型，这些模型在执行速度和放置边界框的准确性之间提供了一种平衡。请参见下表:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ly"><img src="../Images/7bcbfff06ad4fc878c79929e10bd4d7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-EyxSs2OiyWm-E6MSpSJiA.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Tensorflow Detection Models</figcaption></figure><p id="5deb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于这个项目，我决定使用在 coco 数据集上训练的 faster _ rcnn _ resnet101。如果你想了解更多关于 RCNN 模型的知识，这是一个非常好的<a class="ae km" href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="db4f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Tensorflow 提供了几个<a class="ae km" href="https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs" rel="noopener ugc nofollow" target="_blank">示例配置文件</a>来开始使用。我决定使用 faster_rcnn_resnet101_coco 文件，并更新了文件中需要配置的任何路径。别忘了更新 num。班级也是。</p><p id="af64" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 5。训练模型</strong></p><p id="0258" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">终于！所有困难(和无聊)的部分都完成了，我们可以开始训练模型了。由于我有一个合理的 GPU，我决定在当地培训。然而，你可以在云上训练。tensorflow 文档再次简化了这一过程，并提供了所有的<a class="ae km" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_cloud.md" rel="noopener ugc nofollow" target="_blank">步骤</a>。</p><p id="f4de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可以同时在两个独立的终端上启动培训作业和评估作业。启动 tensorboard 监控性能。经过 2-3 个小时的训练，我可以看到总损耗下降到 0.077，精度达到 0.99。通过查看 Tensorboard 中的图像，我们可以看到模型很快变得准确。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi md"><img src="../Images/d4cedc0cae1b5ed29272ee1ed4b6b3a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*g_TkrHZ6GW8zLlbnjAFNOA.png"/></div></figure><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi me"><img src="../Images/4c22814161d9ca2015364a5bdb80b372.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*O-neW733ol0LxameGbn-fg.png"/></div></figure><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/b9bd6b2b497498bf6c7cede8545307b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/1*wQ1-FVSMLM2FN-RtbGyn3g.gif"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Model gets accurate pretty quickly</figcaption></figure><p id="5085" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 6。测试模型</strong></p><p id="2d52" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了测试模型，我们首先选择一个模型检查点(通常是最新的)并导出到一个冻结的推理图中。这个脚本也在我的 github 上。我在我的 Iphone 上录制的新视频中测试了这个模型。正如在我的<a class="ae km" href="https://medium.com/towards-data-science/is-google-tensorflow-object-detection-api-the-easiest-way-to-implement-image-recognition-a8bd1f500ea0" rel="noopener">上一篇</a>文章中，我使用 Python moviepy 库将视频解析成帧，然后在每一帧上运行 object detector，并将结果整理回视频中。</p><h1 id="09b9" class="mg lt iq bd mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc bi translated">后续步骤</h1><p id="eabc" class="pw-post-body-paragraph jn jo iq jp b jq nd js jt ju ne jw jx jy nf ka kb kc ng ke kf kg nh ki kj kk ij bi translated">我注意到的几件事和对未来的额外探索</p><ul class=""><li id="6713" class="kz la iq jp b jq jr ju jv jy lb kc lc kg ld kk ni lf lg lh bi translated">在测试过程中，我发现更快的 RCNN 模型有点慢。接下来，我将探索使用最快的模型——SSD mobilenet，看看准确性是否有明显下降</li><li id="b5a8" class="kz la iq jp b jq nj ju nk jy nl kc nm kg nn kk ni lf lg lh bi translated">对于这个模型，我只是使用了 faster_rcnn_resnet101_coco 的模型配置文件中的默认参数。如果可以调整它们以获得更好的性能，这可能是值得探索的</li><li id="6d9e" class="kz la iq jp b jq nj ju nk jy nl kc nm kg nn kk ni lf lg lh bi translated">稍加努力，这个过程可以扩展到其他类别</li></ul><p id="c7ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">给我一个❤️，如果你喜欢这个职位:)希望你拉代码，并尝试自己。</p><p id="5012" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我有自己的深度学习咨询公司，喜欢研究有趣的问题。我已经帮助许多初创公司部署了基于人工智能的创新解决方案。请到 http://deeplearninganalytics.org/来看看我们吧。</p><p id="1a45" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你也可以在 https://medium.com/@priya.dwivedi 的<a class="ae km" href="https://medium.com/@priya.dwivedi" rel="noopener">看到我的其他作品</a></p><p id="3264" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你有一个我们可以合作的项目，请通过我的网站或 info@deeplearninganalytics.org 联系我</p><p id="0228" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">参考文献:</strong></p><ul class=""><li id="7e76" class="kz la iq jp b jq jr ju jv jy lb kc lc kg ld kk ni lf lg lh bi translated"><a class="ae km" href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="noopener ugc nofollow" target="_blank">谷歌 Tensorflow 物体检测 Github </a></li><li id="33c2" class="kz la iq jp b jq nj ju nk jy nl kc nm kg nn kk ni lf lg lh bi translated">关于媒体的精彩文章<a class="ae km" href="https://medium.com/towards-data-science/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9" rel="noopener">给了我灵感和一些有用的提示</a></li></ul></div></div>    
</body>
</html>