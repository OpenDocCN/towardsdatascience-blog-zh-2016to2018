<html>
<head>
<title>Getting Started with Markov Decision Processes: Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">马尔可夫决策过程入门:强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-markov-decision-processes-reinforcement-learning-ada7b4572ffb?source=collection_archive---------2-----------------------#2018-10-02">https://towardsdatascience.com/getting-started-with-markov-decision-processes-reinforcement-learning-ada7b4572ffb?source=collection_archive---------2-----------------------#2018-10-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e1fd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">第二部分:解释<strong class="ak"> <em class="kf">马尔可夫决策过程、</em> </strong>贝尔曼方程和政策的概念</h2></div><p id="a24d" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这篇博文中，我将解释理解如何解决强化学习问题所需的概念。这一系列的博客文章包含了大卫·西尔弗在<a class="ae lc" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" rel="noopener ugc nofollow" target="_blank">关于强化学习的介绍</a>中解释的概念总结。</p><p id="9561" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">零件:<a class="ae lc" rel="noopener" target="_blank" href="/reinforcement-learning-an-introduction-to-the-concepts-applications-and-code-ced6fbfd882d">1</a>2<a class="ae lc" href="https://medium.com/@taggatle/planning-by-dynamic-programming-reinforcement-learning-ed4924bbaa4c" rel="noopener">3</a><a class="ae lc" rel="noopener" target="_blank" href="/model-free-prediction-reinforcement-learning-507297e8e2ad">4</a>…</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/e6304b643b55c79c3544616d4808d02e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JmwaGbI6eVJ_qdqG"/></div></div></figure><p id="9fe2" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">到目前为止，我们已经在很高的层次上学习了设置强化学习问题所需的组件。现在，我们将更详细地正式描述强化学习的环境。在本帖中，我们将看看一个<em class="lp">完全可观测的</em> <em class="lp">环境</em>以及如何将环境正式描述为<strong class="ki ir"> <em class="lp">马尔可夫决策过程</em> </strong> (MDPs)。</p><blockquote class="lq"><p id="7ca1" class="lr ls iq bd lt lu lv lw lx ly lz lb dk translated">如果我们能解决马尔可夫决策过程，那么我们就能解决一大堆强化学习问题。</p></blockquote><p id="01e8" class="pw-post-body-paragraph kg kh iq ki b kj ma jr kl km mb ju ko kp mc kr ks kt md kv kw kx me kz la lb ij bi translated">MDPs 需要满足<strong class="ki ir"> <em class="lp">马尔可夫性质</em> </strong>。</p><blockquote class="mf mg mh"><p id="a84d" class="kg kh lp ki b kj kk jr kl km kn ju ko mi kq kr ks mj ku kv kw mk ky kz la lb ij bi translated"><strong class="ki ir">马尔可夫性质:</strong> <em class="iq">要求“未来独立于过去给定的现在”。</em></p></blockquote><p id="e649" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lp">性质</em> </strong>:麟州<em class="lp"> Sₜ </em>是马氏当且仅当:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/67df8e5cdf50ce60ff13d43183820794.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*_CjY9kkWBOwGXhb3RP3Huw.png"/></div></figure><p id="5ebe" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">简单地说，这意味着状态<em class="lp"> Sₜ </em>从历史中获取所有相关信息。<em class="lp"> S₁，S₂，…，Sₜ₋₁ </em>可以丢弃，我们仍然得到相同的<strong class="ki ir"> <em class="lp">状态转移概率</em> </strong> <em class="lp"> </em>到下一个状态<em class="lp"> Sₜ₊₁ </em>。</p><blockquote class="mf mg mh"><p id="22d8" class="kg kh lp ki b kj kk jr kl km kn ju ko mi kq kr ks mj ku kv kw mk ky kz la lb ij bi translated"><strong class="ki ir">状态转移概率<em class="iq"> : </em> </strong> <em class="iq">状态转移概率告诉我们，给定我们处于状态</em> s <em class="iq">下一个状态</em>s’<em class="iq">会发生的概率是多少。</em></p></blockquote><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/d597b43260de1873dd07d2f49367a2ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*IiFCmDYpik1OHrvMxMyhBQ.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk"><em class="kf">P without the double lines represents the state transitions. The above equation has the transition from state s to state s’. P with the double lines represents the probability from going from state s to s’.</em></figcaption></figure><p id="d0c1" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们也可以根据一个<em class="lp">状态转移矩阵</em> <strong class="ki ir"> <em class="lp"> P </em> </strong>来定义所有的状态转移，其中每一行都告诉我们从一个状态到所有可能的后续状态的转移概率。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/7bb06c371e3268dcd5217972a90d3674.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*KM8hgVRNKaqQ-NMPFEA5Eg.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">State transition matrix.</figcaption></figure><h1 id="796b" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated"><strong class="ak">马尔可夫过程/马尔可夫链</strong></h1><p id="7596" class="pw-post-body-paragraph kg kh iq ki b kj nk jr kl km nl ju ko kp nm kr ks kt nn kv kw kx no kz la lb ij bi translated">第一个也是最简单的 MDP 是一个<strong class="ki ir"> <em class="lp">马尔可夫过程</em> </strong>。</p><blockquote class="mf mg mh"><p id="a023" class="kg kh lp ki b kj kk jr kl km kn ju ko mi kq kr ks mj ku kv kw mk ky kz la lb ij bi translated"><strong class="ki ir">马尔可夫过程/马尔可夫链</strong> : <em class="iq">具有</em>马尔可夫性质的随机状态序列 S₁、S₂、… <em class="iq">。</em></p></blockquote><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi np"><img src="../Images/76e1397deada20ad16fd27a67aec44e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Zel2uk7f8SJ-PunbViplA.png"/></div></div></figure><p id="f8eb" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">下面是一个马尔可夫链的图示，其中每个节点代表一个状态，有可能从一个状态转移到下一个状态，其中<em class="lp"> Stop </em>代表一个终止状态。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/daf1b954258d9a374dc015dc5d74e9de.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*GuwS_He6GvbAC5IJ4F2I5Q.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">Markov Chain</figcaption></figure><p id="a8c7" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们可以拿一个<strong class="ki ir"> <em class="lp">样本</em> </strong>集来遍历这个链，最后到达终点状态。一个示例情节是从<em class="lp">阶段 1 </em>到<em class="lp">阶段 2 </em>到<em class="lp">赢得</em>到<em class="lp">停止。</em>以下是几个样本集的代表:</p><p id="0e05" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">- <em class="lp"> S1 S2 赢停<br/> - S1 S2 瞬移 S2 赢停<br/> - S1 暂停 S1 S2 赢停</em></p><p id="0b63" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">上述马尔可夫链具有以下转移概率矩阵:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/2908a42e432882c380b84743574d3561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*AeO07kqYUsFJCTg0GpbvTg.png"/></div></figure><p id="94fe" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">对于每个状态，该状态的转移概率之和等于 1。</p><h1 id="8b35" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">马尔可夫奖励过程</h1><p id="c320" class="pw-post-body-paragraph kg kh iq ki b kj nk jr kl km nl ju ko kp nm kr ks kt nn kv kw kx no kz la lb ij bi translated">在上面的马尔可夫链中，我们没有与达到目标的状态相关联的值。一个<strong class="ki ir"> <em class="lp">马尔可夫奖励过程</em> </strong>是一个带有奖励值的马尔可夫链。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ns"><img src="../Images/388f51b10a6859c0e7e359700d3fe251.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iDcbBaNHdUBcT6E90lcvVg.png"/></div></div></figure><blockquote class="mf mg mh"><p id="68a6" class="kg kh lp ki b kj kk jr kl km kn ju ko mi kq kr ks mj ku kv kw mk ky kz la lb ij bi translated"><em class="iq">我们的目标是最大化</em> <strong class="ki ir">回报</strong> <em class="iq">。回报</em> Gₜ <em class="iq">是从时步</em> t <em class="iq">开始的总折扣奖励。</em></p></blockquote><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ca5bb580071035cf9a295acc2f722b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*oMaUx4Rf2328uhtm6AkGzw.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">Equation to calculate return</figcaption></figure><p id="9499" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lp">折现因子</em> </strong> <em class="lp"> γ </em>是介于 0 和 1 之间的一个值(可以选择)。如果γ接近 0，则导致近视评估，而接近 1 的值有利于远视评估。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/1738dc64e9efbcbfd67616e4e3c7e60c.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*OIUshKzPYOJxGghurcQdbA.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">Markov Reward Process</figcaption></figure><p id="08e9" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lp">注</em> </strong> <em class="lp">:由于在马尔可夫奖励过程中我们没有采取行动，Gₜ是通过遍历随机样本序列来计算的。</em></p><h2 id="d923" class="nu mt iq bd mu nv nw dn my nx ny dp nc kp nz oa ne kt ob oc ng kx od oe ni of bi translated">MRP 的价值函数</h2><blockquote class="mf mg mh"><p id="b3ae" class="kg kh lp ki b kj kk jr kl km kn ju ko mi kq kr ks mj ku kv kw mk ky kz la lb ij bi translated"><strong class="ki ir">状态值函数</strong> v(s) <em class="iq">:给出状态</em> s <em class="iq">的长期值。它是从状态</em> s 开始的预期收益</p></blockquote><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi og"><img src="../Images/2f47a2c01e0f81b76ec234625f103234.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*dRWVff1IByUe2ks5_vcqdQ.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">state-value function</figcaption></figure><p id="3beb" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们可以这样看，从状态<em class="lp"> s </em>开始，通过状态<em class="lp"> s </em>的各种样本，我们的预期收益是多少。我们更喜欢给予更多总回报的州。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/22f8ec32d749521c7b2c4d4961f81ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*jxaBPxz_3FD3gXGEtKWxAQ.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">state-values for MRP with <em class="kf">γ=1</em></figcaption></figure><h2 id="3d6a" class="nu mt iq bd mu nv nw dn my nx ny dp nc kp nz oa ne kt ob oc ng kx od oe ni of bi translated">MRPs 的贝尔曼方程</h2><p id="3dd9" class="pw-post-body-paragraph kg kh iq ki b kj nk jr kl km nl ju ko kp nm kr ks kt nn kv kw kx no kz la lb ij bi translated">价值函数可以分解为两部分:</p><ul class=""><li id="ace3" class="oh oi iq ki b kj kk km kn kp oj kt ok kx ol lb om on oo op bi translated"><strong class="ki ir"> <em class="lp">即时奖励:</em> </strong> <em class="lp"> Rₜ₊₁ </em></li><li id="5740" class="oh oi iq ki b kj oq km or kp os kt ot kx ou lb om on oo op bi translated"><strong class="ki ir"> <em class="lp">继承国贴现值</em></strong>:<em class="lp">γ(s</em>ₜ₊₁<em class="lp">)</em></li></ul><p id="7a2a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们可以使用上面的<em class="lp">状态值</em>函数和<em class="lp">返回函数</em>定义一个新的等式来计算状态值函数:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/569bd45fc44031de15481f7c2fcbf28c.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*cgwvBcAk13UXu3zyHm2iqA.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">updated bellman state-value equation</figcaption></figure><p id="5e3b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">或者，这可以写成矩阵形式:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/399759adc4060d50a7addababc146295.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*94klO-ZZpzf3cSb422XN8w.png"/></div></figure><p id="0051" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">使用这个等式，我们可以计算每个状态的状态值。由于我们在上面有一个简单的模型，带有γ=1 的 MRP 的<em class="lp">“状态值”</em>，我们可以使用一个联立方程，使用更新的状态值函数来计算状态值。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/448a8b53eaa8ed97e41a6e9c2215ecc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*IJc9jfe5UDAMnchOr1FVSw.png"/></div></figure><p id="8343" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">对于较小的 MRPs，求解上面的方程很简单，但是对于较大的数目，就变得非常复杂。为了解决大型 MRP，我们需要其他技术，如<em class="lp">动态规划</em>、<em class="lp">蒙特卡洛评估</em>和<em class="lp">时差学习</em>，这些将在后面的博客中讨论。</p><h1 id="4eb8" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">马尔可夫决策过程</h1><p id="f01a" class="pw-post-body-paragraph kg kh iq ki b kj nk jr kl km nl ju ko kp nm kr ks kt nn kv kw kx no kz la lb ij bi translated">一个<strong class="ki ir"> <em class="lp">马尔可夫决策过程</em> </strong>是一个马尔可夫回报过程的扩展，因为它包含了一个代理必须做出的决策。环境中的所有状态都是马尔可夫的。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/2eb06923a10a75adaffe9e66b4164e7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*GnXgVGKpDJaBCZEEO5m0Tw.png"/></div></figure><p id="1a80" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在马尔可夫决策过程中，我们现在对进入哪个状态有了更多的控制。在下面的 MDP 的例子中，如果我们选择进行<em class="lp">传送</em>动作，我们将在 40%的时间里回到状态<em class="lp">阶段 2 </em>中，在 60%的时间里回到状态<em class="lp">阶段 1 </em>中。当选择相应的动作时，其他状态转换以 100%的概率发生，例如采取动作<em class="lp">前进 2 </em>从<em class="lp">阶段 2 </em>将把我们带到<em class="lp">胜利。</em></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/c7ca79f9c4c9833b193d473d4e3804a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*SJuuyzVDmcQ91q0K2a6I4Q.png"/></div></figure><h2 id="0c47" class="nu mt iq bd mu nv nw dn my nx ny dp nc kp nz oa ne kt ob oc ng kx od oe ni of bi translated">政策</h2><blockquote class="mf mg mh"><p id="19a5" class="kg kh lp ki b kj kk jr kl km kn ju ko mi kq kr ks mj ku kv kw mk ky kz la lb ij bi translated">一个<strong class="ki ir">策略</strong> <em class="iq"> π </em>是给定状态下动作的分布。它完全定义了一个代理的行为。MDP 政策取决于现状，而不是历史。</p></blockquote><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/aac716c912a906524435e9a35b8dbc12.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*rxT7AUpHHyEwHpjnnAnFJQ.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">Policy function</figcaption></figure><p id="f5fb" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">策略给出了从一个状态到下一个状态的映射。如果我在状态<em class="lp"> s，</em>它从那个状态映射出采取每一个行动的概率。示例如果我们有策略<em class="lp"> π(杂务|阶段 1)=100%，</em>这意味着代理将在处于状态<em class="lp">阶段 1 时采取动作<em class="lp">杂务 100% </em>。</em></p><h2 id="9a12" class="nu mt iq bd mu nv nw dn my nx ny dp nc kp nz oa ne kt ob oc ng kx od oe ni of bi translated">MDP 的价值函数</h2><p id="a1af" class="pw-post-body-paragraph kg kh iq ki b kj nk jr kl km nl ju ko kp nm kr ks kt nn kv kw kx no kz la lb ij bi translated">因为我们采取行动，所以根据我们的行为会有不同的期望。</p><blockquote class="mf mg mh"><p id="8291" class="kg kh lp ki b kj kk jr kl km kn ju ko mi kq kr ks mj ku kv kw mk ky kz la lb ij bi translated"><em class="iq">MDP 的</em> <strong class="ki ir">状态值函数</strong><em class="iq"/>v _<em class="iq">π</em>(s)<em class="iq">是从状态</em> s <em class="iq">开始，然后遵循策略π的期望收益。</em></p></blockquote><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/06bb785ef26a48de9c79cb3d1325a422.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*Mcn5UIjRIAF1XoV1dimABw.png"/></div></figure><p id="b2c9" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">状态值函数告诉我们通过遵循策略<em class="lp"> π </em>处于状态<em class="lp"> s </em>有多好。</p><blockquote class="mf mg mh"><p id="ac80" class="kg kh lp ki b kj kk jr kl km kn ju ko mi kq kr ks mj ku kv kw mk ky kz la lb ij bi translated"><em class="iq"/><strong class="ki ir">动作值函数</strong> <em class="iq"> </em> q_π(s，a) <em class="iq">是从状态</em> s <em class="iq">开始，采取动作</em> a <em class="iq">，然后跟随策略</em> π的期望收益。</p></blockquote><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/c774803e5107c3141f6159bd29df7e87.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*icMXZv3TDA04jqZ9uOWduA.png"/></div></figure><p id="18ef" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">行动价值函数告诉我们从特定状态采取特定行动有多好。让我们知道我们应该在各州采取什么行动</p><h2 id="942e" class="nu mt iq bd mu nv nw dn my nx ny dp nc kp nz oa ne kt ob oc ng kx od oe ni of bi translated">贝尔曼期望方程</h2><p id="2bdc" class="pw-post-body-paragraph kg kh iq ki b kj nk jr kl km nl ju ko kp nm kr ks kt nn kv kw kx no kz la lb ij bi translated">价值函数也可以以<strong class="ki ir"> <em class="lp">的形式写成贝尔曼期望方程</em> </strong>如下:</p><p id="8e3a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lp">状态值函数:</em> </strong></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/312756664a715f520d384f8e034451cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*T6ayWEKh6J8CAYl1W-b4cw.png"/></div></figure><p id="070e" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lp">动作值功能:</em> </strong></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/fa4476585802ee0a072114bd8ed37244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*HAoT2rL868dHGQWlFyIe_A.png"/></div></figure><h2 id="6c5e" class="nu mt iq bd mu nv nw dn my nx ny dp nc kp nz oa ne kt ob oc ng kx od oe ni of bi translated">最优值函数</h2><p id="61a3" class="pw-post-body-paragraph kg kh iq ki b kj nk jr kl km nl ju ko kp nm kr ks kt nn kv kw kx no kz la lb ij bi translated">在上述所有等式中，我们使用给定的策略来遵循，这可能不是要采取的最佳行动。强化学习的关键目标是找到能使我们的回报最大化的最优策略。</p><blockquote class="mf mg mh"><p id="c8de" class="kg kh lp ki b kj kk jr kl km kn ju ko mi kq kr ks mj ku kv kw mk ky kz la lb ij bi translated"><em class="iq"/><strong class="ki ir">最优状态值函数</strong><em class="iq"/>v∫(s)<em class="iq">是所有策略的最大值函数。它告诉我们你能从系统中获得的最大回报。</em></p></blockquote><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/030d694f6d2d7f2c63769d341d4e2f92.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*YNAIiZNNfrEy35pnTpJdIg.png"/></div></figure><blockquote class="mf mg mh"><p id="c2bf" class="kg kh lp ki b kj kk jr kl km kn ju ko mi kq kr ks mj ku kv kw mk ky kz la lb ij bi translated"><em class="iq"/><strong class="ki ir">最优动作值函数</strong>q∫(s，a) <em class="iq">是所有策略上的最大动作值函数。它告诉我们，从状态</em> s <em class="iq">开始并采取行动</em> a <em class="iq">时，你能从系统中获得的最大回报是什么。</em></p></blockquote><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/30673cc2f827527136184e0a2ab802c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*Cuz8pZ91wA8HKUS8oLnvDA.png"/></div></figure><p id="f93c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">如果你知道<em class="lp">q∫</em>，那么你就知道在 MDP 中应该采取的正确行动和最佳表现，从而解决 MDP 问题。</p><blockquote class="lq"><p id="c5a2" class="lr ls iq bd lt lu lv lw lx ly lz lb dk translated">q∑(s，a)表示采取哪些行动来获得最佳表现。</p></blockquote><h2 id="8657" class="nu mt iq bd mu nv ph dn my nx pi dp nc kp pj oa ne kt pk oc ng kx pl oe ni of bi translated">寻找最佳策略</h2><p id="389a" class="pw-post-body-paragraph kg kh iq ki b kj nk jr kl km nl ju ko kp nm kr ks kt nn kv kw kx no kz la lb ij bi translated">通过最大化超过<em class="lp">q∫(s，a) </em>可以找到最优策略:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/5444cf3242712b8cc4cb557fed13118d.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*30GBo9W8JmXVDkmc5jJljA.png"/></div></figure><p id="782e" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lp">贝尔曼最优方程</em> </strong>是非线性的，很难求解。在后面的博客中，我将讨论使用各种技术来解决这个方程的迭代解决方案，例如<em class="lp">值迭代、策略迭代、Q-Learning </em>和<em class="lp"> Sarsa。</em></p><h1 id="dcca" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">参考</h1><ul class=""><li id="7a30" class="oh oi iq ki b kj nk km nl kp pn kt po kx pp lb om on oo op bi translated"><a class="ae lc" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf" rel="noopener ugc nofollow" target="_blank">关于 RL 的 UCL 课程——第 2 讲</a></li><li id="c033" class="oh oi iq ki b kj oq km or kp os kt ot kx ou lb om on oo op bi translated">《强化学习导论》，萨顿和巴尔托，1998 年</li></ul></div><div class="ab cl pq pr hu ps" role="separator"><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv"/></div><div class="ij ik il im in"><p id="f3dd" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">如果你喜欢这篇文章，并想看到更多，不要忘记关注和/或留下掌声。</p></div></div>    
</body>
</html>