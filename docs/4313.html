<html>
<head>
<title>K-Means++ Implementation in Python and Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-Means++在 Python 和 Spark 中的实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-implementation-in-python-and-spark-856e7eb5fe9b?source=collection_archive---------4-----------------------#2018-08-07">https://towardsdatascience.com/k-means-implementation-in-python-and-spark-856e7eb5fe9b?source=collection_archive---------4-----------------------#2018-08-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/9673d89deb6ab92aa2278d286b0a14cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/1*MWWjKw0f3T8nrv8euEgxOg.gif"/></div></figure><p id="b5b3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于本教程，我们将使用 PySpark，Apache Spark 的 Python 包装器。虽然 PySpark 有一个很好的 K-Means++实现，但我们将从头开始编写自己的实现。</p><h1 id="5a25" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">配置 PySpark 笔记本</h1><p id="3eea" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">如果你在 Jupyter 笔记本上没有 PySpark，我觉得这个教程很有用:</p><div class="lv lw gp gr lx ly"><a href="https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd ir gy z fp md fr fs me fu fw ip bi translated">3 分钟内开始使用 PySpark 和 Jupyter 笔记本</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">Apache Spark 是大数据爱好者的必备品。简而言之，Spark 是一个快速而强大的框架，它提供了一个…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">blog.sicara.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm js ly"/></div></div></a></div><h1 id="f405" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">将数据集作为 RDD 加载</h1><p id="e407" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">开始前，请确保您可以访问气象站数据集:<br/><a class="ae mn" href="https://github.com/yoavfreund/UCSD_BigData_2016/tree/master/Data/Weather" rel="noopener ugc nofollow" target="_blank">https://github . com/yoavfreund/UCSD _ BigData _ 2016/tree/master/Data/Weather</a></p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="433e" class="mx kt iq mt b gy my mz l na nb"><strong class="mt ir">def</strong> parse_data(row):<br/>    <em class="nc">'''</em><br/><em class="nc">    Parse each pandas row into a tuple of <br/>    (station_name,  feature_vec),`l</em><br/><em class="nc">    where feature_vec is the concatenation of the projection vectors</em><br/><em class="nc">    of TAVG, TRANGE, and SNWD.</em><br/><em class="nc">    '''</em><br/>    <strong class="mt ir">return</strong> (row[0],<br/>            np.concatenate([row[1], row[2], row[3]]))</span><span id="3c80" class="mx kt iq mt b gy nd mz l na nb"><em class="nc">## Read data</em><br/>data = pickle.load(open("stations_projections.pickle", "rb"))<br/>rdd = sc.parallelize([parse_data(row[1]) <br/>          <strong class="mt ir">for</strong> row <strong class="mt ir">in</strong> data.iterrows()])</span></pre><p id="dc8d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们看看第一行:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="708b" class="mx kt iq mt b gy my mz l na nb">rdd.take(1)</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/fae1d91ad569fe90f81345c9d910fe77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*8AW8Hr1ddNn2X6hXItU1XA.png"/></div></figure><p id="10b0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">气象站的名称是<strong class="jw ir"> USC00044534 </strong>，其余的是我们将用于聚类的不同天气信息。</p><h1 id="9a11" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">导入库</h1><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="20b2" class="mx kt iq mt b gy my mz l na nb"><strong class="mt ir">import </strong>numpy as np <br/><strong class="mt ir">import </strong>pickle <br/><strong class="mt ir">import </strong>sys <br/><strong class="mt ir">import </strong>time<br/><strong class="mt ir">from </strong>numpy.linalg <strong class="mt ir">import </strong>norm <br/><strong class="mt ir">from </strong>matplotlib <strong class="mt ir">import </strong>pyplot <strong class="mt ir">as </strong>plt</span></pre><h1 id="a39f" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">定义全局参数</h1><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="d046" class="mx kt iq mt b gy my mz l na nb"><em class="nc"># Number of centroids<br/></em>K = 5  </span><span id="12c6" class="mx kt iq mt b gy nd mz l na nb"><em class="nc"># Number of K-means runs that are executed in parallel. Equivalently, number of sets of initial points<br/></em>RUNS = 25  </span><span id="e11d" class="mx kt iq mt b gy nd mz l na nb"># For reproducability of results<br/>RANDOM_SEED = 60295531 </span><span id="2bd7" class="mx kt iq mt b gy nd mz l na nb"><em class="nc"># The K-means algorithm is terminated when the change in the <br/># location of the centroids is smaller than 0.1<br/></em>converge_dist = 0.1</span></pre><h1 id="b9af" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">效用函数</h1><p id="0e26" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">随着我们的发展，以下功能将会派上用场:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="3ff0" class="mx kt iq mt b gy my mz l na nb"><strong class="mt ir">def</strong> print_log(s):<br/>    '''<br/>    Print progress logs<br/>    '''<br/>    sys.stdout.write(s + "<strong class="mt ir">\n</strong>")<br/>    sys.stdout.flush()</span><span id="0a33" class="mx kt iq mt b gy nd mz l na nb"><br/><strong class="mt ir">def </strong>compute_entropy(d):<br/>    <em class="nc">'''</em><br/><em class="nc">    Compute the entropy given the frequency vector `d`</em><br/><em class="nc">    '''</em><br/>    d = np.array(d)<br/>    d = 1.0 * d / d.sum()<br/>    <strong class="mt ir">return</strong> -np.sum(d * np.log2(d))<br/><br/><br/><strong class="mt ir">def </strong>choice(p):<br/>    <em class="nc">'''</em><br/><em class="nc">    Generates a random sample from [0, len(p)),</em><br/><em class="nc">    where p[i] is the probability associated with i. </em><br/><em class="nc">    '''</em><br/>    random = np.random.random()<br/>    r = 0.0<br/>    <strong class="mt ir">for</strong> idx <strong class="mt ir">in</strong> range(len(p)):<br/>        r = r + p[idx]<br/>        <strong class="mt ir">if </strong>r &gt; random:<br/>            return idx<br/>    assert(False)</span></pre><h1 id="b68d" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">质心的初始化</h1><p id="c174" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">对于 K-Means++来说，我们希望在初始化时质心尽可能的分开。这个想法是让质心在初始化时更接近不同的聚类中心，从而更快地达到收敛。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="fc06" class="mx kt iq mt b gy my mz l na nb"><strong class="mt ir">def </strong>kmeans_init(rdd, K, RUNS, seed):<br/>    '''<br/>    Select `RUNS` sets of initial points for `K`-means++<br/>    '''<br/>    # the `centers` variable is what we want to return<br/>    n_data = rdd.count()<br/>    shape = rdd.take(1)[0][1].shape[0]<br/>    centers = np.zeros((RUNS, K, shape))</span><span id="6506" class="mx kt iq mt b gy nd mz l na nb">    <strong class="mt ir">def </strong>update_dist(vec, dist, k):<br/>        new_dist = norm(vec - centers[:, k], axis=1)**2<br/>        <strong class="mt ir">return </strong>np.min([dist, new_dist], axis=0)</span><span id="e231" class="mx kt iq mt b gy nd mz l na nb">    # The second element `dist` in the tuple below is the<br/>    # closest distance from each data point to the selected<br/>    # points in the initial set, where `dist[i]` is the<br/>    # closest distance to the points in the i-th initial set<br/>    data = (rdd<br/>            .map(<strong class="mt ir">lambda</strong> p: (p, [np.inf] * RUNS)) \<br/>            .cache())</span><span id="d02c" class="mx kt iq mt b gy nd mz l na nb">    # Collect the feature vectors of all data points<br/>    # beforehand, might be useful in the following<br/>    # for-loop<br/>    local_data = (rdd<br/>                    .map(<strong class="mt ir">lambda </strong>(name, vec): vec)<br/>                    .collect())</span><span id="edf8" class="mx kt iq mt b gy nd mz l na nb">    # Randomly select the first point for every run of<br/>    # k-means++, i.e. randomly select `RUNS` points<br/>    # and add it to the `centers` variable<br/>    sample = [local_data[k] <strong class="mt ir">for </strong>k <strong class="mt ir">in</strong><br/>        np.random.randint(0, len(local_data), RUNS)]<br/>    centers[:, 0] = sample</span><span id="2eb0" class="mx kt iq mt b gy nd mz l na nb">    <strong class="mt ir">for </strong>idx <strong class="mt ir">in </strong>range(K - 1):<br/>        ########################################################<br/>        # In each iteration, you need to select one point for<br/>        # each set of initial points (so select `RUNS` points<br/>        # in total). For each data point x, let D_i(x) be the<br/>        # distance between x and the nearest center that has<br/>        # already been added to the i-th set. Choose a new<br/>        # data point for i-th set using a weighted probability<br/>        # where point x is chosen with probability proportional<br/>        # to D_i(x)^2 . Repeat each data point by 25 times<br/>        # (for each RUN) to get 12140x25<br/>        ########################################################</span><span id="400f" class="mx kt iq mt b gy nd mz l na nb">        #Update distance<br/>        data = (data<br/>            .map(<strong class="mt ir">lambda </strong>((name,vec),dist):<br/>                    ((name,vec),update_dist(vec,dist,idx)))<br/>            .cache())</span><span id="457a" class="mx kt iq mt b gy nd mz l na nb">        #Calculate sum of D_i(x)^2<br/>        d1 = data.map(<strong class="mt ir">lambda </strong>((name,vec),dist): (1,dist))<br/>        d2 = d1.reduceByKey(<strong class="mt ir">lambda </strong>x,y: np.sum([x,y], axis=0))<br/>        total = d2.collect()[0][1]</span><span id="0617" class="mx kt iq mt b gy nd mz l na nb">        #Normalize each distance to get the probabilities and<br/>        #reshapte to 12140x25<br/>        prob = (data<br/>            .map(<strong class="mt ir">lambda </strong>((name,vec),dist):<br/>                np.divide(dist,total))<br/>            .collect())<br/>        prob = np.reshape(prob,(len(local_data), RUNS))</span><span id="e5be" class="mx kt iq mt b gy nd mz l na nb">        #K'th centroid for each run<br/>        data_id = [choice(prob[:,i]) <strong class="mt ir">for </strong>i <strong class="mt ir">in </strong>xrange(RUNS)]<br/>        sample = [local_data[i] <strong class="mt ir">for </strong>i <strong class="mt ir">in</strong> data_id]<br/>        centers[:, idx+1] = sample</span><span id="6e50" class="mx kt iq mt b gy nd mz l na nb">    <strong class="mt ir">return </strong>centers</span><span id="3ba6" class="mx kt iq mt b gy nd mz l na nb">    # The second element `dist` in the tuple below is the<br/>    # closest distance from each data point to the selected<br/>    # points in the initial set, where `dist[i]` is the<br/>    # closest distance to the points in the i-th initial set<br/>    data = (rdd<br/>            .map(<strong class="mt ir">lambda </strong>p: (p, [np.inf] * RUNS)) \<br/>            .cache())</span><span id="2271" class="mx kt iq mt b gy nd mz l na nb">    # Collect the feature vectors of all data points<br/>    # beforehand, might be useful in the following<br/>    # for-loop<br/>    local_data = (rdd<br/>                    .map(<strong class="mt ir">lambda </strong>(name, vec): vec)<br/>                    .collect())</span><span id="0f3c" class="mx kt iq mt b gy nd mz l na nb">    # Randomly select the first point for every run of<br/>    # k-means++, i.e. randomly select `RUNS` points<br/>    # and add it to the `centers` variable<br/>    sample = [local_data[k] <strong class="mt ir">for </strong>k <strong class="mt ir">in<br/>        </strong>np.random.randint(0, len(local_data), RUNS)]<br/>    centers[:, 0] = sample</span><span id="2730" class="mx kt iq mt b gy nd mz l na nb">    <strong class="mt ir">for </strong>idx <strong class="mt ir">in </strong>range(K - 1):<br/>        ########################################################<br/>        # In each iteration, you need to select one point for<br/>        # each set of initial points (so select `RUNS` points<br/>        # in total). For each data point x, let D_i(x) be the<br/>        # distance between x and the nearest center that has<br/>        # already been added to the i-th set. Choose a new<br/>        # data point for i-th set using a weighted probability<br/>        # where point x is chosen with probability proportional<br/>        # to D_i(x)^2 . Repeat each data point by 25 times<br/>        # (for each RUN) to get 12140x25<br/>        ########################################################</span><span id="ca6f" class="mx kt iq mt b gy nd mz l na nb">        #Update distance<br/>        data = (data<br/>                .map(<strong class="mt ir">lambda </strong>((name,vec),dist):<br/>                        ((name,vec),update_dist(vec,dist,idx)))<br/>                .cache())</span><span id="24cf" class="mx kt iq mt b gy nd mz l na nb">        #Calculate sum of D_i(x)^2<br/>        d1 = data.map(<strong class="mt ir">lambda </strong>((name,vec),dist): (1,dist))<br/>        d2 = d1.reduceByKey(lambda x,y: np.sum([x,y], axis=0))<br/>        total = d2.collect()[0][1]</span><span id="4a96" class="mx kt iq mt b gy nd mz l na nb">        #Normalize each distance to get the probabilities and  <br/>        # reshape to 12140x25<br/>        prob = (data<br/>            .map(<strong class="mt ir">lambda </strong>((name,vec),dist):<br/>                    np.divide(dist,total))<br/>            .collect())<br/>        prob = np.reshape(prob,(len(local_data), RUNS))</span><span id="8cc4" class="mx kt iq mt b gy nd mz l na nb">        #K'th centroid for each run<br/>        data_id = [choice(prob[:,i]) <strong class="mt ir">for </strong>i <strong class="mt ir">in</strong> xrange(RUNS)]<br/>        sample = [local_data[i] <strong class="mt ir">for </strong>i <strong class="mt ir">in </strong>data_id]<br/>        centers[:, idx+1] = sample</span><span id="34f0" class="mx kt iq mt b gy nd mz l na nb">    <strong class="mt ir">return </strong>centers</span></pre><h1 id="c7e3" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">K-Means++实现</h1><p id="a9ff" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">现在我们有了初始化函数，我们可以用它来实现 K-Means++算法。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="63bc" class="mx kt iq mt b gy my mz l na nb"><strong class="mt ir">def</strong> get_closest(p, centers):<br/>    <em class="nc">'''</em><br/><em class="nc">    Return the indices the nearest centroids of `p`.</em><br/><em class="nc">    `centers` contains sets of centroids, where `centers[i]` is</em><br/><em class="nc">    the i-th set of centroids.</em><br/><em class="nc">    '''</em><br/>    best = [0] * len(centers)<br/>    closest = [np.inf] * len(centers)<br/>    <strong class="mt ir">for</strong> idx <strong class="mt ir">in</strong> range(len(centers)):<br/>        <strong class="mt ir">for</strong> j <strong class="mt ir">in</strong> range(len(centers[0])):<br/>            temp_dist = norm(p - centers[idx][j])<br/>            <strong class="mt ir">if</strong> temp_dist &lt; closest[idx]:<br/>                closest[idx] = temp_dist<br/>                best[idx] = j<br/>    <strong class="mt ir">return</strong> best<br/><br/><br/><strong class="mt ir">def</strong> kmeans(rdd, K, RUNS, converge_dist, seed):<br/>    <em class="nc">'''</em><br/><em class="nc">    Run K-means++ algorithm on `rdd`, where `RUNS` is the number of</em><br/><em class="nc">    initial sets to use.</em><br/><em class="nc">    '''</em><br/>    k_points = kmeans_init(rdd, K, RUNS, seed)<br/>    print_log("Initialized.")<br/>    temp_dist = 1.0<br/><br/>    iters = 0<br/>    st = time.time()</span><span id="7e92" class="mx kt iq mt b gy nd mz l na nb">    <strong class="mt ir">while</strong> temp_dist &gt; converge_dist: <br/>        <em class="nc"># Update all `RUNS` sets of centroids using standard k-means <br/>        # algorithm</em><br/>        <em class="nc"># Outline:</em><br/>        <em class="nc">#   - For each point x, select its nearest centroid in i-th <br/>        # centroids set</em><br/>        <em class="nc">#   - Average all points that are assigned to the same <br/>        # centroid</em><br/>        <em class="nc">#   - Update the centroid with the average of all points <br/>        # that are assigned to it</em><br/>        temp_dist = np.max([<br/>                np.sum([norm(k_points[idx][j] - new_points[(idx, <br/>                    j)]) <strong class="mt ir">for</strong> idx,j <strong class="mt ir">in</strong> new_points.keys()])<br/>                            ])<br/><br/>        iters = iters + 1<br/>        <strong class="mt ir">if</strong> iters % 5 == 0:<br/>            print_log("Iteration <strong class="mt ir">%d</strong> max shift: <strong class="mt ir">%.2f</strong> (time: <strong class="mt ir">%.2f</strong>)" %<br/>                      (iters, temp_dist, time.time() - st))<br/>            st = time.time()<br/><br/>        <em class="nc"># update old centroids</em><br/>        <em class="nc"># You modify this for-loop to meet your need</em><br/>        <strong class="mt ir">for</strong> ((idx, j), p) <strong class="mt ir">in</strong> new_points.items():<br/>            k_points[idx][j] = p<br/><br/>    <strong class="mt ir">return</strong> k_points</span></pre><h1 id="42e4" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">基准测试</h1><p id="807a" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">K-Means++优于 K-Means 之处在于它的初始化算法带来的收敛速度。此外，Spark 被用来尽可能地并行化这个算法。因此，让我们对这个实现进行基准测试。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="79d0" class="mx kt iq mt b gy my mz l na nb">st = time.time()<br/><br/>np.random.seed(RANDOM_SEED)<br/>centroids = kmeans(rdd, K, RUNS, converge_dist,  <br/>                   np.random.randint(1000))<br/>group = rdd.mapValues(<strong class="mt ir">lambda</strong> p: get_closest(p, centroids)) \<br/>           .collect()<br/><br/><strong class="mt ir">print</strong> "Time takes to converge:", time.time() - st</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/acd077fa75dc06636b86ec78cd4bb148.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*_fsYyTfRtAiPd-mSebisOQ.png"/></div></figure><p id="0413" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">根据处理器内核的数量、为每个执行器设置的内核内存以及使用的执行器数量，这个结果会有所不同。</p><h1 id="93cc" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">价值函数</h1><p id="44df" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">为了验证模型的准确性，我们需要选择一个成本函数，并尝试使用该模型将其最小化。最终的成本函数会给我们一个准确性的概念。对于 K-Means，我们查看数据点和最近的质心之间的距离。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="deaf" class="mx kt iq mt b gy my mz l na nb"><strong class="mt ir">def</strong> get_cost(rdd, centers):<br/>    <em class="nc">'''</em><br/><em class="nc">    Compute the square of l2 norm from each data point in `rdd`</em><br/><em class="nc">    to the centroids in `centers`</em><br/><em class="nc">    '''</em><br/>    <strong class="mt ir">def</strong> _get_cost(p, centers):<br/>        best = [0] * len(centers)<br/>        closest = [np.inf] * len(centers)<br/>        <strong class="mt ir">for</strong> idx <strong class="mt ir">in</strong> range(len(centers)):<br/>            <strong class="mt ir">for</strong> j <strong class="mt ir">in</strong> range(len(centers[0])):<br/>                temp_dist = norm(p - centers[idx][j])<br/>                <strong class="mt ir">if</strong> temp_dist &lt; closest[idx]:<br/>                    closest[idx] = temp_dist<br/>                    best[idx] = j<br/>        <strong class="mt ir">return</strong> np.array(closest)**2<br/>    <br/>    cost = rdd.map(<strong class="mt ir">lambda</strong> (name, v): _get_cost(v, <br/>           centroids)).collect()<br/>    <strong class="mt ir">return</strong> np.array(cost).sum(axis=0)<br/><br/>cost = get_cost(rdd, centroids)<br/>log2 = np.log2</span><span id="7123" class="mx kt iq mt b gy nd mz l na nb"><strong class="mt ir">print</strong> "Min Cost:\t"+str(log2(np.max(cost)))<br/><strong class="mt ir">print</strong> "Max Cost:\t"+str(log2(np.min(cost)))<br/><strong class="mt ir">print</strong> "Mean Cost:\t"+str(log2(np.mean(cost)))</span></pre><p id="c43a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">最小成本:33.7575332525 <br/>最大成本:33.8254902123 <br/>平均成本:33.2533332335</p><h1 id="0afe" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">决赛成绩</h1><p id="2c2a" class="pw-post-body-paragraph ju jv iq jw b jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr ij bi translated">以下是最终结果:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="2b63" class="mx kt iq mt b gy my mz l na nb"><strong class="mt ir">print</strong> 'entropy=',entropy</span><span id="5fc1" class="mx kt iq mt b gy nd mz l na nb">best = np.argmin(cost)<br/><strong class="mt ir">print</strong> 'best_centers=',list(centroids[best])</span></pre><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ng"><img src="../Images/eefc2c63e4e1b48865985d8b28455424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kfo_Nl9U4o-42PlFYlnpig.png"/></div></div></figure></div></div>    
</body>
</html>