<html>
<head>
<title>A One-Stop Shop for Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析的一站式商店</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c?source=collection_archive---------0-----------------------#2017-04-17">https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c?source=collection_archive---------0-----------------------#2017-04-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="6b4d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我为研究生统计理论课程使用的<a class="ae kl" href="http://people.unica.it/musio/files/2008/10/Casella-Berger.pdf" rel="noopener ugc nofollow" target="_blank">教材的开头，作者(乔治·卡塞拉和罗杰·伯杰)在序言中解释了他们为什么选择编写一本教材:</a></p><blockquote class="km kn ko"><p id="20bb" class="jn jo kp jp b jq jr js jt ju jv jw jx kq jz ka kb kr kd ke kf ks kh ki kj kk ij bi translated">W <!-- -->当有人发现你在写教科书时，他们会问你两个问题中的一个或两个。第一个是“你为什么要写书？第二个问题是“你的书和外面的有什么不同？“第一个问题相当容易回答。你正在写一本书，因为你对现有的文本不完全满意。”</p></blockquote><p id="34c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我在这里应用作者的逻辑。主成分分析(PCA)是统计学和数据科学领域中需要理解的一项重要技术……但是，当我为我的<a class="ae kl" href="https://generalassemb.ly/" rel="noopener ugc nofollow" target="_blank">大会</a>的学生们组织课程时，我发现网上的资源过于专业，没有完全满足我们的需求，并且/或者提供了相互矛盾的信息。可以肯定地说，我对现有的文本并不完全满意。</p><p id="8956" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我想把PCA的“什么”、“何时”、“如何”和“为什么”以及一些有助于进一步解释这个主题的资源链接放在一起。具体来说，我想介绍这种方法的基本原理、幕后的数学、一些最佳实践以及这种方法的潜在缺点。</p><p id="975e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然我想让PCA尽可能容易理解，但是我们将要讨论的算法是非常技术性的。熟悉以下部分或全部内容会使本文和PCA作为一种方法更容易理解:<a class="ae kl" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="noopener ugc nofollow" target="_blank">矩阵运算/线性代数</a>(矩阵乘法、矩阵转置、矩阵求逆、矩阵分解、特征向量/特征值)和统计学/机器学习(标准化、方差、协方差、独立性、线性回归、特征选择)。我在整篇文章中嵌入了这些主题的插图的链接，但希望这些是一个提醒，而不是通读文章的必读内容。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><h1 id="7347" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">什么是PCA？</strong></h1><p id="474e" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">假设你想预测美国2017年的国内生产总值(GDP)是多少。你有很多可用的信息:2017年第一季度的美国GDP，2016年全年的美国GDP，2015年等等。你有任何公开的经济指标，如失业率、通货膨胀率等等。你有2010年的美国人口普查数据，估计每个行业中有多少美国人工作，以及在每次人口普查之间更新这些估计的<a class="ae kl" href="https://www.census.gov/programs-surveys/acs/about.html" rel="noopener ugc nofollow" target="_blank">美国社区调查</a>数据。你知道每个政党有多少名众议员和参议员。你可以收集股票价格数据，一年中发生的<a class="ae kl" href="https://en.wikipedia.org/wiki/Initial_public_offering" rel="noopener ugc nofollow" target="_blank">首次公开募股</a>的数量，以及<a class="ae kl" href="https://www.nytimes.com/2017/03/09/business/bloomberg-iger-business-executives-president.html?_r=0" rel="noopener ugc nofollow" target="_blank">有多少CEO</a><a class="ae kl" href="http://www.latimes.com/business/technology/la-fi-tn-zuckerberg-president-20170120-story.html" rel="noopener ugc nofollow" target="_blank">似乎正在准备竞选公职</a>。尽管有大量的变量需要考虑，但这个<em class="kp">只是触及了表面</em>。</p><p id="7b61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="http://www.urbandictionary.com/define.php?term=tl%3Bdr" rel="noopener ugc nofollow" target="_blank">TL；DR</a>——你有<em class="kp">很多</em>变量要考虑。</p><p id="8d87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你以前处理过很多变量，你就会知道这会带来问题。你了解每个变量之间的关系吗？您是否有太多的变量，以至于您的模型可能会过度适应您的数据，或者您可能会违反您正在使用的任何建模策略的假设？</p><p id="119d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可能会问这样一个问题，“我如何把我收集的所有变量集中在其中的几个上？”用专业术语来说，你想要“减少你的特征空间的维数”通过减少特征空间的维度，您需要考虑的变量之间的关系更少，并且您不太可能过度拟合您的模型。(注意:这并不立即意味着过度拟合等。不再令人担忧，但我们正朝着正确的方向前进！)</p><p id="b1d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不出所料，<strong class="jp ir"> <em class="kp">降维</em> </strong>特征空间的<strong class="jp ir"> <em class="kp">维度</em> </strong>称为<strong class="jp ir"> <em class="kp">降维</em> </strong>。”有许多方法可以实现降维，但这些技术大多属于以下两类之一:</p><ul class=""><li id="c484" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">特征消除</li><li id="d943" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">特征抽出</li></ul><p id="2c68" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">特征消除</strong>顾名思义:我们通过消除特征来减少特征空间。在上面的GDP例子中，除了我们认为最能预测美国国内生产总值的三个变量之外，我们可能会放弃所有变量，而不是考虑每一个变量。特征消除方法的优点包括简单性和维护变量的可解释性。</p><p id="a046" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，作为一个缺点，你不能从你丢弃的变量中获得任何信息。如果我们只使用去年的国内生产总值、根据最新的美国社区调查数据，制造业就业人口的比例以及失业率来预测今年的国内生产总值，我们就错过了任何可能对我们的模型有所贡献的变量。通过消除功能，我们也完全消除了这些变量可能带来的任何好处。</p><p id="1fc0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">特征提取</strong>，不过，不会碰到这个问题。假设我们有十个独立变量。在特征提取中，我们创建十个“新”独立变量，其中每个“新”独立变量是十个“旧”独立变量的组合。然而，我们以特定的方式创建这些新的自变量，并根据它们预测因变量的程度对这些新变量进行排序。</p><p id="8b34" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可能会说，“降维在哪里发挥作用？”好吧，我们保留尽可能多的新的独立变量，但我们放弃了“最不重要的”因为我们根据新变量对因变量的预测程度对它们进行了排序，所以我们知道哪个变量最重要，哪个最不重要。但是——这就是问题所在——因为这些新的独立变量是旧变量的组合，所以我们仍然保留旧变量中最有价值的部分，即使我们放弃了一个或多个“新”变量！</p><p id="13b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">主成分分析是一种用于<em class="kp">特征提取</em>的技术——因此它以一种特定的方式组合我们的输入变量，然后我们可以丢弃“最不重要”的变量，同时仍然保留所有变量中最有价值的部分！<em class="kp">作为一个额外的好处，PCA后的每个“新”变量都是相互独立的。</em>这是一个好处，因为线性模型的<a class="ae kl" href="http://people.duke.edu/~rnau/testing.htm" rel="noopener ugc nofollow" target="_blank">假设要求我们的独立变量彼此独立。如果我们决定用这些“新”变量拟合一个线性回归模型(见下面的“主成分回归”)，这个假设必然会得到满足。</a></p><h1 id="f640" class="la lb iq bd lc ld mr lf lg lh ms lj lk ll mt ln lo lp mu lr ls lt mv lv lw lx bi translated">什么时候应该使用PCA？</h1><ol class=""><li id="afab" class="md me iq jp b jq ly ju lz jy mw kc mx kg my kk mz mj mk ml bi translated">您是否希望减少变量的数量，但却无法确定可以完全不考虑的变量？</li><li id="7220" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mz mj mk ml bi translated">你想确保你的变量是相互独立的吗？</li><li id="0cc1" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mz mj mk ml bi translated">你愿意让你的自变量变得更难解释吗？</li></ol><p id="020f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你对这三个问题的回答都是肯定的，那么PCA是一个很好的方法。如果你对问题3的回答是“否”,你<strong class="jp ir">不应该</strong>使用五氯苯甲醚。</p><h1 id="5d68" class="la lb iq bd lc ld mr lf lg lh ms lj lk ll mt ln lo lp mu lr ls lt mv lv lw lx bi translated">PCA是如何工作的？</h1><p id="b814" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">之后的部分讨论了<em class="kp">为什么</em> PCA有效，但是在进入算法之前提供一个简短的总结可能对上下文有帮助:</p><ul class=""><li id="e4bb" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">我们要计算一个矩阵，这个矩阵总结了变量之间的关系。</li><li id="1a9c" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">然后我们将这个矩阵分解成两个独立的部分:方向和大小。然后，我们可以了解数据的“方向”及其“大小”(或者每个方向有多“重要”)。下面的截图，<a class="ae kl" href="http://setosa.io/ev/principal-component-analysis/" rel="noopener ugc nofollow" target="_blank">来自setosa.io applet </a>，显示了该数据中的两个主要方向:“红色方向”和“绿色方向”在这种情况下，“红色方向”是更重要的一个。我们稍后会讨论为什么会出现这种情况，但是考虑到这些点是如何排列的，你能看出为什么“红色方向”看起来比“绿色方向”更重要吗？(<em class="kp">提示:拟合该数据的最佳拟合线会是什么样子？</em>)</li></ul><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/aca3d27a505d5990e8adc2408a81f904.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*P8_C9uk3ewpRDtevf9wVxg.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Our original data in the xy-plane. (<a class="ae kl" href="http://setosa.io/ev/principal-component-analysis/" rel="noopener ugc nofollow" target="_blank">Source</a>.)</figcaption></figure><ul class=""><li id="5166" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">我们将转换我们的原始数据，以符合这些重要的方向(这是我们的原始变量的组合)。下面的截图(<a class="ae kl" href="http://setosa.io/ev/principal-component-analysis/" rel="noopener ugc nofollow" target="_blank">同样来自setosa.io </a>)是与上面相同的精确数据，但是进行了转换，使得<em class="kp"> x </em> -和<em class="kp">y</em>-轴现在是“红色方向”和“绿色方向”。这里的最佳拟合线是什么样的？</li></ul><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/239853119bd7d93da384eef4132644dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*wsezmnzg-0N_RP3meYNXlQ.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Our original data transformed by PCA. (<a class="ae kl" href="http://setosa.io/ev/principal-component-analysis/" rel="noopener ugc nofollow" target="_blank">Source</a>.)</figcaption></figure><ul class=""><li id="082a" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">虽然这里的可视化示例是二维的(因此我们有两个“方向”)，但请考虑我们的数据有更多维的情况。通过识别哪些“方向”是最“重要”的，我们可以通过丢弃“最不重要”的“方向”来压缩或投影我们的数据到一个更小的空间<strong class="jp ir">通过将我们的数据投影到一个更小的空间，我们减少了特征空间的维度…但是因为我们已经在这些不同的“方向”上转换了我们的数据，我们已经确保在我们的模型中保留所有原始变量！</strong></li></ul><p id="6290" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，我通过一个算法进行主成分分析。我尽量避免太专业，但是不可能忽略这里的细节，所以我的目标是尽可能清晰地介绍事情。在下一节中，我们将更深入地了解该算法为什么会起作用。</p><p id="d2fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在开始之前，您应该用<em class="kp"> n </em>行和可能的<em class="kp"> p+1 </em>列来组织表格数据，其中一列对应于您的因变量(通常表示为<strong class="jp ir"> <em class="kp"> Y </em> </strong>)和<em class="kp"> p </em>列，其中每一列对应于一个自变量(其矩阵通常表示为<strong class="jp ir"> <em class="kp"> X </em> </strong>)。</p><ol class=""><li id="f625" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mz mj mk ml bi translated">如果一个<strong class="jp ir"> <em class="kp"> Y </em> </strong>变量存在，并且是你的数据的一部分，那么把你的数据分成<strong class="jp ir"><em class="kp"/></strong>和<strong class="jp ir"> <em class="kp"> X </em> </strong>，如上定义——我们将主要使用<strong class="jp ir"> <em class="kp"> X </em> </strong>。(注意:如果没有<strong class="jp ir"> Y </strong>的列，没关系——跳到下一点！)</li><li id="f139" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mz mj mk ml bi translated">取独立变量矩阵<strong class="jp ir"> <em class="kp"> X </em> </strong>，对于每一列，从每一项中减去该列的平均值。(这确保了每一列的平均值为零。)</li><li id="3cbe" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mz mj mk ml bi translated">决定是否标准化。给定<strong class="jp ir"> <em class="kp"> X </em> </strong>的列，是方差较高的特征比方差较低的特征更重要，还是特征的重要性与方差无关？(在这种情况下，重要性意味着该特征预测<strong class="jp ir"> <em class="kp"> Y </em> </strong>的好坏。)<strong class="jp ir">如果特征的重要性独立于特征的方差，那么将一列中的每个观察值除以该列的标准差。</strong>(这与步骤2相结合，标准化<em class="kp"> X </em>的每一列，以确保每一列都有平均值0和标准偏差1。)调用居中的(也可能是标准化的)矩阵<strong class="jp ir"> <em class="kp"> Z </em> </strong>。</li><li id="4b7e" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mz mj mk ml bi translated">取矩阵<strong class="jp ir"> <em class="kp"> Z </em> </strong>，<a class="ae kl" href="https://chortle.ccsu.edu/VectorLessons/vmch13/vmch13_14.html" rel="noopener ugc nofollow" target="_blank">转置它</a>，将转置后的矩阵乘以<strong class="jp ir"> <em class="kp"> Z </em> </strong>。(从数学上来说，我们将把它写成<strong class="jp ir"><em class="kp">z</em></strong>ᵀ<strong class="jp ir"><em class="kp">z</em></strong>。)得到的矩阵就是<strong class="jp ir"> <em class="kp"> Z </em> </strong> 的<a class="ae kl" href="http://www.itl.nist.gov/div898/handbook/pmc/section5/pmc541.htm" rel="noopener ugc nofollow" target="_blank">协方差矩阵，最多一个常数。</a></li><li id="c039" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mz mj mk ml bi translated">(这可能是最难的一步——请继续关注我。)计算<strong class="jp ir"><em class="kp">z</em></strong>ᵀ<strong class="jp ir"><em class="kp">z</em></strong>的特征向量及其对应的特征值。这在大多数计算包中都很容易做到——事实上，我们将<strong class="jp ir"><em class="kp">z</em></strong>ᵀ<strong class="jp ir"><em class="kp">z</em></strong>的<a class="ae kl" href="https://cseweb.ucsd.edu/~dasgupta/291-unsup/lec7.pdf" rel="noopener ugc nofollow" target="_blank">特征分解<em class="kp">z</em>ᵀ<strong class="jp ir"><em class="kp">z</em></strong>分解为<strong class="jp ir"> <em class="kp"> PDP </em> </strong> ⁻，其中<strong class="jp ir"> <em class="kp"> P </em> </strong><strong class="jp ir"> <em class="kp"> D </em> </strong>对角线上的特征值会关联到<strong class="jp ir"> <em class="kp"> P </em> </strong>中对应的列——即<strong class="jp ir"> <em class="kp"> D </em> </strong>的第一个元素是λ₁，对应的特征向量是<strong class="jp ir"> <em class="kp"> P </em> </strong>的第一列。这适用于<strong class="jp ir"><em class="kp"/></strong>中的所有元素以及<strong class="jp ir"> <em class="kp"> P </em> </strong>中它们对应的特征向量。我们总是能够以这种方式计算出<strong class="jp ir"> <em class="kp"> PDP </em> </strong> ⁻。(额外收获:对于感兴趣的人，我们总是可以用这种方式计算<strong class="jp ir"> <em class="kp"> PDP </em> </strong> ⁻，因为<strong class="jp ir"><em class="kp">z</em></strong>ᵀ<strong class="jp ir"><em class="kp">z</em></strong>是一个</a><a class="ae kl" href="https://en.wikipedia.org/wiki/Symmetric_matrix" rel="noopener ugc nofollow" target="_blank">对称</a>，<a class="ae kl" href="https://en.wikipedia.org/wiki/Positive-definite_matrix" rel="noopener ugc nofollow" target="_blank">半正定矩阵</a>。)</li><li id="a2cd" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mz mj mk ml bi translated">取特征值λ₁，λ₂，…，λ <em class="kp"> p </em>，从大到小排序。这样做，相应地对<strong class="jp ir"> <em class="kp"> P </em> </strong>中的特征向量进行排序。(例如，如果λ₂是最大特征值，则取第二列<strong class="jp ir"> <em class="kp"> P </em> </strong>放在第一列位置。)取决于计算包，这可以自动完成。把这个特征向量排序后的矩阵叫做<strong class="jp ir"> <em class="kp"> P* </em> </strong>。(<strong class="jp ir"> <em class="kp"> P* </em> </strong>的列应该与<strong class="jp ir"> <em class="kp"> P </em> </strong>的列相同，但顺序可能不同。注意，这些特征向量是相互独立的。</li><li id="ad43" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mz mj mk ml bi translated">计算<strong class="jp ir"><em class="kp">Z *</em></strong>=<strong class="jp ir"><em class="kp">ZP *</em></strong>。这个新矩阵，<strong class="jp ir"><em class="kp">* Z *</em></strong>，是<strong class="jp ir"> <em class="kp"> X </em> </strong>的中心化/标准化版本，但是现在每个观察是原始变量的组合，其中权重由特征向量确定。<strong class="jp ir">另外，因为我们在<em class="kp"> P* </em>中的特征向量是彼此独立的，所以<em class="kp"> Z* </em>的每一列也是彼此独立的！</strong></li></ol><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/9e3c1f97353a1346b4d8b5f8a624e553.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3JWBvxB92Uo116Bpxa3Tw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">An example from <a class="ae kl" href="http://setosa.io/ev/principal-component-analysis/" rel="noopener ugc nofollow" target="_blank">setosa.io</a> where we transform five data points using PCA. The left graph is our original data <strong class="bd ns"><em class="nt">X</em></strong><em class="nt">; the right graph would be our transformed data </em><strong class="bd ns"><em class="nt">Z*</em></strong><em class="nt">.</em></figcaption></figure><p id="400b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意此图中的两点:</p><ul class=""><li id="fedf" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">这两个图表显示了完全相同的数据，但右边的图表反映了原始数据的转换，因此我们的轴现在是主要组成部分。</li><li id="d624" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">在这两幅图中，主成分相互垂直。<strong class="jp ir">事实上，每一个主成分总是与每一个其他主成分</strong><a class="ae kl" href="https://en.wikipedia.org/wiki/Orthogonality#Statistics.2C_econometrics.2C_and_economics" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir"/></a><strong class="jp ir"/>(也就是官方的数学术语，表示垂直)<strong class="jp ir">。</strong>(不信我？<a class="ae kl" href="http://setosa.io/ev/principal-component-analysis/" rel="noopener ugc nofollow" target="_blank">尝试破解小程序</a>！)</li></ul><p id="d871" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">因为我们的主成分彼此正交，所以它们在统计上彼此线性无关…这就是为什么我们的<em class="kp"> Z* </em>列彼此线性无关！</strong></p><p id="067c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">8.最后，我们需要决定保留多少特性，放弃多少特性。有三种常见的方法来确定这一点，下面讨论并给出一个明确的示例:</p><ul class=""><li id="5c98" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated"><strong class="jp ir">方法一</strong>:我们任意选择想要保留多少维度。也许我想用二维来直观地表现事物，所以我可能只保留两个特征。这取决于用例，对于我应该选择多少特性没有硬性规定。</li><li id="8f79" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><strong class="jp ir">方法2 </strong>:计算每个特性的<a class="ae kl" href="https://stats.stackexchange.com/questions/22569/pca-and-proportion-of-variance-explained" rel="noopener ugc nofollow" target="_blank">方差比例</a>(下面简要说明)，选择一个阈值，并添加特性，直到达到该阈值。(例如，如果您想要解释您的模型可能解释的80%的总可变性，则添加具有最大解释方差比例的要素，直到您的解释方差比例达到或超过80%。)</li><li id="30ac" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><strong class="jp ir">方法三</strong>:这个和方法二密切相关。计算每个特征的<a class="ae kl" href="https://stats.stackexchange.com/questions/22569/pca-and-proportion-of-variance-explained" rel="noopener ugc nofollow" target="_blank">解释方差比例</a>，根据解释方差比例对特征进行排序，并绘制保留更多特征时解释方差的累积比例。(该图被称为<a class="ae kl" href="http://ba-finance-2013.blogspot.com/2012/09/scree-plots-interpretation-and.html" rel="noopener ugc nofollow" target="_blank">碎石图</a>，如下所示。)通过识别添加新特征相对于先前特征具有显著差异下降的点，并选择直到该点的特征，可以挑选包括多少特征。(我称之为“找到肘部”方法，因为查看碎石图中的“弯曲”或“肘部”可以确定所解释的方差比例中出现最大下降的位置。)</li></ul><p id="5157" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为每个特征值大致就是它对应的特征向量的重要性，所以解释的方差比例就是你保留的特征的特征值之和除以所有特征的特征值之和。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/3eee5538f950ffe2c71e2b1811a3fc43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*IVSFwjPQJgkFw1raX4l_jg.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Scree Plot for Genetic Data. (<a class="ae kl" href="http://www.improvedoutcomes.com/docs/WebSiteDocs/PCA/Creating_a_Scree_Plot.htm" rel="noopener ugc nofollow" target="_blank">Source</a>.)</figcaption></figure><p id="b88e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑一下这个基因数据的scree图。(来源:<a class="ae kl" href="http://www.improvedoutcomes.com/docs/WebSiteDocs/PCA/Creating_a_Scree_Plot.htm" rel="noopener ugc nofollow" target="_blank">此处</a>。)红线表示由每个特征解释的方差的比例，其计算方法是将该主成分的特征值除以所有特征值的总和。仅包含主成分1解释的方差比例为λ₁/(λ₁ + λ₂ + … + λ <em class="kp"> p </em>)，约为23%。仅包含主成分2解释的方差比例为λ₂/(λ₁ + λ₂ + … + λ <em class="kp"> p </em>)，约为19%。</p><p id="4961" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同时包含主成分1和主成分2解释的方差比例为(λ₁ + λ₂)/(λ₁ + λ₂ + … + λ <em class="kp"> p </em>)，约为42%。这就是黄线出现的地方；黄线表示方差的累积比例，如果您包括到该点为止的所有主成分。例如，PC2上方的黄点表示包含主成分1和2将解释模型中总方差的约42%。</p><p id="7b3d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们来看一些例子:</p><ul class=""><li id="a0f2" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">方法1:我们任意选择一些主要成分。假设我想在我的模型中保留五个主要组件。在上面的遗传数据案例中，这五个主成分解释了通过包括所有13个主成分可以解释的总可变性的大约66%。</li><li id="130f" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">方法2:假设我想包含足够的主成分来解释全部13个主成分所解释的90%的总可变性。在上面的遗传数据案例中，我将包括前10个主成分，并从<strong class="jp ir"><em class="kp"/></strong>中去掉最后三个变量。</li><li id="4a36" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">方法3:在这里，我们要“找到肘部”在上面的碎石图中，我们看到主成分2和主成分3之间解释的可变性比例有很大下降。在这种情况下，我们可能会包括前两个特性，并放弃其余的特性。正如你所看到的，这种方法有点主观，因为“肘”没有数学上精确的定义，在这种情况下，我们将包括一个模型，它只能解释大约42%的总可变性。</li></ul><p id="7b1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kp">(注意:一些scree图将在Y轴上具有特征向量的大小，而不是方差的比例。这导致等效的结果，但是需要用户手动计算方差的比例。</em> <a class="ae kl" href="http://documentation.statsoft.com/STATISTICAHelp.aspx?path=Glossary/GlossaryTwo/S/ScreePlotScreeTest" rel="noopener ugc nofollow" target="_blank"> <em class="kp">这里可以看到</em> </a> <em class="kp">的例子。)</em></p><p id="46d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦我们删除了我们想要删除的转换变量，我们就完成了！那是PCA。</p><h1 id="0c67" class="la lb iq bd lc ld mr lf lg lh ms lj lk ll mt ln lo lp mu lr ls lt mv lv lw lx bi translated">但是，就像，为什么PCA会起作用？</h1><p id="d729" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">虽然PCA是一种非常技术性的方法，依赖于深入的线性代数算法，但当你考虑它时，它是一种相对直观的方法。</p><ul class=""><li id="1735" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">首先，协方差矩阵<strong class="jp ir"><em class="kp">z</em></strong>ᵀ<strong class="jp ir"><em class="kp">z</em></strong>是一个矩阵，包含对<strong class="jp ir"> <em class="kp"> Z </em> </strong>中的每个变量如何与<strong class="jp ir"> <em class="kp"> Z </em> </strong>中的每个其他变量相关的估计。理解一个变量如何与另一个变量相关联是非常重要的。</li><li id="0783" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">第二，特征值和特征向量很重要。特征向量代表方向。想象一下在多维散点图上绘制数据。那么你可以把一个单独的特征向量想象成你的散点图中的一个特定的“方向”。特征值代表数量或重要性。更大的特征值与更重要的方向相关。</li><li id="91ae" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">最后，我们假设特定方向上更多的可变性与解释因变量的行为相关。大量可变性通常表示信号，而少量可变性通常表示噪声。因此，在一个特定的方向上有越多的可变性，从理论上来说，表明我们想要检测一些重要的东西。(<a class="ae kl" href="http://setosa.io/ev/principal-component-analysis/" rel="noopener ugc nofollow" target="_blank"> setosa.io PCA applet </a>是处理数据并说服自己为什么有意义的好方法。)</li></ul><p id="60ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，PCA是一种集合了以下内容的方法:</p><ol class=""><li id="a8bd" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mz mj mk ml bi translated">衡量每个变量如何相互关联。(协方差矩阵。)</li><li id="0966" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mz mj mk ml bi translated">我们的数据分散的方向。(特征向量。)</li><li id="01f6" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mz mj mk ml bi translated">这些不同方向的相对重要性。(特征值。)</li></ol><p id="2ea6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">主成分分析结合了我们的预测，并允许我们放弃相对不重要的特征向量。</p><h1 id="18a1" class="la lb iq bd lc ld mr lf lg lh ms lj lk ll mt ln lo lp mu lr ls lt mv lv lw lx bi translated">PCA有扩展吗？</h1><p id="2a7e" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">是的，在有限的篇幅内我无法一一讲述。我最常看到的一个是<a class="ae kl" href="https://onlinecourses.science.psu.edu/stat857/node/157" rel="noopener ugc nofollow" target="_blank">主成分回归</a>，我们将未变换的<strong class="jp ir"> <em class="kp"> Y </em> </strong>回归到我们没有丢弃的<strong class="jp ir"><em class="kp"/></strong>的子集上。(这就是<strong class="jp ir"> <em class="kp"> Z* </em> </strong>列独立性的来源；通过<strong class="jp ir"> <em class="kp"> Y </em> </strong>对<strong class="jp ir"> <em class="kp"> Z* </em> </strong>的回归，我们知道自变量所要求的独立性必然会得到满足。然而，我们仍然需要检查我们的其他假设。)</p><p id="2df5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我见过的另一个常见变体是<a class="ae kl" href="https://en.wikipedia.org/wiki/Kernel_principal_component_analysis" rel="noopener ugc nofollow" target="_blank">内核PCA </a>。</p><h1 id="b26b" class="la lb iq bd lc ld mr lf lg lh ms lj lk ll mt ln lo lp mu lr ls lt mv lv lw lx bi translated">结论</h1><p id="9898" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">我希望这篇文章对你有帮助！查看下面的一些资源，以获得关于PCA的更深入的讨论。让我知道你的想法，尤其是如果有改进的建议。</p><p id="4540" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我听说这篇文章的中文翻译已经在这里提供了。(谢谢，<a class="nv nw ep" href="https://medium.com/u/c2d21d069ac?source=post_page-----5582fb7e0a9c--------------------------------" rel="noopener" target="_blank"> Jakukyo弗列尔</a>！)</p><p id="12b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我非常感谢我的朋友<a class="ae kl" href="https://medium.com/@mostlyinane" rel="noopener">里蒂卡·巴斯克尔</a>、<a class="ae kl" href="https://medium.com/@josephofiowa" rel="noopener">约瑟夫·尼尔森</a>和<a class="ae kl" href="https://www.linkedin.com/in/corey-smith-03203b78/" rel="noopener ugc nofollow" target="_blank">科里·史密斯</a>的建议和编辑。你应该看看媒体上的Ritika和Joseph他们的帖子比我的有趣多了。(科里太专注于不让自己的博士研究被抢先发表，以至于没有媒体报道。)</p><p id="abfc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我还想给<a class="ae kl" href="http://setosa.io/ev/principal-component-analysis/" rel="noopener ugc nofollow" target="_blank"> setosa.io小程序</a>一个<strong class="jp ir">巨大的</strong> h/t，为其直观形象的展示PCA。</p><p id="aa4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">编辑:感谢<a class="ae kl" href="https://www.linkedin.com/in/michael-j-matthews/" rel="noopener ugc nofollow" target="_blank">迈克尔·马修斯</a>注意到上面第7步中<strong class="jp ir"><em class="kp"/></strong>的公式中的一个错别字。他正确地指出了<strong class="jp ir"><em class="kp">z *</em></strong>=<strong class="jp ir"><em class="kp">ZP *</em></strong>，而不是<strong class="jp ir"><em class="kp">z</em></strong>ᵀ<strong class="jp ir"><em class="kp">p *</em></strong>。也感谢<a class="ae kl" href="https://medium.com/@chienlungcheung" rel="noopener"> Chienlung Cheung </a>注意到上面步骤8中的另一个错别字，并注意到我在一行中将“特征向量”和“特征值”混为一谈。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><h1 id="4439" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">你应该查看的资源:</strong></h1><p id="bebd" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">这是我用来编译这篇PCA文章的资源列表，也是我通常认为有助于理解PCA的其他资源。如果你知道有什么资源可以加入到这个列表中，请留下你的评论，我会添加进去的。</p><h2 id="1a3b" class="nx lb iq bd lc ny nz dn lg oa ob dp lk jy oc od lo kc oe of ls kg og oh lw oi bi translated">非学术文章和资源</h2><ul class=""><li id="4362" class="md me iq jp b jq ly ju lz jy mw kc mx kg my kk mi mj mk ml bi translated"><a class="ae kl" href="http://setosa.io/ev/principal-component-analysis/" rel="noopener ugc nofollow" target="_blank"> Setosa.io的PCA小程序</a>。(一个小程序，允许您可视化什么是主成分以及您的数据如何影响主成分。)</li><li id="07b7" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><a class="ae kl" href="http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf" rel="noopener ugc nofollow" target="_blank">对PCA算法和算法本身构建模块的半学术性演练</a>。</li><li id="0daa" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><a class="ae kl" href="http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues" rel="noopener ugc nofollow" target="_blank">这个StackExchange问题的顶级答案，一言以蔽之，杰出。</a></li><li id="ecde" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><a class="ae kl" href="https://stats.stackexchange.com/questions/200410/is-principal-component-analysis-a-parametric-method" rel="noopener ugc nofollow" target="_blank">讨论五氯苯甲醚是否有参数假设的交叉验证问答。</a>(剧透:PCA本身是非参数方法，但使用PCA后的回归或假设检验可能需要参数假设。)</li><li id="2b50" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">如果没有<a class="ae kl" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">维基百科的链接</a>，资源列表就很难完整，对吗？(尽管维基百科是唾手可得的果实，但它在页面底部有一个额外链接和资源的可靠列表。)</li></ul><h2 id="005b" class="nx lb iq bd lc ny nz dn lg oa ob dp lk jy oc od lo kc oe of ls kg og oh lw oi bi translated">编码资源</h2><ul class=""><li id="8def" class="md me iq jp b jq ly ju lz jy mw kc mx kg my kk mi mj mk ml bi translated"><a class="ae kl" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">sk learn库中PCA的Python文档</a>。(此链接包含例子！)</li><li id="2805" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><a class="ae kl" href="https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/" rel="noopener ugc nofollow" target="_blank"> PCA对AnalyticsVidhya的解释</a>。(此链接包含Python和r。)</li><li id="db59" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><a class="ae kl" href="http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html" rel="noopener ugc nofollow" target="_blank">用Python实现PCA</a>有几个很酷的情节。</li><li id="0668" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><a class="ae kl" href="http://www.sthda.com/english/wiki/principal-component-analysis-in-r-prcomp-vs-princomp-r-software-and-data-mining" rel="noopener ugc nofollow" target="_blank">R中实现PCA的方法比较</a>。</li></ul><h2 id="5455" class="nx lb iq bd lc ny nz dn lg oa ob dp lk jy oc od lo kc oe of ls kg og oh lw oi bi translated">学术教科书和文章</h2><ul class=""><li id="de5d" class="md me iq jp b jq ly ju lz jy mw kc mx kg my kk mi mj mk ml bi translated"><a class="ae kl" href="http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf" rel="noopener ugc nofollow" target="_blank">统计学习导论</a>，第6版，作者:詹姆斯、威滕、哈斯蒂和蒂布希拉尼。(第6.3、6.7和10.2章详细介绍了五氯苯甲醚。这本书假设了线性回归的知识，但总的来说还是很容易理解的。)</li><li id="db80" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><a class="ae kl" href="https://onlinecourses.science.psu.edu/stat505/node/49" rel="noopener ugc nofollow" target="_blank">宾州州立大学STAT 505 </a>(应用多元统计分析)课程笔记。(我发现宾夕法尼亚州立大学的在线统计课程笔记令人难以置信，这里的PCA部分特别有帮助。)</li><li id="e0aa" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><a class="ae kl" href="https://www.amazon.com/Linear-Algebra-Its-Applications-4th/dp/0321385179" rel="noopener ugc nofollow" target="_blank">线性代数及其应用</a>，第四版，作者David Lay。(第7.5章介绍了五氯苯甲醚。)</li><li id="530c" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">谷歌研究院的黄邦贤·施伦斯教授的主成分分析教程。</li><li id="53c7" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">卡耐基梅隆大学的科斯马·沙立兹撰写的关于主成分分析的章节草稿。</li><li id="4fa8" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">来自<a class="ae kl" href="http://appliedpredictivemodeling.com/" rel="noopener ugc nofollow" target="_blank">的</a> g应用预测模型中关于数据预处理的一章包括对主成分分析的介绍性讨论(带视觉效果！)第3.3节。(h/t to <a class="ae kl" href="https://www.linkedin.com/in/jaylucasprofile/" rel="noopener ugc nofollow" target="_blank"> Jay Lucas </a>求推荐！)</li><li id="c943" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><a class="ae kl" href="http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf" rel="noopener ugc nofollow" target="_blank">统计学习的要素</a>，第10版，作者Hastie、Tibshirani和Friedman。(第3.5、14.5和18.6章详细介绍了五氯苯甲醚。这本书假设了线性回归、矩阵代数和微积分的知识，比<em class="kp">统计学习介绍</em>更具技术性，但这两本书遵循相同作者的相似结构。)</li></ul><h2 id="b806" class="nx lb iq bd lc ny nz dn lg oa ob dp lk jy oc od lo kc oe of ls kg og oh lw oi bi translated">次要资源</h2><ul class=""><li id="33e7" class="md me iq jp b jq ly ju lz jy mw kc mx kg my kk mi mj mk ml bi translated"><a class="ae kl" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="noopener ugc nofollow" target="_blank">线性代数本质YouTube系列</a>(包括一个与PCA特别相关的<a class="ae kl" href="https://www.youtube.com/watch?v=PFDu9oVAE-g&amp;index=14&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&amp;t=584s" rel="noopener ugc nofollow" target="_blank">特征向量和特征值</a>的视频；h/t to <a class="ae kl" href="https://www.linkedin.com/in/timothykbook/" rel="noopener ugc nofollow" target="_blank"> Tim Book </a>让我意识到这个不可思议的资源。)</li></ul></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><p id="ecbe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Matt Brems是一名数据科学家，经营着数据科学咨询公司BetaVector。他还是<a class="ae kl" href="https://www.datarobot.com/" rel="noopener ugc nofollow" target="_blank"> DataRobot </a>数据科学产品&amp;战略的高级经理。他解决了计算机视觉、金融、教育、消费品和政治领域的问题。他获得了<a class="ae kl" href="https://generalassemb.ly/" rel="noopener ugc nofollow" target="_blank">大会</a>颁发的2019年度‘杰出教师’奖。他在俄亥俄州获得了统计学硕士学位。Brems是无国界统计组织的志愿者，目前在他们的执行委员会担任副主席。</p><p id="cf24" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以通过<a class="ae kl" href="mailto:matt@betavector.com" rel="noopener ugc nofollow" target="_blank">电子邮件</a>或<a class="ae kl" href="https://twitter.com/MatthewBrems" rel="noopener ugc nofollow" target="_blank">推特</a>联系他。</p></div></div>    
</body>
</html>