<html>
<head>
<title>Vectorized implementation of back-propagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">反向传播的矢量化实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/vectorized-implementation-of-back-propagation-c4ce1d3eb684?source=collection_archive---------8-----------------------#2018-06-01">https://towardsdatascience.com/vectorized-implementation-of-back-propagation-c4ce1d3eb684?source=collection_archive---------8-----------------------#2018-06-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jq jr js jt gh gi paragraph-image"><div class="ab gu cl ju"><img src="../Images/b11062d314ce1db3735cc4b5b4a4d79f.png" data-original-src="https://miro.medium.com/v2/format:webp/1*dYk7hRRFEk7NuinB75_5hQ.jpeg"/></div></figure><p id="d893" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在<a class="ae kv" href="https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e" rel="noopener">之前的帖子</a>中，我们解释了反向传播背后的基本原理以及神经网络是如何工作的。在本帖中，我们将解释如何利用优化的数学库来加速学习过程。</p><blockquote class="kw kx ky"><p id="9f91" class="jx jy kz jz b ka kb kc kd ke kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ku im bi translated">“向量化”(简化)是重写一个循环的过程，这样它不是处理数组的单个元素 N 次，而是同时处理数组的几个或所有的元素<strong class="jz iu">。</strong></p></blockquote><p id="16b6" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">让我们从一个数据集的例子开始，这个数据集在一个特定的城市出售了 1000 套房子。对于每栋房子，我们有 5 个信息:它的面积，房间数量，建造年份，支付的价格和代理费。目标是训练一个模型，根据前 3 个特征预测价格和代理费。</p><figure class="ld le lf lg gt jt gh gi paragraph-image"><div class="ab gu cl ju"><img src="../Images/c0f71c57e44ecb4b864a572e8a2c26d7.png" data-original-src="https://miro.medium.com/v2/format:webp/1*zMN6vYgOAMj7BEQ9lG-jAg.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Dataset example</figcaption></figure><p id="adcf" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">让我们考虑一个简单的线性前馈模型，具有 6 个权重(W11，W12，W13，W21，W22，W23)，其中:</p><ul class=""><li id="e80c" class="ll lm it jz b ka kb ke kf ki ln km lo kq lp ku lq lr ls lt bi translated">价格= W11。面积+ W12。NbRooms + W13。年</li><li id="e86f" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated">费用= W21。面积+ W22。NbRooms + W23。年</li></ul><p id="584b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><a class="ae kv" href="https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e" rel="noopener">正如前面更详细解释的那样</a>，机器学习的目标是找到这 6 个权重的哪个值最符合最接近数据集真实输出的模型输出。我们从随机初始化权重开始。然后，我们向前传播以计算预测价格和代理费。通过将结果与来自数据集的真实价格和费用进行比较，我们可以获得误差的梯度，以便稍后反向传播，并相应地更新权重。</p><p id="69e0" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">一个简单的实现如下所示:</p><p id="fc5b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><a class="ae kv" href="https://gist.github.com/assaad/ec33987c6273293af2f648572fa85a3d" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/Assad/EC 33987 c 6273293 af 2f 648572 fa 85 a3d</a></p><p id="fe3e" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">然而，数据集上的这种顺序 for 循环太慢，并且没有利用 CPU 和 GPU 中的现代并行性。</p><p id="fa9f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">为了获得高性能，我们需要将数据集转换成矩阵表示。如果我们采用基于列的表示，来自数据集的每个输入都被复制到矩阵中的一列。</p><ul class=""><li id="eaf1" class="ll lm it jz b ka kb ke kf ki ln km lo kq lp ku lq lr ls lt bi translated">我们的权重矩阵将是一个 2 行 x 3 列的矩阵。</li><li id="0b2a" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated">我们的输入矩阵将是一个 3 行 x 1000 列的矩阵。</li><li id="9caa" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated">我们的输出矩阵将是一个 2 行 x 1000 列的矩阵。</li></ul><p id="75be" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们正在寻求解决的线性模型可以用以下基于矩阵的形式表示:</p><figure class="ld le lf lg gt jt gh gi paragraph-image"><div class="ab gu cl ju"><img src="../Images/8fdcd517201ecd28e37e9e38ff2e506d.png" data-original-src="https://miro.medium.com/v2/format:webp/1*8ERWyg3k_X2ugwMrNn2P2A.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Matrix or a vectorized-form of: Weights x Inputs = Outputs</figcaption></figure><p id="e658" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这种表示起作用的原因是因为这正是矩阵乘法的运行方式:</p><figure class="ld le lf lg gt jt gh gi paragraph-image"><div class="ab gu cl ju"><img src="../Images/eb3e6b32a5511f08fccb923f549dce9c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*N_Z3vPtuwiJQVBq_z3vh7Q.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Matrix multiplication</figcaption></figure><figure class="ld le lf lg gt jt gh gi paragraph-image"><div class="ab gu cl ju"><img src="../Images/2825b8970a64b337b69a31ba684a7d18.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Q0pWKoJiksT873wvK5FRCw.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Matrix-multiplication: a row i of the first matrix is multiplied by a column j of the second matrix to calculate the value of the cell (i , j) of the output</figcaption></figure><p id="3678" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">有了矢量化实现，以前需要 1000 次迭代的 for 循环现在只需很少的高性能矢量化运算即可完成，如下所示:</p><pre class="ld le lf lg gt lz ma mb mc aw md bi"><span id="2ad3" class="me mf it ma b gy mg mh l mi mj">Predictions = Matrix.Multiply(Weights, Inputs)<br/>Error = Matrix.Substract(Predictions, Outputs)</span></pre><p id="5e44" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在大数据集上，使用 GPU(有些有 1000 个内核)，我们可以期待在<strong class="jz iu">数千倍的加速！</strong>在 CPU 上，有很多实现高性能矩阵运算的高级数学库，比如<a class="ae kv" href="https://www.openblas.net/" rel="noopener ugc nofollow" target="_blank"> openBLAS </a>。</p><p id="3665" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">向量化前向传播简单明了，它遵循模型定义。挑战在于误差反向传播的矢量化。</p><ul class=""><li id="26a8" class="ll lm it jz b ka kb ke kf ki ln km lo kq lp ku lq lr ls lt bi translated">有了数字，如果我们通过一个函数<strong class="jz iu"> f </strong>传递一个数字<strong class="jz iu"> x </strong>得到<strong class="jz iu">y = f(x)</strong><strong class="jz iu">f</strong>的导数<strong class="jz iu">f’</strong>给我们 y 的<strong class="jz iu">变化率，当<strong class="jz iu"> x 变化</strong>。</strong></li><li id="5cb6" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated">对于矩阵，我们需要使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant" rel="noopener ugc nofollow" target="_blank"> <strong class="jz iu">雅可比矩阵</strong> </a>，这是一个由关于输入矩阵的不同元素的偏导数组成的矩阵。</li></ul><p id="f0bd" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">背后的理由是<strong class="jz iu">固定输入矩阵<strong class="jz iu">中的所有元素</strong>，除了一个元素</strong>，其中我们向其添加一个小增量<strong class="jz iu"> 𝛿 </strong>，并查看输出矩阵中的哪些元素受到影响，影响到哪个比率，并将它们相加在一起。我们对输入矩阵的所有元素都这样做，我们在输入端得到它的梯度矩阵。因此它具有相同的形状(行数和列数)。</p><figure class="ld le lf lg gt jt gh gi paragraph-image"><div class="ab gu cl ju"><img src="../Images/b17120320b5db436e4461db66c107d03.png" data-original-src="https://miro.medium.com/v2/format:webp/1*U8N0UhIl2zEdewZw4Jk1iQ.png"/></div></figure><p id="a7ac" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">考虑下面的矩阵运算。RxP=S，(以及计算输出 S 的前 3 个元素的前 3 个等式)</p><p id="1b93" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">等式 1: s11 = r11.p11 + r12.p21 + r13.p31(红色输出)</p><p id="1320" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">等式 2: s12 = r11.p12 + r12.p22 + r13.p32(绿色输出)</p><p id="5920" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">等式 3: s21 = r21.p11 + r22.p21 + r23.p31(黄色输出)</p><p id="4abf" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">假设我们在输出<strong class="jz iu"> S </strong>处已经有了梯度矩阵<strong class="jz iu">δS</strong>，我们想将其反向传播到输入<strong class="jz iu"> R </strong>(分别为<strong class="jz iu"> P </strong>)以计算<strong class="jz iu">δR</strong>(分别为。<strong class="jz iu">δP</strong>)。由于<strong class="jz iu"> r11 </strong>只参与<strong class="jz iu"> s11 </strong>和<strong class="jz iu"> s12 </strong>的计算(红绿不黄)，我们可以预计只有<strong class="jz iu"> 𝛿s11 </strong>和<strong class="jz iu"> 𝛿s12 </strong>反向传播到<strong class="jz iu"> 𝛿r11 </strong>。</p><ul class=""><li id="524c" class="ll lm it jz b ka kb ke kf ki ln km lo kq lp ku lq lr ls lt bi translated">为了找到<strong class="jz iu"> 𝛿s11 </strong>的反向传播的速率，我们相对于<strong class="jz iu"> r11 </strong>对等式 1 进行部分求导(并认为其他一切为常数)，我们得到<strong class="jz iu"> p11 </strong>的速率(另一种解释方式:在<strong class="jz iu"> r11 </strong>中的小变化，将被<strong class="jz iu"> s11 </strong>中的<strong class="jz iu"> p11 </strong>因子放大)。</li><li id="cf8f" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated">通过对<strong class="jz iu"> 𝛿s12 </strong>和等式 2 做同样的事情，我们得到<strong class="jz iu"> p12 </strong>的速率。</li><li id="4080" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated">如果我们试图将<strong class="jz iu"> 𝛿s13 </strong>反向传播到<strong class="jz iu"> r11 </strong>，并且相对于<strong class="jz iu"> r11 </strong>推导等式 3，我们得到 0，因为等式 3 根本不依赖于<strong class="jz iu"> r11。</strong>另一种理解方式:如果我们有一个错误或 s21，在 r11 上无法做任何事情来减少这个错误。由于 r11 不参与 s21 的计算！这同样适用于<strong class="jz iu"> S 矩阵</strong>的所有其他元素(s21、s22、s31、s32)。</li><li id="604a" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated">最后，加起来，我们得到<strong class="jz iu"> 𝛿r11=𝛿s11.p11 + 𝛿s12.p12 </strong></li></ul><p id="32ac" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">通过对矩阵 R 的所有元素做同样的处理，我们得到如下结果:</p><ul class=""><li id="37b9" class="ll lm it jz b ka kb ke kf ki ln km lo kq lp ku lq lr ls lt bi translated"><strong class="jz iu"> 𝛿r11=𝛿s11.p11 + 𝛿s12.p12 </strong></li><li id="dc62" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated"><strong class="jz iu"> 𝛿r12=𝛿s11.p21 + 𝛿s12.p22 </strong></li><li id="4a86" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated"><strong class="jz iu"> 𝛿r13=𝛿s11.p31 + 𝛿s12.p32 </strong></li><li id="e2ed" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated"><strong class="jz iu"> 𝛿r21=𝛿s21.p11 + 𝛿s22.p12 </strong></li><li id="178f" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated"><strong class="jz iu"> 𝛿r22=𝛿s21.p21 + 𝛿s22.p22 </strong></li><li id="a2ef" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated"><strong class="jz iu"> 𝛿r23=𝛿s21.p31 + 𝛿s22.p32 </strong></li><li id="ea80" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated"><strong class="jz iu"> 𝛿r31=𝛿s31.p11 + 𝛿s32.p12 </strong></li><li id="483c" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated"><strong class="jz iu"> 𝛿r32=𝛿s31.p21 + 𝛿s32.p22 </strong></li><li id="07c9" class="ll lm it jz b ka lu ke lv ki lw km lx kq ly ku lq lr ls lt bi translated"><strong class="jz iu"> 𝛿r33=𝛿s31.p31 + 𝛿s32.p32 </strong></li></ul><p id="1924" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">如果我们仔细观察该模式，我们会发现我们可以将其转化为矢量化矩阵乘法，如下所示:</p><pre class="ld le lf lg gt lz ma mb mc aw md bi"><span id="0deb" class="me mf it ma b gy mg mh l mi mj"><strong class="ma iu">ΔR = ΔS x Transpose(P)</strong></span></pre><figure class="ld le lf lg gt jt gh gi paragraph-image"><div class="ab gu cl ju"><img src="../Images/0ec39a5ac2073158c8e5bb863f681dfe.png" data-original-src="https://miro.medium.com/v2/format:webp/1*qxIIgxcEWYUGNpson1Bb7A.png"/></div></figure><p id="d22f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">类似地，如果我们按照相同的过程反向传播到 P，我们得到下面的等式:</p><pre class="ld le lf lg gt lz ma mb mc aw md bi"><span id="e7cc" class="me mf it ma b gy mg mh l mi mj"><strong class="ma iu">ΔP = Transpose(R) x ΔS</strong></span></pre><p id="21b3" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">每个神经网络层由几个数学运算组成。如果我们设法在向前和向后传递中用矩阵运算来定义每个数学运算，我们在学习中得到最大加速。</p><p id="bcf3" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在第一步中，每个矩阵<strong class="jz iu"> M </strong>必须被一个伴随矩阵<strong class="jz iu">δM</strong>增加，以在返回的路上保持其梯度。</p><p id="697f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">第二步，每个矩阵运算都必须定义自己的向前和向后运算。例如:</p><p id="64f0" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在创建了这个数学运算的矢量化库之后，我们可以使用这个库来链接运算，并创建层、激活函数、损失函数、优化器。反向传播将被自动定义为每层中使用的数学函数的回调栈(或计算图)。这就是<a class="ae kv" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>在一定程度上的工作原理。</p><p id="5bdf" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们可以将完整的机器学习过程视为一堆抽象的<strong class="jz iu">:</strong></p><figure class="ld le lf lg gt jt gh gi paragraph-image"><div class="ab gu cl ju"><img src="../Images/5abd5326224939f3367b0f965d01c34e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*DWtIOHJ8gyF0_5T6Z0z20g.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">An abstraction of machine learning library stack</figcaption></figure></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="813a" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="kz">原载于 2018 年 6 月 1 日</em><a class="ae kv" href="https://medium.com/datathings/vectorized-implementation-of-back-propagation-1011884df84" rel="noopener"><em class="kz">【medium.com】</em></a><em class="kz">。</em></p></div></div>    
</body>
</html>