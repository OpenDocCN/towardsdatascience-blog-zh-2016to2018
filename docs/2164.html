<html>
<head>
<title>Training and Visualising Word Vectors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">训练和可视化单词向量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-and-visualising-word-vectors-2f946c6430f8?source=collection_archive---------0-----------------------#2017-12-29">https://towardsdatascience.com/training-and-visualising-word-vectors-2f946c6430f8?source=collection_archive---------0-----------------------#2017-12-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="f631" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">用数据做酷事！</em></p><p id="762d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本教程中，我想展示如何在tensorflow中实现skip gram模型，为您正在处理的任何文本生成单词向量，然后使用tensorboard将它们可视化。我发现这个练习非常有用，有助于我理解skip gram模型是如何工作的，以及在你将它们用于CNN或RNNs之前，感受一下这些向量捕捉到的关于你的文本的关系。</p><p id="5c7c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我在text8数据集上训练了一个skip gram模型，该数据集是英文维基百科文章的集合。我用Tensorboard来可视化嵌入。Tensorboard通过使用PCA选择3个主轴来投影数据，可以让您看到整个单词云。超级爽！你可以输入任何单词，它会显示它的邻居。你也可以分离出最接近它的101个点。</p><p id="74b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参见下面的剪辑。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/e8f9c9bd521c578d755c650a8ef53e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*_80rvfEBcGWDxNJE0rFJHA.gif"/></div></figure><p id="7fc6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以在我的<a class="ae ku" href="https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a> repo上找到完整的代码。</p><p id="406a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了可视化训练，我还查看了与一组随机单词最接近的预测单词。在第一次迭代中，最接近的预测字看起来非常任意，这是有意义的，因为所有的字向量都是随机初始化的</p><pre class="kn ko kp kq gt kv kw kx ky aw kz bi"><span id="4c1b" class="la lb iq kw b gy lc ld l le lf"><strong class="kw ir">Nearest to cost:</strong> sensationalism, adversity, ldp, durians, hennepin, expound, skylark, wolfowitz,<br/><br/><strong class="kw ir">Nearest to engine:</strong> vdash, alloys, fsb, seafaring, tundra, frot, arsenic, invalidate,<br/><br/><strong class="kw ir">Nearest to construction:</strong> dolphins, camels, quantifier, hellenes, accents, contemporary, colm, cyprian,<br/><br/><strong class="kw ir">Nearest to http: </strong>internally, chaffee, avoid, oilers, mystic, chappell, vascones, cruciger,</span></pre><p id="a398" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练结束时，该模型在寻找单词之间的关系方面变得更好。</p><pre class="kn ko kp kq gt kv kw kx ky aw kz bi"><span id="45f7" class="la lb iq kw b gy lc ld l le lf"><strong class="kw ir">Nearest to cost:</strong> expense, expensive, purchase, technologies, inconsistent, part, dollars, commercial,<br/><br/><strong class="kw ir">Nearest to engine:</strong> engines, combustion, piston, stroke, exhaust, cylinder, jet, thrust,<br/><br/><strong class="kw ir">Nearest to construction:</strong> completed, constructed, bridge, tourism, built, materials, building, designed,<br/><br/><strong class="kw ir">Nearest to http: </strong>www, htm, com, edu, html, org, php, ac,</span></pre><h2 id="7185" class="la lb iq bd lg lh li dn lj lk ll dp lm jy ln lo lp kc lq lr ls kg lt lu lv lw bi translated"><strong class="ak"> Word2Vec和Skip Gram型号</strong></h2><p id="1512" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">创建单词向量是这样的过程，即获取大量文本并为每个单词创建向量，使得在语料库中共享共同上下文的单词在向量空间中彼此非常接近。</p><p id="0a47" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些单词向量可以非常好地捕捉单词之间的上下文关系(例如，黑色、白色和红色的向量会很接近)，并且我们使用这些向量代替原始单词来执行NLP任务(如文本分类或新文本生成)会获得更好的性能。</p><p id="e38c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">生成这些词向量有两种主要的模型——连续词袋(CBOW)和跳格模型。CBOW模型试图预测给定上下文单词的中心单词，而skip gram模型试图预测给定中心单词的上下文单词。一个简单的例子是:</p><p id="3a4a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">CBOW:猫吃了_____。填空，在这种情况下，是“食物”。</p><p id="d570" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">跳过-gram: ___ ___ __食物。完成单词的上下文。在这种情况下，是“猫吃了”</p><p id="8550" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你对这两种方法的更详细的比较感兴趣，那么请看这个<a class="ae ku" href="https://iksinc.wordpress.com/tag/continuous-bag-of-words-cbow/" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="5a5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">各种各样的论文已经发现Skip gram模型可以产生更好的单词向量，所以我一直致力于实现它</p><h2 id="e9bd" class="la lb iq bd lg lh li dn lj lk ll dp lm jy ln lo lp kc lq lr ls kg lt lu lv lw bi translated"><strong class="ak">在Tensorflow中实现Skip Gram模型</strong></h2><p id="9bbf" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">这里我将列出构建模型的主要步骤。请看我的<a class="ae ku" href="https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上的详细实现</p><ol class=""><li id="1cb6" class="mc md iq jp b jq jr ju jv jy me kc mf kg mg kk mh mi mj mk bi translated">预处理数据</li></ol><p id="e12e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们首先清理我们的数据。删除所有标点符号、数字，将文本拆分成单个单词。因为程序处理整数比处理单词好得多，所以我们通过创建一个vocab to int字典将每个单词映射到一个int。代码如下。</p><pre class="kn ko kp kq gt kv kw kx ky aw kz bi"><span id="daf0" class="la lb iq kw b gy lc ld l le lf">counts = collections.Counter(words)<br/>vocab = sorted(counts, key=counts.get, reverse=True)<br/>vocab_to_int = {word: ii for ii, word in enumerate(vocab, 0)}</span></pre><p id="d40a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.二次抽样</p><p id="233e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">经常出现的单词，如“the”、“of”和“for ”,不会为附近的单词提供太多的上下文信息。如果我们丢弃其中一些，我们可以从数据中去除一些噪声，反过来得到更快的训练和更好的表示。这个过程被<a class="ae ku" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">米科洛夫</a>称为子采样。对于训练集中的每个单词，我们将根据其频率的倒数给出的概率将其丢弃。</p><p id="df7e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.创建输入和目标</p><p id="bb51" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">skip gram的输入是每个单词(编码为int ),目标是该窗口周围的单词。Mikolov等人发现，如果这个窗口的大小是可变的，并且更靠近中心单词的单词被更频繁地采样，那么性能会更好。</p><p id="6c25" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“由于距离较远的单词通常比距离较近的单词与当前单词的关系更小，因此我们在训练示例中通过从这些单词中抽取较少的样本来降低距离较远的单词的权重……如果我们选择窗口大小=5，则对于每个训练单词，我们将在1和窗口大小之间随机选择一个数字R，然后使用当前单词的历史中的R个单词和未来中的R个单词作为正确的标签。”</p><pre class="kn ko kp kq gt kv kw kx ky aw kz bi"><span id="5956" class="la lb iq kw b gy lc ld l le lf"> R = np.random.randint(1, window_size+1)<br/> start = idx — R if (idx — R) &gt; 0 else 0<br/> stop = idx + R<br/> target_words = set(words[start:idx] + words[idx+1:stop+1])</span></pre><p id="4e0e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4.构建模型</p><p id="731a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从<a class="ae ku" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" rel="noopener ugc nofollow" target="_blank"> Chris McCormick的博客</a>中，我们可以看到我们将要建立的网络的大致结构。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/040cad5110f53f9d8dd85abd013b157f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XBaqjqpnBIXtXLzWjkuLQQ.png"/></div></div></figure><p id="280b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将把像“蚂蚁”这样的输入单词表示为一个热向量。这个向量将有10，000个分量(我们词汇表中的每个单词一个分量)，我们将在对应于单词“ants”的位置放置一个“1”，在所有其他位置放置0。</p><p id="ca32" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">网络的输出是一个单一的向量(也有10，000个分量),它包含我们词汇表中的每个单词，随机选择的邻近单词是该词汇表单词的概率。</p><p id="4257" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在训练结束时，隐藏层将具有训练过的单词向量。隐藏层的大小对应于我们的向量的维数。在上面的例子中，每个单词都有一个长度为300的向量。</p><p id="201d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可能已经注意到，skip-gram神经网络包含大量的权重……对于我们有300个特征和10，000个单词的vocab的示例，隐藏层和输出层各有3M个权重！在大型数据集上进行这种训练是不允许的，因此word2vec的作者引入了一些调整来使训练可行。你可以在<a class="ae ku" href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" rel="noopener ugc nofollow" target="_blank">链接</a>中了解更多信息。<a class="ae ku" href="https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上的代码实现这些来加速训练。</p><p id="5930" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">5.使用Tensorboard可视化</p><p id="0d81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可以使用Tensorboard中的嵌入投影仪来可视化嵌入。为此，您需要做几件事:</p><ul class=""><li id="0a9d" class="mc md iq jp b jq jr ju jv jy me kc mf kg mg kk mq mi mj mk bi translated">在培训结束时，将您的模型保存在检查点目录中</li><li id="52b5" class="mc md iq jp b jq mr ju ms jy mt kc mu kg mv kk mq mi mj mk bi translated">创建一个metadata.tsv文件，将每个int映射回word，以便Tensorboard显示单词而不是int。将这个tsv文件保存在同一个检查点目录中</li><li id="6308" class="mc md iq jp b jq mr ju ms jy mt kc mu kg mv kk mq mi mj mk bi translated">运行以下代码:</li></ul><pre class="kn ko kp kq gt kv kw kx ky aw kz bi"><span id="c170" class="la lb iq kw b gy lc ld l le lf">from tensorflow.contrib.tensorboard.plugins import projector<br/>summary_writer = tf.summary.FileWriter(‘checkpoints’, sess.graph)<br/>config = projector.ProjectorConfig()<br/>embedding_conf = config.embeddings.add()<br/># embedding_conf.tensor_name = ‘embedding:0’<br/>embedding_conf.metadata_path = os.path.join(‘checkpoints’, ‘metadata.tsv’)<br/>projector.visualize_embeddings(summary_writer, config)</span></pre><ul class=""><li id="e229" class="mc md iq jp b jq jr ju jv jy me kc mf kg mg kk mq mi mj mk bi translated">通过将tensorboard指向检查点目录来打开它</li></ul><p id="f4f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">就是这样！</p><p id="c431" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">给我一个❤️，如果你喜欢这个职位:)希望你拉代码，并尝试自己。如果你对这个话题有其他想法，请在这篇文章上发表评论，或者给我发邮件，地址是priya.toronto3@gmail.com</p><p id="5012" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">其他著述</strong>:<a class="ae ku" href="http://deeplearninganalytics.org/blog" rel="noopener ugc nofollow" target="_blank">http://deeplearninganalytics.org/blog</a></p><p id="4b32" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PS:我有自己的深度学习咨询公司，喜欢研究有趣的问题。我已经帮助几家初创公司部署了基于人工智能的创新解决方案。请到http://deeplearninganalytics.org/<a class="ae ku" href="http://deeplearninganalytics.org/" rel="noopener ugc nofollow" target="_blank">来看看我们。</a></p><p id="245c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你有一个我们可以合作的项目，请通过我的网站或priya.toronto3@gmail.com联系我</p><p id="38c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">参考文献:</strong></p><ul class=""><li id="7e76" class="mc md iq jp b jq jr ju jv jy me kc mf kg mg kk mq mi mj mk bi translated"><a class="ae ku" href="https://www.udacity.com/" rel="noopener ugc nofollow" target="_blank"> Udacity </a>深度学习纳米度</li></ul></div></div>    
</body>
</html>