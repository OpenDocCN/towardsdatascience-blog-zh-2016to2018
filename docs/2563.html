<html>
<head>
<title>Building Prediction APIs in Python (Part 3): Automated Testing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python构建预测API(第3部分):自动化测试</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-prediction-apis-in-python-part-3-automated-testing-a7cfa1fa7e9d?source=collection_archive---------1-----------------------#2018-02-07">https://towardsdatascience.com/building-prediction-apis-in-python-part-3-automated-testing-a7cfa1fa7e9d?source=collection_archive---------1-----------------------#2018-02-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/d89fe699a106d362b82188e3e15091f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GRbPfMeUGoFvc0z5ziPn1A.jpeg"/></div></div></figure><div class=""/><p id="13b1" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在上一篇文章中，我们改进了预测API的错误处理，并考虑了我们应该对哪些记录进行评分的微妙决定。在这篇文章中，我们将看看如何使用pytest测试我们的API。</p><div class="ip iq gp gr ir kx"><a href="https://medium.com/@chris.moradi/building-prediction-apis-in-python-part-1-series-introduction-basic-example-fe89e12ffbd3" rel="noopener follow" target="_blank"><div class="ky ab fo"><div class="kz ab la cl cj lb"><h2 class="bd jc gy z fp lc fr fs ld fu fw ja bi translated">用Python构建预测API(第1部分):系列介绍&amp;基本示例</h2><div class="le l"><h3 class="bd b gy z fp lc fr fs ld fu fw dk translated">好吧，你已经训练了一个模型，但是现在呢？如果没有人会使用，所有的工作都是没有意义的。在某些应用中…</h3></div><div class="lf l"><p class="bd b dl z fp lc fr fs ld fu fw dk translated">medium.com</p></div></div><div class="lg l"><div class="lh l li lj lk lg ll ix kx"/></div></div></a></div><p id="dc87" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">和往常一样，我们将使用Python 3，并且我将假设您要么正在使用Anaconda，要么已经设置了一个安装了这些包的环境:flask、scikit-learn和pytest。</p><p id="201a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="lm">注意:我将在这篇文章中浏览代码片段，但是在上下文中查看完整的文件可能会有所帮助。你可以在GitHub </em>  <em class="lm">上找到</em> <a class="ae kw" href="https://github.com/cmoradi/prediction-apis/tree/master/part03-testing/code" rel="noopener ugc nofollow" target="_blank"> <em class="lm">的完整例子。</em></a></p><h1 id="b04a" class="ln lo jb bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">测试案例</h1><p id="247a" class="pw-post-body-paragraph jy jz jb ka b kb ml kd ke kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv ij bi translated">虽然自动化测试是现代软件开发中的核心实践，但它尚未被许多数据科学家完全接受。简单地说，这是在主代码基础上运行测试的附加代码。测试框架，比如pytest，使得定义和执行一套测试变得很容易。随着新功能的实现或现有代码的重构，这些测试帮助开发人员确认现有功能没有被破坏或定位已经引入的错误。</p><p id="26f8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以下是一些不使用我的(尖刻的)回答编写测试的常见借口:</p><ul class=""><li id="8c7d" class="mq mr jb ka b kb kc kf kg kj ms kn mt kr mu kv mv mw mx my bi translated"><em class="lm">代码很少，所以不需要测试。</em>当您需要扩展当前功能时会发生什么？每个增强可能都很小，但是从长远来看，您可能会得到一个庞大的未经测试的代码库。如果目前代码很少，编写测试应该很容易，那么就去做吧！</li><li id="1ee0" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated"><em class="lm">当我写原始代码</em>时，我已经根据多个测试用例检查了每个功能。太好了！构建测试的一个挑战是提出好的测试用例，而你已经做到了。每当你做一个小的改变时，有能力运行所有那些测试用例不是很棒吗？</li><li id="9d15" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated"><em class="lm">我写代码不出错</em>。当然啦！你不需要测试用例来测试你的代码，但是当其他人加入到项目中并开始修改你漂亮、完美的代码时会发生什么呢？我们怎么知道他们是否打碎了什么东西？你愿意详细检查他们所做的每一项承诺吗？你对在你的余生中维护这个代码感到兴奋吗？</li></ul><p id="92b6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在进入这篇文章的内容之前，我想指出我将是一个伪君子。我们将专注于测试API。在这个过程中，我们还需要修改构建模型的代码。敏锐的读者会注意到，我没有为模型构建管道编写任何测试。如果有帮助的话，我真的很抱歉。</p><h1 id="8f57" class="ln lo jb bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">为更好的测试进行调整</h1><p id="c415" class="pw-post-body-paragraph jy jz jb ka b kb ml kd ke kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv ij bi translated">在我们开始编写实际的测试之前，我想改变一下我们正在生成的API响应。目前，我们只发送回预测的类(虹膜类型)。虽然这是我们的预测API的用户所需要的，但它并没有提供大量的信息供我们测试。预测类被选为具有最高模型分数的类(虹膜类型)。由于这种阈值处理，即使潜在的分数有些不同，预测的类别也可以是相同的。这类似于一个函数，它执行复杂而精确的计算，但返回一个舍入到最接近的整数的值。即使许多输入的返回整数值与预期值匹配，基础计算也可能不正确。理想情况下，我们希望在计算的精确结果被阈值化或舍入之前验证它们是否正确。</p><p id="f962" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了给我们自己提供更多的数据来验证我们对模型的评分是正确的，我们将改变API响应来包含每个类的概率。这是通过调用<code class="fe ne nf ng nh b">MODEL.predict_proba()</code>而不是<code class="fe ne nf ng nh b">MODEL.predict()</code>来完成的。然后我们使用<code class="fe ne nf ng nh b">argmax()</code>来获得最大值的索引，这给了我们预测的类。我们将通过<code class="fe ne nf ng nh b">probabilities</code>将原始类别概率返回给用户(参见下面的示例响应)。</p><figure class="ni nj nk nl gt is"><div class="bz fp l di"><div class="nm nn l"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk">Updated API to return class probabilities</figcaption></figure><p id="924a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后我们可以运行API ( <code class="fe ne nf ng nh b">python predict_api.py</code>)并通过<code class="fe ne nf ng nh b">requests</code>进行测试调用:</p><figure class="ni nj nk nl gt is"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h1 id="2d19" class="ln lo jb bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">API的基本测试</h1><p id="d88a" class="pw-post-body-paragraph jy jz jb ka b kb ml kd ke kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv ij bi translated">我们将从一个简单的例子开始，在这个例子中，我们只对如上所示的相同例子进行评分，但是我们将使用pytest来完成这个任务。我们首先需要创建一个测试文件:<code class="fe ne nf ng nh b">test_predict_api.py</code>。现在，我们可以把它放在与我们的API文件相同的目录中:<code class="fe ne nf ng nh b">predict_api.py</code>。<em class="lm">注意:pytest能够自动定位测试文件和函数，但是您需要协助它这样做。默认情况下，它将检查任何文件名以“test_”前缀开头的文件，并运行任何以“test_”开头的测试函数。</em></p><p id="671c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当我们构建测试时，我们通常遵循这种模式:</p><ol class=""><li id="0f3e" class="mq mr jb ka b kb kc kf kg kj ms kn mt kr mu kv ns mw mx my bi translated">设置(可选):测试或环境的初始化。示例:初始化数据库，创建将在测试中使用的类的实例，初始化系统的状态，等等</li><li id="fddb" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv ns mw mx my bi translated">运行代码:在预定义的测试用例上或在预定义的环境中运行主代码库(测试中的代码)中的一些代码。这可能包括:调用一个函数/方法，创建一个类的实例，初始化一个资源，调用一个API等等</li><li id="d1f2" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv ns mw mx my bi translated">验证结果:通过使用<code class="fe ne nf ng nh b">assert</code>语句检查代码的效果是否符合预期:函数调用的返回值是否正确，异常是否被适当地提出，系统是否已经改变到正确的状态等等…</li><li id="faee" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv ns mw mx my bi translated">拆除(可选):测试运行后进行清理，将环境恢复到默认状态。</li></ol><p id="6cdb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以下是本示例如何与这些步骤保持一致:</p><ol class=""><li id="5d88" class="mq mr jb ka b kb kc kf kg kj ms kn mt kr mu kv ns mw mx my bi translated">设置:实例化一个<code class="fe ne nf ng nh b">test_client</code>(如下所述)，它将允许我们模拟对API的调用。</li><li id="f918" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv ns mw mx my bi translated">运行代码:用一组预定义的特性调用<code class="fe ne nf ng nh b"> /predict</code>端点。</li><li id="face" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv ns mw mx my bi translated">验证结果:我们得到一个带有200的<code class="fe ne nf ng nh b">status_code</code>的响应，内容是带有正确格式和值的JSON。</li><li id="2650" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv ns mw mx my bi translated">拆:这个有些含蓄。我们将在<code class="fe ne nf ng nh b">test_client</code>的“设置”过程中使用上下文管理器。退出时，客户端将被清理。</li></ol><figure class="ni nj nk nl gt is"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="9cab" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如上所述，我们使用的<code class="fe ne nf ng nh b">test_client</code>是Flask的一个特性，它允许我们在不运行服务器的情况下模拟调用。这里，我们使用上下文管理器创建一个新的客户机。由此，我们发出一个GET请求。<code class="fe ne nf ng nh b">query_string</code>关键字参数提供的功能类似于<code class="fe ne nf ng nh b">params</code>在<code class="fe ne nf ng nh b">requests.get()</code>中的工作方式；它允许我们传递用于创建查询字符串的数据。</p><p id="8962" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从响应中，我们检查我们是否收到了“200 OK”状态，最后我们检查响应的有效负载是否与我们预期的相匹配。既然它是以<code class="fe ne nf ng nh b">bytes</code>的形式出现，我们可以用<code class="fe ne nf ng nh b">json.loads()</code>把它转换成一个<code class="fe ne nf ng nh b">dict</code>。</p><p id="50e7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们现在可以在命令行使用<code class="fe ne nf ng nh b">pytest</code>来执行测试。</p><figure class="ni nj nk nl gt is"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h1 id="aba0" class="ln lo jb bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">总是先失败</h1><p id="b683" class="pw-post-body-paragraph jy jz jb ka b kb ml kd ke kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv ij bi translated">每当你编写自动化测试时，<strong class="ka jc">验证你确实在测试一些东西是很重要的——你的测试可能会失败</strong>。虽然这是一个显而易见的说法，但一个常见的缺陷是编写的测试实际上并不测试任何东西。当它们通过时，开发人员认为被测试的代码是正确的。然而，测试通过了，因为测试写得很差。</p><p id="4617" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">防止这种错误的一种方法是使用测试驱动开发(TDD)。我们不会对此进行深入探讨，但这是一个发展过程，其中:</p><ol class=""><li id="bfc3" class="mq mr jb ka b kb kc kf kg kj ms kn mt kr mu kv ns mw mx my bi translated">您首先为一个新特性编写一个测试。</li><li id="6ad0" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv ns mw mx my bi translated">您确认测试失败。</li><li id="ded2" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv ns mw mx my bi translated">您编写代码来实现新功能。</li><li id="cf7c" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv ns mw mx my bi translated">您验证测试现在通过了。</li></ol><p id="1743" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你之前没有尝试过TDD，我绝对推荐。这需要纪律，尤其是在开始的时候，还需要其他开发者和利益相关者的支持。然而，在实现新特性时，对过程的投入会带来更少的错误和更低的压力。要了解更多，请阅读Harry Percival的优秀著作，<a class="ae kw" href="https://www.obeythetestinggoat.com/pages/book.html" rel="noopener ugc nofollow" target="_blank">用Python进行测试驱动的开发</a>，这本书是他在网上免费提供的。</p><p id="c7e2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您不支持TDD，我用来验证每个测试是否确实在测试的懒惰方法是将我的assert表达式更改为显式失败。例如，我们将把<code class="fe ne nf ng nh b">assert response.status_code == 200</code>改为<code class="fe ne nf ng nh b">assert response.status_code != 200</code>。如果您进行了此更改并重新运行测试，您应该会收到类似于以下内容的失败消息:</p><figure class="ni nj nk nl gt is"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="b864" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您打算使用这种方法，请注意pytest只会报告第一个发生的<code class="fe ne nf ng nh b">AssertionError</code>。所以，<strong class="ka jc">你必须分别改变每个断言，然后重新测试</strong>。</p><h1 id="74aa" class="ln lo jb bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">更多测试</h1><p id="f91c" class="pw-post-body-paragraph jy jz jb ka b kb ml kd ke kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv ij bi translated">我们现在有一个对我们的API的测试调用，它正在工作。我们如何扩展这个来测试具有不同特征值和不同预期结果(标签和概率)的多个调用？一个快捷的选择是使用我们在模型构建期间创建的测试数据集。但是，我们需要获得类概率和预测标签，以用作每个输入记录的预期结果。</p><p id="f6c6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">需要注意的一点是<strong class="ka jc">我们测试的是API平台，而不是模型本身</strong>。基本上，这意味着我们不关心模型是否做出了错误的预测；我们只想验证API平台上的模型输出是否与构建/离线/开发环境中的模型输出相匹配。我们还需要测试特性的准备(例如，均值插补)是否在API平台上正确完成，但我们将在本文的下一部分讨论这个问题。</p><p id="2d67" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于我们的测试数据集可能会随着模型的每个新版本而改变，我们应该将这些数据的生成合并到我们的模型构建中。我对我们的模型构建脚本做了一些轻微的重构(还需要更多),并添加了在模型构建后生成测试数据集的代码。我们将把我们的测试用例存储在一个JSON文件中，每个测试用例的结构如下:</p><figure class="ni nj nk nl gt is"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="4778" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面是我们的模型构建代码的修改版本，它合并了测试数据集的生成:</p><figure class="ni nj nk nl gt is"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="6cf9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在顶部，有一个名为<code class="fe ne nf ng nh b">prep_test_cases()</code>的函数，它只是将每个测试的特性、分类概率和预测标签重新格式化为我们的测试用例格式。</p><p id="3718" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们已经生成了测试数据，我们需要添加一个新的测试来对该文件中的所有记录进行评分，并检查响应:</p><figure class="ni nj nk nl gt is"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="c926" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因为我们以一种清晰的方式构建了测试数据，其中每个测试用例都有特性(API输入)以及预期的响应(API输出)，所以测试代码相当简单。这种方法的一个缺点是类别概率是浮动的，我们正在对这些值进行精确的比较。通常，在比较浮点值时允许有一定的容差，这样非常接近的值就被认为是等价的。为了处理这个问题，我们需要解析预期的响应，并在比较<code class="fe ne nf ng nh b">probabilities</code>中的每个值时使用<code class="fe ne nf ng nh b">pytest.approx()</code>。它不需要太多的代码，但是我认为这会使这个讨论有点混乱，所以我省略了实现。</p><h1 id="f749" class="ln lo jb bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">处理缺失值</h1><p id="a4b3" class="pw-post-body-paragraph jy jz jb ka b kb ml kd ke kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv ij bi translated">我们的API配置为使用均值插补来替换错误值或缺失值，但我们的测试数据集不包含任何缺失值的记录。然而，这不是一个问题，因为我们可以使用我们已经拥有的数据来模拟这些数据。我们只需要用某个特性的平均值替换现有的值，并重新对记录进行评分。对于我们的模型构建脚本，我们将在原始测试数据生成代码之后添加以下内容:</p><figure class="ni nj nk nl gt is"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="5880" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可能不需要这么彻底，但是对于每一条记录，我们都在测试可能缺失的每一个特征组合。我们为每条记录创建了两个版本:一个有缺失值的<code class="fe ne nf ng nh b">None </code>,另一个有平均值的估算值。第一个将作为<code class="fe ne nf ng nh b">features</code>存储在测试用例中(在<code class="fe ne nf ng nh b">None</code>有价值的特性被删除之后)。第二个将被评分，以获得我们期望看到的API返回的预测概率。为了去掉<code class="fe ne nf ng nh b"> None</code>值的特性，我们必须在<code class="fe ne nf ng nh b">prep_test_cases</code>函数中对<code class="fe ne nf ng nh b">feat_dict</code>的创建做一个小小的改变。下面是修改后的函数:</p><figure class="ni nj nk nl gt is"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="ca5e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们也需要改变我们的测试来使用这个新文件。虽然我们可以复制最后一个测试函数<code class="fe ne nf ng nh b">test_api()</code>并替换文件名<code class="fe ne nf ng nh b">testdata_iris_v1.0.json</code>，但这会导致重复的代码。因为我们需要测试函数除了文件名之外完全相同，所以更好的方法是使用pytest的<code class="fe ne nf ng nh b">parametrize</code>功能。我们只需添加一个装饰器，允许我们为测试函数指定参数，并为每个值重新运行测试。在这种情况下，我们将传入文件名:</p><figure class="ni nj nk nl gt is"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h1 id="b0db" class="ln lo jb bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">测试错误</h1><p id="f22f" class="pw-post-body-paragraph jy jz jb ka b kb ml kd ke kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv ij bi translated">在<a class="ae kw" href="https://medium.com/@chris.moradi/building-prediction-apis-in-python-part-2-basic-error-handling-3ab87b7a93" rel="noopener">上一篇关于错误处理的文章</a>中，我提到我们可以更好地选择我们愿意评分的记录，但是我没有提供这样的例子。这里我们将调整我们的API来看一个简单的例子，在这个例子中，我们将拒绝缺少<code class="fe ne nf ng nh b">petal_width</code>数据的请求。我们将对所有其他记录进行评分，如果需要的话，使用平均插补。</p><p id="5d6b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">稍微扯点题外话，我是怎么选的<code class="fe ne nf ng nh b">petal_width</code>？嗯，如果我们查看特征重要性(使用<code class="fe ne nf ng nh b">model.feature_importances_</code>，我们会看到第四个特征(<code class="fe ne nf ng nh b">petal_width</code>)具有最高值，归一化得分为0.51。因为这是我们模型中最重要的特性，所以拒绝缺少这个特性的记录是最有意义的。</p><p id="4ed7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">实现这一点的一个简单方法是删除<code class="fe ne nf ng nh b">petal_width</code>的默认值，然后在它丢失的情况下处理它。几乎所有的代码都保持不变，但是我在这里把它包括进来是为了便于理解。</p><figure class="ni nj nk nl gt is"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="e9b9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以做一个快速测试，以确保它适用于一个简单的情况:</p><figure class="ni nj nk nl gt is"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="b202" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">太好了！有用！现在，我们只需要将它添加到我们的测试套件中。</p><p id="57bc" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了简单起见，我将跳过如何修改我们旧的缺失值测试，只实现处理<code class="fe ne nf ng nh b">petal_width</code>的缺失值或错误值的新测试。基本上，我从<code class="fe ne nf ng nh b">missing_grps</code>中移除了所有包含3的元组(索引为<code class="fe ne nf ng nh b">petal_width</code>)以及所有特性都缺失的测试。</p><p id="5bd4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于我们的新测试，我们可以使用相同的JSON格式。这将是一个更干净的实现。为了清楚起见，我将在一个单独的函数中实现这些测试，这个函数有两个针对<code class="fe ne nf ng nh b">petal_width</code>的测试用例:特性丢失和特性有一个错误的值(“垃圾”)。</p><figure class="ni nj nk nl gt is"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="0037" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以重新运行我们的测试，并验证这些通过。当然，我们还应该尝试将<code class="fe ne nf ng nh b">==</code>改为<code class="fe ne nf ng nh b">!=</code>，以验证它们在每种情况下都失败。同样，这将有助于确保我们正在测试我们实际认为我们正在测试的东西。</p><h1 id="3aa5" class="ln lo jb bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">识别问题</h1><p id="d5a3" class="pw-post-body-paragraph jy jz jb ka b kb ml kd ke kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv ij bi translated">现在我们有了测试，我们可能想知道它们是否真的能捕捉到我们代码中的错误。也许，当您创建这些测试并在您的API代码上运行它们时，您已经发现了一些问题。如果没有，您可以做一些简单的更改，这些更改会导致一个或多个测试失败(分别进行这些操作):</p><ul class=""><li id="c9a6" class="mq mr jb ka b kb kc kf kg kj ms kn mt kr mu kv mv mw mx my bi translated">在API代码中，将<code class="fe ne nf ng nh b">sepal_length</code>的<code class="fe ne nf ng nh b">default</code>(平均插补)值从5.8更改为5.3。</li><li id="4738" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated">在API中，放回<code class="fe ne nf ng nh b">petal_width</code>的平均值插补。这将允许API对缺少<code class="fe ne nf ng nh b">petal_width</code>的记录进行评分。当<code class="fe ne nf ng nh b">petal_width</code>丢失时，您应该在期望API返回“400错误请求”的测试中看到失败。</li><li id="a429" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated">在API中，更改当<code class="fe ne nf ng nh b">petal_width</code>丢失时发送的错误消息的文本。</li><li id="b382" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated">我们还可以模拟有人意外修改模型并试图部署它的情况。为了测试这一点，我们可以构建您的模型的替代版本，部署它，但是使用原始模型的测试数据文件(JSON)。实现这一点的快速方法是在训练/测试集分割中对<code class="fe ne nf ng nh b">random_state</code>使用不同的值(例如<code class="fe ne nf ng nh b">random_state=30</code>)。记得把<code class="fe ne nf ng nh b">joblib.dump()</code>中的模型输出文件名改成别的(例如<em class="lm">' iris-RF-altmodel . pkl '</em>)；您需要在API中修改<code class="fe ne nf ng nh b">MODEL</code>来引用这个文件。此外，确保您不要执行生成测试数据文件的代码，因为这些代码会基于替代模型重新构建它们。当您重新运行您的测试时，您可能会在所有测试中看到失败，除了那些当<code class="fe ne nf ng nh b">petal_width</code>丢失或无效时拒绝请求的测试。如果您的测试仍然通过，尝试另一个<code class="fe ne nf ng nh b">random_state</code>,因为模型可能是等价的，因为训练集可能保持不变或者变化不足以改变模型。</li></ul><p id="a415" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的测试肯定能发现问题，但是我们能发现所有的问题吗？简单的答案是，我们可能没有捕捉到一切。在创建这个帖子的时候，我试着将<code class="fe ne nf ng nh b">sepal_width</code>的平均插补(默认值)从3.0改为3.1。当我重新测试时，他们都通过了。也许这没什么大不了的；也许我们的模型对平均值附近<code class="fe ne nf ng nh b">sepal_width</code>的微小变化并不敏感。这是最不重要的特征。然而，我们将我们的测试集用于测试用例，这些数据点不一定落在不同类的边界附近。如果我们有更多的测试用例或者更好的测试用例，我们可能已经能够捕捉到这种类型的bug。</p><h1 id="81f2" class="ln lo jb bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">包扎</h1><p id="6c87" class="pw-post-body-paragraph jy jz jb ka b kb ml kd ke kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv ij bi translated">我们已经看到自动化测试可以帮助我们找到代码中的错误。虽然我们从测试单个API调用开始，但我们能够快速转向运行大量测试用例的框架，并且它只需要添加一点额外的代码。</p><p id="7501" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">测试是一个重要的话题，所以我们可能会在以后的文章中再次讨论这个话题。这里有一些我们没有涉及的领域的快速预览，但将来可能会涉及:</p><ul class=""><li id="fa9f" class="mq mr jb ka b kb kc kf kg kj ms kn mt kr mu kv mv mw mx my bi translated">测试速度:当你改变代码时，经常运行测试是有益的。这使得在重构现有代码或添加新功能时更容易及早发现错误。如果测试需要一段时间来运行，开发人员就不太可能这样做。一种方法是将快速运行的测试与耗时较长的测试分开。然后，开发人员可以在进行增量更改时运行快速测试套件，并在将更改集成回主存储库之前运行完整套件。</li><li id="3f52" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated">嘲讽和修补:我们发现对<code class="fe ne nf ng nh b">sepal_width</code>的缺省值(平均插补值)的微小改变不会导致我们的测试失败。如果这是一个需求，我们可以在评分过程中使用补丁拦截对<code class="fe ne nf ng nh b">model.predict_proba()</code>的调用，以验证正确的值被替换。</li><li id="2186" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated">Fixtures:这是pytest的一个特性，您可以创建、配置和销毁资源，以便建立一个干净且一致的环境，每个测试都可以在其中运行。如果你熟悉许多单元测试框架中的“安装”和“拆卸”, fixtures就是这种思想的延伸。</li><li id="2630" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated">子系统的集成:目前，我们只有模型和API。在随后的文章中，我们将看看如何添加一个数据库后端和一些其他服务。我们如何测试这些？我们如何测试整个系统？</li><li id="d211" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated">持续集成工具:这些工具有助于更容易地将代码集成到共享存储库中。通过使它变得更容易，我们希望它能更经常地、更小规模地完成。这些工具的一个共同特征是，它们会在每次提交拉请求时自动运行测试套件，并且通过/失败的结果会提供给评审者。</li><li id="62b0" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated">测试覆盖率:我们测试了我们代码的每一行吗？我们可以创建一个测试覆盖报告来帮助我们了解哪些代码行在测试期间运行了，哪些没有运行。这不会告诉我们是否已经处理了所有可能的情况，但是它可以给我们信息，让我们知道我们的测试套件在哪里不足。</li><li id="fd95" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated">高级测试方法:我们不太可能涵盖这些主题，但是我想提到它们。使用基于属性的测试(参见<a class="ae kw" href="https://github.com/HypothesisWorks/hypothesis-python" rel="noopener ugc nofollow" target="_blank">假设</a>，您创建参数化测试，框架为您生成一组广泛的测试用例。这可以导致更全面的测试，而不需要你想出所有的边缘情况。突变测试(见<a class="ae kw" href="https://github.com/sixty-north/cosmic-ray" rel="noopener ugc nofollow" target="_blank">宇宙射线</a>)采用了一种非常不同的方法。它与您现有的测试用例一起工作，并实际上以某种小的方式(突变)修改您的源代码(测试中的代码)，以查看您现有的测试是否失败。如果所有的测试仍然通过，你的测试代码是不完整的，因为它不能找到由变异引入的错误。</li></ul></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><h2 id="1baa" class="oa lo jb bd lp ob oc dn lt od oe dp lx kj of og mb kn oh oi mf kr oj ok mj ol bi translated">脚注</h2><ol class=""><li id="7093" class="mq mr jb ka b kb ml kf mm kj om kn on kr oo kv ns mw mx my bi translated">不确定是否有人真的会这么说，但有时你会遇到这样想的人。</li><li id="272d" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv ns mw mx my bi translated">你可能在想，<em class="lm">“我的用户不需要知道每个类的底层分数；他们只需要预测。那么，为什么我只是为了测试而改变我的回答呢？”</em>好问题！我们采用这种方法是出于方便和清晰的考虑。在实际实现中，您使用一个标志来指定是否返回基础分数，并且可能将此功能限制于某些用户。我们还可以使用模仿和修补来访问分数，而无需修改响应来包含模型分数。</li><li id="1265" class="mq mr jb ka b kb mz kf na kj nb kn nc kr nd kv ns mw mx my bi translated">如果你在运行<code class="fe ne nf ng nh b">pytest</code>时遇到问题，请尝试以下选项:<code class="fe ne nf ng nh b">py.test</code>或<code class="fe ne nf ng nh b">python -m pytest</code>。</li></ol></div></div>    
</body>
</html>