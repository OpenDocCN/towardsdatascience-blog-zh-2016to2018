<html>
<head>
<title>Building an Artificial Neural Network using pure Numpy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用纯数字构建人工神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-an-artificial-neural-network-using-pure-numpy-3fe21acc5815?source=collection_archive---------0-----------------------#2018-11-11">https://towardsdatascience.com/building-an-artificial-neural-network-using-pure-numpy-3fe21acc5815?source=collection_archive---------0-----------------------#2018-11-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/85fea2fc81be8ee4eb8fb87eb6f3fd13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KwfhpIwxvk8TAuY9_65tPg.jpeg"/></div></div></figure><p id="e585" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">神经网络因其在计算机视觉和自然语言处理等几个领域的出色表现而备受关注。它们不仅远远超过同类产品，而且用途极其广泛，几乎应用于所有可以想象的领域。</p><p id="e95c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是这两个词到底是什么意思呢？</p><p id="1258" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇简短的帖子中，我们将深入研究神经网络的概念，然后用 Python 编写我们自己的代码，使用<strong class="ka ir"> pure NumPy </strong>对 MNIST 数字进行分类(我保证这很有趣)<strong class="ka ir">。</strong>我会尽量保持简短，主要是为了防止你因为我不专业的写作风格而关闭这个标签。所以让我们直入主题:</p><h1 id="bfa0" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">神经网络到底是什么</h1><p id="794e" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">顾名思义，神经网络是一种受我们大脑中生物网络启发的计算系统。如果你没有像我一样在整个生物课上睡觉，你可能会记得我们大脑中的网络由大量的神经元组成。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ma"><img src="../Images/cd2973e0b1900d9fac4c87166dfd6561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OdUsJrLlIsYcAnq_nU_ngw.png"/></div></div></figure><p id="3792" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">出于我们的目的，我们可以将这个神经元建模为一个函数，它接受一组输入，使用一些<strong class="ka ir"><em class="kw"/></strong>权重获得这些输入的加权和，添加一个<strong class="ka ir"> <em class="kw">偏差</em> </strong>，并基于一些<strong class="ka ir"> <em class="kw">激活</em> </strong>函数输出一个数字。有道理？<em class="kw">我也这么想 lol。</em></p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/9ea73949ac89e0816b2951204173aad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*hMvSmAQdNu_c1lbpJyqoWQ.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Mathematical working of a single neuron</figcaption></figure><p id="94f1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">权重</strong>可以被认为是一串旋钮，我们可以调整它们来获得不同的输出。</p><p id="3114" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">偏置</strong>是另一个旋钮，它决定神经元何时保持不活动，或者换句话说，它决定神经元需要多高的加权和才能有意义地活动。</p><p id="6f6a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">激活</strong>功能是将 logit 功能的任意输出映射到任何特定范围值的功能。它通常用于给我们的模型增加一些非线性。这允许网络以更复杂的方式组合输入，并且反过来在它们可以建模的功能中提供更丰富的能力。最常用的激活函数有 sigmoid、softmax、ReLU、tanh 等。</p><h2 id="f7ae" class="mk ky iq bd kz ml mm dn ld mn mo dp lh kj mp mq ll kn mr ms lp kr mt mu lt mv bi translated">网络的结构</h2><p id="6a24" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">现在我们知道了单个神经元是如何工作的，我们可以将它们连接起来，以层的形式形成一个网络。所以人工神经网络只是一个被高估的复合函数。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/f3a415772047232d0309f7d184c94eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NFtgx4yyIpqXZlNW7987rA.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">A simple neural network. Also called a multilayered perceptron</figcaption></figure><p id="5cd4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">典型的神经网络由 3 种类型的层组成:</p><ol class=""><li id="769a" class="mx my iq ka b kb kc kf kg kj mz kn na kr nb kv nc nd ne nf bi translated"><strong class="ka ir">输入层:</strong>给定的数据点馈入该层。只能有一个输入层。这一层中神经元的数量等于输入的数量。</li><li id="972f" class="mx my iq ka b kb ng kf nh kj ni kn nj kr nk kv nc nd ne nf bi translated"><strong class="ka ir">隐层:</strong>这是全网的肉。这些层试图在输入中找到模式，以获得我们需要的输出。网络可以有任意数量的隐藏层。</li><li id="b60a" class="mx my iq ka b kb ng kf nh kj ni kn nj kr nk kv nc nd ne nf bi translated"><strong class="ka ir">输出层</strong>:这一层给我们网络的预测，即。给定当前参数(每个神经元的权重和偏差)，网络认为应该正确的输出。这一层神经元的数量等于我们需要预测的值的数量。因为我们的任务是分类 MNIST 数字，我们将有 10 个神经元，因为有 10 个数字来计算预测。</li></ol><p id="6d89" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，基本网络层可以定义为:</p><figure class="mb mc md me gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="06e9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该层处理输入，最终产生输出值。这称为层上的向前传递。</p><p id="5c06" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我们的实现中，我们将使用两个层:</p><ol class=""><li id="8b0f" class="mx my iq ka b kb kc kf kg kj mz kn na kr nb kv nc nd ne nf bi translated"><strong class="ka ir">密集层</strong>——其中一层中的每个神经元都与下一层中的每个神经元相连。</li><li id="e895" class="mx my iq ka b kb ng kf nh kj ni kn nj kr nk kv nc nd ne nf bi translated"><strong class="ka ir"> ReLU 激活层</strong> —位于密集层之上的层，将 ReLU 激活功能应用于密集层的输出。我本来可以使用最常见的<em class="kw"> sigmoid </em>函数，但是我有时会尝试使用 ReLU 函数。</li></ol><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/61feb87faaae38518f47d08b8658e26c.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*njuH4XVXf-l9pR_RorUOrA.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">ReLU activation function</figcaption></figure><p id="33bd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这两层可以定义为:</p><figure class="mb mc md me gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h2 id="099e" class="mk ky iq bd kz ml mm dn ld mn mo dp lh kj mp mq ll kn mr ms lp kr mt mu lt mv bi translated">训练网络</h2><p id="395a" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">既然我们已经定义了网络的架构，<em class="kw">我们究竟该如何训练它呢？是的，使用</em> <strong class="ka ir"> <em class="kw">参数，</em> </strong> <em class="kw">即权重和偏差。</em></p><p id="82f7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于我们的网络有不止一个神经元，每个神经元都有一套独特的权重和偏好，这给了我们数千个旋钮来调整。如果你是一个小小的受虐狂，愿意手动调节这些成千上万的旋钮，以获得最佳的组合，那就继续吧。如果你是正常人，那么我们可以利用<strong class="ka ir">梯度下降算法。</strong></p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/a373ab7cbfb79b87635435bf2e27252d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*16uR2JPzCtu2qSCGPe7kbw.jpeg"/></div></div></figure><p id="4f4f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">梯度下降是一种通用算法，可用于优化任何微分函数。它的工作方式是计算函数在当前点的<em class="kw">梯度</em>。这种梯度给我们的方向，将最大限度地发挥功能(梯度口音)。但是我们通常需要最小化一个函数，所以我们把计算出的梯度的方向反过来得到最小化函数的方向(梯度下降)。如果你和我一样有点慢，你可以把它想象成一个球滚下山坡，由于重力的原因，它最终到达最低点。</p><p id="120a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了优化我们的网络，我们需要这样一个函数，所以我们定义了一个<strong class="ka ir">损失</strong>函数——我们使用的这个函数被称为<strong class="ka ir"> log softmax 交叉熵损失</strong>(再次变得尖锐)。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a25a267a38750d2d2716271991c268e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*r5K1soLEtpGIX9_vzlIx4w.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">A scary-looking loss function</figcaption></figure><p id="73a8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们一个字一个字地分解它:</p><ol class=""><li id="66cf" class="mx my iq ka b kb kc kf kg kj mz kn na kr nb kv nc nd ne nf bi translated"><strong class="ka ir"> Softmax </strong></li></ol><p id="038f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Softmax 函数获取一个 N 维实数向量，并将其转换为(0，1)范围内的实数向量，其总和为 1。因此，它输出一个概率分布，这使得它适合于分类任务中的概率解释。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/adad79c273ae765942027d45e477fdc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*L-5355UzqMquS_7B3l7sYQ.jpeg"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Softmax function</figcaption></figure><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/be21f6386a99254356a9bd61686b64f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*_aLwCN-Q7V17av-JuIHhrA.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Graph of Softmax function</figcaption></figure><p id="0cc0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2.<strong class="ka ir">交叉熵损失</strong></p><p id="1826" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">交叉熵表示模型认为的输出分布和原始分布之间的距离。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/0fa83cb6fba7813918dd38b3e5ef5352.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*weChdxeVAQEAQZT97pF4jQ.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Cross-Entropy Loss</figcaption></figure><p id="fd7f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们用代码来写:</p><figure class="mb mc md me gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="bd62" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以我们训练网络的方法如下:将网络的输出与预期输出进行比较，计算损耗。这种损失然后通过网络一次一层地传播回来，并且根据它们对误差的贡献量来更新权重和偏差。这种传播由<strong class="ka ir">反向传播算法</strong>执行。</p><p id="a02b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个算法相当复杂，需要一整篇文章来解释，所以我只告诉你几分钟，因为我很懒。</p><p id="5dc9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，对于每一层，为了计算该层参数对总损耗的影响，我们需要计算损耗对这些参数的导数。为了减轻我们的麻烦，我们可以利用链式法则。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/310c06836a23975d656ec8ccfbc231fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*QG4HGswwDvsmzYbaOrNsjA.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Backpropagation Cheat Sheet</figcaption></figure><p id="0978" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，对于每一层，我们可以添加一个后向通道，其中该层的梯度作为输入，用于计算所需的导数，最后，该层的梯度作为输出返回:</p><figure class="mb mc md me gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><h1 id="8c44" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">运行代码</h1><p id="6e2a" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">下面给出了带有精度图的完整代码</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/2b93272f9c0375f2922bcb45d7dc8ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*1vSsAMj-yhXMG77amax0OA.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Accuracy Plot</figcaption></figure><figure class="mb mc md me gt jr"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="0ebf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">谢谢你坚持到最后。写这篇文章确实帮助我在大脑中巩固了一些复杂的概念。这是我第一次尝试写技术文章，如果有人有什么指点，请在下面留下回应！</p></div></div>    
</body>
</html>