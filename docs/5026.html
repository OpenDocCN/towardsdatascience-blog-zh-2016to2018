<html>
<head>
<title>Speed Up your Algorithms Part 1 — PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">加速您的算法第 1 部分— PyTorch</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speed-up-your-algorithms-part-1-pytorch-56d8a4ae7051?source=collection_archive---------1-----------------------#2018-09-23">https://towardsdatascience.com/speed-up-your-algorithms-part-1-pytorch-56d8a4ae7051?source=collection_archive---------1-----------------------#2018-09-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/a63280f58dfb6f92e2dd534675a2067c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*q4EQweaoPYhBFglP"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">“low angle photography of yellow hot air balloon” by <a class="ae jd" href="https://unsplash.com/@sutirtab?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">sutirta budiman</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="3545" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">加速你的 PyTorch 模型</h2></div></div><div class="ab cl kv kw hu kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ij ik il im in"><p id="9b8c" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这是我写的系列文章中的第一篇。所有帖子都在这里:</p><ol class=""><li id="20d0" class="ly lz jg le b lf lg li lj ll ma lp mb lt mc lx md me mf mg bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/speed-up-your-algorithms-part-1-pytorch-56d8a4ae7051">加速您的算法第 1 部分— PyTorch </a></li><li id="b935" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/speed-up-your-algorithms-part-2-numba-293e554c5cc1">加速您的算法第 2 部分— Numba </a></li><li id="3ca5" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/speed-up-your-algorithms-part-3-parallelization-4d95c0888748">加速您的算法第 3 部分—并行化</a></li><li id="0afc" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/speeding-up-your-algorithms-part-4-dask-7c6ed79994ef">加速您的算法第 4 部分— Dask </a></li></ol><p id="083c" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这些与<strong class="le jh"> <em class="mm"> Jupyter 笔记本</em> </strong>搭配在这里可以得到:</p><p id="30e5" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">[<a class="ae jd" href="https://github.com/PuneetGrov3r/MediumPosts/tree/master/SpeedUpYourAlgorithms" rel="noopener ugc nofollow" target="_blank">Github-speedupyourlightms</a>和<strong class="le jh">[</strong><a class="ae jd" href="https://www.kaggle.com/puneetgrover/kernels" rel="noopener ugc nofollow" target="_blank"><strong class="le jh">ka ggle</strong></a><strong class="le jh">]</strong></p></div><div class="ab cl kv kw hu kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ij ik il im in"><p id="6a09" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le jh">(编辑-28/11/18) </strong> —增加了 torch.multiprocessing 部分。</p><h1 id="0e45" class="mn mo jg bd mp mq mr ms mt mu mv mw mx km my kn mz kp na kq nb ks nc kt nd ne bi translated">索引:</h1><ol class=""><li id="a684" class="ly lz jg le b lf nf li ng ll nh lp ni lt nj lx md me mf mg bi translated">介绍</li><li id="85e1" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated">如何检查 cuda 的可用性？</li><li id="9314" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated">如何获得更多关于 cuda 设备的信息？</li><li id="6939" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated">如何在 GPU 上存储张量和运行模型？</li><li id="893c" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated">如果您有多个 GPU，如何选择和使用它们？</li><li id="525e" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated">数据并行性</li><li id="bda8" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated">数据并行性的比较</li><li id="2d4e" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated">torch .多重处理</li><li id="49e1" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated">参考</li></ol><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="8337" class="nt mo jg np b gy nu nv l nw nx"><strong class="np jh"><em class="mm">NOTE:<br/></em></strong>This post goes with <strong class="np jh"><em class="mm">Jupyter Notebook</em></strong> available in my Repo on Github:[<a class="ae jd" href="https://nbviewer.jupyter.org/github/PuneetGrov3r/MediumPosts/blob/master/SpeedUpYourAlgorithms/1%29%20PyTorch.ipynb" rel="noopener ugc nofollow" target="_blank">SpeedUpYourAlgorithms-Pytorch</a>]</span></pre><h1 id="f68b" class="mn mo jg bd mp mq mr ms mt mu mv mw mx km my kn mz kp na kq nb ks nc kt nd ne bi translated">1.简介:</h1><p id="e413" class="pw-post-body-paragraph lc ld jg le b lf nf kh lh li ng kk lk ll ny ln lo lp nz lr ls lt oa lv lw lx ij bi translated">在这篇文章中，我将展示如何使用<code class="fe ob oc od np b">torch</code>和<code class="fe ob oc od np b">pycuda</code>来检查和初始化 GPU 设备，以及如何让你的算法更快。</p><p id="2d6f" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le jh"> PyTorch </strong>是建立在<a class="ae jd" href="https://en.wikipedia.org/wiki/Torch_(machine_learning)" rel="noopener ugc nofollow" target="_blank"> torch </a>之上的机器学习库。它得到了脸书人工智能研究小组的支持。经过最近的发展，它已经获得了很大的普及，因为它的简单性，动态图形，因为它是本质上的 pythonic。它在速度上仍然不落后，在很多情况下甚至可以超越。</p><p id="deef" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">pycuda 让你从 python 访问 Nvidia 的 cuda 并行计算 API。</p><h1 id="5476" class="mn mo jg bd mp mq mr ms mt mu mv mw mx km my kn mz kp na kq nb ks nc kt nd ne bi translated">2.如何检查 cuda 的可用性？</h1><figure class="nk nl nm nn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oe"><img src="../Images/5aa26b49dfc8c82d4fe5b05f70b8dec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*o_E6f5kcwVGlc-qh"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">“brown dried leaves on sand” by <a class="ae jd" href="https://unsplash.com/@srz?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">sydney Rae</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e769" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">要使用<code class="fe ob oc od np b">Torch</code>检查您是否有可用的<code class="fe ob oc od np b">cuda</code>设备，您只需运行:</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="2841" class="nt mo jg np b gy nu nv l nw nx">import torch</span><span id="c439" class="nt mo jg np b gy of nv l nw nx">torch.cuda.is_available()<br/># True</span></pre><h1 id="0ed5" class="mn mo jg bd mp mq mr ms mt mu mv mw mx km my kn mz kp na kq nb ks nc kt nd ne bi translated">3.如何获得更多关于 cuda 设备的信息？</h1><figure class="nk nl nm nn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi og"><img src="../Images/a17aec95496e155e16e60b01c4ca95eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PyDFEiESBGbDDeuq"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">“black smartphone” by <a class="ae jd" href="https://unsplash.com/@rawpixel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">rawpixel</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="13e9" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">要获取设备的基本信息，您可以使用<code class="fe ob oc od np b">torch.cuda</code>。但是要在你的设备上获得更多信息，你可以使用<code class="fe ob oc od np b">pycuda</code>，一个围绕<code class="fe ob oc od np b">CUDA</code>库的 python 包装器。您可以使用类似以下的内容:</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="793b" class="nt mo jg np b gy nu nv l nw nx">import torch<br/>import pycuda.driver as cuda<br/>cuda.init()</span><span id="878e" class="nt mo jg np b gy of nv l nw nx">## Get Id of default device<br/>torch.cuda.current_device()<br/># 0</span><span id="177c" class="nt mo jg np b gy of nv l nw nx">cuda.Device(0).name() # '0' is the id of your GPU<br/># Tesla K80</span></pre><p id="b51f" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">或者，</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="755d" class="nt mo jg np b gy nu nv l nw nx">torch.cuda.get_device_name(0) # Get name device with ID '0'<br/># 'Tesla K80'</span></pre><p id="0d5d" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我编写了一个简单的类来获取关于兼容 GPU 的信息:</p><figure class="nk nl nm nn gt is"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="0c4c" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">要获取当前的内存使用情况，您可以使用<code class="fe ob oc od np b">pyTorch</code>的功能，例如:</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="fd2f" class="nt mo jg np b gy nu nv l nw nx">import torch</span><span id="9014" class="nt mo jg np b gy of nv l nw nx"># Returns the current GPU memory usage by <br/># tensors in bytes for a given device<br/>torch.cuda.memory_allocated()</span><span id="399e" class="nt mo jg np b gy of nv l nw nx"># Returns the current GPU memory managed by the<br/># caching allocator in bytes for a given device<br/>torch.cuda.memory_cached()</span></pre><p id="de0a" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">运行应用程序后，您可以使用一个简单的命令来清除缓存:</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="36c8" class="nt mo jg np b gy nu nv l nw nx"># Releases all unoccupied cached memory currently held by<br/># the caching allocator so that those can be used in other<br/># GPU application and visible in nvidia-smi<br/>torch.cuda.empty_cache()</span></pre><p id="2ccd" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">但是，使用该命令不会释放张量所占用的 GPU 内存，因此它不能增加 PyTorch 可用的 GPU 内存量。</p><p id="2451" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这些内存方法只适用于 GPU。这才是真正需要它们的地方。</p><h1 id="1555" class="mn mo jg bd mp mq mr ms mt mu mv mw mx km my kn mz kp na kq nb ks nc kt nd ne bi translated">4.如何在 GPU 上存储张量和运行模型？</h1><blockquote class="oj ok ol"><p id="9ac6" class="lc ld mm le b lf lg kh lh li lj kk lk om lm ln lo on lq lr ls oo lu lv lw lx ij bi translated"><code class="fe ob oc od np b">.cuda</code>魔法。</p></blockquote><figure class="nk nl nm nn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/c5b957bad78727cc4d65f3041bd27a53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*f2OBzrFNUiuA5mtM"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">“five pigeons perching on railing and one pigeon in flight” by <a class="ae jd" href="https://unsplash.com/@nate_dumlao?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Nathan Dumlao</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="cd5d" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">如果你想在 cpu 上存储一些东西，你可以简单地写:</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="571b" class="nt mo jg np b gy nu nv l nw nx">a = torch.DoubleTensor([1., 2.])</span></pre><p id="74df" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这个向量存储在 cpu 上，你对它的任何操作都将在 cpu 上完成。要将其传输到 gpu，您只需做<code class="fe ob oc od np b">.cuda</code>:</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="2c12" class="nt mo jg np b gy nu nv l nw nx">a = torch.FloatTensor([1., 2.]).cuda()</span></pre><p id="2dbf" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">或者，</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="1706" class="nt mo jg np b gy nu nv l nw nx">a = torch.cuda.FloatTensor([1., 2.])</span></pre><p id="cd02" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这将为它选择默认设备，可以通过以下命令看到:</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="540a" class="nt mo jg np b gy nu nv l nw nx">torch.cuda.current_device()<br/># 0</span></pre><p id="ef46" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">或者，你也可以这样做:</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="cbd6" class="nt mo jg np b gy nu nv l nw nx">a.get_device()<br/># 0</span></pre><p id="254f" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">你也可以发送一个模型到 GPU 设备。例如，考虑一个由<code class="fe ob oc od np b">nn.Sequential</code>制成的简单模块:</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="ead1" class="nt mo jg np b gy nu nv l nw nx">sq = nn.Sequential(<br/>         nn.Linear(20, 20),<br/>         nn.ReLU(),<br/>         nn.Linear(20, 4),<br/>         nn.Softmax()<br/>)</span></pre><p id="8279" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">要将此发送到 GPU 设备，只需:</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="610e" class="nt mo jg np b gy nu nv l nw nx">model = sq.cuda()</span></pre><p id="0c28" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">你可以检查它是否在 GPU 设备上，因为你必须检查它的参数是否在 GPU 上，比如:</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="f109" class="nt mo jg np b gy nu nv l nw nx"># From the discussions here: <a class="ae jd" href="https://discuss.pytorch.org/t/how-to-check-if-model-is-on-cuda/180" rel="noopener ugc nofollow" target="_blank">discuss.pytorch.org/t/how-to-check-if-model-is-on-cuda</a></span><span id="138f" class="nt mo jg np b gy of nv l nw nx">next(model.parameters()).is_cuda<br/># True</span></pre><h1 id="9a5d" class="mn mo jg bd mp mq mr ms mt mu mv mw mx km my kn mz kp na kq nb ks nc kt nd ne bi translated">5.如果您有多个 GPU，如何选择和使用它们？</h1><figure class="nk nl nm nn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oq"><img src="../Images/e4b2ca4fe403439c670ef11865f97930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3MXSomXz5Hv2XgZN"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">“selective focus photography of mechanics tool lot” by <a class="ae jd" href="https://unsplash.com/@neonbrand?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">NeONBRAND</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="71e0" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">您可以为当前应用程序/存储选择一个 GPU，该 GPU 可以不同于您为上一个应用程序/存储选择的 GPU。</p><p id="73fd" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">正如在第(2)部分已经看到的，我们可以使用<code class="fe ob oc od np b">pycuda</code>来获得所有的<code class="fe ob oc od np b">cuda</code>兼容设备和它们的<code class="fe ob oc od np b">Id</code>,我们在这里不讨论。</p><p id="53b8" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">考虑到您有 3 个<code class="fe ob oc od np b">cuda</code>兼容设备，您可以像这样初始化并分配<code class="fe ob oc od np b">tensors</code>给一个特定的设备:</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="9926" class="nt mo jg np b gy nu nv l nw nx">cuda0 = torch.device('cuda:0')<br/>cuda1 = torch.device('cuda:1')<br/>cuda2 = torch.device('cuda:2')<br/># If you use 'cuda' only, Tensors/models will be sent to <br/># the default(current) device. (default= 0)</span><span id="70b0" class="nt mo jg np b gy of nv l nw nx">x = torch.Tensor([1., 2.], device=cuda1)<br/># Or<br/>x = torch.Tensor([1., 2.]).to(cuda1)<br/># Or<br/>x = torch.Tensor([1., 2.]).cuda(cuda1)</span><span id="b1c5" class="nt mo jg np b gy of nv l nw nx"><strong class="np jh"># NOTE:</strong><br/><strong class="np jh">#</strong> If you want to change the default device, use:<br/>torch.cuda.set_device(2) # where '2' is Id of device</span><span id="65e7" class="nt mo jg np b gy of nv l nw nx"><strong class="np jh">#</strong> And if you want to use only 2 of the 3 GPU's, you<br/><strong class="np jh">#</strong> will have to set the environment variable <br/><strong class="np jh"># </strong><em class="mm">CUDA_VISIBLE_DEVICES</em> equal to say, "0,2" if you <br/><strong class="np jh">#</strong> only want to use first and third GPUs. Now if you <br/><strong class="np jh">#</strong> check how many GPUs you have, it will show two<em class="mm">(0, 1)</em>.<br/>import os<br/>os.environ["CUDA_VISIBLE_DEVICES"] = "0,2"</span></pre><p id="65a2" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">当您在这些<code class="fe ob oc od np b">Tensor</code>上进行任何操作时，您可以不考虑所选的设备，结果将保存在与<code class="fe ob oc od np b">Tensor</code>相同的设备上。</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="af74" class="nt mo jg np b gy nu nv l nw nx">x = torch.Tensor([1., 2.]).to(cuda2)<br/>y = torch.Tensor([3., 4.]).to(cuda2)</span><span id="f1da" class="nt mo jg np b gy of nv l nw nx"># This Tensor will be saved on 'cuda2' only<br/>z = x + y</span></pre><p id="f0e9" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">如果你有多个 GPU，你可以在它们之间分配应用程序的工作，但是这会带来它们之间的通信开销。但是如果你不需要太多的信息，你可以试一试。</p><p id="07ad" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">实际上还有一个问题。在<code class="fe ob oc od np b">PyTorch</code>中，默认情况下所有的 GPU 操作都是异步的。虽然它在 CPU 和 GPU 之间或两个 GPU 之间复制数据时进行必要的同步，但如果你在命令<code class="fe ob oc od np b">torch.cuda.Stream()</code>的帮助下创建自己的流，那么你将不得不自己负责指令的同步。</p><p id="8078" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">从<code class="fe ob oc od np b">PyTorch</code>的文档中举一个例子，这是<strong class="le jh">不正确的:</strong></p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="531b" class="nt mo jg np b gy nu nv l nw nx">cuda = torch.device('cuda')<br/>s = torch.cuda.Stream()  <strong class="np jh"><em class="mm">#</em></strong><em class="mm"> Create a new stream.</em><br/>A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)<br/>with torch.cuda.stream(s):<br/>    <strong class="np jh"><em class="mm">#</em></strong><em class="mm"> because sum() may start execution before normal_() finishes!</em><br/>    B = torch.sum(A)</span></pre><p id="c333" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">如果您想要充分发挥多个 GPU 的潜力，您可以:</p><ol class=""><li id="fcc3" class="ly lz jg le b lf lg li lj ll ma lp mb lt mc lx md me mf mg bi translated">将所有 GPU 用于不同的任务/应用，</li><li id="1126" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated">将每个 GPU 用于集合或堆栈中的一个模型，每个 GPU 具有数据的副本(如果可能)，因为大多数处理是在拟合模型期间完成的，</li><li id="fca4" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated">在每个 GPU 中使用带有切片输入和模型副本的每个 GPU。每个 GPU 将分别计算结果，并将它们结果发送到目标 GPU，在那里将进行进一步的计算，等等。</li></ol><h1 id="0bdf" class="mn mo jg bd mp mq mr ms mt mu mv mw mx km my kn mz kp na kq nb ks nc kt nd ne bi translated">6.数据并行？</h1><figure class="nk nl nm nn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi or"><img src="../Images/d4c17d9ce26f99a8a9dc24d04b052c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Zlcttm21tT1vL9qW"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">“photography of tree in forest” by <a class="ae jd" href="https://unsplash.com/@akeenster?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Abigail Keenan</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="2e7a" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">在数据并行中，我们将从数据生成器获得的一批数据分割成更小的小批，然后发送到多个 GPU 进行并行计算。</p><p id="3681" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">在<code class="fe ob oc od np b">PyTorch</code>中，使用<code class="fe ob oc od np b">torch.nn.DataParallel</code>实现数据并行。</p><p id="bfc1" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">但是我们将看到一个简单的例子，看看到底发生了什么。为此我们将不得不使用<code class="fe ob oc od np b">nn.parallel</code>的一些功能，即:</p><ol class=""><li id="e6ba" class="ly lz jg le b lf lg li lj ll ma lp mb lt mc lx md me mf mg bi translated">复制:在多个设备上复制<code class="fe ob oc od np b">Module</code>。</li><li id="f1e1" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated">分散:将第一维度的<code class="fe ob oc od np b">input</code>分布在这些设备中。</li><li id="ea18" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated">收集:从这些设备中收集并连接第一维度的<code class="fe ob oc od np b">input</code>。</li><li id="2e85" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated">parallel_apply:将我们从 Scatter 获得的一组分布式的<code class="fe ob oc od np b">input</code>应用到我们从 Replicate 获得的相应的一组分布式的<code class="fe ob oc od np b">Module</code>。</li></ol><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="b0b8" class="nt mo jg np b gy nu nv l nw nx"># Replicate module to devices in device_ids<br/>replicas <strong class="np jh">=</strong> nn<strong class="np jh">.</strong>parallel<strong class="np jh">.</strong>replicate(module, device_ids)</span><span id="d8cf" class="nt mo jg np b gy of nv l nw nx"># Distribute input to devices in device_ids<br/>inputs <strong class="np jh">=</strong> nn<strong class="np jh">.</strong>parallel<strong class="np jh">.</strong>scatter(input, device_ids)</span><span id="387f" class="nt mo jg np b gy of nv l nw nx"># Apply the models to corresponding inputs<br/>outputs <strong class="np jh">=</strong> nn<strong class="np jh">.</strong>parallel<strong class="np jh">.</strong>parallel_apply(replicas, inputs)</span><span id="c0cc" class="nt mo jg np b gy of nv l nw nx"># Gather result from all devices to output_device<br/>result = nn<strong class="np jh">.</strong>parallel<strong class="np jh">.</strong>gather(outputs, output_device)</span></pre><p id="c475" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">或者，简单地说:</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="47dd" class="nt mo jg np b gy nu nv l nw nx">model = nn.DataParallel(model, device_ids=device_ids)<br/>result = model(input)</span></pre><h1 id="a063" class="mn mo jg bd mp mq mr ms mt mu mv mw mx km my kn mz kp na kq nb ks nc kt nd ne bi translated">7.数据并行比较</h1><figure class="nk nl nm nn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi os"><img src="../Images/13a189379c867707b1512a40aaa50814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tHfI31UEuKJPQ5l-"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">“silver bell alarm clock” by <a class="ae jd" href="https://unsplash.com/@icons8?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Icons8 team</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="9422" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我没有多个 GPU，但我能够找到一个由<a class="ot ou ep" href="https://medium.com/u/7dd01b9c5ed1?source=post_page-----56d8a4ae7051--------------------------------" rel="noopener" target="_blank"> Ilia Karmanov </a> <a class="ae jd" href="https://medium.com/@iliakarmanov/multi-gpu-rosetta-stone-d4fa96162986" rel="noopener">在这里</a>和他的 github repo 比较大多数使用多个 GPU 的框架<a class="ae jd" href="https://github.com/ilkarman/DeepLearningFrameworks" rel="noopener ugc nofollow" target="_blank">在这里</a>的伟大帖子。</p><p id="0b17" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">他的结果是:</p><figure class="nk nl nm nn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ov"><img src="../Images/1280f9e2fc3cbd55838cdf64029cdb3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wyytAQb1WsBPMQGRckft3w.png"/></div></div></figure><blockquote class="oj ok ol"><p id="7505" class="lc ld mm le b lf lg kh lh li lj kk lk om lm ln lo on lq lr ls oo lu lv lw lx ij bi translated">【最后更新:(2018 年 6 月 19 日)】即他的 github 回购。PyTorch 1.0、Tensorflow 2.0 以及新 GPU 的推出可能会改变这一点…</p></blockquote><p id="6f90" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">因此，如您所见，即使必须在开始和结束时与主设备通信，并行处理也绝对有帮助。而且<code class="fe ob oc od np b">PyTorch</code>给出结果的速度比所有的都快，仅在多 GPU 的情况下比<code class="fe ob oc od np b">Chainer</code>快。<code class="fe ob oc od np b">Pytorch</code>也很简单，只需调用一次<code class="fe ob oc od np b">DataParallel</code>。</p><h1 id="8b82" class="mn mo jg bd mp mq mr ms mt mu mv mw mx km my kn mz kp na kq nb ks nc kt nd ne bi translated">8.torch .多重处理</h1><figure class="nk nl nm nn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ow"><img src="../Images/92903b1bb39a0a0af12158958a86743b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hvIAUhW7gtdomsQk"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/@mjhphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Matthew Hicks</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="3065" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><code class="fe ob oc od np b">torch.multiprocessing</code>是 Python <code class="fe ob oc od np b">multiprocessing</code>模块的包装器，其 API 与原始模块 100%兼容。所以可以用<code class="fe ob oc od np b">Queue</code>的、<code class="fe ob oc od np b">Pipe</code>的、<code class="fe ob oc od np b">Array</code>的等等。这些都在 Python 的多重处理模块中。除此之外，为了使它更快，他们增加了一个方法<code class="fe ob oc od np b">share_memory_()</code>，它允许数据进入任何进程都可以直接使用它的状态，因此将该数据作为参数传递给不同的进程不会复制该数据。</p><p id="54f4" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">你可以分享<code class="fe ob oc od np b">Tensors</code>，model 的<code class="fe ob oc od np b">parameters</code>，你可以随心所欲的在 CPU 或者 GPU 上分享。</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="451c" class="nt mo jg np b gy nu nv l nw nx"><strong class="np jh">Warning from Pytorch: (Regarding sharing on GPU)<br/></strong>  CUDA API requires that the allocation exported to other processes remains valid as long as it’s used by them. You should be careful and ensure that CUDA tensors you shared don’t go out of scope as long as it’s necessary. This shouldn’t be a problem for sharing model parameters, but passing other kinds of data should be done with care. Note that this restriction doesn’t apply to shared CPU memory.</span></pre><p id="eea5" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">你可以在这里的“池和进程”部分使用上面的方法，为了获得更快的速度，你可以使用<code class="fe ob oc od np b">share_memory_()</code>方法在所有进程之间共享一个<code class="fe ob oc od np b">Tensor</code>(比方说)而不被复制。</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="4629" class="nt mo jg np b gy nu nv l nw nx"><strong class="np jh"># Training a model using multiple processes:</strong></span><span id="95d4" class="nt mo jg np b gy of nv l nw nx">import torch.multiprocessing as mp<br/>def train(model):<br/>    for data, labels in data_loader:<br/>        optimizer.zero_grad()<br/>        loss_fn(model(data), labels).backward()<br/>        optimizer.step()  <strong class="np jh"><em class="mm">#</em></strong><em class="mm"> This will update the shared parameters</em></span><span id="b17a" class="nt mo jg np b gy of nv l nw nx">model = nn.Sequential(nn.Linear(n_in, n_h1),<br/>                      nn.ReLU(),<br/>                      nn.Linear(n_h1, n_out))</span><span id="b49c" class="nt mo jg np b gy of nv l nw nx">model.share_memory() <strong class="np jh">#</strong> Required for 'fork' method to work</span><span id="c412" class="nt mo jg np b gy of nv l nw nx">processes = []<br/>for i in range(4): # No. of processes<br/>    p = mp.Process(target=train, args=(model,))<br/>    p.start()<br/>    processes.append(p)</span><span id="3614" class="nt mo jg np b gy of nv l nw nx">for p in processes: p.join()</span></pre><p id="b007" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">您也可以使用一组机器。更多信息请参见<a class="ae jd" href="https://pytorch.org/docs/stable/distributed.html" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><h1 id="b319" class="mn mo jg bd mp mq mr ms mt mu mv mw mx km my kn mz kp na kq nb ks nc kt nd ne bi translated">9.参考资料:</h1><ol class=""><li id="b10e" class="ly lz jg le b lf nf li ng ll nh lp ni lt nj lx md me mf mg bi translated"><a class="ae jd" href="https://documen.tician.de/pycuda/" rel="noopener ugc nofollow" target="_blank">https://documen.tician.de/pycuda/</a></li><li id="f51d" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated"><a class="ae jd" href="https://pytorch.org/docs/stable/notes/cuda.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/notes/cuda.html</a></li><li id="4c8d" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated"><a class="ae jd" href="https://discuss.pytorch.org/t/how-to-check-if-model-is-on-cuda" rel="noopener ugc nofollow" target="_blank">https://discuse . py torch . org/t/how-to-check-if-model-is-on-cuda</a></li><li id="8f80" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated"><a class="ae jd" href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html#sphx-glr-beginner-blitz-data-parallel-tutorial-py" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/初学者/blitz/data _ parallel _ tutorial . html</a></li><li id="71b3" class="ly lz jg le b lf mh li mi ll mj lp mk lt ml lx md me mf mg bi translated"><a class="ae jd" href="https://medium.com/@iliakarmanov/multi-gpu-rosetta-stone-d4fa96162986" rel="noopener">https://medium . com/@ iliakarmanov/multi-GPU-Rosetta-stone-d4fa 96162986</a></li></ol><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="45a1" class="nt mo jg np b gy nu nv l nw nx">Suggestions and reviews are welcome.<br/>Thank you for reading!</span></pre><p id="60a1" class="pw-post-body-paragraph lc ld jg le b lf lg kh lh li lj kk lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">签名:</p><figure class="nk nl nm nn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ox"><img src="../Images/ca01c1d315400c09978fb5e62da01d87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*N7tbEUmEr0wEqsdlZNQ5iA.png"/></div></div></figure></div></div>    
</body>
</html>