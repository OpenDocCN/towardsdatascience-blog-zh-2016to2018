<html>
<head>
<title>SUPPORT VECTOR MACHINES(SVM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机(SVM)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machines-svm-c9ef22815589?source=collection_archive---------0-----------------------#2018-10-20">https://towardsdatascience.com/support-vector-machines-svm-c9ef22815589?source=collection_archive---------0-----------------------#2018-10-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="1044" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">简介:</h1><p id="0ceb" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">支持向量机可能是最受欢迎和谈论最多的机器学习算法之一。它们在 20 世纪 90 年代开发的时候非常流行，并且仍然是几乎不需要调整的高性能算法的首选方法。在这篇博客中，我们将描绘 SVC 的各种概念。</p><h2 id="c148" class="lq jv iq bd jw lr ls dn ka lt lu dp ke ld lv lw ki lh lx ly km ll lz ma kq mb bi translated">映射的概念:</h2><p id="c618" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc"> 1。什么是 SVM？</em>T3】</strong></p><p id="21fc" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir">2<em class="mc">。SVM 背后的意识形态。</em> </strong></p><p id="6a86" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc"> 3。直觉发展。</em>T11】</strong></p><p id="7687" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc"> 4。SVM 使用的术语。</em>T15】</strong></p><p id="cb69" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc"> 5。超平面(决策面)。</em>T19】</strong></p><p id="d7cf" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"><em class="mc">⑥。硬保证金 SVM。</em> </strong></p><p id="4c25" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc"> 7。软利润 SVM。</em> </strong></p><p id="e22a" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir">T29】8。SVM 的损失函数解释。 </strong></p><p id="802f" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc"> 9。SVM 的双重形式。</em> </strong></p><p id="5b9e" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir">10<em class="mc">。什么是内核诡计？</em> </strong></p><p id="f336" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir">11<em class="mc">。内核的类型。</em> </strong></p><p id="be2d" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir">12<em class="mc">。SVM 的利与弊。</em>T47】</strong></p><p id="6ad1" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir">13<em class="mc">。为 SVM 准备数据。</em>T51】</strong></p><p id="5a56" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir">14<em class="mc">。模型应用</em> </strong></p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="182d" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">1.什么是 SVM？</h1><p id="62fb" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">被称为 SVM 的支持向量机是一种<strong class="ku ir"> <em class="mc">监督学习算法</em> </strong>，可用于支持向量分类(SVC)和支持向量回归(SVR)等分类和回归问题。它用于较小的数据集，因为处理时间太长。在这一集中，我们将关注 SVC。</p><h1 id="e839" class="ju jv iq bd jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr bi translated">2.SVM 背后的意识形态:</h1><p id="6eb2" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">SVM 是基于寻找一个超平面的想法，该超平面最好地将特征分成不同的域。</p><h1 id="c0c8" class="ju jv iq bd jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr bi translated">3.直觉发展:</h1><p id="5c1a" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">考虑以下情况:</p><p id="c01b" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">有一个跟踪者正在给你发邮件，现在你想设计一个函数(超平面)来明确区分这两种情况，这样无论何时你收到一封跟踪者发来的邮件，它都会被归类为垃圾邮件。下面是绘制超平面的两种情况，你会选择哪一种，为什么？花点时间分析一下情况……</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mn"><img src="../Images/5c3c1569f4fa886389af8b6f03f753e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MPnxatC0-O_HhzLWd83bGA.png"/></div></div></figure><p id="c9bf" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">我猜你会选无花果。你想过你为什么要选无花果吗？因为图(a)中的电子邮件是明确分类的，与图(b)相比，你对此更有信心。基本上，SVM 是由提出一个<strong class="ku ir"> <em class="mc">最优超平面</em> </strong>的想法组成的，它将清楚地分类不同的类(在这种情况下，它们是二元类)。</p><h1 id="fe0c" class="ju jv iq bd jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr bi translated">4.SVM 使用的术语:</h1><p id="5446" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">离超平面最近的点称为<strong class="ku ir"> <em class="mc">支持向量点</em> </strong>，向量离超平面的距离称为<strong class="ku ir"> <em class="mc">边距</em> </strong>。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/8338523e6fedc3725c29f9339d966f56.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/0*luhI3gW7WnXfLnhA.jpg"/></div></figure><p id="327c" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">这里要发展的基本直觉是，SV 点离超平面越远，在它们各自的区域或类中正确分类这些点的概率就越大。SV 点在确定超平面时非常关键，因为如果向量的位置改变，超平面的位置也会改变。技术上这个超平面也可以称为<em class="mc"> </em> <strong class="ku ir"> <em class="mc">边缘最大化超平面</em> </strong>。</p><h1 id="03db" class="ju jv iq bd jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr bi translated">5.超平面(决策面):</h1><p id="f12c" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在这篇文章中，我们已经讨论超平面很久了，让我们在继续讨论之前证明它的意义。超平面是用于区分特征的函数。在 2-D 中，用于在特征之间分类的函数是一条线，而用于在 3-D 中分类特征的函数被称为平面，类似地，用于在更高维度中分类点的函数被称为超平面。现在，既然你知道了超平面，让我们回到 SVM。</p><p id="7764" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">假设有“m”个维度:</p><p id="e965" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">因此,“M”维中超平面的方程可以表示为=</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi na"><img src="../Images/8fc8b47453d5860fd553901731f3ffa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*lSnqrKcgwCcdKcLh9xbqSA.png"/></div></figure><p id="7882" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">在哪里，</p><p id="3c34" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">Wi =向量(W0，W1，W2，W3……Wm)</p><p id="efd7" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">b =有偏项(W0)</p><p id="15c4" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">X =变量。</p><h1 id="c6bc" class="ju jv iq bd jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr bi translated">6.硬利润 SVM:</h1><p id="a8f0" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在，</p><p id="ae55" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><em class="mc">假设 3 个超平面，即(π，π+，π)，使得‘π+’平行于穿过正侧支持向量的‘π’，而‘π’平行于穿过负侧支持向量的‘π’。</em></p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/649fcc968e4cfe5d8a8607539a47aec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*doKKm0KlPusiazXs-W8Mvg.png"/></div></figure><p id="917c" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">每个超平面的方程可以被认为是:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/edca66a034c6c59c9b842164676d394d.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*PrLTv8_JR0jdP7iljVQSow.png"/></div></figure><p id="4d48" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">对于点 X1:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/7882ce5ce977107ac1441d6da37347ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*gsOs7NVwjIvMPBq0Zr7vyQ.png"/></div></figure><p id="e717" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><em class="mc">解释:当点 X1 时，我们可以说该点位于超平面上，并且方程确定我们的实际输出和超平面方程的乘积为 1，这意味着该点被正确地分类在正域中。</em></p><p id="0f5c" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">对于 X3 的观点:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/3c4d00ce5a509d60d1f15fe9cbeed374.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*ADzK9WGKUApRD7hpZKutfw.png"/></div></figure><p id="6c47" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><em class="mc">解释:当点 X3 时，我们可以说该点远离超平面，并且该方程确定我们的实际输出和超平面方程的乘积大于 1，这意味着该点被正确地分类在正域中。</em></p><p id="a71d" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">对于点 X4:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/52b0f74c77d67380f3723a5d2e968878.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*sycFRRPHJip7OtEXK6bUCw.png"/></div></figure><p id="1763" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><em class="mc">解释:当点 X4 时，我们可以说该点位于负区域中的超平面上，并且该方程确定我们的实际输出和超平面方程的乘积等于 1，这意味着该点被正确地分类在负区域中。</em></p><p id="3e00" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">对于点 X6:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/57ce77c29efc3b199e97a4e8b3077db5.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/format:webp/1*6qmr8ZhvIZiHCvqb99aVUQ.png"/></div></figure><p id="9777" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><em class="mc">解释:当点 X6 时，我们可以说该点远离负区域中的超平面，并且该方程确定我们的实际输出和超平面方程的乘积大于 1，这意味着该点被正确地分类在负区域中。</em></p><p id="27fe" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">让我们看看未分类的约束条件:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/5076924196f3677acf93d61b190c9786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*ppsJ51I8o5kTC1q2opSjwQ.png"/></div></figure><p id="3068" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">对于点 X7:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/053a8460559cf59acc31b84e1c117b8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*BPyj-h0CLlidffRerTwDVA.png"/></div></figure><p id="ea7e" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><em class="mc">解释:当 Xi = 7 时，点被错误地分类，因为对于点 7，wT + b 将小于 1，这违反了约束。所以我们发现了由于违反约束而导致的错误分类。同样，我们也可以说对于点 Xi = 8。</em></p><p id="0e5e" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><em class="mc">由此从上面的例子中，我们可以得出，对于任意一点 Xi，</em></p><p id="6ce9" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc">如果易(WT*Xi +b) ≥ 1: </em> </strong></p><p id="fa47" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc">那么 Xi 被正确归类</em> </strong></p><p id="64d5" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc">其他:</em> </strong></p><p id="f0f2" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc"> Xi 被错误归类。</em>T19】</strong></p><p id="e431" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">因此，我们可以看到，如果这些点是线性可分的，那么只有我们的超平面能够区分它们，如果引入任何异常值，那么它就不能将它们分开。所以这些类型的 SVM 被称为<strong class="ku ir"> <em class="mc">作为硬边界的 SVM </em> </strong> <em class="mc">(因为我们有非常严格的约束来正确地分类每一个数据点)。</em></p><h1 id="f151" class="ju jv iq bd jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr bi translated">7.软利润 SVM:</h1><p id="59bf" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们基本上认为数据是线性可分的，这可能不是现实生活中的情况。我们需要更新，以便我们的函数可以跳过一些异常值，并能够对几乎线性可分的点进行分类。为此，我们引入一个新的<strong class="ku ir"> <em class="mc">松弛变量(</em> </strong> ξ)，称为<em class="mc"> Xi。</em></p><p id="559f" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">如果我们把ξ it 引入前面的方程，我们可以把它改写为</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/46537fac8deb4bc7b03cc50fb3b1c4cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*0N2rw2v2UFFGyjggQ5CxHw.png"/></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">Introduction of Xi</figcaption></figure><p id="8b17" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc">如果ξi= 0，</em> </strong></p><p id="a333" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc">这些点可以被认为是正确分类的。</em> </strong></p><p id="efd1" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc">其他:</em> </strong></p><p id="3f96" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc"> ξi &gt; 0，错误分类分。</em>T47】</strong></p><p id="6c3b" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">因此，如果ξi&gt; 0，这意味着 Xi(变量)位于不正确的维度，因此我们可以认为ξi 是与 Xi(变量)相关的误差项。平均误差可由下式给出:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/14df463f2e496afcf3903e8ddac9e899.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*xnM6vrvKPdcC0Ttex-5heQ.png"/></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">average error</figcaption></figure><p id="fb05" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">因此我们的目标，在数学上可以描述为；</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c4f5ee98c8ded0f5d062e89a4dbb2455.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/1*caObJmWU4iaMdzPNwMQ87Q.gif"/></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi np"><img src="../Images/9f8f49988191f8ce1528a348e7058bb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/1*7Bjnjsd3oXTzIDPPnF6tfw.gif"/></div></figure><p id="9c85" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">其中ξi = <em class="mc"> ςi </em></p><p id="d58e" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir">T53】</strong></p><p id="7e08" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">这种方法被称为软边界技术。</p><h1 id="dd2e" class="ju jv iq bd jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr bi translated">8.SVM 的损失函数解释:</h1><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nq"><img src="../Images/cdab30691ba36e82891e7642a18f28c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ubbw0tRM76K_RUproH6T6w.png"/></div></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ac7f6bb438921a96d43423e36786a778.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*RraE_BG_iPWPKqlBuDCbKw.png"/></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">when Zi is ≥ 1 then the loss is 0</figcaption></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/9b30fb44e36cfe10a1ed8222d7085eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*omD8a70fwte_045VvglNLA.png"/></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">when Zi &lt; 1 then loss increases.</figcaption></figure><p id="b887" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">因此，可以解释为铰链损耗最大(0，1-Zi)。</p><h1 id="856b" class="ju jv iq bd jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr bi translated">9.SVM 的双重形式:</h1><p id="7bae" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在，让我们考虑当我们的数据集根本不是线性可分的情况。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/fba228ec4074baed9ba77d92d2edd940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*FTCX4FA6ihoV2LDav2nxZw.png"/></div></figure><p id="cdad" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">基本上，我们可以像在逻辑回归中一样，通过向数据点添加相关的特征，将数据点投影到更高的维度中，从而分离每个数据点。但是有了 SVM，就有了一种强大的方法来完成将数据投射到更高维度的任务。上述公式是 SVMT3 的<strong class="ku ir"> <em class="mc">原形。另一种方法是对偶形式的 SVM，它使用<strong class="ku ir"> <em class="mc">拉格朗日乘数</em> </strong>来解决约束优化问题。</em></strong></p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nt"><img src="../Images/6a6edb5eff45230d2cd816de63de0cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/1*N7oeS_MX_47StziKd_vpxg.gif"/></div></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/61211d36687c6d02aa59d2b577048c0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/1*2wh3ziYg-fVMLP4ObNFJBQ.gif"/></div></figure><p id="fb07" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc">注:</em> </strong></p><p id="2498" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><strong class="ku ir"> <em class="mc">如果αi &gt; 0，则 Xi 是支持向量，当αi=0 时，则 Xi 不是支持向量。</em>T15】</strong></p><p id="688b" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">观察:</p><ol class=""><li id="ac94" class="nv nw iq ku b kv md kz me ld nx lh ny ll nz lp oa ob oc od bi translated">为了解决实际问题，我们不需要实际的数据点，而只需要每对向量之间的点积就足够了。</li><li id="01b6" class="nv nw iq ku b kv oe kz of ld og lh oh ll oi lp oa ob oc od bi translated">为了计算“b”偏差常数，我们只需要点积。</li><li id="6e4b" class="nv nw iq ku b kv oe kz of ld og lh oh ll oi lp oa ob oc od bi translated">SVM 对偶形式相对于拉格朗日公式的主要优点是它只依赖于<strong class="ku ir"> <em class="mc"> α </em> </strong>。</li></ol><h1 id="1571" class="ju jv iq bd jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr bi translated">10.什么是内核诡计？</h1><p id="1f58" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">即将来到 SVM 最著名的主要部分，<strong class="ku ir"> <em class="mc">内核绝招</em> </strong>。核是在某些(非常高维)特征空间中计算两个向量<strong class="ku ir"> x </strong>和<strong class="ku ir"> y </strong>的点积的一种方式，这就是为什么核函数有时被称为“广义点积”。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi oj"><img src="../Images/ff78f240373b20620ae588b05042a8d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/1*1qPw2RgmB5Y0R_6xtHH-pg.gif"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">try reading this equation…</figcaption></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/17b932a03978666b30b3766ab743dcf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/1*ZycBf2Z0uf1VEwn6MMe7zA.gif"/></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">s.t = subjected to</figcaption></figure><p id="ae61" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">应用核技巧意味着用核函数替换两个向量的点积。</p><h1 id="0dee" class="ju jv iq bd jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr bi translated">11.内核类型:</h1><ol class=""><li id="702d" class="nv nw iq ku b kv kw kz la ld ol lh om ll on lp oa ob oc od bi translated">线性核</li><li id="a6a2" class="nv nw iq ku b kv oe kz of ld og lh oh ll oi lp oa ob oc od bi translated">多项式核</li><li id="da86" class="nv nw iq ku b kv oe kz of ld og lh oh ll oi lp oa ob oc od bi translated">径向基函数核(RBF)/高斯核</li></ol><p id="23cd" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">我们将集中在多项式和高斯核，因为它是最常用的。</p><h2 id="b748" class="lq jv iq bd jw lr ls dn ka lt lu dp ke ld lv lw ki lh lx ly km ll lz ma kq mb bi translated">多项式内核:</h2><p id="b3d4" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">一般来说，多项式核定义为；</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/3e13b33b002d35f47d8d06293a873f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*Tt5V_m9iIwXc1xYoDLYmCA.png"/></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">b = degree of kernel &amp; a = constant term.</figcaption></figure><p id="4e6c" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">在多项式核中，我们简单地通过增加核的幂来计算点积。</p><p id="1b4b" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">示例:</p><p id="a113" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">假设最初 X 空间是二维的</p><p id="5e1b" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">Xa = (a1，a2)</p><p id="f5f3" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">Xb = (b1，b2)</p><p id="4ff7" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">现在，如果我们想将数据映射到更高维度，比如说在六维的 Z 空间中，看起来可能是这样的</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi op"><img src="../Images/829ce5ff3883aa73b87a05ae4c63c702.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*3wknOo82o-s51btrLU9Wyg.png"/></div></figure><p id="2d8f" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">为了求解这个对偶 SVM，我们需要(转置)Za ^t 和 Zb 的点积。</p><p id="296a" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">方法 1:</p><p id="92bd" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">传统上，我们会通过以下方式解决这个问题:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/13b2b47240fbb48c06c03504f3f75827.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*otd3-mR4GUY84GrkRg8ZRg.png"/></div></figure><p id="0635" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">这将花费很多时间，因为我们必须在每个数据点上执行点积，然后计算点积，我们可能需要做乘法，想象一下对数千个数据点这样做…</p><p id="d9bb" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">或者我们可以简单地使用</p><p id="de55" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">方法二:</p><p id="0e47" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">使用内核技巧:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi or"><img src="../Images/532ca28420e1e5d519684001f681218f.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*Ro7cu4K4vuAYGbPv2guivA.png"/></div></figure><p id="7bf4" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">在这种方法中，我们可以通过增加幂的值来简单地计算点积。简单不是吗？</p><h2 id="6529" class="lq jv iq bd jw lr ls dn ka lt lu dp ke ld lv lw ki lh lx ly km ll lz ma kq mb bi translated">径向基函数核(RBF)/高斯核:</h2><p id="e38f" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">高斯 RBF(径向基函数)是另一种流行的核方法，用于 SVM 模型。RBF 核是一个函数，它的值取决于到原点或某个点的距离。高斯核具有以下格式:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi os"><img src="../Images/fc318834cb96d4d4b1f652cf21054f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*jTU-kuAWMnMMYwBWj8mTVw.png"/></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">||X1 — X2 || = Euclidean distance between X1 &amp; X2</figcaption></figure><p id="b1af" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">使用原始空间中的距离，我们计算 X1 和 X2 的点积(相似性)。</p><p id="b431" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><em class="mc">注:相似度是两点之间的角距离。</em></p><h2 id="4bd8" class="lq jv iq bd jw lr ls dn ka lt lu dp ke ld lv lw ki lh lx ly km ll lz ma kq mb bi translated">参数:</h2><ol class=""><li id="85c0" class="nv nw iq ku b kv kw kz la ld ol lh om ll on lp oa ob oc od bi translated">c:正则化强度的倒数。</li></ol><p id="cb4e" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><em class="mc">行为:随着“c”值的增加，模型变得过拟合。</em></p><p id="17d9" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><em class="mc">随着“c”值的减小，模型的拟合度降低。</em></p><p id="292a" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated">2.γ:γ(仅用于 RBF 核)</p><p id="ef2b" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><em class="mc">行为:随着“</em> γ <em class="mc">”值的增加，模型变得过拟合。</em></p><p id="ab46" class="pw-post-body-paragraph ks kt iq ku b kv md kx ky kz me lb lc ld mf lf lg lh mg lj lk ll mh ln lo lp ij bi translated"><em class="mc">随着'</em> γ <em class="mc">的值减小，模型欠拟合。</em></p><h1 id="4e3f" class="ju jv iq bd jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr bi translated">12.SVM 的利与弊:</h1><h2 id="2d56" class="lq jv iq bd jw lr ls dn ka lt lu dp ke ld lv lw ki lh lx ly km ll lz ma kq mb bi translated">优点:</h2><ol class=""><li id="64cb" class="nv nw iq ku b kv kw kz la ld ol lh om ll on lp oa ob oc od bi translated">在更高维度真的很有效。</li><li id="65de" class="nv nw iq ku b kv oe kz of ld og lh oh ll oi lp oa ob oc od bi translated">当特征数多于训练样本数时有效。</li><li id="e18f" class="nv nw iq ku b kv oe kz of ld og lh oh ll oi lp oa ob oc od bi translated">类可分时的最佳算法</li><li id="2789" class="nv nw iq ku b kv oe kz of ld og lh oh ll oi lp oa ob oc od bi translated">超平面仅受支持向量的影响，因此异常值的影响较小。</li><li id="58a5" class="nv nw iq ku b kv oe kz of ld og lh oh ll oi lp oa ob oc od bi translated">SVM 适合极端情况下的二元分类。</li></ol><h2 id="4cea" class="lq jv iq bd jw lr ls dn ka lt lu dp ke ld lv lw ki lh lx ly km ll lz ma kq mb bi translated">缺点:</h2><ol class=""><li id="7945" class="nv nw iq ku b kv kw kz la ld ol lh om ll on lp oa ob oc od bi translated">对于较大的数据集，需要大量的时间来处理。</li><li id="1ebf" class="nv nw iq ku b kv oe kz of ld og lh oh ll oi lp oa ob oc od bi translated">在重叠类的情况下性能不佳。</li><li id="ed2c" class="nv nw iq ku b kv oe kz of ld og lh oh ll oi lp oa ob oc od bi translated">适当选择 SVM 的超参数，以获得足够的泛化性能。</li><li id="0798" class="nv nw iq ku b kv oe kz of ld og lh oh ll oi lp oa ob oc od bi translated">选择合适的内核函数可能很棘手。</li></ol><h1 id="e9b7" class="ju jv iq bd jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr bi translated">13.为 SVM 准备数据:</h1><h2 id="5230" class="lq jv iq bd jw lr ls dn ka lt lu dp ke ld lv lw ki lh lx ly km ll lz ma kq mb bi translated"><strong class="ak"> <em class="ot"> 1。数值转换:</em> </strong></h2><p id="34c6" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">SVM 假设你的输入是数字的，而不是分类的。所以你可以用一个最常用的"<a class="ae ou" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" rel="noopener ugc nofollow" target="_blank"><strong class="ku ir"><em class="mc">one hot encodin</em></strong></a><strong class="ku ir"><em class="mc">g、</em></strong><a class="ae ou" href="http://scikit-learn.org/stable/modules/preprocessing_targets.html#preprocessing-targets" rel="noopener ugc nofollow" target="_blank"><strong class="ku ir"><em class="mc">label-encoding</em></strong></a><strong class="ku ir"><em class="mc">etc</em></strong>"来转换它们。</p><h2 id="8065" class="lq jv iq bd jw lr ls dn ka lt lu dp ke ld lv lw ki lh lx ly km ll lz ma kq mb bi translated"><strong class="ak"> <em class="ot"> 2。二进制转换:</em> </strong></h2><p id="9f7f" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">由于 SVM 只能对二进制数据进行分类，所以您需要使用(<a class="ae ou" href="http://scikit-learn.org/stable/modules/multiclass.html#one-vs-the-rest" rel="noopener ugc nofollow" target="_blank"> <strong class="ku ir"> <em class="mc">一对一对其余的</em> </strong> </a> <strong class="ku ir"> <em class="mc">方法/ </em> </strong> <a class="ae ou" href="http://scikit-learn.org/stable/modules/multiclass.html#one-vs-one" rel="noopener ugc nofollow" target="_blank"> <strong class="ku ir"> <em class="mc">一对一对一方法</em> </strong> </a>)转换方法将多维数据集转换为二进制形式。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="df7c" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">14.模型应用:</h1><p id="56d3" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">由于这个帖子已经太长了，所以我想到把编码部分链接到我的 Github 账号(<a class="ae ou" href="https://github.com/Ajayay/SVM_on_AFFR" rel="noopener ugc nofollow" target="_blank"> <strong class="ku ir"> <em class="mc">这里</em> </strong> </a>)。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="669f" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">参考资料:</h1><ul class=""><li id="4a36" class="nv nw iq ku b kv kw kz la ld ol lh om ll on lp ov ob oc od bi translated"><a class="ae ou" href="http://cs229.stanford.edu/notes/cs229-notes3.pdf" rel="noopener ugc nofollow" target="_blank">吴恩达 SVM 笔记。</a></li><li id="93db" class="nv nw iq ku b kv oe kz of ld og lh oh ll oi lp ov ob oc od bi translated"><a class="ae ou" href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank">SVM 的 Sklearn 页面</a></li></ul></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="5a7a" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">导师:</h1><p id="c4c5" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Harshall Lamba，新潘韦尔皮莱工程学院助理教授。</p></div></div>    
</body>
</html>