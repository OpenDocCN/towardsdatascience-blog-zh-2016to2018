<html>
<head>
<title>A Year in Computer Vision — Part 2 of 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算机视觉一年——第 2 部分，共 4 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-year-in-computer-vision-part-2-of-4-893e18e12be0?source=collection_archive---------3-----------------------#2017-09-15">https://towardsdatascience.com/a-year-in-computer-vision-part-2-of-4-893e18e12be0?source=collection_archive---------3-----------------------#2017-09-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="66e5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">—第二部分:细分、超分辨率/色彩/风格转换、动作识别</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><blockquote class="km kn ko"><p id="148c" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">下面这篇文章摘自我们的研究团队最近编辑的关于计算机视觉领域的出版物。第一和第二部分目前可以通过我们的网站获得，其余部分(第三和第四部分)将在不久的将来发布。</p></blockquote><p id="b6ae" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated"><strong class="ks ir">未来几周将在我们的网站上免费提供完整的出版物，第 1-2 部分现在可以通过:</strong><a class="ae lp" href="http://www.themtank.org" rel="noopener ugc nofollow" target="_blank"><em class="kr">【www.themtank.org】</em></a>获得</p><p id="fa1f" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">我们鼓励读者通过我们自己的网站来查看这篇文章，因为我们包括嵌入的内容和简单的导航功能，使报告尽可能地动态。我们的网站不会给团队带来任何收入，只是为了让读者尽可能地感受到这些材料的吸引力和直观性。我们竭诚欢迎对演示的任何反馈！</p><blockquote class="km kn ko"><p id="43c5" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">请关注、分享和支持我们的工作，无论你喜欢的渠道是什么(请尽情鼓掌！).如有任何问题或想了解对未来作品的潜在贡献，请随时联系编辑:info@themtank.com</p></blockquote></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="4394" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">分割</h1><p id="6293" class="pw-post-body-paragraph kp kq iq ks b kt mi jr kv kw mj ju ky lm mk lb lc ln ml lf lg lo mm lj lk ll ij bi translated">计算机视觉的核心是分割过程，它将整个图像分成像素组，然后对这些像素组进行标记和分类。此外，语义分割更进一步，试图从语义上理解图像中每个像素的作用，例如，它是猫、汽车还是其他类型的类别？实例分割通过对不同的类实例进行分割，例如用三种不同的颜色标记三只不同的狗，从而进一步发展了这一点。这是目前自动驾驶技术套件中采用的大量计算机视觉应用之一。</p><p id="e0a4" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">也许，细分领域的一些最佳改进来自 FAIR，他们从 2015 年开始继续建立他们的 DeepMask 工作[46]。DeepMask 在对象上生成粗糙的“遮罩”,作为分割的初始形式。2016 年，Fair 推出了 SharpMask[47]，它细化了 DeepMask 提供的‘masks’，纠正了细节的损失，改善了语义分割。除此之外，MultiPathNet[48]还识别每个掩膜所描绘的对象。</p><blockquote class="mn"><p id="7119" class="mo mp iq bd mq mr ms mt mu mv mw ll dk translated">"<em class="mx">要捕捉一般的物体形状，你必须对你正在看的东西有一个高层次的理解(深度蒙版)，但是要精确地放置边界，你需要回顾低层次的特征，一直到像素(清晰度蒙版)。</em>”—Piotr Dollar，2016。[49]</p></blockquote><blockquote class="km kn ko"><p id="bdf6" class="kp kq kr ks b kt my jr kv kw mz ju ky kz na lb lc ld nb lf lg lh nc lj lk ll ij bi translated"><strong class="ks ir">图 6: </strong>展示公平技术的应用</p></blockquote><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nd"><img src="../Images/27025bed323f088e787bdf788e433761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*V6W7Qq1foM1lj5AL."/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk"><strong class="bd nt">Note</strong>: The above pictures demonstrate the segmentation techniques employed by FAIR. These include the application of DeepMask, SharpMask and MultiPathNet techniques which are applied in that order. This process allows accurate segmentation and classification in a variety of scenes. <strong class="bd nt">Source</strong>: Dollar (2016)[50]</figcaption></figure><p id="1a25" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated"><strong class="ks ir">视频传播网络</strong>【51】试图创建一个简单的模型，通过整个视频序列以及一些附加信息来传播在第一帧分配的精确对象遮罩。</p><p id="b3f6" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">2016 年，研究人员致力于寻找替代网络配置，以解决上述规模和本地化问题。DeepLab[52]就是这样一个例子，它为语义图像分割任务取得了令人鼓舞的结果。Khoreva 等人(2016 年)[53]基于 Deeplab 的早期工作(大约 2015 年)，提出了一种弱监督训练方法，其结果与完全监督网络相当。</p><p id="a3d5" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">计算机视觉通过使用端到端网络进一步完善了有用信息的网络共享方法，减少了多个全方位分类子任务的计算需求。使用这种方法的两篇关键论文是:</p><ul class=""><li id="8a9e" class="nu nv iq ks b kt ku kw kx lm nw ln nx lo ny ll nz oa ob oc bi translated"><strong class="ks ir"> 100 层提拉米苏</strong>【54】<strong class="ks ir"/>是一种全卷积 DenseNet，它以前馈方式将每一层与每一层连接起来。它还可以用更少的参数和训练/处理在多个基准数据集上实现 SOTA。</li><li id="3122" class="nu nv iq ks b kt od kw oe lm of ln og lo oh ll nz oa ob oc bi translated"><strong class="ks ir">全卷积实例感知语义分割</strong>【55】<strong class="ks ir"/>联合执行实例掩码预测和分类(两个子任务)。<br/> <strong class="ks ir"> COCO 分段挑战冠军</strong> MSRA。37.3% AP。<br/>从 2015 年 COCO 挑战赛 MSRAVC 绝对跃升 9.1%。</li></ul><p id="1bda" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">虽然用于实时语义分割的 DNN 架构 ENet<strong class="ks ir"/>【56】不属于这一类，但它确实证明了降低计算成本和为移动设备提供更多访问权限的商业价值。</p><blockquote class="km kn ko"><p id="d34e" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><em class="iq">我们的工作希望将尽可能多的这些进步与有形的公共应用联系起来。考虑到这一点，以下包含了 2016 年细分的一些最有趣的医疗保健应用；</em></p></blockquote><ul class=""><li id="dec9" class="nu nv iq ks b kt ku kw kx lm nw ln nx lo ny ll nz oa ob oc bi translated"><a class="ae lp" href="https://arxiv.org/abs/1612.00799" rel="noopener ugc nofollow" target="_blank">结肠镜图像腔内场景分割的基准</a>【57】</li><li id="73cb" class="nu nv iq ks b kt od kw oe lm of ln og lo oh ll nz oa ob oc bi translated"><a class="ae lp" href="https://arxiv.org/abs/1612.03925v1" rel="noopener ugc nofollow" target="_blank">用于 MRI 皮质下分割的 3D 全卷积网络:一项大规模研究</a>【58】</li><li id="8254" class="nu nv iq ks b kt od kw oe lm of ln og lo oh ll nz oa ob oc bi translated"><a class="ae lp" href="https://arxiv.org/abs/1611.08664v3" rel="noopener ugc nofollow" target="_blank">使用去噪自动编码器的半监督学习用于脑部损伤检测和分割</a>【59】</li><li id="63ad" class="nu nv iq ks b kt od kw oe lm of ln og lo oh ll nz oa ob oc bi translated"><a class="ae lp" href="https://arxiv.org/abs/1611.09811" rel="noopener ugc nofollow" target="_blank">三维超声图像分割综述</a>【60】</li><li id="590e" class="nu nv iq ks b kt od kw oe lm of ln og lo oh ll nz oa ob oc bi translated"><a class="ae lp" href="https://arxiv.org/abs/1611.02064" rel="noopener ugc nofollow" target="_blank">一种基于全卷积神经网络的结构化预测方法，用于视网膜血管分割</a>【61】</li><li id="6d80" class="nu nv iq ks b kt od kw oe lm of ln og lo oh ll nz oa ob oc bi translated"><a class="ae lp" href="https://arxiv.org/abs/1611.04534" rel="noopener ugc nofollow" target="_blank">用于胶质母细胞瘤分割的三维卷积神经网络</a>【62】</li></ul><p id="e66f" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">我们最喜欢的准医学分割应用之一是<strong class="ks ir">fusion net</strong>[63]——一种用于连接组学[64]中图像分割的深度完全残差卷积神经网络，以 SOTA 电子显微镜(EM)分割方法为基准。</p><figure class="ne nf ng nh gt ni"><div class="bz fp l di"><div class="oi oj l"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Semantic Segmentation applied to street views from a car</figcaption></figure><h1 id="8cc2" class="lq lr iq bd ls lt ok lv lw lx ol lz ma jw om jx mc jz on ka me kc oo kd mg mh bi translated">超高分辨率、风格转移和着色</h1><blockquote class="km kn ko"><p id="e2fb" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">并非所有的计算机视觉研究都是为了扩展机器的伪认知能力，通常，神经网络以及其他人工智能技术的虚构延展性有助于其他各种各样的新应用进入公共领域。去年在超分辨率、风格转换和色彩方面的进步占据了我们的空间。</p></blockquote><p id="34d8" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated"><strong class="ks ir">超分辨率</strong>指的是从低分辨率图像估计高分辨率图像的过程，也是预测不同放大倍数下的图像特征，这是人类大脑几乎可以毫不费力地做到的事情。最初，超分辨率是通过双三次插值和最近邻等简单技术实现的。就商业应用而言，克服源自源质量的低分辨率限制和实现“CSI Miami”风格的图像增强的愿望推动了该领域的研究。以下是今年的一些进展及其潜在影响:</p><ul class=""><li id="c8b0" class="nu nv iq ks b kt ku kw kx lm nw ln nx lo ny ll nz oa ob oc bi translated"><strong class="ks ir">Neural Enhance</strong>【65】是 Alex J. Champandard 的创意，结合了四篇不同研究论文的方法来实现其超分辨率方法。</li></ul><p id="7d89" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated"><strong class="ks ir">实时视频超分辨率</strong>在 2016 年也有两次值得注意的尝试；[66], [67]</p><ul class=""><li id="28ee" class="nu nv iq ks b kt ku kw kx lm nw ln nx lo ny ll nz oa ob oc bi translated"><strong class="ks ir"> RAISR: </strong>谷歌的快速准确图像超分辨率[68]通过用低分辨率和高分辨率图像对训练过滤器，避免了神经网络方法的高成本内存和速度要求。RAISR 作为一个基于学习的框架，比竞争算法快两个数量级，并且与基于神经网络的方法相比，具有最小的内存需求。因此，超分辨率可扩展到个人设备。这里有个研究博客<a class="ae lp" href="https://research.googleblog.com/2016/11/enhance-raisr-sharp-images-with-machine.html" rel="noopener ugc nofollow" target="_blank"/>。[69]</li></ul><blockquote class="km kn ko"><p id="e101" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><strong class="ks ir">图 7: </strong>超分辨率 SRGAN 示例</p></blockquote><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi op"><img src="../Images/03b20bec963660c2e38ea5a254df21cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VBb8lF_fyrITeq7C."/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk"><strong class="bd nt">Note</strong>: From left to right: bicubic interpolation (the objective worst performer for focus), Deep residual network optimised for MSE, deep residual generative adversarial network optimized for a loss more sensitive to human perception, original High Resolution (HR) image. Corresponding peak signal to noise ratio (PSNR) and structural similarity (SSIM) are shown in two brackets. [4 x upscaling] The reader may wish to zoom in on the middle two images (SRResNet and SRGAN) to see the difference between image smoothness vs more realistic fine details.<br/><strong class="bd nt">Source</strong>: Ledig et al. (2017) [70]</figcaption></figure><p id="351e" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">生成对抗网络(GANs)的使用代表了超分辨率的当前 SOTA:</p><ul class=""><li id="a830" class="nu nv iq ks b kt ku kw kx lm nw ln nx lo ny ll nz oa ob oc bi translated"><strong class="ks ir"> SRGAN </strong> [71]使用经过训练的鉴别器网络来区分超分辨率和原始照片级逼真图像，在公共基准上从严重欠采样的图像中提供照片级逼真纹理。</li></ul><p id="55cb" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">从质量上来说，SRGAN 表现最好，尽管 SRResNet 在峰值信噪比(PSNR)度量方面表现最好，但 SRGAN 获得了更精细的纹理细节，并获得了最好的平均意见得分(MOS)。据我们所知，这是第一个能够推断出 4 倍放大因子的照片级自然图像的框架【72】所有之前的方法都无法在大的放大因子下恢复更精细的纹理细节。</p><ul class=""><li id="bd5b" class="nu nv iq ks b kt ku kw kx lm nw ln nx lo ny ll nz oa ob oc bi translated"><strong class="ks ir">图像超分辨率的缓冲 MAP 推断</strong>【73】提出了一种使用卷积神经网络计算最大后验概率(MAP)推断的方法。然而，他们的研究提出了三种优化方法，目前 GANs 在真实图像数据上的表现都明显更好。</li></ul><blockquote class="km kn ko"><p id="a475" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><strong class="ks ir">图 8 </strong>:尼库林&amp;诺瓦克风格转移</p></blockquote><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi op"><img src="../Images/f8ca645ee766e16603566682deec7d18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3uZuhksmPZ7Ru2ZN."/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk"><strong class="bd nt">Note</strong>: Transferring different styles to a photo of a cat (original top left).<br/><strong class="bd nt">Source</strong>: Nikulin &amp; Novak (2016)</figcaption></figure><p id="47de" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">毫无疑问，<strong class="ks ir"> Style Transfer </strong>体现了神经网络的一种新用途，这种用途已经退入公共领域，特别是通过去年的 facebook 整合以及 Prisma [74]和 Artomatix [75]等公司。风格转移是一种更古老的技术，但在 2015 年随着艺术风格的神经算法的发表而转化为神经网络[76]。从那以后，风格转换的概念被 Nikulin 和 Novak [77]扩展，并应用于视频[78]，这是计算机视觉中的常见进展。</p><blockquote class="km kn ko"><p id="dde8" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><strong class="ks ir">图 9 </strong>:风格转移的进一步示例</p></blockquote><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi oq"><img src="../Images/54ed19242fc700adbc3a9f61901b9baa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3b1A6qC06MfH-Dzv."/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk"><strong class="bd nt">Note</strong>: The top row (left to right) represent the artistic style which is transposed onto the original images which are displayed in the first column (Woman, Golden Gate Bridge and Meadow Environment). Using conditional instance normalisation a single style transfer network can capture 32 style simultaneously, five of which are displayed here. The full suite of images in available in the source paper’s appendix. This work will feature in the International Conference on Learning Representations (ICLR) 2017. <br/><strong class="bd nt">Source</strong>: Dumoulin et al. (2017, p. 2) [79]</figcaption></figure><p id="fbae" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">一旦形象化，风格转移作为一个主题是相当直观的；拿一幅图像，想象它具有不同图像的风格特征。例如，以著名绘画或艺术家的风格。今年脸书发布了 Caffe2Go，[80]他们的深度学习系统集成到移动设备中。谷歌还发布了一些有趣的作品，试图融合多种风格来产生完全独特的图像风格:研究博客[81]和全文[82]。</p><p id="02ca" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">除了移动集成，风格转移在游戏资产的创建中也有应用。我们团队的成员最近看了 Artomatix 的创始人兼首席技术官 Eric Risser 的演示，他讨论了该技术在游戏内容生成方面的新颖应用(纹理突变等)。)并因此极大地减少了传统纹理艺术家的工作。</p><p id="e843" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated"><strong class="ks ir">着色</strong>是将单色图像转变为新的全彩色图像的过程。最初，这是由人们手工完成的，他们煞费苦心地选择颜色来代表每幅图像中的特定像素。2016 年，这一过程实现了自动化，同时保持了以人为中心的着色过程的真实感。虽然人类可能不能准确地表现给定场景的真实颜色，但是他们的真实世界知识允许以与图像和观看所述图像的另一个人一致的方式来应用颜色。</p><p id="0443" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">着色的过程很有趣，因为网络根据对物体位置、纹理和环境的理解为图像分配最可能的颜色，例如，它知道皮肤是粉红色的，天空是蓝色的。</p><blockquote class="km kn ko"><p id="42c9" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在我们看来，今年最有影响力的三部作品如下:</p></blockquote><ul class=""><li id="5f9d" class="nu nv iq ks b kt ku kw kx lm nw ln nx lo ny ll nz oa ob oc bi translated">张等人发明了一种方法，能够在 32%的实验中成功愚弄人类。他们的方法堪比“着色图灵测试”[83]</li><li id="5a85" class="nu nv iq ks b kt od kw oe lm of ln og lo oh ll nz oa ob oc bi translated">Larsson 等人[84]使用深度学习进行直方图估计，使他们的图像着色系统完全自动化。</li><li id="11cf" class="nu nv iq ks b kt od kw oe lm of ln og lo oh ll nz oa ob oc bi translated">最后，Lizuka、Simo-Serra 和 Ishikawa [85]展示了一个同样基于 CNN 的着色模型。这项工作超过了现有的 SOTA，我们[团队]觉得这项工作在质量上也是最好的，似乎是最现实的。图 10 提供了比较，但是图像取自 Lizuka 等人的文章。</li></ul><blockquote class="km kn ko"><p id="7f2c" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><strong class="ks ir">图 10 </strong>:着色研究对比</p></blockquote><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi or"><img src="../Images/274cf39482971b241d9076d350fac5af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IFFMhkJc9xwW_Fps."/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk"><strong class="bd nt">Note</strong>: From top to bottom — column one contains the original monochrome image input which is subsequently colourised through various techniques. The remaining columns display the results generated by other prominent colourisation research in 2016. When viewed from left to right, these are Larsson et al. [84] 2016 (column two), Zhang et al. [83] 2016 (Column three), and Lizuka, Simo-Serra and Ishikawa. [85] 2016, also referred to as “ours” by the authors (Column four). The quality difference in colourisation is most evident in row three (from the top) which depicts a group of young boys. We believe Lizuka et al.’s work to be qualitatively superior (Column four). <strong class="bd nt">Source</strong>: Lizuka et al. 2016 [86]</figcaption></figure><p id="36c2" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated"><em class="kr">此外，我们的架构可以处理任何分辨率的图像，不像大多数现有的基于 CNN 的方法。</em></p><p id="f409" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">在一项测试中，为了了解他们的色彩有多自然，用户从他们的模型中随机抽取一张图片，并被问及“你觉得这张图片看起来自然吗？”</p><p id="869d" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">他们的方法达到了 92.6%，基线达到了大约 70%，地面真实(实际的彩色照片)在 97.7%的时间里被认为是自然的。</p><h1 id="8367" class="lq lr iq bd ls lt ok lv lw lx ol lz ma jw om jx mc jz on ka me kc oo kd mg mh bi translated">动作识别</h1><p id="3feb" class="pw-post-body-paragraph kp kq iq ks b kt mi jr kv kw mj ju ky lm mk lb lc ln ml lf lg lo mm lj lk ll ij bi translated">动作识别的任务指的是给定视频帧内动作的分类，以及最近的算法，该算法可以预测动作发生前仅给定几帧的交互的可能结果。在这方面，我们看到最近的研究试图将上下文嵌入算法决策，类似于计算机视觉的其他领域。这一领域的一些重要论文包括:</p><ul class=""><li id="5efd" class="nu nv iq ks b kt ku kw kx lm nw ln nx lo ny ll nz oa ob oc bi translated"><strong class="ks ir">动作识别的长期时间卷积</strong>【87】利用人类动作的时空结构，即特定的运动和持续时间，使用 CNN 变体正确识别动作。为了克服细胞神经网络对长期动作的次优时间建模，作者提出了一种具有长期时间卷积的神经网络(LTC-CNN)来提高动作识别的准确性。简而言之，LTCs 可以查看视频的更大部分来识别动作。他们的方法使用并扩展了 3D CNNs,“能够在更完整的时间尺度上表现动作”。</li></ul><p id="f792" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">"<em class="kr">我们报告了两个具有挑战性的人体动作识别基准 UCF101 (92.7%)和 HMDB51 (67.2%)的最新结果。</em>”</p><ul class=""><li id="7453" class="nu nv iq ks b kt ku kw kx lm nw ln nx lo ny ll nz oa ob oc bi translated"><strong class="ks ir">用于视频动作识别的时空残差网络</strong> [88]将双流 CNN 的变体应用于动作识别任务，该网络结合了传统 CNN 方法和最近流行的残差网络(ResNets)的技术。双流方法的灵感来自视觉皮层功能的神经科学假设，即不同的路径识别物体的形状/颜色和运动。作者通过在两个 CNN 流之间注入剩余连接，结合了 ResNets 的分类优势。</li></ul><p id="fdc7" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">"<em class="kr">每个流最初独立执行视频识别，对于最终分类，softmax 分数通过后期融合进行组合。迄今为止，这种方法是将深度学习应用于动作识别的最有效方法，尤其是在训练数据有限的情况下。在我们的工作中，我们直接将图像转换为 3D 架构，并显示出比双流基线有很大提高的性能。</em>”—在 UCF101 上为 94%，在 HMDB51 上为 70.6%。Feichtenhofer 等人对传统的改进密集轨迹(iDT)方法进行了改进，并通过使用这两种技术产生了更好的结果。</p><ul class=""><li id="854b" class="nu nv iq ks b kt ku kw kx lm nw ln nx lo ny ll nz oa ob oc bi translated"><strong class="ks ir">从未标记的视频中预测视觉表示</strong>【89】是一篇有趣的论文，尽管不是严格的动作分类。该计划预测的行动，很可能会发生给定一系列的视频帧前一秒钟的行动。该方法使用视觉表示，而不是逐像素分类，这意味着通过利用深度神经网络的特征学习属性，程序可以在没有标记数据的情况下运行[90]。</li></ul><figure class="ne nf ng nh gt ni"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="7aa2" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">我们方法背后的关键思想是，我们可以训练深度网络来预测未来图像的视觉表现。视觉表示是一个有前途的预测目标，因为它们在比像素更高的语义级别上编码图像，但却是自动计算的。然后，我们将识别算法应用于我们预测的表示，以预测对象和动作。</p><p id="6e5c" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">Thumos 动作识别挑战赛的组织者发布了一篇论文，描述了过去几年中动作识别的一般方法。本文还概述了 2013 年至 2015 年的挑战、挑战的未来方向以及如何通过动作识别让计算机更全面地理解视频的想法。我们希望图莫斯行动认可挑战赛在(似乎)意外中断后于 2017 年回归。</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><blockquote class="km kn ko"><p id="d4d0" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><strong class="ks ir">跟随我们下期关于媒体的简介——第 3 部分，共 4 部分:走向对世界的 3D 理解。</strong></p><p id="25d3" class="kp kq kr ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">请随意将所有反馈和建议放在评论区，我们会尽快回复。或者，您可以通过以下方式直接联系我们:info@themtank.com</p></blockquote><p id="2a08" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">完整版本可在:<a class="ae lp" href="http://www.themtank.org/a-year-in-computer-vision" rel="noopener ugc nofollow" target="_blank">www.themtank.org/a-year-in-computer-vision</a>获得</p><p id="6ef1" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">非常感谢，</p><p id="5112" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">M 坦克</p><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div class="gh gi os"><img src="../Images/1e4ee110ffe53de7137a11f5580b117c.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*Q_zw5-Mu8eksQCveTK_4vQ.png"/></div></figure><h2 id="c806" class="ot lr iq bd ls ou ov dn lw ow ox dp ma lm oy oz mc ln pa pb me lo pc pd mg pe bi translated">按出现顺序排列的参考文献</h2><p id="4866" class="pw-post-body-paragraph kp kq iq ks b kt mi jr kv kw mj ju ky lm mk lb lc ln ml lf lg lo mm lj lk ll ij bi translated">[46]皮涅罗、科洛波特和多勒。2015.学习分割候选对象。<em class="kr">【在线】arXiv: 1506.06204 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1506.06204v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1506.06204 v2</strong></a></p><p id="52d8" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[47]皮涅罗等人，2016 年。学习细化对象段。<em class="kr">【在线】arXiv: 1603.08695 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1603.08695v2" rel="noopener ugc nofollow" target="_blank">T11】arXiv:1603.08695 v2T13】</a></p><p id="091b" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[48] Zagoruyko，S. 2016 年。用于目标检测的多路径网络。<em class="kr">【在线】arXiv: 1604.02135v2. </em>可用:<a class="ae lp" href="https://arxiv.org/abs/1604.02135v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1604.02135 v2</strong></a></p><p id="4332" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[49]美元，第 2016 页。学习分段。<em class="kr">【博客】交易会</em>。可用:<a class="ae lp" href="https://research.fb.com/learning-to-segment/" rel="noopener ugc nofollow" target="_blank">https://research.fb.com/learning-to-segment/</a></p><p id="c538" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[50]美元，第 2016 页。用 SharpMask 分割和细化图像。<em class="kr">【在线】脸书码</em>。可用:<a class="ae lp" href="https://code.facebook.com/posts/561187904071636/segmenting-and-refining-images-with-sharpmask/" rel="noopener ugc nofollow" target="_blank">https://code . Facebook . com/posts/561187904071636/segmented-and-refining-images-with-sharp mask/</a></p><p id="0cb1" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[51] Jampani 等人，2016 年。视频传播网络。<em class="kr">【在线】arXiv: 1612.05478 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1612.05478v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1612.05478 v2</strong></a></p><p id="4ce0" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[52]陈等，2016。DeepLab:使用深度卷积网络、阿特鲁卷积和全连接 CRF 的语义图像分割。<em class="kr">【在线】arXiv: 1606.00915 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1606.00915v1" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1606.00915 v1</strong></a></p><p id="96e5" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[53] Khoreva 等人，2016 年。简单易行:弱监督实例和语义分割。<em class="kr">【在线】arXiv: 1603.07485v2. </em>可用:<a class="ae lp" href="https://arxiv.org/abs/1603.07485v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1603.07485 v2</strong></a></p><p id="71cc" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[54]杰古等人，2016 年。一百层提拉米苏:用于语义分割的全卷积 DenseNets。<em class="kr">【在线】arXiv: 1611.09326v2. </em>可用:<a class="ae lp" href="https://arxiv.org/abs/1611.09326v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1611.09326 v2</strong></a></p><p id="f0a6" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[55]李等 2016。完全卷积的实例感知语义分割。<em class="kr">【在线】arXiv: 1611.07709v1 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1611.07709v1" rel="noopener ugc nofollow" target="_blank">T9】arXiv:1611.07709 v1T11】</a></p><p id="4b4d" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[56] Paszke 等人，2016 年。ENet:一种用于实时语义分割的深度神经网络架构。<em class="kr">【在线】arXiv: 1606.02147v1 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1606.02147v1" rel="noopener ugc nofollow" target="_blank">T15】arXiv:1606.02147 v1T17】</a></p><p id="c53e" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[57]巴斯克斯等人，2016 年。结肠镜图像腔内场景分割的基准。<em class="kr">【在线】arXiv: 1612.00799 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1612.00799v1" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1612.00799 v1</strong></a></p><p id="ffbf" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[58] Dolz 等人，2016 年。MRI 皮质下分割的 3D 全卷积网络:一项大规模研究。<em class="kr">【在线】arXiv: 1612.03925 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1612.03925v1" rel="noopener ugc nofollow" target="_blank">T27】arXiv:1612.03925 v1T29】</a></p><p id="baeb" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[59]亚历克斯等人，2017 年。使用去噪自动编码器的半监督学习用于脑损伤检测和分割。<em class="kr">【在线】arXiv: 1611.08664 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1611.08664v4" rel="noopener ugc nofollow" target="_blank">T33】arXiv:1611.08664 v4T35】</a></p><p id="534d" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[60]莫扎法里和李。2016.三维超声图像分割综述。<em class="kr">【在线】arXiv: 1611.09811 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1611.09811v1" rel="noopener ugc nofollow" target="_blank">T39】arXiv:1611.09811 v1T41】</a></p><p id="31e8" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">61 达斯古普塔和辛格。2016.基于全卷积神经网络的视网膜血管分割结构化预测方法。<em class="kr">【在线】arXiv: 1611.02064 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1611.02064v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1611.02064 v2</strong></a></p><p id="0918" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[62]易等 2016。用于胶质母细胞瘤分割的三维卷积神经网络。<em class="kr">【在线】arXiv: 1611.04534 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1611.04534v1" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1611.04534 v1</strong></a></p><p id="844a" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[63]全等 2016。FusionNet:用于连接组学中图像分割的深度全残差卷积神经网络。<em class="kr">【在线】arXiv: 1612.05360 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1612.05360v2" rel="noopener ugc nofollow" target="_blank">T3】arXiv:1612.05360 v2T5】</a></p><p id="e253" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[64]连接组学是指绘制生物神经系统内的所有连接，即神经元及其连接。</p><p id="c506" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[65]尚标准，美国法学家委员会，2017 年。神经增强(2016 年 11 月 30 日最新提交)。<em class="kr">【在线】Github </em>。可用:<a class="ae lp" href="https://github.com/alexjc/neural-enhance" rel="noopener ugc nofollow" target="_blank">https://github.com/alexjc/neural-enhance</a>【访问时间:2017 年 11 月 2 日】</p><p id="c5c9" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[66] Caballero 等人，2016 年。基于时空网络和运动补偿的实时视频超分辨率。<em class="kr">【在线】arXiv: 1611.05250 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1611.05250v1" rel="noopener ugc nofollow" target="_blank">T13】arXiv:1611.05250 v1T15】</a></p><p id="2ab8" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[67]石等，2016。使用有效的亚像素卷积神经网络的实时单幅图像和视频超分辨率。<em class="kr">【在线】arXiv: 1609.05158 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1609.05158v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1609.05158 v2</strong></a></p><p id="e459" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[68]罗马诺等人，2016 年。RAISR:快速准确的图像超分辨率。<em class="kr">【在线】arXiv: 1606.01299 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1606.01299v3" rel="noopener ugc nofollow" target="_blank">T25】arXiv:1606.01299 v3T27】</a></p><p id="d208" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[69]米兰法尔，第 2016 页。增强！通过机器学习提高清晰图像。<em class="kr">【博客】谷歌研究博客</em>。可用:<a class="ae lp" href="https://research.googleblog.com/2016/11/enhance-raisr-sharp-images-with-machine.html" rel="noopener ugc nofollow" target="_blank">https://research . Google blog . com/2016/11/enhance-raisr-sharp-images-with-machine . html</a>【访问时间:20/03/2017】。</p><p id="0984" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">70 同上</p><p id="7c12" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[71]莱迪格等人，2017 年。使用生成对抗网络的照片级单幅图像超分辨率。<em class="kr">【在线】arXiv: 1609.04802 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1609.04802v3" rel="noopener ugc nofollow" target="_blank">T35】arXiv:1609.04802 v3T37】</a></p><p id="9150" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">72 同上</p><p id="3e2a" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[73]s nderby 等人，2016 年。图像超分辨率的缓冲 MAP 推理。<em class="kr">【在线】arXiv: 1610.04490 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1610.04490v1" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1610.04490 v1</strong></a></p><p id="8cb5" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[74]普里斯马。2017.<em class="kr">【网址】Prisma </em>。可用:<a class="ae lp" href="https://prisma-ai.com/" rel="noopener ugc nofollow" target="_blank">https://prisma-ai.com/</a>【访问时间:2017 年 01 月 04 日】。</p><p id="41e9" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">75 arto matix。2017.<em class="kr">【网址】Artomatix </em>。可用:【https://services.artomatix.com/ T2】【访问时间:2017 年 1 月 4 日】。</p><p id="d489" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[76] Gatys 等人，2015 年。艺术风格的神经算法。<em class="kr">【在线】arXiv: 1508.06576 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1508.06576v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1508.06576 v2</strong>T9】</a></p><p id="cefc" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[77]尼库林&amp;诺瓦克。2016.探索艺术风格的神经算法。<em class="kr">【在线】arXiv: 1602.07188 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1602.07188v2" rel="noopener ugc nofollow" target="_blank">T13】arXiv:1602.07188 v2T15】</a></p><p id="6cd3" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[78] Ruder 等人，2016 年。视频的艺术风格转换。<em class="kr">【在线】arXiv: 1604.08610 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1604.08610v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1604.08610 v2</strong></a></p><p id="ac99" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">79 同上</p><p id="368f" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[80]贾和瓦杰达。2016.在您的手掌中交付实时人工智能。<em class="kr">【在线】脸书电码</em>。可用:<a class="ae lp" href="https://code.facebook.com/posts/196146247499076/delivering-real-time-ai-in-the-palm-of-your-hand/" rel="noopener ugc nofollow" target="_blank">https://code . Facebook . com/posts/196146247499076/delivering-real-time-in-the-palm-of-your-hand/</a>【访问时间:2017 年 1 月 20 日】。</p><p id="ab87" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[81] Dumoulin 等人，2016 年。增压风格转移。<em class="kr">【在线】谷歌研究博客</em>。可用:<a class="ae lp" href="https://research.googleblog.com/2016/10/supercharging-style-transfer.html" rel="noopener ugc nofollow" target="_blank">https://research . Google blog . com/2016/10/supercing-style-transfer . html</a>【访问时间:20/01/2017】。</p><p id="1b63" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[82] Dumoulin 等人，2017 年。艺术风格的学术表现。<em class="kr">【在线】arXiv: 1610.07629 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1610.07629v5" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1610.07629 V5</strong></a></p><p id="f7c9" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[83]张等，2016。彩色图像彩色化。<em class="kr">【在线】arXiv: 1603.08511 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1603.08511v5" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1603.08511 V5</strong></a></p><p id="92fc" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[84] Larsson 等人，2016 年。自动着色的学习表示。<em class="kr">【在线】arXiv: 1603.06668 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1603.06668v2" rel="noopener ugc nofollow" target="_blank">T45】arXiv:1603.06668 v2T47】</a></p><p id="4608" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[85] Lizuka，Simo-Serra 和 Ishikawa。2016.要有色彩！:用于具有同时分类的自动图像彩色化的全局和局部图像先验的联合端到端学习。<em class="kr">【在线】图形上的 ACM 事务(Proc。SIGGRAPH 的)，35(4):110 </em>。可用:【http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/ T2】</p><p id="26cd" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">86 同上</p><p id="9eaf" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[87] Varol 等人，2016 年。动作识别的长时卷积。<em class="kr">【在线】arXiv: 1604.04494 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1604.04494v1" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1604.04494 v1</strong></a></p><p id="e3c3" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[88] Feichtenhofer 等人，2016 年。用于视频动作识别的时空残差网络。<em class="kr">【在线】arXiv: 1611.02155 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1611.02155v1" rel="noopener ugc nofollow" target="_blank">T13】arXiv:1611.02155 v1T15】</a></p><p id="f4fe" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[89] Vondrick 等人，2016 年。预测来自未标记视频的视觉表示。<em class="kr">【在线】arXiv: 1504.08023 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1504.08023v2" rel="noopener ugc nofollow" target="_blank"><strong class="ks ir">arXiv:1504.08023 v2</strong></a></p><p id="33b8" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[90]康纳-西蒙斯，a .，戈登，R. 2016 年。教机器预测未来。<em class="kr">【在线】麻省理工新闻</em>。可用:<a class="ae lp" href="https://news.mit.edu/2016/teaching-machines-to-predict-the-future-0621" rel="noopener ugc nofollow" target="_blank">https://news . MIT . edu/2016/teaching-machines-to-predict-the-future-0621</a>【访问时间:03/02/2017】。</p><p id="41f7" class="pw-post-body-paragraph kp kq iq ks b kt ku jr kv kw kx ju ky lm la lb lc ln le lf lg lo li lj lk ll ij bi translated">[91] Idrees 等人，2016 年。“野外”视频动作识别的 THUMOS 挑战赛。<em class="kr">【在线】arXiv: 1604.06182 </em>。可用:<a class="ae lp" href="https://arxiv.org/abs/1604.06182v1" rel="noopener ugc nofollow" target="_blank">T29】arXiv:1604.06182 v1T31】</a></p></div></div>    
</body>
</html>