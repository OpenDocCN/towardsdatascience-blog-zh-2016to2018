# 克莱姆甘笔记

> 原文：<https://towardsdatascience.com/notes-on-the-cramer-gan-752abd505c00?source=collection_archive---------1----------------------->

以下讨论与最近的论文有关:

《克莱姆距离作为有偏瓦瑟斯坦梯度的解决方案》
[https://deep mind . com/research/publications/Cramer-Distance-Solution-Biased-瓦瑟斯坦梯度/](https://deepmind.com/research/publications/cramer-distance-solution-biased-wasserstein-gradients/)

这篇文章会有点长，所以我从一篇 TL 开始；博士首先，在大于 1 的维度上，*论文没有使用克莱姆距离！*在第五节的 GAN 实验中，使用了能量距离[【1】](#df62)。因此，该模型是使用特定内核的类似于[ [3](#30d4) ， [4](#b178) ]的生成矩匹配网络。由于能量距离是一个整数概率度量，由[ [5](#58fd) 提出的方案可以用于训练输入特征到内核，这比[ [3](#30d4) 改进了结果。我将在下面的第 1 节中对此进行解释。

总的来说，这是一个好主意，并且似乎给出了很好的生成器样本。不幸的是，Cramer GAN 的论文提出了一个有问题的近似，这意味着该算法中的 critic 不能正确地比较发生器和参考(目标)样本分布:*即使发生器和参考分布非常不同，您也可以获得零 critic 损耗*。我会在下面的第 2 节中解释这个问题。

提出类似想法并同时出现的论文[ [6](#fa12) ]似乎也取得了不错的效果，使用了不同的(高斯)核和深度特征。论文[ [7](#7c9a) ]也是相关的，并没有使用[ [5](#58fd) ]的优化，而是使用了方差控制方案，同样得到了很好的结果。

现在谈谈细节。

# 第一部分:能量距离，见证函数，梯度惩罚，核选择

论文中的批评者是能量距离[【1】](#df62)(在他们论文的第 5 页，作者声明，在生成多维样本 d > 1 时，他们使用能量距离，而不是克莱姆距离)。在[ [2](#d5c6) 中显示，这个距离属于积分概率度量(IPM)家族，就像 Wasserstein 距离一样(相比之下，KL 和反向 KL 发散是 f-发散，而不是 IPM)。具体而言，能量距离是最大平均差异(MMD)，首先用于训练[ [3](#30d4) 、 [4](#b178) ]中的生成模型(这些被称为生成矩匹配网络)。如果您不熟悉 MMD，请参见下面的[附录 B](#af2f) 获取简短介绍。IPM 公式是允许我们应用方法[ [5](#58fd) ]训练 GAN 的基本属性。

积分概率度量通过寻找平滑函数(“*见证函数*”)来测量概率分布之间的距离，该函数最大化两种分布下的期望差异。因此，如果 P 是您的生成器样本的分布，Q 是您的参考样本的分布，那么您将寻找一个平滑函数 f，其中 E_X f(X) — E_Y f(Y)很大，其中 X 来自 P，Y 来自 Q，E_X 表示 P 下的期望值。方法[ [5](#58fd) ]引入了一个惩罚，以在训练期间，在生成器和参考样本之间的点处，保持评论家见证函数的梯度接近 1。所以只要我们有了见证函数，就可以用[ [5](#58fd) ]。

能量距离的见证函数是什么？为了弄清楚这一点，我们使用了内核的最大均值差异这一事实:

k(x，x') = d(x，0) + d(x '，0) — d(x，x ')，(1)

其中 d(x，x’)是 x 和 x’(见[ [2](#d5c5) ，定义 13]，忽略 0.5 的因子)之间的距离，而“0”是原点(这实际上是中等字体中的零…)，使用这个核，以及[8，2.3 节]中的见证函数的表达式，我们可以推导出等式 1 上面的见证函数 f*(x)的表达式。5 在克莱姆甘论文(否则似乎出现在哪里！).我在下面证明了这个结果([附录 A](#2171) )。

正如[5]所建议的，你可以惩罚批评家损失中见证函数的梯度；或者可以使用[9]的早期“剪辑”方法，这是由[6]完成的，也工作得很好。

作者声称能量距离核可能是 GAN 训练的更好选择。我不相信这个论点:理论上

*   尚不清楚为什么和不变性和比例不变性对于 GAN 训练是必不可少的。和不变性也适用于任何平移不变核，包括高斯核(证明很简单)！事实上，策克利和里索已经证明了能量距离的和与标度不变性，所以不清楚为什么作者在命题 3 中重复这些早期的证明。
*   无偏梯度估计在更广的范围内是正确的，不仅仅是能量距离。在一定条件下，莱布尼茨规则意味着无偏估计量具有无偏梯度，MMD 对任何核都有无偏估计量[8]。

实验上，我也不相信这种核选择是该方法有效的主要原因，因为[6，7]使用不同的核，也获得了良好的结果([6]使用良好的旧高斯核)。另一方面，作者在他们的附录 D 中声称比高斯和拉普拉斯核更稳定的训练。可能[5]的方法是训练 MMD GAN 的关键，但是注意[6]使用削波，而[7]使用方差控制而不是惩罚梯度，因此[5]不是使其工作的唯一方法。

# 第二部分:批评家是不正确的

不幸的是，这篇论文做了一个有问题的近似，导致评论家不能正确地比较和匹配发电机和参考分布。这是可以解决的:[6]和[7]已经提出了不存在这个问题的方法，并且效果很好。

为了理解这个问题，可以将 x 和 x’视为来自发生器 P 的独立样本，y 和 y’视为来自参考分布 q 的独立样本，能量距离为:

D(P，Q) = E_{X，X'} d(X，X') + E_{Y，Y'} d(Y，Y') — E_{X，Y'} d(X，Y') — E_{X '，Y} d(X '，Y)

其中 E_{X，X'} d(X，X ')是来自生成器 P 的两个独立样本之间的预期距离(同样，Y 和 Y '是来自参考 Q 的独立样本)。让我们来理解这是什么意思:第一项是来自生成器的两个样本之间的平均距离。第二个是两个参考样本之间的平均距离。最后两项都给出了发生器和参考样本之间的平均距离。换句话说，如果来自发生器的样本与参考样本具有相同的分布，则所有项都将相同，并将抵消，当 P=Q 时，给出的能量距离为零。

我们现在转向由评论家在 Cramer GAN 论文的算法 1 中实现的距离。从算法 1 中的评论家见证 f(x)的定义，我们看到用于评论家的预期“代理损失”是:

D_c (P，Q) = E_{X，X'} d(X，X') + E_{Y} d(Y，0) — E_{X} d(X，0) — E_{X '，Y} d(X '，Y)

简而言之，有问题的近似是用原点 0 代替 y ’,在这一点上，批评家对 P 和 Q 之间距离的解释就失去了。很容易得出完全不同的 P 和 Q，但预期的临界损失为零。对于一个简单的一维示例，假设 P 将其所有样本放在原点，Q 将其所有样本放在距离原点 t 的位置。显然 P 和 Q 是不一样的，然而

E_{X，X'} d(X，X') = 0
E_{Y} d(Y，0) = t
E_{X} d(X，0) = 0
E_{X '，Y} d(X '，Y) = t

所以 D_c (P，Q) = 0，批评家误以为 P 和 Q 是相同的。很难预测这个问题会在论文的图 9 和图 10 这样的复杂例子中引起什么问题，但是对于更小尺寸的简单玩具例子，很快就会发现 P 和 Q 没有正确匹配。最后，请注意[6]和[7]不做这种近似，不受影响。

# 参考

[1]能量距离对应于一维中的克莱姆距离，但是我们不使用 GANs 来生成一维中的样本。参见策克利和里佐在他们 2004 年的论文“测试高维中的相等分布”。

[2] Sejdinovic，d .、Sriperumbudur，b .、Gretton，a .和 Fukumizu，k .，“假设检验中基于距离的统计和基于 RKHS 的统计的等效性”，《统计年鉴》，(2013 年)

[3] Dziugaite，G. K .，Roy，D. M .，和 Ghahramani，Z. (2015 年)。基于最大平均偏差优化的生成神经网络训练。子宫活动间期

[4]李，y .，斯维尔斯基，k .，泽梅尔，R. (2015)。生成矩匹配网络。ICML

[5]古尔拉贾尼、艾哈迈德、阿尔乔夫斯基、杜穆林和库维尔(2017 年)。改进了瓦瑟斯坦·甘斯的训练。arXiv 预印本 arXiv:1704.00028。

[6]李春林、张文春、程、杨和博佐斯(2017 年)。甘:加深对矩匹配网络的理解。arXiv 预印本 arXiv:1705.08584。

[7]优素福·姆鲁，汤姆·塞尔库，费希尔·甘，[https://arxiv.org/abs/1705.09675](https://arxiv.org/abs/1705.09675)

[8]a . Gretton，k . m . Borgwardt，m . Rasch，M. J .，schlkopf，b .和 Smola，A. (2012 年)。两样本核检验。2012 年，JMLR

[9]马丁·阿约夫斯基，苏史密斯·钦塔拉，莱昂·博图，瓦瑟斯坦·甘，[https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)

# 附录 A:见证函数的推导

根据[8，第 2.3 节]，见证函数为(直到比例常数):

f*(x) = E_X k(x，X) — E_Y k(x，Y)，(2)

其中 E_X k(x，X)表示内核相对于其参数之一的期望值，X 取自 P，Y 取自 q，将内核(1)代入第一项，我们得到:

E_X k(x，X)
= d(x，0) + E_X d(X，0) — E_X d(x，X)

上述表达式的第二项是常数。代入(2)，

f*(x) = d(x，0) — E_X d(x，X) — d(x，0) + E_Y d(x，Y) + C
= E_Y d(x，Y) — E_X d(x，X) + C

其中 C 是常数，可以忽略。这给出了期望的见证函数。

# 附录 B:MMD 概述

最大平均差异(MMD)是两个概率分布之间距离的简单度量[8]。在 GAN 情况下，P 定义为发电机分布，Q 定义为参考分布。那么 MMD 的平方就是:

MMD (P，Q) = E_{X，X'} k(X，X') + E_{Y，Y'} k(Y，Y') — E_{X，Y'} k(X，Y') — E_{X '，Y} k(X '，Y)

其中 k(x，X’)是 X 和 X’的“相似度”，E_{X，X’} k(X，X’)是来自生成器 p 的两个独立样本的期望“相似度”

我们如何解读这种距离？我们利用 k(x，x’)是一个核的事实，即 x 和 x’的特征的点积。当 x 和 x '相似时，它是大的，当它们不同时，它是小的。回到我们的 MMD 表达式，第一项是来自生成器的两个样本之间的平均相似性。第二个是两个参考样本之间的平均相似度。最后两项都给出了发生器和参考样本之间的平均相似性。换句话说，如果来自发生器的样本具有与参考样本相同的分布，那么所有的项都将是相同的，并且将被抵消，从而给出零 MMD。

MMD 是一种整数概率度量，就像沃瑟斯坦距离一样。该度量的见证函数在上面的附录 A 的等式(2)中给出。

关于使用什么内核:这是一个漫长而又传奇的问题！但是简单地说，一个众所周知的内核是“高斯”(指数平方)内核，

k(x，x') = exp (- d (x，x') * a)

其中 d (x，x’)是 x 和 x’之间的平方欧几里德距离，a 是宽度参数。另一个内核是情商。(1)在前面的文档中，这是用于获得能量距离的内核。这两种核都给出了有效的整数概率度量来衡量 P 和 q 之间的距离。还有许多其他选项，对于什么对 GANs 最好还没有明确的共识。