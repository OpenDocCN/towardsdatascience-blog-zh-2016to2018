<html>
<head>
<title>Dockerizing Airflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">停泊气流</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dockerizing-airflow-58a8888bd72d?source=collection_archive---------5-----------------------#2018-07-25">https://towardsdatascience.com/dockerizing-airflow-58a8888bd72d?source=collection_archive---------5-----------------------#2018-07-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6596" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">面向本地工作负载的 Docker 上的 Apache 气流</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6037d5ef85fba1f2463fa9d58e6aa0e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_gjHN5eycqXc_RMz"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@koushikc?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Koushik Chowdavarapu</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="9f36" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://airflow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Airflow </a>是大多数数据工程师工具箱中事实上的 ETL 编排工具。它为强大的后端提供了一个直观的 web 界面，以便为您的 ETL 工作流安排和管理依赖关系。</p><p id="f8c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我的日常工作流程中，我用它来维护和管理建立在<a class="ae kv" href="https://aws.amazon.com/s3/" rel="noopener ugc nofollow" target="_blank"> AWS S3 </a>之上的数据湖。我的 Airflow DAGs 中的节点包括多节点<a class="ae kv" href="https://aws.amazon.com/emr/" rel="noopener ugc nofollow" target="_blank"> EMR Apache Spark </a>和<a class="ae kv" href="https://aws.amazon.com/fargate/" rel="noopener ugc nofollow" target="_blank"> Fargate </a>集群，这些集群从数据湖中聚合、删减并产生副数据。</p><p id="00f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于这些工作流是在分布式集群(20 多个节点)上执行的，并且具有很强的依赖性(一个 ETL 的输出作为输入提供给下一个 ETL)，所以使用气流来编排它们是有意义的。然而，它没有意义，有一个中央气流部署，因为我将是唯一一个使用它。</p><p id="5747" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我选择<a class="ae kv" href="https://www.docker.com/" rel="noopener ugc nofollow" target="_blank">对接</a>气流，这样我就可以旋转容器，轻松运行这些<a class="ae kv" href="https://airflow.apache.org/concepts.html#workflows" rel="noopener ugc nofollow" target="_blank">工作流</a>，而不必担心气流部署。</p><p id="7e29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我将回顾一下我是如何做到这一点的，并简单解释一下设计过程中的一些决定。</p><h1 id="1e72" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">气流部件</h1><p id="dee4" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在气流 ETL <a class="ae kv" href="https://airflow.apache.org/concepts.html#workflows" rel="noopener ugc nofollow" target="_blank">中，工作流</a>被定义为<a class="ae kv" href="https://en.wikipedia.org/wiki/Directed_acyclic_graph" rel="noopener ugc nofollow" target="_blank">有向非循环图</a> ( <a class="ae kv" href="https://airflow.apache.org/concepts.html#dags" rel="noopener ugc nofollow" target="_blank">气流 DAG </a>)，其中每个节点都是独立的 ETL，每个下游节点都依赖于上游节点的成功完成。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/35883a1ff9ba71683105b67f2b82d62b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C0LeRf-Wolb9FV7l5cmASQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Simple Airflow DAG</figcaption></figure><p id="3b28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">气流有三个展开组件:</p><ul class=""><li id="385c" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">web 服务器(<a class="ae kv" href="http://flask.pocoo.org/" rel="noopener ugc nofollow" target="_blank"> Flask </a>后端用于触发和监控 Dag)</li><li id="ac8a" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">调度程序(调度和运行 DAG 执行程序的守护进程)</li><li id="f543" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">数据库(DAG 和 DAG 实例定义的前置层)</li></ul><h1 id="782a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">气流快速启动</h1><p id="1f5e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">用气流启动快速简单；</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="2e30" class="nj lt iq nf b gy nk nl l nm nn"># airflow needs a home, ~/airflow is the default,<br/># but you can lay foundation somewhere else if you prefer<br/># (optional)<br/>export AIRFLOW_HOME=~/airflow</span><span id="ad2b" class="nj lt iq nf b gy no nl l nm nn"># install from pypi using pip<br/>pip install apache-airflow</span><span id="3cde" class="nj lt iq nf b gy no nl l nm nn"># initialize the database<br/>airflow initdb</span><span id="ec91" class="nj lt iq nf b gy no nl l nm nn"># start the web server, default port is 8080<br/>airflow webserver -p 8080</span></pre><p id="801f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在运行这些命令时，Airflow 将创建一个<code class="fe np nq nr nf b">$AIRFLOW_HOME</code>文件夹，并放置一个<code class="fe np nq nr nf b">airflow.cfg</code>文件，其默认设置可以让您快速运行。您可以在<code class="fe np nq nr nf b">$AIRFLOW_HOME/airflow.cfg</code>中或者通过<code class="fe np nq nr nf b">Admin-&gt;Configuration</code>菜单中的 UI 来检查文件。网络服务器的 PID 文件将存储在<code class="fe np nq nr nf b">$AIRFLOW_HOME/airflow-webserver.pid</code>中，如果由 systemd 启动，则存储在<code class="fe np nq nr nf b">/run/airflow/webserver.pid</code>中。</p><p id="7bdd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">开箱即用，Airflow 使用一个 sqlite 数据库，您应该很快就能适应，因为使用这个数据库后端不可能实现并行化。它与只顺序运行任务实例的<code class="fe np nq nr nf b">SequentialExecutor</code>协同工作。虽然这非常有限，但它允许您快速启动并运行，浏览 UI 和命令行实用程序。</p><p id="3ef3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是几个将触发几个任务实例的命令。当您运行下面的命令时，您应该能够在<code class="fe np nq nr nf b">example1</code> DAG 中看到作业的状态变化。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="7c2f" class="nj lt iq nf b gy nk nl l nm nn"># run your first task instance<br/>airflow run example_bash_operator runme_0 2018-01-01<br/># run a backfill over 2 days<br/>airflow backfill example_bash_operator -s 2018-01-01 -e 2018-01-02</span></pre><h1 id="1526" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">停泊气流</h1><p id="0426" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">一个容器的主要运行过程是<code class="fe np nq nr nf b">ENTRYPOINT</code>和/或<code class="fe np nq nr nf b">Dockerfile</code>末端的<code class="fe np nq nr nf b">CMD</code>。通常建议您通过对每个容器使用一个服务来分隔关注的区域。</p><p id="2f63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，由于我们想让<code class="fe np nq nr nf b">Airflow Webserver</code> &amp;和<code class="fe np nq nr nf b">Airflow Scheduler</code>进程都运行，我们将使用<code class="fe np nq nr nf b">supervisord </code>作为进程管理器。</p><p id="068c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一种中等重量级的方法，要求您将<code class="fe np nq nr nf b">supervisord</code>及其配置打包到 docker 映像中(或者基于包含<code class="fe np nq nr nf b">supervisord</code>的映像)，以及它管理的不同应用程序。</p><p id="ca9f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后你启动<code class="fe np nq nr nf b">supervisord</code>，它为你管理你的进程。首先我们需要定义<code class="fe np nq nr nf b">supervisord.conf</code>:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="8969" class="nj lt iq nf b gy nk nl l nm nn">[supervisord]                       <br/>nodaemon=true</span><span id="aa79" class="nj lt iq nf b gy no nl l nm nn">[program:scheduler]                       <br/>command=airflow scheduler                       stdout_logfile=/var/log/supervisor/%(program_name)s.log                       stderr_logfile=/var/log/supervisor/%(program_name)s.log                       autorestart=true</span><span id="2d06" class="nj lt iq nf b gy no nl l nm nn">[program:server]                       <br/>command=airflow webserver -p 8080                       stdout_logfile=/var/log/supervisor/%(program_name)s.log                       stderr_logfile=/var/log/supervisor/%(program_name)s.log                       autorestart=true</span></pre><p id="fcab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们将使用<code class="fe np nq nr nf b">supervisord</code>作为 docker 文件的<code class="fe np nq nr nf b">ENTRYPOINT</code>:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="2ccc" class="nj lt iq nf b gy nk nl l nm nn">FROM python:3.6.3</span><span id="c45d" class="nj lt iq nf b gy no nl l nm nn"># supervisord setup                       <br/>RUN apt-get update &amp;&amp; apt-get install -y supervisor                       COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf</span><span id="f9f9" class="nj lt iq nf b gy no nl l nm nn"># Airflow setup                       <br/>ENV AIRFLOW_HOME=/app/airflow</span><span id="08d6" class="nj lt iq nf b gy no nl l nm nn">RUN pip install apache-airflow                       <br/>COPY /dags/response_rate_etl.py $AIRFLOW_HOME/dags/</span><span id="2543" class="nj lt iq nf b gy no nl l nm nn">RUN airflow initdb</span><span id="b89d" class="nj lt iq nf b gy no nl l nm nn">EXPOSE 8080</span><span id="ae8b" class="nj lt iq nf b gy no nl l nm nn">CMD ["/usr/bin/supervisord"]</span></pre><p id="29ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">构建 Docker 映像:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="7820" class="nj lt iq nf b gy nk nl l nm nn">docker build . -t airflow</span></pre><p id="bd10" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">运行容器:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="37ea" class="nj lt iq nf b gy nk nl l nm nn">docker run -d -p 8080:8080 --rm \<br/>   --name airflow_container \<br/>   airflow</span></pre><p id="6a69" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">启动 DAG:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="8eec" class="nj lt iq nf b gy nk nl l nm nn">docker exec airflow_container airflow trigger_dag example_bash_operator</span></pre><p id="1e98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">监控 DAG 运行:</p><p id="f435" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">打开浏览器并导航至<code class="fe np nq nr nf b">http://localhost:8080</code>以监控 DAG 实例运行。</p><p id="9364" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是所有人，请继续关注未来的帖子，在那里我将深入定义 AWS EMR dags，定义自定义气流操作符，注入 AWS 凭证等等！</p></div></div>    
</body>
</html>