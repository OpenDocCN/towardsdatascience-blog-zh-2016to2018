# 算法出错:算法的易错性和经济组织

> 原文：<https://towardsdatascience.com/to-err-is-algorithm-algorithmic-fallibility-and-economic-organisation-dbe18bb32abc?source=collection_archive---------4----------------------->

![](img/c48da7c9eb9a6c6e02abbcff4e2cbe5c.png)

*A precision-recall curve in a multi-label classification machine learning problem.*

# 算法失败

深入挖掘当今一些最大的技术争议，你可能会发现一个算法失灵:[【1】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_edn1)

*   [YouTube 广告争议](https://www.theguardian.com/technology/2017/mar/25/google-youtube-advertising-extremist-content-att-verizon):该算法将一些全球最大品牌的广告放在带有仇恨言论的视频上
*   [脸书视频争议](https://www.theguardian.com/technology/2017/apr/25/facebook-live-mark-zuckerberg-murder-video-thailand):该算法在其用户的 feeds 中发布暴力视频。
*   谷歌自动完成争议:该算法引导人们到新纳粹网站寻找关于大屠杀的信息

这些错误主要不是由数据中的问题引起的，这些问题会使算法具有歧视性，或者它们不能创造性地随机应变。不，它们源于更基本的东西:事实上，即使算法是基于无偏见的数据生成常规预测，它们也会出错。犯错是算法。

# 算法决策的成本和收益

我们不应该仅仅因为算法出错就停止使用它们。没有他们，许多受欢迎和有用的服务将无法生存。然而，我们需要认识到算法是容易出错的，它们的失败是有代价的。这指出了在更多(算法支持的)有益决策和更多(算法导致的)代价高昂的错误之间的重要权衡。平衡在哪里？

经济学是权衡的科学，为什么不像经济学家一样思考这个话题呢？这就是我在这篇博客之前所做的，创建了三个简单的经济学短文，着眼于算法决策的关键方面。这些是关键问题:

*   **风险**:什么时候我们应该把决策交给算法，这些算法需要有多精确？
*   **监督**:我们如何结合人类和机器的智能来达到预期的结果？
*   什么因素使我们能够并限制我们加速算法决策的能力？

接下来的两节给出了分析的要点及其含义。最后的附录更详细地描述了这些插图(有方程式！).

# 建模建模

## 风险:随波逐流

正如[美国心理学家和经济学家希尔伯特·西蒙曾经指出的](http://zeus.zeit.de/2007/39/simon.pdf)、*‘在一个信息丰富的世界里，注意力成为一种稀缺资源’*。这既适用于个人，也适用于组织。

正在进行的数据革命有可能超出我们处理信息和做出决策的能力，而算法可以帮助解决这一问题。它们是自动化决策的机器，有可能增加一个组织能够做出的好决策的数量。这解释了为什么他们首先在潜在决策的数量和频率超出人力处理能力的行业中起飞。[【6】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_edn6)

是什么推动了这一过程？对于一个经济学家来说，主要问题是算法的决策会创造多少价值。理性的组织会采用期望值高的算法。

算法的期望值取决于两个因素:它的准确性(它做出正确决策的概率)，以及正确决策的回报和错误决策的惩罚之间的平衡。[【7】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_edn7)高风险的决策(惩罚比奖励大)应该由高度精确的算法做出。你不会想要一个运行核电站的古怪机器人，但如果它只是建议你今晚看什么电视节目，那也没问题。

## 监督:小心

我们可以引入人类监督员来检查算法做出的决定，并修复他们发现的任何错误。如果算法不是非常准确(主管不会花很多时间检查正确的决策)，并且纠正错误决策的净收益(即额外的奖励加上避免的惩罚)很高，这就更有意义了。成本也很重要。一个理性的组织会有更多的激励去雇佣那些报酬不高，而且工作效率高的人(也就是说，只需要几个人就能完成工作)。

根据之前的例子，如果一个人类主管在一个电视网站上修复了一个愚蠢的推荐，这不太可能为所有者创造很多价值。核电站的情况完全不同。

## 规模:机器和现实之间的竞赛

当我们扩大算法决策的数量时会发生什么？它的增长有什么限制吗？

这取决于几个因素，包括算法在做出更多决策时是提高还是降低了准确性，以及加速算法决策的成本。在这种情况下，有两个有趣的比赛正在进行。

1.算法从其做出的决策中学习的能力与它从新决策中获得的信息量之间存在竞赛。新的机器学习技术有助于算法“从经验中学习”，使它们在做出更多决策时更加准确。然而，更多的决策也会降低算法的准确性。也许它被迫处理更奇怪的情况，或者它没有受过处理新情况的训练。[【9】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_edn9)更糟糕的是，当一种算法变得非常流行(做出更多决策)时，人们就有更多的理由去博弈它。

我的先验知识是，降低算法准确性的“熵力”最终将胜出:无论你收集了多少数据，都不可能对复杂、动态的现实做出完美的预测。

2.第二场竞赛是在创建算法的数据科学家和检查这些算法决策的主管之间进行的。数据科学家可能会“击败”人类监管者，因为他们的生产率更高:一个单一的算法，或一个算法的改进，可以在数百万个决策中扩大规模。相比之下，主管需要单独检查每个决定。这意味着，随着决策数量的增加，该组织的大部分劳动力成本将用于监督，随着监督过程变得越来越大、越来越复杂，成本可能会螺旋上升。

最后会发生什么？

综合考虑，我刚刚描述的算法准确性下降和劳动力成本上升，可能会限制一个组织能够经济地做出的算法决策的数量。但是这是否发生以及何时发生取决于具体情况。

# 对组织和政策的影响

我上面讨论的过程有许多有趣的组织和政策含义。以下是其中的一些:

## 1.找到合适的算法-领域匹配

正如我所说，在风险很高的情况下做出决策的算法需要非常准确，以弥补出错时的高额罚款。另一方面，如果犯错误的惩罚很低，即使是不准确的算法也可能完成任务。

例如，亚马逊或网飞等平台的推荐引擎经常会给出不相关的推荐，但这并不是一个大问题，因为这些错误的惩罚相对较低——我们只是忽略它们。数据科学家希拉里·帕克(Hillary Parker)在最近一期的“不那么标准偏差”[播客](https://soundcloud.com/nssd-podcast/episode-35-special-guest-sean-taylor)中提到，需要考虑模型准确性和决策环境之间的匹配性:

> *“大多数统计方法都已针对临床试验的实施进行了调整，在临床试验中，你谈论的是人们的生命和因错误治疗而死亡的人，而在商业环境中，权衡则完全不同”*

由此得出的一个结论是，在“低风险”环境中的组织可以试验新的和未经证实的算法，包括一些早期的低精度算法。随着这些改进，它们可以转移到“高风险领域”。开发这些算法的科技公司通常将它们作为开源软件发布，供其他人下载和改进，从而使这些溢出成为可能。

## 2.在高风险领域，算法决策是有局限性的

在错误造成的惩罚较高的领域，如卫生或刑事司法系统，以及在处理更容易受到算法错误影响的群体时，需要更加谨慎地应用算法。[【11】](http://www.nesta.org.uk/#_edn11)只有高度精确的算法才适合这些高风险的决策，除非有昂贵的能够发现并修复错误的人类监督人员作为补充。这将对算法决策产生自然的限制:你能雇佣多少人来检查更多的决策？人类的注意力仍然是更多决策的瓶颈。

如果政策制定者希望在这些领域更多、更好地使用算法，他们应该投资 R&D，以提高算法的准确性，鼓励采用其他领域的高性能算法，并尝试新的组织方式，帮助算法和它们的监管者更好地作为一个团队工作。

商业机构也不能幸免于这些问题:[例如，YouTube 已经开始屏蔽浏览量少于一万的视频中的广告](https://www.theverge.com/2017/4/6/15209220/youtube-partner-program-rule-change-monetize-ads-10000-views)。在这些视频中，正确的算法广告匹配的回报可能很低(他们的收视率很低)，惩罚可能很高(这些视频中有许多质量可疑)。换句话说，这些决定的期望值很低，所以 YouTube 决定停止做出这些决定。与此同时，脸书刚刚宣布，它正在招聘 3000 名人工监督员(几乎是其现有员工的五分之一)，以调节其网络中的内容。你可以想象监督更多决策的需求会如何限制其无限扩大算法决策的能力。

## 3.众包监管的利弊

保持低监督成本和高决策覆盖率的一个方法是[将监督众包给用户](https://www.theguardian.com/technology/2017/mar/22/facebook-fact-checking-tool-fake-news)，例如通过给他们工具来报告错误和问题。YouTube、脸书和谷歌都这样做了，以回应他们的算法争议。唉，让用户监管在线服务会让人感到不公平和不安。正如法学教授 Sarah T Roberts 在[最近一次关于脸书暴力视频争议的采访中指出的:](https://www.theguardian.com/technology/2017/apr/17/facebook-live-murder-crime-policy)

> 这种材料经常被打断的原因是因为像你我这样的人遇到了它。这意味着一大群人看到了它并标记了它，贡献了他们自己的劳动和未经同意的暴露给一些可怕的东西。我们将如何应对那些可能已经看到这一点并在今天受到创伤的社区成员？”

## 4.为什么你应该总是让一个人在循环中

即使错误的惩罚很低，让人类参与算法决策系统的循环仍然是有意义的。如果算法的准确性下降，他们的监督可以为性能的突然下降提供缓冲。当这种情况发生时，人类发现的错误决策的数量和修正它们的净收益都会增加。他们还可以敲响警钟，让每个人都知道算法有问题，需要修复。[【十三】](http://www.nesta.org.uk/#_edn13)

这在错误造成延迟惩罚或难以衡量或隐藏的惩罚的情况下尤其重要(比如错误的建议导致自我实现的预言，或在组织外部产生的成本)。

这样的例子很多。在 YouTube 广告争议中，之前错误的巨额累积罚款只是在一段时间后才变得明显，当时品牌注意到他们的广告是针对仇恨视频发布的。美国大选后关于假新闻的争议是难以衡量成本的一个例子:算法无法区分真实新闻和恶作剧给社会带来了成本，这可能证明更强有力的监管和更多的人类监督是合理的。政客们在英国大选前呼吁脸书加大打击假新闻的力度时已经表明了这一点[:](https://www.theguardian.com/technology/2017/apr/26/facebook-must-step-up-fake-news-fight-before-uk-election-urges-mp)

> *“看看到目前为止已经完成的一些工作，他们对他们可以获得的一些用户推荐的响应不够快或者根本不响应。他们能很快发现病毒传播。然后，他们应该能够检查这个故事是真是假，如果是假的，就屏蔽它，或者提醒人们这个事实是有争议的。不能只是用户提到故事的真实性。他们(脸书)必须判断一个故事是不是假的。”*

## 5.从抽象模型到真实系统

在我们使用经济模型为行动提供信息之前，我们需要定义和衡量模型的准确性、惩罚和奖励、环境波动导致的算法性能变化、监管水平及其成本，而这仅仅是个开始。[【14】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_edn14)

这是一项艰巨但重要的工作，可以利用现有的技术评估和评价工具，包括量化非经济成果的方法(例如在卫生领域)。人们甚至可以在实施算法决策之前，利用组织信息系统中的丰富数据来模拟算法决策及其组织的影响。我们看到了更多这类应用的例子，比如欧盟委员会正在运行的[金融‘reg tech’试点](http://www.coindesk.com/european-commission-proposes-blockchain-regtech-pilot/)，或者[最近一篇关于价格歧视的经济学人文章](http://www.economist.com/news/finance-and-economics/21721648-trustbusters-might-have-fight-algorithms-algorithms-price-bots-can-collude)中提到的‘共谋孵化器’。

# Coda:算法时代的渐进社会工程

在去年《自然》杂志的一篇文章中，美国研究人员 Ryan Calo 和 Kate Crawford 呼吁“*进行实用且广泛适用的社会系统分析，考虑人工智能系统对各方的所有可能影响*[借鉴]哲学、法律、社会学、人类学和科学技术研究，以及其他学科 T11。卡洛和克劳福德没有把经济学家列入他们的名单。然而，正如这篇博客所表明的，经济学思维对这些重要的分析和辩论有很大的贡献。从收益和成本的角度思考算法决策，我们可以用来管理其负面影响的组织设计，以及更多决策对算法创造的价值的影响，可以帮助我们在何时以及如何使用它们方面做出更好的决定。

这让我想起了杰伦·拉尼尔在他 2010 年的著作《谁拥有未来》中提出的一个观点:“随着时间的推移，经济学必须越来越多地关注调节人类社会行为的机器的设计。一个网络化的信息系统比政策更直接、更详细、更直接地指导人们。换句话说，经济学必须转变为大规模、系统化的用户界面设计。

设计让算法和人类一起做出更好决策的组织，将是这一议程的重要组成部分。

# 承认

*这篇博客得益于 Geoff Mulgan 的评论，灵感来自与约翰·戴维斯的对话。上图代表了多标签分类问题中的精确召回曲线。它显示了当一个人设置不同的规则(概率阈值)将观察结果放入一个类别时，随机森林分类算法出错的倾向。*

# 附录:关于算法决策的三篇经济学短文

下面的三个插图是算法决策情况的非常简化的形式。我的主要灵感来自于乔·斯蒂格利茨(Joe Stiglitz)和拉吉·萨(Raj Sah)在 1985 年发表的一篇论文《人类易犯错误与经济组织》(Human fallability and economic organization),该论文作者模拟了两种组织设计——层级制和“多层级制”(扁平化组织)——如何应对人类错误。他们的分析表明，在层级较低的决策者受到上级监督的层级组织中，倾向于拒绝更多好的项目，而在代理人相互独立决策的多党制组织中，倾向于接受更多坏的项目。从他们的模型中得到的一个关键教训是，错误是不可避免的，最佳的组织设计取决于环境。

## 小插曲 1:算法说也许

让我们想象一下，一家在线视频公司在其目录中匹配广告和视频。这家公司拥有数百万个视频，因此依靠人工来完成这项工作在经济上是不可行的。相反，其数据科学家开发算法来自动完成这项工作。[【16】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_edn16)公司寻找最大化匹配决策期望值的算法。这个值取决于三个因素:[【17】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_edn17)

**-算法精度(a *)*** :算法做出正确决策的概率(0 到 1 之间)。[【18】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_edn18)

**-决策奖励(** `**r**` ***)*** :这是算法做出正确决策时的奖励

**-错误惩罚(** `**p**` ***)*** :这就是做出错误决定的代价。

我们可以结合准确性、收益和惩罚来计算决策的期望值:

`*E = ar – (1-a)p [1]*`

当算法决策的预期收益大于预期成本(或风险)时，此值为正:

`*ar > (1-a)p [2]*`

这就相当于说:

`*a/(1-a) > p/r [3]*`

做出正确决定的几率应该高于惩罚与收益之比。

## 输入人类

我们可以通过引入一名人类主管来降低出错的风险。这位人类主管可以识别并修复算法决策中的错误。这种策略对决策期望值的影响取决于两个参数:

**-覆盖率(** `**k**` **):** k 是人工主管通过算法检查一个决策的概率。如果`k`是 1，这意味着所有的算法决策都由一个人来检查。

**-监督成本(** `**cs(k)**` **):** 这是监督算法决策的成本。成本取决于覆盖率`k`，因为检查更多决策需要时间。

有人工监督的算法决策的期望值如下:[【19】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_edn19)

`Es = ar + (1-a)kr – (1-a)kp – cs(k) [4]`

这个等式表明了这样一个事实，即一些错误被检测到并被纠正，而另一些则没有。我们从[4]中减去[3]以获得来自监管的额外期望值。经过一些代数运算，我们得到了这个。

`(r+p)(1-a)k > cs(k) [5]`

只有当监管的预期收益(取决于算法出错、错误被发现的概率，以及将错误转化为正确决策的净收益)大于监管成本时，监管才有经济意义。

## 按比例放大

在这里，我考虑当我们开始增加`n`时会发生什么，这是由算法做出的决策的数量。

预期值为:

`E(n) = nar + n(1-a)kr – n(1-a)(1-k)p [6]`

成本是`C(n)`

随着`n`的成长，这些东西是如何变化的？

我做了一些假设来简化事情:组织希望保持`k`不变，当 n 增加时，奖励`r`和惩罚`p`保持不变。[【20】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_edn20)

这留给我们两个随着`n`增加而变化的变量:`a`和`C`。

*   我假设算法的准确性随着决策数量的增加而下降，因为降低准确性的过程比提高准确性的过程更强
*   我假设`C`，生产成本，只取决于数据科学家和主管的劳动。这两种职业都有工资`wds`和`ws`。

在此基础上，通过一些计算，我们得到了随着我们做出更多决策，预期收益的变化:

`∂E(n)/∂(n) = r + (a+n(∂a/∂n))*(1-k)(r+p) - p(1-k) [7]`

这意味着，随着决策的增多，总的预期收益会随着算法边际精度的变化而增加。一方面，更多的决策意味着从更正确的决策中获得更大的利益。另一方面，准确性的下降会产生越来越多的错误和损失。其中一些被人类管理者抵消。

这就是成本的情况:

`∂C/∂n = (∂C/∂Lds)(∂Lds/∂n) + (∂C/dLs)(∂Ls/dn) [8]`

随着决策数量的增加，成本也会增加，因为组织必须招聘更多的数据科学家和主管。

[8]等于说:

`∂C/dn = wds/(∂Lds/dn) + ws/zs/(∂Ls/∂n) [9]`

每种职业的劳动力成本与其工资直接相关，与其边际生产率成反比。如果我们假设数据科学家比主管更有生产力，这意味着大部分成本的增加将由主管劳动力的增加引起。

组织决策的预期价值(收益减去成本)通过决策的均衡数量 ne 最大化，其中额外决策的边际价值等于其边际成本:

`r + (a+nda/dn)(1-k)(r+p) - p(1-k) = wds/(∂Lds/∂n) + ws/zs/(∂Ls/∂n) [10]`

## 扩展ˌ扩张

上面，我通过对每个被建模的情况做一些强有力的假设来保持事情的简单。如果我们放松这些假设，会发生什么？

以下是一些想法:

**各种错误:**首先，分析没有考虑不同类型的错误(例如，假阳性和假阴性，不同确定程度的错误等。)可能会有不同的奖励和惩罚。我还假设了奖励和惩罚的确定性，而将它们建模为从概率分布中随机抽取更为现实。这种扩展将有助于将公平和偏见纳入分析。例如，如果错误更有可能影响易受攻击的人(他们遭受更高的惩罚)，并且这些错误不太可能被检测到，这可能会增加错误的预期惩罚。

**人类也不是完美的:**以上所有假设算法会出错，但人类不会。事实显然并非如此。在许多领域，算法可能是带有根深蒂固的偏见和成见的人类的理想替代品。在这些情况下，人类检测和解决错误的能力受到损害，这降低了招募他们的动机(这相当于他们生产力的下降)。组织通过投资于技术(如众包平台)和质量保证系统(包括额外的人力和算法监督层)来处理所有这些问题，这些系统管理人类*和*算法失误的风险。

**非线性奖励和惩罚:**之前，我假设随着算法决策数量的增加，边际惩罚和奖励保持不变。情况不一定如此。下表显示了这些参数随决策数量变化的情况示例:

![](img/6e24d20037ccc741160e888eec6c5e7f.png)

获得这些过程的经验处理非常重要，因为它们可以确定一个组织在一个领域或市场中可以经济地做出的算法决策的数量是否有自然限制，这可能对其监管产生潜在影响。

# 尾注

[【1】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref1)我在狭义上使用“算法”一词，指的是将信息转化为预测的技术(取决于接收预测、决策的系统)。有许多过程可以做到这一点，包括基于规则的系统、统计系统、机器学习系统和人工智能(AI)。这些系统在准确性、可伸缩性、可解释性和从经验中学习的能力方面各不相同，因此在分析算法权衡时应该考虑它们的具体特征。

人们甚至可以说，机器学习是一门科学，它管理因无法消除算法错误而导致的权衡取舍。著名的“偏差-方差”权衡就是一个很好的例子。

[【3】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref3)有些人会说个性化是不可取的，因为它会导致歧视和“过滤泡沫”，但这是另一篇博文的问题。

[【4】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref4)[达尼·罗德里克的《经济学法则》](http://books.wwnorton.com/books/economics-rules/)为模型提供了一个令人信服的理由，认为它是复杂现实的简化但有用的形式。

在 2016 年《哈佛商业评论》的一篇文章中，Ajay Agrawal 和他的同事概述了机器学习作为一种降低预测成本的技术的经济分析。我看待算法的方式是相似的，因为预测是决策的输入。

[【6】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref6)这包括电子商务和社交网站中的个性化体验和推荐，或者金融中的欺诈检测和算法交易。

[【7】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref7)例如，如果 YouTube 向我展示了一个与我的兴趣高度相关的广告，我可能会购买该产品，这为广告商、视频制作者和 YouTube 带来了收入。如果它向我展示了一个完全不相关甚至是攻击性的广告，我可能会停止使用 YouTube，或者在我选择的社交网络上大吵大闹。

[【8】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref8)[强化学习](https://en.wikipedia.org/wiki/Reinforcement_learning)构建使用先前行为的奖励和惩罚来做出新决策的代理。

[【9】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref9)这就是谷歌 FluTrends 系统基于谷歌搜索预测流感爆发所发生的事情——[人们改变了他们的搜索行为，算法崩溃了](http://science.sciencemag.org/content/343/6176/1203)。

[【10】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref10)在许多情况下，惩罚可能是如此之高，以至于我们决定永远不要使用一种算法，除非它受到人类的监督。

[【11】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref11)不幸的是，在高风险的情况下实现算法系统时，并不总是小心谨慎。凯茜·奥尼尔的《数学毁灭的武器》举了很多这样的例子，从刑事司法系统到大学录取。

[【12】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref12)问责机制和正当程序是人为监督的另一个例子。

[【13】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref13)使用 Albert Hirschmann 的退出、声音和忠诚度模型，我们可以说[监督扮演了“声音”的角色](https://en.wikipedia.org/wiki/Exit,_Voice,_and_Loyalty)，帮助组织在用户开始退出之前发现质量下降。

附录标出了我的一些关键假设，并提出了扩展建议。

这包括使用随机对照试验方法对算法决策及其组织进行严格评估，如 Nesta 的[创新增长实验室](http://www.innovationgrowthlab.org/)提出的方法。

[【16】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref16)这一决定可能是基于类似的广告在与不同类型的视频匹配时的表现，基于观看视频的人的人口统计信息，或其他因素。

[【17】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref17)本博客中的分析假设算法决策的结果是相互独立的。在算法产生自我实现预言的情况下，这种假设可能会被违反(例如，从逻辑上讲，用户更有可能点击她看到的广告，而她不是)。这是一个很难解决的问题，但是研究人员正在开发基于随机算法决策的方法来解决这个问题。

[【18】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref18)这并不能区分不同类型的错误(如假阳性和假阴性)。我会在最后回到这个话题。

在这里，我假设人类管理者是完全准确的。正如我们从[行为经济学](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow)中所知，这是一个非常强有力的假设。我在最后考虑这个问题。

[【20】](http://www.nesta.org.uk/blog/err-algorithm-algorithmic-fallibility-and-economic-organisation#_ednref20)我考虑最后对边际奖惩做出不同假设的含义。