<html>
<head>
<title>Deep Learning with DigitalOcean: Redux</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用数字海洋进行深度学习:Redux</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-with-digitalocean-redux-e6f447e64c75?source=collection_archive---------6-----------------------#2017-11-29">https://towardsdatascience.com/deep-learning-with-digitalocean-redux-e6f447e64c75?source=collection_archive---------6-----------------------#2017-11-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/4bae042e1aca48c4f4c146c2b4f8c9b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*52EMe6ewlGY2xc_vrgvpCQ.jpeg"/></div></div></figure><div class=""/><p id="f5e0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这篇文章会和我平时的商务类文章有点不一样。在<a class="ae kw" href="https://medium.com/towards-data-science/deep-learning-on-the-digitalocean-stack-not-quite-yet-5c408e7d1a41" rel="noopener">最近的一篇文章</a>中，我给人的印象是数字海洋不是部署深度学习系统的好地方。<a class="ae kw" href="https://en.wikipedia.org/wiki/Redux_(literary_term)" rel="noopener ugc nofollow" target="_blank">然而</a>，有一些部署到DigitalOcean的良好用例可以节省成本和复杂性。</p><p id="b8bc" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最近，一个客户让我们在云中部署一个回归模型。训练是在K20 GPU上进行的。</p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="ab gu cl lb"><img src="../Images/03b7ef639bb73d1a5e1be2b87782f5cf.png" data-original-src="https://miro.medium.com/v2/format:webp/1*DlbsQJ1UFy2_jFqR6JXv3A.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">This is what the 1-click install button looks like in DigitalOcean</figcaption></figure><p id="dc29" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们尝试在一个2GB内存的新droplet上使用DigitalOcean的“机器学习和人工智能”一键安装。结果相当惊人。</p><p id="9204" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，我们用这个升级了tensorflow:</p><pre class="kx ky kz la gt lg lh li lj aw lk bi"><span id="f081" class="ll lm jb lh b gy ln lo l lp lq"><strong class="lh jc">pip install --upgrade tensorflow<br/></strong>#to resolve <em class="lr">ImportError: 'load_weights' requires h5py.</em><br/>pip3 install --upgrade h5py</span></pre><p id="1603" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要升级keras(可选),请使用以下命令:</p><pre class="kx ky kz la gt lg lh li lj aw lk bi"><span id="231d" class="ll lm jb lh b gy ln lo l lp lq"><strong class="lh jc">pip install keras --upgrade</strong></span></pre><p id="25d6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让代码在Keras 2上工作的现成问题。阅读文档的正确版本:【https://faroit.github.io/keras-docs/2.0.8/<a class="ae kw" href="https://faroit.github.io/keras-docs/2.0.8/" rel="noopener ugc nofollow" target="_blank"/>，用这个检查你的版本:</p><pre class="kx ky kz la gt lg lh li lj aw lk bi"><span id="66c0" class="ll lm jb lh b gy ln lo l lp lq">import keras<br/>print(keras.__version__)</span></pre><h1 id="d749" class="ls lm jb bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">数字海洋的执行时间结果</h1><p id="e711" class="pw-post-body-paragraph jy jz jb ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku kv ij bi translated">一个<strong class="ka jc">DNN(</strong><a class="ae kw" href="https://keras.io/getting-started/sequential-model-guide/" rel="noopener ugc nofollow" target="_blank"><strong class="ka jc">MLP</strong></a><strong class="ka jc">)</strong>在<strong class="ka jc"> 11.4秒</strong>内训练7层(针对10000个样本，20个输入，10个输出类)。对1000个样本的测试花费了<strong class="ka jc"> 968毫秒</strong>。真快。代码如下:</p><figure class="kx ky kz la gt is"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="eaca" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要计算1000个模型预测的时间，只需像这样:</p><figure class="kx ky kz la gt is"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="ced1" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一个<strong class="ka jc">【LSTM】(</strong><a class="ae kw" href="https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/" rel="noopener ugc nofollow" target="_blank"><strong class="ka jc">用于序列预测</strong> </a> <strong class="ka jc"> ) </strong>走得也真快。我们用这种模型来预测股票价格之类的东西。LSTMs的另一个有趣的用途是回归，但让我们坚持这个例子。该示例对于144个数据点花费了33.5秒。<strong class="ka jc">但是，</strong>模型预测(不包括训练)只用了<strong class="ka jc"> 16毫秒！</strong></p><figure class="kx ky kz la gt is gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/e2f381c3b7afd31ed10392001add050c.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*qqFSzq_GuCRt7befA8e_jw.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">The prediction output looks as expected for the LSTM. The real data (ground truth) is in blue, while the training result is in orange, and testing result is in green.</figcaption></figure><figure class="kx ky kz la gt is"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="a0bb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一个<strong class="ka jc">CNN(</strong><a class="ae kw" href="https://keras.io/applications/#vgg19" rel="noopener ugc nofollow" target="_blank"><strong class="ka jc">vgg 19</strong></a><strong class="ka jc">)</strong>2GB的内存不足以运行模型，所以我在8GB的实例上这样做。对于其他一些2GB的CNN，我同样得到了ResourceExhaustedError和其他垃圾。没什么大不了的。在8GB内存的情况下，它的工作非常出色。加载模型并获得一个结果需要8.6秒，但模型预测本身要快得多。</p><figure class="kx ky kz la gt is"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="0846" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">处理100张图像并对每张图像进行模型预测需要4分23秒。所以，以秒计，就是240+23 = 263秒。除以图像的数量，就是2.63秒。不太好。下面是时间代码:</p><figure class="kx ky kz la gt is"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="25d3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我的下一步是在word2vec (Google news)中嵌入一个单词模型，但是，唉，我必须回到真正的工作中去了。嵌入模型有一个类似CNN的内存大小问题。</p><p id="719b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">数字海洋的成本结果</strong></p><p id="062a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在AWS上，在2个20美元/月的实例上运行DNN和LSTM要比1千美元/月的p2或p3实例便宜得多。对于这些较小的模型来说，这很有意义。对于更大的CNN和word2vec模型，没有足够的计算(没有GPU)或RAM(对于大型嵌入模型)来使DigitalOcean具有吸引力。这可能会改变，例如，如果有一种方法可以从SSD而不是RAM加载word2vec模型。我是说，为什么不呢？它基本上只是一个大的记忆垫。这就解决了单词嵌入的问题，但是CNN呢？那是一个更难的问题。我认为对于图像处理来说，GPU在未来很长一段时间内仍将是顶级产品。</p><p id="73b2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你喜欢这篇关于云中深度学习的文章，那么请尝试一下<strong class="ka jc"> clap工具</strong>。轻点那个。跟着我们走。分享这篇文章的链接。去吧。我也很高兴在评论中听到你的反馈。你怎么想呢?</p><p id="01fc" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你喜欢这篇文章，可以看看我过去读过最多的文章，比如“<a class="ae kw" href="https://medium.com/towards-data-science/how-to-price-an-ai-project-f7270cb630a4" rel="noopener">如何给人工智能项目定价</a>”和“<a class="ae kw" href="https://medium.com/towards-data-science/why-hire-an-ai-consultant-50e155e17b39" rel="noopener">如何聘请人工智能顾问</a>”除了与业务相关的文章，我还准备了一些关于寻求采用深度机器学习的公司所面临的其他问题的文章，如“<a class="ae kw" href="https://medium.com/@lemaysolutions/locked-in-a-box-machine-learning-without-cloud-or-apis-76cc54e391c8" rel="noopener">没有云和API的机器学习</a>”</p><p id="77b4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">编码快乐！</p><p id="e8c7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">-丹尼尔<br/> <a class="ae kw" href="mailto:daniel@lemay.ai" rel="noopener ugc nofollow" target="_blank">丹尼尔@lemay.ai </a> ←打个招呼。<br/><a class="ae kw" href="https://lemay.ai" rel="noopener ugc nofollow" target="_blank">LEMAY . AI</a><br/>1(855)LEMAY-AI</p><p id="8427" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可能喜欢的其他文章:</p><ul class=""><li id="167e" class="mx my jb ka b kb kc kf kg kj mz kn na kr nb kv nc nd ne nf bi translated"><a class="ae kw" rel="noopener" target="_blank" href="/artificial-intelligence-and-bad-data-fbf2564c541a">人工智能和不良数据</a></li><li id="d59c" class="mx my jb ka b kb ng kf nh kj ni kn nj kr nk kv nc nd ne nf bi translated"><a class="ae kw" rel="noopener" target="_blank" href="/artificial-intelligence-hyperparameters-48fa29daa516">人工智能:超参数</a></li><li id="4172" class="mx my jb ka b kb ng kf nh kj ni kn nj kr nk kv nc nd ne nf bi translated"><a class="ae kw" href="https://medium.com/towards-data-science/artificial-intelligence-get-your-users-to-label-your-data-b5fa7c0c9e00" rel="noopener">人工智能:让你的用户给你的数据贴上标签</a></li></ul></div></div>    
</body>
</html>