<html>
<head>
<title>Predicting the Survival of Titanic Passengers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测泰坦尼克号乘客的生存</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8?source=collection_archive---------1-----------------------#2018-05-14">https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8?source=collection_archive---------1-----------------------#2018-05-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/7c099d5a2f119cb99a3a0838ced728af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LcmCEzQN8FvkJ6KpzJmIBA.jpeg"/></div></div></figure><p id="2c99" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇博文中，我将介绍在著名的 Titanic 数据集上创建机器学习模型的整个过程，这个数据集被世界各地的许多人使用。它提供了泰坦尼克号上乘客的命运信息，按照经济地位(阶级)、性别、年龄和存活率进行汇总。</p><p id="35a7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我最初在 kaggle.com 上写这篇文章，作为“泰坦尼克号:从灾难中学习的机器”竞赛的一部分。在这个挑战中，我们被要求预测泰坦尼克号上的一名乘客是否会生还。</p><h1 id="1f7f" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">皇家邮轮泰坦尼克号</h1><blockquote class="lv lw lx"><p id="cee1" class="jy jz ly ka b kb kc kd ke kf kg kh ki lz kk kl km ma ko kp kq mb ks kt ku kv ij bi translated">皇家邮轮泰坦尼克号是一艘英国客轮，1912 年 4 月 15 日凌晨在从南安普敦到纽约市的处女航中与冰山相撞后沉没在北大西洋。据估计，船上有 2224 名乘客和船员，超过 1500 人死亡，这是现代史上最致命的和平时期商业海上灾难之一。皇家邮轮泰坦尼克号是当时最大的水上船只，也是白星航运公司运营的三艘奥运级远洋客轮中的第二艘。泰坦尼克号由贝尔法斯特的哈兰和沃尔夫造船厂建造。她的建筑师托马斯·安德鲁斯死于这场灾难。</p></blockquote><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mc"><img src="../Images/1c20f7a1fa798a41a89e08a528bf3c78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePbfZdw6sz397xLWlFZLCQ.jpeg"/></div></div></figure><h1 id="8072" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">导入库</h1><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="a708" class="mm ky iq mi b gy mn mo l mp mq"><em class="ly"># linear algebra</em><br/><strong class="mi ir">import</strong> <strong class="mi ir">numpy</strong> <strong class="mi ir">as</strong> <strong class="mi ir">np</strong> <br/><br/><em class="ly"># data processing</em><br/><strong class="mi ir">import</strong> <strong class="mi ir">pandas</strong> <strong class="mi ir">as</strong> <strong class="mi ir">pd</strong> <br/><br/><em class="ly"># data visualization</em><br/><strong class="mi ir">import</strong> <strong class="mi ir">seaborn</strong> <strong class="mi ir">as</strong> <strong class="mi ir">sns</strong><br/>%matplotlib inline<br/><strong class="mi ir">from</strong> <strong class="mi ir">matplotlib</strong> <strong class="mi ir">import</strong> pyplot <strong class="mi ir">as</strong> plt<br/><strong class="mi ir">from</strong> <strong class="mi ir">matplotlib</strong> <strong class="mi ir">import</strong> style<br/><br/><em class="ly"># Algorithms</em><br/><strong class="mi ir">from</strong> <strong class="mi ir">sklearn</strong> <strong class="mi ir">import</strong> linear_model<br/><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.linear_model</strong> <strong class="mi ir">import</strong> LogisticRegression<br/><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.ensemble</strong> <strong class="mi ir">import</strong> RandomForestClassifier<br/><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.linear_model</strong> <strong class="mi ir">import</strong> Perceptron<br/><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.linear_model</strong> <strong class="mi ir">import</strong> SGDClassifier<br/><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.tree</strong> <strong class="mi ir">import</strong> DecisionTreeClassifier<br/><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.neighbors</strong> <strong class="mi ir">import</strong> KNeighborsClassifier<br/><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.svm</strong> <strong class="mi ir">import</strong> SVC, LinearSVC<br/><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.naive_bayes</strong> <strong class="mi ir">import</strong> GaussianNB</span></pre><h1 id="69d0" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">获取数据</h1><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="9841" class="mm ky iq mi b gy mn mo l mp mq">test_df = pd.read_csv("test.csv")<br/>train_df = pd.read_csv("train.csv")</span></pre><h1 id="43da" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">数据探索/分析</h1><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="2da2" class="mm ky iq mi b gy mn mo l mp mq">train_df.info()</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mr"><img src="../Images/dd65469e41e984c1ecb660de8fd68ba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qDR-vH5il2Qccu84_qUnEQ.png"/></div></div></figure><p id="a371" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">训练集有 891 个样本，11 个特征+目标变量(存活)</strong>。其中 2 个特性是浮点数，5 个是整数，5 个是对象。下面我列出了这些特性并做了简短的描述:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="71ad" class="mm ky iq mi b gy mn mo l mp mq">survival:    Survival <br/>PassengerId: Unique Id of a passenger. <br/>pclass:    Ticket class     <br/>sex:    Sex     <br/>Age:    Age in years     <br/>sibsp:    # of siblings / spouses aboard the Titanic     <br/>parch:    # of parents / children aboard the Titanic     <br/>ticket:    Ticket number     <br/>fare:    Passenger fare     <br/>cabin:    Cabin number     <br/>embarked:    Port of Embarkation</span><span id="e5b8" class="mm ky iq mi b gy ms mo l mp mq">train_df.describe()</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/98088e9ea3494e866d1cccad8726de1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ES0C_z1iVki49edmPGMlUQ.png"/></div></div></figure><p id="388c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从上面我们可以看到<strong class="ka ir"> 38%的训练组成员在泰坦尼克号</strong>中幸存。我们还可以看到乘客年龄从 0.4 岁到 80 岁不等。除此之外，我们已经可以检测到一些包含缺失值的特征，比如“年龄”特征。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="391e" class="mm ky iq mi b gy mn mo l mp mq">train_df.head(8)</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mu"><img src="../Images/e9fc0e94eecdff68092a5f1b6c18e9f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JtEf9FVlJqxRDhhu0ec3fw.png"/></div></div></figure><p id="64b0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从上表中，我们可以注意到一些事情。首先，我们<strong class="ka ir">需要稍后将大量特征转换为数字</strong>特征，以便机器学习算法可以处理它们。此外，我们可以看到<strong class="ka ir">特性具有非常不同的范围</strong>，我们需要将其转换成大致相同的比例。我们还可以发现更多的包含缺失值(NaN =非数字)的特性，我们需要处理这些特性。</p><p id="d40e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">让我们更详细地看看实际上缺少什么数据:</strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="caf9" class="mm ky iq mi b gy mn mo l mp mq">total = train_df.isnull().sum().sort_values(ascending=<strong class="mi ir">False</strong>)<br/>percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100<br/>percent_2 = (round(percent_1, 1)).sort_values(ascending=<strong class="mi ir">False</strong>)<br/>missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])<br/>missing_data.head(5)</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mv"><img src="../Images/cb92cd83550f20752b1db3262f8a3c36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eKBSjbBB5NJtIkZwsGD9Lw.png"/></div></div></figure><p id="aed9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">已装载特征只有两个缺失值，可以很容易地填充。处理“年龄”特性要复杂得多，它缺少 177 个值。“小屋”功能需要进一步调查，但看起来我们可能要从数据集中删除它，因为它的 77 %丢失了。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="9f2d" class="mm ky iq mi b gy mn mo l mp mq">train_df.columns.values</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/1542891eeda066bea8bd484ee9b10e4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UAXTsHwWQmOjGfeei5z5UA.png"/></div></div></figure><p id="c0ea" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">上面你可以看到 11 个特征+目标变量(幸存)。哪些特征有助于提高存活率？</p><p id="62ba" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对我来说，如果除了“乘客 Id”、“车票”和“姓名”之外的一切都与高存活率相关联，那就说得通了。</p><p id="cb65" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 1。年龄性别:</strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="d5a5" class="mm ky iq mi b gy mn mo l mp mq">survived = 'survived'<br/>not_survived = 'not survived'<br/>fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))<br/>women = train_df[train_df['Sex']=='female']<br/>men = train_df[train_df['Sex']=='male']<br/>ax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =<strong class="mi ir">False</strong>)<br/>ax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =<strong class="mi ir">False</strong>)<br/>ax.legend()<br/>ax.set_title('Female')<br/>ax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = <strong class="mi ir">False</strong>)<br/>ax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = <strong class="mi ir">False</strong>)<br/>ax.legend()<br/>_ = ax.set_title('Male')</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mx"><img src="../Images/63781cfe69bf1c922f648c9324cb2e85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q5gn0qatvlkn7HRFy3b7Wg.png"/></div></div></figure><p id="b0ce" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你可以看到男性在 18 岁到 30 岁之间存活的概率很大，女性也是一点点但不完全对。对于 14 至 40 岁的女性来说，存活的几率更高。</p><p id="1519" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于男性来说，在 5 岁到 18 岁之间存活的概率非常低，但对于女性来说并非如此。另一件要注意的事情是，婴儿也有更高的存活率。</p><p id="71cf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于似乎有<strong class="ka ir">特定的年龄，这增加了生存的几率</strong>，并且因为我希望每个特征都大致在相同的范围内，我将稍后创建年龄组。</p><p id="ac52" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 3。登船，Pclass 和 Sex: </strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="d5f3" class="mm ky iq mi b gy mn mo l mp mq">FacetGrid = sns.FacetGrid(train_df, row='Embarked', size=4.5, aspect=1.6)<br/>FacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=<strong class="mi ir">None</strong>,  order=<strong class="mi ir">None</strong>, hue_order=<strong class="mi ir">None</strong> )<br/>FacetGrid.add_legend()</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi my"><img src="../Images/aaf853f64b290251bc8c13224f16fb50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u33PV0cM0Q9t0jCHSU2yTg.png"/></div></div></figure><p id="8d4c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">上船似乎与生存相关，取决于性别。</p><p id="eb64" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Q 端口和 S 端口的女性生存几率更高。反之亦然，如果他们在港口 C。男人有很高的生存概率，如果他们在港口 C，但低概率，如果他们在港口 Q 或 s。</p><p id="ea40" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Pclass 似乎也与存活率相关。我们将在下面生成另一个图。</p><p id="7420" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 4。Pclass: </strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="030e" class="mm ky iq mi b gy mn mo l mp mq">sns.barplot(x='Pclass', y='Survived', data=train_df)</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mz"><img src="../Images/5b21d3c3068a655cf0ad8017aa999da6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y0v-vT-Dk0Eb46x6xVjAaw.png"/></div></div></figure><p id="c9e2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里我们清楚地看到，Pclass 增加了一个人的生存机会，特别是如果这个人在 1 类。我们将在下面创建另一个 pclass 图。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="ef46" class="mm ky iq mi b gy mn mo l mp mq">grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)<br/>grid.map(plt.hist, 'Age', alpha=.5, bins=20)<br/>grid.add_legend();</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi na"><img src="../Images/f4a08736ee0a0106a9929eb805fdc4e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cp6Xk9PHDOnIHwqQd4sJ5Q.png"/></div></div></figure><p id="109b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">上面的图证实了我们对 pclass 1 的假设，但我们也可以发现 pclass 3 中的人很有可能无法存活。</p><p id="1f18" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 5。SibSp 和 Parch: </strong></p><p id="5432" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">SibSp 和 Parch 作为一个组合特征更有意义，它显示了一个人在泰坦尼克号上的亲属总数。我将在下面创建它，也是一个如果有人不孤单时播种的特征。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="4789" class="mm ky iq mi b gy mn mo l mp mq">data = [train_df, test_df]<br/><strong class="mi ir">for</strong> dataset <strong class="mi ir">in</strong> data:<br/>    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']<br/>    dataset.loc[dataset['relatives'] &gt; 0, 'not_alone'] = 0<br/>    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1<br/>    dataset['not_alone'] = dataset['not_alone'].astype(int)</span><span id="c421" class="mm ky iq mi b gy ms mo l mp mq">train_df['not_alone'].value_counts()</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/946f26f21faeeac6f375c832e3b2e118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*psOIhmg7aKppVV4vRvsQ9g.png"/></div></div></figure><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="119d" class="mm ky iq mi b gy mn mo l mp mq">axes = sns.factorplot('relatives','Survived', <br/>                      data=train_df, aspect = 2.5, )</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/d0c5dad0d1fe436d52d7211900b09866.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x0JOO8BpPUZxMrPL01ILmw.png"/></div></div></figure><p id="c1f2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这里我们可以看到，你有 1 到 3 个亲属的存活概率很高，但如果你的亲属少于 1 个或多于 3 个，存活概率就较低(除了一些有 6 个亲属的情况)。</p><h1 id="c4b7" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">数据预处理</h1><p id="61c8" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">首先，我将从训练集中删除“PassengerId ”,因为它对一个人的生存概率没有贡献。我不会把它从测试集中删除，因为提交时需要它。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="2a17" class="mm ky iq mi b gy mn mo l mp mq">train_df = train_df.drop(['PassengerId'], axis=1)</span></pre><h2 id="44dd" class="mm ky iq bd kz ni nj dn ld nk nl dp lh kj nm nn ll kn no np lp kr nq nr lt ns bi translated">缺失数据:</h2><p id="5b20" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated"><strong class="ka ir">小屋:<br/> </strong>提醒一下，我们要处理小屋(687)、登船(2)和年龄(177)。首先我想，我们必须删除“小屋”变量，但后来我发现了一些有趣的东西。一个舱号看起来像‘C123’，字母<strong class="ka ir">指的是甲板</strong>。因此，我们将提取这些并创建一个新的功能，其中包含一个人的甲板。我们将把这个特征转换成一个数字变量。缺少的值将被转换为零。在下面的图片中，你可以看到泰坦尼克号的实际甲板，从 A 到 g。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="d889" class="mm ky iq mi b gy mn mo l mp mq"><strong class="mi ir">import</strong> <strong class="mi ir">re</strong><br/>deck = {"A": 1, "B": 2, "C": 3, "D": 4, "E": 5, "F": 6, "G": 7, "U": 8}<br/>data = [train_df, test_df]<br/><br/><strong class="mi ir">for</strong> dataset <strong class="mi ir">in</strong> data:<br/>    dataset['Cabin'] = dataset['Cabin'].fillna("U0")<br/>    dataset['Deck'] = dataset['Cabin'].map(<strong class="mi ir">lambda</strong> x: re.compile("([a-zA-Z]+)").search(x).group())<br/>    dataset['Deck'] = dataset['Deck'].map(deck)<br/>    dataset['Deck'] = dataset['Deck'].fillna(0)<br/>    dataset['Deck'] = dataset['Deck'].astype(int)</span><span id="8b02" class="mm ky iq mi b gy ms mo l mp mq"><em class="ly"># we can now drop the cabin feature</em><br/>train_df = train_df.drop(['Cabin'], axis=1)<br/>test_df = test_df.drop(['Cabin'], axis=1)</span></pre><p id="a8bc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> Age: <br/> </strong>现在我们可以解决 Age 特性缺少值的问题。我将创建一个包含随机数的数组，这些随机数是根据平均年龄值计算的，与标准偏差和 is_null 有关。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="2435" class="mm ky iq mi b gy mn mo l mp mq">data = [train_df, test_df]<br/><br/><strong class="mi ir">for</strong> dataset <strong class="mi ir">in</strong> data:<br/>    mean = train_df["Age"].mean()<br/>    std = test_df["Age"].std()<br/>    is_null = dataset["Age"].isnull().sum()<br/>    <em class="ly"># compute random numbers between the mean, std and is_null</em><br/>    rand_age = np.random.randint(mean - std, mean + std, size = is_null)<br/>    <em class="ly"># fill NaN values in Age column with random values generated</em><br/>    age_slice = dataset["Age"].copy()<br/>    age_slice[np.isnan(age_slice)] = rand_age<br/>    dataset["Age"] = age_slice<br/>    dataset["Age"] = train_df["Age"].astype(int)</span><span id="521c" class="mm ky iq mi b gy ms mo l mp mq">train_df["Age"].isnull().sum()</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/9c91e28293442f73376a8d67d5c9c0a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QakZkV5rWjdp80ObhPae1g.png"/></div></div></figure><p id="a507" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">着手进行:</strong></p><p id="6e38" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于已装载特征只有两个缺失值，我们将只使用最常见的值来填充它们。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="9189" class="mm ky iq mi b gy mn mo l mp mq">train_df['Embarked'].describe()</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/9cb4d8e28a5d59ccdc17b88fe36a355a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TAYLkmAi3YpkB1LhGKDODw.png"/></div></div></figure><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="0c3b" class="mm ky iq mi b gy mn mo l mp mq">common_value = 'S'<br/>data = [train_df, test_df]<br/><br/><strong class="mi ir">for</strong> dataset <strong class="mi ir">in</strong> data:<br/>    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)</span></pre><h2 id="a5d5" class="mm ky iq bd kz ni nj dn ld nk nl dp lh kj nm nn ll kn no np lp kr nq nr lt ns bi translated">转换功能:</h2><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="b064" class="mm ky iq mi b gy mn mo l mp mq">train_df.info()</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/aef559a4654a624cb6d9d65a32eb4b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bsBIi0l3JdYdr1sR-li4JA.png"/></div></div></figure><p id="a66d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">上面你可以看到“票价”是一个浮动，我们必须处理 4 个分类特征:姓名、性别、车票和上船。让我们一个接一个地调查和转换。</p><p id="83b1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> Fare: <br/> </strong>使用 pandas 提供的函数“astype()”将“Fare”从 float 转换为 int64:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="46c0" class="mm ky iq mi b gy mn mo l mp mq">data = [train_df, test_df]<br/><br/><strong class="mi ir">for</strong> dataset <strong class="mi ir">in</strong> data:<br/>    dataset['Fare'] = dataset['Fare'].fillna(0)<br/>    dataset['Fare'] = dataset['Fare'].astype(int)</span></pre><p id="7622" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> Name: <br/> </strong>我们将使用 Name 特性从名称中提取标题，这样我们就可以从中构建一个新的特性。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="5899" class="mm ky iq mi b gy mn mo l mp mq">data = [train_df, test_df]<br/>titles = {"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Rare": 5}<br/><br/><strong class="mi ir">for</strong> dataset <strong class="mi ir">in</strong> data:<br/>    <em class="ly"># extract titles</em><br/>    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=<strong class="mi ir">False</strong>)<br/>    <em class="ly"># replace titles with a more common title or as Rare</em><br/>    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\<br/>                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')<br/>    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')<br/>    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')<br/>    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')<br/>    <em class="ly"># convert titles into numbers</em><br/>    dataset['Title'] = dataset['Title'].map(titles)<br/>    <em class="ly"># filling NaN with 0, to get safe</em><br/>    dataset['Title'] = dataset['Title'].fillna(0)</span><span id="f9e6" class="mm ky iq mi b gy ms mo l mp mq">train_df = train_df.drop(['Name'], axis=1)<br/>test_df = test_df.drop(['Name'], axis=1)</span></pre><p id="57fe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">性别:<br/> </strong>将‘性别’特征转换为数值。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="4891" class="mm ky iq mi b gy mn mo l mp mq">genders = {"male": 0, "female": 1}<br/>data = [train_df, test_df]<br/><br/><strong class="mi ir">for</strong> dataset <strong class="mi ir">in</strong> data:<br/>    dataset['Sex'] = dataset['Sex'].map(genders)</span></pre><p id="d382" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">票:</strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="7f0e" class="mm ky iq mi b gy mn mo l mp mq">train_df['Ticket'].describe()</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nw"><img src="../Images/091d369a060b9e15898b6bbc18ad1974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1AaZTu8DWS7KDyWLZQU3ew.png"/></div></div></figure><p id="2c8b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于门票属性有 681 个独特的门票，这将是一个有点棘手的转换成有用的类别。所以我们将把它从数据集中删除。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="d43b" class="mm ky iq mi b gy mn mo l mp mq">train_df = train_df.drop(['Ticket'], axis=1)<br/>test_df = test_df.drop(['Ticket'], axis=1)</span></pre><p id="22af" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">已装船:<br/> </strong>将‘已装船’特征转换成数值。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="5f11" class="mm ky iq mi b gy mn mo l mp mq">ports = {"S": 0, "C": 1, "Q": 2}<br/>data = [train_df, test_df]<br/><br/><strong class="mi ir">for</strong> dataset <strong class="mi ir">in</strong> data:<br/>    dataset['Embarked'] = dataset['Embarked'].map(ports)</span></pre><h1 id="4ecc" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">创建类别:</h1><p id="a56c" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">我们现在将在以下功能中创建类别:</p><p id="0452" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">年龄:<br/> </strong>现在我们需要转换‘年龄’特性。首先我们将把它从浮点数转换成整数。然后，我们将创建新的“年龄组”变量，将每个年龄分为一组。请注意，关注如何组成这些组是很重要的，因为您不希望 80%的数据归入组 1。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="d224" class="mm ky iq mi b gy mn mo l mp mq">data = [train_df, test_df]<br/><strong class="mi ir">for</strong> dataset <strong class="mi ir">in</strong> data:<br/>    dataset['Age'] = dataset['Age'].astype(int)<br/>    dataset.loc[ dataset['Age'] &lt;= 11, 'Age'] = 0<br/>    dataset.loc[(dataset['Age'] &gt; 11) &amp; (dataset['Age'] &lt;= 18), 'Age'] = 1<br/>    dataset.loc[(dataset['Age'] &gt; 18) &amp; (dataset['Age'] &lt;= 22), 'Age'] = 2<br/>    dataset.loc[(dataset['Age'] &gt; 22) &amp; (dataset['Age'] &lt;= 27), 'Age'] = 3<br/>    dataset.loc[(dataset['Age'] &gt; 27) &amp; (dataset['Age'] &lt;= 33), 'Age'] = 4<br/>    dataset.loc[(dataset['Age'] &gt; 33) &amp; (dataset['Age'] &lt;= 40), 'Age'] = 5<br/>    dataset.loc[(dataset['Age'] &gt; 40) &amp; (dataset['Age'] &lt;= 66), 'Age'] = 6<br/>    dataset.loc[ dataset['Age'] &gt; 66, 'Age'] = 6<br/><em class="ly"><br/># let's see how it's distributed</em> train_df['Age'].value_counts()</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nx"><img src="../Images/0afcb5b12679cf1fd597e0b551f1dcb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TYX2POUxREPZSxvFaEATlw.png"/></div></div></figure><p id="eee6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> Fare: <br/> </strong>对于‘Fare’特性，我们需要做与‘Age’特性相同的事情。但这并不容易，因为如果我们将票价值的范围分成几个同样大的类别，80%的值将属于第一类。幸运的是，我们可以使用 sklearn 的“qcut()”函数，我们可以使用它来查看我们如何形成类别。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="8fb8" class="mm ky iq mi b gy mn mo l mp mq">train_df.head(10)</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/7d27a7a865252229418054caf276ef78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7wXfsSGDTXHWIQpue2NZlQ.png"/></div></div></figure><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="5c11" class="mm ky iq mi b gy mn mo l mp mq">data = [train_df, test_df]<br/><br/><strong class="mi ir">for</strong> dataset <strong class="mi ir">in</strong> data:<br/>    dataset.loc[ dataset['Fare'] &lt;= 7.91, 'Fare'] = 0<br/>    dataset.loc[(dataset['Fare'] &gt; 7.91) &amp; (dataset['Fare'] &lt;= 14.454), 'Fare'] = 1<br/>    dataset.loc[(dataset['Fare'] &gt; 14.454) &amp; (dataset['Fare'] &lt;= 31), 'Fare']   = 2<br/>    dataset.loc[(dataset['Fare'] &gt; 31) &amp; (dataset['Fare'] &lt;= 99), 'Fare']   = 3<br/>    dataset.loc[(dataset['Fare'] &gt; 99) &amp; (dataset['Fare'] &lt;= 250), 'Fare']   = 4<br/>    dataset.loc[ dataset['Fare'] &gt; 250, 'Fare'] = 5<br/>    dataset['Fare'] = dataset['Fare'].astype(int)</span></pre><h1 id="7cbb" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">创建新功能</h1><p id="096c" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">我将向数据集中添加两个新要素，这两个新要素是从其他要素中计算出来的。</p><p id="e46f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 1。时代时代班</strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="4866" class="mm ky iq mi b gy mn mo l mp mq">data = [train_df, test_df]<br/><strong class="mi ir">for</strong> dataset <strong class="mi ir">in</strong> data:<br/>    dataset['Age_Class']= dataset['Age']* dataset['Pclass']</span></pre><p id="dc6f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2。每人票价</strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="79fd" class="mm ky iq mi b gy mn mo l mp mq"><strong class="mi ir">for</strong> dataset <strong class="mi ir">in</strong> data:<br/>    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)<br/>    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)</span><span id="caee" class="mm ky iq mi b gy ms mo l mp mq"><em class="ly"># Let's take a last look at the training set, before we start training the models.</em><br/>train_df.head(10)</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nz"><img src="../Images/7cfd5a3a4993c6a972c8fecb2ca9ef1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TDEz5ICMHR0qiSJCHgkNNA.png"/></div></div></figure><h1 id="abbf" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">构建机器学习模型</h1><p id="335c" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">现在我们将训练几个机器学习模型，并比较它们的结果。请注意，因为数据集没有为它们的测试集提供标签，所以我们需要使用训练集的预测来相互比较算法。稍后，我们将使用交叉验证。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="7798" class="mm ky iq mi b gy mn mo l mp mq">X_train = train_df.drop("Survived", axis=1)<br/>Y_train = train_df["Survived"]<br/>X_test  = test_df.drop("PassengerId", axis=1).copy()</span></pre><p id="38e4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">随机梯度下降(SGD): </strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="895f" class="mm ky iq mi b gy mn mo l mp mq">sgd = linear_model.SGDClassifier(max_iter=5, tol=<strong class="mi ir">None</strong>)<br/>sgd.fit(X_train, Y_train)<br/>Y_pred = sgd.predict(X_test)<br/><br/>sgd.score(X_train, Y_train)<br/><br/>acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)</span></pre><p id="e0ca" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">随机森林:</strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="12cc" class="mm ky iq mi b gy mn mo l mp mq">random_forest = RandomForestClassifier(n_estimators=100)<br/>random_forest.fit(X_train, Y_train)<br/><br/>Y_prediction = random_forest.predict(X_test)<br/><br/>random_forest.score(X_train, Y_train)<br/>acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)</span></pre><p id="679b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">逻辑回归:</strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="ab7f" class="mm ky iq mi b gy mn mo l mp mq">logreg = LogisticRegression()<br/>logreg.fit(X_train, Y_train)<br/><br/>Y_pred = logreg.predict(X_test)<br/><br/>acc_log = round(logreg.score(X_train, Y_train) * 100, 2)</span></pre><p id="0945" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> K 最近邻:</strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="22d2" class="mm ky iq mi b gy mn mo l mp mq"><em class="ly"># KNN</em> knn = KNeighborsClassifier(n_neighbors = 3) knn.fit(X_train, Y_train)  Y_pred = knn.predict(X_test)  acc_knn = round(knn.score(X_train, Y_train) * 100, 2)</span></pre><p id="9d28" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">高斯朴素贝叶斯:</strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="3577" class="mm ky iq mi b gy mn mo l mp mq">gaussian = GaussianNB() gaussian.fit(X_train, Y_train)  Y_pred = gaussian.predict(X_test)  acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)</span></pre><p id="d637" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">感知器:</strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="c164" class="mm ky iq mi b gy mn mo l mp mq">perceptron = Perceptron(max_iter=5)<br/>perceptron.fit(X_train, Y_train)<br/><br/>Y_pred = perceptron.predict(X_test)<br/><br/>acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)</span></pre><p id="b9da" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">线性支持向量机:</strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="b56f" class="mm ky iq mi b gy mn mo l mp mq">linear_svc = LinearSVC()<br/>linear_svc.fit(X_train, Y_train)<br/><br/>Y_pred = linear_svc.predict(X_test)<br/><br/>acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)</span></pre><p id="a441" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">决策树</strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="196a" class="mm ky iq mi b gy mn mo l mp mq">decision_tree = DecisionTreeClassifier() decision_tree.fit(X_train, Y_train)  Y_pred = decision_tree.predict(X_test)  acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)</span></pre><h1 id="d0ce" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">哪个是最好的型号？</h1><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="4930" class="mm ky iq mi b gy mn mo l mp mq">results = pd.DataFrame({<br/>    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', <br/>              'Random Forest', 'Naive Bayes', 'Perceptron', <br/>              'Stochastic Gradient Decent', <br/>              'Decision Tree'],<br/>    'Score': [acc_linear_svc, acc_knn, acc_log, <br/>              acc_random_forest, acc_gaussian, acc_perceptron, <br/>              acc_sgd, acc_decision_tree]})<br/>result_df = results.sort_values(by='Score', ascending=<strong class="mi ir">False</strong>)<br/>result_df = result_df.set_index('Score')<br/>result_df.head(9)</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oa"><img src="../Images/4813c3dda457259e4836f817aa097530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IEZJkCohvTi7xtJstq8Nsg.png"/></div></div></figure><p id="9abe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">正如我们所看到的，随机森林分类器放在第一位。但是首先，让我们检查一下，当我们使用交叉验证时，random-forest 是如何执行的。</p><h1 id="9903" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">k 倍交叉验证:</h1><p id="26df" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">K-Fold 交叉验证将训练数据随机分成称为折叠的 K 个子集<strong class="ka ir"/>。假设我们将数据分成 4 份(K = 4)。我们的随机森林模型将被训练和评估 4 次，每次使用不同的折叠进行评估，而它将在剩余的 3 个折叠上被训练。</p><p id="fc6a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下图显示了这个过程，使用了 4 次折叠(K = 4)。每行代表一个培训+评估过程。在第一行中，模型 get 在第一、第二和第三子集上被训练，并且在第四子集上被评估。在第二行中，模型 get 在第二、第三和第四个子集上训练，并在第一个子集上评估。K-Fold 交叉验证重复这个过程，直到每个折叠作为一个评估折叠进行一次。</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ob"><img src="../Images/ff6b1ee2b30aefe24f3ad3ff5d466c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HzpaubLj_o-zt1klnB81Yg.png"/></div></div></figure><p id="f5c4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的 K-Fold 交叉验证示例的结果将是包含 4 个不同分数的数组。然后我们需要计算这些分数的平均值和标准差。</p><p id="4ada" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面的代码对我们的随机森林模型执行 K-Fold 交叉验证，使用 10 个折叠(K = 10)。因此，它输出一个有 10 个不同分数的数组。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="664a" class="mm ky iq mi b gy mn mo l mp mq"><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.model_selection</strong> <strong class="mi ir">import</strong> cross_val_score<br/>rf = RandomForestClassifier(n_estimators=100)<br/>scores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = "accuracy")</span><span id="867d" class="mm ky iq mi b gy ms mo l mp mq">print("Scores:", scores)<br/>print("Mean:", scores.mean())<br/>print("Standard Deviation:", scores.std())</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oc"><img src="../Images/8090eeecfe22d87e5cdfae410f0a8cec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sbQv1lFei82k5J81_zr1Vg.png"/></div></div></figure><p id="4173" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这看起来比以前真实多了。我们的模型平均准确率为 82%，标准偏差为 4 %。标准差告诉我们，估计值有多精确。</p><p id="f5db" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这意味着在我们的例子中，我们的模型的精度可以相差<strong class="ka ir"> + </strong> — 4%。</p><p id="7fe6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我认为准确性仍然很好，因为随机森林是一个易于使用的模型，我们将在下一节中尝试进一步提高它的性能。</p><h1 id="11e9" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">随机森林</h1><h2 id="b649" class="mm ky iq bd kz ni nj dn ld nk nl dp lh kj nm nn ll kn no np lp kr nq nr lt ns bi translated">什么是随机森林？</h2><p id="ec70" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">随机森林是一种监督学习算法。就像你已经从它的名字中看到的，它创造了一个森林，使它变得随机。它构建的“森林”是决策树的集合，大部分时间是用“打包”方法训练的。bagging 方法的一般思想是学习模型的组合增加了整体结果。</p><p id="f1f2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">用简单的话来说:随机森林构建多个决策树，将它们合并在一起，得到一个更加准确稳定的预测。</p><p id="649b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">随机森林的一个很大的优点是，它可以用于分类和回归问题，这构成了当前机器学习系统的大多数。除了少数例外，随机森林分类器具有决策树分类器的所有超参数以及 bagging 分类器的所有超参数，以控制整体本身。</p><p id="315d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">随机森林算法在生长树时给模型带来了额外的随机性。它不是在分割节点时搜索最佳特征，而是在随机特征子集中搜索最佳特征。这个过程产生了广泛的多样性，这通常会产生一个更好的模型。因此，当您在随机森林中种植树时，只考虑要素的随机子集来分割节点。您甚至可以通过在每个特征的基础上使用随机阈值，而不是像普通决策树那样搜索最佳阈值，来使树更加随机。</p><p id="5185" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面你可以看到有两棵树的随机森林的样子:</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi od"><img src="../Images/11c9a0b6f1914e49de8d33009b0c5e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GiCvHwZ03tObjkD-6mSqag.png"/></div></div></figure><h2 id="5968" class="mm ky iq bd kz ni nj dn ld nk nl dp lh kj nm nn ll kn no np lp kr nq nr lt ns bi translated">特征重要性</h2><p id="a121" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">随机森林的另一个优点是，它使得测量每个特征的相对重要性变得非常容易。Sklearn 通过查看使用该特征的树节点平均减少杂质的程度(跨越森林中的所有树)来测量特征的重要性。它会在训练后自动计算每个特征的分数，并对结果进行缩放，使所有重要度之和等于 1。我们将在下面讨论这个问题:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="cab1" class="mm ky iq mi b gy mn mo l mp mq">importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})<br/>importances = importances.sort_values('importance',ascending=<strong class="mi ir">False</strong>).set_index('feature')</span><span id="7bf8" class="mm ky iq mi b gy ms mo l mp mq">importances.head(15)</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/3bb779b00941c8b9e9f4290a8fd6339d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-70K0tBYMs_y2ar25KPhDA.png"/></div></div></figure><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="f482" class="mm ky iq mi b gy mn mo l mp mq">importances.plot.bar()</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/47dffb700752d3579e5f4abb3ff47962.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_jE0Ha2nq2zD0OpMHKL0Ig.png"/></div></div></figure><h2 id="ce18" class="mm ky iq bd kz ni nj dn ld nk nl dp lh kj nm nn ll kn no np lp kr nq nr lt ns bi translated"><strong class="ak">结论:</strong></h2><p id="21fc" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">not_alone 和 Parch 在我们的随机森林分类器预测过程中不起重要作用。因此，我将把它们从数据集中删除，并再次训练分类器。我们也可以删除更多或更少的特性，但是这需要更详细地研究特性对我们模型的影响。但是我觉得只单独取出炒一下就可以了。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="3689" class="mm ky iq mi b gy mn mo l mp mq">train_df  = train_df.drop("not_alone", axis=1)<br/>test_df  = test_df.drop("not_alone", axis=1)<br/><br/>train_df  = train_df.drop("Parch", axis=1)<br/>test_df  = test_df.drop("Parch", axis=1)</span></pre><p id="fbff" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">再次训练随机森林:</strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="3a7a" class="mm ky iq mi b gy mn mo l mp mq"><em class="ly"># Random Forest</em><br/><br/>random_forest = RandomForestClassifier(n_estimators=100, oob_score = <strong class="mi ir">True</strong>)<br/>random_forest.fit(X_train, Y_train)<br/>Y_prediction = random_forest.predict(X_test)<br/><br/>random_forest.score(X_train, Y_train)<br/><br/>acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)<br/>print(round(acc_random_forest,2,), "%")</span></pre><p id="4ef0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi">92.82%</p><p id="fb00" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的随机森林模型预测和以前一样好。一般来说，<strong class="ka ir">你拥有的功能越多，你的模型就越有可能过度拟合</strong>，反之亦然。但我认为我们的数据目前看起来不错，没有太多特征。</p><p id="a055" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">还有另一种方法来评估随机森林分类器，这可能比我们之前使用的分数更准确。我说的是<strong class="ka ir">出袋样本</strong>估计泛化精度。我不会在这里详细介绍它是如何工作的。只需注意，使用与训练集大小相同的测试集，可以获得非常准确的估计。因此，使用袋外误差估计消除了对预留测试集的需要。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="104e" class="mm ky iq mi b gy mn mo l mp mq">print("oob score:", round(random_forest.oob_score_, 4)*100, "%")</span></pre><p id="5e8e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">oob 分数:81.82 %</p><p id="c7fd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们可以开始调整随机森林的超参数了。</p><h1 id="6dd0" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">超参数调谐</h1><p id="57f9" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">下面您可以看到参数标准的超参数调整代码，min_samples_leaf、min_samples_split 和 n_estimators。</p><p id="bfae" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我将这段代码放在 markdown 单元格中，而不是 code 单元格中，因为运行它需要很长时间。在它的正下方，我放了一个 gridsearch 输出的截图。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="d95b" class="mm ky iq mi b gy mn mo l mp mq">param_grid = { "criterion" : ["gini", "entropy"], "min_samples_leaf" : [1, 5, 10, 25, 50, 70], "min_samples_split" : [2, 4, 10, 12, 16, 18, 25, 35], "n_estimators": [100, 400, 700, 1000, 1500]}</span><span id="a693" class="mm ky iq mi b gy ms mo l mp mq">from sklearn.model_selection import GridSearchCV, cross_val_score</span><span id="cc66" class="mm ky iq mi b gy ms mo l mp mq">rf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)</span><span id="dfeb" class="mm ky iq mi b gy ms mo l mp mq">clf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)</span><span id="ac48" class="mm ky iq mi b gy ms mo l mp mq">clf.fit(X_train, Y_train)</span><span id="a70b" class="mm ky iq mi b gy ms mo l mp mq">clf.best<em class="ly">params</em></span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi og"><img src="../Images/152ed2ac6d5decf94f8debc6f67d6c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D_Lg43_vnHCChBCi-5Y0MA.png"/></div></div></figure><p id="eaed" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">测试新参数:</strong></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="83a0" class="mm ky iq mi b gy mn mo l mp mq"><em class="ly"># Random Forest</em><br/>random_forest = RandomForestClassifier(criterion = "gini", <br/>                                       min_samples_leaf = 1, <br/>                                       min_samples_split = 10,   <br/>                                       n_estimators=100, <br/>                                       max_features='auto', <br/>                                       oob_score=<strong class="mi ir">True</strong>, <br/>                                       random_state=1, <br/>                                       n_jobs=-1)<br/><br/>random_forest.fit(X_train, Y_train)<br/>Y_prediction = random_forest.predict(X_test)<br/><br/>random_forest.score(X_train, Y_train)<br/><br/>print("oob score:", round(random_forest.oob_score_, 4)*100, "%")</span></pre><p id="e559" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">oob 分数:83.05 %</p><p id="6b8e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们有了一个合适的模型，我们可以开始以更准确的方式评估它的性能。以前我们只使用准确性和 oob 分数，这只是准确性的另一种形式。问题是，评估一个分类模型比评估一个回归模型更复杂。我们将在下一节中讨论这一点。</p><h1 id="0350" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">进一步评估</h1><h2 id="e281" class="mm ky iq bd kz ni nj dn ld nk nl dp lh kj nm nn ll kn no np lp kr nq nr lt ns bi translated">混淆矩阵:</h2><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="f71e" class="mm ky iq mi b gy mn mo l mp mq"><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.model_selection</strong> <strong class="mi ir">import</strong> cross_val_predict<br/><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.metrics</strong> <strong class="mi ir">import</strong> confusion_matrix<br/>predictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)<br/>confusion_matrix(Y_train, predictions)</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oh"><img src="../Images/b9a961cb7c7e6421ce4456d88e851435.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IFLtfeeAZsZgvtjEHuZtfw.png"/></div></div></figure><p id="eaf1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第一行是关于未幸存的预测:<strong class="ka ir"> 493 名乘客被正确分类为未幸存</strong>(称为真阴性)<strong class="ka ir"> 56 名乘客被错误分类为未幸存</strong>(假阳性)。</p><p id="fdc0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第二行是关于幸存预测:<strong class="ka ir"> 93 名乘客被错误地分类为幸存</strong>(假阴性)，而<strong class="ka ir"> 249 名乘客被正确地分类为幸存</strong>(真阳性)。</p><p id="5257" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">混淆矩阵给了你很多关于你的模型表现如何的信息，但是有一种方法可以得到更多，比如计算分类器的精度。</p><h2 id="ab00" class="mm ky iq bd kz ni nj dn ld nk nl dp lh kj nm nn ll kn no np lp kr nq nr lt ns bi translated">精确度和召回率:</h2><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="7df8" class="mm ky iq mi b gy mn mo l mp mq"><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.metrics</strong> <strong class="mi ir">import</strong> precision_score, recall_score<br/><br/>print("Precision:", precision_score(Y_train, predictions))<br/>print("Recall:",recall_score(Y_train, predictions))</span></pre><p id="fc70" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">精度:0.801948051948 <br/>召回:0.8000000001</p><p id="e95c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的模型预测 81%的时间，一个乘客生存正确(精度)。这份回忆告诉我们，它预测了 73 %的幸存者。</p><h2 id="871c" class="mm ky iq bd kz ni nj dn ld nk nl dp lh kj nm nn ll kn no np lp kr nq nr lt ns bi translated">f 分数</h2><p id="fac3" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">你可以把精确度和召回率结合成一个分数，叫做 F 分数。F 分数是用精确度和召回率的调和平均值来计算的。请注意，它将更多的权重分配给低值。因此，如果召回率和精确度都很高，分类器将只获得高 F 值。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="2bfd" class="mm ky iq mi b gy mn mo l mp mq"><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.metrics</strong> <strong class="mi ir">import</strong> f1_score<br/>f1_score(Y_train, predictions)</span></pre><p id="d721" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi">0.7599999999999</p><p id="37b1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好了，77 %的 F 分。分数没有那么高，因为我们的召回率是 73%。但不幸的是，F-score 并不完美，因为它倾向于具有相似精度和召回率的分类器。这是一个问题，因为你有时想要高精度，有时想要高召回率。事实是，精确度的增加，有时会导致回忆的减少，反之亦然(取决于阈值)。这被称为精确度/召回率的权衡。我们将在下一节讨论这一点。</p><h2 id="a0e4" class="mm ky iq bd kz ni nj dn ld nk nl dp lh kj nm nn ll kn no np lp kr nq nr lt ns bi translated">精确召回曲线</h2><p id="07ec" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">对于随机森林算法必须分类的每个人，它基于函数计算概率，并将该人分类为幸存(当分数大于阈值时)或未幸存(当分数小于阈值时)。这就是为什么门槛很重要。</p><p id="3fba" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将使用 matplotlib 绘制精度和召回与阈值的关系:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="7676" class="mm ky iq mi b gy mn mo l mp mq"><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.metrics</strong> <strong class="mi ir">import</strong> precision_recall_curve<br/><br/><em class="ly"># getting the probabilities of our predictions</em><br/>y_scores = random_forest.predict_proba(X_train)<br/>y_scores = y_scores[:,1]<br/><br/>precision, recall, threshold = precision_recall_curve(Y_train, y_scores)</span><span id="8a84" class="mm ky iq mi b gy ms mo l mp mq"><strong class="mi ir">def</strong> plot_precision_and_recall(precision, recall, threshold):<br/>    plt.plot(threshold, precision[:-1], "r-", label="precision", linewidth=5)<br/>    plt.plot(threshold, recall[:-1], "b", label="recall", linewidth=5)<br/>    plt.xlabel("threshold", fontsize=19)<br/>    plt.legend(loc="upper right", fontsize=19)<br/>    plt.ylim([0, 1])<br/><br/>plt.figure(figsize=(14, 7))<br/>plot_precision_and_recall(precision, recall, threshold)<br/>plt.show()</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oi"><img src="../Images/47b662bf94317a9d70d2ee493f3d6784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IYouDo_uqstikiDFNf8hLw.png"/></div></div></figure><p id="efbd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从上图可以清楚地看到，召回率正在迅速下降，大约在 85%左右。因此，在此之前，您可能希望选择精度/召回率之间的权衡——可能在 75 %左右。</p><p id="31c1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您现在可以选择一个阈值，为您当前的机器学习问题提供最佳的精确度/召回率权衡。例如，如果您想要 80%的精度，您可以很容易地查看这些图，并看到您将需要 0.4 左右的阈值。然后你就可以用这个阈值训练一个模型，并得到你想要的精确度。</p><p id="c558" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">另一种方法是将精确度和召回率相互对比:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="113e" class="mm ky iq mi b gy mn mo l mp mq"><strong class="mi ir">def</strong> plot_precision_vs_recall(precision, recall):<br/>    plt.plot(recall, precision, "g--", linewidth=2.5)<br/>    plt.ylabel("recall", fontsize=19)<br/>    plt.xlabel("precision", fontsize=19)<br/>    plt.axis([0, 1.5, 0, 1.5])<br/><br/>plt.figure(figsize=(14, 7))<br/>plot_precision_vs_recall(precision, recall)<br/>plt.show()</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oj"><img src="../Images/7f4e1c31ca4cc320e6ca40ba821ba519.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EOxCKw1myCQeY1zQFnYdiA.png"/></div></div></figure><h2 id="db83" class="mm ky iq bd kz ni nj dn ld nk nl dp lh kj nm nn ll kn no np lp kr nq nr lt ns bi translated">ROC AUC 曲线</h2><p id="95dd" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">ROC AUC 曲线提供了评估和比较二元分类器的另一种方法。该曲线绘制了真阳性率(也称为召回率)对假阳性率(错误分类的阴性实例的比率)，而不是绘制了精度对召回率。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="d06a" class="mm ky iq mi b gy mn mo l mp mq"><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.metrics</strong> <strong class="mi ir">import</strong> roc_curve<br/><em class="ly"># compute true positive rate and false positive rate</em><br/>false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, y_scores)</span><span id="b929" class="mm ky iq mi b gy ms mo l mp mq"><em class="ly"># plotting them against each other</em><br/><strong class="mi ir">def</strong> plot_roc_curve(false_positive_rate, true_positive_rate, label=<strong class="mi ir">None</strong>):<br/>    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)<br/>    plt.plot([0, 1], [0, 1], 'r', linewidth=4)<br/>    plt.axis([0, 1, 0, 1])<br/>    plt.xlabel('False Positive Rate (FPR)', fontsize=16)<br/>    plt.ylabel('True Positive Rate (TPR)', fontsize=16)<br/><br/>plt.figure(figsize=(14, 7))<br/>plot_roc_curve(false_positive_rate, true_positive_rate)<br/>plt.show()</span></pre><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ok"><img src="../Images/e1c7f7402112555dd55bf1d3abfd1a8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ei1HQGEFgwYAI1NwjODX8g.png"/></div></div></figure><p id="2a3f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">中间的红线代表一个纯粹随机的分类器(例如抛硬币)，因此你的分类器应该尽可能远离它。我们的随机森林模型似乎做得很好。</p><p id="c56a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当然，我们在这里也有一个权衡，因为分类器产生的假阳性越多，真阳性率就越高。</p><h2 id="ba61" class="mm ky iq bd kz ni nj dn ld nk nl dp lh kj nm nn ll kn no np lp kr nq nr lt ns bi translated">ROC AUC 得分</h2><p id="f413" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">ROC AUC 得分是 ROC AUC 曲线的相应得分。它只是通过测量曲线下的面积来计算，该面积称为 AUC。</p><p id="06f1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">100%正确的分类器的 ROC AUC 得分为 1，完全随机的分类器的得分为 0.5。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="24dc" class="mm ky iq mi b gy mn mo l mp mq"><strong class="mi ir">from</strong> <strong class="mi ir">sklearn.metrics</strong> <strong class="mi ir">import</strong> roc_auc_score<br/>r_a_score = roc_auc_score(Y_train, y_scores)<br/>print("ROC-AUC-Score:", r_a_score)</span></pre><p id="949e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">ROC_AUC_SCORE: 0.945067587</p><p id="6801" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">不错！我认为这个分数足以将测试集的预测提交给 Kaggle 排行榜。</p><h1 id="508d" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">摘要</h1><p id="e00f" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">我们从数据探索开始，对数据集有所了解，检查缺失的数据，了解哪些特征是重要的。在这个过程中，我们使用 seaborn 和 matplotlib 来做可视化。在数据预处理部分，我们计算缺失值，将特征转换为数值，将值分组并创建一些新特征。之后，我们开始训练 8 个不同的机器学习模型，选择其中一个(随机森林)并对其进行交叉验证。然后我们讨论了随机森林是如何工作的，看看它对不同特性的重要性，并通过优化它的超参数值来调整它的性能。最后，我们查看了它的混淆矩阵，并计算了模型的精度、召回率和 f 值。</p><p id="c734" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面您可以看到“train_df”数据帧的前后图片:</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ol"><img src="../Images/0715024e4f45ce1cee6b55435df3a949.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KD3ltERqCMvQ_HfTGy7wMQ.png"/></div></div></figure><p id="efba" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当然，仍有改进的空间，如通过相互比较和绘制特征并识别和去除有噪声的特征，进行更广泛的特征工程。另一个可以改善 kaggle 排行榜整体结果的事情是对几个机器学习模型进行更广泛的超参数调整。你也可以做一些整体学习。</p></div></div>    
</body>
</html>