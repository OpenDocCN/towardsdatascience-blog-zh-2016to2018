<html>
<head>
<title>My Journey into Machine Learning: Class 5 (Regression)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我的机器学习之旅:第五课(回归)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/my-journey-into-machine-learning-class-5-regression-cb6f04006b29?source=collection_archive---------11-----------------------#2018-03-15">https://towardsdatascience.com/my-journey-into-machine-learning-class-5-regression-cb6f04006b29?source=collection_archive---------11-----------------------#2018-03-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="f096" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">欢迎回来，伙计们！机器学习巡航继续！这是该系列的第五篇文章；请务必阅读以前的职位，因为我从那里建立了很多。</p><p id="9656" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上周，我们讨论了对验证集的需求。我们继续讨论了交叉验证，并讨论了作为交叉验证替代方法的自举。我们也试图直观和数学地理解偏差和方差。</p><p id="2b47" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本帖中，我们将讨论:</p><ul class=""><li id="14c9" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">回归模型—单变量、多变量和多项式</li><li id="0f31" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">正规方程方法</li><li id="10fc" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">梯度下降算法</li><li id="4753" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">正规化</li></ul></div><div class="ab cl kz la hu lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ij ik il im in"><h1 id="57ee" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">来源</h1><p id="ec9a" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">这些笔记的灵感来自各种材料，包括但不限于:</p><ol class=""><li id="7346" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk mj kr ks kt bi translated"><a class="ae mk" href="https://www.amazon.com/Introduction-Machine-Learning-Ethem-Alpaydin/dp/8120350782" rel="noopener ugc nofollow" target="_blank"> Alpaydin的机器学习入门</a></li><li id="c92f" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mj kr ks kt bi translated"><a class="ae mk" href="https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E" rel="noopener ugc nofollow" target="_blank">统计学习的要素</a></li><li id="dd2d" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mj kr ks kt bi translated"><a class="ae mk" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">吴恩达的机器学习课程</a></li><li id="fcb5" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mj kr ks kt bi translated"><a class="ml mm ep" href="https://medium.com/u/1d8994ad0efc?source=post_page-----cb6f04006b29--------------------------------" rel="noopener" target="_blank">艾琳金姆🙏</a>的文章</li><li id="744c" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mj kr ks kt bi translated">赫勒斯坦教授的讲座、笔记和幻灯片</li><li id="245c" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mj kr ks kt bi translated">互联网</li></ol></div><div class="ab cl kz la hu lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ij ik il im in"><h1 id="1d79" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">一元线性回归模型</h1><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/af40dd60a8217eda4c582da9957dc7be.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*7o9PCaTxTRDnhorf10R8uw.jpeg"/></div></figure><p id="75f8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae mk" href="https://goo.gl/cch8Yp" rel="noopener ugc nofollow" target="_blank">第三篇文章</a>中，我介绍了线性回归的核心概念。概括地说，我们希望有一个函数<em class="mv"> f </em>来模拟我们的数据。我们建立一个函数<em class="mv"> f </em>的近似器，称为<em class="mv">g</em>，表示为:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/9a5c517a25b689b84bc44cf3a22a0216.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*-Af2zOW4Qskqa3YHH7kFIQ.png"/></div></figure><p id="fadd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用误差函数来衡量函数对数据的逼近程度。误差函数有许多变体，但我们将利用:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/cd9ccfa2ac498bd4cc2ec29cfcc6cfcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*HP64pVpm-ubMxnD0dMs7aQ.png"/></div></figure><p id="8555" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，假设我们的数据如下:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi my"><img src="../Images/9cfa6b0719ed03db1e4fb45b4433b20c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SSLokMAVk5a1OAgbPxR4VQ.png"/></div></div></figure><p id="796f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上述的误差函数为:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/e81708ea4c32b5eda6a890f13f4b02ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*B743ohVrhQze7yIVg7CnIg.png"/></div></figure><p id="6ec9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的误差函数值不是很大。值越接近0，我们的模型就越好。如您所见，我们的误差函数取决于:</p><ul class=""><li id="51cf" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">属性/特征X</li><li id="c83c" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">参数w1和w0</li></ul><p id="537c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们不能改变属性X的值，因为它代表实际的数据(我们可以对它进行规范化，但不能随意改变它的值)。然而，我们可以改变参数w1和w0的值。一元线性线中的w1和w0表示:</p><ul class=""><li id="b1e3" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">w1 -斜率或梯度(我们的线有多陡)</li><li id="d78d" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">w0 - y轴截距(直线与y轴相交的位置)</li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ne"><img src="../Images/06ae2bc685422468e6bb57afb410203d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zg6bKe4_ZrV7wrRA3ndBDA.jpeg"/></div></div></figure><p id="7bf0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们能够控制w1和w0的值，我们可以使上面的线准确地穿过我们的数据！</p><p id="e13e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们讨论如何精确地优化w1和w0的值之前，我想用矩阵来表示上述误差函数。</p><p id="14cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/f370f3f1e9ea77f5db24bf77bf171033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*XvNmrSfhPBfL7INiZmucgg.png"/></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/c054be66869b4cb2f3d13254b279ffed.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*rkXjhMQiRFs16QxD5pAEow.png"/></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nh"><img src="../Images/7ea2196c24148c5a96a94edfc92e3904.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BPa8Itn3lTj4JFdg-koJcg.png"/></div></div></figure><p id="a024" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意X_0 = 1。这是因为X_0不是一个真正的属性，但我们用它来简化我们的矩阵符号。这只是一个培训示例。如果我们有一个以上的训练示例(比如3个)，我们的矩阵符号就变成:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ni"><img src="../Images/3fd9819c97f4572ace97d7d3efe9bed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4NU9elluUTjf4wYcId5rDw.png"/></div></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/9eb8842cf763920419df688c2b8d248c.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*AgfTBkW0Fb0SAzzhUN2MeQ.png"/></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/51c82f0d132e72051950e40d5cab9119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*kFqSdpHplCbr_32InA67cw.png"/></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/8a4cde575f7721b9f7b0e67d65438137.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*Z-bhxJCgvPB9KgiJ4Ib6rg.png"/></div></figure><p id="29d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">产生的向量(向量是具有单行/列的矩阵)是我们预测值的集合(g(X)的值)。</p><p id="29fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们已经定义了矩阵术语，让我们看看如何优化参数w1和w0的值。</p><p id="d304" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有两种方法可以获得w1和w0的优化值:</p><ul class=""><li id="a229" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">正规方程法，以及</li><li id="1c4e" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">梯度下降算法</li></ul><h1 id="bcca" class="lg lh iq bd li lj nl ll lm ln nm lp lq lr nn lt lu lv no lx ly lz np mb mc md bi translated"><strong class="ak"> 1。正规方程法</strong></h1><p id="00c8" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">正规方程方法是一种非迭代方法，它帮助我们确定W(带有我们希望优化的参数的矩阵)的优化值。</p><p id="59fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们继续之前，我想定义一些术语:</p><p id="f941" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">残差:</strong>观测值(r)和估计值之间的差值(在我们的例子中是g(X))</p><p id="ba48" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">误差:</strong>观察值(r)与r的<strong class="jp ir">真值</strong>(不可观察)之差</p><p id="54d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> RSS(残差平方和):</strong>表示为</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/36922e47cd65637bc888b0634486bc07.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*Q8rU-HwR9GAWiYsuUhWteA.png"/></div></figure><p id="12cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">最小二乘法:</strong>最小化RSS的方法。它测量模型的<strong class="jp ir">平均</strong>拟合缺失。它分为两类:普通(线性)最小二乘法和非线性最小二乘法</p><p id="2fe5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">闭型表达式:</strong>可以在有限次运算中求值并有解的数学表达式。</p><p id="48f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">普通最小二乘法:</strong>有一个封闭形式的解(我们这里的正规方程组方法)</p><p id="6b29" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">非线性最小二乘法:</strong>一般来说，没有封闭形式的解</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nr"><img src="../Images/f1e94742127870e39237c9131c22ddda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vVVe-PztGH9-rKyJvd87nQ.png"/></div></div></figure><p id="84b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们这里考虑的正规方程组方法是<strong class="jp ir">普通最小二乘法</strong>。在这种方法中，我们通过明确地对w求导并将它们设置为0来最小化误差函数。它由以下公式给出:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/72c8e43083d135181c0bc6d383648c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*8C024AHwsDOBo_Q6Omgx_Q.png"/></div></figure><p id="0966" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你对这个等式是如何推导出来的感兴趣，那么Aerin Kim将是一个很好的起点🙏笔下的<a class="ae mk" rel="noopener" target="_blank" href="/big4-tech-interview-question-derive-the-linear-regression-c45ccdd213e3">文章</a>。我还推荐你浏览一下维基百科的页面和博客。</p><p id="ef34" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于正规方程，需要记住几个要点:</p><ul class=""><li id="2796" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">正规方程只需一步即可解析求解W</li><li id="3af9" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">不需要要素缩放(将要素/属性的值转换为相同的比例，通常为-1比1)</li><li id="2273" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">计算(X^T X)的倒数的时间复杂度为O(n ),其中n =特征的数量</li><li id="f64c" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">如果我们有非常多的特征，法线方程将会很慢</li><li id="b78e" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">实际上，当n &gt; 10，000时，可能是从常规解决方案进入迭代过程的好时机</li><li id="0981" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">法线方程非常适用于线性模型，尤其是当要素数量较少时</li><li id="1d0d" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">它不适用于其他模型，如逻辑回归(我们将在后面介绍！)</li><li id="1fa2" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">有时，(X^T X)的逆可能不存在，即(X^T X)是不可逆的(它的行列式是0)</li><li id="0827" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">如果(X^T X)是不可逆的，常见原因可能是:</li></ul><ol class=""><li id="fb9c" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk mj kr ks kt bi translated"><strong class="jp ir">冗余特征</strong>，其中两个或多个特征非常相关(即，它们是线性相关的)</li></ol><p id="a2b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，如果我们有以米为单位的X1 -尺寸，以厘米为单位的X2 -尺寸，X1 = 100X2，那么X1/X2是一个多余的特征，应该被删除</p><p id="24ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.<strong class="jp ir">功能太多</strong>。在这种情况下，删除一些特征或使用<em class="mv">正则化</em></p><p id="8b59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们来看看获得优化参数的迭代解决方案。</p><h1 id="2ab6" class="lg lh iq bd li lj nl ll lm ln nm lp lq lr nn lt lu lv no lx ly lz np mb mc md bi translated">2.梯度下降算法</h1><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nt"><img src="../Images/e69860b917f5bed7ef5076848c9d4a45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JUj1ECgvG68gBXrsFJytYg.jpeg"/></div></div></figure><p id="6cd5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度下降是一种迭代算法，可以最小化我们的误差函数。这是一个非常强大的通用算法，广泛用于机器学习，而不仅仅是线性回归。</p><p id="cd95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们试着直观地理解我们的误差函数和参数的目的是什么。我们将画出g(X)和误差函数e。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nu"><img src="../Images/7994fd6fb8e025b718ac8f971ad9fdda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UvvtU_LLcQKKU_MnXJefKg.png"/></div></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ne"><img src="../Images/9923dd6ba9438a344a6ef84ea9b15064.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mRm-NrdaHjEWNckwsHtVDA.jpeg"/></div></div></figure><p id="5a5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们的例子中，我们得到E = 1.165。理想情况下，我们应该得到E = 0。为此，我们不断改变w1的值，以使该点位于x轴上。然后，我们的E(w)将是0，我们将有一个模型，完美地描述我们的数据。</p><p id="1e98" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上图是我们假设只有一个参数的时候。让我们考虑参数w0和w1，并想象我们的E(w)。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ne"><img src="../Images/ee1aa3baad6cf62a3f70718666eed253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MlokyQGveCPtTzbJjz8nxw.jpeg"/></div></div></figure><p id="27de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我为我糟糕的绘画技巧道歉。当我们考虑参数w0和w1时，我们得到3D表面图。当我们对w0和w1取不同值时，弓形表面就是我们的误差函数。这个弓形表面的底部点是我们得到误差函数最低的地方。同样的图形可以用<a class="ae mk" href="http://www.statisticshowto.com/contour-plots/" rel="noopener ugc nofollow" target="_blank">等高线图</a>绘制在二维平面上，但我们不会在这里深究。</p><p id="a980" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我们可以手动绘制如上图，找到w1和w0的值，并查看我们的误差函数如何响应这些值。但这是一个极其繁琐的过程。更不用说，当我们有两个以上的参数时，我们会得到更高维度的图形，这些图形不容易绘制，也更难可视化。</p><p id="758e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们希望有一种有效的算法，能够自动找到w1和w0的值，使我们的误差函数最小。这个算法就是<strong class="jp ir">梯度下降。</strong></p><p id="7d5c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过一个例子可以更好地理解梯度下降。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/3792d73f0f2116727794aedc42a0d637.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*NlJygaxiAiQSOBP0QehK0g.jpeg"/></div></figure><p id="1571" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想象你和你的爱人去爬山。经过艰苦的努力和时间，你终于到达了山顶！(万岁！).当你沉浸在荣耀中时，你突然意识到你的爱人不在你身边。惊慌失措中，你呼唤着爱人的名字。幸运的是，你的爱人回应说他/她在山的最低点(山很小！).</p><p id="b885" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你想尽快到达这座山的最低点。你可以随意往任何方向走，希望你能找到你的爱人。但是你的爱人可能会有危险(有凶猛的野兽在附近徘徊！).你不想拿你爱人的安全冒险。</p><p id="6b37" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你深吸一口气，开始分析你的环境。从你站的地方，你环顾四周，选择最陡下降的方向。你推理说，如果在每一步，你都选择最陡的下坡路，你会比盲目地朝一个随机的方向走更快地到达你的爱人。这需要你一些时间，但是按照上面的策略，你最终会找到你的爱人(干杯！).</p><p id="d804" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">根据上面的类比:</p><ul class=""><li id="09c6" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">您的位置</strong> -当前故障点功能</li><li id="c108" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">你女朋友的位置</strong>——误差函数最低的点</li><li id="e283" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">最陡下降</strong> -误差函数的导数(该点切线的斜率)</li></ul><p id="d3b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这可以想象如下:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ne"><img src="../Images/01557caa4d227e82d243dbb88ff219ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3D57ViXF-xr-oMH0sc2JaQ.jpeg"/></div></div></figure><p id="8558" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从图中可以看出，我们的目标是到达图表中的最底部，也就是说，当它的值最小时。</p><p id="dd82" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">方法是将误差函数w.r.t对参数w0和w1求导。我们在下降速度最快的方向上逐步降低误差函数。</p><p id="eb1b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每一步的大小由参数<em class="mv"> alpha </em> <strong class="jp ir"> <em class="mv">，</em> </strong>决定，称为<strong class="jp ir">学习率</strong>。较小的α会导致较小的步长，而较大的α会导致较大的步长。采取步骤的方向由E(w0，w1)的偏导数决定。根据一个人在图上的起点，他可能会在不同的点结束。</p><p id="7cde" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上图中，我们注意到对于两个不同的起点，我们到达不同的最低点(局部最小值)。我们稍后将详细讨论这一点。</p><p id="ce3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度下降算法可以封装在一个公式中:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nw"><img src="../Images/eff9f5cbe12dd2272cde5b4f7cd8bd0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cinnMmK2F9M3pGcuTwRHzA.png"/></div></div></figure><p id="d7c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可能会出现一个问题:</p><blockquote class="nx ny nz"><p id="4331" class="jn jo mv jp b jq jr js jt ju jv jw jx oa jz ka kb ob kd ke kf oc kh ki kj kk ij bi translated"><strong class="jp ir">为什么取导数的负值，而不取正值？</strong></p></blockquote><p id="8220" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">任何函数的导数的方向都是该函数最大<strong class="jp ir">增加</strong>的方向。因为我们的目标是E(w0，w1)(我们的函数)的<strong class="jp ir">最小化</strong>，我们选择函数的负导数，因为它给出了函数的最大<strong class="jp ir">减少</strong>的方向。</p><p id="36c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了直观地理解它，考虑下面的例子:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ne"><img src="../Images/5e02a65237d970998625a4504eecde5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p6_lr8QQAek_wOGbOHryyw.jpeg"/></div></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi od"><img src="../Images/5f7f669d1eb49b9da72f6f0ced207f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jEF8AVVwtqhMSwHnUEM_cQ.png"/></div></div></figure><p id="6765" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以清楚地看到:</p><ul class=""><li id="ee42" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">当我们考虑正斜率时，w1减小，因此误差函数也减小</li><li id="8810" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">当我们考虑负斜率时，w1增加，因此误差函数也增加</li></ul><p id="2f47" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一个重要的问题是:</p><blockquote class="nx ny nz"><p id="0bf7" class="jn jo mv jp b jq jr js jt ju jv jw jx oa jz ka kb ob kd ke kf oc kh ki kj kk ij bi translated"><strong class="jp ir">梯度下降如何以固定步长<em class="iq">大小</em> alpha <em class="iq">收敛？</em>T11】</strong></p></blockquote><p id="bf0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">收敛背后的直觉是，当我们接近凸函数的底部时，导数接近0。在最小值点，导数将始终为0，因此，我们得到:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi oe"><img src="../Images/cdc04f220bdc9c40d6c076d11e266064.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*9Iuj5VHPc7BMynBvgqKkAQ.png"/></div></div></figure><p id="9dfc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们接近局部最小值时，梯度下降将自动采取较小的步骤。偏导数的值会随着我们向下移动到局部最小值而减小。这就是为什么，我们确实需要改变学习速度。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ne"><img src="../Images/ad0e499785060260c7196740ce281f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Ki7oFfvAgwPZjjaGdgZ1A.jpeg"/></div></div></figure><p id="7cbb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当梯度下降专门应用于线性回归的情况时，可以导出一种新形式的梯度下降方程:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi of"><img src="../Images/ddbda5b77222b425d267e3a3664c1594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KZWn_a4Kay48EvHmSVnhCA.png"/></div></div></figure><p id="519e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所有这些的要点是，如果我们从估计量(g(X))的猜测开始，然后重复应用梯度下降算法，我们的估计量将变得越来越精确。</p><p id="958e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上述梯度下降算法被称为<strong class="jp ir">批量梯度下降</strong>，因为它查看整个训练集中的每个示例(求和部分)。它对<a class="ae mk" href="https://en.wikipedia.org/wiki/Convex_function" rel="noopener ugc nofollow" target="_blank">凸函数</a>非常有效，因为凸函数只有一个全局最小值(即只有一个局部最小值)。然而，当我们有<a class="ae mk" href="https://en.wikipedia.org/wiki/Concave_function" rel="noopener ugc nofollow" target="_blank">凹函数</a>(凹函数有不止一个局部极小值)时，这在计算上极其昂贵并且根本不可行。</p><p id="bb3e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，为了减轻计算，使用梯度下降的另一个变体，称为<strong class="jp ir">随机梯度下降</strong>。在随机梯度下降中，我们不是查看每步的整个训练集(求和部分)，而是只查看每步的一个样本。单个样本可能有噪声，因此随机梯度下降的许多变体使用小批量(每步几个样本，而不是每步一个样本)。</p><p id="d37e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你想看看批量梯度下降与随机梯度下降，我强烈推荐<a class="ml mm ep" href="https://medium.com/u/1d8994ad0efc?source=post_page-----cb6f04006b29--------------------------------" rel="noopener" target="_blank"> Aerin Kim🙏</a>的<a class="ae mk" rel="noopener" target="_blank" href="/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1">文章</a>和这个<a class="ae mk" href="https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent" rel="noopener ugc nofollow" target="_blank"> StackExchange回答</a>。</p><p id="a98d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要了解凸凹函数的区别，看一下这篇<a class="ae mk" href="https://www.emathhelp.net/notes/calculus-1/convex-and-concave-functions/definition-of-convex-and-concave-functions/" rel="noopener ugc nofollow" target="_blank">文章</a>。</p><p id="c026" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于梯度下降需要记住的几点:</p><ul class=""><li id="f04b" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">梯度下降需要特征缩放</strong>，因为如果缩放不均匀，梯度可能会花费很长时间，并在最终找到全局最小值之前来回振荡</li><li id="f71f" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">在特征缩放中，每个特征(即X)大约在-1 ≤ X ≤ 1的范围内</li><li id="bde4" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">特征缩放将使梯度下降<strong class="jp ir">运行得更快</strong>和<strong class="jp ir">在更少的迭代中收敛</strong></li><li id="8f02" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">要素缩放包括将输入值除以范围(即最大值-最小值)或标准差</li><li id="086d" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">另一个称为<strong class="jp ir">均值归一化</strong>的变量，包括用相同变量的平均值减去输入变量的值</li><li id="c416" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">结合特征缩放和均值归一化，我们得到:</li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/7e1e3ee9f4b0a509d36870cc90a99e2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*yY00BWMIenga9CBGmqP_lw.png"/></div></figure><ul class=""><li id="d116" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">为了确保梯度下降能够正常工作，我们可以绘制一个如下图:</li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi og"><img src="../Images/95acd039958a534b985fdc250055d7bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qiYA1dm-fVZ_W5SPUFER_Q.jpeg"/></div></div></figure><ul class=""><li id="175c" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">我们可以在每100次迭代中评估E(W)的值。要点是E(W)应该在每次迭代后减小</li><li id="7e3e" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">对于特定应用，梯度下降收敛所需的迭代次数可能会有很大变化。可能需要30、3000、300000次迭代</li><li id="a509" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">很难预先知道梯度下降需要多少次迭代才能收敛。通常通过绘制如上图，我们可以发现梯度下降是否收敛。</li><li id="3dfc" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">也有可能提出一个<strong class="jp ir">自动收敛测试</strong>:如果E(W)减少小于ε(比如10^-3)in一次迭代),则宣布收敛</li><li id="3b31" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">ε是一个很小的值，是一个阈值。一般来说，选择阈值的值是非常困难的。因此，依靠图表是更可取的</li><li id="024e" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">有时，你的E(W)可能在每次迭代中增加而不是减少</li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ne"><img src="../Images/84140676582bf8015406ef7f374bb4f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*drKqU9lp2gdzL8wecf5MEg.jpeg"/></div></div></figure><ul class=""><li id="796a" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">发生这种情况是因为:</li></ul><ol class=""><li id="8ec0" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk mj kr ks kt bi translated">梯度下降可能不起作用</li><li id="ab3f" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mj kr ks kt bi translated">阿尔法太大了</li><li id="1ccd" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mj kr ks kt bi translated">你的代码中有一个错误</li></ol><ul class=""><li id="f5a8" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">选择α值是梯度下降算法的一个重要部分</li><li id="7f08" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">如果α太小:收敛速度慢</li><li id="2a4a" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">如果α太大:E(W)可能不会在每次迭代中减少，也可能不会收敛</li><li id="e204" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">Andrew NG提出的一个很好的启发是:</li></ul><ol class=""><li id="0e41" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk mj kr ks kt bi translated">最初，设置alpha = 0.001(或任何小值)</li><li id="0a47" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mj kr ks kt bi translated">将后续alphas设置为大约是其先前值的3倍，即0.001、0.003、0.01、0.03等等</li><li id="f5d2" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mj kr ks kt bi translated">对于每个α值，绘制E(W)作为迭代次数的函数</li><li id="06e1" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mj kr ks kt bi translated">选择看起来能使E(W)迅速减小的α值</li></ol><ul class=""><li id="cee2" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">当特征数较少时，最好使用正规方程组方法</li><li id="8195" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">如果数据集太大，使用小批量随机梯度下降比批量梯度下降更好</li></ul></div><div class="ab cl kz la hu lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ij ik il im in"><h1 id="79e5" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">多元线性回归</h1><p id="6c6a" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">多元意味着我们有不止一个变量(特征)。它的工作方式与一元线性回归完全相同。我们只需要推广多元线性回归的上述方程。</p><h1 id="6a28" class="lg lh iq bd li lj nl ll lm ln nm lp lq lr nn lt lu lv no lx ly lz np mb mc md bi translated">1.正规方程方法</h1><p id="fffe" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">矩阵的维数变了，但我们的正规方程保持不变。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/b681e8bd5bed1e349ef96aee871124b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*9ubjTkyaF5lJkL1a2_PuxA.png"/></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi oi"><img src="../Images/b22d42c68f2208eb94010e7b4b36e0dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k4xv8S3IE-jVQkWJJxLR-A.png"/></div></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi oj"><img src="../Images/f8054783149a497f57e0a3b298e8c1be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qqtq6pLCgFummzOD8Y7OCw.png"/></div></div></figure><h1 id="4981" class="lg lh iq bd li lj nl ll lm ln nm lp lq lr nn lt lu lv no lx ly lz np mb mc md bi translated">2.梯度下降</h1><p id="08c3" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">梯度下降方程本身一般也是这种形式；我们只需要对k个特性重复这个过程。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ok"><img src="../Images/1540ed686bb4c69165942b54c9ad4ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oNifDr4rMf21mdRzzyOn4Q.png"/></div></div></figure><h1 id="38c9" class="lg lh iq bd li lj nl ll lm ln nm lp lq lr nn lt lu lv no lx ly lz np mb mc md bi translated">多项式回归</h1><p id="e074" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">多项式回归是回归分析的一种形式，其中自变量(在我们的例子中是X)和因变量(r)之间的关系被建模为次数≥ 2的多项式。</p><p id="3f66" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">次数= 1的多项式是线性方程。我们上面看到的例子都是次数= 1的多项式。</p><p id="b9e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">多项式回归允许您使用线性回归的机制来拟合非常复杂的非线性函数。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ne"><img src="../Images/e0be6cde651e2e7b78832dfc263bccb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BcL7HjgLpr8Zy-fVIdOBkQ.jpeg"/></div></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/b2a3c1f597acd759d2998c7959837e14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*6NSHyiGXzYmBC0b6ORAlug.png"/></div></figure><p id="b113" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将多项式模型表示为线性模型的一个小技巧是通过替换。内容如下:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi om"><img src="../Images/5053b02e0b208464a4def6b05a6e2df2.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*F4fjN9yo3g5kaLGVC814tQ.png"/></div></figure><p id="c55e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们像这样选择我们的特征，那么特征缩放对于梯度下降算法变得越来越重要。</p><p id="d601" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">高次多项式有过度拟合的趋势。因此，高次多项式的精度将优于低次多项式的精度。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi on"><img src="../Images/9c6f6bee46040116bdc138ca4fe9da4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E_AXJpqiG7tzAvb9St_Avw.jpeg"/></div></div></figure><p id="ef99" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了解决过度拟合的问题并实现良好的通用模型，我们有两种选择:</p><ul class=""><li id="92b7" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">减少功能的数量:</strong>这是通过只选择最重要的功能，扔掉那些不太有用的功能来实现的。为了自动选择重要的特征，我们使用了<strong class="jp ir">模型选择方法</strong>，例如<strong class="jp ir">最佳子集选择</strong>、<strong class="jp ir">逐步选择</strong>(正向和反向)、以及<strong class="jp ir">逐步回归</strong>等等(我们将在后面介绍这些方法！)</li></ul><p id="d487" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上述方法的问题是它导致信息的删除(特征或者被保留或者被丢弃)。因此，它通常表现出很高的方差</p><ul class=""><li id="70c5" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">正则化(或收缩方法):</strong>正则化是引入惩罚项(称为lambda)的过程。该λ确保惩罚高复杂度模型(更高阶多项式)，即，它平滑系数(w0，w1，…，wK)。为了说明正则化如何帮助我们获得更好的拟合，考虑这个例子:</li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi ne"><img src="../Images/1bd0943ff0898ab755e5914e26605bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*suqmowMk5zwzpQzmBtajtw.jpeg"/></div></div></figure><p id="6797" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有两种常用的正则化类型:</p><ol class=""><li id="a132" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk mj kr ks kt bi translated">L2正则化(岭回归)</li><li id="49ca" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mj kr ks kt bi translated">L1正则化(拉索回归)</li></ol><h1 id="dfb1" class="lg lh iq bd li lj nl ll lm ln nm lp lq lr nn lt lu lv no lx ly lz np mb mc md bi translated">1.L2正则化(岭回归)</h1><p id="573c" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">L2正则化由以下公式给出:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi oo"><img src="../Images/c3ceb3d2d0ec80cee1e42120af5920f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZ0Xe1Y4Uw_PeOC3jMji1A.png"/></div></div></figure><p id="ad6c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，我们在正则化项中不考虑w0。为什么？因为这是一种惯例，在实践中，我们是否包括w0关系不大。</p><h1 id="df59" class="lg lh iq bd li lj nl ll lm ln nm lp lq lr nn lt lu lv no lx ly lz np mb mc md bi translated">2.L1正则化(拉索回归)</h1><p id="5f07" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">LASSO(最小绝对收缩和选择运算符)由以下公式给出:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi op"><img src="../Images/53611104a577a170693d9e8790e7e2d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*wqXJMcGfiANNbpYJYdDW7Q.png"/></div></figure><p id="255e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里唯一的区别是我们考虑参数w的模，而不是它们的平方。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi oq"><img src="../Images/6abda86600bd93c3874c4bf382503ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CSl_AmAYtk262lbAwU4Evw.png"/></div></div></figure><p id="6be6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">比脊和套索更好的正则化方法是<strong class="jp ir">弹性网正则化。</strong></p><p id="b15b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">弹性网络正则化结合了脊和套索，由以下公式给出:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi or"><img src="../Images/b524f63a81f48f85d309fb498d21bbde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bg5RxLP5FWbSE8-TSfYsMA.png"/></div></div></figure><p id="1fc1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了理解为什么这更好的直觉，我推荐你阅读提出这种方法的<a class="ae mk" href="https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>。但是简而言之，当你有高度相关的特性时，那么弹性网就是你要走的路！</p><p id="53f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正则化的一个问题是选择λ的值:</p><ul class=""><li id="8a5a" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">如果lambda太高，模型就会太简单，从而导致拟合不足</li><li id="85c9" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">如果lambda太低，你实际上没有受到惩罚，因此，你的模型仍然很复杂</li></ul><p id="ef80" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了选择最佳的λ，在不同的λ值上使用<strong class="jp ir">交叉验证</strong>，选择产生最低E(W)的λ。</p><p id="9db5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一个<strong class="jp ir">要注意的要点</strong>:在使用正则化时，鼓励特征缩放，因此参数的惩罚是基于它们的预测能力，而不是它们的比例。</p></div><div class="ab cl kz la hu lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ij ik il im in"><p id="81fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我希望这篇文章能够帮助您更好地理解回归。伙计们，这几周就到这里吧！下一篇帖子再见！</p></div></div>    
</body>
</html>