<html>
<head>
<title>A quick introduction to Slow Feature Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">慢速特征分析快速介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-brief-introduction-to-slow-feature-analysis-18c901bc2a58?source=collection_archive---------0-----------------------#2017-10-21">https://towardsdatascience.com/a-brief-introduction-to-slow-feature-analysis-18c901bc2a58?source=collection_archive---------0-----------------------#2017-10-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/5e829e207e31b0f89237867585486009.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uStjnI1dTcQa-TejVH4v2Q.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><a class="ae jd" href="https://en.wikipedia.org/wiki/File:Grapevinesnail_01.jpg" rel="noopener ugc nofollow" target="_blank">Photograph</a> by <a class="ae jd" href="https://de.wikipedia.org/wiki/Benutzer:Heliodor" rel="noopener ugc nofollow" target="_blank">Jürgen Schoner</a> / <a class="ae jd" href="https://creativecommons.org/licenses/by-sa/3.0/deed.en" rel="noopener ugc nofollow" target="_blank">CC BY-SA 3.0</a></figcaption></figure><div class=""/><div class=""><h2 id="5bc3" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">使用python和数学的面向应用的SFA介绍</h2></div><p id="6886" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我最近开始在波鸿鲁尔大学攻读机器学习的博士学位。我加入的小组的一个主要研究课题叫做慢特征分析(SFA)。为了了解一个新的话题，在让自己沉浸在数学的严谨中之前，如果可能的话，我喜欢看例子和直观的解释。我写这篇博文是为了那些喜欢以类似方式研究主题的人，因为我相信SFA是非常强大和有趣的。</p><p id="3322" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇文章中，我将以一个应用SFA的代码示例开始，来帮助激发这种方法。然后，我将更详细地介绍该方法背后的数学原理，最后提供该材料中其他优秀资源的链接。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h2 id="478e" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">1.确定平滑潜变量</h2><p id="bdb0" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">SFA是一种无监督学习方法，从时间序列中提取最平滑(最慢)的底层函数或<em class="mw">特征</em>。这可以用于降维、回归和分类。例如，我们可以有一个高度不稳定的序列，它是由一个行为更好的潜在变量决定的。</p><p id="fd72" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们从生成时间序列<em class="mw"> D </em>和<em class="mw"> S: </em>开始</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/be57d7fd2956883a4ebc5aad50a44820.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*ZTtFXX3h0BpmGfNZgEWMVw@2x.png"/></div></figure><figure class="my mz na nb gt is"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="e523" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就是所谓的逻辑地图。通过绘制系列<em class="mw"> S </em>，我们可以考察它的混沌本质。驱动上述曲线行为的基本时间序列<em class="mw"> D </em>要简单得多:</p><figure class="my mz na nb gt is"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/5e61da9e11e8742207bee1665380bea4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*yaULSUcgffWDKJd3VYKm8A.png"/></div></figure><p id="8d78" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们如何从不稳定的时间序列中确定简单的潜在驱动力？</p><p id="6443" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以用SFA来确定一个函数变化最慢的特征。在我们的例子中，我们将从像<em class="mw"> S </em>这样的数据开始，以<em class="mw"> D </em>结束，而不必事先知道<em class="mw"> S </em>是如何生成的。</p><p id="e52c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">SFA的实现旨在找到输入的特征，这些特征是线性的。但是从我们的例子可以看出，驱动力<em class="mw"> D </em>是高度非线性的！这可以通过首先对时间序列<em class="mw"> S </em>进行非线性扩展来弥补，然后找到扩展数据的线性特征。通过这样做，我们发现了原始数据的非线性特征。</p><p id="c937" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们通过堆叠<em class="mw"> S </em>的时间延迟副本来创建一个新的多元时间序列:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/40b8d291094ae4c5405492e2cefa56b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*ab8rTtHFETyk6BNgW7f3hA@2x.png"/></div></figure><figure class="my mz na nb gt is"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="5ffe" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我们对数据进行立方扩展，并提取SFA特征。三次展开将一个四维向量[ <em class="mw"> a </em>、<em class="mw"> b </em>、<em class="mw"> c </em>、<em class="mw">d</em>]ᵀ]变成34个元素向量，其中元素<em class="mw"> t </em> <em class="mw">、t v、tvu、t </em> <em class="mw">、tv、t </em>为不同的<em class="mw"> t、u、v </em> ∈{a、b、c、d}。</p><figure class="my mz na nb gt is"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="146c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请记住，添加时间延迟副本的最佳数量因问题而异。或者，如果原始数据维数过高，则需要进行降维，例如使用<a class="ae jd" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析</a>。</p><p id="d1ad" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，考虑以下是该方法的超参数:维度扩展(缩减)的方法、扩展(缩减)后的输出维度以及待发现的慢特征的数量。</p><p id="8240" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，添加时间延迟副本后，时间序列的长度从300变为297。因此，慢特征时间序列的相应长度也是297。为了更好的可视化，我们将第一个值添加到它前面，并将最后一个值添加两次，从而将它的长度变为300。SFA发现的特征具有零均值和单位方差，因此在可视化结果之前，我们也对<em class="mw"> D </em>进行归一化。</p><figure class="my mz na nb gt is"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="ffa4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">即使只考虑300个数据点，SFA特性也能几乎完全恢复底层源代码——这令人印象深刻！</p><figure class="my mz na nb gt is"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/1dd89234bcdc00549d06bf57d9754b6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*hkkZzhLoHQ8UUxP1vZtgnA.png"/></div></figure></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h2 id="2f1d" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">2.引擎盖下到底发生了什么？</h2><p id="85a1" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">理论上，SFA算法接受一个(多变量)时间序列<strong class="kx jh"> <em class="mw"> X </em> </strong>和一个整数<em class="mw"> m </em>作为输入，该整数表示要从该序列中提取的特征的数量，其中<em class="mw"> m </em>小于时间序列的维数。该算法确定了<em class="mw"> m个</em>函数</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ecb299bb30ad40b6a00dee958acf97cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*7_oeOM9GQvNqzsCyVkCILA@2x.png"/></div></figure><p id="5344" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使得每个<em class="mw"> yᵢ </em>的两个连续时间点的时间导数的平方的平均值最小化。直觉上，我们希望最大化这些特性的缓慢性:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/03a2d377e9697ed741600c2db8aae196.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*dhDysN35AkAdq0dW9QIcGA@2x.png"/></div></figure><p id="55ce" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中点表示时间导数，在离散情况下:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d2c22cd88576d14ed0fd232626c3eea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*ERClWBBTUork8rtAYAeLvw@2x.png"/></div></figure><p id="a949" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">目标函数(1)测量特征的慢度。零均值约束(2)使得特征的二阶矩和方差相等，并简化了符号。单位方差约束(3)丢弃常数解。</p><p id="6e0f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后的约束(4)使我们的特征去相关，并导致它们的慢度排序。这意味着我们首先找到最慢的特征，然后我们找到下一个最慢的特征，它与前一个特征正交，依此类推。对特征进行去相关可以确保我们捕捉到最多的信息。</p><p id="6df1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在接下来的内容中，我浏览了重要的细节并跳过了一些步骤，但是为了完整起见，我想把它包括进来。我建议也看看下面的链接，以获得更全面的解释。</p><p id="ce5c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们只考虑线性特征:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/d5f0e803c4f0a1aa2ec284ae11b76d1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*zDmsOtV4Vn2gi9K11wu81w@2x.png"/></div></figure><p id="e1f9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">时间序列X可以是“原始数据”或其非线性扩展，见上例。请记住，即使这些是扩展数据的线性特征，它们也可能是原始数据的非线性特征。</p><p id="2f20" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设零均值<strong class="kx jh"><em class="mw"/></strong>，通过求解<a class="ae jd" href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Generalized_eigenvalue_problem" rel="noopener ugc nofollow" target="_blank">广义特征值问题</a><strong class="kx jh"><em class="mw">AW</em></strong>=<strong class="kx jh"><em class="mw">bwλ</em></strong>找到线性特征。我们确定<em class="mw"> m个</em>特征值-特征向量元组(<em class="mw"> λᵢ </em>，<em class="mw"> Wᵢ </em>)使得<strong class="kx jh"><em class="mw">a</em></strong><em class="mw">wᵢ</em>=<em class="mw">λᵢ</em><strong class="kx jh"><em class="mw">b</em></strong><em class="mw"/>我们有</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/b7b886dbfb83816e0e46371c0b4f65ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*vu-6vE7tBgFs4vopX7jlTg@2x.png"/></div></figure><p id="30a5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">标量<em class="mw"> λᵢ </em>表示特征的慢度，即<em class="mw"> λᵢ </em>越小，相应的<em class="mw"> yᵢ </em>变化越慢。如果你熟悉广义特征值问题，注意这里的特征值是增加的——而不是减少的。最后，特征向量<em class="mw"> Wᵢ </em>是定义我们学习特征的变换向量。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h2 id="46c1" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">3.进一步阅读</h2><p id="e0ed" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">原文:<a class="ae jd" href="https://www.ini.rub.de/PEOPLE/wiskott/Reprints/WiskottSejnowski-2002-NeurComp-LearningInvariances.pdf" rel="noopener ugc nofollow" target="_blank">https://www . ini . rub . de/PEOPLE/wiskott/reprings/WiskottSejnowski-2002-neur comp-learning invariances . pdf</a></p><p id="9415" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">SFA在分类中的应用:<a class="ae jd" href="http://cogprints.org/4104/1/Berkes2005a-preprint.pdf" rel="noopener ugc nofollow" target="_blank">http://cogprints.org/4104/1/Berkes2005a-preprint.pdf</a></p><p id="bbc2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以上例子改编自:<a class="ae jd" href="http://mdp-toolkit.sourceforge.net/examples/logmap/logmap.html" rel="noopener ugc nofollow" target="_blank">http://MDP-toolkit . SourceForge . net/examples/log map/log map . html</a></p><p id="f150" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">— <br/>在<a class="ae jd" href="https://twitter.com/hlynurd" rel="noopener ugc nofollow" target="_blank">推特</a>上关注我</p><figure class="my mz na nb gt is gh gi paragraph-image"><a href="https://www.buymeacoffee.com/hlynurd"><div class="gh gi nl"><img src="../Images/20e6799fc2e5025bed304da4cfe2d2b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*uuBfpvmZZ1b9X5WTZk-svQ.png"/></div></a></figure></div></div>    
</body>
</html>