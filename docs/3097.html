<html>
<head>
<title>Multinomial Naive Bayes Classifier for Text Analysis (Python)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于文本分析的多项式朴素贝叶斯分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67?source=collection_archive---------1-----------------------#2018-04-09">https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67?source=collection_archive---------1-----------------------#2018-04-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/7e30b390228d5e329e6ab50ebba7fd33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ppG7YzWSYtO1u1oW370svA.jpeg"/></div></div></figure><p id="6f5d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">机器学习最流行的应用之一是分类数据的分析，特别是文本数据。问题是，有很多关于数字数据的教程，但是关于文本的很少。考虑到我过去关于机器学习的大多数博客都是基于Scikit-Learn的，我决定通过自己实现整个事情来玩玩这个。</p><p id="efdd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇博客中，我将介绍如何为<a class="ae kw" href="http://qwone.com/~jason/20Newsgroups/" rel="noopener ugc nofollow" target="_blank"> 20个新闻组</a>数据集实现多项式朴素贝叶斯分类器。20个新闻组数据集包含大约18000个关于20个主题的新闻组帖子，分为两个子集:一个用于培训(或开发)，另一个用于测试(或性能评估)。训练集和测试集之间的划分基于在特定日期之前和之后发布的消息。</p><h2 id="c049" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">图书馆</h2><p id="33af" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">首先，让我们导入编写实现所需的库:</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="7ee0" class="kx ky iq ma b gy me mf l mg mh"><strong class="ma ir">import</strong> <strong class="ma ir">numpy</strong> <strong class="ma ir">as</strong> <strong class="ma ir">np</strong><br/><strong class="ma ir">import</strong> <strong class="ma ir">pandas</strong> <strong class="ma ir">as</strong> <strong class="ma ir">pd</strong><br/><strong class="ma ir">import</strong> <strong class="ma ir">matplotlib.pyplot</strong> <strong class="ma ir">as</strong> <strong class="ma ir">plt</strong><br/><strong class="ma ir">import</strong> <strong class="ma ir">operator</strong></span></pre><h2 id="d578" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">阶级分布</h2><p id="dea3" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">首先，我们计算每个类中文档的比例:</p><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/c3d00fc4f01583622f6be9e8a25fe916.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/1*j74sR42ZoyGO0jZmxc7Efw.gif"/></div></figure><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="2faf" class="kx ky iq ma b gy me mf l mg mh"><em class="mj">#Training label</em><br/>train_label = open('20news-bydate/matlab/train.label')<br/><br/><em class="mj">#pi is the fraction of each class</em><br/>pi = {}<br/><br/><em class="mj">#Set a class index for each document as key</em><br/><strong class="ma ir">for</strong> i <strong class="ma ir">in</strong> range(1,21):<br/>    pi[i] = 0<br/>    <br/><em class="mj">#Extract values from training labels</em><br/>lines = train_label.readlines()<br/><br/><em class="mj">#Get total number of documents</em><br/>total = len(lines)<br/><br/><em class="mj">#Count the occurence of each class</em><br/><strong class="ma ir">for</strong> line <strong class="ma ir">in</strong> lines:<br/>    val = int(line.split()[0])<br/>    pi[val] += 1<br/><br/><em class="mj">#Divide the count of each class by total documents </em><br/><strong class="ma ir">for</strong> key <strong class="ma ir">in</strong> pi:<br/>    pi[key] /= total<br/>    <br/>print("Probability of each class:")<br/>print("<strong class="ma ir">\n</strong>".join("<strong class="ma ir">{}</strong>: <strong class="ma ir">{}</strong>".format(k, v) <strong class="ma ir">for</strong> k, v <strong class="ma ir">in</strong> pi.items()))</span></pre><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/de698d60180c08a160a5babeabd95e1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*jq7BB1CIkcKfq82xyCM7gw.png"/></div></figure><h2 id="510d" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">词汇的概率分布</h2><p id="adca" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">让我们首先创建熊猫数据框架</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="ed25" class="kx ky iq ma b gy me mf l mg mh"><em class="mj">#Training data</em><br/>train_data = open('20news-bydate/matlab/train.data')<br/>df = pd.read_csv(train_data, delimiter=' ', names=['docIdx', 'wordIdx', 'count'])<br/><br/><em class="mj">#Training label</em><br/>label = []<br/>train_label = open('/home/sadat/Downloads/HW2_210/20news-bydate/matlab/train.label')<br/>lines = train_label.readlines()<br/><strong class="ma ir">for</strong> line <strong class="ma ir">in</strong> lines:<br/>    label.append(int(line.split()[0]))<br/><br/><em class="mj">#Increase label length to match docIdx</em><br/>docIdx = df['docIdx'].values<br/>i = 0<br/>new_label = []<br/><strong class="ma ir">for</strong> index <strong class="ma ir">in</strong> range(len(docIdx)-1):<br/>    new_label.append(label[i])<br/>    <strong class="ma ir">if</strong> docIdx[index] != docIdx[index+1]:<br/>        i += 1<br/>new_label.append(label[i]) <em class="mj">#for-loop ignores last value</em><br/><br/><em class="mj">#Add label column</em><br/>df['classIdx'] = new_label<br/><br/>df.head()</span></pre><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/9df9989942f4914a30375526faecf1b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*yW_yR5nYfqHd8hvS516MgQ.png"/></div></figure><h2 id="af9b" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">每类每个单词的概率</h2><p id="991f" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">为了计算我们的概率，我们将找到给定类别中每个单词的平均值。</p><p id="81d6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于j类和单词I，平均值由下式给出:</p><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/39bbbf1e6d1351bd89a0f5f37e5b9689.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*HLazt-2uGfTuZMQbfbeUzQ.png"/></div></figure><p id="944b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是，由于一些单词的计数为0，我们将使用low执行拉普拉斯平滑:</p><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/1c4f1e05fb4e65a2e5f3eb37d7c633eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*L0RGaXZGrsAxkkFoINabUw.png"/></div></figure><p id="be1f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中V是词汇表中所有单词的数组</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="30bf" class="kx ky iq ma b gy me mf l mg mh"><em class="mj">#Alpha value for smoothing</em><br/>a = 0.001<br/><br/><em class="mj">#Calculate probability of each word based on class</em><br/>pb_ij = df.groupby(['classIdx','wordIdx'])<br/>pb_j = df.groupby(['classIdx'])<br/>Pr =  (pb_ij['count'].sum() + a) / (pb_j['count'].sum() + 16689)    <br/><br/><em class="mj">#Unstack series</em><br/>Pr = Pr.unstack()<br/><br/><em class="mj">#Replace NaN or columns with 0 as word count with a/(count+|V|+1)</em><br/><strong class="ma ir">for</strong> c <strong class="ma ir">in</strong> range(1,21):<br/>    Pr.loc[c,:] = Pr.loc[c,:].fillna(a/(pb_j['count'].sum()[c] + 16689))<br/><br/><em class="mj">#Convert to dictionary for greater speed</em><br/>Pr_dict = Pr.to_dict()<br/><br/>Pr</span></pre><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mo"><img src="../Images/66049a454bc4f0de40398ebd4815ea3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rfv9c70use7jv1oguonaA.png"/></div></div></figure><h2 id="e66c" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">停止言语</h2><p id="121c" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">停用词是在每个文档中出现很多的词(例如介词和代词)。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="c709" class="kx ky iq ma b gy me mf l mg mh"><em class="mj">#Common stop words from online</em><br/>stop_words = [<br/>"a", "about", "above", "across", "after", "afterwards", <br/>"again", "all", "almost", "alone", "along", "already", "also",    <br/>"although", "always", "am", "among", "amongst", "amoungst", "amount", "an", "and", "another", "any", "anyhow", "anyone", "anything", "anyway", "anywhere", "are", "as", "at", "be", "became", "because", "become","becomes", "becoming", "been", "before", "behind", "being", "beside", "besides", "between", "beyond", "both", "but", "by","can", "cannot", "cant", "could", "couldnt", "de", "describe", "do", "done", "each", "eg", "either", "else", "enough", "etc", "even", "ever", "every", "everyone", "everything", "everywhere", "except", "few", "find","for","found", "four", "from", "further", "get", "give", "go", "had", "has", "hasnt", "have", "he", "hence", "her", "here", "hereafter", "hereby", "herein", "hereupon", "hers", "herself", "him", "himself", "his", "how", "however", "i", "ie", "if", "in", "indeed", "is", "it", "its", "itself", "keep", "least", "less", "ltd", "made", "many", "may", "me", "meanwhile", "might", "mine", "more", "moreover", "most", "mostly", "much", "must", "my", "myself", "name", "namely", "neither", "never", "nevertheless", "next","no", "nobody", "none", "noone", "nor", "not", "nothing", "now", "nowhere", "of", "off", "often", "on", "once", "one", "only", "onto", "or", "other", "others", "otherwise", "our", "ours", "ourselves", "out", "over", "own", "part","perhaps", "please", "put", "rather", "re", "same", "see", "seem", "seemed", "seeming", "seems", "she", "should","since", "sincere","so", "some", "somehow", "someone", "something", "sometime", "sometimes", "somewhere", "still", "such", "take","than", "that", "the", "their", "them", "themselves", "then", "thence", "there", "thereafter", "thereby", "therefore", "therein", "thereupon", "these", "they",<br/>"this", "those", "though", "through", "throughout",<br/>"thru", "thus", "to", "together", "too", "toward", "towards",<br/>"under", "until", "up", "upon", "us",<br/>"very", "was", "we", "well", "were", "what", "whatever", "when",<br/>"whence", "whenever", "where", "whereafter", "whereas", "whereby",<br/>"wherein", "whereupon", "wherever", "whether", "which", "while", <br/>"who", "whoever", "whom", "whose", "why", "will", "with",<br/>"within", "without", "would", "yet", "you", "your", "yours", "yourself", "yourselves"<br/>]</span></pre><p id="294b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，让我们创建词汇数据框架</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="de64" class="kx ky iq ma b gy me mf l mg mh">vocab = open('vocabulary.txt') <br/>vocab_df = pd.read_csv(vocab, names = ['word']) <br/>vocab_df = vocab_df.reset_index() <br/>vocab_df['index'] = vocab_df['index'].apply(<strong class="ma ir">lambda</strong> x: x+1) vocab_df.head()</span></pre><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/d031972e824f4c1ca977fcec15924e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/format:webp/1*JWEBdwrTFK0UKWUx6QIF5w.png"/></div></figure><p id="b70c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">获取词汇表中每个单词的计数并将停用词设置为0:</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="de8a" class="kx ky iq ma b gy me mf l mg mh"><em class="mj">#Index of all words</em><br/>tot_list = set(vocab_df['index'])<br/><br/><em class="mj">#Index of good words</em><br/>vocab_df = vocab_df[~vocab_df['word'].isin(stop_words)]<br/>good_list = vocab_df['index'].tolist()<br/>good_list = set(good_list)<br/><br/><em class="mj">#Index of stop words</em><br/>bad_list = tot_list - good_list<br/><br/><em class="mj">#Set all stop words to 0</em><br/><strong class="ma ir">for</strong> bad <strong class="ma ir">in</strong> bad_list:<br/>    <strong class="ma ir">for</strong> j <strong class="ma ir">in</strong> range(1,21):<br/>        Pr_dict[j][bad] = a/(pb_j['count'].sum()[j] + 16689)</span></pre><p id="3e5d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">多项式朴素贝叶斯分类器</strong></p><p id="9fe9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">将P的概率分布与属于每个类别的文档的分数相结合。</p><p id="3d72" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于类<strong class="ka ir"> j </strong>，字频为<strong class="ka ir"> f </strong>的字<strong class="ka ir"> i </strong>:</p><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/b66a88c6af361a8ac3585ebb68a60a65.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/1*AAKwBojonSFg2tXzSyR9SQ.gif"/></div></figure><p id="a9e8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了避免下溢，我们将使用对数总和:</p><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/9a3ab55c2524ffeb1a2035070a229fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/1*ncv5t9tZWEVVW24Di5Iahw.gif"/></div></figure><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/10d841d3937164f21a111a660c39d187.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/1*inNuWwwf-2pSQO93vaOhIw.gif"/></div></figure><p id="cb9c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一个问题是，如果一个词再次出现，它再次出现的概率会上升。为了平滑这一点，我们取频率的对数:</p><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/f5469edfd1737742c009661e43d386a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/1*hQbJR5M3SaLSouzE1cPsyw.gif"/></div></figure><p id="ed55" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，为了将停用词考虑在内，我们将为每个词添加逆文档频率(IDF)权重:</p><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/e9daef2c1a7f80f7c540d425cd10b0b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/1*QbeBoi9mCJPLKBG6YlW9Sg.gif"/></div></figure><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/889cc1610e57d274816c8e2f11bc1a32.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/1*4XdLoLXubDsN9Jix_OdJVA.gif"/></div></figure><p id="95fb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">尽管对于这个特定的用例，停用词已经被设置为0，但还是添加了IDF实现来一般化该函数。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="b75b" class="kx ky iq ma b gy me mf l mg mh"><em class="mj">#Calculate IDF</em> <br/>tot = len(df['docIdx'].unique()) <br/>pb_ij = df.groupby(['wordIdx']) <br/>IDF = np.log(tot/pb_ij['docIdx'].count()) <br/>IDF_dict = IDF.to_dict()</span><span id="8f93" class="kx ky iq ma b gy mw mf l mg mh"><strong class="ma ir">def</strong> MNB(df, smooth = <strong class="ma ir">False</strong>, IDF = <strong class="ma ir">False</strong>):<br/>    <em class="mj">'''</em><br/><em class="mj">    Multinomial Naive Bayes classifier</em><br/><em class="mj">    :param df [Pandas Dataframe]: Dataframe of data</em><br/><em class="mj">    :param smooth [bool]: Apply Smoothing if True</em><br/><em class="mj">    :param IDF [bool]: Apply Inverse Document Frequency if True</em><br/><em class="mj">    :return predict [list]: Predicted class ID</em><br/><em class="mj">    '''</em><br/>    <em class="mj">#Using dictionaries for greater speed</em><br/>    df_dict = df.to_dict()<br/>    new_dict = {}<br/>    prediction = []<br/>    <br/>    <em class="mj">#new_dict = {docIdx : {wordIdx: count},....}</em><br/>    <strong class="ma ir">for</strong> idx <strong class="ma ir">in</strong> range(len(df_dict['docIdx'])):<br/>        docIdx = df_dict['docIdx'][idx]<br/>        wordIdx = df_dict['wordIdx'][idx]<br/>        count = df_dict['count'][idx]<br/>        <strong class="ma ir">try</strong>: <br/>            new_dict[docIdx][wordIdx] = count <br/>        <strong class="ma ir">except</strong>:<br/>            new_dict[df_dict['docIdx'][idx]] = {}<br/>            new_dict[docIdx][wordIdx] = count<br/><br/>    <em class="mj">#Calculating the scores for each doc</em><br/>    <strong class="ma ir">for</strong> docIdx <strong class="ma ir">in</strong> range(1, len(new_dict)+1):<br/>        score_dict = {}<br/>        <em class="mj">#Creating a probability row for each class</em><br/>        <strong class="ma ir">for</strong> classIdx <strong class="ma ir">in</strong> range(1,21):<br/>            score_dict[classIdx] = 1<br/>            <em class="mj">#For each word:</em><br/>            <strong class="ma ir">for</strong> wordIdx <strong class="ma ir">in</strong> new_dict[docIdx]:<br/>                <em class="mj">#Check for frequency smoothing</em><br/>                <em class="mj">#log(1+f)*log(Pr(i|j))</em><br/>                <strong class="ma ir">if</strong> smooth: <br/>                    <strong class="ma ir">try</strong>:<br/>                        probability=Pr_dict[wordIdx][classIdx]         <br/>                        power = np.log(1+ new_dict[docIdx][wordIdx])     <br/>                        <em class="mj">#Check for IDF</em><br/>                        <strong class="ma ir">if</strong> IDF:<br/>                            score_dict[classIdx]+=(<br/>                               power*np.log(<br/>                               probability*IDF_dict[wordIdx]))<br/>                        <strong class="ma ir">else</strong>:<br/>                            score_dict[classIdx]+=power*np.log(<br/>                                                   probability)<br/>                    <strong class="ma ir">except</strong>:<br/>                        <em class="mj">#Missing V will have log(1+0)*log(a/16689)=0 </em><br/>                        score_dict[classIdx] += 0                        <br/>                <em class="mj">#f*log(Pr(i|j))</em><br/>                <strong class="ma ir">else</strong>: <br/>                    <strong class="ma ir">try</strong>:<br/>                        probability = Pr_dict[wordIdx][classIdx]        <br/>                        power = new_dict[docIdx][wordIdx]               <br/>                        score_dict[classIdx]+=power*np.log(<br/>                                           probability) <br/>                        <em class="mj">#Check for IDF</em><br/>                        <strong class="ma ir">if</strong> IDF:<br/>                            score_dict[classIdx]+= power*np.log(<br/>                                   probability*IDF_dict[wordIdx]) <br/>                    <strong class="ma ir">except</strong>:<br/>                        <em class="mj">#Missing V will have 0*log(a/16689) = 0</em><br/>                        score_dict[classIdx] += 0      <br/>            <em class="mj">#Multiply final with pi         </em><br/>            score_dict[classIdx] +=  np.log(pi[classIdx])                          <br/><br/>        <em class="mj">#Get class with max probabilty for the given docIdx </em><br/>        max_score = max(score_dict, key=score_dict.get)<br/>        prediction.append(max_score)<br/>        <br/>    <strong class="ma ir">return</strong> prediction</span></pre><p id="b880" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">比较平滑和IDF的效果:</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="686c" class="kx ky iq ma b gy me mf l mg mh">regular_predict = MNB(df, smooth=<strong class="ma ir">False</strong>, IDF=<strong class="ma ir">False</strong>)<br/>smooth_predict  = MNB(df, smooth=<strong class="ma ir">True</strong>, IDF=<strong class="ma ir">False</strong>)<br/>tfidf_predict   = MNB(df, smooth=<strong class="ma ir">False</strong>, IDF=<strong class="ma ir">True</strong>)<br/>all_predict     = MNB(df, smooth=<strong class="ma ir">True</strong>, IDF=<strong class="ma ir">True</strong>)</span><span id="5ca6" class="kx ky iq ma b gy mw mf l mg mh"><em class="mj">#Get list of labels</em><br/>train_label = pd.read_csv('20news-bydate/matlab/train.label',<br/>                          names=['t'])<br/>train_label= train_label['t'].tolist()</span><span id="5349" class="kx ky iq ma b gy mw mf l mg mh">total = len(train_label) <br/>models = [regular_predict, smooth_predict, <br/>          tfidf_predict, all_predict] <br/>strings = ['Regular', 'Smooth', 'IDF', 'Both'] <br/> <br/><strong class="ma ir">for</strong> m,s <strong class="ma ir">in</strong> zip(models,strings):<br/>    val = 0<br/>    <strong class="ma ir">for</strong> i,j <strong class="ma ir">in</strong> zip(m, train_label):<br/>        <strong class="ma ir">if</strong> i != j:<br/>            val +=1<br/>        <strong class="ma ir">else</strong>:<br/>            <strong class="ma ir">pass</strong>   <br/>    print(s,"Error:<strong class="ma ir">\t\t</strong>",val/total * 100, "%")</span></pre><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/e317b6da6e4ef4c6b72bbb331074aa3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*75pwhreRCquE9ib4hK852A.png"/></div></figure><p id="0f06" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">正如我们所看到的，IDF几乎没有影响，因为我们删除了停用词。<br/>然而，平滑使模型更加精确。</p><p id="22e0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，我们的最佳模型是:</p><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/f5469edfd1737742c009661e43d386a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/1*hQbJR5M3SaLSouzE1cPsyw.gif"/></div></figure><h2 id="f9c7" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">测试数据</h2><p id="9179" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">现在我们有了模型，让我们用它来预测我们的测试数据。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="9938" class="kx ky iq ma b gy me mf l mg mh"><em class="mj">#Get test data</em><br/>test_data = open('20news-bydate/matlab/test.data')<br/>df = pd.read_csv(test_data, delimiter=' ', names=['docIdx', 'wordIdx', 'count'])<br/><br/><em class="mj">#Get list of labels</em><br/>test_label = pd.read_csv('/home/sadat/Downloads/HW2_210/20news-bydate/matlab/test.label', names=['t'])<br/>test_label= test_label['t'].tolist()<br/><br/><em class="mj">#MNB Calculation</em><br/>predict = MNB(df, smooth = <strong class="ma ir">True</strong>, IDF = <strong class="ma ir">False</strong>)<br/><br/>total = len(test_label)<br/>val = 0<br/><strong class="ma ir">for</strong> i,j <strong class="ma ir">in</strong> zip(predict, test_label):<br/>    <strong class="ma ir">if</strong> i == j:<br/>        val +=1<br/>    <strong class="ma ir">else</strong>:<br/>        <strong class="ma ir">pass</strong></span><span id="1ed9" class="kx ky iq ma b gy mw mf l mg mh">print("Error:<strong class="ma ir">\t</strong>",(1-(val/total)) * 100, "%")</span></pre><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/1db3bf0e0c9fb8ea1ff187184bd1acf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*Zr50vS2NbUxhM78VHQlPlQ.png"/></div></figure></div></div>    
</body>
</html>