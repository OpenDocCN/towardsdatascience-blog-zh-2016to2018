<html>
<head>
<title>DeepClassifyML Week 2 Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DeepClassifyML 第 2 周第 3 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deepclassifyml-week-2-part-3-7c080af22f8a?source=collection_archive---------10-----------------------#2017-06-22">https://towardsdatascience.com/deepclassifyml-week-2-part-3-7c080af22f8a?source=collection_archive---------10-----------------------#2017-06-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="1532" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><em class="ks">这篇文章是“Hasura 实习”系列文章的一部分，讲述了如何为当地发展建立 Hausra。除此之外，我们终于看到‘神经网络如何学习’。还查看了我以前的帖子:</em> <a class="ae kt" href="https://medium.com/@akshaybhatia10/deepclassifyml-week-1-part-1-b1c53e0a7cc" rel="noopener"> <em class="ks">第一部分</em></a><em class="ks"/><a class="ae kt" href="https://medium.com/towards-data-science/deepclassifyml-week-1-part-2-3b234ca3fcb4" rel="noopener"><em class="ks">第二部分</em></a><em class="ks"/><a class="ae kt" href="https://medium.com/towards-data-science/deepclassifyml-week-2-part-1-1e1bafca79eb" rel="noopener"><em class="ks">第三部分</em></a><em class="ks"/><a class="ae kt" href="https://medium.com/towards-data-science/deepclassifyml-week-2-part-2-1b65739c0f35" rel="noopener"><em class="ks">第四部分</em> </a> <em class="ks">对于 app 的想法以及一些计算机视觉和神经网络的基础知识。</em></p><p id="c0b6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为本地开发设置 Hasura 非常简单，因为 Hasura 在这个<a class="ae kt" href="https://github.com/hasura/local-development" rel="noopener ugc nofollow" target="_blank"> <em class="ks">自述文件</em> </a> <em class="ks"> </em>中提供的指令都有很好的文档记录，并且非常容易实现。以下是我在我的系统上是如何做到的:</p><p id="b9ec" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> <em class="ks">第一步:</em> </strong>安装<a class="ae kt" href="https://www.virtualbox.org/wiki/Downloads" rel="noopener ugc nofollow" target="_blank"><em class="ks">virtualbox</em></a><em class="ks">。</em> <strong class="jw ir"> VirtualBox </strong>是一款免费、开源、跨平台的应用，用于创建和运行虚拟机(VM)——其硬件组件由运行程序的主机计算机模拟的计算机。它允许在其上安装额外的操作系统，作为来宾操作系统，并在虚拟环境中运行。需要注意的是，主机应该至少有 4GB 的内存(因为虚拟机可能会占用高达 2GB 的内存)。还要确保你应该有一个 64 位操作系统。</p><p id="2e08" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> <em class="ks">第二步:</em> </strong>安装<em class="ks"> hasuractl。</em>在我的系统(mac)上安装它的命令是:</p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="a108" class="ld le iq kz b gy lf lg l lh li">curl -Lo hasuractl https://storage.googleapis.com/hasuractl/v0.1.2/darwin-amd64/hasuractl &amp;&amp; chmod +x hasuractl &amp;&amp; sudo mv hasuractl /usr/local/bin/</span></pre><p id="15f1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> <em class="ks">第三步:</em> </strong>安装<a class="ae kt" href="https://kubernetes.io/docs/tasks/kubectl/install/" rel="noopener ugc nofollow" target="_blank"> <em class="ks"> kubectl </em> </a> <em class="ks">。</em></p><p id="7193" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">要在 Hasura 上开始一个项目，在<a class="ae kt" href="https://beta.hasura.io/" rel="noopener ugc nofollow" target="_blank"> beta.hasura.io </a>上创建一个帐户，然后运行以下命令:</p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="54df" class="ld le iq kz b gy lf lg l lh li">hasuractl login</span></pre><figure class="ku kv kw kx gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lj"><img src="../Images/c36e3c665b432ad6d8eb2f9180497018.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-NZFm0pBfqE0bTaXvqD0lg.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Hausra Login</figcaption></figure><p id="26ab" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">登录后，运行以下命令(<em class="ks">注意:如果您是第一次运行下一个命令，它将大致下载大约 1–1.5 GB 的 docker 映像。</em>):</p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="d85d" class="ld le iq kz b gy lf lg l lh li">hasuractl local start</span></pre><figure class="ku kv kw kx gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lv"><img src="../Images/7708ea96e8aa26cefe76f7f27ff743ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*emVr7Y1JWyYlPqOoFZeLtQ.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Starting a project</figcaption></figure><p id="6cf9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">停止和删除 Hasura 项目的附加命令:</p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="7ad6" class="ld le iq kz b gy lf lg l lh li"><strong class="kz ir">hasuractl local stop</strong>      ## To stop the running hasura platform.</span><span id="57c6" class="ld le iq kz b gy lw lg l lh li"><strong class="kz ir"><em class="ks">hasuractl local clean    </em> </strong>## To clean up the incomplete setup.</span><span id="b178" class="ld le iq kz b gy lw lg l lh li"><strong class="kz ir">hasuractl local delete</strong>    ## To delete the underlying VM</span></pre><p id="7483" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们快速进入'<em class="ks">'反向传播'</em>和'<em class="ks">'梯度下降'</em>。在<a class="ae kt" href="https://medium.com/towards-data-science/deepclassifyml-week-2-part-2-1b65739c0f35" rel="noopener"> <em class="ks">第四部分</em> </a>中，我们看到了神经网络如何在我们称之为“<em class="ks">正向传播</em>的过程中做出预测。我们根据一个学生以前的分数来预测他是否能进入大学。现在我们有了一个预测，我们如何知道它是否正确，以及我们离正确答案有多近。这是在“<em class="ks">训练”</em>或更新这些权重以进行预测的过程中发生的。</p><p id="581a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们想要的是一种算法，它能让我们找到这些权重和偏差，从而使网络的输出接近正确答案。(请记住，在培训期间，我们有他们的分数，也知道他们是否被录取，也就是说，我们事先知道正确答案。我们想知道当一个新学生进来时会发生什么。为了衡量这一点，我们需要一个指标来衡量预测的不正确程度。让我们称之为'<em class="ks">误差</em>'(你会注意到它也被称为'<em class="ks">成本函数'</em>或'<em class="ks">损失函数'</em>)。该误差可以用下式表示:</p><figure class="ku kv kw kx gt lk gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/9958d5e14b1a9c381e7b58a90d86e19b.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*B6uW5BuLQH7zeMIag2f_nQ.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Sum of Squared Errors (SSE)</figcaption></figure><p id="acf6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我在这里使用的误差指标被称为误差平方和(SSE)。我决定选择这个(也有其他损失函数)，因为平方确保误差总是正的，较大的误差比较小的误差受到更多的惩罚。此外，它使数学看起来很好，不那么吓人。这里<em class="ks"> f(x) </em>是预测值，y 是真实值，然后我们对所有数据点<em class="ks"> i </em>求和。这也是有意义的，因为最终我们想从正确的答案中发现我们的预测有多差。这意味着如果我们的神经网络做得不好，这个'<em class="ks">误差</em>'将会很大——这将意味着对于大量数据点来说<em class="ks"> f(x) </em>并不接近输出<em class="ks"> y </em>。此外，如果成本(误差)变小，即<em class="ks"> SSE(f) </em> ≈0，精确地说，当<em class="ks"> y </em>近似等于预测值<em class="ks"> f(x) </em>时，对于所有训练输入 I，我们可以得出结论，NN 已经做得很好。因此，我们的训练算法的目标将是最小化作为权重和偏差的函数的这个'<em class="ks">误差'</em>。换句话说，我们希望找到一组权重和偏差，使这个“<em class="ks">误差</em>”尽可能小。我们将使用一种叫做<em class="ks">梯度下降</em>的算法来实现。</p><p id="3736" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">梯度下降是执行优化的最流行算法之一，也是迄今为止优化神经网络的最常见方法。它要求我们计算损失函数(误差)相对于网络中所有权重的梯度，以执行权重更新，从而最小化损失函数。反向传播以系统的方式计算这些梯度。反向传播和梯度下降可以说是训练深度神经网络的最重要的算法，可以说是最近出现的<a class="ae kt" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>的驱动力。</p><p id="ccc2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们通过一个经典的例子来理解这一点。假设你在一座山的山顶，想到达山脚(也就是山的最低点)。那么，你会怎么做呢？最好的方法是环顾四周所有可能的方向，检查你附近的地面，观察哪个方向最容易下降。这会给你一个方向的想法，你应该采取你的第一步。然后我们一遍又一遍地重复这个过程。如果我们继续沿着下降的路径，很可能你会到达底部。</p><figure class="ku kv kw kx gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ly"><img src="../Images/bce4c8ce0cffbbf686f90bf83dc06577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W3Ku8oKvcO1bwHc9c5nr0Q.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Plot for the Loss Function</figcaption></figure><p id="c61b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">把大山想象成误差函数。该图表面上的随机位置是权重和偏差的当前值的成本。山的底部(也是图的底部)是最佳权重和偏差集的成本，加上最小误差。我们的目标是继续尝试这些权重和偏差的不同值，评估误差，并选择导致误差稍好(较低)的新系数。重复这个过程足够多次，就会到达山的底部。</p><p id="737d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们朝着目标迈出了一小步。在这种情况下，我们希望逐步改变权重以减少误差。因为下山最快的路是在最陡的方向，所以应该在误差最小的方向走。我们可以通过计算平方误差的<em class="ks">梯度</em>找到这个方向。</p><p id="c966" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><em class="ks">梯度(</em>或<em class="ks">导数)</em>是变化率或斜率的另一个术语。导数是微积分中的一个概念，指的是函数在给定点的斜率。我们需要知道斜率，以便知道移动系数值的方向(符号),从而在下一次迭代中获得更低的成本。</p><p id="680d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们找到一个函数的导数<em class="ks"> f </em> ( <em class="ks"> x </em>)。我们取一个简单的函数<em class="ks"> f(x) = </em> <em class="ks"> x </em>。导数将给我们另一个函数<em class="ks">f</em>'(<em class="ks">x</em>)，该函数返回<em class="ks"> f </em> ( <em class="ks"> x </em>)在点<em class="ks"> x </em>的斜率。<em class="ks"> x </em>的导数为<em class="ks">f</em>'(<em class="ks">x</em>)= 2<em class="ks">x</em>。所以，在<em class="ks"> x </em> =2 时，斜率为<em class="ks">f</em>'(2)= 4。画出这个，看起来像:</p><figure class="ku kv kw kx gt lk gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/8789c1aea5a7dc27c8390de84e3fd2c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*ewJ4qds1LofK4tiSzwHd0w.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Graph of f(x) = x² and its derivative at x = 2</figcaption></figure><p id="8334" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">梯度只是一个推广到具有多个变量的函数的导数。我们可以使用微积分来找到误差函数中任意一点的梯度，这取决于输入权重。你将在下一页看到梯度下降步骤是如何推导出来的。</p><p id="ef2d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">权重将更新为:</p><p id="c7b4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> W += n*delta </strong></p><p id="4119" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">其中 n 称为“学习率”，δ是误差(<em class="ks"> y-f(x) </em>)和激活函数的导数(<em class="ks">f’(x</em>))的乘积。梯度告诉我们函数具有最大增长率的方向，但是它没有告诉我们应该沿着这个方向走多远。由一个常数:<em class="ks">学习率(</em>或步长)决定，它是<em class="ks"> </em>训练神经网络中最重要的超参数设置之一。</p><p id="a40e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为此，我们再次使用 numpy:</p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="f031" class="ld le iq kz b gy lf lg l lh li">import numpy as np</span><span id="a7c9" class="ld le iq kz b gy lw lg l lh li"><em class="ks">## Sigmoid (Activation) function<br/></em><strong class="kz ir">def sigmoid(x):                    <br/>    return 1/(1+np.exp(-x))</strong></span><span id="0b19" class="ld le iq kz b gy lw lg l lh li"><em class="ks">## Derivative of the Sigmoid (Activation) function<br/></em><strong class="kz ir">def sigmoid_derivative(x):<br/>    return sigmoid(x) * (1 - sigmoid(x))</strong></span><span id="40c7" class="ld le iq kz b gy lw lg l lh li"><em class="ks">## Grades for single student in 4 subjects i.e only 1 data point<br/></em><strong class="kz ir">inputs = np.array([50, 22, 10, 45])</strong></span><span id="c383" class="ld le iq kz b gy lw lg l lh li"><em class="ks">## Correct answer (1 : admitted, 0: not admitted) <br/></em><strong class="kz ir">y = np.array([1])        </strong></span><span id="51c7" class="ld le iq kz b gy lw lg l lh li"><em class="ks">## Initialise the weights and bias randomly<br/></em><strong class="kz ir">initial_weights = np.array([0.01, 0.8, 0.02, -0.7])<br/>bias = -0.1</strong></span><span id="194b" class="ld le iq kz b gy lw lg l lh li">## Set a value for learning rate<br/><strong class="kz ir">learning_rate = 0.001</strong></span><span id="1126" class="ld le iq kz b gy lw lg l lh li"><em class="ks">## Our Prediction (f(x))<br/></em><strong class="kz ir">output = sigmoid(np.dot(weights, inputs) + bias)</strong></span><span id="0013" class="ld le iq kz b gy lw lg l lh li"><em class="ks">## Calculate the error i.e how incorrect are we<br/></em><strong class="kz ir">error = y - output</strong></span><span id="046d" class="ld le iq kz b gy lw lg l lh li"><strong class="kz ir">delta = error * sigmoid_derivative(output)</strong></span><span id="3b49" class="ld le iq kz b gy lw lg l lh li"><em class="ks"># Gradient descent step</em><strong class="kz ir"><em class="ks"><br/></em>change_in_weights = learning_rate * delta * inputs</strong></span><span id="faa6" class="ld le iq kz b gy lw lg l lh li"><em class="ks">## Updating our weights<br/></em><strong class="kz ir">new_weights = initial_weights + change_in_weights</strong></span><span id="08c3" class="ld le iq kz b gy lw lg l lh li"><strong class="kz ir">print ('Initial Weights: {}'.format(initial_weights))<br/>print('Our prediction: {}'.format(output))<br/>print('Amount of Error: {}'.format(error))<br/>print('Change in Weights: {}'.format(change_in_weights))<br/>print('New weights: {}'.format(new_weights))</strong></span></pre><p id="80b7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">输出:</p><pre class="ku kv kw kx gt ky kz la lb aw lc bi"><span id="0e5b" class="ld le iq kz b gy lf lg l lh li">Initial Weights: [ 0.01  0.8   0.02 -0.7 ] <br/>Our prediction: 1.6744904055114616e-06 <br/>Amount of Error: [ 0.99999833] <br/>Change in Weights: [ 0.01249998  0.00549999  0.0025      0.01124998] New weights: [ 0.02249998  0.80549999  0.0225  -0.68875002]</span></pre><p id="2dc4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">虽然理解反向传播背后的概念总是有帮助的，但是如果你发现数学很难理解，那也没关系。我们使用的机器学习和深度学习库(scikit-learn、Tensorflow 等。)有内置的工具为你计算一切。</p><p id="0555" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><em class="ks">(编辑:请在评论中报告任何错误或差异，或者您可以联系我:akshaybhatia10@gmail.com)</em></p></div></div>    
</body>
</html>