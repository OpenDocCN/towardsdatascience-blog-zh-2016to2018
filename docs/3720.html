<html>
<head>
<title>Normalized Direction-preserving Adam, Switching from Adam to SGD, and Nesterov Momentum Adam with Interactive Code [ Manual Back Prop in TF, Regularization Study 1 ]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">标准化的保向 Adam，从 Adam 到 SGD 的切换，以及带有交互代码的内斯特罗夫动量 Adam</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/normalized-direction-preserving-adam-switching-from-adam-to-sgd-and-nesterov-momentum-adam-with-460be5ddf686?source=collection_archive---------6-----------------------#2018-06-11">https://towardsdatascience.com/normalized-direction-preserving-adam-switching-from-adam-to-sgd-and-nesterov-momentum-adam-with-460be5ddf686?source=collection_archive---------6-----------------------#2018-06-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/fbe0cf2b1dc78e1f36804b377ea2c468.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/1*efiHcHJCNJZnoVdGKO-eIg.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/christimmons-pizza-spin-pi-l41lUR5urK4IAk3V6" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="140d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最近，我对调整神经网络的不同方法感兴趣，正如所料，许多不同的研究人员已经研究了许多不同的方法。具体来说，今天，我想看看<a class="ae jy" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank"> Adam </a>优化器的差异。下面是所有不同方法的列表，我们将看看这篇文章。</p><p id="b72e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="kx">案例 a: </em> <a class="ae jy" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank"> <em class="kx">随机梯度下降</em> </a> <em class="kx"> <br/>案例 b: </em> <a class="ae jy" rel="noopener" target="_blank" href="/stochastic-gradient-descent-with-momentum-a84097641a5d"> <em class="kx">随机梯度下降动量</em> </a> <em class="kx"> <br/>案例 c: </em> <a class="ae jy" rel="noopener" target="_blank" href="/stochastic-gradient-descent-with-momentum-a84097641a5d"> <em class="kx">随机梯度下降动量</em> </a> <em class="kx"> <br/>案例 d: </em> <a class="ae jy" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank"> <em class="kx">亚当优化器</em> </a> <em class="kx"> <br/>案例 e: </em></p><blockquote class="ky kz la"><p id="97a6" class="jz ka kx kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated">请注意，这篇文章是让我未来的自己回顾和回顾这些纸上的材料，而不是从头再看一遍。也请注意，一些实施工作仍在进行中。</p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper form this <a class="ae jy" href="https://arxiv.org/pdf/1709.04546.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="https://arxiv.org/pdf/1712.07628.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="http://cs229.stanford.edu/proj2015/054_report.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="c5ea" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">基础网络||数据集||不同方法背后的简单理论</strong></p><div class="ll lm ln lo gt ab cb"><figure class="lr jr ls lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/23a9fec75365c59fcdc0848189ef9ac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1580/format:webp/1*zp4mus_kus7-rtUYjx-YZQ.png"/></div></figure><figure class="lr jr mb lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/03a1693b1daa3ac19c46ffbb71a1d8ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*2sHIUoMrpPQsIgKl4m-KlQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk mc di md me">Image from this website <a class="ae jy" rel="noopener" target="_blank" href="/google-continuously-differentiable-exponential-linear-units-with-interactive-code-manual-back-fcbe7f84e79">left </a><a class="ae jy" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">right</a></figcaption></figure></div><p id="4615" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红色矩形</strong> →输入图像(32*32*3) <br/> <strong class="kb ir">黑色矩形</strong> →与 ELU 卷积()有/无平均池<br/> <strong class="kb ir">橙色矩形</strong> → Softmax 进行分类</p><p id="4efb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">为了简单起见，我将使用我以前的帖子中的基本网络，“全卷积网络”。我们还将在<a class="ae jy" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR 10 数据集</a>上评估每种优化方法。</p><p id="fb41" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，我希望为每一种新方法背后的理论写一个简单的版本。(内斯特罗夫动量亚当、SWAT 和归一化方向保持亚当。)</p><p id="9d63" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">通过从 Adam 切换到 SGD 来提高泛化性能</strong> →在训练过程中简单地从 Adam 切换到 SGD。因此，我们可以在训练开始时利用 Adam 的快速收敛，但稍后我们可以通过 SGD 使模型更好地泛化。</p><p id="c35e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">归一化的方向保持 Adam </strong> →为什么 Adam 可能不擅长泛化的一个问题(或者说后退)是由于没有保持梯度方向。(与 SGD 不同)这篇论文的作者提出了一种方法来解决这个缺点。</p><p id="647b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">将内斯特罗夫动量引入亚当</strong> →关于这篇论文的一个简单解释就是将用于将动量扩展到内斯特罗夫动量的原理引入并应用于亚当。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="7ef1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:案例 a:随机梯度下降</strong></p><div class="ll lm ln lo gt ab cb"><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/5919f3ee1f1118e113d0a96a71ad736e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*4CK2XWxE12pKUhQCHvY_4A.png"/></div></figure><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/3393ef10305dec4d2e02f0f5b162b074.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ztl7Zk1lL4bejBSqOke6Bg.png"/></div></figure></div><p id="1d79" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →测试集精度/时间成本<br/> <strong class="kb ir">右图</strong> →训练集精度/时间成本</p><p id="a214" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">正如所料，随机梯度下降表现非常好。在 CIFAR 10 数据集上的最终准确率为 85.3%，仅用了 20 个历元，对于普通梯度下降来说不算太差。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mg"><img src="../Images/738f21cf84109d5a40bd8bc402b9c289.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JGa6YhDpGP_AncLVa8gSeg.png"/></div></div></figure><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mh"><img src="../Images/aa847378a5cc8f855f32e6cd8255cddb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zOo3C3QN63iJ4DXokmEEsQ.png"/></div></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="86b4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:情况 b:随机梯度下降动量</strong></p><div class="ll lm ln lo gt ab cb"><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/1631500fdefd8a76e10c55bb6f462462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*qg0ua0fu-CUupWA1o51oUw.png"/></div></figure><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/59ae755ed4b8a15445f060119eebdc2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ydv0tezvXLyjknCg7mfTaw.png"/></div></figure></div><p id="96f7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →测试集精度/时间成本<br/> <strong class="kb ir">右图</strong> →训练集精度/时间成本</p><p id="22e0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">尽管 SGD 的表现很好，但具有动量的 SGD 能够以 85.7%的准确率略胜一筹。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mi"><img src="../Images/1440d9fcd054aaf3356698a59906cbaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*934t9qR4Wq4XQW7ahBC7Lg.png"/></div></div></figure><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mj"><img src="../Images/14ec1c04b4d6c801c9c8190705cc9b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nJUEU8cNFT1MVLzyaUcJ4A.png"/></div></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="3b1a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:案例 c:具有内斯特洛夫动量的随机梯度下降</strong></p><div class="ll lm ln lo gt ab cb"><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/e9cd2d079134f8b84902025664106a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*dRV4mIe4Rk2qH8LV3X-2Vw.png"/></div></figure><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/b3e208390a49027ac7d5fa2733dd796e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*QwmO9ShZWZaMDSMh_pgWTQ.png"/></div></figure></div><p id="930b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →测试集精度/时间成本<br/> <strong class="kb ir">右图</strong> →训练集精度/时间成本</p><p id="6817" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">根据在线 stand-ford 课程的实施，具有 nesterov 动量的随机梯度下降给出了相当好的结果。然而，令人失望的是，这种方法优于常规动量法。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mk"><img src="../Images/c563ec98f1c2e8d8337ba847a31848ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lKNpFg09nI4sIUBDsH94ug.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from this <a class="ae jy" href="http://cs231n.github.io/neural-networks-3/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi ml"><img src="../Images/2a3696bcc5a856b67ad29ed1148cb4c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1cPxQXyHExV6sqesvAg0dA.png"/></div></div></figure><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mm"><img src="../Images/379b9ee86bb1188ff6c3b1c9ae6cf7a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ebae0EOceg0Tm1h5Wx1hDg.png"/></div></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="0981" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:案例 d: Adam 优化器</strong></p><div class="ll lm ln lo gt ab cb"><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/104385404b6c94264e252f7f55b898e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*TW0XixwhAeVlLciRI2xy2g.png"/></div></figure><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/99c9548adff84f7273be408b0d95f60a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*7iu7_JAHw8UlqBrzBe8zuw.png"/></div></figure></div><p id="1564" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →测试集精度/时间成本<br/> <strong class="kb ir">右图</strong> →训练集精度/时间成本</p><p id="fbe0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我已经预料到 Adam 会胜过大多数优化算法，然而，这也带来了模型泛化能力差的代价。训练准确率为 97 %,而测试准确率停留在 87%。(其在测试图像上具有最高的准确度，但是只有 3%的改进空间。)</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mn"><img src="../Images/65f4755fbe7a546c64f9d03e21ef9472.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KmyhvTBZwesI9rP5v_uokg.png"/></div></div></figure><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mo"><img src="../Images/96776a956783770024fe5f2b2c14ab0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S9XR2cs9aiu_UMguZX_wog.png"/></div></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="695b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:案例 e:内斯特罗夫动量亚当</strong></p><div class="ll lm ln lo gt ab cb"><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/c2220ab9a76217ac26c6cd63146d2ba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*JPTSBgvoyoi1DWuvkHFNag.png"/></div></figure><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/fd4f1f7b637da9d242e2ef876a69da29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*AXEVXV4DmpW2V2JqiR-FlA.png"/></div></figure></div><p id="6996" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →测试集精度/成本随时间变化<br/>T5】右图 →训练集精度/成本随时间变化</p><p id="4c47" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于那达慕，我 100 %相信我的实现还没有完全完成。因为我没有加入 u 的产品。因此这可能解释了为什么这个模型的性能如此之差。(甚至根本不训练。)</p><div class="ll lm ln lo gt ab cb"><figure class="lr jr mp lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/22fc19b1f4aa376b89af510ec3293d70.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*B-oBiDTokSOtvYLZS0N-gw.png"/></div></figure><figure class="lr jr mq lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/9613b2ed5cc51d07a3b49b3f30751ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*pMkphhiycinSBDU8CnxPPQ.png"/></div></figure></div><p id="8c67" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红框</strong> →图片来自原始论文，我仍在研究的术语</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mr"><img src="../Images/f87e63947dbb5a3d2d4803f2e814fae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nwex4jlRErtoIYC91KdI5g.png"/></div></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="7e2b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:情况 f:从 Adam 切换到 SGD </strong></p><div class="ll lm ln lo gt ab cb"><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/3b506042281470b11824f7d6a9430c89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*GVeCXkhQdTWkwW0b7XlByA.png"/></div></figure><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/816e67d3ffaac1515a7d892065b008b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Tb2zc2RXtI23LMHiwHSlPw.png"/></div></figure></div><p id="7749" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →测试集精度/时间成本<br/> <strong class="kb ir">右图</strong> →训练集精度/时间成本</p><p id="56db" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我相信有了最佳的超参数，这种方法可以胜过其他任何方法。然而，没有最佳的超参数调整，我只能达到 67%的准确性。</p><p id="c9d9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这是我迄今为止遇到的最有趣的优化方法之一。我的实现是混乱的，因此不是最好的，但尝试训练这个模型真的很有趣，也很有挑战性。它如此有趣的原因之一是由于反向传播期间的所有 if 条件。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi ms"><img src="../Images/d5ef3b22041f9b69334b4baab7187201.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vDqfM7d4S2vZlopBjKI5Sw.png"/></div></div></figure><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mt"><img src="../Images/e6a0fa25af95468072a7da7825f76808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QIqYMtyxSakoBiMAfRAdcQ.png"/></div></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="4a76" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果:案例 g:归一化方向保持 Adam </strong></p><div class="ll lm ln lo gt ab cb"><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/f9797f88795ee893002c8a1ecf04dca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*FHuIrk__MhaIgkUYipR3Ig.png"/></div></figure><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/64ef13a97f3387f1117069f8ee38d505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*-iiVHIIJIF4uRE6eRQh1oQ.png"/></div></figure></div><p id="de52" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">尽管我努力三次检查我的实现，并与作者的原始代码进行比较(在此处找到<a class="ae jy" href="https://github.com/zj10/ND-Adam/blob/master/ndadam.py" rel="noopener ugc nofollow" target="_blank">),但在使用 ND-Adam 时，我无法成功地训练这个模型。(我非常确信我在某些地方犯了错误，因为作者能够在 80000 年内达到 90%以上的准确率。)</a></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mu"><img src="../Images/6b0ec2488d7c622614fc69d1108bb21f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GImjdyo488qJ4qRDpnawsg.png"/></div></div></figure><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mv"><img src="../Images/c65a4284be7e4d019d295e03be33906a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oPFtj-_GGCa_l2_KHJl2hw.png"/></div></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="c31f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">互动码</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mw"><img src="../Images/bb7266e8667ae1157fcd7f2cd2b82afd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W1r3QCxrqIJfNS0c0txHYw.png"/></div></div></figure><p id="88e5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于 Google Colab，你需要一个 Google 帐户来查看代码，而且你不能在 Google Colab 中运行只读脚本，所以在你的操场上复制一份。最后，我永远不会请求允许访问你在 Google Drive 上的文件，仅供参考。编码快乐！同样为了透明，我在 github 上上传了所有的训练日志。</p><p id="88a1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1keVGivM5kK-gV3JFw1CLzbvgs_rg6C3D" rel="noopener ugc nofollow" target="_blank">的代码，请点击此处</a>，要查看<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/Generalize_1/a/SGD.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处。</a> <br/>访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1AxLYBybSfugBMbxAyou1nxd9sROoD5_M" rel="noopener ugc nofollow" target="_blank"> b 的代码点击此处</a>，查看<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/Generalize_1/b/momentum.txt" rel="noopener ugc nofollow" target="_blank">日志点击此处。</a> <br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1TOW0LCH1LAoQMM7tYzB0W6jSJ4kekrcD" rel="noopener ugc nofollow" target="_blank"> c 的代码，单击此处</a>，要查看<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/Generalize_1/c/NMom.txt" rel="noopener ugc nofollow" target="_blank">日志，单击此处。</a> <br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1lqZviUUAQ0qJoctM5Hd-uo7m_cUOStl6" rel="noopener ugc nofollow" target="_blank"> d 的代码，单击此处</a>，要查看<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/Generalize_1/d/adam.txt" rel="noopener ugc nofollow" target="_blank">日志，单击此处。</a> <br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1kHVzE4FQyb5eLq_ncX4spuEde3TccwM2" rel="noopener ugc nofollow" target="_blank"> e 的代码，请点击此处</a>，要查看<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/Generalize_1/e/NAdam.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处。</a> <br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1Okr4jfqBMoQ8q4ctZdJMp8jqeA4XDQbK" rel="noopener ugc nofollow" target="_blank"> f 的代码，请点击此处</a>，要查看<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/Generalize_1/f/f.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处。</a> <br/>点击此处访问案例 g <a class="ae jy" href="https://colab.research.google.com/drive/1Ynkbd12pvsnu3m1IytCoPbLTt3PybXOf" rel="noopener ugc nofollow" target="_blank">的代码，点击此处访问<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/Generalize_1/g/g.txt" rel="noopener ugc nofollow" target="_blank">日志的</a>。</a></p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="003c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="a2a9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我很兴奋开始这些关于泛化的系列。然而，我很难过，尽管我尽了最大的努力，我还是不能成功地训练具有内斯特罗夫动量亚当和归一化方向保持亚当的模型。</p><p id="a30d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有写作的列表，请点击这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的推特<a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>关注我，访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文 pos </a> t。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="ad8b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="17ff" class="mx my iq kb b kc kd kg kh kk mz ko na ks nb kw nc nd ne nf bi translated">真，t. (2018)。tensorflow:检查标量布尔张量是否为真。堆栈溢出。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/43263933/tensorflow-check-if-a-scalar-boolean-tensor-is-true" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/43263933/tensor flow-check-if-a-scalar-boolean-tensor-is-true</a></li><li id="6f09" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">tf。打印？，H. (2018)。如何使用 tf 打印张量的一部分？打印？。堆栈溢出。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/47000828/how-to-print-part-of-a-tensor-using-tf-print" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/47000828/how-to-print-part-of-a-tensor-using-TF-print</a></li><li id="6303" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">使用 tf。TensorFlow 中的 print()—走向数据科学。(2018).走向数据科学。检索于 2018 年 6 月 10 日，来自<a class="ae jy" rel="noopener" target="_blank" href="/using-tf-print-in-tensorflow-aa26e1cff11e">https://towards data science . com/using-TF-print-in-tensor flow-aa 26 E1 cf F11 e</a></li><li id="197b" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">tf。打印| TensorFlow。(2018).张量流。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://www.tensorflow.org/api_docs/python/tf/Print" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/Print</a></li><li id="633c" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">Tensorflow？，H. (2018)。如何在 Tensorflow 中给 tf.cond 内部的函数传递参数？。堆栈溢出。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/38697045/how-to-pass-parmeters-to-functions-inside-tf-cond-in-tensorflow/39573566" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/38697045/how-to-pass-parameters-to-functions-inside-TF-cond-in-tensor flow/39573566</a></li><li id="fa00" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">tf.cond(pred，fn1，fn2，name=None) | TensorFlow。(2018).张量流。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/cond" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/versions/r 1.0/API _ docs/python/TF/cond</a></li><li id="1656" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">培训，h. (2018)。如何在培训期间更改 tensorflow optimizer？堆栈溢出。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/48259650/how-can-i-change-tensorflow-optimizer-during-training" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/48259650/how-can-I-change-tensor flow-optimizer-in-training</a></li><li id="461f" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">主成分分析池在 Tensorflow 与互动代码[PCAP]。(2018).中等。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://medium.com/@SeoJaeDuk/principal-component-analysis-pooling-in-tensorflow-with-interactive-code-pcap-43aa2cee9bb" rel="noopener">https://medium . com/@ SeoJaeDuk/principal-component-analysis-pooling-in-tensor flow-with-interactive-code-pcap-43 aa2 CEE 9 bb</a></li><li id="351b" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">tf.logical_and | TensorFlow。(2018).张量流。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://www.tensorflow.org/api_docs/python/tf/logical_and" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/logical_and</a></li><li id="a1cd" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">数学符号列表(+，-，x，/，=，，…)。(2018).Rapidtables.com。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://www.rapidtables.com/math/symbols/Basic_Math_Symbols.html" rel="noopener ugc nofollow" target="_blank">https://www . rapid tables . com/Math/symbols/Basic _ Math _ symbols . html</a></li><li id="0e8a" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">通过从 Adam 切换到 SGD 提高泛化性能第 76 期 kweonwooj/papers。(2018).GitHub。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://github.com/kweonwooj/papers/issues/76" rel="noopener ugc nofollow" target="_blank">https://github.com/kweonwooj/papers/issues/76</a></li><li id="37c4" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">zj10/ND-Adam。(2018).GitHub。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://github.com/zj10/ND-Adam/blob/master/ndadam.py" rel="noopener ugc nofollow" target="_blank">https://github.com/zj10/ND-Adam/blob/master/ndadam.py</a></li><li id="65f0" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">规范？，W. (2018)。超脚本 2 下标 2 在规范的语境下是什么意思？。交叉验证。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://stats.stackexchange.com/questions/181620/what-is-the-meaning-of-super-script-2-subscript-2-within-the-context-of-norms" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/181620/what-is-the-meaning-of-super-script-2-subscript-2-in-the-context-of-norms</a></li><li id="8ed2" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">线性代数 27，向量的范数，例题。(2018).YouTube。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://www.youtube.com/watch?v=mKfn23Ia7QA" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=mKfn23Ia7QA</a></li><li id="7c75" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">tf.float32？，H. (2018)。如何将 tf.int64 转换成 tf.float32？。堆栈溢出。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/35596629/how-to-convert-tf-int64-to-tf-float32" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/35596629/how-to-convert-TF-int 64-to-TF-float 32</a></li><li id="517e" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">tf.reduce_sum | TensorFlow。(2018).张量流。检索于 2018 年 6 月 10 日，来自 https://www.tensorflow.org/api_docs/python/tf/reduce_sum<a class="ae jy" href="https://www.tensorflow.org/api_docs/python/tf/reduce_sum" rel="noopener ugc nofollow" target="_blank"/></li><li id="408e" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">j . brown lee(2017 年)。深度学习的 Adam 优化算法简介。机器学习精通。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/Adam-optimization-algorithm-for-deep-learning/</a></li><li id="3aad" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">随机梯度下降。(2018).En.wikipedia.org。检索于 2018 年 6 月 10 日，来自<a class="ae jy" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Stochastic_gradient_descent</a></li><li id="83cf" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">带动量的随机梯度下降——走向数据科学。(2017).走向数据科学。检索于 2018 年 6 月 10 日，来自<a class="ae jy" rel="noopener" target="_blank" href="/stochastic-gradient-descent-with-momentum-a84097641a5d">https://towards data science . com/random-gradient-descent-with-momentum-a 84097641 a5d</a></li><li id="fe33" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">[ ICLR 2015 ]追求简单:具有交互码的全卷积网。(2018).走向数据科学。检索于 2018 年 6 月 11 日，来自<a class="ae jy" rel="noopener" target="_blank" href="/iclr-2015-striving-for-simplicity-the-all-convolutional-net-with-interactive-code-manual-b4976e206760">https://towards data science . com/iclr-2015-forwards-for-simplicity-the-all-convolutional-net-with-interactive-code-manual-b 4976 e 206760</a></li><li id="9c85" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">CIFAR-10 和 CIFAR-100 数据集。(2018).Cs.toronto.edu。检索于 2018 年 6 月 11 日，来自<a class="ae jy" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">https://www.cs.toronto.edu/~kriz/cifar.html</a></li><li id="c848" class="mx my iq kb b kc ng kg nh kk ni ko nj ks nk kw nc nd ne nf bi translated">用于视觉识别的 CS231n 卷积神经网络。(2018).cs 231n . github . io . 2018 年 6 月 11 日检索，来自<a class="ae jy" href="http://cs231n.github.io/neural-networks-3/" rel="noopener ugc nofollow" target="_blank">http://cs231n.github.io/neural-networks-3/</a></li></ol></div></div>    
</body>
</html>