<html>
<head>
<title>Build Log Analytics Application using Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Apache Spark 构建日志分析应用程序</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-log-analytics-application-using-apache-spark-b5eeca1e53ba?source=collection_archive---------10-----------------------#2018-12-31">https://towardsdatascience.com/build-log-analytics-application-using-apache-spark-b5eeca1e53ba?source=collection_archive---------10-----------------------#2018-12-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c47d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用 Apache Spark 开发真实世界应用程序的一步一步的过程，主要重点是解释 Spark 的体系结构。</p><p id="0d09" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们有 Hadoop，为什么还要 Apache Spark 架构？</p><p id="a2d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Hadoop 分布式文件系统(HDFS ),它以 Hadoop 原生格式存储文件，并在集群中并行化这些文件，同时应用 MapReduce 算法来实际并行处理数据。这里的问题是数据节点存储在磁盘上，处理必须在内存中进行。因此，我们需要进行大量的 I/O 操作来进行处理，而且网络传输操作会跨数据节点传输数据。这些操作总的来说可能会阻碍数据的快速处理。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/f525000789c7860259727de5070603ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IzM-IxBeRr4NkhM-AE3J7Q.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Hadoop Map Reduce Algorithm (Left Image Source : researchgate.net)</figcaption></figure><p id="8159" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上图所述，数据块存储在磁盘上的数据记录中，地图操作或其他处理必须在 RAM 中进行。这需要来回的 I/O 操作，从而导致整体结果的延迟。</p><p id="74a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Apache Spark </strong>:官网描述为:“Apache Spark 是一个<strong class="jp ir">快速</strong>和<strong class="jp ir">通用</strong>集群计算系统”。</p><p id="2649" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">快</strong> : Apache spark 很快，因为计算是在内存中进行并存储的。因此，没有 Hadoop 架构中讨论的 I/O 操作。</p><p id="0078" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">通用:</strong>是一个支持通用执行图的优化引擎。它还支持丰富的 SQL 和结构化数据处理，<a class="ae lb" href="https://spark.apache.org/docs/latest/ml-guide.html" rel="noopener ugc nofollow" target="_blank"> MLlib </a>用于机器学习，<a class="ae lb" href="https://spark.apache.org/docs/latest/graphx-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> GraphX </a>用于图形处理，以及<a class="ae lb" href="https://spark.apache.org/docs/latest/streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> Spark Streaming </a>用于实时数据处理。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/5b33f3a8d0554dfc3ddcbcb4c770c238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6dlaZ3SQdhd4_wpJxiJ4kw.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Spark Architecture. Eg : Detect prime numbers</figcaption></figure><p id="7ef1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Spark 的入口点是 Spark 上下文，它处理执行者节点。Spark 的主要抽象数据结构是弹性分布式数据集(RDD)，它代表了一个可以并行操作的<strong class="jp ir">不可变</strong>元素集合。</p><p id="b69c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们讨论上面的例子来更好地理解:一个文件由数字组成，任务是从这个巨大的数字块中找到质数。如果我们把它们分成三块 B1，B2，B3。这些块是不可变的，由 spark 存储在内存中。这里，复制因子=2，因此我们可以看到，其他节点的副本存储在相应的其他分区中。这使得它具有容错体系结构。</p><p id="ff3c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">步骤 1:使用 Spark 上下文创建 RDD</p><p id="43d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二步:<strong class="jp ir">变换</strong>:当对这些 RDD 应用<strong class="jp ir"> map() </strong>操作时，新的块，即 B4、B5、B6 被创建为新的 RDD，它们又是不可变的。这一切操作都发生在内存中。注:B1，B2，B3 仍然作为原来的存在。</p><p id="64d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第三步:<strong class="jp ir">动作</strong>:当<strong class="jp ir"> collect()，</strong>这个时候实际的结果被收集并返回。</p><p id="682e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">懒评估</strong> : Spark 不会马上评估每个转换，而是将它们批量放在一起，一次性评估全部。在其核心，它通过<strong class="jp ir">规划计算顺序和跳过潜在的不必要步骤</strong>来优化查询执行。主要<strong class="jp ir">优点</strong>:增加可管理性<strong class="jp ir">，</strong>节省计算并提高速度，降低复杂性，优化。</p><p id="624f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">它是如何工作的？</strong>我们执行代码来创建 Spark 上下文，然后使用 sc 创建 RDD，然后使用 map 执行转换来创建新的 RDD。实际上，这些操作并不在后端执行，而是创建一个<strong class="jp ir">有向无环图(DAG)谱系</strong>。仅当执行<strong class="jp ir">动作</strong>时，即获取结果，例如:<strong class="jp ir"> collect() </strong>操作被调用，然后它引用 DAG 并向上爬以获取结果，参考该图，当向上爬时，它看到过滤器 RDD 尚未创建，它向上爬以获取上部结果，最后反向计算以获取精确结果。</p><p id="8eb1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">RDD — <strong class="jp ir">弹性:</strong>即借助 RDD 谱系图容错。RDD 是其输入的确定性函数。这加上不变性也意味着 RDD 的部分可以在任何时候被重新创建。这使得缓存、共享和复制变得容易。</p><p id="ee42" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">分布式</strong>:数据驻留在多个节点上。</p><p id="30cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">数据集:</strong>代表你所处理的数据的记录。用户可以通过 JDBC 从外部加载数据集，数据集可以是 JSON 文件、CSV 文件、文本文件或数据库，没有特定的数据结构。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="5ebd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在文章的一部分，我们将使用 Apache Spark 的<strong class="jp ir"> pyspark 和 SQL </strong>功能从头开始创建一个 Apache 访问日志分析应用程序。Python3 和 pyspark 的最新版本。<strong class="jp ir">数据来源</strong>:<a class="ae lb" href="https://github.com/ahujaraman/live_log_analyzer_spark/blob/master/apache-access_log/access_log/access_log" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir">Apache access log</strong></a></p><p id="4639" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">必备库</p><pre class="km kn ko kp gt lj lk ll lm aw ln bi"><span id="4068" class="lo lp iq lk b gy lq lr l ls lt">pip install pyspark<br/>pip install matplotlib<br/>pip install numpy</span></pre><p id="057e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">步骤 1:由于日志数据是非结构化的，我们从每一行解析并创建一个结构，在分析时，每一行又会变成每一行。</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="lu lv l"/></div></figure><p id="9c2c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">步骤 2:创建 Spark Context、SQL Context、DataFrame(是组织到命名列中的<strong class="jp ir">数据</strong>的分布式集合。它在概念上相当于关系数据库中的表)</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="lu lv l"/></div></figure><p id="ad8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">步骤 3:分析以 MB 为单位传输最多内容的前 10 个端点</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="lu lv l"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lw"><img src="../Images/e280b689a19f6319f92bb99c93715a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tU_7aO4isgIpahVpgYxguw.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Top End Points with Maximum Content Flow</figcaption></figure><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="lu lv l"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lx"><img src="../Images/70b3f1cd32fe1cd8ca2f3ee33f87ae4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VpQaLyfYbS5bPsIFTDCFnw.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Response Code Analysis</figcaption></figure><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="lu lv l"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ly"><img src="../Images/0a1624f3b752a5844bb2fd9b14cd7467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-NOF-foD34RSci9oiN5Ig.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Visitors(IP Address) which access the system most frequently</figcaption></figure><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="lu lv l"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/81c9edbe713cfa11ffeae51a602cf0fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*MNP-y7WeDcDWZoly29LoCw.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Traffic Analysis for Past One Week</figcaption></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/be52ae4ce70eb618b3a9033fc9fa183b.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*x_pFwE-d6luqVSl-qykreQ.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Hour Level Traffic Analysis on Particular Day</figcaption></figure><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="lu lv l"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/bc29f9b66f3f905c380aa571f5d7d8fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*GRdCtV3yk-dBmE8G46hP7Q.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Day Level Traffic Analysis</figcaption></figure><p id="3f4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过分析峰值以及哪些端点被哪些 IP 地址击中，可以清楚地检测出异常值。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/79669f19686aa403c0b831273cacdf73.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*kPd0Zn9t3VFCzLvOw6U67Q.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Week Level Each Day Traffic Analysis</figcaption></figure><p id="b10c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，我们可以看到 3 月 8 日的一个不寻常的峰值，可以对其进行进一步分析，以确定差异。</p><p id="5ea2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">地块分析代码:</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="lu lv l"/></div></figure><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="lu lv l"/></div></figure><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="lu lv l"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mc"><img src="../Images/365fbe6666597c41fe30a01ee23f7529.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xh0pLe8zjoy4M4C5mIUjsg.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">End Points which are most Frequenty Hit</figcaption></figure><p id="3e3c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">获取完整的应用代码:【https://github.com/ahujaraman/live_log_analyzer_spark T2】</p></div></div>    
</body>
</html>