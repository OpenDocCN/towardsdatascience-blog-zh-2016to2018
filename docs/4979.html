<html>
<head>
<title>Feature Encoding Made Simple With Spark 2.3.0 — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark 2.3.0 简化了特征编码—第 2 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-encoding-made-simple-with-spark-2-3-0-part-2-5bfc869a809a?source=collection_archive---------8-----------------------#2018-09-19">https://towardsdatascience.com/feature-encoding-made-simple-with-spark-2-3-0-part-2-5bfc869a809a?source=collection_archive---------8-----------------------#2018-09-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><blockquote class="jq jr js"><p id="94f0" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">"人工智能的关键一直是表现."~杰夫·霍金斯</p></blockquote><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi ks"><img src="../Images/0ff8ac471535ebc057db88a541274fe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GRTmn5EteDuENjMjHcs80w.jpeg"/></div></div></figure><p id="7911" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">这是最新的 Spark 2.3.0 特性编码技巧系列的第 2 部分。请参考之前的<a class="ae lh" href="https://medium.com/@roshinijohri/feature-encoding-with-spark-2-3-0-part-1-9ede45562740" rel="noopener">第一部分</a>，因为那里的很多概念在这里都会用到。如前所述，我假设您对 spark 及其数据类型有基本的了解。如果没有，spark 有一个惊人的<a class="ae lh" href="https://spark.apache.org/docs/latest/ml-guide.html" rel="noopener ugc nofollow" target="_blank">文档</a>，如果能通过就太好了。关于 spark 本身的背景，请点击这里查看<a class="ae lh" href="https://medium.com/@roshinijohri/spark-diaries-ad9eec6d9266" rel="noopener">摘要</a>。</p><p id="284c" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">我们在<a class="ae lh" href="https://medium.com/@roshinijohri/feature-encoding-with-spark-2-3-0-part-1-9ede45562740" rel="noopener">之前的文章</a>中讨论过分类编码。在本帖中，我们将主要关注数字特征的各种转换。对于这个例子，我将使用<a class="ae lh" href="https://www.kaggle.com/piyushgoyal443/red-wine-dataset" rel="noopener ugc nofollow" target="_blank">葡萄酒数据集</a>。(<a class="ae lh" href="https://medium.com/@roshinijohri/adding-a-custom-dataset-to-databricks-community-edition-22104ab7fcfe" rel="noopener">我这里有一个小帖子，看看如何在 databricks 上做到这一点。</a>)</p><p id="7622" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">假设数据现在已经上传到 databricks 上(或者您喜欢的任何地方)，让我们来看看数据。</p><pre class="kt ku kv kw gt li lj lk ll aw lm bi"><span id="82c7" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj iu">val</strong> wineData = spark.read.table("winequalityreds_csv")<br/>.<strong class="lj iu">withColumnRenamed</strong>("_c0", "ID")<br/>.<strong class="lj iu">withColumnRenamed</strong>("fixed.acidity", "fixed_acidity")<br/>.<strong class="lj iu">withColumnRenamed</strong>("volatile.acidity", "volatile_acidity")<br/>.<strong class="lj iu">withColumnRenamed</strong>("citric.acid", "citric_acid")<br/>.<strong class="lj iu">withColumnRenamed</strong>("residual.sugar", "residual_sugar")<br/>.<strong class="lj iu">withColumnRenamed</strong>("free.sulfur.dioxide", "free_sulfur_dioxide")<br/>.<strong class="lj iu">withColumnRenamed</strong>("total.sulfur.dioxide", "total_sulfur_dioxide")</span><span id="7b80" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">display</strong>(wineData)</span><span id="e425" class="ln lo it lj b gy lt lq l lr ls">//note: had to change the names here because spark had a problem with recognising "." as a column name.</span></pre><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi lu"><img src="../Images/7941934659a02f32c637d1b8ab7a388b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0_XA-nNJ64UBlv0fOF_gAg.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Wine Dataset Preview</figcaption></figure><p id="a6c5" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">Spark 为我们提供了许多转换选项来标准化/规范化数据。我将在这里讨论标准缩放器、规格化器和最小最大缩放器。为了实现上述任何一种变换，我们需要将这些特征组合成一个特征向量。猜猜我们能在这里用什么？正确，矢量装配工！</p><p id="9f7c" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">让我们在应用变换之前将特征变换为特征向量。</p><pre class="kt ku kv kw gt li lj lk ll aw lm bi"><span id="3fb7" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj iu">val</strong> wineFeatures = wineData.columns.filterNot(_.contains("ID"))<br/>                                   .filterNot(_.contains("quality"))</span><span id="9b7c" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> wineVectorAssembler = <strong class="lj iu">new</strong> VectorAssembler()<br/>                      .setInputCols(wineFeatures)<br/>                      .setOutputCol("features")</span><span id="81c4" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> pipelineVectorAssembler = <strong class="lj iu">new</strong> Pipeline().setStages(Array(wineVectorAssembler))</span><span id="1b37" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> result_df = pipelineVectorAssembler.fit(wineData).transform(wineData)</span><span id="e005" class="ln lo it lj b gy lt lq l lr ls">result_df.show()</span></pre><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi lz"><img src="../Images/a7e0d71919c9c2320927bf5ec258e8f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2O2UPcs1c6sOBiXRXWxF7A.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Note: the features vector generated here is a dense vector but if there are too many zeros then spark can change that to a sparse vector</figcaption></figure><h2 id="f251" class="ln lo it bd ma mb mc dn md me mf dp mg le mh mi mj lf mk ml mm lg mn mo mp mq bi translated"><a class="ae lh" href="https://spark.apache.org/docs/2.1.0/ml-features.html#standardscaler" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">标准缩放器:</strong> </a></h2><p id="452a" class="pw-post-body-paragraph jt ju it jw b jx mr jz ka kb ms kd ke le mt kh ki lf mu kl km lg mv kp kq kr im bi translated">应用这种转换的格式与我们在编码分类特征时看到的非常相似。它转换矢量行的数据集，将每个要素归一化为单位标准差和零均值。</p><p id="a075" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">然而，spark 中的标准化有一个警告。不幸的是，标准定标器不会在内部将稀疏向量转换为密集向量。<strong class="jw iu">因此，在运行此步骤</strong>之前，将稀疏向量转换为密集向量非常重要。虽然我们的数据帧没有真正的稀疏向量，但 spark 有时会将向量汇编器的输出更改为稀疏向量，这可能会给出一些不正确的结果，因为这不会真正引发错误。</p><p id="62cd" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">它可以应用于我们的葡萄酒数据集，如下所示:</p><pre class="kt ku kv kw gt li lj lk ll aw lm bi"><span id="115c" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj iu">import</strong> org.apache.spark.ml.feature.StandardScaler</span><span id="402b" class="ln lo it lj b gy lt lq l lr ls">//convert the sparse vector to a dense vector as a fail safe<br/><strong class="lj iu">val</strong> sparseToDense = <strong class="lj iu">udf</strong>((v : Vector) =&gt; v.toDense)<br/><strong class="lj iu">val</strong> result_df_dense = result_df.withColumn("features", sparseToDense($"features"))</span><span id="ce0f" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> scaler = new StandardScaler()<br/>  .setInputCol("features")<br/>  .setOutputCol("scaledFeatures")<br/>  .setWithStd(true)<br/>  .setWithMean(true)</span><span id="95d1" class="ln lo it lj b gy lt lq l lr ls">// Compute summary statistics by fitting the StandardScaler.<br/><strong class="lj iu">val</strong> scalerModel = scaler.fit(result_df_dense)</span><span id="db6b" class="ln lo it lj b gy lt lq l lr ls">// Normalize each feature to have unit standard deviation.<br/><strong class="lj iu">val</strong> scaledData = scalerModel.transform(result_df_dense)</span><span id="a45d" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">display</strong>(scaledData.select("features", "scaledFeatures"))</span></pre><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi lu"><img src="../Images/aac9077b34fa7533c4ba492c6c1007f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nYhNr-qZghk0EwOHXzeQZA.png"/></div></div></figure><h2 id="d4aa" class="ln lo it bd ma mb mc dn md me mf dp mg le mh mi mj lf mk ml mm lg mn mo mp mq bi translated"><a class="ae lh" href="https://spark.apache.org/docs/2.1.0/ml-features.html#normalizer" rel="noopener ugc nofollow" target="_blank">T3】规格化器:T5】</a></h2><p id="9d39" class="pw-post-body-paragraph jt ju it jw b jx mr jz ka kb ms kd ke le mt kh ki lf mu kl km lg mv kp kq kr im bi translated">规格化器将每个向量规格化为单位范数。它接受参数 p(默认值=2 ),该参数指定用于归一化的 p 范数。当您的稀疏数据集包含不同比例的要素时，这通常很有用。不要忘记使用密集向量作为输入！</p><pre class="kt ku kv kw gt li lj lk ll aw lm bi"><span id="fbf6" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj iu">import</strong> org.apache.spark.ml.feature.Normalizer<br/><strong class="lj iu">import</strong> org.apache.spark.ml.linalg.Vectors</span><span id="e806" class="ln lo it lj b gy lt lq l lr ls">// Normalize each Vector using $L^1$ norm.<br/><strong class="lj iu">val</strong> normalizer = new Normalizer()<br/>  .setInputCol("features")<br/>  .setOutputCol("normFeatures")<br/>  .setP(1.0)</span><span id="078e" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> l1NormData = normalizer.transform(result_df_dense)</span><span id="f82c" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">display</strong>(l1NormData.select($"features", $"normFeatures"))</span></pre><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi mw"><img src="../Images/b8737adbb2f841a81cb2d9dcf8a7dfc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2FyxkyLC6H6KdqfnQBYiog.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">L1-Norm features</figcaption></figure><h2 id="dabc" class="ln lo it bd ma mb mc dn md me mf dp mg le mh mi mj lf mk ml mm lg mn mo mp mq bi translated"><a class="ae lh" href="https://spark.apache.org/docs/2.1.0/ml-features.html#minmaxscaler" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> MinMaxScaler: </strong> </a></h2><p id="fc6c" class="pw-post-body-paragraph jt ju it jw b jx mr jz ka kb ms kd ke le mt kh ki lf mu kl km lg mv kp kq kr im bi translated">通过将每个要素缩放到给定范围来变换要素。这通常在数据遵循的分布不是高斯分布时使用，如标准定标器所假设的那样。</p><pre class="kt ku kv kw gt li lj lk ll aw lm bi"><span id="de6b" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj iu">import</strong> org.apache.spark.ml.feature.MinMaxScaler<br/><strong class="lj iu">import</strong> org.apache.spark.ml.linalg.Vectors</span><span id="c0ab" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> scaler = new MinMaxScaler()<br/>  .setInputCol("features")<br/>  .setOutputCol("minMaxScaledFeatures")</span><span id="7fb6" class="ln lo it lj b gy lt lq l lr ls">// Compute summary statistics and generate MinMaxScalerModel<br/><strong class="lj iu">val</strong> minMaxScalerModel = scaler.fit(result_df_dense)</span><span id="d200" class="ln lo it lj b gy lt lq l lr ls">// rescale each feature to range [min, max].<br/><strong class="lj iu">val</strong> minMaxScaledData = minMaxScalerModel.transform(result_df_dense)<br/><br/><strong class="lj iu">display</strong>(minMaxScaledData.select("features", "minMaxScaledFeatures"))</span></pre><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi mw"><img src="../Images/d4fddfdcdcf04cb74c67706efc0b3a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZ8Tn8WotAe2t3a9bghvBg.png"/></div></div></figure><p id="e81e" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">以上三种一般是最常用的特征缩放方法。我想通过一些其他的火花转换，当我们必须处理连续的特征时，我们可以这样做。</p><h2 id="6d13" class="ln lo it bd ma mb mc dn md me mf dp mg le mh mi mj lf mk ml mm lg mn mo mp mq bi translated"><a class="ae lh" href="https://spark.apache.org/docs/2.1.0/ml-features.html#bucketizer" rel="noopener ugc nofollow" target="_blank"><strong class="ak"/></a><strong class="ak">:</strong></h2><p id="0935" class="pw-post-body-paragraph jt ju it jw b jx mr jz ka kb ms kd ke le mt kh ki lf mu kl km lg mv kp kq kr im bi translated">bucketizer 将一列连续特征转换为一列特征桶。存储桶由参数“splits”决定。由分割 x，y 定义的存储桶保存范围[x，y]内的值，除了最后一个存储桶也包含 y。此处的分割应该严格递增。如果上限/下限未知，则应提供无穷大作为最小值和最大值，以避免任何超出存储桶界限的异常。这并不处理 NAN 值，所以最好在此之前去掉这些值。</p><p id="2268" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">让我们将它应用于葡萄酒数据集中的一列。</p><pre class="kt ku kv kw gt li lj lk ll aw lm bi"><span id="8372" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj iu">import</strong> org.apache.spark.ml.feature.Bucketizer</span><span id="bff1" class="ln lo it lj b gy lt lq l lr ls">//define the splits that I want<strong class="lj iu"><br/>val</strong> splits = Array(0.0, 0.1, 0.2, 0.3, 0.5, 1.0) </span><span id="b687" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> bucketizer = new Bucketizer()<br/>  .setInputCol("citric_acid") //example column from wineData<br/>  .setOutputCol("bucketedFeatures")<br/>  .setSplits(splits)</span><span id="446c" class="ln lo it lj b gy lt lq l lr ls">// Transform original data into its bucket index.<br/><strong class="lj iu">val</strong> bucketedData = bucketizer.transform(wineData)</span><span id="88de" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">display</strong>(bucketedData.select($"citric_acid", $"bucketedFeatures"))</span></pre><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi mx"><img src="../Images/b833812b1e0988ac559755bf6c1aea79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XXG-J6W2Oi5s-Y7sZlo0UQ.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">n buckets defined for n+1 splits</figcaption></figure><h2 id="9822" class="ln lo it bd ma mb mc dn md me mf dp mg le mh mi mj lf mk ml mm lg mn mo mp mq bi translated"><a class="ae lh" href="https://spark.apache.org/docs/latest/ml-features.html#quantilediscretizer" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">分位数离散化:</strong> </a></h2><p id="8c68" class="pw-post-body-paragraph jt ju it jw b jx mr jz ka kb ms kd ke le mt kh ki lf mu kl km lg mv kp kq kr im bi translated">这非常类似于 bucketizer，在您没有真正定义拆分时非常有用。它也可以处理 NANs。当遇到时，有一个“handleInvalid”设置器来选择移除 NAN 或引发异常。如果设置了 nan，那么将创建一个特殊的桶来处理它们。</p><p id="5b55" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">将此应用于 wineData 中的相同柠檬酸列</p><pre class="kt ku kv kw gt li lj lk ll aw lm bi"><span id="1a85" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj iu">import</strong> org.apache.spark.ml.feature.QuantileDiscretizer</span><span id="300b" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> discretizer = new QuantileDiscretizer()<br/>  .setInputCol("citric_acid")<br/>  .setOutputCol("quantileBins")<br/>  .setNumBuckets(10)<br/>  .setHandleInvalid("skip")</span><span id="4bdb" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> result = discretizer.fit(wineData).transform(wineData)<br/><strong class="lj iu">display</strong>(result.select($"citric_acid", $"quantileBins"))</span></pre><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi my"><img src="../Images/f4dd2c896bb7046fa1c7290d932aa6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eoJLiLGhqS52iwfr1_-eTg.png"/></div></div></figure><p id="a02b" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">其他有趣的变形金刚，我不打算做太多的细节(因为它们非常简单，在<a class="ae lh" href="https://spark.apache.org/docs/latest/ml-features.html" rel="noopener ugc nofollow" target="_blank">文档</a>中有很好的解释)，是<a class="ae lh" href="https://spark.apache.org/docs/latest/ml-features.html#binarizer" rel="noopener ugc nofollow" target="_blank">二进制化器</a>、<a class="ae lh" href="https://spark.apache.org/docs/latest/ml-features.html#imputer" rel="noopener ugc nofollow" target="_blank">估算器</a>、、<a class="ae lh" href="https://spark.apache.org/docs/latest/ml-features.html#polynomialexpansion" rel="noopener ugc nofollow" target="_blank">多项式展开</a>和<a class="ae lh" href="https://spark.apache.org/docs/latest/ml-features.html#pca" rel="noopener ugc nofollow" target="_blank"> PCA </a>。请随意建议在评论区添加一些例子，我很乐意添加一些。我会谨慎对待估算者，因为估算缺失值是一个需要彻底调查的过程。缺少数据并不意味着丢失数据，我们应该始终小心不小心在数据中添加了某种偏差。</p><p id="6f51" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">另一个我没怎么合作过的变形金刚，但在<a class="ae lh" href="https://spark.apache.org/docs/latest/ml-features.html#interaction" rel="noopener ugc nofollow" target="_blank">互动</a>中似乎很有趣。除了输出更多的是输入向量的叉积之外，它的功能类似于向量汇编器。这可能会在我们的数据中产生不同的特征交互。我将在实际管道中对此进行更多的评估，以了解使用它的好处，并更新本文。如果我们将它应用到我们的数据集，我们将会看到:</p><pre class="kt ku kv kw gt li lj lk ll aw lm bi"><span id="31dc" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj iu">import</strong> org.apache.spark.ml.feature.Interaction<br/><strong class="lj iu">import</strong> org.apache.spark.ml.feature.VectorAssembler</span><span id="8a1f" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> assembler1 = new VectorAssembler()<br/>  .setInputCols(Array("fixed_acidity", "volatile_acidity", "citric_acid")) //taking features indicating acidity<br/>  .setOutputCol("vec1")</span><span id="57ad" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> assembled1 = assembler1.transform(wineData)</span><span id="49d8" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> assembler2 = new VectorAssembler()<br/>  .setInputCols(Array("free_sulfur_dioxide", "total_sulfur_dioxide")) //taking features indicating sulfur content<br/>  .setOutputCol("vec2")</span><span id="6049" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> assembled2 = assembler2.transform(assembled1).select("ID", "vec1", "vec2")</span><span id="2d23" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> interaction = new Interaction()<br/>  .setInputCols(Array("ID", "vec1", "vec2"))<br/>  .setOutputCol("interactedCol")</span><span id="9858" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">val</strong> interacted = interaction.transform(assembled2)</span><span id="62e5" class="ln lo it lj b gy lt lq l lr ls"><strong class="lj iu">display</strong>(interacted)</span></pre><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi mz"><img src="../Images/0b9aaf2297b398ba21768763e753d600.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CmO6_JRMs-gDzg_wCRTRqw.png"/></div></div></figure><p id="bb9c" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">我希望这篇文章对开始使用 spark 进行特性转换有所帮助。您可以为您的管道构建的定制转换的类型是无限的，spark 提供了一个可伸缩的、快速的和几乎可靠的( :) )基础设施来做到这一点。请提出你认为有助于这篇文章更好的改进意见。我将很高兴相应地更新。</p><p id="2f3e" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">我将在下一篇文章中用 kaggle 数据集处理特征提取器和端到端管道。快乐阅读！！</p><p id="08da" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke le kg kh ki lf kk kl km lg ko kp kq kr im bi translated">ps:如果你喜欢，请鼓掌！帮助我知道它帮助了你，提升了文章！</p></div></div>    
</body>
</html>