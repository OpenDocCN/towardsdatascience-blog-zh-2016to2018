<html>
<head>
<title>A Case for LightGBM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LightGBM的案例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-case-for-lightgbm-2d05a53c589c?source=collection_archive---------2-----------------------#2017-10-15">https://towardsdatascience.com/a-case-for-lightgbm-2d05a53c589c?source=collection_archive---------2-----------------------#2017-10-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/05e95cd39d67af5e4ab7c0ef86a252e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z7yaKUbhxBHQnCdpH08CDg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Microsoft’s Distributed Machine Learning Toolkit</figcaption></figure><p id="65cb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">正如任何活跃的<a class="ae la" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> Kaggler </a>所知，梯度推进算法，特别是XGBoost，主导着竞赛排行榜。然而，在2016年10月，<a class="lb lc ep" href="https://medium.com/u/940e606ec51a?source=post_page-----2d05a53c589c--------------------------------" rel="noopener" target="_blank">微软</a>的DMTK团队开源了它的<a class="ae la" href="https://github.com/Microsoft/LightGBM" rel="noopener ugc nofollow" target="_blank"> <em class="ld"> LightGBM </em> </a>算法(附带Python和R库)，它肯定会占据优势。因此，社区已经开始比较鲜为人知的LightGBM和XGBoost的性能。</p><p id="2c59" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在这篇文章中，我认为提及我使用LightGBM而不是XGBoost的情况以及我在它们之间经历的一些权衡是有价值的。</p><h1 id="2ba0" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">使用</h1><p id="8d9b" class="pw-post-body-paragraph kc kd iq ke b kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz ij bi translated">我对这两种算法的使用都是通过它们的Python、Scikit-Learn包装器。我通常选择使用算法的Scikit-Learn包装器，因为它创建了一个在相同数据上实现和评估各种算法的标准。它还允许通过Scikit创建预处理管道，然后可以通过多种算法轻松应用该管道。</p><p id="2fd0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">除了每个平台的特定超参数之外，这两种算法的实现几乎完全相同。但是，任何熟悉典型SKLearn流程的人都可以使用LightGBM。</p><h1 id="a1d6" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">装置</h1><p id="23a4" class="pw-post-body-paragraph kc kd iq ke b kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz ij bi translated">安装LightGBM的CPU版本轻而易举，可以通过pip安装。我希望我可以说，GPU版本是轻而易举的，但可悲的是，还有更多额外的步骤要采取。您必须安装Cuda、Boost、CMake、MS Build或Visual Studio和MinGW。在路径中添加了一些东西之后，我很幸运能够通过运行以下命令来安装GPU实现:</p><blockquote class="mh mi mj"><p id="bde2" class="kc kd ld ke b kf kg kh ki kj kk kl km mk ko kp kq ml ks kt ku mm kw kx ky kz ij bi translated">pip安装灯GBM-install-option =-GPU</p></blockquote><h1 id="24cd" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">速度</h1><p id="2418" class="pw-post-body-paragraph kc kd iq ke b kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz ij bi translated">对我来说，这是LightGBM真正闪耀的地方。使用默认参数，我发现我的XGBoost基线通常会超过LightGBM，但是LightGBM运行的速度非常快。它已经针对最大并行化运行进行了优化，这意味着它在CPU和GPU上都可以真正运行。</p><p id="7b3f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在优化模型时，这种加速允许更快的超参数搜索。对我来说，这就是LightGBM如此强大的原因。它允许开发人员在更短的时间内随机/网格搜索参数范围，与在相同时间段内使用XGBoost相比，允许测试更多可能的组合。这种速度还允许添加交叉验证测试，以获得更准确的模型泛化度量。我相信，快速优化的能力在很大程度上超过了默认性能的权衡。</p><p id="0d22" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果你对阅读更多关于LightGBM的性能感兴趣，<a class="lb lc ep" href="https://medium.com/u/1ff412f60adb?source=post_page-----2d05a53c589c--------------------------------" rel="noopener" target="_blank"> Laurae </a>有一篇很棒的<a class="ae la" href="https://medium.com/implodinggradients/benchmarking-lightgbm-how-fast-is-lightgbm-vs-xgboost-15d224568031" rel="noopener">深度文章</a>将它与XGBoost进行了比较。</p><h1 id="4626" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">分类特征支持</h1><p id="ced7" class="pw-post-body-paragraph kc kd iq ke b kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz ij bi translated">使用LightGBM的另一个好处是它通过指定列而不是使用一键编码(OHE)来支持分类特性。OHE可能很重，因为它可以显著增加数据集的维度，这取决于类别+选项的数量。在微软的结果中，他们看到了高达8倍的训练速度提升。这个特性使它更适合GPU训练，因为它利用了更有效的方法来处理分类数据。这种支持最酷的部分可能是它有一个自动模式来检测Pandas数据帧中指定的分类列，大大减少了转换数据所需的预处理量。</p><h1 id="a645" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">包装东西</h1><p id="218f" class="pw-post-body-paragraph kc kd iq ke b kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz ij bi translated">随着梯度推进算法越来越受欢迎(由Kaggle社区提供)，我认为与鲜为人知的库分享一些见解是有价值的。我相信在某些情况下，它对开发人员来说比其他库有着巨大的潜力，并建议每个数据科学家下次开始ML项目时都尝试一下。</p></div></div>    
</body>
</html>