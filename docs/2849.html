<html>
<head>
<title>ICLR 2016 — Implementing Context Module in Tensorflow with Interactive Code [Manual Back Prop with Tensorflow]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ICLR 2016 —用交互代码在Tensorflow中实现上下文模块【用Tensorflow手动背道具】</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/iclr-2016-implementing-context-module-in-tensorflow-with-interactive-code-manual-back-prop-with-a2c84b3ae444?source=collection_archive---------5-----------------------#2018-03-13">https://towardsdatascience.com/iclr-2016-implementing-context-module-in-tensorflow-with-interactive-code-manual-back-prop-with-a2c84b3ae444?source=collection_archive---------5-----------------------#2018-03-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/c02947867d7523728017e182bbf9122b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PFTMCvOIPChMEcgtVS0RBQ.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Image from <a class="ae kc" href="https://pixabay.com/en/roll-the-dice-craps-board-game-1502706/" rel="noopener ugc nofollow" target="_blank">pixabay</a></figcaption></figure><p id="71d0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为我在上一篇文章中提到了<a class="ae kc" rel="noopener" target="_blank" href="/understanding-2d-dilated-convolution-operation-with-examples-in-numpy-and-tensorflow-with-d376b3972b25">膨胀卷积</a>运算，所以只有我用它制作了一个网络才有意义。所以我认为我最好实现本文中提出的上下文模块<a class="ae kc" href="https://arxiv.org/abs/1511.07122" rel="noopener ugc nofollow" target="_blank">通过扩张卷积进行多尺度上下文聚合</a>。在<a class="ae kc" href="https://iclr.cc/archive/www/doku.php%3Fid=iclr2016:main.html" rel="noopener ugc nofollow" target="_blank">2016</a>国际学习代表大会上发表。</p><p id="3662" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有两件事我想指出来。原始论文的作者使用他们自己的方法初始化权重，对于这篇文章，我将从正态分布初始化。<br/> 2。为了好玩，让我们用<a class="ae kc" href="https://hackernoon.com/only-numpy-dilated-back-propagation-and-google-brains-gradient-noise-with-interactive-code-3a527fc8003c" rel="noopener ugc nofollow" target="_blank">扩张反向传播和</a> l来训练网络。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="6cad" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">网络架构(表格形式)/实验设置</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi li"><img src="../Images/ba14e05ba3b4bf9c860d056f86424417.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z650tKAKrnIeYAgSmVqsjg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Screen shot from this <a class="ae kc" href="https://arxiv.org/abs/1511.07122" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><p id="b76c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基本上，上下文模块是一个具有扩展卷积运算的全卷积神经网络。我们的实验设置非常简单，我们将对<a class="ae kc" href="https://www.tensorflow.org/versions/r1.1/get_started/mnist/beginners" rel="noopener ugc nofollow" target="_blank"> MNIST </a>数据集执行多类分类。为了实现这一点，我们将在上下文模块网络的输出端添加一个完全连接的神经网络。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="423a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">网络架构(OOP形式)</strong></p><div class="lj lk ll lm gt ab cb"><figure class="ln jr lo lp lq lr ls paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/a7f93eb3433e5c3e38de23f67d886eaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*iYnIdNKH9X6C6MNpJ6EsVA.png"/></div></figure><figure class="ln jr lt lp lq lr ls paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/e0ac59568e5c6814da7fb92ad5109f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*ViKtXqkKhAUvWvuHChBlOA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk lu di lv lw">Left Image Context Layer / Right Image Fully Connected Neural Network</figcaption></figure></div><p id="1034" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">红线→ </strong>扩张卷积运算</p><p id="8cd3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了上面的红线执行扩张卷积运算外，网络架构与卷积神经网络完全相同。其前面有卷积层，后面有完全连接的神经网络。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="f506" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">膨胀系数/前馈操作</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lx"><img src="../Images/fd0193395e2a529b8a3908cbb18c94aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zityDA-7I8i3Rm6pMenIuw.png"/></div></div></figure><p id="4946" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">红线</strong> →放大系数以匹配原稿</p><p id="a3f6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，为了保持膨胀因子与本文相同，我们将因子分别设置为1、1、2、4、8、16、1和1。</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi li"><img src="../Images/5068618687db96d86502d8c902976568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ukqu8yGnfZEwY1VT_Xa1cA.png"/></div></div></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="4a40" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">训练结果自动微分(ADAM优化器)</strong></p><div class="lj lk ll lm gt ab cb"><figure class="ln jr ly lp lq lr ls paragraph-image"><img src="../Images/ef9fcd95790a75a7c6dca0389bbc81fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*k7GmHCbIUZ-deCPiQTU8QQ.png"/></figure><figure class="ln jr lz lp lq lr ls paragraph-image"><img src="../Images/08987795667fb277fb135594fac6dbd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*S1FFqJgcLt6RLA_ju3N1kA.png"/></figure></div><p id="088b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">左图</strong> →训练图像随时间变化的成本图<br/> <strong class="kf ir">右图</strong> →训练图像随时间变化的精度图</p><div class="lj lk ll lm gt ab cb"><figure class="ln jr ly lp lq lr ls paragraph-image"><img src="../Images/943dc3c59b87c771bcad5787763966f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*_T2GEmEp4kIEU-azYTXWCg.png"/></figure><figure class="ln jr lz lp lq lr ls paragraph-image"><img src="../Images/1894511bca9852514211d0a679c8c6d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*lvEyWkEOChQemFOW4w5LSw.png"/></figure></div><p id="0c47" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">左图</strong> →测试图像随时间变化的成本图<br/> <strong class="kf ir">右图</strong> →测试图像随时间变化的精度图</p><p id="7912" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是我见过的最有趣的结果，尤其是对汽车差异化而言。大约在50世纪，我们可以看到这个模型表现得非常好。然而过了100th？)纪元模型的性能开始下降。最终训练和测试图像的准确率都达到了77%左右。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="64ad" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">训练结果(破损)扩张背道具(ADAM优化器)</strong></p><div class="lj lk ll lm gt ab cb"><figure class="ln jr lz lp lq lr ls paragraph-image"><img src="../Images/eb487af54d3a4851d8765f34c144a922.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*4MYcLwpEnmgR-3OnKiGnig.png"/></figure><figure class="ln jr ly lp lq lr ls paragraph-image"><img src="../Images/fde391aa9702d261782c57bc6c0d718b.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*E1R_SWqrNxNYIuEwFgyqpw.png"/></figure></div><p id="fcdf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">左图</strong> →训练图像随时间变化的成本图<br/>T5】右图 →训练图像随时间变化的精度图</p><div class="lj lk ll lm gt ab cb"><figure class="ln jr ma lp lq lr ls paragraph-image"><img src="../Images/b5c1692366fb76436c2cbc7a3b39afa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*-DB1f_RCKjJ7hhEBUNPHVA.png"/></figure><figure class="ln jr ma lp lq lr ls paragraph-image"><img src="../Images/96cf6789e3c070a7150ae18d88d4c3b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*Op3jmovL97JLFu4BmwJWkg.png"/></figure></div><p id="bd46" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">左图</strong> →测试图像随时间变化的成本图<br/> <strong class="kf ir">右图</strong> →测试图像随时间变化的精度图</p><p id="d942" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在让模型训练的同时，我去了健身房。在我的锻炼过程中，我意识到，我对扩展卷积层的反向传播的实现是错误的。因此，我将这个模型命名为(破碎的)扩张反向传播。然而，非常有趣的是，当这个过程可能被中断时，这个模型仍然能够学习。虽然与自动微分相比，它的表现不尽人意，但它仍能达到约75%的准确率。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="7fa8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">交互代码</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mb"><img src="../Images/552acc7d3876c7f8302d51494d6f2bf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gxcObmQilqS4HdWEAeXlZQ.png"/></div></div></figure><p id="622e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mc">为了交互代码，我搬到了Google Colab！所以你需要一个谷歌帐户来查看代码，你也不能在谷歌实验室运行只读脚本，所以在你的操场上做一个副本。最后，我永远不会请求允许访问你在Google Drive上的文件，仅供参考。编码快乐！</em></p><p id="f5ee" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要获取(破裂)扩张<a class="ae kc" href="https://colab.research.google.com/drive/1AHlY8zBQx6MXYLENPVDij9b1_ZqdKx0W" rel="noopener ugc nofollow" target="_blank">反向传播的代码，请点击此处</a>。<br/>要访问自动<a class="ae kc" href="https://colab.research.google.com/drive/1b0uMV0k-A92qefFyrYrxlJMIKXfoLpEm" rel="noopener ugc nofollow" target="_blank">区分代码，请点击此处。</a></p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="589f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">最后的话</strong></p><p id="f8c6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为，这篇文章更多的是关于实现上下文模块，我不会进一步优化它。但是，如果你能够获得更好的结果，请在下面评论，并说明你是如何达到这个结果的。</p><p id="b5e9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果发现任何错误，请发电子邮件到jae.duk.seo@gmail.com给我，如果你想看我所有写作的列表，请在这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同时，在我的twitter上关注我<a class="ae kc" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae kc" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae kc" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>了解更多内容。如果你感兴趣的话，我还做了解耦神经网络<a class="ae kc" href="https://becominghuman.ai/only-numpy-implementing-and-comparing-combination-of-google-brains-decoupled-neural-interfaces-6712e758c1af" rel="noopener ugc nofollow" target="_blank">的比较。</a></p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="6990" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">参考</strong></p><ol class=""><li id="bdc3" class="md me iq kf b kg kh kk kl ko mf ks mg kw mh la mi mj mk ml bi translated">贴标”？，W. (2018)。相对于「分段」和「场景标注」，什么是「语义分段」？。Stackoverflow.com。检索于2018年3月12日，来自<a class="ae kc" href="https://stackoverflow.com/questions/33947823/what-is-semantic-segmentation-compared-to-segmentation-and-scene-labeling" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/33947823/what-is-semantic-segmentation-comparated-to-segmentation-and-scene-labeling</a></li><li id="279e" class="md me iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">于，冯，科尔敦，伏(2015)。基于扩张卷积的多尺度上下文聚合。arXiv预印本arXiv:1511.07122。</li><li id="4dee" class="md me iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">2017年深度学习语义分割指南。(2017).Blog.qure.ai .检索于2018年3月12日，来自<a class="ae kc" href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" rel="noopener ugc nofollow" target="_blank">http://blog . qure . ai/notes/semantic-segmentation-deep-learning-review # dilation</a></li><li id="cfad" class="md me iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">sk learn . utils . shuffle-sci kit-learn 0 . 19 . 1文档。(2018).Scikit-learn.org。检索于2018年3月12日，来自<a class="ae kc" href="http://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/modules/generated/sk learn . utils . shuffle . html</a></li><li id="f01a" class="md me iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">在Tensorflow中用DeepLab实现语义图像分割。(2018).研究博客。检索于2018年3月12日，来自<a class="ae kc" href="https://research.googleblog.com/2018/03/semantic-image-segmentation-with.html" rel="noopener ugc nofollow" target="_blank">https://research . Google blog . com/2018/03/semantic-image-segmentation-with . html</a></li><li id="5782" class="md me iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">使用具有交互代码的神经网络去噪CT扫描——第3部分，卷积残差神经网络。(2018).走向数据科学。检索于2018年3月12日，来自<a class="ae kc" rel="noopener" target="_blank" href="/denosing-lung-ct-scans-using-neural-networks-with-interactive-code-part-3-convolutional-residual-6dbb36b28be">https://towards data science . com/de nosing-lung-CT-scans-using-neural-networks-with-interactive-code-part-3-convolutionary-residual-6 dbb 36 b 28 be</a></li><li id="9065" class="md me iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">以数值模拟和张量流为例理解2D展开卷积运算。(2018).走向数据科学。检索于2018年3月12日，来自<a class="ae kc" rel="noopener" target="_blank" href="/understanding-2d-dilated-convolution-operation-with-examples-in-numpy-and-tensorflow-with-d376b3972b25">https://towards data science . com/understanding-2d-expanded-convolution-operation-with-examples-in-numpy-and-tensor flow-with-d376b 3972 b25</a></li><li id="b2cc" class="md me iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">于，冯，科尔敦，伏(2015)。基于扩张卷积的多尺度上下文聚合。Arxiv.org。检索于2018年3月12日，来自https://arxiv.org/abs/1511.07122<a class="ae kc" href="https://arxiv.org/abs/1511.07122" rel="noopener ugc nofollow" target="_blank"/></li><li id="4703" class="md me iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">TF . nn . conv 2d _ back prop _ filter | tensor flow。(2018).张量流。检索于2018年3月12日，来自<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_backprop_filter" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/nn/conv2d _ back prop _ filter</a></li><li id="8e5b" class="md me iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">TF . nn . conv 2d _ back prop _ input | tensor flow。(2018).张量流。检索于2018年3月12日，来自<a class="ae kc" href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_backprop_input" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/nn/conv2d _ back prop _ input</a></li><li id="9ac3" class="md me iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">只有Numpy:扩张的反向传播和谷歌大脑的梯度噪声与交互代码。(2018).黑客正午。检索于2018年3月12日，来自<a class="ae kc" href="https://hackernoon.com/only-numpy-dilated-back-propagation-and-google-brains-gradient-noise-with-interactive-code-3a527fc8003c" rel="noopener ugc nofollow" target="_blank">https://hacker noon . com/only-numpy-expanded-back-propagation-and-Google-brains-gradient-noise-with-interactive-code-3a 527 fc 8003 c</a></li><li id="c4af" class="md me iq kf b kg mm kk mn ko mo ks mp kw mq la mi mj mk ml bi translated">价值观？，H. (2018)。如何获取Tensorflow张量维数(形状)作为int值？。Stackoverflow.com。检索于2018年3月13日，来自<a class="ae kc" href="https://stackoverflow.com/questions/40666316/how-to-get-tensorflow-tensor-dimensions-shape-as-int-values" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/40666316/how-to-get-tensor flow-tensor-dimensions-shape-as-int-values</a></li></ol></div></div>    
</body>
</html>