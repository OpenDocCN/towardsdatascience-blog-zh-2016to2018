<html>
<head>
<title>Review: PolyNet — 2nd Runner Up in ILSVRC 2016 (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">点评:PolyNet——ILSVRC 2016 亚军(图像分类)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea?source=collection_archive---------10-----------------------#2018-12-07">https://towardsdatascience.com/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea?source=collection_archive---------10-----------------------#2018-12-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="db1c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过使用多感知模块，优于 Inception-ResNet-v2</h2></div><p id="e12b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">在</span>这个故事里，<strong class="kh ir"> CUHK </strong>和<strong class="kh ir"> SenseTime </strong>的<strong class="kh ir">波利尼特</strong>进行了回顾。介绍了一种称为<strong class="kh ir">多增强模块</strong>的构建模块。基于该模块组成一个<strong class="kh ir">极深多边形</strong>。与<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"><strong class="kh ir">Inception-ResNet-v2</strong></a>相比，PolyNet 将单一作物的前 5 名验证误差从 4.9%降低到 4.25%，将多作物的前 5 名验证误差从 3.7%降低到 3.45%。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/5e890b181a0fe511e0b1433fd3a05f94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*SOMphLLMMaBJDCU5BTpwSQ.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">PolyNet, By using PolyInception module, better than Inception-ResNet-v2</strong></figcaption></figure><p id="a535" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果，PolyNet(团队名称为 CU-DeepLink)在 ILSVRC 2016 分类任务中获得<strong class="kh ir">亚军，如下。并以<strong class="kh ir"> 2017 CVPR </strong>论文发表。(<a class="ly lz ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----8a1a941ce9ea--------------------------------" rel="noopener" target="_blank">曾植和</a> @中)</strong></p><p id="bf44" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">ResNet</a>(2015 年 ILSVRC 的获胜者)的 3.57%相比，PolyNet 的得票率为 3.04%，具体如下:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ma"><img src="../Images/5ea06950f62cad4bb60e159cf2c339a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lwhmm08p2vwc2yVWfEI7kQ.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">ILSVRC 2016 Classification Ranking (Team Name: CU-DeepLink, Model Name: PolyNet) </strong><a class="ae lk" href="http://image-net.org/challenges/LSVRC/2016/results#loc" rel="noopener ugc nofollow" target="_blank">http://image-net.org/challenges/LSVRC/2016/results#loc</a></figcaption></figure><p id="65d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个相对提升大概是 14%左右，这可不是小事！！！</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="95d1" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">涵盖哪些内容</h1><ol class=""><li id="9587" class="ne nf iq kh b ki ng kl nh ko ni ks nj kw nk la nl nm nn no bi translated"><strong class="kh ir">对 Inception-ResNet-v2 (IR-v2)的简要回顾</strong></li><li id="7d14" class="ne nf iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated"><strong class="kh ir">多感知模块</strong></li><li id="cea3" class="ne nf iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated"><strong class="kh ir">消融研究</strong></li><li id="cc88" class="ne nf iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated"><strong class="kh ir">结果</strong></li></ol></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="d5d1" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated"><strong class="ak"> 1。对 Inception-ResNet-v2 (IR-v2)的简要回顾</strong></h1><p id="dca1" class="pw-post-body-paragraph kf kg iq kh b ki ng jr kk kl nh ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">随着<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>和<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener">Google net(Inception-v1)</a>的成功，引入了<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-ResNet-v2(IR-v2)</a>来结合两者:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi nx"><img src="../Images/dd3d3ddc377e8eecd85a7a7a0275e4e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7bQX4FQB3PgDtcekFq_vQg.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Inception-ResNet-v2: Stem (Leftmost), Inception-A (2nd Left), Inception-B (2nd Right), Inception-C (Rightmost)</strong></figcaption></figure><p id="0c42" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上图，有一个跳过连接。还有几个平行的卷积路径，这是由<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener"> GoogLeNet </a>发起的。并且多个 Inception-A、Inception-B 和 Inception-C 在不同的级别级联。最后，<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-ResNet-v2(IR-v2)</a>获得了较高的分类准确率。</p><p id="a7c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">初始模块可以表述为一个抽象的剩余单元，如下所示:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/f31501fc3bc9bbf864707127782a760c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*he9NBby_jFIqQhZgXDc8wg.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Inception Module (Left), Abstract Residual Unit Denoted by F (Right)</strong></figcaption></figure><p id="db11" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出变成 x + F(x ),类似于残差块。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="d1cc" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated"><strong class="ak"> 2。多感知模块</strong></h1><p id="45aa" class="pw-post-body-paragraph kf kg iq kh b ki ng jr kk kl nh ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">为了提高精度，只需添加一个二阶项，便可增加一个多项式组合:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/48f3292b00fc1aceca67c70f7bd71761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*ly0agM3eVSB7MhNtnvz3JQ.png"/></div></figure><p id="02a5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与<strong class="kh ir">二阶项</strong>组成<strong class="kh ir">多感知模块</strong>。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/35890657c2f61b31ca048ff5cc511d5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*efoLd4MlOKTWnqh4oASFtA.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Different Types of PolyInception Module ((a) and (b) are the same)</strong></figcaption></figure><p id="4b65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">提出了不同类型的多感知模块:</p><ul class=""><li id="feda" class="ne nf iq kh b ki kj kl km ko ob ks oc kw od la oe nm nn no bi translated"><strong class="kh ir"> (a) poly-2 </strong>:第一条路径是跳过连接或身份路径。第二条路径是一阶先启块。第三条路径是二阶项，由两个初始块组成。</li><li id="27ab" class="ne nf iq kh b ki np kl nq ko nr ks ns kw nt la oe nm nn no bi translated"><strong class="kh ir"> (b) poly-2 </strong>:由于第一个 Inception F 用于一阶路径和二阶路径，<strong class="kh ir">第一个 Inception F 可以共享</strong>。同样通过共享参数，<strong class="kh ir">第二次启始 F 与第一次相同。</strong>这可以在不引入额外参数的情况下<strong class="kh ir">增加表示能力。<br/>我们也可以想象它是<strong class="kh ir">(RNN)的一种递归神经网络。在二阶路径上，来自 Inception F 的输出再次回到 Inception F 的输入。</strong>这就变成了<strong class="kh ir"> 1+F+F </strong>。</strong></li><li id="109e" class="ne nf iq kh b ki np kl nq ko nr ks ns kw nt la oe nm nn no bi translated"><strong class="kh ir"> (c) mpoly-2 </strong>:如果第二个 Inception G 不与 F 共享参数，我们得到 mpoly-2。这就变成了<strong class="kh ir"> 1+F+GF </strong>。</li><li id="6e26" class="ne nf iq kh b ki np kl nq ko nr ks ns kw nt la oe nm nn no bi translated"><strong class="kh ir"> (d) 2 通</strong>:这是一个<strong class="kh ir">一阶多叠加</strong>，<strong class="kh ir"> I+F+G </strong>。</li></ul><p id="ed36" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个概念可以扩展到<strong class="kh ir">高阶多感知模块</strong>。我们可以有<strong class="kh ir"> poly-3 (1+F+F +F ) </strong>，<strong class="kh ir"> mpoly-3 (1+F+GF+HGF) </strong>，<strong class="kh ir"> 3 向(I+F+G+H) </strong>。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="88b4" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">3.<strong class="ak">消融研究</strong></h1><p id="6175" class="pw-post-body-paragraph kf kg iq kh b ki ng jr kk kl nh ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">现在我们手里有这么多选择。但是哪些组合是最好的呢？作者尝试了许多组合来寻找最佳组合。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi of"><img src="../Images/00a1e95fa088d7ef40aa52d9dbe90f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*bOgvZN9kfHaTHmfyUZ_OSg.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Inception-ResNet-v2 (IR-v2, IR 5–10–5) (Top), PolyNet (Bottom)</strong></figcaption></figure><p id="0823" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-ResNet-v2(IR-v2)</a>表示为<strong class="kh ir">IR 5–10–5</strong>，这意味着它在 A 阶段有 5 个 Inception-A 模块(IR-A)，在 B 阶段有 10 个 Inception-B 模块(IR-B)，在 C 阶段有 5 个 Inception-C 模块(IR-C)</p><h2 id="1c99" class="og mn iq bd mo oh oi dn ms oj ok dp mw ko ol om my ks on oo na kw op oq nc or bi translated">3.1.单一类型替换</h2><p id="bb6c" class="pw-post-body-paragraph kf kg iq kh b ki ng jr kk kl nh ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">为了加快实验速度，一个缩小版的<strong class="kh ir">ir3–6–3 被用作基线</strong>。对于每一次，六个多感知模块中的一个被 Inception-A、Inception-B 或 Inception-C 模块所取代，如下所示。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi os"><img src="../Images/bf6be27f3af3183f4e04a48a445638fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*11ieDBFtCrp51zA5R8DdOA.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Top-5 Accuracy vs Time (Top), Top-5 Accuracy vs #Params (Bottom), with replacement at Inception-A (Left), Inception-B (Middle), and Inception-C (Right)</strong></figcaption></figure><p id="0506" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从上面的数字中，我们可以发现:</p><ul class=""><li id="bcaf" class="ne nf iq kh b ki kj kl km ko ob ks oc kw od la oe nm nn no bi translated">任何二阶多感知模块都比初始模块好。</li><li id="5278" class="ne nf iq kh b ki np kl nq ko nr ks ns kw nt la oe nm nn no bi translated">增强 Inception-B 会带来最大的收益。而 mpoly-3 似乎是最好的一个。但是 poly-3 具有竞争性的结果，而只有 mpoly-3 的 1/3 参数大小。</li><li id="dcb6" class="ne nf iq kh b ki np kl nq ko nr ks ns kw nt la oe nm nn no bi translated">对于其他阶段，A 和 C，3 路性能略好于 mpoly 和 poly。</li></ul><h2 id="2ce6" class="og mn iq bd mo oh oi dn ms oj ok dp mw ko ol om my ks on oo na kw op oq nc or bi translated">3.2.混合类型替换</h2><p id="0a14" class="pw-post-body-paragraph kf kg iq kh b ki ng jr kk kl nh ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">IR 6–12–6 用作基线。而这 12 个 Inception-B 是关注点，因为它在之前的研究中得到了最大的改进。并且只测试一种混合多输入多输出(mixed B)，即<strong class="kh ir"> (3 路&gt; mpoly-3 &gt; poly-3) × 4 </strong>。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ot"><img src="../Images/6a953393d4fb1d118befeae1e76b9997.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*Ajzj9V6MHMrmKdpwziVY-w.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Top-5 Error with different Inception module at Stage B</strong></figcaption></figure><p id="5b50" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">混合 B 的 Top-5 误差最低。</p><h2 id="9814" class="og mn iq bd mo oh oi dn ms oj ok dp mw ko ol om my ks on oo na kw op oq nc or bi translated">3.3.最终模型</h2><ul class=""><li id="8fb3" class="ne nf iq kh b ki ng kl nh ko ni ks nj kw nk la oe nm nn no bi translated">阶段 A: 10 个双向多输入模块</li><li id="b95d" class="ne nf iq kh b ki np kl nq ko nr ks ns kw nt la oe nm nn no bi translated">B 阶段:10 份聚-3 和 2-way 的混合物(总共 20 份)</li><li id="4182" class="ne nf iq kh b ki np kl nq ko nr ks ns kw nt la oe nm nn no bi translated">阶段 C: 5 种聚-3 和 2-way 的混合物(总共 10 种)</li></ul><p id="f0b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">进行了一些修改以适应 GPU 内存，在保持深度的同时降低了成本。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="a749" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">4.结果</h1><h2 id="627f" class="og mn iq bd mo oh oi dn ms oj ok dp mw ko ol om my ks on oo na kw op oq nc or bi translated">4.1.一些训练细节</h2><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ou"><img src="../Images/6ef7eac09f769170f7b9dfd693eee36a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PT7_sr980xzns2ALZvJriA.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Initialization by Insertion (Left), Path Dropped by Stochastic Depth (Right)</strong></figcaption></figure><p id="8685" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">插入初始化</strong>:为了加快收敛，如上图所示，先去掉二阶初始模块，先训练交错模块。因此，在开始时训练一个较小的网络。</p><p id="0489" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-stochastic-depth-image-classification-a4e225807f4a"> <strong class="kh ir">随机深度</strong> </a>:通过随机丢弃网络的某些部分，可以减少过拟合。它可以被视为一种特殊的丢失情况，即丢失一条路径上的所有神经元。</p><h2 id="550a" class="og mn iq bd mo oh oi dn ms oj ok dp mw ko ol om my ks on oo na kw op oq nc or bi translated">4.2.ImageNet</h2><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ov"><img src="../Images/c5ef14f2d0c8d26c78cd464a231fe3da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qopUGFNNj_LSe6Ne81LC5A.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Single Model Results on 1000-Class ImageNet Dataset (Left) Top-5 Accuracy (Right)</strong></figcaption></figure><ul class=""><li id="1550" class="ne nf iq kh b ki kj kl km ko ob ks oc kw od la oe nm nn no bi translated"><strong class="kh ir">非常深的 Inception-ResNet</strong>:<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-ResNet-v2</a>IR 20–56–20。得到 19.10%的前 1 名误差和 4.48%的前 5 名误差。</li><li id="d80b" class="ne nf iq kh b ki np kl nq ko nr ks ns kw nt la oe nm nn no bi translated"><strong class="kh ir">Verp Deep PolyNet(10–20–10 mix)</strong>:<strong class="kh ir">得到 18.71% Top-1 误差和 4.25% Top-5 误差</strong>。</li><li id="8362" class="ne nf iq kh b ki np kl nq ko nr ks ns kw nt la oe nm nn no bi translated">在多作物的情况下，非常深的 PolyNet 得到了<strong class="kh ir"> 17.36%的 Top-1 误差和 3.45%的 Top-5 误差</strong>，这一直优于非常深的<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-ResNet-v2 </a>。</li><li id="6693" class="ne nf iq kh b ki np kl nq ko nr ks ns kw nt la oe nm nn no bi translated">因此，二阶多感知模块确实有助于提高精度。</li></ul></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><p id="312b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过使用多模型和多尺度输入图像进行组合，PolyNet 最终获得了 3.04%的 Top-5 错误率，并在 ILSVRC 2016 分类任务上获得亚军。</p><p id="7273" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于图像分类只有一个目标，即识别图像中的单个对象，因此用于图像分类的好模型通常成为对象检测、语义分割等网络的骨干。因此，研究图像分类模型是很有价值的。我也会学习 ResNeXt。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h2 id="4d8e" class="og mn iq bd mo oh oi dn ms oj ok dp mw ko ol om my ks on oo na kw op oq nc or bi translated">参考</h2><p id="650e" class="pw-post-body-paragraph kf kg iq kh b ki ng jr kk kl nh ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">【2017 CVPR】【波利尼特】<br/> <a class="ae lk" href="https://arxiv.org/abs/1611.05725" rel="noopener ugc nofollow" target="_blank">波利尼特:在非常深的网络中对结构多样性的追求</a></p><h2 id="f728" class="og mn iq bd mo oh oi dn ms oj ok dp mw ko ol om my ks on oo na kw op oq nc or bi translated">我对图像分类的相关综述</h2><p id="79f2" class="pw-post-body-paragraph kf kg iq kh b ki ng jr kk kl nh ju kn ko nu kq kr ks nv ku kv kw nw ky kz la ij bi translated">[<a class="ae lk" href="https://medium.com/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17" rel="noopener">LeNet</a>][<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener">AlexNet</a>][<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103" rel="noopener">ZFNet</a>][<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener">VGGNet</a>][<a class="ae lk" href="https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679" rel="noopener">SPPNet</a>][<a class="ae lk" href="https://medium.com/coinmonks/review-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617" rel="noopener">PReLU-Net</a>][<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener">Google Net/Inception-v1</a>][<a class="ae lk" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">BN-Inception/Inception-v2</a>][<a class="ae lk" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener">Inception-v3</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-v4</a><a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> [ </a><a class="ae lk" rel="noopener" target="_blank" href="/review-ror-resnet-of-resnet-multilevel-resnet-image-classification-cd3b0fcc19bb"> RoR </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-stochastic-depth-image-classification-a4e225807f4a">随机深度</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004">WRN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803">dense net</a>]</p></div></div>    
</body>
</html>