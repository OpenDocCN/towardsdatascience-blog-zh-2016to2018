<html>
<head>
<title>A Brief Review of FlowNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">FlowNet简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-brief-review-of-flownet-dca6bd574de0?source=collection_archive---------1-----------------------#2017-09-09">https://towardsdatascience.com/a-brief-review-of-flownet-dca6bd574de0?source=collection_archive---------1-----------------------#2017-09-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="63e4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">最近，细胞神经网络已经成功地用于估计光流。与传统方法相比，这些方法在质量上有了很大的提高。在此，我们将对以下论文进行简要回顾。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5982093d3afb9320055cc1a7d12b3bc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8HrxH5uns-Chcny-."/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/23df055c19fc626826a345dbabb50055.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*y4za_QHGfUzAaxQt."/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kr"><img src="../Images/5c520cf82f01ab6cfbfee7c1dfaf6342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lXi1VQxLA2_Zs786."/></div></div></figure><h1 id="ba8a" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">概观</h1><p id="858c" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">卷积神经网络(CNN)在各种计算机视觉任务中做出了巨大贡献。最近，细胞神经网络已经成功地用于估计光流。与传统方法相比，这些方法在质量上有了很大的提高。在此，我们将对以下论文进行简要回顾。</p><p id="6aca" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">FlowNet1.0和FlowNet2.0都是端到端架构。FlowNet2.0由FlowNetCorr和FlowNet堆叠而成，比FlowNetCorr和flownet都有好得多的结果。FlowNetS简单地将两个顺序相邻的图像堆叠起来作为输入，而在FlowNetCorr中，两个图像被分别卷积，并通过一个相关层组合在一起。在空间金字塔网络中，作者为每一层独立训练一个深度网络来计算流量更新。SPyNet和FlowNet2.0都以由粗到细的方式估计大的运动。FlowNet2.0在这些架构中性能最好，SPyNet的模型参数最少。</p><h1 id="ae11" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">FlowNet:用卷积网络学习光流</h1><p id="76d4" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">在FlowNet1.0中，本文提出并比较了两种架构:FlowNetSimple和FlowNetCorr。这两种架构都是端到端的学习方法。在FlowNetSimple中，如图1所示，作者简单地将两个顺序相邻的输入图像堆叠在一起，并通过网络传送它们。与FlowNetSimple相比，FlowNetCorr(图2)首先分别产生两幅图像的表示，然后在“相关层”将它们结合在一起，一起学习更高的表示。这两种架构都有用于上采样分辨率的改进。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/880c30a3bf152d5f0eb197d9271ddc75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XVygX0wF3enVQJLe."/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/264198ba6c0b32c161747be928596a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8hfSV1yyguR1NwKm."/></div></div></figure><p id="c705" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">相关层用于在两个特征图之间执行乘法面片比较。更具体地，给定两个多通道特征图f1、f2，其中w、h和c是它们的宽度、高度和通道数量。以第一个图中的x1和第二个图中的x2为中心的两个补片的“相关性”定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/cdda204bb1cf626564cb2ceae3449897.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/0*xOZurv77aIW6Am-D."/></div></figure><p id="51f3" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">其中x1和x2分别是第一地图和第二地图的中心，并且大小为K = 2k+1的正方形空间面片。此外，出于计算原因，作者限制了最大位移。具体来说，对于每个位置x1，作者通过计算大小为D = 2d+1的邻域中的相关性来限制x2的范围，D是给定的最大位移。输出的大小是(w*h*D)。然后，作者将使用卷积层从f1提取的特征图与输出连接。</p><p id="f24b" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">然而，在一系列卷积层和汇集层之后，分辨率已经降低。因此，作者通过“向上进化”层改进了粗略的池化表示，包括取消池化和向上进化。对特征图进行上变换后，作者将其与相应的特征图和上采样的粗流量预测连接起来。如图3所示</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/83189ffcf1cc9b236be37cd3581836db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/0*hUW9OMg6N1fe9p0F."/></div></figure><p id="f956" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">实际上，作者在Github上提供的模型与上图略有不同。图3的第二个框不仅包括来自deconv5和con5_1的特征图，还包括由以下流生成的流6。</p><p id="b4dc" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">con V6—-(conv)—&gt; con V6 _ 1—(conv)→predict _ flow 6—-(conv)—&gt; flow 6</p><p id="c826" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">表1。显示不同数据集上不同方法的平均终点误差(以像素为单位)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e3c642f9defae669c922f01dac009b92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BSRoCTIEUh3-RuuX."/></div></div></figure><h1 id="a9fb" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">FlowNet 2.0:深度网络光流估计的发展</h1><h1 id="eca1" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">简介和贡献</h1><p id="b442" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">FlowNet2.0比FlowNet1.0好得多，与FlowNet1.0相比，FlowNet2.0在质量和速度上都有很大的提高。主要架构如图7所示。本文有四个主要贡献:</p><p id="feb0" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">1.呈现数据的时间表在培训进度中很重要<br/> 2。提出了堆叠架构<br/> 3。介绍了一个专门研究小动作的子网络<br/> 4。提出了融合架构</p><h1 id="9e12" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">数据测试计划</h1><p id="7c54" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">在实验中，不仅训练数据的种类对性能很重要，而且它在训练期间呈现的顺序也很重要。作者分别在椅子和Things3D上测试了FlowNetS和FlowNetCorr。图5显示了使用不同学习速率表的两个数据集样本的等量混合。S_short、S_long和S_fine是不同的学习速率表，如图6所示。图5中的数字表示Sintel数据集的终点误差。从图5中，我们可以知道最好的结果是先在椅子上训练，然后在Things3D上微调。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/e05b3a8683e41ce9859a501c3d258793.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/0*SDMTee3GVcaS0z_-."/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mo"><img src="../Images/d40ce26c35d6d2172a46da42b881400b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5CHvKj1cwXXaDFHE."/></div></div></figure><h1 id="9b11" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">堆叠网络</h1><p id="ef4c" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">为了计算光流的大位移，作者叠加了流网和流网校正，如图7所示。表3显示了叠加流网的效果，其中应用了图5中的最佳流网。第一流网获取图像I1和I2作为输入，第二流网获取图像I1、由第一流网计算的流wi、由流wi扭曲的图像I2以及由流wi扭曲的图像I1和图像I2之间的亮度差误差。</p><p id="c92b" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">在训练栈结构中有两种方法:固定第一网络的权重，或者与第二网络一起更新它们。结果如表3所示，从中我们可以看出，当固定Net1并用warping训练Net2时，在Sintel上获得了最好的结果。</p><p id="0890" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">此外，作者确实对堆叠多个不同的网络进行了实验，他们发现多次堆叠具有相同权重的网络，并对这种循环的过去进行微调不会改善结果。因此，他们添加了不同权重的网络，每个新网络首先在椅子上进行训练，并在Things3D上进行微调。最后，他们通过平衡网络精度和运行时间来实施FlowNet2-CSS。FlowNetCorr是FlowNet2-CSS的第一个网络，后面是两个FlowNet，如图7所示的第一个流</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/c3cb5e03aa3627c944bceca3dd442eba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Sx9qcd7Qx546XVuM."/></div></div></figure><h1 id="efb3" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">小位移网络与融合</h1><p id="f67c" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">但是，对于小排量，FlowNet2-CSS并不可靠。因此，作者创建了一个具有小位移的小数据集，并在该数据集中训练FlowNetSD。FlowNetSD与FlowNetS略有不同。他们用多个3*3内核替换了开头的7*7和5*5内核，去掉了第一层的stride 2。最后，作者引入了一个小而简单的深度网络(Fusion)来融合FlowNet2-CSS和FlowNet2-SD的输出，如图7所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/325257c7f182c3f293206db81701ab2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z9M5PAu2qwKjxQ2C."/></div></div></figure><h1 id="0ff3" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">表演</h1><p id="56aa" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">表4显示了不同基准测试的性能。AEE:平均终点误差；Fl-all:流量估计误差为3个像素和5%的像素比率。在Sintel、Sintel final和Middlebury中，FlowNet2在准确率上超越了所有其他参考方法，在其他准确率相对较高的数据集上也表现良好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/70a7e3a531816ccedc724d4c66eaa11a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NHHfuITGzfShO-6k."/></div></div></figure><h1 id="331d" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">使用空间金字塔网络的光流估计</h1><h1 id="ea46" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">介绍</h1><p id="d343" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">将经典的空间金字塔模型与深度学习相结合，提出了一种新的光流方法。这是一种由粗到细的方法。在空间金字塔的每一层，作者训练一个深度神经网络来估计流量，而不是只训练一个深度网络。这种方法对于任意大的运动是有益的，因为每个网络要做的工作较少，并且每个网络上的运动变得较小。与FlowNet相比，SPyNet要简单得多，在模型参数方面要小96%。此外，对于一些标准基准，SPyNet比FlowNet1.0更准确。</p><h1 id="4f08" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">体系结构</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/945f7d116342343cf52ed80e9320fc52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OXZfCTMkMngme-Nw."/></div></div></figure><p id="eddf" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">图8显示了一个三级金字塔网络:</p><ul class=""><li id="c3be" class="mr ms iq lm b ln mg lq mh lt mt lx mu mb mv mf mw mx my mz bi translated">d()是将m*n图像I减小到m/2*n/2的下采样函数</li><li id="6cbc" class="mr ms iq lm b ln na lq nb lt nc lx nd mb ne mf mw mx my mz bi translated">u()是重采样光流场的重采样函数</li><li id="13a8" class="mr ms iq lm b ln na lq nb lt nc lx nd mb ne mf mw mx my mz bi translated">w(I，V)用于根据光流场V扭曲图像I</li><li id="edf9" class="mr ms iq lm b ln na lq nb lt nc lx nd mb ne mf mw mx my mz bi translated">{ G0，…，GK }是一组经过训练的卷积神经网络</li><li id="c4f3" class="mr ms iq lm b ln na lq nb lt nc lx nd mb ne mf mw mx my mz bi translated">v_k是由convnet Gk在第k个金字塔级计算的剩余流量</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/ca2db05c109e3a71c7c55bed0321558b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/0*sRMEcZm9AnGYxki9."/></div></figure><p id="b079" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">在第k个金字塔等级，剩余流量v_k由G_k使用I_k1(来自前一个金字塔的上采样流量)和I_k2计算，I _ k2由上采样流量补偿。那么，流量V_k可以表示为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/200f0ed0a993895afdb3e4e3090d3457.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/0*5wlHdm-X-KFfXtR6."/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d4b1e831a354632cf46230c24c4afa0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/0*e4SoGXin0iwpJNhl."/></div></figure><p id="3af0" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">修道院{ G0，…GK }被独立地训练以计算残差流v_k。此外，地面真实残差流V^_k是通过减去下采样地面真实流v^_k和u(V_k-1)获得的。如图6所示，作者通过最小化剩余流v_k上的平均端点误差(EPE)损失来训练网络</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/c6db5fb357b925e4a8b64a342223c59f.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/0*w55wofFFJrluCDIk."/></div></figure><h1 id="8290" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">表演</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/e945577f77b81b67220f08a4c386fe0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HYz9zfccHkrqYWig."/></div></div></figure><h1 id="b037" class="ks kt iq bd ku kv kw kx ky kz la lb lc jw ld jx le jz lf ka lg kc lh kd li lj bi translated">个人观点</h1><p id="b145" class="pw-post-body-paragraph lk ll iq lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">与Flownet 1.0相比，Flownet 2.0的精度更高的原因是，通过使用堆叠结构和融合网络，网络模型更大。对于堆叠结构，它通过用中间光流扭曲每一层的第二图像，以由粗到细的方式估计大的运动，并计算流更新。因此，这种方法降低了每一级学习任务的难度，为大位移做出了贡献。对于融合网络，作者引入了FlowNet2-CSS和FlowNet2-SD来分别估计大位移和小位移。然后，融合网络旨在更好地融合从上述两个网络学习到的两种光流，期望提高最终预测光流的整体质量。</p><p id="4369" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">从我的角度来看，重复使用特征映射对FlowNet1.0和FlowNet2.0的良好性能产生了影响。作者将估计流量和输入连接在一起，这些流量和输入在当前层中被上采样为特征映射，从与当前层的上采样输入相同分辨率的前层获得，作为下一个去卷积层的输入，因此这些特征映射可以重复使用，在设计概念上与DenseNet有点类似。</p><p id="c8a1" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">SPyNet也是一个堆叠网络。它也适合通过使用由粗到细的方法来处理大位移，类似于FlowNet2.0。flownet 2.0和SPyNet的区别在于，SPyNet比flownet 2.0小得多，SPyNet的每一层都是独立训练的深度网络。SPyNet的模型参数比FlowNet少很多，因为它是直接使用warping函数，女修道院不需要学习。</p><p id="a1fe" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">总的来说，FlowNet2.0的性能最好，而SPyNet要轻量得多，参数更少，速度更快，可以在移动终端上使用。</p><p id="aef2" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated">FlowNet:用卷积网络学习光流链接:<br/><a class="ae nk" href="https://arxiv.org/pdf/1504.06852" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1504.06852<br/></a>FlowNet 2.0论文链接:<br/><a class="ae nk" href="https://lmb.informatik.uni-freiburg.de/Publications/2017/IMKDB17/paper-FlowNet_2_0__CVPR.pdf" rel="noopener ugc nofollow" target="_blank">https://lmb . informatik . uni-freiburg . de/Publications/2017/imk db 17/Paper-FlowNet _ 2 _ 0 _ _ cvpr . pdf</a><br/>利用空间金字塔网络的光流估计链接:<br/><a class="ae nk" href="https://arxiv.org/pdf/1611.00850.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1611.00850.pdf</a></p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><p id="b3ca" class="pw-post-body-paragraph lk ll iq lm b ln mg jr lp lq mh ju ls lt mi lv lw lx mj lz ma mb mk md me mf ij bi translated"><strong class="lm ir">作者</strong>:李子云| <strong class="lm ir">编辑</strong>:杨| <strong class="lm ir">由Synced全球团队本地化</strong>:陈翔</p></div></div>    
</body>
</html>