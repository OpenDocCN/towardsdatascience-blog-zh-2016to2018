<html>
<head>
<title>Milestones of Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的里程碑</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/milestones-of-deep-learning-1aaa9aef5b18?source=collection_archive---------3-----------------------#2017-07-31">https://towardsdatascience.com/milestones-of-deep-learning-1aaa9aef5b18?source=collection_archive---------3-----------------------#2017-07-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="5929" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">深度学习到现在已经有十年左右了。自问世以来，深度学习因其成功而风靡全球(见我的文章“<a class="ae kl" href="http://www.codesofinterest.com/p/what-is-deep-learning.html" rel="noopener ugc nofollow" target="_blank">什么是深度学习？</a>“关于深度学习如何通过人工智能进化，以及机器学习)。以下是这些年来深度学习的一些更重要的成就。</p><p id="b1cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">Alex net——2012 年</strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/a282f7418a920aefd258daee4b76b19d.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*Bzf0fWk60FBsxCtf.PNG"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The AlexNet Architecture (Image from the research paper: “ImageNet Classification with Deep Convolutional Neural Networks”)</figcaption></figure><ul class=""><li id="c7a4" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">证明了卷积神经网络确实有效。AlexNet 及其由 Alex Krizhevsky、Ilya Sutskever 和 Geoffrey E. Hinton 撰写的研究论文“使用深度卷积神经网络进行 ImageNet 分类”通常被认为是将深度学习引入主流的原因。</li><li id="7624" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">以 15.4%的错误率获得 2012 年 ILSVRC (ImageNet 大规模视觉识别挑战赛)冠军。(作为参考，ILSVRC 第二好的条目有 26.2%的错误率)。</li><li id="2ef7" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">8 层:5 层卷积，3 层全连接。</li><li id="a9b3" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">将 ReLU 用于非线性函数，而不是之前使用的传统双曲正切函数。</li><li id="c7c9" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">介绍了漏失层的使用，以及克服过拟合的数据扩充。</li></ul><p id="3849" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">研究论文:“使用深度卷积神经网络的图像网络分类”——Alex Krizhevsky、Ilya Sutskever、Geoffrey E. Hinton</p><p id="d498" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> ZF 网— 2013 </strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/25373635fe698f2c0a5110ddff6e2073.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*q3Onm4HAn_QFMEv5.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The ZF Net Architecture (Image from the research paper: “Visualizing and Understanding Convolutional Networks”)</figcaption></figure><ul class=""><li id="80d2" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">ILSVRC 2013 的冠军，错误率 11.2%。</li><li id="ac86" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">类似于 AlexNet 架构，通过一些调整和微调来提高性能。</li><li id="a8ca" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">介绍了解卷积网络(又名 DeConvNet)，这是一种观察 CNN 内部工作的可视化技术。</li></ul><p id="68f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">研究论文:“可视化和理解卷积网络”——马修·d·泽勒，罗布·弗格斯</p><p id="555d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> VGG 网— 2014 </strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/105d3ebcc6f5f15357e91108dc29fce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/0*OSSRXTxJruayk3Dx.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The VGG Net Architecture (Image from the Keras Blog: <a class="ae kl" href="https://blog.keras.io" rel="noopener ugc nofollow" target="_blank">https://blog.keras.io</a>)</figcaption></figure><ul class=""><li id="eb77" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">获得 ILSVRC 2014“分类+本地化”类别(非总赢家)，错误率 7.3%。</li><li id="0bd5" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">VGG 架构很好地处理了图像分类和定位。</li><li id="ec4c" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">19 层网络，带 3x3 过滤器。(与 AlexNet 的 11x11 滤镜和 ZF Net 的 7x7 滤镜相比)。</li><li id="a439" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">证明了简单的深层结构适用于分层特征提取。</li></ul><p id="a0f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">研究论文:“用于大规模图像识别的深度卷积网络”——卡伦·西蒙扬，安德鲁·齐塞曼</p><p id="7c7c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">谷歌网— 2014/2015 </strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/1c585cab7086cc9859e401dd51bd362b.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/0*-EftrqRjostihm8f.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The GoogLeNet Architecture (Image from the research paper: “Going Deeper with Convolutions”)</figcaption></figure><ul class=""><li id="35dc" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">ILSVRC 2014 的冠军，错误率 6.7%。</li><li id="c958" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">介绍了 Inception 模块，它强调 CNN 的各层并不总是必须按顺序堆叠。</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/cbf3d7234cd488469897b2eeea1ac23b.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*gsKDD3Kw2JBk4ohx.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The Inception Module (Image from the research paper: “Going Deeper with Convolutions”)</figcaption></figure><ul class=""><li id="33b6" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">22 块层(单独考虑时超过 100 层)。</li><li id="1288" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">没有完全连接的层。</li><li id="42b7" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">证明优化的非顺序结构可能比顺序结构工作得更好。</li></ul><p id="68ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">研究论文:“深入了解卷积”——Christian Szegedy、、Jia、Pierre Sermanet、Scott Reed、Dragomir Anguelov、Dumitru Erhan、Vincent Vanhoucke、Andrew Rabinovich、Google Inc .、北卡罗来纳大学教堂山分校、密歇根大学安娜堡分校、Magic Leap Inc .</p><p id="1fd2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">微软 ResNet — 2015 </strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/fa7ab70134cf04fda43934adf15175ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:220/format:webp/0*061fgNDx41SesAzI.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The ResNet Architecture (Image from the research paper: “Deep Residual Learning for Image Recognition”)</figcaption></figure><ul class=""><li id="16b7" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">赢得了 ILSVRC 2015。</li><li id="5e78" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">ResNet 的错误率为 3.6%，比人类的准确率更高(据说典型人类的错误率约为 5-10%)。</li><li id="3e68" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">超深(引用论文作者)架构，152 层。</li><li id="8540" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">引入了残差块，以减少过拟合。</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/7563dbeafef9c3f57ba53d1992a79922.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*IcnZgENi9Ejgu4Kv.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The Residual Block (Image from the research paper: “Deep Residual Learning for Image Recognition”)</figcaption></figure><p id="9776" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">研究论文:“深度残差学习在图像识别中的应用”，，，，任，，微软研究院</p><p id="3265" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着深度学习模型开始超越人类的能力，我们可以肯定地看到更多有趣的深度学习模型，以及未来几年的成就。</p><p id="b5b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">深度学习只是 CNN 吗？</strong></p><p id="4c5a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，回头看看我们上面的列表，你可能会想知道“深度学习”是否只是卷积神经网络。</p><p id="a413" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不完全是。</p><p id="f18a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实际上，以下所有模型都被认为是深度学习。</p><ul class=""><li id="d421" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">卷积神经网络</li><li id="1bb5" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">深层玻尔兹曼机器</li><li id="e9a2" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">深度信念网络</li><li id="02a9" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">堆叠自动编码器</li></ul><p id="4f31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是，在深度学习领域，CNN 是最“定义”的，并且解决了更多相关的问题空间，至少目前是这样。但是，请记住，CNN 并不是深度学习的全貌。</p><p id="5cda" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我错过了深度学习的什么里程碑吗？请在下面添加您的评论，不要忘记喜欢这篇文章。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/2afb075f7857693e402c652887ddd26f.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*Pc9YxcZfvJHcc8ZjOVjxrA.jpeg"/></div></figure><p id="3fd7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你想了解更多关于深度学习的知识，可以看看我的书<a class="ae kl" href="https://www.amazon.com/dp/1549681060" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir">Build Deep:Deep Learning 初学者指南</strong> </a> <strong class="jp ir"> </strong>，这本书现在已经在亚马逊上出售了。</p><p id="458d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">也可以看看我的博客，<a class="ae kl" href="http://www.codesofinterest.com/" rel="noopener ugc nofollow" target="_blank">感兴趣的代码</a>，亲身体验深度学习。</p><p id="ae61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另请参见:</p><p id="228e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="http://www.codesofinterest.com/p/what-is-deep-learning.html" rel="noopener ugc nofollow" target="_blank">什么是深度学习？</a> —深度学习是如何产生的，以及它与机器学习和人工智能的关系(是的，它们的意思略有不同)。</p><p id="2c04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相关链接:</p><p id="9d6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html" rel="noopener ugc nofollow" target="_blank">你需要了解的 9 篇深度学习论文</a>——这篇文章激发了我写这篇文章的灵感。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><p id="921f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ly">原载于 2017 年 7 月 31 日</em><a class="ae kl" href="http://www.codesofinterest.com/2017/07/milestones-of-deep-learning.html" rel="noopener ugc nofollow" target="_blank"><em class="ly">www.codesofinterest.com</em></a><em class="ly">。</em></p></div></div>    
</body>
</html>