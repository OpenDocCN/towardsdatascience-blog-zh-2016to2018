<html>
<head>
<title>Human-Like Machine Hearing With AI (2/3)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有人工智能的类人机器听觉(2/3)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/human-like-machine-hearing-with-ai-2-3-f9fab903b20a?source=collection_archive---------7-----------------------#2018-08-25">https://towardsdatascience.com/human-like-machine-hearing-with-ai-2-3-f9fab903b20a?source=collection_archive---------7-----------------------#2018-08-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/ae30347cf0dfa22ede0b181a6f54f936.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kF8rwj3ZIVhbvpEP"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo credit: <a class="ae jd" href="https://unsplash.com/@rawpixel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">rawpixel</a></figcaption></figure><div class=""/><div class=""><h2 id="d2e5" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">记忆的重要性</h2></div><p id="fcf1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">嗨，欢迎回来！本系列文章详细介绍了我与<a class="ae jd" href="http://www.au.dk/en/" rel="noopener ugc nofollow" target="_blank">奥胡斯大学</a>和智能扬声器制造商<a class="ae jd" href="https://www.dynaudio.com/" rel="noopener ugc nofollow" target="_blank">丹拿</a>合作开发的一个人工智能实时音频信号处理框架。</p><p id="d84f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果您错过了之前的文章，请点击下面的链接了解最新情况:</p><p id="f93b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">背景</strong>:<a class="ae jd" rel="noopener" target="_blank" href="/the-promise-of-ai-in-audio-processing-a7e4996eb2ca">AI 在音频处理上的承诺</a> <br/> <strong class="kx jh">批评</strong>:<a class="ae jd" rel="noopener" target="_blank" href="/whats-wrong-with-spectrograms-and-cnns-for-audio-processing-311377d7ccd">CNN 和 spectrograms 做音频处理有什么问题？</a> <br/> <strong class="kx jh">第一部分</strong> : <a class="ae jd" rel="noopener" target="_blank" href="/human-like-machine-hearing-with-ai-1-3-a5713af6e2f8">具有人工智能(1/3)的仿人机器听觉</a></p><p id="498f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在前面的部分中，我们描绘了人类如何体验声音的基本原理，即在耳蜗中形成的光谱印象，然后由脑干核序列进行“编码”。这篇文章将探讨我们如何在产生频谱声音嵌入时将<em class="lr">记忆</em>与用于声音理解的人工神经网络相结合。</p><h2 id="e829" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">回声记忆</h2><blockquote class="ml"><p id="99e7" class="mm mn jg bd mo mp mq mr ms mt mu lq dk translated">声音事件的<em class="mv">含义</em>在很大程度上源于频谱特征之间的时间相互作用。</p></blockquote><p id="0369" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">这方面的一个证据是，人类的听觉系统根据时间背景以不同的方式对语音的相同音素进行编码[1]。这意味着一个音素<em class="lr"> /e/ </em>在神经学上对我们来说可能意味着不同的东西，这取决于它之前的东西。</p><p id="78d3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">记忆对于进行声音分析至关重要，因为只有将“那一刻”的印象与之前的印象进行比较才是可能的，前提是它们确实存储在某个地方。</p><p id="ec3d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">人类的短期记忆是由一系列整合了感觉记忆和工作记忆的成分组成的。在对声音感知的检查中，已经在人类中发现了<em class="lr">听觉感觉记忆</em>(有时也称为<em class="lr">回声记忆</em>)。c .阿兰等人。艾尔。将听觉感官记忆描述为<em class="lr">“听觉感知中的关键第一阶段，允许听者将传入的声音信息与先前听觉事件的存储表征进行整合”</em>【2】。</p><blockquote class="ml"><p id="b7eb" class="mm mn jg bd mo mp mq mr ms mt mu lq dk translated">从计算上来说，我们可以把回声记忆看作是即时听觉印象的短暂缓冲。</p></blockquote><p id="421c" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">关于回声记忆的持续时间一直存在争议。在纯音和口语元音掩蔽研究的基础上，D. Massaro 论证了大约 250 ms，而 A. Treisman 基于两耳分听实验论证了大约 4 秒[3]。为了将回声记忆的想法与神经网络相结合，我们可能不需要确定感觉存储的固定持续时间，但我们可以在几秒钟的范围内对记忆进行实验。</p><h2 id="f3c3" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">兜圈子</h2><p id="6823" class="pw-post-body-paragraph kv kw jg kx b ky nb kh la lb nc kk ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">用数字光谱表示法实现感觉记忆可能相当简单。我们可以简单地分配一个<em class="lr">循环缓冲器</em>来存储先前时间步的预定数量的光谱。</p><figure class="nh ni nj nk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ng"><img src="../Images/866026ae721bad3943557968be6a8e0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*m2eqO_nweLr2DSd4"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><em class="mv">An illustrated circular buffer for holding spectral memory (where t denotes the timestep).</em></figcaption></figure><blockquote class="ml"><p id="545c" class="mm mn jg bd mo mp nl nm nn no np lq dk translated">循环缓冲区是一种数据结构，由一个被视为循环的数组组成，当数组长度达到后，其索引循环回 0[4]。</p></blockquote><p id="c76d" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">在我们的例子中，这可能是一个多维数组，其长度为所需的内存量，循环缓冲区的每个索引保存特定时间步长的完整频谱。计算新光谱时，会将它们写入缓冲区，如果缓冲区已满，则会覆盖最早的时间步长。</p><p id="3086" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">随着缓冲区的填充，两个指针被更新:一个<em class="lr">尾指针</em>标记最新添加的元素，一个<em class="lr">头指针</em>标记最老的元素，因此也是缓冲区的开始[4]。</p><p id="dedb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面是一个 Python 中循环缓冲区的例子，改编自<a class="ae jd" href="https://github.com/eric-wieser/numpy_ringbuffer" rel="noopener ugc nofollow" target="_blank">埃里克·威瑟</a>:</p><pre class="nh ni nj nk gt nq nr ns nt aw nu bi"><span id="57e4" class="ls lt jg nr b gy nv nw l nx ny">import numpy as np</span><span id="b886" class="ls lt jg nr b gy nz nw l nx ny">class CircularBuffer():<br/>    # Initializes NumPy array and head/tail pointers<br/>    def __init__(self, capacity, dtype=float):<br/>        self._buffer = np.zeros(capacity, dtype)<br/>        self._head_index = 0<br/>        self._tail_index = 0<br/>        self._capacity = capacity</span><span id="0166" class="ls lt jg nr b gy nz nw l nx ny">    # Makes sure that head and tail pointers cycle back around<br/>    def fix_indices(self):<br/>        if self._head_index &gt;= self._capacity:<br/>            self._head_index -= self._capacity<br/>            self._tail_index -= self._capacity<br/>        elif self._head_index &lt; 0:<br/>            self._head_index += self._capacity<br/>            self._tail_index += self._capacity</span><span id="0545" class="ls lt jg nr b gy nz nw l nx ny">    # Inserts a new value in buffer, overwriting old value if full<br/>    def insert(self, value):<br/>        if self.is_full():<br/>            self._head_index += 1</span><span id="7577" class="ls lt jg nr b gy nz nw l nx ny">        self._buffer[self._tail_index % self._capacity] = value<br/>        self._tail_index += 1<br/>        self.fix_indices()<br/>        <br/>    # Returns the circular buffer as an array starting at head index<br/>    def unwrap(self):<br/>        return np.concatenate((<br/>            self._buffer[self._head_index:min(self._tail_index, self._capacity)],<br/>            self._buffer[:max(self._tail_index - self._capacity, 0)]<br/>        ))</span><span id="6394" class="ls lt jg nr b gy nz nw l nx ny">    # Indicates whether the buffer has been filled yet<br/>    def is_full(self):<br/>        return self.count() == self._capacity</span><span id="87f2" class="ls lt jg nr b gy nz nw l nx ny">    # Returns the amount of values currently in buffer<br/>    def count(self):<br/>        return self._tail_index - self._head_index</span></pre><h2 id="fefa" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">减少输入大小</h2><p id="0961" class="pw-post-body-paragraph kv kw jg kx b ky nb kh la lb nc kk ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">为了以每时间步 5 ms 的分辨率存储完整的第二个频谱，需要容量为 200 个元素的缓冲器。这些元素中的每一个都包含一个频率幅度数组。如果需要类似人类的光谱分辨率，这些数组将包含 3500 个值。对于总共 200 个时间步长，要处理 700，000 个值。</p><p id="1110" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果传递给人工神经网络，长度为 700，000 个值的输入存在计算开销大的风险。这种风险可以通过降低光谱和时间分辨率或在内存中保持较短的光谱信息持续时间来减轻。</p><p id="30b4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们还可以从<em class="lr"> Wavenet </em>架构中获得灵感，该架构利用<em class="lr">扩展因果卷积</em>来优化对原始样本音频中大量顺序数据的分析。正如 A. Van Den Oord 等人在<em class="lr">中所解释的，“扩张卷积(也称为á trous，或带孔卷积)是一种卷积，其中通过跳过某个步长的输入值，在大于其长度的区域上应用滤波器”</em> [5]。</p><p id="06f6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设最近输入的频率数据是瞬时声音分析的最大决定因素，那么<em class="lr">扩展频谱缓冲器</em>可能是减少计算内存大小的有用工具。</p><figure class="nh ni nj nk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/300268a72ad98460177702e9af45014b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sG-wciBnniVMYCff"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><em class="mv">Two methods for dimensionality reduction with dilated spectral buffers (in this figure unrolled for clarity).</em></figcaption></figure><p id="3cff" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过与原始缓冲区成比例地将新缓冲区中的每个时间步长扩大某个比例(例如，<em class="lr"> 2^t </em>的指数增加)，维度可以显著减少，同时保持最近时间步长的光谱发展的高分辨率。可以通过简单地越来越向后查找单个值来从原始缓冲区获得扩展缓冲区的值，但是也可以通过提取持续时间内的平均或中值频谱来组合要折叠的时间步长的数量。</p><blockquote class="ml"><p id="fc72" class="mm mn jg bd mo mp mq mr ms mt mu lq dk translated">扩展光谱缓冲区背后的驱动概念是将最近的光谱印象保留在内存中，同时以有效的方式保留一些关于“大画面”上下文的信息。</p></blockquote><p id="12f9" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">下面是使用 Gammatone 滤波器组制作扩展频谱帧的简化代码片段。注意，这个例子使用离线处理，但是滤波器组也可以实时应用，将频谱帧插入到循环缓冲器中。</p><pre class="nh ni nj nk gt nq nr ns nt aw nu bi"><span id="300a" class="ls lt jg nr b gy nv nw l nx ny">from gammatone import gtgram<br/>import numpy as np<br/><br/>class GammatoneFilterbank:<br/>    # Initialize Gammatone filterbank<br/>    def __init__(self, <br/>                 sample_rate, <br/>                 window_time, <br/>                 hop_time, <br/>                 num_filters, <br/>                 cutoff_low):<br/>        self.sample_rate = sample_rate<br/>        self.window_time = window_time<br/>        self.hop_time = hop_time<br/>        self.num_filters = num_filters<br/>        self.cutoff_low = cutoff_low</span><span id="c0e3" class="ls lt jg nr b gy nz nw l nx ny">    # Make a spectrogram from a number of audio samples<br/>    def make_spectrogram(self, audio_samples):<br/>        return gtgram.gtgram(audio_samples,<br/>                             self.sample_rate,<br/>                             self.window_time,<br/>                             self.hop_time,<br/>                             self.num_filters,<br/>                             self.cutoff_low)</span><span id="b8f9" class="ls lt jg nr b gy nz nw l nx ny">    # Divide audio samples into dilated spectral buffers<br/>    def make_dilated_spectral_frames(self, <br/>                                     audio_samples, <br/>                                     num_frames, <br/>                                     dilation_factor):</span><span id="9251" class="ls lt jg nr b gy nz nw l nx ny">        spectrogram = self.make_spectrogram(audio_samples)<br/>        spectrogram = np.swapaxes(spectrogram, 0, 1)<br/>        dilated_frames = np.zeros((len(spectrogram), <br/>                                  num_frames, <br/>                                  len(spectrogram[0])))<br/><br/>        for i in range(len(spectrogram)):<br/>            for j in range(num_frames):<br/>                dilation = np.power(dilation_factor, j)<br/><br/>                if i - dilation &lt; 0:<br/>                    dilated_frames[i][j] = spectrogram[0]<br/>                else:<br/>                    dilated_frames[i][j] = spectrogram[i - dilation]<br/><br/>        return dilated_frames</span></pre><figure class="nh ni nj nk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/55aa6254793c8d2a0671575121b94e59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FjbbdS65FKlCLBbC"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><em class="mv">Result: Two examples of dilated spectral buffers visualized as a quadrilateral mesh.</em></figcaption></figure><h2 id="2860" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">嵌入缓冲区</h2><p id="d99a" class="pw-post-body-paragraph kv kw jg kx b ky nb kh la lb nc kk ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">在人类记忆的许多模型中，<em class="lr">选择性注意</em>被应用在感觉记忆之后，作为一种过滤器，以防止短期记忆中的信息过载【3】。由于人类的认知资源有限，将注意力分配给某些听觉以优化精神能量的消耗是有利的。</p><p id="e66c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个过程可以通过扩展<em class="lr">自动编码器</em>神经网络架构来实现。使用这种架构，可以将声音的感官记忆与选择性注意力的瓶颈结合起来，方法是向其提供扩展的频谱缓冲区以产生嵌入，而不是仅提供瞬时频率信息。为了处理顺序信息，可以使用一种称为<em class="lr">序列到序列自动编码器</em>的特殊类型架构【6】。</p><p id="b5b5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">序列到序列(<em class="lr"> Seq2Seq </em>)模型通常使用 LSTM 单位将一个数据序列(例如，一个英语句子)编码为一个内部表示，其中包含该序列作为一个整体的压缩“含义”。然后，这种内部表达可以被解码成一个序列(同样的句子，但以西班牙语为例)[7]。</p><blockquote class="ml"><p id="7cb6" class="mm mn jg bd mo mp mq mr ms mt mu lq dk translated">以这种方式嵌入声音的一个特点是，它使得使用简单的前馈神经网络来分析和处理声音成为可能，这种网络运行起来更便宜。</p></blockquote><p id="9f46" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">在训练如下图所示的网络之后，右半部分(解码部分)可以被“切断”，从而生成用于将时间频率信息编码到压缩空间中的网络。Y. Chung 等人在这一研究领域取得了良好的成果。艾尔。通过应用 Seq2Seq 自动编码器架构[6]，成功地生成了描述声音记录的顺序语音结构的嵌入。随着输入数据更加多样化，也有可能产生以更通用的方式描述声音的嵌入。</p><figure class="nh ni nj nk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oc"><img src="../Images/745f2994e5181e3e91a9444494612b31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uRQg2hznaxN8-fya"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><em class="mv">A simplified illustration of a sequential autoencoder which produces temporal sound embeddings.</em></figcaption></figure><h2 id="0f1b" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">用 Keras 倾听</h2><p id="09c6" class="pw-post-body-paragraph kv kw jg kx b ky nb kh la lb nc kk ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">使用上述方法，我们可以用 Keras 实现 Seq2Seq 自动编码器来产生音频嵌入。我称之为<strong class="kx jh">监听器</strong> <strong class="kx jh">网络</strong>，因为它的目的是<em class="lr"/>“监听”传入的声音序列，并将其简化为更紧凑、更有意义的表示，以便我们进行分析和处理。</p><p id="5383" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了训练这个网络，使用了来自<a class="ae jd" href="https://urbansounddataset.weebly.com/" rel="noopener ugc nofollow" target="_blank"> UrbanSound8K </a>数据集的约 3 小时的音频。该数据集包含一组分为不同类别的环境声音剪辑。使用 Gammatone 滤波器组处理声音，并将其分割成 8 个时间步长的扩展频谱缓冲区，每个缓冲区有 100 个频谱滤波器。</p><pre class="nh ni nj nk gt nq nr ns nt aw nu bi"><span id="fd64" class="ls lt jg nr b gy nv nw l nx ny">from keras.models import Model<br/>from keras.layers import Input, LSTM, RepeatVector</span><span id="a8f3" class="ls lt jg nr b gy nz nw l nx ny">def prepare_listener(timesteps,<br/>                     input_dim,<br/>                     latent_dim,<br/>                     optimizer_type,<br/>                     loss_type):<br/>    <em class="lr">"""Prepares Seq2Seq autoencoder model<br/><br/>        Args:<br/>            </em><strong class="nr jh"><em class="lr">:param</em></strong><em class="lr"> timesteps: The number of timesteps in sequence<br/>            </em><strong class="nr jh"><em class="lr">:param</em></strong><em class="lr"> input_dim: The dimensions of the input<br/>            </em><strong class="nr jh"><em class="lr">:param</em></strong><em class="lr"> latent_dim: The latent dimensionality of LSTM<br/>            </em><strong class="nr jh"><em class="lr">:param</em></strong><em class="lr"> optimizer_type: The type of optimizer to use<br/>            </em><strong class="nr jh"><em class="lr">:param</em></strong><em class="lr"> loss_type: The type of loss to use<br/><br/>        Returns:<br/>            Autoencoder model, Encoder model<br/>    """<br/><br/>    </em>inputs = Input(shape=(timesteps, input_dim))</span><span id="a6b4" class="ls lt jg nr b gy nz nw l nx ny">    encoded = LSTM(int(input_dim / 2), <br/>                   activation="relu", <br/>                   return_sequences=True)(inputs)</span><span id="e227" class="ls lt jg nr b gy nz nw l nx ny">    encoded = LSTM(latent_dim, <br/>                   activation="relu", <br/>                   return_sequences=False)(encoded)</span><span id="ebc1" class="ls lt jg nr b gy nz nw l nx ny">    decoded = RepeatVector(timesteps)(encoded)</span><span id="5fef" class="ls lt jg nr b gy nz nw l nx ny">    decoded = LSTM(int(input_dim / 2), <br/>                   activation="relu", <br/>                   return_sequences=True)(decoded)</span><span id="382c" class="ls lt jg nr b gy nz nw l nx ny">    decoded = LSTM(input_dim, <br/>                   return_sequences=True)(decoded)<br/><br/>    autoencoder = Model(inputs, decoded)<br/>    encoder = Model(inputs, encoded)<br/><br/>    autoencoder.compile(optimizer=optimizer_type, <br/>                        loss=loss_type,  <br/>                        metrics=['acc'])<br/><br/>    return autoencoder, encoder</span></pre><p id="22e7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">对于我的数据，这段代码生成了下面的网络架构:</strong></p><figure class="nh ni nj nk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/8626ec86025b19e84e5583fca7ad5f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ROa2sRTAKZkS554k"/></div></div></figure><p id="c4aa" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在 NVIDIA GTX 1070 GPU 上使用<em class="lr">均方误差</em>和<em class="lr"> Adagrad </em>优化对这个监听器网络进行了 50 个时期的训练，达到了 42%的重建精度。训练花了一段时间，所以我很早就停止了，尽管进度似乎还没有停滞。我非常有兴趣看到这样一个具有更大数据集和更多计算能力的模型的性能。</p><p id="8a51" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里当然有改进的空间，但是下面的图像显示了序列的粗略结构是在将输入压缩了 3.2 倍之后捕获的。</p><figure class="nh ni nj nk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oe"><img src="../Images/0210f40c6aa542b4bcc1e76e0b6624c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*U8BwlkBl6JnHS7le"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><em class="mv">Some examples of original data and predictions by the autoencoder to illustrate reconstruction fidelity.</em></figcaption></figure></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><p id="5517" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是我关于神经网络音频处理系列文章的第二部分。在最后一篇文章中，我们将把这些概念用于创建一个分析音频嵌入的网络。</p><p id="bb7d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你喜欢这篇文章，请随时关注并留下你的掌声。</p><h2 id="c63a" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">参考</h2><p id="289b" class="pw-post-body-paragraph kv kw jg kx b ky nb kh la lb nc kk ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">[1] J. J. Eggermont，<strong class="kx jh">“在声音和感知之间:回顾对神经代码的探索。，"</strong>听到。《研究报告》，第 157 卷，第 1-2 期，第 1-42 页，2001 年 7 月。</p><p id="30cb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] C. Alain、D. L. Woods 和 R. T. Knight，<strong class="kx jh">“人类听觉感觉记忆的分布式皮层网络”，</strong>《大脑研究》，第 812 卷，第 1-2 期，第 23-37 页，1998 年 11 月。</p><p id="5f1d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[3] A .温菲尔德，<strong class="kx jh">《工作记忆和认知资源模型的进化》，</strong>耳听。，第 37 卷，第 35S–43S 页，2016 年。</p><p id="4b93" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[4] <strong class="kx jh">“在嵌入式 C 中实现循环/环形缓冲区”</strong>，Embedjournal.com，2014。【在线】。可用:<a class="ae jd" href="https://embedjournal.com/implementing-circular-buffer-embedded-c/." rel="noopener ugc nofollow" target="_blank">https://embed journal . com/implementing-circular-buffer-embedded-c/。</a></p><p id="4485" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[5] A .范登奥尔德等人，<strong class="kx jh">“wave net:原始音频的生成模型。”</strong></p><p id="ce7c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[6] Y.-A. Chung、C.-C. Wu、C.-H. Shen、H.-Y. Lee 和 L.-S. Lee，<strong class="kx jh">“音频词 2Vec:使用序列到序列自动编码器的音频段表示的无监督学习”，</strong>国际语音通信协会年会论文集，2016 年，第 765-769 页。</p><p id="6b4f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[7] F. Chollet，<strong class="kx jh">《Keras 中序列对序列学习的十分钟介绍》</strong>，Blog.keras.io，2018。【在线】。可用:<a class="ae jd" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html." rel="noopener ugc nofollow" target="_blank">https://blog . keras . io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras . html</a></p></div></div>    
</body>
</html>