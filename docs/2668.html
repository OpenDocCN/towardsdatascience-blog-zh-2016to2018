<html>
<head>
<title>Multi-Class Text Classification with Scikit-Learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Scikit-Learn 进行多类文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f?source=collection_archive---------0-----------------------#2018-02-19">https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f?source=collection_archive---------0-----------------------#2018-02-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/29ddd9a58a2cfc8489f3be098eb5e833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mdPG5HAPKDxL1J-QMxTDKg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Image credit: pexels</figcaption></figure><p id="ad57" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">文本分类在商业领域有很多应用。例如，新闻故事通常是按主题组织的；内容或产品通常按类别进行标记；用户可以根据他们在网上谈论产品或品牌的方式进行分类…</p><p id="db81" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">但是网上绝大多数的文本分类文章和教程都是邮件垃圾过滤(spam vs. ham)、情感分析(正面 vs .负面)等二元文本分类。在大多数情况下，我们现实世界的问题要比这复杂得多。因此，这就是我们今天要做的事情:将消费金融投诉分为 12 个预定义的类别。数据可以从<a class="ae la" href="https://catalog.data.gov/dataset/consumer-complaint-database" rel="noopener ugc nofollow" target="_blank">data.gov</a>下载。</p><p id="5322" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们使用<a class="ae la" href="https://www.python.org/" rel="noopener ugc nofollow" target="_blank"> Python </a>和<a class="ae la" href="http://jupyter.org/" rel="noopener ugc nofollow" target="_blank"> Jupyter Notebook </a>来开发我们的系统，依靠<a class="ae la" href="http://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a>作为机器学习组件。如果您想在<a class="ae la" href="https://spark.apache.org/docs/0.9.0/mllib-guide.html" rel="noopener ugc nofollow" target="_blank"> PySpark </a>中看到实现，请阅读<a class="ae la" href="https://medium.com/@actsusanli/multi-class-text-classification-with-pyspark-7d78d022ed35" rel="noopener">下一篇文章</a>。</p><h1 id="c190" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">问题定式化</h1><p id="a94f" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">该问题是有监督的文本分类问题，我们的目标是调查哪些有监督的机器学习方法最适合解决它。</p><p id="ce5d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">假设有新的投诉进来，我们希望将其分配到 12 个类别中的一个。分类器假设每个新投诉被分配到一个且仅一个类别。这是多类文本分类问题。我迫不及待地想看看我们能实现什么！</p><h1 id="5878" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据探索</h1><p id="c535" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">在开始训练机器学习模型之前，我们应该先看看一些例子和每个类中的投诉数量:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="56c5" class="mn lc iq mj b gy mo mp l mq mr">import pandas as pd<br/>df = pd.read_csv('Consumer_Complaints.csv')<br/>df.head()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ms"><img src="../Images/92bdcd14733737c1cb91a7e72bb9da7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*88sJLbqhTHXbZIr9_ZvOXg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1</figcaption></figure><p id="888f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于这个项目，我们只需要两列—“产品”和“消费者投诉叙述”。</p><ul class=""><li id="7d74" class="mt mu iq ke b kf kg kj kk kn mv kr mw kv mx kz my mz na nb bi translated"><strong class="ke ir">输入</strong>:消费者 _ 投诉 _ 叙述</li></ul><p id="0a4d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">例如:“我的信用报告中有过时的信息，我以前质疑过这些信息，但现在还没有删除。这些信息已经存在了七年多，不符合信用报告要求”</p><ul class=""><li id="264b" class="mt mu iq ke b kf kg kj kk kn mv kr mw kv mx kz my mz na nb bi translated"><strong class="ke ir">输出</strong>:产品</li></ul><p id="14e5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">例如:信用报告</p><p id="6e42" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们将删除“消费者投诉叙述”列中缺少的值，并添加一个将产品编码为整数的列，因为分类变量通常用整数表示比用字符串表示更好。</p><p id="fc16" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们还创建了几个字典供将来使用。</p><p id="eab8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">清理之后，这是我们将要处理的前五行数据:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="f2ff" class="mn lc iq mj b gy mo mp l mq mr">from io import StringIO</span><span id="cd13" class="mn lc iq mj b gy nc mp l mq mr">col = ['Product', 'Consumer complaint narrative']<br/>df = df[col]<br/>df = df[pd.notnull(df['Consumer complaint narrative'])]</span><span id="7b35" class="mn lc iq mj b gy nc mp l mq mr">df.columns = ['Product', 'Consumer_complaint_narrative']</span><span id="3aa5" class="mn lc iq mj b gy nc mp l mq mr">df['category_id'] = df['Product'].factorize()[0]<br/>category_id_df = df[['Product', 'category_id']].drop_duplicates().sort_values('category_id')<br/>category_to_id = dict(category_id_df.values)<br/>id_to_category = dict(category_id_df[['category_id', 'Product']].values)<br/>df.head()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/4db43eefcf84ece6e5f1d8830ec8b7fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*6Gv_SJM_CaQHoDa2bcm92w.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2</figcaption></figure><h1 id="7373" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">不平衡的班级</h1><p id="cba2" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">我们看到每个产品的投诉数量不平衡。消费者的投诉更偏向于催债、征信、房贷。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="6209" class="mn lc iq mj b gy mo mp l mq mr">import matplotlib.pyplot as plt<br/>fig = plt.figure(figsize=(8,6))<br/>df.groupby('Product').Consumer_complaint_narrative.count().plot.bar(ylim=0)<br/>plt.show()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/d7ea814cc24c4d9a0b3973315792daa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*OQlJEad4CKFaWxOivngLxA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3</figcaption></figure><p id="c477" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">当我们遇到这样的问题时，我们必然很难用标准算法来解决它们。传统算法往往偏向于多数类，没有考虑数据分布。在最坏的情况下，少数类被视为离群值并被忽略。对于某些情况，如欺诈检测或癌症预测，我们需要仔细配置我们的模型或人工平衡数据集，例如通过对每个类进行<a class="ae la" href="https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis" rel="noopener ugc nofollow" target="_blank">欠采样或过采样</a>。</p><p id="726a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">然而，在我们学习不平衡数据的情况下，多数类可能是我们最感兴趣的。希望有一种对多数类给出高预测精度，同时对少数类保持合理精度的分类器。因此，我们将保持原样。</p><h1 id="783f" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">文本表示</h1><p id="13c2" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">分类器和学习算法不能直接处理原始形式的文本文档，因为它们中的大多数期望具有固定大小的数字特征向量，而不是具有可变长度的原始文本文档。因此，在预处理步骤中，文本被转换成更易于管理的表示。</p><p id="52f6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">从文本中提取特征的一种常见方法是使用单词袋模型:在该模型中，对于每个文档(在我们的情况下是投诉叙述)，考虑单词的存在(通常是频率)，但忽略它们出现的顺序。</p><p id="aa59" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">具体来说，对于数据集中的每个术语，我们将计算一个称为术语频率的度量，即逆文档频率，缩写为 tf-idf。我们将使用<code class="fe nf ng nh mj b">sklearn.feature_extraction.text.TfidfVectorizer </code>来计算每个消费者投诉叙述的<code class="fe nf ng nh mj b">tf-idf</code>向量:</p><ul class=""><li id="5f86" class="mt mu iq ke b kf kg kj kk kn mv kr mw kv mx kz my mz na nb bi translated"><code class="fe nf ng nh mj b">sublinear_df</code>设置为<code class="fe nf ng nh mj b">True</code>以使用对数形式的频率。</li><li id="5d22" class="mt mu iq ke b kf ni kj nj kn nk kr nl kv nm kz my mz na nb bi translated"><code class="fe nf ng nh mj b">min_df</code>是一个单词必须存在的最小文档数。</li><li id="9a28" class="mt mu iq ke b kf ni kj nj kn nk kr nl kv nm kz my mz na nb bi translated"><code class="fe nf ng nh mj b">norm</code>被设置为<code class="fe nf ng nh mj b">l2</code>，以确保我们所有的特征向量具有 1 的欧几里德范数。</li><li id="bd89" class="mt mu iq ke b kf ni kj nj kn nk kr nl kv nm kz my mz na nb bi translated"><code class="fe nf ng nh mj b">ngram_range</code>被设置为<code class="fe nf ng nh mj b">(1, 2)</code>,表示我们既要考虑单元组，也要考虑二元组。</li><li id="e197" class="mt mu iq ke b kf ni kj nj kn nk kr nl kv nm kz my mz na nb bi translated"><code class="fe nf ng nh mj b">stop_words</code>设置为<code class="fe nf ng nh mj b">"english"</code>删除所有常用代词(<code class="fe nf ng nh mj b">"a"</code>，<code class="fe nf ng nh mj b">"the"</code>，...)以减少噪声特征的数量。</li></ul><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="b8f6" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="e172" class="mn lc iq mj b gy nc mp l mq mr">tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')</span><span id="bbe5" class="mn lc iq mj b gy nc mp l mq mr">features = tfidf.fit_transform(df.Consumer_complaint_narrative).toarray()<br/>labels = df.category_id<br/>features.shape</span></pre><p id="3076" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi">(4569, 12633)</p><p id="1f07" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在，4569 个消费者投诉叙述中的每一个都由 12633 个特征表示，代表不同单字和双字的 tf-idf 得分。</p><p id="70e0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们可以使用<code class="fe nf ng nh mj b">sklearn.feature_selection.chi2</code>找到与每种产品最相关的术语:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="17ea" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.feature_selection import chi2<br/>import numpy as np</span><span id="954c" class="mn lc iq mj b gy nc mp l mq mr">N = 2<br/>for Product, category_id in sorted(category_to_id.items()):<br/>  features_chi2 = chi2(features, labels == category_id)<br/>  indices = np.argsort(features_chi2[0])<br/>  feature_names = np.array(tfidf.get_feature_names())[indices]<br/>  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]<br/>  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]<br/>  print("# '{}':".format(Product))<br/>  print("  . Most correlated unigrams:\n. {}".format('\n. '.join(unigrams[-N:])))<br/>  print("  . Most correlated bigrams:\n. {}".format('\n. '.join(bigrams[-N:])))</span></pre><p id="a904" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"># ' <strong class="ke ir">银行账户或服务</strong> ': <br/>。最相关的单字:<br/>。银行<br/>。透支<br/>。最相关的二元模型:<br/>。透支费<br/>。支票账户<br/> # ' <strong class="ke ir">消费贷款</strong> ': <br/>。最相关的单字:<br/>。汽车<br/>。车辆<br/>。最相关的二元模型:<br/>。车辆 xxxx <br/>。丰田金融<br/> # ' <strong class="ke ir">信用卡</strong> ': <br/>。最相关的单字:<br/>。花旗<br/>。卡片<br/>。最相关的二元模型:<br/>。年费<br/>。信用卡<br/> # ' <strong class="ke ir">征信</strong> ': <br/>。最相关的单字:<br/>。experian <br/>。equifax <br/>。最相关的二元模型:<br/>。跨联<br/>。征信报告<br/>#’<strong class="ke ir">讨债</strong>:<br/>。最相关的单字:<br/>。收藏<br/>。债务<br/>。最相关的二元模型:<br/>。讨债<br/>。代收机构<br/>#’<strong class="ke ir">汇款</strong>’:<br/>。最相关的单字:<br/>。吴<br/>。贝宝<br/>。最相关的二元模型:<br/>。西联<br/>。转账<br/> # ' <strong class="ke ir">抵押</strong> ': <br/>。最相关的单字:<br/>。修改<br/>。抵押<br/>。最相关二元模型:<br/>。抵押公司<br/>。贷款修改<br/> # ' <strong class="ke ir">其他金融服务</strong> ': <br/>。最相关的单字:<br/>。牙科<br/>。护照<br/>。最相关二元模型:<br/>。帮付<br/>。声明支付<br/> # ' <strong class="ke ir">发薪日贷款</strong> ': <br/>。最相关的单字:<br/>。借来的<br/>。发薪日<br/>。最相关二元模型:<br/>。大图<br/>。发薪日贷款<br/> # ' <strong class="ke ir">预付卡</strong> ': <br/>。最相关的单字:<br/>。上菜<br/>。预付<br/>。最相关二元模型:<br/>。存取款<br/>。预付卡<br/> # ' <strong class="ke ir">助学贷款</strong> ': <br/>。最相关的单字:<br/>。学生<br/>。导航<br/>。最相关的二元模型:<br/>。助学贷款<br/>。助学贷款<br/>#‘<strong class="ke ir">虚拟货币</strong>‘:<br/>。最相关的单字:<br/>。手柄<br/>。https <br/>。最相关二元模型:<br/>。xxxx 供应商<br/>。缺钱</p><p id="c4f6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">他们都有道理，你不这样认为吗？</p><h2 id="e4a5" class="mn lc iq bd ld nn no dn lh np nq dp ll kn nr ns lp kr nt nu lt kv nv nw lx nx bi translated">多类分类器:特征与设计</h2><ul class=""><li id="6ace" class="mt mu iq ke b kf lz kj ma kn ny kr nz kv oa kz my mz na nb bi translated">为了训练监督分类器，我们首先将“消费者投诉叙述”转换成数字向量。我们研究了向量表示，如 TF-IDF 加权向量。</li><li id="ead6" class="mt mu iq ke b kf ni kj nj kn nk kr nl kv nm kz my mz na nb bi translated">有了这种文本的向量表示后，我们可以训练监督分类器来训练看不见的“消费者投诉叙述”，并预测它们所涉及的“产品”。</li></ul><p id="a3e4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在所有上述数据转换之后，现在我们已经有了所有的特征和标签，是时候训练分类器了。对于这类问题，我们可以使用许多算法。</p><ul class=""><li id="ac83" class="mt mu iq ke b kf kg kj kk kn mv kr mw kv mx kz my mz na nb bi translated"><strong class="ke ir">朴素贝叶斯分类器</strong>:最适合字数统计的一个是多项式变量:</li></ul><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="ddc6" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.model_selection import train_test_split<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.feature_extraction.text import TfidfTransformer<br/>from sklearn.naive_bayes import MultinomialNB</span><span id="3f3d" class="mn lc iq mj b gy nc mp l mq mr">X_train, X_test, y_train, y_test = train_test_split(df['Consumer_complaint_narrative'], df['Product'], random_state = 0)<br/>count_vect = CountVectorizer()<br/>X_train_counts = count_vect.fit_transform(X_train)<br/>tfidf_transformer = TfidfTransformer()<br/>X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)</span><span id="815b" class="mn lc iq mj b gy nc mp l mq mr">clf = MultinomialNB().fit(X_train_tfidf, y_train)</span></pre><p id="f107" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">拟合完训练集之后，我们来做一些预测。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="6aba" class="mn lc iq mj b gy mo mp l mq mr">print(clf.predict(count_vect.transform(["This company refuses to provide me verification and validation of debt per my right under the FDCPA. I do not believe this debt is mine."])))</span></pre><p id="b538" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">['讨债']</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="cda7" class="mn lc iq mj b gy mo mp l mq mr">df[df['Consumer_complaint_narrative'] == "This company refuses to provide me verification and validation of debt per my right under the FDCPA. I do not believe this debt is mine."]</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ob"><img src="../Images/b1265454cbe73528e55130c4548cc61e.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*Kh4tNh4c_RWWfWtUB_BRcg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4</figcaption></figure><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="b988" class="mn lc iq mj b gy mo mp l mq mr">print(clf.predict(count_vect.transform(["I am disputing the inaccurate information the Chex-Systems has on my credit report. I initially submitted a police report on XXXX/XXXX/16 and Chex Systems only deleted the items that I mentioned in the letter and not all the items that were actually listed on the police report. In other words they wanted me to say word for word to them what items were fraudulent. The total disregard of the police report and what accounts that it states that are fraudulent. If they just had paid a little closer attention to the police report I would not been in this position now and they would n't have to research once again. I would like the reported information to be removed : XXXX XXXX XXXX"])))</span></pre><p id="0713" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">['信用报告']</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="10c7" class="mn lc iq mj b gy mo mp l mq mr">df[df['Consumer_complaint_narrative'] == "I am disputing the inaccurate information the Chex-Systems has on my credit report. I initially submitted a police report on XXXX/XXXX/16 and Chex Systems only deleted the items that I mentioned in the letter and not all the items that were actually listed on the police report. In other words they wanted me to say word for word to them what items were fraudulent. The total disregard of the police report and what accounts that it states that are fraudulent. If they just had paid a little closer attention to the police report I would not been in this position now and they would n't have to research once again. I would like the reported information to be removed : XXXX XXXX XXXX"]</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/e723c72545faa142d60677d30506c385.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*EfoFO8AP1yk0hejdiR5o2A.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 5</figcaption></figure><p id="5b9a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">不算太寒酸！</p><h1 id="2881" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">型号选择</h1><p id="156e" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">我们现在准备用不同的机器学习模型进行实验，评估它们的准确性，并找到任何潜在问题的来源。</p><p id="790a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们将对以下四个模型进行基准测试:</p><ul class=""><li id="9d9d" class="mt mu iq ke b kf kg kj kk kn mv kr mw kv mx kz my mz na nb bi translated">逻辑回归</li><li id="db34" class="mt mu iq ke b kf ni kj nj kn nk kr nl kv nm kz my mz na nb bi translated">(多项式)朴素贝叶斯</li><li id="70e2" class="mt mu iq ke b kf ni kj nj kn nk kr nl kv nm kz my mz na nb bi translated">线性支持向量机</li><li id="dc8e" class="mt mu iq ke b kf ni kj nj kn nk kr nl kv nm kz my mz na nb bi translated">随机森林</li></ul><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="f681" class="mn lc iq mj b gy mo mp l mq mr">from sklearn.linear_model import LogisticRegression<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.svm import LinearSVC</span><span id="3ed0" class="mn lc iq mj b gy nc mp l mq mr">from sklearn.model_selection import cross_val_score</span><span id="418c" class="mn lc iq mj b gy nc mp l mq mr">models = [<br/>    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),<br/>    LinearSVC(),<br/>    MultinomialNB(),<br/>    LogisticRegression(random_state=0),<br/>]<br/>CV = 5<br/>cv_df = pd.DataFrame(index=range(CV * len(models)))<br/>entries = []<br/>for model in models:<br/>  model_name = model.__class__.__name__<br/>  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)<br/>  for fold_idx, accuracy in enumerate(accuracies):<br/>    entries.append((model_name, fold_idx, accuracy))<br/>cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])</span><span id="4777" class="mn lc iq mj b gy nc mp l mq mr">import seaborn as sns</span><span id="f01c" class="mn lc iq mj b gy nc mp l mq mr">sns.boxplot(x='model_name', y='accuracy', data=cv_df)<br/>sns.stripplot(x='model_name', y='accuracy', data=cv_df, <br/>              size=8, jitter=True, edgecolor="gray", linewidth=2)<br/>plt.show()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi od"><img src="../Images/8b66734abaabd7dc0ca8385b1677ceb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*NjBnbUbhZV_Cug7canCXuw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 6</figcaption></figure><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="6825" class="mn lc iq mj b gy mo mp l mq mr">cv_df.groupby('model_name').accuracy.mean()</span></pre><p id="740d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">model _ Name<br/><strong class="ke ir">linear SVC:</strong>0.822890<br/><strong class="ke ir">logistic regression:</strong>0.792927<br/><strong class="ke ir">MultinomialNB:</strong>0.688519<br/><strong class="ke ir">RandomForestClassifier:</strong>0.443826<br/>Name:accuracy，dtype: float64</p><p id="7747" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">LinearSVC 和逻辑回归比其他两个分类器表现得更好，LinearSVC 以大约 82%的中值精度略微占优。</p><h1 id="c436" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">模型评估</h1><p id="85c7" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">继续我们的最佳模型(LinearSVC)，我们将查看混淆矩阵，并显示预测和实际标签之间的差异。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="b63a" class="mn lc iq mj b gy mo mp l mq mr">model = LinearSVC()</span><span id="7f5f" class="mn lc iq mj b gy nc mp l mq mr">X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0)<br/>model.fit(X_train, y_train)<br/>y_pred = model.predict(X_test)</span><span id="b1fd" class="mn lc iq mj b gy nc mp l mq mr">from sklearn.metrics import confusion_matrix</span><span id="34f5" class="mn lc iq mj b gy nc mp l mq mr">conf_mat = confusion_matrix(y_test, y_pred)<br/>fig, ax = plt.subplots(figsize=(10,10))<br/>sns.heatmap(conf_mat, annot=True, fmt='d',<br/>            xticklabels=category_id_df.Product.values, yticklabels=category_id_df.Product.values)<br/>plt.ylabel('Actual')<br/>plt.xlabel('Predicted')<br/>plt.show()</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/883b506c0c91e5376e03046198819383.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*mioLqk8LeLvVsHfH1ZHaXA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 7</figcaption></figure><p id="1597" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">绝大多数预测最终出现在对角线上(预测标签=实际标签)，这是我们希望它们出现的位置。但是，存在许多错误分类，看看这些错误分类是由什么引起的可能会很有意思:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="eacb" class="mn lc iq mj b gy mo mp l mq mr">from IPython.display import display</span><span id="9491" class="mn lc iq mj b gy nc mp l mq mr">for predicted in category_id_df.category_id:<br/>  for actual in category_id_df.category_id:<br/>    if predicted != actual and conf_mat[actual, predicted] &gt;= 10:<br/>      print("'{}' predicted as '{}' : {} examples.".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))<br/>      display(df.loc[indices_test[(y_test == actual) &amp; (y_pred == predicted)]][['Product', 'Consumer_complaint_narrative']])<br/>      print('')</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi of"><img src="../Images/9cbe1b0daabaa7b1f74dc8531b040f0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*R43ECkNO8tNPkPPDiZRUPw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 8</figcaption></figure><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi og"><img src="../Images/8cb82d092b36841dba2278c4c64bcd0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*kQTqpHWDKfl3fvfwHvPhVw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 9</figcaption></figure><p id="6e7d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如您所见，一些错误分类的投诉涉及多个主题(例如，涉及信用卡和信用报告的投诉)。这种错误总是会发生。</p><p id="6795" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">同样，我们使用<a class="ae la" href="https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test" rel="noopener ugc nofollow" target="_blank">卡方检验</a>来查找与每个类别最相关的术语:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9125" class="mn lc iq mj b gy mo mp l mq mr">model.fit(features, labels)</span><span id="5d25" class="mn lc iq mj b gy nc mp l mq mr">N = 2<br/>for Product, category_id in sorted(category_to_id.items()):<br/>  indices = np.argsort(model.coef_[category_id])<br/>  feature_names = np.array(tfidf.get_feature_names())[indices]<br/>  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]<br/>  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]<br/>  print("# '{}':".format(Product))<br/>  print("  . Top unigrams:\n       . {}".format('\n       . '.join(unigrams)))<br/>  print("  . Top bigrams:\n       . {}".format('\n       . '.join(bigrams)))</span></pre><p id="d962" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"># ' <strong class="ke ir">银行账户或服务</strong> ': <br/>。Top unigrams: <br/>。银行<br/>。账户<br/>。Top bigrams: <br/>。借记卡<br/>。透支费<br/> # ' <strong class="ke ir">消费贷款</strong> ': <br/>。顶级单字:<br/>。车辆<br/>。汽车<br/>。热门二元模型:<br/>。个人贷款<br/>。历史 xxxx <br/> # ' <strong class="ke ir">信用卡</strong> ': <br/>。顶级单字:<br/>。卡片<br/>。发现<br/>。顶级双元:<br/>。信用卡<br/>。发现卡<br/> # ' <strong class="ke ir">征信</strong> ': <br/>。Top unigrams: <br/>。equifax <br/>。跨联<br/>。Top bigrams: <br/>。xxxx 账户<br/>。跨联<br/> # ' <strong class="ke ir">讨债</strong> ': <br/>。Top unigrams: <br/>。债务<br/>。收藏<br/>。顶级名人:<br/>。账户贷记<br/>。提供时间<br/>#’<strong class="ke ir">汇款</strong>’:<br/>。Top unigrams: <br/>。贝宝<br/>。传送<br/>。Top bigrams: <br/>。汇款<br/>。送钱<br/>#‘<strong class="ke ir">抵押</strong>‘:<br/>。Top unigrams: <br/>。抵押<br/>。托管<br/>。Top bigrams: <br/>。贷款修改<br/>。抵押公司<br/> # ' <strong class="ke ir">其他金融服务</strong> ': <br/>。Top unigrams: <br/>。护照<br/>。牙科<br/>。Top bigrams: <br/>。规定薪酬<br/>。帮助支付<br/> # ' <strong class="ke ir">发薪日贷款</strong> ': <br/>。Top unigrams: <br/>。发薪日<br/>。贷款<br/>。Top bigrams: <br/>。发薪日贷款<br/>。发薪日<br/>#‘<strong class="ke ir">预付卡</strong>‘:<br/>。Top unigrams: <br/>。预付<br/>。上菜<br/>。顶级双面人物:<br/>。预付卡<br/>。用卡<br/>#‘<strong class="ke ir">助学贷款</strong>‘:<br/>。Top unigrams: <br/>。导航<br/>。贷款<br/>。Top bigrams: <br/>。助学贷款<br/>。莎莉美<br/> # ' <strong class="ke ir">虚拟货币</strong> ': <br/>。Top unigrams: <br/>。https <br/>。tx <br/>。顶级双元:<br/>。钱要<br/>。xxxx 提供商</p><p id="ed70" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">它们符合我们的预期。</p><p id="77a8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">最后，我们打印出每个类别的分类报告:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="09c6" class="mn lc iq mj b gy mo mp l mq mr">from sklearn import metrics<br/>print(metrics.classification_report(y_test, y_pred, target_names=df['Product'].unique()))</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/d6995644516e5954a64e88ff0587b778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*adBx92klC_Dkjv4edQuoOQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 9</figcaption></figure><p id="38d3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">源代码可以在<a class="ae la" href="https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。我期待听到任何反馈或问题。</p></div></div>    
</body>
</html>