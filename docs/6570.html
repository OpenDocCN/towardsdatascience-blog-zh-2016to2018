<html>
<head>
<title>Implementation of Uni-Variate Polynomial Regression in Python using Gradient Descent Optimization from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始使用梯度下降优化在 Python 中实现单变量多项式回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0?source=collection_archive---------10-----------------------#2018-12-19">https://towardsdatascience.com/implementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0?source=collection_archive---------10-----------------------#2018-12-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="13e8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">学习、实施和调整… </em></h2></div><p id="df9e" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">回归是一种对特征空间中的数据或数据点进行连续分类的方法。弗朗西斯·高尔顿在 1886 年发明了回归线的用法[1]。</p><p id="1036" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">线性回归</strong></p><p id="86f9" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这是多项式回归的一个特例，假设中多项式的次数是 1。一般多项式回归将在文章的后半部分讨论。顾名思义，“线性”，这意味着关于机器学习算法的假设本质上是线性的，或者仅仅是线性方程。耶！！这确实是一个线性方程。在单变量线性回归中，目标变量依赖于单一特征或变量。</p><p id="02cf" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">单变量线性回归的假设如下:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/8a658e7dd464be13b826ba02b448ca2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rs5oxdI4gWmhTKXHhNgAIA.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">where <strong class="bd lv">theta_0</strong> and <strong class="bd lv">theta_1</strong> are the parameters and <strong class="bd lv">x</strong> is the single feature or variable</figcaption></figure><p id="d8ce" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">上述假设也可以用矩阵乘法格式或向量代数的形式写成:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lw"><img src="../Images/8f62f96648a41a5fee2335b4c504b024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H59j-Q8biP7VaYJvhZmDPg.png"/></div></div></figure><p id="47f7" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">存在与取决于参数θ_ 0 和θ_ 1 的假设相关联的成本函数。</p><p id="e8fa" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">线性回归的成本函数通常如下所示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lx"><img src="../Images/fcdb37e85a1e8e8187c212c029ed269b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7O7ytp_hTS2bYeksQqIrdQ.png"/></div></div></figure><p id="ecf2" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在，这两个参数 theta_0 和 theta_1 必须采用这样的值，使得该成本函数值(即成本)采用可能的最小值。因此，现在的基本目标是找到成本最小的θ_ 0 和θ_ 1 的值，或者简单地找到成本函数的最小值。</p><p id="bb6b" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">梯度下降是最重要的凸优化技术之一，使用它可以找到函数的最小值。梯度下降算法如下所示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ly"><img src="../Images/36b4e9849b1864248746cebfe350a0b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zJFjDZi1Kvq4B9r-BvG9dw.jpeg"/></div></div></figure><p id="38b9" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">梯度下降有两种方法:</p><ol class=""><li id="0105" class="lz ma it kl b km kn kp kq ks mb kw mc la md le me mf mg mh bi translated">随机梯度下降</li><li id="eebd" class="lz ma it kl b km mi kp mj ks mk kw ml la mm le me mf mg mh bi translated">批量梯度下降</li></ol></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="4908" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">使用随机梯度下降实现线性回归:</strong></p><p id="4565" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在随机梯度下降中，运行梯度下降算法，一次从数据集中取一个实例。</p><p id="a4b0" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">实现是通过创建具有不同操作的 3 个模块来完成的:</p><p id="c032" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">=&gt;hypothesis():是在给定 theta (theta_0 和 theta_1)和 Feature，X 作为输入的情况下，计算并输出目标变量的假设值的函数。假设()的实现如下所示:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="460f" class="mz na it mv b gy nb nc l nd ne">def <strong class="mv iu">hypothesis</strong>(theta, X):<br/>    h = np.ones((X.shape[0],1))<br/>    for i in range(0,X.shape[0]):<br/>        x = np.concatenate((np.ones(1), np.array([X[i]])), axis = 0)<br/>        h[i] = float(np.matmul(theta, x))<br/>    h = h.reshape(X.shape[0])<br/>    return h</span></pre><p id="6953" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">=&gt;SGD():该函数执行随机梯度下降算法，将当前值 theta_0 和 theta_1、alpha、迭代次数(num_iters)、假设值(h)、特征集(X)和目标变量集(y)作为输入，并在由实例表征的每次迭代时输出优化的 theta (theta_0 和 theta_1)。SGD()的实现如下所示:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="cff5" class="mz na it mv b gy nb nc l nd ne">def <strong class="mv iu">SGD</strong>(theta, alpha, num_iters, h, X, y):<br/>    for i in range(0,num_iters):<br/>        theta[0] = theta[0] - (alpha) * (h - y)<br/>        theta[1] = theta[1] - (alpha) * ((h - y) * X)<br/>        h = theta[1]*X + theta[0] <br/>    return theta</span></pre><p id="eaaa" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">=&gt;sgd_linear_regression():它是主函数，将特征集(X)、目标变量集(y)、学习速率和迭代次数(num_iters)作为输入，输出最终的优化θ，即代价函数在随机梯度下降后几乎达到最小值的θ_ 0 和θ_ 1 的值。</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="3e06" class="mz na it mv b gy nb nc l nd ne">def <strong class="mv iu">sgd_linear_regression</strong>(X, y, alpha, num_iters):<br/>    # initializing the parameter vector...<br/>    theta = np.zeros(2)<br/>    # hypothesis calculation....<br/>    h = hypothesis(theta, X)    <br/>    # returning the optimized parameters by Gradient Descent...<br/>    for i in range(0, X.shape[0]):<br/>        theta = SGD(theta,alpha,num_iters,h[i],X[i],y[i])<br/>    theta = theta.reshape(1, 2)<br/>    return theta</span></pre></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="cd56" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在，我们来看一个实践数据集，它包含了公司利润如何依赖于城市人口的信息。该数据集可在 GitHub link 上获得，</p><div class="nf ng gp gr nh ni"><a href="https://github.com/navoneel1092283/univariate_regression" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd iu gy z fp nn fr fs no fu fw is bi translated">navoneel 1092283/单变量 _ 回归</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">通过在 GitHub 上创建帐户，为 navoneel 1092283/univariate _ regression 开发做出贡献。</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">github.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw lp ni"/></div></div></a></div><p id="dd01" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">问题陈述</strong>:<em class="nx">给定一个城市的人口，用线性回归分析预测一家公司的利润</em></p><p id="e1b1" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">数据读入 Numpy 数组:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="b0d3" class="mz na it mv b gy nb nc l nd ne">data = np.loadtxt('data1.txt', delimiter=',')<br/>X_train = data[:,0] #the feature_set<br/>y_train = data[:,1] #the labels</span></pre><p id="79e9" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">数据可视化:可以使用散点图来可视化数据集:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="e02a" class="mz na it mv b gy nb nc l nd ne">import matplotlib.pyplot as plt<br/>plt.scatter(X_train, y_train)<br/>plt.xlabel('Population of City in 10,000s')<br/>plt.ylabel('Profit in $10,000s')</span></pre><p id="be2f" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">散点图数据可视化看起来像-</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ny"><img src="../Images/639fa4132d9be88e8caf71e110e721d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*ErDDLd7A5HztztY7zU6TMQ.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Scatter Plot</figcaption></figure><p id="0b40" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">使用三模块线性回归-SGD:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="3846" class="mz na it mv b gy nb nc l nd ne"># calling the principal function with <strong class="mv iu">learning_rate = 0.0001</strong> and <br/># <strong class="mv iu">num_iters = 100000</strong><br/>theta = sgd_linear_regression(X_train, y_train, 0.0001, 100000)</span></pre><p id="00f7" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">θ输出结果为:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nz"><img src="../Images/6c1296db02880fd475986e6ed2c34be6.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*7wgoBZHadQOeJn7ZB26HTA.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">theta after SGD</figcaption></figure><p id="acde" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">散点图上θ的可视化:</p><p id="ea3c" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">获得的θ的回归线可视化可以在散点图上完成:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="c80b" class="mz na it mv b gy nb nc l nd ne">import matplotlib.pyplot as plt <br/># getting the predictions...<br/>training_predictions = hypothesis(theta, X_train)<br/>scatter = plt.scatter(X_train, y_train, label="training data")<br/>regression_line = plt.plot(X_train, training_predictions<br/>                           , label="linear regression")<br/>plt.legend()<br/>plt.xlabel('Population of City in 10,000s')<br/>plt.ylabel('Profit in $10,000s')</span></pre><p id="1882" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">回归线可视化结果为:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/04730d49182ca9b4df7de5f5ed5eacd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*A13GDSWr7-nJch1CBdvboQ.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Regression Line Visualization after SGD</figcaption></figure><p id="6891" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">使用批量梯度下降实现线性回归:</strong></p><p id="904e" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在批量梯度下降中，梯度下降算法运行，一次从数据集中提取所有实例。</p><p id="b749" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">实现是通过创建具有不同操作的 3 个模块来完成的:</p><p id="018a" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">=&gt;hypothesis():是在给定 theta (theta_0 和 theta_1)和 Feature，X 作为输入的情况下，计算并输出目标变量的假设值的函数。假设()的实现保持不变。</p><p id="350d" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">=&gt;BGD():它是执行批量梯度下降算法的函数，将当前的θ_ 0 和θ_ 1、α、迭代次数(num_iters)、所有样本的假设值列表(h)、特征集(X)和目标变量集(y)作为输入，并输出优化的θ(θ_ 0 和θ_ 1)、θ_ 0 历史(θ_ 0)和θ_ 1 历史(θ_ 1)，即每次迭代的θ_ 0 和θ_ 1 的值，以及最终包含成本函数值的成本历史 Gradient_Descent()的实现如下所示:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="41c6" class="mz na it mv b gy nb nc l nd ne">def <strong class="mv iu">BGD</strong>(theta, alpha, num_iters, h, X, y):<br/>    cost = np.ones(num_iters)<br/>    theta_0 = np.ones(num_iters)<br/>    theta_1 = np.ones(num_iters)<br/>    for i in range(0,num_iters):<br/>        theta[0] = theta[0] - (alpha/X.shape[0]) * sum(h - y)<br/>        theta[1] = theta[1] - (alpha/X.shape[0]) * sum((h - y) * X)<br/>        h = hypothesis(theta, X)<br/>        cost[i] = (1/X.shape[0]) * 0.5 * sum(np.square(h - y))<br/>        theta_0[i] = theta[0]<br/>        theta_1[i] = theta[1]<br/>    theta = theta.reshape(1,2)<br/>    return theta, theta_0, theta_1, cost</span></pre><p id="0839" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">=&gt;linear_regression():它是主函数，将特征集(X)、目标变量集(y)、学习速率和迭代次数(num_iters)作为输入，并输出最终优化的 theta，即成本函数在批量梯度下降后几乎达到最小值的 theta_0 和 theta_1 的值，以及存储每次迭代的成本值的<strong class="kl iu"> cost </strong>。</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="2545" class="mz na it mv b gy nb nc l nd ne">def <strong class="mv iu">linear_regression</strong>(X, y, alpha, num_iters):<br/>    # initializing the parameter vector...<br/>    theta = np.zeros(2)<br/>    # hypothesis calculation....<br/>    h = hypothesis(theta, X)    <br/>    # returning the optimized parameters by Gradient Descent...<br/>    theta,theta_0,theta_1,cost= BGD(theta,alpha,num_iters,h,X,y)<br/>    return theta, theta_0, theta_1, cost</span></pre><p id="cd85" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在相同的利润估算数据集上使用 3 模块线性回归 BGD:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="dd1f" class="mz na it mv b gy nb nc l nd ne">data = np.loadtxt('data1.txt', delimiter=',')<br/>X_train = data[:,0] #the feature_set<br/>y_train = data[:,1] #the labels<br/># calling the principal function with <strong class="mv iu">learning_rate = 0.0001 </strong>and<strong class="mv iu"> </strong> <br/># <strong class="mv iu">num_iters = 300</strong><br/>theta,theta_0,theta_1,cost=linear_regression(X_train,y_train,<br/>                                             0.0001,300)</span></pre><p id="92ae" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">θ输出结果为:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/98c7e5b5e16b82e55c16327a18bb9dac.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*3hWndC57spvXefgknjEr6w.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">theta after BGD</figcaption></figure><p id="5d29" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">散点图上θ的可视化:</p><p id="1286" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">获得的θ的回归线可视化可以在散点图上完成:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="dd84" class="mz na it mv b gy nb nc l nd ne">import matplotlib.pyplot as plt<br/>training_predictions = hypothesis(theta, X_train)<br/>scatter = plt.scatter(X_train, y_train, label="training data")<br/>regression_line = plt.plot(X_train, training_predictions, label="linear regression")<br/>plt.legend()<br/>plt.xlabel('Population of City in 10,000s')<br/>plt.ylabel('Profit in $10,000s')</span></pre><p id="bde6" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">回归线可视化结果为:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/6db93d4a3899d2b2f3dd0c1e637ae66b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*3fSApk1buMdo4UZzK0iP1Q.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Regression Line Visualization after BGD</figcaption></figure><p id="9dca" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">此外，在逐迭代的批量梯度下降过程中，成本已经降低。成本的降低借助于线形曲线和曲面图显示出来。</p><p id="405d" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">代表 300 次迭代中成本降低的线形曲线:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="8484" class="mz na it mv b gy nb nc l nd ne">import matplotlib.pyplot as plt<br/>cost = list(cost)<br/>n_iterations = [x for x in range(1,301)]<br/>plt.plot(n_iterations, cost)<br/>plt.xlabel('No. of iterations')<br/>plt.ylabel('Cost')</span></pre><p id="c54a" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">线条曲线显示为:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/1cb8413a2c69f3e04c135c2264a0df0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*yE1czIyIk-_wHhgFY0JC7w.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Line Curve Representation of Cost Minimization using BGD</figcaption></figure><p id="b559" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">表示成本降低的曲面图:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="aff8" class="mz na it mv b gy nb nc l nd ne">J = np.ones((300,300))<br/>in1 = 0<br/>in2 = 0<br/>theta_0 = theta_0.reshape(300)<br/>theta_1 = theta_1.reshape(300)<br/>for i in theta_0:<br/>    for j in theta_1:<br/>        t = np.array([i,j])<br/>        h = hypothesis(t, X_train)<br/>        J[in1][in2]=(1/X_train.shape[0])*0.5*sum(np.square(h-y_train))<br/>        in2 = in2 + 1<br/>    in1 = in1 + 1<br/>    in2 = 0</span><span id="9fb9" class="mz na it mv b gy od nc l nd ne">from mpl_toolkits.mplot3d import Axes3D</span><span id="d0e7" class="mz na it mv b gy od nc l nd ne">fig = plt.figure()<br/>ax = fig.add_subplot(111, projection='3d')</span><span id="7c64" class="mz na it mv b gy od nc l nd ne">X,Y = np.meshgrid(theta_0, theta_1)<br/>ax.plot_surface(X, Y, J)<br/>ax.set_xlabel('theta_0')<br/>ax.set_ylabel('theta_1')<br/>ax.set_zlabel('J')</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/bf629a1cb284e86245ebef3f0ca3a36c.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*arxLV0N0j7KizjAwjrow9g.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Surface Plot Representation of Cost Minimizaion with values of theta_0 and theta_1</figcaption></figure></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="b42c" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">表现分析(新加坡元对 BGD) </strong></p><p id="3038" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">模型性能分析基于以下指标:</p><p id="8351" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">= &gt;平均绝对误差:实例样本中预测值和实际观测值之间的平均模(差)。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/8dc6da54df84b67884a77379fcbf6e90.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*HpZ1kgpcJnoUCKrDqMLI8g.png"/></div></figure><p id="e33a" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">寻找梅:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="1451" class="mz na it mv b gy nb nc l nd ne">ae = 0 # Absolute Error<br/>for i in range(0,y_train.shape[0]):<br/>    ae = ae + abs(training_predictions[i] - y_train[i])<br/>MAE = ae/y_train.shape[0] # Mean Absolute Error</span></pre><p id="2773" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">= &gt;均方误差:实例样本中预测值和实际观测值之间的平方差的平均值。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/c13426e04adbf94b49f05fcbdfddb148.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*S_didVUXppiHVbU84kwnxA.png"/></div></figure><p id="4e7b" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">查找 MSE:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="39c1" class="mz na it mv b gy nb nc l nd ne">from math import *<br/>se = 0 # Square Error<br/>for i in range(0,y_train.shape[0]):<br/>    se = se + pow((training_predictions[i] - y_train[i]), 2)<br/>MSE = se/y_train.shape[0] # Mean Square Error</span></pre><p id="aa9b" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">= &gt;均方根误差:在一个实例样本中，预测值和实际观测值之间的平方差的平均值的平方根。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/e7dad9a2d71b9b8f4b67001d1395f93a.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*MlFpFAZHXRlFDdOs3RdqGw.png"/></div></figure><p id="9f30" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">寻找 RMSE:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="ee99" class="mz na it mv b gy nb nc l nd ne">from math import *<br/>RMSE = sqrt(MSE) # Root Mean Square Error</span></pre><p id="7aec" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">=&gt;R 平方得分或决定系数:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/886bd846483396ac80fa5fbfcc1052ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*HLcUG9fsuRCl_FxHZ3RWLg.png"/></div></figure><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="551d" class="mz na it mv b gy nb nc l nd ne">import numpy as np<br/>y_m = np.mean(y_train)</span><span id="1d39" class="mz na it mv b gy od nc l nd ne">SStot = 0<br/>for i in range(0,y_train.shape[0]):<br/>    SStot = SStot + pow((y_train[i] - y_m), 2)</span><span id="522b" class="mz na it mv b gy od nc l nd ne">SSres = 0<br/>for i in range(0,y_train.shape[0]):<br/>    SSres = SSres + pow((y_train[i] - training_predictions[i]), 2)</span><span id="bdbe" class="mz na it mv b gy od nc l nd ne">R_Square_Score = 1 - (SSres/SStot)</span></pre><p id="a7af" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">新加坡元和 BGD 的比较:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oj"><img src="../Images/b5654ec3f4cba1fdef8df7218c2e3bdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5yvGUuiSlNwQcVieuAYgPQ.png"/></div></div></figure><p id="e6c6" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">因此，批量梯度下降在各方面都明显优于随机梯度下降！！</p><p id="8af4" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这就是使用梯度下降在 Python 中实现单变量线性回归的全部内容。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="dd79" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">多项式回归</strong></p><p id="4a7b" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在涉及对单个特征或变量进行回归的预测分析问题中(称为单变量回归)，多项式回归是回归分析的一个重要变体，主要用作线性回归的性能助推器。在本文中，我将介绍多项式回归、它的 Python 实现以及在一个实际问题上的应用和性能分析。</p><p id="3921" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">正如前缀“多项式”所暗示的，机器学习算法的相应假设是多项式或多项式方程。因此，这可以是任何次数的，比如如果假设是一次多项式，那么它是一个线性方程，因此称为线性回归，如果假设是二次多项式，那么它是一个二次方程，类似地，如果是三次多项式，那么它是一个三次方程，等等。因此，可以说:</p><p id="4180" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><em class="nx">线性回归是多项式回归的真子集或特例方法，因此多项式回归也称为广义回归</em></p><p id="29db" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">多项式回归的假设如下:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/418b9a30041e3b4e840561f44d7a2bfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*12FjJY0qZfB6-I5DLPfvEA.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">where <strong class="bd lv">theta_0, theta_1, theta_2, theta_3,…., theta_n </strong>are the parameters and <strong class="bd lv">x</strong> is the single feature or variable</figcaption></figure><p id="34a8" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">其中<strong class="kl iu">θ_ 0，θ_ 1，θ_ 2，θ_ 3，…，theta_n </strong>是参数，<strong class="kl iu"> x </strong>是单一特征或变量</p><p id="fe57" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">上述假设也可以用矩阵乘法格式或向量代数的形式写成:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/76bcbaed7e8cf90fc952f85bfa02109c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*Sie5kM5hnw3Z9WPjQiTFlQ.png"/></div></figure><p id="ba03" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这里，也存在与依赖于参数θ_ 0，θ_ 1，θ_ 2，θ_ 3，…的假设相关联的成本函数。，theta_n。</p><p id="65eb" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">一般来说，广义回归的成本函数如下所示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/3cbb46ca8b8d6ca7dbbe828eac825347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*XZcngufGAUd0CI7__Ts4sg.png"/></div></figure><p id="1239" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">因此，这些参数θ_ 0，θ_ 1，θ_ 2，…，θ_ n 必须采用这样的值，对于这些值，成本函数(或简单地成本)达到其可能的最小值。换句话说，需要找出成本函数的最小值。</p><p id="e36c" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">批量梯度下降可以用作优化函数。</p><p id="7303" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">使用批量梯度下降实现多项式回归:</strong></p><p id="f398" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">实现是通过创建 3 个执行不同操作的模块来完成的。</p><p id="9f6d" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">=&gt;hypothesis():是计算并输出目标变量的假设值的函数，给定 theta (theta_0，theta_1，theta_2，theta_3，…，theta_n)，特征 X 和多项式回归中多项式的次数(n)作为输入。假设()的实现如下所示:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="573f" class="mz na it mv b gy nb nc l nd ne">def <strong class="mv iu">hypothesis</strong>(theta, X, n):<br/>    h = np.ones((X.shape[0],1))<br/>    theta = theta.reshape(1,n+1)<br/>    for i in range(0,X.shape[0]):<br/>        x_array = np.ones(n+1)<br/>        for j in range(0,n+1):<br/>            x_array[j] = pow(X[i],j)<br/>        x_array = x_array.reshape(n+1,1)<br/>        h[i] = float(np.matmul(theta, x_array))<br/>    h = h.reshape(X.shape[0])<br/>    return h</span></pre><p id="21fa" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">=&gt;BGD():该函数执行批量梯度下降算法，将当前的θ值(theta_0，theta_1，…，theta_n)、学习速率(alpha)、迭代次数(num_iters)、所有样本的假设值列表(h)、特征集(X)、目标变量集(y)和多项式回归中多项式的次数(n)作为输入，并输出优化的 theta (theta_0，theta_1，theta_2，theta_3，…，theta_n)、theta_history(包含每次迭代的 theta 值的列表)和最终 BGD()的实现如下所示:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="ae4b" class="mz na it mv b gy nb nc l nd ne">def <strong class="mv iu">BGD</strong>(theta, alpha, num_iters, h, X, y, n):<br/>    theta_history = np.ones((num_iters,n+1))<br/>    cost = np.ones(num_iters)<br/>    for i in range(0,num_iters):<br/>        theta[0] = theta[0] — (alpha/X.shape[0]) * sum(h — y)<br/>        for j in range(1,n+1):<br/>            theta[j]=theta[j]-(alpha/X.shape[0])*sum((h-y)*pow(X,j))<br/>        theta_history[i] = theta<br/>        h = hypothesis(theta, X, n)<br/>        cost[i] = (1/X.shape[0]) * 0.5 * sum(np.square(h — y))<br/>    theta = theta.reshape(1,n+1)<br/>    return theta, theta_history, cost</span></pre><p id="597f" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">=&gt;poly_regression():它是主函数，将特征集(X)、目标变量集(y)、学习率(alpha)、多项式回归中多项式的次数(n)和迭代次数(num_iters)作为输入，输出最终优化的 theta，即[ <strong class="kl iu"> theta_0，theta_1，theta_2，theta_3，…]的值。成本函数在批量梯度下降后几乎达到最小值的θ_ n</strong><strong class="kl iu">，存储每次迭代的θ值的θ_ history</strong>，以及存储每次迭代的成本值的<strong class="kl iu"> cost </strong>。</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="aacb" class="mz na it mv b gy nb nc l nd ne">def <strong class="mv iu">poly_regression</strong>(X, y, alpha, n, num_iters):<br/>    # initializing the parameter vector…<br/>    theta = np.zeros(n+1)<br/>    # hypothesis calculation….<br/>    h = hypothesis(theta, X, n)<br/>    # returning the optimized parameters by Gradient Descent<br/>    theta,theta_history,cost=BGD(theta,alpha,num_iters,h, X, y, n)<br/>    return theta, theta_history, cost</span></pre><p id="7c85" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在，我们来看一个实践数据集，它包含了公司利润如何依赖于城市人口的信息。</p><p id="dc29" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在利润估算数据集上使用 3 模多项式回归，</p><div class="nf ng gp gr nh ni"><a href="https://github.com/navoneel1092283/univariate_regression" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd iu gy z fp nn fr fs no fu fw is bi translated">navoneel 1092283/单变量 _ 回归</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">通过在 GitHub 上创建帐户，为 navoneel 1092283/univariate _ regression 开发做出贡献。</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">github.com</p></div></div><div class="nr l"><div class="ol l nt nu nv nr nw lp ni"/></div></div></a></div><p id="42dd" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">问题陈述</strong>:<em class="nx">给定一个城市的人口，用多项式回归分析预测一家公司的利润</em></p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="3635" class="mz na it mv b gy nb nc l nd ne">data = np.loadtxt(‘data1.txt’, delimiter=’,’)<br/>X_train = data[:,0] #the feature_set<br/>y_train = data[:,1] #the labels<br/><strong class="mv iu"># calling the principal function with learning_rate = 0.0001 and <br/># n = 2(quadratic_regression) and num_iters = 300000<br/></strong>theta,theta_history,cost=poly_regression(X_train,y_train,<br/>                                         0.00001,2,300000)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b0b0f48574880054786851df17910a57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*AYrj8cWCMsyPv8XrJC3Raw.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">theta after Polynomial Regression</figcaption></figure><p id="745d" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">散点图上θ的可视化:</p><p id="cbd7" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">获得的θ的回归线可视化可以在散点图上完成:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="5d80" class="mz na it mv b gy nb nc l nd ne">import matplotlib.pyplot as plt<br/>training_predictions = hypothesis(theta, X_train, 2)<br/>scatter = plt.scatter(X_train, y_train, label=”training data”) regression_line = plt.plot(X_train, training_predictions<br/>                          ,label=”polynomial (degree 2) regression”)<br/>plt.legend()<br/>plt.xlabel(‘Population of City in 10,000s’)<br/>plt.ylabel(‘Profit in $10,000s’)</span></pre><p id="7970" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">回归线可视化结果为:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/27234a62299810ae0655fbbafec7d590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*piJxuRhGRWEJ7cECHm6o9Q.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Regression Line Visualization after Quadratic Regression</figcaption></figure><p id="39be" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">此外，在逐迭代的批量梯度下降过程中，成本已经降低。成本的降低借助于曲线显示出来。</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="332f" class="mz na it mv b gy nb nc l nd ne">import matplotlib.pyplot as plt<br/>cost = list(cost)<br/>n_iterations = [x for x in range(1,300001)]<br/>plt.plot(n_iterations, cost)<br/>plt.xlabel(‘No. of iterations’)<br/>plt.ylabel(‘Cost’)</span></pre><p id="b6aa" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">线条曲线显示为:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/3dd927bc419cd20a2e4e2460cd0aebff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*vagF1apjuSMxHVT9uHEaQw.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Line Curve Representation of Cost Minimization using BGD in Quadratic Regression</figcaption></figure><p id="c5fe" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在，必须进行模型性能分析以及多项式回归与线性回归的比较(迭代次数相同)。</p><p id="7ee0" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">因此，线性回归是使用批量梯度下降完成的，迭代次数为 3，00，000 次，学习速率(alpha)为 0.0001，使用的是我上一篇文章中的实现:</p><div class="nf ng gp gr nh ni"><a rel="noopener follow" target="_blank" href="/implementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd iu gy z fp nn fr fs no fu fw is bi translated">使用梯度下降优化在 Python 中实现单变量线性回归…</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">学习、编码和调整…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">towardsdatascience.com</p></div></div><div class="nr l"><div class="op l nt nu nv nr nw lp ni"/></div></div></a></div><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="6728" class="mz na it mv b gy nb nc l nd ne">data = np.loadtxt(‘data1.txt’, delimiter=’,’)<br/>X_train = data[:,0] #the feature_set<br/>y_train = data[:,1] #the labels<br/># calling the principal function with <strong class="mv iu">learning_rate = 0.0001 </strong>and<strong class="mv iu"> </strong><br/># <strong class="mv iu">num_iters = 300000</strong><br/>theta,theta_0,theta_1,cost=linear_regression(X_train,y_train,<br/>                                             0.0001,300000)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/379ebd01fc8efc83c67e0b02011b1b75.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*oR0o5wkITzr3QiHglddyoA.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">theta after Linear Regression</figcaption></figure><p id="cb44" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">散点图上θ的可视化:</p><p id="b308" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">获得的θ的回归线可视化可以在散点图上完成:</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="59bc" class="mz na it mv b gy nb nc l nd ne">import matplotlib.pyplot as plt<br/>training_predictions = hypothesis(theta, X_train)<br/>scatter = plt.scatter(X_train, y_train, label=”training data”)<br/>regression_line = plt.plot(X_train, training_predictions<br/>                           , label=”linear regression”)<br/>plt.legend()<br/>plt.xlabel(‘Population of City in 10,000s’)<br/>plt.ylabel(‘Profit in $10,000s’)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/08bc65e437c37110115c3423b52bbe31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*UWl9C4Rxx7GipJJ82wfrng.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Regression Line Visualization after Linear Regression</figcaption></figure><p id="4658" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">此外，在逐迭代的批量梯度下降过程中，成本已经降低。成本的降低借助于曲线显示出来。</p><pre class="lg lh li lj gt mu mv mw mx aw my bi"><span id="b44c" class="mz na it mv b gy nb nc l nd ne">import matplotlib.pyplot as plt<br/>cost = list(cost)<br/>n_iterations = [x for x in range(1,300001)]<br/>plt.plot(n_iterations, cost)<br/>plt.xlabel(‘No. of iterations’)<br/>plt.ylabel(‘Cost’)</span></pre><p id="9843" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">线条曲线显示为:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/c19f8da7d0c66bebf6dddc34247e5c52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*lst2P-b6SnxK4HAtiyYE6A.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Line Curve Representation of Cost Minimization using BGD in Linear Regression</figcaption></figure><p id="4160" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">性能分析(使用 BGD 优化的线性回归与二次回归):</strong></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/02c7232c2cccd6d98b10bacced487826.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*Veh-4odNHQPv6M-CCr4bFg.png"/></div></figure><p id="cce0" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">但是在这里，使用批量梯度下降优化，我们最终得到的线性回归<strong class="kl iu">在所有方面都优于</strong>多项式(二次)回归。但是，在实践中，多项式(高次或广义)回归总是比线性回归表现更好。虽然使用 BGD，但由于 BGD 优化本身的一些缺点，我们无法获得与命题相匹配的实验结果。还有另一种优化或寻找多项式(也称线性)回归中成本函数最小值的方法，称为<strong class="kl iu">【OLS(普通最小二乘法)】</strong>或<strong class="kl iu">正规方程法</strong>。</p><p id="2571" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我将在以后的文章中讨论这些内容，并提到本文的参考资料。使用 OLS，可以清楚地证明多项式(在本实验中是二次的)比线性回归表现得更好。除了单变量问题，使用适当的特征工程技术，多项式回归也可用于多变量问题(多特征)。</p><p id="065b" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这就是多项式回归的全部内容。</p><p id="e716" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">参考文献</strong></p><p id="a088" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">[1]迈克尔·布尔默(2003 年)。弗朗西斯·高尔顿:遗传和生物统计学的先驱。<a class="ae os" href="https://en.wikipedia.org/wiki/Johns_Hopkins_University_Press" rel="noopener ugc nofollow" target="_blank">约翰霍普金斯大学出版社</a>。<a class="ae os" href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" rel="noopener ugc nofollow" target="_blank">ISBN</a><a class="ae os" href="https://en.wikipedia.org/wiki/Special:BookSources/0-8018-7403-3" rel="noopener ugc nofollow" target="_blank">0–8018–7403–3</a>。</p></div></div>    
</body>
</html>