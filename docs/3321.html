<html>
<head>
<title>How to build a Gesture Controlled Web based Game using Tensorflow Object Detection Api</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用Tensorflow对象检测Api构建手势控制的网络游戏</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-a-gesture-controlled-web-based-game-using-tensorflow-object-detection-api-587fb7e0f907?source=collection_archive---------4-----------------------#2018-05-02">https://towardsdatascience.com/how-to-build-a-gesture-controlled-web-based-game-using-tensorflow-object-detection-api-587fb7e0f907?source=collection_archive---------4-----------------------#2018-05-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="f73b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过在网络摄像头前挥动你的手来控制游戏手柄。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/f2908140598b401808e387c02ff84e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*LChMXQ5wyRKUdKqTr9pA0A.gif"/></div></div></figure><p id="cbbb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用<a class="kx ky ep" href="https://medium.com/u/b1d410cb9700?source=post_page-----587fb7e0f907--------------------------------" rel="noopener" target="_blank"> TensorFlow </a>对象检测api，我们已经看到了模型被训练来检测图像中的自定义对象的例子(例如，检测<a class="ae kz" rel="noopener" target="_blank" href="/how-to-build-a-real-time-hand-detector-using-neural-networks-ssd-on-tensorflow-d6bac0e4b2ce">手</a>、<a class="ae kz" rel="noopener" target="_blank" href="/building-a-toy-detector-with-tensorflow-object-detection-api-63c0fdf2ac95">玩具</a>、<a class="ae kz" rel="noopener" target="_blank" href="/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9">浣熊</a>、<a class="ae kz" href="https://pythonprogramming.net/custom-objects-tracking-tensorflow-object-detection-api-tutorial/" rel="noopener ugc nofollow" target="_blank"> mac n cheese </a>)。自然，下一个有趣的步骤是探索如何在真实世界用例中部署这些模型——例如，<em class="la">交互设计</em>。</p><p id="34cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将介绍一个基本的<em class="la">身体作为输入</em>交互示例，其中来自手部跟踪模型(作为输入的网络摄像头流)的实时结果被映射到基于网络的游戏(Skyfall)的控件上。该系统演示了如何集成一个相当精确的轻量级手部检测模型来跟踪玩家的手部，并实现实时的<em class="la">身体作为输入</em>交互。</p><p id="aa6b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想试试吗？项目代码可在<a class="ae kz" href="https://github.com/victordibia/skyfall" rel="noopener ugc nofollow" target="_blank"> Github </a>上获得。</p><div class="lb lc gp gr ld le"><a href="https://github.com/victordibia/skyfall" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">维克托迪亚/天崩地裂</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">skyfall -使用Tensorflow对象检测Api的手势控制网络游戏</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">github.com</p></div></div><div class="ln l"><div class="lo l lp lq lr ln ls kv le"/></div></div></a></div><h1 id="cdc6" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">始终可用(Body as)输入</h1><p id="3b38" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">使用人体部分作为输入的好处是<em class="la">始终可用</em>，因为用户不需要携带任何辅助设备。重要的是，利用人体部分进行基于手势的交互已经被证明可以改善用户体验[2]和整体参与度[1]。虽然身体作为输入的想法并不是全新的，但利用计算机视觉、可穿戴设备和传感器(kinect、wii、[5])等的现有方法有时会遇到准确性挑战，不总是可移植的，并且与第三方软件集成也很困难。轻量级深度神经网络(DNNs)的进展，特别是对象检测(见[3])和关键点提取(见[4])的模型，有望解决这些问题，并进一步实现始终可用(身体作为)输入的目标。这些模型使我们能够使用2D图像以良好的精确度跟踪人体，并具有与一系列应用程序和设备(桌面、网络、移动设备)轻松集成的优势。虽然从2D图像中进行追踪并不能给我们提供太多的深度信息，但它在构建互动方面仍然有着惊人的价值，正如Skyfall游戏示例中所示。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/38215df860bd03640ec798c82a498047.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*MIU-z3d_Q-tdX3SVB04Dyw.gif"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Body as input on a large display. Results from a realtime object detection model (applied to webcam feed) are mapped to the controls of a game.</figcaption></figure><h1 id="594e" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">游戏机制</h1><p id="782a" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">Skyfall是一个简单的基于网络的游戏，使用2D的物理引擎<a class="ae kz" href="https://github.com/shakiba/planck.js" rel="noopener ugc nofollow" target="_blank"> planck.js </a>创建。天崩地裂的玩法很简单。3种类型的球从屏幕顶部随机落下——白色球(值10分)、绿色球(值10分)和红色球(值-10分)。球员通过移动球拍抓住好球(白色和绿色球)并避开坏球(红色球)来得分。在下面的例子中，玩家可以通过移动鼠标或在移动设备上触摸(拖动)来控制球拍。</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="na nb l"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Try out a mouse controlled version of SkyFall: Catch good balls (white, green), avoid bad balls (red) by moving the mouse. Press space bar to pause game and key s to toggle sound. View on <a class="ae kz" href="https://codepen.io/victordibia/full/aGpRZV/" rel="noopener ugc nofollow" target="_blank">Codepen</a> here.</figcaption></figure><p id="2855" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是一个非常简单有趣的游戏。然而，我们可以让用户用他们的身体(手)来控制球拍，这样会更吸引人。目标是通过使用网络摄像头视频流检测手的位置来实现这一点——不需要额外的传感器或可穿戴设备。</p><h1 id="9fcd" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">添加基于手势的交互</h1><p id="01a3" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">为了增加手势交互，我们用一个将玩家手的运动映射到游戏手柄位置的系统来代替上面的鼠标控制。在当前的实现中，python应用程序(app.py)使用<a class="kx ky ep" href="https://medium.com/u/b1d410cb9700?source=post_page-----587fb7e0f907--------------------------------" rel="noopener" target="_blank"> TensorFlow </a>对象检测api检测玩家的手，并通过websockets将手的坐标传输到游戏界面——一个使用FLASK服务的web应用程序。</p><h2 id="c7b0" class="nc lu iq bd lv nd ne dn lz nf ng dp md jy nh ni mh kc nj nk ml kg nl nm mp nn bi translated">手检测/跟踪</h2><div class="lb lc gp gr ld le"><a rel="noopener follow" target="_blank" href="/how-to-build-a-real-time-hand-detector-using-neural-networks-ssd-on-tensorflow-d6bac0e4b2ce"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">如何在Tensorflow上使用神经网络(SSD)构建实时手部检测器</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">这篇文章记录了使用Tensorflow(对象检测API)训练手部检测器的步骤和脚本。我在……</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">towardsdatascience.com</p></div></div><div class="ln l"><div class="no l lp lq lr ln ls kv le"/></div></div></a></div><p id="142a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae kz" rel="noopener" target="_blank" href="/how-to-build-a-real-time-hand-detector-using-neural-networks-ssd-on-tensorflow-d6bac0e4b2ce">之前的一篇文章</a>中，我介绍了如何使用Tensorflow对象检测api构建一个实时手部检测器。请查看<a class="ae kz" rel="noopener" target="_blank" href="/how-to-build-a-real-time-hand-detector-using-neural-networks-ssd-on-tensorflow-d6bac0e4b2ce">的博文</a>，了解更多关于手部跟踪模型是如何构建的。有关加载手模型的任何错误或问题，请参见手跟踪<a class="ae kz" href="https://github.com/victordibia/handtracking" rel="noopener ugc nofollow" target="_blank"> Github repo </a>和<a class="ae kz" href="https://github.com/victordibia/handtracking/issues?utf8=%E2%9C%93&amp;q=" rel="noopener ugc nofollow" target="_blank">问题</a>。本例采用了类似的方法，多线程python应用程序读取网络摄像头视频，并为检测到的每只手输出边界框。</p><p id="eaad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，手检测是在逐帧的基础上完成的，并且系统不会跨帧自动跟踪手。然而，这种类型的帧间跟踪是有用的，因为它可以实现多用户交互，我们需要跨帧跟踪手(想象一群朋友挥舞着他们的手或其他一些共同的对象，每个人都控制着自己的球拍)。为此，当前的实现包括基于<a class="ae kz" href="https://github.com/victordibia/skyfall/blob/master/utils/object_id_utils.py" rel="noopener ugc nofollow" target="_blank">朴素欧几里德距离的</a>跟踪，其中在跨帧的相似位置看到的手被分配相同的id。</p><p id="e34d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦帧中的每只手都被检测到(并且分配了跟踪id)，手的坐标就被发送到<strong class="jp ir">网络套接字服务器</strong>，该服务器将它发送到连接的客户端。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/20f366a8fd2a24ebe66065b381fd28db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*YWMH5vAatVz-3mtzwkl6NQ.gif"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Additional hands, additional paddles … potential for some engaging multiplayer games with friends. A tracking id of 0 is assigned to one hand and 1 to the other. These are then mapped to the paddles on the game interface.</figcaption></figure><p id="e011" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">游戏界面</strong></p><p id="a7b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">游戏接口连接到<strong class="jp ir">网络套接字服务器</strong>并监听手部检测数据。每个检测到的手用于生成一个球拍，并且手在视频帧中的坐标用于在游戏屏幕上相对定位球拍。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi np"><img src="../Images/8e411d20a031ca17a76e128a8d9bb254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QHZSF_2h5VsqJROS1GgBZg.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Realtime detected hand coordinates (center of bounding boxes) are mapped to paddle positions as controls.</figcaption></figure><h2 id="77b7" class="nc lu iq bd lv nd ne dn lz nf ng dp md jy nh ni mh kc nj nk ml kg nl nm mp nn bi translated">后续步骤</h2><p id="b104" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">当前的实现有一些限制——所以贡献，拉请求是最受欢迎的！</p><p id="e653" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">手部检测器改进<br/> </strong>这需要收集额外的训练数据并利用数据增强策略来改进手部检测器。这很重要，因为整个交互(和用户体验)依赖于准确和鲁棒的手部跟踪(误报、漏报会导致糟糕的UX)。</p><p id="1e44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">帧间跟踪</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/0a69cc5c6942eef7a116b12079d3a3ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*IPZZu_ry4N1oL1IKdDT2Ow.gif"/></div></figure><p id="6e67" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当前的实现使用简单的基于欧几里德的度量来跨帧跟踪手(当前帧中的手基于其与先前帧中的手的距离来识别)。如果有几只手重叠，事情会变得复杂——需要一种更鲁棒的跟踪算法。也许集成了OpenCV或其他来源的快速跟踪算法…</p><p id="ddc6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Tensorflowjs实现</strong> <br/>用一个<a class="ae kz" href="https://js.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlowjs </a>实现进行一些实验，它允许整个交互完全在浏览器中原型化！！！Tensorflowjs带来了许多好处——易于部署(无需python或Tensorflow安装),无需websocket服务器和客户端，易于复制，更多潜在用户…</p><blockquote class="nr ns nt"><p id="ef2c" class="jn jo la jp b jq jr js jt ju jv jw jx nu jz ka kb nv kd ke kf nw kh ki kj kk ij bi translated"><strong class="jp ir">更新</strong>..Tensorflow.js浏览器实现如何可用。你可以在<a class="ae kz" href="https://codepen.io/victordibia/full/aGpRZV" rel="noopener ugc nofollow" target="_blank"> CodePen上试试这里的</a>，完整源代码如下:</p></blockquote><div class="lb lc gp gr ld le"><a href="https://github.com/victordibia/handtrack.js/" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">victordibia/handtrack.js</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">一个用于直接在浏览器中原型化实时手部检测(边界框)的库。- victordibia/handtrack.js</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">github.com</p></div></div><div class="ln l"><div class="nx l lp lq lr ln ls kv le"/></div></div></a></div><h1 id="73c0" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">结论</h1><h2 id="0a29" class="nc lu iq bd lv nd ne dn lz nf ng dp md jy nh ni mh kc nj nk ml kg nl nm mp nn bi translated">一些相关的作品。</h2><p id="07e4" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">有一些现有的项目在设计交互时应用了机器学习模型。一个常见的例子是使用LSTMs自动完成文本并创建快速电子邮件回复(参见Google的<a class="ae kz" href="http://www.kdd.org/kdd2016/papers/files/Paper_1069.pdf" rel="noopener ugc nofollow" target="_blank">智能回复</a>论文)，以及最近的实验，其中图像分类模型被用作游戏控制(参见Google Pair的<a class="ae kz" href="https://teachablemachine.withgoogle.com/" rel="noopener ugc nofollow" target="_blank">可教机器</a>和其他<a class="ae kz" href="https://js.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflowjs </a>演示)。这项工作通过探索在创建交互中使用对象检测模型来建立这些趋势。</p><h2 id="599a" class="nc lu iq bd lv nd ne dn lz nf ng dp md jy nh ni mh kc nj nk ml kg nl nm mp nn bi translated">最后的想法</h2><p id="4d88" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">随着人工智能算法的不断成熟(准确性、速度)，有可能利用这些进步来建立更好的交互。这可以是为用户定制内容或预测其预期交互的生成界面、支持基于视觉的新型交互的模型、对话式UI等。在这些领域中，越来越重要的是研究这种交互的机制，严格测试这些交互，并创建设计模式，告知如何在交互设计中使用人工智能模型作为一等公民。</p><p id="29f0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">收到反馈、评论，想要合作吗？随时联系——Twitter，<a class="ae kz" href="https://www.linkedin.com/in/dibiavictor" rel="noopener ugc nofollow" target="_blank"> linkedin </a>。</p><p id="882b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注:Skyfall的早期版本于2018年1月提交给英伟达杰特森挑战赛。</p><h2 id="93e1" class="nc lu iq bd lv nd ne dn lz nf ng dp md jy nh ni mh kc nj nk ml kg nl nm mp nn bi translated">参考</h2><p id="91ab" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">[1]d . m . Shafer，c . p . Carbonara和Popova，L. 2011年。作为基于运动的视频游戏享受的预测因素的空间存在和感知现实。<em class="la">存在:遥操作者和虚拟环境</em><strong class="jp ir">20</strong>(6)591–619。</p><p id="763f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2]m . Birk和r . l . mandrik，2013年。控制你的游戏自我:游戏中控制者类型对快乐、动机和个性的影响。<em class="la">关于计算系统中人的因素的SIGCHI会议记录—CHI ' 13</em>685–694。</p><p id="e2ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[3]黄，j .，拉特霍德，v .，孙，c .，朱，m .，科拉迪卡拉，a .，法蒂，a .，菲舍尔，I .，沃伊纳，z .，宋，y .，s .，墨菲，K. 2017。现代卷积目标探测器的速度/精度权衡<em class="la"> CVPR </em></p><p id="6278" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[4]曹，z .，西蒙，t .，魏，s-e，和谢赫，Y. 2016。使用局部相似场的实时多人2D姿态估计。</p><p id="3ca4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[5]c .哈里森、d .谭和d .莫里斯，2010年。Skinput:将身体作为输入表面。<em class="la">第28届计算机系统中人的因素国际会议论文集—中国’10</em>453。</p></div></div>    
</body>
</html>