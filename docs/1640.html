<html>
<head>
<title>Strongly-Typed Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强类型递归神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/strongly-typed-recurrent-neural-networks-f84772696a86?source=collection_archive---------5-----------------------#2017-09-29">https://towardsdatascience.com/strongly-typed-recurrent-neural-networks-f84772696a86?source=collection_archive---------5-----------------------#2017-09-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3c1d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我对深度学习原理的探索</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/75f5051d9a02b830268cd9868f8d6b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mVc6YEviCV_7sJeuJxkKIQ.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">“Newton” by William Blake.</figcaption></figure><p id="694a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇文章不代表我自己的想法，因为我很少有毫无根据的想法。这是我在寻找深度学习背后的指导原则时，对他人发表的观点进行的分析和思考。在这篇文章中，我提出了对深度学习原理的需求，然后我解释了<a class="ae lr" href="https://arxiv.org/abs/1602.02218" rel="noopener ugc nofollow" target="_blank">强类型递归神经网络的范例。</a>你可以跳过前半部分，但这能激发你展示后半部分的动力。</p><p id="3e37" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设有递归神经网络的基本知识。</p><h2 id="a3dd" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated"><strong class="ak">数学原理简史</strong></h2><p id="bef4" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">在艾萨克·牛顿爵士出版“<a class="ae lr" href="https://en.wikipedia.org/wiki/Philosophi%C3%A6_Naturalis_Principia_Mathematica" rel="noopener ugc nofollow" target="_blank">《哲学自然数学原理》</a>”之前，人们以两种不同的方式对待科学——或者他们称之为自然哲学。大多数科学研究要么基于演绎原理，要么基于归纳原理。定义将有助于:</p><p id="b8ef" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">演绎法</strong>是一种在假设为真理的前提下，从观察中得出结论的方法。<br/>只要前提为真，演绎就是真的，即演绎在我们已知的基础上解释观察结果。这就限制了演绎方法在前提不成立的情况下得出结论。</p><p id="8a84" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">归纳法</strong>是一种通过反复实验和观察来测试新思想和理论的方法。然而，这种方法承认得出的结论可能是错误的。这种方法打开了新思想的研究领域，承认新思想也可能是错误的。</p><p id="21c9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">牛顿——在双方的争论中失败了——采取了一种混合的方法来研究自然现象。他首先通过仔细的实验和观察建立了公理，然后他在已经建立的数学原理的基础上，基于演绎将那些公理形式化。这种对观测数据的逐步概括产生了《数学原理》,它给了我们三个运动定律，奠定了万有引力的原理。然后他用平方反比定律从数学上解释了开普勒的椭圆轨道。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/59d592d16eefee06fd940de5293574a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ncCUcoijX5v3KR5rFZS0A.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">“These forces being unknown, philosophers have hitherto attempted the search of Nature in vain; but I hope the principles here laid down will afford some light either to this or some truer method of natural philosophy” — Newton, Principia Preface.</figcaption></figure><p id="8420" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对《原理》的一个有效批评是，牛顿通过自己的数学模型解释了力的定律，但他无法解释力背后的原因。对此，牛顿回答说，他是想解释万有引力的运作，而不是解释万有引力是由什么产生的。承认自己无知，但有一种新的想法，一种看待世界的新方式。它为美国提供了观察世界的新的数学工具和模型，被许多人认为是已经出版的最伟大的科学著作。正是这些源自归纳和演绎方法的结构化原则推动了科学进步，并最终改变了世界。</p><p id="10c1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们在深度学习中缺乏这些结构化的原则，但这是一个非常新的领域。然而，以目前该领域的进展速度和所做的大量研究来看，寻找深度学习原理还为时不早。</p><p id="2be7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">深度学习指导原则的需求</strong></p><p id="b10c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">形成深度学习主干的想法结构合理。训练过程的工作机制是很好理解的。神经元、损失函数、矩阵运算和我们部署的架构的数学定义非常明确。但是我们无法解释为什么深度神经网络(DNN)工作得如此之好。有一些模糊的想法在流传，但没有确凿的研究来支持证据。随着我们深入架构，我们开始失去每一层学习的抽象概念，因此术语“<strong class="kx ir">黑盒</strong>在深度学习研究的媒体报道中很突出。</p><p id="26e5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，这并没有阻止该领域的创新。<a class="mr ms ep" href="https://medium.com/u/1928cbd0e69c?source=post_page-----f84772696a86--------------------------------" rel="noopener" target="_blank"> Carlos E. Perez </a>在深度学习和炼金术之间进行了类比，在某种意义上，深度学习创新是炮制DNN架构、将它们与不同的超参数混合并应用各种成本函数的结果。他写过一篇大胆的帖子，质疑“<a class="ae lr" href="https://medium.com/intuitionmachine/the-brute-force-method-of-deep-learning-innovation-58b497323ae5" rel="noopener">的必要性，深度学习创新仅仅是因为蛮力吗？</a>”。这表明，大多数研究都是基于演绎的，试图根据我们已知的理论来解释一个模型的工作原理。</p><p id="0332" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">凭借我仅有的一点经验，我开始意识到<strong class="kx ir">我们并不了解通过这些深层神经网络的信息流</strong>。我们不知道当信息通过各种神经网络架构时会发生什么。我们有一个模糊的想法，即隐藏层学习信息的简明表示，但是我们不知道有多少这样的层适合于对信息进行建模以满足我们的需求，并且我们不知道需要哪种层来对该信息进行建模。</p><p id="fddd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">很多次我读到一项新的研究，想知道研究人员是如何设计一个特定的模型的。什么思维过程，什么直觉，最重要的，什么数学？答案几乎总是一样的。他们依靠自己的直觉，尝试不同的架构，选择最有效的架构。这自然产生了疑问，有没有更好更简单的模型？<strong class="kx ir">有没有可能，也许，我们对通过这些模型的信息流还不够了解，因此我们被我们能设计的东西所限制。</strong></p><p id="ccfb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这表明需要一种归纳方法来提出定义信息流数学的新思想，而一篇旨在这样做的论文就是强类型递归神经网络。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h2 id="0075" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated"><strong class="ak">强类型递归神经网络</strong></h2><p id="61ae" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">本文的思想来源于物理学和函数式编程。在物理学中，我们有同质性原理来约束不同类型的两个量之间的运算。例如，你不能把27牛顿的力加到35千克的质量上。</p><p id="12e1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">同样，本文探讨了信息也可能是不同种类的想法，并在流经深层神经网络时改变其类型。作者使用了优美的措辞:</p><blockquote class="na nb nc"><p id="634f" class="kv kw nd kx b ky kz jr la lb lc ju ld ne lf lg lh nf lj lk ll ng ln lo lp lq ij bi translated">“一个范例是物理学家从自然中雕刻出来的测量系统。它规定了表示标准化测量设备读数的单位(例如温度计的开尔文和时钟的秒)以及组合它们的规则。”</p></blockquote><p id="0f40" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我讨论文件的技术细节之前，我想反映一下这里表达的观点。物理学家观察了这个世界，并为热、力、压力等自然现象设计了一套测量系统。，通过归纳推理。深度思维的戴密斯·哈萨比斯认为信息和能量或物质一样是基本的存在。因此，对于研究人员来说，开拓一种新的信息测量系统，开发一种新的数学来理解通过深度神经网络的信息流是很自然的。这篇论文是朝着这个方向迈出的漂亮的一步。</p><p id="e918" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">他们引入<strong class="kx ir">强类型准线性代数</strong>来表示DNNs中的信息流，我用我理解的简单术语解释这些定理。</p><ul class=""><li id="43e5" class="nh ni iq kx b ky kz lb lc le nj li nk lm nl lq nm nn no np bi translated"><strong class="kx ir">定理1:一个类型(T)是一个在‘d’维中具有正交基的<em class="nd">向量空间。</em></strong> <br/>就像二维欧氏空间中的一个点由两个正交基的组合来表示(x坐标，和y坐标；正交意味着x和y坐标是彼此独立的)，一个信息可以在称为“类型”的d维空间中表示。</li><li id="8e46" class="nh ni iq kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated"><strong class="kx ir">定理2:信息可以有多种类型，代表不同的解读。当信息流经深层神经网络的不同部分时，它的类型会发生变化。</strong></li><li id="8227" class="nh ni iq kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated"><strong class="kx ir">定理3:信息可以通过在特定类型中进行一元或二元运算来改变。<br/> *这些操作保留了信息的类型。<br/> *不允许两个不同类型之间的二元运算。</strong> <br/>这类似于通过对特定二维空间中的x和y分量进行一元或二元运算来改变二维空间中的点的坐标。但是，只允许少数操作。<br/> <strong class="kx ir">一元运算:{tanh，relu，sigmoid，标量乘法等。} </strong> <br/> <strong class="kx ir">二元运算:{+，-，max，min} </strong></li></ul><p id="4c64" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我试图通过用一维向量空间进行类比来解释这种类型的范例:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/07119256e2a40dacb91312e90fe79f66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vBWO-S7w7cn6qWd3J9TS5A.jpeg"/></div></div></figure><ul class=""><li id="3082" class="nh ni iq kx b ky kz lb lc le nj li nk lm nl lq nm nn no np bi translated"><strong class="kx ir">定理4: </strong>一个<strong class="kx ir">型变换(T1 → T2) </strong>是通过两个不同型之间的一个<strong class="kx ir">矩阵乘法</strong>来实现的。</li><li id="a53b" class="nh ni iq kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated"><strong class="kx ir">定理5:<strong class="kx ir">类型T1、</strong>和<strong class="kx ir">类型T2 </strong>之间的元素乘法</strong>导致从类型T1到类型T2的<strong class="kx ir">直接转换。<br/> </strong>这与定理4不同，因为T1类型与T2类型的“矩阵乘法”将产生T3类型。但是，类型T1与类型T2的“元素乘法”将产生类型T2。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/65f8e3d4003e556cff24e676153c93d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c2kkRQfR4758ZpC_PiliIg.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Types of the Information changes as it passes through the Deep Neural Network. This is because of various Matrix Multiplications and Elemenwise Multiplications.</figcaption></figure><p id="5338" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是基于归纳的推理。这些操作模式是通过观察rnn的行为而制定的。他们要带着这个去哪里？</p><p id="3645" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些定理将深度神经网络分为两类。<strong class="kx ir">强类型</strong>(类型一致)<strong class="kx ir">弱类型</strong>(类型不一致)。</p><p id="75f7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果将这些约束应用于经典架构，如香草RNNs、LSTMs和GRUs，似乎这些<strong class="kx ir"> <em class="nd">经典网络是</em> </strong> <strong class="kx ir"> <em class="nd">弱类型</em> </strong> <em class="nd">，</em>即它们具有由于添加两种不同类型而产生的类型不一致。让我们以LSTMs为例；更新公式为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/338180e0b5be24208951e3ab7ffae5d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QX93iTei3btiMdNfrdTqPQ.png"/></div></div></figure><p id="ed1d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在文中，一个数学证明表明这种结构是类型不一致的，为了保持简单性，我省略了数学证明。然而我提供了一个直观的解释。考虑上述定理，输出<strong class="kx ir"> h[t]取决于它的前一状态</strong> <strong class="kx ir"> h[t-1]和输入x[t]。</strong>所以:</p><ol class=""><li id="d7b0" class="nh ni iq kx b ky kz lb lc le nj li nk lm nl lq ny nn no np bi translated">如果我们假设h[t-1]是h型的。</li><li id="35ed" class="nh ni iq kx b ky nq lb nr le ns li nt lm nu lq ny nn no np bi translated">假设输入x是x类型。</li><li id="9d3d" class="nh ni iq kx b ky nq lb nr le ns li nt lm nu lq ny nn no np bi translated">假设加权矩阵乘以h[t-1]和x[t]，将它们各自的类型改变为允许它们之间二元运算的单个类型Y。这意味着h[t]将是y类型。</li><li id="d6cc" class="nh ni iq kx b ky nq lb nr le ns li nt lm nu lq ny nn no np bi translated">现在，h[t]和h[t-1]表示跨不同时间步长的相同类型的信息。所以，这意味着Y型和h型是一样的，也就是说，没有类型转换发生。</li><li id="9927" class="nh ni iq kx b ky nq lb nr le ns li nt lm nu lq ny nn no np bi translated">但是，根据定理4，矩阵乘法将导致类型转换，本质上意味着类型Y不能与类型h相同。这推翻了我们在(4)中所做的假设。因此，类型Y不同于类型h，这是不一致的，因为“h”应该在所有状态中保持它的类型。</li></ol><p id="7ee1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">发生这种情况是因为存在递归关系，即当前状态依赖于前一状态。在经典架构中，递归关系也会导致爆炸梯度。因此，受函数式编程的启发，作者介绍了一种设计RNN单元的新范式。这带来了更好、更快的架构，也让我看到了电池设计的前景，我开始觉得它的方程看起来很美。</p><h2 id="4759" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated"><strong class="ak">Learnware和固件的典范</strong></h2><p id="dd84" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">我们不能从RNNs中删除递归关系，因为递归定义了RNNs。但是，我们可以把RNN细胞分成两个区块。一个模块进行学习，另一个模块维护递归关系。术语是<strong class="kx ir"> Learnware(无状态)</strong>和<strong class="kx ir">固件(状态相关)</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/233f640b8fe256d0cf16dd8f33fa8a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ykerjdrKTPhgfb3Ga5cuA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Abstraction showing the difference between RNN and T-RNN. The paradigm of Learnware and Firmware enables parallel operations, thus making computation much faster.</figcaption></figure><p id="399b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个想法很简单，但是很强大。Learnware将具有学习参数，仅取决于输入。并且固件将采用这些学习到的参数并进行依赖于状态的循环操作来保存该存储器。请注意，学习阶段和递归阶段之间的<strong class="kx ir">区别支持学习阶段中的并行性。这使得计算速度更快。</strong></p><p id="9dc0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于没有状态更新返回到learnware中，<strong class="kx ir">在长时间内不会有任何梯度爆炸</strong>，因为学习是在输入上发生的。这也意味着<strong class="kx ir">不需要对输入</strong>应用非线性来抑制它。他们在论文中用数学方法证明了这一点。基于这个范例，他们设计了一个新的LSTM单元块，它是强类型的。他们称之为T-LSTMs:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/3b8a4e9ce2d1b3c114eee23dd7579500.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V0qgW_eOfWyG7_G6K4qtmQ.png"/></div></div></figure><p id="5c0d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">看看这些方程。输入门、遗忘命运和输出门从输入数据中学习。单元状态包含必须传递给下一时间步的信息，输出取决于该单元状态和输出门。文中还有进一步的探索和推论，我就不深究了。当我获得丰富的计算资源时，我会亲自尝试这些想法。</p><p id="7e56" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">总之，以我的理解，强类型RNNs的优点是:</p><ul class=""><li id="3ee3" class="nh ni iq kx b ky kz lb lc le nj li nk lm nl lq nm nn no np bi translated">强类型化信息流似乎比假设所有信息都是同类的更合理。</li><li id="79cf" class="nh ni iq kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated">Learnware和固件之间的区别是一个定义性的设计原则，比经典设计更容易理解。它解决了分解渐变的问题，从而消除了对渐变裁剪的需要。</li><li id="5981" class="nh ni iq kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated">借助Learnware和固件的强大功能，实现并行处理。</li><li id="f981" class="nh ni iq kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated">由于减少了非线性，计算的复杂性已经降低，并且该论文报道强类型lstm比经典lstm快大约1.6倍。</li><li id="efe3" class="nh ni iq kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated">基于元素的状态更新使架构更加稳定。</li><li id="ff36" class="nh ni iq kx b ky nq lb nr le ns li nt lm nu lq nm nn no np bi translated">另一个有趣的发现是，强类型体系结构在各种任务中具有较低的训练错误。</li></ul><p id="d1f3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">强类型指导原则显示出比经典架构稍好的结果，然而，作者说他们没有做足够的实验。虽然后来，<strong class="kx ir"/><a class="ae lr" href="https://einstein.ai/research/new-neural-network-building-block-allows-faster-and-more-accurate-text-understanding" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir">qrns</strong></a><strong class="kx ir">出现了，采用了比经典LSTMs </strong>快16倍的强类型设计的知识。因此，我的结论是，这种限制信息流的想法有些分量，需要进一步探索。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="affa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，我将再次强调深度学习原理的必要性。我个人需要它来尝试不同的架构，但我缺乏资源来进行研究实验室所做的大规模实验。我的直觉告诉我，强类型深度学习不会导致人工智能，而是进一步借鉴深度学习和炼金术之间的类比；炼金术被用来实现一个雄心勃勃的目标，即找到长生不老药——就像深度学习试图解决智力问题一样——但相反，我们发现了化学。天啊，结果还不错。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="4472" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="nd">我感谢</em><a class="ae lr" href="https://www.linkedin.com/in/david-balduzzi-8a964130/?ppe=1" rel="noopener ugc nofollow" target="_blank"><em class="nd">David Balduzzi</em></a><em class="nd">，帮助我理解这篇论文背后的思想。</em></p></div></div>    
</body>
</html>