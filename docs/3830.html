<html>
<head>
<title>Experiments with Compositional Pattern-Producing Networks / Variational / Vanilla Auto Encoders in Tensorflow [ Manual Back Prop with TF ]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Tensorflow 中合成模式生成网络/变分/普通自动编码器的实验</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/experiments-with-compositional-pattern-producing-networks-variational-vanilla-auto-encoders-in-ac2b04c781cd?source=collection_archive---------4-----------------------#2018-06-23">https://towardsdatascience.com/experiments-with-compositional-pattern-producing-networks-variational-vanilla-auto-encoders-in-ac2b04c781cd?source=collection_archive---------4-----------------------#2018-06-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/6056de3ac5384b130e59e97d470fa38b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*rJ_QjINFFqde6n9eVuJdGg.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/design-processing-brOrkeZsS0o6c" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="b00d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我真的很想了解更多关于自动编码器(普通的和变化的)和组合模式生成网络的知识，这就是我写这篇文章的原因。所以我决定做一些实验，比如…..</p><p id="9baa" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="kx">情况 a) </em> <a class="ae jy" href="https://en.wikipedia.org/wiki/Compositional_pattern-producing_network" rel="noopener ugc nofollow" target="_blank"> <em class="kx">合成模式产生网络</em> </a> <em class="kx"> <br/>情况 b)使用自动编码器聚类数据<br/>情况 c)使用变分自动编码器聚类数据<br/>情况 d)使用自动编码器去表情图像</em></p><blockquote class="ky kz la"><p id="6077" class="jz ka kx kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated"><strong class="kb ir">请注意，这篇文章是为我自己做的各种实验的存档结果。所以这篇文章没有任何具体的目标。另外，请注意，我不会在这篇文章中讨论这些架构背后的任何理论。</strong></p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="9b4b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">自动编码器简介</strong></p><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Video from <a class="ae jy" href="https://www.youtube.com/channel/UCNIkB2IeJ-6AmZv7bQ1oBYg" rel="noopener ugc nofollow" target="_blank">Arxiv Insights</a></figcaption></figure><p id="99e4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于任何刚刚开始学习这个主题的人，我在上面链接了一个令人惊叹的 youtube 视频。它很好地描述了什么是自动编码器，以及它可以用在什么地方。</p><p id="4d2a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">此外，如果您想要自动编码器的其他内容，请<a class="ae jy" rel="noopener" target="_blank" href="/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4">单击此处。</a></p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="b849" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">案例一)</strong> <a class="ae jy" href="https://en.wikipedia.org/wiki/Compositional_pattern-producing_network" rel="noopener ugc nofollow" target="_blank"> <strong class="kb ir">组分产网</strong> </a></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/f34aab5070ff36a6ec69c34f3576377b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aQKKxG8gKFJ_k4rJoR8zLA.png"/></div></div></figure><p id="2033" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">黑框</strong> →比例因子输入(我设置为 1.0) <br/> <strong class="kb ir">红框</strong>→X 坐标输入<br/> <strong class="kb ir">蓝框</strong>→Y 坐标输入<br/> <strong class="kb ir">天蓝框</strong> →各坐标半径差输入</p><p id="6c45" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">首先，让我们使用神经网络创建一些艺术品。O <a class="ae jy" href="http://blog.otoro.net/" rel="noopener ugc nofollow" target="_blank"> toro </a>在这里<a class="ae jy" href="http://blog.otoro.net/2016/03/25/generating-abstract-patterns-with-tensorflow/" rel="noopener ugc nofollow" target="_blank">发表了一篇关于 CPPN 是什么的惊人博文</a>，如果你感兴趣，请去阅读。(即使你不是，我也强烈推荐阅读这本书，它真的是一个令人惊叹的解释。).</p><div class="ll lm ln lo gt ab cb"><figure class="lw jr lx ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/f2924a34a1106cc1aac4947ad916df6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*pk8i55SU9y1djha7ZGjS1Q.gif"/></div></figure><figure class="lw jr mc ly lz ma mb paragraph-image"><img src="../Images/929ad699bc67de96c4b8a04464fde46c.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/1*97lRJ4-Mgtu9GfxKCcsSGQ.gif"/><figcaption class="ju jv gj gh gi jw jx bd b be z dk md di me mf">Image created by Neural Network</figcaption></figure></div><p id="a1da" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，当我们将最终输出向量设置为 3 时，我们甚至可以创建彩色图像的颜色序列。(或者甚至是 RGBA，如果你愿意的话。).最后，请注意 CPPN 不一定要有类似于自动编码器的网络架构。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="b2b7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">微型部分——通过对数损失函数进行反向传播</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mg"><img src="../Images/cf6cfc7fba236fcb72fc598a14271f0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*GdpL3V19j-eSzhBfe4kSVw.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from this <a class="ae jy" href="http://wiki.fast.ai/index.php/Log_Loss" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="cd56" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果我们正在处理灰度图像，我们可以很容易地将该问题视为分类任务，因此我们可以使用 log Loss 函数来训练我们的网络。然而，由于我们正在执行手动反向传播，我们需要知道幕后的数学。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/e60e2b1c1fe6c6e448e3c7bb7d99902f.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*ziCa4RcdaGp9yFDKU7XbJw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from this <a class="ae jy" href="https://www.ics.uci.edu/~pjsadows/notes.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="bcec" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在，我将向你展示这个等式是如何折叠的，就像上面看到的那样，但是如果你对数学的细节感兴趣的话。请<a class="ae jy" href="http://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture3_flat.pdf" rel="noopener ugc nofollow" target="_blank">点击这里</a>，这里<a class="ae jy" href="http://arunmallya.github.io/writeups/nn/backprop.html" rel="noopener ugc nofollow" target="_blank">点击</a>或者这里<a class="ae jy" href="https://www.ics.uci.edu/~pjsadows/notes.pdf" rel="noopener ugc nofollow" target="_blank">点击</a>。另外，如果你对最大对数似然和交叉熵之间的差异感兴趣<a class="ae jy" href="https://www.quora.com/What-are-the-differences-between-maximum-likelihood-and-cross-entropy-as-a-loss-function" rel="noopener ugc nofollow" target="_blank">，点击这里</a>。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="6661" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">情况 b)使用自动编码器对数据进行聚类</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/ad76f83ca4a9ea2abad241633000c988.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*oCODlEEbfVc-ZlWhEcn0Og.png"/></div></figure><p id="b930" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">蓝色矩形</strong> →我们网络的编码器部分<br/> <strong class="kb ir">红色矩形</strong> →我们网络的解码器部分</p><p id="2c4b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">让我们首先做最简单的任务，我们将聚类 MNIST 数据集到一个三维潜在空间。看看我们能达到什么样的效果。首先让我们使用<a class="ae jy" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">最大似然估计</a>作为我们的成本函数。</p><div class="ll lm ln lo gt ab cb"><figure class="lw jr mj ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/3b4fb0fb290d617b9af67d69bf5debb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/1*QHzSupEJCEq-zcBCQZ0Ysw.gif"/></div></figure><figure class="lw jr mk ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/e383984515de8b6a22e2e163567f5230.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/1*1Yp0xsRrwd51977DHlYGhg.gif"/></div></figure></div><p id="154e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图 Gif </strong> →为 1000 个数据点创造潜在空间<br/>T22】右图 Gif  →随时间重建图像</p><p id="f008" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，当我们观察 1000 个数据点的潜在空间时，我们可以观察到具有相似属性(如曲线或边缘)的每个数字都聚集在一起。然而，这里需要注意的是，我们可以观察到范围相当大，仅 Y 轴的范围就从 0 到-70，这意味着数据更加分散。(如果我们想要生成新数据，这可能是一个问题。)</p><p id="e333" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">接下来让我们使用作为<a class="ae jy" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">均方误差</a>成本函数。</p><div class="ll lm ln lo gt ab cb"><figure class="lw jr ml ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/5a68a300c6650221a638fd245b394052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/1*t5NNlCP2GMBYD16MtbysMQ.gif"/></div></figure><figure class="lw jr mm ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/4920f1fb6068e38968f3243cc61f0302.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/1*l9bP1C3bMgUVWh5qaHsIzw.gif"/></div></figure></div><p id="76c7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左 Gif </strong> →为 1000 个数据点创建潜在空间<br/> <strong class="kb ir">右 Gif </strong> →随时间重建图像</p><p id="6f50" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">即使我们使用均方误差作为我们的损失函数，网络在聚类数据点方面做得还不错，但是，再次注意每个轴的范围相当大。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="fdbe" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">情况 c)使用变分自动编码器聚类数据</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mn"><img src="../Images/ac325e4ca4cc5e7c24231e0ca470b8ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C9dv2ETgGyprWesBhhvTzw.png"/></div></div></figure><p id="9132" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">蓝色矩形</strong> →我们网络的编码器部分<br/> <strong class="kb ir">红色矩形</strong> →我们网络的解码器部分</p><p id="9f41" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在，让我们使用变型自动编码器对我们的数据进行聚类，关于什么是变型自动编码器的详细解释，请点击<a class="ae jy" href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/" rel="noopener ugc nofollow" target="_blank">此处</a>、<a class="ae jy" href="https://jmetzen.github.io/2015-11-27/vae.html" rel="noopener ugc nofollow" target="_blank">此处</a>或<a class="ae jy" href="http://kvfrans.com/variational-autoencoders-explained/" rel="noopener ugc nofollow" target="_blank">此处</a>。首先，让我们再次使用<a class="ae jy" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">最大似然估计</a>作为我们的损失函数。</p><div class="ll lm ln lo gt ab cb"><figure class="lw jr ml ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/dc2da32a9149967a176fab5f6cd9cb0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/1*15PCdn2if4aKUlIEeUHrnQ.gif"/></div></figure><figure class="lw jr mm ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/75d696eb9279cbff2f49ebe0c6292f3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/1*zH2JoxMQL1Ry8AYKYfxUMQ.gif"/></div></figure></div><p id="598b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左侧 Gif </strong> →为 1000 个数据点创建潜在空间<br/> <strong class="kb ir">右侧 Gif </strong> →随时间重建图像</p><p id="1056" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，网络在聚集每个数据点方面做得很好。然而，一个不同于普通自动编码器的是轴的范围。这是变型自动编码器与普通自动编码器相比的一个特性。再次理论解释请阅读链接。接下来，让我们看看如果使用均方误差函数会发生什么。</p><div class="ll lm ln lo gt ab cb"><figure class="lw jr ml ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/6503aa798356aee757a41b050afe7d3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/1*t_1afKepfxYV5cjifnAG1w.gif"/></div></figure><figure class="lw jr mm ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/e2596b964f2cfd489d326e27c1e3730c.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/1*AlZ70xa-sSo4-iTzL-sCHQ.gif"/></div></figure></div><p id="1207" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">蓝色矩形</strong> →我们网络的编码器部分<br/> <strong class="kb ir">红色矩形</strong> →我们网络的解码器部分</p><p id="85c2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">即使使用 MSE，我们也可以观察到集群看起来很棒，轴的范围不像普通的自动编码器那样大。最后，我想看看我们是否使用了 Kullback-Leibler 散度损失函数。</p><div class="ll lm ln lo gt ab cb"><figure class="lw jr ml ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/b9164f3709adc16909577c24dbdd4403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/1*Tun0LQoAzCa6vdk9QSIkMw.gif"/></div></figure><figure class="lw jr mm ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/999c10cb28942beb0cffc435d27a29a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/1*vxDgHtuoS9sAAxjLiVV7Og.gif"/></div></figure></div><p id="9eb2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左侧 Gif </strong> →为 1000 个数据点创建潜在空间<br/> <strong class="kb ir">右侧 Gif </strong> →随时间重建图像</p><p id="7ad4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">因为我们不再有任何重建损失，所以网络没有任何动机来创建看起来像原始图像的图像。然而，观察一些点如何映射到更高的 z 轴是非常有趣的。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="2bbb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">案例 d)使用自动编码器对图像进行去表情处理</strong></p><div class="ll lm ln lo gt ab cb"><figure class="lw jr mo ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/35635e5b0e29a5b68495352b71b676b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*kqqM499GyR6t1S7Jf1hidg.png"/></div></figure><figure class="lw jr mo ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/0fbe553cd5f9ff370ed70de5d724e74e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*vnghw-2GYEe6sWKFBiCquA.png"/></div></figure><figure class="lw jr mo ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/6a945940cfeabd3f76587e9693c95eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*BdzDSPS-_woM_3V4rXqo0Q.png"/></div></figure></div><p id="f8b6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左侧图像</strong> →添加表情符号的图像<br/> <strong class="kb ir">中间图像</strong> →无表情符号的图像<br/> <strong class="kb ir">右侧图像</strong> →自动编码器去除表情符号的图像</p><p id="fa9f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这是一项非常简单的任务，因为去噪自动编码器是这类网络的标准用例。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/c532b7d56aaad54337fd466a2d1b22ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/1*SkxP18Qbsdx9ugHTfwoOnA.gif"/></div></figure><p id="1b98" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">图像顺序</strong> →表情图像、原始图像、去表情图像</p><p id="a7f4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当我们看到自动编码器的进展时，我们可以观察到，随着网络的训练，它在消除图像中出现的表情符号方面做得更好。然而，我们可以清楚地看到一个问题，网络不知道如何填充表情符号占据的空间。现在我们可以让它保持原样，但将来我会计划解决这个问题。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="146f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">交互代码</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mq"><img src="../Images/75c77a743b66f638f56e24f291004937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yn8zGZ0C1FfAPI8tgrxKWw.png"/></div></div></figure><p id="c450" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="kx">对于谷歌实验室，你需要一个谷歌帐户来查看代码，你也不能在谷歌实验室运行只读脚本，所以在你的操场上做一个副本。最后，我永远不会请求允许访问你在 Google Drive 上的文件，仅供参考。编码快乐！同样为了透明，我在 github 上上传了所有的训练日志。</em></p><p id="114a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1bYUSi2sGqfDsb0sxhoBvrvAWKnyNf0Aw" rel="noopener ugc nofollow" target="_blank"> a 的代码，请点击此处</a>。<br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1-L2pUcy2U79iNPckZgYeBipK0w0fmLEC" rel="noopener ugc nofollow" target="_blank"> b 的代码，请点击此处</a>，要查看<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/var_auto/caseb_auto_log_loss/caseb.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处。</a> <br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/13BiyqG0FfFm-BDsUnQoCjqTYFphe4Gep" rel="noopener ugc nofollow" target="_blank"> c 的代码，请点击此处，<a class="ae jy" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/var_auto/casec_var_log/casec.txt" rel="noopener ugc nofollow" target="_blank">日志的</a>请点击此处。</a> <br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1HwSiJ55uej-xJH79VCi_RMEHHhxtmqNQ" rel="noopener ugc nofollow" target="_blank"> d 的代码，请点击此处</a>。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="9bba" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="2656" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">很抱歉没有解释每个网络背后的理论，但这篇文章的目的实际上是为了实现我的一些实验。</p><p id="f71e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你希望看到我所有写作的列表，请<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">在这里查看我的网站</a>。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的 twitter 上关注我<a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文</a> t。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="609a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="5410" class="mr ms iq kb b kc kd kg kh kk mt ko mu ks mv kw mw mx my mz bi translated">导数表。(2018).Math2.org。检索于 2018 年 6 月 23 日，来自<a class="ae jy" href="http://math2.org/math/derivatives/tableof.htm" rel="noopener ugc nofollow" target="_blank">http://math2.org/math/derivatives/tableof.htm</a></li><li id="5d15" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">tf.atan | TensorFlow。(2018).张量流。检索于 2018 年 6 月 23 日，来自<a class="ae jy" href="https://www.tensorflow.org/api_docs/python/tf/atan" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/atan</a></li><li id="ffbd" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">导数:普通神经网络激活函数的导数。(2014).聪明的机器。检索于 2018 年 6 月 23 日，来自<a class="ae jy" href="https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/" rel="noopener ugc nofollow" target="_blank">https://theclevermachine . WordPress . com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/</a></li><li id="02d5" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">导数表。(2018).Math.com。检索于 2018 年 6 月 23 日，来自<a class="ae jy" href="http://www.math.com/tables/derivatives/tableof.htm" rel="noopener ugc nofollow" target="_blank">http://www.math.com/tables/derivatives/tableof.htm</a></li><li id="f303" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">可变自动编码器。(2018).YouTube。检索于 2018 年 6 月 23 日，来自<a class="ae jy" href="https://www.youtube.com/watch?v=9zKuYvjFFS8" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=9zKuYvjFFS8</a></li><li id="7759" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">matplotlib，R. (2018 年)。在 matplotlib 中删除保存的图像周围的空白。堆栈溢出。检索于 2018 年 6 月 23 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/11837979/removing-white-space-around-a-saved-image-in-matplotlib" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/11837979/remove-white-space-around-a-saved-image-in-matplotlib</a></li><li id="0eea" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">[复本]，H. (2018)。如何在 Matplotlib (python)中隐藏轴和网格线？堆栈溢出。检索于 2018 年 6 月 23 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/45148704/how-to-hide-axes-and-gridlines-in-matplotlib-python" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/45148704/how-to-hide-axes-and-gridlines-in-matplotlib-python</a></li><li id="3f67" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">合成模式生成网络。(2018).En.wikipedia.org。检索于 2018 年 6 月 23 日，来自<a class="ae jy" href="https://en.wikipedia.org/wiki/Compositional_pattern-producing_network" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/composition _ pattern-producing _ network</a></li><li id="4699" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi">大トロ. (2018). Blog.otoro.net. Retrieved 23 June 2018, from <a class="ae jy" href="http://blog.otoro.net/" rel="noopener ugc nofollow" target="_blank">http://blog.otoro.net/</a></li><li id="495d" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">RGBA 颜色空间。(2018).En.wikipedia.org。检索于 2018 年 6 月 23 日，来自 https://en.wikipedia.org/wiki/RGBA_color_space<a class="ae jy" href="https://en.wikipedia.org/wiki/RGBA_color_space" rel="noopener ugc nofollow" target="_blank"/></li><li id="0f97" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">最大似然估计。(2018).En.wikipedia.org。检索于 2018 年 6 月 23 日，来自 https://en.wikipedia.org/wiki/Maximum_likelihood_estimation<a class="ae jy" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank"/></li><li id="18f7" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">均方差。(2018).En.wikipedia.org。检索于 2018 年 6 月 23 日，来自<a class="ae jy" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Mean_squared_error</a></li><li id="6aad" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">Mallya，A. (2018)。反向投影。arun mallya . github . io . 2018 年 6 月 23 日检索，来自<a class="ae jy" href="http://arunmallya.github.io/writeups/nn/backprop.html" rel="noopener ugc nofollow" target="_blank">http://arunmallya.github.io/writeups/nn/backprop.html</a></li><li id="c550" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">(2018).Ttic.uchicago.edu。检索于 2018 年 6 月 23 日，来自<a class="ae jy" href="http://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture3_flat.pdf" rel="noopener ugc nofollow" target="_blank">http://ttic . uchicago . edu/~ shubhendu/Pages/Files/lecture 3 _ flat . pdf</a></li><li id="fcf2" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">(2018).Ics.uci.edu。检索于 2018 年 6 月 23 日，来自<a class="ae jy" href="https://www.ics.uci.edu/~pjsadows/notes.pdf" rel="noopener ugc nofollow" target="_blank">https://www.ics.uci.edu/~pjsadows/notes.pdf</a></li><li id="09c0" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">(2018).[在线]可从以下网址获取:<a class="ae jy" href="https://www.quora.com/What-are-the-differences-between-maximum-likelihood-and-cross-entropy-as-a-loss-function" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/What-is-the-differences-between-maximum-likelihood-and-cross-entropy-as-a-loss-function</a>【2018 年 6 月 23 日获取】。</li><li id="7d78" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">带图例的散点图— Matplotlib 2.2.2 文档。(2018).Matplotlib.org。检索于 2018 年 6 月 23 日，来自<a class="ae jy" href="https://matplotlib.org/gallery/lines_bars_and_markers/scatter_with_legend.html" rel="noopener ugc nofollow" target="_blank">https://matplotlib . org/gallery/lines _ bars _ and _ markers/scatter _ with _ legend . html</a></li><li id="2c62" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">颜色示例代码:colormaps _ reference . py—Matplotlib 2 . 0 . 2 文档。(2018).Matplotlib.org。检索于 2018 年 6 月 23 日，来自<a class="ae jy" href="https://matplotlib.org/examples/color/colormaps_reference.html" rel="noopener ugc nofollow" target="_blank">https://matplotlib . org/examples/color/colormaps _ reference . html</a></li><li id="910e" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">对抗性自动编码器的向导指南:第 1 部分，自动编码器？。(2017).走向数据科学。检索于 2018 年 6 月 23 日，来自<a class="ae jy" rel="noopener" target="_blank" href="/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4">https://towards data science . com/a-wizards-guide-to-adversarial-auto encoders-part-1-auto encoder-d9a5f 8795 af 4</a></li><li id="0f98" class="mr ms iq kb b kc na kg nb kk nc ko nd ks ne kw mw mx my mz bi translated">教程-什么是可变自动编码器？—贾恩·阿尔托萨尔。(2018).贾恩·阿尔托萨尔。检索于 2018 年 6 月 23 日，来自<a class="ae jy" href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/" rel="noopener ugc nofollow" target="_blank">https://jaan . io/what-is-variable-auto encoder-vae-tutorial/</a></li></ol></div></div>    
</body>
</html>