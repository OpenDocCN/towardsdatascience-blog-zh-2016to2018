<html>
<head>
<title>[ Back to Basics ] Deriving Back Propagation on simple RNN/LSTM (feat. Aidan Gomez)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在简单的 RNN/LSTM 上推导反向传播(专长。艾丹·戈麦斯)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/back-to-basics-deriving-back-propagation-on-simple-rnn-lstm-feat-aidan-gomez-c7f286ba973d?source=collection_archive---------0-----------------------#2018-05-02">https://towardsdatascience.com/back-to-basics-deriving-back-propagation-on-simple-rnn-lstm-feat-aidan-gomez-c7f286ba973d?source=collection_archive---------0-----------------------#2018-05-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/459ad66121e885f1d143f9c2ed186dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*nEYLoFa9hXsqC1Ftw-2rjA.gif"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Gif from this <a class="ae kc" href="https://giphy.com/gifs/cbc-comedy-what-3o7btPCcdNniyf0ArS" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="fb10" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Aidan Gomez 做了一项惊人的工作，详细解释了反向传播 LSTM 一般是如何工作的。我也做了我的<a class="ae kc" href="https://medium.com/@SeoJaeDuk/only-numpy-deriving-forward-feed-and-back-propagation-in-long-short-term-memory-lstm-part-1-4ee82c14a652" rel="noopener">这篇文章</a>。然而，我的朋友<a class="ae kc" href="https://medium.com/@abraham.kang_80395" rel="noopener">阿部康</a>对我的帖子有一个问题。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lb"><img src="../Images/95c8152d58e908d42503b643cac4b109.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VaxqCofHQafDhRLKPxAmSg.png"/></div></div></figure><p id="9da7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以今天，我想回顾一下反向传播的一些基本概念，同时将我的帖子和 Aidan 的帖子联系起来。</p><p id="16eb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们学校的所有实验室都在翻新，因此我没有白板可以写，所以我会尽我所能在笔记本上写得尽可能整洁。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="3af4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">衍生品的快速重述</strong></p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ln"><img src="../Images/5701fd95e804f49346888efd67775010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c3ggKNJZYW4C-jubez-vcw.jpeg"/></div></div></figure><p id="d11a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，让我们在继续之前练习一些导数。看着上面的图片，如果你觉得一切都有意义，请继续，否则我会建议你通过阅读<a class="ae kc" href="https://magoosh.com/hs/ap-calculus/2017/calculus-review-derivative-rules/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>来复习更多关于衍生品的知识。</p><p id="5dc2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另外请注意，f(x) = tanh(x)的导数是 1-tanh(x ),可以改写为 1-f(x)。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="2513" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">链式法则的快速重述</strong></p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/b756a3f7421e5856e3f13aed720dd850.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fCr97rr1oslHiIno.jpg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Image from this <a class="ae kc" href="http://slideplayer.com/slide/10776187/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="9385" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以链式法则也非常简单，上面我们可以看到，我们想要对函数 y 的 x 求导，但是，函数 y 本身不包含变量 x，所以我们不能直接求导。谢天谢地，函数 y 包含变量 u，u 包含 x，所以我们最终能够对函数 y 的 x 求导，这要感谢链式法则。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="76a8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">多变量衍生工具的快速重述</strong></p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ln"><img src="../Images/aa8f813eb1774468f4fc21cffd722f00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hkUGCG4cZG2XzvNr6kSVsA.jpeg"/></div></div></figure><p id="e2b1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">蓝色</strong> →对变量 x 的导数<br/> <strong class="kf ir">红色</strong> →对变量 Out 的导数</p><p id="481e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们回顾一下多变量的导数，它只是对每一项单独求导。此外，现在请忽略变量的名称(如 x 或 out ),它没有重要的意义。让我们看另一个例子。</p><div class="lc ld le lf gt ab cb"><figure class="lo jr lp lq lr ls lt paragraph-image"><img src="../Images/ecf4ae5c6ba017642934f2a4d6e38bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*21QxZrs_W_vudOBkZbXPog.png"/></figure><figure class="lo jr lu lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/457bbe94fcec237456e282aa0ecfdba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/0*u7aAK5FWrZtt86er."/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk lv di lw lx">Left Image from this <a class="ae kc" href="http://www.columbia.edu/itc/sipa/math/calc_rules_multivar.html" rel="noopener ugc nofollow" target="_blank">website</a>, Right Image from this <a class="ae kc" href="https://www.mathbootcamps.com/derivative-natural-log-lnx/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure></div><p id="d2d5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为 ln(x)的导数是 1/x，如果我们对每个方程分别对 x 和 y 取偏导数，我们会得到类似上面的结果。最后，让我们看一个在<a class="ae kc" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank">逻辑 sigmoid 函数</a>中的多变量函数的例子。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ln"><img src="../Images/6bd029f9d115eef798010364c2608689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9NYO6L6_PwTT6eXzQd_vqg.jpeg"/></div></div></figure><p id="9cc3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">蓝色</strong> →对变量 x 的导数<br/>T3】红色 →对变量 Out 的导数</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="5955" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">递归神经网络的基本构建模块</strong></p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/bbc707eba432b26f77f1ddb6e95db634.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*TZ-eaZxSpF1UMRB2vw5yaw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Modified Image from this <a class="ae kc" href="https://www.techleer.com/articles/185-backpropagation-through-time-recurrent-neural-network-training-technique/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="946c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以上图告诉了我们简单递归神经网络的基本构建模块，具有<strong class="kf ir">四个状态</strong>(用符号<strong class="kf ir"> s </strong>表示)、<strong class="kf ir">三个输入</strong>(用符号<strong class="kf ir"> x </strong>表示)和一个<strong class="kf ir">误差</strong>(用符号<strong class="kf ir"> E </strong>表示)。现在让我们把它放进数学方程，但是没有任何激活函数。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/6fd4b07666a2a802fa5bc8cea0631830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hom4ih5I1O9jEtfuKUeyKw.png"/></div></div></figure><p id="3d54" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">绿框</strong> →相对于状态 3 的误差导数<br/> <strong class="kf ir">蓝框</strong> →相对于状态 2 的误差导数<br/> <strong class="kf ir">红框</strong> →相对于状态 1 的误差导数</p><p id="7eee" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们看看每个状态的导数，很简单，对吧？使用链式法则以及多变量导数，我们能够对每个状态取导数，以相应地更新权重(U)。(请注意，我没有对状态 0 求导，我们使用的是 L2 损失函数。).现在让我们通过对输入求导来完成这个任务。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ln"><img src="../Images/734b4d1d78f732522748460a1d1b6e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HOa5GczIL1i4BflqRWDORw.png"/></div></div></figure><p id="4ec9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">绿框</strong> →相对于输入 3 的误差导数<br/> <strong class="kf ir">蓝框</strong> →相对于输入 2 的误差导数<br/> <strong class="kf ir">红框</strong> →相对于输入 1 的误差导数</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="9a22" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">LSTM 的前馈操作</strong></p><div class="lc ld le lf gt ab cb"><figure class="lo jr lz lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/e1fd6d00629d28e368e89fbf08c09ed0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*cQKCGU586QiTsifkiAsUKg.jpeg"/></div></figure><figure class="lo jr ma lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/006ad68b4cf5fbcfc323cfcea37e35a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*-sHYgpOW0Hs82C6brzZPZA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk mb di mc lx">Right Image from this <a class="ae kc" href="https://medium.com/@aidangomez/let-s-do-this-f9b699de31d9" rel="noopener">website</a></figcaption></figure></div><p id="0f31" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">前馈操作看起来非常复杂，但是，当我们做实际的数学运算时，它非常简单。左图是 LSTM 的图形表示，右图是来自<a class="ae kc" href="https://medium.com/@aidangomez?source=post_header_lockup" rel="noopener"> Aidan Gomez </a>的数学表示。现在让我们写下状态 1 和状态 2 的数学表达式(请注意，我在这篇文章中互换使用了状态和时间戳这两个术语)。我将使用艾登的符号，因为它会更容易理解。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ln"><img src="../Images/d14a5fda1b8ed7a7c270728fa9dc9981.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BMbNWZCn-A3DQtu0AdQZlg.jpeg"/></div></div></figure><p id="80b8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请再次注意，我们使用的是 L2 成本函数。现在让我们来看一个数字例子，下面两张图片来自<a class="ae kc" href="https://medium.com/@aidangomez?source=post_header_lockup" rel="noopener"> Aidan </a>的博客文章。(请注意，在我的笔记中，为了简单起见，我没有写偏见术语。)</p><p id="df52" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Aidan 也使用了状态 0 和 1，而我使用了状态 1 和 2。</p><div class="lc ld le lf gt ab cb"><figure class="lo jr md lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/360d3e0f4941527a732d7ef3e5f80b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*XVfn5t7GsHoYwWpgDrgU3w.png"/></div></figure><figure class="lo jr me lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/573cf4dcd4e77b605384d0294bf19758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*sVYKTqfp9JUf9AYF7hSnLw.png"/></div></figure></div></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="c4d8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">时间戳 2 处的反向传播</strong></p><div class="lc ld le lf gt ab cb"><figure class="lo jr mf lq lr ls lt paragraph-image"><img src="../Images/f220ec1950daf702dc6b22181414cefb.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*jDbl9u92scNI0u6og3L8SA.png"/></figure><figure class="lo jr mg lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/ae76cacd8725fdeb9a12ef62ce51f54e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*ViBxGgTbzjUul_b53GmbcA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk mh di mi lx">Left Image Back Propagation equations by Aidan</figcaption></figure></div><p id="3ebd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里有两点需要注意…</p><p id="ef27" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1.绿线<strong class="kf ir"> </strong> →请记住 Tanh()的导数可以这样改写。如果您不记得原因，请向上滚动。</p><p id="8786" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.我没有对 i(2)和 f(2)求导。原因是因为那两项与 a(2)具有非常相似的导数结构。我将在下面详细解释原因。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/33ac0891c63ace47c7afbf2272eb2450.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*-cVVSoptfhGlddQu1qOLDw.png"/></div></figure><p id="77f6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们观察 state(t)是如何计算的，我们可以看到涉及到 a(t)，i(t)，f(t)和 state(t-1)项。所以当我们对变量 a(t)求导时，我们可以知道，这和对 i(t)求导非常相似。然而，有一个术语我们需要更深入地研究，那就是 o(t)。因为这个术语是在我们计算状态(t)之后使用的，导数也是不同的。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mk"><img src="../Images/384309c9cdda9e35c86b1bfa2094a13c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iV3VHIb0HO0Ywe3QMbBJkQ.png"/></div></div></figure><p id="b159" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">橙色框</strong> →关于 o(2)的导数</p><p id="97e8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以清楚地看到，导数方程与 a(2)相比有一些差别。</p><p id="d0a6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">记住这些想法，我们可以看到，在时间戳 2 推导反向传播并不困难，因为它是 LSTM 的最外层。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="e675" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">时间戳 1 处的反向传播</strong></p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/a878eb328f9678c6db426a0e77d7e692.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eNGQJw0VMnUwSi6YMpPB3A.png"/></div></div></figure><p id="957f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">绿框</strong> →时间戳 1 <br/> <strong class="kf ir">处误差函数的直接导数部分蓝框</strong> →时间戳 2 <br/> <strong class="kf ir">处的导数部分红框</strong> →将符号汇总为β</p><p id="0358" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上图是时间戳为 1 时的反向传播操作。现在，我再次使用艾登的符号，然而有一部分，我的朋友已经指出。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mm"><img src="../Images/35b2864106a74a2d3eb0bd285251ce13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cXgOIGndiQM1RdoMmt7x8g.png"/></div></div></figure><p id="4219" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">蓝框</strong> →计算 t 点的导数时，为什么需要 t+1 的一项？</p><p id="1676" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在第一个网站上，它肯定看起来令人困惑，所以让我们更深入地看看前馈操作。</p><div class="lc ld le lf gt ab cb"><figure class="lo jr mn lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/edc98b770ad14e7db38268fde44be780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*nirB4TUKBVmOqIXIlhzf8g.png"/></div></figure><figure class="lo jr mo lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/258c40fceb3d6ab934bfdcacff351fc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*g2-zk4X0PcKe7t-BSTQzlQ.png"/></div></figure></div><p id="3507" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">蓝框</strong> →当时间戳为 1(左图)和时间戳为 2(右图)时，显示状态 1</p><p id="4cce" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，我会交替使用“时间戳”和“状态”这两个术语。不管怎样，我们可以观察到，当对状态 1 求导时。我们需要两个方程。</p><ol class=""><li id="5444" class="mp mq iq kf b kg kh kk kl ko mr ks ms kw mt la mu mv mw mx bi translated">首先，当时间戳为 1(绿框)时，我们需要渐变</li><li id="c40c" class="mp mq iq kf b kg my kk mz ko na ks nb kw nc la mu mv mw mx bi translated">第二，我们需要时间戳为 2 时的梯度。(蓝色方框)</li></ol><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nd"><img src="../Images/e1fc755788f1cc4c78a800a5e475b45e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*emHeRyuYtLWQfbfDnY-rOQ.png"/></div></div></figure><p id="1171" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们回顾一下蓝框术语的来源。(关于状态(2)的导数)</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/63f40d0c4d2e7e40029623b16fe79bea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Id_8qS1G_K8XRe4drpOI4A.png"/></div></div></figure><p id="704a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以在上面的导数之后，我们还需要加上 f(2)项。其原因可以解释如下。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nf"><img src="../Images/c863d0bab6f5cc96ac2cd72d808cd960.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w7L5ikvRbgZ1Q7VaTBnyhQ.png"/></div></div></figure><p id="09e9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请记住，在前馈操作期间，当时间戳为 2 时，我们将状态(1)乘以 f(2)。因此，如果我们对状态(1)求导，我们也需要乘以该项。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="aee2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">时间戳 1 时相对于 O 的反向传播</strong></p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/183620d17f4d9dbcea1a6856e96da4e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8peZRIMIs4550yGsMd2f4Q.png"/></div></div></figure><p id="8a3f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">橙色框</strong> →相对于 O 项的导数</p><p id="4b4d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们考虑关于 o 项的反向传播过程，与我们在上一节中所做的反向传播相比，有一个有趣的事实需要我们注意。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/bf16d7e247e53e82a3394c211c323db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fn89Mwlfnq70QdwTZLl6GA.png"/></div></div></figure><p id="78c3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">橙盒</strong> →艾丹反向传播方程</p><p id="e654" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">没有 t+1 的项。对状态(t)求梯度时，这是一个明显的区别。换句话说…..</p><p id="8dc7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">a)在对状态(t)取梯度时→我们需要考虑来自未来时间戳状态(t+1)的梯度</p><p id="aab0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">b)在对 o(t)取梯度时→我们不需要考虑来自未来时间戳状态(t+1)的梯度</p><p id="504e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当考虑时间标记 1 的反向传播时，可以再次看到这一点。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/542447febd7e827723de91be4e0969c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u6qc71uaTDL0g09oXydygg.png"/></div></div></figure><p id="d8c7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">蓝框</strong> →其中状态(2)(换言之状态(t+1)存在<br/> <strong class="kf ir">黄框</strong> →受状态(t+1)影响的导数<br/> <strong class="kf ir">黑框</strong> →不受状态(t+1)影响的导数</p><p id="453a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了进一步证实这个想法，让我们看一下我以前的博客帖子，在那里我更详细地介绍了反向传播。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ln"><img src="../Images/61600a5c6cfa79b65680297deac7fd7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TeEQIUO8c9QCSp0Ol6LBtA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Image from this <a class="ae kc" href="https://becominghuman.ai/only-numpy-deriving-forward-feed-and-back-propagation-in-long-short-term-memory-lstm-part-1-4ee82c14a652" rel="noopener ugc nofollow" target="_blank">post</a></figcaption></figure><p id="70bc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">蓝框</strong> →从时间戳 1 <br/> <strong class="kf ir">的成本函数对 o(用 W(o)表示)求导；绿框</strong> →从时间戳 2 的成本函数对 o(用 W(o)表示)求导</p><p id="337e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">到目前为止，我们可以理解导数，但是现在让我们看看数学。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/6dffc0073511ebf26dccfde76a5cbd0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pQ3MGIi_0M7s5OPdjzXxbA.png"/></div></div></figure><p id="3fa4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请暂时忽略绿色/红色框。好的，这是我们得到的数学公式，但是现在让我们比较一下，我们从之前的帖子到现在的帖子得到的公式。为了便于比较，让我们扩展等式，如下所示。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/aa070ee9b64132fca673d891768acb0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f1MeMseglPfJFKM4quc0cw.png"/></div></div></figure><p id="6d05" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">红框</strong> →公式展开的重复项<br/> <strong class="kf ir">绿框</strong> →时间戳 2 的导数</p><p id="4065" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在把所有的东西放在一起，我们从上面看到的所有的绿盒子元素，进入<strong class="kf ir">黑星</strong>变量。所有的红框元素都完全一样。(除了输入的 x 符号)。</p><p id="4b8f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们可以确认的是，当对 o 项求导时(见下文)，我们不需要考虑状态(t+1)的导数。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/b4dcd3110a4b7c3f21e5c959689b1b07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*tzXeBVck0odqkvmBrXU_vQ.png"/></div></figure></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="9d6f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">最后的话</strong></p><p id="ae0a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">希望这篇文章可以澄清一些困惑，但是我知道我的英文解释不是最好的。所以如果你有任何问题，请在下面评论。</p><p id="f7a0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有写作的列表，请在这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与此同时，在我的 twitter 上关注我<a class="ae kc" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae kc" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae kc" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="316e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">参考</strong></p><ol class=""><li id="00bb" class="mp mq iq kf b kg kh kk kl ko mr ks ms kw mt la mu mv mw mx bi translated">反向传播一个 LSTM:一个数值例子。(2016).中等。检索于 2018 年 5 月 1 日，来自<a class="ae kc" href="https://medium.com/@aidangomez/let-s-do-this-f9b699de31d9" rel="noopener">https://medium.com/@aidangomez/let-s-do-this-f9b699de31d9</a></li><li id="a2fb" class="mp mq iq kf b kg my kk mz ko na ks nb kw nc la mu mv mw mx bi translated">微积分评论:衍生规则——Magoosh 高中博客。(2017).Magoosh 高中博客。2018 年 5 月 1 日检索，来自<a class="ae kc" href="https://magoosh.com/hs/ap-calculus/2017/calculus-review-derivative-rules/" rel="noopener ugc nofollow" target="_blank">https://magoosh . com/hs/AP-calculus/2017/calculus-review-derivative-rules/</a></li><li id="0254" class="mp mq iq kf b kg my kk mz ko na ks nb kw nc la mu mv mw mx bi translated">微积分的规则-多元。(2018).Columbia.edu。检索于 2018 年 5 月 1 日，来自<a class="ae kc" href="http://www.columbia.edu/itc/sipa/math/calc_rules_multivar.html" rel="noopener ugc nofollow" target="_blank">http://www . Columbia . edu/ITC/SIPA/math/calc _ rules _ multivar . html</a></li><li id="0b0e" class="mp mq iq kf b kg my kk mz ko na ks nb kw nc la mu mv mw mx bi translated">lnx 的衍生和例子—mathbootpcamps。(2016).数学夏令营。检索于 2018 年 5 月 1 日，来自<a class="ae kc" href="https://www.mathbootcamps.com/derivative-natural-log-lnx/" rel="noopener ugc nofollow" target="_blank">https://www.mathbootcamps.com/derivative-natural-log-lnx/</a></li><li id="b651" class="mp mq iq kf b kg my kk mz ko na ks nb kw nc la mu mv mw mx bi translated">基于递归神经网络的电价预测。(2018).Slideshare.net。检索于 2018 年 5 月 1 日，来自<a class="ae kc" href="https://www.slideshare.net/TaegyunJeon1/electricity-price-forecasting-with-recurrent-neural-networks" rel="noopener ugc nofollow" target="_blank">https://www . slide share . net/taegyunjeon 1/electric-price-forecasting-with-recurring-neural-networks</a></li><li id="b9b9" class="mp mq iq kf b kg my kk mz ko na ks nb kw nc la mu mv mw mx bi translated">微分 3 微分的基本法则积和商法则链式法则经济学中的边际函数高阶导数。— ppt 下载。(2018).Slideplayer.com。检索于 2018 年 5 月 1 日，来自<a class="ae kc" href="http://slideplayer.com/slide/10776187/" rel="noopener ugc nofollow" target="_blank">http://slideplayer.com/slide/10776187/</a></li><li id="4971" class="mp mq iq kf b kg my kk mz ko na ks nb kw nc la mu mv mw mx bi translated">反向传播一个 LSTM:一个数值例子。(2016).中等。检索于 2018 年 5 月 2 日，来自<a class="ae kc" href="https://medium.com/@aidangomez/let-s-do-this-f9b699de31d9" rel="noopener">https://medium.com/@aidangomez/let-s-do-this-f9b699de31d9</a></li><li id="f0a8" class="mp mq iq kf b kg my kk mz ko na ks nb kw nc la mu mv mw mx bi translated">只有 Numpy:推导长期短期记忆中的前馈和反向传播(LSTM)第 1 部分。(2018).成为人类:人工智能杂志。2018 年 5 月 2 日检索，来自<a class="ae kc" href="https://becominghuman.ai/only-numpy-deriving-forward-feed-and-back-propagation-in-long-short-term-memory-lstm-part-1-4ee82c14a652" rel="noopener ugc nofollow" target="_blank">https://becoming human . ai/only-numpy-derivating-forward-feed-and-back-propagation-in-long-short-term-memory-lstm-part-1-4ee 82 c 14 a 652</a></li></ol></div></div>    
</body>
</html>