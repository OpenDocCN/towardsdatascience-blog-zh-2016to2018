<html>
<head>
<title>Another Twitter sentiment analysis with Python — Part 8 (Dimensionality reduction: Chi2, PCA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python进行的另一个Twitter情感分析—第8部分(降维:Chi2，PCA)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-8-dimensionality-reduction-chi2-pca-c6d06fb3fcf3?source=collection_archive---------2-----------------------#2018-01-25">https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-8-dimensionality-reduction-chi2-pca-c6d06fb3fcf3?source=collection_archive---------2-----------------------#2018-01-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/8612106a7bc787f5e71e422a43974cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ab02Sah7jLSwllsm"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e44f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是我正在进行的推特情感分析项目的第8部分。你可以从下面的链接找到以前的帖子。</p><ul class=""><li id="172e" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/another-twitter-sentiment-analysis-bb5b01ebad90">第一部分:数据清理</a></li><li id="3ac1" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/another-twitter-sentiment-analysis-with-python-part-2-333514854913">第二部分:EDA，数据可视化</a></li><li id="2352" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/another-twitter-sentiment-analysis-with-python-part-3-zipfs-law-data-visualisation-fc9eadda71e7">第三部分:齐夫定律，数据可视化</a></li><li id="501a" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/another-twitter-sentiment-analysis-with-python-part-4-count-vectorizer-b3f4944e51b5">第四部分:特征提取(计数矢量器)、N-gram、混淆矩阵</a></li><li id="8973" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/another-twitter-sentiment-analysis-with-python-part-5-50b4e87d9bdd">第5部分:特征提取(Tfidf矢量器)、机器学习模型比较、词法方法</a></li><li id="6a97" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/another-twitter-sentiment-analysis-with-python-part-6-doc2vec-603f11832504">第6部分:Doc2Vec </a></li><li id="2616" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/another-twitter-sentiment-analysis-with-python-part-7-phrase-modeling-doc2vec-592a8a996867">第七部分:短语建模+ Doc2Vec </a></li></ul><p id="96f7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上一篇文章中，我已经将短语建模与doc2vec模型结合起来，并看到验证准确性略有提高，我将继续讨论神经网络，看看ANN如何提高性能。但是我决定绕道而行，尝试对我从Tfidf矢量器和Doc2Vec矢量获得的特征进行降维。</p><p id="25c8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">到目前为止，在特征提取方面，我尝试了三种不同的方法:count vectorizer、Tfdif vectorizer、doc2vec。我得到的最好的验证结果如下。</p><p id="fe16" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">分类器:逻辑回归(L2正则化，正则化强度:1.0)</p><ul class=""><li id="9add" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">计数矢量器(80，000个特征，n元语法范围:1，2，3，停用词移除:X) — 82.44%</li><li id="5ee7" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">tfidf矢量器(100，000个特征，n元语法范围:1，2，3，停用词移除:X) — 82.92%</li><li id="198e" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">Doc2Vec (unigram DBOW + trigram DMM:每条推文的200维向量总数)— 75.76%</li></ul><p id="7b5c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除此之外，我还使用词法方法自定义了分类器。</p><ul class=""><li id="871c" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">自定义词典分类器(决策阈值设置为0.56) — 75.96%</li></ul><p id="876c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了词法方法之外，模型所需的特征数量似乎相当大，所以我决定看看是否可以减少Tfidf矢量器和doc2vec矢量的特征维数。Doc2Vec模型中的矢量为200维，与Tfidf矢量器的100，000个特征相比，听起来非常小。但是这200维向量是全实数的稠密矩阵，而10万个特征是零很多的稀疏矩阵。因此，在计算方面，Doc2Vec向量也需要一些时间来计算。因此，如果我可以减少维度，那么它将有助于运行模型的超参数调整的各种设置。</p><p id="ecc6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，让我们尝试使用卡方特征选择对Tfidf向量进行降维。</p><p id="a15f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">*除了我将附上的简短代码块，你可以在这篇文章的末尾找到整个Jupyter笔记本的链接。</p><h1 id="1704" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">Chi2特征选择</h1><p id="add0" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">在Scikit-learn库中，有三种方法可用于稀疏矩阵的特征选择，如Tfidf向量或计数向量。通过查看文档，您可以看到chi2、mutual_info_regression、mutual_info_classif将在不使数据密集的情况下处理数据。在我的情况下，我有150万条推文，并希望从10万个特征中减少维度，因此将其转换为密集矩阵不是一个选项。它不适合我的内存。</p><p id="aec0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">卡方统计测量特征(在这种情况下，推文中的一个术语)和类别(推文是正面还是负面)之间缺乏独立性。</p><p id="5973" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以首先把一条推文中的一个术语和这条推文所属的类别之间的关系想象成一个列联表。列联表只是显示频率分布的表的一个花哨的词。</p><p id="831f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们的语料库中有如下三个句子。</p><ol class=""><li id="9ced" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la ms lh li lj bi translated">我喜欢狗</li><li id="7111" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la ms lh li lj bi translated">我讨厌狗</li><li id="fce8" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la ms lh li lj bi translated">我喜欢狗和烹饪</li></ol><p id="e97f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设每条推文的情绪等级是积极的，消极的，积极的。(顺便说一句，我喜欢狗和烹饪，也喜欢猫)</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/0b96bde58d1d3cfb54a765d4524af729.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hAowV6If2OzqgpnXIgwXNw.png"/></div></div></figure><p id="7127" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们将'<em class="my"> t </em>'定义为我们正在查看的特定术语，在本例中为“狗”，将'<em class="my"> c </em>'定义为类，因为该类只有两个类，所以它将是1(正)或0(负)。使用列联表，其中A为'<em class="my"> t </em>发生的次数，<em class="my"> c </em>为正，B为' t '发生的次数，【C】为负，C为'<em class="my"> t </em>未发生的次数，<em class="my"> c </em>为正，最后D为'<em class="my"> t </em>未发生的次数，<em class="my"> c </em>为负。现在我们准备计算卡方统计量。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mz"><img src="../Images/12dd9228646d6091c57ea977bd3fe3b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PQR0D1mg6sDOWRNLB5fITQ.png"/></div></div></figure><p id="e122" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="my"> N </em>是样本总数，术语“狗”的卡方得分是3！由于卡方测量的是特征和类之间缺乏独立性，如果某个特征与其他特征相比具有较高的卡方得分，则意味着该特征对于预测类是有用的。</p><p id="ae23" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我将首先将训练数据转换为100，000个特征的Tfidf向量，并查看chi2选择了哪些特征作为有用的特征。让我们把我们得到的分数画在图上，看看哪些单词特征对预测有用。我将在下面的图表中只列出前20个特征，但是只要你的电脑屏幕允许，你可以尽可能多的列出来。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/56c60de325b566b1faeac1511dbb899f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1XWy0ZeYYkZWMg8IbRb2gw.png"/></div></div></figure><p id="f429" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">chi2选择的最有用的特征是“谢谢”这个词，我假设这主要来自于积极的推文。它选择的第二个最有用的特征是“悲伤”这个词，这次我猜它来自负面推文。如果考虑chi2是如何计算的，它不仅在预测正类的术语上得分高，而且在预测负类的术语上得分高。</p><p id="4491" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然现在我们已经了解了chi2特征选择是如何工作的，那么让我们将维度减少到不同数量的特征，并检查验证集的准确性。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="f4ab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还有一件事。当您拟合和转换语料库时，Tfidf矢量器可以首先限制特征的数量。我想比较相同特征数量下的验证准确性:1)当特征数量受到Tfidf矢量化阶段的限制时，2)当使用chi2统计将特征数量从100，000个特征减少时。</p><p id="a63b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">*在下面的代码块中，我绘制了我在不同的max特性上运行Tfidf矢量器得到的结果，你也可以在本文末尾的Jupyter笔记本上找到完整的代码。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ca"><img src="../Images/1d1ff6921b4374c5f39c3d8620aa25d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SZziJNUjxIxGNH8s97TgUg.png"/></div></div></figure><p id="b097" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上图中，红色虚线是来自降维的验证集精度，蓝色线是拟合Tfidf矢量器时首先限制特征数量的结果。我们可以看到，使用Tfidf矢量器首先限制要素的数量比从较大的要素中减少维度会产生更好的结果。这不是一个笼统的说法，而是我在这个特定背景下的发现。如果你在其他语料库中有不同的结果，我很想知道它有什么不同。</p><h1 id="9750" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">主成分分析</h1><p id="c251" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">PCA是一种降维工具，可用于将一大组变量降维为一个仍包含原始集合中大部分信息的小集合。这听起来很酷，你可以减少数据的特征，但不能保留大部分需要的信息。但是如果你尝试过谷歌搜索“PCA”，你可能会知道它会给你返回所有听起来很难的术语，如“特征值”、“特征向量”、“矩阵投影”等。</p><p id="4703" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我不会详细介绍PCA实际上是如何计算的，但会尽量保持直观的水平，这样任何阅读这篇文章的人都可以理解它的基础知识，并用Python实现。</p><p id="00f9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">PCA所做的是转换坐标系，使坐标轴成为我们整体数据的最简洁、信息最丰富的描述符。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/236bae68dbec51274ea7d997cfc5b4e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NPOLDXHEUPwi1PXTqjELjA.png"/></div></figure><p id="1200" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上图摘自<a class="ae kc" href="http://mengnote.blogspot.co.uk/2013/05/an-intuitive-explanation-of-pca.html" rel="noopener ugc nofollow" target="_blank">徐萌在PCA </a>上的博客文章。我发现徐萌的解释非常有助于直观地理解这个概念。您在图(A)中看到的形状是三维的，但如果我们关注数据的形状，而不是轴，形状本身就是平面的二维表面。通过运行PCA，我们为数据找到新的坐标，这将最好地描述数据是如何形成的。第一个主成分是解释数据中最大差异的成分。在图(B)中，我们看到通过绘制“组件1”线，它能够保留最分散的数据点的信息。通过添加“成分2”行，这条“成分2”行解释了我们数据中的第二大差异。下一步是将原始数据转换到我们新发现的轴上，这个轴是二维的，而不是原来的三维。图(C)中的最终结果给了我们一个很好的画面，展示了即使我们已经放弃了第三维，数据是如何仅用二维来形成的。</p><p id="0516" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并且在实现数据的这种变换时使用特征向量和特征值。特征向量指定通过原始坐标空间的方向，而特征值指示其对应特征向量方向上的变化量。</p><p id="be6e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你想更深入地了解PCA的概念，还有一些我觉得有用的博客文章。</p><ul class=""><li id="9acf" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated"><a class="ae kc" href="http://setosa.io/ev/principal-component-analysis/" rel="noopener ugc nofollow" target="_blank">Victor Powell的一篇博客文章:“主成分分析”</a>(你可以操纵和摆弄二维或三维的数据点，这将极大地帮助你直观地理解主成分分析的作用)</li><li id="4c06" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" href="https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/" rel="noopener ugc nofollow" target="_blank">George Dallas的博客文章:《主成分分析4个假人:特征向量、特征值和降维》</a>(他已经成功解释了这个概念，没有一个数学公式，难以置信)</li></ul><p id="d9ca" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，让我们尝试用PCA降低doc2vec向量的维数。我们还可以将结果绘制在图表上，看看将特征的数量减少到一个较小的主成分集是否可行，以及给定数量的主成分可以解释多少关于原始特征的方差。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/8aa85fe9bf6b1db0df316ea8fce43091.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lj9cTFS4DHU0h0W5vb4zjg.png"/></div></div></figure><p id="d13a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上图中，红线代表累计解释方差，蓝线代表每个主成分的解释方差。通过看上面的图表，尽管红线不是完美的直线，但非常接近直线。这样好吗？不。这意味着每个主成分对方差解释的贡献几乎相等，基于主成分分析降低维度没有多大意义。这一点从蓝线也可以看出来，蓝线非常接近底部的一条直线。</p><p id="2f97" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有点令人失望的是，在我处理tweets文本数据的具体案例中，维数约减并没有太大的帮助。当首先使用Tfidf矢量器限制特征时，Tfidf矢量显示出比随后降低维度更好的结果，并且doc2vec矢量似乎通过其200维特征空间携带大致相似的信息量。</p><p id="c2e6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，这不是一个普遍的说法，这只是我用我的特殊数据发现的。特别是主成分分析，当它应用于数字特征时，我看到它成功地将数据的维度从100个或更多的特征减少到大约10个特征，同时能够解释90%的数据差异。</p><p id="b61a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您对这篇文章的神经网络建模有所期待，很抱歉我不得不绕道而行，但在下一篇文章中，我肯定会进行神经网络建模。一如既往的感谢您的阅读。</p><p id="4289" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以通过以下链接找到Jupyter笔记本:</p><p id="0cc2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://github.com/tthustla/twitter_sentiment_analysis_part8/blob/master/Capstone_part4-Copy6.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/tthustle a/Twitter _情操_分析_ part 8/blob/master/Capstone _ part 4-copy 6 . ipynb</a></p></div></div>    
</body>
</html>