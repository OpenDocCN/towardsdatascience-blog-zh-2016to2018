# 决策树集成——打包和提升

> 原文：<https://towardsdatascience.com/decision-tree-ensembles-bagging-and-boosting-266a8ba60fd9?source=collection_archive---------0----------------------->

## 随机森林和梯度增强

我们每天都在使用决策树技术来规划我们的生活，我们只是没有给那些决策过程起一个花哨的名字。

企业使用这些有监督的机器学习技术，如决策树，来做出更好的决策，并获得更多利润。决策树已经存在了很长一段时间，而且众所周知也存在偏差和差异。简单的树会有很大的偏差，复杂的树会有很大的变化。

***集成方法*** ，将几个决策树结合起来，产生比利用单个决策树更好的预测性能。集成模型背后的主要原理是一组弱学习者聚集在一起形成一个强学习者。

让我们讨论几个执行集合决策树的技术:

1.制袋材料

2.助推

> ***装袋*** (Bootstrap Aggregation)在我们的目标是降低决策树的方差时使用。这里的想法是从随机选择的训练样本中创建几个数据子集*并替换*。现在，每个子集数据的集合都被用来训练他们的决策树。结果，我们得到了不同模型的集合。使用来自不同树的所有预测的平均值，这比单个决策树更健壮。

***随机森林*** 是套袋的延伸。这需要一个额外的步骤，除了获取数据的随机子集，还需要随机选择特征，而不是使用所有特征来生成树。当你有很多随机的树时。它叫做随机森林😊

让我们看一下实施随机森林的步骤:

**1** 。假设训练数据集中有 N 个观测值和 M 个特征。首先，从训练数据集中随机抽取一个样本进行替换。

**2** 。随机选择 M 个特征的子集，并且使用给出最佳分裂的特征来迭代地分裂节点。

**3** 。这棵树长到最大。

**4** 。重复上述步骤，并基于来自 n 棵树的预测的集合给出预测。

**使用随机森林技术的优势:**

*   非常好地处理更高维度的数据。
*   处理缺失值并保持缺失数据的准确性。

**使用随机森林技术的缺点:**

*   由于最终预测基于子集树的平均预测，因此它不会为回归模型提供精确的值。

> **Boosting** 是另一种创建预测器集合的集成技术。在这种技术中，学习者是顺序学习的，早期的学习者将简单的模型与数据拟合，然后分析数据的错误。换句话说，我们拟合连续的树(随机样本),并且在每一步，目标都是解决来自先前树的净误差。

当一个输入被一个假设错误分类时，它的权重会增加，以便下一个假设更有可能将其正确分类。通过在最后组合整个集合，将弱学习者转换成表现更好的模型。

**梯度增强**是对增强方法的扩展。

> 梯度推进=梯度下降+推进。

它使用梯度下降算法，可以优化任何可微损失函数。一个系综树被一个接一个地构建，并且各个树被顺序地求和。下一个采油树试图恢复损失(实际值和预测值之间的差异)。

**使用梯度增强技术的优势:**

*   支持不同的损失函数。
*   互动良好。

**使用梯度增强技术的缺点:**

*   容易过度拟合。
*   需要仔细调整不同的超参数