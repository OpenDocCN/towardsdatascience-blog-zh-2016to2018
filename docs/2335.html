<html>
<head>
<title>Only Numpy: Deriving Forward feed and Back Propagation in Long Short Term Memory (LSTM) part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">只有数字:长期短期记忆中的前向反馈和反向传播(LSTM)第 1 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/only-numpy-deriving-forward-feed-and-back-propagation-in-long-short-term-memory-lstm-part-1-4ee82c14a652?source=collection_archive---------4-----------------------#2018-01-11">https://towardsdatascience.com/only-numpy-deriving-forward-feed-and-back-propagation-in-long-short-term-memory-lstm-part-1-4ee82c14a652?source=collection_archive---------4-----------------------#2018-01-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c689" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以我从我的韩国寒假旅行回来了。(南方)。实际上，我想在去度假之前做这件事，但不管怎样，还是这样吧。此外，我将把本教程分为两部分，因为反向传播变得相当长。我将使用数学符号</p><p id="bfaf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">今天我们将对 LSTM(长短期记忆)网络进行前馈操作和反向传播，所以让我们先看看网络架构。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h1 id="9b33" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">网络体系结构</h1><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/875b405e289181adb085ba97eac4d84f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uCr_sQ60nPQg4fsNiSZkKA.jpeg"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Network Architecture</figcaption></figure><p id="21a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所述，我需要解释的符号很少。<br/> F(k) - &gt;时间戳处的遗忘门<br/> I(k) - &gt;时间戳处的输入门<br/> A(k)- &gt;时间戳处的激活门<br/> O(k)- &gt;时间戳处的输出门<br/> S(k)- &gt;时间戳处的状态<br/> Out(k) - &gt;时间戳处的输出</p><p id="910d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一件要注意的事情是正常体重和经常性体重。输入 X 使用常规权重 W 执行点积，而先前的输出使用递归权重 Wrec 执行点积。所以总共有 8 个权重，特别是在执行反向传播时，注意这一点很重要。</p><h1 id="8fd6" class="ks kt iq bd ku kv mg kx ky kz mh lb lc ld mi lf lg lh mj lj lk ll mk ln lo lp bi translated">热门人工智能文章:</h1><blockquote class="ml"><p id="1a7d" class="mm mn iq bd mo mp mq mr ms mt mu kk dk translated"><a class="ae mv" href="https://becominghuman.ai/how-to-train-a-neural-network-to-code-by-itself-a432e8a120df" rel="noopener ugc nofollow" target="_blank"> 1。如何训练一个神经网络自己编码？</a></p><p id="682d" class="mm mn iq bd mo mp mq mr ms mt mu kk dk translated"><a class="ae mv" href="https://becominghuman.ai/from-perceptron-to-deep-neural-nets-504b8ff616e" rel="noopener ugc nofollow" target="_blank"> 2。从感知器到深度神经网络</a></p><p id="2f8c" class="mm mn iq bd mo mp mq mr ms mt mu kk dk translated"><a class="ae mv" href="https://becominghuman.ai/neural-networks-for-solving-differential-equations-fa230ac5e04c" rel="noopener ugc nofollow" target="_blank"> 3。求解微分方程的神经网络</a></p></blockquote><figure class="mw mx my mz na lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/485c356eb86c9a89080a6697aebfe443.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RXMB-KyHzmvy9iylyBPNxg.jpeg"/></div></div></figure><p id="e223" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">浅绿色框-&gt;成本函数，我们正在使用 L2 成本函数<br/>黄色框与下图所示的一个框相同。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi nb"><img src="../Images/31d40609984b635af9f8fa9b3455e799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ka6k_WyG9B3gXPlvU0vrOw.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Image from <a class="ae mv" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">colah’s blog</a></figcaption></figure></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h1 id="35cc" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">前馈过程</h1><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/11ceb5738b99f0c4afc8f85e18dc0ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ypk-g98J3K3qqCKUmf0FNA.jpeg"/></div></div></figure><p id="7871" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所述，LSTM 的前馈过程非常简单，我们唯一要小心的是时间戳。上图显示了 TS 为 1 和 TS 为 2 时，两个时间戳的前馈过程。</p><p id="e533" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一件要注意的事情是每个时间戳的成本函数，我们可以看到我们得到了每个时间戳的错误率。<br/>1/2 *(Out(1)-Y(1))→TS = 1<br/>1/2 *(Out(2)-Y(2))→TS = 2</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h1 id="7c3b" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">时间戳为 2 时所有权重的反向传播</h1><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/a4cd35278f6d95aadc675889ce4a6750.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PBu6tUmPMxBqxEpZHujfPA.jpeg"/></div></div></figure><p id="62e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">记得我告诉过你我们总共有 8 个砝码吗？(包括常规的和循环的)现在我们需要对它们中的每一个进行反向传播，这就是为什么我们在板上有 8 个方程。</p><p id="7bc0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">等式 1 →关于 Wo(输出门)的权重更新<br/>等式 2→关于 Wreco(循环输出门)的权重更新<br/>等式 3→关于 Wa(激活门)的权重更新<br/>等式 4→关于 Wreca(循环激活门)的权重更新<br/>等式 5→关于 Wi(输入门)的权重更新<br/>等式 6→关于 Wreci(循环输入门)的权重更新<br/>等式 7→关于 Wf(遗忘门)的权重更新<br/>等式 8→关于 Wrecf(循环遗忘门)的权重更新</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/e9c28d11efbb20a72bd7cd9ce2b0a710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1gXWTDXXwSIPTgQe53Dgg.jpeg"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Closer Look for Wo, Wreco, Wa and Wreca</figcaption></figure><p id="4558" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所示，数学术语写在上面，实际的反向传播写在底部。(关于每个术语)。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/d661a0ddf178c4cf7d07282f71800017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TsWXO1430BhPFufI_Gufbg.jpeg"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Closer Look for Wi, Wreci, Wf and Wrecf</figcaption></figure></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h1 id="5bf9" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">时间戳为 1 时相对于 Wo(输出门)的反向传播</h1><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/49d083bf8fbc933d4bd98564fc6d8404.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9l8cxC9icF1B10y6vcPGOA.jpeg"/></div></div></figure><p id="d383" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如你已经注意到的，由于链式法则，当时间戳为 1 时，反向传播变得非常复杂。你在上面看到的这五个方程，只是为了得到相对于 Wo 的<strong class="jp ir">误差率。</strong></p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/6c5bb7075672ae67342ddb02a25b763c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hqukoMIp1n1fL9Bhe4cO9A.jpeg"/></div></div></figure><p id="5715" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当时间戳为 1 时，关于执行反向传播的一个有趣事实是总错误率。我们不仅需要考虑时间戳为 1 时代价函数的错误率，还需要考虑时间戳为 2 时代价函数的错误率。所以…<br/>蓝框→时间戳为 1 时成本函数的错误率<br/>绿框→时间戳为 2 时成本函数的错误率</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/9b5976ea1819bbffa34576989b00d0b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gC41lEbIdeaQaIqBsomcqQ.jpeg"/></div></div></figure><p id="a808" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面是执行反向传播所需的实际数学方程的屏幕截图。我想指出两件事。</p><ol class=""><li id="c987" class="nc nd iq jp b jq jr ju jv jy ne kc nf kg ng kk nh ni nj nk bi translated">我们正在乘以递归权重以执行反向传播的黑星→这就是为什么 LSTM 如此难以训练的原因，它们非常容易受到梯度爆炸和消失梯度的影响。</li><li id="e52f" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated">如上图所示，红框内的数学符号是重复的术语。在未来，我将通过首先计算重复项来简化反向传播。</li></ol></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h2 id="a6d3" class="nq kt iq bd ku nr ns dn ky nt nu dp lc jy nv nw lg kc nx ny lk kg nz oa lo ob bi translated">最后的话</h2><p id="a43f" class="pw-post-body-paragraph jn jo iq jp b jq oc js jt ju od jw jx jy oe ka kb kc of ke kf kg og ki kj kk ij bi translated">这是第一部分，我会很快给你们第二部分和第三部分的更新…(我希望如此)。</p><p id="ccc9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同时，在我的 twitter <a class="ae mv" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>关注我，并访问<a class="ae mv" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或我的<a class="ae mv" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。如果你感兴趣的话，我还在简单的 RNN <a class="ae mv" href="https://medium.com/@SeoJaeDuk/only-numpy-vanilla-recurrent-neural-network-with-activation-deriving-back-propagation-through-time-4110964a9316" rel="noopener">上做了反向传播。</a></p><figure class="lr ls lt lu gt lv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><div class="lr ls lt lu gt ab cb"><figure class="oj lv ok ol om on oo paragraph-image"><a href="https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c"><img src="../Images/20880898f038333e31843bbd07b0e4df.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*2f7OqE2AJK1KSrhkmD9ZMw.png"/></a></figure><figure class="oj lv ok ol om on oo paragraph-image"><a href="https://upscri.be/8f5f8b"><img src="../Images/dd23357ef17960a7bfb82e7b277f50f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*v-PpfkSWHbvlWWamSVHHWg.png"/></a></figure><figure class="oj lv ok ol om on oo paragraph-image"><a href="https://becominghuman.ai/write-for-us-48270209de63"><img src="../Images/91ecfb22295488bc2c6af3d2ac34d857.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*Wt2auqISiEAOZxJ-I7brDQ.png"/></a></figure></div></div></div>    
</body>
</html>