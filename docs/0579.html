<html>
<head>
<title>My journey with TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我与 TensorFlow 的旅程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/my-journey-with-tensorflow-371bc31c2dbf?source=collection_archive---------6-----------------------#2017-05-23">https://towardsdatascience.com/my-journey-with-tensorflow-371bc31c2dbf?source=collection_archive---------6-----------------------#2017-05-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="174c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">是的，不要读这个</h2></div><h2 id="0bf5" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">TL；速度三角形定位法(dead reckoning)</h2><ul class=""><li id="4787" class="lb lc iq ld b le lf lg lh ko li ks lj kw lk ll lm ln lo lp bi translated">对 OpenAI 健身房的环境有一个基本的了解。值得注意的是，<code class="fe lq lr ls lt b">env.observation_space.shape</code>和<code class="fe lq lr ls lt b">env.action_space.n</code>。</li><li id="83a4" class="lb lc iq ld b le lu lg lv ko lw ks lx kw ly ll lm ln lo lp bi translated">理解 Q-Learning 是一个马尔可夫决策过程。你有状态，这对于有连续输入空间的东西来说很棘手。</li><li id="3f7f" class="lb lc iq ld b le lu lg lv ko lw ks lx kw ly ll lm ln lo lp bi translated">实际上，要明白几乎每个强化学习问题都是一个 MDP。甚至那些或多或少具有无限状态/动作空间的仍然被视为 MDP。您可以将这些空间转换为功能空间，使它们更易于管理。</li></ul></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><p id="2ced" class="pw-post-body-paragraph mg mh iq ld b le mi jr mj lg mk ju ml ko mm mn mo ks mp mq mr kw ms mt mu ll ij bi translated">老实说，我对过去几年的整个神经网络运动非常感兴趣。我尤其对强化学习及其在游戏中的应用感兴趣(参见<a class="ae mv" href="https://gym.openai.com/envs#atari" rel="noopener ugc nofollow" target="_blank"> OpenAI Gym </a>)。凭借我在大学时对机器学习和微积分的基本了解，我现在正走在一条构建一些很酷的东西的道路上。</p><h2 id="c0f6" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">卑微的开始</h2><blockquote class="mw mx my"><p id="8342" class="mg mh mz ld b le mi jr mj lg mk ju ml na mm mn mo nb mp mq mr nc ms mt mu ll ij bi translated">“好艺术家借，大艺术家偷”<br/> ~ <a class="ae mv" href="http://levine.sscnet.ucla.edu/papers/b_l_review.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></p></blockquote><p id="40d9" class="pw-post-body-paragraph mg mh iq ld b le mi jr mj lg mk ju ml ko mm mn mo ks mp mq mr kw ms mt mu ll ij bi translated">我想我会把下面这篇文章和 OpenAI 健身房的 CartPole-v0 结合起来。这个想法是从一个“更简单”的算法(Q-Learning)开始，然后转移到神经网络进行一些比较。</p><div class="nd ne gp gr nf ng"><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd ir gy z fp nl fr fs nm fu fw ip bi translated">使用 Tensorflow 的简单强化学习第 0 部分:使用表格和神经网络的 Q 学习</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">在我的强化学习系列的这个教程中，我们将探索一个叫做 Q……</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">medium.com</p></div></div><div class="np l"><div class="nq l nr ns nt np nu nv ng"/></div></div></a></div><p id="fc26" class="pw-post-body-paragraph mg mh iq ld b le mi jr mj lg mk ju ml ko mm mn mo ks mp mq mr kw ms mt mu ll ij bi translated">在对观察空间的大小和一些被否决的函数有一点困惑之后，我意识到 Q-Learning 不适合 CartPole，因为观察是连续空间而不是离散空间。幸运的是，TensorFlow 有一个<a class="ae mv" href="https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">很棒的</strong>初学者指南</a>使用 MNIST 数据集！</p><p id="612d" class="pw-post-body-paragraph mg mh iq ld b le mi jr mj lg mk ju ml ko mm mn mo ks mp mq mr kw ms mt mu ll ij bi translated">这给了我们代码，但是我们仍然需要把这个分类算法变成一个学习算法。<a class="ae mv" href="https://gist.github.com/omdv/98351da37283c8b6161672d6d555cde6" rel="noopener ugc nofollow" target="_blank">Github gist</a>为 CartPole 示例的“奖励模型”提供了一些背景。<a class="ae mv" href="https://gist.github.com/iambrian/2bcc8fc03eaecb2cbe53012d2f505465" rel="noopener ugc nofollow" target="_blank">Github gist</a>提供了一种非统计方法来“解决”问题。这很好，因为我们基本上想要“学习”手动方法。这个来自 tflearn 的 Github repo 的例子很好地模拟了我们正在做的事情，只是我们没有这么大的输入空间。</p><p id="9ebe" class="pw-post-body-paragraph mg mh iq ld b le mi jr mj lg mk ju ml ko mm mn mo ks mp mq mr kw ms mt mu ll ij bi translated">因此，过一段时间后，您可能会看到以下内容:</p><pre class="nw nx ny nz gt oa lt ob oc aw od bi"><span id="f2c1" class="kf kg iq lt b gy oe of l og oh">inputs = tf.placeholder(tf.float32, [<br/>  <strong class="lt ir">None</strong>,<br/>  action_repeat,<br/>  env.observation_space.shape[0]    ])<br/>net = tflearn.fully_connected(inputs, 4, activation='relu')<br/>q_values = tflearn.fully_connected(net, env.action_space.n)</span></pre><p id="7235" class="pw-post-body-paragraph mg mh iq ld b le mi jr mj lg mk ju ml ko mm mn mo ks mp mq mr kw ms mt mu ll ij bi translated">基本上，我们在我们的网络中存储了历史的样本。理想情况下，我们的网络将能够根据连续的帧来判断杆子朝哪个方向落下。</p><p id="cdf4" class="pw-post-body-paragraph mg mh iq ld b le mi jr mj lg mk ju ml ko mm mn mo ks mp mq mr kw ms mt mu ll ij bi translated">如果我们记录直到失败的迭代次数，我们可以看到如下结果:</p><pre class="nw nx ny nz gt oa lt ob oc aw od bi"><span id="4b15" class="kf kg iq lt b gy oe of l og oh">took 51 iterations<br/>took 46 iterations<br/>took 51 iterations<br/>took 29 iterations<br/>took 33 iterations<br/>took 38 iterations<br/>took 73 iterations</span></pre><p id="a88d" class="pw-post-body-paragraph mg mh iq ld b le mi jr mj lg mk ju ml ko mm mn mo ks mp mq mr kw ms mt mu ll ij bi translated">没有明确的迹象表明我们的算法正在收敛到一个解。:(</p><h2 id="dbb2" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">后退几百步</h2><p id="8682" class="pw-post-body-paragraph mg mh iq ld b le lf jr mj lg lh ju ml ko oi mn mo ks oj mq mr kw ok mt mu ll ij bi translated">在这一点上，很明显，即使对强化学习有很高的理解，也不足以构建一个算法。我把这比作使用网络浏览器和构建网络服务器。</p><p id="8b7b" class="pw-post-body-paragraph mg mh iq ld b le mi jr mj lg mk ju ml ko mm mn mo ks mp mq mr kw ms mt mu ll ij bi translated">我花了一点时间看了大卫·西尔弗的强化学习课程。我从第三课开始，虽然我真的应该从头开始。直到第七讲，演员批评的方法才被提出来。(此外，看起来还有另一个参考大卫·西尔弗讲座的综合信息<a class="ae mv" href="https://github.com/dennybritz/reinforcement-learning" rel="noopener ugc nofollow" target="_blank">这里</a>)。</p><p id="8200" class="pw-post-body-paragraph mg mh iq ld b le mi jr mj lg mk ju ml ko mm mn mo ks mp mq mr kw ms mt mu ll ij bi translated">这篇博客文章我已经读过<a class="ae mv" href="http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html" rel="noopener ugc nofollow" target="_blank">一百万遍了。它有很多有用的信息，但我认为如果没有基本面，大部分信息都没有意义。</a></p><p id="bef8" class="pw-post-body-paragraph mg mh iq ld b le mi jr mj lg mk ju ml ko mm mn mo ks mp mq mr kw ms mt mu ll ij bi translated">以下是我收集的一些信息:</p><ul class=""><li id="28e2" class="lb lc iq ld b le mi lg mk ko ol ks om kw on ll lm ln lo lp bi translated">你可以把一个强化学习问题分解成两个步骤:奖励函数和策略函数。</li><li id="00cd" class="lb lc iq ld b le lu lg lv ko lw ks lx kw ly ll lm ln lo lp bi translated">有两种类型的问题:预测和控制。参见本讲座中<a class="ae mv" href="https://www.cs.cmu.edu/~katef/DeepRLControlCourse/lectures/lecture2_mdps.pdf" rel="noopener ugc nofollow" target="_blank">的第 21/22 张幻灯片。一方面，我们可以专注于建立预测</a></li></ul><p id="0717" class="pw-post-body-paragraph mg mh iq ld b le mi jr mj lg mk ju ml ko mm mn mo ks mp mq mr kw ms mt mu ll ij bi translated">我还发现了<a class="ae mv" href="http://kvfrans.com/simple-algoritms-for-solving-cartpole/" rel="noopener ugc nofollow" target="_blank">这篇关于 CartPole-v0 的一些非常简单的策略的博文</a>。我认为这在建立政策梯度方面做得最好。如果你理解了随机搜索和爬山是怎么做的，你就会对政策如何运作有一个直觉。</p><figure class="nw nx ny nz gt op gh gi paragraph-image"><div role="button" tabindex="0" class="oq or di os bf ot"><div class="gh gi oo"><img src="../Images/d2890dd3ef80fc20f92e4b4dfa4da7b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RhZj1-61Zx-ekKXVBq-6dA.png"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">Sample Rewards for my implementation of Hill Climbing</figcaption></figure><p id="5b78" class="pw-post-body-paragraph mg mh iq ld b le mi jr mj lg mk ju ml ko mm mn mo ks mp mq mr kw ms mt mu ll ij bi translated">爬山很容易实现。如果您想获得 TensorFlow 构建模块(摘要、占位符、会话、随机初始化)的经验，这是一个可靠的算法。我犯了一个错误，在“step”循环中添加了占位符。基本上，我们在我们的会话图中增加了新的操作(矩阵乘法),这(我认为)使得图的评估更加复杂。因此，性能会降低并最终停止。</p><p id="1dac" class="pw-post-body-paragraph mg mh iq ld b le mi jr mj lg mk ju ml ko mm mn mo ks mp mq mr kw ms mt mu ll ij bi translated">未来几周，我将深入探讨政策梯度。请在这里随意查看我的一些示例代码:</p><div class="nd ne gp gr nf ng"><a href="https://github.com/jgensler8/my-tensorflow-journey" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd ir gy z fp nl fr fs nm fu fw ip bi translated">jgensler 8/my-tensor flow-journey</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">我的张量流之旅——开放健身房的一些实验</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">github.com</p></div></div><div class="np l"><div class="oz l nr ns nt np nu nv ng"/></div></div></a></div></div></div>    
</body>
</html>