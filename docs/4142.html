<html>
<head>
<title>3 basic approaches in Bag of Words which are better than Word Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">比单词嵌入更好的单词袋中的 3 种基本方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016?source=collection_archive---------2-----------------------#2018-07-22">https://towardsdatascience.com/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016?source=collection_archive---------2-----------------------#2018-07-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/3f08b8e2cc9c227c405726dfb77c66f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4LwK8CAPhZXTuqpQ.jpg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo: <a class="ae kf" href="https://pixabay.com/en/hong-kong-china-city-chinese-asian-383963/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/en/hong-kong-china-city-chinese-asian-383963/</a></figcaption></figure><p id="902b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如今，每个人都在谈论单词(或字符、句子、文档)嵌入。单词袋还值得用吗？我们应该在任何场景中应用嵌入吗？</p><p id="3d80" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看完这篇文章，你会知道:</p><ul class=""><li id="3c8c" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">为什么人们说单词嵌入是灵丹妙药？</li><li id="3f56" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">什么时候单词袋会战胜单词嵌入？</li><li id="8a02" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">词汇袋中的 3 种基本方法</li><li id="d271" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">我们如何用几行文字构建单词包？</li></ul><h1 id="1327" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">为什么有人说嵌入这个词是灵丹妙药？</h1><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div class="ab gu cl mu"><img src="../Images/e223ea5a5396829e7d2c86c150ddd664.png" data-original-src="https://miro.medium.com/v2/format:webp/0*JqmqUtkjnzAGaBfi.jpg"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo: <a class="ae kf" href="https://pixabay.com/en/books-stack-book-store-1163695/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/en/books-stack-book-store-1163695/</a></figcaption></figure><p id="1fe5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在自然语言处理领域，嵌入是解决文本相关问题的成功方法，其性能优于单词袋(BoW)。事实上，BoW 引入了诸如大特征维数、稀疏表示等限制。关于文字嵌入，你可以看看我之前的<a class="ae kf" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a">帖子</a>。</p><p id="c9b7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还应该用弓吗？在某些情况下，我们可以更好地使用 BoW</p><h1 id="3f90" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">什么时候单词袋会战胜单词嵌入？</h1><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/038ae05313f72633c8b402ee0bc5f302.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*utytjaJMc0txiDEf.jpg"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo: <a class="ae kf" href="https://www.ebay.co.uk/itm/Custom-Tote-Bag-Friday-My-Second-Favorite-F-Word-Gift-For-Her-Gift-For-Him-/122974487851" rel="noopener ugc nofollow" target="_blank">https://www.ebay.co.uk/itm/Custom-Tote-Bag-Friday-My-Second-Favorite-F-Word-Gift-For-Her-Gift-For-Him-/122974487851</a></figcaption></figure><p id="accd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在以下情况下，您仍然可以考虑使用 BoW 而不是 Word 嵌入:</p><ol class=""><li id="82a2" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld mw lk ll lm bi translated">构建基线模型。通过使用 scikit-learn，只需要几行代码就可以构建模型。以后，可以用深度学习来咬它。</li><li id="3061" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld mw lk ll lm bi translated">如果您的数据集很小，并且上下文是特定于领域的，BoW 可能比单词嵌入更好。上下文是非常领域特定的，这意味着你不能从预先训练的单词嵌入模型(GloVe，fastText 等)中找到相应的向量。</li></ol><h1 id="e23e" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">我们如何用几行文字构建单词包？</h1><p id="4df5" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">使用传统的强大的 ML 库，有 3 种简单的方法来构建 BoW 模型。</p><p id="a45d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nc">统计出现次数</em> </strong></p><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/d65464d560b61fdc817872076bac7716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9PrYTObfO-NJw7Gl.jpg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo: <a class="ae kf" href="https://pixabay.com/en/home-money-euro-calculator-finance-366927/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/en/home-money-euro-calculator-finance-366927/</a></figcaption></figure><p id="f17e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">统计单词出现次数。使用这种方法的原因是关键字或重要信号会反复出现。所以如果出现的次数代表单词的重要性。更多的频率意味着更多的重要性。</p><pre class="mq mr ms mt gt nd ne nf ng aw nh bi"><span id="9f64" class="ni lt it ne b gy nj nk l nl nm">doc = "In the-state-of-art of the NLP field, Embedding is the \<br/>success way to resolve text related problem and outperform \<br/>Bag of Words ( BoW ). Indeed, BoW introduced limitations \<br/>large feature dimension, sparse representation etc."</span><span id="c6c6" class="ni lt it ne b gy nn nk l nl nm">count_vec = CountVectorizer()<br/>count_occurs = count_vec.fit_transform([doc])<br/>count_occur_df = pd.DataFrame(<br/>    (count, word) for word, count in<br/>     zip(count_occurs.toarray().tolist()[0], <br/>    count_vec.get_feature_names()))<br/>count_occur_df.columns = ['Word', 'Count']<br/>count_occur_df.sort_values('Count', ascending=False, inplace=True)<br/>count_occur_df.head()</span></pre><p id="664b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出</p><pre class="mq mr ms mt gt nd ne nf ng aw nh bi"><span id="d570" class="ni lt it ne b gy nj nk l nl nm">Word: "of", Occurrence: 3<br/>Word: "bow", Occurrence: 2<br/>Word: "way", Occurrence: 1</span></pre><p id="4442" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nc">归一化计数出现次数</em> </strong></p><p id="3349" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你认为极高的频率可能会主导结果并造成模型偏差。标准化可以容易地应用于流水线。</p><pre class="mq mr ms mt gt nd ne nf ng aw nh bi"><span id="dd8f" class="ni lt it ne b gy nj nk l nl nm">doc = "In the-state-of-art of the NLP field, Embedding is the \<br/>success way to resolve text related problem and outperform \<br/>Bag of Words ( BoW ). Indeed, BoW introduced limitations \<br/>large feature dimension, sparse representation etc."</span><span id="e813" class="ni lt it ne b gy nn nk l nl nm">norm_count_vec = TfidfVectorizer(use_idf=False, norm='l2')<br/>norm_count_occurs = norm_count_vec.fit_transform([doc])<br/>norm_count_occur_df = pd.DataFrame(<br/>    (count, word) for word, count in zip(<br/>    norm_count_occurs.toarray().tolist()[0], <br/>    norm_count_vec.get_feature_names()))<br/>norm_count_occur_df.columns = ['Word', 'Count']<br/>norm_count_occur_df.sort_values(<br/>    'Count', ascending=False, inplace=True)<br/>norm_count_occur_df.head()</span></pre><p id="29cc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出</p><pre class="mq mr ms mt gt nd ne nf ng aw nh bi"><span id="1360" class="ni lt it ne b gy nj nk l nl nm">Word: "of", Occurrence: 0.4286<br/>Word: "bow", Occurrence: 0.4286<br/>Word: "way", Occurrence: 0.1429</span></pre><p id="b0f8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nc">术语频率-逆文档频率(TF-IDF) </em> </strong></p><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/eb8ea6e93b8074105743c59e682a72b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7r2GKRepjh5Fl41r.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo: <a class="ae kf" href="http://mropengate.blogspot.com/2016/04/tf-idf-in-r-language.html" rel="noopener ugc nofollow" target="_blank">http://mropengate.blogspot.com/2016/04/tf-idf-in-r-language.html</a></figcaption></figure><p id="8b25" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TF-IDF 采用了另一种方法，这种方法认为高频可能不能提供太多信息增益。换句话说，生僻字为模型贡献了更多的权重。</p><p id="c0cd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果在同一文档(即培训记录)中出现的次数增加，单词的重要性将增加。另一方面，如果它出现在语料库(即其他训练记录)中，则会减少。</p><pre class="mq mr ms mt gt nd ne nf ng aw nh bi"><span id="ef4f" class="ni lt it ne b gy nj nk l nl nm">doc = "In the-state-of-art of the NLP field, Embedding is the \<br/>success way to resolve text related problem and outperform \<br/>Bag of Words ( BoW ). Indeed, BoW introduced limitations \<br/>large feature dimension, sparse representation etc."</span><span id="4056" class="ni lt it ne b gy nn nk l nl nm">tfidf_vec = TfidfVectorizer()<br/>tfidf_count_occurs = tfidf_vec.fit_transform([doc])<br/>tfidf_count_occur_df = pd.DataFrame(<br/>    (count, word) for word, count in zip(<br/>    tfidf_count_occurs.toarray().tolist()[0],   <br/>    tfidf_vec.get_feature_names()))<br/>tfidf_count_occur_df.columns = ['Word', 'Count']<br/>tfidf_count_occur_df.sort_values('Count', ascending=False, inplace=True)<br/>tfidf_count_occur_df.head()</span></pre><p id="46d8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出(该值与标准化计数发生次数完全相同，因为演示代码只包含一个文档)</p><pre class="mq mr ms mt gt nd ne nf ng aw nh bi"><span id="2397" class="ni lt it ne b gy nj nk l nl nm">Word: "of", Occurrence: 0.4286<br/>Word: "bow", Occurrence: 0.4286<br/>Word: "way", Occurrence: 0.1429</span></pre><h1 id="3a6c" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">密码</h1><p id="b8c7" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">此示例代码将在计数发生、标准化计数发生和 TF-IDF 之间进行比较。</p><p id="38b1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用不同的矢量化方法获取模型的样本函数</p><pre class="mq mr ms mt gt nd ne nf ng aw nh bi"><span id="57ce" class="ni lt it ne b gy nj nk l nl nm">def build_model(mode):<br/>    # Intent to use default paramaters for show case<br/>    vect = None<br/>    if mode == 'count':<br/>        vect = CountVectorizer()<br/>    elif mode == 'tf':<br/>        vect = TfidfVectorizer(use_idf=False, norm='l2')<br/>    elif mode == 'tfidf':<br/>        vect = TfidfVectorizer()<br/>    else:<br/>        raise ValueError('Mode should be either count or tfidf')<br/>    <br/>    return Pipeline([<br/>        ('vect', vect),<br/>        ('clf' , LogisticRegression(solver='newton-cg',n_jobs=-1))<br/>    ])</span></pre><p id="e8d5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用另一个示例函数来构建两端管道</p><pre class="mq mr ms mt gt nd ne nf ng aw nh bi"><span id="e093" class="ni lt it ne b gy nj nk l nl nm">def pipeline(df, mode):<br/>    x = preprocess_x(df)<br/>    y = preprocess_y(df)<br/>    <br/>    model_pipeline = build_model(mode)<br/>    cv = KFold(n_splits=10, shuffle=True)<br/>    <br/>    scores = cross_val_score(<br/>        model_pipeline, x, y, cv=cv, scoring='accuracy')<br/>    print("Accuracy: %0.4f (+/- %0.4f)" % (<br/>        scores.mean(), scores.std() * 2))<br/>    <br/>    return model_pipeline</span></pre><p id="8039" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们检查一下需要处理的词汇数量</p><pre class="mq mr ms mt gt nd ne nf ng aw nh bi"><span id="01b4" class="ni lt it ne b gy nj nk l nl nm">x = preprocess_x(x_train)<br/>y = y_train<br/>    <br/>model_pipeline = build_model(mode='count')<br/>model_pipeline.fit(x, y)</span><span id="63ad" class="ni lt it ne b gy nn nk l nl nm">print('Number of Vocabulary: %d'% (len(model_pipeline.named_steps['vect'].get_feature_names())))</span></pre><p id="0d16" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出</p><pre class="mq mr ms mt gt nd ne nf ng aw nh bi"><span id="302f" class="ni lt it ne b gy nj nk l nl nm">Number of Vocabulary: 130107</span></pre><p id="b97a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过传递“计数”(计数发生)、“tf”(正常计数发生)和“tfi df”(TF-IDF)来调用管道</p><pre class="mq mr ms mt gt nd ne nf ng aw nh bi"><span id="ce1b" class="ni lt it ne b gy nj nk l nl nm">print('Using Count Vectorizer------')<br/>model_pipeline = pipeline(x_test, y_test, mode='count')</span><span id="6ae4" class="ni lt it ne b gy nn nk l nl nm">print('Using TF Vectorizer------')<br/>model_pipeline = pipeline(x_test, y_test, mode='tf')</span><span id="579c" class="ni lt it ne b gy nn nk l nl nm">print('Using TF-IDF Vectorizer------')<br/>model_pipeline = pipeline(x_test, y_test, mode='tfidf')</span></pre><p id="dc18" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果是</p><pre class="mq mr ms mt gt nd ne nf ng aw nh bi"><span id="53cb" class="ni lt it ne b gy nj nk l nl nm">Using Count Vectorizer------<br/>Accuracy: 0.8892 (+/- 0.0198)<br/>Using TF Vectorizer------<br/>Accuracy: 0.8071 (+/- 0.0110)<br/>Using TF-IDF Vectorizer------<br/>Accuracy: 0.8917 (+/- 0.0072)</span></pre><h1 id="a8ac" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">结论</h1><p id="395f" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">你可以从<a class="ae kf" href="https://github.com/makcedward/nlp/blob/master/sample/nlp-bag_of_words.ipynb" rel="noopener ugc nofollow" target="_blank"> github </a>中找到所有代码。</p><p id="2132" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据以前的经验，我试图通过给出一个简短的描述来解决产品分类的问题。例如，给定“新鲜苹果”，预期类别是“水果”。仅仅通过使用计数发生方法已经能够有 80+的精确度<strong class="ki iu">。</strong></p><p id="64dd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，由于每个训练记录的字数只有几个字(从 2 个字到 10 个字)。使用单词嵌入可能不是一个好主意，因为没有<strong class="ki iu">的邻居</strong>(单词)来训练向量。</p><p id="00d6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一方面，scikit-learn 提供了其他参数来进一步调整模型输入。您可能需要了解以下特性</p><ul class=""><li id="e4ff" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">ngram_range:除了使用单个单词，还可以定义 ngram</li><li id="3106" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">二进制:除了计算出现次数，还可以选择二进制表示。</li><li id="5686" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">max_features:可以选择最大字数来减少模型的复杂性和大小，而不是使用所有的单词。</li></ul><p id="e2db" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，一些预处理步骤可以在上述库中执行，而不是自己处理。例如，停用字词删除、小写字母等。为了有更好的灵活性，我将使用我自己的代码来完成预处理步骤。</p><h1 id="9126" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">关于我</h1><p id="dce4" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以通过<a class="ae kf" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae kf" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae kf" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p></div></div>    
</body>
</html>