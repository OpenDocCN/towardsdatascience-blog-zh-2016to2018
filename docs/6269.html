<html>
<head>
<title>NIPS/NeurIPS 2018: Best* of the First Two Poster Sessions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NIPS/NeurIPS 2018:前两次海报会议的最佳*</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neurips-2018-reading-list-from-tue-poster-sessions-a-b-fce561e56be8?source=collection_archive---------13-----------------------#2018-12-04">https://towardsdatascience.com/neurips-2018-reading-list-from-tue-poster-sessions-a-b-fce561e56be8?source=collection_archive---------13-----------------------#2018-12-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/4343edba684cf720b8c8be5bd2ddcee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MhcsDC_2hD_xwUrX"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@rohanmakhecha?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Rohan Makhecha</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="c124" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">NeurIPS 是一个伟大的会议，吸引了机器学习研究几乎每个方面的最新技术。</p><p id="c33d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在会议中，该领域的研究人员当然应该注意一些事情。在我看来，这些东西会把类似的研究文章聚集成这些组。 <strong class="kf ir">理解，二。必需品，3。进步，4。大问题/未来</strong></p><p id="828b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，我把所有我认为有影响的事情归为这些类别。这些是 neur IPS 2018 A&amp;B 周二海报会议的海报及其摘要。</p><h1 id="1b7e" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">1.谅解</h1><h2 id="15b9" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7350-are-gans-created-equal-a-large-scale-study" rel="noopener ugc nofollow" target="_blank">甘人生而平等吗？大规模研究</a></h2><p id="e9db" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">回顾</em></p><p id="07c4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管大量的研究活动产生了许多有趣的 GAN 算法，但仍然很难评估哪些算法的性能优于其他算法。我们对最先进的模型和评估方法进行了中立的、多方面的大规模实证研究。我们发现，通过足够的超参数优化和随机重启，大多数模型都可以达到相似的分数。这表明，除了基本的算法变化之外，更高的计算预算和调整也能带来改进。为了克服当前度量的一些限制，我们还提出了几个数据集，在这些数据集上可以计算精度和召回率。我们的实验结果表明，未来的氮化镓研究应基于更系统和客观的评估程序。最后，我们没有发现任何测试算法始终优于在{ cite good fellow 2014 generate }中介绍的不饱和 GAN 的证据。</p><h2 id="c622" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/8169-an-intriguing-failing-of-convolutional-neural-networks-and-the-coordconv-solution" rel="noopener ugc nofollow" target="_blank">卷积神经网络的一个有趣的失败和 CoordConv 解决方案</a></h2><p id="1044" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">有意思，时间差不多了</em></p><p id="0b3b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们已经展示了 CNN 对坐标转换任务建模的奇怪无能，展示了 CoordConv 层形式的简单修复，并给出了表明包括这些层可以在广泛的应用中提高性能的结果。在 GAN 中使用 CoordConv 产生的模式崩溃更少，因为高级空间延迟和像素之间的转换变得更容易学习。在 MNIST 检测上训练的更快的 R-CNN 检测模型显示，当使用 CoordConv 时，IOU 提高了 24%,并且在强化学习(RL)领域中，玩 Atari 游戏的代理从 CoordConv 层的使用中显著受益。</p><h2 id="b24c" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7519-a-linear-speedup-analysis-of-distributed-deep-learning-with-sparse-and-quantized-communication" rel="noopener ugc nofollow" target="_blank">具有稀疏和量化通信的分布式深度学习的线性加速分析</a></h2><p id="0eb9" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">效率</em></p><p id="a454" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">巨大的通信开销已经成为分布式随机梯度下降(SGD)训练深度神经网络的性能瓶颈。先前的工作已经证明了使用梯度稀疏化和量化来降低通信成本的潜力。然而，对于稀疏和量化通信如何影响训练算法的收敛速度，仍然缺乏了解。本文研究了非凸优化的分布式 SGD 在两种减少通信量的策略下的收敛速度:稀疏参数平均和梯度量化。我们证明了如果稀疏化和量化超参数配置得当，可以达到<em class="mq"> O </em> (1/√ <em class="mq"> MK </em>)的收敛速度。我们还提出了一种称为周期量化平均(PQASGD)的策略，在保持<em class="mq"> O </em> (1/√ <em class="mq"> MK </em>)收敛速度的同时，进一步降低了通信开销。我们的评估验证了我们的理论结果，并表明我们的 PQASGD 可以像全通信 SGD 一样快地收敛，而通信数据大小仅为 3 %- 5%。</p><h2 id="b7f5" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding" rel="noopener ugc nofollow" target="_blank">关于单词嵌入的维度</a></h2><p id="03f9" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">授权</em></p><p id="a0c5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们提供了一个单词嵌入及其维度的理论理解。受单词嵌入酉不变性的启发，我们提出了一种新的度量单词嵌入不相似性的方法——成对内积损失。使用来自矩阵扰动理论的技术，我们揭示了在单词嵌入的维度选择中的基本偏差-方差权衡。这种偏差-方差的权衡揭示了许多以前无法解释的经验观察，例如最佳维度的存在。此外，揭示了新的见解和发现，如词嵌入何时以及如何对过拟合具有鲁棒性。通过优化 PIP 损失的偏差-方差权衡，我们可以明确地回答用于单词嵌入的维度选择的公开问题。</p><h2 id="84ff" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7647-adversarial-examples-that-fool-both-computer-vision-and-time-limited-humans" rel="noopener ugc nofollow" target="_blank">欺骗计算机视觉和限时人类的对立例子</a></h2><p id="e517" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">基本面</em></p><p id="c682" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">机器学习模型容易受到对抗性例子的影响:图像的微小变化可能导致计算机视觉模型出错，例如将校车识别为鸵鸟。然而，人类是否容易犯类似的错误仍然是一个悬而未决的问题。在这里，我们通过利用最近的技术来解决这个问题，这些技术将来自具有已知参数和架构的计算机视觉模型的对立示例转移到具有未知参数和架构的其他模型，并且通过匹配人类视觉系统的初始处理来解决这个问题。我们发现，在计算机视觉模型中强烈传递的对立例子会影响受时间限制的人类观察者所做的分类。</p><h2 id="d1f7" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/8089-dendritic-cortical-microcircuits-approximate-the-backpropagation-algorithm" rel="noopener ugc nofollow" target="_blank">树状皮质微电路近似反向传播算法</a></h2><p id="ebea" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">洞察</em></p><p id="6e9d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度学习在过去几年中取得了显著的发展，其中许多都是受神经科学的启发。然而，这些进步背后的主要学习机制——错误反向传播——似乎与神经生物学不一致。在这里，我们介绍了一个多层神经元网络模型，该模型具有简化的树突区室，其中错误驱动的突触可塑性使网络适应全局期望的输出。与以前的工作相比，我们的模型不需要单独的阶段，突触学习是由局部树突预测误差在时间上连续驱动的。这种错误起源于顶端树突，是由于来自外侧中间神经元的预测输入和来自实际自上而下反馈的活动之间的不匹配而发生的。通过使用简单的树突隔室和不同的细胞类型，我们的模型可以代表锥体神经元内的错误和正常活动。我们证明了该模型在回归和分类任务中的学习能力，并分析表明它近似于误差反向传播算法。此外，我们的框架与最近对大脑区域和皮质微电路结构之间学习的观察一致。总的来说，我们介绍了一种关于树突皮层回路学习的新观点，以及大脑如何解决长期存在的突触信用分配问题。</p><h2 id="19e3" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7999-on-neuronal-capacity" rel="noopener ugc nofollow" target="_blank">关于神经元容量</a></h2><p id="4b2e" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">新颖的提法</em></p><p id="d1aa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将学习机的能力定义为它能够实现的功能数量(或容量)的对数。我们回顾了已知的结果，并得出了新的结果，估计了几种神经元模型的容量:线性和多项式阈值门，带约束权重(二进制权重，正权重)的线性和多项式阈值门，以及 ReLU 神经元。我们还推导了完全递归网络和分层前馈网络的容量估计和界限。</p><h2 id="4518" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/8277-bias-and-generalization-in-deep-generative-models-an-empirical-study" rel="noopener ugc nofollow" target="_blank">深度生成模型中的偏差和泛化:一项实证研究</a></h2><p id="9a41" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">真正的理解</em></p><p id="d039" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在高维设置中，密度估计算法主要依赖于它们的归纳偏差。尽管最近取得了经验上的成功，但深度生成模型的归纳偏见并没有得到很好的理解。在本文中，我们提出了一个框架，通过使用精心设计的训练数据集探索学习算法，来系统地研究图像深度生成模型中的偏差和泛化。通过测量学习分布的属性，我们能够发现有趣的概括模式。我们验证这些模式在数据集、通用模型和架构之间是一致的。</p><h2 id="1843" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization" rel="noopener ugc nofollow" target="_blank">批处理规范化如何帮助优化？</a></h2><p id="c89a" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">视角</em></p><p id="7256" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">批处理规范化(BatchNorm)是一种广泛采用的技术，能够更快更稳定地训练深度神经网络(DNNs)。尽管 BatchNorm 无处不在，但它有效的确切原因仍然知之甚少。普遍认为，这种有效性源于在训练期间控制层的输入分布的变化，以减少所谓的“内部协变量移位”。在这项工作中，我们证明了层输入的这种分布稳定性与 BatchNorm 的成功几乎没有关系。相反，我们揭示了 BatchNorm 对训练过程的一个更基本的影响:它使优化前景明显更加平滑。这种平滑导致梯度的更具预测性和稳定性的行为，从而允许更快的训练。</p><h2 id="0fe2" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7938-a-probabilistic-population-code-based-on-neural-samples" rel="noopener ugc nofollow" target="_blank">基于神经样本的概率群体代码</a></h2><p id="5c73" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">神经编码</em></p><p id="e099" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感觉处理通常被描述为实现概率推理:神经元网络在给定感觉输入的情况下，对未观察到的原因计算后验信念。这些信念是如何通过神经反应计算和表示的，这是一个很有争议的问题(Fiser et al. 2010，Pouget et al. 2013)。一个主要的争论是关于神经反应是否代表潜在变量的样本(Hoyer &amp; Hyvarinnen 2003)或其分布的参数(Ma 等人 2006)的问题，并努力区分它们(Grabska-Barwinska 等人 2013)。一个单独的辩论解决了神经反应是否与编码概率成比例相关(Barlow 1969)，或者与这些概率的对数成比例(Jazayeri &amp; Movshon 2006，Ma 等人 2006，Beck 等人 2012)的问题。在这里，我们证明了这些选择——与通常的假设相反——并不是相互排斥的，而且同一个系统可以兼容所有的选择。作为一个主要的分析结果，我们表明，将 V1 区域的神经反应建模为来自图像的线性高斯模型中潜伏期的后验分布的样本，意味着这些神经反应形成了线性概率群体代码(PPC，m a 等人，2006)。特别地，在一些实验者定义的变量如“方向”上的后验分布是指数族的一部分，其具有在基于神经采样的发放率中呈线性的足够统计量。</p><h1 id="20a7" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">2.必需品；要素</h1><h2 id="90d2" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7957-training-deep-models-faster-with-robust-approximate-importance-sampling" rel="noopener ugc nofollow" target="_blank">通过稳健的近似重要性采样更快地训练深度模型</a></h2><p id="785e" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">解决方案</em></p><p id="0fbc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">理论上，重要性采样通过对训练样本进行优先排序来加速监督学习的随机梯度算法。实际上，计算重要性的成本极大地限制了重要性抽样的影响。我们提出了一种用于随机梯度去噪的鲁棒的近似重要性采样过程(RAIS)。通过使用稳健优化来近似理想的采样分布，RAIS 以大幅降低的开销提供了精确重要性采样的许多好处。根据经验，我们发现 RAIS-新加坡元和标准新加坡元遵循相似的学习曲线，但 RAIS 在这些路径上走得更快，实现了至少 20%的加速，有时甚至更多。</p><h2 id="e5a5" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7371-dropmax-adaptive-variational-softmax" rel="noopener ugc nofollow" target="_blank"> DropMax:自适应变分 Softmax </a></h2><p id="a8c7" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">优雅</em></p><p id="a40b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们提出了 DropMax，一个随机版本的 softmax 分类器，它在每次迭代中根据每个实例自适应决定的丢弃概率丢弃非目标类。具体来说，我们在类输出概率上覆盖二进制掩蔽变量，这是通过变分推理输入自适应学习的。这种随机正则化具有从具有不同决策边界的指数级分类器中构建集成分类器的效果。此外，对每个实例上的非目标类的辍学率的学习允许分类器更多地关注针对最混乱的类的分类。我们在多个用于分类的公共数据集上验证了我们的模型，在该数据集上，它获得了比常规 softmax 分类器和其他基线显著提高的准确性。对学习到的丢弃概率的进一步分析表明，我们的模型在执行分类时确实更经常地选择混淆类。</p><h2 id="13df" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7338-how-to-start-training-the-effect-of-initialization-and-architecture" rel="noopener ugc nofollow" target="_blank">如何开始训练:初始化和架构的效果</a></h2><p id="c4d8" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">效用</em></p><p id="0a8c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们识别并研究了两种常见的深层网络早期训练失败模式。对于每一种情况，我们都给出了严格的证明，说明在全连接、卷积和剩余架构中，这种情况何时发生以及如何避免。我们证明，第一种失效模式，即爆炸或消失的平均激活长度，可以通过初始化方差为 2/扇入的对称分布的权重来避免，对于 ResNets，可以通过正确缩放剩余模块来避免。我们证明，一旦避免了第一种失效模式，第二种失效模式，即激活长度的指数大方差，就不会出现在剩余网络中。相比之下，对于完全连接的网络，我们证明这种故障模式可能发生，并通过保持层宽度的倒数之和不变来避免。我们从经验上证明了我们的理论结果在预测网络何时能够开始训练方面的有效性。特别是，我们注意到许多流行的初始化不符合我们的标准，而正确的初始化和架构允许训练更深层次的网络。</p><h2 id="dbf3" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7481-regularizing-by-the-variance-of-the-activations-sample-variances" rel="noopener ugc nofollow" target="_blank">通过激活样本方差的方差进行正则化</a></h2><p id="678b" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">简单而不平凡</em></p><p id="10e7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">规范化技术在支持深度神经网络的高效且通常更有效的训练中起着重要作用。虽然传统的方法显式归一化的激活，我们建议添加一个损失项代替。这个新的损失项鼓励激活的方差保持稳定，而不是从一个随机小批量到下一个随机小批量变化。最后，我们能够将新的正则项与 batchnorm 方法联系起来，这为它提供了正则化的视角。我们的实验表明，对于 CNN 和全连接网络，在准确性上比 batchnorm 技术有所提高。</p><h2 id="c306" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/8242-mesh-tensorflow-deep-learning-for-supercomputers" rel="noopener ugc nofollow" target="_blank"> Mesh-TensorFlow:超级计算机的深度学习</a></h2><p id="7e4e" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">解决方案</em></p><p id="8fea" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">批处理分裂(数据并行)是主要的分布式深度神经网络(DNN)训练策略，因为它的普遍适用性及其对单程序多数据(SPMD)编程的适应性。然而，批量分割存在一些问题，包括无法训练非常大的模型(由于内存限制)、高延迟以及小批量时效率低下。所有这些都可以通过更通用的分布策略(模型并行)来解决。不幸的是，有效的模型并行算法往往难以发现、描述和实现，尤其是在大型集群上。我们介绍网格张量流，这是一种用于描述一般分布式张量计算的语言。在数据并行性可以被视为沿着“批处理”维度拆分张量和操作的情况下，在 Mesh-TensorFlow 中，用户可以指定要在多维处理器网格的任何维度上拆分的任何张量维度。一个网格张量流图编译成一个 SPMD 程序，该程序由并行操作和集体通信原语(如 Allreduce)组成。我们使用 Mesh-TensorFlow 来实现变压器序列到序列模型的高效数据并行、模型并行版本。使用多达 512 个核心的 TPU 网格，我们训练了多达 50 亿个参数的变压器模型，超过了 SOTA 在 WMT 的 14 个英语到法语翻译任务和 10 亿词语言建模基准上的结果。Mesh-Tensorflow 在 https://github.com/tensorflow/mesh<a class="ae kc" href="https://github.com/tensorflow/mesh" rel="noopener ugc nofollow" target="_blank">可用</a></p><h2 id="f3f7" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7287-structure-aware-convolutional-neural-networks" rel="noopener ugc nofollow" target="_blank">结构感知卷积神经网络</a></h2><p id="34d7" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">现实世界</em></p><p id="22f0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">卷积神经网络(CNN)固有地受制于不变的滤波器，该滤波器只能聚集具有相同拓扑结构的局部输入。这导致 CNN 被允许管理具有欧几里得或网格状结构的数据(例如，图像)，而不是具有非欧几里得或图形结构的数据(例如，交通网络)。为了扩大细胞神经网络的范围，我们开发了结构感知卷积来消除不变性，产生了处理欧几里德和非欧几里德结构化数据的统一机制。在技术上，结构感知卷积中的滤波器被推广到单变量函数，其能够聚集具有不同拓扑结构的局部输入。由于确定一个单变量函数需要无限个参数，我们在函数逼近理论的背景下用编号的可学习参数来参数化这些滤波器。通过用结构感知卷积代替细胞神经网络中的经典卷积，很容易建立结构感知卷积神经网络。在 11 个数据集上的大量实验有力地证明了 SACNNs 在各种机器学习任务上优于当前的模型，包括图像分类和聚类、文本分类、基于骨架的动作识别、分子活动检测和出租车流量预测。</p><h2 id="c10c" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets" rel="noopener ugc nofollow" target="_blank">可视化神经网络的损失情况</a></h2><p id="0569" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">视角</em></p><p id="6538" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">神经网络训练依赖于我们找到高度非凸损失函数的“好”极小值的能力。众所周知，某些网络架构设计(例如，跳过连接)产生更容易训练的损失函数，并且精心选择的训练参数(批量大小、学习率、优化器)产生更好地概括的最小化器。然而，这些差异的原因，以及它们对潜在损失状况的影响，并没有得到很好的理解。在本文中，我们探讨了神经损失函数的结构，以及损失景观对泛化的影响，使用了一系列可视化方法。首先，我们引入一个简单的“滤波归一化”方法，帮助我们可视化损失函数曲率，并在损失函数之间进行有意义的并排比较。然后，使用各种可视化，我们探索网络架构如何影响损失景观，以及训练参数如何影响最小化器的形状。</p><h2 id="bd2e" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7327-training-dnns-with-hybrid-block-floating-point" rel="noopener ugc nofollow" target="_blank">用混合块浮点训练 DNNs】</a></h2><p id="51a9" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">渴望解决</em></p><p id="0fa8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DNNs 的广泛采用催生了无情的计算需求，迫使数据中心运营商采用特定领域的加速器来培训他们。这些加速器通常采用密集的全精度浮点运算，以最大限度地提高单位面积的性能。正在进行的研究工作试图通过用定点运算取代浮点运算来进一步提高性能密度。然而，这些尝试的一个重大障碍是定点的狭窄动态范围，这对于 DNN 训练收敛来说是不够的。我们认为块浮点(BFP)是一种很有前途的替代表示法，因为它具有很宽的动态范围，并且能够用定点逻辑执行大多数 DNN 运算。不幸的是，BFP 单独引入了几个限制，妨碍了它的直接应用。在这篇文章中，我们介绍了 HBFP，一种混合的 BFP-FP 方法，它在 BFP 中执行所有的点积操作，在浮点中执行其他操作。HBFP 提供了两个世界的精华:浮点的高精度和定点的高硬件密度。对于各种型号，我们显示 HBFP 匹配浮点精度，同时支持硬件实施，提供高达 8.5 倍的高吞吐量。</p><h2 id="b775" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7408-frage-frequency-agnostic-word-representation" rel="noopener ugc nofollow" target="_blank"> FRAGE:频率不可知的单词表示法</a></h2><p id="e91d" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">有趣的</em></p><p id="2083" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">连续单词表示(也称为单词嵌入)是在自然语言处理任务中使用的许多基于神经网络的模型中的基本构件。尽管人们普遍认为语义相似的词在嵌入空间中应该彼此靠近，但我们发现在几个任务中学习到的词嵌入偏向于词频:高频词和低频词的嵌入位于嵌入空间的不同子区域，一个稀有词和一个流行词的嵌入即使语义相似也可能彼此远离。这使得学习的单词嵌入无效，特别是对于罕见的单词，因此限制了这些神经网络模型的性能。为了缓解这一问题，本文提出了一种简洁、简单而有效的对抗训练方法来模糊高频词和低频词嵌入之间的界限。我们在四个自然语言处理任务的十个数据集上进行了全面的研究，包括单词相似度、语言建模、机器翻译和文本分类。结果表明，我们在所有任务中都取得了比基线更高的性能。</p><h2 id="28fe" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7965-unsupervised-cross-modal-alignment-of-speech-and-text-embedding-spaces" rel="noopener ugc nofollow" target="_blank">语音和文本嵌入空间的无监督跨模态对齐</a></h2><p id="a47b" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">在核心处</em></p><p id="5ef0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最近的研究表明，从不同语言的文本语料库中学习的单词嵌入空间可以在没有任何并行数据监督的情况下对齐。受无监督跨语言单词嵌入成功的启发，本文的目标是在无监督方式下学习语音和文本嵌入空间之间的跨模态对齐。所提出的框架学习单独的语音和文本嵌入空间，并试图通过对抗训练来对齐这两个空间，随后是细化过程。我们展示了如何使用我们的框架来执行口语单词分类和翻译的任务，并且在这两个任务上的实验结果表明，我们的无监督对齐方法的性能与其有监督的方法相当。我们的框架对于为低资源或零资源语言开发自动语音识别(ASR)和语音到文本翻译系统特别有用，这些语言几乎没有用于训练现代监督 ASR 和语音到文本翻译模型的并行音频-文本数据，但占世界上使用的大多数语言。</p><h2 id="881a" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7886-compact-generalized-non-local-network" rel="noopener ugc nofollow" target="_blank">紧凑广义非局域网络</a></h2><p id="3e53" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">清洁</em></p><p id="50ff" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">非局部模块被设计用于捕获图像和视频中的长程时空依赖性。尽管它表现出了出色的性能，但它缺乏对跨通道位置之间的交互进行建模的机制，而这对于识别细粒度的对象和动作至关重要。为了解决这个限制，我们推广了非局部模块，并且考虑了任意两个通道的位置之间的相关性。该扩展利用了具有泰勒展开的多个核函数的紧凑表示，这使得广义非局部模块成为快速和低复杂度的计算流程。此外，我们在通道组内实现了我们的广义非局部方法以简化优化。实验结果表明，广义非局部模块在细粒度对象识别和视频分类方面都有明显的改进和实用性。代码可在:<a class="ae kc" href="https://github.com/KaiyuYue/cgnl-network.pytorch" rel="noopener ugc nofollow" target="_blank">https://github.com/KaiyuYue/cgnl-network.pytorch</a>获得。</p><h1 id="3cb1" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">3.进步</h1><h2 id="e8a6" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/8226-generalizing-point-embeddings-using-the-wasserstein-space-of-elliptical-distributions" rel="noopener ugc nofollow" target="_blank">使用椭圆分布的 Wasserstein 空间概括点嵌入</a></h2><p id="5368" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">拉伸</em></p><p id="d2b4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一种新的嵌入框架，它是数值灵活的，并且扩展了 wessserstein 空间中的点嵌入、椭圆嵌入。Wasserstein 椭圆嵌入更直观，并且产生比具有 Kullback-Leibler 散度的高斯嵌入的替代选择在数值上表现更好的工具。本文通过将椭圆嵌入用于可视化、计算词的嵌入以及反映蕴涵或上下级关系来展示椭圆嵌入的优点。</p><h2 id="67c9" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7356-fishnet-a-versatile-backbone-for-image-region-and-pixel-level-prediction" rel="noopener ugc nofollow" target="_blank">鱼网:图像、区域和像素级预测的多功能支柱</a></h2><p id="fbbb" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">基本面</em></p><p id="6277" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">设计卷积神经网络(CNN)结构以预测不同级别(例如图像级别、区域级别和像素级别)上的对象的基本原理是不同的。通常，专门为图像分类设计的网络结构被直接用作包括检测和分割在内的其他任务的默认主干结构，但是很少有主干结构是在考虑统一为像素级或区域级预测任务设计的网络的优点的情况下设计的，这些预测任务可能需要具有高分辨率的非常深的特征。朝着这个目标，我们设计了一个类似鱼的网络，称为鱼网。在 FishNet 中，所有分辨率的信息都被保留下来，并为最终任务进行优化。此外，我们观察到现有的工作仍然不能直接将梯度信息从深层传播到浅层。我们的设计可以更好的处理这个问题。已经进行了大量的实验来证明鱼网的卓越性能。特别是在 ImageNet-1k 上，FishNet 的精度能够以较少的参数超越 DenseNet 和 ResNet 的性能。渔网被用作 COCO Detection 2018 挑战赛获奖作品的模块之一。该代码可在 https://github.com/kevin-ssy/FishNet 的<a class="ae kc" href="https://github.com/kevin-ssy/FishNet" rel="noopener ugc nofollow" target="_blank">获得。</a></p><h2 id="f2b4" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks" rel="noopener ugc nofollow" target="_blank">通过自解释神经网络实现强大的可解释性</a></h2><p id="b5e1" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">没有副作用</em></p><p id="55ba" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最近关于复杂机器学习模型的可解释性的工作主要集中在围绕特定预测估计先前训练的模型的后验解释。自我解释模型的可解释性在学习过程中已经发挥了关键作用，但却很少受到关注。我们提出了一般解释的三个要求——明确性、忠实性和稳定性——并表明现有的方法不能满足它们。作为回应，我们分阶段设计自我解释模型，逐步将线性分类器推广到复杂但在架构上明确的模型。忠实性和稳定性是通过专门为这些模型定制的正则化来实现的。各种基准数据集的实验结果表明，我们的框架为协调模型复杂性和可解释性提供了一个有希望的方向。</p><h2 id="f939" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7960-relational-recurrent-neural-networks" rel="noopener ugc nofollow" target="_blank">关系递归神经网络</a></h2><p id="42cb" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">革命性</em></p><p id="e94a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基于记忆的神经网络通过利用长时间记忆信息的能力来模拟时态数据。然而，还不清楚他们是否也有能力用他们记忆的信息进行复杂的关系推理。在这里，我们首先确认我们的直觉，即标准内存架构可能会在大量涉及理解实体连接方式的任务中挣扎，即，涉及关系推理的任务。然后，我们通过使用一种新的内存模块(关系内存核心(RMC))来改善这些缺陷，这种内存模块采用多头点积注意力来允许内存进行交互。最后，我们在一组任务上测试了 RMC，这些任务可能受益于跨顺序信息的更有能力的关系推理，并显示了在 RL 域(BoxWorld &amp; Mini PacMan)、程序评估和语言建模方面的巨大收益，在 WikiText-103、Project Gutenberg 和 GigaWord 数据集上实现了最先进的结果。</p><h2 id="cc93" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7938-a-probabilistic-population-code-based-on-neural-samples" rel="noopener ugc nofollow" target="_blank">基于神经样本的概率人口代码</a></h2><p id="5c9f" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">寻求力量</em></p><p id="8485" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感觉处理通常被描述为实现概率推理:神经元网络在给定感觉输入的情况下，对未观察到的原因计算后验信念。这些信念是如何通过神经反应计算和表示的，这是一个很有争议的问题(Fiser et al. 2010，Pouget et al. 2013)。一个主要的争论是关于神经反应是否代表潜在变量的样本(Hoyer &amp; Hyvarinnen 2003)或其分布的参数(Ma 等人 2006)的问题，并努力区分它们(Grabska-Barwinska 等人 2013)。一个单独的辩论解决了神经反应是否与编码概率成比例相关(Barlow 1969)，或者与这些概率的对数成比例(Jazayeri &amp; Movshon 2006，Ma 等人 2006，Beck 等人 2012)的问题。在这里，我们证明了这些选择——与通常的假设相反——并不是相互排斥的，而且同一个系统可以兼容所有的选择。作为一个主要的分析结果，我们表明，将 V1 区域的神经反应建模为来自图像的线性高斯模型中潜伏期的后验分布的样本，意味着这些神经反应形成了线性概率群体代码(PPC，m a 等人，2006)。特别地，在一些实验者定义的变量如“方向”上的后验分布是指数族的一部分，其具有在基于神经采样的发放率中呈线性的足够统计量。</p><h2 id="23c1" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7381-neural-symbolic-vqa-disentangling-reasoning-from-vision-and-language-understanding" rel="noopener ugc nofollow" target="_blank">神经符号 VQA:从视觉和语言理解中解开推理</a></h2><p id="0f60" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">包罗万象</em></p><p id="e897" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们结合了两个强大的想法:用于视觉识别和语言理解的深度表示学习，以及用于推理的符号程序执行。我们的神经符号视觉问题回答(NS-VQA)系统首先从图像中恢复结构场景表示，并从问题中恢复程序轨迹。然后，它对场景表示执行程序以获得答案。将符号结构作为先验知识提供了三个独特的优势。首先，在符号空间上执行程序对于长程序跟踪更健壮；我们的模型可以更好地解决复杂的推理任务，在 CLEVR 数据集上达到 99.8%的准确率。第二，该模型在数据和内存方面效率更高:它在对少量训练数据进行学习后表现良好；它还可以将图像编码成紧凑的表示形式，比现有的离线问答方法需要更少的存储空间。第三，符号程序执行为推理过程提供了完全的透明性；因此，我们能够解释和诊断每个执行步骤。</p><h2 id="bafa" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/8206-neural-voice-cloning-with-a-few-samples" rel="noopener ugc nofollow" target="_blank">用少量样本克隆神经声音</a></h2><p id="ac8c" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">人工需要</em></p><p id="7fd8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">语音克隆是个性化语音界面非常需要的功能。我们介绍了一个神经声音克隆系统，它可以学习仅从几个音频样本中合成一个人的声音。我们研究两种方法:说话人适应和说话人编码。说话人自适应基于对多说话人生成模型的微调。说话人编码基于训练单独的模型来直接推断新的说话人嵌入，其将被应用于多说话人生成模型。在语音的自然性和与原说话人的相似性方面，两种方法都可以达到良好的性能，即使有一些克隆音频。虽然说话者自适应可以实现稍微更好的自然性和相似性，但是说话者编码方法的克隆时间和所需的内存明显更少，这使得它更有利于低资源部署。</p><h2 id="b595" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/8027-neural-arithmetic-logic-units" rel="noopener ugc nofollow" target="_blank">神经算术逻辑单元</a></h2><p id="7849" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">正常</em></p><p id="39a4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">神经网络可以学习表示和操作数字信息，但它们很少在训练期间遇到的数值范围之外进行推广。为了鼓励更系统的数值外推，我们提出了一种架构，将数值表示为线性激活，使用原始算术运算符操纵，由学习门控制。我们称这个模块为神经算术逻辑单元(NALU)，类似于传统处理器中的算术逻辑单元。实验表明，NALU 增强的神经网络可以学习跟踪时间，对数字图像执行算术，将数字语言翻译成实值标量，执行计算机代码，并对图像中的对象进行计数。与传统架构相比，我们在训练期间遇到的数值范围内外都获得了显著更好的概括，通常外推超出训练数值范围的数量级。</p><h1 id="3c93" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">4.大问题/未来</h1><h2 id="6d8d" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7473-embedding-logical-queries-on-knowledge-graphs" rel="noopener ugc nofollow" target="_blank">在知识图上嵌入逻辑查询</a></h2><p id="6239" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">少数民族方法</em></p><p id="e2a9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">学习知识图的低维嵌入是一种用于预测实体之间未观察到的或缺失的边的强大方法。然而，该领域的一个公开挑战是开发能够超越简单边缘预测并处理更复杂逻辑查询的技术，这可能涉及多个未观察到的边缘、实体和变量。例如，给定一个不完整的生物学知识图表，我们可能想要预测“什么药物可能针对与 X 和 Y 疾病都相关的蛋白质？”—需要对可能与疾病 X 和 y 相互作用的所有可能蛋白质进行推理的查询。这里，我们介绍了一个框架，以在不完整的知识图上有效地对合取逻辑查询进行预测，这是一个灵活但易于处理的一阶逻辑子集。在我们的方法中，我们在低维空间中嵌入图节点，并在该嵌入空间中将逻辑运算符表示为学习到的几何运算(例如，平移、旋转)。通过在低维嵌入空间内执行逻辑运算，我们的方法实现了与查询变量的数量成线性的时间复杂度，相比之下，基于简单枚举的方法需要指数复杂度。我们在两个对具有数百万关系的真实世界数据集的应用研究中展示了该框架的效用:预测药物-基因-疾病相互作用网络中的逻辑关系，以及来自流行网络论坛的基于图形的社会相互作用表示。</p><h2 id="e991" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7334-multi-task-learning-as-multi-objective-optimization" rel="noopener ugc nofollow" target="_blank">作为多目标优化的多任务学习</a></h2><p id="ed98" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">整个交易</em></p><p id="cf54" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在多任务学习中，多个任务被联合解决，在它们之间共享归纳偏差。多任务学习本质上是一个多目标的问题，因为不同的任务可能会发生冲突，需要进行权衡。一个常见的折衷方案是优化一个代理目标，使每个任务损失的加权线性组合最小化。但是，这种解决方法仅在任务不竞争时有效，这种情况很少发生。在本文中，我们明确地将多任务学习视为多目标优化，总目标是找到一个帕累托最优解。为此，我们使用在基于梯度的多目标优化文献中开发的算法。这些算法不能直接应用于大规模学习问题，因为它们与梯度的维度和任务的数量的比例很差。因此，我们提出了一个多目标损失的上限，并表明它可以有效地优化。我们进一步证明，在现实的假设下，优化这个上限会产生一个帕累托最优解。我们将我们的方法应用于各种多任务深度学习问题，包括数字分类、场景理解(联合语义分割、实例分割和深度估计)和多标签分类。我们的方法比最近的多任务学习公式或每任务训练产生更高性能的模型。</p><h2 id="583d" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated">RenderNet:一个深度卷积网络，用于 3D 形状的可区分渲染</h2><p id="c333" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">迫在眉睫</em></p><p id="3631" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">传统的计算机图形渲染流水线被设计用于以高性能从 3D 形状程序化地生成 2D 图像。由于离散操作(如可见性计算)导致的不可微性使得很难显式地将渲染参数与结果图像相关联，这给逆向渲染任务带来了巨大的挑战。最近关于可微绘制的工作通过为不可微操作设计代理梯度或者通过近似但可微的渲染器来实现可微性。然而，这些方法在处理遮挡时仍然受到限制，并且局限于特定的渲染效果。我们提出了 RenderNet，一个可区分的渲染卷积网络，它具有一个新颖的投影单元，可以从 3D 形状渲染 2D 图像。空间遮挡和阴影计算在网络中自动编码。我们的实验表明，RenderNet 可以成功地学习实现不同的着色器，并可以用于反向渲染任务，以从单幅图像中估计形状、姿态、光照和纹理。</p><h2 id="7fd9" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/8163-e-snli-natural-language-inference-with-natural-language-explanations" rel="noopener ugc nofollow" target="_blank"> e-SNLI:带有自然语言解释的自然语言推理</a></h2><p id="63b2" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">大梦想</em></p><p id="7bc3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了让机器学习获得广泛的公众采用，模型必须能够为其决策提供可解释的和可靠的解释，并在训练时从人类提供的解释中学习。在这项工作中，我们扩展了斯坦福自然语言推理数据集，增加了一层对蕴涵关系的人类注释的自然语言解释。我们进一步实现模型，将这些解释合并到它们的训练过程中，并在测试时输出它们。我们展示了我们的解释语料库(我们称之为 e-SNLI)如何用于各种目标，例如获得模型决策的完整句子证明，改进通用句子表示和转移到域外 NLI 数据集。因此，我们的数据集为使用自然语言解释开辟了一系列研究方向，既用于改进模型，也用于维护模型的可信度。</p><h2 id="e2e1" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7592-speaker-follower-models-for-vision-and-language-navigation" rel="noopener ugc nofollow" target="_blank">用于视觉和语言导航的说话者跟随模型</a></h2><p id="662c" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">未来</em></p><p id="a005" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由自然语言指令引导的导航为指令追随者提出了一个具有挑战性的推理问题。自然语言指令通常只识别一些高级决策和地标，而不是完整的低级运动行为；许多缺失的信息必须基于感知环境来推断。在机器学习设置中，这具有双重挑战性:很难收集足够的注释数据来从头开始学习这个推理过程，并且也很难使用通用序列模型来实现推理过程。在这里，我们描述了一种视觉和语言导航的方法，该方法利用嵌入式扬声器模型解决了这两个问题。我们使用这个说话者模型来(1)合成用于数据扩充的新指令，以及(2)实现语用推理，语用推理评估候选动作序列如何解释指令。这两个步骤都由全景动作空间支持，该空间反映了人类生成的指令的粒度。实验表明，这种方法的所有三个组成部分——说话者驱动的数据增强、语用推理和全景动作空间——都显著提高了基线指令跟随器的性能，在标准基准上比现有的最佳方法的成功率高出一倍以上。</p><h2 id="f860" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7617-neural-code-comprehension-a-learnable-representation-of-code-semantics" rel="noopener ugc nofollow" target="_blank">神经代码理解:代码语义的可学习表示</a></h2><p id="b4b9" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">炼金术</em></p><p id="c2c3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着嵌入在自然语言处理中最近的成功，已经进行了将类似方法应用于代码分析的研究。大多数作品试图直接处理代码或使用句法树表示法，将其视为用自然语言编写的句子。然而，由于诸如函数调用、分支和可互换的语句顺序等结构特征，现有的方法都不足以鲁棒地理解程序语义。在本文中，我们提出了一种新的处理技术来学习代码语义，并将其应用于各种程序分析任务。特别是，我们规定一个健壮的代码分布假设适用于人和机器生成的程序。根据这个假设，我们基于独立于源编程语言的代码的中间表示(IR)来定义嵌入空间 inst2vec。我们为这个 IR 提供了一个新颖的上下文流定义，利用了程序的底层数据流和控制流。然后，我们使用类比和聚类对嵌入进行定性分析，并评估在三个不同的高级任务上学习到的表征。我们表明，即使没有微调，单个 RNN 架构和固定的 inst2vec 嵌入也优于专门的性能预测方法(计算设备映射、最佳线程粗化)；以及来自原始代码(104 个类)的算法分类，其中我们设置了一个新的最先进的状态</p><h2 id="0b2d" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/8068-generalisation-of-structural-knowledge-in-the-hippocampal-entorhinal-system" rel="noopener ugc nofollow" target="_blank">海马-内嗅系统结构知识的概括</a></h2><p id="9159" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">登月</em></p><p id="de6b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">理解智力的一个中心问题是概括的概念。这使得先前学习的结构被用来解决特殊情况下的新任务。我们从神经科学中获得灵感，特别是已知对归纳很重要的海马-内嗅系统。我们建议，为了概括结构知识，世界结构的表示，即世界中的实体如何相互联系，需要与实体本身的表示分开。我们表明，在这些原则下，嵌入了层次结构和快速 Hebbian 记忆的人工神经网络可以学习记忆的统计和概括结构知识。反映大脑中发现的空间神经元表现出现，表明空间认知是更普遍的组织原则的一个实例。我们进一步统一了许多内嗅细胞类型作为构建转换图的基函数，并表明这些表示有效地利用了记忆。我们实验性地支持模型假设，显示了跨环境的内嗅网格和海马位置细胞之间保存的关系。</p><h2 id="e03a" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated">你想去哪里？:从行为中推断关于动力学的信念</h2><p id="da76" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">内部构件</em></p><p id="7ed3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在贝叶斯逆规划和逆强化学习的框架内，从观察到的行为推断意图已经得到了广泛的研究。这些方法推断出一个目标或奖励函数，它能最好地解释被观察主体的行为，通常是一个人类演示者。另一个代理可以使用这个推断的意图来预测、模仿或帮助人类用户。然而，逆向强化学习的一个核心假设是，演示器接近最优。虽然存在次优行为模型，但它们通常假设次优行为是某种随机噪声或已知认知偏差(如时间不一致性)的结果。在本文中，我们采用了另一种方法，将次优行为建模为内部模型错误设定的结果:用户行为可能偏离近优行为的原因是用户对控制行为如何影响环境的规则(动态)有一套不正确的信念。我们的见解是，虽然演示的动作在现实世界中可能是次优的，但就用户的内部动态模型而言，它们实际上可能是接近最优的。通过从观察到的行为中评估这些内在信念，我们得到了一种推断意图的新方法。我们在模拟和 12 名参与者的用户研究中证明，这种方法使我们能够更准确地模拟人类意图，并可用于各种应用，包括在共享自治框架中提供帮助和推断人类偏好。</p><h2 id="a84e" class="lz lc iq bd ld ma mb dn lh mc md dp ll ko me mf lp ks mg mh lt kw mi mj lx mk bi translated"><a class="ae kc" href="http://papers.nips.cc/paper/7775-task-driven-convolutional-recurrent-models-of-the-visual-system" rel="noopener ugc nofollow" target="_blank">视觉系统的任务驱动卷积递归模型</a></h2><p id="ee9d" class="pw-post-body-paragraph kd ke iq kf b kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><em class="mq">大问题/未来</em></p><p id="fc2c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">前馈卷积神经网络(CNN)目前是用于对象分类任务(如 ImageNet)的最新技术。此外，它们是灵长类大脑视觉系统中神经元的时间平均响应的定量精确模型。然而，生物视觉系统有两个普遍存在的结构特征，与典型的 CNN 不同:皮质区域内的局部复发，以及从下游区域到上游区域的远程反馈。这里我们探讨了递归在提高分类性能中的作用。我们发现在 ImageNet 任务中，标准形式的递归(普通 RNNs 和 LSTMs)在深度 CNN 中表现不佳。相比之下，结合了旁路和门控两种结构特征的新型细胞能够大幅提高任务的准确性。我们在数以千计的模型架构的自动搜索中扩展了这些设计原则，这些架构识别了对对象识别有用的新的局部循环细胞和远程反馈连接。此外，这些任务优化的神经网络比前馈网络更好地匹配了灵长类动物视觉系统中的神经活动动力学，这表明了大脑的循环连接在执行困难的视觉行为中的作用。</p></div></div>    
</body>
</html>