<html>
<head>
<title>Explainable AI vs Explaining AI — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释的人工智能与解释人工智能——第一部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explainable-ai-vs-explaining-ai-part-1-d39ea5053347?source=collection_archive---------13-----------------------#2018-12-21">https://towardsdatascience.com/explainable-ai-vs-explaining-ai-part-1-d39ea5053347?source=collection_archive---------13-----------------------#2018-12-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="17f1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深度学习够深吗？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ab313b43426bc3f92793201923397374.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7kURdm4D0gr1JDuJ-Qa-dw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Framework of Explainable Deep Learning, source Society of Mind (Marvin Minskey)</figcaption></figure><p id="6e18" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尽管最近深度学习(DL)取得了显著的成果，但由于欠拟合、过拟合或不完整的训练数据等几个原因，它总是存在产生妄想和不切实际的结果的风险。例如，职业围棋选手 Lee Sedol 的著名棋步 78 导致了 Alpha Go 的妄想行为、对抗性攻击以及 DeepXplore 在 Nvidia DAVE-2 自动驾驶汽车平台中发现的错误行为，其中系统决定对同一输入图像做出两种不同的决定，唯一的区别是亮度级别。这些例子推动人工智能研究人员更加专注于打开深度学习的黑匣子，避免只依赖模型的准确性。</p><p id="1dbf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">前一段时间，我在为非人工智能职业准备一个关于人工智能状态的研讨会。由于人工智能目前是一个流行词，并且流行词通常有很多定义，我不得不选择一个。人工的定义很清楚，但是智能呢？我搜索并找到了马文·明斯基对人工智能最合理和相关的定义:</p><blockquote class="lu"><p id="107e" class="lv lw it bd lx ly lz ma mb mc md lt dk translated">我们的<strong class="ak">模型</strong>包含<strong class="ak">过程</strong>，使我们能够解决我们认为<strong class="ak">困难</strong>的问题。</p><p id="b1c0" class="lv lw it bd lx ly lz ma mb mc md lt dk translated">“黑盒”是那些我们还不了解的过程的名称。</p></blockquote><p id="0de8" class="pw-post-body-paragraph ky kz it la b lb me ju ld le mf jx lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">实际上，这是这个定义的修改版本，我用“黑盒”代替了“智能”，用“模型”代替了“头脑”虽然我已经改变了它，但定义仍然成立。</p><p id="a7c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些论点告诉我们这些事情:a)黑盒和智能是一样的，b)复杂的问题需要复杂的解决方案，c)简单、可理解的过程很可能不适合复杂的问题，如自动驾驶汽车和机器翻译，以及 d)盒子越暗，它就越智能。</p><blockquote class="lu"><p id="fc76" class="lv lw it bd lx ly lz ma mb mc md lt dk translated">因此，当我们要求深度学习科学家打开黑盒时，这可能意味着限制模型能力。</p></blockquote><p id="a284" class="pw-post-body-paragraph ky kz it la b lb me ju ld le mf jx lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">但是真的吗？</p><p id="43bf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">人类大脑中的大多数过程都是模糊的，我们不理解它们，这是正确的，但我们仍然可以解释我们的决定和想法。我们的解释通常由陈述和理由组成。我们的讲解不涉及统计推断分析(除非和题目有关)。</p><blockquote class="lu"><p id="b891" class="lv lw it bd lx ly lz ma mb mc md lt dk translated">所以问题是:我们如何解释或决定？</p></blockquote><p id="5c59" class="pw-post-body-paragraph ky kz it la b lb me ju ld le mf jx lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">人类的大脑由两个不同的系统组成，大脑用这两个系统来形成思想。这两个系统是:</p><p id="1cac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">系统 1: </strong>是一种快速、直观、无意识和情绪化的、刻板的、自动的，并利用与过去经验的相似性来做出决定</p><p id="dd84" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">系统 2: 是一个缓慢的、有意识的、有逻辑的、努力的系统，使用高级推理来做出决策</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mk"><img src="../Images/8c77af9e2d5a9145cf7bdbde2e264a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o__4FykMIU_gwfv3PagX0A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Two systems of thinking</figcaption></figure><p id="8e93" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">系统 1 </strong>做出不需要太多注意力和常识的自动决定，比如走路、拿着物体、理解简单的句子或者在高速公路上开车。<strong class="la iu">系统 2 </strong>进行高级推理，这需要常识，比如理解法律条款或在拥挤的城市中驾驶汽车，这不仅需要车道间驾驶的知识，还需要城市内交通规则和人为因素的知识。</p><blockquote class="lu"><p id="59f5" class="lv lw it bd lx ly lz ma mb mc md lt dk translated">那么接下来的问题是:系统 1 和系统 2 在 AI 中代表什么？</p></blockquote><p id="d613" class="pw-post-body-paragraph ky kz it la b lb me ju ld le mf jx lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">1988 年，马文·明斯基出版了他的书《心智的社会》。这本书最有趣的部分之一是在人脑中表现知识的框架。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ml"><img src="../Images/c82241c7129613358d6986220a27b848.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*utHAn-8C8ebDEO2XruGP8g.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Framework for representing knowledge, source Society of Mind (Marvin Minskey)</figcaption></figure><p id="dc5e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">大脑中的知识由七层组成(忽略小词，输入层)。考虑到最近的人工智能技术，我将对它们的功能解释如下:</p><ol class=""><li id="9be7" class="mm mn it la b lb lc le lf lh mo ll mp lp mq lt mr ms mt mu bi translated"><strong class="la iu">神经网络:</strong>代表 ANN/DL。这一层的主要目标是避免维数灾难，并构建一个高层次的分布式/非纠缠表示。这一层代表大脑最直观的部分(系统 1)。这是刻板和自动的。学习缓慢而艰难，解释困难</li><li id="6b76" class="mm mn it la b lb mv le mw lh mx ll my lp mz lt mr ms mt mu bi translated"><strong class="la iu"> K 线和 k 线树:</strong>马文·明斯基把 k 线看做记忆线。但我提出 k 线层代表的是所谓的归纳编程。为解决某些问题而学习的一系列程序。这一层是系统 1 和系统 2 之间的第一个桥梁。它比 NN 层更有逻辑性，使用的推理更多。更容易学习和解释。</li><li id="0a79" class="mm mn it la b lb mv le mw lh mx ll my lp mz lt mr ms mt mu bi translated"><strong class="la iu">语义网络:</strong>表示知识图、语义网或本体。它再次更接近系统 2 的方向。通过断开或连接两个事实(节点),一次性学习变得更加容易。</li><li id="0fa2" class="mm mn it la b lb mv le mw lh mx ll my lp mz lt mr ms mt mu bi translated"><strong class="la iu">框架</strong>:框架是一种数据结构，具有关于特定对象或概念的典型知识。它和第三层一样，但是有更多的解释和更容易学习</li><li id="a18d" class="mm mn it la b lb mv le mw lh mx ll my lp mz lt mr ms mt mu bi translated"><strong class="la iu">常识台词</strong>:这里我把最后三层都合并成了一层。这些层的目标是将不同的领域连接在一起，形成常识和常识，例如，在城市中驾驶汽车。它和第四层一样，但是有更多的解释和更容易学习</li></ol><p id="8234" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">考虑到以上几点，我们得出以下结论:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ab313b43426bc3f92793201923397374.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7kURdm4D0gr1JDuJ-Qa-dw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Framework of Explanation Deep Learning, source Society of Mind (Marvin Minskey)</figcaption></figure><ol class=""><li id="3972" class="mm mn it la b lb lc le lf lh mo ll mp lp mq lt mr ms mt mu bi translated">我们所谓的深度学习实际上还没有深到可以解释的程度</li><li id="d6ef" class="mm mn it la b lb mv le mw lh mx ll my lp mz lt mr ms mt mu bi translated">我们所说的深度学习实际上还不足以执行一次性学习</li><li id="d41d" class="mm mn it la b lb mv le mw lh mx ll my lp mz lt mr ms mt mu bi translated">实现可解释人工智能的关键是缩小四层知识之间的差距</li><li id="d3f2" class="mm mn it la b lb mv le mw lh mx ll my lp mz lt mr ms mt mu bi translated">通过弥合这些差距，我们实现了一个可以使用系统 2 解释自己的人工智能，同时它在直观的系统 1 中保持复杂的解决方案</li><li id="9be5" class="mm mn it la b lb mv le mw lh mx ll my lp mz lt mr ms mt mu bi translated">通过弥合这些差距，我们实现了一个可以使用系统 1 解决直观体验问题，并可以使用系统 2 概括和解决推理问题的人工智能</li><li id="5f98" class="mm mn it la b lb mv le mw lh mx ll my lp mz lt mr ms mt mu bi translated">解释人工智能的目标不仅是建立信任，而且是提高性能和实现人工智能的一般智能</li></ol><p id="d079" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本系列文章的下一部分中，我将深入研究旨在弥补上述框架缺陷的最新技术。从使用<a class="ae mj" href="https://arxiv.org/abs/1602.04938" rel="noopener ugc nofollow" target="_blank">时间</a>或<a class="ae mj" href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions" rel="noopener ugc nofollow" target="_blank">形状</a>来解释系统 1 的决策开始，通过最近的神经符号研究和<a class="ae mj" href="https://www.doc.ic.ac.uk/~shm/ilp.html" rel="noopener ugc nofollow" target="_blank">归纳逻辑编程</a>，由 Yoshua Bengio 解决<a class="ae mj" href="https://arxiv.org/abs/1709.08568" rel="noopener ugc nofollow" target="_blank">意识先验</a>，最后如何建立一个解释 AI 模型而不是解释 AI 模型。</p><p id="e15f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">敬请期待！</p></div></div>    
</body>
</html>