<html>
<head>
<title>Machine Learning Basics — Part 1 — Concept of Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习基础—第1部分—回归的概念</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-basics-part-1-concept-of-regression-31982e8d8ced?source=collection_archive---------3-----------------------#2018-02-23">https://towardsdatascience.com/machine-learning-basics-part-1-concept-of-regression-31982e8d8ced?source=collection_archive---------3-----------------------#2018-02-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/71e1e82be5536f02047425fcd0386101.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Owad7V5jYlcUTpOF."/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by Andre Benz on Unsplash — <a class="ae kf" href="https://unsplash.com/photos/cXU6tNxhub0" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/cXU6tNxhub0</a></figcaption></figure><p id="182e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我重温了Andre Ng在coursera 上的《神奇的<a class="ae kf" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习课程》中的学习材料，并对这些概念做了一个概述。除非另有明确说明，否则所有引用都是指本课程的材料。</a></p><h1 id="2fb8" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">目录</h1><ul class=""><li id="a722" class="mc md it ki b kj me kn mf kr mg kv mh kz mi ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#definition" rel="noopener ugc nofollow" target="_blank">定义</a></li><li id="e3a4" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#linear-regression-with-one-variable" rel="noopener ugc nofollow" target="_blank">一元线性回归</a></li><li id="44d5" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#model-representation" rel="noopener ugc nofollow" target="_blank">模型表示</a></li><li id="6a4e" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#cost-function" rel="noopener ugc nofollow" target="_blank">成本函数</a></li><li id="0997" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#gradient-descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a></li><li id="b5e0" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#linear-regression-with-multiple-variables" rel="noopener ugc nofollow" target="_blank">多元线性回归</a></li><li id="8adc" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#feature-scaling-and-mean-normalization" rel="noopener ugc nofollow" target="_blank">特征缩放和均值归一化</a></li><li id="f325" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#learning-rate" rel="noopener ugc nofollow" target="_blank">学习率</a></li><li id="372a" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#polynomial-regression" rel="noopener ugc nofollow" target="_blank">多项式回归</a></li><li id="9d2b" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#normal-equation-for-analytical-computing" rel="noopener ugc nofollow" target="_blank">正规方程(用于分析计算)</a></li><li id="656a" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#logistic-regression" rel="noopener ugc nofollow" target="_blank">逻辑回归</a></li><li id="3712" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#classification" rel="noopener ugc nofollow" target="_blank">分类</a></li><li id="eeb7" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#adapted-cost-function-and-gradient-descent" rel="noopener ugc nofollow" target="_blank">适应的成本函数和梯度下降</a></li><li id="4fe8" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#alternatives-to-gradient-descent" rel="noopener ugc nofollow" target="_blank">梯度下降的替代方案</a></li><li id="0c66" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#multiclass-classification" rel="noopener ugc nofollow" target="_blank">多类分类</a></li><li id="2ac4" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#problem-of-overfitting-and-the-use-of-regularization" rel="noopener ugc nofollow" target="_blank">过拟合问题和正则化的使用</a></li><li id="11f2" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/DDCreationStudios/Writing/blob/master/2018/articles/MLIntroP1.md#regularization" rel="noopener ugc nofollow" target="_blank">正规化</a></li></ul><h1 id="7fba" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">定义</h1><blockquote class="ms"><p id="3a70" class="mt mu it bd mv mw mx my mz na nb ld dk translated"><em class="nc">如果计算机程序在T中的任务上的性能(如P所测量的)随着经验E而提高，则称该程序从关于某类任务T和性能测量P的经验E中学习。—汤姆·米切尔</em></p></blockquote><h1 id="48a5" class="le lf it bd lg lh li lj lk ll lm ln lo lp nd lr ls lt ne lv lw lx nf lz ma mb bi translated">一元线性回归</h1><h1 id="d54a" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">模型表示</h1><p id="54c5" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">线性回归试图将点拟合到由算法生成的直线上。该优化线(模型)能够预测某些输入值的值，并且可以绘制出来。</p><h1 id="7f64" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">价值函数</h1><p id="b37b" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">我们希望设置参数，以实现预测值和实际值之间的最小差异。</p><blockquote class="nj nk nl"><p id="65b9" class="kg kh nm ki b kj kk kl km kn ko kp kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated"><em class="it">我们可以通过使用成本函数来衡量假设函数的准确性。这是假设的所有结果与x的输入和y的实际输出的平均差(实际上是一个更好的平均版本)。</em></p></blockquote><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nq"><img src="../Images/65a6a083558ce602a640c20208d6ca45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*N8WrDRbMOQXXjWOD.png"/></div></div></figure><h1 id="c86c" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">梯度下降</h1><p id="7fc7" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">梯度下降不断改变参数以逐渐降低成本函数。随着每一次迭代，我们将更接近最小值。每次迭代时，参数必须同时调整！“步长”/迭代的大小由参数α(学习速率)决定。</p><blockquote class="nj nk nl"><p id="4973" class="kg kh nm ki b kj kk kl km kn ko kp kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated"><em class="it">我们这样做的方法是对成本函数求导(函数的切线)。切线的斜率是该点的导数，它会给我们一个前进的方向。我们沿着下降速度最快的方向逐步降低成本函数。</em></p></blockquote><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4f0342e3443c8ad09466905d210fa790.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/0*mv3Azt83Ny38xKqV.png"/></div></figure><p id="4513" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">选择α的值至关重要。如果它太小，算法将很慢，如果它太大，它将无法收敛。</p><p id="6c73" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当具体应用于线性回归的情况时，可以导出新形式的梯度下降方程，其中m是训练集的大小。同样，两个参数必须同时更新。</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/5bfdb03f27b574ffbbabc690d9825506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/0*y9v-x2rHnNtFRstL.png"/></div></figure><blockquote class="nj nk nl"><p id="8dd5" class="kg kh nm ki b kj kk kl km kn ko kp kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated"><em class="it">注意，虽然梯度下降通常易受局部极小值的影响，但我们在此提出的线性回归优化问题只有一个全局最优值，没有其他局部最优值；因此梯度下降总是收敛(假设学习率α不太大)到全局最小值。</em></p></blockquote><h1 id="339f" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">多元线性回归</h1><p id="c292" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">现在，我们有多个特征/变量，而不是一个特征/变量负责某个结果。</p><p id="6165" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，假设相应地改变，并考虑多个参数。这同样适用于梯度下降。它只是附加参数的扩展，这些参数必须更新。</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/572b3795a31bfad0bfab5c100b050bda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fshi_G_k5CBiC90v.png"/></div></div></figure><h1 id="103b" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">特征缩放和均值归一化</h1><p id="7c47" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">要确保所有的要素值都在相同的范围内并具有相同的平均值，有必要使用要素缩放和平均值归一化。</p><blockquote class="nj nk nl"><p id="dc7b" class="kg kh nm ki b kj kk kl km kn ko kp kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated"><em class="it">特征缩放包括将输入值除以输入变量的范围(即最大值减去最小值)，从而得到一个仅为1的新范围。均值归一化包括从某个输入变量的值中减去该输入变量的平均值，从而得出该输入变量的新平均值正好为零。</em></p></blockquote><h1 id="6987" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">学习率</h1><p id="fbac" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">为了选择一个合适的学习率，必须绘制梯度下降图并进行“调试”。</p><blockquote class="nj nk nl"><p id="a814" class="kg kh nm ki b kj kk kl km kn ko kp kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated"><em class="it">在x轴上绘制迭代次数的曲线图。现在绘制梯度下降迭代次数的成本函数J(θ)。如果J(θ)增加，那么你可能需要减少α。</em></p></blockquote><p id="226f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果J(0)在迭代步骤中停止显著下降，则可以宣布收敛。</p><h1 id="c7ba" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">多项式回归</h1><p id="79ec" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">可以通过将假设函数重新定义为二次、三次或平方根函数来改善特征。</p><p id="e2fc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，必须特别强调特征缩放！</p><h1 id="799c" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">正规方程(用于分析计算)</h1><p id="a41d" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">法线方程将导数设置为零，而不是使用梯度下降来逐渐最小化成本函数。</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8a9f71c338e3c133e04bb9af68b12ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/0*r_3QqXHZ-RPeZTkJ.png"/></div></figure><p id="fe02" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正规方程不需要学习率α，根本不需要迭代，但需要设计矩阵的转置。当您有大量的要素(例如10000)时，计算将比梯度下降的迭代过程花费更长的时间。为了提高法方程算法的质量，应该正则化特征并删除冗余特征。</p><h1 id="264b" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">逻辑回归</h1><h1 id="5826" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">分类</h1><p id="a33f" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">为了对数据进行分类，结果应该是0或1(二进制分类)。从回归的角度来看，这可能意味着将大于等于0.5的输出分类为1，将小于0.5的输出分类为0(而0.5是决策界限)。</p><p id="c11e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用logistic/sigmoid函数，修改后的假设现在是:</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/e537e8dbc8bbf2e914e3b46f4cb1180d.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/0*ohGxIwlhPPR_tdBN.png"/></div></figure><p id="d934" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它返回输出为1的概率！</p><h1 id="9348" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">适应的成本函数和梯度下降</h1><p id="dc6a" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">由于使用了sigmoid函数，因此必须通过使用对数来相应地调整成本函数。因为现在的目标不是最小化与预测值的距离，而是最小化假设的输出与y (0或1)之间的距离。</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oa"><img src="../Images/60b14955cc5a1370128dee44c3a11b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*C3KZYVyd0WsD_GNK.png"/></div></div></figure><p id="f081" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者对于矢量化实现:</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ob"><img src="../Images/fdf6d02a3802842294eb6d89e7d968e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7OlT5aXYIvdfFwWg.png"/></div></div></figure><p id="443d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，梯度下降保持不变，因为公式使用了假设的导数部分！</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/cb312f3c0b20caaeea477d177ea19244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/0*m29Ou-Ab98hCtW2i.png"/></div></figure><p id="2c30" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者对于矢量化实现:</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/897cc71ba62c67e05a0b0fe1cfab25d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/0*2QB97N8pMceqLl_o.png"/></div></figure><h1 id="1f3b" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">梯度下降的替代方案</h1><p id="c557" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">更复杂的优化算法，如</p><ul class=""><li id="5db6" class="mc md it ki b kj kk kn ko kr oe kv of kz og ld mj mk ml mm bi translated">共轭梯度，</li><li id="dccb" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated">BFGS或</li><li id="db17" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated">左旋BFGS</li></ul><p id="a8c5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常允许更快的计算，而不需要选择学习速率α。</p><h1 id="8445" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">多类分类</h1><p id="c505" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">先前描述的分类问题解决仅适用于二元分类。具有多于n=2的可能结果称为多类分类。为了在多个类别上应用该概念，使用了“一个对所有”方法，其本质上是在每个类别上应用二元分类(一个类别是正面的，所有其余的是负面的)。不是将y设置为0或1，而是将y设置为I，这本身是针对所有其他类进行测试的。基本上这个过程是双重的:</p><ol class=""><li id="9942" class="mc md it ki b kj kk kn ko kr oe kv of kz og ld oh mk ml mm bi translated">将逻辑分类器设置为y。(如果y是3，我们创建3个分类器)</li><li id="413c" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld oh mk ml mm bi translated">针对所有分类器测试新输入，并选择概率最高的分类器。</li></ol><h1 id="5684" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">过拟合问题和正则化的使用</h1><p id="1daf" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">在过度拟合的情况下，模型完美地捕获了数据结构，而在欠拟合的情况下，模型没有捕获足够的数据结构(即模型的图形几乎不接触所有的数据点)。</p><p id="31d7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了解决过度拟合的问题，可以减少特征或者调整它们的值的大小。</p><h1 id="b611" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">正规化</h1><p id="ae51" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">为了正则化模型，必须将参数(λ)添加到成本函数中。它减小或增大了参数θ。</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/a560026e4c59b4f5447e8b904679b47f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_7wgYykBJpFQ9fIV.png"/></div></div></figure><p id="d7e5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，将其应用于逻辑回归看起来像这样:</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oj"><img src="../Images/93485325ede24addfba275dc50e21d6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uqf_amgvewNuZI_h.png"/></div></div></figure><p id="e663" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意正则化参数如何从1开始，而不是正则化<a class="ae kf" href="https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks" rel="noopener ugc nofollow" target="_blank">偏置项θ0</a>。</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ok"><img src="../Images/21e7cb555250df840dd93db0e7259015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*66oYiYoGNH-I_O00.png"/></div></div></figure></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><p id="dab4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就结束了第一部分。在下一篇中，将描述神经网络。敬请期待！</p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><h1 id="ba65" class="le lf it bd lg lh os lj lk ll ot ln lo lp ou lr ls lt ov lv lw lx ow lz ma mb bi translated">关于</h1><p id="6c41" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">丹尼尔是一名企业家、软件开发人员和商业法毕业生。他曾在各种IT公司、税务咨询、管理咨询和奥地利法院工作。</p><p id="b61b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">他的知识和兴趣目前围绕着编程机器学习应用程序及其所有相关方面。从本质上说，他认为自己是复杂环境的问题解决者，这在他的各种项目中都有所体现。</p><p id="9559" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您有想法、项目或问题，请不要犹豫与我们联系。</p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ox"><img src="../Images/943d356762b8c79422b66d1fb3834281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*s1rQuRxV2gi5GxAK.png"/></div></div></figure><p id="bc0d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在https://www.buymeacoffee.com/createdd<a class="ae kf" href="https://www.buymeacoffee.com/createdd" rel="noopener ugc nofollow" target="_blank">上支持我</a></p><figure class="nr ns nt nu gt ju gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/b8920bc2120be70b0ab1a219b3a0e37c.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/0*LFnDIxDnx0ratxEG"/></div></figure><p id="109b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">连接到:</p><ul class=""><li id="af5f" class="mc md it ki b kj kk kn ko kr oe kv of kz og ld mj mk ml mm bi translated"><a class="ae kf" href="https://www.linkedin.com/in/createdd" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a></li><li id="2db1" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://github.com/Createdd" rel="noopener ugc nofollow" target="_blank"> Github </a></li><li id="1d00" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://medium.com/@createdd" rel="noopener">中等</a></li><li id="4799" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://twitter.com/_createdd" rel="noopener ugc nofollow" target="_blank">推特</a></li><li id="8471" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://www.instagram.com/create.dd/" rel="noopener ugc nofollow" target="_blank"> Instagram </a></li><li id="8249" class="mc md it ki b kj mn kn mo kr mp kv mq kz mr ld mj mk ml mm bi translated"><a class="ae kf" href="https://www.createdd.com/" rel="noopener ugc nofollow" target="_blank">createdd.com</a></li></ul></div></div>    
</body>
</html>