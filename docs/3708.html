<html>
<head>
<title>Gradient Descent — Demystified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降——去神秘化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-demystified-bc30b26e432a?source=collection_archive---------2-----------------------#2018-06-10">https://towardsdatascience.com/gradient-descent-demystified-bc30b26e432a?source=collection_archive---------2-----------------------#2018-06-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d429a9ba579b6f80490417d8f7b80d75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-_HQiV9C51FfPH7r7yLDXw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Types of Gradient Descent Algorithms</figcaption></figure><h1 id="741d" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">在本文中，我将详细讨论梯度下降，解释除标准梯度下降算法之外的不同优化算法。</h1><p id="b7c7" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">为了讨论各种梯度下降算法，我将使用逻辑回归的损失函数作为要优化的“<em class="ly"/>”函数。</p><p id="4c61" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">对于那些不知道逻辑回归的人来说，</p><blockquote class="me mf mg"><p id="c119" class="la lb ly lc b ld lz lf lg lh ma lj lk mh mb ln lo mi mc lr ls mj md lv lw lx ij bi translated">逻辑回归是一种统计技术，将输入变量(自变量或回归变量)作为连续变量，将输出变量(因变量或回归变量)作为二元变量。就效果而言，输入变量的单位变化会使输出变量的几率乘以一个常数因子。</p></blockquote><p id="37f4" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">逻辑回归作为一种分类技术被广泛使用。让我们考虑下面的回归方程，其中响应变量<em class="ly"> y </em>是具有两类的分类变量。</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/f7525707a8896abef380084cc7dc0972.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*fvo8OFU2yWlGayE_-YS6iA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Logistic Regression Equation</figcaption></figure><p id="e41c" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">为了对对象进行分类，我们将获得对象属于类别“1”的概率。为了预测概率，我们将使用线性模型和逻辑函数的输出:</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/1ebc32b3709cfea01594bdbb20f5020a.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*jgN0ngSkrZ9BBf0bJs4qSg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Finding probability of an example belonging to a class</figcaption></figure><p id="7dfb" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">梯度下降的目的是找出优化给定函数的最佳参数。在逻辑回归算法中，通过最小化以下损失函数找到最佳参数<em class="ly"> θ </em>:</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mq"><img src="../Images/5b10a994b5fc919429d1c013841110ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*heJmb4tcXvLQhJqSTPhFnw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Loss function of Logistic Regression (m: number of training examples)</figcaption></figure><p id="715a" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">最小化上述函数的最基本和广泛使用的优化技术是<strong class="lc ir"> <em class="ly">梯度下降。</em> </strong>它是一种求函数最小值的迭代优化算法。为了使用梯度下降找到局部最小值，采取与函数在当前点的梯度的负值成比例的步骤。如果取正方向，算法找到局部最大值，这个过程称为<strong class="lc ir"> <em class="ly">梯度上升。</em>T15】</strong></p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mr"><img src="../Images/dcfab31d8cb7d3c28aa00bf65854cdce.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*-HP90aii6HZY7XcJLzvOzg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Illustration of each iteration of Gradient Descent [Source: Wikipedia]</figcaption></figure><blockquote class="me mf mg"><p id="28fd" class="la lb ly lc b ld lz lf lg lh ma lj lk mh mb ln lo mi mc lr ls mj md lv lw lx ij bi translated">梯度下降是基于这样的观察:如果多变量函数<em class="iq"> F </em>在点<em class="iq"> x </em>的邻域中被定义并且是可微分的，那么如果从点<em class="iq"> x </em>沿着点<em class="iq"> x </em>处的<em class="iq"> F </em>的负梯度方向前进，函数<em class="iq"> F </em>下降最快。</p></blockquote><p id="8a45" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">对于上面定义的损失函数<em class="ly"> J </em>，点<em class="ly"> j </em>处的梯度定义为:</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/4d2178359b06340ddf937ba9bd3c0a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*IqpxnoSVq2pbFBPD5ATDCw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Gradient at a point — ΔJ</figcaption></figure><p id="d44a" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">上述等式的推导如下所示，[Credits: <a class="ae mt" href="https://math.stackexchange.com/users/80800/avitus" rel="noopener ugc nofollow" target="_blank"> Avitus </a></p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/fe4ae712bf9f8987260a61eb63e2c056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*1jhzYHb3e5aapCFdaNv9eA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Derivation of Gradient at a point</figcaption></figure><h1 id="7269" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">标准梯度下降算法</h1><p id="1f6e" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">标准梯度下降算法定义如下，其中<em class="ly"> η </em>是学习率。</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/25c2e4dfed513f97e3edc4e561bbfa13.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*Wz6Xvw2qCYMPQ7tGaDRwNw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Gradient Descent Algorithm</figcaption></figure><p id="e594" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">在最后一次迭代之后，上述算法给出了函数<em class="ly"> J </em>最小的<em class="ly"> θ </em>的最佳值。该算法的缺点在于，在每次迭代中，必须根据<em class="ly"> m </em>个训练示例来计算<em class="ly"> m </em>个梯度。如果训练集非常大，上述算法将是内存低效的，并且如果训练集不适合内存，可能会崩溃。在这种情况下，随机梯度下降算法会很有帮助。</p><h1 id="2557" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">随机梯度下降</strong></h1><p id="8481" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">与计算真实梯度的传统算法不同，在随机梯度下降算法的迭代中，梯度是针对单个随机选择的训练样本计算的。该算法定义如下。</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/792e735b9c1c735bd878e0216358dd40.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*nhvvIMGLBLtQQwA_-sf51w.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Stochastic Gradient Descent Algorithm</figcaption></figure><p id="5bd3" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">在每次迭代中，在<em class="ly"> 1 </em>和<em class="ly"> m(训练样本数)</em>之间选择一个随机索引，并且只针对所选索引处的训练样本计算梯度。可以对训练集进行多次遍历，直到算法收敛。如果做到了这一点，就可以在每次传递时打乱数据以防止循环。</p><p id="90fa" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">当然，这种方法有其优点和局限性。</p><ol class=""><li id="28d7" class="mx my iq lc b ld lz lh ma ll mz lp na lt nb lx nc nd ne nf bi translated">有噪声的更新导致波动(有噪声的近似)。</li><li id="e55a" class="mx my iq lc b ld ng lh nh ll ni lp nj lt nk lx nc nd ne nf bi translated">每一步只需要一个例子。</li><li id="aedd" class="mx my iq lc b ld ng lh nh ll ni lp nj lt nk lx nc nd ne nf bi translated">可用于在线设置。</li><li id="b68c" class="mx my iq lc b ld ng lh nh ll ni lp nj lt nk lx nc nd ne nf bi translated">学习率<em class="ly"> η </em>要慎重选择。</li></ol><p id="8976" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">在计算批次梯度和单个示例的梯度之间的折衷是在每个步骤针对多个训练示例(称为“小批次”)计算梯度。</p><h1 id="41b6" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">小批量梯度下降</strong></h1><p id="ac70" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">在小批量梯度下降中，在每次迭代中，在<em class="ly"> 1 </em>和<em class="ly"> m </em>之间选择<em class="ly"> z </em>个随机指数，并在这些随机指数下对训练样本计算梯度。该算法定义如下。</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/18befd4fefe92600f2a43b3e5b15da2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*wvl2WSgxrMrPFfmnRyuBkQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Mini Batch Gradient Descent Algorithm</figcaption></figure><p id="1948" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">这比所描述的随机梯度下降法的性能要好得多，因为代码可以利用矢量化库，而不是单独计算每一步。它还可以导致更平滑的收敛，因为在每一步计算的梯度是在更多训练样本上平均的。</p><h1 id="857b" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">动量梯度下降</h1><p id="f889" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">在这里，我将讨论随机梯度下降算法的一个简单补充，称为动量，它通常比随机梯度下降算法工作得更好更快。动量或带动量的随机梯度下降是一种有助于在正确方向上加速梯度向量的方法(如下所示)，从而导致更快的收敛。这是最流行的优化算法之一，许多最先进的模型都是用它来训练的。在深入数学之前，让我们探索动量梯度下降算法背后的思想</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/1062c88bd2a68706f51d8c1f708a1d59.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/1*LVeYw4-H1Bza1rt-WOyEjA.gif"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">SGD without momentum (Source: <a class="ae mt" href="https://www.willamette.edu/~gorr/classes/cs449/momrate.html" rel="noopener ugc nofollow" target="_blank">Genevieve B. Orr</a>)</figcaption></figure><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/707e1b7fdcba53c67d0c46cb063481dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/1*FLpP2e1l3Id9R9_EWcYrTA.gif"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">SGD with momentum (Source: <a class="ae mt" href="https://www.willamette.edu/~gorr/classes/cs449/momrate.html" rel="noopener ugc nofollow" target="_blank">Genevieve B. Orr</a>)</figcaption></figure><p id="f0fb" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">指数加权平均值处理的是数字序列。假设，我们有一些有噪声的序列。对于这个例子，我绘制了余弦函数，并添加了一些高斯噪声。看起来是这样的:</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/c7f268def62de8acf67783515f498204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*8Nf9OLD-hpKgaU3itky6YA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Credits: <a class="ae mt" href="https://towardsdatascience.com/@bushaev?source=user_popover" rel="noopener" target="_blank">Vitaly Bushaev</a></figcaption></figure><p id="de4f" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">请注意，尽管这些点看起来非常接近，但它们都不共享<em class="ly"> x </em>坐标。对于每个点来说，这是一个唯一的数字。这个数字定义了序列中每个点的索引。</p><p id="be6e" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">我们想对这些数据做的是，不是使用它，而是想要某种“移动”平均值，这将对数据进行“去噪”，并使其更接近原始函数。指数加权平均值可以给我们一个看起来像这样的图片:</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/e9d0d0abacc782b9fe4dd911231ba7e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*v8-x1z4nIb01mIfEKh9HQQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Credits: <a class="ae mt" href="https://towardsdatascience.com/@bushaev?source=user_popover" rel="noopener" target="_blank">Vitaly Bushaev</a></figcaption></figure><p id="2bc8" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">如你所见，这是一个非常好的结果。我们得到的不是有很多噪音的数据，而是更平滑的线，比我们得到的数据更接近原始函数。指数加权平均值用下面的等式定义了新的序列 V:</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/6bd4771e6698308cf15a0305629ab637.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*AhEqwzaQaZ6-lnpr_CG_wg.png"/></div></figure><p id="1539" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">序列 V 就是上面标为黄色的那个。<em class="ly"> β </em>是另一个超参数，取值从 0 到 1。我用的是<em class="ly"> beta </em> = 0.9 以上。这是一个很好的值，最常用于带动量的 SGD。直观上，你可以把<em class="ly"> β </em>想成如下。我们对序列的最后<em class="ly"> 1 / (1- β) </em>个点进行近似平均。让我们看看<em class="ly"> β </em>的选择如何影响我们的新序列 v。</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/4d9998f3be70970be3b7899b2ae1eac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*YsIH_w9Lc5JY7ZcZov3fFA.png"/></div></figure><p id="76d7" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">如您所见，β值越小，新序列的波动越大，因为我们对更小数量的样本求平均值，因此更接近噪声数据。β值越大，如β <em class="ly"> =0.98 </em>，我们得到的曲线越平滑，但它稍微向右偏移了一点，因为我们对大量示例进行了平均(β <em class="ly"> =0.98 </em>时约为 50)。β <em class="ly"> = 0.9 </em>在这两个极端之间提供了良好的平衡。</p><p id="e8f8" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">动量梯度下降算法定义如下。</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/321e5e2d89b6321390ae3d01d0ea469a.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*-5qhfXU-LHKnOquH6z5zbQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Momentum gradient descent algorithm</figcaption></figure><h1 id="7715" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">内斯特罗夫动量梯度下降算法</h1><p id="643f" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="lc ir">内斯特罗夫动量</strong>是动量更新的一个略有不同的版本，最近越来越受欢迎。在这个版本中，我们首先看当前动量指向的点，并从该点计算梯度。当你看这幅画时，它变得清晰多了。</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/f16815ad91ef1b86d278cefd4a8ff907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KiQOTG_hlSoun8vGzVx59Q.jpeg"/></div></div></figure><p id="e63e" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">内斯特罗夫动量算法定义如下。</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/337bcb44cc4a4e503dffcf5031f9d6be.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*jHfTAuWj8Hb8wRyThKveeg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Nesterov momentum algorithm formulas</figcaption></figure><h1 id="691e" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">结论</h1><p id="8c5c" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我希望这篇文章能帮助你学习不同类型的关于逻辑回归的梯度下降算法。动量梯度下降实际上是深度学习中最流行的优化算法之一，比更高级的算法使用得更频繁。</p><p id="f9d3" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">我将发表另一篇文章描述先进的梯度下降算法，如 Ada Grad，RMS Prop，Adam 梯度下降等。</p><p id="dae3" class="pw-post-body-paragraph la lb iq lc b ld lz lf lg lh ma lj lk ll mb ln lo lp mc lr ls lt md lv lw lx ij bi translated">请在下面的评论中发表你对这篇文章的看法。</p><h1 id="cec0" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">参考</h1><ol class=""><li id="27b1" class="mx my iq lc b ld le lh li ll nt lp nu lt nv lx nc nd ne nf bi translated">Coursera 上的高级机器学习课程</li><li id="7d59" class="mx my iq lc b ld ng lh nh ll ni lp nj lt nk lx nc nd ne nf bi translated">蒸馏，<a class="ae mt" href="https://distill.pub/2017/momentum/" rel="noopener ugc nofollow" target="_blank">为什么动量真的起作用</a></li><li id="f1be" class="mx my iq lc b ld ng lh nh ll ni lp nj lt nk lx nc nd ne nf bi translated"><a class="ae mt" rel="noopener" target="_blank" href="/stochastic-gradient-descent-with-momentum-a84097641a5d">走向数据科学</a></li></ol><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nw"><img src="../Images/6fa88621186283a21bf55c349ae1b88e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pl_RyMF2xBbc8FYLBrF6Sg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae mt" href="https://unsplash.com/photos/D_mFA0GZuAs?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Toa Heftiba</a> on <a class="ae mt" href="https://unsplash.com/search/photos/skiing?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div>    
</body>
</html>