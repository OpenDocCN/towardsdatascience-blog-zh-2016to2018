<html>
<head>
<title>Low Precision Inference with TensorRT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorRT低精度推理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/low-precision-inference-with-tensorrt-6eb3cda0730b?source=collection_archive---------2-----------------------#2017-08-09">https://towardsdatascience.com/low-precision-inference-with-tensorrt-6eb3cda0730b?source=collection_archive---------2-----------------------#2017-08-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="e7fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">声明:这是<strong class="jp ir">而不是</strong>一个NVIDIA的官方帖子。只是总结一下我从演讲、听的演讲和与人的互动中学到的和收集到的东西。Post假设对深度学习(DL)有一些基本的了解。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><p id="3f58" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">深度学习程序通常有两个阶段，即,</p><ol class=""><li id="7c18" class="ks kt iq jp b jq jr ju jv jy ku kc kv kg kw kk kx ky kz la bi translated">训练，利用一堆数据和某些优化技术，机器学习归纳(嗯，这是它能得到的最简单的)。通常离线完成。</li><li id="d2ef" class="ks kt iq jp b jq lb ju lc jy ld kc le kg lf kk kx ky kz la bi translated">推理，机器将学习到的概括应用于看不见的数据。通常在生产环境中使用。</li></ol><p id="0523" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练是对大量数据的迭代过程。它的计算量非常大。根据问题定义、数据等，可以使用从单个GPU到一群GPU的任何东西。</p><p id="ba51" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一方面，推理在计算上相对容易(由于较小的批量和没有反向传递)，但是在大多数DNN(深度神经网络)应用中实现实时推理仍然是一个困难的问题。</p><p id="c925" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，训练过程一般没有“<em class="lg">实时</em>的约束。另一方面，实际推理是受时间和力量(尤其是嵌入式)约束的。TensorRT是Nvidia软件解决方案，用于为深度学习模型的生产部署生成优化模型。这篇<a class="ae lh" href="https://devblogs.nvidia.com/parallelforall/production-deep-learning-nvidia-gpu-inference-engine/" rel="noopener ugc nofollow" target="_blank"> parallel forall </a>博客文章对TensorRT，以前被称为GPU推理引擎(GIE ),博客使用旧行话)做了很好的介绍。</p><p id="0479" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇博客将主要集中在一个重要的优化技术上:<strong class="jp ir">低精度推理(LPI) </strong>。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><p id="13de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在开始理解LPI之前，我将快速总结一下所有博文的相似之处。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi li"><img src="../Images/e8fb8b1d1063de3e11270b629a1b7d6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7zvHEeYhiHBqjO_pzlWKww.png"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Fig. 1: TensortRT in one picture</figcaption></figure><p id="f586" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上图很好地概括了TRT的工作。它基本上是作为一个<a class="ae lh" href="https://developer.nvidia.com/tensorrt" rel="noopener ugc nofollow" target="_blank"> SDK </a>公开的。你输入你已经训练好的网络(这将意味着模型定义和学习参数)和其他参数，如推理批量大小和精度，TRT做优化，并建立一个执行计划，可以使用或序列化，并保存到磁盘上，供以后使用。推理时不需要深度学习框架。只要使用TRT输出的执行计划，就可以了。在服务器、台式机甚至嵌入式设备上使用它。此外，对于给定的模型，这是一次性的事情。没有附加条件:)</p><h2 id="de77" class="ly lz iq bd ma mb mc dn md me mf dp mg jy mh mi mj kc mk ml mm kg mn mo mp mq bi translated">优化引擎</h2><p id="a758" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">这就是奇迹发生的地方(嗯，不完全是，这只是科学)。TRT做了一些优化。</p><ul class=""><li id="eb6a" class="ks kt iq jp b jq jr ju jv jy ku kc kv kg kw kk mw ky kz la bi translated">图形优化:</li></ul><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi mx"><img src="../Images/b8fe57e46862ba1518645ef20b77d057.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PyNcjHKZ8rQ48QCPsdQ9wA.png"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Fig.2: Vertical Fusion — Input</figcaption></figure><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi my"><img src="../Images/79f781e9669c7d8e3c180a8225a04fc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bJts223Qo55toZ9AY60Ruw.png"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Fig.3: Vertical Fusion — Optimized graph</figcaption></figure><p id="48f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上图解释了TRT所做的垂直融合优化。卷积(C)、偏置(B)和激活(本例中为R、ReLU)都被合并到一个节点中(就实现而言，这意味着C、B和R只启动一个CUDA内核)。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi mz"><img src="../Images/5ea3cd4c42f9036d1b1b530c4a416314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UKwCx_lq-oHcLYkI.png"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Fig.4: Horizontal Fusion</figcaption></figure><p id="a0ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">是的。还有一种水平融合，其中如果具有相同操作的多个节点正在向多个节点馈送，则它被转换为一个单个节点向多个节点馈送。三个1x1 CBRs被融合成一个，并且它们的输出被定向到适当的节点。</p><h2 id="c017" class="ly lz iq bd ma mb mc dn md me mf dp mg jy mh mi mj kc mk ml mm kg mn mo mp mq bi translated">其他优化</h2><p id="2115" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">除了图形优化之外，TRT还通过实验并基于诸如批量大小、卷积核(过滤器)大小等参数，为网络中的操作选择高效的算法和核(CUDA核)。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h1 id="caaa" class="na lz iq bd ma nb nc nd md ne nf ng mg nh ni nj mj nk nl nm mm nn no np mp nq bi translated">低精度推理</h1><h2 id="7f4d" class="ly lz iq bd ma mb mc dn md me mf dp mg jy mh mi mj kc mk ml mm kg mn mo mp mq bi translated">那么，为什么我们真的需要低精度的推理呢？</h2><p id="6b79" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">正如已经提到的，执行时间和功率并不便宜，在实时应用中是至关重要的。考虑到所需的计算量，我们希望在不影响准确性的情况下，优化我们的推理，以利于时间和能力。因此，我们转移到参数的8位表示和推理时的激活。与32位相比，8位需要更少周期来获取存储器。还有一个新的硬件指令(DP4A)形式的优势，我将在本文稍后介绍。</p><h2 id="ea85" class="ly lz iq bd ma mb mc dn md me mf dp mg jy mh mi mj kc mk ml mm kg mn mo mp mq bi translated">这有用吗？</h2><p id="e44b" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">是的，当然。如果不是这样的话，这个博客就是浪费时间了:)。在训练过程中，参数和激活在<a class="ae lh" href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format" rel="noopener ugc nofollow" target="_blank"> FP32 </a>中显示。FP32的高精度有利于训练，因为每个训练步骤都会对参数进行少量修正。DL算法通常对噪声有弹性。在学习模型的过程中，通常会尝试选择性地保留预测所需的特征。所以，扔掉不必要的东西是这个过程的一部分。所以我们希望低精度表示引入的噪声被模型扔掉(这里也是非常外行的术语)。据我所知，没有严格的数学证明说低精度应该在推理过程中像FP32一样好。</p><h2 id="6950" class="ly lz iq bd ma mb mc dn md me mf dp mg jy mh mi mj kc mk ml mm kg mn mo mp mq bi translated">将FP32映射到8位</h2><p id="45ae" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">这是TensorRT为应用程序/客户端做的事情。应用程序/客户端只需要实现一个提供校准信息和一些缓存相关代码的接口。很快会有更多的报道。在此之前，让我们看看TRT进行32位到8位映射的步骤。</p><p id="2b4a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">映射的候选对象是每一层的输入(将输入到第一层，并激活其余层)和学习参数。最简单的映射/量化形式是线性量化。</p><p id="631d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lg"> FP32张量(T) = scale_factor(sf) * 8位张量(t) </em> <em class="lg"> + FP32_bias (b) </em></p><p id="418c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">很简单，不是吗？让我们把它变得简单些。(从实验中)发现偏差项并没有真正增加任何价值。所以，摆脱它。</p><p id="e1ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lg"> T = sf * t </em></p><p id="50ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意这里的<em class="lg"> sf </em>是每个层中每个张量的比例因子。现在，下一个问题是找到比例因子。一个简单的方法是如下图所示:</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/8f7b75bae3ffe6e47bc3abb5da1bb1aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*_36xi1OWq9zQ8cydVVWCpA.png"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Fig.5: A simple max-max mapping</figcaption></figure><p id="f0e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们简单地将张量中的-|max|和|max| FP32值分别映射到-127和127。其余的值相应地线性缩放。但是，这里有一个问题。实验表明，这种映射会导致显著的精度损失。因此，TRT采取了以下措施:</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/177c86fb094a2205184f023c67fce5e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*1tk6ai9FVV9Debbha2lbkQ.png"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Fig.6: Threshold instead of max</figcaption></figure><p id="cd97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">而不是看|max|值。固定一个阈值(如何？我们稍后会看到)。然后像前面一样进行映射。阈值范围内的任何值都调整为-127或127。例如，在上图中，三个“红叉”被映射到-127。</p><p id="f8b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么接下来的问题就是阈值<em class="lg"> T </em>的最优值是多少？这里有两种分布。我们有FP32张量，它在FP32分布中表现得最好。但是，我们希望用不同的分布(8位)来表示它们，这不是最好的分布。我们希望看到这些分布有多大的不同，并希望将差异最小化。TRT使用Kullback-Leibler散度(<a class="ae lh" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> KL-divergence </a>)来衡量差异，并旨在将其最小化。</p><p id="2d0a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">可选:什么是KL-divergence？为什么是KL-divergence？一些直觉:&lt;可选开头&gt; </strong>让我们在这里稍微进入编码理论。假设我有一系列符号，我知道它们出现的概率。如果我要对符号进行最佳编码，我会使用比如说“T3”T比特。注意<em class="lg"> T </em>是最佳位数。让我们称这个代码为代码'<em class="lg"> A </em>'。现在，我有了同样的一组符号，但是它们出现的概率变了。现在，对于具有新概率的符号，如果我使用代码'<em class="lg"> A </em>'来编码符号，则用于编码的比特数将是次优的，并且大于'<em class="lg"> T </em>'。KL-divergence精确地测量这种差异，即最优和次优之间的差异(由于选择了错误的代码)。在我们的例子中，张量值的正确代码是FP32，但我们选择用8位错误代码来表示(嗯，不是错误，但你明白了)。所以有一个惩罚，用KL散度来衡量。我们的工作是尽量减少处罚。如果你已经用DL解决了一些问题，你可能会用交叉熵作为一个代价函数。KL散度与交叉熵密切相关。虽然KL-divergence表示由于在错误代码中编码而导致的比特数(或所需的额外比特数)的差异，但是交叉熵表示在该错误代码中编码符号所需的确切比特数。所以，<em class="lg"> KL-divergence =(交叉熵)——(最优码所需比特数)</em>。<strong class="jp ir"> &lt;可选结束&gt; </strong></p><p id="6006" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我们需要最小化FP32值和相应的8位值之间的KL偏差。TRT使用简单的迭代搜索最小散度，而不是基于梯度下降的方法。步骤如下:</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi nt"><img src="../Images/7170512228c7c85de3b18403ba0f8c2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MYjwfp6SxxuJvOlZJnKIlg.png"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Fig.7: Threshold finding process</figcaption></figure><h2 id="a3e8" class="ly lz iq bd ma mb mc dn md me mf dp mg jy mh mi mj kc mk ml mm kg mn mo mp mq bi translated">校准</h2><p id="3c85" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">TRT采用基于实验的阈值迭代搜索。校准是其中的主要部分。应用程序/客户向TRT提供了一个样本数据集(理想情况下是验证集的子集)，称为“校准数据集”，用于执行所谓的校准。TRT在校准数据集上运行<strong class="jp ir"> FP32 </strong>推理。收集激活直方图并生成具有不同阈值的8位表示的集合<strong class="jp ir">并选择具有最小KL散度的一个。KL散度在参考分布(FP32激活)和量化分布(8位量化激活)之间。TRT 2.1提供了<code class="fe nu nv nw nx b">IInt8EntropyCalibrator</code>接口，客户端需要实现该接口来提供校准数据集和一些用于缓存校准结果的样板代码。</strong></p><h2 id="ed25" class="ly lz iq bd ma mb mc dn md me mf dp mg jy mh mi mj kc mk ml mm kg mn mo mp mq bi translated">DP4A</h2><p id="d1f6" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">这就是TRT如何进行优化的简要介绍。还有一件事不完全相关，但TRT依赖于8位快速推理。从Pascal一代GPU(选定的SKU)开始，有一种称为DP4A的新硬件指令。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/fdbbc103bda00cfe62a8cf387cd3f8d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*fgyi_gAhpTjPOs8Igutucg.png"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Fig.8: DP4A (<strong class="bd nz">D</strong>ot <strong class="bd nz">P</strong>roduct of <strong class="bd nz">4</strong> 8-bits <strong class="bd nz">A</strong>ccumulated to a 32-bit)</figcaption></figure><p id="7671" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它基本上是一条指令，执行4次8位乘法，并累加成一个32位整数。点积构成了卷积运算的数学基础，该指令通过最小化执行时间和功耗，提供了非常好的提升。<a class="ae lh" href="https://devblogs.nvidia.com/parallelforall/mixed-precision-programming-cuda-8/" rel="noopener ugc nofollow" target="_blank">这篇关于CUDA 8的</a>博客更详细地讨论了DP4A。如果有兴趣，一定要去看看。</p><p id="d270" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，总的来说，优化的空间很大。考虑到TRT的优化和正确的硬件(支持DP4A的GPU)，您不仅可以将GPU推向极限，同时还可以保持其效率。所以去做一些有效的推断，直到你推断出生命的意义:D</p><p id="53db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">一些参考:</strong>我无耻地从博客和演示文稿中抄袭数据:)</p><p id="842c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图1至图4:<a class="ae lh" href="https://devblogs.nvidia.com/parallelforall/deploying-deep-learning-nvidia-tensorrt/" rel="noopener ugc nofollow" target="_blank">tensort</a></p><p id="4899" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图5至图8 : <a class="ae lh" href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf" rel="noopener ugc nofollow" target="_blank"> GTC PPT </a></p><p id="8483" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这大部分是基于在2017年GTC上的这个出色的<a class="ae lh" href="http://on-demand.gputechconf.com/gtc/2017/video/s7310-szymon-migacz-8-bit-inference-with-tensorrt.mp4" rel="noopener ugc nofollow" target="_blank">展示</a>。</p><p id="00b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PS:请随意评论/批评/ ♥这篇文章。</p></div></div>    
</body>
</html>