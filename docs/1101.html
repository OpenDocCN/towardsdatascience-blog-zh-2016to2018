<html>
<head>
<title>Learning Rate Schedules and Adaptive Learning Rate Methods for Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的学习速率表和自适应学习速率方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1?source=collection_archive---------0-----------------------#2017-07-29">https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1?source=collection_archive---------0-----------------------#2017-07-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="df32" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当训练深度神经网络时，随着训练的进行降低学习速率通常是有用的。这可以通过使用预定义的<strong class="jp ir">学习率计划</strong>或<strong class="jp ir">自适应学习率方法</strong>来完成。在本文中，我在<a class="ae kl" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10 </a>上训练了一个卷积神经网络，使用不同的学习速率计划和自适应学习速率方法来比较它们的模型性能。</p><h1 id="0c2b" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">学习费率表</h1><p id="daf3" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">学习率时间表旨在通过根据预定义的时间表降低学习率来调整训练期间的学习率。常见的学习率时间表包括<strong class="jp ir">基于时间的衰减</strong>、<strong class="jp ir">阶跃衰减</strong>和<strong class="jp ir">指数衰减</strong>。为了说明的目的，我构建了一个在<a class="ae kl" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10 </a>上训练的卷积神经网络，使用具有不同学习速率调度的随机梯度下降(SGD)优化算法来比较性能。</p><h2 id="5950" class="lp kn iq bd ko lq lr dn ks ls lt dp kw jy lu lv la kc lw lx le kg ly lz li ma bi translated"><strong class="ak">恒定学习率</strong></h2><p id="2d8c" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在Keras的<a class="ae kl" href="https://keras.io/optimizers/#sgd" rel="noopener ugc nofollow" target="_blank"> SGD optimizer </a>中，恒定学习率是默认的学习率计划。默认情况下，动量和衰减率都设置为零。选择正确的学习速度是很棘手的。在我们的例子中，通过实验学习率的范围，<code class="fe mb mc md me b">lr=0.1</code>显示了相对较好的开始性能。这可以作为我们试验不同学习速度策略的基准。</p><pre class="mf mg mh mi gt mj me mk ml aw mm bi"><span id="aacc" class="lp kn iq me b gy mn mo l mp mq">keras.optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=<strong class="me ir">False</strong>)</span></pre><figure class="mf mg mh mi gt ms gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/511624a1f546f13adbc77150bc9f59c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*Lv7-jMtHOoucryv9mUtFGg.jpeg"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Fig 1 : Constant Learning Rate</figcaption></figure><h2 id="a6d4" class="lp kn iq bd ko lq lr dn ks ls lt dp kw jy lu lv la kc lw lx le kg ly lz li ma bi translated"><strong class="ak">基于时间的衰变</strong></h2><p id="49e5" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">基于时间的衰减的数学形式是<code class="fe mb mc md me b">lr = lr0/(1+kt)</code>，其中<code class="fe mb mc md me b">lr</code>、<code class="fe mb mc md me b">k</code>是超参数，<code class="fe mb mc md me b">t</code>是迭代次数。查看Keras的<a class="ae kl" href="https://github.com/fchollet/keras/blob/master/keras/optimizers.py#L126" rel="noopener ugc nofollow" target="_blank">源代码</a>，SGD优化器采用<code class="fe mb mc md me b">decay</code>和<code class="fe mb mc md me b">lr</code>参数，并在每个时期以递减因子更新学习率。</p><pre class="mf mg mh mi gt mj me mk ml aw mm bi"><span id="165e" class="lp kn iq me b gy mn mo l mp mq">lr *= (1. / (1. + self.decay * self.iterations))</span></pre><p id="d8ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">动量是SGD优化器中的另一个参数，我们可以调整它以获得更快的收敛。与经典的SGD不同，动量法帮助参数向量以恒定的梯度下降在任何方向上建立速度，以防止振荡。动量的典型选择在0.5到0.9之间。</p><p id="ca5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">SGD optimizer还有一个名为<code class="fe mb mc md me b">nesterov</code>的参数，默认设置为false。内斯特罗夫动量法是动量法的一个不同版本，它对凸函数有更强的理论收敛保证。在实践中，它比标准动量法稍微好一点。</p><p id="3f6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Keras中，我们可以通过在SGD优化器中设置初始学习率、衰减率和动量来实现基于时间的衰减。</p><pre class="mf mg mh mi gt mj me mk ml aw mm bi"><span id="78d3" class="lp kn iq me b gy mn mo l mp mq">learning_rate = 0.1<br/>decay_rate = learning_rate / epochs<br/>momentum = 0.8<br/>sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)</span></pre><figure class="mf mg mh mi gt ms gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/164c5680af00a65971979b38ca2bbcae.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*YpzU0MkpNaZ8f6cGvqex7g.jpeg"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Fig 2 : Time-based Decay Schedule</figcaption></figure><h2 id="62f0" class="lp kn iq bd ko lq lr dn ks ls lt dp kw jy lu lv la kc lw lx le kg ly lz li ma bi translated">阶跃衰减</h2><p id="bb83" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">步长衰减计划每隔几个历元就将学习速率降低一个因子。阶跃衰减的数学形式为:</p><pre class="mf mg mh mi gt mj me mk ml aw mm bi"><span id="a262" class="lp kn iq me b gy mn mo l mp mq">lr = lr0 * drop^floor(epoch / epochs_drop) </span></pre><p id="9960" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个典型的方法是每10个时期将学习率降低一半。为了在Keras中实现这一点，我们可以定义一个步长衰减函数，并使用<a class="ae kl" href="https://keras.io/callbacks/#learningratescheduler" rel="noopener ugc nofollow" target="_blank"> LearningRateScheduler </a>回调将步长衰减函数作为参数，并返回更新后的学习速率，以供SGD optimizer使用。</p><pre class="mf mg mh mi gt mj me mk ml aw mm bi"><span id="a4e4" class="lp kn iq me b gy mn mo l mp mq">def step_decay(epoch):<br/>   initial_lrate = 0.1<br/>   drop = 0.5<br/>   epochs_drop = 10.0<br/>   lrate = initial_lrate * math.pow(drop,  <br/>           math.floor((1+epoch)/epochs_drop))<br/>   return lrate</span><span id="b437" class="lp kn iq me b gy mz mo l mp mq">lrate = LearningRateScheduler(step_decay)</span></pre><p id="e887" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">题外话，<a class="ae kl" href="https://keras.io/callbacks/#callback" rel="noopener ugc nofollow" target="_blank">回调</a>是在训练程序的给定阶段应用的一组函数。在训练期间，我们可以使用回调来查看模型的内部状态和统计数据。在我们的示例中，我们通过扩展基类<code class="fe mb mc md me b">keras.callbacks.Callback</code>来创建一个定制回调，以记录培训过程中的损失历史和学习率。</p><pre class="mf mg mh mi gt mj me mk ml aw mm bi"><span id="6ba1" class="lp kn iq me b gy mn mo l mp mq">class LossHistory(keras.callbacks.Callback):<br/>    def on_train_begin(self, logs={}):<br/>       self.losses = []<br/>       self.lr = []<br/> <br/>    def on_epoch_end(self, batch, logs={}):<br/>       self.losses.append(logs.get(‘loss’))<br/>       self.lr.append(step_decay(len(self.losses)))</span></pre><p id="990c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将所有东西放在一起，我们可以传递一个回调列表，该列表由<code class="fe mb mc md me b">LearningRateScheduler</code>回调和我们的定制回调组成，以适应模型。然后，我们可以通过访问<code class="fe mb mc md me b">loss_history.lr</code>和<code class="fe mb mc md me b">loss_history.losses</code>来可视化学习率时间表和损失历史。</p><pre class="mf mg mh mi gt mj me mk ml aw mm bi"><span id="3436" class="lp kn iq me b gy mn mo l mp mq">loss_history = LossHistory()<br/>lrate = LearningRateScheduler(step_decay)<br/>callbacks_list = [loss_history, lrate]</span><span id="1373" class="lp kn iq me b gy mz mo l mp mq">history = model.fit(X_train, y_train, <br/>   validation_data=(X_test, y_test), <br/>   epochs=epochs, <br/>   batch_size=batch_size, <br/>   callbacks=callbacks_list, <br/>   verbose=2)</span></pre><figure class="mf mg mh mi gt ms gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/9d8cdfcaf21422420ae77f0393f2e815.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*qite6RcBZHFmiVAZBxwM0g.jpeg"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Fig 3a : Step Decay Schedule</figcaption></figure><figure class="mf mg mh mi gt ms gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/59a88fb872e4d3ea9dae84f7c2ff6e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*VQkTnjr2VJOz0R2m4hDucQ.jpeg"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Fig 3b : Step Decay Schedule</figcaption></figure><h2 id="2a36" class="lp kn iq bd ko lq lr dn ks ls lt dp kw jy lu lv la kc lw lx le kg ly lz li ma bi translated"><strong class="ak">指数衰减</strong></h2><p id="0d75" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">另一个常见的时间表是指数衰减。其数学形式为<code class="fe mb mc md me b">lr = lr0 * e^(−kt)</code>，其中<code class="fe mb mc md me b">lr</code>、<code class="fe mb mc md me b">k</code>为超参数，<code class="fe mb mc md me b">t</code>为迭代次数。类似地，我们可以通过定义指数衰减函数并将其传递给<code class="fe mb mc md me b">LearningRateScheduler</code>来实现这一点。事实上，使用这种方法可以在Keras中实现任何定制的衰减计划。唯一的区别是定义了不同的自定义衰减函数。</p><pre class="mf mg mh mi gt mj me mk ml aw mm bi"><span id="400c" class="lp kn iq me b gy mn mo l mp mq">def exp_decay(epoch):<br/>   initial_lrate = 0.1<br/>   k = 0.1<br/>   lrate = initial_lrate * exp(-k*t)<br/>   return lrate</span><span id="04e8" class="lp kn iq me b gy mz mo l mp mq">lrate = LearningRateScheduler(exp_decay)</span></pre><figure class="mf mg mh mi gt ms gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/0d67e2f4e22db65fd3b05a1c2d2cfa2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*MpVdh9K7nmpZ0VeT17q5FQ.jpeg"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Fig 4a : Exponential Decay Schedule</figcaption></figure><figure class="mf mg mh mi gt ms gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/6358fda7108fc8fbabe61b2af2e0e620.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*iSZv0xuVCsCCK7Z4UiXf2g.jpeg"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Fig 4b : Exponential Decay Schedule</figcaption></figure><p id="9237" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们在我们的例子中使用不同的学习率时间表来比较模型的准确性。</p><figure class="mf mg mh mi gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/bb88488b72d14cdae29bfa4b4f65a736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5L0VxjeCL3m-k5nfbm2lZg.jpeg"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Fig 5 : Comparing Performances of Different Learning Rate Schedules</figcaption></figure><h1 id="1278" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">自适应学习率方法</h1><p id="bc9c" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">使用学习率时间表的挑战在于它们的超参数必须预先定义，并且它们严重依赖于模型和问题的类型。另一个问题是相同的学习速率被应用于所有的参数更新。如果我们有稀疏的数据，我们可能希望在不同程度上更新参数。</p><p id="90da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自适应梯度下降算法，如<a class="ae kl" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad" rel="noopener ugc nofollow" target="_blank"/><strong class="jp ir">、</strong> Adadelta、<a class="ae kl" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp" rel="noopener ugc nofollow" target="_blank"> RMSprop </a> <strong class="jp ir">、</strong> <a class="ae kl" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam" rel="noopener ugc nofollow" target="_blank"> Adam </a>，提供了经典SGD的替代方案。这些每参数学习率方法提供了启发式方法，而不需要为学习率调度手动调整超参数的昂贵工作。</p><p id="d8a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简而言之，<strong class="jp ir"> Adagrad </strong>对更稀疏的参数执行较大的更新，对不太稀疏的参数执行较小的更新。它在稀疏数据和训练大规模神经网络方面具有良好的性能。然而，当训练深度神经网络时，它的单调学习速率通常被证明过于激进并且过早停止学习。<strong class="jp ir"> Adadelta </strong>是Adagrad的扩展，旨在降低其激进的、单调递减的学习速率。<strong class="jp ir"> RMSprop </strong>以非常简单的方式调整Adagrad方法，试图降低其激进的、单调递减的学习速率。<strong class="jp ir"> Adam </strong>是对RMSProp优化器的更新，它类似于带有momentum的RMSprop。</p><p id="dc49" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Keras中，我们可以使用相应的优化器轻松实现这些自适应学习算法。通常建议将这些优化器的超参数保留为默认值(有时<code class="fe mb mc md me b">lr</code>除外)。</p><pre class="mf mg mh mi gt mj me mk ml aw mm bi"><span id="9350" class="lp kn iq me b gy mn mo l mp mq">keras.optimizers.Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)</span><span id="0f49" class="lp kn iq me b gy mz mo l mp mq">keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)</span><span id="00d8" class="lp kn iq me b gy mz mo l mp mq">keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)</span><span id="d418" class="lp kn iq me b gy mz mo l mp mq">keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)</span></pre><p id="0eba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们看看使用不同的自适应学习率方法的模型性能。在我们的例子中，在其他自适应学习率方法中，Adadelta给出了最好的模型精度。</p><figure class="mf mg mh mi gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/2cccf75af83af70628f565f0af57c18e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ithIBSwTSympfw2zfMh9Fw.jpeg"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Fig 6 : Comparing Performances of Different Adaptive Learning Algorithms</figcaption></figure><p id="d012" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们比较了我们讨论过的所有学习速率表和自适应学习速率方法的性能。</p><figure class="mf mg mh mi gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/8f142c478c207027ada9190df566c8ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OjcTfMw6dmOmP4lRE7Ud-A.jpeg"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Fig 7: Comparing Performances of Different Learning Rate Schedules and Adaptive Learning Algorithms</figcaption></figure><h1 id="6d2e" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">结论</h1><p id="610e" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在我研究的许多例子中，自适应学习率方法比学习率计划表现更好，并且在超参数设置中需要更少的努力。我们还可以使用Keras中的<code class="fe mb mc md me b">LearningRateScheduler</code>来创建针对我们的数据问题的定制学习率计划。</p><p id="a965" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作为进一步的阅读，Yoshua Bengio的论文为深度学习的学习速率调优提供了非常好的实用建议，例如如何设置初始学习速率、小批量大小、纪元数量以及使用早期停止和动量。</p><p id="cd46" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://github.com/sukilau/Ziff/blob/master/3-CIFAR10-lrate/CIFAR10-lrate.ipynb" rel="noopener ugc nofollow" target="_blank">源代码</a></p><h2 id="9a5e" class="lp kn iq bd ko lq lr dn ks ls lt dp kw jy lu lv la kc lw lx le kg ly lz li ma bi translated">参考资料:</h2><ul class=""><li id="c17f" class="nf ng iq jp b jq lk ju ll jy nh kc ni kg nj kk nk nl nm nn bi translated"><a class="ae kl" href="http://arxiv.org/pdf/1206.5533v2.pdf" rel="noopener ugc nofollow" target="_blank">yo shua beng io对深度架构基于梯度的训练的实用建议</a></li><li id="c02e" class="nf ng iq jp b jq no ju np jy nq kc nr kg ns kk nk nl nm nn bi translated"><a class="ae kl" href="http://cs231n.github.io/neural-networks-3/#sgd" rel="noopener ugc nofollow" target="_blank">用于视觉识别的卷积神经网络</a></li><li id="84b2" class="nf ng iq jp b jq no ju np jy nq kc nr kg ns kk nk nl nm nn bi translated"><a class="ae kl" href="http://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/" rel="noopener ugc nofollow" target="_blank">利用Keras在Python中使用深度学习模型的学习速率表</a></li></ul></div></div>    
</body>
</html>