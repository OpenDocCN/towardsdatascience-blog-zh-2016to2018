<html>
<head>
<title>Word Embeddings and Document Vectors — When in Doubt, Simplify</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入和文档向量——如有疑问，请简化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e?source=collection_archive---------6-----------------------#2018-10-16">https://towardsdatascience.com/word-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e?source=collection_archive---------6-----------------------#2018-10-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fb5e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对一些文本集的分类精度和性能指标表明，朴素贝叶斯分类器是一个强有力的竞争者</h2></div><p id="1ce0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是关于在 NLP 任务中使用单词向量和文档向量的系列文章的第三篇也是最后一篇。这里的重点是文本分类。当单词向量与文档向量相结合时，我们总结了其结果和建议。代码可以从<a class="ae lb" href="https://github.com/ashokc/Word-Embeddings-and-Document-Vectors" rel="noopener ugc nofollow" target="_blank"> github </a>下载。让我们快速总结一下过去的两篇文章。</p><ol class=""><li id="740a" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/word-embeddings-and-document-vectors-part-1-similarity-1cd82737cf58">相似度</a>:单词向量是一个单词的表示，它是一个选定长度的数字向量<em class="ll"> p </em>。它们是通过对文本语料库应用 Word2vec、Glove 和 FastText 等工具而获得的。具有相似含义的单词通常产生余弦相似度更接近 1 而不是 0 的数字单词向量。</li><li id="6b83" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la lh li lj lk bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/word-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c">降阶</a>:将单词向量与基于单词包的文档向量相结合，在文本语料库的模型<em class="ll"> Z </em>中产生大的(通常几个数量级，等于<em class="ll"> m/p </em>其中<em class="ll"> m </em>是文本语料库中唯一单词的数量，而<em class="ll"> p </em>是单词向量的长度)降阶。我们构建了一个 scikit 管道(vectorize =&gt;embed words =&gt;classify ),在单词向量矩阵<em class="ll"> W </em>的帮助下，从高阶的<em class="ll"> X </em>导出<em class="ll"> Z </em>。每一行<em class="ll"> W </em>都是文本语料库中一个单词的<em class="ll"> p </em>维数值表示。</li></ol><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/a2d14141d51d7e995b60aad1213cbba5.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*509bCUr8H2gVKQ66CiwUZw.png"/></div></figure><p id="5540" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这样，我们就可以评估单词向量对文本分类的影响，并在流水线步骤中使用我们可以选择的选项来限定结果。我们坚持三个步骤中的每一步都有几个主要选项。</p><ol class=""><li id="1051" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated"><strong class="kh ir">记号化</strong>。我们将使用 scikit 的<a class="ae lb" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">计数矢量器</a>和<a class="ae lb" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" rel="noopener ugc nofollow" target="_blank">tfidfvectorizer</a>。</li><li id="f8fe" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la lh li lj lk bi translated"><strong class="kh ir">词向量</strong>。我们将对<a class="ae lb" href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Word2Vec/SGNS </a>和<a class="ae lb" href="https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip" rel="noopener ugc nofollow" target="_blank"> FastText </a>进行评测。预培训和定制生成(通过<a class="ae lb" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank"> Gensim </a>)</li><li id="a041" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la lh li lj lk bi translated"><strong class="kh ir">量词</strong>。我们将使用 scikit 的<a class="ae lb" href="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html" rel="noopener ugc nofollow" target="_blank">多项式朴素贝叶斯</a>、<a class="ae lb" href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html" rel="noopener ugc nofollow" target="_blank">线性支持向量、</a>和<a class="ae lb" href="http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html" rel="noopener ugc nofollow" target="_blank">神经网络</a></li></ol><h1 id="0d80" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">1.模拟</h1><p id="75f5" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">我们使用两个文档库。来自斯坦福的大型电影评论数据集<a class="ae lb" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">用于二元情感分类，来自 scikit pages 的 reuter </a><a class="ae lb" href="http://scikit-learn.org/stable/datasets/twenty_newsgroups.html" rel="noopener ugc nofollow" target="_blank"> 20-news </a>用于多元分类。为了简单起见，我们坚持使用单一的训练集和单一的测试集。在 20 条新闻的情况下，我们进行分层分割，80%用于训练，20%用于测试。imdb 电影评论数据集带有定义的训练和测试集。下面代码片段中的第 9 行和第 10 行使用了一个令牌类(查看<a class="ae lb" href="https://github.com/ashokc/Word-Embeddings-and-Document-Vectors" rel="noopener ugc nofollow" target="_blank"> github </a> code repo ),它有从索引中提取令牌的方法。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mw"><img src="../Images/fccb27558646ec3759245cedfc242209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wmMgu8h95KWrc92LWgDT3A.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Code Listing 1. Train and Test data sets. Define a pipeline instance for fit and predict.</figcaption></figure><p id="173b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要运行的模型是管道的一个实例——矢量器、转换器和分类器的特定组合。第 13 行到第 15 行定义了模型并运行预测。模拟是通过下面的 shell 脚本运行的，该脚本循环遍历管道步骤的不同选项。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nf"><img src="../Images/ffb7bf7d2578242dedee39569c060df1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rvz3xJvmriKAOsZLWrFRpw.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Code Listing 2. A script to evaluate different options for the pipeline steps</figcaption></figure><p id="b26d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">mlp 分类器实际上由 9 种变体组成，每种变体有 1、2 或 3 个隐藏层，每层有 50、100 或 200 个神经元。此处报告的结果是针对使用 2 个隐藏层，每个隐藏层有 100 个神经元的情况。其他<em class="ll"> mlp </em>分类器运行基本上用于验证分类的质量在隐藏层和神经元的这个级别不是非常敏感。</p><p id="bd68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，有些组合是不允许的，在 Python 实现中会被跳过。这些是:</p><ol class=""><li id="ded8" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">朴素贝叶斯分类器不允许文档向量中有负值。但是我们用文档+词向量的时候，<em class="ll"> Z </em>会有一些否定。应该有可能统一转换/缩放所有矢量以避免负面影响，但我们并不担心，因为我们有足够的模拟来运行。所以基本上朴素贝叶斯分类器在这里只用于纯文档向量。</li><li id="845e" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la lh li lj lk bi translated">预先训练的词向量只适用于正常的词，不适用于词干化的词。因此，我们通过词干标记和预训练向量的组合来跳过运行。</li></ol><h1 id="3fcc" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">2.结果</h1><p id="ad24" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">我们看的唯一度量是分类质量的 F 分数和效率的 cpu 时间。在 20 个新闻数据集的多类分类的情况下，F 分数是所有 20 个类的平均值。分类器+向量组合的运行时间是使用相同组合的所有运行的平均值。</p><h2 id="d719" class="ng ma iq bd mb nh ni dn mf nj nk dp mj ko nl nm ml ks nn no mn kw np nq mp nr bi translated">2.1 20 条新闻数据集的多类分类</h2><p id="1d42" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">下面的图 1 总结了 20 条新闻数据集的结果。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ns"><img src="../Images/6669af6d3750403d00525b722d9dc5a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*77aMqWzMRiAnvu8fnnEo7g.jpeg"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Figure 1. Nulticlass classification of the 20-news data set. (1A) Classification with pure document vectors (1B) Classification with document+word vectors (1C) Run time with pure document vectors and document+word vectors</figcaption></figure><p id="6c3f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的图中包含了很多细节，所以让我们逐点总结一下。</p><p id="0396" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 1。文档向量 vs 文档+单词向量</strong>:看一眼 1A 和 1B，我们会发现 1A 的分类质量更好，也许好不了多少，但却是真实的，而且是全面的。也就是说，如果分类质量是最重要的，那么在这种情况下，文档向量似乎具有优势。</p><p id="0108" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 2。Stopped vs Stemmed </strong> : Stemmed 词汇产生更短的向量，因此对所有分类器的性能更好。对于<em class="ll"> mlp </em>分类器来说尤其如此，在这种分类器中，输入神经元的数量等于输入文档向量的大小。当单词被词干化时，唯一单词的数量从 39k 下降到 28k，减少了大约 30%,这大大减小了纯文档向量的大小。</p><ul class=""><li id="263b" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la nt li lj lk bi translated"><em class="ll">文档向量</em>。图 1A 表明，当纯文档向量是分类的基础时，对获得的 F 分数没有实质性影响。</li><li id="0f24" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la nt li lj lk bi translated"><em class="ll">文档+词向量</em>。然而，在这种情况下，使用词干标记似乎有一些好处。虽然改进很小，但通过对词干化的标记进行训练获得的定制向量显示出比对停止的标记进行训练的向量更好的 F 分数。这如图 1B 所示。</li></ul><p id="5303" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 3。频率计数与 Tf-Idf: </strong> Tf-Idf 矢量化允许根据单词在语料库中出现的频率对单词进行不同的加权。对于基于关键字的搜索方案，它有助于提高搜索结果的相关性。</p><ul class=""><li id="680a" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la nt li lj lk bi translated"><em class="ll">文件向量</em>。虽然<em class="ll">朴素贝叶斯</em>没有被 tf-idf 打动，但是<em class="ll"> linearsvc </em>和<em class="ll"> mlp </em>分类器通过 tf-idf 矢量化产生了更好的 F 分数。这如图 1A 所示。</li><li id="6118" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la nt li lj lk bi translated"><em class="ll">文档+词向量。</em>图 1B 显示，使用 tf-idf 矢量化后，F 值有了很大的提高。既有预先训练的词向量，也有定制的词向量。唯一的例外似乎是当预训练的 word2vec 向量与<em class="ll"> mlp </em>分类器结合使用时。但是将隐藏层的数量从 2 增加到 3，将神经元的数量从 100 增加到 200，tf-idf 矢量化又得到了更好的分数。</li></ul><p id="dd88" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 4。预训练矢量与定制矢量:</strong>这仅适用于图 1B。自定义单词向量似乎有优势。</p><ul class=""><li id="fe69" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la nt li lj lk bi translated">自定义向量显然会产生更好的 F 值，尤其是使用 tf-idf 矢量化时</li><li id="4354" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la nt li lj lk bi translated">预训练的向量似乎稍微好一点</li></ul><p id="3fee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 5。计时结果:</strong>图 1C 显示了 fit &amp;预测运行的平均 cpu 时间。</p><ul class=""><li id="cde0" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la nt li lj lk bi translated">当使用纯文档向量时，<em class="ll"> mlp </em>分类器的运行时间更长是可以理解的。有许多(39k，如果停止和 28k，如果阻止)输入神经元工作。这是我们在之前的文章中讨论的单词嵌入出现的原因之一。</li><li id="59c9" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la nt li lj lk bi translated">使用更小但更密集的<em class="ll">Z</em>,<em class="ll">linear SVC</em>分类器需要更长时间才能收敛。</li><li id="0f69" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la nt li lj lk bi translated">朴素贝叶斯分类器是所有分类器中最快的。</li></ul><h2 id="3ada" class="ng ma iq bd mb nh ni dn mf nj nk dp mj ko nl nm ml ks nn no mn kw np nq mp nr bi translated">2.2 电影评论的二元分类</h2><p id="80f9" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">下面的图 2 显示了从电影评论数据集获得的二进制分类结果。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nu"><img src="../Images/9c4c8f1893bb602f5b30a3a141d62563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2zf-RZSbDS_0tij5A6_98w.jpeg"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Figure 2. Binary classification of the movie review data set. (2A) Classification with pure document vectors (2B) Classification with document+word vectors (2C) Run time with pure document vectors and document+word vectors</figcaption></figure><p id="a8af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的观察与上面的没有本质上的不同，所以我们不会在这上面花太多时间。这里分类的整体质量更好，F 值在 0.8 以上(相比之下，20 个新闻语料库的 F 值约为 0.6)。但这只是这个数据集的本质。</p><ol class=""><li id="5a6e" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated"><strong class="kh ir">文档向量 vs 文档+单词向量:</strong>看一下 2A 和 2B，我们可以说文档+单词向量似乎总体上在分类质量上有优势(除了<em class="ll"> linearsvc </em>与 tf-idf 一起使用的情况)。20 条新闻数据集的情况正好相反。</li><li id="e3e2" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la lh li lj lk bi translated"><strong class="kh ir">Stopped vs Stemmed</strong>:Stopped 令牌似乎在大多数情况下表现更好。在 20 条新闻的数据集上，情况正好相反。词干化导致词汇表的大小减少了 34%,从 44k 减少到 29k。我们对两个数据集应用了相同的词干分析器，但是对于这个语料库中的文本的性质来说，这可能太激进了。</li><li id="1c6b" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la lh li lj lk bi translated"><strong class="kh ir">频率计数与 Tf-Idf: </strong> Tf-Idf 向量在大多数情况下表现更好，就像它们在 20 个新闻数据集的表现一样。</li><li id="b967" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la lh li lj lk bi translated"><strong class="kh ir">预训练向量与定制向量:</strong>定制单词向量在所有情况下都比预训练向量产生更好的 F 分数。这在某种程度上证实了我们对 20 条新闻数据的评估，而这些数据并不那么清晰。</li><li id="8457" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la lh li lj lk bi translated">计时结果:朴素贝叶斯仍然是最好的。</li></ol><h1 id="a09d" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">3.那么有哪些大的收获呢？</h1><p id="9aeb" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">不幸的是，我们不能仅仅根据对两个数据集进行的有些肤浅的测试就得出明确的结论。此外，正如我们上面提到的，在数据集之间，同一管道的分类质量也存在一些差异。但是从高层次上来说，我们或许可以得出以下结论——不要全信！</p><p id="b56e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 1。当有疑问时——简化。</strong>很明显，这是这篇文章的标题。按照下面的方法做，你就不会犯太大的错误，你会很快完成你的工作。使用:</p><ul class=""><li id="be84" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la nt li lj lk bi translated">朴素贝叶斯分类器</li><li id="e5af" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la nt li lj lk bi translated">Tf-idf 文档向量</li><li id="6a8e" class="lc ld iq kh b ki lm kl ln ko lo ks lp kw lq la nt li lj lk bi translated">想干就干。</li></ul><p id="d89e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 2。理解语料库。</strong>适用于一个语料库的特定流水线(标记化方案= &gt;矢量化方案= &gt;单词嵌入算法= &gt;分类器)对于不同的语料库可能不是最好的。即使通用管道可以工作，细节(特定词干分析器、隐藏层数、神经元等)也需要调整，以在不同的语料库上获得相同的性能。</p><p id="99ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 3。单词嵌入很棒。</strong>对于维数减少和运行时间的并行减少来说，肯定的。他们工作得很好，但还有更多工作要做。如果你必须使用神经网络进行文档分类，你应该试试这些。</p><p id="c096" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 4。使用自定义矢量</strong>。使用从手边的语料库生成的定制单词向量可能产生更好质量的分类结果</p></div></div>    
</body>
</html>