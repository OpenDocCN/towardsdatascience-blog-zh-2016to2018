<html>
<head>
<title>Understanding and visualizing SE-Nets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解和可视化 SE-net</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-and-visualizing-se-nets-1544aff0fc68?source=collection_archive---------8-----------------------#2018-10-18">https://towardsdatascience.com/understanding-and-visualizing-se-nets-1544aff0fc68?source=collection_archive---------8-----------------------#2018-10-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c7fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章可以在 PDF <a class="ae kl" href="http://www.pabloruizruiz10.com/resources/CNNs/SE_Nets.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><p id="7a5e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是 CNN 架构系列文章<a class="ae kl" href="https://medium.com/@pabloruizruiz/deep-convolutional-neural-networks-ccf96f830178" rel="noopener">的一部分。</a></p><p id="24e6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">主要目的是提供理解 SE-Nets 的洞察力。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/ce9c7d592c7a481a9c02587e99d4e05d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JAs1Jai3SUfjMWaNb8YLUg.png"/></div></div></figure><p id="84c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">指数</strong></p><p id="6975" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(1)有哪些创新点？</p><ul class=""><li id="9714" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">新数据块与新网络</li><li id="f289" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">解决什么问题？</li><li id="5804" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">什么是 SE 块？</li></ul><p id="e225" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(2)结构</p><ul class=""><li id="af04" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">转换</li><li id="e622" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">挤压</li><li id="60e2" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">激发，兴奋</li><li id="b712" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">缩放比例</li></ul><p id="dafb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(3)SE 模块的应用</p><p id="74d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(4)结果和结论</p><p id="7344" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(5)全球图景</p><h1 id="33a3" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">有哪些创新？</h1><h2 id="6969" class="mk ln iq bd lo ml mm dn ls mn mo dp lw jy mp mq ma kc mr ms me kg mt mu mi mv bi translated">新块而不是新网络</h2><p id="7b91" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">首先，本文作者并没有试图通过开发一种新颖的 CNN 架构来提高经典计算机视觉竞赛的艺术水平。然而，非常有趣的是，作者创建了一个可以与现有模型一起使用的<strong class="jp ir">模块来增强它们的性能</strong>。</p><p id="7897" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个新的模块在完成两个主要操作后被命名为 SE 模块:挤压和激励</p><h2 id="c813" class="mk ln iq bd lo ml mm dn ls mn mo dp lw jy mp mq ma kc mr ms me kg mt mu mi mv bi translated">动机——这些积木解决了什么问题？</h2><p id="7c66" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">作者声称卷积的输出导致与滤波器捕获的空间相关性纠缠的信道依赖性。哇，这听起来太疯狂了！但是不要担心，在一些想象之后，这变得非常简单。</p><p id="cf47" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么作者想用这些积木做什么来解决这个问题呢？</p><p id="7989" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><p id="0581" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nb">他们的目标是通过利用选通网络对通道相互依赖性进行显式建模来提高网络的灵敏度，选通网络出现在 se 模块中</em></p><p id="8d14" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><h2 id="a78a" class="mk ln iq bd lo ml mm dn ls mn mo dp lw jy mp mq ma kc mr ms me kg mt mu mi mv bi translated">那么什么是 SE 块呢？</h2><p id="56e9" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">简而言之，SE 模块是通道关系中的轻量级门控机制。</p><p id="271f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简而言之，网络现在能够学习如何理解在卷积运算<a class="ae kl" href="#_ftn1" rel="noopener ugc nofollow">【1】</a>之后提取的所有特征图的堆栈中的每个特征图的重要性，并且在将卷传递到下一层之前重新校准该输出以反映该重要性。</p><p id="3165" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所有这些都将在结构部分详细介绍，所以如果您现在没有看到某些内容，也不必担心！</p><p id="a6e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">但是等一下，为了学习，我们需要更多的参数，对吗？</strong></p><p id="7876" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">没错。门控机制或门控网络只不过是完全连接的层。这种技术在注意机制中被大量使用。我强烈推荐<a class="ae kl" href="https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129" rel="noopener">这篇文章</a>来更好地理解注意力机制和门控网络。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="2be3" class="lm ln iq bd lo lp nj lr ls lt nk lv lw lx nl lz ma mb nm md me mf nn mh mi mj bi translated">结构</h1><p id="400e" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">让我们首先来看看图 1 中的图。我们可以称这个图为简化图，因为我们总是想要更多的细节！</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi no"><img src="../Images/0e8c1affd9d458ae3ae64e1b2443cad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yx_iF7HEcmxtr5QpzndvfA.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Figure 1. The SE Block from the paper.</figcaption></figure><p id="4210" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它显示了所提供的插图，我在上面分离了它的三个主要部分。重要的是要看到，挤压和激励步骤只发生在中间模块，而第一个和最后一个模块执行不同的操作。</p><p id="e98b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在一次展示详细的图片之前，让我们一部分一部分地阐述它。</p><h1 id="aa3e" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">1 —转换</h1><p id="4c8c" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">该转换简单地对应于将要实现 SE 模块的网络在其自然方案中执行的操作。例如，如果你在一个 ResNet 内的一个块中，<strong class="jp ir"> <em class="nb"> F </em> </strong> <em class="nb"> tr </em>项将对应于整个残差块的处理(卷积、批量归一化、ReLU…)。因此:</p><p id="62e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><p id="1a53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nb">SE 块应用于变换操作的输出体，通过校准提取的特征来丰富它。</em></p><p id="dec9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><p id="6a81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，如果 ResNet 在某个点将输出卷 X，则 SE 块丰富它包含的信息，并传递到下一层，成为 X`。(不能包含上面带~的 X)。</p><p id="dd1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了简化可视化，让我们假设变换是一个简单的卷积运算。我们已经在<a class="ae kl" href="http://pabloruizruiz10.com/aiblog/cv/cv.html" rel="noopener ugc nofollow" target="_blank">之前的文章</a>中描述了卷积运算，但是，让我们试着稍微详细一点，以便更好地理解接下来会发生什么。</p><p id="a7f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我之所以声称这第一步很重要，是因为作者构建 se 块的动机就在于此。</p><p id="49e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><p id="621c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nb">对于常规卷积层，是一组滤波器</em><strong class="jp ir"><em class="nb"/></strong><em class="nb">被学习来表达沿着输入通道的局部空间连通性模式</em> <strong class="jp ir"> <em class="nb"> X </em> </strong> <em class="nb">因此卷积滤波器是局部感受域</em>  <em class="nb">内的</em> <strong class="jp ir"> <em class="nb">通道方式信息的组合。</em></strong></p><p id="2fe6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><p id="7b40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们看一下对应于图 2 的符号，我们有:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nt"><img src="../Images/0f7b6c03b35104802877c8e2cc048dc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*glVBaLqoRxSiO8IVb1jE7Q.png"/></div></div></figure><p id="afc1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><p id="bdc3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nb">他们所说的</em> <strong class="jp ir"> <em class="nb">局部感受野</em> </strong> <em class="nb">恰恰是每个通道 2 的空间</em><strong class="jp ir"><em class="nb">V</em></strong><em class="nb">c，s 在每个过滤器</em><strong class="jp ir"><em class="nb"/></strong><em class="nb">c .</em></p><p id="8378" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><p id="0e83" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么信息是渠道方面的？</p><p id="b438" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嗯，请注意滤波器<strong class="jp ir"> V </strong> 1 的标记<strong class="jp ir"> V </strong> 1，s 将仅在输入音量<strong class="jp ir"> X </strong> s 的通道<em class="nb"> s </em>上卷积。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nu"><img src="../Images/f4519b5ab5b8fe939160d2106055d8d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c_bL3cjFdJFcf_R-vBwhmw.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Figure 2. Step1: Transformation</figcaption></figure><p id="a632" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们也可以按通道来表示卷积运算，以进行双重检查:</p><p id="c882" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在体积<strong class="jp ir"> X </strong>中卷积 1 个单个滤波器<strong class="jp ir"> Vc </strong>的结果在单个特征图上，<strong class="jp ir"> U </strong> c:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/0a1c1c1ab83a0f02c35283e4dddbf46d.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*tvMvd1kwJk06iB8mo4WWEQ.png"/></div></figure><p id="8368" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">换句话说，一个滤波器在体积<strong class="jp ir"> X </strong>上的卷积是输入体积<strong class="jp ir"> X </strong> s 的每个通道与其在滤波器<strong class="jp ir"> V </strong> c，s 中的对应通道的所有通道卷积的总和</p><h1 id="4576" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">2 —挤压</h1><p id="2408" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">挤压步骤可能是最简单的一步。它基本上在每个通道上执行<strong class="jp ir">平均汇集，以创建一个 1x1 <em class="nb">压缩</em>的卷<strong class="jp ir"> U </strong>表示。</strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nw"><img src="../Images/2d2f21db2256bcd0f9ec70ab7a15c740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*95h7WkdABk8FISbmVExg4Q.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Figure 3. Squeezing</figcaption></figure><h1 id="3af8" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">3 —激发</h1><p id="aa3b" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">这是 SE 模块整个成功的关键部分，所以请注意。这确实是一个文字游戏，我们将使用一个使用门控网络的注意力机制: )</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/c2241c717ab49aa8878c23cc2a0d86d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*anil6yGb2cuig0JZ8tIpQA.png"/></div></figure><p id="1b36" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作者引入了一个称为<strong class="jp ir">缩减比</strong> <em class="nb"> r </em>的新参数，在 sigmoid 激活的选通网络之前，引入第一个具有 ReLU 激活的全连接(FC)层。</p><p id="184c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这样做的原因是引入一个瓶颈，它允许我们在引入新的非线性的同时降低维度。</p><p id="9022" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，我们可以更好地控制模型的复杂性和辅助网络的泛化性能。</p><p id="1858" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">具有两个 FC 层将导致具有两个权重矩阵，这两个权重矩阵将由网络在训练期间以端到端的方式学习(所有权重矩阵都与卷积核一起反向传播)。</p><p id="0cf4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该函数的数学表达式得出:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ny"><img src="../Images/3473d7ccac5ae1da48a4e88d36c33b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q9_cCt-Qwg4XnWWdO3nVQQ.png"/></div></div></figure><p id="5585" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们已经让我们的压缩特征地图兴奋了！</p><p id="43fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">看看这些激励只不过是一对经过训练的神经网络，以便在训练过程中更好地校准这些激励。</strong></p><h1 id="d4e8" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">3.规模</h1><p id="82c9" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">最后一步，缩放，确实是重新缩放操作。我们将赋予压缩矢量其原始形状，保持在激发步骤中获得的信息。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nz"><img src="../Images/075770aa3f73fb1e496a96447a679aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gpVVgKaXpCACcNvZ27Tv2g.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Figure 5. Rescaling</figcaption></figure><p id="29ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在数学上，缩放是通过输入体积上的每个通道与激活的 1x1 压缩向量上的相应通道的简单标量积来实现的。</p><p id="0a1b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于 1 个频道:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi oa"><img src="../Images/7fbd7316f248d8857e86f6e13313cb60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aXEJwDfQfdDk1gMnVnrMOg.png"/></div></div></figure><p id="88c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nb">作者讨论:</em></p><p id="fdbf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">激活充当适应于输入特定描述符<strong class="jp ir"> z </strong>的通道权重。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="0a5b" class="lm ln iq bd lo lp nj lr ls lt nk lv lw lx nl lz ma mb nm md me mf nn mh mi mj bi translated">硒块的应用</h1><p id="3f79" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">如作为创新之一介绍的，SE 块不是新的神经网络架构。它试图通过使现有模型对渠道关系更加敏感来改进现有模型。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ob"><img src="../Images/ea9992a95738ace8a3093b7ea9352d7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kQiTCxE8yyZFVYBCeQgyQw.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Figure 6. SE block applied to Inception (left) and ResNets (right) modules</figcaption></figure><p id="c3eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图 6 显示了如何在卷积层的输出音量之后应用相同的 SE 块，以在将其移动到下一层之前丰富其表示。在这种情况下，显示了 Inception 和 ResNets 模块。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="948b" class="lm ln iq bd lo lp nj lr ls lt nk lv lw lx nl lz ma mb nm md me mf nn mh mi mj bi translated">结果和结论</h1><p id="3f9b" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">结果证实，在最先进的网络中引入 SE 块提高了它们在不同计算机视觉应用中的性能:图像网络分类、场景分类(Places365-Challenge 数据集)和对象检测(COCO 数据集)。这些结果太宽泛了，不包括在这个总结中，我建议在原始论文中查看它们。</p><p id="1fdb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">折减系数</strong>的经验研究值带来了一个有趣的观察结果。事实证明，16 是一个很好的值，超过这个值，性能不会随着模型容量的增加而单调地提高。</p><p id="9405" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><p id="dad9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nb">SE 块可能会过度拟合通道相关性</em></p><p id="4ba7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><p id="a1fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，作者在每个阶段(或层)对 50 个均匀采样的通道执行平均激活，其中为 5 个显著不同的类别引入了 SE 块。已经观察到，跨不同类别的分布在较低层中几乎<strong class="jp ir">相同，而每个通道的值在更大深度</strong>处变得更加<strong class="jp ir">类别特定。</strong></p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="cfa2" class="lm ln iq bd lo lp nj lr ls lt nk lv lw lx nl lz ma mb nm md me mf nn mh mi mj bi translated">全球图片</h1><p id="2b11" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">将所有东西放回一起，SE 块: )</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi oc"><img src="../Images/345dcc1450b17a3fa0c4dc1d1e67c7e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hqw2vL_PnApqi2tz8NjbNg.png"/></div></div></figure></div></div>    
</body>
</html>