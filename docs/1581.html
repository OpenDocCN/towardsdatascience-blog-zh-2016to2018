<html>
<head>
<title>Normalized center loss for language modeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语言建模的归一化中心损失</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/normalized-center-loss-for-language-modeling-5cd7b40d67e1?source=collection_archive---------2-----------------------#2017-09-22">https://towardsdatascience.com/normalized-center-loss-for-language-modeling-5cd7b40d67e1?source=collection_archive---------2-----------------------#2017-09-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="d2c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">警告:一些递归神经网络的知识是假定的。</p><h2 id="845b" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">语言建模及其工作原理简介？</h2><blockquote class="le lf lg"><p id="44b9" class="jn jo lh jp b jq jr js jt ju jv jw jx li jz ka kb lj kd ke kf lk kh ki kj kk ij bi translated">什么是语言建模？</p></blockquote><p id="9c83" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在语言建模中，我们试图预测给定单词序列的下一个单词。机器学习模型计算下一个单词的可能值的概率。并且从生成的概率分布中对单词进行采样。</p><blockquote class="le lf lg"><p id="98ce" class="jn jo lh jp b jq jr js jt ju jv jw jx li jz ka kb lj kd ke kf lk kh ki kj kk ij bi translated">问:它是如何工作的？</p></blockquote><p id="f4ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你建立一个递归神经网络架构。该模型一次处理一个单词，并计算下一个单词的可能值的概率。网络的存储状态由零向量初始化，并在读取每个字后更新。<br/>RNN的输出依赖于任意远的输入，这使得反向传播很困难。为了使学习过程易于处理，我们将反向传播缩短到固定的步骤数(我们称之为<code class="fe ll lm ln lo b">num_steps</code>)。然后，根据RNN的有限近似值训练模型。这可以通过一次输入长度为<code class="fe ll lm ln lo b">num_steps</code>的输入，并在每个这样的输入块后执行反向传递来实现。</p><p id="ef7e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用于评估语言模型的度量是困惑度，其等于exp(交叉熵)。</p><p id="df2c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作为我之前博文的延续，我想看看增加一个中心损失是否会改善模型的困惑。</p><h2 id="b0c4" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">归一化中心损耗</h2><p id="e604" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">我建议读者在阅读这篇文章之前，先阅读前一篇博文的第一部分。</p><p id="e050" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当使用博客文章中定义的相同中心损失时，中心损失将呈指数增加(每200次迭代增加一倍)，并最终爆炸。为了克服这个问题，我稍微修改了一下中心损耗，我把修改后的版本叫做<em class="lh">归一化中心损耗</em>。</p><p id="35b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在归一化中心损失中，我在更新它们之后归一化中心，使得每个嵌入的中心向量的范数是1。这样做是为了防止损耗值爆炸。</p><p id="d822" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了计算损失，通过相应嵌入的大小来缩放单词向量的中心。这确保了嵌入得到将它们推向中心向量的梯度。</p><h1 id="41cf" class="lv km iq bd kn lw lx ly kq lz ma mb kt mc md me kw mf mg mh kz mi mj mk lc ml bi translated">结果</h1><p id="911e" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">我在wikitext-2和penn treebank数据集上尝试了这种新的损失方法。结果如下:</p><h2 id="31f0" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">penn treebank数据集上的结果</h2><div class="mm mn mo mp gt ab cb"><figure class="mq mr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/94e30a1b7e5686429e027cbbdff41680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*jL6HpyxcSj-sZ0e24L4mWg.png"/></div></figure><figure class="mq mr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/505717cb1b2a3b220f6061bdb46fe943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Pn6H4wagG6lx-syRxN_6Ag.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk nh di ni nj">cross entropy loss and perplexity on training set</figcaption></figure></div><div class="ab cb"><figure class="mq mr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/1267b1728fb46972acb3f60faa31a340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*zI7Uc3-iAEC4ui3-Hh-zlg.png"/></div></figure><figure class="mq mr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/82577a7259fc582fedc7faeee27da7cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*1XQ4tYFR-sCUl_pOnbVK8w.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk nh di ni nj">cross entropy loss and perplexity on validation set</figcaption></figure></div><p id="14b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从图中可以看出，在验证集上尝试的所有lambda值的复杂度都有所提高。</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nk"><img src="../Images/03ffd4dd15b1fc18a1569247bab4d736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KGHHDbTRVdEmPJp2_zjHAQ.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Values of cross entropy and perplexity values on the test set</figcaption></figure><p id="2e81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在测试集上提高了4，这实际上是非常显著的。</p><h2 id="85b2" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">wikitext-2数据集上的结果</h2><div class="mm mn mo mp gt ab cb"><figure class="mq mr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/8eac3af7727ff97d957112c81b06d4af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*bm0z6uQrTXyo4jN6u-lRSw.png"/></div></figure><figure class="mq mr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/b16eacae5fcf5dcef95dec36e36c4a2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*2KI00IhtoBhdPz1IF4BUEA.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk nh di ni nj">cross entropy loss and perplexity on training set</figcaption></figure></div><div class="ab cb"><figure class="mq mr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/1267b1728fb46972acb3f60faa31a340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*zI7Uc3-iAEC4ui3-Hh-zlg.png"/></div></figure><figure class="mq mr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/82577a7259fc582fedc7faeee27da7cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*1XQ4tYFR-sCUl_pOnbVK8w.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk nh di ni nj">cross entropy loss and perplexity on validation set</figcaption></figure></div><p id="f6a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从图中可以再次看出，在验证集上尝试的所有λ值上，复杂度都有所提高。</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nk"><img src="../Images/2d75e6b9daa122f352594c2560f7bfe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NTfZcuIZ06dilxZpy6Oqdg.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Values of cross entropy and perplexity values on the test set</figcaption></figure><p id="dbab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在测试集上提高了2，这也是显著的。这里的结果不像宾州树木银行那样令人印象深刻。我认为这是因为归一化损失函数作为正则化函数。由于wikitext-2比Penn Treebank数据集大得多，添加正则化子的影响被最小化。</p><h1 id="1f28" class="lv km iq bd kn lw lx ly kq lz ma mb kt mc md me kw mf mg mh kz mi mj mk lc ml bi translated">结论</h1><p id="6e5e" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">由于在两个数据集上的实验中可以观察到复杂度的改善，因此可以预期的是，任何鼓励来自相同类别的特征彼此靠近地聚类的损失函数将导致精确度的改善和过拟合的减少。</p><p id="da9b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本博客中使用的所有实验都可以使用本报告中给出的代码进行复制。</p><blockquote class="le lf lg"><p id="e081" class="jn jo lh jp b jq jr js jt ju jv jw jx li jz ka kb lj kd ke kf lk kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="iq">如果你喜欢这篇文章，请点击下面的小拍手图标帮助他人找到它。非常感谢！</em> </strong></p></blockquote></div></div>    
</body>
</html>