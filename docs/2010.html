<html>
<head>
<title>Stochastic Gradient Descent with momentum</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带动量的随机梯度下降</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d?source=collection_archive---------2-----------------------#2017-12-04">https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d?source=collection_archive---------2-----------------------#2017-12-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/40d041d8767a23889096ab842b8e6721.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BSkYc6Qe4lK2jDmyZsmHOA.jpeg"/></div></div></figure><div class=""/><p id="9465" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是我关于用于训练神经网络和机器学习模型的优化算法系列的第 2 部分。<a class="ae kw" rel="noopener" target="_blank" href="/how-do-we-train-neural-networks-edd985562b73">第一部分</a>讲的是随机梯度下降。在这篇文章中，我假设关于神经网络和梯度下降算法的基本知识。如果你对神经网络或者如何训练神经网络一无所知，在阅读这篇文章之前，请随意阅读我的第一篇文章。</p><p id="7013" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇文章中，我将讨论经典 SGD 算法的简单加法，叫做动量法，它几乎总是比随机梯度下降法更好更快。Momentum [1]或 SGD with momentum 是一种有助于在正确方向上加速梯度向量的方法，从而导致更快的收敛。它是最流行的优化算法之一，许多先进的模型都是用它来训练的。在跳到算法的更新方程之前，让我们看看动量功的数学基础。</p><h1 id="8a92" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">指数加权平均值</h1><p id="511d" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">指数加权平均值处理的是数字序列。假设，我们有一些有噪声的序列。对于这个例子，我绘制了余弦函数，并添加了一些高斯噪声。看起来是这样的:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/d30e7f3449e96b3d87788141ff3eaadc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*U5g-MNIKrZjVnI12ePtbLw.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Out sequence.</figcaption></figure><p id="df1e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请注意，尽管这些点看起来非常接近，但它们都没有共享<em class="mj"> x </em>坐标。对于每个点来说，这是一个唯一的数字。这个数字定义了序列中每个点的索引。</p><p id="be6e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们想对这些数据做的是，不是使用它，而是想要某种“移动”平均值，这将对数据进行“去噪”，并使其更接近原始函数。指数加权平均值可以给我们一个看起来像这样的图片:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/bd935bf117f9b5bfd6e0aec2bc2e9d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*fhHakQ1nWN7HK1KBNdarqw.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">momentum — data from exponentially weighed averages.</figcaption></figure><p id="dba1" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如你所见，这是一个非常好的结果。我们得到的不是有很多噪音的数据，而是更平滑的线，比我们得到的数据更接近原始函数。指数加权平均值用下面的等式定义了新的序列 V:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/2bd0f2d5bdab1441ef91fbae72997aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*KQC1UiYUxdzA5IsSEg4Gow.png"/></div></figure><p id="0b90" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">序列 V 就是上面标为黄色的那个。<em class="mj"> Beta </em>是另一个超参数，取值从 0 到 1。上面我用了<em class="mj"> beta </em> = 0.9。这是一个很好的值，最常用于带动量的 SGD。直觉上，你可以把<em class="mj"> beta </em>想成如下。我们对序列的最后<em class="mj">1/(1-β)</em>个点进行近似平均。让我们看看<em class="mj">β</em>的选择如何影响我们的新序列 v。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/a0c699a848b317b466f0b92bff031c35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*buj-RJg3wW6RSclnpczkzA.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Exponentially weighed averages for different values of beta.</figcaption></figure><p id="d684" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如您所见，beta 值越小，新序列的波动就越大，因为我们对更小数量的样本求平均，因此更接近噪声数据。β值越大，如<em class="mj">β= 0.98</em>，我们得到的曲线越平滑，但它会向右偏移一点，因为我们对大量的示例进行了平均(对于<em class="mj">β= 0.98</em>约为 50)。<em class="mj"> Beta = 0.9 </em>在这两个极端之间提供了一个很好的平衡。</p><h1 id="1a3e" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">数学部分</h1><p id="3948" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">这一部分对于在你的项目中使用动量是不必要的，所以可以跳过它。但是它提供了更多关于动量如何工作的直觉。</p><p id="0a0b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们扩展新序列 v 的三个连续元素的指数加权平均值的定义。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/873042ff79c74a2fdcd15cbe2ceedd05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*Fg2t0oqYaFIDx0EXevTy3g.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">V — New sequence. S — original sequence.</figcaption></figure><p id="ccf0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">将所有这些结合在一起，我们得到:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mn"><img src="../Images/45c0a3d67afb3b07e3f65be508828bc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EXYss1Dsx9OnrygLM3pkjQ.png"/></div></div></figure><p id="0e11" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后稍微简化一下:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mo"><img src="../Images/fa5eec55952883a036f9d5302d184e78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cDsuuj5wMAQ2AJrJ1AcXrA.png"/></div></div></figure><p id="d102" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从这个等式中我们看到，新序列的第<em class="mj">个</em>号的值依赖于所有先前的值<em class="mj"> 1..来自 S 的所有值都被赋予一定的权重。这个权重是 s 的第<em class="mj"> (t - i) </em>个值的<em class="mj">beta</em>I</em>的幂乘以<em class="mj"> (1- beta) </em>，因为 beta 小于 1，所以当我们将 beta 取某个正数的幂时，它会变得更小。因此，S 的旧值得到小得多的权重，因此对 v 的当前点的整体值的贡献更小。在某个点上，权重将变得如此之小，以至于我们几乎可以说我们“忘记”了该值，因为它的贡献变得小到无法注意到。当重量变得小于<em class="mj"> 1 / e </em>时，这是一个很好的近似值。更大的β值要求更大的功率值小于<em class="mj"> 1 / e </em>。这就是为什么β值越大，我们平均的点数就越多。下图显示了与 threshold = <em class="mj"> 1 / e </em>相比，较旧的 S 值的权重变小的速度，在 threshold =<em class="mj">1/e</em>中，我们通常会“忘记”较旧的值。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/018d3b03e1a9fcf87e87d4b0491c9ff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*ptkdnVKPzIXRbrdsKXY4PQ.png"/></div></figure><p id="a9ca" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后要注意的是，前几次迭代将提供一个非常糟糕的平均值，因为我们还没有足够的值来平均。解决方法是不使用 V，我们可以使用 V 的偏差修正版本。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/b94c054202e8ec9f33a3dffec49df9ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*Ysq5Noczr0Zsuyi-9kzk8A.png"/></div></figure><p id="dfa7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<em class="mj">b =β。t 的值很大时，b 的 t 次方与零没有区别，因此根本不会改变 V 的值。但是对于小 t 值，这个等式会产生稍微好一点的结果。有了动量，人们通常不会费心去实现这一部分，因为学习稳定得相当快。</em></p><h1 id="82dd" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">带动量的 SGD</h1><p id="a328" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们已经定义了一种方法来获得一些序列的“移动”平均值，它随着数据一起变化。我们如何将此应用于训练神经网络？可以平均我们的梯度。我将在 momentum 中定义它是如何实现的，然后继续解释为什么它可能会更好。</p><p id="efe9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我将给出 SGD 和动量的两种定义，它们几乎是写同一个方程的两种不同方式。首先，吴恩达在 coursera 上的深度学习专业课程中是如何定义它的。按照他的解释，我们定义了一个动量，它是我们梯度的移动平均值。然后我们用它来更新网络的权重。可以这样写:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mr"><img src="../Images/5b2c0703432c60c7b26dd515c62a8898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V5fNciao4YpMl0Of_8v2yw.png"/></div></div></figure><p id="dfeb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<em class="mj"> L </em> —是损失函数，三角形物—梯度 w.r.t 权重，<em class="mj"> alpha </em> —学习率。另一种方法是编写动量更新规则最流行的方法，它不太直观，只是省略了<em class="mj"> (1 - beta) </em>项。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ms"><img src="../Images/da08652a2eb92c1c54352d0877acd785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3YkczwuuU6xYBpA2HVtclw.png"/></div></div></figure><p id="5c68" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这与第一对等式非常相似，唯一的区别是你需要用<em class="mj"> (1 - beta) </em>因子来衡量学习率。</p><h1 id="551f" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">内斯特罗夫加速梯度</h1><p id="fe37" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated"><strong class="ka jc">内斯特罗夫 Momentum </strong>是 Momentum 更新的一个略有不同的版本，最近越来越受欢迎。在这个版本中，我们首先看当前动量指向的点，并从该点计算梯度。当你看这幅画时，它变得清晰多了。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mt"><img src="../Images/ef2353047851eec1891dc9db9931a61a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hJSLxZMjYVzgF5A_MoqeVQ.jpeg"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Source (<a class="ae kw" href="http://cs231n.github.io/neural-networks-3/" rel="noopener ugc nofollow" target="_blank">Stanford CS231n class</a>)</figcaption></figure><p id="dd8d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">内斯特罗夫动量可以用下面的公式来定义:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mu"><img src="../Images/f55ed26f78d0542eea76d9824cf596e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PgQ7P480s764tSqdp8Gjfw.png"/></div></div></figure><h1 id="81eb" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">为什么动量起作用</h1><p id="5dba" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">在这一节中，我想谈一点为什么动量在大多数时候会比经典 SGD 更好。</p><p id="b362" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于随机梯度下降，我们不计算损失函数的精确导数。相反，我们是在小批量的基础上估算的。这意味着我们并不总是朝着最佳方向前进，因为我们的导数是“嘈杂的”。就像我上面的图表一样。因此，指数加权平均值可以为我们提供更好的估计，比我们嘈杂的计算更接近实际的导数。这就是为什么动量理论可能比经典 SGD 更有效的原因之一。</p><p id="2c93" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">另一个原因在于沟壑。峡谷是这样一个区域，它的表面在一个维度上比在另一个维度上弯曲得更陡。深谷在深度学习的局部极小值附近是常见的，SGD 在导航它们时有困难。SGD 将倾向于在狭窄的峡谷中振荡，因为负梯度将指向陡峭的一侧，而不是沿着峡谷指向最优值。动量有助于加速正确方向的梯度。这表现在下面的图片中:</p><div class="mb mc md me gt ab cb"><figure class="mv is mw mx my mz na paragraph-image"><img src="../Images/4b8cbf80c5ac5ae91537b64146fc733d.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/1*JHYIDkzf1ImuZK487q_kiw.gif"/></figure><figure class="mv is mw mx my mz na paragraph-image"><img src="../Images/2646533a7433c69468d5535e9704d065.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/1*uTiP1uRl2CaHaA-dFu3NKw.gif"/><figcaption class="mf mg gj gh gi mh mi bd b be z dk nb di nc nd">Left — SGD without momentum, right— SGD with momentum. (Source: <a class="ae kw" href="https://www.willamette.edu/~gorr/classes/cs449/momrate.html" rel="noopener ugc nofollow" target="_blank">Genevieve B. Orr</a>)</figcaption></figure></div><h1 id="e24b" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">结论</h1><p id="ab5e" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我希望这篇文章能给你一些直觉，让你知道 SGD 如何以及为什么有动力。它实际上是深度学习中最流行的优化算法之一，甚至比更高级的算法使用得更频繁。</p><p id="943d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面我提供一些参考，在这里你可以学到更多关于深度学习中的优化。</p><h1 id="856b" class="kx ky jb bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">参考</h1><p id="4f60" class="pw-post-body-paragraph jy jz jb ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">[1]钱宁。<a class="ae kw" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.5612&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">关于梯度下降学习算法中的动量项</a>。神经网络:国际神经网络学会官方杂志，12(1):145–151，1999</p><p id="7143" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">【2】提炼，<a class="ae kw" href="https://distill.pub/2017/momentum/" rel="noopener ugc nofollow" target="_blank">为什么动量真的起作用</a></p><p id="7aff" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[3] deeplearning.ai</p><p id="b305" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[4] <strong class="ka jc">鲁德</strong> (2016)。<a class="ae kw" href="https://arxiv.org/abs/1609.04747" rel="noopener ugc nofollow" target="_blank">梯度下降优化算法概述</a>。arXiv 预印本 arXiv:1609.04747</p><p id="79ff" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[5] Ruder (2017) <a class="ae kw" href="http://ruder.io/deep-learning-optimization-2017/index.html" rel="noopener ugc nofollow" target="_blank">深度学习优化 2017 年亮点</a>。</p><p id="915f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[6] <a class="ae kw" href="http://cs231n.github.io/neural-networks-3/" rel="noopener ugc nofollow" target="_blank">斯坦福 CS231n 讲义</a>。</p><p id="f546" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">【7】<a class="ae kw" href="http://fast.ai" rel="noopener ugc nofollow" target="_blank">fast . ai</a></p></div></div>    
</body>
</html>