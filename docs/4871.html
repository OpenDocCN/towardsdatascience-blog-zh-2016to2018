<html>
<head>
<title>Multi-Layer perceptron using Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用张量流的多层感知器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-layer-perceptron-using-tensorflow-9f3e218a4809?source=collection_archive---------4-----------------------#2018-09-12">https://towardsdatascience.com/multi-layer-perceptron-using-tensorflow-9f3e218a4809?source=collection_archive---------4-----------------------#2018-09-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e4b4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">构建多层感知器和张量流的入门指南，用于构建数字识别系统。</h2></div><p id="f0ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇博客中，我们将使用 TensorFlow 构建一个神经网络(多层感知器)，并成功训练它识别图像中的数字。Tensorflow 是由发布的非常受欢迎的深度学习框架，本笔记本将指导使用该库构建神经网络。如果你想了解什么是多层感知器，你可以看看我之前的博客，在那里我用 Numpy 从头构建了一个多层感知器。</p><p id="1aef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们从导入数据开始。作为 Keras，一个高级深度学习库已经将 MNIST 数据作为其默认数据的一部分，我们只是从那里导入数据集，并将其分为训练集和测试集。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="f69c" class="ll lm iq lh b gy ln lo l lp lq"><em class="lr">## Loading MNIST dataset from keras</em><br/><strong class="lh ir">import</strong> <strong class="lh ir">keras</strong><br/><strong class="lh ir">from</strong> <strong class="lh ir">sklearn.preprocessing</strong> <strong class="lh ir">import</strong> LabelBinarizer<br/><strong class="lh ir">import</strong> <strong class="lh ir">matplotlib.pyplot</strong> <strong class="lh ir">as</strong> <strong class="lh ir">plt</strong><br/>%matplotlib inline</span><span id="3a2f" class="ll lm iq lh b gy ls lo l lp lq"><strong class="lh ir">def</strong> load_dataset(flatten=<strong class="lh ir">False</strong>):<br/>    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()</span><span id="116b" class="ll lm iq lh b gy ls lo l lp lq">    <em class="lr"># normalize x</em><br/>    X_train = X_train.astype(float) / 255.<br/>    X_test = X_test.astype(float) / 255.</span><span id="78e0" class="ll lm iq lh b gy ls lo l lp lq">    <em class="lr"># we reserve the last 10000 training examples for validation</em><br/>    X_train, X_val = X_train[:-10000], X_train[-10000:]<br/>    y_train, y_val = y_train[:-10000], y_train[-10000:]</span><span id="174e" class="ll lm iq lh b gy ls lo l lp lq">    <strong class="lh ir">if</strong> flatten:<br/>        X_train = X_train.reshape([X_train.shape[0], -1])<br/>        X_val = X_val.reshape([X_val.shape[0], -1])<br/>        X_test = X_test.reshape([X_test.shape[0], -1])</span><span id="f2a3" class="ll lm iq lh b gy ls lo l lp lq">    <strong class="lh ir">return</strong> X_train, y_train, X_val, y_val, X_test, y_test</span><span id="dc0d" class="ll lm iq lh b gy ls lo l lp lq">X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()<br/><em class="lr">## Printing dimensions</em><br/>print(X_train.shape, y_train.shape)<br/><em class="lr">## Visualizing the first digit</em><br/>plt.imshow(X_train[0], cmap="Greys");</span></pre><figure class="lc ld le lf gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lt"><img src="../Images/e6f55303d7c27210131ab2e6b3ff7650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7vs-BZLBTMGUsR0A3lzRxQ.png"/></div></div></figure><p id="6bd3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们所看到的，我们当前的数据的维数为 N <em class="lr"> 28* </em> 28，我们将从展平 N*784 中的图像开始，并对我们的目标变量进行一次性编码。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="0be1" class="ll lm iq lh b gy ln lo l lp lq"><em class="lr">## Changing dimension of input images from N*28*28 to  N*784</em><br/>X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]))<br/>X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]))</span><span id="260f" class="ll lm iq lh b gy ls lo l lp lq">print('Train dimension:');print(X_train.shape)<br/>print('Test dimension:');print(X_test.shape)</span><span id="b496" class="ll lm iq lh b gy ls lo l lp lq"><em class="lr">## Changing labels to one-hot encoded vector</em><br/>lb = LabelBinarizer()<br/>y_train = lb.fit_transform(y_train)<br/>y_test = lb.transform(y_test)<br/>print('Train labels dimension:');print(y_train.shape)<br/>print('Test labels dimension:');print(y_test.shape)</span></pre><figure class="lc ld le lf gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mb"><img src="../Images/0c9260296486f884a92fd772087b2dd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lQqhKWO-m9u5GYR2qxwZlw.png"/></div></div></figure><p id="eaca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们已经处理了数据，让我们开始使用 tensorflow 构建我们的多层感知器。我们将从导入所需的库开始。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="f3be" class="ll lm iq lh b gy ln lo l lp lq"><em class="lr">## Importing required libraries</em><br/><strong class="lh ir">import</strong> <strong class="lh ir">numpy</strong> <strong class="lh ir">as</strong> <strong class="lh ir">np</strong><br/><strong class="lh ir">import</strong> <strong class="lh ir">tensorflow</strong> <strong class="lh ir">as</strong> <strong class="lh ir">tf</strong><br/><strong class="lh ir">from</strong> <strong class="lh ir">sklearn.metrics</strong> <strong class="lh ir">import</strong> roc_auc_score, accuracy_score<br/>s = tf.InteractiveSession()</span></pre><p id="b6f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lr"> tf。InteractiveSession() </em>是一种无论何时我们想要运行一个模型，都可以直接运行 tensorflow 模型而无需实例化一个图的方法。我们将建立 784(输入)-512(隐藏层 1)-256(隐藏层 2)-10(输出)神经网络模型。让我们从定义初始化变量开始我们的模型构建。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="447d" class="ll lm iq lh b gy ln lo l lp lq"><em class="lr">## Defining various initialization parameters for 784-512-256-10 MLP model</em><br/>num_classes = y_train.shape[1]<br/>num_features = X_train.shape[1]<br/>num_output = y_train.shape[1]<br/>num_layers_0 = 512<br/>num_layers_1 = 256<br/>starter_learning_rate = 0.001<br/>regularizer_rate = 0.1</span></pre><p id="d1af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 tensorflow 中，我们为输入变量和输出变量以及任何我们想要跟踪的变量定义了一个占位符。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="d033" class="ll lm iq lh b gy ln lo l lp lq"><em class="lr"># Placeholders for the input data</em><br/>input_X = tf.placeholder('float32',shape =(<strong class="lh ir">None</strong>,num_features),name="input_X")<br/>input_y = tf.placeholder('float32',shape = (<strong class="lh ir">None</strong>,num_classes),name='input_Y')<br/><em class="lr">## for dropout layer</em><br/>keep_prob = tf.placeholder(tf.float32)</span></pre><p id="0f26" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为密集图层需要权重和偏差，并且需要使用零均值和小方差(1/要素数量的平方根)的随机正态分布进行初始化。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="2a42" class="ll lm iq lh b gy ln lo l lp lq"><em class="lr">## Weights initialized by random normal function with std_dev = 1/sqrt(number of input features)</em><br/>weights_0 = tf.Variable(tf.random_normal([num_features,num_layers_0], stddev=(1/tf.sqrt(float(num_features)))))<br/>bias_0 = tf.Variable(tf.random_normal([num_layers_0]))</span><span id="e11a" class="ll lm iq lh b gy ls lo l lp lq">weights_1 = tf.Variable(tf.random_normal([num_layers_0,num_layers_1], stddev=(1/tf.sqrt(float(num_layers_0)))))<br/>bias_1 = tf.Variable(tf.random_normal([num_layers_1]))</span><span id="cdfb" class="ll lm iq lh b gy ls lo l lp lq">weights_2 = tf.Variable(tf.random_normal([num_layers_1,num_output], stddev=(1/tf.sqrt(float(num_layers_1)))))<br/>bias_2 = tf.Variable(tf.random_normal([num_output]))</span></pre><p id="8122" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们将开始编写图形计算来开发我们的<em class="lr"> 784(输入)-512(隐藏层 1)-256(隐藏层 2)-10(输出)模型</em>。我们将把每一层的输入乘以其各自的权重，并添加偏差项。在权重和偏差之后，我们需要添加一个激活；我们将对隐藏层使用 ReLU activation，对最终输出层使用 softmax 来获得类概率分数。也是为了防止过度拟合；让我们在每个隐藏层后添加一些 drop out。辍学是在我们的网络中产生冗余的一个基本概念，这导致了更好的泛化。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="ec25" class="ll lm iq lh b gy ln lo l lp lq"><em class="lr">## Initializing weigths and biases</em><br/>hidden_output_0 = tf.nn.relu(tf.matmul(input_X,weights_0)+bias_0)<br/>hidden_output_0_0 = tf.nn.dropout(hidden_output_0, keep_prob)</span><span id="5abc" class="ll lm iq lh b gy ls lo l lp lq">hidden_output_1 = tf.nn.relu(tf.matmul(hidden_output_0_0,weights_1)+bias_1)<br/>hidden_output_1_1 = tf.nn.dropout(hidden_output_1, keep_prob)</span><span id="ac31" class="ll lm iq lh b gy ls lo l lp lq">predicted_y = tf.sigmoid(tf.matmul(hidden_output_1_1,weights_2) + bias_2)</span></pre><p id="c91a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们需要定义一个损失函数来优化我们的权重和偏差，我们将使用 softmax 交叉熵和 logits 来预测和正确的标签。我们也将增加一些 L2 正则化到我们的网络。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="2643" class="ll lm iq lh b gy ln lo l lp lq"><em class="lr">## Defining the loss function</em><br/>loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predicted_y,labels=input_y)) \<br/>        + regularizer_rate*(tf.reduce_sum(tf.square(bias_0)) + tf.reduce_sum(tf.square(bias_1)))</span></pre><p id="dba3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们需要为我们的网络定义一个优化器和学习率，以优化给定损失函数的权重和偏差。我们将每五个时期对我们的学习速率使用指数衰减，以减少 15%的学习。对于优化器，我们将使用 Adam 优化器。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="673d" class="ll lm iq lh b gy ln lo l lp lq"><em class="lr">## Variable learning rate</em><br/>learning_rate = tf.train.exponential_decay(starter_learning_rate, 0, 5, 0.85, staircase=<strong class="lh ir">True</strong>)<br/><em class="lr">## Adam optimzer for finding the right weight</em><br/>optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss,var_list=[weights_0,weights_1,weights_2,<br/>                                                                         bias_0,bias_1,bias_2])</span></pre><p id="fb2a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们已经完成了模型构建。让我们定义准确性度量来评估我们的模型性能，因为损失函数是非直观的。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="40b1" class="ll lm iq lh b gy ln lo l lp lq"><em class="lr">## Metrics definition</em><br/>correct_prediction = tf.equal(tf.argmax(y_train,1), tf.argmax(predicted_y,1))<br/>accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span></pre><p id="acbd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在将开始在训练数据上训练我们的网络，同时在测试数据集上评估我们的网络。我们将使用大小为 128 的批处理优化，并训练它 14 个时期，以获得 98%以上的准确性。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="f89b" class="ll lm iq lh b gy ln lo l lp lq"><em class="lr">## Training parameters</em><br/>batch_size = 128<br/>epochs=14<br/>dropout_prob = 0.6</span><span id="0d18" class="ll lm iq lh b gy ls lo l lp lq">training_accuracy = []<br/>training_loss = []<br/>testing_accuracy = []</span><span id="d891" class="ll lm iq lh b gy ls lo l lp lq">s.run(tf.global_variables_initializer())<br/><strong class="lh ir">for</strong> epoch <strong class="lh ir">in</strong> range(epochs):    <br/>    arr = np.arange(X_train.shape[0])<br/>    np.random.shuffle(arr)<br/>    <strong class="lh ir">for</strong> index <strong class="lh ir">in</strong> range(0,X_train.shape[0],batch_size):<br/>        s.run(optimizer, {input_X: X_train[arr[index:index+batch_size]],<br/>                          input_y: y_train[arr[index:index+batch_size]],<br/>                        keep_prob:dropout_prob})<br/>    training_accuracy.append(s.run(accuracy, feed_dict= {input_X:X_train, <br/>                                                         input_y: y_train,keep_prob:1}))<br/>    training_loss.append(s.run(loss, {input_X: X_train, <br/>                                      input_y: y_train,keep_prob:1}))<br/>    <br/>    <em class="lr">## Evaluation of model</em><br/>    testing_accuracy.append(accuracy_score(y_test.argmax(1), <br/>                            s.run(predicted_y, {input_X: X_test,keep_prob:1}).argmax(1)))<br/>    print("Epoch:<strong class="lh ir">{0}</strong>, Train loss: <strong class="lh ir">{1:.2f}</strong> Train acc: <strong class="lh ir">{2:.3f}</strong>, Test acc:<strong class="lh ir">{3:.3f}</strong>".format(epoch,<br/>                                                                    training_loss[epoch],<br/>                                                                    training_accuracy[epoch],<br/>                                                                   testing_accuracy[epoch]))</span></pre><figure class="lc ld le lf gt lu gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/47549be381098d778de3e177619dd7a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*P7xwuR97NOs4Y-hdoLBNRg.png"/></div></figure><p id="a628" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们可视化训练并测试作为历元数的函数的准确性。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="09b2" class="ll lm iq lh b gy ln lo l lp lq"><em class="lr">## Plotting chart of training and testing accuracy as a function of iterations</em><br/>iterations = list(range(epochs))<br/>plt.plot(iterations, training_accuracy, label='Train')<br/>plt.plot(iterations, testing_accuracy, label='Test')<br/>plt.ylabel('Accuracy')<br/>plt.xlabel('iterations')<br/>plt.show()<br/>print("Train Accuracy: <strong class="lh ir">{0:.2f}</strong>".format(training_accuracy[-1]))<br/>print("Test Accuracy:<strong class="lh ir">{0:.2f}</strong>".format(testing_accuracy[-1]))</span></pre><figure class="lc ld le lf gt lu gh gi paragraph-image"><div class="gh gi md"><img src="../Images/5102960b63d9f8754d19d24671e327ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*LXGUlCNRPSKHXaRXlYK2Mw.png"/></div></figure><p id="f3af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们所看到的，我们已经成功训练了一个多层感知器，它是用 tensorflow 编写的，具有很高的验证准确性！</p><p id="6665" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望你喜欢阅读，并随时使用我的代码(也可以在<a class="ae lb" href="https://github.com/aayushmnit/Deep_learning_explorations/blob/master/2_MLP_tensorflow/my1stNN.ipynb" rel="noopener ugc nofollow" target="_blank"> jupyter 笔记本</a>中找到)来为你的目的进行测试。此外，如果对代码或博客有任何反馈，请随时联系 aayushmnit@gmail.com 的<a class="ae lb" href="https://www.linkedin.com/in/aayushmnit/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或给我发电子邮件。</p></div></div>    
</body>
</html>