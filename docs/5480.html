<html>
<head>
<title>Deploying Keras models using TensorFlow Serving and Flask</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 TensorFlow 服务和 Flask 部署 Keras 模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deploying-keras-models-using-tensorflow-serving-and-flask-508ba00f1037?source=collection_archive---------2-----------------------#2018-10-21">https://towardsdatascience.com/deploying-keras-models-using-tensorflow-serving-and-flask-508ba00f1037?source=collection_archive---------2-----------------------#2018-10-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d5e306099e56e91fea7e80ce9fa56523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FCvoR3ffHysD6lDRzRUYSw.png"/></div></div></figure><p id="42d2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通常需要抽象出机器学习模型的细节，并将其部署或集成到易于使用的 API 端点中。例如，我们可以提供一个 URL 端点，任何人都可以使用它来发出 POST 请求，他们将获得模型推断的 JSON 响应，而不必担心它的技术细节。</p><p id="da75" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本教程中，我们将创建一个 TensorFlow 服务服务器来部署我们在 Keras 中构建的<code class="fe kw kx ky kz b">InceptionV3</code>图像分类卷积神经网络(CNN)。然后，我们将创建一个简单的 Flask 服务器，它将接受 POST 请求并进行 Tensorflow 服务服务器所需的一些图像预处理，然后返回一个 JSON 响应。</p><h2 id="4e98" class="la lb iq bd lc ld le dn lf lg lh dp li kj lj lk ll kn lm ln lo kr lp lq lr ls bi translated">TensorFlow 提供的是什么？</h2><p id="99b9" class="pw-post-body-paragraph jy jz iq ka b kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ij bi translated">服务就是你在训练完机器学习模型后如何应用它。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ly"><img src="../Images/5cadcd3f360767f282e9b4bfa9f982a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mdVoukO08uhHpmdVeQnTAg.png"/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Know more about TensorFlow Serving <a class="ae mh" href="https://www.youtube.com/watch?v=q_IkJcPyNl0" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><p id="9f4f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">TensorFlow 服务使模型投入生产的过程变得更加简单快捷。它允许您安全地部署新模型和运行实验，同时保持相同的服务器架构和 API。开箱即用，它提供了与 TensorFlow 的集成，但它可以扩展为服务于其他类型的模型。</p><h2 id="e235" class="la lb iq bd lc ld le dn lf lg lh dp li kj lj lk ll kn lm ln lo kr lp lq lr ls bi translated">安装 TensorFlow 服务</h2><p id="f313" class="pw-post-body-paragraph jy jz iq ka b kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ij bi translated"><em class="mi">先决条件</em>:请创建一个 python 虚拟环境，并在其中安装带有 TensorFlow 后端的 Keras。点击阅读更多<a class="ae mh" href="https://keras.io/#installation" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="7712" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="mi">注意:</em>所有命令都已经在 Ubuntu 18.04.1 LTS 上的 python 虚拟环境中执行。</p><p id="2d1f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，在同一个虚拟环境中运行以下命令(使用<code class="fe kw kx ky kz b">sudo</code>获得 root 权限):</p><pre class="lz ma mb mc gt mj kz mk ml aw mm bi"><span id="705f" class="la lb iq kz b gy mn mo l mp mq">$ apt install curl</span><span id="c3de" class="la lb iq kz b gy mr mo l mp mq">$ echo "deb [arch=amd64] <a class="ae mh" href="http://storage.googleapis.com/tensorflow-serving-apt" rel="noopener ugc nofollow" target="_blank">http://storage.googleapis.com/tensorflow-serving-apt</a> stable tensorflow-model-server tensorflow-model-server-universal" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list &amp;&amp; curl <a class="ae mh" href="https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg" rel="noopener ugc nofollow" target="_blank">https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg</a> | sudo apt-key add -</span><span id="3dc9" class="la lb iq kz b gy mr mo l mp mq">$ apt-get update</span><span id="e84d" class="la lb iq kz b gy mr mo l mp mq">$ apt-get install tensorflow-model-server</span><span id="823b" class="la lb iq kz b gy mr mo l mp mq">$ tensorflow_model_server --version<br/>TensorFlow ModelServer: 1.10.0-dev<br/>TensorFlow Library: 1.11.0</span><span id="854b" class="la lb iq kz b gy mr mo l mp mq">$ python  --version<br/>Python 3.6.6</span></pre><p id="9f3e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可以通过以下方式升级到<code class="fe kw kx ky kz b">tensorflow-model-server</code>的新版本:</p><pre class="lz ma mb mc gt mj kz mk ml aw mm bi"><span id="6fc8" class="la lb iq kz b gy mn mo l mp mq">$ apt-get upgrade tensorflow-model-server</span></pre><h2 id="22b4" class="la lb iq bd lc ld le dn lf lg lh dp li kj lj lk ll kn lm ln lo kr lp lq lr ls bi translated">我们将要构建的目录概述</h2><p id="3e47" class="pw-post-body-paragraph jy jz iq ka b kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ij bi translated">在我们开始之前，理解目录结构将有助于我们清楚地了解每一步的进展情况。</p><pre class="lz ma mb mc gt mj kz mk ml aw mm bi"><span id="7e8b" class="la lb iq kz b gy mn mo l mp mq">(tensorflow) ubuntu@Himanshu:~/Desktop/Medium/keras-and-tensorflow-serving$ tree -c<br/>└── keras-and-tensorflow-serving<br/>    ├── README.md<br/>    ├── my_image_classifier<br/>    │   └── 1<br/>    │       ├── saved_model.pb<br/>    │       └── variables<br/>    │           ├── variables.data-00000-of-00001<br/>    │           └── variables.index<br/>    ├── test_images<br/>    │   ├── car.jpg<br/>    │   └── car.png<br/>    ├── flask_server<br/>    │   ├── app.py<br/>    │   ├── flask_sample_request.py<br/>    └── scripts<br/>        ├── download_inceptionv3_model.py<br/>        ├── inception.h5<br/>        ├── auto_cmd.py<br/>        ├── export_saved_model.py<br/>        ├── imagenet_class_index.json<br/>        └── serving_sample_request.py</span><span id="81b0" class="la lb iq kz b gy mr mo l mp mq">6 directories, 15 files</span></pre><p id="fa91" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可以从我的 GitHub 存储库中获得所有这些文件:</p><div class="ms mt gp gr mu mv"><a href="https://github.com/himanshurawlani/keras-and-tensorflow-serving" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd ir gy z fp na fr fs nb fu fw ip bi translated">himanshurawlani/keras-和-tensor flow-服务</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">使用 TensorFlow 服务和 Flask 部署 Keras 模型-himanshurawlani/Keras-and-tensor flow-Serving</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">github.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj jw mv"/></div></div></a></div><h2 id="41fa" class="la lb iq bd lc ld le dn lf lg lh dp li kj lj lk ll kn lm ln lo kr lp lq lr ls bi translated">导出张量流服务的 Keras 模型</h2><p id="2643" class="pw-post-body-paragraph jy jz iq ka b kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ij bi translated">对于本教程，我们将下载和保存<code class="fe kw kx ky kz b">InceptionV3</code> CNN，使用<code class="fe kw kx ky kz b">download_inceptionv3_model.py</code>在 Keras 中使用 Imagenet 权重。您可以下载<code class="fe kw kx ky kz b">keras.applications</code>库中的任何其他可用模型(此处为<a class="ae mh" href="https://github.com/keras-team/keras-applications" rel="noopener ugc nofollow" target="_blank"/>)，或者如果您已经在 Keras 中构建了自己的模型，则可以跳过这一步。</p><figure class="lz ma mb mc gt jr"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="ff85" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">执行上述脚本后，您应该得到以下输出:</p><pre class="lz ma mb mc gt mj kz mk ml aw mm bi"><span id="2a11" class="la lb iq kz b gy mn mo l mp mq">$ python download_inceptionv3_model.py<br/>Using TensorFlow backend.<br/>Downloading data from <a class="ae mh" href="https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5" rel="noopener ugc nofollow" target="_blank">https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5</a><br/>96116736/96112376 [==============================] - 161s 2us/step</span></pre><p id="21fd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们有了以 Keras 格式保存的 CNN ( <code class="fe kw kx ky kz b">inception.h5</code>)。我们希望以 TensorFlow 服务器可以处理的格式导出我们的模型。我们通过执行<code class="fe kw kx ky kz b">export_saved_model.py</code>脚本来做到这一点。</p><p id="7195" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">TensorFlow 提供了<code class="fe kw kx ky kz b">SavedModel</code>格式作为导出模型的通用格式。在幕后，我们的 Keras 模型完全是根据 TensorFlow 对象指定的，因此我们可以使用 Tensorflow 方法很好地导出它。TensorFlow 提供了一个便利的函数<code class="fe kw kx ky kz b">tf.saved_model.simple_save()</code>，它抽象出了一些细节，对于大多数用例来说都很好。</p><figure class="lz ma mb mc gt jr"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="de24" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">输出:</p><pre class="lz ma mb mc gt mj kz mk ml aw mm bi"><span id="1606" class="la lb iq kz b gy mn mo l mp mq">$ python export_saved_model.py<br/>WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.</span></pre><p id="ed7c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们得到这个警告是因为我们下载了一个预先训练好的模型。我们可以按原样使用这个模型进行推理，但是如果我们想进一步训练它，我们需要在加载它之后运行<code class="fe kw kx ky kz b">compile()</code>函数。现在可以安全地忽略这个警告。执行该脚本后，以下文件保存在<code class="fe kw kx ky kz b">my_image_classifier</code>目录中:</p><pre class="lz ma mb mc gt mj kz mk ml aw mm bi"><span id="8256" class="la lb iq kz b gy mn mo l mp mq">├── my_image_classifier<br/>   └── 1<br/>       ├── saved_model.pb<br/>       └── variables<br/>           ├── variables.data-00000-of-00001<br/>           └── variables.index</span><span id="7a32" class="la lb iq kz b gy mr mo l mp mq">2 directories, 3 files</span></pre><p id="5575" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">假设我们希望在未来更新我们的模型(可能是因为我们收集了更多的训练数据，并在更新的数据集上训练了模型)，我们可以通过，</p><ol class=""><li id="a99d" class="nm nn iq ka b kb kc kf kg kj no kn np kr nq kv nr ns nt nu bi translated">在新的 keras 模型上运行相同的脚本</li><li id="2a44" class="nm nn iq ka b kb nv kf nw kj nx kn ny kr nz kv nr ns nt nu bi translated">将<code class="fe kw kx ky kz b">export_saved_model.py</code>中的<code class="fe kw kx ky kz b">export_path = ‘../my_image_classifier/1’</code>更新为<code class="fe kw kx ky kz b">export_path = ‘../my_image_classifier/2’</code></li></ol><p id="ac30" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">TensorFlow Serving 会在<code class="fe kw kx ky kz b">my_image_classifier</code>目录下自动检测模型的新版本，并在服务器中进行更新。</p><h2 id="c3e3" class="la lb iq bd lc ld le dn lf lg lh dp li kj lj lk ll kn lm ln lo kr lp lq lr ls bi translated">启动 TensorFlow 服务服务器</h2><p id="73d1" class="pw-post-body-paragraph jy jz iq ka b kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ij bi translated">要在本地计算机上启动 TensorFlow 服务服务器，请运行以下命令:</p><pre class="lz ma mb mc gt mj kz mk ml aw mm bi"><span id="49eb" class="la lb iq kz b gy mn mo l mp mq">$ tensorflow_model_server --model_base_path=/home/ubuntu/Desktop/Medium/keras-and-tensorflow-serving/my_image_classifier --rest_api_port=9000 --model_name=ImageClassifier</span></pre><ul class=""><li id="4653" class="nm nn iq ka b kb kc kf kg kj no kn np kr nq kv oa ns nt nu bi translated"><code class="fe kw kx ky kz b">--model_base_path</code>:这必须是一个绝对路径，否则你会得到一个错误消息:</li></ul><pre class="lz ma mb mc gt mj kz mk ml aw mm bi"><span id="5191" class="la lb iq kz b gy mn mo l mp mq">Failed to start server. Error: Invalid argument: Expected model ImageClassifier to have an absolute path or URI; got base_path()=./my_image_classifier</span></pre><ul class=""><li id="effa" class="nm nn iq ka b kb kc kf kg kj no kn np kr nq kv oa ns nt nu bi translated"><code class="fe kw kx ky kz b">--rest_api_port</code> : Tensorflow 服务将在端口 8500 上启动 gRPC ModelServer，REST API 将在端口 9000 上可用。</li><li id="a3d6" class="nm nn iq ka b kb nv kf nw kj nx kn ny kr nz kv oa ns nt nu bi translated"><code class="fe kw kx ky kz b">--model_name</code>:这将是您用来发送 POST 请求的服务器的名称。您可以在此键入任何名称。</li></ul><h2 id="6b5a" class="la lb iq bd lc ld le dn lf lg lh dp li kj lj lk ll kn lm ln lo kr lp lq lr ls bi translated">测试我们的 TensorFlow 服务器</h2><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ob"><img src="../Images/53d0fef29b638774c08756100f1c2739.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mHePtJJbR2TUtM6sYZNdzA.jpeg"/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk">From raw data to production models (<a class="ae mh" href="https://twitter.com/tensorflow/status/832008382408126464" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="021d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe kw kx ky kz b">serving_sample_request.py</code>脚本向 TensorFlow 服务服务器发出 POST 请求。输入图像通过命令行参数传递。</p><figure class="lz ma mb mc gt jr"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="3f0f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">输出:</p><pre class="lz ma mb mc gt mj kz mk ml aw mm bi"><span id="08eb" class="la lb iq kz b gy mn mo l mp mq">$ python serving_sample_request.py -i ../test_images/car.png<br/>Using TensorFlow backend.<br/>[["n04285008", "sports_car", 0.998414], ["n04037443", "racer", 0.00140099], ["n03459775", "grille", 0.000160794], ["n02974003", "car_wheel", 9.57862e-06], ["n03100240", "convertible", 6.01581e-06]]</span></pre><p id="62bc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">与后续调用相比，TensorFlow 服务服务器响应第一个请求的时间稍长。</p></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><h2 id="ec23" class="la lb iq bd lc ld le dn lf lg lh dp li kj lj lk ll kn lm ln lo kr lp lq lr ls bi translated">为什么我们需要 Flask 服务器？</h2><p id="db11" class="pw-post-body-paragraph jy jz iq ka b kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ij bi translated">正如我们所看到的，我们已经在<code class="fe kw kx ky kz b">serving_sample_request.py</code>(前端调用程序)中执行了一些图像预处理步骤。以下是在 TensorFlow 服务器上创建 Flask 服务器的原因:</p><ul class=""><li id="d47f" class="nm nn iq ka b kb kc kf kg kj no kn np kr nq kv oa ns nt nu bi translated">当我们向前端团队提供 API 端点时，我们需要确保我们不会用预处理技术淹没他们。</li><li id="0430" class="nm nn iq ka b kb nv kf nw kj nx kn ny kr nz kv oa ns nt nu bi translated">我们可能并不总是有 Python 后端服务器(例如 Node.js 服务器),所以使用 numpy 和 keras 库进行预处理可能会很痛苦。</li><li id="b866" class="nm nn iq ka b kb nv kf nw kj nx kn ny kr nz kv oa ns nt nu bi translated">如果我们计划服务多个模型，那么我们将不得不创建多个 TensorFlow 服务服务器，并将不得不添加新的 URL 到我们的前端代码。但是我们的 Flask 服务器会保持域名 URL 不变，我们只需要添加一个新的路由(一个函数)。</li><li id="9a0b" class="nm nn iq ka b kb nv kf nw kj nx kn ny kr nz kv oa ns nt nu bi translated">提供基于订阅的访问，异常处理和其他任务可以在 Flask app 中进行。</li></ul><p id="a8bb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们正在努力消除 TensorFlow 服务服务器和我们前端之间的紧密耦合。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/0d15fa6d6463732a2e037af8a9a8ae53.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*QdmZRkMjxlv13CJLEE7EjA.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Multiple TensorFlow Serving servers hidden behind a Flask server</figcaption></figure><p id="45ee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本教程中，我们将在与 TensorFlow 服务器相同的计算机和虚拟环境中创建一个 Flask 服务器，并使用已安装的库。理想情况下，两者都应该在不同的机器上运行，因为大量的请求会导致 Flask 服务器由于正在执行图像预处理而变慢。此外，如果请求数量非常多，单个 Flask 服务器可能不够用。如果我们有多个前端调用者，我们可能还需要一个排队系统。尽管如此，我们可以使用这种方法来开发一个令人满意的概念证明。</p><h2 id="81f2" class="la lb iq bd lc ld le dn lf lg lh dp li kj lj lk ll kn lm ln lo kr lp lq lr ls bi translated">创建 Flask 服务器</h2><p id="7249" class="pw-post-body-paragraph jy jz iq ka b kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ij bi translated"><em class="mi">先决条件</em>:从<a class="ae mh" href="http://flask.pocoo.org/docs/1.0/installation/" rel="noopener ugc nofollow" target="_blank">这里</a>在 python 虚拟环境中安装 Flask。</p><p id="fcce" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们只需要一个<code class="fe kw kx ky kz b">app.py</code>文件来创建我们的 Flask 服务器。</p><figure class="lz ma mb mc gt jr"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="6de3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">转到保存<code class="fe kw kx ky kz b">app.py</code>文件的目录，使用以下命令启动 Flask 服务器:</p><pre class="lz ma mb mc gt mj kz mk ml aw mm bi"><span id="9c15" class="la lb iq kz b gy mn mo l mp mq">$ export FLASK_ENV=development &amp;&amp; flask run --host=0.0.0.0</span></pre><ul class=""><li id="8a70" class="nm nn iq ka b kb kc kf kg kj no kn np kr nq kv oa ns nt nu bi translated"><code class="fe kw kx ky kz b">FLASK_ENV=development</code>:这启用了调试模式，基本上给你完整的错误日志。不要在生产环境中使用它。</li><li id="a1b3" class="nm nn iq ka b kb nv kf nw kj nx kn ny kr nz kv oa ns nt nu bi translated"><code class="fe kw kx ky kz b">flask run</code>命令自动执行当前目录下的<code class="fe kw kx ky kz b">app.py</code>文件。</li><li id="49ec" class="nm nn iq ka b kb nv kf nw kj nx kn ny kr nz kv oa ns nt nu bi translated"><code class="fe kw kx ky kz b">--host=0.0.0.0</code>:这使您能够从任何其他机器向 Flask 服务器发出请求。要从不同的机器发出请求，您必须指定运行 Flask 服务器的机器的<strong class="ka ir"> IP 地址</strong>来代替<code class="fe kw kx ky kz b">localhost</code>。</li></ul><p id="cdb4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">输出:</p><pre class="lz ma mb mc gt mj kz mk ml aw mm bi"><span id="8d9a" class="la lb iq kz b gy mn mo l mp mq">* Running on <a class="ae mh" href="http://0.0.0.0:5000/" rel="noopener ugc nofollow" target="_blank">http://0.0.0.0:5000/</a> (Press CTRL+C to quit)<br/>* Restarting with stat<br/>* Debugger is active!<br/>* Debugger PIN: 1xx-xxx-xx4<br/>Using TensorFlow backend.</span></pre><p id="8758" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用前面相同的命令启动 TensorFlow 服务器:</p><pre class="lz ma mb mc gt mj kz mk ml aw mm bi"><span id="d7fc" class="la lb iq kz b gy mn mo l mp mq">$ tensorflow_model_server --model_base_path=/home/ubuntu/Desktop/Medium/keras-and-tensorflow-serving/my_image_classifier --rest_api_port=9000 --model_name=ImageClassifier</span></pre><p id="439a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里有一个脚本(<code class="fe kw kx ky kz b">auto_cmd.py</code>)来自动启动和停止两个服务器(TensorFlow Serving 和 Flask)。您也可以为两台以上的服务器修改该脚本。</p><figure class="lz ma mb mc gt jr"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="e5f4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">记得在<code class="fe kw kx ky kz b">auto_cmd.py</code>的<strong class="ka ir">行第 10 </strong>处更改路径，使其指向您的<code class="fe kw kx ky kz b">app.py</code>目录。你可能还需要修改第 6 行<strong class="ka ir">以使它指向你的虚拟环境的 bin。然后，通过在终端中执行以下命令，您可以从任何目录执行上述脚本:</strong></p><pre class="lz ma mb mc gt mj kz mk ml aw mm bi"><span id="14d5" class="la lb iq kz b gy mn mo l mp mq">$ python auto_cmd.py</span></pre><h2 id="dde2" class="la lb iq bd lc ld le dn lf lg lh dp li kj lj lk ll kn lm ln lo kr lp lq lr ls bi translated">测试我们的 Flask 服务器和 TensorFlow 服务器</h2><p id="34d3" class="pw-post-body-paragraph jy jz iq ka b kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ij bi translated">我们使用<code class="fe kw kx ky kz b">flask_sample_request.py</code>脚本制作一个示例请求。该脚本基本上模拟了来自前端的请求:</p><ol class=""><li id="1ca5" class="nm nn iq ka b kb kc kf kg kj no kn np kr nq kv nr ns nt nu bi translated">我们获取一个输入图像，将其编码为 base64 格式，并使用 POST 请求将其发送到我们的 Flask 服务器。</li><li id="9a3d" class="nm nn iq ka b kb nv kf nw kj nx kn ny kr nz kv nr ns nt nu bi translated">Flask server 对这个 base64 图像进行解码，并为我们的 TensorFlow 服务器进行预处理。</li><li id="5cd0" class="nm nn iq ka b kb nv kf nw kj nx kn ny kr nz kv nr ns nt nu bi translated">然后，Flask server 向我们的 TensorFlow 服务服务器发出 POST 请求，并对响应进行解码。</li><li id="0880" class="nm nn iq ka b kb nv kf nw kj nx kn ny kr nz kv nr ns nt nu bi translated">解码后的响应被格式化并发送回前端。</li></ol><figure class="lz ma mb mc gt jr"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="3ea7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">输出:</p><pre class="lz ma mb mc gt mj kz mk ml aw mm bi"><span id="08d7" class="la lb iq kz b gy mn mo l mp mq">$ python flask_sample_request.py -i ../test_images/car.png<br/>[<br/>  [<br/>    "n04285008", <br/>    "sports_car", <br/>    0.998414<br/>  ], <br/>  [<br/>    "n04037443", <br/>    "racer", <br/>    0.00140099<br/>  ], <br/>  [<br/>    "n03459775", <br/>    "grille", <br/>    0.000160794<br/>  ], <br/>  [<br/>    "n02974003", <br/>    "car_wheel", <br/>    9.57862e-06<br/>  ], <br/>  [<br/>    "n03100240", <br/>    "convertible", <br/>    6.01581e-06<br/>  ]<br/>]</span></pre><p id="a2c6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的 flask 服务器目前只有一条路由用于我们的单个 Tensorflow 服务服务器。我们可以通过在不同或相同的机器上创建多个 Tensorflow 服务服务器来服务多个模型。为此，我们只需向我们的<code class="fe kw kx ky kz b">app.py</code>文件中添加更多的路线(功能),并在其中执行所需的特定于模型的预处理。我们可以将这些路线交给我们的前端团队，以便根据需要调用模型。</p><h2 id="1408" class="la lb iq bd lc ld le dn lf lg lh dp li kj lj lk ll kn lm ln lo kr lp lq lr ls bi translated">处理跨源 HTTP 请求</h2><p id="1743" class="pw-post-body-paragraph jy jz iq ka b kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ij bi translated">考虑这样一个场景，我们使用 Angular 发出一个 POST 请求，我们的 Flask 服务器收到 OPTIONS header 而不是 POST，因为，</p><ul class=""><li id="f433" class="nm nn iq ka b kb kc kf kg kj no kn np kr nq kv oa ns nt nu bi translated">当 web 应用程序请求来源(域、协议和端口)不同于其自身来源的资源时，它会发出跨来源 HTTP 请求。</li><li id="ac13" class="nm nn iq ka b kb nv kf nw kj nx kn ny kr nz kv oa ns nt nu bi translated">CORS(跨源资源共享)是一种机制，它使用额外的 HTTP 报头来告诉浏览器，让在一个源(域)上运行的 web 应用程序有权访问来自不同源的服务器的选定资源。在这里阅读更多关于 CORS 的信息。</li></ul><p id="7a3e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，Angular 没有从 Flask 服务器得到任何响应。为了解决这个问题，我们必须在我们的<code class="fe kw kx ky kz b">app.py</code>中启用弗拉斯克-CORS。了解更多<a class="ae mh" href="https://flask-cors.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h2 id="2f37" class="la lb iq bd lc ld le dn lf lg lh dp li kj lj lk ll kn lm ln lo kr lp lq lr ls bi translated">结论</h2><p id="9063" class="pw-post-body-paragraph jy jz iq ka b kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ij bi translated">这就是我们服务于机器学习模型所需要的一切。TensorFlow 服务使得将机器学习集成到网站和其他应用程序中变得非常容易。keras 中有大量预构建的模型可用(<a class="ae mh" href="https://github.com/keras-team/keras-applications" rel="noopener ugc nofollow" target="_blank">此处</a>)，有可能用最少的机器学习和深度学习算法知识开发出超级有用的应用。</p><p id="640a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你觉得这个教程有帮助，请分享给你的朋友，并留下掌声:-)。如果你有任何疑问、反馈或建议，请在评论中告诉我。另外，你可以在推特和 T2【LinkedIn】和我联系。有太多的东西要与你们分享，而我才刚刚开始。敬请关注更多内容！</p></div></div>    
</body>
</html>