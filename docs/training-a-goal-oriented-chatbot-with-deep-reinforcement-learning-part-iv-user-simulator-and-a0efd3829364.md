# 用深度强化学习训练面向目标的聊天机器人——第四部分

> 原文：<https://towardsdatascience.com/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364?source=collection_archive---------9----------------------->

## 第四部分:用户模拟器和误差模型控制器

[![](img/94bfd3adb930fe8ed50780b828c92ce6.png)](http://www.themattrix.com/matrix-moving-high-resolution-wallpaper-xjgs-hd-matrix-wallpaper-moving-animated-android-iphone-windows-7-gif-5-4-download/)

[http://www.themattrix.com/matrix-moving-high-resolution-wallpaper-xjgs-hd-matrix-wallpaper-moving-animated-android-iphone-windows-7-gif-5-4-download/](http://www.themattrix.com/matrix-moving-high-resolution-wallpaper-xjgs-hd-matrix-wallpaper-moving-animated-android-iphone-windows-7-gif-5-4-download/)

查看本系列之前的部分，包括训练循环、DQN 代理和状态跟踪器，天啊！

这一部分解释了非常重要的用户模拟器和错误模型控制器。

这个系列教程是基于 [TC-Bot](https://github.com/MiuLab/TC-Bot) 的。这个系列的代码可以在[这里](https://github.com/maxbren/GO-Bot-DRL)找到。这部分重点介绍的两个文件是`[user_simulator.py](https://github.com/maxbren/GO-Bot-DRL/blob/master/user_simulator.py)`和`[error_model_controller.py](https://github.com/maxbren/GO-Bot-DRL/blob/master/error_model_controller.py)`。

我们正在遵循的图表:

![](img/95c8931d8c7beae082c62a80ec16ee9f.png)

Dialogue flow of a single round

# 用户模拟器

用户模拟器用于模拟真实的用户，这样可以比真实用户被迫坐下来接受几个小时的培训更快地培训代理。聊天机器人领域的用户模拟器研究是一个热门的研究课题。本教程中涉及的是一个相当简单的确定性的基于规则的模拟器，基于 TC-Bot 的[用户 sim，有一些小的改动。这个用户模拟器，像目前大多数一样，是一个基于议程的系统。这意味着用户对于该情节具有目标，并且它根据该目标采取行动，同时跟踪内部状态，这允许它跟随对话以采取明智的行动。每一轮动作都是响应于主体动作而精心设计的，主要使用确定性规则和一些随机规则来创建不同的响应。](https://arxiv.org/pdf/1612.05688.pdf)

## 什么是用户目标？

用户目标是从真实对话或手工制作(或两者兼有)的语料库中抽取的。每个目标都由通知时段和请求时段组成，就像一个动作，但没有意图。从语料库中获取目标有几种标准方法。第一:在一集里，用户最初动作的所有时段(请求和通知)都有一个目标。第二:一集里所有用户动作的所有时段被组合起来，形成一个单一的目标。除了这种自动收集之外，一些手工制作的规则用于降低用户目标的多样性，以便它们对于代理来说更容易实现。幸运的是，TC-Bot 附带了我们将使用的用户目标的[文件，因此我们不需要自己从语料库中生成它们。](https://github.com/maxbren/GO-Bot-DRL/blob/master/data/movie_user_goals.txt)

用户目标的通知时段模拟用户在寻找适合他们的票据时的约束。请求槽模拟用户获取关于什么票可用的信息。该系统与真实用户之间的主要区别在于，当用户更多地了解有哪些票可用时，他们可能会改变主意。这在这里不会发生，因为在一集里目标不会改变。*想想你可以用什么方法来增强这个系统，让它更像一个真正的用户！*

最后注意，默认槽(在本例中为“ticket ”)被添加到每个目标的请求槽中。代理必须成功地通知一个值来填充默认时段，以实现目标并在一集中成功。

来自第一部分的几个用户目标的例子:

## 用户模拟器内部状态

用户 sim 的内部状态(不同于来自状态跟踪器的状态)跟踪目标位置和当前对话的历史。它用于设计每一步的用户操作。具体来说，状态是 4 个槽字典和一个意图的列表:

*   Rest 槽:代理或用户尚未通知的来自目标的所有通知和请求槽
*   历史插槽:迄今为止用户和代理操作的所有通知插槽
*   请求槽:用户希望在不久的将来的操作中请求的请求槽
*   通知插槽:通知插槽，代理打算在当前正在制定的操作中通知插槽
*   意图:当前操作的意图

## 用户模拟器动作

用户可以采取的操作比可能的代理操作更加多样，有时甚至更加复杂。一个用户操作可以有多个请求槽或多个通知槽。在某些情况下，例如初始操作，它实际上还可以包含请求和通知槽。

现在进入代码！

## 对话配置

以下是`[dialogue_config.py](https://github.com/maxbren/GO-Bot-DRL/blob/master/dialogue_config.py)`中用户 sim 的对话配置常量:

## 重置

用户 sim 重置很重要，因为它选择新的用户目标，重置状态并返回初始动作。

解剖学:

1.  选择一个随机目标
2.  将默认位置(“入场券”)添加到目标中
3.  清除状态的所有方面
4.  将所有目标通知和请求槽添加到`‘rest_slots’`；在整个谈话过程中，他们都需要被告知
5.  将约束检查初始化为失败(在下面的成功约束中讨论)
6.  返回剧集的初始动作

## 初始用户操作

1.  初始操作必须始终是一个请求
2.  它还必须始终包含在`dialogue_config.py`中定义的`usersim_required_init_inform_keys`中列出的所有通知槽。在我们的例子中，这仅仅意味着槽`‘moviename’`，它也是每个用户目标中必需的通知槽，即它总是在初始动作中被通知
3.  必须包含来自目标的随机请求槽，而不是默认槽，除非这是唯一的一个

## 分步方法

用户响应 step 方法中的代理操作，该方法将代理操作作为输入。Step 接受这个动作和用户 sim 的内部状态，并返回一个精心制作的响应(动作)、标量奖励、完成布尔值和成功布尔值。这类似于 [openai gym environments](https://gym.openai.com/docs/) 中的 step 函数，目的相同！

具体来说:

1.  清除状态通知槽和意图，因为它们不需要逐轮结转，不像历史、休息和请求那样结转
2.  如果回合等于最大回合，则回复意图`‘done’`并设置`success = FAIL`
3.  否则，基于代理动作意图来设计动作。3.a)如果意图是`‘done’`，那么也计算该集是否算作代理的成功，并回复`‘done’`
4.  回应国家的意图，请求和通知槽
5.  计算该步骤的标量奖励
6.  返回响应、奖励、完成布尔值和指示成功的布尔值

顺便说一句，意图`‘done’`的这两种使用是用户 sim 动作将其作为意图的唯一时间，即`‘done’`象征着对话的结束

注意:`success` 可以是`NO_OUTCOME`如果该集没有完成，可以是`FAIL`如果该集已经完成并且失败，或者`SUCCESS`如果该集已经完成并且成功

`[utils.py:](https://github.com/maxbren/GO-Bot-DRL/blob/master/utils.py)`中的奖励功能

这个奖励函数通过给代理一个巨大的成功奖励来帮助它学习成功。让它学会避免失败，方法是给它一个大的失败惩罚，但没有成功的奖励那么大。我相信这有助于代理人不要太害怕为成功的巨大奖励而冒险，否则它可能会过早地结束一集以减少负面奖励。最终，像这样的奖励形成往往是一种平衡行为。试试这个奖励函数，看看是否能得到更好的结果。

# 回应的类型

一些回应的规则很复杂或者看起来很随意。但是*请记住，许多设计响应的规则都很复杂，因此用户 sim 卡可以更像人类，这将有助于代理与真实用户打交道。然而，这些绝不是最好的规则。*

响应的重要*要求*:

1.  对于用户动作:如果意图是`‘inform’`，那么必须有通知槽，但是*没有*请求槽，以便不与下面的要求#2 冲突
2.  对于用户动作:如果意图是`‘request’`，它可以让*同时拥有*通知和请求槽，但必须至少拥有请求槽:这是一个复杂的动作，目的是向请求添加信息，使其更像人类
3.  每当代理操作或用户操作包含通知槽时，该通知槽必须从 rest 槽中移除(如果在 rest 槽中键入),并在历史槽中添加/更新
4.  每当代理动作包含通知槽时，必须将其从状态请求槽中移除(如果键入状态请求槽)，并遵循上述要求 3

## 对请求的答复

用于响应请求的 4 种主要情况:

案例 1)如果代理请求目标通知槽中的某个内容，但它没有被通知，则从目标本身通知它

情况 2)如果代理请求目标请求槽中的某些东西，并且它已经被通知，那么从历史中通知它

情况 3)如果代理请求目标请求槽中的某样东西，但它没有得到通知，那么也用随机通知请求相同的槽

情况 4)否则用户 sim 不关心被请求的时隙；告知特殊值`‘anything’`为请求槽的值

## 回复:通知

两种主要的应对情况:

情况 1)如果代理告知目标告知中的内容，而其告知的值不匹配，则告知正确的值

情况 2)否则选择一些时隙来请求或通知

2.a)如果国家有任何要求，则提出要求

2.b)如果在休息时段有什么要说的，请选择一些内容

2.c)否则用`‘thanks’`回应，真正的意思是“无话可说”；事实上，这向代理表明它的 rest 槽是空的，这是下面讨论的成功的必要条件

## 成功限制

在继续讨论“匹配发现响应”和“完成响应”之前，了解一集的成功限制很重要。请记住，目标通知槽代表找到的匹配必须包含的约束。以下是代理要取得成功必须做的事情:

1.  带着`‘match_found’`的意图采取行动，检查比赛是否满足所有目标约束
2.  在`‘match_found’`之后采取意图为`‘done’`的动作，检查剩余槽是否为空

这些成功约束需要匹配的票据，因为这是任务的目标，并且空的 rest 槽显示代理已经通知了所有目标请求槽。这有助于代理学会让用户在提交匹配之前提出问题(请求)。

## 回复:找到匹配项

1.  动作中的默认槽必须有一个实际的匹配 ID 作为值，而不是`‘no match available’`,这意味着它实际上找不到与当前状态跟踪器通知的匹配
2.  目标中的所有通知时段必须位于此操作的通知时段中，因为这些时段是票证本身的属性
3.  该操作的所有通知时段值必须与目标通知时段值相匹配
4.  如果所有这些都成功，那么`self.constraint_check`被设置为`SUCCESS`，否则设置为`FAIL`

如果匹配成功，则回复 intent `‘thanks’`和任何仍处于请求状态的请求槽。否则回复意图`‘reject’`并且没有通知或请求槽。`‘thanks’`向代理表明该票据有效。`‘reject’`表示该票不起作用。

## 回复:完成

约束 1) `self.constraint_check`必须设置为`SUCCESS`，表示找到一个有效的匹配

约束 2)剩余槽必须为空

注意:如 step 方法所示，回复的意图是`‘done’`，没有槽

# 误差模型控制器

在从步骤接收到用户动作之后，它被发送到错误模型控制器(EMC)以注入错误。在 TC-Bot 中，发现这样做实际上有助于代理处理现实生活中的自然语言组件错误或用户在回复中出错。EMC 可以将错误添加到通知槽和用户动作的意图中。

各通知槽中*动作的槽级错误类型:*

*   用该键的随机值替换该值
*   替换整个槽:随机键和该键的随机值
*   删除该插槽
*   对于一个插槽，上述所有 3 种错误的概率相等

意图级别错误:

*   用随机意图替换意图

在**总结**中，我们在这里创建的用户模拟器使用简单的规则来创建类似人类的响应，目的是训练代理与现实生活中的用户打交道。错误模型控制器的目的是将错误添加到用户 sim 动作的意图和/或通知槽中，这提高了真实测试中代理的质量。

在[的最后一部分](https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-v-running-the-agent-and-63d8cd27d1d)中，我们将介绍如何实际运行我们创建的代码(来自[回购](https://github.com/maxbren/GO-Bot-DRL))以及今后的发展方向。T4:有很多需要改进的地方，还有很多研究要做！

**最后部分再见！**