<html>
<head>
<title>Secret Sauce behind the beauty of Deep Learning: Beginners guide to Activation Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习之美背后的秘密酱:激活函数初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/secret-sauce-behind-the-beauty-of-deep-learning-beginners-guide-to-activation-functions-a8e23a57d046?source=collection_archive---------1-----------------------#2017-08-23">https://towardsdatascience.com/secret-sauce-behind-the-beauty-of-deep-learning-beginners-guide-to-activation-functions-a8e23a57d046?source=collection_archive---------1-----------------------#2017-08-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/0f2cb4ff68be246fecb4481b050a7232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zwp7ZQLR4cXgLjI2qrMOKQ.png"/></div></div></figure><div class=""/><p id="4eaf" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">激活功能是获取输入信号并将其转换为输出信号的功能。激活函数将非线性引入网络，这就是为什么我们称之为非线性。神经网络是通用函数逼近器，深度神经网络使用反向传播来训练，反向传播需要<a class="ae kw" href="https://en.wikipedia.org/wiki/Differentiable_function" rel="noopener ugc nofollow" target="_blank">可微分的</a>激活函数。反向传播在这个函数上使用梯度下降来更新网络权重。理解激活函数非常重要，因为它们在深度神经网络的质量中起着至关重要的作用。在这篇文章中，我列出并描述了不同的激活功能。</p><ul class=""><li id="9cbc" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc">恒等式或线性激活函数— </strong>恒等式或线性激活函数是所有激活函数中最简单的。它对你的数据进行恒等运算，输出数据与输入数据成正比。线性激活函数的问题是，它的导数是一个常数，它的梯度也是一个常数，下降将是一个常数梯度。</li></ul><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/e77b17340a31235ab091d58d2f0b8302.png" data-original-src="https://miro.medium.com/v2/resize:fit:218/format:webp/1*gklL4_EwFpXPSzFC4sPT1g.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Equation for Identity or Linear activation function</figcaption></figure><p id="0c86" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">范围:(-∞，+∞)</p><p id="5b5e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">例如:f(2) = 2或f(-4) = -4</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/797198d05dbd5a90cf83bd3190734895.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*x0lkaiCj7i8ut6Dis2O6rg.png"/></div></figure><ul class=""><li id="459a" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc"> Heaviside(二进制步长，0或1，高或低)步长函数</strong>通常只在单层感知器中有用，这是一种早期的神经网络，可用于输入数据可线性分离的情况下的分类。这些函数对于二元分类任务非常有用。如果输入和高于某个阈值，则输出为某个值A1，如果输入和低于某个阈值，则输出为A0。感知器使用的值是A1 = 1和A0 = 0</li></ul><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/46d285f3f0dc857966f6a0a86621e17f.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*LfKVxBfSYSFyUwEw5YInFg.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Equation for Heaveside/Binary Step Function (0 or 1, high or low)</figcaption></figure><p id="0b49" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">范围:{0或1 }或1</p><p id="36b9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">例子:f(2) = 1，f(-4) = 0，f(0) = 0，f(1) = 1</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lr"><img src="../Images/0b4b383a4284da318c0488aba9e5706f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lZMnAj3RbuHGMLC68Av1Sw.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Source — <a class="ae kw" href="https://en.wikipedia.org/wiki/Heaviside_step_function" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Heaviside_step_function</a></figcaption></figure><ul class=""><li id="2bf2" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc"> Sigmoid或逻辑激活函数(软步骤)- </strong>主要用于二元分类问题(即输出范围为0-1的值)。它有渐变消失的问题。由于输入(X)在输出(Y)中引起非常小的变化，网络在某些时期后拒绝学习或学习非常慢。它是一个广泛用于分类问题的激活函数，但最近。这个函数更容易使后面的层饱和，使得训练更加困难。计算Sigmoid函数的导数非常容易。</li></ul><p id="d09b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于神经网络中的反向传播过程，你的误差将在每一层被压缩(至少)四分之一。所以，你的关系网越深，从数据中获得的知识就会越多。我们从输出层获得的一些“大”错误可能不会对相对较浅的层中的神经元的突触权重产生太大影响(“浅”意味着它接近输入层)——来源<a class="ae kw" href="https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions" rel="noopener ugc nofollow" target="_blank">https://github . com/kul bear/deep-learning-nano-foundation/wiki/ReLU-and-soft max-Activation-Functions</a></p><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/56ee4fe39679e99c16e31a418f0fa9f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*Bhzlu8WmM1UFo8TltoRHOQ.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Sigmoid or Logistic activation function</figcaption></figure><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/a6e4f3c7e0e9d65cffdfcd55879d06d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*Q_fCmWPcz4F8IoNXm9tqcg.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Derivative of the sigmoid function</figcaption></figure><p id="ba08" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">范围:(0，1)</p><p id="1638" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">例如:f(4) = 0.982，f(-3) = 0.0474，f(-5) = 0.0067</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lu"><img src="../Images/98c2556096f1d60f227d97c2895a26c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1aJzppuf9eoC2Ojwr7myw.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Source — <a class="ae kw" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Logistic_function</a></figcaption></figure><figure class="lh li lj lk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lv"><img src="../Images/99bcde68e8d8b9927f589f0173166b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lKk5Q2arktxQ0m1hJYZqTA.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Source — <a class="ae kw" href="https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions" rel="noopener ugc nofollow" target="_blank">https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions</a></figcaption></figure><ul class=""><li id="fdaa" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc">双曲正切(TanH)——</strong>它看起来像一个缩放的sigmoid函数。数据以零为中心，所以导数会更高。Tanh比sigmoid和logistic激活函数收敛更快</li></ul><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/b24ab4257e3dc3a90a863729cd258aaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*rACT-asoF6gANDE2fW07IQ.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Equation for Hyperbolic Tangent(TanH) activation function</figcaption></figure><p id="bbf2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">范围:(-1，1)</p><p id="227a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">示例:tanh(2) = 0.9640，tanh(-0.567) = -0.5131，tanh(0) = 0</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lx"><img src="../Images/b5d88d9cae1a95988e544c82347074e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RtVgvbjI7v8CuUcAC2QM8Q.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Source — <a class="ae kw" href="https://commons.wikimedia.org/wiki/File:Hyperbolic_Tangent.svg" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/wiki/File:Hyperbolic_Tangent.svg</a></figcaption></figure><ul class=""><li id="f4f3" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc">整流线性单元(ReLU)——</strong>训练速度比tanh快6倍。当输入值小于零时，输出值将为零。如果输入大于或等于零，则输出等于输入。当输入值为正值时，导数为1，因此不会出现在sigmoid函数反向传播误差的情况下出现的压缩效应。</li></ul><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/45ae7141703aff2bafeeaf4a836529c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*ZH-D-NXMq82joIHyJocZ3w.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Equation for Rectified Linear Unit(ReLU) activation function</figcaption></figure><p id="1ce2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">范围:[0，x]</p><p id="ca2e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">例如:f(-5) = 0，f(0) = 0 &amp; f(5) = 5</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/6fcc745a5cdb256d24b4376de172edf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*w275Sin5bKAIaWBaJ6zXcA.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Source — <a class="ae kw" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Rectifier_(neural_networks)</a></figcaption></figure><figure class="lh li lj lk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ma"><img src="../Images/bf01a34b29a66af6ab319b5f215ea19d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BQeReUpqHQgVOTjLHpeyLQ.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">From Andrej Karpathy’s CS231n course</figcaption></figure><ul class=""><li id="4601" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc">泄漏整流线性单元(泄漏ReLU) — </strong>当单元不工作时，泄漏ReLU允许一个小的非零梯度。0.01是小的非零梯度qhere。</li></ul><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/32a4a0f74c1aac6fd880a87dd25e24d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*qVrYlFchG7YiTX5WWx1r7A.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Equation for Leaky Rectified Linear Unit (Leaky ReLU) activation function</figcaption></figure><p id="4e60" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">范围:(-∞，+∞)</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/8780fc2079b4f10ed8f740115da1ed92.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*W6BURQnUE62qyxJxMDpdnA.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Leaky Rectified Linear Unit(Leaky ReLU)</figcaption></figure><ul class=""><li id="5edb" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc">参数校正线性单元(PReLU)——</strong>它使<strong class="ka jc"> </strong>泄漏系数成为一个参数，该参数与其他神经网络参数一起学习。α(α)是这里的泄漏系数。</li></ul><p id="b6b2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于α ≤ 1 f(x) = max(x，αx)</p><p id="2a0b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">范围:(-∞，+∞)</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/ea7d5988df0d1133f7e9b6666b76b45b.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*pZ5_JgEGDHEWsTFoVfK_2g.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Equation for Parametric Rectified Linear Unit(PReLU)</figcaption></figure><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/8780fc2079b4f10ed8f740115da1ed92.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*W6BURQnUE62qyxJxMDpdnA.png"/></div></figure><ul class=""><li id="50a5" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc">随机泄漏整流线性单元(RReLU) </strong></li></ul><p id="b3f9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">范围:(-∞，+∞)</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi md"><img src="../Images/ae13678d3aee1e663143f67563606007.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*NwoeUo9Nn85fzUfr9s5tlA.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Randomized Leaky Rectified Linear Unit(RReLU)</figcaption></figure><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi me"><img src="../Images/0610f06a18153f690eb34271fbdb212d.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*2hEeawGNKs0WwGz9I0JqAg.png"/></div></figure><ul class=""><li id="6ba7" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc">指数线性单元(ELU)——</strong>指数线性单元试图使平均激活接近零，从而加快学习速度。研究表明，ELUs比ReLUs具有更高的分类精度。这里，α是一个超参数，需要调整，约束条件是α ≥ 0(零)。</li></ul><p id="f9a6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">范围:(-α，+∞)</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/bf0b88df2e17fa08ece9c54061349c44.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*rc2g2ZIm4lRCNt8gWMhjyA.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Exponential Linear Unit (ELU)</figcaption></figure><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/ec314697ad6b0a4947e74ef16d53582c.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*gfEr6eAKDZT8hHf2t7u7Lw.png"/></div></figure><ul class=""><li id="30c9" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc">比例指数线性单位(SELU) </strong></li></ul><p id="b246" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">范围:(-λα，+∞)</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mh"><img src="../Images/81e875bc9f030eccb41445780df075ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*BJS-0XHaB_BSjqMDIPeUJQ.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Scaled Exponential Linear Unit(SELU)</figcaption></figure><ul class=""><li id="85dd" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc"> S形整流线性激活单元(SReLU) </strong></li></ul><p id="efbd" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">范围:(-∞，+∞)</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/09ff0d1627e7521b306f3b9f4478c69f.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*u35Rsj78T6nOoZJheODMRQ.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">S-shaped Rectified Linear Activation Unit</figcaption></figure><ul class=""><li id="bc41" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc">自适应分段线性(APL) </strong></li></ul><p id="e136" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">范围:(-∞，+∞)</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/9d5554951b235f12f0cf0281ab54a656.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*NNuE0OdrkeRDVADeARq4Pw.png"/></div></figure><ul class=""><li id="7ae0" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc">soft plus—</strong>soft plus函数的导数是逻辑函数。ReLU和Softplus在很大程度上是相似的，除了在0(零)附近，softplus非常平滑且可微分。计算ReLU及其导数比使用log(.)和exp(。)的提法。</li></ul><p id="761f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">范围:(0，∞)</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/3c0a482cb06630624940664cff717665.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*EyVsonpsBRp5fdNa-djnBw.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Softplus</figcaption></figure><p id="8f89" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">softplus函数的导数是逻辑函数。</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/069d34cf67625329338d89e74f53da58.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*D3YKEgImpixP1lst0_uljQ.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Derivative of the softplus function</figcaption></figure><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/6fcc745a5cdb256d24b4376de172edf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*w275Sin5bKAIaWBaJ6zXcA.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Source — <a class="ae kw" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Rectifier_(neural_networks)</a></figcaption></figure><ul class=""><li id="ac7e" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc">本特的身份</strong></li></ul><p id="935a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">范围:(-∞，+∞)</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/89f246a7f0f2cd671619461756ac71ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*Vsfd4_mfx5SA7TdHrCZhww.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Bent Identity</figcaption></figure><ul class=""><li id="81d4" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka jc"> Softmax- </strong> Softmax函数将原始值转换成后验概率。这提供了一定程度的确定性。它将每个单元的输出压缩到0和1之间，就像一个sigmoid函数一样。但是它也划分每个输出，使得输出的总和等于1。</li></ul><figure class="lh li lj lk gt is gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/3f7a21de6d372f9900c7f8ece76a0bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*XipIlq5eCmQMUDxr4PNH_g.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Equation for Softmax function</figcaption></figure><p id="8846" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">softmax函数的输出相当于分类概率分布，它告诉您任何类为真的概率</p><p id="8d40" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">结论:ReLU及其变体应优先于乙状结肠或tanh激活功能。以及ReLUs训练起来更快。如果ReLU导致神经元死亡，使用Leaky ReLUs或它的其他变体。Sigmoid和tanh遭受消失梯度问题，不应用于隐藏层。ReLUs最适合隐藏层。应该使用易于微分和易于训练的激活函数。</p><p id="37dd" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">参考资料:</p><ol class=""><li id="c2b0" class="kx ky jb ka b kb kc kf kg kj kz kn la kr lb kv ml ld le lf bi translated"><a class="ae kw" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Activation_function</a></li><li id="1aad" class="kx ky jb ka b kb mm kf mn kj mo kn mp kr mq kv ml ld le lf bi translated"><a class="ae kw" href="https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions" rel="noopener ugc nofollow" target="_blank">https://github . com/kul bear/deep-learning-nano-foundation/wiki/ReLU-and-soft max-Activation-Functions</a></li><li id="a959" class="kx ky jb ka b kb mm kf mn kj mo kn mp kr mq kv ml ld le lf bi translated"><a class="ae kw" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Rectifier _(neural _ networks)</a></li></ol></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="f099" class="my mz jb bd na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv bi translated">关于</h1><blockquote class="nw nx ny"><p id="2cf4" class="jy jz nz ka b kb kc kd ke kf kg kh ki oa kk kl km ob ko kp kq oc ks kt ku kv ij bi translated">在<a class="ae kw" href="http://matelabs.in/" rel="noopener ugc nofollow" target="_blank"><em class="jb">Mate Labs</em></a><em class="jb">我们</em>已经构建了<a class="ae kw" href="https://www.mateverse.com/" rel="noopener ugc nofollow" target="_blank"> Mateverse </a>，一个全自动的机器学习平台，在这里你可以构建<strong class="ka jc">定制的ML模型，比</strong> <strong class="ka jc">快10倍，而无需编写一行代码</strong>。我们利用专有技术、复杂管道、大数据支持、自动化数据预处理(使用ML模型的缺失值插补、异常值检测和格式化)、自动化超参数优化和<a class="ae kw" href="http://bit.ly/2UKMO2J" rel="noopener ugc nofollow" target="_blank">等等，让分析师和数据科学家的工作变得更加轻松。</a></p><p id="2071" class="jy jz nz ka b kb kc kd ke kf kg kh ki oa kk kl km ob ko kp kq oc ks kt ku kv ij bi translated">为了帮助您的企业采用机器学习，而不会浪费您的团队在数据清理和创建有效数据模型方面的时间，请填写<strong class="ka jc"> </strong> <a class="ae kw" href="https://matelabs.typeform.com/to/LIAau1" rel="noopener ugc nofollow" target="_blank"> <strong class="ka jc">类型表</strong> </a> <a class="ae kw" href="http://bit.ly/formcontactus" rel="noopener ugc nofollow" target="_blank"> <strong class="ka jc">此处</strong> </a>，我们将与您联系。</p></blockquote><h1 id="3e27" class="my mz jb bd na nb od nd ne nf oe nh ni nj of nl nm nn og np nq nr oh nt nu nv bi translated">让我们携起手来。</h1><blockquote class="nw nx ny"><p id="d29d" class="jy jz nz ka b kb kc kd ke kf kg kh ki oa kk kl km ob ko kp kq oc ks kt ku kv ij bi translated">在<a class="ae kw" href="https://twitter.com/matelabs_ai" rel="noopener ugc nofollow" target="_blank"> <strong class="ka jc"> Twitter </strong> </a> <strong class="ka jc">、LinkedIn </strong>上与我们分享您的想法</p><p id="2c10" class="jy jz nz ka b kb kc kd ke kf kg kh ki oa kk kl km ob ko kp kq oc ks kt ku kv ij bi translated">如果你有新的建议，请告诉我们。我们的耳朵和眼睛总是为真正令人兴奋的事情而张开。</p></blockquote></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><figure class="lh li lj lk gt is"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h1 id="5462" class="my mz jb bd na nb od nd ne nf oe nh ni nj of nl nm nn og np nq nr oh nt nu nv bi translated">之前我们分享过。</h1><ol class=""><li id="1232" class="kx ky jb ka b kb ok kf ol kj om kn on kr oo kv ml ld le lf bi translated"><a class="ae kw" href="https://medium.com/@matelabs_ai/how-to-train-a-machine-learning-model-in-5-minutes-c599fa20e7d5" rel="noopener">如何在5分钟内训练出一个机器学习模型。</a></li><li id="17ff" class="kx ky jb ka b kb mm kf mn kj mo kn mp kr mq kv ml ld le lf bi translated"><a class="ae kw" href="https://medium.com/@matelabs_ai/public-data-sets-use-these-to-train-machine-learning-models-on-mateverse-4dda18a27851" rel="noopener">公共数据集:用这些在Mateverse上训练机器学习模型。</a></li><li id="bd24" class="kx ky jb ka b kb mm kf mn kj mo kn mp kr mq kv ml ld le lf bi translated"><a class="ae kw" href="https://medium.com/@matelabs_ai/big-announcement-mateverse-is-in-public-beta-a968e727cfdc" rel="noopener"> Mateverse公测公告。</a></li><li id="53aa" class="kx ky jb ka b kb mm kf mn kj mo kn mp kr mq kv ml ld le lf bi translated"><a class="ae kw" href="https://medium.com/startup-grind/why-do-we-need-the-democratization-of-machine-learning-80104e43c76f" rel="noopener">为什么我们需要机器学习民主化？</a></li><li id="fe94" class="kx ky jb ka b kb mm kf mn kj mo kn mp kr mq kv ml ld le lf bi translated">一个非常清晰的解释全有线电视新闻网的实施。<a class="ae kw" href="https://medium.com/@matelabs_ai/how-these-researchers-tried-something-unconventional-to-came-out-with-a-smaller-yet-better-image-544327f30e72" rel="noopener">这些研究人员如何尝试一些非常规的东西来实现更小但更好的图像识别。</a></li><li id="eaa8" class="kx ky jb ka b kb mm kf mn kj mo kn mp kr mq kv ml ld le lf bi translated">我们的愿景。<a class="ae kw" href="https://medium.com/startup-grind/what-everyone-is-not-telling-you-about-artificial-intelligence-36c8552f3f53" rel="noopener">关于人工智能大家都没告诉你的事情</a></li></ol></div></div>    
</body>
</html>