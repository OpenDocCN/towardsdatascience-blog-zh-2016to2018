<html>
<head>
<title>The 5 Clustering Algorithms Data Scientists Need to Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学家需要知道的 5 种聚类算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68?source=collection_archive---------0-----------------------#2018-02-05">https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68?source=collection_archive---------0-----------------------#2018-02-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><blockquote class="jq jr js"><p id="7c5d" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">想获得灵感？快来加入我的<a class="ae ks" href="https://www.superquotes.co/?utm_source=mediumtech&amp;utm_medium=web&amp;utm_campaign=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="jw iu">超级行情快讯</strong> </a>。😎</p></blockquote><p id="4bcf" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">聚类是一种涉及数据点分组的机器学习技术。给定一组数据点，我们可以使用聚类算法将每个数据点分类到特定的组中。理论上，同一组中的数据点应该具有相似的属性和/或特征，而不同组中的数据点应该具有非常不同的属性和/或特征。聚类是一种无监督学习的方法，并且是在许多领域中使用的统计数据分析的常用技术。</p><p id="f79d" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">在数据科学中，我们可以使用聚类分析从数据中获得一些有价值的见解，方法是在应用聚类算法时查看数据点属于哪些组。今天，我们将看看数据科学家需要了解的 5 种流行的聚类算法及其优缺点！</p><h2 id="43af" class="kw kx it bd ky kz la dn lb lc ld dp le kt lf lg lh ku li lj lk kv ll lm ln lo bi translated">k 均值聚类</h2><p id="7850" class="pw-post-body-paragraph jt ju it jw b jx lp jz ka kb lq kd ke kt lr kh ki ku ls kl km kv lt kp kq kr im bi translated">K-Means 可能是最著名的聚类算法。它在许多介绍数据科学和机器学习的课程中讲授。很容易理解，用代码实现！请看下图的插图。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/36034c1849d12a436840f8c75a8697ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*KrcZK0xYgTa4qFrVr0fO2w.gif"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">K-Means Clustering</figcaption></figure><ol class=""><li id="59c3" class="mg mh it jw b jx jy kb kc kt mi ku mj kv mk kr ml mm mn mo bi translated">首先，我们首先选择一些要使用的类/组，并随机初始化它们各自的中心点。为了确定要使用的类的数量，最好快速浏览一下数据，并尝试识别任何不同的分组。中心点是与每个数据点向量长度相同的向量，是上图中的“X”。</li><li id="8d8c" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">每个数据点通过计算该点和每个组中心之间的距离来分类，然后将该点分类到其中心最接近它的组中。</li><li id="86c4" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">基于这些分类的点，我们通过取组中所有向量的平均值来重新计算组中心。</li><li id="2ff6" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">重复这些步骤一定次数的迭代，或者直到组中心在迭代之间没有太大变化。您也可以选择随机初始化组中心几次，然后选择看起来提供最佳结果的运行。</li></ol><p id="75e3" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">K-Means 的优点是它非常快，因为我们真正做的是计算点和组中心之间的距离；非常少的计算！因此它具有线性复杂度<em class="jv"> O </em> ( <em class="jv"> n </em>)。</p><p id="921d" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">另一方面，K-Means 也有一些缺点。首先，你必须选择有多少组/类。这并不总是微不足道的，在理想情况下，我们希望使用聚类算法来为我们找出这些问题，因为它的目的是从数据中获得一些洞察力。K-means 也是从随机选择聚类中心开始的，因此它可能在不同的算法运行中产生不同的聚类结果。因此，结果可能不可重复且缺乏一致性。其他聚类方法更加一致。</p><p id="4d37" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">K-Medians 是另一种与 K-Means 相关的聚类算法，只是我们使用组的中值向量，而不是使用平均值来重新计算组中心点。这种方法对异常值不太敏感(因为使用了中值)，但对于较大的数据集来说速度要慢得多，因为在计算中值向量时，每次迭代都需要排序。</p><h2 id="71b9" class="kw kx it bd ky kz la dn lb lc ld dp le kt lf lg lh ku li lj lk kv ll lm ln lo bi translated">均值漂移聚类</h2><p id="dd43" class="pw-post-body-paragraph jt ju it jw b jx lp jz ka kb lq kd ke kt lr kh ki ku ls kl km kv lt kp kq kr im bi translated">均值漂移聚类是一种基于滑动窗口的算法，它试图找到数据点的密集区域。这是一种基于质心的算法，意味着目标是定位每个组/类的中心点，它通过将中心点的候选更新为滑动窗口内的点的平均值来工作。然后，在后处理阶段过滤这些候选窗口，以消除近似重复，形成最终的中心点集合及其相应的组。请看下图的插图。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/010bdf93663f2500ea6515a58fd56476.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/1*bkFlVrrm4HACGfUzeBnErw.gif"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Mean-Shift Clustering for a single sliding window</figcaption></figure><ol class=""><li id="8d9d" class="mg mh it jw b jx jy kb kc kt mi ku mj kv mk kr ml mm mn mo bi translated">为了解释均值漂移，我们将考虑二维空间中的一组点，如上图所示。我们从以点 C(随机选择的)为中心并且以半径 r 为核心的圆形滑动窗口开始。均值漂移是一种爬山算法，它涉及在每一步将该核迭代地转移到更高密度的区域，直到收敛。</li><li id="cc56" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">在每次迭代中，通过将中心点移动到窗口内点的平均值，滑动窗口向更高密度的区域移动(因此得名)。滑动窗口内的密度与窗口内的点数成比例。自然地，通过移动到窗口中点的平均值，它将逐渐向更高点密度的区域移动。</li><li id="61f9" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">我们继续根据平均值移动滑动窗口，直到没有移动可以容纳内核中更多点的方向。看看上面的图片；我们继续移动圆圈，直到我们不再增加密度(即窗口中的点数)。</li><li id="1fd8" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">对许多滑动窗口进行步骤 1 到 3 的过程，直到所有点都位于一个窗口内。当多个滑动窗口重叠时，包含最多点的窗口被保留。然后根据数据点所在的滑动窗口对其进行聚类。</li></ol><p id="2d2a" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">下图显示了使用所有滑动窗口的端到端的整个过程。每个黑点代表滑动窗口的质心，每个灰点是数据点。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/7cc0bc47f3ef7108f2aa9386369b799d.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*vyz94J_76dsVToaa4VG1Zg.gif"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">The entire process of Mean-Shift Clustering</figcaption></figure><p id="0485" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">与 K-means 聚类相比，不需要选择聚类的数量，因为 mean-shift 会自动发现这一点。这是一个巨大的优势。聚类中心向最大密度的点汇聚的事实也是非常理想的，因为它非常容易理解，并且在自然数据驱动的意义上非常适合。缺点是窗口大小/半径“r”的选择可能不重要。</p><h2 id="4591" class="kw kx it bd ky kz la dn lb lc ld dp le kt lf lg lh ku li lj lk kv ll lm ln lo bi translated">基于密度的噪声应用空间聚类(DBSCAN)</h2><p id="3f3c" class="pw-post-body-paragraph jt ju it jw b jx lp jz ka kb lq kd ke kt lr kh ki ku ls kl km kv lt kp kq kr im bi translated">DBSCAN 是一种基于密度的聚类算法，类似于 mean-shift，但有几个显著的优点。看看下面的另一幅精美图片，让我们开始吧！</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/2838b8fa2fca2895918817a6225f7563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/1*tc8UF-h0nQqUfLC8-0uInQ.gif"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">DBSCAN Smiley Face Clustering</figcaption></figure><ol class=""><li id="b636" class="mg mh it jw b jx jy kb kc kt mi ku mj kv mk kr ml mm mn mo bi translated">DBSCAN 从一个尚未访问的任意起始数据点开始。使用距离ε提取该点的邻域(在ε距离内的所有点都是邻域点)。</li><li id="7dab" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">如果在这个邻域内有足够数量的点(根据最小点),则聚类过程开始，并且当前数据点成为新聚类中的第一个点。否则，该点将被标记为噪声(稍后该噪声点可能成为聚类的一部分)。在这两种情况下，该点都被标记为“已访问”。</li><li id="a6f6" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">对于新聚类中的第一个点，其ε距离邻域内的点也成为同一聚类的一部分。然后，对于刚刚添加到聚类组的所有新点，重复使ε邻域中的所有点属于同一聚类的过程。</li><li id="17b3" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">重复步骤 2 和 3 的过程，直到聚类中的所有点都被确定，即聚类的ε邻域内的所有点都被访问和标记。</li><li id="c6e5" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">一旦我们完成了当前的聚类，就会检索和处理一个新的未访问点，从而发现下一个聚类或噪声。重复此过程，直到所有点都被标记为已访问。由于在此结束时已经访问了所有点，每个点将被标记为属于一个聚类或者是噪声。</li></ol><p id="f5fd" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">DBSCAN 与其他聚类算法相比有很大的优势。首先，它根本不需要 pe-set 数量的集群。它还将异常值识别为噪声，这与均值漂移不同，即使数据点非常不同，均值漂移也只是将它们放入一个聚类中。此外，它可以很好地找到任意大小和任意形状的簇。</p><p id="20f7" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">DBSCAN 的主要缺点是，当集群的密度不同时，它的性能不如其他方法。这是因为当密度变化时，用于识别邻域点的距离阈值ε和最小点的设置将随着聚类的不同而不同。这一缺点在非常高维的数据中也会出现，因为距离阈值ε再次变得难以估计。</p><h2 id="4c05" class="kw kx it bd ky kz la dn lb lc ld dp le kt lf lg lh ku li lj lk kv ll lm ln lo bi translated">使用高斯混合模型的期望最大化(EM)聚类(GMM)</h2><p id="18aa" class="pw-post-body-paragraph jt ju it jw b jx lp jz ka kb lq kd ke kt lr kh ki ku ls kl km kv lt kp kq kr im bi translated">K-Means 的一个主要缺点是它简单地使用了聚类中心的平均值。通过下面的图片，我们可以明白为什么这不是最好的做事方式。在左手边，对于人眼来说，很明显有两个半径不同的圆形星团，它们以相同的平均值为中心。K-Means 不能处理这个问题，因为聚类的平均值非常接近。K-Means 在聚类不是圆形的情况下也会失败，这也是使用平均值作为聚类中心的结果。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/1464bc9420893251093ade5645a55c61.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*Xvl-pXxsLAZ7gbTUuvgMtA.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Two failure cases for K-Means</figcaption></figure><p id="09b4" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">高斯混合模型(GMM)比 K-Means 给了我们更多的灵活性。对于 GMM，我们假设数据点是高斯分布的；这是一个限制性较小的假设，而不是通过使用平均值说它们是圆形的。这样，我们就有两个参数来描述聚类的形状:均值和标准差！以二维为例，这意味着集群可以采取任何类型的椭圆形状(因为我们在 x 和 y 方向上都有标准偏差)。因此，每个高斯分布被分配给单个聚类。</p><p id="1155" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">为了找到每个聚类的高斯参数(例如平均值和标准偏差)，我们将使用一种称为期望最大化(EM)的优化算法。请看下图，图中显示了高斯分布正被装配到集群中。然后，我们可以继续使用 GMM 进行期望最大化聚类的过程。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi my"><img src="../Images/da78dd1b1895f53d8c05b55818e6eaac.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/1*OyXgise21a23D5JCss8Tlg.gif"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">EM Clustering using GMMs</figcaption></figure><ol class=""><li id="b71b" class="mg mh it jw b jx jy kb kc kt mi ku mj kv mk kr ml mm mn mo bi translated">我们首先选择聚类的数量(像 K-Means 一样)并随机初始化每个聚类的高斯分布参数。人们也可以通过快速查看数据来尝试提供对初始参数的良好猜测。尽管注意，从上图中可以看出，这并不是 100%必要的，因为高斯分布一开始很差，但很快就被优化了。</li><li id="78e6" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">给定每个聚类的高斯分布，计算每个数据点属于特定聚类的概率。一个点离高斯中心越近，它就越有可能属于该聚类。这应该有直观的意义，因为对于高斯分布，我们假设大部分数据更靠近聚类的中心。</li><li id="db56" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">基于这些概率，我们为高斯分布计算一组新的参数，从而最大化聚类内数据点的概率。我们使用数据点位置的加权和来计算这些新参数，其中权重是数据点属于该特定聚类的概率。为了直观地解释这一点，我们可以看看上面的图表，特别是以黄色星团为例。该分布在第一次迭代时随机开始，但我们可以看到大多数黄色点位于该分布的右侧。当我们计算一个概率加权的和时，尽管有一些点在中心附近，但大多数都在右边。因此，分布的平均值自然会向这些点的集合移动。我们也可以看到，大部分的点都是“右上到左下”。因此，标准偏差会发生变化，以创建更适合这些点的椭圆，从而最大化概率加权和。</li><li id="e471" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">重复第 2 步和第 3 步，直到收敛，每次迭代的分布变化不大。</li></ol><p id="c40b" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">使用 GMM 有两个主要优势。首先，在<strong class="jw iu">聚类协方差</strong>方面，GMM 比 K-Means 更加<strong class="jw iu">灵活</strong>；由于标准偏差参数，聚类可以呈现任何椭圆形状，而不局限于圆形。K-Means 实际上是 GMM 的一个特例，其中每个聚类在所有维度上的协方差都接近 0。第二，由于 GMM 使用概率，所以每个数据点可以有多个聚类。因此，如果一个数据点位于两个重叠聚类的中间，我们可以简单地定义它的类别，即 X %属于类别 1，Y %属于类别 2。即 GMMs 支持<strong class="jw iu">混合</strong> <strong class="jw iu">会员</strong>。</p><h2 id="3c62" class="kw kx it bd ky kz la dn lb lc ld dp le kt lf lg lh ku li lj lk kv ll lm ln lo bi translated">凝聚层次聚类</h2><p id="3178" class="pw-post-body-paragraph jt ju it jw b jx lp jz ka kb lq kd ke kt lr kh ki ku ls kl km kv lt kp kq kr im bi translated">分层聚类算法分为两类:自顶向下或自底向上。自底向上算法在开始时将每个数据点视为单个聚类，然后连续合并(或<em class="jv">聚集</em>)成对的聚类，直到所有聚类都已合并为包含所有数据点的单个聚类。自底向上的层次聚类因此被称为<em class="jv">层次凝聚聚类</em>或<em class="jv"> HAC </em>。这种聚类层次结构被表示为一棵树(或树状图)。树根是收集所有样本的唯一聚类，树叶是只有一个样本的聚类。在进入算法步骤之前，请看下图</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mz"><img src="../Images/a62c4ec2a9799f1cd6381507b9a81d2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ET8kCcPpr893vNZFs8j4xg.gif"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Agglomerative Hierarchical Clustering</figcaption></figure><ol class=""><li id="bd76" class="mg mh it jw b jx jy kb kc kt mi ku mj kv mk kr ml mm mn mo bi translated">我们首先将每个数据点视为一个单独的聚类，即如果我们的数据集中有 X 个数据点，那么我们就有 X 个聚类。然后，我们选择一个距离度量来度量两个集群之间的距离。例如，我们将使用<em class="jv">平均链接</em>，其将两个聚类之间的距离定义为第一个聚类中的数据点和第二个聚类中的数据点之间的平均距离。</li><li id="5d0a" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">在每次迭代中，我们将两个集群合并为一个。将被组合的两个聚类被选择为具有最小平均链接的那些聚类。即，根据我们选择的距离度量，这两个聚类彼此之间具有最小的距离，因此是最相似的，并且应该被组合。</li><li id="e1c6" class="mg mh it jw b jx mp kb mq kt mr ku ms kv mt kr ml mm mn mo bi translated">重复步骤 2，直到我们到达树的根，即我们只有一个包含所有数据点的聚类。通过这种方式，我们可以选择最终需要多少个集群，只需选择何时停止组合集群，即何时停止构建树！</li></ol><p id="8beb" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">分层聚类不需要我们指定聚类的数量，我们甚至可以选择哪个数量的聚类看起来最好，因为我们正在构建一个树。此外，该算法对距离度量的选择不敏感；所有这些算法都可以很好地工作，而对于其他聚类算法，距离度量的选择是至关重要的。层次聚类方法的一个特别好的用例是当基础数据具有层次结构并且您想要恢复该层次结构时；其他聚类算法做不到这一点。层次聚类的这些优势是以较低的效率为代价的，因为它的时间复杂度为<em class="jv"> O(n ) </em>，不像 K-Means 和 GMM 的线性复杂度。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h2 id="220c" class="kw kx it bd ky kz la dn lb lc ld dp le kt lf lg lh ku li lj lk kv ll lm ln lo bi translated">结论</h2><p id="d1f1" class="pw-post-body-paragraph jt ju it jw b jx lp jz ka kb lq kd ke kt lr kh ki ku ls kl km kv lt kp kq kr im bi translated">这是数据科学家应该知道的 5 大聚类算法！感谢 Scikit Learn，我们将以这些算法和其他一些算法的出色表现的令人敬畏的可视化来结束本文！非常酷地看到不同的算法如何比较和对比不同的数据！</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nl"><img src="../Images/881dbed49b55f2f71e3cf3c8a0806d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oNt9G9UpVhtyFLDBwEMf8Q.png"/></div></div></figure></div></div>    
</body>
</html>