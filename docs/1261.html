<html>
<head>
<title>Statistics is Freaking Hard: WTF is Activation function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">统计异常困难:WTF是激活函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/statistics-is-freaking-hard-wtf-is-activation-function-df8342cdf292?source=collection_archive---------4-----------------------#2017-08-16">https://towardsdatascience.com/statistics-is-freaking-hard-wtf-is-activation-function-df8342cdf292?source=collection_archive---------4-----------------------#2017-08-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="191a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">和任何事情一样，我不是在描述任何独特的事情。有很多人比我更了解这个话题。这个故事是写给我脑海中的另一个人的😬。我确实经常和他聊天，他突然表达了对统计学和机器学习的兴趣。</p><p id="b26d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么，什么是激活功能呢？神经网络中的神经元松散地模仿我们的大脑神经元。啊！现在，我明白为什么它的名字是一样的了。我们大脑中的神经元会根据输入信息来激发，这不知何故让我们变得聪明！！！神经网络神经元中的激活函数是决定神经元是否应该触发以及以何种强度触发的函数。</p><p id="736f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们来看看不同的常用激活函数及其特性。</p><p id="412f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是神经元的样子</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/f16aa2ff9c888174ea0c8f6c22a349cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Neuron</figcaption></figure><p id="5b82" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">激活函数的输入值将在-∞和∞之间</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="89af" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak">步进功能</strong></h1><p id="a3aa" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">阶跃函数由阈值定义。<br/>如果输入值大于阈值，则输出为1，否则为0。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mh"><img src="../Images/2184b4a30e87c7a9d5f6924ba0f4c4ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aq8laYoRslGXlLFPg35mHw.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">step function</figcaption></figure><p id="2cf3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">是因为长得像台阶才叫台阶函数的吗？听起来比说基于阈值的函数更酷。</p><p id="0cdb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">缺点是非常二进制。它会丢失输入值中的任何梯度知识。阈值的10000000倍对阈值具有相同的影响。此外，忽略任何低于阈值的输入值。</p><p id="6cb5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另外，考虑一个你正在开发分类器的例子。如果多个神经元触发，我们如何确定选择哪一个？所有放电神经元的值都是1。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="963d" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">线性函数</h1><p id="c9b7" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">函数是y = cx。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mm"><img src="../Images/857b14592e73fb4ffea6d2afeb7a0f90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MWZPcureJzj8WyEDMYCXHA.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">linear function</figcaption></figure><p id="10f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这样做的一个好处是，在分类器的情况下，你现在可以决定选择哪个神经元。</p><p id="66ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">缺点是你建立了多层神经元，每一层都是线性的，然后它们组合在一起形成一个更大的线性函数。因此，这些额外的层不提供任何价值。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="7d33" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">Sigmoid函数</h1><p id="2049" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">y = 1 / 1 + e⁻ˣ</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mn"><img src="../Images/00730ef80cffabac7826924f3a170797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yv_6SL5tzsf5TEqIyJZ8iA.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">sigmoid function</figcaption></figure><p id="5600" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是更平滑和非线性的特征。这没有线性函数的缺点。另一个优点是该值介于0和1之间。</p><p id="5fa9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">缺点是在更高的值上梯度知识消失。这就是所谓的消失梯度问题。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="a5b7" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">Tanh函数</h1><p id="119e" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">y = tanh(x)</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mo"><img src="../Images/c1b662ac5994d6df70b2b70311f07914.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6H7HV4p7W-4aNRQ9FU0gCw.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">tanh function</figcaption></figure><p id="048f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这非常类似于sigmoid函数。tanh的梯度比sigmoid更陡。tanh也有渐变消失的问题。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="2f8d" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">ReLu函数</h1><p id="f469" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">y =最大值(0，x)</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mp"><img src="../Images/fb8864f075973057069ef54eff4f3335.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5WcV2gGh7-wODFUJY55kUA.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">ReLu function</figcaption></figure><p id="6d9d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是非线性的，所以它没有线性函数的缺点。ReLu的一个优点是激发的神经元数量较少。</p><p id="16be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从计算的角度来看，这是比较便宜的。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="bb35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">依我拙见，你可以根据你喜欢的特征选择任何激活函数。您可以组合这些功能，甚至调整上述基本激活功能，以获得您想要的特性。</p></div></div>    
</body>
</html>