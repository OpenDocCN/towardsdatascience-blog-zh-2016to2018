<html>
<head>
<title>Tflearn: Solving XOR with a 2x2x1 feed forward neural network in Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Tflearn:在 Tensorflow 中使用 2x2x1 前馈神经网络求解 XOR</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tflearn-soving-xor-with-a-2x2x1-feed-forward-neural-network-6c07d88689ed?source=collection_archive---------2-----------------------#2017-07-22">https://towardsdatascience.com/tflearn-soving-xor-with-a-2x2x1-feed-forward-neural-network-6c07d88689ed?source=collection_archive---------2-----------------------#2017-07-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="5943" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">关于如何在 python tflearn 中仅使用 12 行代码训练 2x2x1 前馈神经网络来解决 XOR 问题的简单指南 python tflearn 是基于 Tensorflow 构建的深度学习库。</strong></p><p id="5af7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的网络的目标是训练一个网络接收两个布尔输入，并且仅当一个输入为真而另一个为假时才返回真。</p><h1 id="713f" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">包装</h1><pre class="lj lk ll lm gt ln lo lp lq aw lr bi"><span id="dfac" class="ls km iq lo b gy lt lu l lv lw"><strong class="lo ir">from</strong> <strong class="lo ir">tflearn</strong> <strong class="lo ir">import</strong> DNN<br/><strong class="lo ir">from</strong> <strong class="lo ir">tflearn.layers.core</strong> <strong class="lo ir">import</strong> input_data, dropout, fully_connected <strong class="lo ir">from</strong> <strong class="lo ir">tflearn.layers.estimator</strong> <strong class="lo ir">import</strong> regression</span></pre><h1 id="4035" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">输入数据</h1><p id="58d6" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我们将输入数据<strong class="jp ir"> X </strong>和预期结果<strong class="jp ir"> Y </strong>定义为列表的列表。<strong class="jp ir"> <br/> </strong>由于神经网络本质上只处理数值，我们将把布尔表达式转换成数字，这样<strong class="jp ir"> True=1 </strong>和<strong class="jp ir"> False=0 </strong></p><pre class="lj lk ll lm gt ln lo lp lq aw lr bi"><span id="c9d6" class="ls km iq lo b gy lt lu l lv lw">X = [[0,0], [0,1], [1,0], [1,1]]<br/>Y = [[0], [1], [1], [0]]</span></pre><h1 id="334a" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">模型</h1><p id="1206" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我们定义输入、隐藏和输出层。<br/>语法再简单不过了——对输入层使用<strong class="jp ir"> input_layer() </strong>，对后续层使用<strong class="jp ir">full _ connected()</strong>。</p><pre class="lj lk ll lm gt ln lo lp lq aw lr bi"><span id="31ae" class="ls km iq lo b gy lt lu l lv lw">input_layer = input_data(shape=[<strong class="lo ir">None</strong>, 2])<br/>hidden_layer = fully_connected(input_layer , 2, activation='tanh') <br/>output_layer = fully_connected(hidden_layer, 1, activation='tanh') </span></pre><p id="acd6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么形状的输入是<strong class="jp ir">【无，2】</strong>？<strong class="jp ir"> </strong>网络一次馈入多个学习实例。由于我们在每个学习示例中使用了<strong class="jp ir">两个特征</strong>，并且有<strong class="jp ir">四个示例</strong>，所以我们的数据是<strong class="jp ir">形状的[4，2]。但是有时我们喜欢定义我们的网络，这样它可以接收任意数量的训练样本。我们可以将<strong class="jp ir"> None </strong>用于任意数量的训练示例，并将输入形状定义为<strong class="jp ir">【None，number_of_features，…】</strong></strong></p><pre class="lj lk ll lm gt ln lo lp lq aw lr bi"><span id="368d" class="ls km iq lo b gy lt lu l lv lw">regression = regression(output_layer , optimizer='sgd', loss='binary_crossentropy', learning_rate=5)<br/>model = DNN(regression)</span></pre><p id="bd74" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的代码块中，我们定义了将执行反向传播和训练网络的回归变量。我们将使用随机梯度下降<strong class="jp ir"> </strong>作为优化方法，使用二元交叉熵作为损失函数。</p><p id="bcbc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们简单地使用<strong class="jp ir"> DNN()在 ftlearn 中定义我们的(几乎)深度神经网络。</strong></p><p id="3255" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们需要训练模型。在此过程中，回归器将尝试优化损失函数。训练的最终结果只是连接层节点的权重(和偏差)。</p><h1 id="ee89" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">培养</h1><pre class="lj lk ll lm gt ln lo lp lq aw lr bi"><span id="e294" class="ls km iq lo b gy lt lu l lv lw">model.fit(X, Y, n_epoch=5000, show_metric=<strong class="lo ir">True</strong>)</span></pre><p id="f350" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">运行 model.fit()后，Tensorflow 会馈送输入数据 5000 次，并尝试拟合模型。</p><p id="74ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你的输出看起来像这样(以小损耗和高精度为目标)，</p><pre class="lj lk ll lm gt ln lo lp lq aw lr bi"><span id="a8a9" class="ls km iq lo b gy lt lu l lv lw">&gt;&gt;&gt; Training Step: 5048  | total loss: <strong class="lo ir">0.31394</strong> | time: 0.002s<br/>| SGD | epoch: 5048 | loss: 0.31394 - binary_acc: 0.9994 -- iter: 4/4</span></pre><p id="0c2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你的模型有 0.999%的准确性，这意味着它成功地学会了解决问题。</p><blockquote class="mc md me"><p id="ed7f" class="jn jo mf jp b jq jr js jt ju jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj kk ij bi translated">请注意，您的回归变量不会总是产生相同的结果。它甚至可能无法学会正确解决我们的问题。这是因为网络权重是每次随机初始化的。神经网络<strong class="jp ir">也需要大量的训练数据</strong>来使反向传播正常工作。因此，我们的代码非常依赖于权重的初始化方式。</p></blockquote><h1 id="07d9" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">预言；预测；预告</h1><p id="e07a" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">为了检查我们的模型是否真的有效，让我们预测所有可能的组合，并使用简单的列表理解将输出转换为布尔值</p><pre class="lj lk ll lm gt ln lo lp lq aw lr bi"><span id="64b1" class="ls km iq lo b gy lt lu l lv lw">[i[0] &gt; 0 <strong class="lo ir">for</strong> i <strong class="lo ir">in</strong> model.predict(X)]<br/>&gt;&gt;&gt; [False, True, True, False]</span></pre><p id="03d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">太好了！我们的模型有效。</p><p id="865d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是模型使用了什么逻辑来解决异或问题呢？让我们检查引擎盖下面。</p><h1 id="283e" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">重量分析</h1><p id="8487" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">与 AND 和 OR 不同，<strong class="jp ir"> XOR 的输出不是线性可分的</strong>。<br/>因此，我们需要引入另一个隐藏层来解决它。原来，隐藏层中的每个节点都代表一种更简单的线性可分逻辑运算(AND、OR、NAND、…)，输出层将充当由前一层的输出提供的另一种逻辑运算。</p><p id="210f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们仅限于使用简单的逻辑运算，我们可以将 XOR 定义为</p><blockquote class="mj"><p id="8b25" class="mk ml iq bd mm mn mo mp mq mr ms kk dk translated"><em class="mt"> XOR(X1，X2) = AND(OR(X1，X2)，NAND(X1，X2)) </em></p></blockquote><p id="fd1c" class="pw-post-body-paragraph jn jo iq jp b jq mu js jt ju mv jw jx jy mw ka kb kc mx ke kf kg my ki kj kk ij bi translated">为了理解我们的网络使用什么逻辑来得出结果，我们需要分析它的权重(和偏差)。<br/>我们用<strong class="jp ir"> model.get_weights(layer。W) </strong>获取权重向量和<strong class="jp ir"> model.get_weights(层。W) </strong>得到偏差向量。</p><pre class="lj lk ll lm gt ln lo lp lq aw lr bi"><span id="bb10" class="ls km iq lo b gy lt lu l lv lw">print(model.get_weights(hidden_layer.W), model.get_weights(hidden_layer.b))<br/>print(model.get_weights(output_layer.W), model.get_weights(output_layer.b))</span><span id="86cb" class="ls km iq lo b gy mz lu l lv lw">&gt;&gt;&gt; [[ 3.86708593 -3.11288071] [ 3.87053323 -3.1126008 ]]<br/>    [-1.82562542  4.58438063]<br/>&gt;&gt;&gt; [[ 5.19325304]<br/>    [-4.87336922]</span></pre><p id="551f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下图显示了各个权重属于哪个节点(为简单起见，数字四舍五入)</p><figure class="lj lk ll lm gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi na"><img src="../Images/5e20b71e86948c09779535bb780b36c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A7FHkcYUFPXn6ceJ_X-fLw.png"/></div></div></figure><blockquote class="mc md me"><p id="84b2" class="jn jo mf jp b jq jr js jt ju jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj kk ij bi translated">X1 <em class="iq">和</em> X2 <em class="iq">是我们的输入节点。</em> a1 <em class="iq">和</em> a2 <em class="iq">是我们隐藏层中的节点，而</em> O <em class="iq">是输出节点。</em> B1 <em class="iq">和</em> B2 <em class="iq">是偏差。</em></p></blockquote><p id="edb3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这告诉我们什么？嗯，还没什么进展。但是通过计算单个输入的节点激活，我们可以看到特定节点的行为。<br/>我们正在使用公式(<em class="mf"> ×代表矩阵乘法</em>):</p><blockquote class="mj"><p id="9c05" class="mk ml iq bd mm mn mo mp mq mr ms kk dk translated">激活= tanh(输入×权重+偏差)</p></blockquote><blockquote class="mc md me"><p id="64e5" class="jn jo mf jp b jq mu js jt ju mv jw jx mg mw ka kb mh mx ke kf mi my ki kj kk ij bi translated">注意，我们使用<em class="iq"/><a class="ae ni" href="http://mathworld.wolfram.com/HyperbolicTangent.html" rel="noopener ugc nofollow" target="_blank">tanh()</a><em class="iq"/>表示激活将在[-1，1] <em class="iq">范围内。</em></p></blockquote><figure class="lj lk ll lm gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nj"><img src="../Images/a0b09e28a47ea0dfc17faeb47e39dfe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dW5tqSyXhqvkJ79RBc_6dw.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">Rounded node activations for individual input combinations for acquired XOR neural network</figcaption></figure><ul class=""><li id="963f" class="no np iq jp b jq jr ju jv jy nq kc nr kg ns kk nt nu nv nw bi translated"><strong class="jp ir"> a1 </strong>当输入中至少有一个 1 时为真(1)。<strong class="jp ir"> a1 </strong>节点<strong class="jp ir"> </strong>因此<strong class="jp ir"> </strong>代表<strong class="jp ir">或</strong>逻辑运算</li><li id="8328" class="no np iq jp b jq nx ju ny jy nz kc oa kg ob kk nt nu nv nw bi translated"><strong class="jp ir"> a2 </strong>始终为真，除非两个输入都为真。<strong class="jp ir"> a2 </strong>节点因此代表<strong class="jp ir">与</strong>逻辑运算</li><li id="d80a" class="no np iq jp b jq nx ju ny jy nz kc oa kg ob kk nt nu nv nw bi translated"><strong class="jp ir">输出节点</strong>只有在<strong class="jp ir"> a1 </strong>和<strong class="jp ir"> a2 </strong>都为真时才为真。</li></ul><p id="e2be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输出节点可以重写为:</p><blockquote class="mj"><p id="6173" class="mk ml iq bd mm mn mo mp mq mr ms kk dk translated"><em class="mt"> O(X1，X2) = </em>和<em class="mt"> (a1(X1，X2)，a2(X1，X2))=</em>(OR(X1<em class="mt">，X2)，NAND(X1，X2)) </em></p></blockquote><p id="85c8" class="pw-post-body-paragraph jn jo iq jp b jq mu js jt ju mv jw jx jy mw ka kb kc mx ke kf kg my ki kj kk ij bi translated">因此，经过训练的网络是 OR(X1，X2)和 NAND(X1，X2)的 AND 运算</p><p id="dc38" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mf">请注意，由于随机权重初始化，结果会有所不同，这意味着每次训练模型时，您的权重可能会有所不同。</em></p><p id="9fc4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">完整的来源可以在<a class="ae ni" href="https://github.com/tadejmagajna/DeepTuts/blob/master/tflearn_simple_xor/tflearn_simple_xor.ipynb" rel="noopener ugc nofollow" target="_blank">这里找到</a>:</p><figure class="lj lk ll lm gt nb"><div class="bz fp l di"><div class="oc od l"/></div></figure></div></div>    
</body>
</html>