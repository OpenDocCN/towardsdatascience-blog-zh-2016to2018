<html>
<head>
<title>How to build your own Neural Network from scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用 Python 从零开始构建自己的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6?source=collection_archive---------0-----------------------#2018-05-14">https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6?source=collection_archive---------0-----------------------#2018-05-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/d894409e88badfa45d60ccd09f679707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*36MELEhgZsPFuzlZvObnxA.gif"/></div></div></figure><div class=""/><div class=""><h2 id="c551" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">理解深度学习内部运作的初学者指南</h2></div><p id="941f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">更新</strong>:当我一年前写这篇文章的时候，我没有想到它会受到<em class="lp">这个</em>的欢迎。从那以后，这篇文章被观看了超过 45 万次，有超过 3 万次鼓掌。它还登上了谷歌的首页，并且是“<em class="lp">神经网络</em>”的前几个搜索结果之一。你们中的许多人都联系过我，这篇文章对你们学习之旅的影响让我深感谦卑。</p><p id="b48d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这篇文章也引起了 Packt 出版社编辑的注意。这篇文章发表后不久，我就被提出作为《用 Python 做   <em class="lp">的<a class="ae lq" href="https://www.amazon.com/Neural-Network-Projects-Python-ultimate-ebook/dp/B07P77QWW7/" rel="noopener ugc nofollow" target="_blank"> <strong class="kv jf"> <em class="lp">神经网络项目》一书的唯一作者。</em>今天，很高兴和大家分享我的书出版了！</strong></a></em></p><p id="7b47" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这本书是这篇文章的延续，它涵盖了神经网络项目在人脸识别、情感分析、噪声消除等领域的端到端实现。每章都有一个独特的神经网络结构，包括卷积神经网络，长期短期记忆网络和暹罗神经网络。如果你正在寻找用深度学习项目创建一个强大的机器学习组合，请考虑购买这本书！</p><p id="f33c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">你可以从亚马逊得到这本书:<a class="ae lq" href="https://www.amazon.com/Neural-Network-Projects-Python-ultimate-ebook/dp/B07P77QWW7/" rel="noopener ugc nofollow" target="_blank"> <strong class="kv jf">用 Python 做的神经网络项目</strong> </a></p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><a href="https://www.amazon.com/Neural-Network-Projects-Python-ultimate-ebook/dp/B07P77QWW7/"><div class="gh gi lr"><img src="../Images/63038564b630e87d711301c685a8ccf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CpzvvYSJVXTc_xCOAhMjIw.jpeg"/></div></a></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="304b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">动机:</strong>作为我个人更好地理解深度学习之旅的一部分，我已经决定从零开始建立一个神经网络，而不需要像 TensorFlow 这样的深度学习库。我认为，理解神经网络的内部工作对任何有抱负的数据科学家都很重要。</p><p id="fe66" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这篇文章包含了我所学到的东西，希望对你也有用！</p><h1 id="b6e2" class="md me je bd mf mg mh mi mj mk ml mm mn kk mo kl mp kn mq ko mr kq ms kr mt mu bi translated">什么是神经网络？</h1><p id="166f" class="pw-post-body-paragraph kt ku je kv b kw mv kf ky kz mw ki lb lc mx le lf lg my li lj lk mz lm ln lo im bi translated">大多数介绍神经网络的文章在描述它们时都会提到大脑类比。在不深究大脑类比的情况下，我发现简单地将神经网络描述为将给定输入映射到期望输出的数学函数更容易。</p><p id="ba8e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">神经网络由以下组件组成</p><ul class=""><li id="e787" class="na nb je kv b kw kx kz la lc nc lg nd lk ne lo nf ng nh ni bi translated">一个<strong class="kv jf">输入层</strong>、<strong class="kv jf">、<em class="lp"> x </em>、</strong></li><li id="a482" class="na nb je kv b kw nj kz nk lc nl lg nm lk nn lo nf ng nh ni bi translated">任意数量的<strong class="kv jf">隐藏层</strong></li><li id="9d03" class="na nb je kv b kw nj kz nk lc nl lg nm lk nn lo nf ng nh ni bi translated">一个<strong class="kv jf">输出层</strong>，<strong class="kv jf">，<em class="lp">，</em>，</strong></li><li id="6ba4" class="na nb je kv b kw nj kz nk lc nl lg nm lk nn lo nf ng nh ni bi translated">一组<strong class="kv jf">权重</strong>和<strong class="kv jf">在各层之间偏置</strong>、<strong class="kv jf">W 和 bT7】</strong></li><li id="2df1" class="na nb je kv b kw nj kz nk lc nl lg nm lk nn lo nf ng nh ni bi translated">选择<strong class="kv jf">激活功能</strong>用于每个隐藏层，<strong class="kv jf"><em class="lp"/></strong>。在本教程中，我们将使用一个 Sigmoid 激活函数。</li></ul><p id="43e7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">下图显示了 2 层神经网络的架构(<em class="lp">注意，在计算神经网络的层数时，输入层通常被排除在外</em></p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/8acb4e64141713630d359e95fd91850a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*sX6T0Y4aa3ARh7IBS_sdqw.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Architecture of a 2-layer Neural Network</figcaption></figure><p id="1304" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">用 Python 创建神经网络类很容易。</p><figure class="ls lt lu lv gt iv"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="780f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">训练神经网络</strong></p><p id="694c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">简单 2 层神经网络的输出<strong class="kv jf"> <em class="lp"> ŷ </em> </strong>为:</p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/8df1fb2223fb40c8cb2a0132ecfb9438.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*E1_l8PGamc2xTNS87XGNcA.png"/></div></figure><p id="5894" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">您可能会注意到，在上面的等式中，权重<strong class="kv jf"> <em class="lp"> W </em> </strong>和偏差<strong class="kv jf"> <em class="lp"> b </em> </strong>是影响输出<strong class="kv jf"> <em class="lp"> ŷ.的唯一变量</em> </strong></p><p id="39d4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">自然，权重和偏差的正确值决定了预测的强度。从输入数据中微调权重和偏差的过程称为<strong class="kv jf">训练神经网络。</strong></p><p id="3b47" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">训练过程的每次迭代包括以下步骤:</p><ul class=""><li id="4cbe" class="na nb je kv b kw kx kz la lc nc lg nd lk ne lo nf ng nh ni bi translated">计算预测输出<strong class="kv jf"> <em class="lp"> ŷ </em> </strong>，称为<strong class="kv jf">前馈</strong></li><li id="7124" class="na nb je kv b kw nj kz nk lc nl lg nm lk nn lo nf ng nh ni bi translated">更新权重和偏差，称为<strong class="kv jf">反向传播</strong></li></ul><p id="6c26" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">下面的序列图说明了这个过程。</p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nw"><img src="../Images/c03ff5ae8008cdd5a57bed024553fe8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CEtt0h8Rss_qPu7CyqMTdQ.png"/></div></div></figure><h2 id="a361" class="nx me je bd mf ny nz dn mj oa ob dp mn lc oc od mp lg oe of mr lk og oh mt oi bi translated">前馈</h2><p id="d9cb" class="pw-post-body-paragraph kt ku je kv b kw mv kf ky kz mw ki lb lc mx le lf lg my li lj lk mz lm ln lo im bi translated">正如我们在上面的序列图中看到的，前馈只是简单的微积分，对于一个基本的 2 层神经网络，神经网络的输出是:</p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/8df1fb2223fb40c8cb2a0132ecfb9438.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*E1_l8PGamc2xTNS87XGNcA.png"/></div></figure><p id="ebcb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们在 python 代码中添加一个前馈函数来做到这一点。注意，为了简单起见，我们假设偏差为 0。</p><figure class="ls lt lu lv gt iv"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="08d0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然而，我们仍然需要一种方法来评估我们预测的“好性”(即我们的预测有多远)？<strong class="kv jf">损失函数</strong>允许我们这样做。</p><h2 id="6dcc" class="nx me je bd mf ny nz dn mj oa ob dp mn lc oc od mp lg oe of mr lk og oh mt oi bi translated">损失函数</h2><p id="30eb" class="pw-post-body-paragraph kt ku je kv b kw mv kf ky kz mw ki lb lc mx le lf lg my li lj lk mz lm ln lo im bi translated">有许多可用的损失函数，我们问题的性质决定了我们对损失函数的选择。在本教程中，我们将使用简单的<strong class="kv jf">平方和误差</strong>作为我们的损失函数。</p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/b1fb4b8130d06fa98f3742abadf82bf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*iNa1VLdaeqwUAxpNXs3jwQ.png"/></div></figure><p id="56ff" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">也就是说，平方和误差就是每个预测值和实际值之间的差值之和。差值是平方的，因此我们可以测量差值的绝对值。</p><p id="3f37" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">我们训练的目标是找到使损失函数最小化的最佳权重和偏差集。</strong></p><h2 id="afa4" class="nx me je bd mf ny nz dn mj oa ob dp mn lc oc od mp lg oe of mr lk og oh mt oi bi translated">反向传播</h2><p id="4bf7" class="pw-post-body-paragraph kt ku je kv b kw mv kf ky kz mw ki lb lc mx le lf lg my li lj lk mz lm ln lo im bi translated">既然我们已经测量了预测的误差(损失)，我们需要找到一种方法来<strong class="kv jf">传播</strong>误差回来，并更新我们的权重和偏差。</p><p id="3e4d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了知道调整权重和偏差的合适量，我们需要知道损失函数相对于权重和偏差的导数。</p><p id="369e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">回想一下微积分，函数的导数就是函数的斜率。</p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/3465832ac6932fdfa850652682911374.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3FgDOt4kJxK2QZlb9T0cpg.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Gradient descent algorithm</figcaption></figure><p id="a459" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果我们有导数，我们可以简单地通过增加/减少来更新权重和偏差(参考上图)。这就是所谓的<strong class="kv jf">梯度下降</strong>。</p><p id="febd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然而，我们不能直接计算损失函数关于权重和偏差的导数，因为损失函数的方程不包含权重和偏差。因此，我们需要<strong class="kv jf">链式法则</strong>来帮助我们计算它。</p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/8e2e07d02b1db0bb32f425b562f3dd2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7zxb2lfWWKaVxnmq2o69Mw.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Chain rule for calculating derivative of the loss function with respect to the weights. Note that for simplicity, we have only displayed the partial derivative assuming a 1-layer Neural Network.</figcaption></figure><p id="603d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">唷！这很难看，但它让我们得到了我们需要的东西——损失函数相对于权重的导数(斜率)，这样我们就可以相应地调整权重。</p><p id="6d1c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在我们有了这个，让我们把反向传播函数添加到我们的 python 代码中。</p><figure class="ls lt lu lv gt iv"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="0455" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了更深入的了解微积分的应用和反向传播中的链式法则，我强烈推荐 3Blue1Brown 的这个教程。</p><figure class="ls lt lu lv gt iv"><div class="bz fp l di"><div class="om nu l"/></div></figure><h1 id="71b3" class="md me je bd mf mg mh mi mj mk ml mm mn kk mo kl mp kn mq ko mr kq ms kr mt mu bi translated">把所有的放在一起</h1><p id="7505" class="pw-post-body-paragraph kt ku je kv b kw mv kf ky kz mw ki lb lc mx le lf lg my li lj lk mz lm ln lo im bi translated">现在我们已经有了完整的 python 代码来进行前馈和反向传播，让我们将我们的神经网络应用于一个例子，看看它做得有多好。</p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/f71ae5124cc944c2d477cf7283e569d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*HaC4iILh2t0oOKi6S6FwtA.png"/></div></figure><p id="dd34" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们的神经网络应该学习一组理想的权重来表示这个函数。请注意，仅仅通过检查来计算重量对我们来说并不简单。</p><p id="d0c1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们训练神经网络 1500 次迭代，看看会发生什么。查看下面的每次迭代损失图，我们可以清楚地看到损失<strong class="kv jf">朝着最小值单调递减。</strong>这与我们之前讨论过的梯度下降算法是一致的。</p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/087eabda07c611e45f072d0f2646fb4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*fWNNA2YbsLSoA104K3Z3RA.png"/></div></figure><p id="7f8c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们看看神经网络经过 1500 次迭代后的最终预测(输出)。</p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/db1f06851f2a04e9176bdb59a481f57b.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*9oOlYhhOSdCUqUJ0dQ_KxA.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Predictions after 1500 training iterations</figcaption></figure><p id="783c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们做到了！我们的前馈和反向传播算法成功地训练了神经网络，并且预测收敛于真实值。</p><p id="0eb6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">请注意，预测值和实际值之间略有不同。这是可取的，因为它防止<strong class="kv jf">过度拟合</strong>，并允许神经网络<strong class="kv jf">更好地将</strong>推广到看不见的数据。</p><h1 id="923b" class="md me je bd mf mg mh mi mj mk ml mm mn kk mo kl mp kn mq ko mr kq ms kr mt mu bi translated">下一步是什么？</h1><p id="4b7e" class="pw-post-body-paragraph kt ku je kv b kw mv kf ky kz mw ki lb lc mx le lf lg my li lj lk mz lm ln lo im bi translated">幸运的是，我们的旅程还没有结束。关于神经网络和深度学习，还有很多东西需要学习。例如:</p><ul class=""><li id="62c5" class="na nb je kv b kw kx kz la lc nc lg nd lk ne lo nf ng nh ni bi translated">除了 Sigmoid 函数之外，我们还可以使用什么其他的<strong class="kv jf">激活函数</strong>？</li><li id="f888" class="na nb je kv b kw nj kz nk lc nl lg nm lk nn lo nf ng nh ni bi translated">训练神经网络时使用<strong class="kv jf">学习率</strong></li><li id="e302" class="na nb je kv b kw nj kz nk lc nl lg nm lk nn lo nf ng nh ni bi translated">使用<strong class="kv jf">卷积</strong>进行图像分类任务</li></ul><p id="dafb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我很快会写更多关于这些话题的文章，所以请在 Medium 上关注我，留意它们！</p><h1 id="e537" class="md me je bd mf mg mh mi mj mk ml mm mn kk mo kl mp kn mq ko mr kq ms kr mt mu bi translated">最后的想法</h1><p id="ce81" class="pw-post-body-paragraph kt ku je kv b kw mv kf ky kz mw ki lb lc mx le lf lg my li lj lk mz lm ln lo im bi translated">从零开始编写自己的神经网络，我确实学到了很多。</p><p id="6a38" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">尽管 TensorFlow 和 Keras 等深度学习库可以在不完全了解神经网络内部工作原理的情况下轻松构建深度网络，但我发现，对有抱负的数据科学家来说，更深入地了解神经网络是有益的。</p><p id="3dc7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这个练习花费了我大量的时间，我希望它对你也有用！</p></div></div>    
</body>
</html>