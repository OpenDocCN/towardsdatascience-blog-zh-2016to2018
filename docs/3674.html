<html>
<head>
<title>Multi-Stream RNN, Concat RNN, Internal Conv RNN, Lag 2 RNN in Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多流 RNN，串联 RNN，内部 Conv RNN，在 Tensorflow 中落后 2 RNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-stream-rnn-concat-rnn-internal-conv-rnn-lag-2-rnn-in-tensorflow-f4f17189a208?source=collection_archive---------8-----------------------#2018-06-06">https://towardsdatascience.com/multi-stream-rnn-concat-rnn-internal-conv-rnn-lag-2-rnn-in-tensorflow-f4f17189a208?source=collection_archive---------8-----------------------#2018-06-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d7194a55407d89103c308a3395baa0ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*M4eVfjIi7b3gNlv8ytTmyg.gif"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">GIF from this <a class="ae kc" href="https://giphy.com/stickers/parley-happy-4NkrKIHNO2fLtfxuWa" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="3cfe" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在过去的两周里，我一直渴望实现不同种类的<a class="ae kc" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络</a> (RNN)，最终我有时间实现它们。以下是我想尝试的不同 RNN 案例列表。</p><p id="7217" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lb">案例 a:香草递归神经网络<br/>案例 b:多流递归神经网络<br/>案例 c:级联递归神经网络<br/>案例 d:内部卷积递归神经网络<br/>案例 e:滞后 2 递归神经网络</em> </strong></p><blockquote class="lc ld le"><p id="97f3" class="kd ke lb kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated"><strong class="kf ir">请注意，所有这些模型只是为了好玩和表达我的创意。还有，我要在这篇文章中使用的基础代码来自我的旧文章“</strong><a class="ae kc" rel="noopener" target="_blank" href="/nips-2017-tensorflow-gated-recurrent-convolution-neural-network-for-ocr-part-1-with-47bb2a8a7ab3"><strong class="kf ir"><em class="iq">OCR 的门控递归卷积 NN</em></strong></a><strong class="kf ir">”。</strong></p></blockquote></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="53d3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">香草递归神经网络</strong></p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lp"><img src="../Images/d588b03ea4606e4f6e229bb3cc4f00d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TFgOo1tXu_fJ4yRI"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Image from this <a class="ae kc" href="https://machinelearning-blog.com/2018/02/21/recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="9b92" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总共有 5 个不同的 RNN 案例我想执行。然而，为了完全理解所有的实现，最好对普通 RNN 有一个很好的理解(案例 a 是普通 RNN，所以如果你理解案例 a 的代码，你就可以开始了。)</p><p id="46b3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果有人想回顾简单的 RNN，请访问我的旧博客文章“<a class="ae kc" href="https://medium.com/@SeoJaeDuk/only-numpy-vanilla-recurrent-neural-network-back-propagation-practice-math-956fbea32704" rel="noopener"> <em class="lb"> Only Numpy:香草递归神经网络通过时间实践推导反向传播</em> </a>”。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="7fe8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">案例 a:普通递归神经网络(结果)</strong></p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lu"><img src="../Images/ffb04bf834175af73746129f345b62d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jITnYDL6hDhW9m5c4aWHDg.png"/></div></div></figure><p id="8adc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">红框</strong> → 3 卷积层<br/>→T31】橙色 →全局平均池化和 SoftMax <br/> <strong class="kf ir">绿圈</strong> →时间为 0 的隐藏单元<br/> <strong class="kf ir">蓝圈</strong> →输入 4 个时间戳<br/> <strong class="kf ir">黑框</strong> →带 4 个时间戳的递归神经网络</p><p id="683d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上所述，基本网络是简单的 RNN 结合卷积神经网络用于分类。RNN 的时间戳为 4，这意味着我们将在每个时间戳向网络提供 4 种不同的输入。为此，我将在原始图像中添加一些噪声。</p><figure class="lq lr ls lt gt jr"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="e7df" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">蓝线</strong> →一段时间内的训练成本<br/> <strong class="kf ir">橙线</strong> →一段时间内的训练精度<br/> <strong class="kf ir">绿线</strong> →一段时间内的测试成本<br/> <strong class="kf ir">红线</strong> →一段时间内的测试精度</p><p id="2ddd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上所述，我们的基础网络已经运行良好。现在的问题是其他方法表现如何，它是否能够比我们的基础网络更好地正规化。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="9aaf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">案例 b:多流递归神经网络(想法/结果)</strong></p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lx"><img src="../Images/eab83f1a0b7921ba016c4bc27211e8fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s8anRV4pnpJEH2IAn_74OQ.png"/></div></div></figure><p id="f4c0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">红框</strong> → 3 卷积层<br/>T5】橙框 →全局平均池化和 SoftMax <br/> <strong class="kf ir">绿框</strong> →时间为 0 的隐藏单元<br/> <strong class="kf ir">蓝框</strong> →卷积输入流<br/> <strong class="kf ir">黄框</strong> →全连通网络流<br/> <strong class="kf ir">黑框</strong> →带 4 时间戳的递归神经网络</p><p id="846e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种 RNN 背后的想法只是为 RNN 提供不同的数据表示。在我们的基本网络中，我们有原始图像或添加了一些噪声的图像。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ly"><img src="../Images/9a575f56d37b6f1baabab0d55c6cb3b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CYLmhhgAyidhbM9ZylBVRQ.png"/></div></div></figure><p id="5ff7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">红框</strong> →增加四个 CNN/FNN 层来“处理”输入<br/> <strong class="kf ir">蓝框</strong> →在每个不同的时间戳创建输入</p><p id="cabb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如下图所示，我们的 RNN 使用[batch_size，26，26，1]将宽度和高度减少了 2 倍。我希望数据的不同表示可以作为一种正则化。(类似于数据扩充)</p><figure class="lq lr ls lt gt jr"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="6e95" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">蓝线</strong> →一段时间内的训练成本<br/> <strong class="kf ir">橙线</strong> →一段时间内的训练精度<br/> <strong class="kf ir">绿线</strong> →一段时间内的测试成本<br/> <strong class="kf ir">红线</strong> →一段时间内的测试精度</p><p id="e235" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如上面所看到的，网络做得很好，在测试图像上比我们的基础网络高出 1%。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="1bc9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">情况 c:级联递归神经网络(想法/结果)</strong></p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lz"><img src="../Images/6c6e1f945be62400abb6b74158595a87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XnZsmoao3gHaVDOzRVIs9g.png"/></div></div></figure><p id="ab2f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">红框</strong> → 3 卷积层<br/>→橙色 →全局平均池化和 SoftMax <br/> <strong class="kf ir">绿圈</strong> →时间为 0 的隐藏单元<br/> <strong class="kf ir">蓝圈</strong> →输入 4 个时间戳<br/>→黑框 →带 4 个时间戳的递归神经网络<br/> <strong class="kf ir">黑弯箭头</strong> →级联输入每个时间戳</p><p id="f116" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种方法非常简单，其思想是在每个时间戳上提取不同的特征，随着时间的推移，网络具有更多的特征可能是有用的。(对于循环层。)</p><figure class="lq lr ls lt gt jr"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="c634" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">蓝线</strong> →一段时间内的训练成本<br/> <strong class="kf ir">橙线</strong> →一段时间内的训练精度<br/> <strong class="kf ir">绿线</strong> →一段时间内的测试成本<br/> <strong class="kf ir">红线</strong> →一段时间内的测试精度</p><p id="bee3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可悲的是，这是一个巨大的失败。我猜空的隐藏值对网络的良好运行没有任何帮助。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="abdb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">案例 d:内部卷积递归神经网络(想法/结果)</strong></p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ma"><img src="../Images/8bec9d6a90df26044fa0aac4762f3248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G34qi6C4Co1xLGmeQUex-Q.png"/></div></div></figure><p id="0e37" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">红框</strong> → 3 卷积层<br/> <strong class="kf ir">橙框</strong> →全局平均池和 SoftMax <br/> <strong class="kf ir">绿圈</strong> →时间为 0 的隐藏单元<br/> <strong class="kf ir">蓝圈</strong> →输入 4 个时间戳<br/> <strong class="kf ir">黑框</strong> →带 4 个时间戳的递归神经网络<br/> <strong class="kf ir">灰色箭头</strong> →在传递到下一个时间戳之前执行内部卷积</p><p id="fde2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上所述，这个网络与我们的基础网络接收完全相同的输入。然而，这次我们将在数据的内部表示中执行额外的卷积运算。</p><div class="lq lr ls lt gt ab cb"><figure class="mb jr mc md me mf mg paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/7c09c0cc5812f62caa5d1de8c91d8593.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*CrcCAVCxxCNc-ONrJvQQCw.png"/></div></figure><figure class="mb jr mh md me mf mg paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/5ce1f6b7bfef76f4f668bcb660a2261f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*dZLwwNqwHz7OTQd2LISZWQ.png"/></div></figure></div><p id="f52c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">右图</strong> →声明 3 个新的卷积层<br/> <strong class="kf ir">左图(红框)</strong> →如果当前内层不为无，我们将进行额外的卷积操作。</p><p id="d335" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我实际上并没有在这个实现背后的理论原因，我只是想看看它是否工作 LOL。</p><figure class="lq lr ls lt gt jr"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="cd42" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">蓝线</strong> →一段时间内的训练成本<br/> <strong class="kf ir">橙线</strong> →一段时间内的训练精度<br/> <strong class="kf ir">绿线</strong> →一段时间内的测试成本<br/> <strong class="kf ir">红线</strong> →一段时间内的测试精度</p><p id="c87e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上所述，网络在融合方面做得很好，但是它无法超越我们的基础网络。(可悲)。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="8474" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">情况 e:滞后 2 递归神经网络(想法/结果)</strong></p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mi"><img src="../Images/3828a40b26b3e013e0e195f5f2c363b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K_oLZnJ8HboFuQKltWoOxw.png"/></div></div></figure><p id="a013" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">红框</strong> → 3 卷积层<br/>→T38】橙框 →全局平均池化和 SoftMax <br/> <strong class="kf ir">绿圈</strong> →时间为 0(或滞后 1)的隐藏单元<br/>→蓝圈 →输入 4 个时间戳<br/> <strong class="kf ir">黑框</strong> →带有 4 个时间戳的递归神经网络<br/> <strong class="kf ir">紫圈</strong> →隐藏状态滞后 2</p><p id="d8c0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在传统的 RNN 设置中，我们仅依靠最早的值来确定当前值。有一段时间我在想，我们没有理由把回看时间(或滞后)限制为 1。我们可以将这个想法扩展到滞后 3 或滞后 4 等。(为了简单起见，我采用了滞后 2)</p><figure class="lq lr ls lt gt jr"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="5e65" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">蓝线</strong> →一段时间内的训练成本<br/> <strong class="kf ir">橙线</strong> →一段时间内的训练精度<br/> <strong class="kf ir">绿线</strong> →一段时间内的测试成本<br/> <strong class="kf ir">红线</strong> →一段时间内的测试精度</p><p id="40c7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">谢天谢地，这个网络比基础网络做得更好。(但是具有非常小的余量)，然而这种类型的网络将最适合于时间序列数据。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="1f77" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">交互式代码/透明度</strong></p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/a9544515b1614f36b7c0290b6e43a7aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JQTUq6wBAvxs02Lxi4TRYg.png"/></div></div></figure><p id="986b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于 Google Colab，你需要一个 Google 帐户来查看代码，而且你不能在 Google Colab 中运行只读脚本，所以在你的操场上复制一份。最后，我永远不会请求允许访问你在 Google Drive 上的文件，仅供参考。编码快乐！同样为了透明，我在 github 上上传了所有的训练日志。</p><p id="88a1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要访问<a class="ae kc" href="https://colab.research.google.com/drive/1d4cKR1VxFsxyXAck1Mk-zQGvlVuWdOEw" rel="noopener ugc nofollow" target="_blank">案例的代码，请点击这里</a>，要查看<a class="ae kc" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/review_RNN/casea/casea.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击这里。</a> <br/>点击此处查看<a class="ae kc" href="https://colab.research.google.com/drive/1U4zthQ9CmcwAi_mWrKVuTPqf88R0lxZz" rel="noopener ugc nofollow" target="_blank">案例 b 的代码</a>，点击此处查看<a class="ae kc" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/review_RNN/caseb/caseb.txt" rel="noopener ugc nofollow" target="_blank">日志。</a> <br/>要访问<a class="ae kc" href="https://colab.research.google.com/drive/1oYvkITUp4WdfAx_xYw_Otg7dJr2rc2mz" rel="noopener ugc nofollow" target="_blank">案例 c 的代码，请点击此处</a>，要查看<a class="ae kc" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/review_RNN/casec/casec.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处。</a> <br/>要访问<a class="ae kc" href="https://colab.research.google.com/drive/1cQ48nzeBCGm5shW634TQmi9mYLIzP4JE" rel="noopener ugc nofollow" target="_blank">案例 c 的代码，请点击此处</a>，要查看<a class="ae kc" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/review_RNN/cased/cased.txt" rel="noopener ugc nofollow" target="_blank">日志，请点击此处。</a> <br/>点击此处查看<a class="ae kc" href="https://colab.research.google.com/drive/1ahrQMwLMhpqQLjO7AHSL3707JdnMH-VN" rel="noopener ugc nofollow" target="_blank">案例 c 的代码</a>，点击此处查看<a class="ae kc" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/NeuralNetwork/review_RNN/casee/casee.txt" rel="noopener ugc nofollow" target="_blank">日志。</a></p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="2670" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">最后的话</strong></p><p id="b033" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我想回顾 RNN 已经很久了，现在我终于可以这样做了。</p><p id="efdf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有的写作清单，请在这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同时，在我的 twitter 上关注我<a class="ae kc" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae kc" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae kc" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae kc" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文</a> t。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="fef8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">参考</strong></p><ol class=""><li id="37d5" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated">【NIPS 2017/Part 1】用于 OCR 的带交互码的门控递归卷积 NN【手动反推…(2018).走向数据科学。检索于 2018 年 6 月 6 日，来自<a class="ae kc" rel="noopener" target="_blank" href="/nips-2017-tensorflow-gated-recurrent-convolution-neural-network-for-ocr-part-1-with-47bb2a8a7ab3">https://towards data science . com/nips-2017-tensor flow-gated-recurrent-convolution-neural-network-for-ocr-part-1-with-47 bb 2 A8 a7 ab 3</a></li><li id="7af5" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">带 Tensorflow 的软签激活功能【带 TF 的手动背道具】。(2018).走向数据科学。检索于 2018 年 6 月 6 日，来自<a class="ae kc" rel="noopener" target="_blank" href="/soft-sign-activation-function-with-tensorflow-manual-back-prop-with-tf-5a04f3c8e9c1">https://towards data science . com/soft-sign-activation-function-with-tensor flow-manual-back-prop-with-TF-5a 04 F3 c8 e9 c 1</a></li><li id="8645" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">东格斯，V. (2018)。递归神经网络。machinelearning-blog.com。检索于 2018 年 6 月 6 日，来自<a class="ae kc" href="https://machinelearning-blog.com/2018/02/21/recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning-blog . com/2018/02/21/recurrent-neural-networks/</a></li><li id="c51b" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">唯一 Numpy:香草递归神经网络通过时间实践推导反向传播-部分…(2017).中等。检索于 2018 年 6 月 6 日，来自<a class="ae kc" href="https://medium.com/@SeoJaeDuk/only-numpy-vanilla-recurrent-neural-network-back-propagation-practice-math-956fbea32704" rel="noopener">https://medium . com/@ SeoJaeDuk/only-numpy-vanilla-recurrent-neural-network-back-propagation-practice-math-956 fbea 32704</a></li><li id="331e" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">递归神经网络。(2018).En.wikipedia.org。检索于 2018 年 6 月 6 日，来自<a class="ae kc" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Recurrent_neural_network</a></li></ol></div></div>    
</body>
</html>