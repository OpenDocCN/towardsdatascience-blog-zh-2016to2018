<html>
<head>
<title>Creating Art with Conv Neural Nets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Conv神经网络创作艺术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-art-with-conv-neural-nets-3ea5ebd7ef?source=collection_archive---------1-----------------------#2017-06-09">https://towardsdatascience.com/creating-art-with-conv-neural-nets-3ea5ebd7ef?source=collection_archive---------1-----------------------#2017-06-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="5246" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我正在使用卷积神经网络来制作一些精美的艺术品！</p><p id="760d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">迄今为止，艺术一直是最好留给创意者的想象作品。艺术家有一种独特的方式来表达自己和他们生活的时代，通过独特的镜头，具体到他们看待周围世界的方式。无论是达芬奇和他令人惊叹的作品，还是梵高和他扭曲的世界观，艺术总是激励着一代又一代的人。</p><p id="636d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">技术总是激励艺术家去突破界限，探索已经完成的事情之外的可能性。第一部电影摄影机不是作为辅助艺术的技术而发明的，而仅仅是捕捉现实的工具。显然，艺术家们对它的看法不同，它催生了整个电影和动画产业。对于我们创造的每一项主要技术来说都是如此，艺术家们总能找到创造性地使用这一新颖工具的方法。</p><p id="71fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着机器学习的最新进展，我们可以在几分钟内创作出令人难以置信的艺术作品，而这在大约一个世纪前可能需要专业艺术家数年才能完成。机器学习创造了一种可能性，即在让媒体与艺术家合作的同时，以至少100倍的速度制作艺术作品的原型。这里的美妙之处在于，这一新的技术进步浪潮将通过升级手边的工具来增强艺术的创作和观赏方式。</p><h1 id="7179" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">介绍</h1><p id="9889" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在这里，我将使用python来获取任何图像，并将其转换为我选择的任何艺术家的风格。谷歌在2015年发布了一款名为“深度梦想”的类似产品，互联网对它产生了强烈的热情。他们本质上训练了一个卷积神经网络，该网络对图像进行分类，然后使用一种优化技术来增强输入图像中的模式，而不是基于网络所学的自身权重。此后不久,“Deepart”网站出现了，它允许用户点击鼠标将任何图像转换成他们选择的绘画风格！</p><h1 id="6d61" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">那么这是如何工作的呢？</h1><p id="c438" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">为了理解这个被称为样式转换过程的“魔法”是如何工作的，我们将使用TensorFlow后端在Keras中编写我们自己的脚本。我将使用一个基础图像(我最喜欢的动物的照片)和一个风格参考图像。我的剧本将使用文森特·梵高的《星夜》作为参考，并将其应用于基础图像。这里，我们首先导入必要的依赖项:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="adbb" class="lx km iq lt b gy ly lz l ma mb"><strong class="lt ir">from</strong> <strong class="lt ir">__future__</strong> <strong class="lt ir">import</strong> print_function<br/><br/><strong class="lt ir">import</strong> <strong class="lt ir">time</strong><br/><strong class="lt ir">from</strong> <strong class="lt ir">PIL</strong> <strong class="lt ir">import</strong> Image<br/><strong class="lt ir">import</strong> <strong class="lt ir">numpy</strong> <strong class="lt ir">as</strong> <strong class="lt ir">np</strong><br/><br/><strong class="lt ir">from</strong> <strong class="lt ir">keras</strong> <strong class="lt ir">import</strong> backend<br/><strong class="lt ir">from</strong> <strong class="lt ir">keras.models</strong> <strong class="lt ir">import</strong> Model<br/><strong class="lt ir">from</strong> <strong class="lt ir">keras.applications.vgg16</strong> <strong class="lt ir">import</strong> VGG16<br/><br/><strong class="lt ir">from</strong> <strong class="lt ir">scipy.optimize</strong> <strong class="lt ir">import</strong> fmin_l_bfgs_b<br/><strong class="lt ir">from</strong> <strong class="lt ir">scipy.misc</strong> <strong class="lt ir">import</strong> imsave</span></pre><p id="c86a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以我们将这些图像输入神经网络，首先将它们转换成所有神经网络的实际格式，张量。Keras backend Tensorflow中的变量函数相当于tf.variable。它的参数将是转换为数组的图像，然后我们对样式图像做同样的事情。然后，我们创建一个组合图像，通过使用占位符将它初始化为给定的宽度和高度，可以在以后存储我们的最终结果。</p><p id="ee5a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是内容图片:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="4726" class="lx km iq lt b gy ly lz l ma mb">height = 512<br/>width = 512<br/><br/>content_image_path = 'images/elephant.jpg'<br/>content_image = Image.open(content_image_path)<br/>content_image = content_image.resize((height, width))<br/>content_image</span></pre><figure class="lo lp lq lr gt md gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/92c2d8b9ee039d42ca16d81f6524c55c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*_mNjKK0Fa9-2wAPHAfeP6w.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Content Image (Elephants are cool)</figcaption></figure><p id="3748" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，我加载了样式图像:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="6fe5" class="lx km iq lt b gy ly lz l ma mb">style_image_path = '/Users/vivek/Desktop/VanGogh.jpg'<br/>style_image = Image.open (style_image_path)<br/>style_image = style_image.resize((height, width))<br/>style_image</span></pre><figure class="lo lp lq lr gt md gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/7ff0fe23d806818c786ef15ff590e18f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*2fvoxTeBLmbf9qrwlei40w.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Style Image (gotta pull out the classic)</figcaption></figure><p id="7c45" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们转换这两个图像，使它们具有适合数字处理的形式。我们添加了另一个维度(除了高度、宽度和正常的3个维度之外),这样我们以后可以将两个图像的表示连接成一个公共的数据结构:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="9bca" class="lx km iq lt b gy ly lz l ma mb">content_array = np.asarray(content_image, dtype='float32')<br/>content_array = np.expand_dims(content_array, axis=0)<br/>print(content_array.shape)<br/><br/>style_array = np.asarray(style_image, dtype='float32')<br/>style_array = np.expand_dims(style_array, axis=0)<br/>print(style_array.shape)</span></pre><p id="fe01" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将继续使用VGG网络。Keras已经很好地包装了这个模型，以便我们在前进的过程中可以轻松使用。VGG16是一个16层卷积网，由牛津大学的视觉几何小组创建，在2014年赢得了ImageNet竞赛。这里的想法是，对于成千上万不同图像的图像分类，预先训练的CNN已经知道如何在容器图像中编码信息。我已经了解了每一层的特征，这些特征可以检测某些一般化的特征。这些是我们将用来执行风格转移的功能。我们不需要这个网络顶部的卷积块，因为它的全连接层和softmax函数通过挤压维度特征图和输出概率来帮助分类图像。我们不仅仅是对转移进行分类。这本质上是一种优化，我们有一些损失函数来衡量我们将试图最小化的误差值。在这种情况下，我们的损失函数可以分解为两部分:</p><p id="994d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1)内容损失我们将总损失初始化为零，并将这些中的每一个加到其上。首先是内容损失。图像总是有一个内容组件和一个样式组件。我们知道CNN学习的特征是按照越来越抽象的成分的顺序排列的。由于更高层的特征更抽象，比如检测人脸，我们可以把它们和内容联系起来。当我们通过网络运行我们的输出图像和我们的参考图像时，我们从我们选择的隐藏层获得两者的一组特征表示。然后我们测量它们之间的欧几里德距离来计算我们的损失。</p><p id="67a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2)风格损失这也是我们的网络的隐藏层输出的函数，但是稍微复杂一些。我们仍然通过网络传递两幅图像来观察它们的激活，但是我们没有直接比较原始激活的内容，而是增加了一个额外的步骤来测量激活之间的相关性。对于网络中给定层的两个活化图像，我们采用所谓的格拉姆矩阵。这将测量哪些功能倾向于一起激活。这基本上代表了不同特征在图像的不同部分同时出现的概率。一旦有了这些，我们就可以将这种风格损失定义为参考图像和输出图像之间的gram矩阵之间的欧几里德距离，并将总风格损失计算为我们选择的每一层的风格损失的加权和。</p><p id="c171" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">既然我们有了损失，我们需要定义输出图像相对于损失的梯度，然后使用这些梯度迭代地最小化损失。</p><p id="da7c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在需要对输入数据进行处理，以匹配在<a class="ae mk" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank"> Simonyan和Zisserman (2015) </a>介绍VGG网络模型的论文中所做的工作。</p><p id="8d3f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为此，我们需要执行两个转换:</p><ol class=""><li id="7276" class="ml mm iq jp b jq jr ju jv jy mn kc mo kg mp kk mq mr ms mt bi translated">从每个像素中减去平均RGB值(之前在<a class="ae mk" href="http://image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet训练集</a>上计算的，并且很容易从谷歌搜索中获得)。</li><li id="25a3" class="ml mm iq jp b jq mu ju mv jy mw kc mx kg my kk mq mr ms mt bi translated">将多维数组的排序从<em class="mz"> RGB </em>翻转到<em class="mz"> BGR </em>(文中使用的排序)。</li></ol><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="3d0b" class="lx km iq lt b gy ly lz l ma mb">content_array[:,:,:,0] -= 103.99<br/>content_array[:, :, :, 1] -= 116.779<br/>content_array[:, :, :, 2] -= 123.68<br/>content_array = content_array[:, :, :, ::-1]<br/><br/>style_array[:, :, :, 0] -= 103.939<br/>style_array[:, :, :, 1] -= 116.779<br/>style_array[:, :, :, 2] -= 123.68<br/>style_array = style_array[:, :, :, ::-1]</span></pre><p id="76ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们准备使用这些数组来定义Keras后端(TensorFlow图)中的变量。我们还引入了一个占位符变量来存储<em class="mz">组合</em>图像，它保留了内容图像的内容，同时合并了样式图像的样式。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="726b" class="lx km iq lt b gy ly lz l ma mb">content_image = backend.variable(content_array)<br/>style_image = backend.variable(style_array)<br/>combination_image = backend.placeholder((1, height, width, 3))</span></pre><p id="9f49" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们将所有这些图像数据连接成一个张量，用于处理Kera的VGG16模型。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="3791" class="lx km iq lt b gy ly lz l ma mb">input_tensor = backend.concatenate([content_image,<br/>                                    style_image,<br/>                                    combination_image], axis = 0)</span></pre><p id="b2d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如前所述，由于我们对分类问题不感兴趣，我们不需要完全连接的层或最终的softmax分类器。我们只需要下表中用绿色标记的模型部分。</p><figure class="lo lp lq lr gt md gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/885d1be5c8b73ecaaa1a222d114576a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9C-awiSEO6teZhMZYxrBUw.png"/></div></div></figure><p id="c8d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对我们来说，访问这个截断的模型是微不足道的，因为Keras附带了一组预训练的模型，包括我们感兴趣的VGG16模型。注意，通过在下面的代码中设置<code class="fe nf ng nh lt b">include_top=False</code>，我们不包括任何完全连接的层。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="ae10" class="lx km iq lt b gy ly lz l ma mb"><strong class="lt ir">import</strong> <strong class="lt ir">h5py</strong><br/>model = VGG16(input_tensor=input_tensor, weights='imagenet',<br/>              include_top=<strong class="lt ir">False</strong>)</span></pre><p id="6645" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从上表可以清楚地看出，我们正在使用的模型有很多层。Keras对这些层有自己的名字。让我们把这些名字列一个表，这样我们以后可以很容易地引用各个层。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="cc4a" class="lx km iq lt b gy ly lz l ma mb">layers = dict([(layer.name, layer.output) <strong class="lt ir">for</strong> layer <strong class="lt ir">in</strong> model.layers])<br/>layers</span></pre><p id="4d05" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在选择重量，可以这样做:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="3228" class="lx km iq lt b gy ly lz l ma mb">content_weight = 0.025<br/>style_weight = 5.0<br/>total_variation_weight = 1.0</span></pre><p id="1930" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在将使用由我们的模型的特定层提供的特征空间来定义这三个损失函数。我们首先将总损耗初始化为0，然后分阶段增加。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="18dd" class="lx km iq lt b gy ly lz l ma mb">loss = backend.variable(0.)</span></pre><p id="63b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在内容丢失:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="31a8" class="lx km iq lt b gy ly lz l ma mb"><strong class="lt ir">def</strong> content_loss(content, combination):<br/>    <strong class="lt ir">return</strong> backend.sum(backend.square(combination - content))<br/><br/>layer_features = layers['block2_conv2']<br/>content_image_features = layer_features[0, :, :, :]<br/>combination_features = layer_features[2, :, :, :]<br/><br/>loss += content_weight * content_loss(content_image_features,<br/>                                      combination_features)</span></pre><p id="32e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">和风格损失:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="38c1" class="lx km iq lt b gy ly lz l ma mb"><strong class="lt ir">def</strong> gram_matrix(x):<br/>    features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))<br/>    gram = backend.dot(features, backend.transpose(features))<br/>    <strong class="lt ir">return</strong> gram</span><span id="192d" class="lx km iq lt b gy ni lz l ma mb"><strong class="lt ir">def</strong> style_loss(style, combination):<br/>    S = gram_matrix(style)<br/>    C = gram_matrix(combination)<br/>    channels = 3<br/>    size = height * width<br/>    <strong class="lt ir">return</strong> backend.sum(backend.square(S - C)) / (4. * (channels ** 2) * (size ** 2))<br/><br/>feature_layers = ['block1_conv2', 'block2_conv2',<br/>                  'block3_conv3', 'block4_conv3',<br/>                  'block5_conv3']<br/><strong class="lt ir">for</strong> layer_name <strong class="lt ir">in</strong> feature_layers:<br/>    layer_features = layers[layer_name]<br/>    style_features = layer_features[1, :, :, :]<br/>    combination_features = layer_features[2, :, :, :]<br/>    sl = style_loss(style_features, combination_features)<br/>    loss += (style_weight / len(feature_layers)) * sl</span></pre><p id="b679" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后是总变异损失:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="2db2" class="lx km iq lt b gy ly lz l ma mb"><strong class="lt ir">def</strong> total_variation_loss(x):<br/>    a = backend.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])<br/>    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])<br/>    <strong class="lt ir">return</strong> backend.sum(backend.pow(a + b, 1.25))<br/><br/>loss += total_variation_weight * total_variation_loss(combination_image)</span></pre><p id="7751" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们继续定义解决优化问题所需的梯度:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="3c06" class="lx km iq lt b gy ly lz l ma mb">grads = backend.gradients(loss, combination_image)</span></pre><p id="73a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我们引入一个赋值器类，它在一次运算中计算损失和梯度，同时通过两个独立的函数loss和grads检索它们。这样做是因为scipy.optimize需要单独的损失和梯度函数，但是单独计算它们是低效的。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="58e4" class="lx km iq lt b gy ly lz l ma mb">outputs = [loss]<br/>outputs += grads<br/>f_outputs = backend.function([combination_image], outputs)<br/><br/><strong class="lt ir">def</strong> eval_loss_and_grads(x):<br/>    x = x.reshape((1, height, width, 3))<br/>    outs = f_outputs([x])<br/>    loss_value = outs[0]<br/>    grad_values = outs[1].flatten().astype('float64')<br/>    <strong class="lt ir">return</strong> loss_value, grad_values<br/><br/><strong class="lt ir">class</strong> <strong class="lt ir">Evaluator</strong>(object):<br/><br/>    <strong class="lt ir">def</strong> __init__(self):<br/>        self.loss_value = <strong class="lt ir">None</strong><br/>        self.grads_values = <strong class="lt ir">None</strong><br/><br/>    <strong class="lt ir">def</strong> loss(self, x):<br/>        <strong class="lt ir">assert</strong> self.loss_value <strong class="lt ir">is</strong> <strong class="lt ir">None</strong><br/>        loss_value, grad_values = eval_loss_and_grads(x)<br/>        self.loss_value = loss_value<br/>        self.grad_values = grad_values<br/>        <strong class="lt ir">return</strong> self.loss_value<br/><br/>    <strong class="lt ir">def</strong> grads(self, x):<br/>        <strong class="lt ir">assert</strong> self.loss_value <strong class="lt ir">is</strong> <strong class="lt ir">not</strong> <strong class="lt ir">None</strong><br/>        grad_values = np.copy(self.grad_values)<br/>        self.loss_value = <strong class="lt ir">None</strong><br/>        self.grad_values = <strong class="lt ir">None</strong><br/>        <strong class="lt ir">return</strong> grad_values<br/><br/>evaluator = Evaluator()</span></pre><p id="c7b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们终于准备好解决我们的优化问题。这个组合图像从(有效)像素的随机集合开始，我们使用L-BFGS算法(一种比标准梯度下降收敛快得多的拟牛顿算法)对其进行迭代改进。我们在8次迭代后停止，因为输出对我来说看起来很好，损失停止显著减少。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="fc29" class="lx km iq lt b gy ly lz l ma mb">x = np.random.uniform(0, 255, (1, height, width, 3)) - 128.<br/><br/>iterations = 8<br/><br/><strong class="lt ir">for</strong> i <strong class="lt ir">in</strong> range(iterations):<br/>    print('Start of iteration', i)<br/>    start_time = time.time()<br/>    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),<br/>                                     fprime=evaluator.grads, maxfun=20)<br/>    print('Current loss value:', min_val)<br/>    end_time = time.time()<br/>    print('Iteration <strong class="lt ir">%d</strong> completed in <strong class="lt ir">%d</strong>s' % (i, end_time - start_time))</span></pre><p id="1ecf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你像我一样在笔记本电脑上工作，那就去吃顿大餐吧，因为这需要一段时间。这是上一次迭代的输出！</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="783e" class="lx km iq lt b gy ly lz l ma mb">x = x.reshape((height, width, 3))<br/>x = x[:, :, ::-1]<br/>x[:, :, 0] += 103.939<br/>x[:, :, 1] += 116.779<br/>x[:, :, 2] += 123.68<br/>x = np.clip(x, 0, 255).astype('uint8')<br/><br/>Image.fromarray(x)</span></pre><figure class="lo lp lq lr gt md gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/4e9c99c8b01b17304a0883d6030d8880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*dnxZzVzzv2R30_s3YgYFBw.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Merger!</figcaption></figure><p id="d55c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">整洁！我们可以通过改变两幅图像、它们的大小、我们的损失函数的权重等等来继续玩这个游戏。重要的是要记住，我的MacBook air仅运行了8次迭代就花了大约4个小时。这是一个非常CPU密集型的过程，因此在扩展时，这是一个相对昂贵的问题。</p><p id="a786" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>