<html>
<head>
<title>Unsupervised deep learning for data interpolation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于数据插值的无监督深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unsupervised-learning-for-data-interpolation-e259cf5dc957?source=collection_archive---------9-----------------------#2018-10-06">https://towardsdatascience.com/unsupervised-learning-for-data-interpolation-e259cf5dc957?source=collection_archive---------9-----------------------#2018-10-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6c0c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用前馈自动编码器的数据去噪</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/319a3f4e5af47fc6cc80329cc12ffca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x5A6PODffPO1RU_sl1f1wg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="6556" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">真实世界的数据是嘈杂的。噪声可能以不正确或缺失值的形式出现。本文描述了用自动编码器填充缺失值的方法。自动编码器在没有参考值可用于缺失条目的噪声数据上被训练。该程序是对图像重建的任务进行解释。该方法过去已成功应用于地震数据重建。Tensorflow 实现可用<a class="ae lr" href="https://github.com/mikhailiuk/image_reconstruction" rel="noopener ugc nofollow" target="_blank">这里的<strong class="kx ir"/></a><strong class="kx ir">。</strong></p><h1 id="3605" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">问题概述</strong></h1><p id="e50f" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">想象一下这样一种情况:数据丰富，但是数据有噪声，并且无法访问参考。一个很好的例子是图像，如图 1 所示。这里，图片中的黑点是噪声——图像值设置为 0，我们假设没有它们可用的地面真实数据。数据类型不限于图像，可以是任何东西，例如，丢失部分数据的患者记录。</p><div class="kg kh ki kj gt ab cb"><figure class="mp kk mq mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/70261f99517e38868aa4e3f77c99d821.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*dhhGCiVdeYsEHzs55oQEfg.png"/></div></figure><figure class="mp kk mq mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/813e3de4d4b774eaabb6aeadfae200f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ZewDC8cC3fgbRiQWOSnRDA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk mv di mw mx">Figure 1: Example of corrupted data. In the given example images have 0’s in places where pixels are missing. Images are taken from the <a class="ae lr" href="http://www.imageprocessingplace.com/root_files_V3/image_databases.htm" rel="noopener ugc nofollow" target="_blank">Standard Test image dataset</a>.</figcaption></figure></div><p id="c23a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">理想情况下，如果具有参考的训练数据可用，我们可以通过将重构与目标进行比较来训练网络重构缺失值。但是在没有目标数据的情况下，这是不可能的，因为将重构与 0(即缺失值)进行比较是没有意义的。在这里，我讨论了一种在没有参考资料的情况下训练自动编码器完成数据重建任务的方法。</p><h1 id="d248" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">理论概述</h1><p id="93a0" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">这一节简要回顾了神经网络和自动编码器。</p><h2 id="cf89" class="my lt iq bd lu mz na dn ly nb nc dp mc le nd ne me li nf ng mg lm nh ni mi nj bi translated">神经网络</h2><p id="9dba" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">简单的前馈人工神经网络(ANN)有几层。第一层是输入层，输入数据。接下来是一些隐藏层。最后是输出层。层由基本单元组成— <em class="nk">神经元</em>。一层中的神经元与下一层中的神经元相连。在训练过程中优化了两种参数类型:<em class="nk">权重</em>，对应于每条连接神经元的边，以及一个<em class="nk">偏差</em>关联于每一个神经元。因此，神经元的输入是前一层神经元的加权输出和偏差的线性组合。通过<em class="nk">激活函数，例如 sigmoid </em>，将神经元的输入映射到一个新的空间，从而获得神经元的输出。回归的任务可以与特征学习的任务相关。在人工神经网络中，一个特征是隐层中单个神经元的输入连接。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/c10626abfd460e86c0f80304f6ffd65c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMlFvrjt3J3rhPtWo9-vqw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 2: A simple feed forward neural network. Image by Author.</figcaption></figure><p id="23a4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">训练是一个迭代过程，分为两个阶段:前馈和反向传播。前馈是将数据输入到输入中，并通过人工神经网络向前传播，从而产生输出的过程。对于每一层，该过程可以描述为:</p><p id="b67f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">y</strong>=<strong class="kx ir">T3】sT5】(<strong class="kx ir">Wx</strong>+<strong class="kx ir">b</strong>)，</strong></p><p id="b3f4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中<strong class="kx ir"> x </strong>是当前层的输入向量，<strong class="kx ir"> y </strong>是当前层的输出，<strong class="kx ir"> W </strong>是权重矩阵，其中<strong class="kx ir">W</strong><em class="nk">ki</em>=<strong class="kx ir">W</strong><em class="nk">ki</em>和<strong class="kx ir"> w </strong> <em class="nk"> ki </em>是<strong class="kx ir"> x </strong> <em class="nk"> i </em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/204823a65d49be1b83e1e8f237384829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TS82jsvgAwQxF9nHYHzhNg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 3: A single neurone. Image by Author.</figcaption></figure><p id="7825" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在前馈通过之后，需要评估目标函数，并且需要估计使其最小化的参数。一种常用的优化方法是<a class="ae lr" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank"> <em class="nk">随机梯度下降</em> </a> (SGD)或<a class="ae lr" href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer" rel="noopener ugc nofollow" target="_blank"> <em class="nk">亚当</em> </a>优化器。</p><h2 id="69e1" class="my lt iq bd lu mz na dn ly nb nc dp mc le nd ne me li nf ng mg lm nh ni mi nj bi translated">自动编码器</h2><p id="fac7" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">这项工作考虑了基于自动编码器的人工神经网络。这种选择是合理的，因为它旨在消除任何关于数据的先验假设。简单的<em class="nk">自动编码器</em>只有一个隐藏层，其单元数少于输入数。深度自动编码器由具有非线性激活函数的多个隐藏层形成。这种结构允许在人工神经网络的更深层学习抽象特征和复杂关系。</p><p id="cf34" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一个自动编码器被拆分成一个<em class="nk">编码器- </em> <strong class="kx ir"> <em class="nk"> f </em> </strong>和一个<em class="nk">解码器</em> <strong class="kx ir"> <em class="nk"> g </em> </strong>。编码器的目的是将 D 维输入<strong class="kx ir"> x </strong>映射到通常更低的 K 维特征空间，即<strong class="kx ir"><em class="nk">f</em></strong>(<strong class="kx ir">x</strong>)=<strong class="kx ir">h</strong>。解码器旨在将特征空间映射回原始空间，从而重构输入，即<strong class="kx ir"><em class="nk">g</em></strong>(<strong class="kx ir">h</strong>)=<strong class="kx ir"><em class="nk">g</em></strong>(<strong class="kx ir"><em class="nk">f</em></strong>(<strong class="kx ir">x</strong>)=<strong class="kx ir"><em class="nk">r</em></strong>(<strong class="kx ir">x</strong>)~<strong class="kx ir">x</strong></p><h1 id="98bb" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">缺少值的培训</h1><p id="22e2" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">因为没有给出信号丢失部分的参考值，所以训练神经网络仅重建信号的未被破坏部分。被破坏的数据被输入到神经网络中，然而，仅用于未被破坏的数据点的重建的误差被评估并用于反向传播。该过程基于这样的假设，即面片具有不同的噪声分布。如果在当前迭代的反向传播中没有使用信号值，则输入到神经网络中的后续面片将丢失不同位置的值。因此，在当前迭代中未更新的权重在下一次迭代中被更新。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/07227071125bac62ff68502d8000b329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IvtQL9bbx2W6j_2wYpwAMQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 5: Training with missing data. Edges marked with red colour are not updated in the given iteration. Image by Author.</figcaption></figure><p id="0bba" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于在基于给定值的训练期间监控重建的良好性是有偏差的，所以对数据应用附加的二进制验证掩码。验证掩码包含 2%的噪声，并被投影到数据上。我确保数据中缺失的值和验证掩码中值为 0 的值不会重叠。在训练期间使用被验证掩码破坏的点。在每个时期之后，使用参考值计算被验证掩码破坏的值的误差。每当验证误差达到局部最小值时，保存网络的当前配置，并继续训练。</p><h2 id="26b3" class="my lt iq bd lu mz na dn ly nb nc dp mc le nd ne me li nf ng mg lm nh ni mi nj bi translated">算法</h2><p id="d5ac" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">该算法包括七个阶段(图 2)。首先，对数据进行预处理——在 0 和 1 之间进行缩放，并分割成小块。下一步，补丁通过人工神经网络。然后计算干净数据的误差，并更新参数。在下一阶段，使用验证掩码计算验证误差。如果 ANN 达到了局部最小值，则保存网络的当前状态。如果 ANN 不在其最小值，则跳过这一步。在下一步中，超参数被更新并且数据被混洗。如果满足停止条件，即达到最大历元，则获得缺失数据的重建，否则继续训练。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/298e6bd675ae3e3ff09cc1b5a789b30a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vldnx-P2hPdZ9BuaOsu8tg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 6: Algorithm stages. Image by Author.</figcaption></figure><h1 id="674b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结果</h1><p id="2b46" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">具有低频分量的图像比具有高频分量的图像重建得更好，即更复杂的场景更难重建。当使用更大的片时，即网络具有更多信息时，重构更好。最差的重建是在碎片的边缘和角落，那里的像素有五个或三个邻居，而在碎片的中心，像素被八个邻居包围。一种可能的解决方案是使用更大尺寸的补片，并且仅使用中心部分进行重建。这项工作可以通过使用 CNN 处理图像来改进，并在 RGB 图像上进行测试和应用于其他数据类型。</p><div class="kg kh ki kj gt ab cb"><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/4e9dcb74728db59f3709992a674f7470.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*YshiDP8ImjanR1hGm2Krvw.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/8ed1df5e4339e1c4dedea9ab4544b526.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*amwN_2fq2CSAevBUL_i9kQ.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/3962de4f8f25b6cbd6008c0855050b69.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*JU1bOXK45SuFTAivKFYZEw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk nq di nr mx">House: 30% missing(left) reconstructed (center) reference (right). 16x16 patches, 8 pix shift, averaged. Images are taken from the <a class="ae lr" href="http://www.imageprocessingplace.com/root_files_V3/image_databases.htm" rel="noopener ugc nofollow" target="_blank">Standard Test image dataset</a>.</figcaption></figure></div><div class="ab cb"><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/26963556168667b56c077c64e1b09566.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*eJkrHTJR_zsQRa3G6lWLrg.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/761085fa148fb2219052af49ffb1535b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*jraiDW1u38ReGyEOdT39pg.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/36f1ca566f4707d53bde4572e46f643d.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*pCa6yBy_c7qUBttzpK98ow.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk nq di nr mx">Room: 50% missing(left) reconstructed (center) reference (right). 32x32 patches, 16pix shift, averaged. Images are taken from the <a class="ae lr" href="http://www.imageprocessingplace.com/root_files_V3/image_databases.htm" rel="noopener ugc nofollow" target="_blank">Standard Test image dataset</a>.</figcaption></figure></div><div class="ab cb"><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/a6ea396dcbb1b12fd6bbabc17a214149.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*A2IcxffBhSD5jCVePlg01Q.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/4d4caf64b821507a8ed3f9f422a3220e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*b154WxfqUpVBRCALJU2jDA.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/0413708bca4ecc026a3e773b8a7bfb85.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*wuUg9Bih7-G5A2k255sOoA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk nq di nr mx">Lena: 50% missing(left) reconstructed (center) reference (right). 32x32 patches, 4pix shift, averaged. Images are taken from the <a class="ae lr" href="http://www.imageprocessingplace.com/root_files_V3/image_databases.htm" rel="noopener ugc nofollow" target="_blank">Standard Test image dataset</a>.</figcaption></figure></div><div class="ab cb"><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/425b5f460d2c5ff6a6e82e8dc656a8b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*pWTLRMfwgPPHnskY_3M3aA.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/0c62a965dbd0feaa5100fefa32a6edbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*a7X1Bvd1yMP4zHsPe0-cqA.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/789e635da04c990abad1e9a5256f6db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*AeUOibqLcqxPvtG4XCaJxw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk nq di nr mx">Baboon: 50% missing(left) reconstructed (center) reference (right). 32x32 patches, 4pix shift, averaged. Images are taken from the <a class="ae lr" href="http://www.imageprocessingplace.com/root_files_V3/image_databases.htm" rel="noopener ugc nofollow" target="_blank">Standard Test image dataset</a>.</figcaption></figure></div><div class="ab cb"><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/87d7c8d6552b46cd9cdb5b3377a9fec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*oTqQrMP4lu5HG3EqTQ7yyg.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/d97a9e8336fbed9942e81caaa8af62d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*T4rcjO8BDPWRW_qF3IUp1A.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/0906ec36e8bd5397d6ecd23caa0e81e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*R6Jt1xc26i-ghc6xlHewsQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk nq di nr mx">Peppers: 70% missing(left) reconstructed (center) reference (right). 32x32 patches, 4pix shift, averaged. Images are taken from the <a class="ae lr" href="http://www.imageprocessingplace.com/root_files_V3/image_databases.htm" rel="noopener ugc nofollow" target="_blank">Standard Test image dataset</a>.</figcaption></figure></div><div class="ab cb"><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/b8a47efcff854fb412e9294f6220b733.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*8a-pGRYtAV028_zs5l2fKw.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/10e47b6e23bd6dbce8091a6686ebea14.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*fZFvTeRWCwk-s0Jg3MrQVQ.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/9d39b4255b8d0f5c63b44ca650ad53b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*w48RDxDItxfndbISfkVCvg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk nq di nr mx">Lena: 90% missing(left) reconstructed (center) reference (right). 32x32 patches, 2pix shift, averaged. Images are taken from the <a class="ae lr" href="http://www.imageprocessingplace.com/root_files_V3/image_databases.htm" rel="noopener ugc nofollow" target="_blank">Standard Test image dataset</a>.</figcaption></figure></div><div class="ab cb"><figure class="mp kk ns mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/0cc4af7ea472fc85331792089e941782.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*wuUg9Bih7-G5A2k255sOoA.png"/></div></figure><figure class="mp kk ns mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/ec839ce35428db59be78c54b812ad156.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*b154WxfqUpVBRCALJU2jDA.png"/></div></figure><figure class="mp kk nt mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/6fbebfd86a37fb5afbbf597d3437d3a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*iYngWIMCDkZQEqRbK3xTZw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk nu di nv mx">Lena 50% missing: reference (left) reconstructed (center) RMSE heatmap (right). 32x32 patches, 4pix shift, averaged. Images are taken from the <a class="ae lr" href="http://www.imageprocessingplace.com/root_files_V3/image_databases.htm" rel="noopener ugc nofollow" target="_blank">Standard Test image dataset</a>. The error map is generated by the author</figcaption></figure></div><h2 id="b000" class="my lt iq bd lu mz na dn ly nb nc dp mc le nd ne me li nf ng mg lm nh ni mi nj bi translated"><strong class="ak">看不见的数据</strong></h2><p id="07ab" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">有趣的是，网络是否能够泛化，即在一幅图像上训练并重建另一幅图像。正如所料，当重建的图像与用于训练的图像相同时，重建会更好。下面是两个例子:网络训练狒狒和重建辣椒和训练辣椒和重建狒狒。</p><div class="kg kh ki kj gt ab cb"><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/c53ec915786d234fda91bc1e10a29497.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*MxuCnW0oUkjJ3L9NHRhqJg.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/47a033e92a6295554e0e4abb0dfbca80.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*wsCfnX1i3H5wt-1NQiDaGQ.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/3cf46a13db81b6f4e5cf63a21af1f12e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*t7U_P53Ds6rFsLfUwyGAoQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk nq di nr mx">Peppers trained on baboon: 50% missing(left) reconstructed (center) reference (right). 32x32 patches, 4pix shift, averaged. Images are taken from the <a class="ae lr" href="http://www.imageprocessingplace.com/root_files_V3/image_databases.htm" rel="noopener ugc nofollow" target="_blank">Standard Test image dataset</a>.</figcaption></figure></div><div class="ab cb"><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/ff3cb7bfbc3e9de30c47d9d997446582.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*vbOTDe9_V6XLJXDkgD-tEg.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/a10d6aff9d40ce5859ba63ea1e56770c.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*IHERfqBI7BjeEGqlRq_LwA.png"/></div></figure><figure class="mp kk np mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/8f0c299532d5347a96d683e812e65301.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*8GT8iSSqoulnWSmcGKRT6g.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk nq di nr mx">Baboon trained on peppers: 50% missing(left) reconstructed (center) reference (right). 32x32 patches, 4pix shift, averaged. Images are taken from the <a class="ae lr" href="http://www.imageprocessingplace.com/root_files_V3/image_databases.htm" rel="noopener ugc nofollow" target="_blank">Standard Test image dataset</a>.</figcaption></figure></div></div></div>    
</body>
</html>