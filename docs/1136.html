<html>
<head>
<title>Support Vector Machines — A Brief Overview</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机——概述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machines-a-brief-overview-37e018ae310f?source=collection_archive---------8-----------------------#2017-08-02">https://towardsdatascience.com/support-vector-machines-a-brief-overview-37e018ae310f?source=collection_archive---------8-----------------------#2017-08-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="0039" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有多种方法可以用机器学习对数据进行分类。你可以运行一个逻辑回归，使用决策树，或建立一个神经网络来完成这项任务。1963年，Vladimir Vapnik和Alexey Chervonenkis开发了另一种分类工具，即支持向量机。Vapnik在20世纪90年代完善了这种分类方法，并扩展了支持向量机的用途。支持向量机已经成为数据科学家的一个很好的工具。</p><p id="215c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇博文中，我计划提供一个关于支持向量机的高层次概述。我将谈论支持向量机背后的基本理论，为什么它们是相关的，以及这种分类器的优点和缺点。我还向您展示了一个用Python实现SVM的快速示例。我还提供了一个资源列表，这些资源有助于理解支持向量机。如果你想理解数学，我鼓励你查阅更多的支持向量机深度指南。这些信息的大部分是从Tibshirani的<em class="kl">统计学习导论中提炼出来的。</em></p><p id="0721" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">论</strong>:</p><p id="dd02" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">支持向量机试图通过数据集传递一个线性可分的超平面，以便将数据分为两组。这个超平面对于任何维度都是线性分隔符；它可以是直线(2D)、平面(3D)和超平面(4D+)。请看<em class="kl">统计学习简介</em>中的这张图表:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/5d8255d07ade1af5ee9db8baf56f43a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UGsHP6GeQmLBeteRz80OPw.png"/></div></div></figure><p id="e4c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以用无限多的超平面来分离红色和蓝色物体。哪个超平面最好？好吧，最好的超平面是最大化边际的超平面。边缘是超平面和几个接近点之间的距离。这些接近点是支持向量，因为它们控制超平面。下图显示了红色和蓝色物体的最佳超平面。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ky"><img src="../Images/a6c137f3dbc4088eb55e90190203eb13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3UXkIOXDZoBE9saNmefW4A.png"/></div></div></figure><p id="7da4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是最大间隔分类器。它使超平面的边缘最大化。这是最好的超平面，因为它最大程度地减少了泛化误差。如果我们添加新数据，最大间隔分类器是正确分类新数据的最佳超平面。最大间隔分类器是我们的第一个SVM。但是这个SVM要求两个类完全线性分离。情况并非总是如此，因此在1993年，Vapnik开发了他的另一台机器。</p><p id="b35a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下图显示了不完全可分的数据。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi kz"><img src="../Images/85e2ffc36930ce0cf9d04547317899a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rTPs_OvId9gtUxwcpEtnrw.png"/></div></div></figure><p id="a328" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这种情况下，最大间隔分类器将不起作用。Vapnik开发了一个软余量，允许对数据进行一些错误分类。这被称为软边界分类器或支持向量分类器。它还试图最大限度地扩大两个阶层之间的差距。下图说明了这个SVM。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi la"><img src="../Images/e002936c595c30c64700c7579309c740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Eef0R5rEtaitLP6bVUTUyg.png"/></div></div></figure><p id="d724" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">支持向量分类器包含一个调整参数，以便控制它将允许多少错误分类。当希望最小化误差时，这个调谐参数是重要的。和所有监督学习一样，有一个偏差-方差权衡。当调整参数(通常表示为C)较小时，分类器只允许少量的误分类。支持向量分类器将具有低偏差，但是可能不能很好地概括，并且具有高方差。如果我们的调整参数太小，我们可能会过度拟合训练数据。如果C很大，则允许的错误分类数会增加。这个分类器可以更好地进行归纳，但是可能会有很大的偏差。当调整参数为零时，不可能有错误分类，并且我们有最大间隔分类器。下图说明了这一点。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi lb"><img src="../Images/53798086a17be0f9802150a83d03bd94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mlx5qaB0y3bBS9_rUPdm0w.png"/></div></div></figure><p id="84cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果数据不是线性可分的，则支持向量分类器可能会失败。1992年，Vapnik开发了一种处理非线性可分类的方法。这个方法使用内核技巧。我们需要“扩大特征空间，以适应类别之间的非线性边界”(<em class="kl">统计学习介绍</em>)。核是量化观察值之间相似性的函数。用于分离非线性数据的常见类型的核是多项式核、径向基核和线性核(与支持向量分类器相同)。简单地说，这些核转换我们的数据，以便通过一个线性超平面，从而对我们的数据进行分类。下面是各种内核类型的一些可视化指南。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/a4618a9b85faf5097db4d71390c1895b.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*JrRFa4tyBH3DlVCjPTtwig.png"/></div></figure><p id="eb3c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">【http://svmcompbio.tuebingen.mpg.de/img/poly.png T2】号</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi le"><img src="../Images/fc67fde9ad77e4f25c1f9cb7c92633dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-EKtwnCA6aEsoQVRzKS3aA.png"/></div></div></figure><p id="c8b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">支持向量机的扩展可以用来解决各种其他问题。我们可以使用一对一分类或一对所有分类来建立多个类别的支持向量机。在<em class="kl">统计学习简介</em>中可以找到对这些的简要描述。此外，支持向量回归机存在的回归问题。你也可以研究支持向量聚类、SVM排名、直推式SVM等等。</p><p id="0314" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么，我们应该什么时候使用支持向量机呢？当组被清楚地分开时，支持向量机在分类方面做得很好。当我们的数据非线性分离时，它们也能做得很好。您可以转换数据以线性分离它，或者让SVM转换数据并线性分离这两个类。这是使用支持向量机的主要原因之一。您不必自己转换非线性数据。支持向量机的一个缺点是这些函数的黑盒性质。使用核来分离非线性数据使得它们很难(如果不是不可能的话)解释。</p><p id="54c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是一个用Scikit-Learn在Python中快速实现SVM的例子。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi lf"><img src="../Images/09f63cb92958aed2a56bf6d5f488eac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5cbaxIYKEcnPPsUaHvtlAQ.png"/></div></div></figure><p id="b71b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的代码中，我用特定的内核函数实例化了SVM，并使用交叉验证来检查模型的平均准确性。正如您所见，使用SVM可以非常简单。</p><p id="66f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总的来说，支持向量机是针对特定情况的优秀分类器。理解它们将会给你一个替代GLMs和决策树的分类方法。一定要看看我下面的引用，尤其是如果你想要更深入的支持向量机的数学解释。如果您有在这些资源中尚未找到答案的问题，请向Vapnik发送消息。他目前在脸书人工智能研究所工作。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/cbf3f180e20f4104be207e06afeb9358.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*usEw25msff7r0MhhqNZi0g.jpeg"/></div></figure><p id="27c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">资源:</p><p id="346b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">加雷斯·詹姆斯、丹妮拉·威滕、特雷弗·哈斯蒂和罗伯特·蒂布拉尼的《统计学习导论》</p><p id="673c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">【https://www.svm-tutorial.com/ T2】号</p><p id="0302" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">【http://www.yaksis.com/ T4】</p><p id="8e7e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Joseph Nelson和Matt Speck在DC SVM大会上的演讲</p><p id="d577" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">麻省理工学院6.034人工智能，2010秋季课程——导师:帕特里克·温斯顿</p></div></div>    
</body>
</html>