<html>
<head>
<title>Combing LDA and Word Embeddings for topic modeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">结合 LDA 和词嵌入的主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/combing-lda-and-word-embeddings-for-topic-modeling-fe4a1315a5b4?source=collection_archive---------5-----------------------#2018-09-15">https://towardsdatascience.com/combing-lda-and-word-embeddings-for-topic-modeling-fe4a1315a5b4?source=collection_archive---------5-----------------------#2018-09-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/49b5549ee1f01d897ccfde1ab939dafc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MxbVFt5pqN_8LGxm"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">“Business newspaper article” by <a class="ae kf" href="https://unsplash.com/@freegraphictoday?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">G. Crescoli</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="54f8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">潜在狄利克雷分配(LDA)是一种经典的主题建模方法。主题建模是无监督学习，目标是将不同的文档分组到同一个“主题”中。</p><p id="4441" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个典型的例子是将新闻聚集到相应的类别，包括“金融”、“旅游”、“体育”等。在单词嵌入之前，我们可能大部分时间使用单词袋。然而，在 Mikolov 等人于 2013 年引入 word2vec(单词嵌入的例子之一)后，世界发生了变化。Moody 宣布了 lda2vec，它将 lda 和词嵌入结合起来解决主题建模问题。</p><p id="ac80" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看完这篇文章，你会明白:</p><ul class=""><li id="9582" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">潜在狄利克雷分配</li><li id="9908" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">单词嵌入</li><li id="a6af" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">lda2vec</li></ul><h1 id="97d0" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">潜在狄利克雷分配</h1><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div class="ab gu cl mu"><img src="../Images/0095b1565e81d671caf38c8fb5be0f48.png" data-original-src="https://miro.medium.com/v2/format:webp/0*doqjI4Nw8RVWbDGy.jpg"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo: <a class="ae kf" href="https://pixabay.com/en/golden-gate-bridge-women-back-1030999/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/en/golden-gate-bridge-women-back-1030999/</a></figcaption></figure><p id="f083" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LDA 在主题建模领域很有名。基于单词使用的文档聚类。简单来说，LDA 使用<a class="ae kf" rel="noopener" target="_blank" href="/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016">单词袋</a>作为聚类的特征。详细情况，你可以看看这个<a class="ae kf" rel="noopener" target="_blank" href="/2-latent-methods-for-dimension-reduction-and-topic-modeling-20ff6d7d547">博客</a>。</p><h1 id="2099" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">单词嵌入</h1><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div class="ab gu cl mu"><img src="../Images/8725fed1629283340023f9d3c3b664ef.png" data-original-src="https://miro.medium.com/v2/format:webp/0*7D26_3Oc5hwZpKei.jpg"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Credit: <a class="ae kf" href="https://pixabay.com/en/books-stack-book-store-1163695/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/en/books-stack-book-store-1163695/</a></figcaption></figure><p id="56cb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单词嵌入的目标是解决自然语言处理问题中的稀疏和高维特征。有了单词嵌入(或向量)，我们可以使用低维(大多数时候是 50 或 300)来表示所有单词。详细情况，你可以看看这个<a class="ae kf" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a">博客</a>。</p><h1 id="8d89" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">lda2vec</h1><p id="a610" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">lda2vec 包括两个部分，即单词向量和文档向量，用于预测单词，以便同时训练所有向量。它通过跳格模型构建了一个<strong class="ki iu">词向量</strong>。简而言之，它使用目标词预测周围的词来学习向量。第二部分是<strong class="ki iu">文档向量</strong>,由</p><ul class=""><li id="11df" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">文档权重向量:每个主题的权重。利用 softmax 将权重转换为百分比。</li><li id="fbb5" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">主题矩阵:主题向量。一列表示一个主题，而一行存储每个主题附近的相关单词。</li></ul><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a7382dc128d167f649b35a07d3bf702c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*Z6lSSsfScUl-ZQ3c-OH87w.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae kf" href="https://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/#topic=38&amp;lambda=1&amp;term=" rel="noopener ugc nofollow" target="_blank">https://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/#topic=38&amp;lambda=1&amp;term=</a></figcaption></figure><p id="06fc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">文档向量的公式为</p><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/d9e26cbb2a444b07236bb5d74f28e68d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*CFZEU28Q1vLY93LBnP61ew.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Moody, Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec (2016)</figcaption></figure><ul class=""><li id="0ccd" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">DJ:j 文档向量</li><li id="91ba" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">pj0:“0”主题中 j 文档的权重</li><li id="ab8e" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">pjn:“n”主题中 j 文档的权重</li><li id="a011" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">t0:“0”话题的向量</li><li id="e818" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">TN:“n”话题的向量</li></ul><p id="9f27" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当主题向量被共享时，文档之间的权重是不同的。更多细节，你可以查看穆迪的<a class="ae kf" href="https://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/#topic=38&amp;lambda=1&amp;term=" rel="noopener ugc nofollow" target="_blank">原创博客</a>。</p><h1 id="c825" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">拿走</h1><p id="2f72" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">对于源代码，你可以看看这个<a class="ae kf" href="https://github.com/cemoody/lda2vec/blob/master/examples/hacker_news/lda2vec/lda2vec.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a></p><ul class=""><li id="70ca" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">正如作者所建议的，如果你想拥有人类可读的主题，你应该使用 LDA。如果你想用另一种方式重做光学模型或者预测用户的话题，你可以试试 lda2vec。</li></ul><h1 id="6982" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以通过<a class="ae kf" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae kf" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae kf" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="6710" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">参考</h1><p id="0816" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">穆迪·克里斯托弗。“混合狄利克雷主题模型和单词嵌入来制作 lda2vec”。2016.<a class="ae kf" href="https://arxiv.org/pdf/1605.02019.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1605.02019.pdf</a></p></div></div>    
</body>
</html>