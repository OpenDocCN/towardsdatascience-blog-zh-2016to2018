<html>
<head>
<title>Naive Bayes Classifier: A Geometric Analysis of the Naivete. Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朴素贝叶斯分类器:朴素贝叶斯的几何分析。第一部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1-51f462a858bb?source=collection_archive---------9-----------------------#2018-08-06">https://towardsdatascience.com/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1-51f462a858bb?source=collection_archive---------9-----------------------#2018-08-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fc3c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">当类被线性和非线性真决策边界分开时，为朴素贝叶斯预测导出封闭形式的解…</h2></div><p id="0a18" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">维数灾难</a>是所有分类问题的祸根。什么是维度的诅咒？随着特征(维度)数量的线性增长，分类所需的训练数据量也呈指数增长。如果分类是由单个特征确定的，我们需要该特征的一系列值的先验分类数据，因此我们可以预测新数据点的类别。对于具有 100 个可能值的特征<em class="lc"> x </em>，所需的训练数据的数量级为 O(100)。但是，如果还需要第二个特征<em class="lc"> y </em>来确定类别，并且<em class="lc"> y </em>具有 50 个可能值，那么我们将需要 O(5000)阶的训练数据，即在对“<em class="lc"> x，y”</em>的可能值的网格上。因此，所需数据的度量是特征空间的体积，并且随着更多特征的添加，它呈指数增长。</p><p id="e4f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但这总是这么糟糕吗？我们是否可以做一些简化的假设来减少所需的数据量，同时保留所有的特性？在上面的例子中，我们说我们需要 O(5000)阶的训练度量。但是朴素贝叶斯分类器只需要 O(150)阶的测量值——即只是线性增加，而不是指数增加！这太棒了，但是我们知道没有免费的午餐，朴素贝叶斯分类器应该做一些简化(<em class="lc">朴素</em>？)假设。这就是这篇文章的目的——检查<em class="lc">天真</em>在<a class="ae lb" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯分类器</a>中的影响，该分类器允许它避开维数灾难。实际上，我们不希望代数分散我们对我们所追求的东西的欣赏，所以我们坚持二维 x，y 和两类 C_1 和 C _ 2 T21。</p><p id="908e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">决定边界</strong></p><p id="765a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">决策边界是我们的二维特征空间中的一条曲线，它将两个类别<em class="lc">C1</em>和<em class="lc">C2</em>分开。在图 1 中，<em class="lc"> y-f(x) &gt; 0 </em>表示等级<em class="lc">C1</em>和<em class="lc"> y -f(x) &lt; 0 </em>表示<em class="lc">C2</em>的区域。沿着决策边界<em class="lc"> y = f(x)，</em>并且属于任一类的概率相等。任一类的等概率是获得决策边界的准则。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ld"><img src="../Images/bc265aa902681f1fa857fb0521c37808.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/0*Yey-tKzIq-9hPUBD.jpg"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Figure 1. <em class="lp">The decision boundary separates the two classes C_1 and C_2 in the feature space. For points on the decision boundary, the probability of belonging to either class is the same.</em></figcaption></figure><p id="5e72" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章的目的是分析非线性对决策边界的影响，以及由不平衡的班级规模引起的相关问题。这个职位的总体轮廓如下。</p><ol class=""><li id="1c3d" class="lq lr iq kh b ki kj kl km ko ls ks lt kw lu la lv lw lx ly bi translated">选择一个精确的函数形式<em class="lc"> y = f(x) </em>作为真正的决策边界。</li><li id="7bcf" class="lq lr iq kh b ki lz kl ma ko mb ks mc kw md la lv lw lx ly bi translated">获得由朴素贝叶斯分类器预测的决策边界的封闭形式的解</li><li id="74f1" class="lq lr iq kh b ki lz kl ma ko mb ks mc kw md la lv lw lx ly bi translated">分析班级规模对预测的影响</li><li id="b130" class="lq lr iq kh b ki lz kl ma ko mb ks mc kw md la lv lw lx ly bi translated">当一个类压倒另一个类时，获得渐近行为</li><li id="b1b5" class="lq lr iq kh b ki lz kl ma ko mb ks mc kw md la lv lw lx ly bi translated">当<em class="lc"> f(x) </em>为线性和非线性时，重复 1 至 4。</li></ol><p id="1e6c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">帖子的内容有些学术性，因为对于真实数据，我们不知道真正的决策边界，必须处理数据噪声，并且不知道哪些特征实际上负责我们所寻求的分类等。但这里的要点是理解在理想化设置中天真假设的影响，以便我们能够更好地应用和解释真实数据的朴素贝叶斯分类器。</p><h1 id="ae6a" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">1.朴素贝叶斯分类</h1><p id="a24e" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">朴素贝叶斯分类基于关联条件概率和边际概率的<a class="ae lb" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯规则</a>。这在文献中有很好的描述，因此我们简单地写下具有 2 个特征<em class="lc"> x，y </em>的 2 类(<em class="lc">C1</em>和<em class="lc">C2</em>)情况的方程。</p><p id="0051" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(1)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/a53158d033041aaa559eca27dfe7e6b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/0*6w5x87Kj2jK90yG1.png"/></div></figure><p id="3459" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于任何新的测量值<em class="lc"> x，y </em>，我们将根据等式<a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/#id3793327209" rel="noopener ugc nofollow" target="_blank"> 1 </a>计算<em class="lc"> P(C_1|x，y) </em>和<em class="lc"> P(C_2|x，y) </em>，并选择具有较大值的类。因为分母是相同的，所以计算它们的比值更容易，所以我们不需要计算<em class="lc"> P(x，y) </em>。</p><p id="0569" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lc"> P(C_i) </em>是任何测量值落入<em class="lc"> C_i 类的概率。</em>作为<em class="lc"> C_i </em>样本的相对丰度，它很容易从训练数据中计算出来。正是 P(x，y|C_i) 的计算充满了我们谈到的数据需求的挑战。为了做到这一点，我们需要估计每一类<em class="lc"> C_i </em>中<em class="lc"> x，y </em>的联合概率分布，这需要在<em class="lc"> x，y </em>值的网格上的训练数据。这就是朴素贝叶斯的<em class="lc">朴素</em>部分缓解维数灾难的地方。</p><h1 id="5776" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">1.1 天真的假设</h1><p id="c02d" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">如果特征<em class="lc"> x，y </em>是不相关的，给定类别<em class="lc"> C_i </em>，那么联合概率分布将简单地是个体概率的乘积。即</p><p id="98ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(2)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/65c53936c10be74129bb00fb91e20033.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/0*V7RtKRi9DDeI1IQG.png"/></div></figure><p id="432a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们做这个假设，那么我们只需要分别估计<em class="lc"> P(x|C_i) </em>和<em class="lc"> P(y|C_i) </em>。并且这只需要在范围<em class="lc"> x </em>和范围<em class="lc"> y </em>上的训练数据，而不是在<em class="lc"> x，y </em>值的网格上。使用方程<a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/#id3477009362" rel="noopener ugc nofollow" target="_blank">1</a>中的方程<a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/#id3477009362" rel="noopener ugc nofollow" target="_blank"> 2 </a>，我们得到新的<em class="lc"> x，y </em>测量值及其类别分配的类别概率的比率如下</p><p id="8d47" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(3)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/7f997093782a49d5b375a721d21f1339.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/0*XkHmAvN6Jo--FJlv.png"/></div></figure><p id="f9d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然不相关特征的天真假设简化了事情，但它的有效性值得商榷。例如，如果<em class="lc"> xy &lt; 1 </em>在一个类中，那么当<em class="lc"> y </em>较小时，找到大的<em class="lc"> x </em>值的概率较高，当<em class="lc"> y </em>较大时，找到大的<em class="lc">x</em>值的概率较低。事实上，在这个类中找到测量值[x=0.9，y=1.1]的概率将是 1，而对于测量值[x=0.9，y=1.2]来说，它将是 0。显然，<em class="lc"> x </em>和<em class="lc"> y </em>的概率分布通常不会相互独立，并且对独立性的天真假设会导致新数据的错误分类。</p><h1 id="a538" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">1.2 推导 P(x|C_i)和 P(y|C_i)</h1><p id="34a1" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">如果我们可以获得作为<em class="lc"> x，y </em>的函数的<em class="lc"> P(x|C_i) </em>和<em class="lc"> P(y|C_i) </em>，那么我们可以得到作为<em class="lc"> x，y </em>的函数的公式<a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/#id3874001059" rel="noopener ugc nofollow" target="_blank"> 3 </a>中的比值。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/f2d580c12f23ad19638e54049ee58416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/0*oZQ9L8Ar5hrFr5UY.jpg"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Figure 2. <em class="lp">Deriving the naive bayes probabilities, given a true decision boundary y = f(x). To keep the algebra simple, the function f(x) is taken to have a simple inverse x = f^-1(y). The fractional areas of the rectangular strips in a class are indicative of the probability.</em></figcaption></figure><p id="5a4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑图 2，其中真实判定边界<em class="lc">y = f(x)</em>将特征空间分成两类。为了避免代数问题，我们进一步假设<em class="lc"> f </em>是显式可逆的，因此我们可以写成<em class="lc"> x = f^-1(y).</em>一个类<em class="lc"> </em>的先验概率与它的整体大小成正比——在这种情况下是面积。</p><p id="03cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(4)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/65fc6a486fc453548ccfc680aa556c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/0*RkM4InaKhFJM-YbD.png"/></div></figure><p id="2fa3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了计算<em class="lc"> P(x|C_i)，</em>假设我们将 x 轴离散化，并在<em class="lc"> x </em>周围选取一个宽度为δx 的小矩形条，如图 2 所示。类别<em class="lc"> C_i </em>中的条带面积除以类别面积 A_i 将是概率<em class="lc"> P (x|C_i)。</em>同样对于概率<em class="lc"> P (y|C_i) </em></p><p id="58f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(5)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/965c0f07cd426aad1f5249762f0d4c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/0*pAVhESdA6XFDb1NG.png"/></div></figure><p id="f0cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lc">x1</em>越长，<em class="lc">P(y | C1)</em>的概率就越高。同样，更大的<em class="lc"> y_2 </em>意味着更高的概率用于<em class="lc"> P(x|C_2) </em>等等。一旦我们理解了这一点，剩下的就是简单的几何和代数了。将等式<a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/#id3009263874" rel="noopener ugc nofollow" target="_blank"> 4 </a>和<a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/#id2595860965" rel="noopener ugc nofollow" target="_blank"> 5 </a>与等式<a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/#id3874001059" rel="noopener ugc nofollow" target="_blank"> 3 </a>中的朴素贝叶斯分类相结合，我们得到:</p><p id="b7c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(6)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/59323d5861e479043e042cc564ee3b56.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/0*wsMhwcDTCo3Rj-6k.png"/></div></figure><p id="51b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上述比率等于 1 的<em class="lc"> x，y </em>的轨迹是朴素贝叶斯分类器预测的决策边界。</p><p id="ac8b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(7)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/abfd919778d62bb248f16f811f9dce13.png" data-original-src="https://miro.medium.com/v2/resize:fit:190/format:webp/0*lPgPnw4u4M-3GLXj.png"/></div></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nj"><img src="../Images/1424c01287220896adbe91aadb04270e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kB4pkRBF1tR-hiKi.jpg"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Figure 3. <em class="lp">For a point on a linear decision boundary, a geometric proof that: (a) x_1 y_1 = x_2 y_2 in case of balanced classes and (b) x_1 y_1 ≠ x_2 y_2 when classes are unbalanced.</em></figcaption></figure><p id="b49e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了直观理解等式<a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/#id3940497998" rel="noopener ugc nofollow" target="_blank"> 7 </a>，考虑图 3 中具有相等/不相等班级规模的线性决策边界的情况。如图 3.1 所示，当班级规模相等时，决策边界将是对角线，对于对角线上的每一点，矩形 ABOC 的面积等于矩形 ODEF 的面积。但那不过是关于对角线上任意一点的<em class="lc"> x_1 y_1 </em>和<em class="lc"> x_2 y_2 </em>相等的陈述。因此，根据等式<a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/#id3940497998" rel="noopener ugc nofollow" target="_blank"> 7 </a>，由朴素贝叶斯预测的决策边界将匹配对角线——真正的决策边界。然而，在图 3.2 中，类的大小是不一样的，并且这两个矩形的面积不必对分离边界上的每一点都相等。因此，即使在线性情况下，当类别大小不同时，我们也不能期望朴素贝叶斯产生真实的决策边界。我们将在下一节通过分析再次验证这一点。</p><p id="724e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就完成了我们推导朴素贝叶斯预测的决策边界的一般方法。在这篇文章的剩余部分，我们为真正的决策边界选择不同的函数形式<em class="lc"> f(x) </em>，并与我们从朴素贝叶斯得到的进行对比。</p><h1 id="ef60" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">2.线性决策边界</h1><p id="8dd4" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">我们从图 4 中的线性情况开始，其中直线<em class="lc"> y = qx/a </em>在矩形特征空间中将两个类分开。当<em class="lc"> q </em>等于<em class="lc"> b </em>时，我们得到平衡类。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi no"><img src="../Images/0394377c8426b09f329da7c07023a4d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/0*SxFtmGE8E-IyHTEi.jpg"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Figure 4. <em class="lp">Evaluating x_1 , y_1 , x_2 and y_2 for a linear decision boundary y=qx/a</em></figcaption></figure><p id="132c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">类面积<em class="lc">a1，a2</em>和长度<em class="lc">x1，x2，y1，</em>和<em class="lc">y2</em>直接从几何图形中得出。在等式<a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/#id3940497998" rel="noopener ugc nofollow" target="_blank"> 7 </a>中使用它们并进行简化，我们得到朴素贝叶斯预测的决策边界是一条双曲线。</p><p id="ec31" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(8)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi np"><img src="../Images/68dc8d9d3f4eab58dc5305fabdc6898e.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/0*o9dTUuR_EgzHXZ5b.png"/></div></figure><p id="aa7b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了预测分离边界的闭合解，我们可以看看当 A_1 类的规模保持增加而 A_2 保持不变时，类规模的影响及其渐近性。这是通过增加<em class="lc"> b </em>同时保持<em class="lc"> a </em>和<em class="lc"> q </em>不变来实现的。</p><h1 id="ea3d" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">2.1 均衡类。A_1 = A_2</h1><p id="531b" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">当<em class="lc"> b = q </em>时，类别大小将相等，并且分离边界减少到:</p><p id="d45a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(9)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/9ca7341109721e4546a364be75760717.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/0*PfNZzeEcfaSvFs7A.png"/></div></figure><p id="1588" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是真正的决策界限。也就是说，当类由直线分隔并且大小相同时，朴素贝叶斯分类不会有错误。这是对图 3 中相同几何论证的严格证明。</p><h1 id="1c2a" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">2.2 不平衡的阶层。增加 a1 和常数 a2</h1><p id="ecb4" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">图 5 显示了当<em class="lc"> b </em>增加时的预测决策边界(即，我们拖动特征空间的顶部边界)，从而增加 A_1，同时保持 A_2 不变。在所有情况下，对于较小的<em class="lc"> x </em>，朴素贝叶斯预测开始偏向于类别<em class="lc"> C_2 </em>(预测的边界在真实边界之上——意味着将真实 C_1 测量分类为 C_2 的偏好),并且随着<em class="lc"> x </em>的增加，切换到偏好类别<em class="lc"> C_1 </em>。然而，有趣的是，它们都在真实决策边界的同一点上切换。</p><h2 id="83c4" class="nr mf iq bd mg ns nt dn mk nu nv dp mo ko nw nx mq ks ny nz ms kw oa ob mu oc bi translated">2.2.1 切换点</h2><p id="221e" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">切换点<em class="lc"> (x*，y*) </em>是真实决策边界和预测决策边界的交点。利用方程<a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/#id218994774" rel="noopener ugc nofollow" target="_blank"> 8 </a>中的<em class="lc"> y = qx/a </em>并简化得到:</p><p id="1538" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(10)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi od"><img src="../Images/edeae58c71ad84cc29ec3de0814080e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/0*uRqUDOMhPFXhZdUX.png"/></div></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/ea098c9b475735136c2f71de0b4ad402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/0*5Z2RAv1uWHbiFwhv.jpg"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Figure 5. <em class="lp">Linear decision boundary. Naive bayes classifier predicts a hyperbola as the class boundary. Only when the classes are balanced does the prediction match the prescribed boundary y = x</em></figcaption></figure><p id="5256" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该点的坐标独立于<em class="lc"> b，</em>不同曲线上的唯一变量。因此它们都与真实边界相交于同一点。</p><h2 id="6586" class="nr mf iq bd mg ns nt dn mk nu nv dp mo ko nw nx mq ks ny nz ms kw oa ob mu oc bi translated">2.2.2 渐近边界:a1→∞</h2><p id="20b0" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">随着<em class="lc"> b </em>增加，比率<em class="lc"> A_1/A_2 </em>增加。当<em class="lc"> b </em>趋于无穷大时，我们得到渐近预测边界<em class="lc"> y_s(x)。</em></p><p id="58c8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(11)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi of"><img src="../Images/252eefcae050410c4a6ecd79883135fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/0*Y1pLbKLlqnnHZlth.png"/></div></figure><h1 id="e092" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">3.非线性决策边界</h1><p id="646e" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">我们现在考虑图 6 中的情况，其中抛物线判决边界<em class="lc"> y=x^2 </em>将两个类别分开。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi og"><img src="../Images/bcf2af9ff62de7326fd25beff6557011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/0*FLShOL2hJKbHopDK.jpg"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Figure 6. <em class="lp">Evaluating x_1 , y_1 , x_2 and y_2 for a nonlinear decision boundary y=x²</em></figcaption></figure><p id="8ef2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">再次，x_1，y_1，x_2 和 y_2 作为 x，y 的函数的表达式直接来自几何。应用等式<a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/#id3940497998" rel="noopener ugc nofollow" target="_blank"> 7 </a>并进行简化，我们得到<em class="lc"> y </em>作为预测决策边界的<em class="lc"> x </em>的四阶函数</p><p id="5b76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(12)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/7ec10efdd8d396e24b5b888f4b2b38ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/0*Xp-Pu-B7e1zKS8D0.png"/></div></figure><h1 id="255f" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">3.1 增加 A_2 和常数 A_1 的影响</h1><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/df6750e7d997d74069516a47472ec40e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/0*AGhzQZ8iQNT07DLJ.jpg"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Figure 7. <em class="lp">Nonlinear decision boundary. The naive bayes prediction does not match the true boundary even in the case of balanced classes.</em></figcaption></figure><p id="b050" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图 7 显示了当类大小 A_2 变化而保持 A_1 不变时，预测的决策边界与真实边界的对比。这可以简单地通过拖动特征空间的右边界来完成，即改变<em class="lc">为</em>。</p><p id="b6bd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里有一些与线性情况相反的快速观察。</p><ul class=""><li id="4382" class="lq lr iq kh b ki kj kl km ko ls ks lt kw lu la oj lw lx ly bi translated">即使在平衡类的情况下(A_1/A_2 = 1 即 a = 4q/3)，预测的边界也与真实的边界不匹配。这不同于我们前面看到的线性情况。使用等式<a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/#id1924063270" rel="noopener ugc nofollow" target="_blank"> 12 </a>中的 q = 3a/4，我们得到:</li></ul><p id="8f77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(13)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/195c4d86127942b2013523d291447e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/0*d4u_qfF2hMhFqUKm.png"/></div></figure><ul class=""><li id="6ad5" class="lq lr iq kh b ki kj kl km ko ls ks lt kw lu la oj lw lx ly bi translated">对于小的<em class="lc"> x </em>，预测倾向于<em class="lc"> C_1 </em>而不是<em class="lc"> C_2 </em>(在所有情况下，预测边界在真实边界以下开始)，但是对于较大的<em class="lc"> x </em>则切换到偏好<em class="lc"> C_2 </em>。</li><li id="6faf" class="lq lr iq kh b ki lz kl ma ko mb ks mc kw md la oj lw lx ly bi translated">当 a2 较小时，预测边界在<em class="lc"> x </em>范围内大部分在真实边界之下。对于较大的 A_2，预测边界变得更陡，越来越早地与真实边界相交。因此，与线性情况不同，它们与真实边界相交于不同的点。</li><li id="1df0" class="lq lr iq kh b ki lz kl ma ko mb ks mc kw md la oj lw lx ly bi translated">即使它们在不同的点上与真实边界相交，有趣的是它们都在一个点上相交。那当然可以证明。不同曲线上的唯一变量是<em class="lc">和</em>。所以我们要做的就是找到两个预测边界的交点，并证明它独立于<em class="lc"> a </em>。考虑 a_ <em class="lc"> 1 </em>和 a_2 的两个边界。使用方程<a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/#id1924063270" rel="noopener ugc nofollow" target="_blank"> 12 </a>在它们的交点处并简化我们得到，</li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/4431a5dd41a17a9b791cf985aef84fbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/0*YaX1z-d6yYozLPII.png"/></div></figure><p id="72a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">产生独立于<em class="lc"> a </em>的交点，从而证明观察结果。</p><p id="39d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(14)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/38f3858b243adbe05087f7ed77858b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/0*Xlo-cg2_zQ6BSTB0.png"/></div></figure><h1 id="9cc3" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">3.2 渐近边界:a2→∞</h1><p id="896c" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">随着<em class="lc"> a </em>增加，面积<em class="lc"> A_2 </em>增加。当<em class="lc"> a </em>趋于无穷大时，我们得到渐近预测边界<em class="lc"> y_s(x)。</em></p><p id="34bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">(15)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi om"><img src="../Images/5914b2b41e2ed832b52fbbbecf5e1655.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/0*N7AgZ0Fj8vFoH04V.png"/></div></figure><h1 id="73ce" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">4.后续步骤</h1><p id="5656" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">当真实的决策边界在具有两个特征的两个类之间已知时，我们已经导出了由朴素贝叶斯分类器预测的决策边界的封闭形式的解决方案。我们挑选了简单的线性和非线性边界，并评估了当一个类别压倒另一个类别时，类别大小及其渐近性的影响。接下来的步骤如下。</p><ul class=""><li id="3d20" class="lq lr iq kh b ki kj kl km ko ls ks lt kw lu la oj lw lx ly bi translated">在总规模保持不变的情况下，获得作为班级规模比率的函数的混淆矩阵。例如，当我们在图 4 中将 q 从 0 变到 1 时，比率<em class="lc"> A_1/A_2 </em>从∞变到 0，而<em class="lc"> A_1 + A_2 </em>保持恒定在<em class="lc"> ab </em>。在这种情况下，通过混淆矩阵测量的朴素贝叶斯分类器的性能是令人感兴趣的。</li><li id="d48b" class="lq lr iq kh b ki lz kl ma ko mb ks mc kw md la oj lw lx ly bi translated">模拟在<a class="ae lb" href="http://aussian/%20Naive%20Bayes%20algorithm%20to%20evaluate%20the%20additional%20errors%20introduced%20by%20the%20gaussian%20approximation," rel="noopener ugc nofollow" target="_blank"> SciKit </a>中实现的高斯朴素贝叶斯算法，以评估高斯近似概率引入的额外误差。</li><li id="9972" class="lq lr iq kh b ki lz kl ma ko mb ks mc kw md la oj lw lx ly bi translated">评估朴素贝叶斯预测相对于其他竞争分类器，如逻辑回归，神经网络等…</li></ul><p id="149a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于这篇文章太长了，所以我将在这个系列的下一篇文章中讨论以上内容。</p></div><div class="ab cl on oo hu op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="ij ik il im in"><p id="7c45" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lc">原载于 2018 年 8 月 6 日</em><a class="ae lb" href="http://xplordat.com/2018/08/06/naive-bayes-classifier-a-geometric-analysis-of-the-naivete-part-1/" rel="noopener ugc nofollow" target="_blank"><em class="lc">xplordat.com</em></a><em class="lc">。</em></p></div></div>    
</body>
</html>