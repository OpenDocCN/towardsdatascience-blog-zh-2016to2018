<html>
<head>
<title>Differentiable Neural Computers (DNCs) — Nature article thoughts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可微分神经计算机(DNCs)——自然文章思考</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/humphrey-sheil-differentiable-neural-computers-dncs-nature-article-thoughts-bd22939c2d97?source=collection_archive---------5-----------------------#2017-08-15">https://towardsdatascience.com/humphrey-sheil-differentiable-neural-computers-dncs-nature-article-thoughts-bd22939c2d97?source=collection_archive---------5-----------------------#2017-08-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1e93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2016年10月17日</p><p id="3344" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们将人工神经网络与冯·诺依曼CPU架构相比较时，人工神经网络(ann)中工作记忆的添加是一个明显的升级，并且是去年NIPS的RAM(推理、注意力、记忆)研讨会中出现的一个(挤满了人)。显而易见和建筑然而是两回事..</p><p id="480f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz" rel="noopener ugc nofollow" target="_blank">谷歌Deepmind </a>最近发表的关于可分化神经计算机(DNCs)的论文代表了在人工神经网络中添加工作记忆的旅程中又迈出了重要的一步，因此值得更深入地研究一下。</p><h1 id="0676" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">这是一个变化的时代</h1><p id="f458" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">为了向新晋的<a class="ae kl" href="https://www.theguardian.com/music/2016/oct/15/bob-dylan-deserves-nobel-laureate-literature-singular-talent" rel="noopener ugc nofollow" target="_blank">诺奖得主</a>鲍勃·迪伦致敬，我们用他1964年的经典书名来引起人们对目前神经网络格局重构的惊人变化的关注。</p><p id="c221" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="http://www.movidius.com/" rel="noopener ugc nofollow" target="_blank"> Movidius </a>被英特尔收购——他们的口号“物联网视觉传感”为他们的重点提供了线索——他们的VPU或视觉处理单元可以执行TensorFlow或Caffe神经网络模型。</p><p id="627a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">英特尔自己正在<a class="ae kl" href="https://twitter.com/soumithchintala/status/786569573365538816" rel="noopener ugc nofollow" target="_blank">为新的x86指令准备Linux内核</a>，致力于在CPU而不是GPU上运行神经网络(英特尔在这一领域已经落后英伟达很长时间了)。</p><p id="f15a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">就采用和公共性能而言，英伟达仍然是明确的硬件领导者——他们在深度学习上下了很大的赌注，在今年的GTC 2016上，它是大会的基石——从<a class="ae kl" href="https://blogs.nvidia.com/blog/2016/07/11/how-nvidia-built-dgx-1/" rel="noopener ugc nofollow" target="_blank"> DGX-1 </a>到<a class="ae kl" href="https://devblogs.nvidia.com/parallelforall/cuda-8-features-revealed/" rel="noopener ugc nofollow" target="_blank">CUDA 8</a>/<a class="ae kl" href="https://developer.nvidia.com/cudnn" rel="noopener ugc nofollow" target="_blank">cud nn 5 . x</a>版本。</p><p id="c6ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们知道Google有他们自己的TPU(张量处理单元),但不太了解它们，也不知道它们如何达到GPU或CPU。</p><p id="acb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简而言之，硬件正在变形，以更有效地运行更大的神经网络，并使用更少的功率。现在每个主要的软件公司都与学术机构有联系，并积极致力于将深度学习/神经计算应用到他们的平台和产品中。</p><p id="35ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种水平的硬件和软件活动在神经计算领域是前所未有的，而且没有减弱的迹象。那么，民主党全国委员会的文件如何在所有这些活动中发挥作用，如果有的话？</p><h1 id="820b" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">可微分神经计算机</h1><p id="762f" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">那么，什么是DNC呢？分解这张纸，我们得到以下要点:</p><ul class=""><li id="8946" class="lp lq iq jp b jq jr ju jv jy lr kc ls kg lt kk lu lv lw lx bi translated">本质上，DNC“只是”一个递归神经网络(RNN)。</li><li id="9dbb" class="lp lq iq jp b jq ly ju lz jy ma kc mb kg mc kk lu lv lw lx bi translated">然而，该RNN被允许读取/写入/更新存储器(M)中的位置——RNN存储大小为W的数据的向量或张量，其中M具有大小为W的N行，因此M = N*W</li><li id="e42e" class="lp lq iq jp b jq ly ju lz jy ma kc mb kg mc kk lu lv lw lx bi translated">DNC使用<em class="md">可区分注意力</em>来决定从哪里读取/写入/更新内存中的现有行。这是一个关键点，因为这现在使得诸如随机梯度下降(SGD)的充分理解的学习算法能够用于训练DNC。</li><li id="c6a0" class="lp lq iq jp b jq ly ju lz jy ma kc mb kg mc kk lu lv lw lx bi translated">存储体M是关联的——实现使用余弦相似性，从而支持部分匹配和精确匹配。</li><li id="efac" class="lp lq iq jp b jq ly ju lz jy ma kc mb kg mc kk lu lv lw lx bi translated">还有另一个数据结构(名为L ),它独立于内存m。L用于通过记住内存读取和写入的顺序来提供临时上下文和链接。因此“L”只是一个链表，它允许DNC记住它向“M”读取或写入信息的顺序。</li></ul><p id="fbb9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我发现在论文中提到认知计算/生物合理性很有趣(在这个领域并不常见——20世纪90年代连接主义与计算主义<a class="ae kl" href="https://en.wikipedia.org/wiki/Connectionism#Connectionism_vs._computationalism_debate" rel="noopener ugc nofollow" target="_blank">辩论</a>的遗留物——多次提到DNC和海马体之间的相似性，或者突触如何编码时间上下文和链接。</p><p id="e8a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下图摘自Deepmind的博客文章，清楚地显示了RNN、读写头、N*W内存(M)和编码M，l中的时间关联的链表</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi me"><img src="../Images/f29090126e2e5a0a1a81f862c0583538.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AWk7d6UMzQvqzIfY.png"/></div></div></figure><h1 id="a1c0" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">韦斯顿等人的记忆网络呢？</h1><p id="0366" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">脸书的韦斯顿等人也一直在这一领域努力工作。下图来自他们2016年6月的<a class="ae kl" href="https://arxiv.org/abs/1606.03126" rel="noopener ugc nofollow" target="_blank"> Arxiv论文</a>，这篇论文是追溯到2014年的一系列内存网络工作的最新成果，或许内存组件受到了早期关于<a class="ae kl" href="http://www.thespermwhale.com/jaseweston/papers/wsabie-ijcai.pdf" rel="noopener ugc nofollow" target="_blank"> WSABIE </a>的工作的启发/激励。</p><p id="bf00" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Nature论文更好地阐述了他们解决方案的通用性(涵盖文档分析和理解、对话、图形规划等。)，但这并不一定意味着<em class="md">的方法</em>更好。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi mq"><img src="../Images/d1dbeb09ef4a563f7336eaaf12577c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4ZkV7JqVjWPhV4Cf.png"/></div></div></figure><h1 id="eead" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">影响和相关性</h1><p id="55af" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在我看来，DNCs / RAM代表了自LSTM以来循环架构的最大进步。内存的增加，加上定义明确的机制来区分并训练它，显然提高了rnn执行更复杂任务的能力，如规划，正如关于<a class="ae kl" href="https://research.facebook.com/research/babi/" rel="noopener ugc nofollow" target="_blank"> bAbl数据集</a>或伦敦地铁任务的论文所证明的那样。</p><p id="d6ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">业务应用程序可以充分利用DNC和类似的架构。规划或更好地理解大型文档的能力对于决策支持系统、数据分析、项目管理和信息检索具有重大意义。不难想象，比如ElasticSearch和Solr的DNC插件，或者微软Project Server的DNC版。</p><p id="c8da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将软件支持与对张量中心运算的新兴本机CPU指令集支持相结合，再加上正在进行的GPU改进和TPUs，神经计算的未来将越来越光明。</p><h1 id="ecac" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">未来工作？</h1><p id="a158" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated"><a class="ae kl" href="https://en.wikipedia.org/wiki/SHRDLU" rel="noopener ugc nofollow" target="_blank">wino grad的SHRDLU </a>被广泛认为是人工智能的一个高点，于1972年达到，自那以来没有实质性的改进或复制(<a class="ae kl" href="http://icml.cc/2015/tutorials/icml2015-nlu-tutorial.pdf" rel="noopener ugc nofollow" target="_blank">梁，2015幻灯片100–105</a>)。《自然》杂志文章第三页引用的迷你SHRDLU方块拼图实验是否指向Deepmind的下一个实质性研究领域——提高1972年以来SHRDLU的性能？</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><p id="d92c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="md">最初发表于</em><a class="ae kl" href="http://humphreysheil.com/blog/differentiable-neural-computers-dncs-nature-article-thoughts" rel="noopener ugc nofollow" target="_blank"><em class="md">humphreysheil.com</em></a><em class="md">。</em></p></div></div>    
</body>
</html>