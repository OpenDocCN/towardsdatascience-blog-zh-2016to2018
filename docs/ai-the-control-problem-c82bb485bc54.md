# 人工智能——控制问题

> 原文：<https://towardsdatascience.com/ai-the-control-problem-c82bb485bc54?source=collection_archive---------8----------------------->

当设计一个系统变得更智能、更快，甚至负责我们传统上交给人类的活动时，我们需要建立规则和控制机制，以确保人工智能是安全的，并做我们希望它做的事情。

即使是我们通常不会认为是人工智能的系统，如亚马逊的推荐引擎，如果控制不当，也会产生深远的影响。该系统查看您已经购买或打算购买的商品。然后，它会建议你可能会额外购买的其他物品，这可能会导致一些非常令人惊讶的事情，就像这样:

![](img/e71170c8500f16777edc4913a4f8e21b.png)

想买一段棉绳吗？亚马逊可能会建议你在旁边买一个木凳。作为一个人，我们不会建议这两个项目放在一起。然而，亚马逊的算法发现了购买棉绳的人和购买木凳的人之间的相关性。这是在暗示买绳子的人，他们可能也想要一个凳子，希望能多捞 17.42 英镑。充其量，这似乎是一个不幸的错误。在最坏的情况下，它会促使极度脆弱的人说‘为什么不呢？这种事经常发生吗？“你为什么不把凳子放在你的篮子里，”。

如果这种情况发生在一种推荐算法上，而这种算法是为了向我们追加销售产品而设计的，那么显然这个问题很严重。我们需要找到一种可靠的方法来保证人工智能或自动化系统采取的行动取得积极的结果。

**解决方案？**

**终端值加载**

那么，我们为什么不直接告诉一个 AI 来保护人类的生命呢？这就是艾萨克·阿西莫夫在《我是机器人》中提出的观点。以下是三大定律:

1.  机器人不得伤害人类，也不得坐视人类受到伤害。
2.  机器人必须服从人类给它的命令，除非这些命令与第一定律相冲突。
3.  机器人必须保护自己的存在，只要这种保护不违反第一或第二定律。

它们听起来非常滴水不漏。通过行动或不行动增加无伤害似乎避免了一个反乌托邦，在那里人工智能接管并让人类结束自己。

尽管这些法律听起来很好，但它们不起作用。阿西莫夫写这些定律是为了在小说中使用，当事情出错时，小说会有趣得多。否则我们可能会以一本《从前，结束》而告终。

阿西莫夫提出了第四定律，即“第零定律”。这条额外的规则原本是为了弥补其他三条规则的缺陷，也就是那些让威尔·史密斯过了糟糕一天的规则。我承认，我没读过这本书，但我知道其中一本也不太好。

规则甚至不一定要提到人是一种风险。它们可能是关于一些非常平凡的事情。以 Nick Bostrom 提出的曲别针最大化器的想法为例。这将是一台由假想的未来人类制造的机器，用来管理回形针的制作。回形针只是一种简单的资源，似乎不需要太多的考虑来确保它们的安全，如果我们告诉 AI 它的目的是制造回形针，而这正是它所做的。

但是，如果我们最终拥有一个超级智能系统，超出我们的控制，拥有聚集宇宙资源制造回形针的能力，那会怎么样呢？这个系统的首要任务是把它周围的一切都变成回形针，如果它看到它的创造者试图阻止它达到这个目标，最好的办法就是根除他们。即使它没有决定根除他们，那些人类仍然是由有价值的物质组成的，如果它变成一些回形针会看起来更好，所以把他们变成回形针吧。

我们如何改变终端值？告诉机器做 1000 个回形针而不是把整个宇宙变成回形针？可惜也好不了多少。同一个人工智能可以制作 1000 个回形针，然后继续使用可观察宇宙(我们的宇宙天赋)中的所有资源，以确保它制作了精确的 1000 个回形针，而不是 999 个或 1001 个，并且这些回形针是它的创造者打算让它制作的，并且所有这些回形针都具有满足他们愿望的完美质量。

给一台超级智能机器如此平凡的终值(T1)甚至可能不公平——假设我们找到了一种方法让它的值保持不变，尽管它变得非常智能。

> *我有一颗行星大小的大脑，他们让我拿起一张纸。这叫工作满足感吗？我不知道。*
> 
> *马文——银河系漫游指南，作者道格拉斯·亚当*

TL；DR —终端值似乎不太好用。

**间接规范性**

除了给机器一个终值，我们是否可以间接地暗示我们想要它做什么？

如果我们成功地用终极价值完美地总结了维京时代道德对人类的意义，我们可能会有一个高度重视体力的人工智能。我们可能认为我们今天已经达到了更高的道德标准，但这并不是说 1000 年后我们不会回头看我们正在采取的行动是无知的。过去的暴行发生在人类的时间尺度上，只有人类水平的智慧才能让它们发生。如果用机器来做，速度可能会快几个数量级，而且是不可逆的。

对于间接规范性，我们甚至不试图对终值求和；相反，我们要求一台机器去弄清楚我们想要它做什么。使用类似 Eliezer Yudkowski 的'[连贯推断意志'](https://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition)的东西，它要求人工智能预测如果“如果我们知道得更多，思考得更快，更像我们希望的那样，在一起成长得更远”，我们会希望它做什么

我们不是遵循我们在发布 AI 时的任何道德准则，而是创造一些随着我们的发展而变化的东西，创造我们可能想要的未来，而不是我们今天拥有的更极端的版本。

这个系统和终值加载之间可能还有一些重叠，以及系统会发现的矛盾。如果要求一台机器做对我们最有价值的事情，并奖励它做出比其他任何事情都正确的决定，也许它的决定将是取出我们的大脑，把它们放在培养皿中，并弄清楚我们到底想让它做什么。像“do the intended meaning of this statement”这样的句子似乎可以减少这种担心，但是，要知道我们的意图，机器需要能够预测我们的行为。

一个完美的预测系统看起来很像一集《黑镜》。毫不犹豫地使用一个应用程序来管理你的家庭自动化或寻找你的下一个约会。不知道机器正在模拟成千上万的思维和感觉人类的思想，以准确预测你的欲望和行为，包括那些有知觉的模拟在成千上万次模拟约会中被彼此撕裂时所感觉到的所有痛苦，以衡量你们有多大可能克服所有困难留在一起。

控制问题非常棘手，它要寻找哲学家们几千年来研究未能达成共识的问题的答案。我们必须找到这些问题的答案，不仅仅是在创造超级智能人工智能之前，而是在我们自动化的任何系统中。目前，我们的大部分资源和努力都投入到使这些系统更快、更智能上，只有一小部分专注于控制问题或人工智能和自动化的社会影响。

让我们恢复平衡。

*原载于 2018 年 5 月 24 日*[*【blog.soprasteria.co.uk*](https://blog.soprasteria.co.uk/2018/05/24/ai-the-control-problem/)*。*