<html>
<head>
<title>Deep Neural Networks for Regression Problems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回归问题的深度神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-neural-networks-for-regression-problems-81321897ca33?source=collection_archive---------1-----------------------#2018-09-29">https://towardsdatascience.com/deep-neural-networks-for-regression-problems-81321897ca33?source=collection_archive---------1-----------------------#2018-09-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/2238fc67c29557cdd3cc74cab5740404.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vUKwarc7rCouMzSt0Ksakw.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae kf" href="https://paulvanderlaken.files.wordpress.com/2017/10/1-0flvittznpkh8qkj7upleq.png?w=700" rel="noopener ugc nofollow" target="_blank">Image Source</a></figcaption></figure><p id="5a80" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经网络在分类问题上是众所周知的，例如，它们被用于手写数字分类，但问题是，如果我们将它们用于回归问题，会有成效吗？</p><p id="777d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我将使用一个深度神经网络，通过 Kaggle 的数据集来预测房价。</p><p id="e729" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以从<a class="ae kf" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" rel="noopener ugc nofollow" target="_blank">这里</a>下载数据集</p><p id="75fa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我强烈推荐你试着用我的笔记本在 Google colab [ <a class="ae kf" href="https://colab.research.google.com/drive/1J8ZTI2UIJCwml2nrLVu8Gg0GXEz-7ZK0" rel="noopener ugc nofollow" target="_blank">这里</a> ]上运行代码</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="9e32" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">内容:</h1><blockquote class="mj mk ml"><p id="6b05" class="kg kh mm ki b kj kk kl km kn ko kp kq mn ks kt ku mo kw kx ky mp la lb lc ld im bi translated"><code class="fe mq mr ms mt b">1- Process the dataset<br/>2- Make the deep neural network<br/>3- Train the DNN<br/>4- Test the DNN<br/>5- Compare the result from the DNN to another ML algorithm</code></p></blockquote></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><p id="f23e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">首先，我们将导入所需的依赖项:</strong></p><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="b38a" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">首先:处理数据集</h1><p id="389e" class="pw-post-body-paragraph kg kh it ki b kj na kl km kn nb kp kq kr nc kt ku kv nd kx ky kz ne lb lc ld im bi translated">我们不会深入处理数据集，我们想要做的只是让数据集准备好输入我们的模型。</p><p id="b5b3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将去掉任何具有缺失值的特征，然后我们将对分类特征进行编码，就这样。</p><h2 id="36f5" class="nf lm it bd ln ng nh dn lr ni nj dp lv kr nk nl lz kv nm nn md kz no np mh nq bi translated">加载数据集:</h2><ul class=""><li id="32b7" class="nr ns it ki b kj na kn nb kr nt kv nu kz nv ld nw nx ny nz bi translated">将训练和测试数据加载到 pandas 数据框架中</li><li id="509f" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">组合训练和测试数据以一起处理它们</li></ul><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><pre class="mu mv mw mx gt of mt og oh aw oi bi"><span id="af74" class="nf lm it mt b gy oj ok l ol om">combined.describe()</span></pre><figure class="mu mv mw mx gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi on"><img src="../Images/7f4ce5a7ae69b832e5766abde040cb87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mx0xGJPdknNRZmiZOv_VGg.png"/></div></div></figure><p id="b341" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们定义一个函数来获取没有任何缺失值的列</p><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="24d7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">获取没有任何缺失值的列。</p><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="cfb2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看有多少列</p><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><pre class="mu mv mw mx gt of mt og oh aw oi bi"><span id="e064" class="nf lm it mt b gy oj ok l ol om">[out]:<br/>Number of numerical columns with no nan values : 25 <br/>Number of nun-numerical columns with no nan values : 20</span></pre><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="mu mv mw mx gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oo"><img src="../Images/a4861865cef770505c28fc79d95b4ae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cBbEbSv-BWn1BCJg0c7fPw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Histogram of the features</figcaption></figure><p id="e806" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">特征之间的相关性</strong></p><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="mu mv mw mx gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi op"><img src="../Images/4110e47c2b0a82a25d8bbb78cff5f797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mhgimekf-wVw4gvFQr3Ecg.png"/></div></div></figure><p id="ef51" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面的关联热图中，我们看到大约 15 个特征与目标高度相关。</p><p id="debf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">一个热编码分类特征:</strong></p><p id="87fc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用一个热编码对分类特征进行编码。</p><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><pre class="mu mv mw mx gt of mt og oh aw oi bi"><span id="d61a" class="nf lm it mt b gy oj ok l ol om">[out]:<br/>There were 45 columns before encoding categorical features <br/>There are 149 columns after encoding categorical features</span></pre><p id="ba51" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，将组合数据帧拆分为训练数据和测试数据</p><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="19eb" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">第二:制作深度神经网络</h1><ul class=""><li id="a338" class="nr ns it ki b kj na kn nb kr nt kv nu kz nv ld nw nx ny nz bi translated">定义顺序模型</li><li id="122e" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">添加一些密集层</li><li id="3f9e" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">使用'<strong class="ki iu"> relu </strong>'作为隐藏层的激活功能</li><li id="4b9a" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">使用一个'<strong class="ki iu">普通的</strong>'初始化器作为内核初始化器</li></ul><blockquote class="oq"><p id="b900" class="or os it bd ot ou ov ow ox oy oz ld dk translated"><code class="fe mq mr ms mt b">Initializers define the way to set the initial random weights of Keras layers.</code></p></blockquote><ul class=""><li id="ec1e" class="nr ns it ki b kj pa kn pb kr pc kv pd kz pe ld nw nx ny nz bi translated">我们将使用平均绝对误差作为损失函数</li><li id="c1a1" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">定义只有一个节点的输出层</li><li id="0b4f" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">使用'<strong class="ki iu"> linear </strong>'作为输出层的激活函数</li></ul><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><pre class="mu mv mw mx gt of mt og oh aw oi bi"><span id="3c6c" class="nf lm it mt b gy oj ok l ol om">[Out]:<br/>_________________________________________________________________ Layer (type)                 Output Shape              Param #    ================================================================= dense_1 (Dense)              (None, 128)               19200      _________________________________________________________________ dense_2 (Dense)              (None, 256)               33024      _________________________________________________________________ dense_3 (Dense)              (None, 256)               65792      _________________________________________________________________ dense_4 (Dense)              (None, 256)               65792      _________________________________________________________________ dense_5 (Dense)              (None, 1)                 257        ================================================================= Total params: 184,065 Trainable params: 184,065 Non-trainable params: 0 _________________________________________________________________</span></pre><p id="508e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">定义检查点回调:</strong></p><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="72d7" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">第三:训练模型:</h1><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><pre class="mu mv mw mx gt of mt og oh aw oi bi"><span id="a0e5" class="nf lm it mt b gy oj ok l ol om">[out]:<br/>Train on 1168 samples, validate on 292 samples<br/>Epoch 1/500<br/>1168/1168 [==============================] - 0s 266us/step - loss: 19251.8903 - mean_absolute_error: 19251.8903 - val_loss: 23041.8968 - val_mean_absolute_error: 23041.8968  <br/>Epoch 00001: val_loss did not improve from 21730.93555 <br/>Epoch 2/500<br/> 1168/1168 [==============================] - 0s 268us/step - loss: 18180.4985 - mean_absolute_error: 18180.4985 - val_loss: 22197.7991 - val_mean_absolute_error: 22197.7991  <br/>Epoch 00002: val_loss did not improve from 21730.93555<br/>.<br/>.<br/>.<br/>Epoch 00500: val_loss did not improve from 18738.19831</span><span id="7471" class="nf lm it mt b gy pf ok l ol om">&lt;keras.callbacks.History at 0x7f4324aa80b8&gt;</span></pre><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="e5a2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们看到最佳模型的验证损失是 18738.19</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="a81b" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">第四:测试模型</h1><p id="0411" class="pw-post-body-paragraph kg kh it ki b kj na kl km kn nb kp kq kr nc kt ku kv nd kx ky kz ne lb lc ld im bi translated">我们将把测试数据上的预测提交给 Kaggle，看看我们的模型有多好。</p><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="mu mv mw mx gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pg"><img src="../Images/2207829799c5b66ff347bb7491a7ed7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mXbUrGB9yB9RrBscwugP9g.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">The result of the deep neural network’s submission on Kaggle</figcaption></figure><p id="2306" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一点也不差，再做一些预处理和更多的训练，我们可以做得更好。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="0276" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">第五:尝试另一种 ML 算法:</h1><p id="cd60" class="pw-post-body-paragraph kg kh it ki b kj na kl km kn nb kp kq kr nc kt ku kv nd kx ky kz ne lb lc ld im bi translated">现在，让我们尝试另一种 ML 算法来比较结果。</p><p id="d5b5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用随机森林回归器和 xgb 回归器。</p><p id="2674" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">将训练数据分割成训练和验证数据</strong></p><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="77f8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">我们先试试随机森林模式:</strong></p><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><pre class="mu mv mw mx gt of mt og oh aw oi bi"><span id="58d7" class="nf lm it mt b gy oj ok l ol om">Random forest validation MAE =  19089.71589041096</span></pre><p id="b555" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">制作提交文件，提交给 Kaggle 看结果:</strong></p><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="mu mv mw mx gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ph"><img src="../Images/ba11994d58f41903a8d3739f18acfbb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wcJqKzBsbLARqMjwvv0Xjw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">The result of random forest’s submission on Kaggle</figcaption></figure><p id="ce68" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">现在，让我们试试 XGBoost 模型:</strong></p><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><pre class="mu mv mw mx gt of mt og oh aw oi bi"><span id="b94a" class="nf lm it mt b gy oj ok l ol om">[out]: <br/>XGBoost validation MAE =  17869.75410958904</span></pre><p id="6988" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">制作一个提交文件，提交给 Kaggle 看结果:</strong></p><figure class="mu mv mw mx gt ju"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="mu mv mw mx gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ph"><img src="../Images/fcef6452ef101fc7b3570cd899a63a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PO0jxykz1hv-aSN5kkjItg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">The result of XGBoost’s submission on Kaggle</figcaption></figure></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><p id="ea9b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这不是一个惊喜吗，我真的不认为神经网络会击败随机森林和 XGBoost 算法，但让我们试着不要太乐观，记住我们没有在随机森林和 XGBoost 模型上配置任何超参数，我相信如果我们这样做，这两个模型会超过神经网络。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h2 id="d320" class="nf lm it bd ln ng nh dn lr ni nj dp lv kr nk nl lz kv nm nn md kz no np mh nq bi translated">总结一下我们所做的事情:</h2><ul class=""><li id="5be0" class="nr ns it ki b kj na kn nb kr nt kv nu kz nv ld nw nx ny nz bi translated">我们加载并处理数据集</li><li id="fd72" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">我们通过绘制一些直方图和特征的相关热图来熟悉数据集</li><li id="9bba" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">我们使用了具有三个隐藏层的深度神经网络，每个隐藏层具有 256 个节点。</li><li id="30eb" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">我们在输出层使用了线性激活函数</li><li id="5a44" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">我们训练了模型，然后在 Kaggle 上测试。</li><li id="8377" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">我们还测试了另外两个模型</li><li id="a69f" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">我们的深度神经网络能够超越这两个模型</li><li id="b335" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">我们相信，如果我们调整它们的超参数，这两个模型可以击败深度神经网络模型。</li></ul><h1 id="57a7" class="ll lm it bd ln lo pi lq lr ls pj lu lv lw pk ly lz ma pl mc md me pm mg mh mi bi translated">后续步骤:</h1><ul class=""><li id="8cc6" class="nr ns it ki b kj na kn nb kr nt kv nu kz nv ld nw nx ny nz bi translated">尝试在处理数据集上投入更多精力</li><li id="ed34" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">尝试其他类型的神经网络</li><li id="6b3e" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">尝试调整我们使用的两个模型的超参数</li><li id="93a9" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated">如果你真的想在回归问题上做得更好，遵循这个教程:</li></ul><p id="f01b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[ <a class="ae kf" href="https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard" rel="noopener ugc nofollow" target="_blank">累积回归:排行榜前 4% | ka ggle</a>]</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="025f" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">参考资料:</h1><ul class=""><li id="cf33" class="nr ns it ki b kj na kn nb kr nt kv nu kz nv ld nw nx ny nz bi translated"><a class="ae kf" href="https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/" rel="noopener ugc nofollow" target="_blank">Python 中 Keras 深度学习库回归教程</a></li></ul><p id="a286" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以在推特上关注我<a class="ae kf" href="https://twitter.com/ModMaamari" rel="noopener ugc nofollow" target="_blank"> @ModMaamari </a></p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h2 id="ccbe" class="nf lm it bd ln ng nh dn lr ni nj dp lv kr nk nl lz kv nm nn md kz no np mh nq bi translated">您可能还喜欢:</h2><ul class=""><li id="f420" class="nr ns it ki b kj na kn nb kr nt kv nu kz nv ld nw nx ny nz bi translated"><a class="ae kf" href="https://blog.goodaudience.com/ai-generates-taylor-swifts-song-lyrics-6fd92a03ef7e" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu"> AI 生成泰勒斯威夫特的歌词</strong> </a></li><li id="9674" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated"><a class="ae kf" href="https://medium.com/datadriveninvestor/introduction-to-random-forest-algorithm-with-python-9efd1d8f0157" rel="noopener"> <strong class="ki iu">用 Python 介绍随机森林算法</strong> </a></li><li id="91e6" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated"><a class="ae kf" href="https://medium.com/@mamarih1/machine-learning-crash-course-with-tensorflow-apis-summary-524e0fa0a606" rel="noopener"> <strong class="ki iu">带 TensorFlow APIs 的机器学习速成班汇总</strong> </a></li><li id="5133" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated"><a class="ae kf" href="https://medium.com/@mamarih1/how-to-make-a-cnn-using-tensorflow-and-keras-dd0aaaed8ab4" rel="noopener"> <strong class="ki iu">如何用 Tensorflow 和 Keras </strong> </a> <strong class="ki iu">制作一个 CNN？</strong></li><li id="086b" class="nr ns it ki b kj oa kn ob kr oc kv od kz oe ld nw nx ny nz bi translated"><a class="ae kf" href="https://medium.com/@mamarih1/how-to-choose-the-best-machine-learning-model-e1dbb46bdd4d" rel="noopener"> <strong class="ki iu">如何选择最好的机器学习模型？</strong>T9】</a></li></ul></div></div>    
</body>
</html>