<html>
<head>
<title>Cat or Dog? Introduction to Naive Bayes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">猫还是狗？朴素贝叶斯简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cat-or-dog-introduction-to-naive-bayes-c507f1a6d1a8?source=collection_archive---------14-----------------------#2018-11-19">https://towardsdatascience.com/cat-or-dog-introduction-to-naive-bayes-c507f1a6d1a8?source=collection_archive---------14-----------------------#2018-11-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/0a2a42efaa1b1595739717976109c29c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KuHx0sD046Qa97SEmmADPg.jpeg"/></div></div></figure><p id="ea1e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你读过任何关于机器学习的介绍性书籍或文章，你可能会偶然发现<a class="ae kw" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Semi-supervised_parameter_estimation" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯</a>。这是一种通俗易懂的分类算法。</p><p id="991b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们退一步，解开朴素贝叶斯。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="9e98" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">简而言之，朴素贝叶斯就是</p><ul class=""><li id="0c7c" class="le lf iq ka b kb kc kf kg kj lg kn lh kr li kv lj lk ll lm bi translated">分类算法</li><li id="af5e" class="le lf iq ka b kb ln kf lo kj lp kn lq kr lr kv lj lk ll lm bi translated">监督学习算法</li><li id="57dc" class="le lf iq ka b kb ln kf lo kj lp kn lq kr lr kv lj lk ll lm bi translated">概率分类器</li></ul><p id="934d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">朴素贝叶斯是一种分类算法</strong>。一个复杂的名字来说，给定一个例子，朴素贝叶斯能够给它分配一个类别，就像给它贴上一个标签说<em class="ls">猫</em>或<em class="ls">狗</em>，如果它<em class="ls">看到</em>一只猫的图像或一只狗的图像。</p><p id="6b07" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">它也是被称为<strong class="ka ir">监督学习算法的算法家族的一部分。</strong>这是<em class="ls">通过查看正确分类的示例来学习</em>的算法类型。按照机器学习的说法，每个例子都是一组<em class="ls">特征</em>，即描述特定例子的属性。该算法用来学习的一组例子被称为<em class="ls">训练集</em>，它用来检查其分类能力的新的和从未见过的例子被称为<em class="ls">测试集。</em>算法最终向其分配一个<em class="ls">类</em>或<em class="ls">标签。</em></p><p id="f5c9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">朴素贝叶斯也是一个概率分类器。算法<em class="ls">学习</em>来预测的类或标签是创建它所显示的所有类的<a class="ae kw" href="https://en.wikipedia.org/wiki/Probability_distribution" rel="noopener ugc nofollow" target="_blank">概率分布</a>的结果，然后决定将哪个分配给每个示例。概率分类器查看<a class="ae kw" href="https://en.wikipedia.org/wiki/Conditional_probability_distribution" rel="noopener ugc nofollow" target="_blank">条件概率分布</a>，即在给定一组特定特征的情况下分配一个类别的概率。</p><p id="28a4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">假设你有这个(微小的)数据集，将动物分为两类:猫和狗。</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lt"><img src="../Images/726eae61b01d77e6799bc7cd0d5b9819.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mfwgjXaqx_09PZSQSIpWsQ.png"/></div></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">A minuscule, dummy dataset 😁</figcaption></figure><p id="fd93" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果该算法要计算出将哪个类分配给与上述类似的示例，即预测其类，它将考虑两件事:</p><p id="7749" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">a)假设毛发颜色为黑色，体长为 18 英寸，身高为 9.2，体重为 8.1 磅，…</p><p id="61eb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">b)在给定相同特征的情况下，成为狗的概率。</p><p id="442c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一般来说，条件概率是 P(类别|特征集)。在我们的例子中，classes = {cat，dog}和 feature set = {hair color，body length，height，weight，ear length，爪子}。</p><p id="3a5a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">朴素贝叶斯将计算所有类的条件概率，因为它是朴素的，<strong class="ka ir">它将假设每个特征都是相互独立的</strong>。它将假设任何特征之间没有相关性，因此，它们对预测类的贡献不受其他特征的影响。</p><p id="3c8d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">名字说明了一切。它使用<a class="ae kw" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯定理</a>来计算条件概率</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/f597681d67f5af86976fb257cb91aee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*Y2vxqzcjtDyLm-RqqG-gcw.jpeg"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Bayes Theorem</figcaption></figure><p id="ec9c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">概率 P( <em class="ls">类</em> | <em class="ls">特征集</em>)也被称为<em class="ls">后验</em>，即在考虑所有给定条件后，事实发生后的概率。它是给定一组可以在猫身上观察到的特征(属性)，将一幅图像分类为一只猫的图像的概率。P( <em class="ls">类</em>)被称为<em class="ls">先验</em>，因为它是你事先知道的所有信息，是猫还是狗的概率。P( <em class="ls">特征集</em>)被称为<em class="ls">证据</em>，因为它是你所观察到的概率，特征集。而 P( <em class="ls">特征集</em> | <em class="ls">类</em>)被称为<em class="ls">可能性</em>，意思是给定这个特定的特征集，这是一个猫的图像的可能性有多大。</p><p id="2b56" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">计算完所有类别的概率后，就该做决定了。</p><h2 id="63ac" class="md me iq bd mf mg mh dn mi mj mk dp ml kj mm mn mo kn mp mq mr kr ms mt mu mv bi translated">我应该给这个例子分配什么类呢？</h2><p id="dc09" class="pw-post-body-paragraph jy jz iq ka b kb mw kd ke kf mx kh ki kj my kl km kn mz kp kq kr na kt ku kv ij bi translated">将一个类别分配给一个新的、从未见过的特征集的决策是一个<a class="ae kw" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank"> <em class="ls">最大后验决策</em> </a>。一种是估计哪个<em class="ls">后验</em>概率，例如 P(特征集|猫)或 P(特征集|狗)，将最大化看到该特定类的实例的可能性。</p><p id="7256" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一般来说，你有</p><figure class="lu lv lw lx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/4dd355c1894028c9c48a4c3400311c02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dyv3bN6AHFezvJFMHE9i2A.jpeg"/></div></div></figure><p id="46ea" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">左手边那个戴着<em class="ls">帽子的</em>就是你的预测。</p><p id="8a7b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该算法将挑选具有较高概率的类。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="cec6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">总之，朴素贝叶斯利用贝叶斯定理来<em class="ls">学习</em>最能描述<em class="ls">训练集</em>中提供的示例的特征。它依靠最大后验决策规则来预测<em class="ls">测试集</em>中提供的每个新的、以前从未见过的示例的类别。</p><p id="c8c4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">主要优点是</strong></p><ul class=""><li id="cec2" class="le lf iq ka b kb kc kf kg kj lg kn lh kr li kv lj lk ll lm bi translated">简单</li><li id="3c5a" class="le lf iq ka b kb ln kf lo kj lp kn lq kr lr kv lj lk ll lm bi translated">需要小的训练集</li><li id="0d18" class="le lf iq ka b kb ln kf lo kj lp kn lq kr lr kv lj lk ll lm bi translated">计算速度快</li><li id="1c74" class="le lf iq ka b kb ln kf lo kj lp kn lq kr lr kv lj lk ll lm bi translated">与特征和训练示例的数量成线性比例</li><li id="6bc0" class="le lf iq ka b kb ln kf lo kj lp kn lq kr lr kv lj lk ll lm bi translated">处理二进制(猫，不是猫)和多类分类(猫，狗，老鼠)</li></ul><p id="b686" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">缺点</strong></p><ul class=""><li id="32f3" class="le lf iq ka b kb kc kf kg kj lg kn lh kr li kv lj lk ll lm bi translated">在现实世界中很少成立的强特征独立性假设。记住，这是天真的</li><li id="bd35" class="le lf iq ka b kb ln kf lo kj lp kn lq kr lr kv lj lk ll lm bi translated">基于其独立性假设，可能提供较差的估计</li></ul><p id="17cc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ls">感谢阅读！</em></p></div></div>    
</body>
</html>