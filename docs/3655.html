<html>
<head>
<title>How are Logistic Regression &amp; Ordinary Least Squares Regression (Linear Regression) Related? Why the “Regression” in Logistic?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归和普通最小二乘回归(线性回归)有什么关系？为什么会出现逻辑上的“回归”？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-are-logistic-regression-ordinary-least-squares-regression-related-1deab32d79f5?source=collection_archive---------3-----------------------#2018-06-05">https://towardsdatascience.com/how-are-logistic-regression-ordinary-least-squares-regression-related-1deab32d79f5?source=collection_archive---------3-----------------------#2018-06-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ccbe80e09c4f9f4a1487b582dbe60b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k2bLmeYIG7z7dCyxADedhQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="https://cdn-images-1.medium.com/max/1436/1*_TqRJ9SmwFzRigJhMiN2uw.png" rel="noopener">https://cdn-images-1.medium.com/max/1436/1*_TqRJ9SmwFzRigJhMiN2uw.png</a></figcaption></figure><p id="66ba" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你和我一样被“逻辑回归”中的“回归”所困扰，而逻辑回归实际上应该被称为“逻辑分类”，考虑到它确实分类，我对你的困扰有一个答案！</p><h1 id="7d5a" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">逻辑回归和普通最小二乘回归(又名线性回归)简介:</h1><p id="68f7" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir">逻辑回归</strong>适用于根据一组预测变量的值预测某个特征或结果是否存在的情况。它类似于线性回归模型，但适用于因变量为二分变量的模型。它的系数可以用来估计模型中每个独立变量的奇数比率。它比判别分析适用于更广泛的研究情况。另一方面，逻辑回归用于确定事件的概率，该事件以二进制格式捕获，即 0 或 1。</p><h2 id="70af" class="me lc iq bd ld mf mg dn lh mh mi dp ll ko mj mk lp ks ml mm lt kw mn mo lx mp bi translated">如你所知，使用逻辑回归，多类分类是可能的，而不仅仅是二元分类。但 logistic 回归多用于二元分类。</h2><p id="1ade" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir">线性回归</strong>又名最小二乘回归，估计线性方程的系数，涉及一个或多个自变量，能最好地预测因变量的值。例如，可以根据年龄、教育程度和经验年限等独立变量来预测销售人员的年总销售额(因变量)。</p><p id="ac5d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">线性回归是连续的，而逻辑回归是离散的。</p><p id="5b90" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里有更多关于<a class="ae kc" href="https://hackernoon.com/continuous-vs-discrete-variables-in-the-context-of-machine-learning-15d9005e2525" rel="noopener ugc nofollow" target="_blank">连续变量和离散变量的内容。</a></p><h1 id="7ba3" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">它们有什么特别的联系？</h1><ul class=""><li id="d15b" class="mq mr iq kf b kg lz kk ma ko ms ks mt kw mu la mv mw mx my bi translated">逻辑回归估计结果的概率。事件被编码为二进制变量，值 1 表示目标结果的出现，值 0 表示不出现。</li><li id="ce38" class="mq mr iq kf b kg mz kk na ko nb ks nc kw nd la mv mw mx my bi translated">最小二乘回归也可以使用线性概率模型来模拟二元变量。最小二乘回归可能会给出超出范围(0，1)的预测值，但该分析仍可用于分类和假设检验。</li><li id="f48e" class="mq mr iq kf b kg mz kk na ko nb ks nc kw nd la mv mw mx my bi translated">逻辑回归模型将事件的概率估计为独立变量的函数。设 y 表示情况 I 的因变量上的一个值，同样情况下 k 个自变量的值表示为 x (j = l，k)。假设 Y 是一个二元变量，用来衡量某个群体的成员关系。如果情况 I 是该组的成员，编码 y = 1，否则编码 0，那么让 p = y = 1 的概率。y = 1 的几率由 p/(l-p)给出。p 的对数概率或 logit 等于 p/(l-p)的自然对数。逻辑回归将对数优势估计为独立变量的线性组合</li></ul><p id="83d6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">logit(p) =B0 + B1X1 + B2X2+ ……。+ BkXk</p><p id="c532" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(0、1、2、k 都是在时<a class="ae kc" href="https://help.medium.com/hc/en-us/articles/226314627-How-can-I-add-superscript-" rel="noopener">缺乏媒介订阅能力的下标)</a></p><ul class=""><li id="e268" class="mq mr iq kf b kg kh kk kl ko ne ks nf kw ng la mv mw mx my bi translated">最小二乘回归对因变量和自变量集合之间的关系进行建模。因变量的值被定义为自变量加上误差项ϵ.的线性组合</li></ul><p id="dd8b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Y = B0 + B1X1 + B2X2+ ……。+ BkXk + ϵ</p><p id="1588" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(0，1，2，k 都是下标)</p><p id="d70b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中(B0 … Bk)是回归系数，Xs 是独立变量的列向量，e 是预测误差的向量</p><ul class=""><li id="6c2f" class="mq mr iq kf b kg kh kk kl ko ne ks nf kw ng la mv mw mx my bi translated">逻辑回归应用于二元因变量的建模。逻辑回归模型的结构是为二元结果设计的。</li><li id="6495" class="mq mr iq kf b kg mz kk na ko nb ks nc kw nd la mv mw mx my bi translated">最小二乘回归不是为二元分类而构建的，因为与最小二乘回归相比，逻辑回归在分类数据点方面表现更好，并且具有更好的对数损失函数。</li></ul><p id="6e7a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">它们在哪里排列在一起:</strong></p><p id="f9ac" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">线性回归使用一般线性</strong>方程 Y=b0+∑(biXi)+ϵ，其中 y 是连续的因变量，自变量 Xi 通常是连续的(但也可以是二元的，例如当线性模型用于 t-检验时)或其他离散域。ϵ是模型无法解释的方差的术语，通常被称为“误差”。用 Yj 表示的独立相关值可以通过稍微修改等式来求解:</p><p id="fa98" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Yj=b0+∑(biXij)+ϵj</p><p id="bdc1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(j，0，I，j 都是下标，具有与所解释的相同的表示)</p><p id="39ec" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">线性回归的输出如下所示:</p><figure class="ni nj nk nl gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/27eec70e65702479743f55036b6002bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*OEPyTDINRK3sxd4zXWX2Ew.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/400px-Linear_regression.svg.png" rel="noopener ugc nofollow" target="_blank">https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/400px-Linear_regression.svg.png</a></figcaption></figure><p id="52ba" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">逻辑回归是另一种广义线性模型</strong> (GLM)程序，使用相同的基本公式，但它不是连续的 Y，而是回归分类结果的概率。简单来说，这意味着我们只考虑一个结果变量和该变量的两种状态——0 或 1。</p><p id="dec9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Y=1 的概率公式如下:</p><p id="e347" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">p(y = 1)= 1<strong class="kf ir">/</strong>(1+e−^(b0+∑(bixi)))</p><p id="a440" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(0，我都是下标)</p><p id="2964" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中术语与先前解释的相同。</p><p id="173b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自变量 Xi 可以是连续的，也可以是二进制的。回归系数 bi 可以被指数化，以给出 Xi 每改变一次 Y 的几率的改变。</p><p id="7a61" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出是一条如下所示的 s 形曲线:</p><figure class="ni nj nk nl gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/79029d07d8ec9f1ecf88a4b829120d13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HXCBO-Wx5XhuY_OwMl0Phw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png" rel="noopener ugc nofollow" target="_blank">https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png</a></figcaption></figure><p id="84cf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">两者都是线性模型，然而:</strong></p><p id="b8c3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">逻辑回归本身绝对不是一种分类算法。只有分类算法与决策规则相结合，才能使结果的预测概率二分法。逻辑回归是一种回归模型，因为它将类成员的概率估计为要素的多元线性函数(的变换)。</p><p id="35dc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">我们可能会问，为什么它被称为“逻辑回归”，而不是“逻辑分类”？</strong></p><p id="2d53" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要回答这个问题，我们必须追溯到 19 世纪，在那里逻辑回归找到了它的目的。它被广泛用于发现种群的增长和自催化化学反应的过程，如这里的<a class="ae kc" href="http://papers.tinbergen.nl/02119.pdf" rel="noopener ugc nofollow" target="_blank">所示</a>。</p><p id="df6e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样需要明确的是，正如一些专家指出的那样,“逻辑回归”这个名字远在任何“监督学习”出现之前就被创造出来了。<strong class="kf ir">此外，术语“回归”并不意味着结果总是连续的，正如本文</strong>  <strong class="kf ir">中指出的</strong> <a class="ae kc" href="http://papers.tinbergen.nl/02119.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">。所以，并不是每个“回归”都是连续变量预测。</strong></a></p><p id="6db1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">线性回归通常通过最小化模型对数据的最小二乘误差来解决，因此大的误差被平方惩罚。逻辑回归正好相反。</p><p id="ffc0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">仅供参考:以下是线性回归的损失函数:</p><figure class="ni nj nk nl gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nn"><img src="../Images/464b077a1e93c7b3ca109d15f691fdfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*wOtaeC2YkkWfj57n5kV_tg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="https://www.researchgate.net/profile/Alexandros_Karatzoglou/publication/221515860/figure/fig1/AS:339586132791298@1457975051470/Figure-1-Mean-Squared-Error-formula-used-to-evaluate-the-user-model.ppm" rel="noopener ugc nofollow" target="_blank">https://www.researchgate.net/profile/Alexandros_Karatzoglou/publication/221515860/figure/fig1/AS:339586132791298@1457975051470/Figure-1-Mean-Squared-Error-formula-used-to-evaluate-the-user-model.ppm</a></figcaption></figure><p id="9b97" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用逻辑损失函数会导致较大的误差被罚为渐近常数。</p><p id="e34c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑对分类{0，1}结果的线性回归，看看为什么这是一个问题。如果模型预测当真值为 1 时结果为 67，没有太大损失。线性回归会试图减少 67，而逻辑回归不会(同样多)，这意味着，对这一连续产出使用逻辑回归不会解释更多的损失。它认为损失并不大，换句话说，逻辑回归并不惩罚使“最佳拟合线”根本不是“最佳拟合线”的损失。</p><p id="c310" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">逻辑回归的结果在许多方面可以与最小二乘回归的结果相比，但它能更准确地预测相关结果的概率。最小二乘回归在从因变量预测连续值方面是准确的。</p><p id="d98b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上所述，重要的是要知道“回归”是一个抽象的术语。它根据上下文有不同的解释。</p><p id="1c19" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个对比图像:</p><figure class="ni nj nk nl gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/6e13ffcee3cab11a678383ca78169678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YNZEHUdHspzkOpCsPu_E9g.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="http://slideplayer.com/slide/6183997/18/images/8/Linear+versus+Logistic+Regression.jpg" rel="noopener ugc nofollow" target="_blank">http://slideplayer.com/slide/6183997/18/images/8/Linear+versus+Logistic+Regression.jpg</a></figcaption></figure><h1 id="788c" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">最后，让我们来看看每个属性:</h1><h1 id="42f5" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">产出:</h1><p id="862c" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir">线性回归:</strong>连续值【2 个以上输出】。</p><p id="9f00" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> Logistic 回归:</strong>离散值。通常是 2 个输出{0，1}。输出是在舍入到最接近的值 0 或 1 后得出的。请记住，多类是允许的。</p><h1 id="821e" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">解释系数:</h1><p id="53ab" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir">线性回归:</strong>线性回归系数代表预测变量变化一个单位的响应变量的平均变化，同时保持模型中其他预测变量不变。换句话说，保持所有其他变量不变，这个变量增加一个单位，因变量预计会增加或减少某个值 x。</p><p id="f7cb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">逻辑回归:</strong>解释逻辑回归系数需要解释概率，而概率本身就是另一个话题。然而，这里有一个<a class="ae kc" href="https://www.youtube.com/watch?v=eX2sY2La4Ew" rel="noopener ugc nofollow" target="_blank">直观的解释</a>。</p><h1 id="deb9" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">方程式:</h1><p id="b3ef" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir">线性回归:</strong>线性回归是一种对两个变量之间的关系进行建模的方法。你可能还会把这个方程看作<strong class="kf ir">斜率公式</strong>。该等式的形式为<strong class="kf ir"> Y=a+bX </strong>，其中 Y 是因变量(Y 轴上的变量)，X 是自变量(即，它绘制在 X 轴上)，b 是直线的斜率，a 是 Y 截距。</p><figure class="ni nj nk nl gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/bde8d335f523e7e438545e012e4dd594.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*H4b9Mq5n3cAzAyTmCVmoPA.jpeg"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Simple linear regression equation : <a class="ae kc" href="http://www.statisticshowto.com/probability-and-statistics/regression-analysis/find-a-linear-regression-equation/" rel="noopener ugc nofollow" target="_blank">http://www.statisticshowto.com/probability-and-statistics/regression-analysis/find-a-linear-regression-equation/</a></figcaption></figure><p id="2803" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总有一个误差项，又名剩余项<strong class="kf ir"> ϵ </strong>，如图所示:</p><figure class="ni nj nk nl gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ccbe80e09c4f9f4a1487b582dbe60b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k2bLmeYIG7z7dCyxADedhQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="https://cdn-images-1.medium.com/max/1436/1*_TqRJ9SmwFzRigJhMiN2uw.png" rel="noopener">https://cdn-images-1.medium.com/max/1436/1*_TqRJ9SmwFzRigJhMiN2uw.png</a></figcaption></figure><p id="645c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">逻辑回归:</strong>逻辑回归使用一个等式作为表示，非常类似于线性回归。输入值(<em class="nq"> x </em>)使用权重或系数值(称为希腊大写字母，beta)进行线性组合，以预测输出值(<em class="nq"> y </em>)。与线性回归的一个关键区别是，被建模的输出值是二进制值(<em class="nq"> 0 </em>或<em class="nq"> 1 </em>)，而不是数值<a class="ae kc" href="https://www.safaribooksonline.com/library/view/ensemble-machine-learning/9781788297752/e2d207ff-3690-4e74-9663-2d946e2a7a1c.xhtml" rel="noopener ugc nofollow" target="_blank">(来自 Safari 在线图书)</a>。</p><figure class="ni nj nk nl gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/c36666023b71fe825fbbe3e6b6a269b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rlOfJfHOsbo3FfnXf_fxgg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Example for logistic regression: <a class="ae kc" href="https://www.safaribooksonline.com/library/view/ensemble-machine-learning/9781788297752/e2d207ff-3690-4e74-9663-2d946e2a7a1c.xhtml" rel="noopener ugc nofollow" target="_blank">https://www.safaribooksonline.com/library/view/ensemble-machine-learning/9781788297752/e2d207ff-3690-4e74-9663-2d946e2a7a1c.xhtml</a></figcaption></figure><h1 id="9a48" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">误差方程:</h1><h2 id="7176" class="me lc iq bd ld mf mg dn lh mh mi dp ll ko mj mk lp ks ml mm lt kw mn mo lx mp bi translated">线性回归:均方误差:</h2><figure class="ni nj nk nl gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nn"><img src="../Images/64db6ae2c805fe2541505598e0d16587.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*PibgmwZ-0qS12MvX6pN5Bw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="https://www.researchgate.net/profile/Alexandros_Karatzoglou/publication/221515860/figure/fig1/AS:339586132791298@1457975051470/Figure-1-Mean-Squared-Error-formula-used-to-evaluate-the-user-model.ppm" rel="noopener ugc nofollow" target="_blank">https://www.researchgate.net/profile/Alexandros_Karatzoglou/publication/221515860/figure/fig1/AS:339586132791298@1457975051470/Figure-1-Mean-Squared-Error-formula-used-to-evaluate-the-user-model.ppm</a></figcaption></figure><p id="a649" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">逻辑回归:</strong>逻辑回归的输出是概率，这些概率随后被分类。底线是，你不能像以前看到的那样用逻辑回归做线性回归。</p><p id="48e4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">逻辑回归的通常成本或损失函数也称为误差方程，被称为“分类交叉熵”,如神经网络中所见。</p><figure class="ni nj nk nl gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/15b46e2b2d02c5176a55c453e4f07e5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n1T0iYxmckzLGMMpRH6TuA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Binary outcomes loss function aka binary categorical cross entropy (BCE): <a class="ae kc" href="http://cat.birdhabitat.site/categorical-cross-entropy-loss-formula/" rel="noopener ugc nofollow" target="_blank">http://cat.birdhabitat.site/categorical-cross-entropy-loss-formula/</a></figcaption></figure><p id="77dd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，由于这是一个分类，这里是来自<a class="ae kc" href="http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics" rel="noopener ugc nofollow" target="_blank"> sklearn 的分类常用指标。</a></p><h1 id="6fea" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">线性关系:</h1><p id="fd3f" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir">线性回归:</strong>需要因变量和自变量之间的线性关系。</p><p id="492e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> Logistic 回归:</strong>因变量和自变量之间不需要线性关系。</p><h1 id="bbf0" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">剩余分配:</h1><p id="4bf8" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir">线性回归:</strong>要求误差项呈正态分布。</p><p id="97c9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> Logistic 回归:</strong>不要求误差项呈正态分布。</p><p id="b5a5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然我们知道了线性回归和逻辑回归之间的关系。</p><p id="94e0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你喜欢这篇文章，那就鼓掌吧！:)也许一个跟随？</p><p id="88b1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在社交网站上与我联系:</p><div class="nt nu gp gr nv nw"><a href="https://www.linkedin.com/in/rakshith-vasudev/" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd ir gy z fp ob fr fs oc fu fw ip bi translated">Rakshith Vasudev | LinkedIn</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">查看拉克什特·瓦苏德夫在全球最大的职业社区 LinkedIn 上的个人资料。拉克什特教育上市…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">www.linkedin.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok jw nw"/></div></div></a></div><div class="nt nu gp gr nv nw"><a href="https://www.facebook.com/imrakshithvasudev/" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd ir gy z fp ob fr fs oc fu fw ip bi translated">拉克什特·瓦苏德夫</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">拉克什·瓦苏德夫。和我一起学习人工智能，让这个世界变得更美好。张量流…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">www.facebook.com</p></div></div><div class="of l"><div class="ol l oh oi oj of ok jw nw"/></div></div></a></div><div class="nt nu gp gr nv nw"><a href="https://www.youtube.com/c/rakshithvasudev" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd ir gy z fp ob fr fs oc fu fw ip bi translated">拉克什特·瓦苏德夫</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">Datascience 入门，最佳编程实践。主题包括机器学习和其他。</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">www.youtube.com</p></div></div><div class="of l"><div class="om l oh oi oj of ok jw nw"/></div></div></a></div><h1 id="43d4" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">来源:</h1><p id="2edb" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><a class="ae kc" href="http://papers.tinbergen.nl/02119.pdf" rel="noopener ugc nofollow" target="_blank">http://papers.tinbergen.nl/02119.pdf</a></p><p id="9c28" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://pdfs.semanticscholar.org/5a20/ff2760311af589617ba1b82192aa42de4e08.pdf" rel="noopener ugc nofollow" target="_blank">https://pdfs . semantic scholar . org/5a 20/ff 2760311 af 589617 ba 1b 82192 aa 42 de 4 e 08 . pdf</a></p><p id="db92" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://stats.stackexchange.com/questions/29325/what-is-the-difference-between-linear-regression-and-logistic-regression" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/29325/线性回归和逻辑回归的区别是什么</a></p><p id="6477" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://stats.stackexchange.com/questions/24904/least-squares-logistic-regression" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/24904/least-squares-logistic-regression</a></p><p id="c41d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="http://www.statisticssolutions.com/what-is-logistic-regression/" rel="noopener ugc nofollow" target="_blank">http://www . statistics solutions . com/what-is-logistic-regression/</a></p><p id="9010" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/12146914/线性回归和逻辑回归的区别是什么</a></p></div></div>    
</body>
</html>