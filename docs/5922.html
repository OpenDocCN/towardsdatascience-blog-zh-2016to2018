<html>
<head>
<title>Interpretable Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpretable-neural-networks-45ac8aa91411?source=collection_archive---------3-----------------------#2018-11-16">https://towardsdatascience.com/interpretable-neural-networks-45ac8aa91411?source=collection_archive---------3-----------------------#2018-11-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="d845" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">解释黑盒模型是机器学习中的一个重大挑战，可以显著减少采用该技术的障碍。</p><p id="dd19" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae kl" href="https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83" rel="noopener">之前的一篇文章</a>中，我讨论了使用<a class="ae kl" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank">形状值</a>解释复杂的机器学习模型。总而言之，对于特定的特征，当模型可以看到该特征时和当它不能看到该特征时，会比较模型对于特定数据点的预测，这种差异的大小告诉我们该特征对于模型的预测有多重要。</p><p id="c1f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在下面的例子中，我们建立了一个模型来预测某人是否想要冰淇淋，它由三个特征组成:他们是否喜欢冷食，他们现在所处的季节以及他们是否喜欢甜食。Shap 值允许我们解释单个数据点的模型输出—在本例中，对于 Bob:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/11210e7e0f0e5d9f2b2cc4c8b3820785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pFqWTD5Sr16fMkdTsSZa8g.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Inspired by the diagrams in the <a class="ae kl" href="https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf" rel="noopener ugc nofollow" target="_blank">shap values paper</a></figcaption></figure><p id="07fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个冰淇淋模型有两点需要注意:首先，当模型看不到特征时，特征的效果与模型预测的基线进行比较。其次，特征重要性的总和(红色和蓝色箭头)是该基线和 Bob 的模型实际预测之间的差异。</p><p id="9724" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不幸的是，虽然某些机器学习算法(如 XGBoost)可以处理空特征值(即看不到某个特征)，但神经网络不能，因此需要一种略有不同的方法来解释它们。迄今为止，最常见的方法是考虑输入相对于预测的梯度。</p><p id="20b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将讲述使用这些渐变背后的直觉，以及由此产生的两种具体技术:集成渐变和深度提升。</p><h1 id="ee01" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">使用梯度解释神经网络</h1><p id="53d0" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">可能最难解释的模型——也是我们将用作灵感的模型——是回归。在一个回归中，每个特征<em class="mf"> x </em>被分配一个权重<em class="mf"> w </em>，这直接告诉我该特征对模型的重要性。</p><p id="a611" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">具体来说，对于特定数据点的第<em class="mf"> i </em>个特征，该特征对模型输出的贡献为</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/2a297f4bb1a93006a4b78a2bfd5699ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*op9bP6gJlqq_7Jf1PqYnbg@2x.png"/></div></figure><p id="79db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个重量<em class="mf"> w </em>代表什么？因为回归是</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mh"><img src="../Images/63b74f651e66189e6e688ed17df796d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GuwngWL8m01Xr9voW8fNFg@2x.png"/></div></div></figure><p id="9d8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mi"><img src="../Images/5c620f8293cf562ec3f9d96efaa5c7a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*Y2thNSvxMVtMa8WiPOIntA@2x.png"/></div></div></figure><p id="7956" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">换句话说，分配给第<em class="mf"> i </em>个特征的权重告诉我们该特征相对于模型预测的梯度:模型的预测如何随着特征的改变而改变。</p><p id="3628" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">方便的是，这个梯度对于神经网络来说很容易计算。因此，与回归一样，一个特性的贡献是</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/32b17e85375266a270bfe3b0afa36138.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*v9b0YgU-JgI-SsDSDBfaDQ@2x.png"/></div></figure><p id="f8e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">，也许梯度可以用来解释神经网络的输出。</p><p id="c4c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在尝试使用这种方法时，我们会遇到两个问题:</p><p id="3e30" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，特性的重要性是相对的。对于梯度增强决策树，特征的 shap 值告诉我，相对于没有看到该特征的模型，特征如何改变模型<em class="mf">的预测。由于神经网络不能处理空输入特征，我们需要重新定义一个特征相对于其他东西的影响。</em></p><p id="212a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了克服这一点，我们将定义一个新的基线:我的输入与什么进行比较？一个例子是 MNIST 数字数据集。因为所有的数字都是白色的，背景是黑色的，所以一个合理的背景应该是全黑的图像，因为这不代表关于数字的任何信息。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mk"><img src="../Images/3a3581e4263bcb8eff33a6c6b5fd2d10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7RCsAj9MU2oaVrajTmOS7A.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Some MNIST digit examples, and a reasonable baseline for them</figcaption></figure><p id="fe59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为其他数据集选择背景要简单得多——例如，<a class="ae kl" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>数据集的背景应该是什么？我们稍后将讨论这个问题的解决方案，但是现在让我们假设我们可以为每个数据集找到一个基线。</p><p id="81c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二个问题是，使用输出相对于输入的梯度对于线性模型(如回归)很有效，但对于非线性模型则很快失效。为了了解原因，让我们考虑一个仅由一个<a class="ae kl" href="https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29" rel="noopener ugc nofollow" target="_blank"> ReLU 激活</a>组成的“神经网络”，其基线输入为<em class="mf"> x=2 </em>。</p><p id="07ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们考虑第二个数据点，在<em class="mf"> x = -2 </em> <strong class="jp ir"> <em class="mf">。</em>T13】</strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ml"><img src="../Images/512e76d5f39f345be6e0692620b0580f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Do1klOI8GBMrgESewyP47g.png"/></div></div></figure><p id="d021" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mf"> ReLU(x=2) = 2 </em>，而<em class="mf"> ReLU(x=-2) = 0 </em>，所以我的输入特征<em class="mf"> x = -2 </em>相对于基线改变了我的模型的输出 2。我的模型输出的这种变化必须归因于 x 的变化，因为它是这个模型的唯一输入特征，但是 ReLU(x)在点<em class="mf"> x = -2 </em>的梯度是 0！这告诉我 x 对产出的贡献是 0，这显然是一个矛盾。</p><p id="cfcb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">出现这种情况有两个原因:首先，我们关心函数中的一个有限差(当<em class="mf"> x = 2 </em>时的函数和当<em class="mf"> x = -2 </em>时的函数之差)，但是梯度计算的是无穷小的差。其次，ReLU 函数可能会饱和——一旦<em class="mf"> x </em>小于 0，它变小多少都无关紧要，因为该函数只会输出 0。正如我们在上面看到的，这导致了不一致的模型解释，其中输出相对于基线发生了变化，但是没有特征被标记为引起了这种变化。</p><p id="c584" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些不一致正是 Integrated Gradients 和 DeepLIFT 试图解决的问题。他们都认识到，最终，我们关心的不是点 x 的梯度。我们关心当输入从基线改变<em class="mf">时，输出如何从基线</em>改变<em class="mf">。</em></p><h1 id="e8b1" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">集成渐变</h1><p id="4ab1" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">积分梯度所采用的方法是提出以下问题:我可以计算什么，它类似于也承认基线存在的梯度？</p><p id="3885" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">部分问题在于基线处的梯度(相对于输入的输出)将不同于我测量的输出处的梯度；理想情况下，我会考虑两点的梯度。但是这还不够:考虑一个<a class="ae kl" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid 函数</a>，其中我的基线输出接近 0，我的目标输出接近 1:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mm"><img src="../Images/56257abc075e748e4519180f6073dd3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zmMVZNAu9y-jJyWc_yRXJg.png"/></div></div></figure><p id="04c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，我的基线和数据点的梯度都接近于 0。事实上，所有有趣的——也是信息丰富的——梯度都在两个数据点之间，所以理想情况下，我们也会找到一种方法来捕捉所有这些信息。</p><p id="9c3c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这正是积分梯度所做的，通过计算基线和感兴趣点之间的梯度积分。实际上，计算梯度的积分是很难的，因此，相反，它们是使用<a class="ae kl" href="https://en.wikipedia.org/wiki/Riemann_sum" rel="noopener ugc nofollow" target="_blank">赖曼和</a>来近似的:梯度是在基线和感兴趣的点之间的许多小步长上获得的。</p><p id="7e0a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这非常容易实现——事实上，Integrated Gradients 的作者在他们的<a class="ae kl" href="https://github.com/ankurtaly/Integrated-Gradients/blob/master/icml_slides.pdf" rel="noopener ugc nofollow" target="_blank"> ICML 演讲</a>中演示了一个七行代码的实现:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mn"><img src="../Images/7b13ac242d1efae471c06f6612fc6f8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pMg-VmpxvYG0F7TkXzVIJQ.png"/></div></div></figure><p id="35be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，缺点是，由于我们是在近似一个积分，我们的解释也将是一个近似。此外，这种方法非常耗时；理想情况下，我们的 Reimann 和中的步长较小，因此我们可以非常接近积分，但每一步都需要网络的反向传递。</p><h1 id="c8c9" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">深层提升</h1><p id="c177" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">采用的方法<a class="ae kl" href="https://arxiv.org/abs/1704.02685" rel="noopener ugc nofollow" target="_blank"> DeepLIFT </a>在概念上非常简单，但实现起来很棘手。DeepLIFT 认识到我们关心的不是梯度，它描述了在点<em class="mf">x</em>T15 处<em class="mf"> y </em>如何随着<em class="mf"> x </em>改变<strong class="jp ir">而改变，而是<em class="mf">斜率</em>，它描述了<em class="mf"> y </em>如何随着<em class="mf"> x </em>与基线</strong>不同<strong class="jp ir">而改变。</strong></p><p id="3dd7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">事实上，如果我们考虑斜率而不是梯度，那么我们可以将特征的重要性重新定义为</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mo"><img src="../Images/0eca051e6b24656823d74685cde56846.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bUW4Gpv7brV37tUpYCeWgg@2x.png"/></div></div></figure><p id="c2ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是很好的动机，因为我们关心与基线相关的一切。将此应用于我们的网络:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mp"><img src="../Images/c1693b54223932ef88ef017329944774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AhTlur5H3waS6h7H16exOg.png"/></div></div></figure><p id="5e61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个方法告诉我们，输入<em class="mf"> x </em>的重要性值为-2。或者，换句话说:“与基线相比，输入<em class="mf"> x = -2 </em>改变了模型的输出-2”。既然事实如此，这种方法就很有意义。</p><p id="1950" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，对于每一层，我将计算斜率而不是梯度，其中</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/9193b522e047c7a10c98014aabf4ec8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*iw8AdlDQ3lfT13i1LXzS4Q@2x.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Note that the inputs and outputs here are of a certain operation — so here, y and x are the outputs and inputs of (for instance) a certain layer in the network.</figcaption></figure><p id="40bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">DeepLIFT 将这个斜率称为“乘数”，并将其象征性地表示为<em class="mf"> m </em>。既然我们已经将梯度重新定义为一个乘数，正常的链式法则(因此也是反向传播法则)就适用了，但是一切都是相对于基线进行的。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/9be0819bf96135cc11429e83cc01892a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*ujrD_djPXYgfScQ0GnRJvw@2x.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Chain rule with multipliers works the same as chain rule with gradients</figcaption></figure><p id="6532" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，您现在可以沿着这些乘数反向传播，以找到输入相对于模型输出的斜率，从而轻松定义要素的重要性。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ms"><img src="../Images/3142c6794fae2785d2706205acc64a44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uA9V3g6tSF4Pf1JSoTwV1Q@2x.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Note that I am calculating the ‘partial slope’ of Y, similar to calculating the <a class="ae kl" href="https://en.wikipedia.org/wiki/Partial_derivative" rel="noopener ugc nofollow" target="_blank">partial derivative</a>.</figcaption></figure><p id="62a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">DeepLIFT 对模型互操作性的方法的问题在于，它重新定义了梯度的计算方式，这意味着你需要非常深入地挖掘大多数深度学习框架的内部来实现它。</p><p id="e232" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其优点是速度快(只需要模型的一次反向传递来计算特征重要性值)且精确(因为与积分梯度不同，没有发生近似)。</p><h1 id="5e2c" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">选择基线</h1><p id="ca0c" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">正如所承诺的，我们现在将回到挑选基线。除了一些非常明显的例子(例如上面的 MNIST 例子)，决定基线输入是非常重要的，可能需要领域的专业知识。</p><p id="36ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">手动选取基线的替代方法是考虑已训练模型的先验分布。这可以让我们很好地了解模型在完全没有信息的情况下在想什么。</p><p id="c9c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，如果我在 ImageNet 上训练了一个模型，它之前的假设是什么，它看到的一张新照片是一只猫鼬吗？如果 ImageNet 数据集中有 2%的照片是猫鼬，那么模型会认为它看到的新照片有 2%的机会是猫鼬。当它真正看到照片时，它会相应地调整它的预测。衡量输入相对于先前假设的影响是有意义的。</p><p id="2667" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么，我如何选择一个 2%猫鼬的基线呢？好吧，一个好的方法可能是取数据集的平均值，简单地将数据集中的图像平均在一起。这是在<a class="ae kl" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> shap 库</a>的集成渐变和深度提升实现中使用的方法。方便的是，它消除了作为领域专家为被解释的模型选择合适基线的需要。</p><h1 id="1fb6" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">结论</h1><p id="80ba" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">模型可解释性仍然是机器学习中一个有趣的挑战。最大的问题是，没有一种定量的方法来衡量一种解释是否优于另一种解释；我们所能做的最好的事情就是定义我们希望我们的解释具有什么样的属性(比如特征重要性的总和等于预测与基线的差异)。</p><p id="803f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">希望这篇文章直观地展示了两种强大的解释技术以及它们之间的区别，但是还有很多内容我没有涉及(比如 Integrated Gradients 和 DeepLIFT 的作者在开发他们各自的技术时优化了哪些属性)，所以我鼓励大家看看这些论文。</p><p id="7e67" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，如果你想试验这些方法，<a class="ae kl" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> shap 库</a>有两者的多框架实现。</p><h1 id="09de" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">来源/延伸阅读</h1><p id="26b7" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated"><strong class="jp ir">综合梯度</strong> : <em class="mf">穆昆德·孙达拉拉詹，安库尔·塔利，奇奇颜</em>，<a class="ae kl" href="https://arxiv.org/abs/1703.01365" rel="noopener ugc nofollow" target="_blank">深度网络公理化归属</a>，2017</p><p id="1d8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> DeepLIFT </strong> : <em class="mf">两代情·什里库马尔，佩顿·格林塞德，安舒尔·昆达耶</em>，<a class="ae kl" href="https://arxiv.org/abs/1704.02685" rel="noopener ugc nofollow" target="_blank">通过传播激活差异学习重要特征</a>，2017</p><p id="3a62" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> SHAP 价值观</strong> : <em class="mf"> Scott M. Lundberg，Su-In Lee，</em> <a class="ae kl" href="https://arxiv.org/abs/1705.07874" rel="noopener ugc nofollow" target="_blank">解释模型预测的统一方法</a>，2017</p></div></div>    
</body>
</html>