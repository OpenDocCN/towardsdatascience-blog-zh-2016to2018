<html>
<head>
<title>Machine Learning —Fundamentals</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习—基础知识</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-basics-part-1-a36d38c7916?source=collection_archive---------0-----------------------#2018-08-15">https://towardsdatascience.com/machine-learning-basics-part-1-a36d38c7916?source=collection_archive---------0-----------------------#2018-08-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="aa85" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">机器学习领域的基础理论</h2></div><p id="8b9c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文介绍了机器学习理论的基础，奠定了所涉及的常见概念和技术。这篇文章是为刚开始学习机器的人准备的，让他们很容易理解核心概念，并熟悉机器学习的基础知识。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/dd257725c25ea051e2738ac232eee290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PVdHybB6Vvj3jleY.jpg"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><a class="ae lr" href="https://www.expertsystem.com/machine-learning-definition/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="be10" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么是机器学习？</h1><p id="dc0f" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">1959 年，人工智能研究的先驱、计算机科学家亚瑟·塞缪尔(Arthur Samuel)将机器学习描述为“在没有明确编程的情况下赋予计算机学习能力的研究。”</p><p id="2411" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">艾伦·图灵的开创性论文(<strong class="kh ir">图灵，</strong> 1950 年)介绍了一个展示机器智能的基准标准，即机器必须具有智能和响应能力，其方式不能与人类有所不同。</p><blockquote class="mp"><p id="6e51" class="mq mr iq bd ms mt mu mv mw mx my la dk translated">机器学习是人工智能的一种应用，其中计算机/机器从过去的经验(输入数据)中学习，并做出未来的预测。这样一个系统的性能至少应该是人的水平。</p></blockquote><p id="fe72" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated"><strong class="kh ir">Tom m . Mitchell</strong>(1997)给出了一个更具技术性的定义:“如果一个计算机程序在 T 类任务中的性能(如 P 所测量的)随着经验 E 而提高，那么就可以说它从经验 E 中学习了一些任务 T 和性能测量 P。”例如:</p><pre class="lc ld le lf gt ne nf ng nh aw ni bi"><span id="bf11" class="nj lt iq nf b gy nk nl l nm nn"><strong class="nf ir">A handwriting recognition learning problem:</strong></span><span id="8950" class="nj lt iq nf b gy no nl l nm nn"><strong class="nf ir">Task T</strong>: recognizing and classifying handwritten words within images<br/><strong class="nf ir">Performance measure P</strong>: percent of words correctly classified, accuracy<br/><strong class="nf ir">Training experience E</strong>: a data-set of handwritten words with given classifications</span></pre><p id="8a79" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了执行任务 T，系统从所提供的数据集进行学习。数据集是许多例子的集合。一个例子是特征的集合。</p><h1 id="9920" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">机器学习类别</h1><p id="03b1" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">机器学习通常分为三种类型:监督学习、非监督学习、强化学习</p><h2 id="c1f5" class="nj lt iq bd lu np nq dn ly nr ns dp mc ko nt nu me ks nv nw mg kw nx ny mi nz bi translated"><strong class="ak">监督学习:</strong></h2><p id="ee7b" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在监督学习中，机器会经历这些例子以及每个例子的标签或目标。数据中的标签有助于算法关联特征。</p><p id="e652" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">两个最常见的监督机器学习任务是<strong class="kh ir">分类</strong>和<strong class="kh ir">回归</strong>。</p><pre class="lc ld le lf gt ne nf ng nh aw ni bi"><span id="1ad5" class="nj lt iq nf b gy nk nl l nm nn">In <strong class="nf ir">classification</strong> problems the machine must learn to predict discrete values. That is, the machine must predict the most probable category, class, or label for new examples. Applications of classification include predicting whether a stock's price will rise or fall, or deciding if a news article belongs to the politics or leisure section. </span><span id="2458" class="nj lt iq nf b gy no nl l nm nn">In <strong class="nf ir">regression</strong> problems the machine must predict the value of a continuous response variable. Examples of regression problems include predicting the sales for a new product, or the salary for a job based on its description.</span></pre><h2 id="8f1a" class="nj lt iq bd lu np nq dn ly nr ns dp mc ko nt nu me ks nv nw mg kw nx ny mi nz bi translated"><strong class="ak">无监督学习:</strong></h2><p id="c91a" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">当我们有未分类和未标记的数据时，系统试图从数据中发现模式。这些示例没有给出标签或目标。一个常见的任务是将相似的例子组合在一起，称为聚类。</p><h2 id="d64c" class="nj lt iq bd lu np nq dn ly nr ns dp mc ko nt nu me ks nv nw mg kw nx ny mi nz bi translated"><strong class="ak">强化学习:</strong></h2><p id="f51f" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">强化学习是指面向目标的算法，它学习如何在许多步骤中实现复杂的目标或沿着特定的维度最大化。这种方法允许机器和软件代理自动确定特定上下文中的理想行为，以便最大化其性能。代理人需要简单的奖励反馈来学习哪一个动作是最好的；这就是所谓的强化信号。例如，在一场游戏中，通过多次移动来最大化赢得的点数。</p><h1 id="e1a1" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">监督机器学习技术</h1><p id="88fd" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">回归是一种技术，用于从一个或多个预测变量(自变量)预测响应变量(因变量)的值。</p><p id="1914" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最常用的回归技术有:<strong class="kh ir">线性回归</strong>和<strong class="kh ir">逻辑回归</strong>。我们将讨论这两种突出技术背后的理论，同时解释机器学习中涉及的许多其他关键概念，如<code class="fe oa ob oc nf b">Gradient-descent</code>算法、<code class="fe oa ob oc nf b">Over-fit/Under-fit</code>、<code class="fe oa ob oc nf b">Error analysis</code>、<code class="fe oa ob oc nf b">Regularization</code>、<code class="fe oa ob oc nf b">Hyper-parameters</code>、<code class="fe oa ob oc nf b">Cross-validation</code>技术。</p><h1 id="3705" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">线性回归</h1><p id="dc42" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在线性回归问题中，目标是从给定的模式<code class="fe oa ob oc nf b"><em class="od">X</em></code>中预测实值变量<code class="fe oa ob oc nf b"><em class="od">y</em> </code>。在线性回归的情况下，输出是输入的线性函数。假设<code class="fe oa ob oc nf b"><em class="od">ŷ</em></code>是我们的模型预测的输出:<code class="fe oa ob oc nf b"><em class="od">ŷ </em>= <em class="od">WX</em>+<em class="od">b</em></code></p><p id="377f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里<code class="fe oa ob oc nf b"><em class="od">X</em></code>是向量(示例的特征)，<code class="fe oa ob oc nf b"><em class="od">W</em></code>是确定每个特征如何影响预测的权重(参数的向量)，<code class="fe oa ob oc nf b"><em class="od">b</em></code>是偏差项。因此，我们的任务<code class="fe oa ob oc nf b"><em class="od">T</em></code>是从<code class="fe oa ob oc nf b"><em class="od">X</em></code>预测<code class="fe oa ob oc nf b"><em class="od">y</em></code>，现在我们需要测量性能<code class="fe oa ob oc nf b"><em class="od">P</em></code>以了解模型的表现如何。</p><p id="2af7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在来计算模型的性能，我们首先计算每个例子<em class="od"> </em> <code class="fe oa ob oc nf b"><em class="od">i</em></code>的误差为:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/34624baaef76d721529ead107b9df773.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*aXN3c6e3KH_RrhYr3BRS1g@2x.png"/></div></figure><p id="94bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们采用误差的绝对值来考虑误差的正值和负值。</p><p id="9b81" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们计算所有记录的绝对误差的平均值(所有绝对误差的平均和)。</p><p id="b098" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">平均绝对误差(MAE) </strong> =所有绝对误差的平均值</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi of"><img src="../Images/aca2b217c901298548eba56d93f553ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*3q6asvit7SAQHdanhExgnw@2x.png"/></div></figure><p id="79e0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更流行的测量模型性能的方法是使用</p><p id="8011" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">均方误差(MSE) </strong>:预测值与实际观测值的平方差的平均值。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi og"><img src="../Images/76ffcb4835cf9571b18d1278aeeefc7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*UlHJ3AY30MhjebwekeBXwA@2x.png"/></div></figure><p id="293a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">平均值被减半(1/2 ),以便于计算梯度下降(稍后讨论),因为平方函数的导数项将抵消 1/2 项。有关 MAE 与 MSE 的更多讨论，请参考[1]和[2]。</p><blockquote class="mp"><p id="ced8" class="mq mr iq bd ms mt mu mv mw mx my la dk translated">训练 ML 算法的主要目的是调整权重<code class="fe oa ob oc nf b"><em class="oh">W</em></code>以减少 MAE 或 MSE。</p></blockquote><p id="ed3e" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">为了最小化<em class="od">误差</em>，模型在经历训练集的例子时，更新模型参数<code class="fe oa ob oc nf b"><em class="od">W</em></code>。这些根据<em class="od"> </em> <code class="fe oa ob oc nf b"><em class="od">W</em></code>绘制的误差计算也被称为<strong class="kh ir">成本函数</strong> <code class="fe oa ob oc nf b"><em class="od">J(w)</em></code>，因为它决定了模型的成本/惩罚。因此最小化误差也被称为最小化成本函数 j。</p><h1 id="42b3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">梯度下降算法:</strong></h1><p id="beac" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">当我们绘制成本函数<code class="fe oa ob oc nf b"><em class="od">J(w) vs w</em></code>时。它表示如下:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/80f8e6cbd105aba5759685108b88b717.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*9j2Vj8L8jm55NpM4LsE8Ew.png"/></div></figure><p id="7ff8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从曲线中我们可以看出，存在一个参数值<code class="fe oa ob oc nf b"><em class="od">W</em></code>，它具有最小的成本<code class="fe oa ob oc nf b"><em class="od">Jmin</em></code>。现在我们需要找到一种方法来达到这个最低成本。</p><p id="af8d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在梯度下降算法中，我们从随机模型参数开始，计算每次学习迭代的误差，不断更新模型参数，以更接近产生最小成本的值。</p><p id="809f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">重复直到最小成本:{</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/47d873f205ca4aaaf58bceca7cd1db81.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*farY97_uib-Z7ZKuoLdDdA@2x.png"/></div></figure><p id="a6a0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">}</p><p id="f7f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的等式中，我们在每次迭代后更新模型参数。方程的第二项计算每次迭代时曲线的斜率或梯度。</p><p id="5dc8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">成本函数的梯度被计算为成本函数<code class="fe oa ob oc nf b"><em class="od">J</em></code> <em class="od"> </em>相对于每个模型参数<code class="fe oa ob oc nf b"><em class="od">wj</em></code> <em class="od">，</em> <code class="fe oa ob oc nf b"><em class="od">j</em></code>取特征数量<code class="fe oa ob oc nf b">[1 to n]</code>的值。<code class="fe oa ob oc nf b"><em class="od">α</em></code>，<em class="od"> alpha </em>，是学习率，或者说我们想要多快地向最小值移动。如果<code class="fe oa ob oc nf b"><em class="od">α</em></code>太大，我们可以超调。如果<code class="fe oa ob oc nf b"><em class="od">α</em></code>太小，意味着学习的步骤很小，因此模型观察所有示例所花费的总时间会更多。</p><p id="f49c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">梯度下降有三种方式:</p><p id="fb7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">批量梯度下降:</strong>使用所有的训练实例来更新每次迭代中的模型参数。</p><p id="65b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">小批量梯度下降:</strong>小批量梯度下降不是使用所有的例子，而是将训练集分成更小的称为“b”的批量。因此，小批量“b”用于在每次迭代中更新模型参数。</p><p id="50f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">随机梯度下降(SGD): </strong>在每次迭代中仅使用单个训练实例更新参数。训练实例通常是随机选择的。当有成千上万或更多的训练实例时，随机梯度下降通常是优化成本函数的首选，因为它比批量梯度下降收敛得更快[3]。</p><h1 id="2979" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">逻辑回归</h1><p id="09db" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在某些问题中，响应变量不是正态分布的。例如，抛硬币会有两种结果:正面或反面。伯努利分布描述了随机变量的概率分布，该随机变量可以采用概率为<code class="fe oa ob oc nf b"><em class="od">P</em></code>的正情况或概率为<code class="fe oa ob oc nf b"><em class="od">1-P</em></code>的负情况。如果响应变量代表一个概率，它必须被限制在<code class="fe oa ob oc nf b">{0,1}</code>的范围内。</p><p id="7c5d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在逻辑回归中，响应变量描述了结果是正面情况的概率。如果响应变量等于或超过判别阈值，则预测阳性类别；否则，预测负类。</p><p id="648d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用逻辑函数将响应变量建模为输入变量的线性组合的函数。</p><p id="c13b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于我们的假设<code class="fe oa ob oc nf b"><em class="od">ŷ</em></code>必须满足<code class="fe oa ob oc nf b">0 ≤ <em class="od">ŷ</em> ≤ 1</code>，这可以通过插入逻辑函数或“Sigmoid 函数”来实现</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/901846edbde05e67a0a07bc54d1a1cc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*nLEhqS2Ojrz7tQlXdtnkOg@2x.png"/></div></figure><p id="ee2e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">函数<code class="fe oa ob oc nf b"><em class="od">g(z)</em></code>将任何实数映射到<code class="fe oa ob oc nf b">(0, 1)</code>区间，这对于将任意值函数转换为更适合分类的函数非常有用。以下是范围<code class="fe oa ob oc nf b">{-6,6}</code>内 sigmoid 函数值的曲线图:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/017b646764eba0f02d5000618f61b454.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*Urgck1u8mS5F_nzDRvpwmw.png"/></div></figure><p id="8e16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在回到我们的逻辑回归问题，让我们假设<code class="fe oa ob oc nf b"><em class="od">z</em></code>是单个解释变量<code class="fe oa ob oc nf b"><em class="od">x</em></code>的线性函数。我们可以将<code class="fe oa ob oc nf b"><em class="od">z</em></code>表达如下:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi om"><img src="../Images/d7ff36e035d9b9712f69b639ce9dde01.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*N1MVqbFdhqNIt6L6EB8sow@2x.png"/></div></figure><p id="8193" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">逻辑函数现在可以写成:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi on"><img src="../Images/48e7485bc0e7b9fe812c071da6d54672.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*j9k8TeNOi_ohgiS2khMosA@2x.png"/></div></figure><p id="2b9f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意<code class="fe oa ob oc nf b"><em class="od">g(x)</em></code>解释为因变量的概率。<br/> <code class="fe oa ob oc nf b"><em class="od">g(x) = 0.7</em></code>，给我们 70%的概率，我们的输出是 1。我们预测为 0 的概率正好是我们预测为 1 的概率的补充(例如，如果预测为 1 的概率是 70%，那么预测为 0 的概率是 30%)。</p><p id="acf1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">sigmoid 函数<code class="fe oa ob oc nf b">‘g’</code>的输入不需要是线性函数。它可以是圆形或任何形状。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/87f1bfffa4f41afe88e07df15fd4c27c.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*lx6-ZSAdqzCIeevaG3kz3w@2x.png"/></div></figure><h2 id="0bab" class="nj lt iq bd lu np nq dn ly nr ns dp mc ko nt nu me ks nv nw mg kw nx ny mi nz bi translated">价值函数</h2><p id="df41" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">我们不能使用用于线性回归的相同成本函数，因为 Sigmoid 函数将导致输出波动，从而导致许多局部最优。换句话说，它不会是凸函数。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi op"><img src="../Images/78a3b991d5ebe0bd659bb3ce98dca196.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*7GzRSI3kyYqc5PZPc8JVbg.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Non-convex cost function</figcaption></figure><p id="4a8a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了确保成本函数是凸的(并因此确保收敛到全局最小值)，使用 sigmoid 函数的对数来变换成本函数。逻辑回归的成本函数如下所示:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/960288d9cb045b3598b751f7eb28dd2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*RFhjWKvYkQt1XoLlvhyniw@2x.png"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi or"><img src="../Images/b101f3409c5236f61d0c011579167c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F6vDZF21lqM2vVm83drPlA@2x.png"/></div></div></figure><p id="fa04" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可以写成:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi os"><img src="../Images/97179efb5ee0ecbcf7f4e85f9a7a5102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hZQs-kcnmJIfEpHRScA5Og@2x.png"/></div></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/6af6c684d90d6ff9120ca8b5742cbd5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*xssIjwrPbEjTLGBwzQze0Q.png"/></div></figure><p id="3c3b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以逻辑回归的成本函数是:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/014f0e8d6a123fb5763aa3612aa7454e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*5-_0vjehX9Sif2pD7kgXAw@2x.png"/></div></figure><p id="8a0b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于成本函数是凸函数，我们可以运行梯度下降算法来找到最小成本。</p><h1 id="6f4b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">装配不足和过度装配</h1><p id="f6f1" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">我们试图通过增加或减少模型容量来使机器学习算法适应输入数据。在线性回归问题中，我们增加或减少多项式的次数。</p><p id="16b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑从<code class="fe oa ob oc nf b"><em class="od">x ∈ R</em></code>预测<code class="fe oa ob oc nf b"><em class="od">y</em></code>的问题。下面最左边的图显示了将一条线拟合到一个数据集的结果。由于数据不在一条直线上，所以拟合不是很好(左图)。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ov"><img src="../Images/8d16ec28ccf9654bda46e53dc00b5b4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zvdOQZRLDIAs_zQhXejsqg.png"/></div></div></figure><p id="ec37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了增加模型容量，我们通过添加术语<code class="fe oa ob oc nf b"><em class="od">x²</em></code>来添加另一个特性。这产生了更好的拟合(中间的数字)。但是如果我们继续这样做(<code class="fe oa ob oc nf b"><em class="od">x⁵</em></code>，5 阶多项式，图在右边)，我们可能能够更好地拟合数据，但是对于新数据将不能很好地概括。第一个数字表示欠拟合，最后一个数字表示过拟合。</p><h2 id="31e9" class="nj lt iq bd lu np nq dn ly nr ns dp mc ko nt nu me ks nv nw mg kw nx ny mi nz bi translated"><strong class="ak">欠拟合:</strong></h2><p id="50a4" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">当模型具有较少的特征，因此不能很好地从数据中学习时。这个模型有很高的偏差。</p><h2 id="2cda" class="nj lt iq bd lu np nq dn ly nr ns dp mc ko nt nu me ks nv nw mg kw nx ny mi nz bi translated">过度装配:</h2><p id="3734" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">当模型具有复杂的函数，因此能够很好地拟合数据，但不能进行归纳以预测新数据时。这个模型有很高的方差。</p><p id="efce" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有三个主要选项来解决过度拟合问题:</p><ol class=""><li id="0a5a" class="ow ox iq kh b ki kj kl km ko oy ks oz kw pa la pb pc pd pe bi translated"><strong class="kh ir">减少特征数量:</strong>手动选择保留哪些特征。这样做，我们可能会错过一些重要的信息，如果我们扔掉一些功能。</li><li id="c362" class="ow ox iq kh b ki pf kl pg ko ph ks pi kw pj la pb pc pd pe bi translated"><strong class="kh ir">正则化:</strong>保留所有特征，但减少权重 w 的大小。当我们有许多稍微有用的特征时，正则化工作得很好。</li><li id="c5aa" class="ow ox iq kh b ki pf kl pg ko ph ks pi kw pj la pb pc pd pe bi translated"><strong class="kh ir">提前停止:</strong>当我们迭代地训练一个学习算法时，比如使用梯度下降，我们可以测量模型的每次迭代执行得有多好。达到一定的迭代次数后，每次迭代都会改进模型。然而，在这一点之后，模型的概括能力会减弱，因为它开始过度拟合训练数据。</li></ol><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/62013e5d32a24814978ffc491434df95.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*5J89uGKCwYi6ae0OlT2Gww.jpeg"/></div></figure><h1 id="f512" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">正规化</strong></h1><p id="36c5" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">通过向误差函数添加惩罚项，正则化可以应用于线性和逻辑回归，以阻止系数或权重达到大值。</p><h2 id="88f3" class="nj lt iq bd lu np nq dn ly nr ns dp mc ko nt nu me ks nv nw mg kw nx ny mi nz bi translated"><strong class="ak">正则化线性回归</strong></h2><p id="27a3" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">最简单的这种罚项采取所有系数的平方和的形式，导致修正的线性回归误差函数:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/dc0f587b8daf314987e8d3045d6ec798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*Atf9IL31p5fmlbri4KD79A@2x.png"/></div></figure><p id="ed67" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中λ是我们的正则化参数。</p><p id="7986" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在为了使误差最小化，我们使用梯度下降算法。我们不断更新模型参数，以更接近产生最小成本的值。</p><p id="2cc4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">重复直到收敛(使用正则化):{</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/64479f26c940ad83a0909c5e26ff5463.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*1YnY0-hK-1mbXYAit-4G9A@2x.png"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/7b7e6ec95d24e29e80d523d31963a6c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*uFamPqatHXTAF_8MpUiwqg@2x.png"/></div></figure><p id="073a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">}</p><p id="cb1e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过一些操作，上述等式也可以表示为:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi po"><img src="../Images/1f514717b0a98aa1e61df99372eb0d85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*0TwjISvF4hAUq6dyuZnVZw@2x.png"/></div></figure><p id="8b3d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上述等式中的第一项，</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/77ee8ce0f981b752a6e179210a70c6d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*6WGf3KuQfu7h7m4BVZSC7g@2x.png"/></div></figure><p id="4bd9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将始终小于 1。直观上，你可以看到每次更新时，系数的值都会减少一些。</p><h2 id="cc90" class="nj lt iq bd lu np nq dn ly nr ns dp mc ko nt nu me ks nv nw mg kw nx ny mi nz bi translated"><strong class="ak">正则化逻辑回归</strong></h2><p id="57b2" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">正则化逻辑回归的成本函数为:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi pq"><img src="../Images/d82ad23aea5036c10bdd36a27fd73203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5eIbnaoB-dbcp65RpOIZ0A@2x.png"/></div></div></figure><p id="19a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">重复直到收敛(使用正则化):{</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/87ed28710ff76fd4c35df8fa1aa47fd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*gWsBhwzb1NQGZyYNnszNdw@2x.png"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ps"><img src="../Images/c1c0662a13c87136fef2a3817b58bbce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*71b4qrU0zBEQWgISzoFmWg@2x.png"/></div></div></figure><p id="e564" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">}</p><h2 id="a7ce" class="nj lt iq bd lu np nq dn ly nr ns dp mc ko nt nu me ks nv nw mg kw nx ny mi nz bi translated"><strong class="ak"> L1 和 L2 正规化</strong></h2><p id="9ad3" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">前面方程中使用的正则项称为 L2 正则化或岭正则化。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/c4d24a8b968df1f92df094ead63bfd37.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*kK2PXWWN9eVVGDpf-veETw@2x.png"/></div></figure><p id="ef30" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">L2 罚旨在最小化权重的平方。</p><p id="9293" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还有一种称为 L1 或拉索的正则化:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/156336ee0c9c7d0f1442e2133c96be77.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*t6WQIhfgqIY_vnKEV2WN9Q@2x.png"/></div></figure><p id="d3eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">L1 罚旨在最小化权重的绝对值</p><p id="bdd8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">L1 和 L2 的区别</strong> <br/> L2 以相同的比例缩小所有系数，但不消除任何系数，而 L1 可以将一些系数缩小到零，从而执行特征选择。欲了解更多详情，请阅读<a class="ae lr" href="https://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/" rel="noopener ugc nofollow" target="_blank">本</a>。</p><h2 id="f643" class="nj lt iq bd lu np nq dn ly nr ns dp mc ko nt nu me ks nv nw mg kw nx ny mi nz bi translated"><strong class="ak">超参数</strong></h2><p id="de81" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">超参数是描述关于模型的结构信息的“高级”参数，该结构信息必须在拟合模型参数之前决定，到目前为止我们讨论的超参数的例子有:<br/>学习率<em class="od">α</em>，正则化<em class="od">λ。</em></p><h2 id="f8f4" class="nj lt iq bd lu np nq dn ly nr ns dp mc ko nt nu me ks nv nw mg kw nx ny mi nz bi translated"><strong class="ak">交叉验证</strong></h2><p id="59f9" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">选择超参数最优值的过程称为模型选择。如果我们在模型选择过程中反复使用相同的测试数据集，它将成为我们训练数据的一部分，因此模型更有可能过度拟合。</p><p id="6495" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">整个数据集分为:</p><ol class=""><li id="935a" class="ow ox iq kh b ki kj kl km ko oy ks oz kw pa la pb pc pd pe bi translated">训练数据集</li><li id="be84" class="ow ox iq kh b ki pf kl pg ko ph ks pi kw pj la pb pc pd pe bi translated">验证数据集</li><li id="bc31" class="ow ox iq kh b ki pf kl pg ko ph ks pi kw pj la pb pc pd pe bi translated">测试数据集。</li></ol><p id="01d9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练集用于拟合不同的模型，然后验证集的性能用于模型选择。在训练和模型选择步骤中保留模型之前未见过的测试集的优点是，我们避免了过度拟合模型，并且模型能够更好地推广到未见过的数据。</p><p id="6b5e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，在许多应用中，用于训练和测试的数据供应将是有限的，并且为了建立良好的模型，我们希望使用尽可能多的可用数据来进行训练。然而，如果验证集很小，它将给出预测性能的相对嘈杂的估计。解决这个难题的一个方法是使用交叉验证，如下图所示。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/9a295caf1f9ef78045f54c9dbebcc724.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*_2fSMeUk6_O3udioPvwT4A.png"/></div></figure><p id="02d8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面的交叉验证步骤是从<a class="ae lr" href="https://elitedatascience.com/machine-learning-iteration#micro" rel="noopener ugc nofollow" target="_blank">到这里的</a>进行的，在这里添加是为了完整。</p><h2 id="d2fb" class="nj lt iq bd lu np nq dn ly nr ns dp mc ko nt nu me ks nv nw mg kw nx ny mi nz bi translated"><strong class="ak">逐步交叉验证:</strong></h2><p id="e2b0" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">使用 K-fold 交叉验证选择超参数的步骤如下:</p><ol class=""><li id="1b72" class="ow ox iq kh b ki kj kl km ko oy ks oz kw pa la pb pc pd pe bi translated">将你的训练数据分成 K = 4 等份，或者“折叠”</li><li id="f7b1" class="ow ox iq kh b ki pf kl pg ko ph ks pi kw pj la pb pc pd pe bi translated">选择一组您希望优化的超参数。</li><li id="6bd8" class="ow ox iq kh b ki pf kl pg ko ph ks pi kw pj la pb pc pd pe bi translated">使用前 3 个折叠的超参数集训练您的模型。</li><li id="620d" class="ow ox iq kh b ki pf kl pg ko ph ks pi kw pj la pb pc pd pe bi translated">在第四次折叠时评估它，或“保持”折叠。</li><li id="c166" class="ow ox iq kh b ki pf kl pg ko ph ks pi kw pj la pb pc pd pe bi translated">用相同的超参数集重复步骤(3)和(4) K (4)次，每次保持不同的折叠。</li><li id="327f" class="ow ox iq kh b ki pf kl pg ko ph ks pi kw pj la pb pc pd pe bi translated">汇总所有 4 次折叠的性能。这是一组超参数的性能指标。</li><li id="c844" class="ow ox iq kh b ki pf kl pg ko ph ks pi kw pj la pb pc pd pe bi translated">对您希望考虑的所有超参数集重复步骤(2)至(6)。</li></ol><p id="0697" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">交叉验证允许我们仅用我们的训练集来调整超参数。这使得我们可以将测试集作为真正不可见的数据集来选择最终模型。</p><h2 id="400c" class="nj lt iq bd lu np nq dn ly nr ns dp mc ko nt nu me ks nv nw mg kw nx ny mi nz bi translated">结论</h2><p id="93ac" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">我们已经涵盖了机器学习领域的一些关键概念，从机器学习的定义开始，然后涵盖了不同类型的机器学习技术。我们讨论了最常见的回归技术(线性和逻辑)背后的理论，并讨论了机器学习的其他关键概念。</p><p id="3ff0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="od">感谢阅读。</em></p><h2 id="4e01" class="nj lt iq bd lu np nq dn ly nr ns dp mc ko nt nu me ks nv nw mg kw nx ny mi nz bi translated">参考</h2><p id="b7e2" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">[1]<a class="ae lr" href="https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d" rel="noopener">https://medium . com/human-in-a-machine-world/Mae-and-RMSE-metric-is-better-e 60 AC 3 bde 13d</a></p><p id="0cb4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]<a class="ae lr" rel="noopener" target="_blank" href="/ml-notes-why-the-least-square-error-bf27fdd9a721">https://towardsdatascience . com/ml-notes-why-the-least-square-error-BF 27 FDD 9 a 721</a></p><p id="e830" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]<a class="ae lr" rel="noopener" target="_blank" href="/gradient-descent-algorithm-and-its-variants-10f652806a3">https://towards data science . com/gradient-descent-algorithm-and-its-variants-10f 652806 a3</a></p><p id="1c2e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4]<a class="ae lr" href="https://elitedatascience.com/machine-learning-iteration#micro" rel="noopener ugc nofollow" target="_blank">https://elitedata science . com/machine-learning-iteration # micro</a></p></div></div>    
</body>
</html>