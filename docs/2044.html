<html>
<head>
<title>Intuition (and maths!) behind univariate gradient descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">直觉(还有数学！)在单变量梯度下降之后</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-bit-by-bit-univariate-gradient-descent-9155731a9e30?source=collection_archive---------1-----------------------#2017-12-08">https://towardsdatascience.com/machine-learning-bit-by-bit-univariate-gradient-descent-9155731a9e30?source=collection_archive---------1-----------------------#2017-12-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e960" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一点一点的机器学习:关于机器学习的小文章</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/046cb2c15b8eb2efd46ff68a54eb6b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqWWrQLIzKnWOI33dSjW9Q.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/photos/Sot0f3hQQ4Y?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Dominik Scythe</a> on <a class="ae kv" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="dd7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">欢迎光临！</p><p id="96fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">机器学习一点一滴</em>旨在分享我自己在机器学习方面的探索和实验。</p><p id="92bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">理解并能够玩转背后的<strong class="ky ir">数学是理解机器学习的关键。它允许我们选择最合适的算法，并根据我们想要解决的问题对其进行量身定制。</strong></p><p id="4fe6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，我遇到过许多教程和讲座，其中使用的方程是<em class="ls">根本无法理解的</em>。所有的符号看起来都很神秘，在被解释的东西和那些方程式之间似乎有一个巨大的鸿沟。我只是不能把所有的点联系起来。不幸的是，当一些知识被假定，重要的步骤被跳过时，数学往往会妨碍理解。</p><p id="c362" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，只要有可能，我将展开方程式并避免走捷径，这样每个人都可以跟随我们如何从方程式的左边到达右边。</p><p id="d394" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们直接进入有趣的事情吧！</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="8af1" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated"><strong class="ak">什么是梯度下降？</strong></h2><p id="fb91" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">第一个主题是<strong class="ky ir">梯度下降</strong>——寻找目标函数最小值的迭代算法。</p><p id="5a26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它通常用于<strong class="ky ir">线性回归</strong>中，以找到给定数据的最佳拟合线。通过这样做，我们可以更好地了解数据集的输入和输出之间的关系，更有趣的是，我们能够在给定新输入的情况下以一定的信心预测输出。</p><p id="9e4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将探索:</p><ol class=""><li id="7fa6" class="my mz iq ky b kz la lc ld lf na lj nb ln nc lr nd ne nf ng bi translated">什么是线性回归</li><li id="f225" class="my mz iq ky b kz nh lc ni lf nj lj nk ln nl lr nd ne nf ng bi translated">如何使用成本函数找到最佳拟合线</li><li id="651c" class="my mz iq ky b kz nh lc ni lf nj lj nk ln nl lr nd ne nf ng bi translated">为什么梯度下降在线性回归中很重要</li><li id="d696" class="my mz iq ky b kz nh lc ni lf nj lj nk ln nl lr nd ne nf ng bi translated">梯度下降到底是如何工作的</li></ol></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="b2c2" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated"><strong class="ak">线性回归</strong></h2><p id="32be" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">监督学习中最常用的模型之一是<strong class="ky ir">线性回归</strong>。</p><p id="5015" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">线性回归模型允许你在给定自变量 x(输入)的情况下<em class="ls">预测因变量 y(输出)，假设两个变量之间存在线性关系。</em></p><p id="04c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里有一些现实生活中的例子，线性回归可以用来找出两个变量之间的关系:</p><blockquote class="nm nn no"><p id="72db" class="kw kx ls ky b kz la jr lb lc ld ju le np lg lh li nq lk ll lm nr lo lp lq lr ij bi translated">身高对体重的影响</p><p id="dae3" class="kw kx ls ky b kz la jr lb lc ld ju le np lg lh li nq lk ll lm nr lo lp lq lr ij bi translated">教育水平对财富的影响</p><p id="ba1a" class="kw kx ls ky b kz la jr lb lc ld ju le np lg lh li nq lk ll lm nr lo lp lq lr ij bi translated">降雨量对水果产量的影响</p><p id="f298" class="kw kx ls ky b kz la jr lb lc ld ju le np lg lh li nq lk ll lm nr lo lp lq lr ij bi translated">血液酒精含量对身体协调性的影响</p></blockquote><p id="39e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用更实际的话来说，线性回归的目标是找到一条线(称为<strong class="ky ir">假设</strong>),即<em class="ls">最能代表(符合)数据点</em>。</p><p id="5782" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但真正的问题是——我们如何知道这种契合有多“好”?</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="c03a" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated"><strong class="ak">成本函数</strong></h2><p id="8db8" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">事实上，我们用一个<strong class="ky ir">成本函数</strong>来衡量一个假设的适合度。</p><p id="ee3d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本质上，成本函数是一个<strong class="ky ir">均方误差(MSE) </strong> —真实值(标签)和从假设中得出的估计值(预测)之间偏差的集合度量。成本函数的<em class="ls">越小，线和数据点之间的偏差越小，因此<em class="ls">的假设</em>越好。</em></p><p id="5e13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们的目标是<em class="ls">找到一个最小化成本函数</em>的假设，因为它给了我们一条最佳拟合线。</p><p id="a455" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，还记得梯度下降的描述吗？</p><blockquote class="ns"><p id="bf82" class="nt nu iq bd nv nw nx ny nz oa ob lr dk translated">梯度下降是一种寻找函数最小值的迭代算法。</p></blockquote><p id="e231" class="pw-post-body-paragraph kw kx iq ky b kz oc jr lb lc od ju le lf oe lh li lj of ll lm ln og lp lq lr ij bi translated">没错，这就是梯度下降在机器学习中发挥作用的地方。我们用它来最小化线性回归中的成本函数，以拟合数据集的直线。</p><p id="63ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi">­­­­­­­­</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="6696" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated"><strong class="ak">梯度下降算法</strong></h2><p id="c3f0" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">话虽如此，梯度下降法并不是专门用来解决线性回归问题的。它是一个通用算法，可以应用于任何一个<em class="ls">可微</em>函数求其最小值。</p><p id="b806" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了更好的理解直觉，我们从最简单的例子开始:<strong class="ky ir">单变量梯度下降</strong>。也就是说，梯度下降适用于单变量函数。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="7e5a" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated"><strong class="ak">单变量梯度下降</strong></h2><p id="8a6c" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">让我们定义θ的一元函数 J，<em class="ls"> J(θ) </em>，如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/78cd5748dcac5342b5d6c3cdc46eb60c.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*3ZE9kkoxCvNRE7wbNIaZbA@2x.png"/></div></div></figure><p id="ed34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当相对于<em class="ls"> θ </em>作图时，函数<em class="ls"> J(θ) </em>看起来像<strong class="ky ir">图 1 </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a7fe29445a69a80f2b26f3c18464db4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*CtOHGPl3drDu9zyJ-EYzKQ@2x.png"/></div></figure><p id="2654" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们希望找到一个使<em class="ls"> J(θ) </em>最小的<em class="ls"> θ </em>值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/82df4acc4c27c669f98b0bb6306fd36f.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*RdlTuS8eSAvqiAPLsgYydQ@2x.png"/></div></figure><p id="6a20" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了实现这一点，我们需要尝试不同的<em class="ls"> θ </em>值，直到<em class="ls"> J(θ) </em>达到最小值。通常情况下，我们从<em class="ls"> θ = 0 </em>开始，但由于在这种情况下，这产生了<em class="ls"> J(θ) </em>的最小值，所以假设我们从<em class="ls"> θ = 6 </em>开始。</p><p id="0089" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在详细研究该算法之前，这里有两个关键问题:</p><ol class=""><li id="b177" class="my mz iq ky b kz la lc ld lf na lj nb ln nc lr nd ne nf ng bi translated">我们如何确定<em class="ls"> J(θ) </em>何时达到最小值？</li><li id="9648" class="my mz iq ky b kz nh lc ni lf nj lj nk ln nl lr nd ne nf ng bi translated">如果<em class="ls"> J(θ) </em>不在最小值，我们怎么知道接下来要尝试的<em class="ls"> θ </em>的值是多少？</li></ol><p id="0f2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些问题可以通过对<em class="ls"> J(θ) </em> 进行<strong class="ky ir">求导来回答——换句话说，计算出在给定<em class="ls"> θ </em>处与<em class="ls"> J(θ) </em>相交的切线的<em class="ls">斜率</em>。如果斜率为正，这意味着<em class="ls"> θ </em>的值需要在下一次迭代中减小，以接近最小值，而如果斜率为负，则<em class="ls"> θ </em>需要增大。</strong></p><p id="0b9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑<strong class="ky ir">图 2 </strong>。由于在<em class="ls"> θ = 6 </em>处切线的斜率是正的<em class="ls"> (12) </em>，我们知道<em class="ls"> θ </em>需要减小以接近使<em class="ls"> J(θ) (θ = 0) </em>最小的值。另一方面，一条切线在<em class="ls"> θ = -2 </em>处的斜率为负<em class="ls"> (-4) </em>，所以我们知道在下一次迭代中需要增加<em class="ls"> θ </em>的值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/498565186ac79e22183f72c05d77647d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tn2f0GTkEusMB6VvrEbBsg@2x.png"/></div></div></figure><p id="a060" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样重要的是，当我们达到最小值<em class="ls"> J(θ) </em>时，<em class="ls">斜率变为 0 </em>。这就是我们如何知道梯度下降何时收敛，即<em class="ls"> J(θ) </em>的值接近<em class="ls">足够接近</em>的最小值。在实践中，如果在一次迭代中 J(θ)的下降小于 10^(-3(= 0.001)，我们宣布收敛<em class="ls">。</em></p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="6a1b" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated"><strong class="ak">更新规则</strong></h2><p id="4d97" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">现在，我们需要一种系统有效的方法来更新<em class="ls"> θ </em>，同时我们寻找一个使<em class="ls"> J(θ)最小的<em class="ls"> θ </em>值。</em></p><p id="1378" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在梯度下降中，它是通过在每次迭代后自动应用<strong class="ky ir">更新规则</strong>来完成的，因此<em class="ls"> θ </em>越来越接近我们的目标值。</p><p id="6877" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">单变量函数的更新规则如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/b5b9a6d9d54795a5d8e38d4e4c1d3a17.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*Bp54JMd287sDQO4oaHWlFw@2x.png"/></div></figure><p id="f1ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，<em class="ls"> α </em>称为<strong class="ky ir">学习率</strong>，<em class="ls"> dJ(θ)/dθ </em>是<em class="ls"> J(θ) </em> 的<strong class="ky ir">导数——即切线的斜率。</strong></p><p id="555a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">学习速率</strong>决定<em class="ls"> θ </em>移动的快慢。选择一个好的学习速率是至关重要的，如果它太大，我们可能会跳过它而错过最佳值，甚至可能不会收敛。另一方面，如果它太小，算法收敛将需要太多的迭代。稍后我会用一些视觉辅助来解释这一点。</p><p id="49ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于<em class="ls"> J(θ) = θ </em>，随着<em class="ls"> dθ </em>向零收缩，<em class="ls"> J(θ) </em> 的<strong class="ky ir">导数计算如下:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/a69196a8852d1712bbda233a3ef1551a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*25m4fCfgwOg6rFy-HGmSmg@2x.png"/></div></div></figure><p id="542c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由此，<em class="ls"> J(θ) = θ </em>的更新规则可以简化为<em class="ls">θ:=(1–2α)θ</em></p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="22b7" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">梯度下降在行动</h2><p id="8406" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">让我们最后看看梯度下降的作用。</p><p id="640b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将对<em class="ls"> J(θ) = θ </em>进行梯度下降，其中<em class="ls"> α = 0.3 </em>，初始值为<em class="ls"> θ = 6 </em> ( <strong class="ky ir">图 3 </strong>)。</p><p id="84f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一次迭代后，<em class="ls"> θ </em>更新为<em class="ls">θ:=(1–2α)θ= 0.4 * 6 = 2.4<br/></em>第二次迭代后:<em class="ls">θ:=(1–2α)θ= 0.4 * 2.4 = 0.96</em><br/>第三次迭代后:<em class="ls">θ:=(1–2α)θ= 0.4 * 0.96 = 0</em></p><p id="9987" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">诸如此类…</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/39738fdb75e15c85c45282f16a0dd8a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zGom-t59IFdCAs8kvT9VVA@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd oo">Fig.3a</strong> — Steps of gradient descent as it approaches minimum of J(<em class="op">θ). </em><strong class="bd oo"><em class="op">Fig.3b</em></strong><em class="op"> — A plot of </em>J(<em class="op">θ) against the number of iteration, useful to visually monitor convergence.</em></figcaption></figure><p id="6151" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，从第六次迭代到第七次迭代，<em class="ls"> J(θ) </em>减少<em class="ls"> 0.0005 </em>，小于<em class="ls"> 10^(-3) </em>的阈值，此时我们可以宣告收敛。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="2e07" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">调整学习速度</h2><p id="ac1d" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">正如我所承诺的，这里有一些学习率过大或过小的例子，这将帮助你直观地理解为什么选择正确的学习率是至关重要的。</p><p id="7da7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">图 4 </strong>是学习率<em class="ls">过大</em>时。<em class="ls"> θ </em>的值来回振荡跳过最小值，而不是逐渐接近最小值。即使在第十次迭代之后，<em class="ls"> θ </em>仍然离零很远，并且我们预计<em class="ls"> θ </em>会经历更多不必要的曲折，以最终接近<em class="ls"> J(θ) </em>的最小值。事实上，在这种情况下，它需要 23 次迭代才能达到收敛。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/4c88767afb6968f8db56be9e924a53ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GF5pQA8E6pOHpy97J511Bg@2x.png"/></div></div></figure><p id="b92c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相反，<strong class="ky ir">图 5 </strong>显示了过小的学习率如何显著减缓收敛。在这个极端的例子中，即使经过 100 次迭代，仍然远离收敛。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/ead2d5093bd65c5b91b666f5f367a24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6zJzF2HkcKCLO0He5nsD-A@2x.png"/></div></div></figure><p id="8e47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望这证明了监控收敛和相应调整学习速率的重要性。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="ea86" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">总结</h2><p id="4040" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">好了，我们讨论了梯度下降的基本应用，使用一元函数作为目标函数。下次讲它在多元函数和线性回归中的应用。</p><p id="0d0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请发表任何反馈、问题或主题请求。我也希望👏如果你喜欢这篇文章，那么其他人也可以找到这篇文章。</p><p id="076d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">谢谢！</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="5ff4" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">机器学习一点一滴系列</h2><ol class=""><li id="c22b" class="my mz iq ky b kz mt lc mu lf oq lj or ln os lr nd ne nf ng bi translated"><a class="ae kv" href="https://hackernoon.com/machine-learning-bit-by-bit-univariate-gradient-descent-9155731a9e30" rel="noopener ugc nofollow" target="_blank">单变量梯度下降</a></li><li id="865d" class="my mz iq ky b kz nh lc ni lf nj lj nk ln nl lr nd ne nf ng bi translated"><a class="ae kv" href="https://medium.com/@misaogura/machine-learning-bit-by-bit-multivariate-gradient-descent-e198fdd0df85" rel="noopener">多元梯度下降</a></li></ol></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="ba3d" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">资源</h2><div class="ot ou gp gr ov ow"><a href="https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">梯度下降和线性回归导论</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">梯度下降算法是那些“最成功”的算法之一，可以为解决问题提供一个新的视角…</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">spin.atomicobject.com</p></div></div><div class="pf l"><div class="pg l ph pi pj pf pk kp ow"/></div></div></a></div><div class="ot ou gp gr ov ow"><a href="http://mccormickml.com/2014/03/04/gradient-descent-derivation/" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">梯度下降求导克里斯麦考密克</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">对于线性回归，我们有一个线性假设函数，h(x)= ѳ0+ѳ1*x.我们要找出ѳ0 和ѳ1…的值</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">mccormickml.com</p></div></div><div class="pf l"><div class="pl l ph pi pj pf pk kp ow"/></div></div></a></div><div class="ot ou gp gr ov ow"><a href="https://machinelearningmastery.com/linear-regression-for-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">机器学习的线性回归-机器学习掌握</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">线性回归也许是统计学和机器中最著名和最容易理解的算法之一</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">machinelearningmastery.com</p></div></div><div class="pf l"><div class="pm l ph pi pj pf pk kp ow"/></div></div></a></div><div class="ot ou gp gr ov ow"><a href="https://medium.com/@tylerneylon/how-to-write-mathematics-on-medium-f89aa45c42a0" rel="noopener follow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">如何在介质上写数学</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">技术作者简明指南</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">medium.com</p></div></div><div class="pf l"><div class="pn l ph pi pj pf pk kp ow"/></div></div></a></div></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="8069" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">修正</h2><p id="232c" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">【2018 年 12 月 9 日—<em class="ls">“梯度下降在行动”</em>一节学习率出现错别字。最初写为<em class="ls"> α = 3 </em>，而正确的值是<em class="ls"> α = 0.3 </em>。感谢阿尼班·杜塔指出这一点。</p></div></div>    
</body>
</html>