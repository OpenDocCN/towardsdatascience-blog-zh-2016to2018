<html>
<head>
<title>Pytorch Implementation of Perceptual Losses for Real-Time Style Transfer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pytorch实现实时风格转换的感知损失</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-implementation-of-perceptual-losses-for-real-time-style-transfer-8d608e2e9902?source=collection_archive---------2-----------------------#2017-07-31">https://towardsdatascience.com/pytorch-implementation-of-perceptual-losses-for-real-time-style-transfer-8d608e2e9902?source=collection_archive---------2-----------------------#2017-07-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="29ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将简要回顾我在<a class="ae kl" href="http://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>中编写和训练实时风格转换模型的经历。这项工作在很大程度上基于<a class="ae kl" href="https://github.com/abhiskk/fast-neural-style" rel="noopener ugc nofollow" target="_blank"> Abhishek Kadian的实现，</a>，它工作得非常好。我做了一些修改，既是为了好玩，也是为了更熟悉Pytorch。</p><p id="a5f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该模型使用<a class="ae kl" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">中描述的方法进行实时风格转换和超分辨率</a>以及<a class="ae kl" href="https://arxiv.org/pdf/1607.08022.pdf" rel="noopener ugc nofollow" target="_blank">实例归一化</a>。(未实现超分辨率)</p><p id="87cc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我在实现中添加了三个主要部分:</p><ol class=""><li id="2b19" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">使用<a class="ae kl" href="http://pytorch.org/docs/master/torchvision/models.html" rel="noopener ugc nofollow" target="_blank">官方预培训的VGG模型</a></li><li id="70eb" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">在培训期间输出中间结果</li><li id="6a3c" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">添加<a class="ae kl" href="https://www.wikiwand.com/en/Total_variation_denoising" rel="noopener ugc nofollow" target="_blank">总变差正则化</a>如文中所述</li></ol><h2 id="1caf" class="la lb iq bd lc ld le dn lf lg lh dp li jy lj lk ll kc lm ln lo kg lp lq lr ls bi translated">使用官方预先训练的VGG模型</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/816a894e8965c254ba2b99ed8bbbe21f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*QSejfuXzRDhsd_ZYFjhK4A.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Model structure from <a class="ae kl" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">the paper</a></figcaption></figure><p id="dd36" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我们需要快速浏览一下模型结构。本文的主要贡献是提出将生成的图像前馈到预训练的图像分类模型，并从一些中间层提取输出来计算损失，这将产生类似于Gatys等人的<a class="ae kl" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir"><em class="mf"/></strong></a><strong class="jp ir"><em class="mf"/></strong>的结果，但具有明显更少的计算资源。因此，该结构的第一部分是一个“图像变换网络”,它从输入图像生成新的图像。而第二部分简单来说就是一个“损耗网络”，也就是前馈部分。损失网络的权重是固定的，在训练过程中不会更新。</p><p id="57fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Abhishek的实现使用传统的VGG模型，具有BGR信道顺序和[-103.939，-116.779，-123.680]到中心信道的偏移(这似乎也是该论文所使用的)。【pytorch官方预训车型使用统一格式:</p><blockquote class="mg mh mi"><p id="fd05" class="jn jo mf jp b jq jr js jt ju jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj kk ij bi translated">所有预训练模型都期望输入图像以相同的方式归一化，即形状为(3 x H x W)的3通道RGB图像的小批量，其中H和W预计至少为224。必须将图像加载到[0，1]的范围内，然后使用<code class="fe mm mn mo mp b">mean = [0.485, 0.456, 0.406]</code>和<code class="fe mm mn mo mp b">std = [0.229, 0.224, 0.225]</code>进行归一化</p></blockquote><p id="7b51" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是从官方预训练模型中提取输出的代码:</p><pre class="lu lv lw lx gt mq mp mr ms aw mt bi"><span id="3d40" class="la lb iq mp b gy mu mv l mw mx">LossOutput = namedtuple("LossOutput", ["relu1_2", "relu2_2", "relu3_3", "relu4_3"])<br/># <a class="ae kl" href="https://discuss.pytorch.org/t/how-to-extract-features-of-an-image-from-a-trained-model/119/3" rel="noopener ugc nofollow" target="_blank">https://discuss.pytorch.org/t/how-to-extract-features-of-an-image-from-a-trained-model/119/3</a><br/>class LossNetwork(torch.nn.Module):<br/>    def __init__(self, vgg_model):<br/>        super(LossNetwork, self).__init__()<br/>        self.vgg_layers = vgg_model.features<br/>        self.layer_name_mapping = {<br/>            '3': "relu1_2",<br/>            '8': "relu2_2",<br/>            '15': "relu3_3",<br/>            '22': "relu4_3"<br/>        }<br/>    <br/>    def forward(self, x):<br/>        output = {}<br/>        for name, module in self.vgg_layers._modules.items():<br/>            x = module(x)<br/>            if name in self.layer_name_mapping:<br/>                output[self.layer_name_mapping[name]] = x<br/>        return LossOutput(**output)</span></pre><p id="35da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">启动:</p><pre class="lu lv lw lx gt mq mp mr ms aw mt bi"><span id="c94f" class="la lb iq mp b gy mu mv l mw mx">vgg_model = vgg.vgg16(pretrained=True)<br/>if torch.cuda.is_available():<br/>    vgg_model.cuda()<br/>loss_network = LossNetwork(vgg_model)<br/>loss_network.eval()</span></pre><p id="2529" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除非明确指定，否则在VGG模型中没有批处理规范化。因此，激活值与之前的实现相比有很大不同。一般来说，你需要按比例增加样式损失(gram矩阵),因为大多数激活小于1，采用点积会使它更小。</p><h2 id="5c73" class="la lb iq bd lc ld le dn lf lg lh dp li jy lj lk ll kc lm ln lo kg lp lq lr ls bi translated">在培训期间输出中间结果</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi my"><img src="../Images/ec5ba7c94e0c9153352ab578fd2d44de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*ypjisdZxQoiI5vpcd-jhXg.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Epoch 2, 75200th training sample</figcaption></figure><p id="e38b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这在根据风格权重比例调整内容权重时很有帮助。您可以在训练过程中停止训练并重新调整参数，而不必等待4个小时才能完成训练。</p><h2 id="eeda" class="la lb iq bd lc ld le dn lf lg lh dp li jy lj lk ll kc lm ln lo kg lp lq lr ls bi translated">添加<a class="ae kl" href="https://www.wikiwand.com/en/Total_variation_denoising" rel="noopener ugc nofollow" target="_blank">总变差正则化</a>如文中所述</h2><p id="c0af" class="pw-post-body-paragraph jn jo iq jp b jq mz js jt ju na jw jx jy nb ka kb kc nc ke kf kg nd ki kj kk ij bi translated">论文在实验部分提到了这一点，但似乎Abhishek并没有实现:</p><blockquote class="mg mh mi"><p id="dad2" class="jn jo mf jp b jq jr js jt ju jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj kk ij bi translated">输出图像用总变分正则化进行正则化，强度在1 × 10e-6和1 ×10e-4之间，通过每个样式目标的交叉验证进行选择。</p></blockquote><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ne"><img src="../Images/6ca1ee61d1ff2d453fc521bd1cbcdf85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eZLoCIWu9cwajj3v6NTBxg.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">The total variation norm formula for 2D signal images from <a class="ae kl" href="https://www.wikiwand.com/en/Total_variation_denoising" rel="noopener ugc nofollow" target="_blank">Wikipedia</a></figcaption></figure><p id="7150" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这很容易实现:</p><pre class="lu lv lw lx gt mq mp mr ms aw mt bi"><span id="4859" class="la lb iq mp b gy mu mv l mw mx">reg_loss = REGULARIZATION * (<br/>    torch.sum(torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])) + <br/>    torch.sum(torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :]))<br/>)</span></pre><p id="3548" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="http://pytorch.org/docs/master/autograd.html" rel="noopener ugc nofollow" target="_blank"> Pytorch亲笔签名</a>将为您处理反向传播。在实践中，我还没有发现如何调整正则化权重。到目前为止，我使用的权重似乎对输出图像没有太大影响。</p><h1 id="340a" class="nj lb iq bd lc nk nl nm lf nn no np li nq nr ns ll nt nu nv lo nw nx ny lr nz bi translated">培训结果</h1><p id="ce69" class="pw-post-body-paragraph jn jo iq jp b jq mz js jt ju na jw jx jy nb ka kb kc nc ke kf kg nd ki kj kk ij bi translated">该模型使用Microsoft COCO数据集进行训练。图像的大小被调整为256×256，网络被训练大约2个时期，批次大小为4(与纸张相同)。使用GTX1070的训练时间约为4到4.5小时，与论文报道的时间相当。根据我粗略的实验，大量的计算时间被用于标准化输入图像。如果我们使用原始的VGG模型(未测试)，训练可能会更快。在一些手动调整之后，内容权重与样式的比率通常被设置为1 : 10e3 ~ 10e5。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/e1bad91d17e4f96bbe4e0b3f1e90bb1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*6qi0sDapeaYYO_N0f74g4Q.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">An example model (waterfall) (pardon my poor GIMP skill…)</figcaption></figure><p id="a8df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为网络是完全卷积的，你可以在测试时间向网络提供比256 x 256更大或更小的图像。我写了一些脚本来转换动画gif和视频，使用<a class="ae kl" href="http://www.scikit-video.org/" rel="noopener ugc nofollow" target="_blank"> scikit-video </a>和<a class="ae kl" href="https://www.ffmpeg.org/" rel="noopener ugc nofollow" target="_blank"> ffmpeg </a>来取乐:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/9a434979a22e63962a5e7e5327b59e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*BbNyNZOkJrrxFFM0G_YWyA.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Some additional style images used</figcaption></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/34be97d21a5aa502a19491241d67849a.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*UJCCcKG8cNJ_-frEQyx52Q.gif"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Mosaic-styled cat typing</figcaption></figure><p id="bd02" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">原视频鸣谢:<a class="ae kl" href="https://www.youtube.com/watch?v=QdEsM7-_DLI" rel="noopener ugc nofollow" target="_blank">背包环游新西兰</a>。处理速度在每秒4帧左右。我没有调准帧率，所以下面几个视频的时间都比原来慢。</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="od oe l"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Mosaic style</figcaption></figure><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="od oe l"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Candy Style</figcaption></figure><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="od oe l"/></div></figure><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="od oe l"/></div></figure><h2 id="b816" class="la lb iq bd lc ld le dn lf lg lh dp li jy lj lk ll kc lm ln lo kg lp lq lr ls bi translated">经验教训</h2><ol class=""><li id="7425" class="km kn iq jp b jq mz ju na jy of kc og kg oh kk kr ks kt ku bi translated">记住将输出numpy数组裁剪到[0，255]范围，并将其转换为uint8。否则matplot.pyplot.imshow会显示奇怪的结果。花了相当多的时间在错误的方向上调试，认为模型训练代码有一些严重的错误。</li><li id="7ac7" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">记得使用<a class="ae kl" href="http://pytorch.org/docs/master/nn.html#torch.nn.Module.train" rel="noopener ugc nofollow" target="_blank"> model.train() </a>和model.eval()。它仅对包含丢弃或批量规范化图层的模型有效，但这是一个需要保持的好习惯。如果你来自喀拉斯，尤其容易忘记。</li></ol><h2 id="f37e" class="la lb iq bd lc ld le dn lf lg lh dp li jy lj lk ll kc lm ln lo kg lp lq lr ls bi translated">可能的改进和未来的工作</h2><ol class=""><li id="0ef1" class="km kn iq jp b jq mz ju na jy of kc og kg oh kk kr ks kt ku bi translated">有时网络会在开放区域生成一些奇怪的补丁。还不知道是哪里来的，怎么修。</li><li id="20f5" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">对于不同风格的图像，为relu1_2、relu2_2、relu3_3、relu4_3输出分配不同的权重可能会产生更好的结果。</li><li id="b99c" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">尝试使用不同的预训练网络作为损耗网络。</li><li id="80cd" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">也尝试实现超分辨率</li><li id="00a1" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">比较来自<a class="ae kl" href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" rel="noopener ugc nofollow" target="_blank">循环的结果</a>。我试过CycleGAN，但训练时间太长，我当时失去了耐心。应该稍后再试。</li><li id="5d50" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">生成视频时，调整连续帧之间的变化。这应该有助于减少播放过程中的闪烁。我听说有人使用这种技术，但还不知道如何实际操作。</li></ol><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/d19309f220279af2aa3e188c3df321cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:110/format:webp/1*fF8-eqOvNk7JZFryHc8ldw.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">A sample of the weird patches</figcaption></figure><h2 id="f15f" class="la lb iq bd lc ld le dn lf lg lh dp li jy lj lk ll kc lm ln lo kg lp lq lr ls bi translated">感谢阅读！</h2><p id="3bd2" class="pw-post-body-paragraph jn jo iq jp b jq mz js jt ju na jw jx jy nb ka kb kc nc ke kf kg nd ki kj kk ij bi translated">代码位于本Github repo 的<a class="ae kl" href="https://github.com/ceshine/fast-neural-style" rel="noopener ugc nofollow" target="_blank">。由于这基本上是一个个人有趣的项目，文档是不存在的，代码的主要部分驻留在两个Jupyter笔记本中:</a><a class="ae kl" href="https://github.com/ceshine/fast-neural-style/blob/201707/style-transfer.ipynb" rel="noopener ugc nofollow" target="_blank"> style-transfer.ipynb </a>和<a class="ae kl" href="https://github.com/ceshine/fast-neural-style/blob/201707/Video.ipynb" rel="noopener ugc nofollow" target="_blank"> Video.ipynb </a>。很抱歉。</p><p id="63c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(2011/11/30更新:修复了上述笔记本的链接。与此同时，这里有更新版本的笔记本:<a class="ae kl" href="https://github.com/ceshine/fast-neural-style/blob/master/notebooks/01-image-style-transfer.ipynb" rel="noopener ugc nofollow" target="_blank">01-image-style-transfer . ipynb</a>和<a class="ae kl" href="https://github.com/ceshine/fast-neural-style/blob/master/notebooks/03-Video-Creation.ipynb" rel="noopener ugc nofollow" target="_blank"> 03-Video-Creation.ipynb </a>。)</p></div></div>    
</body>
</html>