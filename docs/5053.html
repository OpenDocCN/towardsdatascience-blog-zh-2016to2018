<html>
<head>
<title>Multi-Class Text Classification Model Comparison and Selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多类文本分类模型的比较与选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568?source=collection_archive---------0-----------------------#2018-09-25">https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568?source=collection_archive---------0-----------------------#2018-09-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/c995474a9dba4edf24d50327b6443423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_kVRQJ-of1MW7dGXnRn38w.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo credit: Pixabay</figcaption></figure><div class=""/><div class=""><h2 id="c0fa" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">自然语言处理，word2vec，支持向量机，词袋，深度学习</h2></div><p id="6e56" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">当使用给定的数据集处理<a class="ae lq" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank">监督机器学习</a>问题时，我们尝试不同的算法和技术来搜索模型以产生一般假设，然后对未来实例做出最准确的预测。相同的原理适用于文本(或文档)分类，其中有许多模型可用于训练文本分类器。<a class="ae lq" href="https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice" rel="noopener ugc nofollow" target="_blank">问题“我应该用什么机器学习模型”的答案总是“视情况而定”即使是最有经验的数据科学家也无法在试验之前判断哪种算法的性能最佳</a>。</p><p id="8a6a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这就是我们今天要做的:使用我们在以前的文章(以及更多)中介绍的关于文本分类的所有内容，并在我们训练的文本分类模型之间进行比较，以便为我们的问题选择最准确的模型。</p><h1 id="3c12" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">数据</h1><p id="4652" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated">我们使用的是堆栈溢出问题和标签的相对较大的数据集。这些数据可以在<a class="ae lq" href="https://bigquery.cloud.google.com/dataset/bigquery-public-data:stackoverflow" rel="noopener ugc nofollow" target="_blank">的 Google BigQuery </a>中找到，也可以在这个云存储 URL 上公开获得:<a class="ae lq" href="https://storage.googleapis.com/tensorflow-workshop-examples/stack-overflow-data.csv" rel="noopener ugc nofollow" target="_blank">https://Storage . Google APIs . com/tensor flow-workshop-examples/stack-overflow-data . CSV</a>。</p><h1 id="fbdd" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">探索数据</h1><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">explore</figcaption></figure><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mu"><img src="../Images/f45dda185dd3192c5788a80ca89054f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HT0LEIu3vrMC031X32GEaQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 1</figcaption></figure><p id="b3f1" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="mv"> 10276752 </em> </strong></p><p id="045b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们的数据中有超过 1000 万个单词。</p><pre class="mo mp mq mr gt mw mx my mz aw na bi"><span id="ff1e" class="nb ls jf mx b gy nc nd l ne nf">my_tags = ['java','html','asp.net','c#','ruby-on-rails','jquery','mysql','php','ios','javascript','python','c','css','android','iphone','sql','objective-c','c++','angularjs','.net']<br/>plt.figure(figsize=(10,4))<br/>df.tags.value_counts().plot(kind='bar');</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ng"><img src="../Images/77777166378cb7b2459883f85c733156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zIUX8_ybwKC7UDlPOmvTeA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 2</figcaption></figure><p id="15aa" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">班级非常平衡。</p><p id="2fc0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们想看看一些帖子和标签对。</p><pre class="mo mp mq mr gt mw mx my mz aw na bi"><span id="95bd" class="nb ls jf mx b gy nc nd l ne nf">def print_plot(index):<br/>    example = df[df.index == index][['post', 'tags']].values[0]<br/>    if len(example) &gt; 0:<br/>        print(example[0])<br/>        print('Tag:', example[1])</span><span id="0c94" class="nb ls jf mx b gy nh nd l ne nf">print_plot(10)</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/1600d0d867e227cc148ac8355fb57959.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VHjd7ueJCRF1Eo0Lnin4rw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 3</figcaption></figure><pre class="mo mp mq mr gt mw mx my mz aw na bi"><span id="6d85" class="nb ls jf mx b gy nc nd l ne nf">print_plot(30)</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/6a7507fdbb69a7a94bc4dff7a25e0ef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-Vc1i5PjZ96XeUtps-HLA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 4</figcaption></figure><p id="ff43" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">如你所见，文本需要清理。</p><h1 id="9326" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">文本预处理</h1><p id="42b7" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated">到目前为止，我们看到的文本清理技术在实践中非常有效。根据您可能遇到的文本类型，可能需要包括更复杂的文本清理步骤。但是请记住，我们添加的步骤越多，文本清理需要的时间就越长。</p><p id="aeef" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">对于这个特定的数据集，我们的文本清理步骤包括 HTML 解码、删除停用词、将文本改为小写、删除标点符号、删除不良字符等等。</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">clean_text</figcaption></figure><p id="ea56" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在我们可以看看一个干净的帖子:</p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/eaa7b53db5660b3a31f97082428ec30d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6uxeYLgeMsEh2xe0aIXc4w.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 5</figcaption></figure><p id="060b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">好多了。</p><pre class="mo mp mq mr gt mw mx my mz aw na bi"><span id="4aa0" class="nb ls jf mx b gy nc nd l ne nf">df['post'].apply(lambda x: len(x.split(' '))).sum()</span></pre><p id="9092" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="mv"> 3421180 </em> </strong></p><p id="0abf" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在文本清理和删除停用词后，我们只有 300 多万个单词可以使用！</p><p id="e113" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">分割数据集后，接下来的步骤包括特征工程。我们将把文本文档转换成令牌计数矩阵(CountVectorizer)，然后将计数矩阵转换成规范化的 tf-idf 表示(tf-idf transformer)。之后，我们从<a class="ae lq" href="http://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn 库</a>中训练几个分类器。</p><pre class="mo mp mq mr gt mw mx my mz aw na bi"><span id="5746" class="nb ls jf mx b gy nc nd l ne nf">X = df.post<br/>y = df.tags<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)</span></pre><h1 id="d04a" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated"><strong class="ak">多项式模型的朴素贝叶斯分类器</strong></h1><p id="b1d8" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated">有了我们的特征之后，我们可以训练一个分类器来预测一篇文章的标签。我们将从一个<a class="ae lq" href="http://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯</a>分类器开始，它为这个任务提供了一个很好的基线。<code class="fe nl nm nn mx b">scikit-learn</code>包括该分类器的几种变体；最适合文本的是多项式变量。</p><p id="e0ff" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为了使矢量器= &gt;转换器= &gt;分类器更容易使用，我们将在 Scilkit-Learn 中使用<code class="fe nl nm nn mx b">Pipeline</code>类，它的行为类似于一个复合分类器。</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">nb</figcaption></figure><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/0b9884cbd8aede126e31d91a6fa397ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rSiw_RQjar22G9Y-GE1isA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 6</figcaption></figure><p id="cbd0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们达到了 74%的准确率。</p><h1 id="2b16" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated"><strong class="ak">线性支持向量机</strong></h1><p id="5647" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated"><a class="ae lq" href="http://scikit-learn.org/stable/modules/svm.html#svm" rel="noopener ugc nofollow" target="_blank">线性支持向量机</a>被广泛认为是最好的文本分类算法之一。</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">svm</figcaption></figure><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/95200e186d4b1ee1836315d628ac74e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OYttvEbF64M46gkJYdiBIA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 7</figcaption></figure><p id="d580" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们获得了 79%的更高准确率，比朴素贝叶斯提高了 5%。</p><h1 id="94a6" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">逻辑回归</h1><p id="be4b" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated">Logistic 回归是一种简单易懂的分类算法，可以很容易地推广到多个类。</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">logreg</figcaption></figure><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nq"><img src="../Images/9c02c0cd5a35ef7c6ae489e506327519.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jAfdo2z-duy5o0vBGb21AQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 8</figcaption></figure><p id="7a4e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们达到了 78%的准确率，比朴素贝叶斯高 4%，比 SVM 低 1%。</p><p id="8678" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">正如你所看到的，遵循一些非常基本的步骤并使用一个简单的线性模型，我们能够在这个多类文本分类数据集上达到高达 79%的准确率。</p><p id="1f6e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">使用相同的数据集，我们将尝试一些高级技术，如单词嵌入和神经网络。</p><p id="878d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在，让我们尝试一些复杂的功能，而不仅仅是简单地计算单词。</p><h1 id="c533" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">Word2vec 和逻辑回归</h1><p id="4654" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> Word2vec </a>和<a class="ae lq" href="https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e" rel="noopener"> doc2vec </a>一样，属于文本预处理阶段。特别是将文本转换成一行数字的部分。Word2vec 是一种映射类型，它允许具有相似含义的单词具有相似的向量表示。</p><p id="b416" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Word2vec 背后的想法相当简单:我们希望使用周围的单词，用神经网络来表示目标单词，神经网络的隐藏层对单词表示进行编码。</p><p id="cecf" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">首先，我们加载一个 word2vec 模型。它已经由谷歌在一个<a class="ae lq" href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit" rel="noopener ugc nofollow" target="_blank">1000 亿字的谷歌新闻语料库</a>上进行了预训练。</p><pre class="mo mp mq mr gt mw mx my mz aw na bi"><span id="5988" class="nb ls jf mx b gy nc nd l ne nf">from gensim.models import Word2Vec</span><span id="2e34" class="nb ls jf mx b gy nh nd l ne nf">wv = gensim.models.KeyedVectors.load_word2vec_format("GoogleNews-vectors-negative300.bin.gz", binary=True)<br/>wv.init_sims(replace=True)</span></pre><p id="7001" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们可能想要探索一些词汇。</p><pre class="mo mp mq mr gt mw mx my mz aw na bi"><span id="7ebb" class="nb ls jf mx b gy nc nd l ne nf">from itertools import islice<br/>list(islice(wv.vocab, 13030, 13050))</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0e176f9047e9cbc22c94cfd834a5d06d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*YdYyuoHtwxzUU6kMcqgfjA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 9</figcaption></figure><p id="a1e3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">基于 BOW 的方法，包括平均、求和、加权加法。常见的方法是对两个字向量进行平均。因此，我们将遵循最常见的方式。</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">word_averaging</figcaption></figure><p id="23f9" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们将对文本进行标记化，并将标记化应用于“post”列，并将单词向量平均应用于标记化的文本。</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">w2v_tokenize_text</figcaption></figure><p id="c752" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">是时候看看逻辑回归分类器在这些单词平均文档特征上的表现了。</p><pre class="mo mp mq mr gt mw mx my mz aw na bi"><span id="2880" class="nb ls jf mx b gy nc nd l ne nf">from sklearn.linear_model import LogisticRegression<br/>logreg = LogisticRegression(n_jobs=1, C=1e5)<br/>logreg = logreg.fit(X_train_word_average, train['tags'])<br/>y_pred = logreg.predict(X_test_word_average)<br/>print('accuracy %s' % accuracy_score(y_pred, test.tags))<br/>print(classification_report(test.tags, y_pred,target_names=my_tags))</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/bff034e068cdb327e2bf0c85d239840e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sIjSfAIXEEBUweN4sBGrzw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 10</figcaption></figure><p id="795d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这很令人失望，是我们迄今为止看到的最糟糕的一次。</p><h1 id="a9a8" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated"><strong class="ak"> Doc2vec 和 Logistic 回归</strong></h1><p id="ba17" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated">word2vec 的相同想法可以扩展到文档，我们不是学习单词的特征表示，而是学习句子或文档的特征表示。为了对一个<a class="ae lq" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec </a>有个大概的了解，可以把它想象成文档中所有单词的单词向量表示的数学平均值。<a class="ae lq" href="https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e" rel="noopener"> Doc2Vec </a>扩展了<a class="ae lq" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec </a>的概念，然而单词只能捕捉这么多，有时我们需要文档之间的关系，而不仅仅是单词。</p><p id="6b9c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为我们的堆栈溢出问题和标签数据训练 doc2vec 模型的方式与我们用 Doc2vec 和逻辑回归训练<a class="ae lq" rel="noopener" target="_blank" href="/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4">多类文本分类时非常相似。</a></p><p id="ff63" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">首先，我们给句子贴上标签。<a class="ae lq" href="https://radimrehurek.com/gensim/models/doc2vec.html" rel="noopener ugc nofollow" target="_blank"> Gensim 的 Doc2Vec </a>实现要求每个文档/段落都有一个与之关联的标签。我们通过使用<code class="fe nl nm nn mx b">TaggedDocument</code>方法来做到这一点。格式为“TRAIN_i”或“TEST_i ”,其中“I”是帖子的虚拟索引。</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">label_sentences</figcaption></figure><p id="ff0c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">根据<a class="ae lq" href="https://github.com/RaRe-Technologies/gensim/blob/ca0dcaa1eca8b1764f6456adac5719309e0d8e6d/docs/notebooks/doc2vec-IMDB.ipynb" rel="noopener ugc nofollow" target="_blank"> Gensim doc2vec 教程</a>，它的 doc2vec 类是针对整个数据进行训练的，我们也会这样做。让我们看看带标签的文档是什么样子的:</p><pre class="mo mp mq mr gt mw mx my mz aw na bi"><span id="4704" class="nb ls jf mx b gy nc nd l ne nf">all_data[:2]</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/3265ab67e05b9fa1deba43947062a1fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hDUj9TvHp3pNbXPa12xNVw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 11</figcaption></figure><p id="a172" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">当训练 doc2vec 时，我们将改变以下参数:</p><ul class=""><li id="2dd3" class="nu nv jf kw b kx ky la lb ld nw lh nx ll ny lp nz oa ob oc bi translated"><code class="fe nl nm nn mx b">dm=0</code>，使用分布式单词包(DBOW)。</li><li id="e22b" class="nu nv jf kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated"><code class="fe nl nm nn mx b">vector_size=300</code>，300 个向量维特征向量。</li><li id="ec73" class="nu nv jf kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated"><code class="fe nl nm nn mx b">negative=5</code>，指定要抽取多少个“干扰词”。</li><li id="d9af" class="nu nv jf kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated"><code class="fe nl nm nn mx b">min_count=1</code>，忽略总频率低于此的所有单词。</li><li id="20e9" class="nu nv jf kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated"><code class="fe nl nm nn mx b">alpha=0.065</code>，初始学习率。</li></ul><p id="9d62" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们初始化模型并训练 30 个时期。</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">train_doc2vec</figcaption></figure><p id="99e3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">接下来，我们从训练好的 doc2vec 模型中获取向量。</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">get_vectors</figcaption></figure><p id="22f5" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">最后，我们得到一个由 doc2vec 特征训练的逻辑回归模型。</p><pre class="mo mp mq mr gt mw mx my mz aw na bi"><span id="6e90" class="nb ls jf mx b gy nc nd l ne nf">logreg = LogisticRegression(n_jobs=1, C=1e5)<br/>logreg.fit(train_vectors_dbow, y_train)<br/>logreg = logreg.fit(train_vectors_dbow, y_train)<br/>y_pred = logreg.predict(test_vectors_dbow)<br/>print('accuracy %s' % accuracy_score(y_pred, y_test))<br/>print(classification_report(y_test, y_pred,target_names=my_tags))</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/330d52792580821c97779320a2b24fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-HC_4f5tBStj_1kp6v_iDA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 12</figcaption></figure><p id="1cfb" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们达到了 80%的准确率，比 SVM 高出 1%。</p><h1 id="dc03" class="lr ls jf bd lt lu lv lw lx ly lz ma mb kl mc km md ko me kp mf kr mg ks mh mi bi translated">用 Keras 鞠躬</h1><p id="bcd8" class="pw-post-body-paragraph ku kv jf kw b kx mj kg kz la mk kj lc ld ml lf lg lh mm lj lk ll mn ln lo lp ij bi translated">最后，我们将使用 Python 深度学习库<a class="ae lq" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>进行文本分类。</p><p id="77fe" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">下面的代码很大程度上取自谷歌的一个研讨会。过程是这样的:</p><ul class=""><li id="0b73" class="nu nv jf kw b kx ky la lb ld nw lh nx ll ny lp nz oa ob oc bi translated">将数据分成训练集和测试集。</li><li id="e8e9" class="nu nv jf kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">使用<code class="fe nl nm nn mx b">tokenizer</code>方法来统计我们词汇中的独特单词，并给每个单词分配索引。</li><li id="73d3" class="nu nv jf kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">调用<code class="fe nl nm nn mx b">fit_on_texts()</code>自动创建我们词汇表的单词索引查找。</li><li id="e87d" class="nu nv jf kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">我们通过向记号赋予器传递一个<code class="fe nl nm nn mx b">num_words</code>参数来限制我们的词汇到顶部的单词。</li><li id="f257" class="nu nv jf kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">有了我们的标记器，我们现在可以使用<code class="fe nl nm nn mx b">texts_to_matrix</code>方法来创建训练数据，我们将通过我们的模型。</li><li id="9123" class="nu nv jf kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">我们给我们的模型输入一个热点向量。</li><li id="b06f" class="nu nv jf kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">将我们的特征和标签转换成 Keras 可以读取的格式后，我们就可以构建我们的文本分类模型了。</li><li id="f0c6" class="nu nv jf kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">当我们构建模型时，我们需要做的就是告诉 Keras 我们的输入数据、输出数据的形状，以及每一层的类型。keras 会照看剩下的。</li><li id="9e6c" class="nu nv jf kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">在训练模型时，我们将调用<code class="fe nl nm nn mx b">fit()</code>方法，向它传递我们的训练数据和标签、批量大小和时期。</li></ul><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">keras_training</figcaption></figure><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/72171eaf6521425f82adb20532adb673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XjOMQqdINovS8X5ZJRiKIg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 13</figcaption></figure><p id="bbef" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">精确度为:</p><pre class="mo mp mq mr gt mw mx my mz aw na bi"><span id="4938" class="nb ls jf mx b gy nc nd l ne nf">score = model.evaluate(x_test, y_test,<br/>                       batch_size=batch_size, verbose=1)<br/>print('Test accuracy:', score[1])</span></pre><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/7a80b7dcb296666c046770935f7f1a71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MgbFn4GxHrl5estHq1Kk7w.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 14</figcaption></figure><p id="a735" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">那么，哪个模型最适合这个特定的数据集呢？我将让你来决定。</p><p id="fb76" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><a class="ae lq" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>可以在<a class="ae lq" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。祝你有丰富的一天！</p><p id="34e9" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">参考资料:</p><p id="9ca8" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><a class="ae lq" href="https://github.com/RaRe-Technologies/movie-plots-by-genre/blob/master/ipynb_with_output/Document%20classification%20with%20word%20embeddings%20tutorial%20-%20with%20output.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/RaRe-Technologies/movie-plots-by-genre/blob/master/ipynb _ with _ output/Document % 20 class ification % 20 with % 20 word % 20 embedding % 20 tutorial % 20-% 20 with % 20 output . ipynb</a></p><p id="981c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><a class="ae lq" href="https://github.com/tensorflow/workshops/blob/master/extras/keras-bag-of-words/keras-bow-model.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/tensor flow/workshop/blob/master/extras/keras-bag-of-words/keras-bow-model . ipynb</a></p><p id="9a89" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><a class="ae lq" href="https://datascience.stackexchange.com/questions/20076/word2vec-vs-sentence2vec-vs-doc2vec" rel="noopener ugc nofollow" target="_blank">https://data science . stack exchange . com/questions/20076/word 2 vec-vs-sentence 2 vec-vs-doc 2 vec</a></p></div></div>    
</body>
</html>