<html>
<head>
<title>Word Representation in Natural Language Processing Part II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的词表示第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-representation-in-natural-language-processing-part-ii-1aee2094e08a?source=collection_archive---------10-----------------------#2018-12-09">https://towardsdatascience.com/word-representation-in-natural-language-processing-part-ii-1aee2094e08a?source=collection_archive---------10-----------------------#2018-12-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/fff9baf1b206c6c4c3253565ad96b583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PRwFsUqvxbORbzAA3Lmo7g.jpeg"/></div></div></figure><div class=""/><p id="736f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在单词表征系列的前一部分(<a class="ae kw" href="https://medium.com/p/e4cd54fed3d4/edit" rel="noopener"> <strong class="ka jc">第一部分</strong> </a> <strong class="ka jc"> ) </strong>中，我谈到了固定的单词表征，它对单词的语义(意义)和相似性不做任何假设。在这一部分，我将描述一系列的<strong class="ka jc">分布式单词表示</strong>。主要思想是将单词表示为特征向量。vector 中的每个条目代表单词含义中的一个隐藏特征。它们可以揭示语义或句法依赖性。在下面的例子中，我们看到了<em class="kx"> 300 维的</em>单词表示。我们可以观察到<em class="kx">‘flight’</em>和<em class="kx">‘plane’</em>的向量值与<em class="kx"> s </em>的值相似，它们之间的数值差异很小。这同样适用于<em class="kx">‘河’</em>和<em class="kx">‘湖’</em>，因为它们具有密切相关的含义。</p><figure class="kz la lb lc gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ky"><img src="../Images/b54dfc11ddbf295f98b048204b53a450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jl6FC5oRZtfdjyugJo1Y6Q.png"/></div></div></figure><p id="f944" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，它生成有用的属性，如线性关系。一个广为使用的例子是这样的类比:国王对女王就像男人对女人<em class="kx">一样。</em>国王和王后之间的差异几乎等同于向量空间中男人和女人之间的差异，这导致以下运算有效。</p><blockquote class="ld"><p id="9600" class="le lf jb bd lg lh li lj lk ll lm kv dk translated"><em class="ln">“国王—王后+男人”最接近“女人”</em></p></blockquote><h2 id="7960" class="lo lp jb bd lq lr ls dn lt lu lv dp lw kj lx ly lz kn ma mb mc kr md me mf mg bi translated">Word2Vec</h2><p id="c0ec" class="pw-post-body-paragraph jy jz jb ka b kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ml kt ku kv ij bi translated">一个广泛使用的分布式单词表示是 Skip-Gram 模型，它是 Word2Vec 库的一部分。它是由谷歌的托马斯·米科洛夫领导的研究小组创造的。主要思想是通过单词的邻居来表示单词。它试图预测给定单词的所有相邻单词(上下文)。</p><p id="c441" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">根据<a class="ae kw" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank">文件</a>，模型的目标定义如下:</p><figure class="kz la lb lc gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mm"><img src="../Images/f13a027e284e360d56b5b8d8cae657de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Btopxk5VzGPsKMUjprvcHw.png"/></div></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">Objective function for Skip-Gram model</figcaption></figure><p id="e0d2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<em class="kx"> w </em>是训练字，<em class="kx"> c </em>是上下文的大小。所以它的目标是找到可以预测周围单词的单词表征。</p><p id="b7ef" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以在我们的数据集上训练 Word2Vec，或者加载预训练的向量。谷歌公布了在谷歌新闻数据集(约 1000 亿字)的一部分上训练的预训练向量。该模型包含 300 万个单词和短语的 300 维向量。这里有<a class="ae kw" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank">这里有</a>。</p><p id="6a09" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这些步骤使我们能够使用 Word2Vec 库中预先训练好的向量。</p><p id="78e4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以使用 gensim 加载预训练的单词向量，例如:</p><figure class="kz la lb lc gt is"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="6ed1" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">预训练向量表示的查找是这样的</p><figure class="kz la lb lc gt is"><div class="bz fp l di"><div class="mr ms l"/></div></figure><h2 id="9a42" class="lo lp jb bd lq lr mt dn lt lu mu dp lw kj mv ly lz kn mw mb mc kr mx me mf mg bi translated">手套</h2><p id="2a01" class="pw-post-body-paragraph jy jz jb ka b kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ml kt ku kv ij bi translated">分布式单词表示的另一个成员是 Glove，它是全局向量的缩写。虽然 Word2Vec 捕获某些局部上下文窗口，但 GloVe 利用了来自语料库(一个大型文本集合)的单词的总体共现统计。它包括两个重要步骤。首先，构建一个词共现矩阵。对于每个单词，我们计算条件概率，例如对于单词<em class="kx"> water P(k|water) </em>，其中<em class="kx"> k </em>是来自词汇表的单词。如果<em class="kx"> k </em>是<em class="kx">流</em>，则<em class="kx"> P </em>的值为高，如果<em class="kx"> k </em>是<em class="kx">时尚</em>，则期望值为低，因为它们通常不会同时出现。在完成所有的统计计算后，大矩阵就形成了。然后通过归一化计数和对数平滑来减少高维上下文矩阵，如下所示。</p><figure class="kz la lb lc gt is"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="1efa" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们还可以使用 gensim 加载预训练的手套向量，这些向量是在维基百科数据上训练的。</p><figure class="kz la lb lc gt is"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="0b59" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们需要将 GloVe 格式转换为 Word2Vec，以便与 gensim 一起使用，例如:</p><figure class="kz la lb lc gt is"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="e5d0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">更多技术细节可以在<a class="ae kw" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="3adc" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Glove 和 Word2Vec 都允许对单词进行有用的操作，比如查找语义相似的单词。让我们比较一下最相似单词的结果。</p><p id="1bc3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用 Glove 获得与'<em class="kx"> flight </em>'意思相似的单词。</p><figure class="kz la lb lc gt is"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="ed31" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用 Word2Vec 获得与'<em class="kx">航班</em>'意思相似的单词。</p><figure class="kz la lb lc gt is"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="43b4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">正如我们所观察到的，这两个向量的输出并不精确。不同之处在于相似度得分和返回的单词。</p><p id="93a7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在上述方法中，分布式单词表示是一种强大的技术。它没有简单方法的不良特性，并且可能将单词的语义信息合并到它们的表示中。然而，它不能为不在词汇表中的单词产生向量。此外，罕见词的向量表示法学得不够好。在这些情况下，最好使用 FastText 模型，这将在下一部分描述。</p></div></div>    
</body>
</html>