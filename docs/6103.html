<html>
<head>
<title>Neural Networks I: Notation and building blocks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络 I:符号和构建模块</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-i-notation-and-building-blocks-817b1d2ea04b?source=collection_archive---------7-----------------------#2018-11-26">https://towardsdatascience.com/neural-networks-i-notation-and-building-blocks-817b1d2ea04b?source=collection_archive---------7-----------------------#2018-11-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="0478" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/neural-network-notes/latest" rel="noopener">神经网络简介</a></h2><div class=""/><p id="7ce2" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这个关于神经网络的<a class="ae ku" href="https://medium.com/@pabloruizruiz/neural-networks-notes-fa42ab388bb8" rel="noopener">系列帖子</a>是脸书 PyTorch 挑战赛期间笔记收集的一部分，在 Udacity 的<a class="ae ku" href="https://eu.udacity.com/course/deep-learning-nanodegree--nd101" rel="noopener ugc nofollow" target="_blank">深度学习纳米学位项目之前。</a></p><h1 id="9e9a" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">内容</h1><ol class=""><li id="f4d4" class="lt lu iq jy b jz lv kd lw kh lx kl ly kp lz kt ma mb mc md bi translated"><strong class="jy ja">神经元</strong></li><li id="1ac1" class="lt lu iq jy b jz me kd mf kh mg kl mh kp mi kt ma mb mc md bi translated"><strong class="jy ja">连接</strong></li><li id="1a35" class="lt lu iq jy b jz me kd mf kh mg kl mh kp mi kt ma mb mc md bi translated"><strong class="jy ja">层——神经元与连接</strong></li></ol><p id="ee26" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">3.1 神经元层</p><p id="0c71" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">3.2.连接层— <em class="mj"> PyTorch 示例</em></p><p id="a21e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> 4。符号歧义:Y = X W vs Wt X </strong></p><h1 id="af4a" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">1.神经元</h1><p id="1589" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">神经元是神经网络的组成部分。</p><p id="f63c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">毕竟，神经网络是神经元的简单集合，它们朝着同一个目标一起工作，通常是执行给定的任务，实现尽可能少的错误。</p><p id="5006" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">由于我们用于神经网络的图形表示，如图 1 所示，神经元也接收节点的名称，因为每个节点对应于图形表示中的一个唯一节点。图形表示不仅仅是用来简单地观察这些网络，也是我们如何计算实际发生的操作的一个基本方面。这将在更高级的部分“图形方法”中介绍，不在本节的讨论范围之内。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/ceb774904c424533991e41dac48a04d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*lNf5c1hCW1JSTzArLN6mOQ.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Figure 1. Graph representation of a neural network</figcaption></figure><p id="9a95" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">只要看一下图 1，我们就可以通过观察神经元看起来是按层组织的，一个接一个地堆叠起来，从而对下一部分有一个直觉。然而，让我们先来看看单个网络的细节，并评估它们与真实生物神经元的接近程度。</p><p id="771a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在图 2 中，我们只是从网络中随机选取了一个神经元，并观察它由不同的部分组成。放大镜所指的那个神经元将对应于它上面的生物表示中的第二个网络。</p><p id="dcc3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们观察哪些部分？</p><p id="a42d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">收集器</strong></p><p id="0f73" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我将其命名为收集器，因为它的功能是聚集来自其他网络的传入连接。用生物学术语来说就是突触连接。请注意，合计行为的数学等价形式是求和。我们将看到，这种求和不仅仅是对所有连接求和，而是对它们进行加权，这意味着每个连接对收集器都有不同的“重要性”。</p><p id="1fc8" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">激活器</strong></p><p id="438f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">聚集在收集器中的接收信号的激活是发生在网络主体内部的过程。目前还不清楚对那个过程来说什么是更好的表示，在深度学习中，它是通过所谓的<strong class="jy ja">激活函数</strong>来建模的。这些函数的形式各不相同，但它们只是将一个函数应用于采集器传递的信号。这些函数是非线性的，因为激活器是神经元(或网络)的唯一部分，在那里我们可以引入学习非线性映射的能力，这是绝大多数真实世界场景所需要的。</p><p id="d0f3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">经销商</strong></p><p id="b45f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在它的名字之后，分配器简单地收集激活器之后的信号，并以相同的强度将它发送到它所属的神经元所连接的其余神经元。在神经网络的图形表示中，它指的是堆叠神经元的下一层中的每一个单个神经元。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mz"><img src="../Images/7c3dfe358b1ff1cdf5fe9636fa37f846.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*49iC8nh9YhIntYukjCjHLQ.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Figure 2. Biological neuron vs Artificial neuron</figcaption></figure><h1 id="fdd7" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">2.连接</h1><p id="b40b" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">在人工神经网络的图形表示中，不同神经元之间的连接由连接两个节点的边来表示。它们被称为<strong class="jy ja">权重</strong>，通常表示为<em class="mj">和</em>。神经网络上的权重是任何参数模型上的<strong class="jy ja">参数</strong>的特例。</p><p id="0df3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如图 3 所示，我们可以给组成每一层的每一个神经元一个数字。权重的子指数表示哪些神经元被连接。因此，<em class="mj"> wij </em>表示在前一层神经元<em class="mj"> i </em>与后一层神经元<em class="mj"> j </em>之间建立的连接。</p><p id="4621" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">一种更有效的表示两层之间所有连接的数学方法是将它们收集到<strong class="jy ja">权重矩阵</strong>、<strong class="jy ja">、<em class="mj"> W </em>、</strong>中。该矩阵将具有以下形式:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/1b71676097a5507e5627858093219958.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*Y4_-7cp7ngl7rph1FbL-5w.png"/></div></figure><p id="7905" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">其中<em class="mj"> l </em>表示这些权重的层的索引。<em class="mj"> N </em>为上一层神经元的数量，<em class="mj"> M </em>为下一层神经元的数量。因此，行数由第一层神经元的数量定义，而列数由第二层神经元的数量定义，这当然不必相同。为简单起见，图 3 中省略了<em class="mj"> l </em>索引。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nf"><img src="../Images/22170ee94bcfbe09e2350e9dfd077cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h2HobJORlJp8PVqAYl_0Ng.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Figure 3. Connections of neurons between 2 layers</figcaption></figure><p id="b6da" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">为了对权重矩阵有更强的直觉，让我们给网络的输入命名，看看矩阵的行和列代表什么。</p><p id="24f8" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们可以把图 4 中的输入看作是，例如花在学习理论上的小时数和花在做练习上的小时数。模型的输出可以是通过考试的概率。所以我们要输入变量和<em class="mj"> x1 </em>和<em class="mj"> x2 </em>可以简化为输入向量<strong class="jy ja"> <em class="mj"> X </em> </strong> <em class="mj">。</em></p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ng"><img src="../Images/7cd8e9cbef11138177ed92baab853651.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pRmedcXssB_FC-TF-_HUKg.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Figure 4. Computing the output of a layer</figcaption></figure><p id="bd58" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">通过矩阵乘法，我们看到输入如何乘以每个权重，以产生输出<strong class="jy ja"> <em class="mj"> H </em> </strong>(隐藏后)，其维数与该隐藏层的神经元数相匹配(我们将在下一节中更多地讨论层)。在这种情况下，<em class="mj"> h1 </em>，<em class="mj"> h2 </em>，<em class="mj"> h3 </em>。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/cdcf37ec4a6d99d81e04c7b6a98187bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*w6iYPr60FWGvPGuY_cGwXA.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Figure 5. Weights matrix</figcaption></figure><p id="5dfd" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们更仔细地看看权重矩阵。在图 5 中，我们可以看到每一列实际上代表了后一层中每个神经元的收集器。因此，每一行都代表前一层中每个神经元的分布器。</p><h1 id="a5ae" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">3.层——神经元与连接</h1><p id="99f8" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">我们已经介绍了人工神经元是如何分组分层的。特定层的每个神经元接收其所属层之前的层的每个单个神经元的信号，并将信号发送到下一层的每个单个神经元。</p><h2 id="e682" class="ni kw iq bd kx nj nk dn lb nl nm dp lf kh nn no lj kl np nq ln kp nr ns lr iw bi translated">神经元层</h2><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/38b78ba531b6546b52978b728e44ccf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*VlaMEq_L5lGT_te6AO8jCg.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Figure 6. Layers of an artificial neural network</figcaption></figure><p id="4ac6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">深度学习中的单词层然后被用来称为神经元的每个堆叠聚集。在图 6 之后，第一层通常称为输入层。这是因为这是将输入引入网络以初始化正向传递的层，如图 4 所示。最后一层接收输出层的名称。直观上，它将把每个神经元执行的所有计算的输出结果提供给输入。输入层和输出层之间的所有中间层称为隐藏层。这些层通过创建数据特征的分层表示来执行所谓的表示学习。在使用了许多隐藏层之后，术语“深度学习”“T21”出现了，就这些层的数量而言，它让位于更深的网络。</p><h2 id="4ef6" class="ni kw iq bd kx nj nk dn lb nl nm dp lf kh nn no lj kl np nq ln kp nr ns lr iw bi translated">连接的层次</h2><p id="c64e" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">在某些情况下,“层”这个词也用来指网络的权值层。因此，我们可以在网络中有两种堆叠实体表示，如图 7 所示。</p><p id="a7ed" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">左手边代表典型的神经元层。右手边代表连接的层，其中每一层代表封装该特定层的参数的权重矩阵。</p><p id="5f67" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">记住这一点是很有用的，因为当我们实现我们的网络时，例如在 PyTorch 中，我们通常定义两层神经元之间连接的隐藏单元的数量。因此，我们给出权重矩阵的维数，并遵循右侧的表示。在这种情况下，PyTorch 中的深度前馈神经网络实现为:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nu"><img src="../Images/55164dfccc247472b42a9171af786afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y_6aRMi4A5HTJR7Y9rCjKA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Figure 7. Layers of neuron vs Layers of weights</figcaption></figure><p id="2652" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">图 8 表示在<em class="mj"> PyTorch </em>中实现的图中使用的上述网络的输出。我们简单地传递了输入和输出维度以及一个列表，其中每个条目是每个隐藏层中的神经元数量<em class="mj"> (2，2，[3，3]) </em>。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nv"><img src="../Images/7c68eab765107ef7fbeb3ee99ec46cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vC1ppUT0pVZo8a-06_w7lQ.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Figure 8. PyTorch implementation of the network</figcaption></figure><p id="c40f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">注意，当我们通过<em class="mj">网络</em>简单地调用网络时，<em class="mj"> PyTorch </em>打印出一个表示，将层理解为连接层！如图 7 的右手侧。根据<em class="mj"> PyTorch </em>的隐藏层数是 1，对应于<em class="mj"> W2 </em>，而不是 2 层 3 个神经元，这将对应于<em class="mj">隐藏层 1 </em>和<em class="mj">隐藏层 2 </em>。</p><h1 id="fe1e" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">4.<strong class="ak">符号歧义:Y = X W vs Wt X </strong></h1><p id="b4b0" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">深度学习中一个非常常见的模糊符号是如何在数学上表示输入和权重之间的乘积，以产生输出或任何特定层的隐藏表示。</p><p id="6e11" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">首先，无论哪种记数法，如果使用得当，输出都是一样的。它们之间的区别在于如何理解变量(向量)，是水平向量还是垂直向量。</p><h2 id="ec31" class="ni kw iq bd kx nj nk dn lb nl nm dp lf kh nn no lj kl np nq ln kp nr ns lr iw bi translated">4.1.Y = Wt * X</h2><p id="2bdd" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">在我们看到权重矩阵的转置出现在输入向量之前的情况下，意味着向量被表示为列向量。这种符号如此常见的原因是，特别是在神经网络被纯粹视为统计黑盒模型的背景下，遵循文献中其余模型的符号。</p><p id="c804" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">说到底，神经网络只是做所谓的<strong class="jy ja"><em class="mj"/></strong>曲线拟合的另一个工具而已。这意味着我们试图找到一个更好地适应数据分布的模型，当然，神经网络不是用来解决这一任务的第一个模型。线性回归模型是这种做法的最简单形式，如图 9 所示，用于图 7 所示网络中的二维输入。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nw"><img src="../Images/aac2c8beffbe33f2e548712cd3fffeb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a8dzltk8YiEEfyCG-YbkYQ.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Figure 9. Curve fitting for 2D input space</figcaption></figure><p id="094f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">搜索该表面的表达式将以一些未知的系数开始，这些系数对输入值进行“加权”:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nx"><img src="../Images/6feac347aad9dcb9781160319632f9d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q5apiP1GKMSY_MD4o6Oiwg.png"/></div></div></figure><p id="cff9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">为简单起见，我们将从现在起省略偏置项<em class="mj"> b </em>。</p><blockquote class="ny nz oa"><p id="a450" class="jw jx mj jy b jz ka kb kc kd ke kf kg ob ki kj kk oc km kn ko od kq kr ks kt ij bi translated"><strong class="jy ja">要点是系数通常放在它们所附属的变量之前。</strong></p></blockquote><p id="c29c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">听起来很熟悉，对吗？我们可以用一个非常简单的神经网络来表示这个方程，如图 10 所示。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/b84e0a74174f53189866dc03707f54be.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*O03E_tc1l3E1TaFMP-5sww.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Figure 10. 2D Surface fitting NN</figcaption></figure><p id="4573" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们还省略了权重的第二个子索引，因为只有一个收集器。</p><p id="143f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在，如果我们将输入理解为两个条目的列向量，则这些网络的组件将是:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi of"><img src="../Images/8106b61387d720300be1af8dc2d5c915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R5PDuLQp7fFzTJ11Ir_Hdw.png"/></div></div></figure><p id="602f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因此，计算输出的最终公式为:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi og"><img src="../Images/f67e0fdfea2e1784101d37baaf58ef2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cAP-t3HzYNn7DyqP6sAqKQ.png"/></div></div></figure><p id="22f2" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">延伸到一个更复杂的例子，对于图 7 中的网络，第一个隐藏层(没有任何偏置和激活)之后的隐藏值将是:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oh"><img src="../Images/20f69b473ca3a4134d8c421f471c7a5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z92M8UwjYgIXFB1GGrGsXQ.png"/></div></div></figure><h2 id="6255" class="ni kw iq bd kx nj nk dn lb nl nm dp lf kh nn no lj kl np nq ln kp nr ns lr iw bi translated">4.2.Y = X* W</h2><p id="c516" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">在文献中找到神经网络的前向传递的另一种可能性是在没有任何转置的情况下利用权重矩阵之前的输入变量。</p><p id="5100" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这自然是理解变量的相反方式。这一次，作为一个行向量，因此:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oi"><img src="../Images/a0ef59557a650f0b0dd26565d02b1e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Pf28JtOSKhxvYYTHM4cLA.png"/></div></div></figure><p id="060b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在我个人看来，这种方式感觉更自然。</p><p id="d9ab" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">作为人类，我们非常习惯于表格数据，我们通常赋予列维度的含义，赋予行不同记录(或观察)的含义。因此，类似于图 11 中描述的表格信息，结果非常直观。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oj"><img src="../Images/b5900fd1ca1830afb70c8485583d2035.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*06VlSrfpx_ZECf_raQyK7w.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Figure 11. Tabular data to row vector representation</figcaption></figure><p id="1f3b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在图 11 中，每一列代表现实世界中的一件真实的事情。<em class="mj"> x1 </em>可能是花在为考试学习理论上的时间，而<em class="mj"> x2 </em>可能是花在做练习上的时间。这个模型可以试着预测学生将得到的分数。#不是一个栏目，而是一个索引，简单地说就是每个不同的学生询问他是如何分配学习时间的。</p><p id="db9f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在这些帖子中，我们将更详细地讨论批量大小。这一批仅仅意味着如果我们不是一次选择一个学生，而是选择更多的学生在网络上做同样的转发。绿框表示批量为 1。</p></div></div>    
</body>
</html>