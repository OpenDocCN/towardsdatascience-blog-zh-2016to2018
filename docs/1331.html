<html>
<head>
<title>Build Neural Network From Scratch — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始构建神经网络—第2部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-neural-network-from-scratch-part-2-673ec7cdd89f?source=collection_archive---------4-----------------------#2017-08-24">https://towardsdatascience.com/build-neural-network-from-scratch-part-2-673ec7cdd89f?source=collection_archive---------4-----------------------#2017-08-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="6fd0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">神经网络系列简介(GINNS)——第二部分</em></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/e5f3c197d3e52709520695b74552c516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*HkMjxhe1VV9ldn9l2UaDHg.jpeg"/></div></figure><h1 id="3987" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">简介</strong></h1><p id="e759" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">在这篇文章中，我们将为And逻辑门构建一个感知机，这个模型我们将使用python和numpy从头开始构建。在这里，我假设你已经阅读了神经网络入门系列—第1部分，并且你已经熟悉了神经网络的基本概念。如果您不熟悉这个主题，我强烈建议您从第1部分开始。你还在吗？太好了，这篇文章将是我迄今为止写得最短的文章之一，我想在你建立良好的直觉和对神经网络的理解的同时保持事情的简单，所以让我们开始吧，但是首先，让我解释一下为什么从零开始建立一个超级虚拟神经网络<strong class="jp ir">为什么从零开始实现一个神经网络？</strong></p><p id="a6d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是为什么要从头开始实现一个神经网络呢？我相信你真正想要的是创建强大的神经网络，可以预测你照片中的人是乔治·卢卡斯还是卢克·天行者？不要！，哦好吧，我不怪你。</p><p id="ff43" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，玩笑归玩笑。即使你计划在未来使用Tensorflow、Keras或另一个神经网络库，从头实现一个网络至少一次也是非常有价值的练习。它帮助你理解神经网络是如何工作的，这对设计有效的模型是必不可少的。所以，我希望我说服你留在这里。</p><h1 id="1c34" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">单层神经网络(感知机)</h1><p id="6f44" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">感知器算法是最简单的人工神经网络类型，它的灵感来自于称为神经元的单个神经细胞的信息处理。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/b58c2e98e999c1b5c78ad1065e5225af.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*Tf1wMeglFw6XZdua9PTc9g.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Perceptron</figcaption></figure><p id="d589" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在感知器中，为了计算输出，我们将输入乘以各自的权重并与阈值进行比较，概括来说，输入的加权和通过一个阶跃/激活函数，如下所示:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/c35e3b70c92eccd4ecd9f2c387fbbeba.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*63JL4KKh-lTt2Rs3pEg_5w.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Weighted sum passed to an activation function.</figcaption></figure><h1 id="a6d4" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">激活功能</h1><p id="b87f" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">给定一个输入或一组输入，节点的激活函数定义该节点的输出。一个标准的计算机芯片电路可以被看作是一个激活功能的数字网络，激活功能可以是“开”(1)或“关”(0)，这取决于输入。在我们的示例中，我们将使用二元阶跃函数，如下所示:</p><div class="kn ko kp kq gt ab cb"><figure class="md kr me mf mg mh mi paragraph-image"><img src="../Images/806b6fd69e7efd924adfdd1aebdd0145.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*214_rT524EEhEvTqqaQhKg.png"/></figure><figure class="md kr mj mf mg mh mi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><img src="../Images/983e3064c864cced41a7f945fb0c9751.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*gNorTJgk1JRxZpxeM5B2gg.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk mo di mp mq">step function</figcaption></figure></div><h1 id="b544" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">下面是代码！</h1><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="mr ms l"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Perceptron</figcaption></figure><ol class=""><li id="ae32" class="mt mu iq jp b jq jr ju jv jy mv kc mw kg mx kk my mz na nb bi translated">负载输入(x)和输出(y)</li><li id="a780" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk my mz na nb bi translated">初始化权重和其他参数:<strong class="jp ir">学习率</strong>是控制权重更新量的配置参数，<strong class="jp ir">训练步骤(也称为历元)</strong>是进行向前和向后传播的单个步骤。</li><li id="2684" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk my mz na nb bi translated">计算输入(X)和权重矩阵(W)之间的点积，这里我们用np.dot()做矩阵乘法，点积也叫内积，比如X和W之间的内积是x1*w1 + x2*w2 + x3*w3 + … + Xn*Wn。</li><li id="470a" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk my mz na nb bi translated">在l1上应用激活功能。l1实际上是我们网络的输出。</li><li id="ee06" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk my mz na nb bi translated">当我们开始训练网络猜测预测时，计算模型产生的损失或错误，然后我们比较预测的类和真实的类。理想情况下，当我们训练神经网络时，我们想要的是找到最佳权重(W ),使我们的模型误差最小化。</li><li id="9b6f" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk my mz na nb bi translated">计算变化因子，我们称之为更新，如果我们的模型正确地预测了类，误差等于零，并且权重没有变化，但是如果它出错，那么我们可以对权重进行负(减少)或正(增加)更新。</li><li id="4f9e" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk my mz na nb bi translated">我们在100个训练步骤中训练我们的模型，并使用在线或随机学习，这意味着我们使用单个训练示例来进行参数更新。这里，单个训练示例是随机选取的。</li></ol><p id="cc1c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">培训后的输出:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/28f0e687998309f8ec5c09e448df8388.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*Kb_Qocmk0uoi7HAa8watuQ.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">y_pred after training</figcaption></figure><h1 id="b10f" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">使用案例和限制</h1><p id="74e8" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">虽然感知器对我们示例中的实例进行了很好的分类，但该模型有局限性，实际上，感知器是一个线性模型，在大多数情况下，线性模型不足以学习有用的模式。</p><div class="kn ko kp kq gt ab cb"><figure class="md kr ni mf mg mh mi paragraph-image"><img src="../Images/ed845ac736ecb32856754ec65392d68d.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*qLDLfAcXUCFKNmO4nUel5g.jpeg"/></figure><figure class="md kr nj mf mg mh mi paragraph-image"><img src="../Images/3419a3dd6d9d3e26cdd5649ad99fb1cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*wNBuYFYZGnMf6Io_tVdgDw.jpeg"/><figcaption class="ly lz gj gh gi ma mb bd b be z dk nk di nl mq">Left: And Gate, Right: Not and OR Logic Gates</figcaption></figure></div><p id="6c8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">像感知器这样的线性模型不是通用函数逼近器，如果您试图改变XOR逻辑门的问题，感知器会失败，因为XOR不具有线性可分性，如下图所示:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/69636d70a1ec07ee3e61ba220ca1a8c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*h_lIOOoFgVBEgGG2HdRFcw.jpeg"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk"><a class="ae nn" href="https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781783988365/8/ch08lvl1sec59/limitations-of-the-perceptron" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="fefd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经网络是众所周知的通用函数逼近器，那么我们遗漏了什么？这就是隐藏层的来源，也是我们接下来要讨论的内容。</p><h2 id="5c18" class="no kv iq bd kw np nq dn la nr ns dp le jy nt nu li kc nv nw lm kg nx ny lq nz bi translated">下一步是什么？</h2><p id="879f" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">我们将讨论隐藏层的力量和它们能够进行的处理，以及我们如何使用基于反向传播和梯度下降的算法来训练复杂的神经网络，如果你愿意更深入地研究神经网络的内部细节，请查看下面的链接，敬请关注！</p><h2 id="a26e" class="no kv iq bd kw np nq dn la nr ns dp le jy nt nu li kc nv nw lm kg nx ny lq nz bi translated">参考资料:</h2><ul class=""><li id="7647" class="mt mu iq jp b jq ls ju lt jy oa kc ob kg oc kk od mz na nb bi translated"><a class="ae nn" href="http://neuralnetworksanddeeplearning.com/index.html" rel="noopener ugc nofollow" target="_blank">神经网络和深度学习</a></li><li id="bf50" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk od mz na nb bi translated">http://cs231n.github.io/neural-networks-1/<a class="ae nn" href="http://cs231n.github.io/neural-networks-1/" rel="noopener ugc nofollow" target="_blank"/></li><li id="6ea4" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk od mz na nb bi translated"><a class="ae nn" href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/" rel="noopener ugc nofollow" target="_blank">用Python从零开始实现神经网络——简介</a></li><li id="60d9" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk od mz na nb bi translated"><a class="ae nn" href="https://beckernick.github.io/neural-network-scratch/" rel="noopener ugc nofollow" target="_blank">在Python和TensorFlow中从头开始构建神经网络</a></li><li id="c1d5" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk od mz na nb bi translated"><a class="ae nn" href="https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/" rel="noopener ugc nofollow" target="_blank">用Python和R从零开始理解和编码神经网络</a></li><li id="e5b2" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk od mz na nb bi translated"><a class="ae nn" href="https://iamtrask.github.io/2015/07/12/basic-python-network/" rel="noopener ugc nofollow" target="_blank">11行Python中的神经网络(第1部分)</a></li></ul><h2 id="7b88" class="no kv iq bd kw np nq dn la nr ns dp le jy nt nu li kc nv nw lm kg nx ny lq nz bi translated">在你走之前！</h2><p id="7d47" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">如果你喜欢这些作品，请留下你的掌声👏<em class="kl">推荐这篇文章，让别人也能看到。</em></p><p id="81de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大卫·福莫的《❤与T4》。</p><p id="ee6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">愿原力与你同在！</p></div></div>    
</body>
</html>