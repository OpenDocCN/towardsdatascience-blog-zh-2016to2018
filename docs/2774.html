<html>
<head>
<title>My Journey into Machine Learning: Class 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我的机器学习之旅:第四课</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/my-journey-into-machine-learning-class-4-eb0f681cec65?source=collection_archive---------5-----------------------#2018-03-03">https://towardsdatascience.com/my-journey-into-machine-learning-class-4-eb0f681cec65?source=collection_archive---------5-----------------------#2018-03-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3337" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大家好！感谢你和我一起踏上机器学习之旅！这是该系列的第四篇文章；如果你还没有看过第一篇、<a class="ae kl" href="https://goo.gl/BD13rR" rel="noopener ugc nofollow" target="_blank">第二篇</a>和<a class="ae kl" href="https://goo.gl/cch8Yp" rel="noopener ugc nofollow" target="_blank">第三篇</a>的文章，请查看一下。</p><p id="991c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上周，我们学习了线性回归的核心概念。我们一直从贝叶斯的角度思考，研究朴素贝叶斯算法。我们还简要地了解了模型是如何被评估的。</p><p id="4725" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本周，我们将:</p><ul class=""><li id="3191" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">讨论验证集的必要性</li><li id="281c" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">继续研究交叉验证</li><li id="7e36" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">讨论引导</li><li id="3dd3" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">直观和数学地理解偏差和方差</li></ul><p id="7be8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，上周，尽管我提到我们将从头实现一个朴素贝叶斯算法，但我不会在本文中这样做(我们将在接下来的文章中讨论它，所以不要担心！).</p></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><h1 id="a46a" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">来源</h1><p id="cb15" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">这些笔记的灵感来自各种材料，包括但不限于:</p><ol class=""><li id="7346" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk mk ks kt ku bi translated"><a class="ae kl" href="https://www.amazon.com/Introduction-Machine-Learning-Ethem-Alpaydin/dp/8120350782" rel="noopener ugc nofollow" target="_blank"> Alpaydin的机器学习入门</a></li><li id="c92f" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mk ks kt ku bi translated"><a class="ae kl" href="https://www.amazon.com/Machine-Learning-Tom-M-Mitchell/dp/0070428077" rel="noopener ugc nofollow" target="_blank">汤姆·米切尔的机器学习</a></li><li id="dd2d" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mk ks kt ku bi translated"><a class="ae kl" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank"> Andrew NG的机器学习课程</a></li><li id="fcb5" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mk ks kt ku bi translated">加州大学伯克利分校的机器学习博客</li><li id="744c" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mk ks kt ku bi translated">赫勒斯坦教授的讲座、笔记和幻灯片</li><li id="245c" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mk ks kt ku bi translated">互联网</li></ol></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><h1 id="7276" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">模型评估(续)</h1><p id="0b26" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">上周，我们讨论了交叉验证，以及它如何成为评估模型的一种强有力的方式，尤其是在数据较少的情况下。我们还讨论了数据科学家通常如何将数据集分为两组:训练集和测试集。但在实践中，大多数数据科学家将数据集分为三组:</p><ul class=""><li id="b359" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">训练集，</li><li id="8dfb" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">测试集，以及</li><li id="55ca" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">验证集</li></ul><p id="aaca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">人们经常问的一个常见问题是:</p><blockquote class="ml mm mn"><p id="1b80" class="jn jo mo jp b jq jr js jt ju jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj kk ij bi translated">"测试集和验证集之间有什么区别？"</p></blockquote><p id="debf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">验证集主要用于调整学习算法的参数，而测试集仅用于评估算法的性能。</p><p id="5262" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以你有一个学习算法，你想评估它的表现。的确，您可以将数据集划分为训练集和测试集，并通过对训练数据进行训练和对测试数据进行评估来评估您的模型有多准确(好)。但是像多项式回归这样的学习算法有许多参数，我们想要测试参数的每一个设置。因此，通过在验证集上评估学习算法，我们可以得到一个学习算法的最佳参数设置。然后使用学习算法的这个最佳设置，我们在测试集上评估它的性能。这通常能创造奇迹！</p><p id="e34f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要点是:</p><ul class=""><li id="584c" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">更新学习算法的参数以获得数据的最佳算法设置所必需的验证集</li><li id="8c64" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">测试集对于评估我们从验证集中得到的算法的最佳设置的性能是必不可少的</li></ul><p id="b634" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在你会问，</p><blockquote class="ml mm mn"><p id="83dd" class="jn jo mo jp b jq jr js jt ju jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj kk ij bi translated">“但是等等易勒雅斯！高次多项式(在回归中)的性能通常不会比低次多项式好吗？”</p></blockquote><p id="beed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">是的，这是真的。高次多项式有过度拟合数据的趋势。我们得到的精度可能很高，但在推广方面做得很差。因此，如果您在不同的数据集上使用相同的模型(比方说具有相同的属性)，它的性能会大大降低。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ms"><img src="../Images/d8770cc91476194223015e1bfc73c1be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*POnxcbS9cpQIBpkn-Kt4WA.jpeg"/></div></div></figure><p id="5405" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图1是低次多项式，而图2是高次多项式。正如我们可以清楚地看到，高次多项式试图执行太难，甚至适合离群值。</p><p id="c966" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">反之亦然:多项式次数过少可能无法很好地推广。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ne"><img src="../Images/b51ddcc00a627ed5f1b3aba868326f54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xM3GuhnCmyknxw581qjCzA.jpeg"/></div></div></figure><p id="cf95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图1是低次多项式，而图2是高次多项式。可以清楚地看到，在这种情况下，高次多项式更适合数据。</p><p id="9a6b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么，当我们考虑一个验证集时，我们如何处理这个问题呢？我们如何选择多项式的最佳次数，使模型既不欠拟合也不过拟合？</p><p id="7e3a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嗯，我们使用交叉验证！</p><p id="8821" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们只记录总体数据的误差，显然高次多项式会表现得更好。因此，我们使用交叉验证(回收数据)，以便数据中的每个集合都可以作为训练集和验证集重用。这样，在一个验证集中起作用的高次多项式不太可能在另一个验证集中起同样的作用。这样，我们可以得到我们一直在寻找的多项式的最佳次数！</p><p id="41aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一般程序如下:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ne"><img src="../Images/dcdc034625fbf0557c104862f57d7a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qylttwyHnbvqLxVx2Sf7Ng.jpeg"/></div></div></figure><p id="29df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让</p><ul class=""><li id="2d82" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">D1 —培训套件</li><li id="e603" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">D2 —验证(调谐)装置</li><li id="18df" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">D3 —测试集</li></ul><p id="2476" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">执行k重交叉验证时，保持D3独立，不要接触它</p><p id="d0b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于每个k:(其中k = 1，2，3，4，…..多项式的次数)</p><ol class=""><li id="45ff" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk mk ks kt ku bi translated">使用设置为k的学习算法训练假设</li></ol><p id="78df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.计算D2假设的误差</p><p id="efe8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.对所有折叠重复此操作</p><p id="087f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4.选择k，它给出了所有褶皱的D2平均误差的最小值</p><p id="9fda" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">5.现在，在测试集D3上用k的这个设置运行学习算法。如果它表现良好(即准确性值得称赞)，就保留它。</p><p id="1e22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一个可能被问到的问题是:</p><blockquote class="ml mm mn"><p id="4f3b" class="jn jo mo jp b jq jr js jt ju jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj kk ij bi translated">“如果学习算法在D2上跨所有折叠都执行得很好，但在D3上(从未见过的测试集)却不行，会怎么样？”</p></blockquote><p id="1ddd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这意味着三者之一:</p><ul class=""><li id="23fa" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">学习算法是不能够的</li><li id="fc72" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">怀疑过度拟合</li><li id="10dc" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">出了问题，比如D1、D2非常小或者D3非常小(可能不适用于交叉验证场景)</li></ul><p id="2017" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们遭受类不平衡问题，我们应该使用分层k-fold交叉验证。分层是一个重新排列数据的过程，以确保每个折叠都是整体的良好代表。例如，如果我们有两个类，分层确保每个折叠都有两个类的50%。</p><p id="a6fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">另一个关键问题是:</strong></p><blockquote class="ml mm mn"><p id="e13e" class="jn jo mo jp b jq jr js jt ju jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj kk ij bi translated"><strong class="jp ir">我们如何选择交叉验证中折叠数“k”的值？</strong></p></blockquote><p id="10fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">没有金科玉律来定义训练、验证和测试集应该有多大。我们希望每个数据集足够大，以减少<strong class="jp ir">方差</strong>(方差衡量数据集分布的程度；我们很快就会谈到它！)的估计。估计方差有两种形式:</p><ul class=""><li id="d5a6" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">适合训练数据的假设(学习算法)的方差</li><li id="6bc8" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">验证/测试风险估计值的方差</li></ul><p id="65cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">好的启发法(通常)是:</p><ul class=""><li id="e334" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">50%/25%/25%用于培训/val/测试分割，以及</li><li id="6e7e" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">k = 5或k = 10</li><li id="d177" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">随机拆分数据，集合之间的实例没有重叠</li></ul><p id="2a0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们应该总是设法减少验证/测试风险估计的方差以及学习算法估计的方差。</p><p id="24f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以差异是由两个因素驱动的:</p><ul class=""><li id="7c78" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated"><strong class="jp ir">训练/拟合:</strong> <br/> -随着模型的复杂度与训练数据大小之比的增加，模型可能会过度拟合，这增加了模型的方差。</li><li id="066c" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated"><strong class="jp ir">小型测试/验证集:</strong><br/>——测试/验证风险是整个数据集中每个样本风险的平均值。均值估计值的标准误差与sqrt(数据集的大小)成比例减少</li></ul><p id="85f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">交叉验证的替代方法是<strong class="jp ir">引导</strong>。</p><p id="269b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Bootstrapping是一种通过从原始样本<strong class="jp ir">中抽取实例并替换</strong>来生成新样本的方法。这类似于从单个样本生成多个样本。原始样本在这里被当作一个“群体”。</p><p id="2031" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更具体地说:</p><ul class=""><li id="0905" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">训练数据集—从总共N个数据实例中随机选择N个替换样本</li><li id="7737" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">验证数据集-原始数据集(不属于训练数据集的实例)</li></ul><p id="d73a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">bootstrap样本可能比交叉验证样本重叠更多(因为我们是用替换样本进行采样的)，因此它们的估计更具依赖性；但是它被认为是对非常小的数据集进行重采样的最佳方式。</p><p id="6600" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在:</p><p id="fcea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们选择一个实例的概率= 1/N</p><p id="aeb1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们不选择实例的概率= 1 - 1/N</p><p id="0dd1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在N个实例后我们不挑选的概率</p><p id="7814" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">= (1 - 1/N)^N</p><p id="58c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">=(大约。)e^-1</p><p id="e877" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">= 0.368</p><p id="2d5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这意味着训练数据包含大约63.2%的实例；即，系统将不会对36.8%的数据进行训练。因此，误差估计将是悲观的。为了使我们的误差估计更准确，我们复制；即多次重复该过程并观察平均行为。</p><p id="9013" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在某种程度上，我们使用bootstrap程序根据经验计算估计量的方差，并将方差估计纳入我们的选择机制。</p><p id="c283" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于引导要记住的要点:</p><ul class=""><li id="d2c6" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">是一种<a class="ae kl" href="http://www.statisticshowto.com/parametric-and-non-parametric-data/" rel="noopener ugc nofollow" target="_blank">非参数方法</a>(即，不假设任何关于基础分布的事情)，不需要假设人口分布的参数形式</li><li id="2729" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">用替代品取样</li><li id="f318" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">将原始样本视为“总体”</li><li id="44f9" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">潜在的假设是，从大小为N的原始样本中进行替换的抽样模拟了从更大的总体中抽取大小为N的样本</li><li id="81bc" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">适用于各种各样的问题，如非线性回归，分类，置信区间估计，偏差估计，调整p值，和时间序列分析</li><li id="6d3d" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">必须记住，它并不真正代表原始人口</li></ul><p id="1979" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自举不仅仅是我们讨论的内容。我推荐你通读以下文章，以便更好地理解它:</p><ul class=""><li id="edbd" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">要获得自举的<a class="ae kl" href="http://www.statisticshowto.com/probability-and-statistics/confidence-interval/" rel="noopener ugc nofollow" target="_blank">置信区间</a>，请参考<a class="ae kl" href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf" rel="noopener ugc nofollow" target="_blank">这个</a></li><li id="8a97" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">关于为什么自举运行得非常好，请参考<a class="ae kl" href="https://stats.stackexchange.com/questions/26088/explaining-to-laypeople-why-bootstrapping-works" rel="noopener ugc nofollow" target="_blank">这个</a></li></ul></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><h1 id="267b" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">偏见和差异:两难</h1><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/6df11278a13e9b819774555a116ac86a.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*UTfNP_f0SJswfIYG0xb7ug.jpeg"/></div></figure><p id="1b8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">偏差-方差困境是机器学习和数据科学领域中最流行的困境之一。理解它也很重要。为了理解偏差和方差，我将用两种方式来解释它:</p><ul class=""><li id="863e" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">直观的方式(你可以很容易地跟随)</li><li id="eadb" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">一种更数学化的方式</li></ul><h1 id="23ad" class="lh li iq bd lj lk ng lm ln lo nh lq lr ls ni lu lv lw nj ly lz ma nk mc md me bi translated">对偏差和方差的直观理解</h1><p id="9e65" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">偏差-方差困境具有现实世界的意义。它导致成千上万人死亡。流行的<strong class="jp ir"> <em class="mo">【福岛核电站灾难】</em> </strong>就是过度拟合的结果。让我们回过头来看看欠拟合和过拟合与偏差和方差有什么关系。</p><p id="968b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们希望我们的学习算法能够非常接近地模拟训练数据，即捕捉相关的属性和趋势。然而，如果我们过于接近我们的学习算法，我们可能会高估离群值。这是机器学习算法试图踩在预测上的微妙平衡。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nl"><img src="../Images/e61396206120f10687966e4ee91e489e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tCjPBtv-ml5O2Z8I1A0jQA.jpeg"/></div></div></figure><p id="aabb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">事情是这样的:</p><ul class=""><li id="7774" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">我们希望避免过度拟合，以停止重视离群值</li><li id="c934" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">但是在我们尝试这样做的时候，我们可能会适得其反:忽略训练数据中的重要特征</li></ul><p id="3a6d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可能拥有宇宙中最快的计算机、最先进的算法、计算密集型GPU，但如果你的模型过拟合/欠拟合，你的学习算法的预测能力将仍然很糟糕。</p><p id="6654" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">偏差和方差是统计领域的术语。偏差是一个定义非常松散的术语，但它通常被定义为<em class="mo">“实际参数和计算统计值之间的差异”</em>。如果您不知道统计数据和参数之间的区别，请参考<a class="ae kl" href="http://www.statisticshowto.com/how-to-tell-the-difference-between-a-statistic-and-a-parameter/" rel="noopener ugc nofollow" target="_blank">这个</a>。另一方面，方差被定义为<em class="mo">“对数据分布程度的度量”</em>。</p><p id="c46c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">偏差对应于欠拟合，而方差对应于过拟合。看这个图:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ne"><img src="../Images/78e3b916e1ff5bb1eadb360b5e8673c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*neOBZKt-ISg23DLHlbDA9Q.jpeg"/></div></div></figure><p id="5345" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，我们看到线性模型表现出<strong class="jp ir">高偏差</strong>，即它欠拟合。这个模型不够复杂，无法捕捉信息的潜在趋势。我们的模型是有偏差的，因为它隐含地假设数据以某种方式运行(在这个例子中，是线性的),即使这个假设不是真的。需要记住的一个关键点是，我们的线性模型没有任何问题；它正尽力而为。问题出在模型本身，也就是说，它不够复杂，无法从数据中获取足够的信息。</p><p id="74b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看一下这个例子:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ne"><img src="../Images/86529af5952d15631a21b2f68f7564e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sJboWEMx_RoWGkL9h4cRbA.jpeg"/></div></div></figure><p id="d35e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从图中可以看出，模型过拟合。这也叫<strong class="jp ir">高方差</strong>。数据表明，曲线对于捕捉信息可能是必不可少的，但我们的模型选择了复杂的曲线来做到这一点。在这种情况下，它表现出高方差和非常低的偏差(低偏差，因为它不对数据做任何假设)。事实上，它对数据的适应性太强了。</p><p id="b79b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">需要记住的一个关键点是，模型本身没有任何问题；它适应每一个数据点。对上述数据使用这种模型存在问题。我们的模型想要考虑每一个数据点，因此过度概括。过度概括的模型具有很高的方差，因为它基于无关紧要的数据细节而变化太多。</p><p id="78c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">太好了！但是现在你问:</p><blockquote class="ml mm mn"><p id="a33d" class="jn jo mo jp b jq jr js jt ju jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj kk ij bi translated">“嘿，易勒雅斯！我明白你说的偏差和差异是什么意思。但是为什么它们的背后会有取舍呢？为什么我们不能利用两者的优点，即低偏差和低方差？”</p></blockquote><p id="5102" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">碰巧偏差和方差是一个因素的副作用:我们模型的复杂性。</p><p id="8de0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们的模型的复杂性低，我们有高偏差和低方差。另一方面，当我们的模型的复杂性很高时，我们有低偏差和高方差。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nm"><img src="../Images/c578feb057f5e86fb8c4228b20f09fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_LGZLuhasRIZ1wxyMtJWrw.png"/></div></div></figure><p id="38f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们能做的最好的事情就是在两者之间找到一个平衡点，也就是说，选择一个既不太复杂也不太简单的模型。</p><p id="2e4d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还有一件事我想让你记住:</p><ul class=""><li id="6284" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">训练误差总是会随着模型复杂度的增加而减少(从上图中的绿线可以看出)</li><li id="182e" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">测试误差会降低到某一点，然后增加(从上图中的红线可以看出)</li><li id="a27d" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">因此，这些误差是确定您的模型是否欠拟合/过拟合的良好度量</li></ul><p id="8c3c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">扩展上述观点:</p><ul class=""><li id="c11a" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated"><strong class="jp ir">低复杂度</strong>模型导致训练和测试数据的<strong class="jp ir">精度差</strong>和<strong class="jp ir">误差大</strong> <strong class="jp ir">。这是因为模型本质上缺乏足够的复杂性来描述数据。</strong></li><li id="9c73" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated"><strong class="jp ir">高复杂度</strong>模型导致<strong class="jp ir">高精度</strong>和<strong class="jp ir">高测试误差</strong>。这是因为该模型将能够很好地描述训练数据，因此不能推广到测试数据。</li></ul><h1 id="a08d" class="lh li iq bd lj lk ng lm ln lo nh lq lr ls ni lu lv lw nj ly lz ma nk mc md me bi translated">参数方法</h1><p id="dc28" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">别再凭直觉了！让我们试着从数学上理解偏差和方差！在具体谈论偏差和方差之前，让我们先了解什么是参数方法。</p><p id="d77a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在参数方法中，我们假设样本来自服从已知模型的某种分布，例如高斯(正态)。参数化方法的优点是模型被定义为少量的参数(例如，均值和方差)。这些被称为分布的充分统计量。这意味着一旦从样本中估计出参数，整个分布就是已知的。</p><p id="26a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们:</p><ul class=""><li id="c369" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">根据给定的样本估计分布的参数，</li><li id="811d" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">将这些估计值代入假设模型，</li><li id="cd06" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">得到一个估计的分布，然后</li><li id="7a6f" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">用它来做决定</li></ul><p id="20cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们用来估计分布参数的方法是最大似然估计(MLE)(如果你读过以前的文章，现在应该知道了)。</p><p id="cf3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们将样本定义为独立同分布。这意味着我们样本中的每个随机变量都具有与其他变量相同的概率分布，并且相互独立。它表示如下:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/2a1569291ff6485c544e6e048f4748b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*hboLBYxkS80owdemzUnNyg.png"/></div></figure><p id="5121" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们假设x^t是从一些已知的概率密度函数p(x |θ)中提取的实例，定义为参数θ:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi no"><img src="../Images/e420580e49247a78cc6c9dc4a2a144ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*IlxSYsluSdPQNf86gRgv5Q.png"/></div></figure><p id="db7a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们想找到θ，使得从p(x |θ)采样x^t尽可能的可能。因为x^t是独立的，给定样本x的参数θ的可能性是各个点的可能性的乘积:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi np"><img src="../Images/8a59a1d108fdc323b788d23ac5d9e269.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*UnHxzv7_cSEgi8DUsGLntQ.png"/></div></figure><p id="eb4c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在最大似然估计中，我们感兴趣的是找到使X最有可能被画出的θ。因此，我们寻找最大化上述可能性的θ。我们可以最大化可能性的对数，而不改变取最大值的地方的值。应用对数技巧(对数将乘积转换为总和)，我们将对数似然性定义为:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/1066acc23d855eadc72898584e07cb19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*maLUou-pOWmdv0eDft66hQ.png"/></div></figure><p id="67a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">对于高斯(正常)密度:</strong></p><p id="000a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">x是高斯分布，具有:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0c85c3f5803be0a0dbc14880cb9a3044.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*PckelbsWYn8f2V0tpEdZ-Q.png"/></div></figure><p id="a684" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你在想上面的E是什么，那就是随机变量x的期望值，对于连续型随机变量，期望值一般是均值。更多信息，请参考<a class="ae kl" href="http://www.statisticshowto.com/probability-and-statistics/expected-value/" rel="noopener ugc nofollow" target="_blank">本</a>。</p><p id="8a2c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">密度函数如下所示:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ns"><img src="../Images/e56d6b52ddc9773585c13b0ea06d0e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ph4z7rK1ChWRrAyVE8Ltvg.png"/></div></div></figure><p id="30b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，考虑到:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nt"><img src="../Images/0fa94e388ae79621b54a616f0ac498e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ClLOJfcXSklshE1UBrzUXQ.png"/></div></div></figure><p id="44d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对数可能性是:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nu"><img src="../Images/8542e31103fc21b1529117063e2e17f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MTiE0MNPCi_mvubAyUGCeQ.png"/></div></div></figure><p id="6300" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们通过对上述导出的对数似然进行偏导数(w.r.t mu和sigma以获得估计值)并将其设置为0来找到MLE。我们得到:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nv"><img src="../Images/e2ec2d7c3bced2bfc37b99b453ebf063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ua2B3p6VMKxsxZkKt7X7qg.png"/></div></div></figure><h1 id="4759" class="lh li iq bd lj lk ng lm ln lo nh lq lr ls ni lu lv lw nj ly lz ma nk mc md me bi translated">评估一个估计量:偏差和方差</h1><p id="7ca7" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">我们终于准备好讨论偏差和方差了！</p><p id="2743" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nw"><img src="../Images/6c7b542d14e8ce0065e3402bfb9c2372.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KlYp3U1QfCGa2ttQbzjYnw.png"/></div></div></figure><p id="cee2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了评估该估计量的质量，我们可以测量它与θ的差异，即，</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/b4ac0e18a41796a3d310e07e929633b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*AqsdBtsm0mSqG7UzFg_sBA.png"/></div></figure><p id="af8c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是由于估计量是一个随机变量(它取决于样本X)，我们需要<strong class="jp ir">在所有可能的X上平均</strong>，并考虑r(d，θ)，估计量d的均方误差定义为:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ny"><img src="../Images/729a7b6969d92373c72b6822c4aa844f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P0H_pxWZMd_xoEUpZ2M_2Q.png"/></div></div></figure><p id="d526" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">估计量的偏差是估计量的平均值和实际参数θ之间的差值:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/7607202fb4abc0accaed6338631f0ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*UtKciy7272F2-VL4vYNl1A.png"/></div></figure><p id="dbcf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果上述bias值等于0(零)，我们说d是<em class="mo">θ</em>的无偏估计量。如果x^t是从具有实际均值<em class="mo"> mu </em>的某个密度中抽取的，则样本平均值m是实际均值<em class="mo"> mu </em>的无偏估计量，因为:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi oa"><img src="../Images/162f9a156312b92942f77d4c8214d729.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MUmOTHClJZGbYD1QOh_9aw.png"/></div></div></figure><p id="bb94" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，<em class="mo"> m </em>是样本的估计平均值，<em class="mo"> mu </em>是总体的实际平均值。所以上面基本上说的是，如果我们从很多这样的样本中取很多这样的<em class="mo"> m </em>，所有这样的<em class="mo"> m的平均值</em>就会接近实际平均值(mu)。因此，随着样本数量的增加，样本均值将(几乎)等于实际均值。</p><p id="37a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mo"> m </em>也是一致估计量，这意味着Var(m)—&gt;0为N—&gt;无穷大。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ob"><img src="../Images/048264af99b501a416067dd8102f89f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cKQp01g-jrdNFf0DTSVq6A.png"/></div></div></figure><p id="2efd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着样本中的点数N变大，<em class="mo"> m </em>偏离<em class="mo"> mu </em>越小。</p><p id="cf06" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们检查s，实际方差σ的MLE(最大似然估计):</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/57bcbf3352f33b5ed21fbeefaf550a21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*35eHhrqPro4M4MSoAVWrhw.png"/></div></figure><p id="8666" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">鉴于:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi oa"><img src="../Images/fb6743b83da0080b6376f09b13fda5db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lygvbKPNXYXVBYI5NBb6KQ.png"/></div></div></figure><p id="6222" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你对上面的证明感到好奇，可以参考<a class="ae kl" href="https://en.wikipedia.org/wiki/Variance#Definition" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><p id="5027" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从(4)中，我们可以写出:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi od"><img src="../Images/0be88e399b4b0a4f0ba10d5ad5bb86ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*rZtboO_CMJYQyW10grUd6g.png"/></div></figure><p id="2365" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将上述值代入(3)，我们得到:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi oe"><img src="../Images/6ebc99338062e6a9cdaaa4063af5db16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QezxJB2vsJNeM8K9ObyoPQ.png"/></div></div></figure><p id="ead5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上式表明s是<em class="mo">μ的有偏估计量。</em>如果我们将N项从R.H.S移动到L.H.S，我们得到(N/(N-1))s，这是一个无偏估计量(即，它等于实际σ)。但是，当<em class="mo"> N </em>变大时，差别可以忽略不计。这是一个渐近无偏估计量的例子，当<em class="mo"> N </em>趋于无穷大时，其偏差趋于0。</p><p id="3227" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，回到我们的均方差:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi of"><img src="../Images/0b82e47ddf7b36574f6f7d38fae38fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*12-iyFCIOr7Prpo89Eva7g.png"/></div></div></figure><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi og"><img src="../Images/154f0c914a72e62063c3eaff04b652f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CaBk5lN6eRknyHE89aCXtw.jpeg"/></div></div><figcaption class="oh oi gj gh gi oj ok bd b be z dk"><strong class="bd ol"><em class="om">Fig. Analogy by my friend, </em></strong><a class="ae kl" href="https://www.linkedin.com/in/rakshitsareen/" rel="noopener ugc nofollow" target="_blank"><strong class="bd ol"><em class="om">Rakshit Sareen</em></strong></a></figcaption></figure><p id="30f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">E[(d - E[d]) ]是公式中的方差。它衡量特定di在期望值(即估计平均值)附近的平均变化程度(即期望值E)。</p><p id="96b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了记住这一点，想想地球和月球之间的类比。地球的位置是估计的平均值(E[d])。月球在任何给定时间的位置都是D1。方差就是两者的平均差。</p><p id="3fb0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，具有实际参数θ的模型的误差(均方误差)为:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi on"><img src="../Images/df096959ba5ccaea77ce28adc3ba82ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*68N4rWu2SpFojQh2NTG3tQ.png"/></div></figure><p id="e60e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，如果偏差= 0，即估计平均值(E[d]) =实际平均值(θ):</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi np"><img src="../Images/2606ad4bc299ca1192e13517ce10bf49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*RBjP_6svtPgk3NwWsQCQZQ.png"/></div></figure><p id="e507" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你有它！偏差和方差的数学分解！现在你在直觉上和数学上已经准备好处理偏差和方差了。</p></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><p id="1541" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这就是本周的全部内容，各位！下周见！</p></div></div>    
</body>
</html>