<html>
<head>
<title>NLP: Extracting the main topics from your dataset using LDA in minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP:使用 LDA 在几分钟内从数据集中提取主要主题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925?source=collection_archive---------1-----------------------#2018-08-22">https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925?source=collection_archive---------1-----------------------#2018-08-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="219b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用数据做很酷的事情！</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/b31de0f0e430bbcaff7f1ec7f01ac55f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*yDrRJur-3EOMTXmnRyRFnQ.gif"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Power of NLP</figcaption></figure><p id="c316" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我最近开始学习用于主题建模的潜在狄利克雷分配(LDA ),惊讶于它的强大和快速运行。主题建模是使用无监督学习来提取文档集合中出现的主要主题(表示为一组单词)的任务。</p><p id="8ee2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我在 20 个新闻组数据集上测试了该算法，该数据集包含来自新闻报道的许多部分的数千篇新闻文章。在这个数据集中，我提前知道了主要的新闻主题，并且可以验证 LDA 是否正确地识别了它们。</p><p id="af61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">代码运行起来非常简单快速。可以在<a class="ae kx" href="https://github.com/priya-dwivedi/Deep-Learning/blob/master/topic_modeling/LDA_Newsgroup.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。我鼓励你拉一下试试。</p><h1 id="8cae" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">关于 LDA</h1><p id="9f6b" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">LDA 用于将文档中的文本分类到特定主题。它建立了每个文档的主题模型和每个主题的单词模型，建模为 Dirichlet 分布。</p><ul class=""><li id="f92e" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">每个文档被建模为主题的多项式分布，并且每个主题被建模为单词的多项式分布。</li><li id="bc6a" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">LDA 假设我们输入的每一个文本块都包含有某种关联的单词。因此，选择正确的数据是至关重要的。</li><li id="6d20" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">它还假设文档是由多个主题混合而成的。然后，这些主题根据它们的概率分布生成单词。</li></ul><p id="e58a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要了解更多关于 LDA 的信息，请点击这个<a class="ae kx" href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><h1 id="0be7" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated"><strong class="ak">使用的数据集</strong></h1><p id="afd2" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">我用的数据集是 20Newsgroup 数据集。它可以在 sklearn 数据集下获得，并且可以作为</p><pre class="km kn ko kp gt mp mq mr ms aw mt bi"><span id="f648" class="mu kz iq mq b gy mv mw l mx my">from sklearn.datasets import fetch_20newsgroups<br/>newsgroups_train = fetch_20newsgroups(subset=’train’, shuffle = True)<br/>newsgroups_test = fetch_20newsgroups(subset=’test’, shuffle = True)</span></pre><p id="2916" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个数据集已经将新闻分组为关键主题。你可以熬过来</p><pre class="km kn ko kp gt mp mq mr ms aw mt bi"><span id="38a9" class="mu kz iq mq b gy mv mw l mx my">print(list(newsgroups_train.target_names))</span></pre><p id="0c0a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据集中有 20 个目标——<em class="mz">‘alt .无神论’，<br/>‘comp . graphics’，<br/>‘comp . OS . ms-windows . misc’，<br/>‘comp . sys . IBM . PC . hardware’，<br/>‘comp . sys . MAC . hardware’，<br/>‘comp . windows . x’，<br/>‘misc . for sale’，<br/>‘rec . autos’，<br/>‘rec . motors’，<br/>‘rec .’</em></p><p id="ced7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从视觉上看，我们可以说这个数据集有几个广泛的主题，如:</p><ul class=""><li id="58b9" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">科学</li><li id="4f11" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">政治</li><li id="4753" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">运动</li><li id="8f8c" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">宗教</li><li id="df2d" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">技术等</li></ul><h1 id="7160" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated"><strong class="ak">使用 Python 中的 LDA 提取主题</strong></h1><ol class=""><li id="183d" class="mb mc iq jp b jq lw ju lx jy na kc nb kg nc kk nd mh mi mj bi translated"><strong class="jp ir">预处理原始文本</strong></li></ol><p id="b5f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这包括以下内容:</p><ul class=""><li id="1c2c" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated"><strong class="jp ir">分词</strong>:将文本拆分成句子，句子拆分成单词。将单词小写，去掉标点符号。</li><li id="86bb" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">少于 3 个字符的单词将被删除。</li><li id="b00b" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">所有的<strong class="jp ir">停用词</strong>都被删除。</li><li id="adab" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">单词被<strong class="jp ir">词汇化</strong>——第三人称的单词被改为第一人称，过去时态和将来时态的动词被改为现在时态。</li><li id="1bbb" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">单词被<strong class="jp ir">词干化</strong>——单词被还原成它们的词根形式。</li></ul><p id="31fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用 NLTK 和 gensim 库来执行预处理</p><pre class="km kn ko kp gt mp mq mr ms aw mt bi"><span id="cde8" class="mu kz iq mq b gy mv mw l mx my">def lemmatize_stemming(text):<br/>    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))</span><span id="8d14" class="mu kz iq mq b gy ne mw l mx my"># Tokenize and lemmatize<br/>def preprocess(text):<br/>    result=[]<br/>    for token in gensim.utils.simple_preprocess(text) :<br/>        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:<br/>            result.append(lemmatize_stemming(token))<br/>            <br/>    return result</span></pre><p id="7465" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">生成的文本如下所示:</p><pre class="km kn ko kp gt mp mq mr ms aw mt bi"><span id="bc9f" class="mu kz iq mq b gy mv mw l mx my">Original document: <br/>['This', 'disk', 'has', 'failed', 'many', 'times.', 'I', 'would', 'like', 'to', 'get', 'it', 'replaced.']<br/><br/><br/>Tokenized and lemmatized document: <br/>['disk', 'fail', 'time', 'like', 'replac']</span></pre><p id="3e76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 2。将文本转换成单词包</strong></p><p id="816c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在主题建模之前，我们将标记化和词条化的文本转换为一个单词包，你可以把它想象成一个字典，其中的关键字是单词，值是单词在整个语料库中出现的次数。</p><pre class="km kn ko kp gt mp mq mr ms aw mt bi"><span id="5431" class="mu kz iq mq b gy mv mw l mx my">dictionary = gensim.corpora.Dictionary(processed_docs)</span></pre><p id="0ad7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以进一步筛选出现次数很少或出现频率很高的单词。</p><p id="2d26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，对于每个预处理过的文档，我们使用刚刚创建的 dictionary 对象将文档转换成单词包。也就是说，我们为每个文档创建一个字典，报告有多少单词以及这些单词出现了多少次。</p><pre class="km kn ko kp gt mp mq mr ms aw mt bi"><span id="a3ca" class="mu kz iq mq b gy mv mw l mx my">bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]</span></pre><p id="24bf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">结果看起来像:</p><pre class="km kn ko kp gt mp mq mr ms aw mt bi"><span id="01bf" class="mu kz iq mq b gy mv mw l mx my">Word 453 ("exampl") appears 1 time.<br/>Word 476 ("jew") appears 1 time.<br/>Word 480 ("lead") appears 1 time.<br/>Word 482 ("littl") appears 3 time.<br/>Word 520 ("wors") appears 2 time.<br/>Word 721 ("keith") appears 3 time.<br/>Word 732 ("punish") appears 1 time.<br/>Word 803 ("california") appears 1 time.<br/>Word 859 ("institut") appears 1 time.</span></pre><p id="d154" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 3。运行 LDA </strong></p><p id="c4a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这实际上很简单，因为我们可以使用 gensim LDA 模型。我们需要指定数据集中有多少主题。假设我们从 8 个独特的主题开始。通过次数是对文档进行培训的次数。</p><pre class="km kn ko kp gt mp mq mr ms aw mt bi"><span id="94ed" class="mu kz iq mq b gy mv mw l mx my">lda_model =  gensim.models.LdaMulticore(bow_corpus, <br/>                                   num_topics = 8, <br/>                                   id2word = dictionary,                                    <br/>                                   passes = 10,<br/>                                   workers = 2)</span></pre><h1 id="9389" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">结果并解释它们</h1><p id="653b" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">就是这样！模型已经建立。现在让我们来解读它，看看结果是否有意义。</p><p id="12dc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该模型的输出是 8 个主题，每个主题由一系列单词分类。LDA 模型不会给这些词起一个主题名，而是由我们人类来解释它们。请参见下面的模型输出示例，以及“我”如何为这些单词分配潜在主题。</p><pre class="km kn ko kp gt mp mq mr ms aw mt bi"><span id="46a2" class="mu kz iq mq b gy mv mw l mx my">Topic 1: Possibly Graphics Cards<br/>Words: "drive" , "sale" , "driver" , *"wire" , "card" , "graphic" , "price" , "appl" ,"softwar", "monitor"</span><span id="3e20" class="mu kz iq mq b gy ne mw l mx my">Topic 2: Possibly Space<br/>Words: "space","nasa" , "drive" , "scsi" , "orbit" , "launch" ,"data" ,"control" , "earth" ,"moon"</span><span id="0ab6" class="mu kz iq mq b gy ne mw l mx my">Topic 3: Possibly Sports<br/>Words: "game" , "team" , "play" , "player" , "hockey" , season" , "pitt" , "score" , "leagu" , "pittsburgh"</span><span id="f71b" class="mu kz iq mq b gy ne mw l mx my">Topic 4: Possibly Politics<br/>Words: "armenian" , "public" , "govern" , "turkish", "columbia" , "nation", "presid" , "turk" , "american", "group"</span><span id="102e" class="mu kz iq mq b gy ne mw l mx my">Topic 5: Possibly Gun Violence<br/>Words: "kill" , "bike", "live" , "leav" , "weapon" , "happen" , *"gun", "crime" , "car" , "hand"</span></pre><p id="d4e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">查看<a class="ae kx" href="https://github.com/priya-dwivedi/Deep-Learning/blob/master/topic_modeling/LDA_Newsgroup.ipynb" rel="noopener ugc nofollow" target="_blank"> github </a>代码来查看所有主题，并使用模型来增加或减少主题的数量。</p><p id="2245" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">观察:</strong></p><ul class=""><li id="caa6" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">该模型在提取数据集中的独特主题方面做得非常好，我们可以在已知目标名称的情况下确认这些主题</li><li id="287c" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">模型运行非常快。我可以在几分钟内从数据集中提取主题</li><li id="427a" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">它假设数据集中有不同的主题。因此，如果数据集是一堆随机的推文，那么模型结果可能就不可解释了。</li></ul><p id="a4f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">未来的改进</strong></p><p id="7361" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我对这个关于<a class="ae kx" href="https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164" rel="noopener ugc nofollow" target="_blank"> Guided LDA </a>的帖子很感兴趣，很想尝试一下。</p><p id="4a01" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我有自己的深度学习咨询公司，喜欢研究有趣的问题。我已经帮助许多初创公司部署了基于人工智能的创新解决方案。请到 http://deeplearninganalytics.org/来看看我们吧。</p><p id="5012" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你也可以在 https://medium.com/@priya.dwivedi 的<a class="ae kx" href="https://medium.com/@priya.dwivedi" rel="noopener">看到我的其他作品</a></p><p id="941d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你有一个我们可以合作的项目，请通过我的网站或 info@deeplearninganalytics.org 联系我</p><p id="760a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">参考文献</strong></p><ul class=""><li id="aa27" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">非常感谢 Udacity，特别是他们的 NLP 纳米学位让学习变得有趣！</li><li id="858e" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated"><a class="ae kx" href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf" rel="noopener ugc nofollow" target="_blank">LDA 上的纸</a></li></ul></div></div>    
</body>
</html>