<html>
<head>
<title>Neural Networks III: The Big Picture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络 III:大图</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-iii-the-big-picture-dfd0ad953d4c?source=collection_archive---------14-----------------------#2018-11-29">https://towardsdatascience.com/neural-networks-iii-the-big-picture-dfd0ad953d4c?source=collection_archive---------14-----------------------#2018-11-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="d3ea" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/neural-network-notes/latest" rel="noopener">神经网络简介</a></h2><div class=""/><p id="e4b0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这个关于神经网络的<a class="ae ku" href="https://medium.com/@pabloruizruiz/neural-networks-notes-fa42ab388bb8" rel="noopener">系列帖子</a>是脸书 PyTorch 挑战赛期间笔记收集的一部分，在 Udacity 的<a class="ae ku" href="https://eu.udacity.com/course/deep-learning-nanodegree--nd101" rel="noopener ugc nofollow" target="_blank">深度学习纳米学位项目之前。</a></p><h1 id="2ad4" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">内容</h1><ol class=""><li id="f4d4" class="lt lu iq jy b jz lv kd lw kh lx kl ly kp lz kt ma mb mc md bi translated"><strong class="jy ja">简介</strong></li><li id="1ac1" class="lt lu iq jy b jz me kd mf kh mg kl mh kp mi kt ma mb mc md bi translated"><strong class="jy ja">作为功能的网络</strong></li><li id="1a35" class="lt lu iq jy b jz me kd mf kh mg kl mh kp mi kt ma mb mc md bi translated"><strong class="jy ja">让我们增加尺寸</strong></li><li id="13c2" class="lt lu iq jy b jz me kd mf kh mg kl mh kp mi kt ma mb mc md bi translated"><strong class="jy ja">批量大小</strong></li></ol><h1 id="af4a" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">1.介绍</h1><p id="1589" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mj kj kk kl mk kn ko kp ml kr ks kt ij bi translated">在第一章中，我们已经看到了什么是神经网络，以及我们可以从它们那里得到什么。这种方法是从外部观察神经网络。它们是我们见过的实际上在执行数学运算的黑盒。</p><p id="6c10" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">为了完全理解神经网络，是时候从内部来看它们了。在这一章中，我们将看看这些神经网络如何被分解成最小的块，以及如何根据我们将要去神秘化的行为从零开始编码。事实上，我们要做的这种分解是神经网络和计算效率的原因，我们可以用它们来解决需要大型数据集的复杂问题。</p><p id="344f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">首先，让我们再看看我们的老朋友，让我们试着找出我们能从中提取的最小片段。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="ab gu cl mr"><img src="../Images/e5d74863b6f2098ef5ef083d87ce436d.png" data-original-src="https://miro.medium.com/v2/format:webp/1*JOvnkqDOkJxJHwM0M7F_ZQ.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">Figure 1. Schematic Feed Forward Neural Network</figcaption></figure><p id="b1b6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">唯一的区别是输入的维度和隐藏层中的神经元将是固定的，以便在我们回顾过程时获得更好的理解。我们将在途中发现所有的魔法从何而来。</p><h1 id="e7b4" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">2.让网络发挥作用</h1><p id="38bc" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mj kj kk kl mk kn ko kp ml kr ks kt ij bi translated">正如我们在<a class="ae ku" href="https://medium.com/@pabloruizruiz/neural-networks-ii-first-contact-7ad5094db1d2" rel="noopener">前一部分</a>中所做的那样，神经网络只是正在发生的数学运算的一种表示。因此，我们可以将所有变量表示为不同维度的向量。</p><p id="8e04" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">首先，我们有一个二维输入。这意味着，我们有两个不同的概念，例如，温度和压力。我们的输入神经元将这 2 个输入维度映射到 3 个隐藏维度，这是隐藏层中神经元的数量。因此，矩阵的维数必须是<em class="my">(2×3)</em>。此外，为了更清晰地显示，它没有被表示出来，但是我们有一个“b”项，用于在矩阵乘法之后添加偏差。这导致了这种情况:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mz"><img src="../Images/c71adbd8e2343ef966f8e8967c613869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MgLDny1UuEgHxoVRRFyV4Q.png"/></div></div></figure><p id="eac5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在我们有了到达隐藏层所需的所有工具。我们只需要应用矩阵乘法:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ne"><img src="../Images/3020ae076f89c5266b30db54733c3c0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e3qsXrCzrxe4GxddhzCqpQ.png"/></div></div></figure><p id="c0b0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这样做的结果将是:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nf"><img src="../Images/3d72c0c9bc966f083416b85639a7bd28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*anXPkQoYWFRI01c-OSjEGg.png"/></div></div></figure><blockquote class="ng nh ni"><p id="b7f9" class="jw jx my jy b jz ka kb kc kd ke kf kg nj ki kj kk nk km kn ko nl kq kr ks kt ij bi translated">记住，每个输入权重将每个输入<em class="iq"> i </em>与每个神经元<em class="iq"> j </em>连接起来。这就是为什么符号<em class="iq"> Wij </em>，连接<em class="iq"> i </em>和<em class="iq"> j. </em></p></blockquote><p id="9e34" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">一旦我们进入图层，就该应用激活功能了。这样做的结果将是:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nm"><img src="../Images/d118c6bd97c0cc3e8b5d6b633616eccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*44L8RhRERZbKEdd0Cd7UCQ.png"/></div></div></figure><p id="deff" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们将函数<strong class="jy ja">逐元素地</strong>应用于每个神经元的网络输入的每个元素，因此我们将 3 维作为隐藏神经元的数量。</p><p id="c6c1" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">根据神经网络方案，现在需要的变换是从 3 到 1，因为下一层只有 1 个神经元。最后一个神经元通常不执行任何功能，因为我们不想在这一点上应用非线性。因此，连接这两层的矩阵必须是<em class="my"> (3x1) </em>。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nn"><img src="../Images/e97fb1da3e6e1657186b5936cc10b935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*clU3j-7vt9ZQApwYU-mA9g.png"/></div></div></figure><p id="d3ad" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在我们有了所有的工具来应用隐藏层的权重。那么需要执行等式是:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi no"><img src="../Images/3069f03d5d2d5e669fbe2be8e530d962.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nZXulvRgHEN8RTay9-4r1w.png"/></div></div></figure><p id="74e2" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">由于我们的最终输入不执行任何操作(还没有)，我们可以说<em class="my"> z3 </em>是输入通过所有网络后的最终输出<em class="my">(在前面的部分中称为 Y _ hat)</em>。这就是我们所说的前馈操作。</p><h1 id="1521" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">3.让我们增加维度</h1><p id="0ab3" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mj kj kk kl mk kn ko kp ml kr ks kt ij bi translated">对于二维的单个观察，我们已经经历了整个前进过程。例如，我们有<em class="my">温度= 20°C，压力= 1 巴</em>。但是谁能仅凭一次观察就学会任何东西呢？我们需要知道更多，以提高我们对周围事物的认识，并能够从中找到规律。如果我们想学习如何下棋，我们肯定要尝试几次。这个事实会使过程复杂化吗？当然不是！让我们过一遍。</p><p id="6d57" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">比方说，我们有 4 个观察值(当然你需要更多，但我只是选择了与已经使用的 1，2，3 不同的最小数字，所以最后你可以使关系更容易；我们会达到这一点，不要担心)。</p><p id="567f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因此，我们的输入如下所示:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi np"><img src="../Images/8133f84a52f4d28fdc936ea74285aa1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0JLUn1fovCHUbvkGsFkxEQ.png"/></div></div></figure><p id="746c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在，对应于网络本身的变量保持不变。我们没有改变网络中的任何东西，我们只是用<strong class="jy ja">一批观测值作为输入</strong>，而不是单个观测值。因此，如果我们执行向前，我们的结果是:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nq"><img src="../Images/331d99d3bb724b0f77902c76149ffff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J6hSDy59keUV2VY3JP7QZw.png"/></div></div></figure><p id="b68a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">同样，唯一的区别是我们对每个不同的观察都有一个新的行。接下来的步骤是使用<em class="my"> z2 </em>作为每个隐藏神经元的净输入，以元素方式应用激活函数。因此<em class="my"> a3 </em>的形状与<em class="my"> z2 </em>的大小完全相同。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nq"><img src="../Images/3bcd226d481403aab97513a8b90b9d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QGQDfTxym-yjsw1u1KLBYg.png"/></div></div></figure><p id="e309" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">最后，应用等式。2 计算输出:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ne"><img src="../Images/05998d8f101b7c2b63252c8e327d054d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5u60DAPHXoXtDFUHZzJ4GA.png"/></div></div></figure><h1 id="92d1" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">4.批量</h1><p id="8b06" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh mj kj kk kl mk kn ko kp ml kr ks kt ij bi translated">我们一直在使用一批输入示例。在深度学习中，最重要的突破之一是对众所周知的优化器随机梯度下降(SGD)进行小规模修改，成为<strong class="jy ja">小批量随机梯度下降。</strong></p><p id="6386" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">有什么区别？</strong></p><p id="5173" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">那么，SGD 的普通实现将输入一个输入，并执行向前传递。这将导致误差，该误差将被反向传播到网络中以更新权重。</p><p id="c501" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如第 3 节所示，mini-batchSGD 接收一批输入，并在转发过程中将它们一起转发，将每个输入的误差平均为 mini-batch 的单个误差。</p><p id="4a38" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">使用小批量有什么好处？</strong></p><p id="3d81" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">首先，由于我们对每批输入进行 1 次权重更新，总的更新次数将会更少！因此，我们减少了训练中的计算量，从而减少了训练时间。</p><p id="67ba" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">其次，硬件和软件都被优化以执行矩阵乘法。因此，在单次矩阵乘法中进行批量计算所需的时间比顺序进行要少。</p><p id="d011" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">最后，结果表明，使用小批量优化器具有更好的收敛特性，对训练有很大帮助。</p><p id="48d1" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这个<a class="ae ku" href="https://www.kaggle.com/residentmario/full-batch-mini-batch-and-online-learning" rel="noopener ugc nofollow" target="_blank"> kaggle 内核</a>是一个很好的资源，可以更好地理解批处理大小如何影响优化器。</p></div></div>    
</body>
</html>