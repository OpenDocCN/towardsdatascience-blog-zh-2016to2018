<html>
<head>
<title>Principal Component Analysis Pooling in Tensorflow with Interactive Code [PCAP]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Tensorflow 中的主成分分析池与交互代码[PCAP]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-pooling-in-tensorflow-with-interactive-code-pcap-c621c7d927ed?source=collection_archive---------10-----------------------#2018-06-02">https://towardsdatascience.com/principal-component-analysis-pooling-in-tensorflow-with-interactive-code-pcap-c621c7d927ed?source=collection_archive---------10-----------------------#2018-06-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/a13876b197b634c29db1669079572609.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*4pXR2f07hZV20tHY0pfdMg.gif"/></div></figure><p id="c86b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">其思想很简单，卷积神经网络中的最大/平均池操作用于降低输入的维数。虽然引入了更复杂的池操作，如<a class="ae ks" href="https://arxiv.org/abs/1509.08985" rel="noopener ugc nofollow" target="_blank"> Max-Avg (Mix) Pooling </a>操作，但我想知道我们是否可以用<a class="ae ks" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析(PCA) </a>做同样的事情。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><p id="53ba" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">PCA 概述及简单教程</strong></p><figure class="la lb lc ld gt jr"><div class="bz fp l di"><div class="le lf l"/></div></figure><p id="d46f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">上面的论文在解释什么是 PCA 方面做了大量的工作，并且给出了使用哪种数学的简单例子。这将是一个好主意，先浏览 pdf 文件，然后再继续阅读。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><p id="0c73" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> PCA /单值分解</strong></p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/0c4a7862ca8aacedd7cbbf3562cfdb5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*2FD8VykEVal9jpKX0wNoRA.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Image from this <a class="ae ks" href="https://ewanlee.github.io/2018/01/17/PCA-With-Tensorflow/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="82d8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">红框</strong> →我们将如何在 Tensorflow 中执行 PCA</p><p id="5eb2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这篇<a class="ae ks" href="https://ewanlee.github.io/2018/01/17/PCA-With-Tensorflow/" rel="noopener ugc nofollow" target="_blank">博文</a>很好地解释了我们如何使用<a class="ae ks" href="https://en.wikipedia.org/wiki/Singular-value_decomposition" rel="noopener ugc nofollow" target="_blank">单值分解</a>来执行 PCA。谢天谢地 Tensorflow 已经有了<a class="ae ks" href="https://www.tensorflow.org/api_docs/python/tf/svd" rel="noopener ugc nofollow" target="_blank"> tf.svd() </a>操作来执行单值分解。</p><figure class="la lb lc ld gt jr"><div class="bz fp l di"><div class="ll lf l"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Video from this <a class="ae ks" href="https://www.youtube.com/watch?v=P5mlg91as1c" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="423a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我个人不得不观看/阅读额外的材料来具体理解什么是单值分解，并且我已经链接了我在上面观看的视频。(这个<a class="ae ks" href="https://www.quora.com/What-is-an-intuitive-explanation-of-singular-value-decomposition-SVD" rel="noopener ugc nofollow" target="_blank"> Quora </a>帖子和这个<a class="ae ks" href="https://blog.statsbot.co/singular-value-decomposition-tutorial-52c695315254" rel="noopener ugc nofollow" target="_blank"> medium 帖子</a>也是一个很好的阅读来源。)</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><p id="68ef" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">(愚蠢的)主成分分析池背后的想法</strong></p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/686b613391de5f1dd7b08ec8bbfc609c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*awGPFUZsiwzTC1yxHiJmQQ.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Image from this <a class="ae ks" href="https://www.youtube.com/watch?v=P5mlg91as1c" rel="noopener ugc nofollow" target="_blank">video</a></figcaption></figure><p id="c4a6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">红框</strong> →原矩阵<br/> <strong class="jw ir">蓝框</strong> →左奇异向量<br/> <strong class="jw ir">紫框</strong> →奇异值(对角矩阵)<br/> <strong class="jw ir">绿框</strong> →右奇异向量</p><p id="4ff6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我试图尽可能简单地理解难懂的概念，所以这里是我的简单(非常愚蠢的)SVD 版本。假设我们有一个蛋糕叫做 A，谢天谢地这个蛋糕 A 只由两种成分组成。牛奶 M 和糖 s 所以 A = M + S。</p><p id="888d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">然而，这里的诀窍是知道我们制作原始蛋糕 A 所需的牛奶和糖的确切组合。假设我们需要 30 升牛奶和 50 毫克糖来正确制作蛋糕 A，这意味着我们也需要在某个地方获得该信息。</p><p id="85ec" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这样做的好处是，我们不必随身携带蛋糕(这需要大量的存储空间),我们可以简单地携带牛奶、糖和说明书(告诉我们需要多少比例的牛奶和糖),作为一种更紧凑的形式。这是我对奇异值分解的理解，从这里我们可以转到主成分分析。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/c62e71bcc30a581bb3dd6b2de8570caf.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*M99gdJjVIq0jCEuCHAdn8A.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Image from this <a class="ae ks" href="http://Li, E. (2018). PCA With Tensorflow. Abracadabra. Retrieved 25 May 2018, from https://ewanlee.github.io/2018/01/17/PCA-With-Tensorflow/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="97bc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如上面在 PCA 中所看到的，我们丢弃右奇异向量并修改奇异值矩阵的维数，以便降低维数。我们可以利用这一点，使我们的 PCA 充当池操作。</p><p id="1e26" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">例如，假设我们有一批 8*8*1 的张量格式的 200 幅图像，我们可以将它写成(200，8，8，1)。但我们已经知道，我们可以对图像进行矢量化，将张量整形为(200，8*8*1) = (200，64)。现在，如果我们执行 PCA 以将维度从 64 降低到 16，我们可以将其重塑回 3D 图像，使张量变成(200，4，4，1)。因此得到的张量具有与执行平均汇集操作相同的维数。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><p id="d304" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">奇异矩阵的指数加权移动平均值/α，β</strong></p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lo"><img src="../Images/6a36bd597565ac90f7337aa746b30860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1YQt0NFAd8FJvXVwL5dGpg.png"/></div></div></figure><p id="55b2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">红框→ </strong>在训练期间更新移动奇异矩阵(sigma ),但是在测试期间，我们将使用移动平均 sigma 值来执行维度缩减。</p><p id="0e8b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们需要注意的一个小细节是奇异矩阵的指数加权移动平均。就像我们执行批量标准化一样，我们在训练期间跟踪平均值和标准差的权重。在测试期间，我们不使用数据的平均值/标准值，而是使用移动平均值。</p><p id="f04b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因为我们希望模型的预测只依赖于测试阶段的给定测试数据。因此，我们将使用奇异矩阵的移动加权平均值，而不是使用测试数据中的奇异矩阵。(如果有人想了解更多关于批量标准化<a class="ae ks" href="https://medium.com/@SeoJaeDuk/deeper-understanding-of-batch-normalization-with-interactive-code-in-tensorflow-manual-back-1d50d6903d35" rel="noopener">的信息，请点击这里。)</a></p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/bcad484808a12f58612d01a6ea4b7aea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*8P60HU1SoGZLF2qmC5WIkQ.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Original Image from the <a class="ae ks" href="https://arxiv.org/pdf/1502.03167v3.pdf" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><p id="aca9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如上所述，在批量标准化中，我们将标准化数据乘以α，并添加β项。我将遵循这个想法，也给我们的 PCAP 层一些能力来取消 PCA 操作，如下所示。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lu"><img src="../Images/ac37589eea1072e5daca6bb92f62fa25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FrHAip0RSJ2rdWhy1oCYgg.png"/></div></div></figure><p id="7ec9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">红框</strong> →将α和β项添加到整形后的 PCA 中</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><p id="3fe6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">网络架构</strong></p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lv"><img src="../Images/6947ef9cde22c67b010f7410cd6f409e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s0GF-7B77e4ValbKwm48nw.png"/></div></div></figure><p id="e2c7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">黑盒</strong> →卷积层<br/> <strong class="jw ir">黄盒</strong> →卷积+批量归一化<br/> <strong class="jw ir">绿盒</strong> →平均池层<br/> <strong class="jw ir">红盒</strong> →主成分分析池层<br/> <strong class="jw ir">粉盒</strong> →全局平均池和 Softmax</p><p id="eebe" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们将要使用的基本网络是<a class="ae ks" rel="noopener" target="_blank" href="/iclr-2015-striving-for-simplicity-the-all-convolutional-net-with-interactive-code-manual-b4976e206760">全卷积网络</a>，如上所示(浅蓝色方框)，整个网络主要分为三个不同的部分。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><p id="f7cf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">结果</strong></p><div class="la lb lc ld gt ab cb"><figure class="lw jr lx ly lz ma mb paragraph-image"><img src="../Images/36facaf7846237524e4c4db66f76fb13.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*NV2896P5XGFvOgAHuEZ9zw.png"/></figure><figure class="lw jr lx ly lz ma mb paragraph-image"><img src="../Images/d70856bc029f9a1b71fb8ef9b7575ce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*T7wosCMiXr4hQDCPYo-V2w.png"/></figure></div><p id="2bb4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">左图</strong> →随时间变化的训练精度/随时间变化的成本<br/> <strong class="jw ir">右图</strong> →随时间变化的测试精度/随时间变化的成本</p><p id="14f2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">令人惊讶的是，该模型正在学习如何使用 PCA 池层对 MNIST 图像进行分类。尽管我们需要注意这样一个事实，它几乎没有达到 50%的准确率。(都在测试/训练图像上)。</p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/86f632c6fe3cef3ee3aa4dc61005162c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*rkczAV9DkfnDhLhkQHBFpg.png"/></div></figure><p id="ed99" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">第 21 个纪元后的最终精度为 53%。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><p id="dcfa" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">交互代码</strong></p><figure class="la lb lc ld gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi md"><img src="../Images/d431dec3781892bbc6957f45a6b3c08f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YM0xneqvGI4cPzvZ-6P9ow.png"/></div></div></figure><p id="7052" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于 Google Colab，你需要一个 Google 帐户来查看代码，而且你不能在 Google Colab 中运行只读脚本，所以在你的操场上复制一份。最后，我永远不会请求允许访问你在 Google Drive 上的文件，仅供参考。编码快乐！同样为了透明，我在训练期间上传了所有的日志。</p><p id="88a1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">要访问代码以及<a class="ae ks" href="https://colab.research.google.com/drive/1yHk1y-O4PEDv4yAuhAZzCPHgL7hnSrDf" rel="noopener ugc nofollow" target="_blank">培训日志，请点击此处。</a></p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><p id="8a89" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">最后的话</strong></p><p id="ff1c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">最终结果非常有趣，因为该模型实际上能够学习如何对 MNIST 图像进行分类。我很高兴地知道，在网络中插入更复杂的操作(如 PCA)是可能的。(带端到端培训)最后，顺便提一下，如果有人有兴趣学习更多关于线性代数的知识，请观看下面的视频系列。(<a class="ae ks" href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw" rel="noopener ugc nofollow" target="_blank"> 3Blue1Brown </a>是一个产出高质量内容的土豆/数学家)</p><figure class="la lb lc ld gt jr"><div class="bz fp l di"><div class="ll lf l"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Video From this <a class="ae ks" href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="0ce3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你希望看到我所有写作的列表，请在这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">同时，在我的 twitter 上关注我<a class="ae ks" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae ks" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae ks" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae ks" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文</a> t。</p></div><div class="ab cl kt ku hu kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ij ik il im in"><p id="8e76" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">参考</strong></p><ol class=""><li id="53c2" class="mf mg iq jw b jx jy kb kc kf mh kj mi kn mj kr mk ml mm mn bi translated">李，E. (2018)。张量流 PCA。胡言乱语。检索于 2018 年 5 月 25 日，来自<a class="ae ks" href="https://ewanlee.github.io/2018/01/17/PCA-With-Tensorflow/" rel="noopener ugc nofollow" target="_blank">https://ewanlee.github.io/2018/01/17/PCA-With-Tensorflow/</a></li><li id="2e1d" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">tf.set_random_seed | TensorFlow。(2018).张量流。检索于 2018 年 5 月 25 日，来自<a class="ae ks" href="https://www.tensorflow.org/api_docs/python/tf/set_random_seed" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/set _ random _ seed</a></li><li id="52cb" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">Iris 数据集-sci kit-学习 0.19.1 文档。(2018).Scikit-learn.org。检索于 2018 年 5 月 25 日，来自<a class="ae ks" href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/auto _ examples/datasets/plot _ iris _ dataset . html</a></li><li id="3ade" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">tf.cumsum | TensorFlow。(2018).张量流。检索于 2018 年 5 月 25 日，来自<a class="ae ks" href="https://www.tensorflow.org/api_docs/python/tf/cumsum" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/cumsum</a></li><li id="168c" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">输入，T. (2018)。Tensorflow:使用 tf.slice 拆分输入。堆栈溢出。检索于 2018 年 5 月 25 日，来自<a class="ae ks" href="https://stackoverflow.com/questions/39054414/tensorflow-using-tf-slice-to-split-the-input" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/39054414/tensor flow-using-TF-slice-to-split-the-input</a></li><li id="1d6c" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">李，c，，p .，，涂，Z. (2015)。卷积神经网络中的一般化池函数:混合、门控和树。Arxiv.org。检索于 2018 年 5 月 26 日，来自<a class="ae ks" href="https://arxiv.org/abs/1509.08985" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1509.08985</a></li><li id="2722" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">主成分分析。(2018).En.wikipedia.org。检索于 2018 年 5 月 26 日，来自<a class="ae ks" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Principal_component_analysis</a></li><li id="30c5" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">(2018).cs . otago . AC . NZ . 2018 年 5 月 26 日检索，来自<a class="ae ks" href="http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf" rel="noopener ugc nofollow" target="_blank">http://www . cs . otago . AC . NZ/cosc 453/student _ tutorials/principal _ components . pdf</a></li><li id="2922" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">李，E. (2018)。张量流 PCA。胡言乱语。检索于 2018 年 5 月 25 日，来自<a class="ae ks" href="https://ewanlee.github.io/2018/01/17/PCA-With-Tensorflow/" rel="noopener ugc nofollow" target="_blank">https://ewanlee.github.io/2018/01/17/PCA-With-Tensorflow/</a></li><li id="9e1e" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">奇异值分解。(2018).En.wikipedia.org。检索于 2018 年 5 月 26 日，来自 https://en.wikipedia.org/wiki/Singular-value_decomposition<a class="ae ks" href="https://en.wikipedia.org/wiki/Singular-value_decomposition" rel="noopener ugc nofollow" target="_blank"/></li><li id="8f0c" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">tf.svd |张量流。(2018).张量流。检索于 2018 年 5 月 26 日，来自 https://www.tensorflow.org/api_docs/python/tf/svd<a class="ae ks" href="https://www.tensorflow.org/api_docs/python/tf/svd" rel="noopener ugc nofollow" target="_blank"/></li><li id="fd9b" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">线性代数预习精华。(2018).YouTube。检索于 2018 年 5 月 26 日，来自<a class="ae ks" href="https://www.youtube.com/watch?v=kjBOesZCoqc&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=kjBOesZCoqc&amp;list = plzhqobowt qd D3 mizm 2 xvfitgf 8 he _ ab</a></li><li id="a14a" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">[在线]可从以下网址获取:<a class="ae ks" href="https://www.quora.com/What-is-an-intuitive-explanation-of-singular-value-decomposition-SVD" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/What-is-an-intuitive-explain-of-singular-value-decomposition-SVD</a>【2018 年 5 月 26 日获取】。</li><li id="0d2a" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">对 Tensorflow 中使用交互式代码进行批处理规范化有了更深入的理解。(2018).中等。检索于 2018 年 5 月 26 日，来自<a class="ae ks" href="https://medium.com/@SeoJaeDuk/deeper-understanding-of-batch-normalization-with-interactive-code-in-tensorflow-manual-back-1d50d6903d35" rel="noopener">https://medium . com/@ SeoJaeDuk/deeper-understanding-of-batch-normalization-with-interactive-code-in-tensor flow-manual-back-1d50d 6903 d35</a></li><li id="1c76" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">[在线]可从以下网址获取:<a class="ae ks" href="https://www.quora.com/What-is-an-intuitive-explanation-of-singular-value-decomposition-SVD" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/What-is-an-intuitive-explain-of-singular-value-decomposition-SVD</a>【2018 年 5 月 26 日获取】。</li><li id="1a2d" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">奇异值分解教程:应用，例子，练习。(2017).统计和机器人。检索于 2018 年 5 月 26 日，来自<a class="ae ks" href="https://blog.statsbot.co/singular-value-decomposition-tutorial-52c695315254" rel="noopener ugc nofollow" target="_blank">https://blog . statsbot . co/singular-value-decomposition-tutorial-52c 695315254</a></li><li id="0eee" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">(2018).Arxiv.org。检索于 2018 年 5 月 26 日，来自<a class="ae ks" href="https://arxiv.org/pdf/1502.03167v3.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1502.03167v3.pdf</a></li><li id="eab5" class="mf mg iq jw b jx mo kb mp kf mq kj mr kn ms kr mk ml mm mn bi translated">[ ICLR 2015 ]追求简单:具有交互码的全卷积网。(2018).走向数据科学。检索于 2018 年 5 月 26 日，来自<a class="ae ks" rel="noopener" target="_blank" href="/iclr-2015-striving-for-simplicity-the-all-convolutional-net-with-interactive-code-manual-b4976e206760">https://towards data science . com/iclr-2015-努力简化-所有卷积网-交互式代码-手册-b4976e206760 </a></li></ol></div></div>    
</body>
</html>