<html>
<head>
<title>Learning and performing in the real world</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在现实世界中学习和表演</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-and-performing-in-the-real-world-7e53eb46d9c3?source=collection_archive---------3-----------------------#2017-12-08">https://towardsdatascience.com/learning-and-performing-in-the-real-world-7e53eb46d9c3?source=collection_archive---------3-----------------------#2017-12-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="452e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">又名强化学习</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2e8377539aeef68b1e2a42b234a23819.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cYszIAiWlfzf9ppkh2YegQ.jpeg"/></div></div></figure><p id="0816" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了与复杂的环境互动，生物实体需要产生一个“正确”的<em class="ln">动作序列</em>，以实现延迟的未来<em class="ln">回报</em>。这些生命实体，或者说<em class="ln">角色</em>，能够感知环境，并对环境和代理先前历史的一系列状态做出反应。见图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lo"><img src="../Images/c36c71e94536404a3c24d84ef9ca5abb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aa9Akd8Vf9oA-ALpURejUA.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">The reinforcement learning loop: an agent performs actions in an environment, and gets rewards. Learning has to occur through sparse rewards</figcaption></figure><p id="1fa9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们想要学习执行一些任务的场景是:</p><ul class=""><li id="c6f3" class="lt lu iq kt b ku kv kx ky la lv le lw li lx lm ly lz ma mb bi translated">环境中的演员</li><li id="2762" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">演员有他/她想要实现的目标</li><li id="70c3" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">循环:演员感知自身和环境</li><li id="22d7" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">循环:演员动作影响自身和环境</li><li id="0a22" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">延迟奖励或惩罚——我们达到目标了吗？</li><li id="ae0d" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">学习:如何最大化奖励和最小化惩罚</li></ul><p id="5809" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这些动作促使演员更接近多个目标，即使实现一个目标也可能需要不确定的步骤。代理需要学习模型<em class="ln">状态</em>和行动的正确顺序，通过与世界互动和执行在线学习来获得延迟的奖励。</p><p id="1524" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">人工智能(AI)的最新工作使用强化学习算法，利用稀疏和零星的奖励对代理的行为进行建模。该领域的最新进展允许训练智能体在复杂的人工环境中执行，甚至在相同的背景下超越人类的能力[Mnih2013，-2016]。该领域一些令人兴奋的最新成果是在执行强化学习任务的同时学习语言——见<a class="ae mh" href="https://arxiv.org/abs/1706.07230" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="53a7" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">形式主义</h1><p id="2e8e" class="pw-post-body-paragraph kr ks iq kt b ku na jr kw kx nb ju kz la nc lc ld le nd lg lh li ne lk ll lm ij bi translated">为了学习在现实世界中表演，给定一系列状态<em class="ln"> s </em>我们想要获得一系列动作a，以最大化奖励或获胜概率。</p><p id="ae24" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">基本需求(假设我们不知道游戏规则，只知道我们可以执行的一系列可能的操作):</p><ul class=""><li id="7733" class="lt lu iq kt b ku kv kx ky la lv le lw li lx lm ly lz ma mb bi translated">需要能够<strong class="kt ir">评估</strong>一个状态——我们是赢了，还是输了，离奖励有多远？</li><li id="72c6" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">需要能够<strong class="kt ir">从状态、行动<em class="ln"> {s，a} </em>对预测</strong>结果——假设我们决定采取行动<em class="ln"> a </em>，我们期望什么回报或结果？玩心理游戏</li><li id="c184" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">需要记住一系列可以延续很长时间的配对</li></ul><p id="2336" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在棋盘游戏中，如国际象棋、围棋等，整个游戏是完全可观察的，而在现实世界中，如自动驾驶机器人汽车，只有一部分环境是可见的，这两种游戏有很大的不同。</p><h1 id="cfdd" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">完全可观测的</h1><p id="8f37" class="pw-post-body-paragraph kr ks iq kt b ku na jr kw kx nb ju kz la nc lc ld le nd lg lh li ne lk ll lm ij bi translated">AlphaZero的论文展示了一种完全可观察游戏的优雅方法。</p><p id="04a7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"/><a class="ae mh" href="https://arxiv.org/abs/1712.01815" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir">alpha zero</strong></a><strong class="kt ir">:</strong></p><p id="9fc7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> One (AZ1): </strong> a函数<em class="ln"> f </em>(神经网络)基于参数<em class="ln">θ</em>输出动作概率<em class="ln"> a </em>和动作值<em class="ln"> v </em>(对出招值的预测)。</p><blockquote class="nf"><p id="7f7e" class="ng nh iq bd ni nj nk nl nm nn no lm dk translated"><em class="np"> a，v = f(θ，s) </em></p></blockquote><p id="4d5c" class="pw-post-body-paragraph kr ks iq kt b ku nq jr kw kx nr ju kz la ns lc ld le nt lg lh li nu lk ll lm ij bi translated"><strong class="kt ir">二(AZ2): </strong>一个预测模型，可以评估不同走法和自我玩法(脑子里的玩法场景)的结果。AlphaZero 中的蒙特卡罗树搜索。这些游戏结果、移动概率<em class="ln"> a </em>和值<em class="ln"> v </em>然后被用于更新函数<em class="ln"> f </em>。</p><p id="8451" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这两种成分为学习玩这些桌上游戏提供了一种非常简单和优雅的方式，并且在相对较短的时间内在没有人类支持的情况下学习。</p><h1 id="2c24" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">部分可观察的</h1><p id="eae5" class="pw-post-body-paragraph kr ks iq kt b ku na jr kw kx nb ju kz la nc lc ld le nd lg lh li ne lk ll lm ij bi translated">但是非完全可观察博弈呢？比如第一人称射击游戏(Doom)？还是学习驾驶的自动驾驶汽车？</p><p id="2cd1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">只有环境的一部分是可见的，因此预测行动的结果和下一个状态是一个困难得多的问题。</p><p id="e27e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">有太多可能的选项需要搜索。搜索树太大了。我们可以使用AZ2，但我们需要聪明和快速地评估哪些选项，因为有太多的选项，而我们只有有限的时间来决定下一步的行动！</p><p id="83b1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">可能没有其他玩家可以与之竞争。这里的代理人必须与自己竞争，或者与自己的预测竞争，从其预见事件的能力或不预见事件的能力中获得中间报酬。</p><blockquote class="nf"><p id="d958" class="ng nh iq bd ni nj nk nl nm nn no lm dk translated">我们如何做到这一点？</p></blockquote><h1 id="d26a" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw nv jx mu jz nw ka mw kc nx kd my mz bi translated">建议</h1><p id="9f40" class="pw-post-body-paragraph kr ks iq kt b ku na jr kw kx nb ju kz la nc lc ld le nd lg lh li ne lk ll lm ij bi translated">不幸的是，尽管近年来做出了努力，RL算法仅在小的人工场景中工作，而没有扩展到更复杂或真实生活的环境中。原因是，目前这些系统的大部分参数都是基于过于稀疏的奖励，用耗时的强化学习来训练的。在现实世界的场景中，模型变得非常大(有许多参数),几乎不可能在短时间内进行训练。</p><p id="aa85" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们需要的是能够训练一个<em class="ln">模型大脑</em>熟悉环境，并且能够预测行动和事件组合。有了这个预先训练好的<em class="ln">模型大脑</em>，用于实现特定目标的强化学习分类器，以及一小组可训练的参数，使用稀疏奖励来训练要容易得多。我们需要一个预先训练好的神经网络，至少可以处理视觉输入。它需要在视频序列上进行训练，并且需要能够提供输入的未来表示的预测。</p><p id="d0e2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了超越这些限制，我们正在研究一种具有以下特征的合成模型大脑:</p><ul class=""><li id="5214" class="lt lu iq kt b ku kv kx ky la lv le lw li lx lm ly lz ma mb bi translated">活跃在环境中，能够感知和行动</li><li id="4d1b" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">感觉序列、动作和奖励的记忆</li><li id="9366" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">注意:只关注重要的数据</li><li id="020b" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">预测:预测演员和环境的未来状态，这样我们就可以进行在线学习——通过预测，我们可以了解我们知道或不知道什么，并在我们需要学习时有一个“惊喜”信号(<a class="ae mh" href="https://medium.com/towards-data-science/a-new-kind-of-deep-neural-networks-749bcde19108" rel="noopener">见这个</a>和<a class="ae mh" href="https://medium.com/towards-data-science/memory-attention-sequences-37456d271992" rel="noopener">这个</a>)。预测也不需要监督信号，因为它们可以针对实际的未来事件及其结果进行误差测试。</li></ul><blockquote class="nf"><p id="13e2" class="ng nh iq bd ni nj ny nz oa ob oc lm dk translated">关键是能够预测结果——函数f (AZ1)需要有预测能力，换句话说，它需要有预见未来的能力。</p></blockquote><figure class="oe of og oh oi kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/351b5f1853ebdf7dff19bc2a27e755e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*RLAGZK62TODCUkkt-psbeQ.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">A proposal for a neural network that can understand the world and act in it. This network uses video and can predict future representation based on a combination of the state s and its own associative memory. A multi-head attention can recall memories and combine them with state s to predict the best action to take.</figcaption></figure><p id="1644" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在左边，你可以看到一个可以理解世界并在其中行动的神经网络的提案。该网络使用视频，并且可以基于状态s和它自己的关联存储器的组合来预测未来的表示。多头注意力可以回忆起记忆，并将它们与状态结合起来，以预测要采取的最佳行动。</p><p id="03c4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该模型能够在多种条件下运行良好:</p><ul class=""><li id="31c9" class="lt lu iq kt b ku kv kx ky la lv le lw li lx lm ly lz ma mb bi translated">持续学习:我们需要这个模型能够同时学习多项任务，并且学习时不会忘记，所以旧的任务不会被忘记</li><li id="c4a1" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">一次性学习和迁移学习:我们需要这个模型能够从真实的和合成的例子中学习</li><li id="c5bd" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">虚拟重放:该模型是可预测的，即使在事件被目睹后也能预测不同的结果。它可以在脑海中播放和回放可能的动作，并选择最好的一个。关联存储器充当搜索树。</li></ul><p id="5d73" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们如何训练这个合成大脑？</p><ul class=""><li id="9348" class="lt lu iq kt b ku kv kx ky la lv le lw li lx lm ly lz ma mb bi translated">大部分是无人监督的，或者自我监督的</li><li id="22e8" class="lt lu iq kt b ku mc kx md la me le mf li mg lm ly lz ma mb bi translated">这里或那里几乎没有监督，但不能保证什么时候</li></ul><blockquote class="nf"><p id="9498" class="ng nh iq bd ni nj ny nz oa ob oc lm dk translated">但是设计和训练预测神经网络是人工智能领域当前的挑战。</p></blockquote><p id="b5e9" class="pw-post-body-paragraph kr ks iq kt b ku nq jr kw kx nr ju kz la ns lc ld le nt lg lh li nu lk ll lm ij bi translated">我们在过去争论过一种<a class="ae mh" rel="noopener" target="_blank" href="/a-new-kind-of-deep-neural-networks-749bcde19108">的新型神经网络</a>，它已经被证明在学习RL任务中更加有效。</p><p id="717f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们还喜欢<a class="ae mh" href="https://arxiv.org/abs/1710.09829" rel="noopener ugc nofollow" target="_blank">胶囊</a>的预测能力，它不需要地面实况表示，但能够根据前一层预测下一层输出。</p><p id="2d59" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">注1: </strong>这是一篇<a class="ae mh" href="http://A great summary of DL work and success or lack thereof https://www.alexirpan.com/2018/02/14/rl-hard.html" rel="noopener ugc nofollow" target="_blank">的好文章</a>，讲述了为什么RL工作得好或者不好，以及与之相关的问题。我同意现在很多学习效率很低，也没有迁移学习的成功案例。这就是为什么我们应该推行预训练网络、课程学习和将任务分解成许多更简单的任务，每个任务都有简单的奖励的方法。如果你的搜索空间如此之大，一个解决所有问题的方法是很难的！</p><h1 id="820e" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">关于作者</h1><p id="7e75" class="pw-post-body-paragraph kr ks iq kt b ku na jr kw kx nb ju kz la nc lc ld le nd lg lh li ne lk ll lm ij bi translated">我在硬件和软件方面都有将近20年的神经网络经验(一个罕见的组合)。在这里看关于我:<a class="ae mh" href="https://medium.com/@culurciello/" rel="noopener">传媒</a>、<a class="ae mh" href="https://e-lab.github.io/html/contact-eugenio-culurciello.html" rel="noopener ugc nofollow" target="_blank">网页</a>、<a class="ae mh" href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ" rel="noopener ugc nofollow" target="_blank">学者</a>、<a class="ae mh" href="https://www.linkedin.com/in/eugenioculurciello/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>等等…</p></div></div>    
</body>
</html>