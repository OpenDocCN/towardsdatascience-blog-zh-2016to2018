<html>
<head>
<title>Review: DSSD — Deconvolutional Single Shot Detector (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:DSSD —解卷积单次探测器(目标探测)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=collection_archive---------4-----------------------#2018-12-23">https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=collection_archive---------4-----------------------#2018-12-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f12d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">反卷积层:引入额外的大规模背景，提高小对象的准确性</h2></div><p id="c1df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di"> T </span>他的时代，<strong class="kh ir"> DSSD(反卷积单次检测器)</strong>被回顾。DSSD，用<strong class="kh ir">去进化路径</strong>，改进了之前的<a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>。是<strong class="kh ir"> 2017 arXiv </strong>中的一篇技术报告，引用超过<strong class="kh ir"> 100 次</strong>。(<a class="ll lm ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----d4821a2bbeb5--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @ Medium)。</p><ul class=""><li id="6e8b" class="ln lo iq kh b ki kj kl km ko lp ks lq kw lr la ls lt lu lv bi translated"><strong class="kh ir">逐步反卷积</strong>放大特征图</li><li id="388b" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated"><strong class="kh ir">来自卷积路径和反卷积路径的特征组合</strong></li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="da45" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">涵盖哪些内容</h1><ol class=""><li id="56e3" class="ln lo iq kh b ki na kl nb ko nc ks nd kw ne la nf lt lu lv bi translated"><strong class="kh ir">整体架构</strong></li><li id="0c75" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la nf lt lu lv bi translated"><strong class="kh ir">反卷积模块</strong></li><li id="ed89" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la nf lt lu lv bi translated"><strong class="kh ir">预测模块</strong></li><li id="5d75" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la nf lt lu lv bi translated"><strong class="kh ir">一些训练细节</strong></li><li id="7f0d" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la nf lt lu lv bi translated"><strong class="kh ir">结果</strong></li></ol></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="3c4c" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated"><strong class="ak"> 1。整体架构</strong></h1><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ng"><img src="../Images/db70e983fd0ea73595456e7ff840d925.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HlSO0Glv6jW9t4x4qHGojg.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">SSD (Top) DSSD (Bottom)</strong></figcaption></figure><ul class=""><li id="4fa9" class="ln lo iq kh b ki kj kl km ko lp ks lq kw lr la ls lt lu lv bi translated"><strong class="kh ir">白色转换</strong>:可以是<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> <strong class="kh ir"> VGGNet </strong> </a> <strong class="kh ir">或</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="kh ir"> ResNet </strong> </a>骨干进行特征提取</li><li id="fa37" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated"><strong class="kh ir">蓝色转换率</strong>:是<strong class="kh ir">原来的</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> <strong class="kh ir"> SSD </strong> </a>部分，涉及到去掉原来的<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a> / <a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>的全连通层，使用萎缩/扩张卷积(源自小波，由<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLab </a>或<a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5"> DilatedNet </a>使用)添加 conv 层。(有兴趣请访问<a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>。)</li><li id="f205" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated"><strong class="kh ir">剩余转换</strong>:由反卷积模块和预测模块组成，后面会详细介绍。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="f986" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">2.去卷积模块</h1><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/0cb0fca2d9179b3f879ebc30ba389646.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*jw5q1fQQU4QVyhhrT2dS0Q.png"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Deconvolution Module</strong></figcaption></figure><ul class=""><li id="e701" class="ln lo iq kh b ki kj kl km ko lp ks lq kw lr la ls lt lu lv bi translated">反卷积路径上的那些特征图通过 Deconv2×2 然后 Conv3×3+ <a class="ae lk" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> BN </a>进行上采样。</li><li id="78f0" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated">另一方面，对应的相同大小的特征图具有 con v3×3+<a class="ae lk" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">BN</a>+ReLU+con v3×3+<a class="ae lk" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">BN</a>。</li><li id="0549" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated">然后，它们按元素相乘(Eltw 乘积),并进行 ReLU，然后传递给预测模块。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="3784" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated"><strong class="ak"> 3。预测模块</strong></h1><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ny"><img src="../Images/ac86cd6c508fd78f40f25d191f490d91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9uj2-XjVi0igy2yP9Sc_Lw.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Various Prediction Module</strong></figcaption></figure><ul class=""><li id="2cf9" class="ln lo iq kh b ki kj kl km ko lp ks lq kw lr la ls lt lu lv bi translated">测试了各种预测模块。</li><li id="a1b1" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated">(a):是<a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>中使用的最基本的一个，直接预测对象类，进行包围盒回归。</li><li id="cc1f" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated">(b):在特征图上执行 Conv1×1 的附加集合，以增加维度。还有一个与元素相加的 skip 连接。</li><li id="bb2b" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated">(c):除了在跳过连接路径上执行一个额外的 Conv1×1 之外，它是(b)的一个。</li><li id="b453" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated">(d):两个(c)是级联的。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="744b" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">4.一些训练细节</h1><h2 id="cbf1" class="nz mj iq bd mk oa ob dn mo oc od dp ms ko oe of mu ks og oh mw kw oi oj my ok bi translated">两阶段训练</h2><ul class=""><li id="ad7b" class="ln lo iq kh b ki na kl nb ko nc ks nd kw ne la ls lt lu lv bi translated">使用 ImageNet 预训练模型训练有素的 SSD 。</li><li id="d2df" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated">对于第一阶段，仅训练反卷积侧。</li><li id="04dd" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated">第二阶段，对整个网络进行微调。</li></ul><h2 id="fae8" class="nz mj iq bd mk oa ob dn mo oc od dp ms ko oe of mu ks og oh mw kw oi oj my ok bi translated"><strong class="ak">其他</strong></h2><ul class=""><li id="e16e" class="ln lo iq kh b ki na kl nb ko nc ks nd kw ne la ls lt lu lv bi translated">还使用了广泛的数据扩充，包括随机裁剪、翻转和随机光度失真。</li><li id="a0e8" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated">在使用 K-均值聚类进行分析之后，添加纵横比为 1.6 的先前盒子，即使用{1.6，2.0，3.0}。</li></ul><p id="4826" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基于以上所述，在 PASCAL VOC 2007 上进行消融研究:</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/5667b061eb7c2ba6855f37f78aa06e72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*Qsc9mQkSVBXlnmGsQ0EkMQ.png"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Results on PASCAL VOC 2007</strong></figcaption></figure><ul class=""><li id="c231" class="ln lo iq kh b ki kj kl km ko lp ks lq kw lr la ls lt lu lv bi translated"><strong class="kh ir"> SSD 321 </strong>:原装<a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>带输入 321×321，76.4% mAP。</li><li id="a7c9" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated"><strong class="kh ir"> SSD 321 + PM(c) </strong>:原装<a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>使用预测模块(c)，77.1 % mAP，比使用 PM (b)和 PM (d)的要好。</li><li id="de56" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated"><strong class="kh ir">SSD 321+PM(c)+DM(Eltw-prod)</strong>:DM 表示解卷积模块，因此，这是 DSSD 使用 PM(c)和用于特征组合的元素式产品，78.6% mAP。它比使用元素相加的方法要好一点。</li><li id="c07c" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated"><strong class="kh ir">SSD 321+PM(c)+DM(Eltw-prod)+Stage 2</strong>:采用两阶段训练，成绩下降。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="7920" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">5.结果</h1><h2 id="deda" class="nz mj iq bd mk oa ob dn mo oc od dp ms ko oe of mu ks og oh mw kw oi oj my ok bi translated">5.1.帕斯卡 VOC 2007</h2><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi om"><img src="../Images/c737c5cf6f23149fc047f25bb6c10c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mKPSh9pX9bELHyflQRO3ag.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Results on PASCAL VOC 2007 Test</strong></figcaption></figure><p id="892f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SSD 和 DSSD 在 2007 trainval 和 2012 trainval 的联合上接受培训。</p><ul class=""><li id="ac58" class="ln lo iq kh b ki kj kl km ko lp ks lq kw lr la ls lt lu lv bi translated"><strong class="kh ir"> SSD300* </strong>和<strong class="kh ir"> SSD512* </strong> (*表示使用了新的数据扩充技巧。):凭借*，最初的<a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">固态硬盘</a>已经超越了除<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>之外的其他最先进的方法。</li><li id="9e49" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated"><strong class="kh ir"> SSD 321 </strong>和<strong class="kh ir"> SSD 513 </strong>:以<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>为骨干，性能已经和 SSD300*和 SSD512*差不多。</li><li id="5595" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated"><strong class="kh ir"> DSSD 321 </strong>和<strong class="kh ir"> DSSD 513 </strong>:通过反卷积路径，它们分别优于 SSD 321 和 SSD 513。</li><li id="217c" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated">特别是，<strong class="kh ir"> DSSD513 的表现优于</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"><strong class="kh ir">R-FCN</strong></a><strong class="kh ir">。</strong></li></ul><h2 id="e95e" class="nz mj iq bd mk oa ob dn mo oc od dp ms ko oe of mu ks og oh mw kw oi oj my ok bi translated">5.2.帕斯卡 VOC 2012</h2><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi on"><img src="../Images/8be47d095bcd12fb590796095478c16c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K58AfD8d8R2L_WaYUFVlZg.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Results on PASCAL VOC 2012 Test</strong></figcaption></figure><p id="1956" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">VOC 2007 trainval+测试和 2012 trainval 用于培训。既然发现两阶段训练没用，这里就用一阶段训练。</p><ul class=""><li id="feb5" class="ln lo iq kh b ki kj kl km ko lp ks lq kw lr la ls lt lu lv bi translated"><strong class="kh ir"> DSSD 513 以 80.0%的 mAP 表现优于其他品种。</strong>并且不使用 COCO 数据集的额外训练数据进行训练。</li></ul><h2 id="978a" class="nz mj iq bd mk oa ob dn mo oc od dp ms ko oe of mu ks og oh mw kw oi oj my ok bi translated">5.3.可可女士</h2><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi oo"><img src="../Images/2a00ad17122ee7ed30ef99a026e71244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sKqVMSbybrmkT6LAqsotNw.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Results on MS COCO Test</strong></figcaption></figure><p id="fae5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还是那句话，没有两阶段训练。</p><ul class=""><li id="d08d" class="ln lo iq kh b ki kj kl km ko lp ks lq kw lr la ls lt lu lv bi translated"><strong class="kh ir"> SSD300* </strong>已经比<a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快 R-CNN </a>和 ION。</li><li id="01e2" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated"><strong class="kh ir"> DSSD321 在小物体上的 AP 更好</strong>，7.4%相比 SSD321 只有 6.2%。</li><li id="d669" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated">对于更大的车型，<strong class="kh ir"> DSSD513 </strong>获得 33.2%的 mAP，比 29.9% mAP 的 <a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> <strong class="kh ir"> R-FCN </strong> </a>要好<strong class="kh ir">。它已经和更快的 R-CNN++取得了竞争结果。(+++表示它也使用 VOC2007 和 VOC2012 进行培训。)</strong></li></ul><h2 id="7418" class="nz mj iq bd mk oa ob dn mo oc od dp ms ko oe of mu ks og oh mw kw oi oj my ok bi translated">5.4.推理时间</h2><p id="8898" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko op kq kr ks oq ku kv kw or ky kz la ij bi translated">为了在测试过程中简化网络，<a class="ae lk" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> BN </a>被移除，并与 conv 合并，如下所示:</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi os"><img src="../Images/5ff0bc8750bc7d2e1301382c31fb9f7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*l-uP_tRYd0iYNH58PaoPBw.png"/></div></figure><p id="c32c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之，他们试图将 BN 效应合并到 conv 层的权重和偏差计算中，从而简化网络。这将速度提高了 1.2 到 1.5 倍，并将内存减少了三倍。</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ot"><img src="../Images/12b722c12e678bd7c5035cc84d7b142e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rDZ8u7UP9WZo9Cs8Prtrqg.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">Speed &amp; Accuracy on PASCAL VOC 2007 Test</strong></figcaption></figure><ul class=""><li id="413f" class="ln lo iq kh b ki kj kl km ko lp ks lq kw lr la ls lt lu lv bi translated"><strong class="kh ir"> SSD 513 </strong>与<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> <strong class="kh ir"> R-FCN </strong> </a> (9 FPS)相比，速度(8.7 fps)和精度差不多。<strong class="kh ir">去除 BN 层</strong>并与 conv 层合并，得到<strong class="kh ir"> 11.0 fps </strong>更快。</li><li id="cf6c" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated">DSSD513 比<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>精度更高，但速度稍慢。</li><li id="327c" class="ln lo iq kh b ki lw kl lx ko ly ks lz kw ma la ls lt lu lv bi translated">DSSD321 比<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>精度低，但速度更快。</li></ul><h2 id="ce89" class="nz mj iq bd mk oa ob dn mo oc od dp ms ko oe of mu ks og oh mw kw oi oj my ok bi translated">5.5.定性结果</h2><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ou"><img src="../Images/631987253f620ed84ee4a5c14cd5405f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6TsybXzdVBVM8TtxS7nTNA.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk"><strong class="bd nw">SSD (Left) DSSD (Right)</strong></figcaption></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="8859" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于输入尺寸较小，<a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>在小对象上效果不佳。随着反卷积路径，DSSD 显示出明显的改善。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="25ef" class="nz mj iq bd mk oa ob dn mo oc od dp ms ko oe of mu ks og oh mw kw oi oj my ok bi translated">参考</h2><p id="194f" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko op kq kr ks oq ku kv kw or ky kz la ij bi translated">【2017 arXiv】【DSSD】<br/><a class="ae lk" href="https://arxiv.org/abs/1701.06659" rel="noopener ugc nofollow" target="_blank">DSSD:解卷积单粒子探测器</a></p><h2 id="00d9" class="nz mj iq bd mk oa ob dn mo oc od dp ms ko oe of mu ks og oh mw kw oi oj my ok bi translated">我的相关评论</h2><p id="2dcb" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko op kq kr ks oq ku kv kw or ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae lk" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lk" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae lk" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">yolo v1</a><a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a><a class="ae lk" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">yolo v2/yolo 9000</a></p><p id="6582" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/>T23】[<a class="ae lk" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a>]</strong></p></div></div>    
</body>
</html>