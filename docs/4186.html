<html>
<head>
<title>Not-So-Deep Reinforcement Learning for dummies— Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">虚拟人的非深度强化学习——第一部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/not-so-deep-reinforcement-learning-for-dummies-part-1-c22416a55535?source=collection_archive---------6-----------------------#2018-07-26">https://towardsdatascience.com/not-so-deep-reinforcement-learning-for-dummies-part-1-c22416a55535?source=collection_archive---------6-----------------------#2018-07-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5edd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">强化学习初学者指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/18ebb64531a7acae0e9ef48c5518f9b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lVtHeC3q2HOfR09UDOryQA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="https://unsplash.com/photos/1qkyck-UL3g" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="e089" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我是 Udacity 深度强化学习纳米学位第一批的学生。这一系列的博客文章对我来说更像是给自己的笔记。我希望，通过写这些帖子，我能理清我的概念，也许还能帮助别人理解它们。在这篇文章中，我们将讨论什么是强化学习，并概述一个基本的强化学习环境。</p><blockquote class="ls"><p id="3452" class="lt lu iq bd lv lw lx ly lz ma mb lr dk translated">当我们思考学习的本质时——强化学习:导论——我们最先想到的可能是通过与环境互动来学习</p></blockquote><p id="215f" class="pw-post-body-paragraph kw kx iq ky b kz mc jr lb lc md ju le lf me lh li lj mf ll lm ln mg lp lq lr ij bi translated">上面这条线，就是强化学习的全部，从互动中学习；从经验来看。作为刚出生到这个世界的人类和动物，我们不知道我们周围的世界是如何运作的。只有在与世界互动多年后，我们才开始理解它如何回应我们的行为，只有那时我们才能采取具体行动来实现我们的目标。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mh"><img src="../Images/eba6171b03067af4a0dbdeca72b62324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mRq0tF2PHWNArckxPBPpGw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Basic diagram of a Reinforcement Learning scenario</figcaption></figure><p id="9276" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似地，在强化学习中，我们有一个与环境交互的代理。在每个时间步，代理接收环境的当前<strong class="ky ir">状态</strong>，并且代理必须选择一个适当的<strong class="ky ir">动作</strong>作为响应。代理执行动作后，代理会收到一个<strong class="ky ir">奖励</strong>和一个新的<strong class="ky ir">状态</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mi"><img src="../Images/e5154fe9a938e48c742f4a0130ee4870.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*f2k7ElyMNt2Jras3abKqlw.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Robot moving in a strainght line (Source: <a class="ae kv" href="https://s3.amazonaws.com/video.udacity-data.com/topher/2018/March/5aa05f61_screen-shot-2018-03-07-at-3.53.08-pm/screen-shot-2018-03-07-at-3.53.08-pm.png" rel="noopener ugc nofollow" target="_blank">Udacity</a>)</figcaption></figure><p id="dd3d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就拿上面的例子来说吧。在上述具有从 0 到 4 的<strong class="ky ir">状态</strong>的环境中，我们有一个<strong class="ky ir">代理</strong>(机器人)可以左右采取两个<strong class="ky ir">动作</strong>。机器人的<strong class="ky ir">目标</strong>是到达状态 3。如果代理在位置 0 时选择向左移动，或者在位置 4 时选择向右移动，代理就保持在原地。同样，每当代理采取行动时，环境给代理奖励-1，并在达到状态 3 时给+10。为了理解第一张图，让我们考虑一个时间步长。</p><p id="dab6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代理的<strong class="ky ir">当前状态</strong>为 1，随机采取<strong class="ky ir">动作</strong>离开。因此，环境给它的奖励是-1，它的新状态也是 0。让我们考虑几个小插曲来理清事情。顺便说一下，一集是代理应用随机动作的一次迭代，直到它达到目标，然后终止。</p><p id="3159" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在第一集里，代理从<strong class="ky ir">状态</strong> 1、<em class="mj"> S(1) </em>开始，有一个<strong class="ky ir">奖励</strong>为 0、<em class="mj"> R(0) </em>。然后它向左执行一个<strong class="ky ir">动作</strong>并接收一个-1 的<strong class="ky ir">奖励</strong>，<em class="mj"> R(-1) </em>并在<strong class="ky ir">下一个状态</strong> 2，<em class="mj"> S(2) </em>中结束…最终在几个动作之后，它到达 S(3)并终止。在这一集的最后，代理人得到的奖励是 0–1–1–1+10 = 7</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="e517" class="mp mq iq ml b gy mr ms l mt mu">Episode 1<br/>R(0)|S(1) → Left → R(-1)|S(0) → Right → R(-1)|S(1) → Right → R(-1)|S(2) →Right → R(+10)|S(3) → Episode Ends<br/>--------------------------------------------------------------------Episode 2<br/>R(0)|S(1) → Right → R(-1)|S(2) →Right → R(+10)|S(3) → Episode Ends</span></pre><p id="08cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似地，代理完成了第二集。但是注意，这一次，代理人获得的奖励是 0–1+10 = 9，高于第一次行动。</p><p id="a662" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，强化学习就是学习做什么；将特定行动与情况/状态对应起来，以最大化数字奖励。代理人必须探索环境，并通过尝试了解哪些行为会产生最大的回报。代理人采取的行动可能会影响即时奖励以及下一个状态，进而影响所有后续奖励。你可以把这想象成蝴蝶效应，一个小小的举动可能会在未来导致巨大的回报或者非常糟糕的回报。</p><p id="9f26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们研究的上面的例子是我们所说的<strong class="ky ir">片断任务</strong>，它有一个明确定义的终点。还有一种类型的任务叫做<strong class="ky ir">持续任务，</strong>永远持续下去。课程中提到的一个令人敬畏的连续任务的例子是股票交易机器人，它连续分析市场并做出相应的决策，而不是每次都停下来。稍后我们将学习用于连续任务的算法。</p><h1 id="2b53" class="mv mq iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">为什么不那么深？</h1><p id="0bee" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">深度学习是当你使用一个深度神经网络来预测一个代理的行为。在这个系列中，我们将讨论几十年前开发的传统算法，它构成了强化学习的基础。</p><p id="37be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在接下来的几个部分中，我们将讨论可用于强化学习的各种算法。一些例子是 SARSA、SARSA-Max、蒙特卡罗方法等。所以，敬请关注。</p></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><p id="0483" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二部分已经出来了。点击查看<a class="ae kv" href="https://medium.com/@thisisppn/not-so-deep-reinforcement-learning-for-dummies-part-2-854216d1fe0d" rel="noopener">。</a></p></div></div>    
</body>
</html>