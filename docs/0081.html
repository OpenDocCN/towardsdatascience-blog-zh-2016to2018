<html>
<head>
<title>Can neural networks solve any problem?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络能解决任何问题吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/can-neural-networks-really-learn-any-function-65e106617fc6?source=collection_archive---------0-----------------------#2017-03-07">https://towardsdatascience.com/can-neural-networks-really-learn-any-function-65e106617fc6?source=collection_archive---------0-----------------------#2017-03-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dd6c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">可视化通用逼近定理</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/795cb3c1d9811a5f70eb1036fbe94ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fTOOck9OcezPF9aeJ1WrMQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Let’s see you try modeling that, neural network!</figcaption></figure><p id="a59b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在你深度学习之旅的某个时候，你可能会遇到<a class="ae lr" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank">通用近似定理</a>。</p><blockquote class="ls"><p id="31b1" class="lt lu iq bd lv lw lx ly lz ma mb lq dk translated">具有单一层的前馈网络足以表示任何函数，但是该层可能大得不可行，并且可能无法正确地学习和概括。</p><p id="1a0a" class="lt lu iq bd lv lw lx ly lz ma mb lq dk translated">——伊恩·古德菲勒，<a class="ae lr" href="http://www.deeplearningbook.org/contents/mlp.html" rel="noopener ugc nofollow" target="_blank"> DLB </a></p></blockquote><p id="cc9a" class="pw-post-body-paragraph kv kw iq kx b ky mc jr la lb md ju ld le me lg lh li mf lk ll lm mg lo lp lq ij bi translated">这是一个令人难以置信的说法。如果你认为大多数问题都可以归结为函数，那么这种说法意味着神经网络在理论上可以解决任何问题。如果人类智能可以用函数来建模(也许是非常复杂的函数)，那么我们今天就有了复制人类智能的工具。神经网络可能是人工智能版的巴贝奇分析引擎(1822)，而终结者需要一台Macbook Pro，但仍然如此。也许UAT解释了为什么深度学习如此成功地解决了人工智能中的“难题”——图像识别、机器翻译、语音转文本等。</p><p id="b794" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">TLDR；</strong>我用视觉和经验向自己证明，使用单个隐藏层和6个神经元，UAT适用于非平凡函数(<em class="mh"> x +x -x -1) </em>。我假装自己是一个神经网络，试图自己“学习”正确的权重。我还在代码中验证了这一点。</p><h2 id="1724" class="mi mj iq bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">神经网络怎么可能模拟任何功能？</h2><p id="47e8" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">这个问题难倒了我很久，我在网上找不到很好的解释。大多数句子与等式的比率高于0.57的解释应该是这样的:</p><blockquote class="ng nh ni"><p id="2678" class="kv kw mh kx b ky kz jr la lb lc ju ld nj lf lg lh nk lj lk ll nl ln lo lp lq ij bi translated">通过激活函数引入非线性允许我们逼近任何函数。很简单，真的。—埃隆·马斯克</p></blockquote><p id="fc25" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以非线性激活函数是秘方？我们真的可以通过将一系列Sigmoid激活链接在一起来模拟任何功能吗？<a class="ae lr" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU </a>功能怎么样？肯定不是——里面有单词<em class="mh"> linear </em>！校正线性单位！</p><p id="6bdc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我最终发现了迈克尔·尼尔森的<a class="ae lr" href="http://neuralnetworksanddeeplearning.com/chap4.html" rel="noopener ugc nofollow" target="_blank">教程</a>，它是如此之好，以至于几乎使这篇文章过时(我强烈建议你阅读它！)，但现在让我们回到过去，假装迈克尔那天带着他的家人去了迪斯尼乐园，而不是写有史以来世界上最伟大的神经网络教程。谢谢你，迈克尔；)</p><h2 id="b3e4" class="mi mj iq bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">手动执行梯度下降</h2><p id="75cc" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">我很早就意识到我不会赢得这场钻研数学<a class="ae lr" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=5&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiZgu7fsqfSAhUK2mMKHTqEARoQFggyMAQ&amp;url=https%3A%2F%2Fpdfs.semanticscholar.org%2F05ce%2Fb32839c26c8d2cb38d5529cf7720a68c3fab.pdf&amp;usg=AFQjCNFmNUFS5QB5nPkijKhEpVPJS6hCFg&amp;sig2=6NRz_NrV6p6rv_i63CQV-w" rel="noopener ugc nofollow" target="_blank">证明</a>的战斗，所以我决定采取一种实验性的方法。我去了<a class="ae lr" href="https://www.desmos.com/calculator/cfvtjusqmq" rel="noopener ugc nofollow" target="_blank">德斯莫斯</a>，开始将ReLU激活函数链接在一起，看看我是否能构建一些看起来有趣的东西。每次尝试后，我都调整我的函数，使它们看起来更像目标——听起来熟悉吗？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/667e3dcb59ad92e20a29d3a762e5ccb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6c0BjULsvVe5zqlkB_Vb3w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd nn">Left:</strong> target function <strong class="bd nn">Right:</strong> raw materials</figcaption></figure><p id="2a84" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我选择<em class="mh"> x +x -x -1 </em>作为我的目标函数。仅使用ReLU <code class="fe no np nq nr b">max(0,x)</code> <em class="mh">，</em>我反复尝试不同的ReLU组合，直到我有一个大致类似目标的输出。这是我用3个ReLUs的加权和得出的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/cd8792a7a4e3ecddba1187d56d438147.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qt4SaoYphChAreRTDJewIw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd nn">Left</strong>: 3 ReLU functions <strong class="bd nn"><em class="nt">Right</em></strong><em class="nt">: Weighted sum of the 3 ReLU functions</em></figcaption></figure><p id="2177" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">还不错？左图显示了ReLU函数。右图显示了我的模型与目标相比的输出。你可以把每个ReLU函数想象成一个神经元。因此，组合3个ReLU函数就像训练一个由3个隐藏神经元组成的网络。这是我用来生成这些图表的方程式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/f44d2eac4210e301f09cbec11d40ce83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*fdICiWJocvOTJPoRrhek_Q.png"/></div></figure><p id="61e9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每个神经元的输出等于ReLU环绕加权输入<strong class="kx ir"> <em class="mh"> wx + b </em> </strong> <em class="mh">。</em></p><p id="0c2a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我发现我可以通过改变偏差来左右移动ReLU函数，并通过改变权重来调整斜率。我将这3个函数组合成加权输入的最终和(<strong class="kx ir"> Z </strong>)，这是大多数神经网络的标准做法。</p><p id="419f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> Z </strong>中的负号代表最后一层的权重，我将其设置为-1，以便沿x轴“翻转”图形，以匹配我们的凹形目标。在又玩了一会儿之后，我终于得出了下面的7个等式，这7个等式加在一起大约是x +x -x -1。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/b2697a15517ba3f7fc23f9c68bf72885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lihbPNQgl7oKjpCsmzPDKw.png"/></div></div></figure><p id="248d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以至少在视觉上，用一个隐藏层和一些神经元来模拟非平凡函数是可能的。相当酷。</p><h2 id="8952" class="mi mj iq bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">将我的体重硬编码到一个真实的网络中</h2><p id="135f" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">这是一个用我的假权重和偏见初始化的神经网络图。如果你给这个网络一个类似于<em class="mh"> x +x -x-1 </em>的数据集，它<em class="mh">应该</em>能够近似得到-2和2之间的输入的正确输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/478140e480d74379c5c4f32f429e93c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RWcNXtQSrIVoiw99bkcA8w.png"/></div></div></figure><p id="f6cc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后一个语句，<em class="mh">是-2和2之间任何输入的近似正确输出，</em>是关键。通用逼近定理指出，对于在<strong class="kx ir"> <em class="mh">特定范围</em> </strong>内的输入，具有1个隐层的神经网络可以逼近任何<strong class="kx ir"> <em class="mh">连续</em> </strong>函数。如果函数跳跃或者有很大的间隙，我们就无法逼近它。此外，如果我们在10和20之间的输入上训练一个网络，我们不能肯定地说它是否将在40和50之间的输入上工作。</p><h2 id="9cf3" class="mi mj iq bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">我的重量真的有用吗？</h2><p id="6cd0" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">我想以编程的方式证明，当插入一个有一个隐藏层和6个神经元的基本神经网络时，我得出的权重实际上是有效的。然而，我没有训练网络来学习权重，而是用自己挑选的值替换了它的默认权重和偏差。下面的方法<code class="fe no np nq nr b">feed_forward()</code> <em class="mh"> </em>采用输入向量(例如[-2，-1，0，1..])并使用我的权重输出预测向量。事情是这样的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/de51af0a9d302c148dc8dd613dcc77e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_h0oKjm4j-Op1G8U7usgzA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Zh,Zo: weighted inputs . Wh,Wo: weights. Bh,Bo: biases. Ah,Ao: activations</figcaption></figure><p id="411c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">看那个！我是个天才。这正是我们想要的。但是如果我们的老板要求我们将范围从-2扩大到2呢？如果她想要-5比5呢？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/65232c7b6453d17104557540ed65e2a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*59sjlTXZQcBWiMqTRPPiaA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">x = np.arange(-5, 5, .1)</figcaption></figure><p id="525a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">啊，不太好。但这实际上支持了我们早先的结论:一个具有一个隐藏层的神经网络可以逼近任何连续函数，但仅适用于特定范围<strong class="kx ir"><em class="mh"/></strong>内的输入。如果像我们一样，在-2和2之间的输入上训练一个网络，那么它将对类似范围的输入工作良好，但你不能指望它在不重新训练模型或添加更多隐藏神经元的情况下推广到其他输入。</p><h2 id="5e06" class="mi mj iq bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">我的体重可以学吗？</h2><p id="23c6" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">现在我们知道Brendan Fortuner可以自己学习这些权重，但是具有1个隐藏层和6个神经元的真实世界神经网络是否也可以学习这些参数或导致相同结果的其他参数？让我们用<a class="ae lr" href="http://scikit-neuralnetwork.readthedocs.io/en/latest/module_mlp.html#regressor" rel="noopener ugc nofollow" target="_blank"> scikit-neuralnetwork </a>来测试这个理论。我们将设计一个网络，使其适用于回归问题，并将其配置为使用ReLU、随机梯度下降和均方误差——通常的组合。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="defb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">成功了！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/9d7fa1b4b653f4e4eb871208fef4c80a.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*MikfOYaKf9pe-fJXP7lNWg.png"/></div></figure><p id="61e6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下是Scikit学到的重量。它们显然与我自己的不同，但数量级是相同的。</p><pre class="kg kh ki kj gt oc nr od oe aw of bi"><span id="0f44" class="mi mj iq nr b gy og oh l oi oj">hiddenLayerWeights = [<br/> [-1.5272, -1.0567, -0.2828, 1.0399, 0.1243, 2.2446]<br/>]<br/>finalLayerWeights = [<br/>  [-2.2466],<br/>  [ 1.0707],<br/>  [-1.0643],<br/>  [ 1.8229],<br/>  [-0.4581],<br/>  [ 2.9386]<br/>]</span></pre><p id="e8f5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">也许如果我把这篇博文重写10万遍，我会自己得出这些参数，但是现在我们只能推测。也许有一天，我会对我的赞成票求导，并朝着观点最大化的方向更新我的句子。</p></div><div class="ab cl ok ol hu om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="ij ik il im in"><p id="c41a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你喜欢阅读，请点击下面的<em class="mh">【♥】</em><strong class="kx ir"><em class="mh">按钮！</em>T9】</strong></p></div></div>    
</body>
</html>