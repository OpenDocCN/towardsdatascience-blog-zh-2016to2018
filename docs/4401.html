<html>
<head>
<title>Comparing Different Methods of Achieving Sparse Coding in Tensorflow [ Manual Back Prop in TF ]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Tensorflow 中实现稀疏编码的不同方法比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comparing-different-methods-of-achieving-sparse-coding-in-tensorflow-manual-back-prop-in-tf-d47053da4d8e?source=collection_archive---------16-----------------------#2018-08-13">https://towardsdatascience.com/comparing-different-methods-of-achieving-sparse-coding-in-tensorflow-manual-back-prop-in-tf-d47053da4d8e?source=collection_archive---------16-----------------------#2018-08-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/f98a91f3bb0ddbbd890297944971ccab.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/1*2YMaVgOpbsUy-XNf14di0w.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/deep-learning-jLrmkvDAn21fW" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="8dd5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在韩国度假期间，我一直在研究更多关于稀疏编码和实现它的不同方法，今天我想比较其中的一些。下面是我将要比较的所有案例。(以防我将案例 d 和案例 e 的两篇论文联系起来。)</p><p id="ef69" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="kx">案例 a:纯自动编码器<br/>案例 b:具有 L2 正则化的自动编码器<br/>案例 c:稀疏自动编码器来自</em> <a class="ae jy" href="https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="kx">安德鲁·吴的课程</em> </a> <em class="kx"> <br/>案例 d: </em> <a class="ae jy" href="https://arxiv.org/pdf/1503.00778.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="kx">简单、高效的稀疏编码神经算法</em> </a> <em class="kx"> <br/>案例 e:</em><a class="ae jy" href="https://arxiv.org/pdf/1312.5663.pdf" rel="noopener ugc nofollow" target="_blank"><em class="kx">k-稀疏自动编码器</em> </a></p><blockquote class="ky kz la"><p id="fc94" class="jz ka kx kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated"><strong class="kb ir">请注意，这个帖子是为了我未来的自己，也是为了存档已经取得的成果。</strong></p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="https://arxiv.org/pdf/1503.00778.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="https://arxiv.org/pdf/1312.5663.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="ea9b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">简介、数据集和总体思路</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/866bed578684147b922065341eedd28b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*kz2a5i2gwVqpbs7TpopwoQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from this <a class="ae jy" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="21e1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我已经假设读者对神经网络和自动编码器有一些了解，所以我不会深入细节。此外，我只打算介绍使用自动编码器进行稀疏编码的方法。请记住，hyper 参数的设置是相同的，但这并不能保证它对每种情况都是最佳的！最后，我将使用的所有数据都来自<a class="ae jy" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST 数据集。</a></p><div class="ll lm ln lo gt ab cb"><figure class="ls jr lt lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/1f862bd88682bd47e8e436d4072cbd09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*isWqO3FLVgspHR8U3bad5A.png"/></div></figure><figure class="ls jr mc lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/e06a98f70575f42ea6d17fee8f3db862.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*o-yyx3J94RaDcRwNJCokyg.png"/></div></figure></div><p id="3c13" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">橙色框</strong> →矢量形式的原始数据<br/> <strong class="kb ir">红色框</strong> →压缩数据<br/> <strong class="kb ir">蓝色框</strong> →重构数据</p><p id="3fe9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">自动编码器的一般思想非常简单，在压缩后重建原始数据。在今天的帖子中，我们的目标是学习到的权重(橙色框和红色框之间的权重)，因为这些权重代表了我们正在捕捉的数据类型(或特征)。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="6741" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">案例 a 的结果:纯自动编码器</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi md"><img src="../Images/cde0ea94315eb20a9cfa1417de39f41c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*YTqqVPYsdwNfw7EyHi-gkw.gif"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Progression of learned weights for 500 epoch</figcaption></figure><p id="1625" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从上面的过程中，我们可以注意到一些事情，学习的权重(或特征)并不特定于某些数字。这是因为由于网络可以使用学习字典中的所有原子来重构原始数据，因此不需要学习给定数据的专门过滤器。</p><div class="ll lm ln lo gt ab cb"><figure class="ls jr me lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/92236e679f47b993b42461f2023e20e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*aD3ivgmLhMLZ1wSYBrq2CA.png"/></div></figure><figure class="ls jr me lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/35941110f9c4ff5fc68858007bba1372.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*0Sa3-52pJBNz7Owa9dpRkA.png"/></div></figure></div><p id="b43f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →原始数据<br/> <strong class="kb ir">右图</strong> →重建数据</p><p id="ba43" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">重建的数据有点模糊，但是，一般来说，它包含了正确的数字形状。</p><div class="ll lm ln lo gt ab cb"><figure class="ls jr mf lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/d0756e85b75e8c879e329c58e38dce64.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*l4AW0zJrevAH1CbNYC_UFg.png"/></div></figure><figure class="ls jr mf lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/67cd67f0dd0d1ab986e2e0a7ecf55ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*bf1p9JaolIP2LSSSjYHCqQ.png"/></div></figure><figure class="ls jr mf lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/da88c2bb1b39dcbd1f556bc35e0613c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*NIazXEItrVTt6AAF3TJIvg.png"/></div></figure></div><p id="8e3c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →对比度归一化的学习权重<br/> <strong class="kb ir">中图</strong> →对比度归一化的学习权重<br/> <strong class="kb ir">右图</strong> →训练期间的时间成本</p><p id="2c9e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当我们再次查看学习到的权重时，我们可以再次观察到过滤器并不特定于某些数字。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="da81" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">情况 b 的结果:采用 L2 正则化的自动编码器</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi md"><img src="../Images/33fb0a8518827ab5bbb7e6f7c93e2eaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Mk3G-cixzsjf4p0m_6KOTA.gif"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Progression of learned weights for 500 epoch</figcaption></figure><p id="2115" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当我们将 L2 正则化添加到原始自动编码器时，我们看不到学习到的权重之间的极端差异。这是自然的，因为在重构原始数据时，网络仍然可以完全访问字典中的所有原子。</p><div class="ll lm ln lo gt ab cb"><figure class="ls jr me lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/92236e679f47b993b42461f2023e20e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*aD3ivgmLhMLZ1wSYBrq2CA.png"/></div></figure><figure class="ls jr me lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/35941110f9c4ff5fc68858007bba1372.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*0Sa3-52pJBNz7Owa9dpRkA.png"/></div></figure></div><p id="5792" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →原始数据<br/> <strong class="kb ir">右图</strong> →重建数据</p><p id="4581" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同样，类似于纯自动编码器，重建的数据是模糊的，但它仍然保留了一般的形状。</p><div class="ll lm ln lo gt ab cb"><figure class="ls jr mg lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/ec12ef82debf43dbf47510efcfc35634.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*tFMzLTw9SYI2XMa2SNIJQw.png"/></div></figure><figure class="ls jr mg lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/5e42fcc77d71575bbacf713d7ef6e484.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*6nQcRH-Oopm14ki8GaV0lA.png"/></div></figure><figure class="ls jr mh lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/22676ef375eeeb13dacb63f0b3755e29.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*Xc1zaeb9PJopOvpVNgLONw.png"/></div></figure></div><p id="98b4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →对比度归一化的学习权重<br/> <strong class="kb ir">中图</strong> →对比度归一化的学习权重<br/> <strong class="kb ir">右图</strong> →训练期间的时间成本</p><p id="bfa0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们仍然看不到特定于某些数字的过滤器。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="d75c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">案例 c 的结果:来自</strong> <a class="ae jy" href="https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kb ir"> Andrew NG 的课程</strong> </a>的稀疏自动编码器</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi md"><img src="../Images/6ce310502426fa3387b1a6deaa6a20c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*HN4OCwzr4jJvaYuU2Hz1KA.gif"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Progression of learned weights for 500 epoch</figcaption></figure><p id="5941" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当我们在隐藏层中引入稀疏性的概念时，并不是字典中的所有原子都可以使用。我们可以清楚地观察到，学习到的过滤器正变得更加特定于某些数字，并且在最终时期之后，过滤器代表数字的手动敲击。</p><div class="ll lm ln lo gt ab cb"><figure class="ls jr me lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/92236e679f47b993b42461f2023e20e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*aD3ivgmLhMLZ1wSYBrq2CA.png"/></div></figure><figure class="ls jr me lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/e74b537f5f4603181939787cf12a0981.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*6i8svnNTWdZZjBq6kQ-6OA.png"/></div></figure></div><p id="b121" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →原始数据<br/> <strong class="kb ir">右图</strong> →重建数据</p><p id="b78f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我个人认为，重建的数据比其他两个(上图)更模糊，但它们是清晰可辨的。</p><div class="ll lm ln lo gt ab cb"><figure class="ls jr mg lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/d69db189a6b5c141fb7f706250b51407.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*5mded92fKa7CVBLbhKGHdA.png"/></div></figure><figure class="ls jr mg lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/ecfe0f940356289423562b6eb9edc1b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*kuwAZglMhWa8JXcAESk9Rw.png"/></div></figure><figure class="ls jr mh lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/dda5119923296547a30c206b177c87d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*LfN2MdlyFIkjo655T-uvlg.png"/></div></figure></div><p id="c229" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →对比度归一化的学习权重<br/> <strong class="kb ir">中图</strong> →对比度归一化的学习权重<br/> <strong class="kb ir">右图</strong> →训练期间的时间成本</p><p id="b234" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当我们将学习到的权重可视化时，我们可以看到网络正试图从给定的数据中提取不同的笔画。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="4f47" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">案例 d 的结果:</strong> <a class="ae jy" href="https://arxiv.org/pdf/1503.00778.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kb ir">简单、高效、用于稀疏编码的神经算法</strong> </a></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi md"><img src="../Images/fd61af280446044e2fd259eeb082a087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*g9g6R7ElIl1IOJpEfImIuQ.gif"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Progression of learned weights for 500 epoch</figcaption></figure><p id="49a8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果没有要优化的重建损失函数，我们可以观察到收敛需要更长的时间。然而，我们可以观察到，如果字典中的某个原子被使用，它会产生一个类似的过滤，就像我们有稀疏的概念时一样。(案例 c)。</p><div class="ll lm ln lo gt ab cb"><figure class="ls jr me lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/92236e679f47b993b42461f2023e20e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*aD3ivgmLhMLZ1wSYBrq2CA.png"/></div></figure><figure class="ls jr me lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/7a9e0a652704a5583fadeedf96d86424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*oVHoghYzI85VOLVdmDnFGg.png"/></div></figure></div><p id="3b32" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →原始数据<br/> <strong class="kb ir">右图</strong> →重建数据</p><p id="8d21" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这种方法的一个缺点是没有重建损失，虽然它产生更干净的滤波器，但它不能像其他方法一样重建原始数据。</p><div class="ll lm ln lo gt ab cb"><figure class="ls jr mg lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/b319afa5ef5d35fadd139eba02ae1fd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*Ju723HoD3gIbF0_niA-gOQ.png"/></div></figure><figure class="ls jr mg lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/de664a8def0f5b90c39e60bbc676e9eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*mgrvgHpmeO9mluiQnOx6NQ.png"/></div></figure><figure class="ls jr mh lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/f285884c00fd1d67f293e4b43af41701.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*2kG-PgzzYtJ5WmhWEfV88w.png"/></div></figure></div><p id="6530" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →对比度归一化的学习权重<br/> <strong class="kb ir">中图</strong> →对比度归一化的学习权重<br/> <strong class="kb ir">右图</strong> →训练期间的时间成本</p><p id="9ff1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当我们可视化学习到的权重时，我们可以看到这种方法的优点，即它产生更干净的滤波器。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="6864" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">情况 e 的结果:</strong><a class="ae jy" href="https://arxiv.org/pdf/1312.5663.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kb ir">k-稀疏自动编码器</strong> </a></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi md"><img src="../Images/208547f5dfa1115c633a6c806a27f287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*8k-P5h3OhKTELKVr8WrSMQ.gif"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Progression of learned weights for 500 epoch</figcaption></figure><p id="c9f9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于给定的数据，前 K 个稀疏自动编码器收敛到最佳点要快得多。并且我们可以观察到，在捕捉手写笔画类型特征的同时，所学习的权重是干净的。</p><div class="ll lm ln lo gt ab cb"><figure class="ls jr me lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/92236e679f47b993b42461f2023e20e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*aD3ivgmLhMLZ1wSYBrq2CA.png"/></div></figure><figure class="ls jr me lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/86d8d247f56a5cb91777f9d563a4d943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*yiAT9TcK7YzShi22ePbgjg.png"/></div></figure></div><p id="7bd6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →原始数据<br/>T22】右图 →重建数据</p><p id="0ec1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">由于重建误差的存在，当与情况 d 相比时，重建的数据更清晰，然而，我们可以清楚地观察到它在这里和那里缺乏对比度的事实。</p><div class="ll lm ln lo gt ab cb"><figure class="ls jr mg lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/365fe185cdfa387d17a2b382cb2e1969.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*HAuIeZAQjNCfMrYUDShycg.png"/></div></figure><figure class="ls jr mg lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/f34192a5f72c0519f68a158c20bf7f7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*TlTTCJWJTmSLrUrUUjGNbA.png"/></div></figure><figure class="ls jr mh lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/4e910dc8d62fc53bba89a4f9f7980cb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*i3xRxIFXUMGLZ5gWHwrURg.png"/></div></figure></div><p id="9955" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →对比度归一化的学习权重<br/> <strong class="kb ir">中图</strong> →对比度归一化的学习权重<br/> <strong class="kb ir">右图</strong> →训练期间的时间成本</p><p id="78ba" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同样，学习过的过滤器在捕捉手笔画类型特征时更干净。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="b1d2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">交互代码</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mi"><img src="../Images/0ad3c3bef1bbea3bd416b3d6457112fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QOnfmaIY-BbIlE3TOtoIgg.png"/></div></div></figure><p id="0332" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于谷歌 Colab，你需要一个谷歌帐户来查看代码，而且你不能在谷歌 Colab 中运行只读脚本，所以在你的操场上做一个副本。最后，我永远不会请求允许访问你在 Google Drive 上的文件，仅供参考。编码快乐！</p><p id="6e47" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/15vST8Gma0uyQa1DOwO5weUWQ-I55FQoG" rel="noopener ugc nofollow" target="_blank"> a 的代码，请点击此处。</a> <br/>要获取案例<a class="ae jy" href="https://colab.research.google.com/drive/1m2IfnKFdXN5KMOvDMfFSj4vc94kcch73" rel="noopener ugc nofollow" target="_blank"> b 的代码，请点击此处。</a> <br/>要访问案例<a class="ae jy" href="https://colab.research.google.com/drive/1-OHYBSYxsQ0dYRvq5wzzBhzlQRs2p169" rel="noopener ugc nofollow" target="_blank"> c 的代码，请点击此处。</a> <br/>要获取案例<a class="ae jy" href="https://colab.research.google.com/drive/1HtXdq6hcX2rbywjl8I1BnLmelXy7P2sZ" rel="noopener ugc nofollow" target="_blank"> d 的代码，请点击此处。</a> <br/>要获取案例<a class="ae jy" href="https://colab.research.google.com/drive/1J6cTgJnUXi64ndOx7X_1eoiStGzpt4fU" rel="noopener ugc nofollow" target="_blank"> e 的代码，请点击此处。</a></p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="4dac" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="5e93" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这让我如此满意的原因是因为我没有使用任何自动微分。</p><p id="3dcc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有写作的列表，请在这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的推特上关注我<a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文 pos </a> t。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="bf70" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="d26e" class="mj mk iq kb b kc kd kg kh kk ml ko mm ks mn kw mo mp mq mr bi translated">FNNDSC/med2image。(2018).GitHub。检索于 2018 年 8 月 4 日，来自<a class="ae jy" href="https://github.com/FNNDSC/med2image" rel="noopener ugc nofollow" target="_blank">https://github.com/FNNDSC/med2image</a></li><li id="3465" class="mj mk iq kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated">张量，T. (2018)。张量中的前 n 个值。堆栈溢出。2018 年 8 月 4 日检索，来自<a class="ae jy" href="https://stackoverflow.com/questions/40808772/tensorflow-top-n-values-in-tensor" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/40808772/tensor flow-top-n-values-in-tensor</a></li><li id="33d6" class="mj mk iq kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated">t . way(2018)。使用 top_k 和 any way 的 tesorflow 排序。堆栈溢出。检索于 2018 年 8 月 4 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/46045867/tesorflow-sort-using-top-k-and-any-way" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/46045867/tesorflow-sort-using-top-k-and-any-way</a></li><li id="4464" class="mj mk iq kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated">tf.nn.top_k |张量流。(2018).张量流。检索于 2018 年 8 月 4 日，来自<a class="ae jy" href="https://www.tensorflow.org/api_docs/python/tf/nn/top_k" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/nn/top_k</a></li><li id="b7df" class="mj mk iq kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated">tf.greater | TensorFlow。(2018).张量流。检索于 2018 年 8 月 4 日，来自 https://www.tensorflow.org/api_docs/python/tf/greater<a class="ae jy" href="https://www.tensorflow.org/api_docs/python/tf/greater" rel="noopener ugc nofollow" target="_blank"/></li><li id="51ac" class="mj mk iq kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated">x？，H. (2018)。如何在 OS X 上将 Python 的默认版本设置为 3.3？。堆栈溢出。2018 年 8 月 5 日检索，来自<a class="ae jy" href="https://stackoverflow.com/questions/18425379/how-to-set-pythons-default-version-to-3-3-on-os-x" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/18425379/how-to-set-python-default-version-to-3-3-on-OS-x</a></li><li id="d041" class="mj mk iq kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated">张量，T. (2018)。张量中的前 n 个值。堆栈溢出。检索于 2018 年 8 月 12 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/40808772/tensorflow-top-n-values-in-tensor" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/40808772/tensor flow-top-n-values-in-tensor</a></li><li id="7001" class="mj mk iq kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated">(2018).Web.stanford.edu。检索于 2018 年 8 月 13 日，来自<a class="ae jy" href="https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf" rel="noopener ugc nofollow" target="_blank">https://web . Stanford . edu/class/cs 294 a/sparseautoencer _ 2011 new . pdf</a></li><li id="b905" class="mj mk iq kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated">(2018).Arxiv.org。检索于 2018 年 8 月 13 日，来自<a class="ae jy" href="https://arxiv.org/pdf/1503.00778.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1503.00778.pdf</a></li><li id="9ec6" class="mj mk iq kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated">(2018).Arxiv.org。检索于 2018 年 8 月 13 日，来自<a class="ae jy" href="https://arxiv.org/pdf/1312.5663.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1312.5663.pdf</a></li><li id="f0e1" class="mj mk iq kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated">MNIST 手写数字数据库，Yann LeCun，Corinna Cortes 和 Chris Burges。(2018).Yann.lecun.com。检索于 2018 年 8 月 13 日，来自<a class="ae jy" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">http://yann.lecun.com/exdb/mnist/</a></li></ol></div></div>    
</body>
</html>