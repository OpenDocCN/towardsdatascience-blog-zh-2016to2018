<html>
<head>
<title>Character level CNN with Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带 Keras 的字符级 CNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/character-level-cnn-with-keras-50391c3adf33?source=collection_archive---------4-----------------------#2018-07-09">https://towardsdatascience.com/character-level-cnn-with-keras-50391c3adf33?source=collection_archive---------4-----------------------#2018-07-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/cf20b469f913eff01bf516ddf8b695bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RqxoI85-ePs65wzN6S2jpg.jpeg"/></div></div></figure><p id="94af" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这本笔记本中，我们将使用 Keras 构建一个角色级别的 CNN 模型。可以在本文中找到模型细节:<a class="ae kz" href="http://arxiv.org/abs/1509.01626" rel="noopener ugc nofollow" target="_blank">用于文本分类的字符级卷积网络</a>。</p><p id="5673" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">文章的其余部分组织如下。</p><ul class=""><li id="cd13" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">模型介绍</li><li id="9f4b" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">为什么是这种模式？</li><li id="7c1e" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">预处理</li><li id="edae" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">负载嵌入重量</li><li id="e7bf" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">模型构建</li><li id="c823" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">培养</li></ul><h2 id="7e47" class="lo lp it bd lq lr ls dn lt lu lv dp lw km lx ly lz kq ma mb mc ku md me mf mg bi translated">车型<strong class="ak">简介</strong></h2><p id="9e78" class="pw-post-body-paragraph kb kc it kd b ke mh kg kh ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky im bi translated">模型结构:</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="ab gu cl mq"><img src="../Images/ad8ef78ca3e5d1641085268d1b22e116.png" data-original-src="https://miro.medium.com/v2/format:webp/0*fovAEUSdSkbsnJw5.png"/></div></figure><p id="0dcf" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这个图表可能看起来很难理解。这是模型设置。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/f0406be8f8b0884a142f2b55afa5c896.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*Os4KG8BRf7f8f6JnhgZlAw.jpeg"/></div></figure><p id="7f2f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你想看这个模型的细节，请移动到这个<a class="ae kz" href="https://github.com/BrambleXu/nlp-beginner-guide-keras/blob/master/char-level-cnn/notebooks/char_cnn_zhang.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a></p><p id="87d0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们选择小帧，卷积层 256 个滤波器，密集层 1024 个输出单元。</p><ul class=""><li id="d304" class="la lb it kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">嵌入层</li><li id="978d" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">6 个卷积层，3 个卷积层，然后是最大池层</li><li id="3792" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">两个完全连接的层(keras 中的致密层)，神经元单位是 1024 个。</li><li id="b2c7" class="la lb it kd b ke lj ki lk km ll kq lm ku ln ky lf lg lh li bi translated">输出层(密集层)，神经元单位取决于类。在这个任务中，我们将它设置为 4。</li></ul><h2 id="0244" class="lo lp it bd lq lr ls dn lt lu lv dp lw km lx ly lz kq ma mb mc ku md me mf mg bi translated">为什么是这种模式？</h2><p id="21d2" class="pw-post-body-paragraph kb kc it kd b ke mh kg kh ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky im bi translated">在 Kim 提出用于句子分类的<a class="ae kz" href="https://arxiv.org/abs/1408.5882" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a>之后，我们知道 CNN 可以在 NLP 任务中有很好的表现。我也实现了这个模型，如果你有一些兴趣，你可以在这里找到细节:<a class="ae kz" href="https://github.com/BrambleXu/nlp-beginner-guide-keras/tree/master/cnn-text-classification" rel="noopener ugc nofollow" target="_blank"> cnn-text-classification </a>。但是在这个模型中，它是从词的层面来取句子特征的，这就造成了<a class="ae kz" href="http://www.festvox.org/bsv/x1407.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu">无词汇</strong>(<strong class="kd iu"/>)</a>的问题。</p><p id="f2c4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了处理 OOV 问题，人们提出了许多方法。这个人物级别的 CNN 模型就是其中之一。顾名思义，这个模型在字符层次上处理句子。通过这种方式，可以在很大程度上减少未登录词，使 CNN 能够提取模式特征，提高文本分类性能。</p><h1 id="750b" class="ms lp it bd lq mt mu mv lt mw mx my lw mz na nb lz nc nd ne mc nf ng nh mf ni bi translated">预处理</h1><p id="7db6" class="pw-post-body-paragraph kb kc it kd b ke mh kg kh ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky im bi translated">这里为了简单起见，我将所有预处理代码写在一起。如果你对预处理步骤中发生的事情感兴趣，请移至这里:<a class="ae kz" href="https://medium.com/@zhuixiyou/how-to-preprocess-character-level-text-with-keras-349065121089" rel="noopener">如何用 Keras 预处理字符级文本</a></p><figure class="mm mn mo mp gt ju"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h1 id="a39a" class="ms lp it bd lq mt mu mv lt mw mx my lw mz na nb lz nc nd ne mc nf ng nh mf ni bi translated">负载嵌入重量</h1><p id="bb80" class="pw-post-body-paragraph kb kc it kd b ke mh kg kh ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky im bi translated">为了理解如何给嵌入层分配嵌入权重，这里我们手动初始化嵌入权重，而不是随机初始化。</p><p id="ec05" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首先，我们要确认我们的词汇量有多少。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nl"><img src="../Images/51f3f5a512a80cade5c90860bedea71a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bSlE7G63sk4irKwz"/></div></div></figure><p id="af1b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们可以看到，除了 68 个字符之外，我们还有一个<code class="fe nm nn no np b">UNK</code>(未知令牌)来表示词汇中的稀有字符。</p><p id="f98a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后我们用一热向量来表示这 69 个单词，也就是说每个字符有 69 个维度。因为 Keras 使用 0 表示填充，所以我们添加一个零向量来表示填充。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nq"><img src="../Images/ff74975bfe5c8457370670409178849a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_wWW7sKzO-T2-BND"/></div></div></figure><p id="2de6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，这个句子由索引来表示。例如，<code class="fe nm nn no np b">I love NLP</code>被表示为<code class="fe nm nn no np b">[9, 12, 15, 22, 5, 14, 12, 16]</code>。第一个索引<code class="fe nm nn no np b">9</code>对应的是<code class="fe nm nn no np b">embedding_weights[9]</code>，它是字符<code class="fe nm nn no np b">I</code>的向量。</p><p id="2307" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在我们得到这个嵌入权重之后，我们应该通过它来初始化嵌入层。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1121323b0739f4b8c0edd7be734a65a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/0*0wwSgqk4Yh1nfcv1"/></div></figure><h1 id="b595" class="ms lp it bd lq mt mu mv lt mw mx my lw mz na nb lz nc nd ne mc nf ng nh mf ni bi translated"><strong class="ak">模型构建</strong></h1><p id="1685" class="pw-post-body-paragraph kb kc it kd b ke mh kg kh ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky im bi translated">首先，我们给出了参数设置。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/68ee634f2d767f830bc4313065212811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/0*4mXUhe6ke_zmZXjw"/></div></figure><p id="c98f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，我们按照设置所说的那样构建模型。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/7e55870ead7794042ffe329a79787361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dJe9QnvbsJTH2LCY"/></div></div></figure><p id="5c4c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><code class="fe nm nn no np b">model.summary()</code>的输出</p><figure class="mm mn mo mp gt ju"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h1 id="6b2d" class="ms lp it bd lq mt mu mv lt mw mx my lw mz na nb lz nc nd ne mc nf ng nh mf ni bi translated">培养</h1><p id="2774" class="pw-post-body-paragraph kb kc it kd b ke mh kg kh ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky im bi translated">我们的目标是学习如何构造模型，所以这里我只是用 CPU 来运行模型，只用 1000 个样本进行训练，100 个样本进行测试。由于数据集较小，该模型容易过拟合。</p><figure class="mm mn mo mp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nu"><img src="../Images/d68bfa722631db2b5e2bcc3d3a4ca3c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iKzL8niMahZOGGLHlClFmw.png"/></div></div></figure><p id="235a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">将所有代码汇总在一起。</p><figure class="mm mn mo mp gt ju"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="917e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="nv">这篇文章的笔记本这里是</em><a class="ae kz" href="http://nbviewer.jupyter.org/github/BrambleXu/nlp-beginner-guide-keras/blob/master/char-level-cnn/notebooks/char-cnn-zhang-with-keras-pipeline.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="nv"/></a><em class="nv">，整个剧本这里是</em><a class="ae kz" href="https://github.com/BrambleXu/nlp-beginner-guide-keras/blob/master/char-level-cnn/char_cnn.py" rel="noopener ugc nofollow" target="_blank"><em class="nv"/></a><em class="nv">。预处理条是</em> <a class="ae kz" href="https://medium.com/@zhuixiyou/how-to-preprocess-character-level-text-with-keras-349065121089" rel="noopener"> <em class="nv">这里是</em> </a></p><p id="3ea9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我创建了一个存储库来存放我作为初学者学习 NLP 时的工作。如果你觉得有用，请启动这个项目。我很高兴听到反馈或建议。<br/><a class="ae kz" href="https://github.com/BrambleXu/nlp-beginner-guide-keras" rel="noopener ugc nofollow" target="_blank"><strong class="kd iu">NLP-初学者-指南-keras </strong> </a></p><blockquote class="nw nx ny"><p id="dfd5" class="kb kc nv kd b ke kf kg kh ki kj kk kl nz kn ko kp oa kr ks kt ob kv kw kx ky im bi translated"><strong class="kd iu"> <em class="it">查看我的其他帖子</em> </strong> <a class="ae kz" href="https://medium.com/@bramblexu" rel="noopener"> <strong class="kd iu"> <em class="it">中</em> </strong> </a> <strong class="kd iu"> <em class="it">同</em> </strong> <a class="ae kz" href="https://bramblexu.com/posts/eb7bd472/" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu"> <em class="it">一个分类查看</em> </strong> </a> <strong class="kd iu"> <em class="it">！<br/>GitHub:</em></strong><a class="ae kz" href="https://github.com/BrambleXu" rel="noopener ugc nofollow" target="_blank"><strong class="kd iu"><em class="it">bramble Xu</em></strong></a><strong class="kd iu"><em class="it"><br/>LinkedIn:</em></strong><a class="ae kz" href="https://www.linkedin.com/in/xu-liang-99356891/" rel="noopener ugc nofollow" target="_blank"><strong class="kd iu"><em class="it">徐亮</em> </strong> </a> <strong class="kd iu"> <em class="it"> <br/>博客:</em></strong><a class="ae kz" href="https://bramblexu.com" rel="noopener ugc nofollow" target="_blank"><strong class="kd iu"><em class="it">bramble Xu</em></strong></a></p></blockquote></div></div>    
</body>
</html>