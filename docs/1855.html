<html>
<head>
<title>Constructing your own Recurrent Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建你自己的递归神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recurrent-neural-networks-6b67535550ca?source=collection_archive---------8-----------------------#2017-11-03">https://towardsdatascience.com/recurrent-neural-networks-6b67535550ca?source=collection_archive---------8-----------------------#2017-11-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/b284344de508e5b8c7423291c673d619.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jJeFu9KjT9TYlYxABtroVA.png"/></div></div></figure><div class=""/><p id="e787" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">神经网络是一类模仿人脑的机器学习算法，而<a class="ae kw" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank"> <strong class="ka jc">递归神经网络</strong> </a>是一个子类，可以很好地处理数据序列，如文本。</p><p id="e725" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">用一个普通的神经网络，你得到一组输入数据，让它通过网络，得到一组输出</p><p id="6c76" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了训练深度学习模型，你需要知道模型应该理想地输出什么，这通常被称为你的标签或目标变量。神经网络将其输出的数据与目标进行比较，并更新网络学习以更好地模拟目标。</p><p id="ca52" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">递归神经网络(RNNs)在建模序列数据中是有用的，序列数据涉及时间模式，如文本、图像字幕、ICU患者数据等。这是一个简单的带反馈的前馈神经网络。在每个时间步，基于当前的输入和过去的输出，它生成新的输出。一个简单的递归神经网络架构看起来像:</p><figure class="ky kz la lb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi kx"><img src="../Images/81d90a951a99be6a5ea66f12548653e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*buMl05BvEPJ5P7sB5ImTCg.jpeg"/></div></div></figure><p id="97a1" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">rnn比简单的前馈神经网络灵活得多。我们可以向rnn传递可变大小的输入，甚至得到可变大小的输出。例如，可以模拟RNN来一点一点地学习二进制加法。它学习二进制加法器的状态机图。经过训练后，我们只需向它传递两个任意大小的输入，它就可以生成正确的输出，而无需执行任何加法运算！RNN能够自己隐含地学习这些语义。</p><p id="44c8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了让RNNs工作，需要大量的数据来找到一个好的模型，我尝试了各种RNNs架构，这些架构具有不同的层数、每层中隐藏单元的数量、序列长度和批量大小。所有这些超参数都应该根据数据集进行智能调整，否则可能会出现过度拟合或拟合不足的情况。生成的结果将严重依赖于数据，并且由于数据是从各种网站爬取的，因此有时可能会令人反感且不合适。</p><h1 id="27bb" class="lc ld jb bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">递归神经网络的步骤</h1><h1 id="b532" class="lc ld jb bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><strong class="ak">初始化</strong></h1><p id="cf20" class="pw-post-body-paragraph jy jz jb ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">这是初始化权重的第一步，你可以用三种不同的方式初始化。</p><p id="ba82" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">1)全零。</p><p id="5296" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2)随机值(取决于激活函数)</p><p id="4b01" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">3) Xavier初始化:<strong class="ka jc"> <br/> </strong>区间内的随机值从[-1/√n，1/√n]，<br/>其中n为入局连接数</p><p id="19a1" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">构建一个</p><h1 id="162e" class="lc ld jb bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><strong class="ak">正向传播</strong></h1><p id="ac8b" class="pw-post-body-paragraph jy jz jb ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">前向传播的作用是穿过终点。</p><p id="3b58" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">s_t=f(Ux_t+Ws_(t-1))</p><p id="20d4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">新状态</p><p id="421f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">s_(t-1):旧状态</p><p id="dfc0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">x_t:某一时间步的输入向量</p><p id="8b63" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">函数f通常是非线性的，例如<strong class="ka jc"> tanh </strong>或<strong class="ka jc"> ReLU </strong></p><h1 id="2b0f" class="lc ld jb bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">计算损失</h1><p id="14f8" class="pw-post-body-paragraph jy jz jb ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">寻找损失是训练的重要阶段损失函数决定了你的模型的状态让我们试试</p><p id="06ab" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">交叉熵损失:</p><p id="8546" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">L(y，o)=-1/n∑_(n∈n)ࣼy _ nlog⁡(o_n)〗</p><h1 id="b1f0" class="lc ld jb bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><strong class="ak">随机梯度下降(SGD) </strong></h1><p id="b628" class="pw-post-body-paragraph jy jz jb ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">这是优化你的模型的最流行的方法，SGD是你分批发送输入而不是全部输入的方法，通过这种方法你可以很容易地缩放你的模型。</p><h1 id="c175" class="lc ld jb bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><strong class="ak">穿越时间的反向传播(BPTT) </strong></h1><p id="2c09" class="pw-post-body-paragraph jy jz jb ka b kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv ij bi translated">这是一种技术，在这种技术中，您将再次遍历到起点，并根据优化器的结果更新您的权重。</p><p id="dee7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">用于脚本生成的递归神经网络的实现</p><div class="ip iq gp gr ir mf"><a href="https://github.com/karthiktsaliki/script_generation_rnn/blob/master/dlnd_tv_script_generation.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd jc gy z fp mk fr fs ml fu fw ja bi translated">karthiktsaliki/script _ generation _ rnn</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">训练了一个生成电视节目脚本的rnn。</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">github.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ix mf"/></div></div></a></div></div></div>    
</body>
</html>