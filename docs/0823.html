<html>
<head>
<title>How to deploy Machine Learning models with TensorFlow. Part 2— containerize it!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用 TensorFlow 部署机器学习模型？第 2 部分—容器化！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-deploy-machine-learning-models-with-tensorflow-part-2-containerize-it-db0ad7ca35a7?source=collection_archive---------0-----------------------#2017-06-26">https://towardsdatascience.com/how-to-deploy-machine-learning-models-with-tensorflow-part-2-containerize-it-db0ad7ca35a7?source=collection_archive---------0-----------------------#2017-06-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/0d15887361c3c5025e09fe0cd330243b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*hCBt5o0qcVwga4pxPzIoUw.png"/></div></figure></div><div class="ab cl ju jv hu jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="ij ik il im in"><p id="57b5" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">如<a class="ae kz" href="https://medium.com/@vitaly.bezgachev/how-to-deploy-machine-learning-models-with-tensorflow-part-1-make-your-model-ready-for-serving-776a14ec3198" rel="noopener">第 1 部分</a>所述，我想将我的深度学习模型部署到生产中。我已经展示了如何为<a class="ae kz" href="https://tensorflow.github.io/serving/" rel="noopener ugc nofollow" target="_blank"> TensorFlow 服务</a>准备模型。我们将 GAN 模型作为<a class="ae kz" href="https://github.com/google/protobuf" rel="noopener ugc nofollow" target="_blank"> Protobuf </a>导出，现在它已经可以托管了。</p><h1 id="727b" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">可部署人工制品的步骤</h1><p id="4d5e" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">TensorFlow 服务实现了一个处理传入请求并将其转发给模型的服务器。这个服务器可能运行在某个地方，很可能是在你的云提供商(比如 Amazon AWS，Google Cloud Platform，Microsoft Azure)那里，对全世界开放。</p><p id="77a8" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">如今，将这样的服务器及其所有依赖项打包到一个包中，并作为一个整体进行配置和部署是很常见的。</p><p id="375d" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">为此，您可以创建一个虚拟机映像，但是它非常笨重，我不建议将其用于服务部署。(尽管预配置的虚拟机对于为开发人员提供工作环境非常有用)。</p><p id="5509" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">通常我们使用容器来创建可部署的工件。然后我们可以在任何地方部署它们——本地、内部网、云中。这样的容器将包括您的服务及其所有依赖项。你只需要在操作系统上安装一个运行你的容器的薄层。</p><p id="c866" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">最流行的容器平台是<a class="ae kz" href="https://www.docker.com/" rel="noopener ugc nofollow" target="_blank"> Docker </a>。我们需要创建一个 Docker 映像，在该映像上创建一个容器并运行它。首先—在我们的本地操作系统中。</p><p id="ccba" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">与我们的情况相对应——我们应该测试容器是否运行，TensorFlow 提供的服务器是否成功启动，接受对我们模型的请求并对它们做出响应。</p><h1 id="35eb" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">在本地获取码头工人</h1><p id="3448" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">在本地，我们应该安装 Docker 来运行我们的容器。我参考官方<a class="ae kz" href="https://docs.docker.com/engine/installation/" rel="noopener ugc nofollow" target="_blank">文档</a>，因为我无法更好地描述安装程序。</p><p id="226e" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated"><strong class="kd ir">提示</strong>:如果你在 Ubuntu 下，很可能你必须对每个 Docker 命令使用<em class="md"> sudo </em>。我强烈建议采取以下措施来避免这种情况:</p><ul class=""><li id="a42e" class="me mf iq kd b ke kf ki kj km mg kq mh ku mi ky mj mk ml mm bi translated">如果<strong class="kd ir"> docker </strong>组不存在，添加该组</li><li id="b38a" class="me mf iq kd b ke mn ki mo km mp kq mq ku mr ky mj mk ml mm bi translated">将连接的用户<em class="md"> $USER </em>添加到<em class="md"> docker </em>组</li></ul><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="44bb" class="nb lb iq mx b gy nc nd l ne nf">sudo groupadd docker<br/>sudo usermod -aG docker $USER</span></pre><ul class=""><li id="71f9" class="me mf iq kd b ke kf ki kj km mg kq mh ku mi ky mj mk ml mm bi translated">重启你的电脑</li></ul><p id="2993" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">现在你应该可以不用 T21 就能执行 Docker 命令了。例如</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="a71f" class="nb lb iq mx b gy nc nd l ne nf">docker version</span></pre><p id="e01e" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">应该显示系统和版本信息，而没有任何关于缺少权限的错误。</p><h1 id="5576" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">为 TensorFlow 服务创建 Docker 映像并运行容器</h1><p id="748f" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">TensorFlow 服务提供 Docker 映像，因此我们可以克隆存储库并使用它们。</p><h2 id="ebd9" class="nb lb iq bd lc ng nh dn lg ni nj dp lk km nk nl lo kq nm nn ls ku no np lw nq bi translated">获取 TensorFlow 服务</h2><p id="a652" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">这非常简单——只需克隆存储库:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="2415" class="nb lb iq mx b gy nc nd l ne nf">cd ~</span><span id="9ea6" class="nb lb iq mx b gy nr nd l ne nf">git clone https://github.com/tensorflow/serving.git</span></pre><h2 id="1c86" class="nb lb iq bd lc ng nh dn lg ni nj dp lk km nk nl lo kq nm nn ls ku no np lw nq bi translated">构建 Docker 映像并运行容器</h2><p id="c6be" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">Docker 映像的配置是通过 Docker 文件定义的。TensorFlow 服务提供了其中的两种—一种用于 CPU 构建，一种用于 GPU 构建。两者都可以在<em class="md">serving/tensor flow _ serving/tools/docker</em>下找到:</p><ul class=""><li id="9173" class="me mf iq kd b ke kf ki kj km mg kq mh ku mi ky mj mk ml mm bi translated">用于 CPU 构建的<em class="md"> Dockerfile.devel </em></li><li id="786c" class="me mf iq kd b ke mn ki mo km mp kq mq ku mr ky mj mk ml mm bi translated"><em class="md">用于 gpu 构建的 Dockerfile.devel-gpu </em></li></ul></div><div class="ab cl ju jv hu jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="ij ik il im in"><p id="1b54" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated"><strong class="kd ir">英伟达 Docker </strong></p><p id="2c35" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">如果你的机器上有 NVidia GPU 并安装了 CUDA 和 cuDNN 库，我强烈建议创建一个支持 GPU 的 Docker 容器。之前要安装<a class="ae kz" href="https://github.com/NVIDIA/nvidia-docker" rel="noopener ugc nofollow" target="_blank"> <em class="md"> nvidia-docker </em> </a>才能正常工作。这里的<a class="ae kz" href="https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)" rel="noopener ugc nofollow" target="_blank">简单明了，描述得很好</a>。在我的例子中，我使用了以下命令序列:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="83f3" class="nb lb iq mx b gy nc nd l ne nf">curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \<br/>  sudo apt-key add -</span><span id="c825" class="nb lb iq mx b gy nr nd l ne nf">distribution=$(. /etc/os-release;echo $ID$VERSION_ID)</span><span id="7a6c" class="nb lb iq mx b gy nr nd l ne nf">curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \<br/>  sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><span id="8f6f" class="nb lb iq mx b gy nr nd l ne nf">sudo apt-get update<br/>sudo apt-get install nvidia-docker2<br/>sudo pkill -SIGHUP dockerd</span></pre></div><div class="ab cl ju jv hu jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="ij ik il im in"><p id="79e1" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated"><strong class="kd ir">提示</strong>:由于我使用了 GPU Dockerfile，所以在接下来的步骤中我会参考它。使用 CPU Dockerfile，您可以以同样的方式工作。唯一的区别是，你不需要通过运行容器<em class="md"> nvidia-docker </em>和<em class="md"> runtime=nvidia </em>标志。</p><p id="9183" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">让我们创建我们的 Docker 图像:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="e597" class="nb lb iq mx b gy nc nd l ne nf">cd serving<br/>docker build --pull -t $USER/tensorflow-serving-devel-gpu -f tensorflow_serving/tools/docker/Dockerfile.devel-gpu .</span></pre><p id="3619" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">下载依赖项和构建映像大约需要一个小时或更长时间…现在我们可以运行 Docker 容器:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="5ff2" class="nb lb iq mx b gy nc nd l ne nf">docker run --runtime=nvidia --name=tf_container_gpu -it $USER/tensorflow-serving-devel-gpu</span></pre><p id="b629" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">如果一切正常，恭喜你！您现在位于 TensorFlow 服务 Docker 容器的外壳中。</p><p id="cdf4" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated"><strong class="kd ir">提示</strong>:当您用<em class="md">退出</em>命令离开集装箱外壳时，集装箱停止。要再次启动它，请执行:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="509e" class="nb lb iq mx b gy nc nd l ne nf">docker start -i tf_container_gpu</span></pre><p id="15eb" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">不要移除容器！否则，您所做的更改将会消失。另外，请注意，我们在这里不需要<em class="md"> runtime=nvidia </em>参数。</p><p id="3b2e" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">Docker 容器中包含所有必要的组件，如 Python、Bazel 等。已安装，TensorFlow 服务器已准备就绪。如果我们执行</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="2f60" class="nb lb iq mx b gy nc nd l ne nf">tensorflow_model_server</span></pre><p id="bf57" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">在 Shell 内部运行 Docker 容器，我们应该看到使用信息。</p><h2 id="5949" class="nb lb iq bd lc ng nh dn lg ni nj dp lk km nk nl lo kq nm nn ls ku no np lw nq bi translated">部署模型</h2><p id="59ce" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">首先，让我们创建一个工作目录。在运行容器的外壳中执行:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="b1e2" class="nb lb iq mx b gy nc nd l ne nf">mkdir serving</span></pre><p id="9088" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">接下来，我们将导出的模型(参见<a class="ae kz" href="https://medium.com/towards-data-science/how-to-deploy-machine-learning-models-with-tensorflow-part-1-make-your-model-ready-for-serving-776a14ec3198" rel="noopener">第 1 部分</a>)复制到 TensorFlow 容器中。从您的<strong class="kd ir"> PC </strong>上的 Shell:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="7158" class="nb lb iq mx b gy nc nd l ne nf">cd &lt;path to GAN project&gt;<br/>docker cp ./gan-export tf_container_gpu:/serving</span></pre><p id="33cd" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">您应该将导出的模型文件夹放在<strong class="kd ir">容器</strong>中。从它的外壳来看:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="5807" class="nb lb iq mx b gy nc nd l ne nf">cd /serving<br/>ls ./gan-export/1</span></pre><p id="b494" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">你应该看到<em class="md">变量</em>文件夹和<em class="md"> saved_model.pb </em>文件。</p><h2 id="e4c7" class="nb lb iq bd lc ng nh dn lg ni nj dp lk km nk nl lo kq nm nn ls ku no np lw nq bi translated">开始上菜</h2><p id="b121" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">万岁！我们已经准备好通过 TensorFlow 托管我们的模型:-)</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="580c" class="nb lb iq mx b gy nc nd l ne nf">tensorflow_model_server --port=9000 --model_name=gan --model_base_path=/serving/gan-export &amp;&gt; gan_log &amp;</span></pre><p id="d7da" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">如果你检查日志…</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="ff36" class="nb lb iq mx b gy nc nd l ne nf">cat gan_log</span></pre><p id="eace" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">…您应该在最后一个字符串中看到类似这样的内容:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="7339" class="nb lb iq mx b gy nc nd l ne nf">I tensorflow_serving/model_servers/main.cc:298] Running ModelServer at 0.0.0.0:9000 …</span></pre><h1 id="5264" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">断点</h1><p id="3371" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">我们走了很多步，但都很简单明了。他们需要大量的时间，因为要从源代码下载和构建。</p><p id="7d98" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">所以我们在 Docker 容器中开始我们的模型，它在本地运行。但是我们可以保存容器和我们的更改，并在以后部署它，例如，部署到云中。在此之前，我们希望确保我们的模型接受请求和对请求的响应。我们的模型需要一个客户！</p><h1 id="6b64" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">创建客户端</h1><p id="abc6" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">TensorFlow 服务基于<a class="ae kz" href="http://www.grpc.io/" rel="noopener ugc nofollow" target="_blank"> gRPC </a>协议，因此您确实需要创建一个能够通过它进行“对话”的客户端。在<a class="ae kz" href="https://tensorflow.github.io/serving/serving_advanced" rel="noopener ugc nofollow" target="_blank">教程</a>中，他们使用了一个构建在 Docker 容器中的示例客户端。但是我想要一个在我的 PC 上运行的客户端，它可以向 TensorFlow 容器中托管的模型发出请求。</p><p id="8b41" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated"><strong class="kd ir">提示:</strong>我们在<strong class="kd ir"> PC </strong>上执行的所有后续步骤(不在 Docker 容器中！).请记住，我们已经克隆了 TensorFlow 服务存储库。</p><h2 id="c9ed" class="nb lb iq bd lc ng nh dn lg ni nj dp lk km nk nl lo kq nm nn ls ku no np lw nq bi translated">其他 Python 库</h2><p id="df49" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">首先，我们需要 Python 环境中的 gRPC 库。我从 PyPI 安装了它们:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="ba40" class="nb lb iq mx b gy nc nd l ne nf">pip install grpcio grpcio-tools</span></pre><h2 id="a3ec" class="nb lb iq bd lc ng nh dn lg ni nj dp lk km nk nl lo kq nm nn ls ku no np lw nq bi translated">张量流预测服务</h2><p id="0916" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">对于我们的客户，我们需要 TensorFlow 服务 API。问题— API 是以 Protobuf 的形式提供的(可以在<em class="md">serving/tensor flow _ serving/APIs)</em>下找到)。如果我们想在客户机中使用它们，我们需要从这些 Protobuf 中生成 Python 文件。我这样做了，并将生成的文件放入我的<a class="ae kz" href="https://github.com/Vetal1977/tf_serving_example/tree/master/tensorflow_serving/apis" rel="noopener ugc nofollow" target="_blank">存储库</a>。如果你愿意，你可以自己做(虽然有点棘手):</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="6921" class="nb lb iq mx b gy nc nd l ne nf"># 1<br/>cd &lt;tensorflow serving source folder&gt;</span><span id="563d" class="nb lb iq mx b gy nr nd l ne nf"># 2<br/>git clone <a class="ae kz" href="https://github.com/tensorflow/tensorflow.git" rel="noopener ugc nofollow" target="_blank">https://github.com/tensorflow/tensorflow.git</a></span><span id="5984" class="nb lb iq mx b gy nr nd l ne nf"># 3<br/>python -m grpc.tools.protoc ./tensorflow_serving/apis/*.proto --python_out=&lt;path to GAN project&gt; --grpc_python_out=&lt;path to GAN project&gt; --proto_path=.</span></pre><p id="6afb" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">步骤的简短说明:</p><ol class=""><li id="4400" class="me mf iq kd b ke kf ki kj km mg kq mh ku mi ky ns mk ml mm bi translated">更改到包含 TensorFlow 服务源的文件夹</li><li id="c0e5" class="me mf iq kd b ke mn ki mo km mp kq mq ku mr ky ns mk ml mm bi translated">TensorFlow Serving 使用核心框架 Protobuf 的(<em class="md">Serving/tensor flow/tensor flow/core/framework</em>)。获得它们最简单的方法是从 Git 存储库中克隆 TensorFlow 框架</li><li id="54e5" class="me mf iq kd b ke mn ki mo km mp kq mq ku mr ky ns mk ml mm bi translated">这里我们从 Protobuf 为我们的客户生成 Python 文件</li></ol><h2 id="07d5" class="nb lb iq bd lc ng nh dn lg ni nj dp lk km nk nl lo kq nm nn ls ku no np lw nq bi translated">客户</h2><p id="e3ac" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">我已经<a class="ae kz" href="https://github.com/Vetal1977/tf_serving_example/blob/master/svnh_semi_supervised_client.py" rel="noopener ugc nofollow" target="_blank">实现了</a>客户端，这比上面所有准备工作都要简单:-)这里您只需使用 gRPC 和 TensorFlow 服务 API 来发出请求。最有趣的部分是:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="46d4" class="nb lb iq mx b gy nc nd l ne nf"># Send request <br/>with open(FLAGS.image, ‘rb’) as f: <br/>    # 1<br/>    data = f.read()</span><span id="3713" class="nb lb iq mx b gy nr nd l ne nf">    # 2<br/>    request = predict_pb2.PredictRequest() </span><span id="5d20" class="nb lb iq mx b gy nr nd l ne nf">    # 3    <br/>    request.model_spec.name = 'gan' <br/>    request.model_spec.signature_name = 'predict_images' <br/>    request.inputs['images'].CopyFrom( <br/>        tf.contrib.util.make_tensor_proto(<br/>            data, shape=[1]))</span><span id="59b5" class="nb lb iq mx b gy nr nd l ne nf">    # 4<br/>    result = stub.Predict(request, 60.0)</span></pre><ol class=""><li id="c9e0" class="me mf iq kd b ke kf ki kj km mg kq mh ku mi ky ns mk ml mm bi translated">打开(JPEG)文件</li><li id="e435" class="me mf iq kd b ke mn ki mo km mp kq mq ku mr ky ns mk ml mm bi translated">创建预测请求对象</li><li id="2a9a" class="me mf iq kd b ke mn ki mo km mp kq mq ku mr ky ns mk ml mm bi translated">初始化对我们的 GAN 模型的预测请求。在这里，我们指定:模型的名称(它必须与我们启动<em class="md"> tensorflow_model_server </em>时的<em class="md"> model_name </em>参数相匹配)、签名名称(我们只有<em class="md"> predict_images </em>和数据(在我们的例子中是 JPEG 图像)</li><li id="eb0b" class="me mf iq kd b ke mn ki mo km mp kq mq ku mr ky ns mk ml mm bi translated">服务器上的呼叫预测。</li></ol><h1 id="5218" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">呼叫预测</h1><p id="03c7" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">现在我们可以调用运行在 Docker 容器中的模型。获取容器 IP 地址:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="fcb3" class="nb lb iq mx b gy nc nd l ne nf">docker network inspect bridge | grep IPv4Address</span></pre><p id="cd5c" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">我的情况是 172.17.0.2。</p><p id="bde2" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">接下来，我们需要测试图像——我从街景门牌号码测试集中提取了几张图像(以 MATLAB 格式提供),并将它们放在这里<a class="ae kz" href="https://github.com/Vetal1977/tf_serving_example/tree/master/svnh_test_images" rel="noopener ugc nofollow" target="_blank">处</a>。现在—发出客户端请求:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="65f1" class="nb lb iq mx b gy nc nd l ne nf">cd &lt;path to GAN project&gt;<br/>python svnh_semi_supervised_client.py --server=172.17.0.2:9000 --image=./svnh_test_images/image_3.jpg</span></pre><p id="1424" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated"><strong class="kd ir">提示</strong>:有时候，Docker 容器的 IP 地址不起作用。在这种情况下，我们可以使用<em class="md">0 . 0 . 0</em>或<em class="md"> localhost </em>。如果你有问题，试试这些。例如:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="53c6" class="nb lb iq mx b gy nc nd l ne nf">python svnh_semi_supervised_client.py --server=localhost:9000 ...</span></pre><p id="88de" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">您应该会看到类似这样的内容:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="4e32" class="nb lb iq mx b gy nc nd l ne nf">outputs {<br/>  key: “scores”<br/>  value {<br/>    dtype: DT_FLOAT<br/>    tensor_shape {<br/>      dim {<br/>        size: 1<br/>      }<br/>      dim {<br/>        size: 10<br/>      }<br/>    }<br/>    float_val: 8.630897802584857e-17<br/>    float_val: 1.219293777054986e-09<br/>    float_val: 6.613714575998131e-10<br/>    float_val: 1.5203355241411032e-09<br/>    <strong class="mx ir">float_val: 0.9999998807907104</strong><br/>    float_val: 9.070973139291283e-12<br/>    float_val: 1.5690838628401593e-09<br/>    float_val: 9.12262028080068e-17<br/>    float_val: 1.0587883991775016e-07<br/>    float_val: 1.0302327879685436e-08<br/>  }<br/>}</span></pre><p id="abec" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">我用<strong class="kd ir">粗体</strong>标记了最高分，它实际上对应于样本图像中的“4”(数字从 0 到 9) :-)我的 GAN 模型相对简单，精确度约为 68%。</p><h1 id="4ca5" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">而不是结论</h1><p id="509c" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">在第二部分中，我们创建了 TensorFlow 服务容器，将 GAN 模型复制到其中，并启动公开我们模型的服务器。为了进行测试，我们创建了 gRPC 客户端，可以在 PC 上本地运行，并向我们的模型发出请求。</p><p id="5933" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">在<a class="ae kz" href="https://medium.com/@vitaly.bezgachev/how-to-deploy-machine-learning-models-with-tensorflow-part-3-into-the-cloud-7115ff774bb6" rel="noopener">下一部</a>中，我们将把我们的模型公之于众！</p></div></div>    
</body>
</html>