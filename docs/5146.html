<html>
<head>
<title>DBSCAN clustering for data shapes k-means can’t handle well (in Python)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据形状的 DBSCAN 聚类 k-means 不能很好地处理(在 Python 中)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dbscan-clustering-for-data-shapes-k-means-cant-handle-well-in-python-6be89af4e6ea?source=collection_archive---------0-----------------------#2018-09-30">https://towardsdatascience.com/dbscan-clustering-for-data-shapes-k-means-cant-handle-well-in-python-6be89af4e6ea?source=collection_archive---------0-----------------------#2018-09-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="155b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇文章中，我想从 Andreas C. Müller &amp; Sarah Guido 的<a class="ae ko" href="http://shop.oreilly.com/product/0636920030515.do" rel="noopener ugc nofollow" target="_blank">使用 Python 进行机器学习简介</a>中选取一些内容，并简要阐述其中一个示例，以展示当<a class="ae ko" href="https://en.wikipedia.org/wiki/K-means_clustering" rel="noopener ugc nofollow" target="_blank"> k-means 集群</a>似乎不能很好地处理数据形状时<a class="ae ko" href="https://en.wikipedia.org/wiki/DBSCAN" rel="noopener ugc nofollow" target="_blank"> DBSCAN </a>集群的一些优势。我将直奔主题，所以我鼓励你阅读第三章的全部内容，从 168 页开始，如果你想扩展这个话题的话。当描述算法的工作时，我将引用这本书。</p><h2 id="f0d5" class="kp kq it bd kr ks kt dn ku kv kw dp kx kb ky kz la kf lb lc ld kj le lf lg lh bi translated">使聚集</h2><ul class=""><li id="cc8f" class="li lj it js b jt lk jx ll kb lm kf ln kj lo kn lp lq lr ls bi translated"><strong class="js iu">它的任务是将数据集划分成组，称为集群</strong></li><li id="79a7" class="li lj it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated"><strong class="js iu">目标是以这样的方式分割数据，使得单个聚类中的点非常相似，而不同聚类中的点不同</strong></li></ul><h2 id="40a5" class="kp kq it bd kr ks kt dn ku kv kw dp kx kb ky kz la kf lb lc ld kj le lf lg lh bi translated">k 均值聚类</h2><ul class=""><li id="49ea" class="li lj it js b jt lk jx ll kb lm kf ln kj lo kn lp lq lr ls bi translated"><strong class="js iu">试图找到代表数据</strong>的特定区域的<em class="ly">聚类中心</em></li><li id="72c0" class="li lj it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated"><strong class="js iu">在两个步骤之间交替:将每个数据点分配给最近的聚类中心，然后将每个聚类中心设置为分配给它的数据点的平均值</strong></li><li id="68fa" class="li lj it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated"><strong class="js iu">当实例到集群的分配不再改变时，算法结束</strong></li></ul><p id="80c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是 k-means 在可视化表示中的工作方式:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="65a1" class="kp kq it me b gy mi mj l mk ml">import mglearn</span><span id="8b19" class="kp kq it me b gy mm mj l mk ml">mglearn.plots.plot_kmeans_algorithm()</span></pre><figure class="lz ma mb mc gt mo gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/b7c2048433793401f1ea47859b0fb550.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*yThVPL3TBBMf44KGefyIbw.png"/></div></figure><p id="3c11" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">k-means 聚类的一个问题是，它假设所有方向对于每个聚类都是同等重要的。这通常不是大问题，除非我们遇到一些形状奇怪的数据。</p><p id="31ed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本例中，我们将人工生成该类型的数据。使用本书作者提供的以下代码(对聚类数做了一些小的修改)，我们可以生成一些 k-means 无法正确处理的数据:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="cbff" class="kp kq it me b gy mi mj l mk ml">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.datasets import make_blobs<br/>from sklearn.cluster import KMeans</span><span id="e8f8" class="kp kq it me b gy mm mj l mk ml"># generate some random cluster data<br/>X, y = make_blobs(random_state=170, n_samples=600, centers = 5)<br/>rng = np.random.RandomState(74)</span><span id="0040" class="kp kq it me b gy mm mj l mk ml"># transform the data to be stretched<br/>transformation = rng.normal(size=(2, 2))<br/>X = np.dot(X, transformation)</span><span id="837b" class="kp kq it me b gy mm mj l mk ml"># plot<br/>plt.scatter(X[:, 0], X[:, 1])<br/>plt.xlabel("Feature 0")<br/>plt.ylabel("Feature 1")<br/>plt.show()</span></pre><figure class="lz ma mb mc gt mo gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/12189332a05ead4e21e579835926f930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*JxCBOYVyZpipYyY9az85Bg.png"/></div></figure><p id="55a0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如你所看到的，我们可以说有 5 个定义好的具有拉伸对角线形状的集群。</p><p id="4feb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们应用 k 均值聚类:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="9b2c" class="kp kq it me b gy mi mj l mk ml"># cluster the data into five clusters<br/>kmeans = KMeans(n_clusters=5)<br/>kmeans.fit(X)<br/>y_pred = kmeans.predict(X)</span><span id="c82e" class="kp kq it me b gy mm mj l mk ml"># plot the cluster assignments and cluster centers<br/>plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap="plasma")<br/>plt.scatter(kmeans.cluster_centers_[:, 0],   <br/>            kmeans.cluster_centers_[:, 1],<br/>            marker='^', <br/>            c=[0, 1, 2, 3, 4], <br/>            s=100, <br/>            linewidth=2,<br/>            cmap="plasma")</span><span id="5f37" class="kp kq it me b gy mm mj l mk ml">plt.xlabel("Feature 0")<br/>plt.ylabel("Feature 1")</span></pre><figure class="lz ma mb mc gt mo gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/a340276cc69a6a5a312d7829a001b50e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*cunMOq8dknmftVcaZhMDTQ.png"/></div></figure><p id="ace7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们在这里可以看到，<em class="ly"> k-means </em>已经能够正确地检测到中间和底部的集群，而对于顶部的集群则存在问题，这些集群彼此非常接近。作者说:<em class="ly">“这些群体向对角线拉伸。由于 k-means 只考虑到最近的聚类中心的距离，它不能处理这类数据”</em></p><p id="d38a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看看 DBSCAN 聚类如何帮助处理这种形状:</p><h2 id="2926" class="kp kq it bd kr ks kt dn ku kv kw dp kx kb ky kz la kf lb lc ld kj le lf lg lh bi translated">基于密度的噪声应用空间聚类</h2><p id="3ed5" class="pw-post-body-paragraph jq jr it js b jt lk jv jw jx ll jz ka kb mt kd ke kf mu kh ki kj mv kl km kn im bi translated">关于 DBSCAN 集群的一些亮点摘自该书:</p><ul class=""><li id="fea5" class="li lj it js b jt ju jx jy kb mw kf mx kj my kn lp lq lr ls bi translated">代表<em class="ly">“基于密度的有噪声应用的空间聚类”</em></li><li id="5339" class="li lj it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated">不需要用户预先设置聚类数<em class="ly">和</em></li><li id="0874" class="li lj it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated">可以捕捉复杂形状的集群</li><li id="beb5" class="li lj it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated">可以识别不属于任何聚类的点(作为异常值检测器非常有用)</li><li id="4abc" class="li lj it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated">比凝聚聚类和 k-means 稍慢，但仍可扩展到相对较大的数据集。</li><li id="6e81" class="li lj it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated">工作原理是识别特征空间的<em class="ly">拥挤</em>区域中的点，其中许多数据点靠得很近(特征空间中的密集区域)</li><li id="5d81" class="li lj it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated">密集区域内的点称为<em class="ly">岩心样本</em>(或岩心点)</li><li id="d2a6" class="li lj it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated">DBSCAN 中有两个参数:<code class="fe mz na nb me b">min_samples</code>和<code class="fe mz na nb me b">eps</code></li><li id="ef76" class="li lj it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated">如果在到给定数据点的距离<code class="fe mz na nb me b">eps</code>内至少有<code class="fe mz na nb me b">min_samples</code>个数据点，则该数据点被分类为<em class="ly">岩心</em>样本</li><li id="3394" class="li lj it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated">通过 DBSCAN 将彼此距离比距离<code class="fe mz na nb me b">eps</code>更近的<em class="ly">核心</em>样本放入同一簇中。</li></ul><p id="83a4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是聚类如何根据两个参数的选择而变化的示例:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="e35a" class="kp kq it me b gy mi mj l mk ml">mglearn.plots.plot_dbscan()</span></pre><figure class="lz ma mb mc gt mo gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/6ebb58fd2e7ed4a252fac31cbe0f9798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*hRAg5Re1LuBwBdSTHKosGQ.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">In this plot, points that belong to clusters are solid, while the noise points are shown in white. Core samples are shown as large markers, while boundary points are displayed as smaller markers. Increasing eps (going from left to right in the figure) means that more points will be included in a cluster. This makes clusters grow, but might also lead to multiple clusters joining into one. Increasing min_samples (going from top to bottom in the figure) means that fewer points will be core points, and more points will be labeled as noise.</figcaption></figure><p id="9a9e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">参数<code class="fe mz na nb me b">eps</code>在某种程度上更重要，因为它决定了点靠近<em class="ly">意味着什么。</em>将<code class="fe mz na nb me b">eps</code>设置得很小将意味着没有点是核心样本，并且可能导致所有点被标记为噪声。将<code class="fe mz na nb me b">eps</code>设置得非常大将导致所有点形成一个单独的簇。</p><p id="f6f8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们回到我们的例子，看看 DBSCAN 如何处理它:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="0dc3" class="kp kq it me b gy mi mj l mk ml">from sklearn.cluster import DBSCAN<br/>from sklearn.preprocessing import StandardScaler</span><span id="b944" class="kp kq it me b gy mm mj l mk ml">scaler = StandardScaler()<br/>X_scaled = scaler.fit_transform(X)</span><span id="c2ac" class="kp kq it me b gy mm mj l mk ml"># cluster the data into five clusters<br/>dbscan = DBSCAN(eps=0.123, min_samples = 2)<br/>clusters = dbscan.fit_predict(X_scaled)</span><span id="85e1" class="kp kq it me b gy mm mj l mk ml"># plot the cluster assignments<br/>plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap="plasma")<br/>plt.xlabel("Feature 0")<br/>plt.ylabel("Feature 1")</span></pre><figure class="lz ma mb mc gt mo gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/0f5253467ab9b580900b8406183e6a9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*Qejguy_FRpEZkOwtor1BKg.png"/></div></figure><p id="fc22" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在扭曲<code class="fe mz na nb me b">eps</code>和<code class="fe mz na nb me b">min_samples</code>一段时间后，我得到了一些相当一致的集群，仍然包括一些噪声点。</p><ul class=""><li id="ebed" class="li lj it js b jt ju jx jy kb mw kf mx kj my kn lp lq lr ls bi translated">虽然 DBSCAN 不需要显式地设置簇的数量，但是设置<code class="fe mz na nb me b">eps</code>隐式地控制将找到多少个簇。</li><li id="95e6" class="li lj it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated">缩放数据后，为<code class="fe mz na nb me b">eps</code>找到一个好的设置有时会更容易，因为使用这些缩放技术将确保所有特征具有相似的范围。</li></ul><p id="d587" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，考虑到我们创建了明确定义 5 个聚类的数据点，我们可以使用 adjusted_rand_score 来衡量性能。这种情况并不常见，因为在实际情况中，我们一开始就没有聚类标签(因此我们需要应用聚类技术)。因为在这种情况下，我们有标签，我们可以测量性能:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="3ba1" class="kp kq it me b gy mi mj l mk ml">from sklearn.metrics.cluster import adjusted_rand_score</span><span id="d958" class="kp kq it me b gy mm mj l mk ml">#k-means performance:<br/>print("ARI =", adjusted_rand_score(y, y_pred).round(2))<br/>ARI = 0.76</span><span id="6828" class="kp kq it me b gy mm mj l mk ml">#DBSCAN performance:<br/>print("ARI =", adjusted_rand_score(y, clusters).round(2))<br/>ARI = 0.99</span></pre><p id="1b84" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你有它！DBSCAN 得分为 0.99，而 k-means 仅得 0.76</p></div></div>    
</body>
</html>