<html>
<head>
<title>Review: Trimps-Soushen — Winner in ILSVRC 2016 (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:Trimps-Soushen——2016 年 ILSVRC(图像分类)获奖者</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-trimps-soushen-winner-in-ilsvrc-2016-image-classification-dfbc423111dd?source=collection_archive---------8-----------------------#2018-12-12">https://towardsdatascience.com/review-trimps-soushen-winner-in-ilsvrc-2016-image-classification-dfbc423111dd?source=collection_archive---------8-----------------------#2018-12-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="59d6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">深度特征融合的良好实践:第一个获得 3%以下的错误率(当人类性能只能获得 5%时)</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/df5c485792a5e0f7234f99da958a7790.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*cvEu72GW5TIIMXbt.jpg"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Trimps Is Active at Different Technology Aspects </strong><a class="ae ks" href="http://news.sina.com.cn/c/2017-09-17/doc-ifykynia7850053.shtml" rel="noopener ugc nofollow" target="_blank">http://news.sina.com.cn/c/2017-09-17/doc-ifykynia7850053.shtml</a></figcaption></figure><p id="bc4a" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi lp"><span class="l lq lr ls bm lt lu lv lw lx di">In</span> this story, the approach by the winner, <strong class="kv ir">Trimps-Soushen</strong>, in ILSVRC 2016 classification task, is reviewed. <strong class="kv ir">Trimps </strong>stands for The <strong class="kv ir">T</strong>hird <strong class="kv ir">R</strong>esearch <strong class="kv ir">I</strong>nstitute of <strong class="kv ir">M</strong>inistry of <strong class="kv ir">P</strong>ublic <strong class="kv ir">S</strong>ecurity, or in chinese 公安部三所. In brief, Trimps is the research institute for advancing the technologies for public security in China, which was launched in 1978 at Shanghai. <strong class="kv ir">Soushen</strong> should be the <strong class="kv ir">team name under Trimps</strong>, in chinese 搜神. It means <strong class="kv ir">god of search</strong>, where Sou (搜) means search and Shen (神) means god. (<a class="ly lz ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----dfbc423111dd--------------------------------" rel="noopener" target="_blank">Sik-Ho Tsang</a> @ Medium)</p><p id="ca74" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">Trimps-Soushen 在 2016 年赢得了多项比赛:</p><ul class=""><li id="3483" class="ma mb iq kv b kw kx kz la lc mc lg md lk me lo mf mg mh mi bi translated"><strong class="kv ir">物体定位:第一名，7.71%误差</strong></li><li id="1a7e" class="ma mb iq kv b kw mj kz mk lc ml lg mm lk mn lo mf mg mh mi bi translated"><strong class="kv ir">物体分类:第一名，2.99%误差</strong></li><li id="ece1" class="ma mb iq kv b kw mj kz mk lc ml lg mm lk mn lo mf mg mh mi bi translated">物体检测:第三名，61.82%地图</li><li id="4750" class="ma mb iq kv b kw mj kz mk lc ml lg mm lk mn lo mf mg mh mi bi translated">场景分类:第三名，误差 10.3%</li><li id="ba0e" class="ma mb iq kv b kw mj kz mk lc ml lg mm lk mn lo mf mg mh mi bi translated">视频目标检测:第三名，70.97%地图</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/2e17e83f80d2bc7e01fbdfb696ff3187.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wbDhLHouNROwdRWfjcxiJg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">ILSVRC 2016 Classification Ranking </strong><a class="ae ks" href="http://image-net.org/challenges/LSVRC/2016/results#loc" rel="noopener ugc nofollow" target="_blank">http://image-net.org/challenges/LSVRC/2016/results#loc</a></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mt"><img src="../Images/a53d361c5b23f3833c2fe27a40d735ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kiXf4aE2H-vZ7CBOjXIF0Q.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">ILSVRC Classification Results from 2011 to 2016</strong></figcaption></figure><p id="1ace" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">虽然 Trimps-Soushen 在多个识别任务上拥有最先进的成果，但<strong class="kv ir">Trimps-Soushen</strong>没有新的创新技术或新颖性。可能因为这个原因，<strong class="kv ir">他们没有发表任何关于它的论文</strong>或技术报告。</p><p id="8ba3" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">相反，<strong class="kv ir">他们只在 2016 年 ECCV </strong>的 ImageNet 和 COCO 联合研讨会上分享了他们的成果。他们有一些关于数据集的有趣事实。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="c04a" class="nb nc iq bd nd ne nf ng nh ni nj nk nl jw nm jx nn jz no ka np kc nq kd nr ns bi translated">涵盖哪些内容</h1><ol class=""><li id="63b3" class="ma mb iq kv b kw nt kz nu lc nv lg nw lk nx lo ny mg mh mi bi translated"><strong class="kv ir">使用不同模型的集成(图像分类)</strong></li><li id="efa7" class="ma mb iq kv b kw mj kz mk lc ml lg mm lk mn lo ny mg mh mi bi translated"><strong class="kv ir">基于前 20 名准确度的一些发现(图像分类)</strong></li><li id="d36b" class="ma mb iq kv b kw mj kz mk lc ml lg mm lk mn lo ny mg mh mi bi translated"><strong class="kv ir">区域融合(图像定位)</strong></li><li id="e2f6" class="ma mb iq kv b kw mj kz mk lc ml lg mm lk mn lo ny mg mh mi bi translated"><strong class="kv ir">用于其他任务的多模型融合(场景分类/对象检测/从视频中检测对象)</strong></li></ol></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="d204" class="nb nc iq bd nd ne nf ng nh ni nj nk nl jw nm jx nn jz no ka np kc nq kd nr ns bi translated">1.使用不同模型的集成</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nz"><img src="../Images/875183681cbff70a257b860f3c448839.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IU72eGidzSxagQP_vlAaew.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">ImageNet Classification Errors for Top-10 Difficult Categories</strong></figcaption></figure><p id="31da" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">Trimps-Soushen 使用了来自<a class="ae ks" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener"> Inception-v3 </a>、<a class="ae ks" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-v4 </a>、<a class="ae ks" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-ResNet-v2 </a>、<a class="ae ks" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e">预激活 ResNet-200 </a>、<a class="ae ks" rel="noopener" target="_blank" href="/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004">Wide ResNet(WRN-68–2)</a>的预训练模型进行分类，并找出了如上的前 10 个困难类别。</p><ul class=""><li id="b579" class="ma mb iq kv b kw kx kz la lc mc lg md lk me lo mf mg mh mi bi translated"><strong class="kv ir">获得了不同的结果</strong>，这意味着没有模型在所有类别中占主导地位。<strong class="kv ir">每个模型都擅长对某些类别进行分类，但也擅长对某些类别进行分类。</strong></li><li id="982d" class="ma mb iq kv b kw mj kz mk lc ml lg mm lk mn lo mf mg mh mi bi translated">模型的多样性可以用来提高精确度。</li></ul><p id="4373" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">在培训期间，Trimps-Soushen 只是进行了多规模扩增和大迷你批量。在测试期间，多尺度+翻转与密集融合一起使用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oa"><img src="../Images/fccb8dbcf0e67a6a2d9640902c222e64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2NwJ_d4v8hEHLh6gS8Cp1A.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">ImageNet Top-5 Error Rate Results</strong></figcaption></figure><ul class=""><li id="181c" class="ma mb iq kv b kw kx kz la lc mc lg md lk me lo mf mg mh mi bi translated">5 个最佳模型的验证误差为 3.52%-4.65%。</li><li id="1edb" class="ma mb iq kv b kw mj kz mk lc ml lg mm lk mn lo mf mg mh mi bi translated">通过组合这 5 个模型(<a class="ae ks" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-ResNet-v2 </a>具有更高的权重)，获得了 2.92%的验证误差。</li><li id="d053" class="ma mb iq kv b kw mj kz mk lc ml lg mm lk mn lo mf mg mh mi bi translated">获得了 2.99%的测试误差，这是第一个在 3%误差率下获得的测试误差。</li></ul></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="645f" class="nb nc iq bd nd ne nf ng nh ni nj nk nl jw nm jx nn jz no ka np kc nq kd nr ns bi translated">2.<strong class="ak">基于前 20 名准确度的一些发现</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi ob"><img src="../Images/a288eda07edc5ec5024a644eb35aee31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CITaB76GciyNfALv0At8eg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Top-k Accuracy</strong></figcaption></figure><p id="69b8" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">Top- <em class="oc"> k </em>精度如上图所示。当<em class="oc"> k </em> =20 时，获得 99.27%的精度。误差率小于 1%。</p><blockquote class="od"><p id="389f" class="oe of iq bd og oh oi oj ok ol om lo dk translated"><strong class="ak">为什么使用 Top-20 精度仍有误差？</strong></p></blockquote><p id="4ea8" class="pw-post-body-paragraph kt ku iq kv b kw on jr ky kz oo ju lb lc op le lf lg oq li lj lk or lm ln lo ij bi translated">Trimp-Soushen 非常详细地分析了这些 1%的误差图像！！！</p><p id="5e1f" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">他们从验证集中手动分析了 1458 个错误图像。大致得到 7 类误差如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi os"><img src="../Images/d984057451779dfb39031fafe894f88d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6FuxWWw0d4p2b3CS5gOLag.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">7 Error Categories</strong></figcaption></figure><h2 id="6e16" class="ot nc iq bd nd ou ov dn nh ow ox dp nl lc oy oz nn lg pa pb np lk pc pd nr pe bi translated">2.1.标签可能有误</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi pf"><img src="../Images/c7017cf4fb04de3f16377a1ad2546f4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WMCmxsSh1lMvcqTIovsQZQ.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Label May Wrong (Maybe it is really a sleeping bag for Hello Kitty? lol)</strong></figcaption></figure><p id="788c" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">地面真相是睡袋，但显然，它是一个铅笔盒！！！！</p><p id="207a" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">这是因为地面真相在 ImageNet 数据集中是手动标注的。由于 ImageNet 是一个包含超过 1500 万个带标签的高分辨率图像的数据集，约有 22，000 个类别，并且 1000 个类别的 ImageNet 数据集的子集用于竞争，因此可能会有一些错误的标签。</p><p id="82ad" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">在 1458 幅错误图像中，有 211 幅是“标签可能错误”，约占 15.16%。</p><h2 id="0e60" class="ot nc iq bd nd ou ov dn nh ow ox dp nl lc oy oz nn lg pa pb np lk pc pd nr pe bi translated">2.2.多个对象(&gt; 5)</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi pg"><img src="../Images/e6ef9ccfa7b5758f7e371143792b8235.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*8b2S6oeC6Z6jQvkV-XKMYQ.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Multiple Objects (&gt;5) (Which is the main object?)</strong></figcaption></figure><p id="2a59" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">上图包含多个对象(&gt; 5)。实际上，这种图像不适合于 ILSVRC 分类任务。因为在 ILSVRC 分类任务中，对于每幅图像应该只识别一个类别。</p><p id="eefa" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">在 1458 幅错误图像中有 118 幅是“多物体(&gt; 5)”，约占 8.09%。</p><h2 id="d4d3" class="ot nc iq bd nd ou ov dn nh ow ox dp nl lc oy oz nn lg pa pb np lk pc pd nr pe bi translated">2.3.不明显的主要对象</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi ph"><img src="../Images/993f32233ccd7fc08d3186271c6cf81a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KMbj0T2w2UK7JCdVkhASaw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Non-Obvious Main Object (Please find the paper towel in the image, lol !!)</strong></figcaption></figure><p id="901a" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">由于在分类任务中应该只识别一个类别，所以上面的图像在图像中没有一个明显的主要对象。可以是船，也可以是码头。但事实是纸巾。</p><p id="6f00" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">在 1458 幅错误图像中，有 355 幅是“非明显主目标”，约占 24.35%。</p><h2 id="5ce0" class="ot nc iq bd nd ou ov dn nh ow ox dp nl lc oy oz nn lg pa pb np lk pc pd nr pe bi translated">2.4.混淆标签</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi pi"><img src="../Images/b7e8486c0e6d7cc68a6a7373c0449384.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WbFUP5m-Dlaxnir5vSzdMg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Confusing Label (Maybe there is no sunscreen inside, lol.)</strong></figcaption></figure><p id="b4ae" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">事实是防晒霜。这一次，标签似乎是正确的，因为纸箱上说的 SPF30。但任务将变成理解纸箱上文字的含义，这与基于形状和颜色识别物体的最初目标相去甚远。</p><p id="683a" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">1458 幅错误图像中有 206 幅是“混淆标签”，约占 14.13%。</p><h2 id="3c91" class="ot nc iq bd nd ou ov dn nh ow ox dp nl lc oy oz nn lg pa pb np lk pc pd nr pe bi translated">2.5.细粒度标签</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi pj"><img src="../Images/7cdbea39cc7c1a803acf2a2a834a39a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*8alcFG6Tfw-n59VTzH5VuQ.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Fine-Grained Label</strong></figcaption></figure><p id="63a6" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">事实是正确的。牛肝菌和臭牛肝菌都是真菌的种类。事实上，这种类型的标签甚至很难被人类识别。</p><p id="405e" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">1458 幅错误图像中有 258 幅是“细粒度标签”，约占 17.70%。</p><p id="cee1" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">网络可以改善这个范畴。</strong></p><h2 id="cd9e" class="ot nc iq bd nd ou ov dn nh ow ox dp nl lc oy oz nn lg pa pb np lk pc pd nr pe bi translated">2.6.明显错误</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi pk"><img src="../Images/6dae1f87068b836b517436696a9d3840.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJmQk0KKxWmZdKcD_QpjuQ.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Obvious Wrong</strong></figcaption></figure><p id="3644" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">事实是正确的。即使使用前 20 名预测，网络也无法预测它。</p><p id="2625" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">1458 幅错误图像中有 234 幅“明显错误”，约占 16.05%。</p><p id="32f9" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">网络可以改善这个范畴。</strong></p><h2 id="e132" class="ot nc iq bd nd ou ov dn nh ow ox dp nl lc oy oz nn lg pa pb np lk pc pd nr pe bi translated">2.7.部分对象</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/946910dc1b32bb11d6483bedc88f3db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*RmkiBxEek5vJ64ct4tzn2A.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Partial Object</strong></figcaption></figure><p id="5c37" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">图像可能只包含对象的一部分，这很难识别。如果把多张桌椅拉远一点，看起来像个餐厅，可能图像会更好。</p><p id="1d79" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">在 1458 幅错误图像中，有 66 幅是“部分目标”，约占 4.53%。</p><blockquote class="od"><p id="654e" class="oe of iq bd og oh oi oj ok ol om lo dk translated">所以精度很难提高 1%。</p></blockquote></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="4d06" class="nb nc iq bd nd ne nf ng nh ni nj nk nl jw nm jx nn jz no ka np kc nq kd nr ns bi translated">3.区域融合(图像定位)</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi pm"><img src="../Images/a5ecaded736b767f9595a60866950cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u9k8BfVBdtoW9ErutuCzpg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Region Fusion for Image Localization</strong></figcaption></figure><p id="b6b7" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">为了定位图像中的前 5 个预测标签，使用了使用多个模型的更快的 R-CNN 架构。多个模型用于通过<a class="ae ks" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快速 R-CNN </a>中的区域提案网络(RPN)生成区域提案。然后基于前 5 个分类预测标签，执行定位预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi pn"><img src="../Images/1dcbaa9c9fbdab0da7b8b0f3a4ab78ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YGR5nS_PSE_MaCKmmX_sbg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Image Localization Top-5 Validation Error Results</strong></figcaption></figure><ul class=""><li id="a13e" class="ma mb iq kv b kw kx kz la lc mc lg md lk me lo mf mg mh mi bi translated"><strong class="kv ir">以前最先进的方法</strong> : <strong class="kv ir">得到 8.51%到 9.27% </strong>的误差。</li><li id="8b23" class="ma mb iq kv b kw mj kz mk lc ml lg mm lk mn lo mf mg mh mi bi translated"><strong class="kv ir">集成所有</strong>:将所有方法融合在一起，得到<strong class="kv ir"> 7.58% </strong>误差。</li><li id="0c6c" class="ma mb iq kv b kw mj kz mk lc ml lg mm lk mn lo mf mg mh mi bi translated"><strong class="kv ir">除了一个模型之外的集合</strong>:仅获得 7.75%到 7.93%的误差。</li></ul><p id="85d2" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">因此，模型之间差异是重要的，并有助于预测精度的大幅度提高。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi po"><img src="../Images/7e6b714c2e2d50e5f295c8cc95045909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dLNH6yLO5B8GMEjyXVgaNw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">ILSVRC Localization Top-5 Test Error Results from 2012 to 2016</strong></figcaption></figure></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="1d9a" class="nb nc iq bd nd ne nf ng nh ni nj nk nl jw nm jx nn jz no ka np kc nq kd nr ns bi translated"><strong class="ak"> 4。用于其他任务的多模型融合(场景分类/对象检测/从视频中检测对象)</strong></h1><h2 id="3d83" class="ot nc iq bd nd ou ov dn nh ow ox dp nl lc oy oz nn lg pa pb np lk pc pd nr pe bi translated">4.1.场景分类</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi pp"><img src="../Images/bf84582a364bf2d16ba0b691dbef6ee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aULmp1gwpXVGHu3IRqMzFw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Multi-Scale &amp; Multi-Model Fusion for Scene Classification</strong></figcaption></figure><p id="1100" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">通过连接结果而不是在网络末端将结果相加，使用改进的多尺度方法。</p><p id="8f2f" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">除了使用相同模型使用多尺度输入进行预测之外，通过连接结果并通过 FC 和 softmax 使用两个训练模型(我相信使用相同的模型网络)。验证误差为 10.80%。</p><p id="fa1d" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">采用 7×2 模型，验证误差为 10.39%，检验误差为 10.42%。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi po"><img src="../Images/5b3e7c57d7b24ce00f95c24a235c0fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rV2Rgz_xOl6NiFu3Qb87Qg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Scene Classification Top-5 Test Error Results</strong></figcaption></figure><p id="9868" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">使用外部数据集 Places2 的模型也用于预训练，获得了 10.3%的 top-5 测试误差，在场景分类中获得第三名。</p><h2 id="be8a" class="ot nc iq bd nd ou ov dn nh ow ox dp nl lc oy oz nn lg pa pb np lk pc pd nr pe bi translated">4.2.目标检测</h2><p id="2619" class="pw-post-body-paragraph kt ku iq kv b kw nt jr ky kz nu ju lb lc pq le lf lg pr li lj lk ps lm ln lo ij bi translated">类似于图像定位，<a class="ae ks" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>架构用于多模型融合。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi pt"><img src="../Images/ffbba2fbd4069f4f7687984c7f0db1b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTHtDRbkDcA0wL8iVlGTKA.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Object Detection mAP Results</strong></figcaption></figure><p id="5344" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">获得了 61.82%的 mAP。</p><h2 id="bc7c" class="ot nc iq bd nd ou ov dn nh ow ox dp nl lc oy oz nn lg pa pb np lk pc pd nr pe bi translated">4.3.视频中的目标检测</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi pu"><img src="../Images/9fa900580c29c267baf49420b851de6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pHpCIBqnZAITKDmyWByQOA.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Object Detection from Video mAP Results</strong></figcaption></figure><p id="f15b" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">光流引导的运动预测也用于减少假阴性检测。获得了 70.97%的 mAP。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><p id="1c60" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">由于模型的多样性，模型融合是有效的。通过模型融合，Trimps-Soushen 胜过<a class="ae ks" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt </a>和<a class="ae ks" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea"> PolyNet </a>，获得图像分类第一名。模型融合也被成功地应用到其他任务中。这意味着除了网络架构的创新优化或新颖设计，其他技术东西如<strong class="kv ir">多模型融合也可以帮助提高准确性很多</strong>。</p><p id="bd25" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">另一方面，<strong class="kv ir">如果 Trimps-Soushen 使用</strong><a class="ae ks" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"><strong class="kv ir">ResNeXt</strong></a><strong class="kv ir">和</strong><a class="ae ks" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea"><strong class="kv ir">poly net</strong></a><strong class="kv ir">进行模型融合，也许可以进一步减小</strong>的误差，因为<a class="ae ks" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt </a>和<a class="ae ks" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea"> PolyNet </a>比用于模型融合的那些模型获得更高的精度。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h2 id="a9bf" class="ot nc iq bd nd ou ov dn nh ow ox dp nl lc oy oz nn lg pa pb np lk pc pd nr pe bi translated">参考</h2><p id="ae71" class="pw-post-body-paragraph kt ku iq kv b kw nt jr ky kz nu ju lb lc pq le lf lg pr li lj lk ps lm ln lo ij bi translated">【2016 ECCV】【Trimps-Soushen】(仅幻灯片)<br/> <a class="ae ks" href="http://image-net.org/challenges/talks/2016/Trimps-Soushen@ILSVRC2016.pdf" rel="noopener ugc nofollow" target="_blank">深度特征融合的良好实践</a></p><h2 id="6b05" class="ot nc iq bd nd ou ov dn nh ow ox dp nl lc oy oz nn lg pa pb np lk pc pd nr pe bi translated">我对图像分类的相关综述</h2><p id="79f2" class="pw-post-body-paragraph kt ku iq kv b kw nt jr ky kz nu ju lb lc pq le lf lg pr li lj lk ps lm ln lo ij bi translated">[<a class="ae ks" href="https://medium.com/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17" rel="noopener">LeNet</a>][<a class="ae ks" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener">AlexNet</a>][<a class="ae ks" href="https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103" rel="noopener">ZFNet</a>][<a class="ae ks" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener">VGGNet</a>][<a class="ae ks" href="https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679" rel="noopener">SPPNet</a>][<a class="ae ks" href="https://medium.com/coinmonks/review-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617" rel="noopener">PReLU-Net</a>][<a class="ae ks" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener">Google Net/Inception-v1</a>][<a class="ae ks" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">BN-Inception/Inception-v2</a>][<a class="ae ks" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener">Inception-v3</a>][<a class="ae ks" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-v4</a><a class="ae ks" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> [ </a><a class="ae ks" rel="noopener" target="_blank" href="/review-ror-resnet-of-resnet-multilevel-resnet-image-classification-cd3b0fcc19bb"> RoR </a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-stochastic-depth-image-classification-a4e225807f4a">随机深度</a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004"> WRN </a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea">波利尼西亚</a>][<a class="ae ks" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac">ResNeXt</a>][<a class="ae ks" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803">dense net</a>]</p><h2 id="030e" class="ot nc iq bd nd ou ov dn nh ow ox dp nl lc oy oz nn lg pa pb np lk pc pd nr pe bi translated">我对物体检测的相关评论</h2><p id="2a8b" class="pw-post-body-paragraph kt ku iq kv b kw nt jr ky kz nu ju lb lc pq le lf lg pr li lj lk ps lm ln lo ij bi translated">[ <a class="ae ks" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a></p></div></div>    
</body>
</html>