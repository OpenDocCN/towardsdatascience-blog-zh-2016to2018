<html>
<head>
<title>How ‘Big’ should be your Data?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">您的数据应该有多大？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-big-should-be-your-data-fdace6e627e4?source=collection_archive---------6-----------------------#2018-02-20">https://towardsdatascience.com/how-big-should-be-your-data-fdace6e627e4?source=collection_archive---------6-----------------------#2018-02-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="e29b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在设计机器学习模型时，所有数据科学家都会想到一个简单的问题:“数据集应该有多大？建模应该使用多少特征？”。本文将通过一项实验研究，向您展示如何决定数据集大小和模型所需的特性数量。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/b7367eb532b18ad67b38520d5beefec9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P6PWPLZ7vv1LzN2jo1lNvA.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://commons.wikimedia.org/wiki/File:BigData_2267x1146_white.png" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/wiki/File:BigData_2267x1146_white.png</a></figcaption></figure></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="4573" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是我的 github 资源库的链接。它包含用于分析的 python 代码。您可以派生或克隆存储库，并使用代码:-P</p><div class="lj lk gp gr ll lm"><a href="https://github.com/bmonikraj/machine-learning-data-analysis-01.git" rel="noopener  ugc nofollow" target="_blank"><div class="ln ab fo"><div class="lo ab lp cl cj lq"><h2 class="bd ir gy z fp lr fr fs ls fu fw ip bi translated">bmonikraj/机器学习数据分析-01</h2><div class="lt l"><h3 class="bd b gy z fp lr fr fs ls fu fw dk translated">机器学习-数据分析-01 -使用统计方法对数据集进行实验分析，以获得更好的机器…</h3></div><div class="lu l"><p class="bd b dl z fp lr fr fs ls fu fw dk translated">github.com</p></div></div><div class="lv l"><div class="lw l lx ly lz lv ma kv lm"/></div></div></a></div></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="d383" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是从 UCI 机器学习知识库到数据仓库的链接。它包含与数据集和属性相关的信息。这里提到，这个问题是一个回归问题。</p><p id="1a97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lb" href="https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure" rel="noopener ugc nofollow" target="_blank">https://archive . ics . UCI . edu/ml/datasets/理化+属性+of+蛋白质+三级+结构</a></p><pre class="km kn ko kp gt mb mc md me aw mf bi"><span id="4227" class="mg mh iq mc b gy mi mj l mk ml"><strong class="mc ir">Description of Dataset</strong><br/>Dataset inititally has 10 columns. <br/>Column with name 'RMSD' is the target column of our data set<br/>Rest 9 columns with name F1, F2, ... , F9 are the features columns of the data set. </span><span id="1b1a" class="mg mh iq mc b gy mm mj l mk ml">m = Number of observations in the data set<br/>Dimension of Y : m X 1<br/>Dimension of X : m X 9</span></pre></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="4bf4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我将向您展示数据帧的每一列之间的关联矩阵，包括 RMSD <em class="mn">(数据帧是一个术语，用来表示 Python 的 Pandas 模块读取的数据集)。</em></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mo"><img src="../Images/d2cfa63aca2c3ee1394f9dddad7e943d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hnibR6dA5YmS9MYUG8oALQ.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Correlation Matrix — Heat Map , showing the correlation between each and every column of data frame</figcaption></figure><blockquote class="mp mq mr"><p id="b48b" class="jn jo mn jp b jq jr js jt ju jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj kk ij bi translated"><em class="iq">从上图中，我们可以得出结论，RMSD 与其他列几乎是独立的，因为理论上说如果，correlation(x，y)~0，那么‘x’和‘y’是彼此独立的(其中‘x’和‘y’是相同维数的向量)。</em></p><p id="e6c3" class="jn jo mn jp b jq jr js jt ju jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj kk ij bi translated"><strong class="jp ir">在我们不知道哪一列将被选为目标列的情况下，我们可以找到相关矩阵，并找到与所有其他列最独立的列。</strong></p></blockquote><p id="7120" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是 x 中所有特征列之间的相关矩阵图。</p><p id="35c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mn">对于 X 的列‘I’和列‘j ’,它显示了两个变量之间的散点图，当‘I’时！= 'j '。如果' i'=='j '，那么它显示了变量的频率分布。</em></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mv"><img src="../Images/6b161558ddf406c8cb828aee35f34d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dvH0jcUuVlkZtXMFz8B60A.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Correlation Matrix plot — Correlation between two variables (feature column) and frequency distribution of the variable itself</figcaption></figure><blockquote class="mp mq mr"><p id="7b7f" class="jn jo mn jp b jq jr js jt ju jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj kk ij bi translated"><em class="iq">上图讲述了变量之间的相互关系。同样，我们可以用图形来推断矩阵之间的相关性，以便更好地进行分析。</em></p><p id="49fe" class="jn jo mn jp b jq jr js jt ju jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="iq">变量的频率分布显示了它所遵循的分布类型，如果遵循一个已知的标准分布，如</em> </strong> <a class="ae lb" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> <em class="iq">【高斯】</em> </strong> </a> <strong class="jp ir"> <em class="iq">分布或</em> </strong> <a class="ae lb" href="https://en.wikipedia.org/wiki/Rayleigh_distribution" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> <em class="iq">【瑞利】</em> </strong> </a> <strong class="jp ir"> <em class="iq">分布，我们可以预测和分析变量的行为，这在进一步研究数据时会很方便。</em> </strong></p><p id="02b3" class="jn jo mn jp b jq jr js jt ju jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj kk ij bi translated"><strong class="jp ir">注:变量的频率分布或直方图通常用于将变量拟合为标准分布。</strong></p></blockquote></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="3949" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我想请大家注意功能选择。在这里，我将阐明决定保留哪些特性的方法。我将使用 Python 的 Sci-kit 学习模块中的 SelectKBest 模块来完成这项工作。他们使用 f 检验估计从数据集中找到最佳特征。关于模块和理论的更多信息可以在参考资料中找到。</p><pre class="km kn ko kp gt mb mc md me aw mf bi"><span id="4ce7" class="mg mh iq mc b gy mi mj l mk ml">from sklearn.feature_selection import SelectKBest, f_regression<br/>#Assuming X and Y are numpy array of dimension (m,9) and (m,1) #respectively<br/>#We use f_regression function since our's is a regression problem<br/>X_best = SelectKBest(f_regression, k=5).fit_transform(X,Y)<br/>'''<br/>After the code snippet is run, X_best keeps top k=5 features from X.<br/>'''</span></pre><p id="dc90" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我将使用线性回归向您展示最佳“k”特征对模型的“<a class="ae lb" href="http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score" rel="noopener ugc nofollow" target="_blank"> R2 分数</a>”指标的影响。这里你可以看到，通过增加“k”的值，我们的线性回归模型得到了更好的 R2 分数。我还存储了与'<em class="mn"> SelectKBest </em>'方法相关的功能的降序。同样的代码可以在我上面的 github 链接中找到。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/598ed8a97419945f3fd384421eeabd37.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*-hcftR6msaYGZrB7lmJMQg.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">‘k’ vs ‘R2 Score ‘ plot on linear regression model (‘k’ determines the <strong class="bd mx">top</strong> <strong class="bd mx">k features from the data set</strong>)</figcaption></figure><p id="44e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这样，我们可以选择一个合适的“k”值作为模型的 R2 分数的相应值，这对于我们的问题陈述<em class="mn">来说是可接受的(我可能会选择 k=5，但对于您来说，可能会根据问题的需求而有所不同)。<br/>以下是按降序排列的‘k’个特性列表:</em>【F3，F4，F2，F9，F6，F1，F5，F7，F8】</p><blockquote class="mp mq mr"><p id="9eba" class="jn jo mn jp b jq jr js jt ju jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj kk ij bi translated"><strong class="jp ir">特征选择的主要应用之一是降低数据的维数，以获得更好的计算性能。</strong></p></blockquote><p id="fb04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析</a>是将维度(特征)分解成更小维度空间的另一种方法。使用主成分分析的主要好处是:</p><ul class=""><li id="103a" class="my mz iq jp b jq jr ju jv jy na kc nb kg nc kk nd ne nf ng bi translated"><strong class="jp ir"> <em class="mn">降维</em> </strong></li><li id="d886" class="my mz iq jp b jq nh ju ni jy nj kc nk kg nl kk nd ne nf ng bi translated"><strong class="jp ir"> <em class="mn">将高维空间数据转化为低维空间数据进行可视化</em> </strong></li></ul><p id="024a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是将高维空间分解为低维空间后的数据可视化(对于“X”)。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/23438f8e77f1795c1031a6e2f308ada8.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*At-GBPcA39ZfmKONPEyd5g.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Visualizing ‘X(1-D space)’ vs ‘Y’ after decomposing X into lower single dimension space</figcaption></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/c410c711d5f4ae5dba07781ec07edc95.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*Hrvb5SZIG6dzMgthBhXS2g.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Visualizing ‘X(2-D space)’ vs ‘Y’ after decomposing X into lower 2 dimension space</figcaption></figure><p id="4af9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我将向您展示使用线性回归将数据投影到“p”分量特征空间对模型的“<a class="ae lb" href="http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score" rel="noopener ugc nofollow" target="_blank"> R2 分数</a>”度量的影响。这里你可以看到，通过增加“p”的值，我们可以得到更好的线性回归模型的 R2 分数。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/d8deb0df48f06e455e1ac7c254fd5559.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*ja6emtSaEO7o6IPosOWDxw.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">‘p’ vs ‘R2 Score ‘ plot on linear regression model (‘p’ determines the <strong class="bd mx">top</strong> <strong class="bd mx">p components</strong> <strong class="bd mx">from the eigen vector projection of data set</strong>)</figcaption></figure><p id="1dce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">查看图表，我们可以为我们的解决方案选择一个合适的“p”值。根据问题的要求，应通过相应的可接受的 R2 评分值来选择“p”的值。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="ab67" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我将讨论数据集大小的影响，<strong class="jp ir"><em class="mn">【m】=数据集</em> </strong>中的观察次数。我保留了最初出现在数据集中的 9 个特征，我改变了“m”，在训练集<em class="mn"> (80%) </em>上训练，并使用线性回归模型计算了测试集<em class="mn"> (20%) </em>上每个“m”的 R2 分数。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/367b317115e543603a5d8af424dd516e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*P54lK-n4TMJYlNWVz2xKNQ.png"/></div></figure><p id="1f39" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">分析上面的图表，我们可以推断出以下几点:</p><ul class=""><li id="7f82" class="my mz iq jp b jq jr ju jv jy na kc nb kg nc kk nd ne nf ng bi translated">R2 分数的最初峰值是由于较低的“m”值造成的数据偏差。</li><li id="9c41" class="my mz iq jp b jq nh ju ni jy nj kc nk kg nl kk nd ne nf ng bi translated">在尖峰之后，R2 分数在某种程度上饱和(忽略中间的尖峰，这是因为在分裂成‘m’’时随机选择数据点)，这意味着逐渐增加数据集大小，最终增加 R2 分数，直到饱和。</li><li id="0d3a" class="my mz iq jp b jq nh ju ni jy nj kc nk kg nl kk nd ne nf ng bi translated">在某一点之后，它饱和，增加“m”，不影响 R2 分数。</li><li id="7220" class="my mz iq jp b jq nh ju ni jy nj kc nk kg nl kk nd ne nf ng bi translated">较低的“m”导致模型中的偏差或较低的 R2 分数。</li></ul></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="1cdc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi nm translated"><span class="l nn no np bm nq nr ns nt nu di"> D </span>设计机器学习模型包括许多因素，如选择特征、决定数据大小以获得更好的模型性能、计算成本、实现实时数据的可行性(对于某些情况)，以及根据问题的要求的许多其他因素。这总是对数据的探索性分析，这导致了更好的模型设计，因为</p><blockquote class="mp mq mr"><p id="a7b2" class="jn jo mn jp b jq jr js jt ju jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj kk ij bi translated">机器学习中的每个问题都是不同的</p></blockquote></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="808e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参考:</p><ol class=""><li id="5382" class="my mz iq jp b jq jr ju jv jy na kc nb kg nc kk nv ne nf ng bi translated">Sci-kit 模块:<a class="ae lb" href="http://scikit-learn.org" rel="noopener ugc nofollow" target="_blank">http://scikit-learn.org</a></li><li id="bd73" class="my mz iq jp b jq nh ju ni jy nj kc nk kg nl kk nv ne nf ng bi translated">SelectKBest 模块:<a class="ae lb" href="http://scikit-learn.org/stable/modules/feature_selection.html" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/modules/feature _ selection . html</a></li><li id="b9a7" class="my mz iq jp b jq nh ju ni jy nj kc nk kg nl kk nv ne nf ng bi translated">熊猫模块:<a class="ae lb" href="https://pandas.pydata.org/pandas-docs/stable/" rel="noopener ugc nofollow" target="_blank">https://pandas.pydata.org/pandas-docs/stable/</a></li><li id="c2db" class="my mz iq jp b jq nh ju ni jy nj kc nk kg nl kk nv ne nf ng bi translated">用于可视化的 Seaborn 模块:<a class="ae lb" href="https://seaborn.pydata.org/" rel="noopener ugc nofollow" target="_blank">https://seaborn.pydata.org/</a></li><li id="2882" class="my mz iq jp b jq nh ju ni jy nj kc nk kg nl kk nv ne nf ng bi translated">主成分分析:<a class="ae lb" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Principal_component_analysis</a></li></ol></div></div>    
</body>
</html>