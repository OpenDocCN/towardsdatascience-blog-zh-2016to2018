<html>
<head>
<title>Paper repro: Deep Metalearning using “MAML” and “Reptile”</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文复制:使用“MAML”和“爬行动物”进行深度元学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-repro-deep-metalearning-using-maml-and-reptile-fd1df1cc81b0?source=collection_archive---------1-----------------------#2018-04-02">https://towardsdatascience.com/paper-repro-deep-metalearning-using-maml-and-reptile-fd1df1cc81b0?source=collection_archive---------1-----------------------#2018-04-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/01f8dcb2295fcb29af12e76475e13806.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w1EJekcD7P79uko04NJBAA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Source: <a class="ae kc" href="https://imgur.com/CuE2jCg" rel="noopener ugc nofollow" target="_blank">imgur</a></figcaption></figure><p id="3031" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我复制了最近在元学习领域的两篇论文:<a class="ae kc" href="https://arxiv.org/abs/1703.03400" rel="noopener ugc nofollow" target="_blank"> MAML </a>和类似的<a class="ae kc" href="https://arxiv.org/abs/1803.02999" rel="noopener ugc nofollow" target="_blank">爬行动物</a>。这个复制的完整笔记本可以在<a class="ae kc" href="https://github.com/AdrienLE/ANIML/blob/master/ANIML.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="f50f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这两篇论文的目标都是解决<em class="lb"> K-shot </em>学习问题。在K-shot学习中，我们需要训练一个神经网络来基于非常少量的例子(通常在10个左右的数量级)进行归纳，而不是我们在ImageNet等数据集中看到的通常成千上万的例子。</p><p id="9b3a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，在为K-shot学习做准备时，你可以就许多类似的K-shot问题进行训练，以学习仅基于K个示例进行归纳的最佳方式。</p><p id="64b4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是<em class="lb">学学</em>或<strong class="kf ir">元学</strong>。我们已经在我的文章“通过梯度下降学习学习”中看到了元学习，你可以在这里找到:</p><div class="lc ld gp gr le lf"><a href="https://becominghuman.ai/paper-repro-learning-to-learn-by-gradient-descent-by-gradient-descent-6e504cc1c0de" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab fo"><div class="lh ab li cl cj lj"><h2 class="bd ir gy z fp lk fr fs ll fu fw ip bi translated">论文报告:“通过梯度下降学习”</h2><div class="lm l"><h3 class="bd b gy z fp lk fr fs ll fu fw dk translated">这篇文章是一系列深度学习论文复制文章的第一篇。</h3></div><div class="ln l"><p class="bd b dl z fp lk fr fs ll fu fw dk translated">becominghuman.ai</p></div></div><div class="lo l"><div class="lp l lq lr ls lo lt jw lf"/></div></div></a></div><p id="8a94" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">爬行动物和MAML的元学习方法都是为神经网络提出一个<strong class="kf ir">初始化</strong>，这个初始化很容易推广到类似的任务。这与“通过梯度下降学习”不同，在“梯度下降学习”中，我们不是在学习初始化，而是在学习优化器。</p><h1 id="1431" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">迁移学习</h1><p id="c244" class="pw-post-body-paragraph kd ke iq kf b kg ms ki kj kk mt km kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">这种方法非常类似于<strong class="kf ir">迁移学习</strong>，其中我们在比如说ImageNet上训练一个网络，后来证明，微调这个网络可以很容易地学习另一个数据少得多的图像数据集。事实上，迁移学习可以被视为元学习的一种形式。事实上，它可以用来从非常小的数据集学习，正如你在这里看到的<a class="ae kc" rel="noopener" target="_blank" href="/fun-with-small-image-data-sets-8c83d95d0159"/>。</p><p id="dbd0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的区别在于，最初的网络是以易于推广为明确目的进行训练的，而迁移学习只是“偶然”发生作用，因此可能不会最佳地发挥作用。</p><p id="d281" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事实上，很容易找到迁移学习无法学习到好的初始化的例子。为此我们需要看一下<em class="lb"> 1D正弦波回归问题</em>。</p><p id="3194" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个K-shot问题中，每个任务包括学习一个修改的正弦函数。具体来说，对于每个任务，底层函数的形式将是<em class="lb"> y = a sin(x + b) </em>，随机选择<em class="lb"> a </em>和<em class="lb"> b </em>，我们的神经网络的目标是学习仅基于10 (x，y)对找到给定的<em class="lb"> x </em>的<em class="lb"> y </em>。</p><p id="5718" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们绘制几个正弦波任务示例:</p><figure class="mx my mz na gt jr"><div class="bz fp l di"><div class="nb nc l"/></div></figure><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/8bac998dda9dd4f8a6fd4b6e98b1538a.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*qCPnRIMiRD_AXwsxfojSMg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">3 random tasks</figcaption></figure><p id="a50f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了理解为什么这将是迁移学习的一个问题，让我们画出其中的1000个:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/3491b366f5f96ad53e6bdfefe18930e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*K695r6AlOPjqyffOgpLGOQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">1,000 random tasks</figcaption></figure><p id="406b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">至少可以说，看起来每个x值都有很多重叠…</p><p id="4c0d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于跨多个任务的每个x有多个可能的值，如果我们训练单个神经网络同时处理多个任务，它的最佳选择将只是返回每个x跨所有任务的平均y值。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/5f5f286774037e1b55a113de23d6894b.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*jvkJBTVWFSJmClcIWyZ70w.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Average value for each x, with random task shown for scale</figcaption></figure><p id="a66c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">平均值基本上是0，这意味着一个经过大量任务训练的神经网络在任何地方都会返回0！不清楚这是否真的会有很大帮助，然而这个<strong class="kf ir">是</strong>在这种情况下的迁移学习方法…</p><p id="e090" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们通过实际实现一个简单的模型来解决这些正弦波任务，并使用迁移学习来训练它，来看看它做得有多好。首先，模型本身:</p><figure class="mx my mz na gt jr"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="34a6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你会注意到它是以一种奇怪的方式实现的(什么是“可修改模块”？什么是“梯度线性”？).这是因为我们稍后将使用MAML来训练它。关于这些类的细节，请查看笔记本，但是现在你可以假设它们类似于nn。模和线性神经网络。</p><p id="050f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们按顺序对它进行一系列不同的随机任务的训练:</p><figure class="mx my mz na gt jr"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="4eca" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是当我们试图将这个转移模型微调到一个特定随机任务时发生的情况:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/2b6ecb8f3c1eb8addd66af2d89f2c65b.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*mpzVEQR9uNUoII03fPQ4sA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Transfer learning on a specific random task</figcaption></figure><p id="8104" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基本上，看起来我们的传递模型学习了一个常量函数，很难将它微调到更好的状态。甚至不清楚我们的迁移学习是否比随机初始化更好……的确不是！随着时间的推移，随机初始化最终会比微调我们的传输模型获得更好的损失。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/cbd913352fba8465a33eb0342f2d1fec.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*nXLS1Opnk2nt-gSQW3LUvA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Learning curve for transfer learning vs random initialization</figcaption></figure><h1 id="3887" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">MAML</h1><p id="3cf4" class="pw-post-body-paragraph kd ke iq kf b kg ms ki kj kk mt km kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">我们现在来看MAML，这是我们今天要看的两个算法中的第一个。</p><p id="a55a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如前所述，我们试图找到一组权重，以便在类似的任务上运行梯度下降可以尽可能快地取得进展。MAML通过运行梯度下降的一次迭代，然后根据一次迭代朝着真正的任务前进了多少来更新初始权重，非常严格地执行了这个<em class="lb">。更具体地说，它:</em></p><ul class=""><li id="9d6f" class="nh ni iq kf b kg kh kk kl ko nj ks nk kw nl la nm nn no np bi translated">创建初始化权重的副本</li><li id="dd79" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">对副本上的随机任务运行梯度下降迭代</li><li id="88c8" class="nh ni iq kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">通过梯度下降的迭代反向传播在<em class="lb">测试</em>集合上的损失，并返回到初始权重，以便我们可以在它们更容易更新的方向上更新初始权重。</li></ul><p id="7aa6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此我们需要一个梯度的梯度，也就是这个过程中的二阶导数。幸运的是，这是PyTorch现在支持的东西，不幸的是，PyTorch使更新模型的参数变得有点笨拙，我们仍然可以通过它们运行梯度下降(我们已经看到这是“通过梯度下降学习学习”)，这解释了模型编写的奇怪方式。</p><p id="ab5c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为我们将使用二阶导数，我们需要确保允许我们计算原始梯度的计算图保持不变，这就是为什么我们将<code class="fe nv nw nx ny b">create_graph=True</code>传递给<code class="fe nv nw nx ny b">.backward()</code>。</p><figure class="mx my mz na gt jr"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="3237" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么它是如何在特定的随机任务中发挥作用的呢？</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/299e4c57c9730ee8e4da209ba8514532.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*7I7P9ZCqftQLigyjZylhgg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Training on a random function using MAML</figcaption></figure><p id="7c62" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">哇，那好多了，甚至在梯度下降一步后，正弦形状开始可见，10步后，波的中心几乎完全正确。这是否反映在学习曲线上？是啊！</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/263a0a25284754e535c6f6418347a86f.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*K2GRwQ-Rzb6uF20AGiS4-Q.png"/></div></figure><p id="5c96" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，有点烦人的是，我们必须为此使用二阶导数…这迫使代码变得复杂，也使事情变得相当慢(根据论文，大约33%，这与我们将在这里看到的相符)。</p><p id="90c2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有没有不使用二阶导数的MAML近似值？当然啦！我们可以简单地假设，我们用于内部梯度下降的梯度只是凭空出现，因此只是改善初始参数，而不考虑这些二阶导数。让我们为MAML训练函数添加一个一阶参数来处理这个问题:</p><figure class="mx my mz na gt jr"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="edc6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么这个一阶近似有多好呢？事实证明，它几乎和原来的MAML一样好，而且确实快了大约33%。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/a5e8d1b0df710f99cebfaead6d9bd02d.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*U3c1UY1yvzvV7MITYoKOzA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Learning curve of MAML vs MAML first order</figcaption></figure><h1 id="7cf5" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">爬行动物</h1><p id="8df1" class="pw-post-body-paragraph kd ke iq kf b kg ms ki kj kk mt km kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">MAML的一阶近似告诉我们，一些有趣的事情正在发生:毕竟，梯度是如何产生的似乎应该与一个好的初始化相关，然而它显然不是那么多。</p><p id="0d27" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">爬虫进一步发展了这个想法，告诉我们做以下事情:在一个给定的任务上对<strong class="kf ir">运行几次</strong>的SGD迭代，然后在SGD的k次迭代后，将你的初始化权重向你获得的权重方向移动一点。一个非常简单的算法，只需要几行伪代码:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0fd532e6bca2a7409c2dcc97d2402645.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*WmG_R90SyXjWwu6qPfwtQQ.png"/></div></figure><p id="454c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我第一次读到这里的时候，我很紧张:这不就和在每项任务中交替训练你的重量一样吗，就像在迁移学习中一样？这怎么可能行得通呢？</p><p id="e1c3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事实上，爬行动物的论文预见到了这种反应:</p><blockquote class="ob oc od"><p id="5b2f" class="kd ke lb kf b kg kh ki kj kk kl km kn oe kp kq kr of kt ku kv og kx ky kz la ij bi translated"><em class="iq">你可能会想“这不就和训练预期损失</em> Eτ [Lτ] <em class="iq">一样吗？”然后检查日期是否是4月1日。</em></p></blockquote><p id="8ca9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">碰巧的是，我在4月2日写这篇文章，所以这一切都是严肃的。这是怎么回事？</p><p id="7ba2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事实上，如果我们运行SGD进行单次迭代，我们会得到与上述迁移学习相同的东西，但我们没有，我们使用了几次迭代，因此我们每次更新的权重实际上间接取决于损失的二阶导数，类似于MAML。</p><p id="3581" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好吧，但是，为什么会这样呢？爬行动物为此提供了一个令人信服的直觉:对于每项任务，都有最优的权重。事实上，可能有<strong class="kf ir">许多</strong>组权重是最优的。这意味着，如果您承担几项任务，那么应该有一组权重，对于每项任务，这些权重与至少一组最佳权重之间的距离是最小的。这组权重是我们想要初始化我们的网络的地方，因为它可能是对任何任务来说达到最优所必需的最少工作的权重。这是爬行动物发现的一组砝码。</p><p id="483f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以在下图中直观地看到这一点:两条黑线代表两个不同任务的最优权重集，而灰线代表初始化权重。爬虫试图使初始化权重越来越接近最佳权重彼此最接近的点。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/d7ec76ce9f68260641f08f5b0690874a.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*GpVv4Ebb93OwFQdGb18dEg.png"/></div></figure><p id="f0e8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们实现爬行动物，并将其与MAML进行比较:</p><figure class="mx my mz na gt jr"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="c15f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它如何看待一个随机问题？漂亮:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/b93394d7398c0f340cc148c082a4d5db.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*rBDd3TbL_OjlGtRY01YuDw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Reptile performance on a random problem</figcaption></figure><p id="54ba" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">学习曲线呢？</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c0a5034da0327946f4e1853ed4f9c532.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*27r-rpH_Yg3ebBRWXW6YdQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Learning curve of Reptile, MAML and MAML first-order</figcaption></figure><p id="fd64" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看起来爬虫确实用一个更简单和更快的算法实现了和MAML相似甚至更好的性能！</p><p id="fc1e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有这些都适用于更多的问题，而不仅仅是这个正弦波的玩具例子。要了解更多细节，我真的建议你阅读报纸。在这一点上，你应该有足够的背景来理解他们相当容易。</p><p id="c39e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">未来真正有趣的是将这些方法不仅应用于K-shot学习问题，还应用于更大的问题:对于基于中等规模数据集(几百或几千个，而不是K-shot学习中常见的大约10个)的训练模型，迁移学习在图像分类领域非常成功。使用爬行动物训练resnet网络会产生比我们现有的模型更适合迁移学习的东西吗？</p></div></div>    
</body>
</html>