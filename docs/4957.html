<html>
<head>
<title>Multi-Class Text Classification with Doc2Vec &amp; Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 Doc2Vec 和 Logistic 回归的多类文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4?source=collection_archive---------1-----------------------#2018-09-18">https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4?source=collection_archive---------1-----------------------#2018-09-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/ed5bd84a02574c1bd040a9df7358b5c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lAu0XNZ0LWqQQAxZY5jypA.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo credit: Pexels</figcaption></figure><div class=""/><div class=""><h2 id="7d07" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">目标是使用 Doc2Vec 和逻辑回归将消费者金融投诉分为 12 个预定义的类别</h2></div><p id="5243" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><a class="ae lq" href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" rel="noopener ugc nofollow" target="_blank"> Doc2vec </a>是一个<a class="ae lq" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"> NLP </a>工具，用于将文档表示为一个向量，并且是<a class="ae lq" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec </a>方法的推广。</p><p id="1bf3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为了理解 doc2vec，最好理解 word2vec 方法。然而，完整的数学细节超出了本文的范围。如果您不熟悉 word2vec 和 doc2vec，以下资源可以帮助您入门:</p><ul class=""><li id="f591" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated"><a class="ae lq" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank">单词和短语的分布式表示及其组合性</a></li><li id="b118" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" rel="noopener ugc nofollow" target="_blank">句子和文档的分布式表示</a></li><li id="f97a" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" href="https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e" rel="noopener">对 Doc2Vec 的温和介绍</a></li><li id="29ad" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" href="https://github.com/RaRe-Technologies/gensim/blob/3c3506d51a2caf6b890de3b1b32a8b85f7566ca5/docs/notebooks/doc2vec-IMDB.ipynb" rel="noopener ugc nofollow" target="_blank">关于 IMDB 情感数据集的 Gensim Doc2Vec 教程</a></li><li id="6d76" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" href="https://github.com/RaRe-Technologies/movie-plots-by-genre/blob/master/ipynb_with_output/Document%20classification%20with%20word%20embeddings%20tutorial%20-%20with%20output.ipynb" rel="noopener ugc nofollow" target="_blank">带单词嵌入的文档分类教程</a></li></ul><p id="5856" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">当我们使用 Scikit-Learn 进行<a class="ae lq" rel="noopener" target="_blank" href="/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f">多类文本分类时，使用相同的数据集，在本文中，我们将使用</a><a class="ae lq" href="https://radimrehurek.com/gensim/models/doc2vec.html" rel="noopener ugc nofollow" target="_blank"> Gensim </a>中的 doc2vec 技术按产品对投诉叙述进行分类。我们开始吧！</p><h2 id="ebbb" class="mf mg jf bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">数据</h2><p id="920e" class="pw-post-body-paragraph ku kv jf kw b kx my kg kz la mz kj lc ld na lf lg lh nb lj lk ll nc ln lo lp ij bi translated">目标是将消费者金融投诉分为 12 个预定义的类别。数据可以从 data.gov 下载。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="905b" class="mf mg jf ni b gy nm nn l no np">import pandas as pd<br/>import numpy as np<br/>from tqdm import tqdm<br/>tqdm.pandas(desc="progress-bar")<br/>from gensim.models import Doc2Vec<br/>from sklearn import utils<br/>from sklearn.model_selection import train_test_split<br/>import gensim<br/>from sklearn.linear_model import LogisticRegression<br/>from gensim.models.doc2vec import TaggedDocument<br/>import re<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</span><span id="87b9" class="mf mg jf ni b gy nq nn l no np">df = pd.read_csv('Consumer_Complaints.csv')<br/>df = df[['Consumer complaint narrative','Product']]<br/>df = df[pd.notnull(df['Consumer complaint narrative'])]<br/>df.rename(columns = {'Consumer complaint narrative':'narrative'}, inplace = True)<br/>df.head(10)</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/719e538cf23580817a2ea7328f78f938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zf7BrYaNGLnvaafd14ohJA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 1</figcaption></figure><p id="579a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在删除叙述列中的空值后，我们将需要重新索引数据框。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="1608" class="mf mg jf ni b gy nm nn l no np">df.shape</span></pre><p id="f1de" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="ns"> (318718，2) </em> </strong></p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="6584" class="mf mg jf ni b gy nm nn l no np">df.index = range(318718)</span><span id="c137" class="mf mg jf ni b gy nq nn l no np">df['narrative'].apply(lambda x: len(x.split(' '))).sum()</span></pre><p id="6b51" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"><em class="ns">63420212</em>T29】</strong></p><p id="350d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们有超过 6300 万个单词，这是一个相对较大的数据集。</p><h2 id="1230" class="mf mg jf bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">探索</h2><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="443e" class="mf mg jf ni b gy nm nn l no np">cnt_pro = df['Product'].value_counts()</span><span id="71e7" class="mf mg jf ni b gy nq nn l no np">plt.figure(figsize=(12,4))<br/>sns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)<br/>plt.ylabel('Number of Occurrences', fontsize=12)<br/>plt.xlabel('Product', fontsize=12)<br/>plt.xticks(rotation=90)<br/>plt.show();</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/9c09e6de83569a1eb167ad0e8faa04c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eGuGyh2Xj2rRzxyMr_WBaQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 2</figcaption></figure><p id="76ed" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这些分类是不平衡的，然而，一个简单的分类器预测所有的事情都是债务收集只会达到超过 20%的准确率。</p><p id="e1b5" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我们看几个投诉叙述及其相关产品的例子。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="91a7" class="mf mg jf ni b gy nm nn l no np">def print_complaint(index):<br/>    example = df[df.index == index][['narrative', 'Product']].values[0]<br/>    if len(example) &gt; 0:<br/>        print(example[0])<br/>        print('Product:', example[1])</span><span id="cb6e" class="mf mg jf ni b gy nq nn l no np">print_complaint(12)</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/72e098535ed0f61e7c659c1d1e5782a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JVDETkCbrBCC-uAa-Xw9pQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 3</figcaption></figure><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="864c" class="mf mg jf ni b gy nm nn l no np">print_complaint(20)</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/12e4bbc3706fa51e918003d19694c0b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TcnGV6v_T1ZGLLIr_w9sXQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 4</figcaption></figure><h2 id="3a4a" class="mf mg jf bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">文本预处理</h2><p id="99c7" class="pw-post-body-paragraph ku kv jf kw b kx my kg kz la mz kj lc ld na lf lg lh nb lj lk ll nc ln lo lp ij bi translated">下面我们定义一个函数来将文本转换成小写，并从单词中去掉标点/符号等等。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="4269" class="mf mg jf ni b gy nm nn l no np">from bs4 import BeautifulSoup<br/>def cleanText(text):<br/>    text = BeautifulSoup(text, "lxml").text<br/>    text = re.sub(r'\|\|\|', r' ', text) <br/>    text = re.sub(r'http\S+', r'&lt;URL&gt;', text)<br/>    text = text.lower()<br/>    text = text.replace('x', '')<br/>    return text<br/>df['narrative'] = df['narrative'].apply(cleanText)</span></pre><p id="24ac" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">以下步骤包括 70/30 的训练/测试分割，删除停用词并使用<a class="ae lq" href="https://www.nltk.org/api/nltk.tokenize.html" rel="noopener ugc nofollow" target="_blank"> NLTK 标记器</a>标记文本。在我们的第一次尝试中，我们将每个投诉叙述都贴上了产品标签。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="8b4b" class="mf mg jf ni b gy nm nn l no np">train, test = train_test_split(df, test_size=0.3, random_state=42)</span><span id="21a3" class="mf mg jf ni b gy nq nn l no np">import nltk<br/>from nltk.corpus import stopwords<br/>def tokenize_text(text):<br/>    tokens = []<br/>    for sent in nltk.sent_tokenize(text):<br/>        for word in nltk.word_tokenize(sent):<br/>            if len(word) &lt; 2:<br/>                continue<br/>            tokens.append(word.lower())<br/>    return tokens</span><span id="020a" class="mf mg jf ni b gy nq nn l no np">train_tagged = train.apply(<br/>    lambda r: TaggedDocument(words=tokenize_text(r['narrative']), tags=[r.Product]), axis=1)<br/>test_tagged = test.apply(<br/>    lambda r: TaggedDocument(words=tokenize_text(r['narrative']), tags=[r.Product]), axis=1)</span></pre><p id="85d0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这就是培训条目的样子——一个带有“信用报告”标签的投诉叙述示例。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="2bb2" class="mf mg jf ni b gy nm nn l no np">train_tagged.values[30]</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/199e5f04056620e0294486e567dba729.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pRQEVAaY6xhU6wTQC0105w.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 5</figcaption></figure><h2 id="b47b" class="mf mg jf bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">建立 Doc2Vec 培训和评估模型</h2><p id="c5fc" class="pw-post-body-paragraph ku kv jf kw b kx my kg kz la mz kj lc ld na lf lg lh nb lj lk ll nc ln lo lp ij bi translated">首先，我们实例化一个 doc2vec 模型——分布式单词包(DBOW)。在 word2vec 架构中，两个算法名分别是“连续字包”(CBOW)和“skip-gram”(SG)；在 doc2vec 架构中，对应的算法是“分布式内存”(DM)和“分布式单词包”(DBOW)。</p><h2 id="3c0b" class="mf mg jf bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">分布式单词包(DBOW)</h2><p id="8a64" class="pw-post-body-paragraph ku kv jf kw b kx my kg kz la mz kj lc ld na lf lg lh nb lj lk ll nc ln lo lp ij bi translated">DBOW 是 doc2vec 模型，类似于 word2vec 中的 Skip-gram 模型。段落向量是通过训练神经网络来获得的，该神经网络的任务是在给定从段落中随机采样的单词的情况下，预测段落中单词的概率分布。</p><p id="7a0f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们将改变以下参数:</p><ul class=""><li id="f4bc" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">如果<code class="fe nx ny nz ni b">dm=0</code>，使用分布式单词包(PV-DBOW)；如果<code class="fe nx ny nz ni b">dm=1</code>，则使用‘分布式内存’(PV-DM)。</li><li id="cbca" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">300 维特征向量。</li><li id="db6c" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><code class="fe nx ny nz ni b">min_count=2</code>，忽略总频率低于此的所有单词。</li><li id="e27a" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><code class="fe nx ny nz ni b">negative=5</code>，指定需要抽取多少个“干扰词”。</li><li id="be88" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><code class="fe nx ny nz ni b">hs=0</code>，且负数为非零，将使用负数采样。</li><li id="9b2e" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><code class="fe nx ny nz ni b">sample=0</code>，用于配置哪些高频词被随机下采样的阈值。</li><li id="c4e8" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">使用这些工作线程来训练模型。</li></ul><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="a33d" class="mf mg jf ni b gy nm nn l no np">import multiprocessing</span><span id="c7b6" class="mf mg jf ni b gy nq nn l no np">cores = multiprocessing.cpu_count()</span></pre><h2 id="0213" class="mf mg jf bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">积累词汇</h2><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="a7d4" class="mf mg jf ni b gy nm nn l no np">model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)<br/>model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/de05292d54c2cb3b2287f68e9dc8f4ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*de6U-F1k12N4DBtcGDKtUQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 6</figcaption></figure><p id="b7b1" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在 Gensim 中训练 doc2vec 模型相当简单，我们初始化模型并训练 30 个时期。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="821d" class="mf mg jf ni b gy nm nn l no np">%%time<br/>for epoch in range(30):<br/>    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)<br/>    model_dbow.alpha -= 0.002<br/>    model_dbow.min_alpha = model_dbow.alpha</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/257915ccc10b073e0bc339b7d241419d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SkkqPjdhslmn9gNMx8is-g.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 7</figcaption></figure><h2 id="6755" class="mf mg jf bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">为分类器构建最终矢量特征</h2><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="932d" class="mf mg jf ni b gy nm nn l no np">def vec_for_learning(model, tagged_docs):<br/>    sents = tagged_docs.values<br/>    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])<br/>    return targets, regressorsdef vec_for_learning(model, tagged_docs):<br/>    sents = tagged_docs.values<br/>    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])<br/>    return targets, regressors</span></pre><h2 id="31b5" class="mf mg jf bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">训练逻辑回归分类器。</h2><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="b208" class="mf mg jf ni b gy nm nn l no np">y_train, X_train = vec_for_learning(model_dbow, train_tagged)<br/>y_test, X_test = vec_for_learning(model_dbow, test_tagged)</span><span id="b3cf" class="mf mg jf ni b gy nq nn l no np">logreg = LogisticRegression(n_jobs=1, C=1e5)<br/>logreg.fit(X_train, y_train)<br/>y_pred = logreg.predict(X_test)</span><span id="3072" class="mf mg jf ni b gy nq nn l no np">from sklearn.metrics import accuracy_score, f1_score</span><span id="7d48" class="mf mg jf ni b gy nq nn l no np">print('Testing accuracy %s' % accuracy_score(y_test, y_pred))<br/>print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))</span></pre><p id="e21c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="ns">检测精度 0.6683609437751004 </em> </strong></p><p id="3d6b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="ns">测试 F1 分数:0.651646431211616 </em> </strong></p><h2 id="1e93" class="mf mg jf bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated"><strong class="ak">分布式内存(DM) </strong></h2><p id="2150" class="pw-post-body-paragraph ku kv jf kw b kx my kg kz la mz kj lc ld na lf lg lh nb lj lk ll nc ln lo lp ij bi translated">分布式记忆(DM)作为一种记忆，可以记住当前上下文中缺少的内容，或者段落的主题。虽然单词向量表示单词的概念，但是文档向量旨在表示文档的概念。我们再次实例化具有 300 个单词的向量大小的 Doc2Vec 模型，并且在训练语料库上迭代 30 次。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="b6ee" class="mf mg jf ni b gy nm nn l no np">model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)<br/>model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oc"><img src="../Images/f847a95bf873c287713a97c7aecc1afe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K2rvmA0Uq2I1Lahx9vRUGw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 8</figcaption></figure><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="76b7" class="mf mg jf ni b gy nm nn l no np">%%time<br/>for epoch in range(30):<br/>    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)<br/>    model_dmm.alpha -= 0.002<br/>    model_dmm.min_alpha = model_dmm.alpha</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/828dddea3a2b690c3ca18197c7f193b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qvb4Ang3vq3cCmQDefebag.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 9</figcaption></figure><h2 id="a8e8" class="mf mg jf bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">训练逻辑回归分类器</h2><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="1e0a" class="mf mg jf ni b gy nm nn l no np">y_train, X_train = vec_for_learning(model_dmm, train_tagged)<br/>y_test, X_test = vec_for_learning(model_dmm, test_tagged)</span><span id="5f13" class="mf mg jf ni b gy nq nn l no np">logreg.fit(X_train, y_train)<br/>y_pred = logreg.predict(X_test)</span><span id="e155" class="mf mg jf ni b gy nq nn l no np">print('Testing accuracy %s' % accuracy_score(y_test, y_pred))<br/>print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))</span></pre><p id="5822" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="ns">检测精度 0.47498326639892907 </em> </strong></p><p id="d35a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="ns">测试 F1 分数:0.4445833078167434 </em> </strong></p><h2 id="9eb7" class="mf mg jf bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">模型配对</h2><p id="3897" class="pw-post-body-paragraph ku kv jf kw b kx my kg kz la mz kj lc ld na lf lg lh nb lj lk ll nc ln lo lp ij bi translated">根据关于 IMDB 情感数据集的<a class="ae lq" href="https://github.com/RaRe-Technologies/gensim/blob/3c3506d51a2caf6b890de3b1b32a8b85f7566ca5/docs/notebooks/doc2vec-IMDB.ipynb" rel="noopener ugc nofollow" target="_blank"> Gensim doc2vec 教程，将来自分布式单词包(DBOW)和分布式内存(DM)的段落向量相结合可以提高性能。我们将随后将模型配对在一起进行评估。</a></p><p id="c6b9" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">首先，我们删除临时训练数据来释放 RAM。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="432a" class="mf mg jf ni b gy nm nn l no np">model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)<br/>model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)</span></pre><p id="f9b7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">连接两个模型。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="7e67" class="mf mg jf ni b gy nm nn l no np">from gensim.test.test_doc2vec import ConcatenatedDoc2Vec<br/>new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])</span></pre><p id="7411" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">构建特征向量。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="88c1" class="mf mg jf ni b gy nm nn l no np">def get_vectors(model, tagged_docs):<br/>    sents = tagged_docs.values<br/>    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])<br/>    return targets, regressors</span></pre><p id="3752" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">训练逻辑回归</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="b8ae" class="mf mg jf ni b gy nm nn l no np">y_train, X_train = get_vectors(new_model, train_tagged)<br/>y_test, X_test = get_vectors(new_model, test_tagged)</span><span id="9588" class="mf mg jf ni b gy nq nn l no np">logreg.fit(X_train, y_train)<br/>y_pred = logreg.predict(X_test)</span><span id="393b" class="mf mg jf ni b gy nq nn l no np">print('Testing accuracy %s' % accuracy_score(y_test, y_pred))<br/>print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))</span></pre><p id="0f76" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="ns">检测精度 0.6778572623828648 </em> </strong></p><p id="a00c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="ns">测试 F1 分数:0.664561533967402 </em> </strong></p><p id="c5c6" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">结果提高了 1%。</p><p id="a2f0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">对于本文，我使用训练集来训练 doc2vec，然而，在<a class="ae lq" href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb" rel="noopener ugc nofollow" target="_blank"> Gensim 的教程</a>中，整个数据集用于训练，我尝试了这种方法，使用整个数据集来训练 doc2vec 分类器，用于我们的消费者投诉分类，我能够达到 70%的准确率。你可以在这里找到那个<a class="ae lq" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Doc2Vec%20Consumer%20Complaint.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>，它的做法有点不同。</p><p id="c1b9" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">用于上述分析的<a class="ae lq" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Doc2Vec%20Consumer%20Complaint_3.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>可以在<a class="ae lq" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Doc2Vec%20Consumer%20Complaint_3.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。我期待听到任何问题。</p></div></div>    
</body>
</html>