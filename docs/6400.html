<html>
<head>
<title>Machine Learning Going Meta</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习走向元</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-going-meta-514dd5027817?source=collection_archive---------13-----------------------#2018-12-11">https://towardsdatascience.com/machine-learning-going-meta-514dd5027817?source=collection_archive---------13-----------------------#2018-12-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1787" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">反思 2018 年 ML 的一些最大趋势</h2></div><p id="b86f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器学习有自己的三条法则:为了使用它，你需要三个必不可少的成分，即 1)标记的<strong class="kk iu">数据</strong>；2)一个<strong class="kk iu">模型</strong>架构你可以优化；以及 3)明确定义的<strong class="kk iu">目标</strong>函数。许多关于应用 ML 解决问题的讨论被简单地打断了，因为这三者中没有一个是可用的。达到这种神奇的结合越来越容易:“大数据”趋势使数据访问在行业中更加普遍，深度学习使找到适用于广泛问题的良好模型架构变得更加简单。有趣的是，大部分困难仍然存在于定义正确的目标:一个有商业意义的目标，提供足够的监督，并包含问题固有的所有目标，例如公平性、安全性或可解释性。</p><p id="37cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我今年观察到的主要趋势是，我们可能最终会打破这种“数据、模型、目标:选择三个”的局面，变成看起来更像是:<strong class="kk iu">数据、模型、目标:选择两个</strong>。许多人会说，真正的“一般智力”是当我们不需要这些成分中的任何成分时:没有标签，没有专门的学习架构，没有精细的目标，因此从三个减少到两个确实感觉是一些进步。</p><p id="49b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我最感兴趣的是我们打破这三条规则的方式:我们采用监督学习，并把它本身作为研究的主题。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/59c32933192a176ca6560391341c867a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WedW0DFumGwkuDbw"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Credit: <a class="ae lv" href="https://unsplash.com/@andremouton?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Andre Mouton</a> on <a class="ae lv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="fe9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本质上，我们在机器学习上正在<strong class="kk iu">走向元</strong>。</p><p id="2fd5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">拿<strong class="kk iu">模型</strong>架构来说:如果没有会怎么样？学着点！这是“学会学习”方法背后的基本理念，又名<strong class="kk iu"> AutoML </strong>方法。令人惊讶的是，计算机已经基本上自动化了我三年前的工作:那时我正努力为<a class="ae lv" href="https://ai.google/research/pubs/pub43022" rel="noopener ugc nofollow" target="_blank">谷歌网</a>找出最好的模型架构。<a class="ae lv" href="https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html" rel="noopener ugc nofollow" target="_blank">当今最好的</a><a class="ae lv" href="https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html" rel="noopener ugc nofollow" target="_blank"/><a class="ae lv" href="https://openreview.net/forum?id=ByQZjx-0-" rel="noopener ugc nofollow" target="_blank">架构</a><a class="ae lv" href="https://openreview.net/forum?id=r1Zi2Mb0-" rel="noopener ugc nofollow" target="_blank"/>都是由机器学习系统<a class="ae lv" href="https://arxiv.org/abs/1806.09055" rel="noopener ugc nofollow" target="_blank">自己</a>设计的，并且正在迅速<a class="ae lv" href="https://www.blog.google/products/google-cloud/cloud-automl-making-ai-accessible-every-business/" rel="noopener ugc nofollow" target="_blank">找到</a> <a class="ae lv" href="https://arxiv.org/abs/1807.11626" rel="noopener ugc nofollow" target="_blank">自己的</a> <a class="ae lv" href="https://arxiv.org/abs/1808.09830" rel="noopener ugc nofollow" target="_blank">方式</a>进入工业应用。这也适用于模型训练的所有其他方面，例如<a class="ae lv" href="https://arxiv.org/abs/1805.09501" rel="noopener ugc nofollow" target="_blank">数据增强</a>、<a class="ae lv" href="https://ai.google/research/pubs/pub46646" rel="noopener ugc nofollow" target="_blank">设备放置</a>和<a class="ae lv" href="https://arxiv.org/abs/1709.07417" rel="noopener ugc nofollow" target="_blank">优化器</a>。这条研究路线的大部分仍然依赖于人们作为构建模块提出的大型建筑动物园，这没关系——问问你的邻居 Redditor，如果没有相当数量的模因循环，这就不是“元”了。</p><p id="8a71" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">取标有<strong class="kk iu">的数据</strong>。如果你没有呢？大量的文献正在围绕<strong class="kk iu">少量/一次/零次</strong>学习而发展，其中目标标签只被观察几次，恰好一次，或者根本不被观察。正如你所猜测的，解决这些问题的一些最有前途的方法是关于“去元化”:<a class="ae lv" href="https://arxiv.org/abs/1703.03400" rel="noopener ugc nofollow" target="_blank">元学习</a>将重新学习一些东西的问题定义为要解决的实际学习问题，从而优化了“少数镜头”任务，而不是传统的大数据集问题设置。并且可以自然的扩展到<a class="ae lv" href="https://arxiv.org/abs/1810.02334" rel="noopener ugc nofollow" target="_blank">无监督</a> <a class="ae lv" href="https://arxiv.org/abs/1806.04640" rel="noopener ugc nofollow" target="_blank">设置</a>。</p><p id="227c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对我来说，元学习越来越像是对我们集体痴迷于强化学习的完美治疗，强化学习是通向终身学习的道路。它提供了一个不同但同样自然的问题框架，从找出如何随着时间的推移从单个例子中更好地学习的角度。我希望有一天，我们不再把这称为元学习，而只是简单地称为“学习”:使用非常稀疏的信号捕捉新奇体验的本质，同时利用一个人的整个存在作为如何更好地学习的背景，这才是真正的概括。</p><p id="eabd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，当您有数据和模型，但没有定义明确的<strong class="kk iu">目标</strong>函数时会发生什么？这通常是一种“当你看到它时你就知道”的情况:你想要一个好看的图像，或者你想要听起来不错的音频，但是写下这些概念的适当损失几乎是不可能的。这就是<a class="ae lv" href="https://en.wikipedia.org/wiki/Generative_adversarial_network" rel="noopener ugc nofollow" target="_blank">生成性对抗网络</a>完全改变了现状的地方，它拒绝回答什么是好的目标函数的问题，而是将问题交给各种有学问的批评家，试图区分好的解决方案和坏的解决方案。还有什么比训练一个机器学习系统来判断你的机器学习系统做得有多好更‘元’的呢？可以说，gan 并不特别新，但它们在过去几年里真的蓬勃发展，现在越来越多地找到进入(sur-) <a class="ae lv" href="https://en.wikipedia.org/wiki/Deepfake" rel="noopener ugc nofollow" target="_blank">现实世界的方式</a>。</p><p id="c2cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当然，这回避了一个问题，为什么还没有人发表题为“生成对立模型不可知的元神经架构搜索”的论文。来吧，我谅你也不敢。</p><p id="829b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们在元列车上:这也是对 ML 社区本身反思的一年。几十年来，我们一直默默无闻，研究的问题要么太小而不重要，要么太深奥而不为世人所关注，我们的社区终于发现了几乎所有其他工程学科长期以来都必须努力解决的问题:我们的工作确实很重要。我们做什么，我们如何做，为谁的利益而做，真的很重要，并能实质性地影响我们生活的世界的未来。这对我们的责任观念和道德观念意味着什么？我们能否帮助解决当今地球面临的一些重大问题——并且<a class="ae lv" href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/" rel="noopener ugc nofollow" target="_blank">确保</a> <a class="ae lv" href="https://sustainability.google/projects/announcement-100/" rel="noopener ugc nofollow" target="_blank">我们</a>不会成为问题的一部分？我们能否努力成为一个更加包容<a class="ae lv" href="https://neurips.cc/Conferences/2018/News?article=2118" rel="noopener ugc nofollow" target="_blank">和</a><a class="ae lv" href="https://ai.googleblog.com/2018/12/adding-diversity-to-images-with-open.html" rel="noopener ugc nofollow" target="_blank">多样化</a>的社区，更好地代表我们服务的用户？</p><p id="2a05" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我期待 2019 年，期待更多的学习，更多的自省，更多的对世界的积极影响。我个人很高兴看到明年会有更多的 ML 进入物理世界。我的一些同事似乎领先了一步。我等不及了。我们很可能需要所有我们能够聚集的“元”来真正取得进展:物理世界将 ML 三重问题带到了最前沿:访问数据，如何设计有效的实时模型，以及如何处理模糊且往往相互冲突的目标。</p></div></div>    
</body>
</html>