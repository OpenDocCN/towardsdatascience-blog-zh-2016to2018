<html>
<head>
<title>When Bayes, Ockham, and Shannon come together to define machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">当贝叶斯、奥卡姆和香农一起定义机器学习时</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/when-bayes-ockham-and-shannon-come-together-to-define-machine-learning-96422729a1ad?source=collection_archive---------1-----------------------#2018-09-08">https://towardsdatascience.com/when-bayes-ockham-and-shannon-come-together-to-define-machine-learning-96422729a1ad?source=collection_archive---------1-----------------------#2018-09-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="47d4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习基础</h2><div class=""/><div class=""><h2 id="da43" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一个美丽的想法，它将统计学、信息论和哲学的概念结合在一起，为机器学习奠定了基础。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a067e735a700c01881f312b695147213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ROtB1gTp3HUh00wXKA5NEg.png"/></div></div></figure><h2 id="d5ec" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">感谢</h2><p id="49a7" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated">感谢我在佐治亚理工学院的<a class="ae mr" href="https://www.omscs.gatech.edu/cs-7641-machine-learning" rel="noopener ugc nofollow" target="_blank"> CS7641 课程，在我的</a><a class="ae mr" href="https://pe.gatech.edu/degrees/analytics" rel="noopener ugc nofollow" target="_blank"> MS 分析项目</a>中，我发现了这个概念，并受到了写这个概念的启发。感谢<a class="ms mt ep" href="https://medium.com/u/a0bc63d95eb0?source=post_page-----96422729a1ad--------------------------------" rel="noopener" target="_blank">马修·梅奥</a>编辑并在<a class="ae mr" href="https://www.kdnuggets.com/2018/09/when-bayes-ockham-shannon-come-together-define-machine-learning.html" rel="noopener ugc nofollow" target="_blank"> KDnuggets </a>重新发布这篇文章。</p><h1 id="9525" class="mu le it bd lf mv mw mx li my mz na ll ki nb kj lp kl nc km lt ko nd kp lx ne bi translated">介绍</h1><p id="6f9b" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated">有点令人惊讶的是，在所有机器学习的热门词汇中，我们很少听到一个短语将统计学习、信息论和自然哲学的一些核心概念融合成一个三个词的组合。</p><p id="0e7f" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">此外，它不仅仅是一个为机器学习(ML)博士和理论家准备的晦涩而学究式的短语。对于任何有兴趣探索的人来说，它有一个精确和容易理解的含义，并且对于 ML 和数据科学的实践者来说，它有一个实际的回报。</p><p id="5173" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">我说的是<strong class="ma jd"> <em class="nk">最小描述长度</em> </strong>。你可能会想这到底是什么…</p><p id="c0ac" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">让我们把这些层剥开，看看它有多有用…</p><h1 id="e23f" class="mu le it bd lf mv mw mx li my mz na ll ki nb kj lp kl nc km lt ko nd kp lx ne bi translated">贝叶斯和他的定理</h1><h2 id="f7c5" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">谁是托马斯·贝叶斯？</h2><p id="51f1" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated">我们从(不是按时间顺序)托马斯·贝叶斯牧师<a class="ae mr" href="https://en.wikipedia.org/wiki/Thomas_Bayes" rel="noopener ugc nofollow" target="_blank">开始，顺便说一下，他从未发表过他关于如何进行统计推断的想法，但后来因同名定理而名垂千古。</a></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/eac3a0188068101e4bd58fc6b59275c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/1*fsYO1lD-THfyK2Mk_mxsmg.gif"/></div></figure><p id="43eb" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">那是 18 世纪下半叶，数学科学中还没有叫“概率论”的分支。它被简单地称为听起来相当奇怪的“<a class="ae mr" href="https://en.wikipedia.org/wiki/The_Doctrine_of_Chances" rel="noopener ugc nofollow" target="_blank"> <em class="nk">机会主义</em></a>”——以<a class="ae mr" href="https://en.wikipedia.org/wiki/Abraham_de_Moivre" rel="noopener ugc nofollow" target="_blank">亚伯拉罕·德·莫伊弗尔</a>的一本书命名。一篇名为《<a class="ae mr" href="http://rstl.royalsocietypublishing.org/content/53/370" rel="noopener ugc nofollow" target="_blank"> <em class="nk">解决机会主义</em> </a>中的一个问题的论文》的文章，最初由贝叶斯撰写，但由他的朋友<a class="ae mr" href="https://en.wikipedia.org/wiki/Richard_Price" rel="noopener ugc nofollow" target="_blank">理查德·普莱斯</a>编辑和修改，被宣读给皇家学会，并发表在 1763 年伦敦皇家学会的<em class="nk">哲学会刊上。在这篇文章中，Bayes 以一种相当频繁的方式描述了一个关于联合概率的简单定理，它导致了逆概率的计算，即 Bayes 定理。</em></p><h2 id="e7b8" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">两个阵营的故事</h2><p id="39bb" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated"><a class="ae mr" href="https://www.theregister.co.uk/2017/06/22/bayesian_vs_frequentist_ai/" rel="noopener ugc nofollow" target="_blank">从那以后，统计科学的两个敌对派别——贝叶斯主义者和频繁主义者——之间进行了多次斗争。但是为了本文的目的，让我们暂时忽略历史，把注意力集中在贝叶斯推理机制的简单解释上。关于这个话题的超级直观的介绍，请看布兰登·罗勒的伟大教程</a>。我会专注于方程式。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/93a45fb6639dc2d72009d4da61f295f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/1*zGhyddrw4dcXk1U6HZcpTQ.gif"/></div></figure><p id="8533" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">这实质上是告诉你在看到数据/证据(<em class="nk">可能性</em>)后更新你的信念(<em class="nk">先验概率</em>，并将更新后的信念度赋予术语<em class="nk">后验概率</em>。你可以从一个信念开始，但是每一个数据点都会加强或削弱这个信念，并且你一直在更新你的假设。</p><h2 id="dcc5" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">让我们来谈谈“假设”</h2><p id="436b" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated">听起来简单直观？太好了。</p><p id="7b8d" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">不过，我在这一段的最后一句中耍了一个花招。你注意到了吗？我把<strong class="ma jd">这个词溜进了<em class="nk">假设</em> </strong>。那不是正常的英语。那是正式的东西:-)</p><p id="b991" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">在统计推断的世界里，假设是一种信念。这是一种对过程的真实性质的信念(我们永远无法观察到)，这是随机变量产生的背后(我们可以观察或测量，尽管不是没有噪音)。在统计学中，一般定义为概率分布。但在机器学习的背景下，可以想到任何一组规则(或逻辑或过程)，我们相信，这些规则可以产生<em class="nk">示例</em>或训练数据，我们被赋予学习这一神秘过程的隐藏性质。</p><p id="a196" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">所以，让我们试着用不同的符号——属于数据科学的符号——来重铸贝叶斯定理。让我们用<strong class="ma jd"> <em class="nk"> D </em> </strong>来表示数据，用<strong class="ma jd"> <em class="nk"> h </em> </strong>来表示假设。这意味着在给定数据 的情况下，我们应用贝叶斯公式来尝试确定<strong class="ma jd"> <em class="nk">数据来自什么假设。我们将定理改写为，</em></strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/f94ffd08ae6ab8421664524c2a416023.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*n3LVJTMW1n64HTcVseWuig.png"/></div></figure><p id="fd09" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">现在，一般来说，我们有一个大的(通常是无限的)假设空间，也就是说，有许多假设可供选择。贝叶斯推理的本质是我们要检查数据，以最大化最有可能产生观察数据的一个假设的概率。我们主要是想确定<strong class="ma jd"> <em class="nk"> argmax </em> </strong>中的<strong class="ma jd"><em class="nk">P</em></strong>(<strong class="ma jd"><em class="nk">h</em></strong>|<strong class="ma jd"><em class="nk">D</em></strong>)<strong class="ma jd"/>即我们想知道对于哪个<strong class="ma jd"><em class="nk"/></strong>，观察到的<strong class="ma jd"> <em class="nk"> D </em> </strong>最有可能。为此，我们可以放心地把这个术语放在分母<em class="nk">P</em>(<strong class="ma jd"><em class="nk">D</em></strong>)中，因为它不依赖于假设。这个方案被称为一个相当拗口的名字<a class="ae mr" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank"> <strong class="ma jd">【最大后验概率(MAP)】</strong></a>。</p><p id="4383" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">现在，我们运用下面的数学技巧，</p><ul class=""><li id="f727" class="no np it ma b mb nf me ng lm nq lq nr lu ns mq nt nu nv nw bi translated">事实上，最大化对对数的作用与对原始函数的作用相似，即取对数不会改变最大化问题。</li><li id="e449" class="no np it ma b mb nx me ny lm nz lq oa lu ob mq nt nu nv nw bi translated">乘积的对数是各个对数的和</li><li id="2818" class="no np it ma b mb nx me ny lm nz lq oa lu ob mq nt nu nv nw bi translated">一个数量的最大化等价于负数量的最小化</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/c96b595ce4b07e8bd4b927e49cd42286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cYEdz-dk6m5SKPh10yFzRg.png"/></div></div></figure><p id="15ba" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated"><em class="nk">好奇者</em>和<em class="nk">好奇者……</em>那些负对数为 2 的术语看起来很熟悉……来自<strong class="ma jd">信息论</strong>！</p><p id="7b1c" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">克劳德·香农进入<strong class="ma jd"/>。发明信息时代的天才。</p><h1 id="441b" class="mu le it bd lf mv mw mx li my mz na ll ki nb kj lp kl nc km lt ko nd kp lx ne bi translated">香农和信息论</h1><p id="b88f" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated">将需要<a class="ae mr" href="https://www.amazon.com/Mind-Play-Shannon-Invented-Information/dp/1476766681" rel="noopener ugc nofollow" target="_blank">多卷</a>来描述克劳德·香农的天才和奇特人生，他几乎是单枪匹马奠定了信息论的基础，引领我们进入了现代高速通信和信息交换的时代。</p><p id="eb09" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">这是一本关于他的生活和工作的好书，</p><div class="od oe gp gr of og"><a href="https://www.amazon.com/dp/B01M5IJN1P/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd jd gy z fp ol fr fs om fu fw jc bi translated">游戏中的思维:克劳德·香农如何发明信息时代</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">一种思想在发挥作用:克劳德香农如何发明了信息时代。下载…</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">www.amazon.com</p></div></div><div class="op l"><div class="oq l or os ot op ou lb og"/></div></div></a></div><p id="01f8" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated"><a class="ae mr" href="https://en.wikipedia.org/wiki/A_Symbolic_Analysis_of_Relay_and_Switching_Circuits" rel="noopener ugc nofollow" target="_blank">香农的麻省理工学院硕士论文</a>电气工程被称为 20 世纪最重要的硕士论文:在这篇论文中，22 岁的香农展示了如何使用继电器和开关的电子电路实现 19 世纪数学家乔治·布尔的逻辑代数。数字计算机设计的最基本特征——将“真”和“假”以及“0”和“1”表示为打开或关闭的开关，以及使用电子逻辑门来做出决定和执行算术——可以追溯到香农论文中的见解。</p><p id="30f2" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">但这还不是他最大的成就。</p><p id="6b0c" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">1941 年，香农去了贝尔实验室，在那里他研究战争问题，包括密码学。他还致力于信息和通信背后的原创理论。1948 年，这项工作出现在贝尔实验室研究期刊上发表的<a class="ae mr" href="https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication" rel="noopener ugc nofollow" target="_blank">著名论文中。</a></p><h2 id="c747" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">熵和最小长度</h2><p id="bcae" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated">香农通过一个类似于物理学中定义热力学熵的公式<a class="ae mr" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="noopener ugc nofollow" target="_blank"> <strong class="ma jd">定义了一个来源产生的信息量——例如，一条消息中的信息量。用最基本的术语来说，香农的信息熵是对一条信息进行编码所需的二进制位数。并且对于概率为<strong class="ma jd"> <em class="nk"> p </em> </strong>的消息或事件，该消息的最有效(即紧凑)编码将需要<strong class="ma jd"> - <em class="nk"> log2(p) </em> </strong>比特。</strong></a></p><p id="5a88" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">这正是出现在从贝叶斯定理导出的<em class="nk">最大后验概率</em>表达式中的那些术语的本质！</p><p id="06bc" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">因此，我们可以说，在贝叶斯推理的世界里，最可能的假设依赖于两个引起长度感的术语——<strong class="ma jd">而不是最小长度。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/d1cae2389a5d5cd65719d93d317bdf35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JLmuzYrrPjv_nDw59qIeow.png"/></div></div></figure><blockquote class="ow ox oy"><p id="b3bd" class="ly lz nk ma b mb nf kd md me ng kg mg oz nh mi mj pa ni ml mm pb nj mo mp mq im bi translated">但是在这些术语中，长度的概念是什么呢？</p></blockquote><h1 id="93cb" class="mu le it bd lf mv mw mx li my mz na ll ki nb kj lp kl nc km lt ko nd kp lx ne bi translated">长度(h):奥卡姆剃刀</h1><p id="afc0" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated"><a class="ae mr" href="https://en.wikipedia.org/wiki/William_of_Ockham" rel="noopener ugc nofollow" target="_blank">奥卡姆的威廉</a> ( <em class="nk">约</em>1287–1347)是英国方济各会修士和<a class="ae mr" href="https://en.wikipedia.org/wiki/Theologian" rel="noopener ugc nofollow" target="_blank">神学家</a>，也是一位有影响力的中世纪<a class="ae mr" href="https://en.wikipedia.org/wiki/Philosopher" rel="noopener ugc nofollow" target="_blank">哲学家</a>。他作为一个伟大的逻辑学家的声望主要来自于他的格言，被称为<a class="ae mr" href="https://en.wikipedia.org/wiki/Occam%27s_razor" rel="noopener ugc nofollow" target="_blank">奥卡姆剃刀。术语<em class="nk">剃刀</em>指的是通过“剔除”不必要的假设或将两个相似的结论分开来区分两个假设。</a></p><p id="6b4c" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">归功于他的准确的话是:<em class="nk"> entia non sunt 被乘数 da praeter necessitem</em>(实体不得被乘以超过必要性)。用统计学的术语来说，这意味着我们必须努力用最简单的假设来解释所有令人满意的数据。</p><p id="7d88" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">类似的原则得到了其他杰出人士的响应。</p><p id="cb60" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">伊萨克·牛顿爵士:“<em class="nk">除了那些既真实又足以解释自然现象的原因之外，我们不会承认更多的原因。</em></p><p id="1b64" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">伯特兰·罗素:“<em class="nk">只要有可能，用已知实体的结构代替对未知实体的推断。</em></p><blockquote class="ow ox oy"><p id="8d46" class="ly lz nk ma b mb nf kd md me ng kg mg oz nh mi mj pa ni ml mm pb nj mo mp mq im bi translated"><strong class="ma jd">总是偏爱较短的假设</strong>。</p></blockquote><p id="3a02" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">需要举例说明什么是<strong class="ma jd"> <em class="nk">一个假设的长度</em> </strong>是多少？</p><p id="1e8d" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">以下哪个决策树的<em class="nk">长度小于</em>长度？<strong class="ma jd"> A </strong>还是<strong class="ma jd"> B </strong>？</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/6d4db8eb45bdc75dd64961b6dd3ea0e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ORjW6p5x2B6ovoMSSMM0lA.png"/></div></div></figure><p id="4fe4" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">即使没有一个假设的“长度”的精确定义，我确信你会认为左边(A)的树看起来<em class="nk">更小</em>或<em class="nk">更短</em>。当然，你是对的。因此，<em class="nk">更短的</em>假设是具有更少的自由参数，或者不太复杂的决策边界(对于分类问题)，或者这些<strong class="ma jd">属性的某种组合，这可以代表其简洁性。</strong></p><p id="2f08" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">事实上，奥卡姆剃刀的数学形式主义最终形成了一种叫做“<a class="ae mr" href="https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference" rel="noopener ugc nofollow" target="_blank"> <strong class="ma jd">索洛莫洛夫的归纳推理理论</strong> </a>”的东西，它被认为是通向人工一般智能(AGI)的<a class="ae mr" href="https://en.wikipedia.org/wiki/AIXI" rel="noopener ugc nofollow" target="_blank">基础垫脚石</a>。</p><p id="5d5c" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">著名的麻省理工学院物理学家和人工智能研究人员<a class="ae mr" href="https://space.mit.edu/home/tegmark/" rel="noopener ugc nofollow" target="_blank"> Max Tegmark </a>最近使用奥卡姆剃刀原理构建了被吹捧为“<a class="ae mr" href="https://www.technologyreview.com/2018/11/01/1895/an-ai-physicist-can-derive-the-natural-laws-of-imagined-universes/" rel="noopener ugc nofollow" target="_blank"> <strong class="ma jd">”的人工智能物理学家</strong> </a>”，这是一个无监督的学习代理，以<a class="ae mr" href="https://arxiv.org/abs/1810.10525" rel="noopener ugc nofollow" target="_blank">学习和理论操纵</a>为中心，能够简约地预测未来的两个方面(从过去的观察结果)以及这些预测准确的领域。</p><h2 id="b53b" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">那‘长度(D | h)’呢？</h2><p id="7f12" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated">它是给定假设的数据长度。那是什么意思？</p><p id="d055" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">直观上，与假设的正确性或表示力有关。在给定一个假设的情况下，它决定了数据可以“推断”得多好。<strong class="ma jd">如果假设很好地生成了数据，并且我们可以无误差地测量数据，那么我们根本不需要数据。</strong></p><p id="7513" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">想到<a class="ae mr" href="https://en.wikipedia.org/wiki/Newton%27s_laws_of_motion" rel="noopener ugc nofollow" target="_blank">牛顿运动定律</a>。</p><p id="f7b7" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">它们最初出现在<a class="ae mr" href="https://en.wikipedia.org/wiki/Philosophi%C3%A6_Naturalis_Principia_Mathematica" rel="noopener ugc nofollow" target="_blank"> <em class="nk">原理</em> </a>中时，背后并没有任何严谨的数学证明。它们不是定理。它们很像假设，基于对自然物体运动的观察。但是他们很好的描述了数据。因此，它们变成了物理定律。</p><p id="c6a7" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">这就是为什么你不需要保存和记忆，所有可能的加速度，作为物体所受力的函数。你只要相信紧致假设 aka 定律<strong class="ma jd"> <em class="nk"> F=ma </em> </strong>并且相信你需要的所有数字，都可以在必要的时候从中计算出来。这使得<strong class="ma jd"> <em class="nk">长度【D | h】</em></strong>确实很小。</p><p id="e795" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">但是如果数据偏离紧凑假设很多，那么你需要有一个关于这些偏离是什么的很长的描述，对它们可能的解释，等等。</p><blockquote class="ow ox oy"><p id="28f0" class="ly lz nk ma b mb nf kd md me ng kg mg oz nh mi mj pa ni ml mm pb nj mo mp mq im bi translated">因此，<strong class="ma jd"> <em class="it"> Length(D|h) </em> </strong>简洁地捕捉到了“<strong class="ma jd">数据与给定假设</strong>的吻合程度”这一概念。</p></blockquote><p id="1749" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">本质上，这是错误分类或错误率的概念。对于一个完美的假设，它很短，在极限情况下可能为零。不完全符合数据的假设往往很长。</p><blockquote class="ow ox oy"><p id="1596" class="ly lz nk ma b mb nf kd md me ng kg mg oz nh mi mj pa ni ml mm pb nj mo mp mq im bi translated">而且，这是一种权衡。</p></blockquote><p id="291b" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">如果你用奥卡姆剃刀剃掉你的假设，你可能会剩下一个简单的模型，一个不能适合所有数据的模型。因此，你必须提供更多的数据来增强信心。另一方面，如果您创建一个复杂(长)的假设，您可能能够很好地拟合您的训练数据，但这实际上可能不是正确的假设，因为它违反了具有小熵假设的 MAP 原则。</p><blockquote class="ow ox oy"><p id="48be" class="ly lz nk ma b mb nf kd md me ng kg mg oz nh mi mj pa ni ml mm pb nj mo mp mq im bi translated">听起来像是偏差-方差权衡？是的，还有那个:-)</p></blockquote><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/0e6fb62d65b5ee2ca11ea4134daf2726.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SsN5zK4_I9P5hXfwl-w2kw.png"/></div></div><figcaption class="pe pf gj gh gi pg ph bd b be z dk"><strong class="bd pi">Source</strong>: <a class="ae mr" href="https://www.reddit.com/r/mlclass/comments/mmlfu/a_nice_alternative_explanation_of_bias_and/" rel="noopener ugc nofollow" target="_blank">https://www.reddit.com/r/mlclass/comments/mmlfu/a_nice_alternative_explanation_of_bias_and/</a></figcaption></figure><h2 id="b68a" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">把所有的放在一起</h2><p id="0f41" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated">因此，贝叶斯推断告诉我们，<strong class="ma jd">最佳假设是使两项之和最小的假设:假设长度和错误率</strong>。</p><blockquote class="ow ox oy"><p id="6ab0" class="ly lz nk ma b mb nf kd md me ng kg mg oz nh mi mj pa ni ml mm pb nj mo mp mq im bi translated">在这一句意义深远的话中，它几乎囊括了所有(受监督的)机器学习。</p></blockquote><p id="faea" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">想想它的后果，</p><ul class=""><li id="b791" class="no np it ma b mb nf me ng lm nq lq nr lu ns mq nt nu nv nw bi translated">一个<strong class="ma jd">线性模型</strong>的模型复杂度——选择什么次数的多项式，如何减少残差平方和</li><li id="2af3" class="no np it ma b mb nx me ny lm nz lq oa lu ob mq nt nu nv nw bi translated">一个<strong class="ma jd">神经网络</strong>架构的选择——如何不过度拟合训练数据，达到良好的验证精度，同时减少分类误差。</li><li id="f050" class="no np it ma b mb nx me ny lm nz lq oa lu ob mq nt nu nv nw bi translated"><strong class="ma jd">支持向量机</strong>正则化和核选择——软硬余量之间的平衡，即在准确性和决策边界非线性之间进行权衡。</li></ul><h2 id="162d" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">我们真正的结论是什么？</h2><p id="0e90" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated">从对最小描述长度(MDL)原则的分析中，我们可以得出什么结论？</p><blockquote class="ow ox oy"><p id="adfb" class="ly lz nk ma b mb nf kd md me ng kg mg oz nh mi mj pa ni ml mm pb nj mo mp mq im bi translated">这是否一劳永逸地证明了短假设是最好的？</p></blockquote><p id="7ede" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">号码</p><p id="e6a2" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">MDL 显示的是，如果假设的表示被选择为使得假设的大小为 log2 P( <strong class="ma jd"> <em class="nk"> h </em> </strong>)，并且如果异常(错误)的表示被选择为使得编码长度为 D，给定为<strong class="ma jd"> <em class="nk"> h，</em> </strong>等于-log2p(<strong class="ma jd"><em class="nk">D</em></strong>|<strong class="ma jd"><em class="nk"/></strong></p><p id="5107" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">然而，为了表明我们有这样的表示，我们必须知道所有的先验概率 P( <strong class="ma jd"> <em class="nk"> h </em> </strong>)，以及 P(<strong class="ma jd"><em class="nk">D</em></strong>|<strong class="ma jd"><em class="nk">h</em></strong>)。没有理由相信 MDL 假设相对于假设和错误/误分类的任意编码应该是优选的。</p><p id="b83d" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">对于实际的机器学习来说，对于人类设计者来说，指定一个表示来获取关于假设的相对概率的知识可能比完全指定每个假设的概率更容易。</p><p id="2dd4" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi translated">这就是知识表示和领域专长的问题变得至关重要的地方。它缩短了(通常)无限大的假设空间，并将我们引向一组极有可能的假设，我们可以对这些假设进行最佳编码，并努力从中找到一组映射假设。</p><blockquote class="pj"><p id="7978" class="pk pl it bd pm pn po pp pq pr ps mq dk translated"><strong class="ak">…最佳假设是最小化两项之和的假设:假设的长度和错误率</strong>。</p></blockquote><h1 id="d352" class="mu le it bd lf mv mw mx li my mz na ll ki pt kj lp kl pu km lt ko pv kp lx ne bi translated">总结和反思</h1><p id="2f77" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated">这是一个奇妙的事实，对概率论的一个基本恒等式进行这样一组简单的数学操作，就可以对监督机器学习的基本限制和目标进行如此深刻和简洁的描述。对于这些问题的简明处理，读者可以参考这篇博士论文，名为<a class="ae mr" href="http://www.cs.cmu.edu/~gmontane/montanez_dissertation.pdf" rel="noopener ugc nofollow" target="_blank">“为什么机器学习有效”，来自卡耐基梅隆大学</a>。同样值得思考的是，所有这些如何与<a class="ae mr" href="https://en.wikipedia.org/wiki/No_free_lunch_theorem" rel="noopener ugc nofollow" target="_blank">没有免费的午餐定理</a>联系在一起。</p><h2 id="850e" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">如果你对这方面的深入阅读感兴趣</h2><ol class=""><li id="d4ba" class="no np it ma b mb mc me mf lm pw lq px lu py mq pz nu nv nw bi translated">"<a class="ae mr" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.160.798&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">没有免费的午餐和最小描述长度</a>"</li><li id="77f0" class="no np it ma b mb nx me ny lm nz lq oa lu ob mq pz nu nv nw bi translated">"<a class="ae mr" href="https://pdfs.semanticscholar.org/83cd/86c2c7e507e8ebba9563a9efaba7c966a1b3.pdf" rel="noopener ugc nofollow" target="_blank">没有免费的午餐对抗监督学习中的奥卡姆剃刀</a>"</li><li id="1c31" class="no np it ma b mb nx me ny lm nz lq oa lu ob mq pz nu nv nw bi translated"><a class="ae mr" href="http://www.no-free-lunch.org/ScVW01.pdf" rel="noopener ugc nofollow" target="_blank">没有免费的午餐和问题描述长度</a></li></ol></div><div class="ab cl qa qb hx qc" role="separator"><span class="qd bw bk qe qf qg"/><span class="qd bw bk qe qf qg"/><span class="qd bw bk qe qf"/></div><div class="im in io ip iq"><p id="05c3" class="pw-post-body-paragraph ly lz it ma b mb nf kd md me ng kg mg lm nh mi mj lq ni ml mm lu nj mo mp mq im bi qh translated"><span class="l qi qj qk bm ql qm qn qo qp di">如果</span>您有任何问题或想法要分享，请通过<a class="ae mr" href="mailto:tirthajyoti@gmail.com" rel="noopener ugc nofollow" target="_blank"><strong class="ma jd">tirthajyoti【AT】Gmail . com</strong></a>联系作者。此外，您可以查看作者的<a class="ae mr" href="https://github.com/tirthajyoti?tab=repositories" rel="noopener ugc nofollow" target="_blank"> <strong class="ma jd"> GitHub 资源库</strong> </a>中其他有趣的 Python、R 或 MATLAB 代码片段和机器学习资源。如果你像我一样，对机器学习/数据科学充满热情，请随时<a class="ae mr" href="https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/" rel="noopener ugc nofollow" target="_blank">在 LinkedIn 上添加我</a>或<a class="ae mr" href="https://twitter.com/tirthajyotiS" rel="noopener ugc nofollow" target="_blank">在 Twitter 上关注我。</a></p><div class="od oe gp gr of og"><a href="https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd jd gy z fp ol fr fs om fu fw jc bi translated">Tirthajyoti Sarkar -高级首席工程师-低压设计工程-半导体上|…</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">查看 Tirthajyoti Sarkar 在世界上最大的职业社区 LinkedIn 上的个人资料。Tirthajyoti 有 8 份工作…</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">www.linkedin.com</p></div></div><div class="op l"><div class="qq l or os ot op ou lb og"/></div></div></a></div></div></div>    
</body>
</html>