<html>
<head>
<title>Improving Vanilla Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">改进香草梯度下降</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/improving-vanilla-gradient-descent-f9d91031ab1d?source=collection_archive---------2-----------------------#2018-02-22">https://towardsdatascience.com/improving-vanilla-gradient-descent-f9d91031ab1d?source=collection_archive---------2-----------------------#2018-02-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c525" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">应用于训练神经网络的性能改进</h2></div><h2 id="6245" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">简介</strong></h2><p id="bd32" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">当我们使用梯度下降来训练神经网络时，我们会冒着网络陷入局部最小值的风险，在这种情况下，网络会停止在误差曲面上的某个位置，而该位置不是整个曲面上的最低点。这是因为误差曲面不是固有凸的，所以曲面可能包含许多独立于全局最小值的独立局部最小值。此外，虽然网络可能达到全局最小值并收敛到训练数据的期望点，但是不能保证它将如何很好地概括它所学习的内容。这意味着它们容易在训练数据上过度拟合。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi lx"><img src="../Images/f5795d99cafae910f70e17353e16a267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*E-5K5rHxCRTPrSWF60XLWw.gif"/></div></div></figure><p id="29f5" class="pw-post-body-paragraph le lf it lg b lh mj ju lj lk mk jx lm kr ml lo lp kv mm lr ls kz mn lu lv lw im bi translated">为了帮助减轻这些问题，我们可以使用一些东西，尽管没有办法明确地防止它们发生，因为这些网络的误差表面往往很难遍历，并且神经网络作为一个整体很难解释。</p><h2 id="a3a3" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">随机和小批量随机梯度下降</h2><p id="b2f6" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">标准梯度下降算法的这些修改为算法的每次迭代使用训练数据的子集。SGD将在每次权重更新时使用一个样本，小批量SGD将使用预定义的数量(通常比训练样本的总数小得多)。这使得训练进行得更快，因为它需要更少的计算，因为我们在每次迭代中不使用整个数据集。这也有望带来更好的性能，因为网络在训练期间的剧烈运动应该允许它更好地避免局部最小值，并且只使用数据集的一小部分应该有助于防止过度拟合。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/5f2d78ec0a2853ec436871c7c26c6a9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*jjcOf5V66UUHNfuniT4z-g.png"/></div></figure><h2 id="9b5d" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">正规化</h2><p id="7d9b" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">一般而言，正则化是一种通过向表示模型复杂性的损失函数添加一项来惩罚模型复杂性的机制。在神经网络的情况下，它惩罚大的权重，这可能指示网络已经过度适应训练数据。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi mp"><img src="../Images/61114a53b8b9e0bc1e280721f5e45601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dc_OuN0fAKkcTaY6VVUryg.jpeg"/></div></div></figure><p id="bc57" class="pw-post-body-paragraph le lf it lg b lh mj ju lj lk mk jx lm kr ml lo lp kv mm lr ls kz mn lu lv lw im bi translated">使用L2正则化，我们可以将损失函数重写如下，将网络的原始损失函数表示为<strong class="lg iu"> L(y，t) </strong>，正则化常数表示为<strong class="lg iu"> λ </strong>:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/d6deb4808cf9964c3e3c030f1a98c740.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*tu7uXio2o52mjLHfvYLoCg.png"/></div></figure><p id="6d50" class="pw-post-body-paragraph le lf it lg b lh mj ju lj lk mk jx lm kr ml lo lp kv mm lr ls kz mn lu lv lw im bi translated">正则化将网络中每个权重的平方和添加到损失函数中，惩罚模型对任何一个连接赋予过多的权重，并有望减少过度拟合。</p><h2 id="7672" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">动力</h2><p id="2358" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">简单地说，动量将过去权重更新的一部分添加到当前权重更新中。这有助于防止模型陷入局部最小值，因为即使当前梯度为0，过去的梯度很可能不是，所以它很容易陷入。通过使用动量，沿着误差表面的运动通常也更平滑，并且网络可以更快地穿过误差表面。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/afa1b46935294743c7f13626f3c88fd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*5-GPmnonHVQiIj2EPG3Fgw.png"/></div></figure><p id="1178" class="pw-post-body-paragraph le lf it lg b lh mj ju lj lk mk jx lm kr ml lo lp kv mm lr ls kz mn lu lv lw im bi translated">对于简单动量，我们可以将权重更新方程重写如下，将<strong class="lg iu"> α </strong>表示为动量因子:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ms"><img src="../Images/f93f71a7e864bdbd74fdbc60310afe95.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*S0A6wO0EkDCtYGcQ0yDgIw.png"/></div></div></figure><p id="3b17" class="pw-post-body-paragraph le lf it lg b lh mj ju lj lk mk jx lm kr ml lo lp kv mm lr ls kz mn lu lv lw im bi translated">也有其他更高级的动量形式，比如T2内斯特罗夫方法T3。</p><h2 id="65a5" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">学习速率退火</h2><p id="a57f" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">我们可以调整学习速度，让它随着时间的推移而下降，而不是在整个训练过程中使用一个恒定的学习速度。</p><p id="9d4a" class="pw-post-body-paragraph le lf it lg b lh mj ju lj lk mk jx lm kr ml lo lp kv mm lr ls kz mn lu lv lw im bi translated">最常见的调度具有如下的<strong class="lg iu"> 1/t </strong>关系，其中<strong class="lg iu"> T </strong>和<strong class="lg iu"> μ_0 </strong>被提供超参数，并且<strong class="lg iu"> μ </strong>是当前学习率:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/e734e682b8e0634533b82862f9d18a94.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*IUjHFnnbU45rRb9F_O8vag.png"/></div></figure><p id="e2a7" class="pw-post-body-paragraph le lf it lg b lh mj ju lj lk mk jx lm kr ml lo lp kv mm lr ls kz mn lu lv lw im bi translated">这通常被称为“搜索然后收敛”退火时间表，因为直到<strong class="lg iu"> t </strong>到达<strong class="lg iu"> T </strong>，网络处于“搜索”阶段，并且学习速率没有降低太多，之后，学习速率变慢，网络到达“收敛”阶段。这大致与<em class="mu">开发</em>和<em class="mu">探索</em>之间的平衡有关。开始时，我们优先探索搜索空间并扩展我们对该空间的整体知识，随着时间的推移，我们过渡到利用我们已经发现的搜索空间中的好区域，并缩小到特定的最小值。</p><h2 id="e002" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论</h2><p id="c83e" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">这些是改进标准梯度下降算法的一些方法。当然，这些方法中的每一种都会向您的模型添加超参数，因此会增加调整网络所花费的时间。最近，更新的算法如<strong class="lg iu">亚当</strong>、<strong class="lg iu">阿达格拉德</strong>、<strong class="lg iu">、T25】和<strong class="lg iu">阿达德尔塔</strong>如雨后春笋般涌现，它们使用了其中一些技术以及许多其他技术。他们倾向于在每个参数的基础上优化，而不是全局优化，因此他们可以根据个人情况微调学习速度。他们在实践中往往工作得更快更好；然而，要正确实施它们要困难得多。下图说明了同时工作的上述每个梯度下降变化。观察到更复杂的版本比简单的动量或SGD版本收敛得更快。</strong></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/25f8a007d6079560cc306585f3a248fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*ZDCzle8_OKEhSKc488l_VA.gif"/></div></figure></div></div>    
</body>
</html>