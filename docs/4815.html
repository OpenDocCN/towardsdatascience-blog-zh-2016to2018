<html>
<head>
<title>2nd Place Solution for CIKM AnalytiCup 2018</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CIKM AnalytiCup 2018 第二名解决方案</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simtext-2nd-solution-for-cikm-analyticup-2018-b3347e026e67?source=collection_archive---------12-----------------------#2018-09-08">https://towardsdatascience.com/simtext-2nd-solution-for-cikm-analyticup-2018-b3347e026e67?source=collection_archive---------12-----------------------#2018-09-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/9c2a747c5a59f4df0b006a0bea3a8e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6p0B44gR6jflqV1o.jpg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="http://www.cikm2018.units.it/#firstPage" rel="noopener ugc nofollow" target="_blank">Conference on Information and Knowledge Management (CIKM)</a> will be held at Torino, Italy, 22–26 October 2018.</figcaption></figure><p id="5463" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是我为<a class="ae kc" href="https://tianchi.aliyun.com/markets/tianchi/CIKM2018" rel="noopener ugc nofollow" target="_blank"> CIKM AnalytiCUP 2018 </a>做的解决方案总结，是一个关于短文本语义相似度和知识转移的比赛。</p><h1 id="802c" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><a class="ae kc" href="https://tianchi.aliyun.com/markets/tianchi/CIKM2018" rel="noopener ugc nofollow" target="_blank">问题定义</a></h1><blockquote class="lz ma mb"><p id="5fd5" class="kd ke mc kf b kg kh ki kj kk kl km kn md kp kq kr me kt ku kv mf kx ky kz la ij bi translated">这项挑战的目标是建立一个跨语言的短文本匹配模型。源语言是英语，目标语言是西班牙语。参与者可以通过应用先进的技术来训练他们的模型，以对问题对是否具有相同的含义进行分类。</p></blockquote><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mg"><img src="../Images/8b9da6bd3a90603d79cbfe38c75a11d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EKbn7i-FFnS1jQqLvfL9AQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Overview of this dataset. We are asked to distinguish the intents between sentence 1 and sentence 2. Label being 1 indicates that spn_1 and spn_2 have the same meaning.</figcaption></figure><h1 id="e667" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">概观</h1><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/b2d79966c941cf44bf8243cb80af78b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GtdDrnIenq4XsWdBKrWgOA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Overview of the workflow. It is a 2-stage training framework where the first stage is to aggregates the word features and hand-crafted features using various symmetric models. And the second stage is inspired by the idea of knowledge distillation, I fine-tuned each model trained in the first stage with the soft labels.</figcaption></figure><h1 id="353e" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">特征工程</h1><p id="8d12" class="pw-post-body-paragraph kd ke iq kf b kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">除了预先训练的单词嵌入，我还额外创建了三种特征来丰富句子信息。</p><ol class=""><li id="96e8" class="mr ms iq kf b kg kh kk kl ko mt ks mu kw mv la mw mx my mz bi translated">距离特征</li></ol><p id="7d13" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我测量句子向量之间的距离，句子向量可以通过三种方式构建:</p><ul class=""><li id="1ec6" class="mr ms iq kf b kg kh kk kl ko mt ks mu kw mv la na mx my mz bi translated">词袋模型</li><li id="5a2e" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated">带 TF/IDF 的词袋模型</li><li id="e1df" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated">基于 TF/IDF 的加权平均单词嵌入</li></ul><p id="9b02" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.主题特征</p><p id="6743" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个数据集中有许多常见的模式，因为当客户咨询相同的意图时，他们倾向于使用相同的前缀或后缀。</p><p id="9736" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如:</p><ul class=""><li id="d9f6" class="mr ms iq kf b kg kh kk kl ko mt ks mu kw mv la na mx my mz bi translated">“我想”创造一个论点</li><li id="cf33" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated">“我想”和一个人说话</li><li id="6db4" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated">“我想”问是否…</li></ul><p id="5358" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以假设句子有相同的前缀或后缀，主题相同。为了捕捉共现信息，我用 LDA/LSI 对句子进行了矢量化，并用余弦距离比较了它们之间的差异。</p><p id="5573" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.文本特征</p><p id="43f5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">文本特征都是关于句子的性质，例如:</p><ul class=""><li id="f2bc" class="mr ms iq kf b kg kh kk kl ko mt ks mu kw mv la na mx my mz bi translated">句子的长度:句子越长，概率应该越低。</li><li id="4868" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated">停用词和唯一词的数量:它可以显示冗余的程度。</li><li id="f49e" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated">词语的多样性:它通常反映了上诉的复杂性。</li></ul><p id="9d0d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我创建了 56 个统计特征。以下是功能重要性和相关性热图的概述:</p><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/f65f052e70926f41e79bbe44d275780d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/0*aX_u3rSL6R8pyGrW.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">The feature importances, generated by a random forest classifier</figcaption></figure><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/5766ed771b0070fab7850da165fa59c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6vJMVamk7gayMSo-"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">The correlation heatmap of all my features</figcaption></figure></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><h1 id="87ad" class="lb lc iq bd ld le np lg lh li nq lk ll lm nr lo lp lq ns ls lt lu nt lw lx ly bi translated">模型</h1><p id="729a" class="pw-post-body-paragraph kd ke iq kf b kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">由于一些模型仍在开发中，我目前只在我的<a class="ae kc" href="https://github.com/zake7749/Closer" rel="noopener ugc nofollow" target="_blank"> github </a>上传了部分解决方案。在清理完我的代码后，我会分享它们。回到主题，我尝试了三种基于不同假设的模型。</p><h2 id="954e" class="nu lc iq bd ld nv nw dn lh nx ny dp ll ko nz oa lp ks ob oc lt kw od oe lx of bi translated">可分解注意力</h2><p id="1553" class="pw-post-body-paragraph kd ke iq kf b kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">可分解注意力是判断两句话是否相同的一个非常标准的解法。简而言之，可分解注意力将 sent1 中的单词与 sent2 中的相似单词对齐。</p><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div class="gh gi og"><img src="../Images/26ffe5f6d6ea84ee5123792df196812e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*L4rVSScRHlOXtP-rL-KjGA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure from <a class="ae kc" href="https://arxiv.org/abs/1606.01933" rel="noopener ugc nofollow" target="_blank">A Decomposable Attention Model for Natural Language Inference</a></figcaption></figure><p id="072b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这在这个数据集中很有帮助，因为有如此多的表面模式不断重复。例如:</p><ul class=""><li id="0963" class="mr ms iq kf b kg kh kk kl ko mt ks mu kw mv la na mx my mz bi translated">什么是<x>？</x></li><li id="fa94" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated"><em class="mc"> —什么是区号？</em></li><li id="3ef0" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated">什么事？</li><li id="103a" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated"><em class="mc"> —什么是愿望清单？</em></li><li id="054e" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated">怎么才能拿到<x>？</x></li><li id="329a" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated"><em class="mc"> —我如何获得订单？</em></li><li id="3786" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated"><em class="mc"> —我如何获得退款？</em></li></ul><p id="4388" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以找到这么多可能的选择，这就是我们要解决的核心问题。如果我们只关注前面提到的统计特征，模型在这些情况下会失败，因为我们将一个单词编码为标识符，并放弃了它的意义。为了清楚起见，该模型会将“我如何才能获得订单”和“我如何才能获得退款”视为相同，因为它们共享了如此多的词(“如何”、“可以”、“我”、“获得”、“该”)。然而，由于“订单”和“退款”之间的区别，人类知道它们是不一样的。</p><p id="febd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可能会说我们可以用 TF/IDF 来解决这个问题，但有时关键字不同但语义相同，就像这样:</p><ul class=""><li id="0369" class="mr ms iq kf b kg kh kk kl ko mt ks mu kw mv la na mx my mz bi translated">我想和一个男人说话</li><li id="2947" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated">我想和人类说话</li><li id="0058" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated">我想和一个员工谈谈</li></ul><p id="1ef2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些应该认为是一样的。顾客真正想要的是与人交谈，不管这个人是女人、男人还是雇员。</p><p id="e6de" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可分解的注意力比较它认为相关的东西。如果两个词确实有关联，也没关系。但是，如果没有，它可以更新上下文嵌入并使它们不同。一般来说，可分解的注意力建立了每个单词的类比关系，这让我们很容易抓住句子之间的显著差异。</p><p id="df73" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了可分解注意力之外，我还为这个任务设计了另外两个模型。</p><h2 id="0873" class="nu lc iq bd ld nv nw dn lh nx ny dp ll ko nz oa lp ks ob oc lt kw od oe lx of bi translated">为什么不用可分解注意力呢？</h2><p id="654a" class="pw-post-body-paragraph kd ke iq kf b kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">在我看来，可分解注意力有两个缺点:</p><ol class=""><li id="d240" class="mr ms iq kf b kg kh kk kl ko mt ks mu kw mv la mw mx my mz bi translated">当应用注意力时，我们丢弃了单词的顺序，这意味着我们也丢失了像短语或单词之间的依赖关系这样的信息。这可能是一场灾难，因为我们被禁止使用任何其他外部数据。一旦特征消失了，我们就再也找不回来了。</li><li id="ed72" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">冷启动词。我发现 google translator 倾向于将一些相似的概念总结成一个单一的概念，这导致训练数据缺乏词汇的多样性。然而，测试数据是原始的西班牙语句子，并没有那么简单。我想这就是为什么即使可分解的注意力在本地简历上做得很好，它总是大大超过 LB。</li><li id="f203" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">为了应对这两个弱点，我深入研究了另外两个模型。RNN 模型用于重建依赖关系，而 CNN 模型用于捕捉区域语义表示。让我们从 RNN 模型开始。</li></ol></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><h2 id="0ac5" class="nu lc iq bd ld nv nw dn lh nx ny dp ll ko nz oa lp ks ob oc lt kw od oe lx of bi translated">比较传播递归神经网络</h2><p id="3d52" class="pw-post-body-paragraph kd ke iq kf b kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">简而言之，我遵循 Cafe 的精神，也提出了其他的想法，使它更好地完成这项任务。</p><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/9382962b512a055cf870421860172626.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*xiidO4G_UkPCqyF-5nQwjQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">The high level overview of CAFE. Figure from <a class="ae kc" href="https://arxiv.org/abs/1801.00102" rel="noopener ugc nofollow" target="_blank">A Compare-Propagate Architecture with Alignment Factorization for Natural Language Inference</a></figcaption></figure><ol class=""><li id="8a42" class="mr ms iq kf b kg kh kk kl ko mt ks mu kw mv la mw mx my mz bi translated">我用 MLP 替换了 FM 层，并将所有交互特征混合为一个矢量。我发现在尝试用 DeepFM 改变 FM 部分时效果更好。在我看来，这是因为 MLP 可以塑造高层次的互动，可以很好地与辍学正规化。</li><li id="7608" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">至于交互层，我加了求和运算，同时比较各种特征。有趣的是，这样一个简单的动作将 LB 分数提高了 0.015。</li><li id="0806" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">我将这个模型修改为对称架构。原因我会在正规化部分讲。</li></ol></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><h2 id="d280" class="nu lc iq bd ld nv nw dn lh nx ny dp ll ko nz oa lp ks ob oc lt kw od oe lx of bi translated">密集增强卷积神经网络</h2><p id="3ede" class="pw-post-body-paragraph kd ke iq kf b kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">在实现 Cafe 时，我想出了一个主意。为什么不重复增加和转发的循环，这使得模型更好地融合了本地和序列信息。我试了一下。这个想法对 RNN 来说失败了，但对 CNN 来说却成功了。我认为这是因为 RNN 保留了每个时间步长的冗余信息，而 CNN 没有。CNN 只关注地区信息，这也是这场竞争的关键部分。</p><p id="57f2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们回到可分解注意力的第一个缺点——掉话序。例如，它不是“我的”和“你的”。我们不应该独立地比较这几类词。我们需要的是看着它们附近的单词，比较整个短语。此外，该方法还减轻了冷启动字的影响。就像 word2vec 的核心信念一样，我们可以通过一个词的邻居来理解它的意思。</p><p id="4ab3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DACNN 的架构是可分解注意力的扩展。我设计了一个比较循环，将结果与原始嵌入重复连接起来，就像 DenseNet 一样。这对于特征重用和正则化是一个很好的想法，使得 DACNN 在几乎相同的性能下运行速度比 CPRNN 快 3 倍。</p><p id="e41f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还尝试通过应用两次软对齐来对齐句子本身。这是另一种在语义摘要中起重要作用的自我注意方式。</p></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><h1 id="2045" class="lb lc iq bd ld le np lg lh li nq lk ll lm nr lo lp lq ns ls lt lu nt lw lx ly bi translated">模型种类</h1><p id="06dc" class="pw-post-body-paragraph kd ke iq kf b kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">正如我们在上一节中看到的，这三个模型基于不同的假设。每个模型之间的相关性也可以表明这一事实。</p><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/b2aa50f2bafaf84095b81fb67a88c405.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*6FLIa7EyJZHcuDxY0ExtPg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">The correlation matrix of prediction from each model</figcaption></figure><p id="8f8d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以我不打算谈论不同型号之间的差异。相反，我将展示如何在模型内部创建变体。方法很简单。如果我们在同一个网络中插入不同的输入，直觉上预测会不同。</p><p id="782a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我为每个模型准备了三种类型的输入:</p><ol class=""><li id="a335" class="mr ms iq kf b kg kh kk kl ko mt ks mu kw mv la mw mx my mz bi translated">字级输入。我从官方的预训练嵌入中查找每个单词。我没有直接训练嵌入层，而是在它上面放置一个公路层来微调权重。</li><li id="c528" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">字符级输入。我按照论文“字符感知神经语言模型”来创建字符级嵌入。这里是它的架构概述。</li></ol><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/2390632585a61ad2acc56230de414960.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/0*95RDD1OPd2qeHu4t"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">The way to costruct word embedding from char level.Figure from <a class="ae kc" href="https://arxiv.org/abs/1508.06615" rel="noopener ugc nofollow" target="_blank">Character-Aware Neural Language Models</a></figcaption></figure><ol class=""><li id="62f8" class="mr ms iq kf b kg kh kk kl ko mt ks mu kw mv la mw mx my mz bi translated">具有元特征的单词级输入。元特征在特征工程中得到了很好的讨论。在公路图层之后，我将它们与交互功能连接起来。</li></ol><p id="a80f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这三个输入确实产生不同的预测，并在打包时导致显著的提升。</p><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/eac261abf33a7c5a7859338e24c7c152.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*DEYr703icsUMRgEORHdkfg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">The LB socre of single model and blended.</figcaption></figure><p id="26b1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可能会问，为什么我们不同时接受各种输入呢？的确，这将是一个很好的尝试。准确地说，我们可以试验 2 - 1 种组合。我训练了其中的三个，因为我只有一台电脑。我相信我们可以通过包装所有类型的模型来进一步提高性能。</p></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><h1 id="b62b" class="lb lc iq bd ld le np lg lh li nq lk ll lm nr lo lp lq ns ls lt lu nt lw lx ly bi translated">全体</h1><p id="6ef4" class="pw-post-body-paragraph kd ke iq kf b kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">受“提取神经网络中的知识”的启发，我为我的合奏使用了两级堆叠。第一层是各种模型的简单混合。我使用混合预测作为伪标签，并用它们微调所有模型。</p><p id="c059" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二层是 DART(漏失符合多重加性回归树),它从微调模型中获取输入。</p><p id="4026" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">至于伪标签，我使用软标签而不是硬标签有两个原因:</p><p id="1051" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1.训练数据有噪声。我可以用传递法则做一个简单的证明。</p><ul class=""><li id="b565" class="mr ms iq kf b kg kh kk kl ko mt ks mu kw mv la na mx my mz bi translated">规则 1:假设 A 和 B 相同，B 和 C 也相同。我们会认为 A 和 C 是相同的。</li><li id="40c2" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la na mx my mz bi translated">规则 2:假设 A 和 B 相同，B 和 C 不相同。我们会认为 A 和 C 是不同的。</li></ul><p id="c273" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">规则 1 和规则 2 分别只适用于 75%和 95%的情况。我认为标签没有绝对的规则，可能因人而异。</p><p id="b457" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.软标签适合正则化，通常更有意义。关于软标签在“通过惩罚置信输出分布来正则化神经网络”和“用有噪声的标签学习”中的好处，有充分的讨论。特别是当采用对数损失作为评估标准时，我们希望输出的分布更加平滑，而不是更加极端。此外，在一个没有完美答案的任务中，我认为用概率来表示等级关系的程度会更有意义。</p></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><h1 id="6435" class="lb lc iq bd ld le np lg lh li nq lk ll lm nr lo lp lq ns ls lt lu nt lw lx ly bi translated">正规化</h1><h2 id="fa89" class="nu lc iq bd ld nv nw dn lh nx ny dp ll ko nz oa lp ks ob oc lt kw od oe lx of bi translated">软标签</h2><p id="c94a" class="pw-post-body-paragraph kd ke iq kf b kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">我们已经在合奏部分讨论过了。</p><h2 id="2335" class="nu lc iq bd ld nv nw dn lh nx ny dp ll ko nz oa lp ks ob oc lt kw od oe lx of bi translated">拒绝传统社会的人</h2><p id="1ba9" class="pw-post-body-paragraph kd ke iq kf b kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">我倾向于在两个地方使用 dropout，而不是在每个密集层后设置小比率的 dropout:</p><ul class=""><li id="60ac" class="mr ms iq kf b kg kh kk kl ko mt ks mu kw mv la na mx my mz bi translated">与其他公司相比，我们的训练数据实在是太少了。也就是说，我们不能构建一个大模型，否则维度的诅咒会惩罚我们。<br/> <br/>所以我使用嵌入 dropout 在训练批次中随机丢弃一些单词。这一举动可以被认为是一种数据扩充。通过嵌入 dropout，我们可以仅使用少量数据来概括像 CAFE 这样的复杂模型。</li></ul><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ol"><img src="../Images/676d928a2869be46eaa458242f87ad12.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*aux9FMeCKcGtFMNVBKX0PQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">We can see that embedding dropout (ED) improve the performances of all models dramatically.</figcaption></figure><ul class=""><li id="799e" class="mr ms iq kf b kg kh kk kl ko mt ks mu kw mv la na mx my mz bi translated">Concat-Feature dropout: <br/>我将 dropout 放在交互层之后。它遵循随机森林的精神，因为交互层只不过是一个特性池。该模型有时关注来自求和与相乘的特征，有时选择来自点和连接的特征。这种策略有助于模型对不同的数据视图进行归纳。</li></ul><h2 id="1729" class="nu lc iq bd ld nv nw dn lh nx ny dp ll ko nz oa lp ks ob oc lt kw od oe lx of bi translated">对称</h2><p id="ae56" class="pw-post-body-paragraph kd ke iq kf b kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">在“NLP 中的外推法”中，他们观察到对称架构是一般化的关键。我认为这在这个任务中是有意义的——相似性度量也是对称的。我们期望 Sim(A，B)应该与 Sim(B，A)相同。但是，我们通常不会把它放在心上。当创建交互特征时，我们总是应用连接操作，该操作破坏了对称性。</p><p id="25fa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么去掉这个功能怎么样？<br/>没那么容易。根据 Cafe 的消融研究，concat 功能是模型性能的功臣。</p><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div class="gh gi om"><img src="../Images/61ef6faba75e2b62cf79b45145c4fd55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/0*z-ZdPoGbShPvyZ0Z"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">The ablation from <a class="ae kc" href="https://arxiv.org/abs/1801.00102" rel="noopener ugc nofollow" target="_blank">CAFE</a>, we can see the scroe descreases significantly when removing Concat Feat</figcaption></figure><p id="c574" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了掌握对称性和连接特性，我做了两次连接操作。第一次用于[Sent1，Sent2]，第二次用于[Sent2，Sent1]。最后，平均这两个结果。</p><p id="861e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">顺便说一下，我尝试了其他方法来解决对称的问题。例如，我们可以将训练数据加倍，一个用于句子对<a b="">，另一个用于句子对<b a="">。或者，我们可以在测试阶段对对<a b="">和对<b a="">的预测进行平均。然而，这两种解决方案背后都有一定的成本，这就是我最终放弃它们的原因。</b></a></b></a></p></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><h1 id="1265" class="lb lc iq bd ld le np lg lh li nq lk ll lm nr lo lp lq ns ls lt lu nt lw lx ly bi translated">性能比较</h1><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/f31f25a2c05e76857e239b922f9de065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*hJuj0lWW2LojwCR6qWmH4g.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><em class="on">The log loss of blending methods are measured by corresponding out-of-fold predictions.</em></figcaption></figure><h1 id="c809" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">参考</h1><ol class=""><li id="962c" class="mr ms iq kf b kg mm kk mn ko oo ks op kw oq la mw mx my mz bi translated">一种用于自然语言推理的带有对齐因子的比较传播结构</li><li id="877b" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">自然语言推理的可分解注意模型</li><li id="7688" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">字符感知神经语言模型</li><li id="7404" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">用于句子分类的卷积神经网络</li><li id="d0be" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">DART:退出者遇到多重加法回归树</li><li id="802d" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">DeepFM:一种基于因子分解机的神经网络用于 CTR 预测</li><li id="6c3e" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">密集连接的卷积网络</li><li id="c820" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">提取神经网络中的知识</li><li id="ed73" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">用于自然语言推理的增强型 LSTM</li><li id="a9e2" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">自然语言处理中的外推</li><li id="8eaf" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">从单词嵌入到文档距离</li><li id="b375" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">用嘈杂的标签学习</li><li id="1c34" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">规范和优化 LSTM 语言模型</li><li id="672d" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">通过惩罚置信输出分布来正则化神经网络</li><li id="ba71" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">R-net:具有自匹配网络的机器阅读理解</li><li id="2d1d" class="mr ms iq kf b kg nb kk nc ko nd ks ne kw nf la mw mx my mz bi translated">使用数百万个表情符号来学习任何领域的表达，以检测情绪、情感和讽刺</li></ol></div></div>    
</body>
</html>