<html>
<head>
<title>Gradient boosting using Random Forests for application on the New York Taxi Fare Prediction Challenge</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用随机森林的梯度推进在纽约出租车费用预测挑战中的应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-boosting-using-random-forests-for-application-on-the-new-york-taxi-fare-prediction-f6101c592bf9?source=collection_archive---------15-----------------------#2018-10-22">https://towardsdatascience.com/gradient-boosting-using-random-forests-for-application-on-the-new-york-taxi-fare-prediction-f6101c592bf9?source=collection_archive---------15-----------------------#2018-10-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/6f4e1ee8ce6bc6f5694a50aa1ad7de93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uQ4wwv5e5zpNrvae0VwanQ.jpeg"/></div></div></figure><p id="faf5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇短文中，我将描述使用随机森林的梯度推进在 Kaggle 上的纽约出租车费用预测挑战中的应用。我还将比较两个最流行的软件包:微软的<a class="ae kw" href="https://github.com/Microsoft/LightGBM" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>和 Yandex 的<a class="ae kw" href="https://github.com/catboost/catboost" rel="noopener ugc nofollow" target="_blank"> CatBoost </a>。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="f7ac" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先我将简要地谈论随机森林。随机森林基本上是决策树的集合，通常通过 bagging 方法训练，其中最大样本数设置为训练集的大小。在构建树的同时，每次都有一个分裂，并且从完整的<em class="le"> p </em>预测值集中选择随机样本的<em class="le"> m </em>预测值作为分裂候选值。这些 m 个预测值中只有一个被选择。在每次分裂时选择 m 个预测值的新样本。通常情况下，<em class="le">m</em>√<em class="le">p:</em>每次分割时考虑的预测值数量大约等于预测值总数的根。这具有以下效果:</p><ul class=""><li id="f759" class="lf lg iq ka b kb kc kf kg kj lh kn li kr lj kv lk ll lm ln bi translated">将树彼此去相关</li><li id="63c0" class="lf lg iq ka b kb lo kf lp kj lq kn lr kr ls kv lk ll lm ln bi translated">交易较高的偏差和较低的方差</li></ul><p id="4fe5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">随机森林有助于快速了解哪些特征很重要，尤其是在特征选择方面。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="0a94" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> Boosting </strong>是一种将几个弱学习者组合成一个强学习者的技术。您按顺序训练预测器，每次都试图纠正前一个预测器。不涉及 bagging(减少统计学习方法方差的通用程序)。每棵树都适合原始数据集的修改版本。</p><p id="a629" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我们谈论梯度增强之前，我们必须了解一下 Adaboost。自适应增强关注前一个欠适应的训练实例。增加错误分类的训练实例的相对权重，然后使用更新的权重训练第二分类器，等等。</p><p id="cdf8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">类似于梯度下降，但不是改变单个预测参数来最小化成本函数，Adaboost 将预测器添加到集成中，逐渐使其更好。一旦训练了所有预测器，集成就像装袋一样进行预测，只是预测器具有不同的权重。然而，这种技术不能并行化，并且与装袋和粘贴相比不能很好地扩展。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="fe06" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">渐变增强</strong>，和 Adaboost 一样也是一种序列技术。然而，在每次迭代中，新的预测器被拟合到由前一个预测器产生的残差，而不是使用实例权重。</p><p id="0d37" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">整体算法如下:</p><ol class=""><li id="0d28" class="lf lg iq ka b kb kc kf kg kj lh kn li kr lj kv lt ll lm ln bi translated">使决策树回归器适合训练集</li><li id="b83a" class="lf lg iq ka b kb lo kf lp kj lq kn lr kr ls kv lt ll lm ln bi translated">根据 1 产生的残差训练第二个决策树回归器。</li><li id="bf59" class="lf lg iq ka b kb lo kf lp kj lq kn lr kr ls kv lt ll lm ln bi translated">根据 2 产生的剩余误差训练第三个回归变量。</li><li id="06ff" class="lf lg iq ka b kb lo kf lp kj lq kn lr kr ls kv lt ll lm ln bi translated">通过将所有树的预测相加，使用这三棵树的集合对新实例进行预测。</li></ol><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lu"><img src="../Images/3906c83435526a2282f83afc67575798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hC6xhppWFVqlmceWgSXzug.jpeg"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">Schematic of how gradient boosting works.</figcaption></figure></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi md"><img src="../Images/6741a4fe968a68406ecdec3d895a3d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*gtVfx3Ob1e02KEIe2DQWcw.png"/></div></figure><p id="aa2b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在说说代码:这个代码大量借用了 Kaggle 上的 Sylas。评估指标是 RMSE(均方根误差)。首先，您需要导入库，然后从 csv 文件输入数据。我们将使用 LightGBM。注意<strong class="ka ir"> nrows </strong>参数。你做得越高，你应该得到的分数就越好，但是跑的时间就越长。</p><pre class="lv lw lx ly gt me mf mg mh aw mi bi"><span id="0397" class="mj mk iq mf b gy ml mm l mn mo">import numpy as np <br/>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>import lightgbm as lgbm<br/>import gc</span><span id="342c" class="mj mk iq mf b gy mp mm l mn mo"># Reading Data<br/>train_df =  pd.read_csv('train.csv', nrows = 800000)<br/>#Drop rows with null values<br/>train_df = train_df.dropna(how = 'any', axis = 'rows')</span><span id="1d69" class="mj mk iq mf b gy mp mm l mn mo">def clean_df(df):<br/>    return df[(df.fare_amount &gt; 0)  &amp; (df.fare_amount &lt;= 500) &amp;<br/>          # (df.passenger_count &gt;= 0) &amp; (df.passenger_count &lt;= 8)  &amp;<br/>           ((df.pickup_longitude != 0) &amp; (df.pickup_latitude != 0) &amp; (df.dropoff_longitude != 0) &amp; (df.dropoff_latitude != 0) )]</span><span id="ee44" class="mj mk iq mf b gy mp mm l mn mo">train_df = clean_df(train_df)</span></pre><p id="ac99" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Train_df 基本上包含纽约出租车的所有信息，包括上下车地点、乘客、日期和时间以及票价本身。</p><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mq"><img src="../Images/ceadf776fb291a3f8162045f724916c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DJdY1WbQxSGhiLkO7SIxXg.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">How train_df looks like</figcaption></figure><p id="4984" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们需要定义几个要调用的函数。</p><pre class="lv lw lx ly gt me mf mg mh aw mi bi"><span id="4442" class="mj mk iq mf b gy ml mm l mn mo"># To Compute Haversine distance<br/>def sphere_dist(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):<br/>    """<br/>    Return distance along great radius between pickup and dropoff coordinates.<br/>    """<br/>    #Define earth radius (km)<br/>    R_earth = 6371<br/>    #Convert degrees to radians<br/>    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,<br/>                                                             [pickup_lat, pickup_lon, <br/>                                                              dropoff_lat, dropoff_lon])<br/>    #Compute distances along lat, lon dimensions<br/>    dlat = dropoff_lat - pickup_lat<br/>    dlon = dropoff_lon - pickup_lon<br/>    <br/>    #Compute haversine distance<br/>    a = np.sin(dlat/2.0)**2 + np.cos(pickup_lat) * np.cos(dropoff_lat) * np.sin(dlon/2.0)**2<br/>    return 2 * R_earth * np.arcsin(np.sqrt(a))</span><span id="523f" class="mj mk iq mf b gy mp mm l mn mo">def sphere_dist_bear(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):<br/>    """<br/>    Return distance along great radius between pickup and dropoff coordinates.<br/>    """<br/>    #Define earth radius (km)<br/>    R_earth = 6371<br/>    #Convert degrees to radians<br/>    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,<br/>                                                             [pickup_lat, pickup_lon, <br/>                                                              dropoff_lat, dropoff_lon])<br/>    #Compute distances along lat, lon dimensions<br/>    dlat = dropoff_lat - pickup_lat<br/>    dlon = pickup_lon - dropoff_lon<br/>    <br/>    #Compute bearing distance<br/>    a = np.arctan2(np.sin(dlon * np.cos(dropoff_lat)),np.cos(pickup_lat) * np.sin(dropoff_lat) - np.sin(pickup_lat) * np.cos(dropoff_lat) * np.cos(dlon))<br/>    return a</span><span id="363f" class="mj mk iq mf b gy mp mm l mn mo">def radian_conv(degree):<br/>    """<br/>    Return radian.<br/>    """<br/>    return  np.radians(degree)</span><span id="8e90" class="mj mk iq mf b gy mp mm l mn mo">def add_airport_dist(dataset):<br/>    """<br/>    Return minumum distance from pickup or dropoff coordinates to each airport.<br/>    JFK: John F. Kennedy International Airport<br/>    EWR: Newark Liberty International Airport<br/>    LGA: LaGuardia Airport<br/>    SOL: Statue of Liberty <br/>    NYC: Newyork Central<br/>    """<br/>    jfk_coord = (40.639722, -73.778889)<br/>    ewr_coord = (40.6925, -74.168611)<br/>    lga_coord = (40.77725, -73.872611)<br/>    sol_coord = (40.6892,-74.0445) # Statue of Liberty<br/>    nyc_coord = (40.7141667,-74.0063889) <br/>    <br/>    <br/>    pickup_lat = dataset['pickup_latitude']<br/>    dropoff_lat = dataset['dropoff_latitude']<br/>    pickup_lon = dataset['pickup_longitude']<br/>    dropoff_lon = dataset['dropoff_longitude']<br/>    <br/>    pickup_jfk = sphere_dist(pickup_lat, pickup_lon, jfk_coord[0], jfk_coord[1]) <br/>    dropoff_jfk = sphere_dist(jfk_coord[0], jfk_coord[1], dropoff_lat, dropoff_lon) <br/>    pickup_ewr = sphere_dist(pickup_lat, pickup_lon, ewr_coord[0], ewr_coord[1])<br/>    dropoff_ewr = sphere_dist(ewr_coord[0], ewr_coord[1], dropoff_lat, dropoff_lon) <br/>    pickup_lga = sphere_dist(pickup_lat, pickup_lon, lga_coord[0], lga_coord[1]) <br/>    dropoff_lga = sphere_dist(lga_coord[0], lga_coord[1], dropoff_lat, dropoff_lon)<br/>    pickup_sol = sphere_dist(pickup_lat, pickup_lon, sol_coord[0], sol_coord[1]) <br/>    dropoff_sol = sphere_dist(sol_coord[0], sol_coord[1], dropoff_lat, dropoff_lon)<br/>    pickup_nyc = sphere_dist(pickup_lat, pickup_lon, nyc_coord[0], nyc_coord[1]) <br/>    dropoff_nyc = sphere_dist(nyc_coord[0], nyc_coord[1], dropoff_lat, dropoff_lon)<br/>    <br/>    <br/>    <br/>    dataset['jfk_dist'] = pickup_jfk + dropoff_jfk<br/>    dataset['ewr_dist'] = pickup_ewr + dropoff_ewr<br/>    dataset['lga_dist'] = pickup_lga + dropoff_lga<br/>    dataset['sol_dist'] = pickup_sol + dropoff_sol<br/>    dataset['nyc_dist'] = pickup_nyc + dropoff_nyc<br/>    <br/>    return dataset<br/>    <br/>def add_datetime_info(dataset):<br/>    #Convert to datetime format<br/>    dataset['pickup_datetime'] = pd.to_datetime(dataset['pickup_datetime'],format="%Y-%m-%d %H:%M:%S UTC")<br/>    <br/>    dataset['hour'] = dataset.pickup_datetime.dt.hour<br/>    dataset['day'] = dataset.pickup_datetime.dt.day<br/>    dataset['month'] = dataset.pickup_datetime.dt.month<br/>    dataset['weekday'] = dataset.pickup_datetime.dt.weekday<br/>    dataset['year'] = dataset.pickup_datetime.dt.year<br/>    <br/>    return dataset</span></pre><p id="59ae" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">上面的代码主要计算哈弗线距离，它计算两个纬度经度点之间的距离。我们还希望以合适的日期和时间格式获得到机场的距离以及旅程的持续时间，以便稍后进行处理。</p><p id="6a49" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在让我们修改 train_df，使这些附加属性从上面的函数中计算出来。</p><pre class="lv lw lx ly gt me mf mg mh aw mi bi"><span id="467e" class="mj mk iq mf b gy ml mm l mn mo">train_df = add_datetime_info(train_df)<br/>train_df = add_airport_dist(train_df)<br/>train_df['distance'] = sphere_dist(train_df['pickup_latitude'], train_df['pickup_longitude'], <br/>                                   train_df['dropoff_latitude'] , train_df['dropoff_longitude'])</span><span id="2da9" class="mj mk iq mf b gy mp mm l mn mo">train_df['bearing'] = sphere_dist_bear(train_df['pickup_latitude'], train_df['pickup_longitude'], <br/>                                   train_df['dropoff_latitude'] , train_df['dropoff_longitude'])                                    <br/>train_df['pickup_latitude'] = radian_conv(train_df['pickup_latitude'])<br/>train_df['pickup_longitude'] = radian_conv(train_df['pickup_longitude'])<br/>train_df['dropoff_latitude'] = radian_conv(train_df['dropoff_latitude'])<br/>train_df['dropoff_longitude'] = radian_conv(train_df['dropoff_longitude'])</span><span id="b60d" class="mj mk iq mf b gy mp mm l mn mo">train_df.drop(columns=['key', 'pickup_datetime'], inplace=True)</span><span id="5f31" class="mj mk iq mf b gy mp mm l mn mo">y = train_df['fare_amount']<br/>train_df = train_df.drop(columns=['fare_amount'])</span><span id="bd76" class="mj mk iq mf b gy mp mm l mn mo">print(train_df.head())</span><span id="8570" class="mj mk iq mf b gy mp mm l mn mo">x_train,x_test,y_train,y_test = train_test_split(train_df,y,random_state=123,test_size=0.10)</span></pre><p id="3608" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Train_df 现在有了这些附加列:</p><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mr"><img src="../Images/aaeb3b861e4f06e1459583ae92f93bea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WHbumIQxiz_8ee8zhTBaJw.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">train_df with the additional columns after data processing.</figcaption></figure><p id="a108" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以去掉 train_df，因为我们现在已经将数据分为 x_train 和 x_test 以及 y_train 和 y_test。</p><pre class="lv lw lx ly gt me mf mg mh aw mi bi"><span id="0713" class="mj mk iq mf b gy ml mm l mn mo">del train_df<br/>del y<br/>gc.collect()</span></pre><p id="97e4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在的关键部分是设置用于 LightGBM 的参数。我调整的关键参数是 max_bin，learning_rate，num_leaves。另一个重要参数是升压类型。在这里，我尝试了 dart(<a class="ae kw" href="https://arxiv.org/abs/1505.01866" rel="noopener ugc nofollow" target="_blank">droppets meet Multiple Additive Regression Trees</a>)，因为它应该有助于实现更高的准确性。</p><pre class="lv lw lx ly gt me mf mg mh aw mi bi"><span id="918a" class="mj mk iq mf b gy ml mm l mn mo">params = {<br/>        'boosting_type':'dart',<br/>        'objective': 'regression',<br/>        'nthread': 4,<br/>        'num_leaves': 31,<br/>        'learning_rate': 0.05,<br/>        'max_depth': -1,<br/>        'subsample': 0.8,<br/>        'bagging_fraction' : 1,<br/>        'max_bin' : 5000 ,<br/>        'bagging_freq': 20,<br/>        'colsample_bytree': 0.6,<br/>        'metric': 'rmse',<br/>        'min_split_gain': 0.5,<br/>        'min_child_weight': 1,<br/>        'min_child_samples': 10,<br/>        'scale_pos_weight':1,<br/>        'zero_as_missing': True,<br/>        'seed':0,<br/>        'num_rounds':50000<br/>    }</span></pre><p id="a3e5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们开始使用 LightGBM，并将这些参数输入模型。我们删除最后不需要的数据帧以节省内存。</p><pre class="lv lw lx ly gt me mf mg mh aw mi bi"><span id="0af2" class="mj mk iq mf b gy ml mm l mn mo">train_set = lgbm.Dataset(x_train, y_train, silent=False,categorical_feature=['year','month','day','weekday'])<br/>valid_set = lgbm.Dataset(x_test, y_test, silent=False,categorical_feature=['year','month','day','weekday'])<br/>model = lgbm.train(params, train_set = train_set, num_boost_round=10000,early_stopping_rounds=500,verbose_eval=500, valid_sets=valid_set)<br/>del x_train<br/>del y_train<br/>del x_test<br/>del y_test<br/>gc.collect()</span></pre><p id="2aa2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们现在可以测试模型并将结果输入到 test_df 数据帧中。</p><pre class="lv lw lx ly gt me mf mg mh aw mi bi"><span id="d8fe" class="mj mk iq mf b gy ml mm l mn mo">test_df =  pd.read_csv('test.csv')<br/>print(test_df.head())<br/>test_df = add_datetime_info(test_df)<br/>test_df = add_airport_dist(test_df)<br/>test_df['distance'] = sphere_dist(test_df['pickup_latitude'], test_df['pickup_longitude'], <br/>                                   test_df['dropoff_latitude'] , test_df['dropoff_longitude'])</span><span id="0a20" class="mj mk iq mf b gy mp mm l mn mo">test_df['bearing'] = sphere_dist_bear(test_df['pickup_latitude'], test_df['pickup_longitude'], <br/>                                    test_df['dropoff_latitude'] , test_df['dropoff_longitude'])  <br/>test_df['pickup_latitude'] = radian_conv(test_df['pickup_latitude'])<br/>test_df['pickup_longitude'] = radian_conv(test_df['pickup_longitude'])<br/>test_df['dropoff_latitude'] = radian_conv(test_df['dropoff_latitude'])<br/>test_df['dropoff_longitude'] = radian_conv(test_df['dropoff_longitude'])</span><span id="1e8c" class="mj mk iq mf b gy mp mm l mn mo">test_key = test_df['key']<br/>test_df = test_df.drop(columns=['key', 'pickup_datetime'])</span></pre><p id="2da4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后输出模型预测，并输入到 Kaggle 的提交文件中！</p><pre class="lv lw lx ly gt me mf mg mh aw mi bi"><span id="2ad0" class="mj mk iq mf b gy ml mm l mn mo">prediction = model.predict(test_df, num_iteration = model.best_iteration)      <br/>submission = pd.DataFrame({<br/>        "key": test_key,<br/>        "fare_amount": prediction<br/>})</span><span id="caa2" class="mj mk iq mf b gy mp mm l mn mo">submission.to_csv('taxi_fare_submission_800k.csv',index=False)</span></pre><p id="6fe9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我使用 dart boosting 方法在整个 22m 行中取得的最好成绩是 2.88。我在总共 1488 支队伍中取得了第 79 名。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><figure class="lv lw lx ly gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/a156db8b16b4379d193b190e0f34a7b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*2nPch75N4Ci7JQEZbHTyRA.png"/></div></figure><p id="1fbe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在让我们来看看 Yandex 的 Catboost。因为我们已经有了所需格式的数据，所以使用 CatBoost 只需要几行代码。</p><pre class="lv lw lx ly gt me mf mg mh aw mi bi"><span id="667b" class="mj mk iq mf b gy ml mm l mn mo">from catboost import Pool, CatBoostRegressor</span><span id="2011" class="mj mk iq mf b gy mp mm l mn mo">train_pool = Pool(x_train, y_train, cat_features=[5,6,7,8])<br/>test_pool = Pool(x_test, y_test, cat_features=[5,6,7,8])<br/>model = CatBoostRegressor(iterations=4000, depth=10, learning_rate=1, loss_function='RMSE')<br/>model.fit(train_pool)<br/>preds2 = model.predict(test_df)<br/>print(preds2)</span><span id="9c86" class="mj mk iq mf b gy mp mm l mn mo">submission = pd.DataFrame({<br/>        "key": test_key,<br/>        "fare_amount": prediction<br/>})</span><span id="a64f" class="mj mk iq mf b gy mp mm l mn mo">submission.to_csv('taxi_fare_submission_800k_cat_boost_it4k_depth10.csv',index=False)</span></pre><p id="1e9f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用 Catboost，我使用全部 22m 行只处理了 3.13。</p><p id="b75b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了优化模型的参数，以及正确地比较 LightGBM 和 CatBoost，我们需要进行更多的实验，并很好地了解决策树和随机森林的作用。</p><p id="b3cd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">看看其他人用类似的方法用 LightGBM 和 CatBooost 实现了什么会很有趣。</p></div></div>    
</body>
</html>