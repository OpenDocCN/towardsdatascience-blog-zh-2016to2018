<html>
<head>
<title>Fighting Cancer with Artificial Intelligence: Part 0 — Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用人工智能对抗癌症:第0部分——深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fighting-cancer-with-artificial-intelligencepart-0-deep-learning-a6f0b375c8c?source=collection_archive---------3-----------------------#2018-02-13">https://towardsdatascience.com/fighting-cancer-with-artificial-intelligencepart-0-deep-learning-a6f0b375c8c?source=collection_archive---------3-----------------------#2018-02-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/a9dfb72229230713d521bc8dbd280fdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bFNHm1fWoS6Pa7irwXrJLw.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Cutaneous anaplastic large cell lymphoma (ALCL) <a class="ae jd" href="https://commons.wikimedia.org/w/index.php?curid=22119671" rel="noopener ugc nofollow" target="_blank">By L. Wozniak &amp; K. W. Zielinski — Own work, CC BY-SA 3.0</a></figcaption></figure><div class=""/><p id="5a4e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">今年8月，我听到了没有人想从他们的医生那里听到的话:“你得了癌症。”我被诊断患有一种罕见的非霍奇金淋巴瘤。经过几周混乱的测试和第二意见后，很明显我的预测是好的。几个月的治疗让我开始思考我的运气；尽管我不得不忍受癌症，但我很幸运有一个良好的预后。我发现自己在思考一个古老的问题，“这有什么原因吗？”</p><p id="d286" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这段时间里，我真正开始钻研数据科学。我开始思考机器学习(ML)和人工智能(AI)对癌症治疗的影响及其对预后的影响。鉴于我的情况，我想知道我是否应该开始专注于研究数据科学的这个领域。这篇文章是这样做的第一步。</p><p id="4505" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2018年1月12日星期五，我的医生很高兴地告诉我，我的PET扫描结果很清楚。这一系列文章对我来说是一种庆祝和宣泄。在第一篇文章中，我将介绍深度学习。</p><p id="208d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度学习正在开发，以帮助<a class="ae jd" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5447633/pdf/kjr-18-570.pdf" rel="noopener ugc nofollow" target="_blank">阅读放射学扫描</a> (PET/CT，MRI，X射线等)。)以提高癌症的早期检测，帮助<a class="ae jd" href="http://news.mit.edu/2017/artificial-intelligence-early-breast-cancer-detection-1017" rel="noopener ugc nofollow" target="_blank">减少乳腺癌筛查中的假阳性和不必要的手术</a>，以<a class="ae jd" href="https://www.usatoday.com/story/tech/talkingtech/2017/10/30/artificial-intelligence-detect-colorectal-cancer-polyps/812859001/" rel="noopener ugc nofollow" target="_blank">实现结肠直肠癌筛查期间息肉</a>的近实时分析，以及其他提高检测的方法。我将在随后的文章中看到这些。</p><h1 id="6c90" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">什么是深度学习？</h1><p id="8d7c" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">由于我将探索的大多数与癌症诊断相关的工作都严重依赖于深度学习，所以我决定该系列应该从深度学习的一些基础知识的简短介绍开始。<br/> <strong class="kf jh">免责声明:</strong>我不是深度学习方面的专家，但我已经对这篇文章的准确性进行了审查。</p><p id="8661" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度学习是机器学习领域中人工智能的子集。它使用大型神经网络来学习数据中哪些特征是重要的，以及如何利用这些特征来预测新数据集[ <a class="ae jd" href="#6c28" rel="noopener ugc nofollow"> 1 </a>、<a class="ae jd" href="#6c28" rel="noopener ugc nofollow"> 2 </a>、<a class="ae jd" href="#6c28" rel="noopener ugc nofollow"> 3 </a> ]。</p><p id="82ce" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们所说的(有监督的)学习是什么意思？嗯，机器学习算法需要三样东西才能发挥作用:<strong class="kf jh">输入数据</strong>，预期输出的例子(<strong class="kf jh">训练数据</strong>，以及一个<strong class="kf jh">反馈信号</strong>来表明算法执行得有多好。本质上，机器学习算法以一种有用的方式转换数据，从而产生有意义的输出。例如，<a class="ae jd" href="https://www.nature.com/articles/nature21056.epdf?referrer_access_token=NmdMKd94tRltrBVuaNZJK9RgN0jAjWel9jnR3ZoTv0NXpMHRAJy8Qn10ys2O4tuPQzN7VitPjMSrm-_eh79EcExo1WAnnlf70oBfM5FzHYKLGatw3xdEW8Mu3-AsAbrzzyZabuLGpTaPjDP2pS2WhJPedPB5crRYPsoXLY4wMZb-bZpqrwviB3OTPBiv-tp8&amp;tracking_referrer=www.medscape.com" rel="noopener ugc nofollow" target="_blank">我将查阅的一篇论文</a>拍摄了一张皮肤病变的手机照片，并输出了该病变是否为恶性的预测。因此，在这种情况下，学习就是自动寻找更好的表达方式。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi me"><img src="../Images/75dc935bbeefccee996344c4c5c4a1c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*BYEEzDv6Fp94FcG8o0lsLw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd mj">FIG 1</strong>: Diagram of a multi-layer feedforward artificial neural network. By Chrislb (derivative work: — HELLKNOWZ, TALK, enWP TALK ), <a class="ae jd" href="https://creativecommons.org/licenses/by-sa/3.0" rel="noopener ugc nofollow" target="_blank">CC BY-SA 3.0</a> via <a class="ae jd" href="https://commons.wikimedia.org/wiki/File:MultiLayerNeuralNetworkBigger_english.png" rel="noopener ugc nofollow" target="_blank">Wikimedia Commons</a></figcaption></figure><p id="89f1" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">我说深度学习用的是大型神经网络，但是<em class="mk">是什么</em>是神经网络？</strong>神经网络是基于人类对大脑的理解。与大脑不同，在大脑中，神经元可以与任何其他神经元建立连接，神经网络被离散成具有信息流方向性的层[ <a class="ae jd" href="#6c28" rel="noopener ugc nofollow"> 4 </a>，<a class="ae jd" href="#6c28" rel="noopener ugc nofollow"> 5 </a> ]。这在图1 的<a class="ae jd" href="#5960" rel="noopener ugc nofollow">中进行了说明。数据被传递到第一层神经元并被处理。处理后的输出被向前传递到另一层，依此类推，直到它返回一个结果。</a></p><h2 id="ff50" class="ml lc jg bd ld mm mn dn lh mo mp dp ll ko mq mr lp ks ms mt lt kw mu mv lx mw bi translated">神经网络的基础——神经元</h2><p id="c512" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">神经网络由称为神经元的类似物组成。我们将看看神经元的两种常见实现:感知器和sigmoid神经元[ <a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 1 </a> ]。历史上，神经网络起源于感知器。感知器最初是在20世纪50年代和60年代由弗兰克·罗森布拉特(Frank Rosenblatt)提出的。</p><p id="857d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个感知机本质上表现得像一个<a class="ae jd" href="https://en.wikipedia.org/wiki/Logic_gate" rel="noopener ugc nofollow" target="_blank">逻辑门</a> [ <a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 3 </a>，<a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 4 </a>，<a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 5 </a>，<a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 6 </a>。如果你对逻辑门不熟悉，它们是实现布尔逻辑[ <a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 3 </a>、<a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 4 </a>、<a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 6 </a> ]的模拟电路。它们将获取输入值并对其进行处理，以确定是否满足特定条件[ <a class="ae jd" href="#e4ec" rel="noopener ugc nofollow">图2 </a> ]。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mx"><img src="../Images/f5ea2c3d15ad5446f774821b0f7a766f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a9USyRj6E4rQsby6EFCLTA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd mj">FIG 2</strong>: A schematic representation of a neuron. The inputs, <strong class="bd mj">X</strong>, are weighted (indicated by the varying width of the arrows) and processed by the neuron’s activation function producing the output, y.</figcaption></figure><p id="e44b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个特别重要的门是<a class="ae jd" href="https://en.wikipedia.org/wiki/NAND_gate" rel="noopener ugc nofollow" target="_blank">与非门</a>(一个负与门)。与非门在计算中是通用的，这意味着任何其他布尔函数都可以表示为与非门[ <a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 1 </a>、<a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 7 </a>、<a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 8 </a> ]的组合。这种想法的含义是，由于感知器可以实现与非门，感知器在计算中也是通用的[ <a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 1 </a> ]。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi my"><img src="../Images/28aaf861d80238d4743bbfb0aee14189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*z8g_yxUWrbGyL6i0PZcKxA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd mj">FIG 3</strong>: Illustration of the threshold function. This variation has a third component: if the product of the inputs is equal to the bias, that is they sum to zero, the output is 0.5. By PAR~commonswiki, <a class="ae jd" href="http://creativecommons.org/licenses/by-sa/3.0/" rel="noopener ugc nofollow" target="_blank">CC-BY-SA-3.0</a> via <a class="ae jd" href="https://commons.wikimedia.org/wiki/File:Dirac_distribution_CDF.png" rel="noopener ugc nofollow" target="_blank">Wikimedia Commons</a></figcaption></figure><p id="b24f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">神经元的两种实现在它们的<strong class="kf jh">激活/传递函数</strong>(决定神经元是开还是关的方程)[ <a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 1 </a>，<a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 5 </a> ]上有所不同。罗森布拉特感知机的美妙之处在于它的简单性。感知器将接受输入，并根据它们的权重组合它们，如果输入表明被分析的对象是该类的成员，则输出值1，如果不是成员，则输出值0[<a class="ae jd" href="#e4ec" rel="noopener ugc nofollow">图2 </a> ]。感知器使用阈值函数，该函数将输出值0，除非加权输入的和大于感知器的<strong class="kf jh">偏差</strong>或<strong class="kf jh">阈值</strong>，在这种情况下，它将输出1 [ <a class="ae jd" href="#283f" rel="noopener ugc nofollow">图3 </a>、<a class="ae jd" href="#62f0" rel="noopener ugc nofollow">图4 </a>。</p><p id="62f0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">阈值函数的问题是，感知器的权重或偏差的微小变化会在输出[ <a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 1 </a> ]的行为中产生难以控制的结果。这是因为激活函数的<em class="mk">全有或全无</em>性质。当试图优化神经元网络时，这就成了一个问题。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mz"><img src="../Images/f3621d0a2bbe341e30731ca63cb88469.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Io9QXZQi2vuqpF8yWMVZbg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd mj">FIG 4</strong>: Vector representations of the activation functions of the two main types of neurons.</figcaption></figure><p id="a401" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以通过使用一个<strong class="kf jh"> sigmoid函数</strong> [ <a class="ae jd" href="#7f56" rel="noopener ugc nofollow">图5 </a> ]而不是阈值函数来避免这个问题。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi my"><img src="../Images/ade708bc324bf1c955b435acf7d13938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*SqC9N88gCl2p2LfQaDxbxw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd mj">FIG 5</strong>: Illustration the the sigmoid function. Public Domain via <a class="ae jd" href="https://en.wikipedia.org/wiki/File:Logistic-curve.svg" rel="noopener ugc nofollow" target="_blank">Wikimedia Commons</a></figcaption></figure><p id="fa81" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">sigmoid函数(也称为逻辑函数，因为它是所用sigmoid函数的特殊形式)具有扁平S形的特性[ <a class="ae jd" href="#7f56" rel="noopener ugc nofollow">图5 </a> ]。这与阈值函数曲线的“开/关”形状相比较[ <a class="ae jd" href="#283f" rel="noopener ugc nofollow">图3 </a> ]。</p><p id="b340" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">sigmoid函数为神经元的输出产生更平滑的过渡。当我们接近图形的边缘时，神经元的行为就像感知器输出大约0或1。神经元将随着<strong class="kf jh"> wx </strong> + <em class="mk"> b </em>的值变大而开启，随着其变小而关闭。零附近的区域是我们行为变化最大的地方。这个平滑和连续的区域允许我们对感知器的权重和偏差进行小的改变，而不会产生阈值函数可能产生的输出的大的改变。事实上，乙状结肠神经元是这些权重和偏差变化的线性函数[ <a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 1 </a> ]。这两个函数之间的另一个区别是，sigmoid函数的允许输出是0和1之间的任何值，而threshold函数将输出0或1 [ <a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 1 </a> ]。这对于确定某些特征是有用的，但是当应用于分类时就不那么清楚了[ <a class="ae jd" href="#63b5" rel="noopener ugc nofollow"> 1 </a> ]。解释这个单个神经元输出的一种方法是作为属于正在讨论的类别的概率。</p><h2 id="3a52" class="ml lc jg bd ld mm mn dn lh mo mp dp ll ko mq mr lp ks ms mt lt kw mu mv lx mw bi translated">将神经元结合成网络</h2><p id="3635" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">单个神经元对于做出单个决定是有用的，但是我们如何扩展它们的有用性呢？在<a class="ae jd" href="#5960" rel="noopener ugc nofollow">图1 </a>中，网络显示多个神经元可以组合起来组成一层神经元。只看图中的输入层，我们可以看到它由三个神经元组成，因此它能够做出三个非常简单的决策(尽管由于有两个输出神经元，网络本身只会进行两种分类)。我们可以通过增加层中神经元的数量来扩展简单决策的数量。该层的输出是数据[ <a class="ae jd" href="#cc0a" rel="noopener ugc nofollow"> 1 </a>，<a class="ae jd" href="#cc0a" rel="noopener ugc nofollow"> 2 </a> ]的<strong class="kf jh">表示</strong>。理想情况下，该表示提供了对正在解决的问题的一些洞察[ <a class="ae jd" href="#0c6b" rel="noopener ugc nofollow">图6 </a> ]。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi na"><img src="../Images/0c5fbc4ba00503cba5b917b460889f1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G460NWZ1vSN1YiwPOdedsw.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd mj">FIG 6</strong>: Example mean activation shapes in a learned hierarchy. These are the representations of the data that are learned by the network. <a class="ae jd" href="http://www.bmva.org/bmvc/2009/Papers/Paper443/Paper443.html" rel="noopener ugc nofollow" target="_blank">ViCoS Lab</a></figcaption></figure><p id="7571" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">神经元层可以堆叠起来进一步处理数据。我们可以把每一个后续层看作是获取前一层的输出，并进一步提取数据中包含的信息。多层网络能够提取足够的数据来做出复杂的决策。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nb"><img src="../Images/752a5c4e0d607ae2ab0eb34a79801f6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f5eUnIX54XQH3ag3gQBIvw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd mj">FIG 7</strong>: A schematic representation of a multi-layer neural network. The input layer takes data from <strong class="bd mj">X</strong> and processes it and passes it to the first hidden layer. Each hidden layer processes and passes the data to the next. The output layer makes a prediction <strong class="bd mj">&lt;y&gt;</strong> that is an array of probabilities of the input belonging to each of the possible classes. The result is checked by the loss function to determine the networks accuracy and this is passed to the optimizer which adjusts the weights of the information passing to each layer.</figcaption></figure><p id="41c9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图7 中显示了一个多层网络的例子。一些输入数据由神经网络的第一层转换(输入层<strong class="kf jh"/>)。转换后的数据由第二层处理(称为<strong class="kf jh">隐藏层</strong>，因为它既不是输入也不是输出)[ <a class="ae jd" href="#cc0a" rel="noopener ugc nofollow"> 1 </a>。这个输出将或者到下一个隐藏层或者到输出层。<strong class="kf jh">输出层</strong>将返回一个概率分数数组(其总和为1) [ <a class="ae jd" href="#cc0a" rel="noopener ugc nofollow"> 2 </a> ]。</p><p id="2e05" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">网络的输出通过<strong class="kf jh">损失函数(或反馈信号)</strong>进行分析，该函数测量预测的准确性<a class="ae jd" href="#cc0a" rel="noopener ugc nofollow"> 2 </a>。网络的权重由一个<strong class="kf jh">优化器</strong> [ <a class="ae jd" href="#cc0a" rel="noopener ugc nofollow"> 2 </a> ]改变。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/456cbc9b95f50c64f33ac68d02aef18b.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*HsGfDZXgGeEbvyBH-b5RMQ.jpeg"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd mj">FIG 8</strong>: A schematic of the information flow during optimization. The data loss shows the difference between the scores and the labels. The regularization loss is only dependent on the weights. Gradient descent allows us to find the slope of the weights and update them. <a class="ae jd" href="http://cs231n.github.io/optimization-1/#summary" rel="noopener ugc nofollow" target="_blank">Optimization: Stochastic Gradient Descent</a>.</figcaption></figure><p id="9cc0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">优化器使用<strong class="kf jh">梯度下降</strong>来确定如何更新权重。本质上，该算法遵循损失函数表面的斜率，直到找到一个谷[ <a class="ae jd" href="#cc0a" rel="noopener ugc nofollow"> 3 </a>、<a class="ae jd" href="#cc0a" rel="noopener ugc nofollow"> 4 </a>、<a class="ae jd" href="#5257" rel="noopener ugc nofollow">图8 </a> ]。该信息通过<strong class="kf jh">反向传播的过程得到增强:</strong>“反向传播可以[……]被认为是门相互通信(通过梯度信号)它们是否希望它们的输出增加或减少(以及多强烈)，以便使最终输出值更高”<a class="ae jd" href="#cc0a" rel="noopener ugc nofollow"> 5 </a>。反向传播提供了关于改变权重和偏差将如何影响整个网络的详细信息。[ <a class="ae jd" href="#cc0a" rel="noopener ugc nofollow"> 6 </a></p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nd"><img src="../Images/9cc8cdd7f837c565d3bcb4e52a4f0643.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G7xNYtuJX2YeffHNnBpU-Q.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd mj">FIG 9</strong>: Detailed activation of neurons in five layers. The activations show the representations that are deemed important by the network. <a class="ae jd" href="https://arxiv.org/pdf/1311.2901.pdf" rel="noopener ugc nofollow" target="_blank">Zeiler and Fergus, 2013</a>.</figcaption></figure><h2 id="9f39" class="ml lc jg bd ld mm mn dn lh mo mp dp ll ko mq mr lp ks ms mt lt kw mu mv lx mw bi translated">包裹</h2><p id="e5b8" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">总而言之:</p><ul class=""><li id="d8ff" class="ne nf jg kf b kg kh kk kl ko ng ks nh kw ni la nj nk nl nm bi translated">深度学习在癌症检测、活检分析中发挥着重要作用，并为在难以找到医生的地区增加筛查提供了可能性</li><li id="e3e0" class="ne nf jg kf b kg nn kk no ko np ks nq kw nr la nj nk nl nm bi translated">神经网络的基本构件是神经元</li><li id="029c" class="ne nf jg kf b kg nn kk no ko np ks nq kw nr la nj nk nl nm bi translated">神经元可以用不同的激活函数起作用，每个激活函数都有其优点和缺点</li><li id="a477" class="ne nf jg kf b kg nn kk no ko np ks nq kw nr la nj nk nl nm bi translated">神经元可以堆叠成层，并且这些层可以堆叠在一起，这增加了网络可以做出的决策的复杂性</li><li id="f3c2" class="ne nf jg kf b kg nn kk no ko np ks nq kw nr la nj nk nl nm bi translated">这些决策提供了数据的新表示[ <a class="ae jd" href="#0c6b" rel="noopener ugc nofollow">图6 </a>、<a class="ae jd" href="#8669" rel="noopener ugc nofollow">图9 </a> ],这些数据有望<em class="mk">提供洞察力</em></li><li id="f681" class="ne nf jg kf b kg nn kk no ko np ks nq kw nr la nj nk nl nm bi translated">可以使用损失函数和反向传播来优化神经元的输出，以提高网络的复杂度</li></ul><p id="b5c3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢<a class="ae jd" href="https://towardsdatascience.com/@irhumshafkat" rel="noopener" target="_blank">伊尔胡姆·沙夫卡特</a>和<a class="ae jd" href="https://medium.com/@generativist" rel="noopener">约翰·比约恩·尼尔森</a>阅读我的草稿，提出很好的建议并确保准确性。我还要感谢我的妻子校对并提出建议，以提高本文的流畅度和清晰度。我要特别感谢我的朋友们，他们对技术的不同熟悉程度让我确信这能被广大的观众所接受。</p><p id="eec4" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望您喜欢这篇关于深度学习的介绍，并会回来阅读本系列的下一篇文章。<a class="ae jd" href="http://akdm.ml" rel="noopener ugc nofollow" target="_blank">关注我</a>获取新帖子通知；留下评论让我知道你的想法；<a class="ae jd" href="https://twitter.com/kaumaron" rel="noopener ugc nofollow" target="_blank">发送带有反馈的推文</a>或开始对话；或者看看我的<a class="ae jd" href="http://decotiismauro.ml" rel="noopener ugc nofollow" target="_blank">项目组合</a>。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="7b42" class="lb lc jg bd ld le nz lg lh li oa lk ll lm ob lo lp lq oc ls lt lu od lw lx ly bi translated">参考</h1><p id="5e54" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">标题将链接到适当的资源，而[<a class="ae jd" href="#7b42" rel="noopener ugc nofollow">^</a>将带你回到你正在阅读的部分。</p><h2 id="73d8" class="ml lc jg bd ld mm mn dn lh mo mp dp ll ko mq mr lp ks ms mt lt kw mu mv lx mw bi translated">什么是深度学习？</h2><p id="6c28" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf jh">【1】</strong>机器学习精通。<a class="ae jd" href="https://machinelearningmastery.com/what-is-deep-learning/" rel="noopener ugc nofollow" target="_blank">什么是深度学习？</a><a class="ae jd" href="#6c90" rel="noopener ugc nofollow">^</a><br/><strong class="kf jh">【2】</strong>英伟达博客。<a class="ae jd" href="https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/" rel="noopener ugc nofollow" target="_blank">AI和ML有什么区别？</a><a class="ae jd" href="#6c90" rel="noopener ugc nofollow">【^】</a><br/><strong class="kf jh">【3】</strong>技术评审。深度学习:凭借海量的计算能力，机器现在可以实时识别物体和翻译语音。人工智能终于变聪明了。<a class="ae jd" href="#6c90" rel="noopener ugc nofollow">^</a><br/><strong class="kf jh">【4】</strong>弗朗索瓦·乔莱。<a class="ae jd" href="https://www.manning.com/books/deep-learning-with-python" rel="noopener ugc nofollow" target="_blank">用Python进行深度学习</a>。第一章。[<a class="ae jd" href="#6c90" rel="noopener ugc nofollow">【^】</a><br/><strong class="kf jh"><br/></strong><a class="ae jd" href="http://michaelnielsen.org/" rel="noopener ugc nofollow" target="_blank">迈克尔尼尔森</a>。<a class="ae jd" href="http://neuralnetworksanddeeplearning.com/chap1.html" rel="noopener ugc nofollow" target="_blank">神经网络和深度学习</a>。第一章。^ </p><h2 id="63b5" class="ml lc jg bd ld mm mn dn lh mo mp dp ll ko mq mr lp ks ms mt lt kw mu mv lx mw bi translated">神经网络的基础——神经元</h2><p id="bfba" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf jh"/><a class="ae jd" href="http://michaelnielsen.org/" rel="noopener ugc nofollow" target="_blank">迈克尔·尼尔森</a>。<a class="ae jd" href="http://neuralnetworksanddeeplearning.com/chap1.html" rel="noopener ugc nofollow" target="_blank">神经网络和深度学习</a>。第一章。【<a class="ae jd" href="#ff50" rel="noopener ugc nofollow"/><br/><strong class="kf jh">【2】</strong>维基百科。弗兰克·罗森布拉特——感知器。【<a class="ae jd" href="#ff50" rel="noopener ugc nofollow">^</a><br/><strong class="kf jh">【3】</strong>维基百科。<a class="ae jd" href="https://en.wikipedia.org/wiki/Logic_gate" rel="noopener ugc nofollow" target="_blank">逻辑门。</a><a class="ae jd" href="#ff50" rel="noopener ugc nofollow">^</a><br/><strong class="kf jh">【4】</strong>数据怀疑论者。<a class="ae jd" href="https://dataskeptic.com/blog/episodes/2017/feed-forward-neural-networks" rel="noopener ugc nofollow" target="_blank">前馈神经网络</a>。[<a class="ae jd" href="#ff50" rel="noopener ugc nofollow">^</a>]<br/><strong class="kf jh"/>凯文·斯温勒。第二讲:单层感知器。^，弗朗西斯克·卡米洛。<a class="ae jd" rel="noopener" target="_blank" href="/neural-representation-of-logic-gates-df044ec922bc">逻辑门的神经表示。</a><a class="ae jd" href="#ff50" rel="noopener ugc nofollow">【^】</a><br/><strong class="kf jh">【7】</strong>百科。<a class="ae jd" href="https://en.wikipedia.org/wiki/NAND_gate" rel="noopener ugc nofollow" target="_blank">与非门</a>。【<a class="ae jd" href="#ff50" rel="noopener ugc nofollow">^</a><br/><strong class="kf jh"/>维基百科。<a class="ae jd" href="https://en.wikipedia.org/wiki/NAND_logic#Making_other_gates_by_using_NAND_gates" rel="noopener ugc nofollow" target="_blank">与非门逻辑——利用与非门制作其他门</a>。^ </p><h2 id="cc0a" class="ml lc jg bd ld mm mn dn lh mo mp dp ll ko mq mr lp ks ms mt lt kw mu mv lx mw bi translated">将神经元结合成网络</h2><p id="a897" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf jh"/><a class="ae jd" href="http://michaelnielsen.org/" rel="noopener ugc nofollow" target="_blank">迈克尔·尼尔森</a>。<a class="ae jd" href="http://neuralnetworksanddeeplearning.com/chap1.html" rel="noopener ugc nofollow" target="_blank">神经网络和深度学习</a>。第一章。[<a class="ae jd" href="#3a52" rel="noopener ugc nofollow">^</a><br/><strong class="kf jh">【2】</strong>弗朗索瓦·乔莱。<a class="ae jd" href="https://www.manning.com/books/deep-learning-with-python" rel="noopener ugc nofollow" target="_blank">用Python进行深度学习</a>。第二章。[<a class="ae jd" href="#3a52" rel="noopener ugc nofollow">^</a>]<br/><strong class="kf jh">【3】</strong>塞巴斯蒂安·鲁德。<a class="ae jd" href="http://ruder.io/optimizing-gradient-descent/" rel="noopener ugc nofollow" target="_blank">梯度下降优化算法概述</a>。【<a class="ae jd" href="#3a52" rel="noopener ugc nofollow">^</a><br/><strong class="kf jh">【4】</strong><a class="ae jd" href="http://cs.stanford.edu/people/karpathy/" rel="noopener ugc nofollow" target="_blank">安德烈·卡帕西</a>。<a class="ae jd" href="http://cs231n.stanford.edu/" rel="noopener ugc nofollow" target="_blank"> CS231n:视觉识别的卷积神经网络</a>。<a class="ae jd" href="http://cs231n.github.io/optimization-1/" rel="noopener ugc nofollow" target="_blank">优化:随机梯度下降</a>。【<a class="ae jd" href="#3a52" rel="noopener ugc nofollow">^</a><br/><strong class="kf jh"/><a class="ae jd" href="http://cs.stanford.edu/people/karpathy/" rel="noopener ugc nofollow" target="_blank">安德烈·卡帕西</a>。<a class="ae jd" href="http://cs231n.stanford.edu/" rel="noopener ugc nofollow" target="_blank"> CS231n:视觉识别的卷积神经网络</a>。<a class="ae jd" href="http://cs231n.github.io/optimization-2/" rel="noopener ugc nofollow" target="_blank">反向传播，直觉</a>。【<a class="ae jd" href="#3a52" rel="noopener ugc nofollow">^</a><br/><strong class="kf jh"/><a class="ae jd" href="http://michaelnielsen.org/" rel="noopener ugc nofollow" target="_blank">迈克尔尼尔森</a>。<a class="ae jd" href="http://neuralnetworksanddeeplearning.com/chap2.html" rel="noopener ugc nofollow" target="_blank">神经网络和深度学习</a>。第二章。^ </p></div></div>    
</body>
</html>