<html>
<head>
<title>Extracting Keywords From Short Text</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从短文本中提取关键词</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/extracting-keywords-from-short-text-fce39157166b?source=collection_archive---------0-----------------------#2017-07-27">https://towardsdatascience.com/extracting-keywords-from-short-text-fce39157166b?source=collection_archive---------0-----------------------#2017-07-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/60d1a8bc40d18c2688e375ccfc0a2e50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*lNZBf283qKB8nccPyvEFxw.png"/></div></figure><blockquote class="ju jv jw"><p id="f60c" class="jx jy jz ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="iq">专业提示:更多关于人工智能和机器学习的文章、故事、代码和研究，请访问我的网站:</em></strong><a class="ae kw" href="http://www.theapemachine.com" rel="noopener ugc nofollow" target="_blank"><strong class="ka ir"><em class="iq">【www.theapemachine.com】</em></strong></a></p></blockquote><p id="47d4" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">多年来，我多次遇到提取关键词的问题，尤其是从单句、标题或问题等短文本中提取关键词。</p><p id="e929" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">在NLP 中处理<a class="ae kw" href="http://www.wangzhongyuan.com/tutorial/ACL2016/Understanding-Short-Texts/" rel="noopener ugc nofollow" target="_blank">短文本一直是一个众所周知的问题，而识别关键词</a><a class="ae kw" href="https://www.google.fr/search?q=particularly&amp;spell=1&amp;sa=X&amp;ved=0ahUKEwjBkKeJlqLVAhXCWBoKHRCCDTYQBQglKAA" rel="noopener ugc nofollow" target="_blank">尤其是</a>很难做到。<br/>这些年来，我尝试了许多方法，从我自己基于一个非常流行的文字游戏非常天真地实现了一个简单的评分算法，到快速自动关键词提取(RAKE ),如果没有大型文档要处理，没有一种方法表现得非常好。</p><p id="69cb" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">最终，随着我对现代常见的机器学习模型越来越熟悉，我开始将关键词提取视为“简单！翻译问题，在这篇文章中，我们将回顾导致一个伟大的关键字提取器，我们现在可以在生产中使用的旅程。</p><p id="86ff" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">我第一次需要找到一个短句中最重要的关键词时，我正在为一家心理学家公司开发一些软件，他们正在开发一个平台，其中有一个模块需要从语言挑战数据库中提取相关的句子。</p><p id="4bb6" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">这个想法是要将要检索的句子与前一个句子的关键词进行匹配，由于我当时对机器学习知之甚少，所以我用自己的天真解决方案来计算句子中每个单词的拼字得分，并从这些结果中取出得分最高的第n个单词，并假设它们是关键词。</p><p id="9d76" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">如果不是因为一些在拼字游戏中得分很高的停用词，这实际上非常有效。<br/>当然，你可以在这个方法中部署一个非索引列表，事情就会迎刃而解，这甚至是相当可扩展的，因为任何语言都只包含你想要过滤掉的单词，你只需要定义这个列表一次。</p><h1 id="be7d" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">耙子</h1><p id="f340" class="pw-post-body-paragraph jx jy iq ka b kb ly kd ke kf lz kh ki kx ma kl km ky mb kp kq kz mc kt ku kv ij bi translated">下一个实现是快速自动关键词提取，它实际上只比前一个方法好一点点，并且仍然使用非索引列表。</p><p id="8689" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">如今，RAKE已经作为即插即用模块在许多编程语言中实现，您应该不用花太多时间来试验它，但是我警告您，不要让它在短文本上欺骗您。大多数时候，它实际上并没有提取任何关键词，只是删除了停用词。因为文本很短，所以没有那么多其他的词可以“搜索”。</p><h1 id="124a" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">发明一种新的语言！</h1><p id="fc0b" class="pw-post-body-paragraph jx jy iq ka b kb ly kd ke kf lz kh ki kx ma kl km ky mb kp kq kz mc kt ku kv ij bi translated">最终，我明白了一些事情:如果像序列到序列神经网络这样的现代翻译方法可以翻译语言，或者给出从序列中学习到的响应，为什么我们不能将关键词视为其自身的“语言”。<br/>一种没有语法的语言，可以翻译成(甚至可能翻译成？).</p><p id="8054" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">这似乎是一个很好的想法:简单地训练一个序列，对标有关键词的短句数据集进行神经网络排序，并让网络将新句子“翻译”成我们的关键词“语言”。</p><p id="29c8" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">所以让我们开始吧！</p><h1 id="d0d2" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">1.生成数据集</h1><p id="1737" class="pw-post-body-paragraph jx jy iq ka b kb ly kd ke kf lz kh ki kx ma kl km ky mb kp kq kz mc kt ku kv ij bi translated">现在我有了新方法，我面临的一个挑战是找到一个数据集进行训练，我相信你会认识到这是一个困难。可能会让你吃惊的是，真的没有任何现成的数据集有短句与文本中实际存在的关键词相匹配。当然，有许多关于关键词提取的数据集，但它们都有较大的文本，这些文本已经被手动标注了关键词。</p><p id="4bba" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">幸运的是，我身边的一个人给了我一个绝妙的主意:使用博客圈真正的大玩家的博客标题，并使用这些页面上的关键字元标签来找到与标题相关的关键字。事实上，这些博客非常关注SEO，因此我们可以假设这些数据是经过精心策划的。</p><p id="3fa8" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">剩下唯一要做的事情是确保删除所有实际上不在标题中的关键词，尽管你可能想考虑甚至保留这些关键词，因为一旦你的神经网络训练完毕，你不仅能够从一个句子中提取准确的关键词，甚至能够提取相关的关键词。<br/>不过，我会把这留给未来的实验。</p><p id="ea84" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">下面的Ruby脚本正是我用来在Lifehacker和TechCrunch上搜索关键词的脚本。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="fdf8" class="mm lb iq mi b gy mn mo l mp mq">require 'mechanize'<br/><br/><strong class="mi ir">class</strong> <strong class="mi ir">Scrape</strong><br/><br/>  <strong class="mi ir">def</strong> initialize<br/>    @mechanize = Mechanize.new<br/>    @visited   = []<br/>    @v_titles  = []<br/><br/>    @csv = File.open('/home/theapemachine/data/keywords-web2.csv', 'a')<br/>  <strong class="mi ir">end</strong><br/><br/>  <strong class="mi ir">def</strong> run(urls)<br/>    scraped_urls = []<br/><br/>    urls.each <strong class="mi ir">do</strong> |url|<br/>      <strong class="mi ir">if</strong> !@visited.include?(url)<br/>        <strong class="mi ir">begin</strong><br/>          rawpage = @mechanize.get(url)<br/><br/>          rawpage.links.uniq.each <strong class="mi ir">do</strong> |a|<br/>            scraped_urls &lt;&lt; a.href<br/>          <strong class="mi ir">end</strong><br/><br/>          page_title = ''<br/><br/>          rawpage.search('title').each <strong class="mi ir">do</strong> |title|<br/>            page_title = title.text.downcase.strip<br/>          <strong class="mi ir">end</strong><br/><br/>          keywords = rawpage.at('head meta[name="keywords"]')['content'].split(', ').delete_if{|k| page_title.scan(k.downcase).empty?}<br/><br/>          <strong class="mi ir">if</strong> !page_title.empty? &amp;&amp; !keywords.empty? &amp;&amp; !@v_titles.include?(page_title)<br/>            puts<br/>            puts "TITLE:    #{page_title}"<br/>            puts "KEYWORDS: #{keywords.join(',')}"<br/><br/>            @csv &lt;&lt; %("#{page_title}","#{keywords.join(',').downcase}"\n)<br/>          <strong class="mi ir">end</strong><br/><br/>          @v_titles &lt;&lt; page_title<br/>        <strong class="mi ir">rescue</strong><br/>        <strong class="mi ir">end</strong><br/>      <strong class="mi ir">end</strong><br/>    <strong class="mi ir">end</strong><br/><br/>    @visited += urls<br/>    run(scraped_urls.uniq)<br/>  <strong class="mi ir">end</strong><br/><br/><strong class="mi ir">end</strong><br/><br/>@scrape = Scrape.new<br/>@scrape.run([<br/>  'http://lifehacker.com/',<br/>  'http://www.techcrunch.com'<br/>])</span></pre><h1 id="f846" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">2.准备数据</h1><p id="0446" class="pw-post-body-paragraph jx jy iq ka b kb ly kd ke kf lz kh ki kx ma kl km ky mb kp kq kz mc kt ku kv ij bi translated">我们现在需要准备收集的数据，这样我们就可以在Pytorch序列中使用它来排序我接下来将列出的模型。我使用了下面非常简单的Ruby脚本来确保训练数据中的每一行首先有文章标题，然后是制表位，然后是关键字。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="441c" class="mm lb iq mi b gy mn mo l mp mq">require 'csv'<br/><br/>train = File.open('./keywords.train', 'w')<br/><br/>csv_text = File.read('/home/theapemachine/data/keywords-web.csv')<br/>csv      = CSV.parse(csv_text, headers: <strong class="mi ir">false</strong>)<br/><br/>csv.each <strong class="mi ir">do</strong> |row|<br/>  <strong class="mi ir">begin</strong><br/>    train.puts row[0] + "\t" + row[1]<br/>  <strong class="mi ir">rescue</strong><br/>  <strong class="mi ir">end</strong><br/><strong class="mi ir">end</strong></span></pre><h1 id="9f76" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">3.序列对序列模型</h1><p id="9e2e" class="pw-post-body-paragraph jx jy iq ka b kb ly kd ke kf lz kh ki kx ma kl km ky mb kp kq kz mc kt ku kv ij bi translated">我们现在可以使用Pytorch编写我们的序列到序列神经网络，事实上，简单地使用他们的教程部分列出的代码就可以很好地完成这个任务。我在培训周期结束时添加了一个简单的提示，这样我就可以轻松地测试网络。<br/>在以后的版本中，你会想要保存你的模型以备后用，因为在我的笔记本电脑上训练花了很长时间。</p><p id="dafa" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">由肖恩·罗伯逊(Sean Robertson)的原Pytorch <a class="ae kw" href="http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html" rel="noopener ugc nofollow" target="_blank">源代码</a>翻译而来。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="e404" class="mm lb iq mi b gy mn mo l mp mq"><strong class="mi ir">import</strong> unicodedata<br/><strong class="mi ir">import</strong> string<br/><strong class="mi ir">import</strong> re<br/><strong class="mi ir">import</strong> random<br/><strong class="mi ir">import</strong> time<br/><strong class="mi ir">import</strong> math<br/><br/><strong class="mi ir">import</strong> torch<br/><strong class="mi ir">import</strong> torch.nn <strong class="mi ir">as</strong> nn<br/><strong class="mi ir">from</strong> torch.autograd <strong class="mi ir">import</strong> Variable<br/><strong class="mi ir">from</strong> torch <strong class="mi ir">import</strong> optim<br/><strong class="mi ir">import</strong> torch.nn.functional <strong class="mi ir">as</strong> F<br/><br/>USE_CUDA  = True<br/>SOS_token = 0<br/>EOS_token = 1<br/><br/><strong class="mi ir">class</strong> <strong class="mi ir">Lang</strong>:<br/>    <strong class="mi ir">def</strong> __init__(self, name):<br/>        self.name       = name<br/>        self.word2index = {}<br/>        self.word2count = {}<br/>        self.index2word = {0: "SOS", 1: "EOS"}<br/>        self.n_words    = 2<br/><br/>    <strong class="mi ir">def</strong> index_words(self, sentence):<br/>        <strong class="mi ir">for</strong> word in sentence.split(' '):<br/>            self.index_word(word)<br/><br/>    <strong class="mi ir">def</strong> index_word(self, word):<br/>        <strong class="mi ir">if</strong> word not in self.word2index:<br/>            self.word2index[word]          = self.n_words<br/>            self.word2count[word]          = 1<br/>            self.index2word[self.n_words]  = word<br/>            self.n_words                  += 1<br/>        <strong class="mi ir">else</strong>:<br/>            self.word2count[word] += 1<br/><br/><strong class="mi ir">def</strong> unicode_to_ascii(s):<br/>    <strong class="mi ir">return</strong> ''.join(<br/>        c <strong class="mi ir">for</strong> c in unicodedata.normalize('NFD', u'' + s)<br/>        <strong class="mi ir">if</strong> unicodedata.category(c) != 'Mn'<br/>    )<br/><br/><strong class="mi ir">def</strong> normalize_string(s):<br/>    s = unicode_to_ascii(s.lower().strip())<br/>    s = re.sub(r"([.!?])", r" \1", s)<br/>    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)<br/><br/>    <strong class="mi ir">return</strong> s<br/><br/><strong class="mi ir">def</strong> read_langs(lang1, lang2, reverse=False):<br/>    <strong class="mi ir">print</strong>("Reading lines...")<br/><br/>    lines = open('keywords.train').read().strip().split('\n')<br/>    pairs = [[normalize_string(s) <strong class="mi ir">for</strong> s in l.split('\t')] <strong class="mi ir">for</strong> l in lines]<br/><br/>    <strong class="mi ir">if</strong> reverse:<br/>        pairs       = [list(reversed(p)) <strong class="mi ir">for</strong> p in pairs]<br/>        input_lang  = Lang(lang2)<br/>        output_lang = Lang(lang1)<br/>    <strong class="mi ir">else</strong>:<br/>        input_lang  = Lang(lang1)<br/>        output_lang = Lang(lang2)<br/><br/>    <strong class="mi ir">return</strong> input_lang, output_lang, pairs<br/><br/>MAX_LENGTH = 100<br/><br/>good_prefixes = (<br/>    "i am ", "i m ",<br/>    "he is", "he s ",<br/>    "she is", "she s",<br/>    "you are", "you re "<br/>)<br/><br/><strong class="mi ir">def</strong> filter_pair(p):<br/>    <strong class="mi ir">return</strong> len(p[0].split(' ')) &lt; MAX_LENGTH and len(p[1].split(' ')) &lt; MAX_LENGTH<br/><br/><strong class="mi ir">def</strong> filter_pairs(pairs):<br/>    <strong class="mi ir">return</strong> [pair <strong class="mi ir">for</strong> pair in pairs <strong class="mi ir">if</strong> filter_pair(pair)]<br/><br/><strong class="mi ir">def</strong> prepare_data(lang1_name, lang2_name, reverse=False):<br/>    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)<br/>    <strong class="mi ir">print</strong>("Read %s sentence pairs" % len(pairs))<br/><br/>    pairs = filter_pairs(pairs)<br/>    <strong class="mi ir">print</strong>("Trimmed to %s sentence pairs" % len(pairs))<br/><br/>    <strong class="mi ir">print</strong>("Indexing words...")<br/>    <strong class="mi ir">for</strong> pair in pairs:<br/>        input_lang.index_words(pair[0])<br/>        output_lang.index_words(pair[1])<br/><br/>    <strong class="mi ir">return</strong> input_lang, output_lang, pairs<br/><br/>input_lang, output_lang, pairs = prepare_data('eng', 'fra', False)<br/><strong class="mi ir">print</strong>(random.choice(pairs))<br/><br/><strong class="mi ir">def</strong> indexes_from_sentence(lang, sentence):<br/>    <strong class="mi ir">return</strong> [lang.word2index[word] <strong class="mi ir">for</strong> word in sentence.split(' ')]<br/><br/><strong class="mi ir">def</strong> variable_from_sentence(lang, sentence):<br/>    indexes = indexes_from_sentence(lang, sentence)<br/>    indexes.append(EOS_token)<br/><br/>    var = Variable(torch.LongTensor(indexes).view(-1, 1))<br/><br/>    <strong class="mi ir">if</strong> USE_CUDA:<br/>        var = var.cuda()<br/><br/>    <strong class="mi ir">return</strong> var<br/><br/><strong class="mi ir">def</strong> variables_from_pair(pair):<br/>    input_variable  = variable_from_sentence(input_lang, pair[0])<br/>    target_variable = variable_from_sentence(output_lang, pair[1])<br/><br/>    <strong class="mi ir">return</strong> (input_variable, target_variable)<br/><br/><strong class="mi ir">class</strong> <strong class="mi ir">EncoderRNN</strong>(nn.Module):<br/>    <strong class="mi ir">def</strong> __init__(self, input_size, hidden_size, n_layers=1):<br/>        super(EncoderRNN, self).__init__()<br/><br/>        self.input_size  = input_size<br/>        self.hidden_size = hidden_size<br/>        self.n_layers    = n_layers<br/>        self.embedding   = nn.Embedding(input_size, hidden_size)<br/>        self.gru         = nn.GRU(hidden_size, hidden_size, n_layers)<br/><br/>    <strong class="mi ir">def</strong> forward(self, word_inputs, hidden):<br/>        seq_len        = len(word_inputs)<br/>        embedded       = self.embedding(word_inputs).view(seq_len, 1, -1)<br/>        output, hidden = self.gru(embedded, hidden)<br/><br/>        <strong class="mi ir">return</strong> output, hidden<br/><br/>    <strong class="mi ir">def</strong> init_hidden(self):<br/>        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))<br/><br/>        <strong class="mi ir">if</strong> USE_CUDA:<br/>            hidden = hidden.cuda()<br/><br/>        <strong class="mi ir">return</strong> hidden<br/><br/><strong class="mi ir">class</strong> <strong class="mi ir">BahdanauAttnDecoderRNN</strong>(nn.Module):<br/>    <strong class="mi ir">def</strong> __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):<br/>        super(AttnDecoderRNN, self).__init__()<br/><br/>        self.hidden_size = hidden_size<br/>        self.output_size = output_size<br/>        self.n_layers    = n_layers<br/>        self.dropout_p   = dropout_p<br/>        self.max_length  = max_length<br/>        self.embedding   = nn.Embedding(output_size, hidden_size)<br/>        self.dropout     = nn.Dropout(dropout_p)<br/>        self.attn        = GeneralAttn(hidden_size)<br/>        self.gru         = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)<br/>        self.out         = nn.Linear(hidden_size, output_size)<br/><br/>    <strong class="mi ir">def</strong> forward(self, word_input, last_hidden, encoder_outputs):<br/>        word_embedded  = self.embedding(word_input).view(1, 1, -1)<br/>        word_embedded  = self.dropout(word_embedded)<br/>        attn_weights   = self.attn(last_hidden[-1], encoder_outputs)<br/>        context        = attn_weights.bmm(encoder_outputs.transpose(0, 1))<br/>        rnn_input      = torch.cat((word_embedded, context), 2)<br/>        output, hidden = self.gru(rnn_input, last_hidden)<br/>        output         = output.squeeze(0)<br/>        output         = F.log_softmax(self.out(torch.cat((output, context), 1)))<br/><br/>        <strong class="mi ir">return</strong> output, hidden, attn_weights<br/><br/><strong class="mi ir">class</strong> <strong class="mi ir">Attn</strong>(nn.Module):<br/>    <strong class="mi ir">def</strong> __init__(self, method, hidden_size, max_length=MAX_LENGTH):<br/>        super(Attn, self).__init__()<br/><br/>        self.method      = method<br/>        self.hidden_size = hidden_size<br/><br/>        <strong class="mi ir">if</strong> self.method == 'general':<br/>            self.attn = nn.Linear(self.hidden_size, hidden_size)<br/><br/>        <strong class="mi ir">elif</strong> self.method == 'concat':<br/>            self.attn  = nn.Linear(self.hidden_size * 2, hidden_size)<br/>            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))<br/><br/>    <strong class="mi ir">def</strong> forward(self, hidden, encoder_outputs):<br/>        seq_len       = len(encoder_outputs)<br/>        attn_energies = Variable(torch.zeros(seq_len))<br/><br/>        <strong class="mi ir">if</strong> USE_CUDA:<br/>            attn_energies = attn_energies.cuda()<br/><br/>        <strong class="mi ir">for</strong> i in range(seq_len):<br/>            attn_energies[i] = self.score(hidden, encoder_outputs[i])<br/><br/>        <strong class="mi ir">return</strong> F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)<br/><br/>    <strong class="mi ir">def</strong> score(self, hidden, encoder_output):<br/>        <strong class="mi ir">if</strong> self.method == 'dot':<br/>            energy = hidden.dot(encoder_output)<br/>            <strong class="mi ir">return</strong> energy<br/><br/>        <strong class="mi ir">elif</strong> self.method == 'general':<br/>            energy = self.attn(encoder_output)<br/>            energy = hidden.dot(energy)<br/>            <strong class="mi ir">return</strong> energy<br/><br/>        <strong class="mi ir">elif</strong> self.method == 'concat':<br/>            energy = self.attn(torch.cat((hidden, encoder_output), 1))<br/>            energy = self.other.dot(energy)<br/>            <strong class="mi ir">return</strong> energy<br/><br/><strong class="mi ir">class</strong> <strong class="mi ir">AttnDecoderRNN</strong>(nn.Module):<br/>    <strong class="mi ir">def</strong> __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout_p=0.1):<br/>        super(AttnDecoderRNN, self).__init__()<br/><br/>        self.attn_model  = attn_model<br/>        self.hidden_size = hidden_size<br/>        self.output_size = output_size<br/>        self.n_layers    = n_layers<br/>        self.dropout_p   = dropout_p<br/>        self.embedding   = nn.Embedding(output_size, hidden_size)<br/>        self.gru         = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)<br/>        self.out         = nn.Linear(hidden_size * 2, output_size)<br/><br/>        <strong class="mi ir">if</strong> attn_model != 'none':<br/>            self.attn = Attn(attn_model, hidden_size)<br/><br/>    <strong class="mi ir">def</strong> forward(self, word_input, last_context, last_hidden, encoder_outputs):<br/>        word_embedded      = self.embedding(word_input).view(1, 1, -1)<br/>        rnn_input          = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)<br/>        rnn_output, hidden = self.gru(rnn_input, last_hidden)<br/>        attn_weights       = self.attn(rnn_output.squeeze(0), encoder_outputs)<br/>        context            = attn_weights.bmm(encoder_outputs.transpose(0, 1))<br/>        rnn_output         = rnn_output.squeeze(0)<br/>        context            = context.squeeze(1)<br/>        output             = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)))<br/><br/>        <strong class="mi ir">return</strong> output, context, hidden, attn_weights<br/><br/>encoder_test = EncoderRNN(MAX_LENGTH, MAX_LENGTH, 2 )<br/>decoder_test = AttnDecoderRNN('general', MAX_LENGTH, MAX_LENGTH, 2)<br/><br/><strong class="mi ir">print</strong>(encoder_test)<br/><strong class="mi ir">print</strong>(decoder_test)<br/><br/>encoder_hidden = encoder_test.init_hidden()<br/>word_input     = Variable(torch.LongTensor([1, 2, 3]))<br/><br/><strong class="mi ir">if</strong> USE_CUDA:<br/>    encoder_test.cuda()<br/>    word_input = word_input.cuda()<br/><br/>encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)<br/><br/>word_inputs     = Variable(torch.LongTensor([1, 2, 3]))<br/>decoder_attns   = torch.zeros(1, 3, 3)<br/>decoder_hidden  = encoder_hidden<br/>decoder_context = Variable(torch.zeros(1, decoder_test.hidden_size))<br/><br/><strong class="mi ir">if</strong> USE_CUDA:<br/>    decoder_test.cuda()<br/><br/>    word_inputs     = word_inputs.cuda()<br/>    decoder_context = decoder_context.cuda()<br/><br/><strong class="mi ir">for</strong> i in range(3):<br/>    decoder_output, decoder_context, decoder_hidden, decoder_attn = decoder_test(word_inputs[i], decoder_context, decoder_hidden, encoder_outputs)<br/>    <strong class="mi ir">print</strong>(decoder_output.size(), decoder_hidden.size(), decoder_attn.size())<br/>    decoder_attns[0, i] = decoder_attn.squeeze(0).cpu().data<br/><br/>teacher_forcing_ratio = 0.5<br/>clip                  = 5.0<br/><br/><strong class="mi ir">def</strong> train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):<br/>    encoder_optimizer.zero_grad()<br/>    decoder_optimizer.zero_grad()<br/>    loss = 0<br/><br/>    input_length                    = input_variable.size()[0]<br/>    target_length                   = target_variable.size()[0]<br/>    encoder_hidden                  = encoder.init_hidden()<br/>    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)<br/>    decoder_input                   = Variable(torch.LongTensor([[SOS_token]]))<br/>    decoder_context                 = Variable(torch.zeros(1, decoder.hidden_size))<br/>    decoder_hidden                  = encoder_hidden<br/><br/>    <strong class="mi ir">if</strong> USE_CUDA:<br/>        decoder_input   = decoder_input.cuda()<br/>        decoder_context = decoder_context.cuda()<br/><br/>    use_teacher_forcing = random.random() &lt; teacher_forcing_ratio<br/><br/>    <strong class="mi ir">if</strong> use_teacher_forcing:<br/>        <strong class="mi ir">for</strong> di in range(target_length):<br/>            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)<br/>            loss += criterion(decoder_output[0], target_variable[di])<br/>            decoder_input = target_variable[di] # Next target is next input<br/><br/>    <strong class="mi ir">else</strong>:<br/>        <strong class="mi ir">for</strong> di in range(target_length):<br/>            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)<br/>            loss += criterion(decoder_output[0], target_variable[di])<br/><br/>            topv, topi = decoder_output.data.topk(1)<br/>            ni         = topi[0][0]<br/><br/>            decoder_input = Variable(torch.LongTensor([[ni]]))<br/><br/>            <strong class="mi ir">if</strong> USE_CUDA:<br/>                decoder_input = decoder_input.cuda()<br/><br/>            <strong class="mi ir">if</strong> ni == EOS_token:<br/>                <strong class="mi ir">break</strong><br/><br/>    loss.backward()<br/><br/>    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)<br/>    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)<br/><br/>    encoder_optimizer.step()<br/>    decoder_optimizer.step()<br/><br/>    <strong class="mi ir">return</strong> loss.data[0] / target_length<br/><br/><strong class="mi ir">def</strong> as_minutes(s):<br/>    m  = math.floor(s / 60)<br/>    s -= m * 60<br/><br/>    <strong class="mi ir">return</strong> '%dm %ds' % (m, s)<br/><br/><strong class="mi ir">def</strong> time_since(since, percent):<br/>    now = time.time()<br/>    s   = now - since<br/>    es  = s / (percent)<br/>    rs  = es - s<br/><br/>    <strong class="mi ir">return</strong> '%s (- %s)' % (as_minutes(s), as_minutes(rs))<br/><br/>attn_model  = 'general'<br/>hidden_size = 500<br/>n_layers    = 2<br/>dropout_p   = 0.05<br/><br/>encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers)<br/>decoder = AttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers, dropout_p=dropout_p)<br/><br/><strong class="mi ir">if</strong> USE_CUDA:<br/>    encoder.cuda()<br/>    decoder.cuda()<br/><br/>learning_rate     = 0.0001<br/>encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)<br/>decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)<br/>criterion         = nn.NLLLoss()<br/>n_epochs          = 50000<br/>plot_every        = 200<br/>print_every       = 1000<br/>start             = time.time()<br/>plot_losses       = []<br/>print_loss_total  = 0<br/>plot_loss_total   = 0<br/><br/># Begin!<br/><strong class="mi ir">for</strong> epoch in range(1, n_epochs + 1):<br/><br/>    # Get training data for this cycle<br/>    training_pair   = variables_from_pair(random.choice(pairs))<br/>    input_variable  = training_pair[0]<br/>    target_variable = training_pair[1]<br/><br/>    # Run the train function<br/>    loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)<br/><br/>    # Keep track of loss<br/>    print_loss_total += loss<br/>    plot_loss_total  += loss<br/><br/>    <strong class="mi ir">if</strong> epoch == 0: <strong class="mi ir">continue</strong><br/><br/>    <strong class="mi ir">if</strong> epoch % print_every == 0:<br/>        print_loss_avg   = print_loss_total / print_every<br/>        print_loss_total = 0<br/>        print_summary    = '(%d %d%%) %.4f' % (epoch, epoch / n_epochs * 100, print_loss_avg)<br/><br/>        <strong class="mi ir">print</strong>(print_summary)<br/><br/>    <strong class="mi ir">if</strong> epoch % plot_every == 0:<br/>        plot_loss_avg = plot_loss_total / plot_every<br/>        plot_losses.append(plot_loss_avg)<br/>        plot_loss_total = 0<br/><br/><strong class="mi ir">import</strong> matplotlib.pyplot <strong class="mi ir">as</strong> plt<br/><strong class="mi ir">import</strong> matplotlib.ticker <strong class="mi ir">as</strong> ticker<br/><strong class="mi ir">import</strong> numpy <strong class="mi ir">as</strong> np<br/><br/><strong class="mi ir">def</strong> show_plot(points):<br/>    plt.figure()<br/><br/>    fig, ax = plt.subplots()<br/>    loc     = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals<br/><br/>    ax.yaxis.set_major_locator(loc)<br/>    plt.plot(points)<br/><br/>show_plot(plot_losses)<br/><br/><strong class="mi ir">def</strong> evaluate(sentence, max_length=MAX_LENGTH):<br/>    input_variable = variable_from_sentence(input_lang, sentence)<br/>    input_length = input_variable.size()[0]<br/><br/>    # Run through encoder<br/>    encoder_hidden = encoder.init_hidden()<br/>    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)<br/><br/>    # Create starting vectors for decoder<br/>    decoder_input = Variable(torch.LongTensor([[SOS_token]])) # SOS<br/>    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))<br/>    <strong class="mi ir">if</strong> USE_CUDA:<br/>        decoder_input = decoder_input.cuda()<br/>        decoder_context = decoder_context.cuda()<br/><br/>    decoder_hidden = encoder_hidden<br/><br/>    decoded_words = []<br/>    decoder_attentions = torch.zeros(max_length, max_length)<br/><br/>    # Run through decoder<br/>    <strong class="mi ir">for</strong> di in range(max_length):<br/>        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)<br/>        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data<br/><br/>        # Choose top word from output<br/>        topv, topi = decoder_output.data.topk(1)<br/>        ni = topi[0][0]<br/>        <strong class="mi ir">if</strong> ni == EOS_token:<br/>            decoded_words.append('&lt;EOS&gt;')<br/>            <strong class="mi ir">break</strong><br/>        <strong class="mi ir">else</strong>:<br/>            decoded_words.append(output_lang.index2word[ni])<br/><br/>        # Next input is chosen word<br/>        decoder_input = Variable(torch.LongTensor([[ni]]))<br/>        <strong class="mi ir">if</strong> USE_CUDA:<br/>            decoder_input = decoder_input.cuda()<br/><br/>    <strong class="mi ir">return</strong> decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]<br/><br/><strong class="mi ir">def</strong> evaluate_randomly():<br/>    pair = random.choice(pairs)<br/><br/>    output_words, decoder_attn = evaluate(pair[0])<br/>    output_sentence = ' '.join(output_words)<br/><br/>    <strong class="mi ir">print</strong>('&gt;', pair[0])<br/>    <strong class="mi ir">print</strong>('=', pair[1])<br/>    <strong class="mi ir">print</strong>('&lt;', output_sentence)<br/>    <strong class="mi ir">print</strong>('')<br/><br/>evaluate_randomly()<br/><br/><strong class="mi ir">while</strong> True:<br/>    <strong class="mi ir">try</strong>:<br/>        raw = raw_input("&gt;")<br/>        output_words, attentions = evaluate(raw)<br/>        <strong class="mi ir">print</strong> output_words<br/>    <strong class="mi ir">except</strong>:<br/>        <strong class="mi ir">pass</strong><br/><br/><strong class="mi ir">def</strong> show_attention(input_sentence, output_words, attentions):<br/>    # Set up figure with colorbar<br/>    fig = plt.figure()<br/>    ax = fig.add_subplot(111)<br/>    cax = ax.matshow(attentions.numpy(), cmap='bone')<br/>    fig.colorbar(cax)<br/><br/>    # Set up axes<br/>    ax.set_xticklabels([''] + input_sentence.split(' ') + ['&lt;EOS&gt;'], rotation=90)<br/>    ax.set_yticklabels([''] + output_words)<br/><br/>    # Show label at every tick<br/>    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))<br/>    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))<br/><br/>    plt.show()<br/>    plt.close()<br/><br/><strong class="mi ir">def</strong> evaluate_and_show_attention(input_sentence):<br/>    output_words, attentions = evaluate(input_sentence)<br/>    <strong class="mi ir">print</strong>('input =', input_sentence)<br/>    <strong class="mi ir">print</strong>('output =', ' '.join(output_words))<br/>    show_attention(input_sentence, output_words, attentions)</span></pre><p id="1ae1" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">一旦这段代码完成训练，您将看到一个简单的提示，您可以在这里测试结果。在我的笔记本电脑上训练需要相当长的时间，因为我让我的scraper脚本运行了很长时间。你收集的训练数据越多，效果就越好，但是你必须自己试验，看看什么对你来说效果最好。</p><p id="47e6" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">以下是我在笔记本电脑上进行了两个小时的训练后得出的句子结果:</p><p id="0fe8" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">&gt;我需要食物[u ' food]，'【T4]']<br/>&gt;我在哪里可以找到食物[u ' food]，'&lt; EOS &gt; '] <br/> &gt;我想要一些食物&gt;我的胃需要食物<br/> &gt;你好我需要食物[u ' food]，'&lt; EOS &gt; '] <br/> &gt;我想要巴黎的免费食物[u ' food]，'&lt; EOS &gt; '] &lt; EOS &gt; '] <br/> &gt;我要踢一场足球赛<br/> &gt;足球赛【u'football】，&lt; EOS &gt; '] <br/> &gt;我可以换酒店吗【u'change】，&lt;EOS&gt;'】<br/>&gt;换酒店我可以【u'change】，&lt; EOS &gt; '] <br/> &gt;怎么可以</p><p id="5aad" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">如果根据更多数据进行训练，结果会显著改善。最终，这不是你在笔记本电脑上做的事情，我使用的最终版本是在云中训练的，以便能够在合理的时间框架内完成训练。</p><p id="7592" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">幸运的是，那里有几乎无穷无尽的数据供应，您可以向scraper脚本添加更多的博客URL。</p><h1 id="8278" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">我们来实验一下！</h1><p id="d617" class="pw-post-body-paragraph jx jy iq ka b kb ly kd ke kf lz kh ki kx ma kl km ky mb kp kq kz mc kt ku kv ij bi translated">这将是一个非常有趣的实验，看看我们是否可以颠倒整个过程，看看我们是否可以得出一个完整的句子，只给关键字作为输入，这肯定是我会尝试的事情，我会很快报告。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><p id="725e" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated"><em class="jz">原载于</em><a class="ae kw" href="http://theapemachine.com/code/keyword-extraction" rel="noopener ugc nofollow" target="_blank"><em class="jz"/></a><em class="jz">。</em></p></div></div>    
</body>
</html>