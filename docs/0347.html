<html>
<head>
<title>Attention and “Explainability” in RNNs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RNNs 中的注意和“可解释性”</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/attention-and-explainability-in-rnns-7fd114bb4f1f?source=collection_archive---------1-----------------------#2017-04-20">https://towardsdatascience.com/attention-and-explainability-in-rnns-7fd114bb4f1f?source=collection_archive---------1-----------------------#2017-04-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="8fb8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我对 NN 工程师中的运动感到兴奋(我们应该称他们为“NN <em class="kl">育种者</em>”？)走向在多个神经网络之间进行“对话”的架构，如 NIVIDIA 的自动驾驶汽车<a class="ae km" href="https://www.youtube.com/watch?v=URmxzxYlmtg" rel="noopener ugc nofollow" target="_blank">架构</a>和谷歌的<a class="ae km" href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> seq2seq </a>翻译器所展示的。然而，还缺少一些东西…</p><p id="9cbe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些实现仅部分解决了两个问题，即<strong class="jp ir">注意力</strong>和<strong class="jp ir">可解释性</strong>。我将解释一下这两者，然后我将检查将它们联系在一起的缺失部分。</p><p id="a8bc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">注意</strong>:</p><p id="3982" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，我将坚持一个句子解析的例子。当给出一个长句子时，神经网络通常会努力注意相关信息。句子开头的<em class="kl">处的一个单词可能决定句子结尾<em class="kl">附近的一个单词的变位，但是神经网络“忘记了它”。研究人员试图让网络“记住整个事情”(通过将整个句子转换为固定长度的向量)，但这只对像训练中那样复杂的句子有效。</em></em></p><p id="6b2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，“吉姆喜欢和他的朋友聚会，喜欢他们一起做的食物和他们的装饰品。”解析器必须将代词“他的”、“他们的”和“他们的”链接到它们的所指对象:“吉姆”、“朋友”和“当事人”。一个简单的 LSTM 架构将到达单词“their”，并从前面的单词“and”中记住它的状态。研究人员认为，希望这个状态仍然包括一些早期单词“parties”的知识！对于短句，网络会记住——网络状态不会被破坏太多。当给出更长的句子时，网络倾向于选择最近的单词，而不是有意义的单词。(一个 LSTM 人可能会认为'<em class="kl">他们的</em>装饰品'意味着'<em class="kl">食物'</em>装饰品'……)</p><p id="8724" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了克服这个问题，seq2seq 方法生成一个<strong class="jp ir">上下文</strong>向量。当<em class="kl">编码器</em>网络遇到一个新单词时，网络的一部分会在句子中寻找其他可能相关的单词。被认为相关的字被提供给<em class="kl">解码器</em>网络。实际上，上下文向量<strong class="jp ir"> <em class="kl">突出显示了句子的</em> </strong>部分，并且只发送那些部分。它集中了网络的注意力。这样，解码器网络可以从更远的后方(或前方，对于双向 RNN)接收相关信息。句子长度不太重要，因为只有几个词被认为是相关的。而且，一个真正的一般注意力可以“跳来跳去”，从任何长度的句子中挑选单词。(我们来找你了，普鲁斯特！)</p><p id="8ae7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">说明性</strong>:</p><p id="c26e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经网络变得越来越大，也越来越神秘。当他们做出决定时，我们想知道是什么信息引导了他们的决定。英伟达的解决方案是<strong class="jp ir">显示</strong>网络在‘想象’什么，这样司机就能<em class="kl">看到汽车对</em>周围环境的看法。如果他们的 Driveworks 神经网络只显示<em class="kl">两辆</em>车在你旁边的车道上，而你却能看到你窗外的<em class="kl">三辆</em>车，那你就知道出事了。</p><p id="5f70" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总的来说，显示网络所想的能力是一个强有力的工具。配有解释的译文会对你说<strong class="jp ir"/>:“这里用‘la’，因为下一个词是阴性的。”解释向我们展示了网络何时使用合理的概括而不是记忆几个像素来做出决策。</p><p id="c6f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，该行业正在这两个领域取得进展。少了什么？</p><p id="1009" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">连贯性</strong>:</p><p id="8975" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这两种情况下，我们希望找到一致性。“当我的 RNN 分析一个句子时，它是不是在我应该去的地方寻找信息？当它找到信息时，它会像我一样做决定吗？”</p><p id="a472" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里的核心直觉来自我们自己的经验:</p><p id="9bd6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当你从“我不明白这个词指的是什么……”变成“啊，它指的是另一个词，在这里”，你的注意力和你思想的可解释性发生了什么？一般来说，当你理解事物时，你会满足两个标准:</p><p id="ad93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你移动你的注意力，直到你找到帮助<em class="kl">理解</em>的信息。</p><p id="5ae8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当你理解了某件事，你就能够<em class="kl">解释</em>它为什么有意义。</p><p id="6652" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些陈述虽然简单，但仍然足以指导设计。一个解码器网络应该<em class="kl">很难取悦</em>——它想要接收一组<em class="kl">特定的单词</em>作为<strong class="jp ir">正确的</strong>上下文。如果上下文向量带来“有点意义”的单词，解码器网络应该<strong class="jp ir"> <em class="kl">更喜欢继续寻找</em> </strong>。(一会儿对“如何继续寻找”的模糊描述……)</p><p id="c4d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当解码器网络决定“<strong class="jp ir">这个</strong>字通过<strong class="jp ir">这些其他字</strong>变得一致时，特别是”，那应该是<em class="kl">可供我们看到</em>。例如:一个编码器-上下文-解码器架构，读着“约翰给了苏珊一个 cookie”，会看着“cookie”并告诉我们:“cookie”是“give”的间接宾语。这让我们可以检查网络在想什么，看看它是否有合理的判断！</p><p id="d01a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们结合这两个标准，看一个多句子的例子:“它不会很快停止。爱丽丝不喜欢，很不情愿地穿上鞋子，抓起一把伞。这场雨让她的通勤变得更加糟糕。”在这里，“它”是雨。此外，为了发现“it”意味着“rain”，网络需要使用上下文向量忽略句子中的大多数单词(大多数单词的得分为“0”，而名词“Alice”、“shoes”、“umbrella”、“commute”和“rain”有一些非零值)。</p><p id="fe59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设网络的上下文向量最初倾向于将“鞋子”作为“它”的所指对象，因为那些单词彼此接近。然而，将这个向量传递给解码器网络会输出“不相干的！”为了遵循我们上面的标准，网络将<em class="kl">然后从它的上下文向量</em>中丢弃‘鞋子’,并再次寻找。在第二次尝试时，最强的信号来自“雨伞”(再次，因为接近)。我们挑剔的解码器网络仍然以“不连贯！”来回应所以，伞从上下文向量中被丢弃，解码器再次寻找。最后，“雨”是一个连贯的选择——不像雨伞和鞋子，雨“不会停止”。此外，“鞋子”是复数。</p><p id="014f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最重要的是，在上面的例子中,' It '的第一个实例是<strong class="jp ir">有一段时间不明确的</strong>。当一个句子以“it”开头时，解码者需要发脾气——没有所指对象！而且，当更多的信息可用时，解码器需要<strong class="jp ir">更新</strong>它对‘它’的评估。“it”这个词在句子的第一遍会非常含糊不清。网络尽可能多地解析，然后返回进行第二次传递，扩大对“它”的连贯解释的搜索。有些事情是永久暧昧的！(&lt;-例题)网络应该能告诉你哪些词是歧义的，用什么方式。</p><p id="a1c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">把注意力和可解释性拉在一起</strong>:</p><p id="827f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们需要看到我们的网络做出决定的原因，检查他们思想的一致性。后人可能认为可解释性和编写调试测试一样重要。</p><p id="0883" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，网络应该将其注意力转移到数据上，以便能够观察任何大小的输入(例如，巨幅图像、长句子、复杂的游戏)。</p><p id="d919" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经网络可以为我们提供这两个目标，通过<strong class="jp ir"> <em class="kl">移动它的注意力，直到它找到增加连贯性的信息</em> </strong>。一致性测试可能是会话中一组神经网络中的另一个“模块”。一个简单的架构可能看起来像这样:一致性模块接受上下文向量和解码器的输出，也许，并输出“一致”或“不一致”。如果它返回“相干”，那么使用解码器的输出。否则，“不一致”触发对上下文向量中的一些活动特征的抑制。实际上，网络的注意力转移到了不太突出的词上。</p><p id="06c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">比起我上面简单的描述，当然有更好的方法在神经网络中建立一致性检查。随着研究人员处理机器学习中更复杂的任务，连接网络<strong class="jp ir">模块</strong>的架构可能变得比每个细胞的架构更重要。“LSTM 对 GRU”是一个小分歧，而巨大的分歧可能来自多个核国家的连接方式。鉴于这种复杂性，某种程度上保证思维的连贯性是必不可少的；这些模块中至少有一个将负责检查一致性，并改变输入的显著性。</p></div></div>    
</body>
</html>