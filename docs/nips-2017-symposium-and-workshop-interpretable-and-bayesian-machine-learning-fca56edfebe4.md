# NIPS 2017 研讨会和讲习班:可解释和贝叶斯机器学习

> 原文：<https://towardsdatascience.com/nips-2017-symposium-and-workshop-interpretable-and-bayesian-machine-learning-fca56edfebe4?source=collection_archive---------6----------------------->

## 我的第一次 NIPS 出席和总结

![](img/62f8516816980452697dfc071354e611.png)

上周，我参加了在加州长滩举行的 NIPS(神经信息处理系统)2017 会议。这是我第一次参加 NIPS。

# 什么是 NIPS？

[NIPS](https://nips.cc/) 是世界上最大的机器学习(ML) /人工智能(AI)会议之一。NIPS 会议由三个项目组成:指导、主要活动(包括研讨会)和研讨会。今年，约有 8000 人出席，比去年增加了约 30%，显示出对机器学习的兴趣越来越大。

我只参加了研讨会和专题讨论会，但是根据它们的内容，我可以看到每个研讨会都是一个专门的专题，一整天都致力于一个单一的主题。每个研讨会由 5-6 个讲座和海报会议组成。大多数工作坊最后还会有小组讨论。我参加了关于贝叶斯 ML 的研讨会和关于可解释 ML 的座谈会。

# 赞助商

在谈论大会演讲之前，我首先想让你看看赞助商名单，因为它可以显示当前机器学习在行业中的格局。仅举几个赞助商的例子，**微软、IBM Research、奥迪、英特尔 Nervana** 是钻石赞助商(8 万美元赞助)，其次是白金赞助商(4 万美元)，如**苹果、优步、脸书、谷歌、百度、阿里巴巴、亚马逊、DeepMind** 等。综合列表见[此处](https://nips.cc/Conferences/2017/Sponsors)。

# 主题

研讨会原定了两天。每个研讨会都是全天的，包括来自学术界和工业界的演讲以及海报展示。今年，大约有 50 个讲习班。这些主题涵盖了学科和技术/方法方面的广泛材料。

就学科而言，有专门针对学术领域的研讨会，

*   生物
*   神经系统科学
*   医学
*   自然科学

IT 相关行业，

*   机器人学
*   网络安全
*   系统
*   超级计算
*   物联网设备
*   软件开发
*   运输

以及更多与人类相关的领域

*   卫生保健
*   AI 透明度/公平性
*   发展中国家的 ML 应用
*   创意和设计。

ML 技术方面的主题也变化很大；

*   深度学习(通用)
*   深度强化学习
*   贝叶斯深度学习
*   元学习
*   半监督学习
*   建议
*   因果推理
*   自然语言处理
*   音频信号处理
*   时间序列
*   可量测性
*   可视化和交流

# 研讨会:[可解释机器学习](http://interpretable.ml/)

我在研讨会前一天到达，参加了关于可解释 ML 的研讨会。

可解释的 ML 不仅有利于帮助用户理解 ML 模型的结果，而且有利于使人工智能安全可靠。然而，可解释的 ML 是具有挑战性的。总的来说，演讲者同意很难定义术语“可解释性”，因为它是一个以人为中心的术语，依赖于用户的类型，这甚至可能涉及用户体验方面。另外，当前的可解释 ML 方法也有局限性。目前，大致有两种方式来构建可解释的 ML 算法:1)使用简单(例如，线性)算法，或者 2)使用局部描述黑盒模型的“模仿”算法(例如，LIME)。两者都有缺点；如果数据具有复杂的分布，简单的算法很可能具有较低的精度，并且模拟算法很容易失败。无论如何，这一部分的讨论试图解决当前 ML 景观中的问题，并检验可解释 ML 的不同元素。

为了在图像分类/识别任务中使用可解释性 ML，奇莉·瓦格斯塔夫(JPL)展示了如何通过使用 [DEMUD](https://github.com/wkiri/DEMUD) 来可视化深度神经网络中的可解释性和学习过程，DEMUD 学习连续信息之间的残差(差异)。Kilian Q. Weinberger (Cornell)提出了对现代神经网络的担忧，这些网络具有很差的校准置信度(即[过度自信](https://arxiv.org/pdf/1706.04599.pdf)，这在自动驾驶汽车和自动医疗诊断等实际应用中可能是一个问题。

Jenn Wortman Vaughan(微软)建议可解释性应该跨学科理解，应该考虑人类；根据用户的不同，可解释性可能有不同的含义。沃恩使用亚马逊机械土耳其人进行了一项有趣的实验[，在那里她测试了人类受试者对简单模型的理解是否优于更复杂的模型(例如，黑盒、更多功能)。受试者能够更好地遵循更简单的模型，尽管他们对两个模型的信心水平相似(与人们对黑盒模型的信任度低于简单模型的普遍看法相反)。这是一项初步的工作，但它简单而新颖，因为这项研究试图真正包括人类，并测量他们对 ML 模型的反应及其预测。](http://s.interpretable.ml/nips_interpretable_ml_2017_jenn_wortman_vaughan.pdf)

还进行了小组讨论，发言者回答了一系列问题。首先，他们承认定义“可解释性”是困难的。他们同意贝叶斯方法可以通过提供一些因素和预测不确定性之间关系的信息来帮助黑盒模型。他们还证实，金融和医药等高风险行业领域更抵制黑箱模型，更喜欢可解释性。也有一些对“模仿模型”的批评，因为他们没有提供一个基本的解释，而是更多的事后解释。为了比较可解释模型的不同方法，研究人员同意需要进行涉及人类的实验。总的来说，他们一致认为，要在这个问题上取得突破，首先应该进行系统的人体实验，并更好地定义可解释性。

# 车间

今年大约有 50 场研讨会，有很多有趣的话题。很难选择参加哪一个，但我决定参加贝叶斯机器学习。我做出这个决定背后的动机是 1)我在博士期间学习了贝叶斯推理，我对这个主题略有熟悉，2)我一直对深度学习持怀疑态度，深度学习的贝叶斯治疗听起来像是对当前深度学习炒作的一种补救措施，3)贝叶斯机器学习是机器学习中的一个新兴主题。尽管两个研讨会都侧重于技术细节(如同大多数贝叶斯推理材料)，但我还是想总结几个要点。

# 第一天:[近似贝叶斯推理的进展](http://approximateinference.org/)

尽管在深度学习方面取得了显著的进步，但这些深度神经网络缺乏解决其预测中的不确定性的能力，并且没有利用概率论。因此，最近研究人员一直在尝试融合贝叶斯方法和深度学习。贝叶斯推理的主要挑战是在计算后验概率时近似难以处理的边际概率分布。有两种方法可以解决这个问题；一种是设计一个易于估计的概率分布，该概率分布接近真实分布并试图减小这两个分布之间的距离，另一种是使用抽样方法(蒙特卡罗模拟)建立后验概率。第一种方法通常被称为变分推理(VI)。大概，第二种方法中最流行的方法就是马尔可夫链蒙特卡罗(MCMC)。尽管每种方法都有自己的优点和缺点，但大多数人对 VI 比对抽样方法更感兴趣，主要是因为 VI 更快，更具确定性。

在近似贝叶斯推理研讨会上提出了许多新颖的方法。Josip Djolonga(苏黎世联邦理工学院)提出了一种近似方法，这种方法利用了欺骗双样本统计测试的想法。“如果我们能骗过统计测试，学习到的分布应该是真实数据的一个很好的模型。”李英珍提出了[直接估计得分函数而不是近似优化目标的思想，用于基于梯度的优化](https://arxiv.org/abs/1705.07107)，可以缓解广义敌对网络(GAN)的缺陷，如过拟合和低估损失函数。Kira Kempinska(伦敦大学学院)给出了另一个辉煌，他引入了[对抗序列蒙特卡罗](http://bayesiandeeplearning.org/2017/papers/6.pdf)方法，将近似问题公式化为两人游戏，类似于 GAN。

来自工业界的受邀演讲也很有趣。来自网飞的 Dawen Liang 展示了我们如何使用变分自动编码器(VAE)来构建推荐系统，特别是因为该问题更多地是一个小数据问题，并且用户-项目交互矩阵仅在它是正的情况下才被观察到(即，在负和无之间没有区别)。来自亚马逊的 Andreas Damianou 介绍了[深度高斯过程](http://proceedings.mlr.press/v31/damianou13a.pdf)，其中的动机是，(再次)利用当前深度学习方法中通过更好地逼近难以处理的分布而包含的推理(即，模型应该根据它们对预测的确定程度以不同的方式表现)。

# 第二天:[贝叶斯深度学习](http://bayesiandeeplearning.org/)

从主题上看，这次研讨会与第一次非常相似。然而，在这里，演讲者通过关注概率规划和贝叶斯神经网络(不仅仅是近似方法)展示了一个略大的画面。

会议以 Dustin Tran(哥伦比亚大学/谷歌)的演讲开始，他是概率编程库的首席开发人员 [Edward](http://edwardlib.org/) 。他的演讲是关于概率编程的库和概述。Edward 通过支持有向图形模型来支持贝叶斯推理。它是基于 TensorFlow 构建的，尽管我没有使用过 Edward，但根据演示文稿中的代码截图，它的语法似乎与我以前使用过的 pymc3 非常相似；你定义你的变量的图形结构，它们是随机的还是确定的，这些变量应该有什么分布，等等。它支持变分推理和蒙特卡罗方法。达斯汀预测，即使概率编程可能有很高的认知负担，但由于分布式、编译和加速系统，它将变得更容易使用，这允许在多台机器上进行概率编程。

本次会议的特别演讲人是 Max Welling(阿姆斯特丹大学/高通)，他发表了大量关于贝叶斯机器学习的研究，并因其关于[变分自动编码器](https://arxiv.org/abs/1312.6114)的论文而闻名。他对 Bayesain 深度学习进行了概述。首先，他通过重申贝叶斯方法的好处开始了他的讲话:1)我们可以用一个原则来正则化模型，但不会浪费数据(不需要交叉验证)，2)我们可以测量不确定性，3)贝叶斯方法可以用于严格的模型选择。然后他提出了一个开放式的问题；做决策时，我们应该如何处理量化的不确定性？对我来说，测量不确定性和量化不确定性的实际应用可能是两回事。在演讲的最后，他提到贝叶斯深度学习有三条主线；1)深度高斯过程，2)信息瓶颈，以及 3)贝叶斯丢失。关于最后一个，他提到事实证明快速退出法是[实际上不是贝叶斯方法](https://arxiv.org/abs/1711.02989)，研究人员正在研究全贝叶斯方法。

Gintare Karolina Dziugaite(剑桥大学/矢量研究所)做了一个有趣的技术演讲，她和 Daniel M. Roy(多伦多大学/矢量研究所)设计了一种改进的随机梯度下降(SGD)方法，即“[熵-SGD](http://bayesiandeeplearning.org/2017/papers/53.pdf) ”，以缓解 SGD 的过拟合问题。这个问题已经由 [Koh 和梁](https://arxiv.org/abs/1703.04730)提出，他们的论文被选为今年国际机器学习大会的最佳论文。在 Koh 和 Liang 的论文中，证明了具有 SGD 的深度神经网络可以学习训练数据中完全随机化的标签。

# 个人想法

尽管我只参加了整个会议的一半，但我可以说，我在 NIPS 的机器学习领域经历了两个看似非常不同的领域。关于可解释 ML 的研讨会更多的是关于实际应用和它对人们的影响。研讨会集中讨论了在现代神经网络中注入贝叶斯思想的技术。尽管这些从表面上看起来很不一样，但我实际上认为这两者之间有共同点:研究人员最终希望通过从模型(不确定性)中提取更多信息来建立一个“更智能”的机器，它可以对人类更加负责和透明。

*原载于 2017 年 12 月 16 日*[*hongsupshin.com*](https://hongsupshin.com/2017/12/16/nips-2017-symposium-and-workshop-interpretable-and-bayesian-machine-learning/)*。*