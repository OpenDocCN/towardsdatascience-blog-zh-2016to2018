<html>
<head>
<title>Machine Learning Kaggle Competition: Part Three Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习竞赛:第三部分优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-kaggle-competition-part-three-optimization-db04ea415507?source=collection_archive---------9-----------------------#2018-07-20">https://towardsdatascience.com/machine-learning-kaggle-competition-part-three-optimization-db04ea415507?source=collection_archive---------9-----------------------#2018-07-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/0ac4b89b29ae660c574a868ddb91d162.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0EijCO7wpxw5d5YLSqAyFQ.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="53a4" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">充分利用机器学习模型</h2></div><p id="fc3e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">如何最好地描述一场 Kaggle 比赛？是伪装成比赛的机器学习教育！尽管有对<a class="ae lm" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的<a class="ae lm" href="https://www.quora.com/Why-did-Kaggle-start-having-a-bad-reputation" rel="noopener ugc nofollow" target="_blank">有效的批评</a>，但总体而言，这是一个伟大的社区，提供了<a class="ae lm" href="https://www.kaggle.com/competitions" rel="noopener ugc nofollow" target="_blank">有趣的问题</a>，数千名数据科学家愿意分享他们的知识，以及<a class="ae lm" href="https://www.kaggle.com/learn/overview" rel="noopener ugc nofollow" target="_blank">探索新想法的理想环境</a>。作为证据，如果不是因为<a class="ae lm" href="https://www.kaggle.com/c/home-credit-default-risk/" rel="noopener ugc nofollow" target="_blank"> Kaggle 家庭信贷竞赛</a>，我永远也不会知道梯度推进机器，或者，这篇文章的主题之一，自动化模型优化。</p><p id="34ec" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在本文中，系列文章的第三部分(<a class="ae lm" rel="noopener" target="_blank" href="/machine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426">第一部分:入门</a>和<a class="ae lm" rel="noopener" target="_blank" href="/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8">第二部分:改进)</a>记录我为这次竞赛所做的工作，我们将关注机器学习管道的一个关键方面:通过超参数调整进行模型优化。在第二篇文章中，我们决定选择梯度推进机作为我们的模型，现在我们必须通过优化来充分利用它。我们将主要通过两种方法来实现这一点:随机搜索和使用贝叶斯优化的自动调优。</p><p id="334e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这里介绍的所有工作都可以在 Kaggle 上运行。文章本身将突出关键思想，但代码细节都在笔记本中(无需安装即可免费运行！)</p><ol class=""><li id="cf57" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll ls lt lu lv bi translated"><a class="ae lm" href="https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search" rel="noopener ugc nofollow" target="_blank">随机和网格搜索</a></li><li id="e9e4" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated"><a class="ae lm" href="https://www.kaggle.com/willkoehrsen/automated-model-tuning" rel="noopener ugc nofollow" target="_blank">自动超参数调整</a></li><li id="4c02" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated"><a class="ae lm" href="https://www.kaggle.com/willkoehrsen/model-tuning-results-random-vs-bayesian-opt/notebook" rel="noopener ugc nofollow" target="_blank">调谐结果</a></li></ol><p id="0b14" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">对于这篇文章，我们将跳过背景，所以如果你在任何时候感到迷茫，我鼓励你去以前的文章和笔记本。所有的笔记本都可以在 Kaggle 上运行，而不需要下载任何东西，所以我强烈推荐你去看看它们或者参加比赛！</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="0e14" class="mi mj jb bd mk ml mm dn mn mo mp dp mq kz mr ms mt ld mu mv mw lh mx my mz na bi translated">概述</h2><p id="659c" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">在本系列的第<a class="ae lm" rel="noopener" target="_blank" href="/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8">部分，我们熟悉了数据集，执行了探索性数据分析，尝试了功能工程，并构建了一些基线模型。我们这一轮的公开排行榜得分是<strong class="ks jc"> 0.678 </strong>(目前我们在排行榜上的排名低于 4000)。</a></p><p id="06e9" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><a class="ae lm" rel="noopener" target="_blank" href="/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8">第二部分</a>涉及深入的手工特征工程，接着是特征选择和更多建模。使用扩展(然后收缩)的功能集，我们最终得到了 0.779 的分数<strong class="ks jc"/>，比基线有了很大的提高，但还不到竞争对手的前 50%。在这篇文章中，我们将再次提高我们的分数，并在排行榜上上升 1000 位。</p><h1 id="298c" class="ng mj jb bd mk nh ni nj mn nk nl nm mq kh nn ki mt kk no kl mw kn np ko mz nq bi translated">机器学习优化</h1><p id="37f7" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">机器学习环境中的优化意味着找到一组模型超参数值，该组模型超参数值对于给定的数据集产生最高的交叉验证分数。与在训练期间学习的模型参数相反，模型超参数由数据科学家在训练之前设置。深度神经网络中的层数是模型超参数，而决策树中的分裂是模型参数。我喜欢把模型超参数看作是我们需要为数据集调整的设置:<strong class="ks jc">对于每个问题来说，理想的值组合都是不同的</strong>！</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ad66f89e2d8afdf7849785e95083316d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*E1uK77d20JIP7LAhAj4bLA.jpeg"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">A machine learning model has to be tuned like a radio — if anyone remembers what a radio was!</figcaption></figure><p id="521d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">有几种方法可以调整机器学习模型:</p><ol class=""><li id="044f" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll ls lt lu lv bi translated"><strong class="ks jc">手动</strong>:用直觉/经验/猜测选择超参数，用数值训练模型，寻找验证分数重复过程，直到你失去耐心或者对结果满意。</li><li id="a054" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated"><strong class="ks jc">网格搜索</strong>:设置一个超参数网格，对于每个数值组合，训练一个模型，找到验证分数。在这种方法中，尝试了超参数值的每个组合，这是非常低效的！</li><li id="ed31" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated"><strong class="ks jc">随机搜索</strong>:设置超参数网格，选择<em class="oa">随机</em>数值组合，训练模型，寻找验证分数。搜索迭代的次数基于时间/资源。</li><li id="f65e" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated"><strong class="ks jc">自动超参数调整</strong>:使用梯度下降、贝叶斯优化或进化算法等方法引导搜索最佳超参数。与随机或网格<em class="oa">非知情</em>方法相比，这些方法使用先前的结果在<em class="oa">知情搜索</em>中选择下一个超参数值。</li></ol><p id="8f05" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这些是按照效率增加的顺序排列的，手动搜索花费的时间最多(通常产生的结果最差)，而自动方法最快地收敛到最佳值，尽管与机器学习中的许多主题一样，情况并不总是如此！<a class="ae lm" href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank">正如这篇伟大的论文所示，</a>随机搜索做得出奇的好(我们很快就会看到)。</p><p id="08da" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">(还有其他超参数调谐方法，如<a class="ae lm" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Evolutionary_optimization" rel="noopener ugc nofollow" target="_blank">进化</a>和<a class="ae lm" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Gradient-based_optimization" rel="noopener ugc nofollow" target="_blank">基于梯度</a>。不断有更好的方法被开发出来，所以一定要跟上当前的最佳实践！)</p><p id="17f7" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在本系列的第二部分中，我们决定使用<a class="ae lm" href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/" rel="noopener ugc nofollow" target="_blank"> Gradient Boosting Machine (GBM)模型</a>,因为它在具有许多特性的结构化数据上具有卓越的性能。GBM 非常强大(但是很容易用 Python 实现)，但是它有几十个超参数，这些参数会显著影响性能，必须针对问题进行优化。如果您想感到不知所措，请查看 LightGBM 库上的文档:</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/18e3485625e4b0200f8a3f46e2feb529.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2wUnzNd7t1s-nNUztoFvvw.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">LightGBM Documentation</figcaption></figure><p id="1f0f" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我不认为世界上有任何人能够看到所有这些并挑选出最佳的价值观！因此，我们需要实现选择超参数的四种方法之一。</p><p id="3fbe" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">对于这个问题，我甚至没有尝试手动调优，因为这是我第一次使用 GBM，也因为我不想浪费任何时间。我们将直接跳到使用贝叶斯优化的随机搜索和自动化技术。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="99cd" class="mi mj jb bd mk ml mm dn mn mo mp dp mq kz mr ms mt ld mu mv mw lh mx my mz na bi translated">实现网格和随机搜索</h2><p id="5490" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated"><a class="ae lm" href="https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search" rel="noopener ugc nofollow" target="_blank">在第一本笔记本</a>中，我们介绍了网格和随机搜索的实现，涵盖了优化问题的四个部分:</p><ol class=""><li id="c9e5" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll ls lt lu lv bi translated"><strong class="ks jc">目标函数:</strong>接受超参数并返回我们试图最小化或最大化的分数的函数</li><li id="68d6" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated"><strong class="ks jc">域:</strong>我们要搜索的超参数值的集合。</li><li id="1ecc" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated"><strong class="ks jc">算法:</strong>选择目标函数中要评估的下一组超参数的方法。</li><li id="b950" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated"><strong class="ks jc">结果历史:</strong>包含每组超参数和目标函数的结果分数的数据结构。</li></ol><p id="7319" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这四个部分也构成了贝叶斯优化的基础，所以在这里列出它们将有助于实现。关于代码的细节，请参考笔记本，但这里我们将简要地触及每个概念。</p><h2 id="c59a" class="mi mj jb bd mk ml mm dn mn mo mp dp mq kz mr ms mt ld mu mv mw lh mx my mz na bi translated">目标函数</h2><p id="c1f6" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">目标函数接受一组输入，并返回一个我们希望最大化的分数。在这种情况下，输入是模型超参数，得分是对训练数据的 5 重交叉验证 ROC AUC。伪代码中的目标函数是:</p><pre class="ns nt nu nv gt oc od oe of aw og bi"><span id="b7b3" class="mi mj jb od b gy oh oi l oj ok">def objective(hyperparameters):<br/>    """Returns validation score from hyperparameters"""<br/>    <br/>    model = Classifier(hyperparameters)</span><span id="8c17" class="mi mj jb od b gy ol oi l oj ok">    validation_loss = cross_validation(model, training_data, <br/>                                       nfolds = 5)</span><span id="af70" class="mi mj jb od b gy ol oi l oj ok">    return validation_loss</span></pre><p id="1660" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">超参数优化的目标是找到传递给<code class="fe om on oo od b">objective</code>函数时返回最佳值的<code class="fe om on oo od b">hyperparameters</code>。这似乎很简单，但问题是评估目标函数在时间和计算资源方面非常昂贵。我们无法尝试超参数值的每个组合，因为我们的时间有限，因此需要随机搜索和自动化方法。</p><h2 id="b7b0" class="mi mj jb bd mk ml mm dn mn mo mp dp mq kz mr ms mt ld mu mv mw lh mx my mz na bi translated">领域</h2><p id="cc21" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">定义域是我们搜索的一组值。对于 GBM 的这个问题，域如下:</p><figure class="ns nt nu nv gt is"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="df4e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们可以看到其中的两种分布，学习率是对数正态分布，叶子的数量是均匀正态分布:</p><div class="ns nt nu nv gt ab cb"><figure class="or is os ot ou ov ow paragraph-image"><img src="../Images/5c7ce4a4bbad06184926be33e109d98c.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*2iqJrPvzSc2Si9sjrqubBg.png"/></figure><figure class="or is ox ot ou ov ow paragraph-image"><img src="../Images/e0a9bc715e8aca7ef18dc98e4baf76f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*70m7uS4Q1Nc7M4hZ8UfaiA.png"/></figure></div><h1 id="8acd" class="ng mj jb bd mk nh ni nj mn nk nl nm mq kh nn ki mt kk no kl mw kn np ko mz nq bi translated">算法</h1><p id="d4bf" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">虽然我们通常不这样认为，但网格和随机搜索都是算法。在网格搜索的情况下，我们输入域，算法为有序序列中的每个超参数选择下一个值。网格搜索的惟一要求是，它对网格中的每种组合都尝试一次(且只尝试一次)。对于随机搜索，我们输入域，每次算法给我们一个超参数值的随机组合来尝试。除了随机选择下一个值之外，对随机搜索没有任何要求。</p><p id="54c4" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">随机搜索可以按如下方式实现:</p><pre class="ns nt nu nv gt oc od oe of aw og bi"><span id="ce43" class="mi mj jb od b gy oh oi l oj ok">import random </span><span id="438f" class="mi mj jb od b gy ol oi l oj ok"><em class="oa"># Randomly sample from dictionary of hyperparameters</em><br/>random_params = {k: random.sample(v, 1)[0] for k, v <strong class="od jc">in</strong>          <br/>                    param_grid.items()}<br/></span><span id="b830" class="mi mj jb od b gy ol oi l oj ok"><strong class="od jc">{'boosting_type': 'goss',<br/> 'colsample_bytree': 0.8222222222222222,<br/> 'is_unbalance': False,<br/> 'learning_rate': 0.027778881111994384,<br/> 'min_child_samples': 175,<br/> 'num_leaves': 88,<br/> 'reg_alpha': 0.8979591836734693,<br/> 'reg_lambda': 0.6122448979591836,<br/> 'subsample': 1.0,<br/> 'subsample_for_bin': 220000}</strong></span></pre><p id="01d5" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这实际上是一个非常简单的算法！</p><h2 id="6d52" class="mi mj jb bd mk ml mm dn mn mo mp dp mq kz mr ms mt ld mu mv mw lh mx my mz na bi translated">结果历史</h2><p id="768f" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">结果历史是包含超参数组合和目标函数的结果分数的数据结构。当我们进行贝叶斯优化时，模型实际上<em class="oa">使用过去的结果来决定下一个要评估的超参数</em>。随机和网格搜索是<em class="oa">不了解情况的</em>方法，不使用过去的历史，但是我们仍然需要历史，以便我们可以找出哪个超参数工作得最好！</p><p id="fb4f" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在这种情况下，结果历史只是一个数据框架。</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/045d5849a7009eb15666d7a7c20c5046.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*A6cthQkrEmECpjKjy6ilDg.png"/></div></figure><h1 id="2ab1" class="ng mj jb bd mk nh ni nj mn nk nl nm mq kh nn ki mt kk no kl mw kn np ko mz nq bi translated">贝叶斯超参数优化</h1><p id="313b" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">使用贝叶斯优化的自动超参数调整听起来很复杂，但事实上它使用了与随机搜索相同的四个部分，唯一的区别是使用了<strong class="ks jc">算法</strong>。对于这次比赛，我使用了 Hyperopt 库和 Tree Parzen Estimator (TPE)算法，并在<a class="ae lm" href="https://www.kaggle.com/willkoehrsen/automated-model-tuning" rel="noopener ugc nofollow" target="_blank">的笔记本</a>中做了展示。对于概念性的解释，请参考<a class="ae lm" rel="noopener" target="_blank" href="/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f">这篇文章</a>，对于 Python 的实现，请查看笔记本或<a class="ae lm" rel="noopener" target="_blank" href="/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a">这篇文章</a>。</p><p id="5851" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">贝叶斯优化<a class="ae lm" href="https://sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf" rel="noopener ugc nofollow" target="_blank">的基本概念</a>是使用之前的评估结果来<em class="oa">推理</em>哪些超参数表现更好，并使用该推理来选择下一个值。因此，该方法应该花费较少的迭代次数来评估具有较差值的目标函数。理论上，贝叶斯优化比随机搜索能以更少的迭代收敛到理想值(虽然随机搜索还是能侥幸成功)！贝叶斯优化的<a class="ae lm" href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization" rel="noopener ugc nofollow" target="_blank">目标是:</a></p><ol class=""><li id="59c7" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll ls lt lu lv bi translated">为了找到由性能测量的更好的超参数值</li><li id="28e8" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated">使用比网格或随机搜索更少的迭代次数进行优化</li></ol><p id="832c" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这是一个强大的方法，有望带来巨大的成果。问题是，实践中的证据表明情况是这样的吗？为了回答这个问题，我们转向<a class="ae lm" href="https://www.kaggle.com/willkoehrsen/model-tuning-results-random-vs-bayesian-opt" rel="noopener ugc nofollow" target="_blank">最终笔记本</a>，深入了解模型调整结果！</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="c667" class="ng mj jb bd mk nh oz nj mn nk pa nm mq kh pb ki mt kk pc kl mw kn pd ko mz nq bi translated">超参数优化结果</h1><p id="af16" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">在实现随机搜索和贝叶斯优化的艰苦工作之后，<a class="ae lm" href="https://www.kaggle.com/willkoehrsen/model-tuning-results-random-vs-bayesian-opt" rel="noopener ugc nofollow" target="_blank">第三个笔记本</a>是一个有趣的和揭示性的探索结果。有超过 35 个情节，所以如果你喜欢视觉效果，那就去看看吧。</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/b5dd88dd81b5d9cb1d097b73bbce8e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*rjzCVVvLhpB3ZytOCO6Fwg.png"/></div></figure><p id="7822" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">虽然我试图在 Kaggle 上完成整个比赛，但对于这些搜索，我在一台 64 GB 内存的计算机上进行了 500 次随机搜索和 400 次贝叶斯优化迭代，每次耗时约 5 天(感谢亚马逊 AWS)。所有的结果都是可用的，但是你需要一些真正的硬件来重做实验！</p><p id="fd3a" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">首先:哪种方法做得更好？下图总结了随机搜索的 501 次迭代和贝叶斯优化的 402 次迭代的结果(在数据框中称为<code class="fe om on oo od b">opt</code>):</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/77113d92879abbe4446452c8c4048a83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*WqD_mgrCr2xc_jsid9aZ3Q.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">Random Search and Bayesian Optimization ( `opt`) results</figcaption></figure><p id="f8ad" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><strong class="ks jc">按最高分计算，随机搜索略胜一筹，但如果我们按平均分衡量，贝叶斯优化胜出。</strong></p><p id="64b9" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">好消息是，这几乎正是我们所期望的:<strong class="ks jc">随机搜索可以在一组很大的值上发生，因为它彻底探索了搜索空间，但贝叶斯优化将通过从以前的结果进行推理来“关注”最高得分的超参数值。</strong>让我们来看一个非常有启发性的图表，交叉验证分数的值与迭代次数的关系:</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/8afe6e548e01ef10cfc2d3ef6d932186.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*dBjBQRxKYl06MF3URjN-Zg.png"/></div></figure><p id="c35e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们绝对没有看到随机搜索的趋势，而贝叶斯优化(再次由<code class="fe om on oo od b">opt</code>显示)在试验中有所改善。如果你能理解这个图表，那么你可以看到这两种方法的好处:随机搜索探索搜索领域，但贝叶斯优化随着时间的推移变得更好。我们还可以看到，贝叶斯优化似乎达到了一个平台，表明进一步试验的回报递减。</p><p id="b9bc" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">第二个主要问题:什么是最佳超参数值？以下是贝叶斯优化的最佳结果:</p><pre class="ns nt nu nv gt oc od oe of aw og bi"><span id="abdd" class="mi mj jb od b gy oh oi l oj ok"><strong class="od jc">boosting_type             gbdt<br/>colsample_bytree      0.614938<br/>is_unbalance              True<br/>learning_rate        0.0126347<br/>metric                     auc<br/>min_child_samples          390<br/>n_estimators              1327<br/>num_leaves                 106<br/>reg_alpha             0.512999<br/>reg_lambda            0.382688<br/>subsample             0.717756<br/>subsample_for_bin        80000<br/>verbose                      1<br/>iteration                  323<br/>score                 0.788793</strong></span></pre><p id="d144" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们可以使用这些结果来建立一个模型并将预测提交给竞争对手，或者通过允许我们围绕最佳值定义一个集中的搜索空间，它们可以用于通知进一步的搜索。</p><h2 id="26f0" class="mi mj jb bd mk ml mm dn mn mo mp dp mq kz mr ms mt ld mu mv mw lh mx my mz na bi translated">超参数图</h2><p id="c7fa" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">要考虑的一个有趣的方面是每个超参数的每个搜索方法尝试的值。下图显示了每种搜索方法的核密度估计(KDE)函数以及采样分布(超参数网格)。垂直虚线表示每种方法的最佳值。首先是学习率:</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ph"><img src="../Images/ffddf6c62c672ced34040b35b15fdd21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HG44HZoftTLrzFNNat7llw.png"/></div></div></figure><p id="c9b8" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">尽管学习率分布跨越了几个数量级，但两种方法都发现最优值在该域中相当低。通过将我们的搜索集中在这个区域，我们可以使用这个知识来通知进一步的超参数搜索。在大多数情况下，较低的学习率会提高交叉验证的性能，但代价是增加运行时间，这是我们必须做出的权衡。</p><p id="7baa" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">让我们看看其他一些图表。对于大多数超参数，两种方法得出的最佳值相当接近，但对于<code class="fe om on oo od b">colsample_bytree</code>却不是这样:</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pi"><img src="../Images/8d9141628b63aee0ef4dd4c7067d13c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lj_pvF51I3rAwworV1cu7A.png"/></div></div></figure><p id="79ce" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这是指在 GBM 中构建每个树时使用的列的分数，随机搜索发现最优值高于贝叶斯优化。同样，这些结果可以用于进一步的搜索，因为我们看到贝叶斯方法倾向于集中在 0.65 左右的值。</p><p id="ee70" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们将展示该分析中的另外两个图，因为它们相当有趣，涉及两个正则化参数:</p><div class="ns nt nu nv gt ab cb"><figure class="or is pj ot ou ov ow paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/c5a46d104c2a38bcf7937e8fcbaacbd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*_L5i3g7mMsh6KkRf5QwUOg.png"/></div></figure><figure class="or is pj ot ou ov ow paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/56010d30fcc3fd5f1ba89635b977b45c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*wEE8xCP53GQTuILbpuHgYQ.png"/></div></figure></div><p id="0226" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这里值得注意的是，这两个超参数似乎是相互补充的:如果一个正则化值高，那么我们希望另一个正则化值低，反之亦然。也许这有助于模型在<a class="ae lm" href="http://www.cs.cmu.edu/~wcohen/10-601/bias-variance.pdf" rel="noopener ugc nofollow" target="_blank">偏差/方差</a>之间取得平衡，这是机器学习中最常见的问题。</p><h2 id="d553" class="mi mj jb bd mk ml mm dn mn mo mp dp mq kz mr ms mt ld mu mv mw lh mx my mz na bi translated">超参数与迭代</h2><p id="1d8a" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">虽然随机搜索不会改变值在搜索中的分布，但贝叶斯优化会改变，因为它会专注于搜索域中它认为最佳的值。我们可以通过绘制迭代的超参数值来看到这一点:</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pk"><img src="../Images/e6716e735fa1c2e30aa7cb001e40b296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y8d1xu6gCZ-beBuVPZumAQ.png"/></div></div></figure><figure class="ns nt nu nv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pl"><img src="../Images/f10f2fbd7df7b81b2f07b265df44f1d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GTBebAjyhCyzS19EGXUFRw.png"/></div></div></figure><p id="7594" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">趋势最明显的超参数是<code class="fe om on oo od b">colsample_bytree</code>和<code class="fe om on oo od b">learning_rate</code>，这两个参数在试验中持续下降。<code class="fe om on oo od b">reg_lambda</code>和<code class="fe om on oo od b">reg_alpha</code>正在背离，这证实了我们之前的假设，即我们应该减少其中一个，同时增加另一个。</p><p id="9b1d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们要小心不要把太多的值放在这些结果中，因为贝叶斯优化可能已经找到了它正在利用的交叉验证损失的局部最小值。这里的趋势很小，但令人鼓舞的是，在接近搜索结束时发现了最佳值，表明交叉验证分数在继续提高。</p><h2 id="2576" class="mi mj jb bd mk ml mm dn mn mo mp dp mq kz mr ms mt ld mu mv mw lh mx my mz na bi translated">超参数与交叉验证分数</h2><p id="566f" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">接下来的这些图显示了单个超参数的值与分数的关系。我们希望避免过于强调这些图表，因为我们不会一次改变一个超参数，而且多个超参数之间可能存在复杂的相互作用。一个真正精确的图表应该是 10 维的，显示所有<strong class="ks jc">超参数的值和结果分数。如果我们能够理解一个<strong class="ks jc"> 10 维的</strong>图，那么我们也许能够计算出超参数的最佳组合！</strong></p><p id="84a4" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这里随机搜索是蓝色的，贝叶斯搜索是绿色的:</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pm"><img src="../Images/a151f296640beff9d75d3188f705353e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g3o2ZwBibpnaI9QdIVCVtw.png"/></div></div></figure><p id="f00a" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><strong class="ks jc">唯一明显的区别是分数随着学习率的增加而降低。</strong>我们不能说这是由于学习率本身，还是其他因素(我们将很快研究学习率和估计数之间的相互作用)。</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pm"><img src="../Images/e6cc873fcc57eebb3513cb2a23ca2bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lelcJWuc-bdRvMKrD_ywuw.png"/></div></div></figure><p id="83a9" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这里没有任何强劲的趋势。由于<code class="fe om on oo od b">boosting_type = 'goss'</code>不能使用<code class="fe om on oo od b">subsample</code>(必须设置为等于 1.0)，所以相对于子样本的分数有点偏差。虽然我们不能一次查看所有 10 个超参数，但如果我们转向 3D 图，我们可以一次查看两个！</p><h1 id="195b" class="ng mj jb bd mk nh ni nj mn nk nl nm mq kh nn ki mt kk no kl mw kn np ko mz nq bi translated">三维绘图</h1><p id="6d26" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">为了尝试和检验超参数的同时效应，我们可以用 2 个超参数和分数制作 3D 图。真正精确的图应该是 10 维的(每个超参数一个),但在这种情况下，我们将坚持 3 维。(详见代码，Python 中的 3D 绘图非常简单)。首先，我们可以显示<code class="fe om on oo od b">reg_alpha</code>和<code class="fe om on oo od b">reg_lambda</code>，正则化超参数与分数的关系(针对贝叶斯选择):</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/34821a6285222899bd7992356dcd3aca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*_w2t9uOx3BOJBULnGJM4aQ.png"/></div></figure><p id="c431" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这有点难以理解，但如果我们记得最好的分数出现在<code class="fe om on oo od b">reg_alpha</code>的 0.5 分和<code class="fe om on oo od b">reg_lambda</code>的 0.4 分左右，我们可以看到该地区的分数普遍较高。</p><p id="e8cb" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">接下来是<code class="fe om on oo od b">learning_rate</code>和<code class="fe om on oo od b">n_estimators</code>(集合中训练的决策树数量)。虽然学习率是网格中的一个超参数，但决策树的数量(也称为推进轮数)是通过 5 重交叉验证的早期停止来发现的:</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/def32093e915e9ebd8ad4f2d55bd1250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*bkNtLNnlPZ2e8x6K8NQ94g.png"/></div></figure><p id="3fa8" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这一次有一个明显的趋势:较低的学习率和较高的估计数会增加分数。这种关系是预期的，因为较低的<a class="ae lm" href="https://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/" rel="noopener ugc nofollow" target="_blank">学习率</a>意味着每棵树的贡献减少，这需要训练更多的树。扩展的树数量增加了模型适应训练数据的能力(同时也增加了运行时间)。此外，只要我们使用足够折叠的早期停止，我们就不必担心与更多的树过度拟合。当结果与我们的理解一致时，感觉真好(<em class="oa">尽管当结果与我们的理解不一致时，我们可能会学到更多！</em>)</p><h2 id="1ff5" class="mi mj jb bd mk ml mm dn mn mo mp dp mq kz mr ms mt ld mu mv mw lh mx my mz na bi translated">关联热图</h2><p id="4982" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">对于最后的图，我想显示每个超参数之间的相互关系和分数。这些图不能证明因果关系，但它们可以告诉我们哪些变量是相关的:</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pn"><img src="../Images/63a82a7609f2fe2d5938ca2022f85c32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vb0fS1cbraQDdxIZuV2Jmg.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">Correlation Heatmap for Bayesian Optimization</figcaption></figure><p id="f5a6" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们已经从图表中找到了大部分信息，但是我们可以看到学习率和分数之间的<em class="oa">负相关</em>，以及估计数和分数之间的<em class="oa">正相关</em>。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="5383" class="ng mj jb bd mk nh oz nj mn nk pa nm mq kh pb ki mt kk pc kl mw kn pd ko mz nq bi translated">测试最佳超参数</h1><p id="7784" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">探索的最后一步是在一个完整的数据集上实现随机搜索最佳超参数和贝叶斯优化最佳超参数(数据集来自<a class="ae lm" href="https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features" rel="noopener ugc nofollow" target="_blank">这个内核</a>，我要感谢<a class="ae lm" href="https://www.kaggle.com/jsaguiar" rel="noopener ugc nofollow" target="_blank">作者</a>将其公之于众)。我们训练模型，在测试集上进行预测，最后上传到比赛中，看看我们在公共排行榜上的表现如何。努力了这么久，成绩还撑得住吗？</p><ul class=""><li id="bb77" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll po lt lu lv bi translated"><strong class="ks jc">随机搜索结果得分 0.790 </strong></li><li id="6dfa" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll po lt lu lv bi translated"><strong class="ks jc">贝叶斯优化结果得分 0.791 </strong></li></ul><p id="95c2" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">如果我们按照公共排行榜上的最佳分数，贝叶斯优化胜出！然而，公共排行榜仅基于 10%的测试数据，因此这可能是过度拟合测试数据的特定子集的结果。<strong class="ks jc">总的来说，我认为完整的结果——包括交叉验证和公共排行榜——表明当运行足够多的迭代时，两种方法产生相似的结果。</strong>我们可以确定的是，这两种方法都比手动调优！与我们之前的工作相比，我们最终的模型足以让我们在排行榜上上升 1000 位。</p><p id="f122" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">最后，以另一个图结束，我们可以看看来自训练过的 GBM 的特征重要性:</p><figure class="ns nt nu nv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pp"><img src="../Images/9bcdace52b9eff5c98bd0b7ed5bc4100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X2odTfj2JV910W18VuVJcQ.png"/></div></div></figure><p id="ab33" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><code class="fe om on oo od b">NEW_CREDIT_TO_ANNUITY_RATIO</code>和<code class="fe om on oo od b">NEW_EXT_SOURCES_MEAN</code>是 Kaggle 上的数据科学社区导出的特征，而不是原始数据中的特征。看到这些重要性如此之高令人欣慰，因为它显示了特性工程的<a class="ae lm" href="https://www.featurelabs.com/blog/secret-to-data-science-success/" rel="noopener ugc nofollow" target="_blank">价值。</a></p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="dbd8" class="ng mj jb bd mk nh oz nj mn nk pa nm mq kh pb ki mt kk pc kl mw kn pd ko mz nq bi translated">结论和后续步骤</h1><p id="8fb0" class="pw-post-body-paragraph kq kr jb ks b kt nb kc kv kw nc kf ky kz nd lb lc ld ne lf lg lh nf lj lk ll ij bi translated">这项工作的主要收获是:</p><ul class=""><li id="b973" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll po lt lu lv bi translated"><strong class="ks jc">使用贝叶斯优化的随机搜索和自动超参数调整是模型调整的有效方法</strong></li><li id="07e8" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll po lt lu lv bi translated"><strong class="ks jc">贝叶斯优化倾向于“集中”在更高的得分值上，而随机搜索更好地探索可能性</strong></li><li id="7da4" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll po lt lu lv bi translated"><strong class="ks jc">当给定足够的迭代次数时，两种方法在交叉验证和测试分数方面产生相似的结果</strong></li><li id="c89d" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll po lt lu lv bi translated"><strong class="ks jc">优化超参数对性能有显著影响，可与特征工程相比拟</strong></li></ul><p id="c9e8" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">从这里去哪里？嗯，总有很多其他方法可以尝试，比如<a class="ae lm" rel="noopener" target="_blank" href="/automated-feature-engineering-in-python-99baf11cc219">自动化特征工程</a>或者将问题作为时间序列来处理。我已经写了一本关于<a class="ae lm" href="https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics" rel="noopener ugc nofollow" target="_blank">自动化特征工程</a>的笔记本，所以这可能是我下一步的重点。我们还可以尝试其他模型，甚至进入深度学习领域！</p><p id="191b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我乐于接受建议，所以请在 Kaggle 或 Twitter 上告诉我。感谢阅读，如果你想看看我在这个问题上的其他作品，这里有一整套笔记本:</p><ul class=""><li id="6bbc" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll po lt lu lv bi translated"><a class="ae lm" href="https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction" rel="noopener ugc nofollow" target="_blank">温柔的介绍</a></li><li id="ecaf" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll po lt lu lv bi translated"><a class="ae lm" href="https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering" rel="noopener ugc nofollow" target="_blank">手动特征工程第一部分</a></li><li id="7d2e" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll po lt lu lv bi translated"><a class="ae lm" href="https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2" rel="noopener ugc nofollow" target="_blank">手册特征工程第二部分</a></li><li id="4066" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll po lt lu lv bi translated"><a class="ae lm" href="https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics" rel="noopener ugc nofollow" target="_blank">自动化特征工程简介</a></li><li id="19c6" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll po lt lu lv bi translated"><a class="ae lm" href="https://www.kaggle.com/willkoehrsen/tuning-automated-feature-engineering-exploratory" rel="noopener ugc nofollow" target="_blank">高级自动化特征工程</a></li><li id="ecb0" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll po lt lu lv bi translated"><a class="ae lm" href="https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection" rel="noopener ugc nofollow" target="_blank">功能选择</a></li><li id="7667" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll po lt lu lv bi translated"><a class="ae lm" href="https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search" rel="noopener ugc nofollow" target="_blank">模型调整简介:网格和随机搜索</a></li><li id="1e20" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll po lt lu lv bi translated"><a class="ae lm" href="https://www.kaggle.com/willkoehrsen/automated-model-tuning" rel="noopener ugc nofollow" target="_blank">自动模型调整</a></li><li id="eaf8" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll po lt lu lv bi translated"><a class="ae lm" href="https://www.kaggle.com/willkoehrsen/model-tuning-results-random-vs-bayesian-opt" rel="noopener ugc nofollow" target="_blank">模型调优结果</a></li></ul><p id="c00d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">那里应该有足够的内容让任何人忙碌一会儿。现在可以开始为下一篇文章做更多的学习/探索了！数据科学最棒的地方在于，你一直在移动，寻找下一个需要征服的技术。每当我发现那是什么，我一定会分享它！</p><p id="7b43" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">一如既往，我欢迎反馈和建设性的批评。我可以在 Twitter <a class="ae lm" href="http://twitter.com/@koehrsen_will" rel="noopener ugc nofollow" target="_blank"> @koehrsen_will </a>上找到。</p></div></div>    
</body>
</html>