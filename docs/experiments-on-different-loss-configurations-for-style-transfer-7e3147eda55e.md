# 风格转换的不同损失配置实验

> 原文：<https://towardsdatascience.com/experiments-on-different-loss-configurations-for-style-transfer-7e3147eda55e?source=collection_archive---------4----------------------->

当我参加[杰瑞米·霍华德的](https://medium.com/@jeremyhoward)优秀的[程序员前沿深度学习](http://course.fast.ai/part2.html)时，我对风格转移很感兴趣。我想探究改变损耗配置如何改变生成的图像。我想看看使用不同的损耗网络如何产生不同的图像。我用的不同损耗网络是 vgg-16 和 vgg-19。这是一篇关于我的发现的博文。

那些不熟悉风格转移的人可以阅读[这篇](https://medium.com/@singlasahil14/introduction-to-style-transfer-faa60f3f5257)介绍其工作原理的博客文章。

所有实验都是使用以下内容和样式图像完成的:

![](img/aee20768c01b9a606065a67ab1ff6351.png)![](img/e587d82b91695c0f7f4a9afd9b6de7c7.png)

left (content image), right (style image) used in these experiments

这些实验中使用的所有预训练损耗网络都是从 [tensorflow 瘦模型库](https://github.com/tensorflow/models/tree/master/slim)下载的。除非特别说明，否则初始图像是内容图像，使用的预训练网络是 vgg-16。要找出 conv1_2、conv2_2 等指的是哪些层，请运行

![](img/8c7a22de76d6c518b4b593660f5cc2e6.png)

[在本次回购中。](https://github.com/singlasahil14/style-transfer)以下所有实验均使用本报告中给出的代码进行。

**实验 1:一次从一层使用风格损失进行训练**

我使用这 5 层的输出来计算样式损耗:conv1_2、conv2_2、conv3_3、conv4_1 和 conv5_1。作为一个开始实验，我将内容权重和电视权重设置为零，将图像初始化为“噪声”。并使用这些层中的每一层生成样式图像(每次使用一层来计算样式损失)。生成的风格图像应该与绘画的仿作相对应。

以下是产出:

![](img/35d4394c9981281332fb2a0507702329.png)![](img/084125f2b363d7eccc9a2cc49f3acfba.png)![](img/18bc461513b45a64a24ecb184b69ca51.png)

image created using loss from conv1_2, conv2_2 and conv3_3 (left to right)

![](img/3653ac93e2d905b353aa351d28d34476.png)![](img/25d15e1a2b8ce8b0057a6e4cfd87ae8f.png)

image created using loss from conv4_1 (left) and conv5_1 (right)

在早期层(conv1_2 和 conv2_2)的输出中，风格图像中需要较小感受野(如咖啡色背景)的图案是显著的。在后面的层(conv3_3 和 conv4_1)中，更大的图案更明显。conv5_1 的输出看起来更像垃圾。我认为这是因为在该层很少激活。所以对损失贡献不大。下面的初始损失也显示了同样的情况。

为了生成风格化的图像，我使用 conv3_3 来计算内容损失。

当我保持所有 5 层的权重等于 1 时，以下是图像初始化为“内容”时的层内容和样式损失值:

![](img/237eb93a42c680b8f83895fe27100c5e.png)

layer-wise style loss values for initial image ‘content’

当我保持所有 5 层的权重相等时，以下是图像初始化为“噪波”时的层内容和样式损失值:

![](img/8c46c083b62b8e8b82b90aa4d2d4cf03.png)

layer-wise style loss values for initial image ‘noise’

显然，在这两种情况下，后面的层对样式损失的贡献都非常小。在大多数出版的关于风格转换的作品中，所有层的权重都是一样的，这没有意义。

接下来，我想看看如果我只使用其中一层来训练，会产生什么样的风格化图像。

![](img/b41b623a444cab8f87296914f3b7b166.png)![](img/ad607e01336d6eb92aeadb178db5db64.png)

Output image when trained with gram matrix from layer conv1_2 and conv2_2 (left to right)

![](img/d07750c25621f798ca2eba2716e6271a.png)![](img/2f284d652c1aeb2310e6674ae7ee615c.png)

Output image when trained with gram matrix from layer conv3_3 and conv4_1 (left to right)

![](img/4a6e9cfe2d7c246c18c6684df13ad1d6.png)

Output image when trained with gram matrix from layer conv5_1

**从上面生成的图像可以明显看出，当我们从 conv1_2 移动到 conv5_1 时，笔触的大小会增加。【conv5 _ 1 的输出中有一些奇怪的模式。很可能是因为 conv5_1 没有包含太多的信息(我不得不将这一层的损失缩放 1e6 来生成此图像)。所以这只是噪音。这 5 个中我最喜欢的输出？conv3_3 的输出。**

**实验 2:利用一次一层的内容损失进行训练**

我使用 3 层的输出来逐一计算内容损失:conv2_2、conv3_3、conv4_1。

我首先尝试通过将样式权重和电视权重设置为 0 来查找生成的图像，并计算每个内容层的损失，即仅使用内容损失。

![](img/b87bea8dc00ca7622ef5de9a6f5bda27.png)

Image created using content loss from conv2_2

![](img/53a53f6529a5823335f3a9807846c1bb.png)

Image created using content loss from conv3_3

![](img/4b37c406a73a6667dcef4199443266a7.png)

Image created using content loss from conv4_1

可以看出，当我们从 conv2_2 移至 conv4_1 时，仍能捕捉到窗口等高层信息，但边缘、拐角等低层信息越来越模糊。

接下来，我生成了风格化的图像(使用内容，风格和电视损失)。对于风格损失，我使用层 conv3_3。我想检查一下，如果我更改内容层，上一个实验中 conv3_3 层的图像输出会有什么变化。

以下是生成的图像:

![](img/d04bc58d1434fb915d041e29e906d04b.png)![](img/477d7c39434b5536e96fb872ca50182a.png)

Output image when trained with features from layer conv2_2 and conv3_3 (left to right)

![](img/56ea4fb583999b5aeadace843b11cfb1.png)

Output image when trained with features from layer conv4_1

很明显。**当我们从 conv2_2 移到 conv4_1 时，建筑的边缘变得更钝，颜色变得更淡。捕获的原始内容要少得多。**

**实验 3:使用损失网络中两个最大汇集层之间不同层的风格损失进行训练**

接下来，我想看看在两个最大池层之间使用不同的层来计算样式损失是否会改变生成的风格化图像。为此，我使用了以下层“conv3_1”、“conv3_2”和“conv3_3”。

![](img/be2f2277dbd92d366347d77385ceaf7b.png)![](img/e10b742fced4d1623256041d82e687e0.png)![](img/6ac38c02d5f7302bbd63aa7f98ee1320.png)

images generated by training using only the style loss: conv3_1, conv3_2, conv3_3 (left to right)

以上是仅使用风格损失进行训练时的输出。conv3_3 的输出与 conv3_1 和 conv3_2 的输出截然不同。

接下来，我尝试通过使用这些图层来计算风格损失，从而创建风格化的图像。用于内容丢失的层是 conv2_2。

以下是生成的图像:

![](img/79367dc7c6827022d65a28f894c2b902.png)

image generated for loss calculated from layer conv3_1

![](img/8cc164eaacffecdca16aacdca79c1467.png)![](img/c0555dcd9f3a1162a09f26028b33906f.png)

image generated for loss calculated from layer conv3_2, conv3_3 (left, right)

![](img/79367dc7c6827022d65a28f894c2b902.png)

magnified view of image generated using style loss from conv3_1

![](img/67741c724c082ae77c86f107fb0f74d6.png)

artifacts shown with black underlining (the image is the same as conv3_3 shown above)

在 conv3_1 和 conv3_3 之间，上图(conv3_3 输出)中带黑色下划线的伪像明显更高。**有趣的是，conv3_3 的仿体不同于 conv3_1 和 conv3_2。生成的图像也是如此。**

**实验 4:使用不同大小的风格图像进行训练**

我使用 conv2_2 层的输出来计算内容损失:conv2_2。对于风格损失，我使用权重为 1 的 conv2_2。我没有使用原始尺寸(928x514)的样式图像，而是调整了图像的大小，使短边等于特定的尺寸，并在中间裁剪了较大的尺寸，以提取一个正方形。我试的尺码是 128，256，384 和 512。

以下是生成的图像:

![](img/50a7dea5debdfdd5c0c7cb92f664dc72.png)![](img/b328cf2a9043be6b1d7b72671920252c.png)

stylized images for style image sizes 128 (left) and 256 (right)

![](img/db0c7f28b5240b74346f64c05e595a53.png)![](img/a39a30520a1fee50abdf4d25fa86d6b0.png)

stylized images for style image sizes 384 (left) and 512 (right)

![](img/40d70981e0f88e1ab2464f06aaaa5c01.png)

stylized image for default style image size : 514x928

使用较小尺寸的样式图像生成的图像不好看，甚至不能显示样式图像的笔触。

**同样，很明显，随着风格图像尺寸的增加，生成的图像更好地捕捉了画家的笔触。**

**实验 5:使用不同的 vgg 网络(vgg16 和 vgg19)进行训练**

接下来，我从 vgg16 和 vgg19 生成图像。内容损耗用 conv2_2 计算，风格损耗用 conv3_1 计算。

首先，我只生成了模仿作品(保持内容和电视权重为零)。生成了以下仿作:

![](img/b68d98808f98843f8d9326c6f28e7d08.png)![](img/8c006036b950e14de9e88f0a1ab5a9e8.png)

pastiches generated left: vgg-16, right: vgg-19

生成的仿作几乎没有任何不同。因此，我期望程式化的图像几乎没有什么不同，但我完全错了。这些是生成的图像:

![](img/7bd8e825a50e09e2a116e1b54c5dbca8.png)![](img/af99809bb5662dbbf1ecdc256203a0ff.png)

Output by training using vgg-16 (left) and vgg-19 (right) loss networks

使用 vgg-19 生成的图像看起来更好，捕捉内容更好。我对此很感兴趣，并决定绘制图像在训练过程中的内容损失、风格损失和总损失。

![](img/39961cf445c012d97c227a0a49ab3140.png)![](img/271efd8be1bdfb86f79271ee9a8f925c.png)![](img/d4dbb0834a7acd2136512369304bea76.png)

content loss, style loss and total loss of image as it gets trained

**更重要的是，vgg-19 的起始风格损失值远低于 vgg-16。我的猜测是，生成的图像之所以不同，是因为网络对内容和风格损失赋予了不同的“隐含”权重。**所谓“隐含”权重，我指的是网络本身赋予内容和风格损失的权重，而不是因为我指定的内容风格和电视权重。

注意，这里 vgg16 和 vgg19 表示不同的功能。因此，这种比较并不完全意味着“vgg-19”的绝对含量损失低于“vgg-16”。这只是表明，在优化图像的同时，vgg-16 的风格损失变得更加重要。

注:-对于后续实验，使用 conv2_2 计算(6，7，8)含量损失，conv3_1 计算风格损失。

**实验 6:使用不同初始化(“噪声”/“内容”)的训练**

![](img/97e9f71964b357d1dfd061212fa4243f.png)![](img/a0b3096c633b352ce06be3fc35c88190.png)

image generated with initial ‘content’ (left) and initial ‘noise’ (right)

![](img/369d786b5102b5b9e68e1074a4d9cf98.png)![](img/417fd296b7ff864e42b6984902d85e37.png)![](img/871602906c87ec6737036224b4d22a7c.png)

content loss, style loss and total loss of image for both these initial conditions as it gets trained

可以看出，内容和噪声图像都收敛到几乎相同的损失值。尽管生成的图像不同。**使用“内容”初始化时，损耗收敛更快。**

**实验 7:使用不同类型的衬垫进行训练(相同/有效)**

![](img/702660b72487f66bc3011b7cc71145ba.png)![](img/f8e26ee131b9ee68fa203cd913ea4c70.png)

images generated using padding ‘SAME’ (left) and ‘VALID’ (right) in the loss network

![](img/82850bf4a1c6a340fbe92601fbdb304e.png)![](img/7b947decaa2c79c3e74554b81e5f42a4.png)![](img/7097133a1286ddd42c30b6827e0f64d1.png)

content loss, style loss and total loss of image for both these types of padding as it gets trained

**损失值相差不大。结果完全符合我的预期。**

**实验 8:使用不同类型的汇集(最大/平均)进行训练**

![](img/ffd4fdde48bcd4e107fc49efbede55f8.png)![](img/c412c66523eaed9049a15b7e3fb5f5fa.png)

image generated with pooling ‘avg’ (left) and pooling ‘max’ (right)

![](img/687bac7f4d4401a7de04d8c50ab5a5e4.png)![](img/8774df10cdd7350cb0b6fcf5c6addf75.png)![](img/8ec260f76eef43c34b9d5530fd21f5e6.png)

content loss, style loss and total loss of image for both these types of pooling as it gets trained

可以看到，使用“max”生成的图像有奇怪的点。**使用“avg”生成的图像更加均匀。更重要的是,“平均值”的总损失收敛速度比“最大值”快得多。**

**实验 9:使用不同风格权重值进行训练**

我想看看样式权重对生成的图像有什么影响。为此，我决定对内容层 conv2_2 和样式层 conv3_1 使用 vgg-19，样式权重设置为 200、400、800、1600 和 3200。生成了以下图像:

![](img/771a589c5506b6a3f2fa16db2687047f.png)![](img/962f5a95f08d1fa3825683e5cc001a92.png)

left image: style-weight 200, right image: style-weight 400

![](img/9685fe386654f57035f6fe2abc9a42ff.png)![](img/c2ea0c9726d165918cbac37e890856fc.png)

left image: style-weight 800, right image: style-weight 1600

![](img/602587b1e3ed042cf696e4dc3004ecfa.png)

style-weight 3200

**可以看出，在使用风格权重 3200 生成的图像中，画家的笔触要突出得多。**原始内容(颜色等)不太突出。

**实验 10:使用不同的总变化损失值进行训练**

我想看看总变差损失对生成的图像有什么影响。为此，我决定对内容层 conv2_2 和样式层 conv3_1 使用 vgg-19，总变差损失为零，下一个电视重量设置为 200。生成了以下图像:

![](img/48d9be470597cdb4c6c8441a1a5d59c4.png)

Image created using default content and style weight and tv weight = 0\. On careful observation, it becomes visible that image is quite rough.

![](img/fb150d76eddd1fd2eb65891c8271dbd7.png)

Image created using default content and style weight and tv weight = 200\. Roughness present in the previous image is gone

总变差损失有助于去除生成图像的粗糙纹理。**从上面两幅图像中可以观察到，一幅有总变差损失，另一幅没有，总变差损失非零的图像非常平滑。**

**实验 11:使用风格图像的不同裁剪区域进行训练**

我裁剪了样式图像的不同区域，以了解它如何影响生成的图像的质量。

以下是提取的样式图像部分:

![](img/3b733cccef7bf23e481e4bbc92d349c2.png)![](img/b821a03c876c0fa61c3258e757a80182.png)![](img/7333b924c883c5f267946f359e53c678.png)

style image portions used for training the image: left region, center region, right region of the style image (left to right)

![](img/fdca8e5ac346634f639372e68fce4118.png)

complete style image

和以前一样，我首先为左侧区域、中间区域、右侧区域和完整的图像生成了仿制品。

使用了 vgg-16 网络。使用的样式层是 conv3_1。

![](img/94e9c01ff9d5c25a185450f7383ea53c.png)![](img/28526f776a87ccf3db743cf43f6da02d.png)

left region (left), center region (right)

![](img/24de0c987594db6bd7ced9e6d29f5610.png)![](img/1cf7cbbd9ee507b8256f7770656946cf.png)

right region (left) and complete image (right)

右侧区域的仿作明显不同于左侧、中间区域和完整图像的仿作。这是预料之中的，因为样式图像的右边区域明显不同于所有其他区域，甚至不同于整个样式图像本身。

接下来生成风格化的图像，内容层是 conv2_2，样式层是 conv3_1。内容重量、风格重量和电视重量在规范中给出(分别为 8 和 200)。

生成了以下图像:

![](img/e38453e5ecdfdf570a91a9482507a85f.png)![](img/8ab31a9eaa5bbd757f8dab14cbfbe9ff.png)

stylized image generated using left region (left) and center region (right)

![](img/e2093729db4a00d6d4cafdf781662078.png)![](img/2a74cf4304739acb3b0087d9a5693391.png)

stylized image generated using right region (left) and using whole style image (right)

如我们所见，从右侧区域生成的图像明显不同于从左侧、中间区域和完整图像生成的图像。

> ***如果您喜欢这篇文章，请点击下面的小拍手图标帮助他人找到它。非常感谢！***