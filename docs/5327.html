<html>
<head>
<title>DenseNet on CIFAR10</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CIFAR10 上的 DenseNet</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/densenet-on-cifar10-d5651294a1a8?source=collection_archive---------11-----------------------#2018-10-11">https://towardsdatascience.com/densenet-on-cifar10-d5651294a1a8?source=collection_archive---------11-----------------------#2018-10-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="4156" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章可以在 PDF <a class="ae kl" href="http://www.pabloruizruiz10.com/resources/CNNs/DenseNets.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><p id="4f64" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是关于 CNN 架构的<a class="ae kl" href="https://medium.com/@pabloruizruiz/deep-convolutional-neural-networks-ccf96f830178" rel="noopener">系列教程的一部分</a>。</p><p id="cf84" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">主要目的是在应用于 CIFAR-10 数据集时提供理解 DenseNets 的洞察力。</p><ul class=""><li id="2cdb" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">对于 DenseNets 应用于 ImageNet，这是一个更深入的教程，有另一个教程<a class="ae kl" href="https://medium.com/@pabloruizruiz/understanding-and-visualizing-densenets-7f688092391a" rel="noopener">在这里</a>。</li><li id="84c5" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">在这里找到<a class="ae kl" href="https://github.com/PabloRR100/Convolutional-Neural-Networks/blob/master/3_DenseNets/densenets_Paper.py" rel="noopener ugc nofollow" target="_blank">构建这些架构的代码:</a></li></ul><p id="225f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">索引</strong></p><ul class=""><li id="e1bb" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">介绍</li><li id="e9a0" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">结构</li><li id="613a" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">卷积 1</li><li id="0ffc" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">密集块</li><li id="2d26" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">过渡块</li><li id="562a" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">致密层</li><li id="a3f9" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">摘要</li></ul></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><h1 id="354a" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">介绍</h1><p id="526a" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">这项工作是<a class="ae kl" href="https://medium.com/@pabloruizruiz/understanding-and-visualizing-densenets-7f688092391a" rel="noopener">之前教程</a>的延续，在那里我们在<a class="ae kl" href="https://arxiv.org/pdf/1608.06993.pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>之后揭开了 DenseNet 的神秘面纱。但是，这种结构是为了在 ImageNet 数据集上运行良好而构建的。</p><p id="804e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ImageNet 数据集由属于 1000 个不同类别的一组大小(224x224)的图像(作者使用了 128 万幅训练图像、5 万幅验证图像和 10 万幅测试图像)组成。但是，CIFAR10 由一组不同的映像(45k 训练映像、5k 验证映像和 10k 测试映像)组成，这些映像分布在 10 个不同的类别中。</p><p id="4d4e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为输入体积(图像)的大小完全不同，所以很容易认为相同的结构不适合在该数据集上训练。我们无法在没有维度不匹配的情况下对数据集执行相同的缩减。</p><p id="eb59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将遵循作者给出的解决方案(让 DenseNets 在 CIFAR10 上进行训练，以构建具有 100 个层和 12 的增长因子的 DenseNet-BC)，这也像 ImageNet 数据集一样难以遵循。在论文[1]的第<strong class="jp ir"> <em class="mk"> 3 节 DenseNet—实现细节</em> </strong>中，提供了为 CIFAR10 构建 dense net 的配置。</p><p id="2710" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们按照他们给出的字面解释来构造 DenseNets。最重要的是，<strong class="jp ir">对于 CIFAR10 实施，有 3 个 DenseBlock</strong>，而不是 4 个，<strong class="jp ir">，每个 dense block 中的 DenseLayers 数量相等。</strong>因为作者给出的参数是网络的总层数<em class="mk"> L </em>，所以我们计算每个密集块中需要包含多少个密集层才能达到该配置。被压缩系数为<em class="mk">θ</em>:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ml"><img src="../Images/c508ac754d4e785e78953325861cef00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2tqT8YMpPQ2sJrbZivWxsA.png"/></div></div></figure><p id="e376" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">移除 4 层的原因是因为我们只需要考虑那些属于密集块的层。此外，压缩在过渡层引入了一种新的操作(第一个 1x1 卷积，如我们在原始作品中看到的)</p><p id="cea9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，对于 100 层 DenseNet-BC，每个 DenseBlock 将由 16 个密集层组成。由于我们将在“DenseNet-Layers-GrowthRate”之后调用 DenseNet，因此本文中涉及的 DenseNet 对应于 dense net-100–12。</p><h1 id="cc56" class="lh li iq bd lj lk mx lm ln lo my lq lr ls mz lu lv lw na ly lz ma nb mc md me bi translated">结构</h1><p id="d89a" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">遵循之前关于 DenseNets 工作的相同方法，让我们先看一下整体情况，然后再一层一层地深入细节。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nc"><img src="../Images/ccb8f16e016b5717af630a45cd76e3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vH9hL-zuO8fdAw9yg9pnbw.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Figure 1. Scheme DenseNet-100–12 on CIFAR10</figcaption></figure><p id="87cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">揭开 ResNet-121 的神秘面纱后，图 1 看起来已经很熟悉了。我们可以观察到相同的模式，第一个单卷积层，然后是两对密集块—过渡块对，第三个密集块，然后是全局平均池，以将其减少到 1x1x342 矢量，从而提供给密集层。</p><h1 id="b7a9" class="lh li iq bd lj lk mx lm ln lo my lq lr ls mz lu lv lw na ly lz ma nb mc md me bi translated">卷积 1</h1><p id="f2eb" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">在进入第一个密集块之前，DenseNet 的第一步是一个 3×3 卷积和一个批处理归一化操作。步幅为 1，填充为 1，以使输出大小与输入大小相匹配。请注意，我们与 DenseNet for ImageNet 的第一个重大区别是，我们没有在第一个块中包括 max pooling 操作，以减少输入卷的维数。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/f718c065ae6e291ca789ac4131b6ffe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*H51464nEyPm1oV5JrcbrpQ.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Figure 2. Conv1</figcaption></figure><p id="e5f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以从图 2 中看出，Conv1 的输出音量确实是 32x32x24。下一步是将这个新卷作为输入引入下面的密集数据块 1 (D1)。</p><h1 id="01bc" class="lh li iq bd lj lk mx lm ln lo my lq lr ls mz lu lv lw na ly lz ma nb mc md me bi translated">密集块</h1><p id="b1b3" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">致密块(DB)由 16 个致密层组成，如之前对 dense net-BC-100–12 的计算。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ni"><img src="../Images/a868bd3d16d9169cd1ea6b85d3c0ae13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T2v1Bi0e78iXDLFU1H0c_g.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Figure 3. One level deeper look at Dense-100–12. Dense Block and Transition Block. DLx: Dense Layer x</figcaption></figure><p id="9649" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图 3 表示了一个简化的版本，因为 16 个块在一个图像中压缩太多了，但是我们可以看到特征图的数量是如何以增长率增加的，每次 12 个(24、36、48……)。因为有 16 层，DB1 的最终体积将是 216。</p><h1 id="331a" class="lh li iq bd lj lk mx lm ln lo my lq lr ls mz lu lv lw na ly lz ma nb mc md me bi translated">过渡块</h1><p id="6ce4" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">两个 db 之间的过渡块(TB)充当下采样，通过合并步幅= 2、核大小= 2 和填充= 1，将特征图的数量减少一半(θ= 0.5)，并且还将特征图的大小减少一半。</p><h1 id="0459" class="lh li iq bd lj lk mx lm ln lo my lq lr ls mz lu lv lw na ly lz ma nb mc md me bi translated">致密层</h1><p id="2310" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">我们只需要看看数据库中发生了什么，以确认为什么特征地图的大小增加了<em class="mk">(增长率*密集图层的数量)/ 2 </em> —查看 D1 是如何从 24 增加到 216 的—而特征地图的大小保持不变。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nj"><img src="../Images/4847964fda8ffe8d6009895106376923.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RTXc_og9DrE16k-wYtmJ8A.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Figure 4. 2nd level deep. Full schematic representation of DenseNet-BC-100–12</figcaption></figure><p id="7eb7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以检查每个密集层(DL)如何以 4 *过滤器数量的增长率执行 1x1 卷积。数字 4 是作者在论文中给出的，大多数知识库称之为 bn_size(瓶颈层数的乘法因子)。这个 1x1 卷积在 3x3 卷积之前应用线性变换，滤波器的数量是增长率。</p><p id="820c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">查看第二个卷积如何唯一负责连接的过滤器贴图的数量，因此它在连接它们之后完美地匹配增长率配置。每个 DL 都向它们的输入卷添加 k 个新的特征地图。这再次证实了为什么致密块体增加了<em class="mk">增长率*致密层数</em>。</p><h1 id="980b" class="lh li iq bd lj lk mx lm ln lo my lq lr ls mz lu lv lw na ly lz ma nb mc md me bi translated">摘要</h1><p id="d6c5" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">通过改变图 1 中<em class="mk"> n </em>的值，遵循作者建立的解释规则的结果产生以下结构:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nk"><img src="../Images/b6483f4ffd905f1a243a62850903118c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-BbUCAddjZZ1OJ0Xf55HoA.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">Table 1. DenseNets architectures for CIFAR-10</figcaption></figure><p id="5da4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，直观地说，这些架构与在 ImageNet 的<a class="ae kl" href="https://medium.com/@pabloruizruiz/understanding-and-visualizing-densenets-7f688092391a" rel="noopener">工作结束时展示的 ImageNet 架构并不匹配。</a></p><p id="1d61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里找到<a class="ae kl" href="https://github.com/PabloRR100/Convolutional-Neural-Networks/blob/master/3_DenseNets/densenets_Paper.py" rel="noopener ugc nofollow" target="_blank">构建这些架构的代码</a>:</p></div></div>    
</body>
</html>