<html>
<head>
<title>Machine Learning — Text Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习—文本处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-text-processing-1d5a2d638958?source=collection_archive---------0-----------------------#2018-09-13">https://towardsdatascience.com/machine-learning-text-processing-1d5a2d638958?source=collection_archive---------0-----------------------#2018-09-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="36fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">文本处理是许多 ML 应用程序中最常见的任务之一。下面是这种应用的一些例子。</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="3e41" class="ku kv iq kq b gy kw kx l ky kz">• <strong class="kq ir">Language Translation: Translation of a sentence from one language to another.</strong></span><span id="97a1" class="ku kv iq kq b gy la kx l ky kz">• <strong class="kq ir">Sentiment Analysis: To determine, from a text corpus, whether the  sentiment towards any topic or product etc. is positive, negative, or neutral.</strong></span><span id="fd1e" class="ku kv iq kq b gy la kx l ky kz">• <strong class="kq ir">Spam Filtering:  Detect unsolicited and unwanted email/messages.</strong></span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi lb"><img src="../Images/c9707d39ce7ac12bc8e527886680ace4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KljmJybQDj1mJAL-oS8VWQ.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Courtesy (<a class="ae ln" href="https://sigmoidal.io/machine-learning-terminology-explained-top-8-must-know-concepts/" rel="noopener ugc nofollow" target="_blank">sigmoidal</a>)</figcaption></figure><p id="647b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些应用程序处理大量文本来执行分类或翻译，并且涉及大量后端工作。将文本转换成算法可以消化的东西是一个复杂的过程。在本文中，我们将讨论文本处理中涉及的步骤。</p><h2 id="a413" class="ku kv iq bd lo lp lq dn lr ls lt dp lu jy lv lw lx kc ly lz ma kg mb mc md me bi translated">步骤 1:数据预处理</h2><ul class=""><li id="5392" class="mf mg iq jp b jq mh ju mi jy mj kc mk kg ml kk mm mn mo mp bi translated">标记化—将句子转换成单词</li><li id="e5ed" class="mf mg iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">删除不必要的标点、标签</li><li id="cc93" class="mf mg iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">删除停用词—常用词，如“the”、“is”等。它们没有特定的语义</li><li id="3e7e" class="mf mg iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">词干—通过删除不必要的字符(通常是后缀)来消除词形变化，从而将单词简化为词根。</li><li id="1295" class="mf mg iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">词汇化——通过确定词类和利用语言的详细数据库来消除屈折的另一种方法。</li></ul><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="857d" class="ku kv iq kq b gy kw kx l ky kz">The stemmed form of studies is: studi<br/>The stemmed form of studying is: study</span><span id="8c88" class="ku kv iq kq b gy la kx l ky kz">The lemmatized form of studies is: study<br/>The lemmatized form of studying is: study</span></pre><p id="fb1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，词干化和词元化有助于将像“研究”、“学习”这样的词简化为普通的基本形式或词根“学习”。关于词干化和词汇化的详细讨论，请参考<a class="ae ln" href="https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/" rel="noopener ugc nofollow" target="_blank">这里的</a>。请注意，并非所有步骤都是强制性的，这取决于应用程序的使用情形。对于垃圾邮件过滤，我们可能会遵循上述所有步骤，但可能不会考虑语言翻译问题。</p><p id="abdc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以用 python 做很多文本预处理操作。</p><ul class=""><li id="075c" class="mf mg iq jp b jq jr ju jv jy mv kc mw kg mx kk mm mn mo mp bi translated"><a class="ae ln" href="http://www.nltk.org/" rel="noopener ugc nofollow" target="_blank">NLTK</a>——自然语言工具包是最著名和最常用的 NLP 库之一，可用于从标记化、词干提取、标记、解析等各种任务</li><li id="b52d" class="mf mg iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated"><a class="ae ln" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank"> BeautifulSoup </a> —用于从 HTML 和 XML 文档中提取数据的库</li></ul><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="2c93" class="ku kv iq kq b gy kw kx l ky kz">#using NLTK library, we can do lot of text preprocesing<br/><strong class="kq ir">import nltk<br/>from nltk.tokenize import word_tokenize</strong><br/>#function to split text into word<br/><strong class="kq ir">tokens = word_tokenize("The quick brown fox jumps over the lazy dog")<br/>nltk.download('stopwords')<br/>print(tokens)</strong></span></pre><p id="bce2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">OUT: ['The '，' quick '，' brown '，' fox '，' jumps '，' over '，' The '，' lazy '，' dog']</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="9116" class="ku kv iq kq b gy kw kx l ky kz"><strong class="kq ir">from nltk.corpus import stopwords<br/>stop_words = set(stopwords.words(‘english’))<br/>tokens = [w for w in tokens if not w in stop_words]<br/>print(tokens)</strong></span></pre><p id="7a5c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">OUT: ['The '，' quick '，' brown '，' fox '，' jumps '，' lazy '，' dog']</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="9486" class="ku kv iq kq b gy kw kx l ky kz">#NLTK provides several stemmer interfaces like Porter stemmer, #Lancaster Stemmer, Snowball Stemmer<br/><strong class="kq ir">from nltk.stem.porter import PorterStemmer<br/>porter = PorterStemmer()<br/>stems = []<br/>for t in tokens:    <br/>    stems.append(porter.stem(t))<br/>print(stems)</strong></span></pre><p id="7ddb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">OUT: ['the '，' quick '，' brown '，' fox '，' jump '，' lazi '，' dog']</p><h2 id="d9b6" class="ku kv iq bd lo lp lq dn lr ls lt dp lu jy lv lw lx kc ly lz ma kg mb mc md me bi translated">步骤 2:特征提取</h2><p id="5ea7" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">在文本处理中，文本中的词代表离散的、分类的特征。我们如何以一种算法可以使用的方式对这些数据进行编码？从文本数据到实值向量的映射称为特征提取。用数字表示文本的最简单的技术之一是<strong class="jp ir">单词包。</strong></p><p id="16fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">单词包(BOW): </strong>我们在称为词汇的文本语料库中制作唯一单词的列表。然后，我们可以将每个句子或文档表示为一个向量，每个单词用 1 表示存在，用 0 表示不在词汇表中。另一种表示可以是计算每个单词在文档中出现的次数。最流行的方法是使用<strong class="jp ir">术语频率-逆文档频率(TF-IDF) </strong>技术。</p><ul class=""><li id="52a0" class="mf mg iq jp b jq jr ju jv jy mv kc mw kg mx kk mm mn mo mp bi translated"><strong class="jp ir">术语频率(TF) </strong> =(术语 t 在文档中出现的次数)/(文档中的术语数)</li><li id="d4fb" class="mf mg iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated"><strong class="jp ir">逆文档频率(IDF) = </strong> log(N/n)，其中，N 是文档的数量，N 是术语 t 出现过的文档的数量。罕见词的 IDF 很高，而常用词的 IDF 可能很低。从而具有突出不同单词的效果。</li><li id="f7a7" class="mf mg iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">我们计算一项的<strong class="jp ir"> TF-IDF </strong>值为= TF * IDF</li></ul><p id="8989" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们举一个例子来计算文档中一个术语的 TF-IDF。</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/87c23a442349646bc995e4555af26bdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*S3jAdLlMm5w0AUXkhwSIzw.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Example text corpus</figcaption></figure><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="ee67" class="ku kv iq kq b gy kw kx l ky kz"><strong class="kq ir">TF('beautiful',Document1) = 2/10, IDF('beautiful')=log(2/2) = 0<br/>TF(‘day’,Document1) = 5/10,  IDF(‘day’)=log(2/1) = 0.30<br/><br/>TF-IDF(‘beautiful’, Document1) = (2/10)*0 = 0<br/>TF-IDF(‘day’, Document1) = (5/10)*0.30 = 0.15</strong></span></pre><p id="744c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如你在文献 1 中看到的，TF-IDF 方法对“美丽”一词进行了严重的惩罚，但对“天”赋予了更大的权重。这是由于 IDF 部分，它赋予不同的单词更大的权重。换句话说，从整个语料库的上下文来看，“day”是 Document1 的一个重要单词。Python <a class="ae ln" href="http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>库为文本数据挖掘提供了高效的工具，并提供了在给定文本语料库的情况下计算文本词汇 TF-IDF 的函数。</p><p id="da50" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用 BOW 的一个主要缺点是它放弃了单词顺序，从而忽略了上下文，进而忽略了文档中单词的含义。对于自然语言处理(NLP ),保持单词的上下文是最重要的。为了解决这个问题，我们使用另一种叫做<strong class="jp ir"> </strong>单词嵌入的方法。</p><blockquote class="nc nd ne"><p id="9134" class="jn jo nf jp b jq jr js jt ju jv jw jx ng jz ka kb nh kd ke kf ni kh ki kj kk ij bi translated"><strong class="jp ir">单词嵌入:</strong>它是一种文本表示，其中具有相同含义的单词具有相似的表示。换句话说，它在一个坐标系统中表示单词，在该坐标系统中，基于关系语料库的相关单词被放置得更靠近。</p></blockquote><p id="474c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们讨论一些众所周知的<em class="nf">单词嵌入</em>模型:</p><h2 id="ca54" class="ku kv iq bd lo lp lq dn lr ls lt dp lu jy lv lw lx kc ly lz ma kg mb mc md me bi translated"><strong class="ak"> Word2Vec </strong></h2><p id="e394" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated"><a class="ae ln" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank"> Word2vec </a>将大量文本作为其输入，并产生一个向量空间，每个唯一的单词在该空间中被分配一个相应的向量。单词向量被定位在向量空间中，使得语料库中共享共同上下文的单词在空间中彼此非常接近。Word2Vec 非常擅长捕捉意思并在任务中展示它，如计算类比问题，形式为<em class="nf"> a </em>对<em class="nf"> b </em>如同<em class="nf"> c </em>对<em class="nf">？</em>。比如<em class="nf">男</em>之于<em class="nf">女</em>如同<em class="nf">叔</em>之于<em class="nf">？</em> ( <em class="nf">大妈</em>)使用基于余弦距离的简单矢量偏移方法。例如，以下是说明性别关系的三个单词对的向量偏移量:</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/56cc5df0a78f42cc9ec6c47a40ce0a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*YG622JLh0bLaX4ushe4D2Q.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">vector offsets for gender relation</figcaption></figure><p id="6958" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种矢量构图也让我们回答“国王—男人+女人=？”提问并得出结果“女王”！当你想到所有这些知识仅仅来自于在上下文中查看大量单词，而没有提供关于它们的语义的其他信息时，所有这些都是非常了不起的。更多详情请参考<a class="ae ln" href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h2 id="a5d7" class="ku kv iq bd lo lp lq dn lr ls lt dp lu jy lv lw lx kc ly lz ma kg mb mc md me bi translated"><strong class="ak">手套</strong></h2><p id="281f" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">单词表示的全局向量或<a class="ae ln" href="http://doi.org/10.3115/v1/D14-1162" rel="noopener ugc nofollow" target="_blank"> GloVe </a>算法是对 word2vec 方法的扩展，用于有效地学习单词向量。GloVe 使用跨整个文本语料库的统计来构建显式的单词上下文或单词共现矩阵。结果是一个学习模型，它通常可以产生更好的单词嵌入。</p><p id="203e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑下面的例子:</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi nk"><img src="../Images/449fcae3e6462210513d687489904228.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dxuG0YGiwVloJ7fWPAx1ww@2x.png"/></div></div></figure><p id="4caa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nf">目标词:</em>冰、<br/>蒸汽、<em class="nf">探测词:</em>固体、气体、水、时尚</p><p id="5216" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设<em class="nf"> P(k|w) </em>为单词<em class="nf"> k </em>在单词<em class="nf"> w </em>的上下文中出现的概率。考虑一个与<em class="nf">冰</em>强相关，而与<em class="nf">汽</em>不相关的词，比如<em class="nf">固</em>。<em class="nf"> P(固|冰)</em>会比较高，<em class="nf"> P(固|汽)</em>会比较低。因此<em class="nf"> P(固体|冰)/ P(固体|蒸汽)</em>的比值将会很大。如果我们取一个与<em class="nf">蒸汽</em>相关而与<em class="nf">冰</em>不相关的词，比如<em class="nf">气体</em>，那么<em class="nf"> P(气体|冰)/ P(气体|蒸汽)</em>的比值反而会很小。对于一个既与<em class="nf">冰</em>又与<em class="nf">蒸汽</em>相关的词，比如<em class="nf">水</em>我们预计比率接近 1。更多详情请参考此处的<a class="ae ln" href="https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/" rel="noopener ugc nofollow" target="_blank">和</a>。</p><blockquote class="nc nd ne"><p id="c375" class="jn jo nf jp b jq jr js jt ju jv jw jx ng jz ka kb nh kd ke kf ni kh ki kj kk ij bi translated">单词嵌入将每个单词编码到一个向量中，该向量捕获文本语料库中单词之间的某种关系和相似性。这意味着甚至像大小写、拼写、标点符号等单词的变化都会被自动学习。反过来，这可能意味着可能不再需要上述的一些文本清理步骤。</p></blockquote><h2 id="774c" class="ku kv iq bd lo lp lq dn lr ls lt dp lu jy lv lw lx kc ly lz ma kg mb mc md me bi translated">步骤 3:选择 ML 算法</h2><p id="6332" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">根据问题空间和可用数据，有多种方法为各种基于文本的应用程序构建 ML 模型。</p><p id="9e6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用于垃圾邮件过滤的经典 ML 方法，如“朴素贝叶斯”或“支持向量机”已被广泛使用。深度学习技术正在为情感分析和语言翻译等 NLP 问题提供更好的结果。深度学习模型的训练非常慢，并且已经看到，对于简单的文本分类问题，经典的 ML 方法也以更快的训练时间给出类似的结果。</p><p id="34fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们使用到目前为止讨论的技术，在<a class="ae ln" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank"> IMDB </a>电影评论数据集上构建一个<strong class="jp ir">情感分析器</strong>。</p><h2 id="be4a" class="ku kv iq bd lo lp lq dn lr ls lt dp lu jy lv lw lx kc ly lz ma kg mb mc md me bi translated">下载 IMDb 电影评论数据</h2><p id="db5f" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">IMDB 电影评论集可以从<a class="ae ln" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">这里</a>下载。这个用于二元情感分类的数据集包含用于训练的 25，000 条高度极性的电影评论，以及用于测试的 25，000 条评论。这个数据集被用于非常受欢迎的论文<a class="ae ln" href="http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf" rel="noopener ugc nofollow" target="_blank">‘学习用于情感分析的词向量’</a>。</p><h2 id="7489" class="ku kv iq bd lo lp lq dn lr ls lt dp lu jy lv lw lx kc ly lz ma kg mb mc md me bi translated">预处理</h2><p id="f377" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">数据集被结构化为测试集和训练集，每个都有 25000 个文件。让我们首先将文件读入 python 数据帧，以便进一步处理和可视化。测试和训练集被进一步分成 12500 个“正面”和“负面”评论。我们阅读每个文件，并将负面评价标记为“0 ”,正面评价标记为“1”</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="74e3" class="ku kv iq kq b gy kw kx l ky kz">#convert the dataset from files to a python DataFrame</span><span id="4572" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">import pandas as pd<br/>import os</strong></span><span id="38cb" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">folder = 'aclImdb'</strong></span><span id="be0f" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">labels = {'pos': 1, 'neg': 0}</strong></span><span id="f56d" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">df = pd.DataFrame()</strong></span><span id="76f5" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">for f in ('test', 'train'):    <br/>    for l in ('pos', 'neg'):<br/>        path = os.path.join(folder, f, l)<br/>        for file in os.listdir (path) :<br/>            with open(os.path.join(path, file),'r', encoding='utf-8') as infile:<br/>                txt = infile.read()<br/>            df = df.append([[txt, labels[l]]],ignore_index=True)</strong></span><span id="6bed" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">df.columns = ['review', 'sentiment']</strong></span></pre><p id="3d34" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们将收集的数据保存为。csv 文件供进一步使用。</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/5536b3eafaf4630f4648c3ff3209870f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*IIIRW2kWF5a7qGjZObiJ3Q.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Five reviews and the corresponding sentiment</figcaption></figure><p id="9fb4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要获得文本中单词的频率分布，我们可以利用<code class="fe nm nn no kq b">nltk.FreqDist()</code>函数，该函数列出了文本中使用最多的单词，提供了文本数据中主要主题的大致概念，如以下代码所示:</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="6bb3" class="ku kv iq kq b gy kw kx l ky kz"><strong class="kq ir">import nltk<br/>from nltk.tokenize import word_tokenize</strong></span><span id="a872" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">reviews = df.review.str.cat(sep=' ')</strong></span><span id="d80c" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">#function to split text into word<br/>tokens = word_tokenize(reviews)</strong></span><span id="9307" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">vocabulary = set(tokens)<br/>print(len(vocabulary))</strong></span><span id="36c7" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">frequency_dist = nltk.FreqDist(tokens)<br/>sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:50]</strong></span></pre><p id="1fe0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这给出了文本中使用的前 50 个单词，尽管很明显，一些停用词，如<code class="fe nm nn no kq b">the</code>，在英语中频繁出现。</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi np"><img src="../Images/861b92a8585bba6db3df92055422af97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YLmyQRtNroTvNiOBAOZQeg.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Top 50 words</figcaption></figure><p id="36b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">仔细看，你会发现许多不必要的标点符号和标签。通过排除单个和两个字母的单词，像<code class="fe nm nn no kq b">the</code>、<code class="fe nm nn no kq b">this</code>、<code class="fe nm nn no kq b">and</code>、<code class="fe nm nn no kq b">that</code>这样的停用词在下面所示的词频分布图中占据顶部位置。</p><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/f7b836839e70e2cc0eb5569fffd866a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*8Gnd17LJBux4fLz1uiJzXQ.png"/></div></figure><p id="8f77" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们删除停用词，以进一步清理文本语料库。</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="50b1" class="ku kv iq kq b gy kw kx l ky kz"><strong class="kq ir">from nltk.corpus import stopwords</strong></span><span id="8ea8" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">stop_words = set(stopwords.words('english'))<br/>tokens = [w for w in tokens if not w in stop_words]</strong></span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi nr"><img src="../Images/bba61958876fadca9f8015505b69d497.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0O-nozZ2csh8doqiGEZhlQ.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Top 50 words</figcaption></figure><p id="b9a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这看起来像一个干净的文本语料库，像<code class="fe nm nn no kq b">went</code>、<code class="fe nm nn no kq b">saw</code>、<code class="fe nm nn no kq b">movie </code>等等。如预期的那样占据了前几名。</p><p id="096c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一个有用的可视化工具<code class="fe nm nn no kq b">wordcloud</code>包通过在画布上随机放置单词来帮助创建单词云，大小与它们在文本中的频率成比例。</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="0143" class="ku kv iq kq b gy kw kx l ky kz"><strong class="kq ir">from wordcloud import WordCloud<br/>import matplotlib.pyplot as plt</strong></span><span id="0ba5" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">wordcloud = WordCloud().<br/>generate_from_frequencies(frequency_dist)</strong></span><span id="4793" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">plt.imshow(wordcloud)<br/>plt.axis("off")<br/>plt.show()</strong></span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b9257046ae9070b93a8035fbe0af2de7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*xHKuQdJrg24oMfad8PyyKg.png"/></div></figure><h2 id="11b2" class="ku kv iq bd lo lp lq dn lr ls lt dp lu jy lv lw lx kc ly lz ma kg mb mc md me bi translated">构建分类器</h2><p id="1407" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">在清理之后，是时候构建分类器来识别每个电影评论的情感。从 IMDb 数据集中，划分 25000 个测试集和训练集:</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="1f65" class="ku kv iq kq b gy kw kx l ky kz">X_train = df.loc[:24999, 'review'].values<br/>y_train = df.loc[:24999, 'sentiment'].values<br/>X_test = df.loc[25000:, 'review'].values<br/>y_test = df.loc[25000:, 'sentiment'].values</span></pre><p id="9e76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe nm nn no kq b"><strong class="jp ir">scikit-learn</strong></code>提供一些很酷的工具对文本做预处理。我们使用<code class="fe nm nn no kq b">TfidTransformer</code>将文本语料库转换为特征向量，我们将最大特征限制为 10000 个。有关如何使用<code class="fe nm nn no kq b">TfidTransformer</code>的更多详情，请参考此处的<a class="ae ln" href="http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html" rel="noopener ugc nofollow" target="_blank"/>。</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="99e9" class="ku kv iq kq b gy kw kx l ky kz"><strong class="kq ir">from sklearn.feature_extraction.text import TfidfTransformer<br/>from sklearn.feature_extraction.text import TfidfVectorizer</strong></span><span id="a67a" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">vectorizer = TfidfVectorizer()<br/>train_vectors = vectorizer.fit_transform(X_train)<br/>test_vectors = vectorizer.transform(X_test)<br/>print(train_vectors.shape, test_vectors.shape)</strong></span></pre><figure class="kl km kn ko gt lc gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/c082cb4397fd7df0cecab3d6dfec9fc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*9gGscyg7dKpjabT78TUqRQ.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Training and Test set: 25K with 10K Features</figcaption></figure><p id="ada3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有许多算法可供选择，我们将使用一个基本的朴素贝叶斯分类器，并在训练集上训练模型。</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="c5e7" class="ku kv iq kq b gy kw kx l ky kz"><strong class="kq ir">from sklearn.naive_bayes import MultinomialNB</strong></span><span id="e919" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">clf = MultinomialNB().fit(train_vectors, y_train)</strong></span></pre><p id="f25a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的情感分析仪已经准备好并接受过训练。现在，让我们在测试集上测试我们的模型的性能，以预测情感标签。</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="ca72" class="ku kv iq kq b gy kw kx l ky kz"><strong class="kq ir">from  sklearn.metrics  import accuracy_score</strong></span><span id="f104" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">predicted = clf.predict(test_vectors)</strong></span><span id="d34c" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir">print(accuracy_score(y_test,predicted))</strong></span><span id="effd" class="ku kv iq kq b gy la kx l ky kz"><strong class="kq ir"><em class="nf">Output 0.791</em></strong></span></pre><p id="ef7a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">哇！！！基于基本 NB 分类器的情感分析器做得很好，给出大约 79%的准确度。您可以尝试改变特征向量长度和改变<code class="fe nm nn no kq b">TfidTransformer</code>的参数，以查看对模型精度的影响。</p><p id="863c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">结论:</strong>我们已经详细讨论了 NLP 中使用的文本处理技术。我们还演示了使用文本处理和构建情感分析器，用经典的 ML 方法取得了相当好的效果。</p><p id="4b69" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读本文，喜欢就推荐分享。</p><h2 id="060e" class="ku kv iq bd lo lp lq dn lr ls lt dp lu jy lv lw lx kc ly lz ma kg mb mc md me bi translated">延伸阅读:</h2><div class="nu nv gp gr nw nx"><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd ir gy z fp oc fr fs od fu fw ip bi translated">对单词嵌入的直观理解:从计数向量到 Word2Vec</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">在我们开始之前，看看下面的例子。你打开谷歌搜索一篇关于……</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">www.analyticsvidhya.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol lh nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a href="https://machinelearningmastery.com/what-are-word-embeddings/" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd ir gy z fp oc fr fs od fu fw ip bi translated">什么是文本的单词嵌入？</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">词嵌入是一种词的表征类型，它允许具有相似意义的词具有相似的…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">machinelearningmastery.com</p></div></div><div class="og l"><div class="om l oi oj ok og ol lh nx"/></div></div></a></div></div></div>    
</body>
</html>