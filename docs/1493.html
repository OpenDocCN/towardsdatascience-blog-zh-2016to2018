<html>
<head>
<title>Scikit-Learn for Text Analysis of Amazon Fine Food Reviews</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">sci kit-了解亚马逊美食评论的文本分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scikit-learn-for-text-analysis-of-amazon-fine-food-reviews-ea3b232c2c1b?source=collection_archive---------1-----------------------#2017-09-11">https://towardsdatascience.com/scikit-learn-for-text-analysis-of-amazon-fine-food-reviews-ea3b232c2c1b?source=collection_archive---------1-----------------------#2017-09-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/507e56126e5f45c988e3cae7271520e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZATJc-tE50HZ7y-wRher8w.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo credit: Pixabay</figcaption></figure><p id="f392" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">(本文首发于<a class="ae la" href="https://datascienceplus.com/scikit-learn-for-text-analysis-of-amazon-fine-food-reviews/" rel="noopener ugc nofollow" target="_blank"> DataScience+ </a>)</p><p id="ed1a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们知道<a class="ae la" href="https://www.entrepreneur.com/article/253361#" rel="noopener ugc nofollow" target="_blank">亚马逊产品评论对商家很重要</a>，因为这些评论对我们如何做出购买决定有着巨大的影响。所以，我从<a class="ae la" href="https://www.kaggle.com/snap/amazon-fine-food-reviews" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载了一个亚马逊美食评论数据集，这个数据集最初来自<a class="ae la" href="http://snap.stanford.edu/data/web-FineFoods.html" rel="noopener ugc nofollow" target="_blank"> SNAP </a>，看看我能从这个庞大的数据集里学到什么。</p><p id="2edb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们在这里的目的不是掌握 Scikit-Learn，而是探索单个 csv 文件上的一些主要 Scikit-Learn 工具:通过分析截至 2012 年 10 月(包括 10 月)的一组文本文档(568，454 篇食品评论)。让我们开始吧。</p><h2 id="54f9" class="lb lc iq bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">数据</h2><p id="24a5" class="pw-post-body-paragraph kc kd iq ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">查看数据帧的头部，我们可以看到它包含以下信息:</p><ol class=""><li id="9e6a" class="lz ma iq ke b kf kg kj kk kn mb kr mc kv md kz me mf mg mh bi translated">产品 Id</li><li id="db17" class="lz ma iq ke b kf mi kj mj kn mk kr ml kv mm kz me mf mg mh bi translated">用户标识</li><li id="3f1d" class="lz ma iq ke b kf mi kj mj kn mk kr ml kv mm kz me mf mg mh bi translated">ProfileName</li><li id="13dd" class="lz ma iq ke b kf mi kj mj kn mk kr ml kv mm kz me mf mg mh bi translated">帮助分子</li><li id="ee11" class="lz ma iq ke b kf mi kj mj kn mk kr ml kv mm kz me mf mg mh bi translated">有用性分母</li><li id="0001" class="lz ma iq ke b kf mi kj mj kn mk kr ml kv mm kz me mf mg mh bi translated">得分</li><li id="0555" class="lz ma iq ke b kf mi kj mj kn mk kr ml kv mm kz me mf mg mh bi translated">时间</li><li id="07f9" class="lz ma iq ke b kf mi kj mj kn mk kr ml kv mm kz me mf mg mh bi translated">摘要</li><li id="1239" class="lz ma iq ke b kf mi kj mj kn mk kr ml kv mm kz me mf mg mh bi translated">文本</li></ol><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="f572" class="lb lc iq ms b gy mw mx l my mz">import pandas as pd<br/>import numpy as np<br/><br/>df = pd.read_csv('Reviews.csv')<br/>df.head()</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi na"><img src="../Images/74a8122726a129617d7a71ba8bbe32c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oJeBobNnlD4He5EZE5-5Yg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1</figcaption></figure><p id="c2da" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">就我们今天的目的而言，我们将重点关注分数和文本列。</p><p id="d146" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们从清理数据框开始，删除任何缺少值的行。</p><p id="5ab6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">分数列的范围是从 1 到 5，我们将删除所有等于 3 的分数，因为我们假设这些分数是中性的，没有为我们提供任何有用的信息。然后，我们添加了一个名为“积极性”的新列，其中任何高于 3 的分数都被编码为 1，表明它得到了积极的评价。否则，它将被编码为 0，表明它是负面评价。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="a95d" class="lb lc iq ms b gy mw mx l my mz">df.dropna(inplace=True)<br/>df[df['Score'] != 3]<br/>df['Positivity'] = np.where(df['Score'] &gt; 3, 1, 0)<br/>df.head()</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/d5f5fe951adc009e6c9bcea01fc14a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QVIZrUJT7uvwZO6WNNnjOA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2</figcaption></figure><p id="eff3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">看起来不错。</p><p id="3dbb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在，让我们使用“Text”和“Positivity”列将数据分成随机的训练和测试子集，然后打印出第一个条目和训练集的形状。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="6668" class="lb lc iq ms b gy mw mx l my mz">from sklearn.model_selection import train_test_split</span><span id="fa44" class="lb lc iq ms b gy nc mx l my mz">X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Positivity'], random_state = 0)</span><span id="8d1d" class="lb lc iq ms b gy nc mx l my mz">print('X_train first entry: \n\n', X_train[0])<br/>print('\n\nX_train shape: ', X_train.shape)</span><span id="0034" class="lb lc iq ms b gy nc mx l my mz">from sklearn.model_selection import train_test_split</span><span id="3b88" class="lb lc iq ms b gy nc mx l my mz">X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Positivity'], random_state = 0)</span><span id="d0d3" class="lb lc iq ms b gy nc mx l my mz">print('X_train first entry: \n\n', X_train[0])<br/>print('\n\nX_train shape: ', X_train.shape)</span></pre><p id="5198" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nd"> X_train 第一条目:</em> </strong></p><p id="34f2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nd">我买过几款活力狗粮罐头，发现质量都很好。这种产品看起来更像炖肉，而不是加工过的肉，而且闻起来更香。我的拉布拉多是挑剔的，她比大多数人更欣赏这个产品。</em>T15】</strong></p><p id="6a4a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nd"> X_train 形状:(26377，)</em> </strong></p><p id="f4b3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">查看 X_train，我们可以看到我们收集了超过 26000 条评论或文档。为了对文本文档执行机器学习，我们首先需要将这些文本内容转换为 Scikit-Learn 可以使用的数字特征向量。</p><h2 id="04e9" class="lb lc iq bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">词汇袋</h2><p id="e25d" class="pw-post-body-paragraph kc kd iq ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">最简单和最直观的方法是“单词袋”表示法，它忽略了结构，只计算每个单词出现的频率。CountVectorizer 允许我们使用单词袋方法，将一组文本文档转换成一个令牌计数矩阵。</p><p id="84d9" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们实例化 CountVectorizer 并使其适合我们的训练数据，将我们的文本文档集合转换为令牌计数矩阵。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="3a1e" class="lb lc iq ms b gy mw mx l my mz">from sklearn.feature_extraction.text import CountVectorizer</span><span id="1bc6" class="lb lc iq ms b gy nc mx l my mz">vect = CountVectorizer().fit(X_train)<br/>vect</span></pre><p id="adb5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="nd">count vectorizer(analyzer = ' word '，binary=False，decode_error='strict '，dtype =&lt;class ' numpy . int 64 '&gt;，encoding='utf-8 '，input='content '，lowercase=True，max_df=1.0，max_features=None，min_df=1，ngram_range=(1，1)，preprocessor=None，stop_words=None，strip_accents=None，token_pattern= '(？u)\\b\\w\\w+\\b '，tokenizer=None，vocabulary = None)</em>T3】</strong></p><p id="d410" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这个模型有许多参数，但是默认值对于我们的目的来说是相当合理的。</p><p id="6703" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">默认配置通过提取至少两个字母或数字的单词(由单词边界分隔)来标记字符串，然后将所有内容转换为小写，并使用这些标记构建词汇表。我们可以通过使用<em class="nd"> get_feature_names </em>方法来获取一些词汇表，如下所示:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="b0d6" class="lb lc iq ms b gy mw mx l my mz">vect.get_feature_names()[::2000]</span></pre><p id="3215" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nd"> ['00 '，' anyonr '，'漂白'，'矮胖'，'战败'，' er '，'贾尼尼'，'印象'，'小'，'项链'，'宠物'，'缩减'，'衬衫'，'夜宵'] </em> </strong></p><p id="312c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">看着这些词汇，我们可以对它们的内容有一个小小的了解。通过检查<em class="nd"> get_feature_names </em>的长度，我们可以看到我们正在处理 29990 个特性。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="b64e" class="lb lc iq ms b gy mw mx l my mz">len(vect.get_feature_names())</span></pre><p id="8044" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nd"> 29990 </em> </strong></p><p id="10dc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">接下来，我们将 X_train 中的文档转换成一个文档术语矩阵，它给出了 X_train 的单词包表示。结果存储在一个 SciPy 稀疏矩阵中，其中每一行对应一个文档，每一列是来自我们训练词汇表的一个单词。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="78c3" class="lb lc iq ms b gy mw mx l my mz">X_train_vectorized = vect.transform(X_train)<br/>X_train_vectorized</span></pre><p id="25a6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nd"> &lt; 26377x29990 以压缩稀疏行格式存储了 1406227 个元素的“&lt;类“numpy . int 64”&gt;”稀疏矩阵&gt; </em> </strong></p><p id="05d7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">列的这种解释可以按如下方式检索:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="22c9" class="lb lc iq ms b gy mw mx l my mz">X_train_vectorized.toarray()<br/>array([[0, 0, 0, ..., 0, 0, 0],<br/>       [0, 0, 0, ..., 0, 0, 0],<br/>       [0, 0, 0, ..., 0, 0, 0],<br/>       ..., <br/>       [0, 0, 0, ..., 0, 0, 0],<br/>       [0, 0, 0, ..., 0, 0, 0],<br/>       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)</span></pre><p id="7e05" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">该矩阵中的条目是每个单词在每个文档中出现的次数。因为词汇表中的单词数量比单个文本中可能出现的单词数量大得多，所以这个矩阵的大多数条目都是零。</p><h2 id="a0c1" class="lb lc iq bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">逻辑回归</h2><p id="b3b5" class="pw-post-body-paragraph kc kd iq ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">现在，我们将基于这个特征矩阵<em class="nd"> X_ train_ vectorized，</em>来训练逻辑回归分类器，因为逻辑回归对于高维稀疏数据工作得很好。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="9dce" class="lb lc iq ms b gy mw mx l my mz">from sklearn.linear_model import LogisticRegression</span><span id="82c1" class="lb lc iq ms b gy nc mx l my mz">model = LogisticRegression()<br/>model.fit(X_train_vectorized, y_train)</span></pre><p id="991d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="nd"> LogisticRegression(C=1.0，class_weight=None，dual=False，fit_intercept=True，intercept_scaling=1，max_iter=100，multi_class='ovr '，n_jobs=1，penalty='l2 '，random_state=None，solver='liblinear '，tol=0.0001，verbose=0，warm_start=False) </em></p><p id="c922" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">接下来，我们将使用 X_test 进行预测，并计算曲线得分下的面积。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="7d19" class="lb lc iq ms b gy mw mx l my mz">from sklearn.metrics import roc_auc_score</span><span id="ec80" class="lb lc iq ms b gy nc mx l my mz">predictions = model.predict(vect.transform(X_test))</span><span id="63c9" class="lb lc iq ms b gy nc mx l my mz">print('AUC: ', roc_auc_score(y_test, predictions))</span></pre><p id="4bb9" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="nd">AUC:0.797745838184</em></strong></p><p id="959a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">成绩还不错。为了更好地理解我们的模型是如何做出这些预测的，我们可以使用每个特征(一个词)的系数来确定它在积极和消极方面的权重。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="5f2e" class="lb lc iq ms b gy mw mx l my mz">feature_names = np.array(vect.get_feature_names())</span><span id="f041" class="lb lc iq ms b gy nc mx l my mz">sorted_coef_index = model.coef_[0].argsort()</span><span id="2241" class="lb lc iq ms b gy nc mx l my mz">print('Smallest Coefs: <strong class="ms ir">\n{}\n</strong>'.format(feature_names[sorted_coef_index[:10]]))<br/>print('Largest Coefs: <strong class="ms ir">\n{}\n</strong>'.format(feature_names[sorted_coef_index[:-11:-1]]))</span></pre><p id="a412" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nd">最小系数:['最差' '失望' '可怕' '糟糕' '还行' '都不是' '耻辱' '不幸' '失望' '恶心'] </em> </strong></p><p id="e4e0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nd">最大系数:['上钩' '光明' '美味' '惊艳' '怀疑' '担忧' '好吃' '极好' '再订购' '好吃'] </em> </strong></p><p id="9392" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对十个最小和十个最大的系数进行排序，我们可以看到该模型预测了像“最差”、“令人失望”和“可怕”这样的负面评论，以及像“着迷”、“明亮”和“美味”这样的正面评论。</p><p id="0457" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">但是，我们的模型可以改进。</p><h2 id="f1e6" class="lb lc iq bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">TF–IDF 术语权重</h2><p id="f294" class="pw-post-body-paragraph kc kd iq ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">在大型文本语料库中，一些单词会经常出现，但很少携带关于文档实际内容的有意义的信息(例如“the”、“a”和“is”)。如果我们将计数数据直接输入到分类器中，那些非常频繁的词将会掩盖那些更罕见但更有趣的词的频率。Tf-idf 允许我们根据术语对文档的重要性来衡量它们。</p><p id="a841" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因此，我们将实例化 TF–IDF 矢量器，并使其适合我们的训练数据。我们指定<em class="nd"> min_df </em> = 5，这将从我们的词汇表中删除出现在少于五个文档中的任何单词。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="658b" class="lb lc iq ms b gy mw mx l my mz">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="33a6" class="lb lc iq ms b gy nc mx l my mz">vect = TfidfVectorizer(min_df = 5).fit(X_train)<br/>len(vect.get_feature_names())</span></pre><p id="cd55" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nd"> 9680 </em> </strong></p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="5b51" class="lb lc iq ms b gy mw mx l my mz">X_train_vectorized = vect.transform(X_train)</span><span id="f50f" class="lb lc iq ms b gy nc mx l my mz">model = LogisticRegression()<br/>model.fit(X_train_vectorized, y_train)<br/>predictions = model.predict(vect.transform(X_test))<br/>print('AUC: ', roc_auc_score(y_test, predictions))</span></pre><p id="daf5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="nd">AUC:0.759768072872</em></strong></p><p id="fc48" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因此，尽管我们能够将特征的数量从 29990 减少到仅仅 9680，我们的 AUC 分数下降了几乎 4%。</p><p id="824a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">使用下面的代码，我们能够获得一个具有最小 tf-idf 的功能列表，这些功能通常出现在所有评论中，或者很少出现在很长的评论中，以及一个具有最大 TF-IDF 的功能列表，这些功能包含经常出现在评论中，但通常不会出现在所有评论中的单词。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="0c61" class="lb lc iq ms b gy mw mx l my mz">feature_names = np.array(vect.get_feature_names())</span><span id="2f82" class="lb lc iq ms b gy nc mx l my mz">sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()</span><span id="0374" class="lb lc iq ms b gy nc mx l my mz">print('Smallest Tfidf: \n{}\n'.format(feature_names[sorted_tfidf_index[:10]]))<br/>print('Largest Tfidf: \n{}\n'.format(feature_names[sorted_tfidf_index[:-11:-1]]))</span></pre><p id="9b29" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nd">最小 tfi df:[' blazin ' ' 4 thd ' ' nations ' ' Committee ' ' 300 MGS ' ' 350 MGS ' ' sciences ' ' biochemical ' ' nas ' ' fnb ']</em></strong></p><p id="c482" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nd">最大 Tfidf: ['芥末' ' br ' '挺举' '唐' '辣椒' '伤口' '鱼子酱' '莎莎' '垃圾' ' el'] </em> </strong></p><p id="0398" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们测试我们的模型:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="d33c" class="lb lc iq ms b gy mw mx l my mz">print(model.predict(vect.transform(['The candy is not good, I will never buy them again','The candy is not bad, I will buy them again'])))</span></pre><p id="1a25" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="nd">【1 0】</em></strong></p><p id="9e22" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们当前的模型将“糖果不好吃，我再也不会买它们”的文档错误分类为正面评论，也将“糖果不好吃，我会再买它们”的文档错误分类为负面评论。</p><h2 id="0f4c" class="lb lc iq bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">n-grams</h2><p id="ec91" class="pw-post-body-paragraph kc kd iq ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">解决这种错误分类的一种方法是添加 n 元语法。例如，二元模型计算相邻单词对的数量，并且可以给我们一些特征，比如坏和不错。因此，我们正在重新调整我们的训练集，指定最小文档频率为 5，并提取 1-grams 和 2-grams。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="0cf8" class="lb lc iq ms b gy mw mx l my mz">vect = CountVectorizer(min_df = 5, ngram_range = (1,2)).fit(X_train)<br/>X_train_vectorized = vect.transform(X_train)<br/>len(vect.get_feature_names())</span></pre><p id="d25b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nd"> 61958 </em> </strong></p><p id="43a7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在我们有了更多的功能，但是我们的 AUC 分数增加了:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="2746" class="lb lc iq ms b gy mw mx l my mz">model = LogisticRegression()<br/>model.fit(X_train_vectorized, y_train)</span><span id="624b" class="lb lc iq ms b gy nc mx l my mz">predictions = model.predict(vect.transform(X_test))<br/>print('AUC: ', roc_auc_score(y_test, predictions))</span></pre><p id="56ae" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">T5 AUC:0.838772959029T7】</strong></p><p id="0d7d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">使用系数来检查每个特征，我们可以看到</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="e131" class="lb lc iq ms b gy mw mx l my mz">feature_names = np.array(vect.get_feature_names())<br/>sorted_coef_index = model.coef_[0].argsort()</span><span id="fe5e" class="lb lc iq ms b gy nc mx l my mz">print('Smallest Coef: \n{}\n'.format(feature_names[sorted_coef_index][:10]))<br/>print('Largest Coef: \n{}\n'.format(feature_names[sorted_coef_index][:-11:-1]))</span></pre><p id="9186" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nd">最小系数:['最差' '还行' '不推荐' '不值得' '糟糕' '最好' '不幸' '糟糕' '非常失望' '失望'] </em> </strong></p><p id="f255" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nd">最大系数:['好吃' '惊艳' '不太' '优秀' '失望' '不苦' '好吃' '上瘾' '很棒' '喜欢这个]]</em></strong></p><p id="451d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们的新模型已经正确地预测了负面评论的“不推荐”、“不值得”等二元模型，以及正面评论的“不苦”、“不太”等二元模型。</p><p id="a427" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们测试我们的新模型:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="76bc" class="lb lc iq ms b gy mw mx l my mz">print(model.predict(vect.transform(['The candy is not good, I would never buy them again','The candy is not bad, I will buy them again'])))</span></pre><p id="2a1a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi">[0 1]</p><p id="f687" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们的最新模型现在可以正确地将它们分别识别为负面和正面评论。</p><h2 id="0084" class="lb lc iq bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">自己试试</h2><p id="7fb4" class="pw-post-body-paragraph kc kd iq ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">我希望你喜欢这篇文章，并享受在文本数据上练习机器学习技能的乐趣！请随意留下反馈或问题。</p><p id="05df" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">参考资料:</p><p id="2051" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae la" href="http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a></p></div></div>    
</body>
</html>