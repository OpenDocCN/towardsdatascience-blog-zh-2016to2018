<html>
<head>
<title>Deep Learning and Visual Question Answering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习和视觉问答</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-and-visual-question-answering-c8c8093941bc?source=collection_archive---------1-----------------------#2018-02-13">https://towardsdatascience.com/deep-learning-and-visual-question-answering-c8c8093941bc?source=collection_archive---------1-----------------------#2018-02-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jn jo jp jq"><div class="bz fp l di"><div class="jr js l"/></div></figure><p id="c604" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated"><strong class="jv ir">视觉问答</strong>是关于建立一个计算机系统来回答用图像和自然语言表达的问题的研究领域。首先，我们来考察一下<strong class="jv ir">视觉问答</strong>中的三个数据集。</p><h1 id="29cd" class="kr ks iq bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated"><a class="ae lp" href="http://www.visualqa.org" rel="noopener ugc nofollow" target="_blank"> VQA数据集</a></h1><figure class="lr ls lt lu gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lq"><img src="../Images/b4180a8d9201e4be8637bc24ab27ebfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_rbZ8OXinIS4aQxDFiqIOw.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="bd mf">Figure 1. VQA Dataset from www.visualqa.org</strong></figcaption></figure><p id="b858" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">在来自www.visualqa.org的<strong class="jv ir"> </strong> <a class="ae lp" href="http://www.visualqa.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ir"> VQA数据集中，计算机系统需要解决问题，例如，二进制分类问题(伞是不是颠倒的？)，一个计数问题(床上有几个孩子？)，或者一个开放式的问题(谁戴着眼镜？孩子设定在哪里？)</strong></a></p><h1 id="55b5" class="kr ks iq bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated"><a class="ae lp" href="https://cs.stanford.edu/people/jcjohns/clevr/" rel="noopener ugc nofollow" target="_blank"> CLEVR数据集</a></h1><figure class="lr ls lt lu gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mg"><img src="../Images/6929573ece63afa11c358aed3b93134e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-L4BpsD-NjUkWKLlNYtotg.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="bd mf">Figure 2. CLEVR Dataset from Stanford</strong></figcaption></figure><p id="14cd" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">在斯坦福 的<a class="ae lp" href="https://cs.stanford.edu/people/jcjohns/clevr/" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ir"> CLEVR数据集里，计算机系统需要回答关于物体的形状/颜色/大小/材料，以及其空间/逻辑关系的问题。</strong></a></p><h1 id="d88a" class="kr ks iq bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated"><a class="ae lp" href="https://datasets.maluuba.com/FigureQA" rel="noopener ugc nofollow" target="_blank">图QA数据集</a></h1><figure class="lr ls lt lu gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mh"><img src="../Images/24d56a2886e78f758576d94f12aa6dc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5fbzMzlBmHSNl8d6TIM66A.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="bd mf">Figure 3. FigureQA from Maluuba</strong></figcaption></figure><p id="6602" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">在来自Maluuba  的<a class="ae lp" href="https://datasets.maluuba.com/FigureQA" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ir">图QA数据集中，计算机系统需要回答以条形图、饼状图或线图呈现的问题。</strong></a></p><h1 id="baaf" class="kr ks iq bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">视觉问答和深度学习</h1><p id="b5e7" class="pw-post-body-paragraph jt ju iq jv b jw mi jy jz ka mj kc kd ke mk kg kh ki ml kk kl km mm ko kp kq ij bi translated">因为<strong class="jv ir">视觉问答</strong>需要涉及图像识别和自然语言处理的技术，所以研究的一个主要方向是在<strong class="jv ir">深度学习</strong>上:使用<strong class="jv ir">卷积神经网络(CNN) </strong>进行图像识别，使用<strong class="jv ir">递归神经网络(RNN) </strong>进行自然语言处理，然后将结果组合起来给出最终答案，如图<strong class="jv ir">图4 </strong>所示。</p><figure class="lr ls lt lu gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mn"><img src="../Images/d8c01c085324fa2da2dc2ac73321abd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QbWaFSNaO3GTgjQZOxhdDg.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><a class="ae lp" href="https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering" rel="noopener ugc nofollow" target="_blank"><strong class="bd mf">Figure 4. Combining CNN/RNN for VQA</strong></a></figcaption></figure><p id="c864" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated"><a class="ae lp" href="https://keras.io/getting-started/functional-api-guide/" rel="noopener ugc nofollow" target="_blank"><strong class="jv ir">Keras</strong></a><strong class="jv ir"/>给出了<strong class="jv ir">视觉问答</strong>的通用模型，如图<strong class="jv ir">图5 </strong>所示。</p><figure class="lr ls lt lu gt jq"><div class="bz fp l di"><div class="mo js l"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><a class="ae lp" href="https://keras.io/getting-started/functional-api-guide/" rel="noopener ugc nofollow" target="_blank"><strong class="ak">Figure 5. VQA/CNN/RNN Model from keras.io</strong></a></figcaption></figure><ul class=""><li id="d10c" class="mp mq iq jv b jw jx ka kb ke mr ki ms km mt kq mu mv mw mx bi translated"><strong class="jv ir">第1–4行</strong>:导入Keras</li><li id="222d" class="mp mq iq jv b jw my ka mz ke na ki nb km nc kq mu mv mw mx bi translated"><strong class="jv ir">第6–21行</strong>:实现CNN进行图像识别</li><li id="392f" class="mp mq iq jv b jw my ka mz ke na ki nb km nc kq mu mv mw mx bi translated"><strong class="jv ir">第23–26行</strong>:实现自然语言处理的RNN</li><li id="683f" class="mp mq iq jv b jw my ka mz ke na ki nb km nc kq mu mv mw mx bi translated">综合CNN和RNN的结果，给出最终答案</li></ul><h1 id="d818" class="kr ks iq bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">视觉问答与关系网络</h1><p id="912b" class="pw-post-body-paragraph jt ju iq jv b jw mi jy jz ka mj kc kd ke mk kg kh ki ml kk kl km mm ko kp kq ij bi translated">在<strong class="jv ir">视觉问答</strong>领域，一个有趣且重要的想法是由<strong class="jv ir">deep mind[</strong><a class="ae lp" href="https://arxiv.org/abs/1706.01427" rel="noopener ugc nofollow" target="_blank"><strong class="jv ir">1</strong></a><strong class="jv ir">，</strong><a class="ae lp" href="https://hackernoon.com/deepmind-relational-networks-demystified-b593e408b643" rel="noopener ugc nofollow" target="_blank"><strong class="jv ir">2</strong></a><strong class="jv ir">]</strong>提出的<strong class="jv ir">关系网络</strong>。<strong class="jv ir">关系网</strong>的主要目的是探索图像和问题中呈现的对象之间的空间关系或逻辑关系，如<strong class="jv ir"><em class="nd">“…与<strong class="jv ir">图6 </strong>问题中的</em> </strong>大小相同，以及<strong class="jv ir"><em class="nd">“…在<strong class="jv ir">图7问题中的</strong></em> </strong> <em class="nd"> </em>的左侧。</p><figure class="lr ls lt lu gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi ne"><img src="../Images/5a02b20d752329272f735d48e1b6e387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HSNUIFbcV_d0aywHQ_nsig.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="bd mf">Figure 6. non-relational questions and relational questions in CLEVR Dataset</strong></figcaption></figure><figure class="lr ls lt lu gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nf"><img src="../Images/ea9629f8d065aa85f83660256741f1fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FFRMQbA-w1UMXsA1cgIydg.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="bd mf">Figure 7. the model of relation network</strong></figcaption></figure><p id="f04e" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated"><strong class="jv ir">图7 </strong>示出了视觉问答系统内部的关系网络的架构。注意，关系网络可能在基于<strong class="jv ir">对象到对象的</strong>或基于<strong class="jv ir">特征到特征的</strong>中探索关系。<strong class="jv ir">图8 </strong>显示了Keras/Theano中<strong class="jv ir">特征提取</strong>和<strong class="jv ir">关系提取</strong>的简单实现。</p><figure class="lr ls lt lu gt jq"><div class="bz fp l di"><div class="mo js l"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="ak">Figure 8. a simple implementaion of feature extraction and relation extraction in Keras/Theano</strong></figcaption></figure><h1 id="1653" class="kr ks iq bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">结论</h1><p id="b228" class="pw-post-body-paragraph jt ju iq jv b jw mi jy jz ka mj kc kd ke mk kg kh ki ml kk kl km mm ko kp kq ij bi translated"><strong class="jv ir">视觉问答</strong>是一项有趣的挑战，结合了不同的学科，包括计算机视觉、自然语言理解和深度学习。希望我们能在Medium下看到更多这方面的文章。</p><h1 id="ec2b" class="kr ks iq bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">参考</h1><p id="38c6" class="pw-post-body-paragraph jt ju iq jv b jw mi jy jz ka mj kc kd ke mk kg kh ki ml kk kl km mm ko kp kq ij bi translated">[1] <a class="ae lp" href="http://www.visualqa.org" rel="noopener ugc nofollow" target="_blank"> VQA数据集</a></p><p id="9c1b" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">[2] <a class="ae lp" href="https://cs.stanford.edu/people/jcjohns/clevr/" rel="noopener ugc nofollow" target="_blank"> CLEVR数据集</a></p><p id="4674" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">[3] <a class="ae lp" href="https://datasets.maluuba.com/FigureQA" rel="noopener ugc nofollow" target="_blank">图QA数据集</a></p><p id="e41f" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">[4] <a class="ae lp" href="https://keras.io/getting-started/functional-api-guide/" rel="noopener ugc nofollow" target="_blank">科拉斯VQA模型</a></p><p id="1a5d" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">[5] <a class="ae lp" href="https://deepmind.com/blog/neural-approach-relational-reasoning/" rel="noopener ugc nofollow" target="_blank">来自DeepMind的关系网</a></p><p id="9170" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">[6] <a class="ae lp" href="https://www.eff.org/ai/metrics#Visual-Question-Answering" rel="noopener ugc nofollow" target="_blank">视觉问答的人工智能进度测量</a></p></div></div>    
</body>
</html>