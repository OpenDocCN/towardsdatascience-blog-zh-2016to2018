<html>
<head>
<title>Multi-Class Text Classification with PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于PySpark的多类文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-class-text-classification-with-pyspark-7d78d022ed35?source=collection_archive---------2-----------------------#2018-02-19">https://towardsdatascience.com/multi-class-text-classification-with-pyspark-7d78d022ed35?source=collection_archive---------2-----------------------#2018-02-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/90dfe2d93cf83db118eddecc56094efd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9NSpC8MG5duObcUPGxFwA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo credit: Pixabay</figcaption></figure><p id="7340" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">Apache Spark 在头条新闻和现实世界中的采用率都迅速上升，这主要是因为它处理流媒体数据的能力。由于每天都有如此多的数据需要处理，因此对我们来说，实时传输和分析这些数据变得至关重要。此外，Apache Spark足够快，无需采样就可以执行探索性查询。许多行业专家提供了所有的理由<a class="ae la" href="https://www.infoworld.com/article/3031690/analytics/why-you-should-use-spark-for-machine-learning.html" rel="noopener ugc nofollow" target="_blank">为什么你应该使用Spark进行机器学习</a>？</p><p id="5ab5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">所以，我们现在在这里，使用<a class="ae la" href="https://spark.apache.org/docs/1.1.0/mllib-guide.html" rel="noopener ugc nofollow" target="_blank"> Spark机器学习库</a>来解决一个多类文本分类问题，特别是PySpark。</p><p id="b347" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果你想看看用<a class="ae la" href="http://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a>的实现，请阅读<a class="ae la" href="https://medium.com/@actsusanli/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f" rel="noopener">以前的文章</a>。</p><h1 id="e95f" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据</h1><p id="6c5a" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">我们的任务是将旧金山的犯罪描述分为33个预定义的类别。数据可以从<a class="ae la" href="https://www.kaggle.com/c/sf-crime/data" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载。</p><p id="c524" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">给定一个新的犯罪描述，我们希望将其分配到33个类别中的一个。分类器假设每个新的犯罪描述被分配到一个且仅一个类别。这是多类文本分类问题。</p><ul class=""><li id="1f13" class="me mf iq ke b kf kg kj kk kn mg kr mh kv mi kz mj mk ml mm bi translated"><strong class="ke ir">输入</strong>:描述</li></ul><p id="0a4d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">例如:“被盗汽车”</p><ul class=""><li id="264b" class="me mf iq ke b kf kg kj kk kn mg kr mh kv mi kz mj mk ml mm bi translated"><strong class="ke ir">输出</strong>:类别</li></ul><p id="14e5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">示例:车辆盗窃</p><p id="3e79" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了解决这个问题，我们将在Spark中使用各种特征提取技术以及不同的监督机器学习算法。我们开始吧！</p><h1 id="6401" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据摄取和提取</h1><p id="528e" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">使用<a class="ae la" href="https://github.com/databricks/spark-csv" rel="noopener ugc nofollow" target="_blank"> Spark csv包</a>加载CSV文件非常简单。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="2ebd" class="mw lc iq ms b gy mx my l mz na">from pyspark.sql import SQLContext<br/>from pyspark import SparkContext<br/>sc =SparkContext()<br/>sqlContext = SQLContext(sc)</span><span id="9753" class="mw lc iq ms b gy nb my l mz na">data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('train.csv')</span></pre><p id="89c1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">就是这样！我们已经加载了数据集。我们开始探索吧。</p><p id="fbd8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">删除我们不需要的列，看看前五行:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="07b0" class="mw lc iq ms b gy mx my l mz na">drop_list = ['Dates', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']</span><span id="990c" class="mw lc iq ms b gy nb my l mz na">data = data.select([column for column in data.columns if column not in drop_list])<br/>data.show(5)</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/2c6b58b9649fce01892f7344a674cfe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*EQvpvFbtV6vrExz0fktQjQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1</figcaption></figure><p id="bb98" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对数据应用printSchema()，这将以树格式打印模式:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="55a4" class="mw lc iq ms b gy mx my l mz na">data.printSchema()</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/08a297ae865721bce9d1b005def74666.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*BJ8_gXfjED9L7f1SC_-_rg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2</figcaption></figure><p id="d86c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">20大犯罪类别:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="b2df" class="mw lc iq ms b gy mx my l mz na">from pyspark.sql.functions import col</span><span id="6421" class="mw lc iq ms b gy nb my l mz na">data.groupBy("Category") \<br/>    .count() \<br/>    .orderBy(col("count").desc()) \<br/>    .show()</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/5fe89485d4c35e8c1dbb0f6b6d33643c.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*NXtwh3Wmbi9YItMMGfiD5g.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3</figcaption></figure><p id="5106" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">20大犯罪描述:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="bf49" class="mw lc iq ms b gy mx my l mz na">data.groupBy("Descript") \<br/>    .count() \<br/>    .orderBy(col("count").desc()) \<br/>    .show()</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/44b237553bd820c736e89bb10b89328f.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*VcEvjYOmTxtRhX4Ny-YWxA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4</figcaption></figure><p id="2c67" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">模型管道</strong></p><p id="6045" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae la" href="https://spark.apache.org/docs/2.2.0/ml-pipeline.html" rel="noopener ugc nofollow" target="_blank"> Spark机器学习管道API </a>类似于<a class="ae la" href="http://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a>。我们的渠道包括三个步骤:</p><ol class=""><li id="b700" class="me mf iq ke b kf kg kj kk kn mg kr mh kv mi kz ng mk ml mm bi translated"><code class="fe nh ni nj ms b">regexTokenizer</code>:标记化(使用正则表达式)</li><li id="df22" class="me mf iq ke b kf nk kj nl kn nm kr nn kv no kz ng mk ml mm bi translated"><code class="fe nh ni nj ms b">stopwordsRemover</code>:删除停用词</li><li id="b73a" class="me mf iq ke b kf nk kj nl kn nm kr nn kv no kz ng mk ml mm bi translated"><code class="fe nh ni nj ms b">countVectors</code>:计数向量(“文档术语向量”)</li></ol><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="024e" class="mw lc iq ms b gy mx my l mz na">from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer<br/>from pyspark.ml.classification import LogisticRegression</span><span id="1754" class="mw lc iq ms b gy nb my l mz na"># regular expression tokenizer<br/>regexTokenizer = RegexTokenizer(inputCol="Descript", outputCol="words", pattern="\\W")</span><span id="8983" class="mw lc iq ms b gy nb my l mz na"># stop words<br/>add_stopwords = ["http","https","amp","rt","t","c","the"] </span><span id="7767" class="mw lc iq ms b gy nb my l mz na">stopwordsRemover = StopWordsRemover(inputCol="words", outputCol="filtered").setStopWords(add_stopwords)</span><span id="a699" class="mw lc iq ms b gy nb my l mz na"># bag of words count<br/>countVectors = CountVectorizer(inputCol="filtered", outputCol="features", vocabSize=10000, minDF=5)</span></pre><p id="c03a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> StringIndexer </strong></p><p id="f7f7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><code class="fe nh ni nj ms b">StringIndexer</code>将一列标签编码成一列标签索引。索引在<code class="fe nh ni nj ms b">[0, numLabels)</code>中，按照标签频率排序，因此最频繁的标签得到索引<code class="fe nh ni nj ms b">0</code>。</p><p id="403d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在我们的例子中，标签列(类别)将被编码为标签索引，从0到32；最频繁的标签(盗窃/偷窃)将被索引为0。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="3325" class="mw lc iq ms b gy mx my l mz na">from pyspark.ml import Pipeline<br/>from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler<br/>label_stringIdx = StringIndexer(inputCol = "Category", outputCol = "label")</span><span id="8fc8" class="mw lc iq ms b gy nb my l mz na">pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])</span><span id="3443" class="mw lc iq ms b gy nb my l mz na"># Fit the pipeline to training documents.<br/>pipelineFit = pipeline.fit(data)<br/>dataset = pipelineFit.transform(data)<br/>dataset.show(5)</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/5118fee155666f383b36f85d46e65ee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UMBgLh6tlHISmmVZZInIgg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 5</figcaption></figure><p id="cb50" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">分区训练&amp;测试集</strong></p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="9c02" class="mw lc iq ms b gy mx my l mz na"># set seed for reproducibility<br/>(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)<br/>print("Training Dataset Count: " + str(trainingData.count()))<br/>print("Test Dataset Count: " + str(testData.count()))</span></pre><p id="fb2c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nq">训练数据集计数:5185 </em> </strong> <br/> <strong class="ke ir"> <em class="nq">测试数据集计数:2104 </em> </strong></p><h1 id="eac1" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">模型训练和评估</h1><p id="8d63" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated"><strong class="ke ir">使用计数向量特征的逻辑回归</strong></p><p id="fd6b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们的模型将在测试集上进行预测和评分；然后，我们从最高概率的角度来看前10个预测。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="a7a5" class="mw lc iq ms b gy mx my l mz na">lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)<br/>lrModel = lr.fit(trainingData)</span><span id="a310" class="mw lc iq ms b gy nb my l mz na">predictions = lrModel.transform(testData)</span><span id="89b2" class="mw lc iq ms b gy nb my l mz na">predictions.filter(predictions['prediction'] == 0) \<br/>    .select("Descript","Category","probability","label","prediction") \<br/>    .orderBy("probability", ascending=False) \<br/>    .show(n = 10, truncate = 30)</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/55cd881093f08e4928f23c2211625237.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KLmO69jHvYgJqAYL2ygk1Q.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 6</figcaption></figure><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="428a" class="mw lc iq ms b gy mx my l mz na">from pyspark.ml.evaluation import MulticlassClassificationEvaluator<br/>evaluator = MulticlassClassificationEvaluator(predictionCol="prediction")<br/>evaluator.evaluate(predictions)</span></pre><p id="5cbf" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="nq">0.9610787444388802</em></strong></p><p id="fd6c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">准确度极好！</p><p id="8bdd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">使用TF-IDF特征的逻辑回归</strong></p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="5462" class="mw lc iq ms b gy mx my l mz na">from pyspark.ml.feature import HashingTF, IDF</span><span id="878d" class="mw lc iq ms b gy nb my l mz na">hashingTF = HashingTF(inputCol="filtered", outputCol="rawFeatures", numFeatures=10000)<br/>idf = IDF(inputCol="rawFeatures", outputCol="features", minDocFreq=5) #minDocFreq: remove sparse terms<br/>pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])</span><span id="0cb0" class="mw lc iq ms b gy nb my l mz na">pipelineFit = pipeline.fit(data)<br/>dataset = pipelineFit.transform(data)</span><span id="1455" class="mw lc iq ms b gy nb my l mz na">(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)<br/>lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)<br/>lrModel = lr.fit(trainingData)</span><span id="586b" class="mw lc iq ms b gy nb my l mz na">predictions = lrModel.transform(testData)</span><span id="7150" class="mw lc iq ms b gy nb my l mz na">predictions.filter(predictions['prediction'] == 0) \<br/>    .select("Descript","Category","probability","label","prediction") \<br/>    .orderBy("probability", ascending=False) \<br/>    .show(n = 10, truncate = 30)</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/c2cc9a0a5ba9732db5d96d81343af3d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-oymKpVrgZQlw_CeKLvyDg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 7</figcaption></figure><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="8a28" class="mw lc iq ms b gy mx my l mz na">evaluator = MulticlassClassificationEvaluator(predictionCol="prediction")<br/>evaluator.evaluate(predictions)</span></pre><p id="a718" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">T32】0.9616202660247297T34】</strong></p><p id="299f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">结果是一样的。</p><p id="1b21" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">交叉验证</strong></p><p id="cc26" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在让我们尝试交叉验证来调整我们的超参数，我们将只调整计数向量逻辑回归。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="9fcc" class="mw lc iq ms b gy mx my l mz na">pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])</span><span id="7d83" class="mw lc iq ms b gy nb my l mz na">pipelineFit = pipeline.fit(data)<br/>dataset = pipelineFit.transform(data)<br/>(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)</span><span id="bb5c" class="mw lc iq ms b gy nb my l mz na">lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)</span><span id="8a0d" class="mw lc iq ms b gy nb my l mz na">from pyspark.ml.tuning import ParamGridBuilder, CrossValidator</span><span id="7cee" class="mw lc iq ms b gy nb my l mz na"># Create ParamGrid for Cross Validation<br/>paramGrid = (ParamGridBuilder()<br/>             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter<br/>             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)<br/>#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations<br/>#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features<br/>             .build())</span><span id="29cc" class="mw lc iq ms b gy nb my l mz na"># Create 5-fold CrossValidator<br/>cv = CrossValidator(estimator=lr, \<br/>                    estimatorParamMaps=paramGrid, \<br/>                    evaluator=evaluator, \<br/>                    numFolds=5)</span><span id="9fbf" class="mw lc iq ms b gy nb my l mz na">cvModel = cv.fit(trainingData)<br/><br/>predictions = cvModel.transform(testData)<br/># Evaluate best model<br/>evaluator = MulticlassClassificationEvaluator(predictionCol="prediction")<br/>evaluator.evaluate(predictions)</span></pre><p id="8cf5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">0.9851796929217101T40】</strong></p><p id="2360" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">性能提高了。</p><p id="1970" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">朴素贝叶斯</strong></p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="d39d" class="mw lc iq ms b gy mx my l mz na">from pyspark.ml.classification import NaiveBayes</span><span id="cdb8" class="mw lc iq ms b gy nb my l mz na">nb = NaiveBayes(smoothing=1)<br/>model = nb.fit(trainingData)</span><span id="0828" class="mw lc iq ms b gy nb my l mz na">predictions = model.transform(testData)<br/>predictions.filter(predictions['prediction'] == 0) \<br/>    .select("Descript","Category","probability","label","prediction") \<br/>    .orderBy("probability", ascending=False) \<br/>    .show(n = 10, truncate = 30)</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/5b74d889799bc02279d6b533947397db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P46GkuXnQkY35XJmj-MuCA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 8</figcaption></figure><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="9c4c" class="mw lc iq ms b gy mx my l mz na">evaluator = MulticlassClassificationEvaluator(predictionCol="prediction")<br/>evaluator.evaluate(predictions)</span></pre><p id="95b0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="nq">0.962541462988848</em></strong></p><p id="02b3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">随机森林</strong></p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="d06a" class="mw lc iq ms b gy mx my l mz na">from pyspark.ml.classification import RandomForestClassifier</span><span id="7203" class="mw lc iq ms b gy nb my l mz na">rf = RandomForestClassifier(labelCol="label", \<br/>                            featuresCol="features", \<br/>                            numTrees = 100, \<br/>                            maxDepth = 4, \<br/>                            maxBins = 32)</span><span id="749b" class="mw lc iq ms b gy nb my l mz na"># Train model with Training Data<br/>rfModel = rf.fit(trainingData)</span><span id="2051" class="mw lc iq ms b gy nb my l mz na">predictions = rfModel.transform(testData)</span><span id="9150" class="mw lc iq ms b gy nb my l mz na">predictions.filter(predictions['prediction'] == 0) \<br/>    .select("Descript","Category","probability","label","prediction") \<br/>    .orderBy("probability", ascending=False) \<br/>    .show(n = 10, truncate = 30)</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/3572fd25214fb2e0b59ab5ecca1782cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-8jaZP3rBvn-yLpQRolToQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 9</figcaption></figure><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="341a" class="mw lc iq ms b gy mx my l mz na">evaluator = MulticlassClassificationEvaluator(predictionCol="prediction")<br/>evaluator.evaluate(predictions)</span></pre><p id="7fec" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">0.6600326922344301T52】</strong></p><p id="f9ca" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">随机森林是一种非常好的、健壮的和通用的方法，但是对于高维稀疏数据来说，它不是最佳选择，这并不奇怪。</p><p id="6cea" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">显然，逻辑回归将是我们在这个实验中的模型，具有交叉验证。</p><p id="fdec" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这就把我们带到了文章的结尾。创建这篇文章的源代码可以在<a class="ae la" href="https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/SF_Crime_Text_Classification_PySpark.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。我期待听到任何反馈或问题。</p></div></div>    
</body>
</html>