<html>
<head>
<title>Why Can a Machine Beat Mario but not Pokemon?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么机器能打得过马里奥却打不过口袋妖怪？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-can-a-machine-beat-mario-but-not-pokemon-ff61313187e1?source=collection_archive---------5-----------------------#2018-10-02">https://towardsdatascience.com/why-can-a-machine-beat-mario-but-not-pokemon-ff61313187e1?source=collection_archive---------5-----------------------#2018-10-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/42339374c74d98718e734e7b302c052b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yv-vtiG2cOdC3_MKmwKo3Q.jpeg"/></div></div></figure><p id="76d4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">到现在为止，你可能已经听说过机器人以超人的水平玩视频游戏。这些机器人可以被显式编程，以设定的输出对设定的输入做出反应，或者<a class="ae kz" href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel="noopener ugc nofollow" target="_blank"> <em class="la">学习</em> </a> <em class="la">和</em> <a class="ae kz" href="https://en.wikipedia.org/wiki/Genetic_algorithm" rel="noopener ugc nofollow" target="_blank"> <em class="la">进化</em> </a>，以不同的方式对相同的输入做出反应，以期找到最佳的反应。</p><p id="e960" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">几个著名的例子是:</p><ul class=""><li id="70c7" class="lb lc it kd b ke kf ki kj km ld kq le ku lf ky lg lh li lj bi translated">AlphaZero ，一个经过 24 小时训练后成为地球上最伟大的棋手的国际象棋机器人</li><li id="e20f" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lg lh li lj bi translated">AlphaGo ，一个著名的围棋机器人，击败了世界级棋手李·塞多尔和柯洁</li><li id="0726" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lg lh li lj bi translated">MarI/O ，一个超级马里奥机器人，它可以自己学习快速运行任何级别的马里奥</li></ul><p id="6405" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这些游戏很复杂，训练这些机器需要复杂算法、重复模拟和时间的巧妙结合。我想重点谈谈 MarI/O 以及为什么我们不能用类似的方法打败一个口袋妖怪的游戏(如果你不熟悉它的工作原理，请观看上面链接中的视频)。</p><p id="62af" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">马里奥和口袋妖怪在这方面有 3 个主要区别:</p><ol class=""><li id="5d8d" class="lb lc it kd b ke kf ki kj km ld kq le ku lf ky lp lh li lj bi translated">目标数量</li><li id="cfc6" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lp lh li lj bi translated">分子因子</li><li id="de29" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lp lh li lj bi translated">全局优化与局部优化</li></ol><p id="9587" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们用这些因素来比较这些游戏。</p><h2 id="a0e0" class="lq lr it bd ls lt lu dn lv lw lx dp ly km lz ma mb kq mc md me ku mf mg mh mi bi translated">目标数量</h2><blockquote class="mj mk ml"><p id="d226" class="kb kc la kd b ke kf kg kh ki kj kk kl mm kn ko kp mn kr ks kt mo kv kw kx ky im bi translated">机器<strong class="kd iu">学习</strong> <em class="it"> </em>的方式是通过优化某种目标函数。无论是最大化一个<a class="ae kz" href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel="noopener ugc nofollow" target="_blank">奖励</a>或<a class="ae kz" href="https://en.wikipedia.org/wiki/Fitness_function" rel="noopener ugc nofollow" target="_blank">适应度</a>函数(在强化学习和遗传算法中)，还是最小化一个<a class="ae kz" href="https://en.wikipedia.org/wiki/Loss_function" rel="noopener ugc nofollow" target="_blank">成本函数</a>(在监督学习中)，目标都是相似的:尽可能获得最好的分数。</p></blockquote><p id="ec83" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">马里奥有一个目标:<strong class="kd iu">到达关卡</strong> <em class="la">的末尾。简单地说，你临死前得到的权利越多，你做得越好。这是你的单一目标函数，你的模型能力可以直接用这个数字来衡量。</em></p><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mp"><img src="../Images/a31ce74e7317b660107f7c27ada3e4fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FVJNDZ14EsooYDvFI5X1ww.png"/></div></div></figure><p id="03fa" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">口袋妖怪有…很多。让我们试着确定我们的目标。是为了打败精英 4 吗？去抓所有的口袋妖怪？培养最强团队？是以上所有的还是其他的？仅仅问这个问题就很奇怪，因为答案可能是所有这些问题的某种个人主观组合。</p><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mu"><img src="../Images/f71a2685a650b586c5b799d470d601eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zfSqbJVSlTyOyhJOebvqEA.png"/></div></div></figure><p id="1b24" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们不仅要定义最终目标，还要定义进展是什么样的，所以每一个行动单元都对应着一个奖励或损失，这个奖励或损失是基于任何时候许多许多可能的选择。</p><p id="f5a1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这就引出了下一个话题。</p><h2 id="6740" class="lq lr it bd ls lt lu dn lv lw lx dp ly km lz ma mb kq mc md me ku mf mg mh mi bi translated">分子因子</h2><blockquote class="mj mk ml"><p id="0532" class="kb kc la kd b ke kf kg kh ki kj kk kl mm kn ko kp mn kr ks kt mo kv kw kx ky im bi translated">简单来说，分支因子就是你在任何一步可以做出多少选择。在国际象棋中，平均分支因子是 35；在围棋里是 250。对于未来的每一步，你都有(factor)^(steps 分行)数量的选择要评估。</p></blockquote><p id="62c5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在马里奥里，你要么向左，向右，跳，要么什么都不做。机器要评估的选择数量很少。而且，分支因子越小，机器人就能在计算上看得更远。</p><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mv"><img src="../Images/6406690a6fb71144bb8baa5cbf417995.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gUFeJqNL6YCY5exZbdT9DA.png"/></div></div></figure><p id="a8e5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">口袋妖怪是一个开放的世界游戏，这意味着你在任何时候都有很多选择。简单地向上、向下、向左或向右移动并不是计算分支因子的有用方法。相反，我们着眼于下一个有意义的行动。下一个行动是去打仗，与 NPC 人交谈，还是去左、右、上、下的下一个地方？随着游戏的进行，可能的选择范围从大到非常大。</p><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mw"><img src="../Images/d8015df2540aebdcf9b1549ca6b01c63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fX5UfQKNBaiU30t0Gm-VlA.png"/></div></div></figure><p id="231a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">建造一台能够计算出最佳选择集的机器需要它考虑自己的短期和长期目标，这就引出了最后一个话题。</p><h2 id="9ed0" class="lq lr it bd ls lt lu dn lv lw lx dp ly km lz ma mb kq mc md me ku mf mg mh mi bi translated">局部优化与全局优化</h2><blockquote class="mj mk ml"><p id="3496" class="kb kc la kd b ke kf kg kh ki kj kk kl mm kn ko kp mn kr ks kt mo kv kw kx ky im bi translated">局部和全局优化可以从空间和时间上考虑。短期目标和直接的地理区域被认为是局部的，而长期目标和相对较大的区域如城市甚至整个地图被认为是全局的。</p></blockquote><p id="039a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">将每次运行分解成它的组成部分是一种将口袋妖怪问题分解成小块的方法。局部优化在一个区域内从 A 点到 B 点很容易，但是决定<em class="la">哪个</em>目的地是最佳 B 点是一个困难得多的问题。<a class="ae kz" href="https://en.wikipedia.org/wiki/Greedy_algorithm" rel="noopener ugc nofollow" target="_blank">贪婪算法</a>在这里失败，因为局部最优决策步骤不一定导致全局最优。</p><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/1d193da1735572209a12bdd5250388f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*cNVnt-S5e-YHv9OfPtHs9Q.png"/></div></figure><p id="8cdb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">马里奥地图很小，是线性的。口袋妖怪地图很大，错综复杂，并且是非线性的。为了追求更高的目标，你的最高优先级会随着时间而改变，将全局目标转化为优先的局部优化问题并不是一件容易的事情。这不是我们目前的型号所能处理的。</p><h2 id="1cd6" class="lq lr it bd ls lt lu dn lv lw lx dp ly km lz ma mb kq mc md me ku mf mg mh mi bi translated">最后一件事</h2><p id="910e" class="pw-post-body-paragraph kb kc it kd b ke my kg kh ki mz kk kl km na ko kp kq nb ks kt ku nc kw kx ky im bi translated">从机器人的角度来看，口袋妖怪不仅仅是一个游戏。机器人是专家，当你遇到想要战斗的 NPC 人时，帮助你在地图上移动的机器人无能为力。从他们的角度来看，这是两个完全不同的任务。</p><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nd"><img src="../Images/e1123c5d662833505d81037f20acbaf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S75ib03mWgZVMh3eApvQ1g.png"/></div></div></figure><p id="7161" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在战斗中，每一回合都有几十种选择。选择使用哪个动作，换哪个口袋妖怪，以及何时使用不同的物品本身就是一个复杂的优化问题。环顾四周，我找到了<a class="ae kz" href="https://realworldcoding.io/machine-learning-in-pokemon-db32dcd96f33" rel="noopener ugc nofollow" target="_blank">这篇文章</a>，其中有人解释了他建造战斗模拟器的过程。这是经过深思熟虑的，非常复杂，甚至没有考虑到物品的使用，这是决定战斗结果的一个关键因素。</p><p id="1e53" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">到目前为止，我们应该庆幸的是，我们可以建立比我们更擅长自己游戏的机器人。到目前为止，这些游戏在数学上很复杂，但在目标上很简单。随着人工智能的进步，我们将创造出能够解决越来越有影响力的现实世界问题的机器，所有这些都是通过它们自己对复杂优化问题的学习来实现的。请放心，我们仍然有更擅长的事情，包括我们童年的游戏——至少现在是这样。感谢阅读！</p></div></div>    
</body>
</html>