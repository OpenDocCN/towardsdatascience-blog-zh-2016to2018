<html>
<head>
<title>Active Learning on MNIST — Saving on Labeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主动学习 MNIST——节省标签</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/active-learning-on-mnist-saving-on-labeling-f3971994c7ba?source=collection_archive---------13-----------------------#2018-12-18">https://towardsdatascience.com/active-learning-on-mnist-saving-on-labeling-f3971994c7ba?source=collection_archive---------13-----------------------#2018-12-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/92003224d7569f5a3248e1959fb7b7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VUUClL79OaGHup-ACUI2Ug.png"/></div></div></figure><div class=""/><div class=""><h2 id="c260" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">标记数据的聪明方法是从决策边界开始</h2></div><p id="1521" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">主动学习是一种半监督技术，允许通过从学习过程(损失)的角度选择最重要的样本来标记较少的数据。在数据量大且标记率高的情况下，它会对项目成本产生巨大影响。例如，对象检测和 NLP-NER 问题。<br/>文章基于以下代码:<a class="ae lm" href="https://github.com/andy-bosyi/articles/blob/master/ActiveLearning-MNIST.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ks jc">主动学习 MNIST </strong> </a></p><h2 id="4ac1" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">实验数据</h2><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="6247" class="ln lo jb ml b gy mp mq l mr ms"><em class="mt">#load 4000 of MNIST data for train and 400 for testing</em><br/>(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()<br/>x_full = x_train[:4000] / 255<br/>y_full = y_train[:4000]<br/>x_test = x_test[:400] /255<br/>y_test = y_test[:400]<br/>x_full.shape, y_full.shape, x_test.shape, y_test.shape</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mu"><img src="../Images/55871e93637f2f976f646d07501b4205.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gA31SonKIeJ0HUQjjhs1bA.png"/></div></div></figure><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="df58" class="ln lo jb ml b gy mp mq l mr ms">plt.imshow(x_full[3999])</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/f88359ee78884282cb45f9d92b7a2ff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*TSs_WARei2qbGthrSoiPrA.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">&lt;matplotlib.image.AxesImage at 0x7f59087e5978&gt;</figcaption></figure><p id="718f" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我将使用 MNIST 数据集的子集，这是 6 万张带标签的数字图片和 10K 测试样本。为了更快地训练，训练需要 4000 个样本(图片)，测试需要 400 个样本(神经网络在训练过程中永远看不到)。为了标准化，我将灰度图像点除以 255。</p><h1 id="cd8b" class="na lo jb bd lp nb nc nd ls ne nf ng lv kh nh ki ly kk ni kl mb kn nj ko me nk bi translated">模型、培训和标签流程</h1><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="6f85" class="ln lo jb ml b gy mp mq l mr ms"><em class="mt">#build computation graph</em><br/>x = tf.placeholder(tf.float32, [<strong class="ml jc">None</strong>, 28, 28])<br/>x_flat = tf.reshape(x, [-1, 28 * 28])<br/>y_ = tf.placeholder(tf.int32, [<strong class="ml jc">None</strong>])<br/>W = tf.Variable(tf.zeros([28 * 28, 10]), tf.float32)<br/>b = tf.Variable(tf.zeros([10]), tf.float32)<br/>y = tf.matmul(x_flat, W) + b<br/>y_sm = tf.nn.softmax(y)<br/>loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_, logits=y))<br/>train = tf.train.AdamOptimizer(0.1).minimize(loss)<br/>accuracy = tf.reduce_mean(tf.cast(tf.equal(y_, tf.cast(tf.argmax(y, 1), tf.int32)), tf.float32))</span></pre><p id="bcbc" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">作为一个框架，我们可以使用 TensorFlow 计算图来构建十个神经元(每个数字)。w 和 b 是神经元的权重。softmax 输出 y_sm 将有助于数字的概率(置信度)。损失将是预测和标记数据之间典型的“软最大化”交叉熵。优化器的选择是一个流行的 Adam，学习率几乎是默认的— 0.1。作为一个主要的衡量标准，我将使用测试数据集的准确性。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="a4eb" class="ln lo jb ml b gy mp mq l mr ms"><strong class="ml jc">def</strong> reset():<br/>    <em class="mt">'''Initialize data sets and session'''</em><br/>    <strong class="ml jc">global</strong> x_labeled, y_labeled, x_unlabeled, y_unlabeled<br/>    x_labeled = x_full[:0]<br/>    y_labeled = y_full[:0]<br/>    x_unlabeled = x_full<br/>    y_unlabeled = y_full<br/>    tf.global_variables_initializer().run()<br/>    tf.local_variables_initializer().run() <br/><br/><strong class="ml jc">def</strong> fit():<br/>    <em class="mt">'''Train current labeled dataset until overfit.'''</em><br/>    trial_count = 10<br/>    acc = sess.run(accuracy, feed_dict={x:x_test, y_:y_test})<br/>    weights = sess.run([W, b])<br/>    <strong class="ml jc">while</strong> trial_count &gt; 0:<br/>        sess.run(train, feed_dict={x:x_labeled, y_:y_labeled})<br/>        acc_new = sess.run(accuracy, feed_dict={x:x_test, y_:y_test})<br/>        <strong class="ml jc">if</strong> acc_new &lt;= acc:<br/>            trial_count -= 1<br/>        <strong class="ml jc">else</strong>:<br/>            trial_count = 10<br/>            weights = sess.run([W, b])<br/>            acc = acc_new<br/><br/>    sess.run([W.assign(weights[0]), b.assign(weights[1])])    <br/>    acc = sess.run(accuracy, feed_dict={x:x_test, y_:y_test})<br/>    print('Labels:', x_labeled.shape[0], '<strong class="ml jc">\t</strong>Accuracy:', acc)<br/><br/><strong class="ml jc">def</strong> label_manually(n):<br/>    <em class="mt">'''Human powered labeling (actually copying from the prelabeled MNIST dataset).'''</em><br/>    <strong class="ml jc">global</strong> x_labeled, y_labeled, x_unlabeled, y_unlabeled<br/>    x_labeled = np.concatenate([x_labeled, x_unlabeled[:n]])<br/>    y_labeled = np.concatenate([y_labeled, y_unlabeled[:n]])<br/>    x_unlabeled = x_unlabeled[n:]<br/>    y_unlabeled = y_unlabeled[n:]</span></pre><p id="dfa9" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这里我定义了这三个过程，以便更方便地编码。<br/> <strong class="ks jc"> reset() </strong> —清空已标记的数据集，将所有数据放入未标记的数据集中，并重置会话变量</p><p id="51ac" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><strong class="ks jc"> fit() </strong> —运行训练，试图达到最佳准确度。如果在前十次尝试中没有提高，训练将在最后一次最佳结果时停止。我们不能使用任何大量的训练时期，因为模型往往很快过拟合或需要一个密集的 L2 正则化。</p><p id="9e1d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><strong class="ks jc"> label_manually() </strong> —这是对人工数据标注的模拟。实际上，我们从已经标注的 MNIST 数据集中获取标注。</p><h2 id="1bbf" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">地面实况; 真值（机器学习）</h2><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="fd99" class="ln lo jb ml b gy mp mq l mr ms"><em class="mt">#train full dataset of 1000</em><br/>reset()<br/>label_manually(4000)<br/>fit()</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nl"><img src="../Images/a453270f1deaa9c5e2d49f2ba3caffcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pq2vE9a6eGPUxFyOAhXglA.png"/></div></div></figure><p id="e6ea" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">如果我们足够幸运，有足够的资源来标记整个数据集，我们将获得 92.25%的准确率。</p><h2 id="348a" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">使聚集</h2><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="881a" class="ln lo jb ml b gy mp mq l mr ms"><em class="mt">#apply clustering</em><br/>kmeans = tf.contrib.factorization.KMeansClustering(10, use_mini_batch=<strong class="ml jc">False</strong>)<br/>kmeans.train(<strong class="ml jc">lambda</strong>: tf.train.limit_epochs(x_full.reshape(4000, 784).astype(np.float32), 10))</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/05cc62f6b275013ffc2da59be4b56ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D7aI2Wx6LRGxuC5BJUGHqg.png"/></div></div></figure><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="597e" class="ln lo jb ml b gy mp mq l mr ms">centers = kmeans.cluster_centers().reshape([10, 28, 28])<br/>plt.imshow(np.concatenate([centers[i] <strong class="ml jc">for</strong> i <strong class="ml jc">in</strong> range(10)], axis=1))</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/667d1e41b0dbd477375dace5619158c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*EM-U1i9xA4owViC8IGXmOw.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">&lt;matplotlib.image.AxesImage at 0x7f58d033a860&gt;</figcaption></figure><p id="2422" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在这里，我尝试使用 k-means 聚类来找到一组数字，并使用这些信息进行自动标记。我运行 Tensorflow 聚类估计器，然后可视化得到的十个质心。如你所见，结果远非完美——数字“9”出现了三次，有时与“8”和“3”混在一起。</p><h2 id="9d75" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">随机标记</h2><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="4074" class="ln lo jb ml b gy mp mq l mr ms"><em class="mt">#try to run on random 400</em><br/>reset()<br/>label_manually(400)<br/>fit()</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nl"><img src="../Images/81505d7a5c5681633e35a1d0a44d631a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JCOlMNLxDAhDobbhxE3d4A.png"/></div></div></figure><p id="ced2" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">让我们尝试只标记 10%的数据(400 个样本)，我们将获得 83.75%的准确性，这与 92.25%的基本事实相差甚远。</p><h2 id="88fb" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">主动学习</h2><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="b670" class="ln lo jb ml b gy mp mq l mr ms"><em class="mt">#now try to run on 10</em><br/>reset()<br/>label_manually(10)<br/>fit()</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/d10e8b66429cfbdee34270181069b456.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3WjbIYUS1hPpYn79oN_KyQ.png"/></div></div></figure><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="ba10" class="ln lo jb ml b gy mp mq l mr ms"><em class="mt">#pass unlabeled rest 3990 through the early model</em><br/>res = sess.run(y_sm, feed_dict={x:x_unlabeled})<br/><em class="mt">#find less confident samples</em><br/>pmax = np.amax(res, axis=1)<br/>pidx = np.argsort(pmax)<br/><em class="mt">#sort the unlabeled corpus on the confidency</em><br/>x_unlabeled = x_unlabeled[pidx]<br/>y_unlabeled = y_unlabeled[pidx]<br/>plt.plot(pmax[pidx])</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/19f82aab9ce9710988b4be4a5f9b5bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*z_fjP6IfjqJJy56CBb5fBQ.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">[&lt;matplotlib.lines.Line2D at 0x7f58d0192f28&gt;]</figcaption></figure><p id="360c" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">现在我们将使用主动学习来标记相同的 10%的数据(400 个样本)。为此，我们从 10 个样本中抽取一批样本，并训练一个非常原始的模型。然后，我们将剩余的数据(3990 个样本)通过该模型，并评估最大 softmax 输出。这将显示所选类是正确答案的概率(换句话说，神经网络的置信度)。排序后，我们可以在图上看到置信度的分布从 20%到 100%不等。这个想法是从信心不足的样品中选择下一批进行标记。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="a89a" class="ln lo jb ml b gy mp mq l mr ms"><em class="mt">#do the same in a loop for 400 samples</em><br/><strong class="ml jc">for</strong> i  <strong class="ml jc">in</strong> range(39):<br/>    label_manually(10)<br/>    fit()<br/>    <br/>    res = sess.run(y_sm, feed_dict={x:x_unlabeled})<br/>    pmax = np.amax(res, axis=1)<br/>    pidx = np.argsort(pmax)<br/>    x_unlabeled = x_unlabeled[pidx]<br/>    y_unlabeled = y_unlabeled[pidx]</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nq"><img src="../Images/d318ad4121da162b6bdf41dc88dc8677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rnvqZqHfOWC1BjgDc82DpA.png"/></div></div></figure><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/24661d99baceb31deebd7aa91815843e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yWO_eg0HuseGN14MNN0XVA.png"/></div></div></figure><p id="bc73" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在对 40 个批次的 10 个样品运行这样的程序后，我们可以看到最终的准确度几乎是 90%。这远远超过在随机标记数据的情况下达到的 83.75%。</p><h2 id="8808" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">如何处理剩余的未标记数据</h2><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="2d5a" class="ln lo jb ml b gy mp mq l mr ms"><em class="mt">#pass rest unlabeled data through the model and try to autolabel</em><br/>res = sess.run(y_sm, feed_dict={x:x_unlabeled})<br/>y_autolabeled = res.argmax(axis=1)<br/>x_labeled = np.concatenate([x_labeled, x_unlabeled])<br/>y_labeled = np.concatenate([y_labeled, y_autolabeled])<br/><em class="mt">#train on 400 labeled by active learning and 3600 stochasticly autolabeled data</em><br/>fit()</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/15f19a543eb88d065c97614f9267e4f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-T6Tmjvm7_BShv4TbEY9hA.png"/></div></div></figure><p id="ea7d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">传统的方法是通过现有模型运行数据集的其余部分，并自动标记数据。然后，在训练过程中推动它可能有助于更好地调整模型。然而在我们的例子中，它并没有给我们带来任何更好的结果。</p><p id="2bcb" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我的方法是做同样的事情，但是，如同在主动学习中一样，考虑到信心:</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="aade" class="ln lo jb ml b gy mp mq l mr ms"><em class="mt">#pass rest of unlabeled (3600) data trough the model for automatic labeling and show most confident samples</em><br/>res = sess.run(y_sm, feed_dict={x:x_unlabeled})<br/>y_autolabeled = res.argmax(axis=1)<br/>pmax = np.amax(res, axis=1)<br/>pidx = np.argsort(pmax)<br/><em class="mt">#sort by confidency</em><br/>x_unlabeled = x_unlabeled[pidx]<br/>y_autolabeled = y_autolabeled[pidx]<br/>plt.plot(pmax[pidx])</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/24e9e43da00ee6093932b1c12fd26feb.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*37XHL8q9kDv2PZKrmLXvpA.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">[&lt;matplotlib.lines.Line2D at 0x7f58cf918fd0&gt;]</figcaption></figure><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="3528" class="ln lo jb ml b gy mp mq l mr ms"><em class="mt">#automatically label 10 most confident sample and train for it</em><br/>x_labeled = np.concatenate([x_labeled, x_unlabeled[-10:]])<br/>y_labeled = np.concatenate([y_labeled, y_autolabeled[-10:]])<br/>x_unlabeled = x_unlabeled[:-10]<br/>fit()</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mu"><img src="../Images/ded2dbe61ff7c0202830010868213126.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NQh_cA3jihzGkE2QlhLqYg.png"/></div></div></figure><p id="aea0" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在这里，我们通过模型评估运行剩余的未标记数据，我们仍然可以看到剩余样本的置信度不同。因此，想法是采取一批十个最有信心的样本，并训练模型。</p><pre class="mg mh mi mj gt mk ml mm mn aw mo bi"><span id="80ba" class="ln lo jb ml b gy mp mq l mr ms"><em class="mt">#run rest of unlabelled samples starting from most confident</em><br/><strong class="ml jc">for</strong> i <strong class="ml jc">in</strong> range(359):<br/>    res = sess.run(y_sm, feed_dict={x:x_unlabeled})<br/>    y_autolabeled = res.argmax(axis=1)<br/>    pmax = np.amax(res, axis=1)<br/>    pidx = np.argsort(pmax)<br/>    x_unlabeled = x_unlabeled[pidx]<br/>    y_autolabeled = y_autolabeled[pidx]<br/>    x_labeled = np.concatenate([x_labeled, x_unlabeled[-10:]])<br/>    y_labeled = np.concatenate([y_labeled, y_autolabeled[-10:]])<br/>    x_unlabeled = x_unlabeled[:-10]<br/>    fit()</span></pre><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nl"><img src="../Images/49e2c6e7e194395a0c9d181a56486d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F8j66_ZVh-Z1JyeJEbp_Cw.png"/></div></div></figure><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/fd51fd5ffbd4d51454031a24acb8ae7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_rHpc_aa86txzoTJPzaBcw.png"/></div></div></figure><p id="2d01" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这个过程需要一些时间，并给我们额外的 0.8%的准确性。</p><h2 id="59df" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">结果</h2><p id="6757" class="pw-post-body-paragraph kq kr jb ks b kt ns kc kv kw nt kf ky kz nu lb lc ld nv lf lg lh nw lj lk ll ij bi translated">实验准确率<br/> 4000 个样本 92.25% <br/> 400 个随机样本 83.75% <br/> 400 个主动学习样本 89.75% <br/> +自动标注 90.50%</p><h2 id="8fd6" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">结论</h2><p id="a623" class="pw-post-body-paragraph kq kr jb ks b kt ns kc kv kw nt kf ky kz nu lb lc ld nv lf lg lh nw lj lk ll ij bi translated">当然，这种方法有其缺点，如计算资源的大量使用，以及数据标记与早期模型评估混合需要特殊过程的事实。此外，出于测试目的，数据也需要被标记。但是，如果标签的成本很高(特别是对于 NLP、CV 项目)，这种方法可以节省大量资源，并推动更好的项目结果。</p><h2 id="c037" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">附言（同 postscript）；警官（police sergeant）</h2><p id="82e9" class="pw-post-body-paragraph kq kr jb ks b kt ns kc kv kw nt kf ky kz nu lb lc ld nv lf lg lh nw lj lk ll ij bi translated">如果你有私人问题，请在<a class="ae lm" href="https://www.linkedin.com/in/andybosyi/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>或<a class="ae lm" href="https://www.facebook.com/profile.php?id=100013517439841" rel="noopener ugc nofollow" target="_blank">脸书</a>联系我，我有时会在那里发布关于人工智能的简短新闻和想法。</p><p id="4035" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我要感谢我的同事们<a class="ae lm" href="https://www.linkedin.com/in/oleksiy-simkiv/" rel="noopener ugc nofollow" target="_blank">亚历克斯·西姆基夫</a>、<a class="ae lm" href="https://www.linkedin.com/in/mykola-kozlenko/" rel="noopener ugc nofollow" target="_blank">米科拉·科兹连科</a>、<a class="ae lm" href="https://medium.com/@volodymyrsendetskyi" rel="noopener">沃洛季米尔·森德茨基</a>、维亚奇·博西和<a class="ae lm" href="https://www.linkedin.com/in/nazar-savchenko/" rel="noopener ugc nofollow" target="_blank">纳扎尔·萨维琴科</a>富有成效的讨论、合作和有益的建议，以及整个 MindCraft.ai 团队的持续支持。<br/> Andy Bosyi，<br/>mind craft . ai 首席执行官<br/>信息技术&amp;数据科学</p></div></div>    
</body>
</html>