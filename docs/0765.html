<html>
<head>
<title>Notes on the Cramer GAN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">克莱姆甘笔记</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/notes-on-the-cramer-gan-752abd505c00?source=collection_archive---------1-----------------------#2017-06-19">https://towardsdatascience.com/notes-on-the-cramer-gan-752abd505c00?source=collection_archive---------1-----------------------#2017-06-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="22c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下讨论与最近的论文有关:</p><p id="6853" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">《克莱姆距离作为有偏瓦瑟斯坦梯度的解决方案》<br/><a class="ae kl" href="https://deepmind.com/research/publications/cramer-distance-solution-biased-wasserstein-gradients/" rel="noopener ugc nofollow" target="_blank">https://deep mind . com/research/publications/Cramer-Distance-Solution-Biased-瓦瑟斯坦梯度/ </a></p><p id="381c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章会有点长，所以我从一篇 TL 开始；博士首先，在大于 1 的维度上，<em class="km">论文没有使用克莱姆距离！</em>在第五节的 GAN 实验中，使用了能量距离<a class="ae kl" href="#df62" rel="noopener ugc nofollow">【1】</a>。因此，该模型是使用特定内核的类似于[ <a class="ae kl" href="#30d4" rel="noopener ugc nofollow"> 3 </a>，<a class="ae kl" href="#b178" rel="noopener ugc nofollow"> 4 </a> ]的生成矩匹配网络。由于能量距离是一个整数概率度量，由[ <a class="ae kl" href="#58fd" rel="noopener ugc nofollow"> 5 </a>提出的方案可以用于训练输入特征到内核，这比[ <a class="ae kl" href="#30d4" rel="noopener ugc nofollow"> 3 </a>改进了结果。我将在下面的第 1 节中对此进行解释。</p><p id="1f22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总的来说，这是一个好主意，并且似乎给出了很好的生成器样本。不幸的是，Cramer GAN 的论文提出了一个有问题的近似，这意味着该算法中的 critic 不能正确地比较发生器和参考(目标)样本分布:<em class="km">即使发生器和参考分布非常不同，您也可以获得零 critic 损耗</em>。我会在下面的第 2 节中解释这个问题。</p><p id="28df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">提出类似想法并同时出现的论文[ <a class="ae kl" href="#fa12" rel="noopener ugc nofollow"> 6 </a> ]似乎也取得了不错的效果，使用了不同的(高斯)核和深度特征。论文[ <a class="ae kl" href="#7c9a" rel="noopener ugc nofollow"> 7 </a> ]也是相关的，并没有使用[ <a class="ae kl" href="#58fd" rel="noopener ugc nofollow"> 5 </a> ]的优化，而是使用了方差控制方案，同样得到了很好的结果。</p><p id="8ec4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在谈谈细节。</p><h1 id="10d8" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">第一部分:能量距离，见证函数，梯度惩罚，核选择</h1><p id="e8b2" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">论文中的批评者是能量距离<a class="ae kl" href="#df62" rel="noopener ugc nofollow">【1】</a>(在他们论文的第 5 页，作者声明，在生成多维样本 d &gt; 1 时，他们使用能量距离，而不是克莱姆距离)。在[ <a class="ae kl" href="#d5c6" rel="noopener ugc nofollow"> 2 </a>中显示，这个距离属于积分概率度量(IPM)家族，就像 Wasserstein 距离一样(相比之下，KL 和反向 KL 发散是 f-发散，而不是 IPM)。具体而言，能量距离是最大平均差异(MMD)，首先用于训练[ <a class="ae kl" href="#30d4" rel="noopener ugc nofollow"> 3 </a>、<a class="ae kl" href="#b178" rel="noopener ugc nofollow"> 4 </a> ]中的生成模型(这些被称为生成矩匹配网络)。如果您不熟悉 MMD，请参见下面的<a class="ae kl" href="#af2f" rel="noopener ugc nofollow">附录 B </a>获取简短介绍。IPM 公式是允许我们应用方法[ <a class="ae kl" href="#58fd" rel="noopener ugc nofollow"> 5 </a> ]训练 GAN 的基本属性。</p><p id="ed44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">积分概率度量通过寻找平滑函数(“<em class="km">见证函数</em>”)来测量概率分布之间的距离，该函数最大化两种分布下的期望差异。因此，如果 P 是您的生成器样本的分布，Q 是您的参考样本的分布，那么您将寻找一个平滑函数 f，其中 E_X f(X) — E_Y f(Y)很大，其中 X 来自 P，Y 来自 Q，E_X 表示 P 下的期望值。方法[ <a class="ae kl" href="#58fd" rel="noopener ugc nofollow"> 5 </a> ]引入了一个惩罚，以在训练期间，在生成器和参考样本之间的点处，保持评论家见证函数的梯度接近 1。所以只要我们有了见证函数，就可以用[ <a class="ae kl" href="#58fd" rel="noopener ugc nofollow"> 5 </a> ]。</p><p id="6897" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">能量距离的见证函数是什么？为了弄清楚这一点，我们使用了内核的最大均值差异这一事实:</p><p id="b8e6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">k(x，x') = d(x，0) + d(x '，0) — d(x，x ')，(1)</p><p id="569d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中 d(x，x’)是 x 和 x’(见[ <a class="ae kl" href="#d5c5" rel="noopener ugc nofollow"> 2 </a>，定义 13]，忽略 0.5 的因子)之间的距离，而“0”是原点(这实际上是中等字体中的零…)，使用这个核，以及[8，2.3 节]中的见证函数的表达式，我们可以推导出等式 1 上面的见证函数 f*(x)的表达式。5 在克莱姆甘论文(否则似乎出现在哪里！).我在下面证明了这个结果(<a class="ae kl" href="#2171" rel="noopener ugc nofollow">附录 A </a>)。</p><p id="4a5d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如[5]所建议的，你可以惩罚批评家损失中见证函数的梯度；或者可以使用[9]的早期“剪辑”方法，这是由[6]完成的，也工作得很好。</p><p id="4546" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作者声称能量距离核可能是 GAN 训练的更好选择。我不相信这个论点:理论上</p><ul class=""><li id="84bc" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">尚不清楚为什么和不变性和比例不变性对于 GAN 训练是必不可少的。和不变性也适用于任何平移不变核，包括高斯核(证明很简单)！事实上，策克利和里索已经证明了能量距离的和与标度不变性，所以不清楚为什么作者在命题 3 中重复这些早期的证明。</li><li id="8afd" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">无偏梯度估计在更广的范围内是正确的，不仅仅是能量距离。在一定条件下，莱布尼茨规则意味着无偏估计量具有无偏梯度，MMD 对任何核都有无偏估计量[8]。</li></ul><p id="5dd6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实验上，我也不相信这种核选择是该方法有效的主要原因，因为[6，7]使用不同的核，也获得了良好的结果([6]使用良好的旧高斯核)。另一方面，作者在他们的附录 D 中声称比高斯和拉普拉斯核更稳定的训练。可能[5]的方法是训练 MMD GAN 的关键，但是注意[6]使用削波，而[7]使用方差控制而不是惩罚梯度，因此[5]不是使其工作的唯一方法。</p><h1 id="7878" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">第二部分:批评家是不正确的</h1><p id="583a" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">不幸的是，这篇论文做了一个有问题的近似，导致评论家不能正确地比较和匹配发电机和参考分布。这是可以解决的:[6]和[7]已经提出了不存在这个问题的方法，并且效果很好。</p><p id="9f50" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了理解这个问题，可以将 x 和 x’视为来自发生器 P 的独立样本，y 和 y’视为来自参考分布 q 的独立样本，能量距离为:</p><p id="6db6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">D(P，Q) = E_{X，X'} d(X，X') + E_{Y，Y'} d(Y，Y') — E_{X，Y'} d(X，Y') — E_{X '，Y} d(X '，Y)</p><p id="762c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中 E_{X，X'} d(X，X ')是来自生成器 P 的两个独立样本之间的预期距离(同样，Y 和 Y '是来自参考 Q 的独立样本)。让我们来理解这是什么意思:第一项是来自生成器的两个样本之间的平均距离。第二个是两个参考样本之间的平均距离。最后两项都给出了发生器和参考样本之间的平均距离。换句话说，如果来自发生器的样本与参考样本具有相同的分布，则所有项都将相同，并将抵消，当 P=Q 时，给出的能量距离为零。</p><p id="3d86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在转向由评论家在 Cramer GAN 论文的算法 1 中实现的距离。从算法 1 中的评论家见证 f(x)的定义，我们看到用于评论家的预期“代理损失”是:</p><p id="9070" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">D_c (P，Q) = E_{X，X'} d(X，X') + E_{Y} d(Y，0) — E_{X} d(X，0) — E_{X '，Y} d(X '，Y)</p><p id="8f0e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简而言之，有问题的近似是用原点 0 代替 y ’,在这一点上，批评家对 P 和 Q 之间距离的解释就失去了。很容易得出完全不同的 P 和 Q，但预期的临界损失为零。对于一个简单的一维示例，假设 P 将其所有样本放在原点，Q 将其所有样本放在距离原点 t 的位置。显然 P 和 Q 是不一样的，然而</p><p id="7545" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">E_{X，X'} d(X，X') = 0 <br/> E_{Y} d(Y，0) = t <br/> E_{X} d(X，0) = 0 <br/> E_{X '，Y} d(X '，Y) = t</p><p id="4827" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以 D_c (P，Q) = 0，批评家误以为 P 和 Q 是相同的。很难预测这个问题会在论文的图 9 和图 10 这样的复杂例子中引起什么问题，但是对于更小尺寸的简单玩具例子，很快就会发现 P 和 Q 没有正确匹配。最后，请注意[6]和[7]不做这种近似，不受影响。</p><h1 id="fe3c" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">参考</h1><p id="df62" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">[1]能量距离对应于一维中的克莱姆距离，但是我们不使用 GANs 来生成一维中的样本。参见策克利和里佐在他们 2004 年的论文“测试高维中的相等分布”。</p><p id="d5c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2] Sejdinovic，d .、Sriperumbudur，b .、Gretton，a .和 Fukumizu，k .，“假设检验中基于距离的统计和基于 RKHS 的统计的等效性”，《统计年鉴》，(2013 年)</p><p id="30d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[3] Dziugaite，G. K .，Roy，D. M .，和 Ghahramani，Z. (2015 年)。基于最大平均偏差优化的生成神经网络训练。子宫活动间期</p><p id="b178" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[4]李，y .，斯维尔斯基，k .，泽梅尔，R. (2015)。生成矩匹配网络。ICML</p><p id="58fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[5]古尔拉贾尼、艾哈迈德、阿尔乔夫斯基、杜穆林和库维尔(2017 年)。改进了瓦瑟斯坦·甘斯的训练。arXiv 预印本 arXiv:1704.00028。</p><p id="fa12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[6]李春林、张文春、程、杨和博佐斯(2017 年)。甘:加深对矩匹配网络的理解。arXiv 预印本 arXiv:1705.08584。</p><p id="7c9a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[7]优素福·姆鲁，汤姆·塞尔库，费希尔·甘，<a class="ae kl" href="https://arxiv.org/abs/1705.09675" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1705.09675</a></p><p id="e7b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[8]a . Gretton，k . m . Borgwardt，m . Rasch，M. J .，schlkopf，b .和 Smola，A. (2012 年)。两样本核检验。2012 年，JMLR</p><p id="22f0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[9]马丁·阿约夫斯基，苏史密斯·钦塔拉，莱昂·博图，瓦瑟斯坦·甘，<a class="ae kl" href="https://arxiv.org/abs/1701.07875" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1701.07875</a></p><h1 id="2171" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">附录 A:见证函数的推导</h1><p id="9760" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">根据[8，第 2.3 节]，见证函数为(直到比例常数):</p><p id="8a1a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">f*(x) = E_X k(x，X) — E_Y k(x，Y)，(2)</p><p id="2182" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中 E_X k(x，X)表示内核相对于其参数之一的期望值，X 取自 P，Y 取自 q，将内核(1)代入第一项，我们得到:</p><p id="2cda" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">E_X k(x，X) <br/> = d(x，0) + E_X d(X，0) — E_X d(x，X)</p><p id="c6d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上述表达式的第二项是常数。代入(2)，</p><p id="6779" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">f*(x) = d(x，0) — E_X d(x，X) — d(x，0) + E_Y d(x，Y) + C <br/> = E_Y d(x，Y) — E_X d(x，X) + C</p><p id="31fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中 C 是常数，可以忽略。这给出了期望的见证函数。</p><h1 id="af2f" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">附录 B:MMD 概述</h1><p id="0ccb" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">最大平均差异(MMD)是两个概率分布之间距离的简单度量[8]。在 GAN 情况下，P 定义为发电机分布，Q 定义为参考分布。那么 MMD 的平方就是:</p><p id="564f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">MMD (P，Q) = E_{X，X'} k(X，X') + E_{Y，Y'} k(Y，Y') — E_{X，Y'} k(X，Y') — E_{X '，Y} k(X '，Y)</p><p id="62b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中 k(x，X’)是 X 和 X’的“相似度”，E_{X，X’} k(X，X’)是来自生成器 p 的两个独立样本的期望“相似度”</p><p id="d181" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们如何解读这种距离？我们利用 k(x，x’)是一个核的事实，即 x 和 x’的特征的点积。当 x 和 x '相似时，它是大的，当它们不同时，它是小的。回到我们的 MMD 表达式，第一项是来自生成器的两个样本之间的平均相似性。第二个是两个参考样本之间的平均相似度。最后两项都给出了发生器和参考样本之间的平均相似性。换句话说，如果来自发生器的样本具有与参考样本相同的分布，那么所有的项都将是相同的，并且将被抵消，从而给出零 MMD。</p><p id="b20a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">MMD 是一种整数概率度量，就像沃瑟斯坦距离一样。该度量的见证函数在上面的附录 A 的等式(2)中给出。</p><p id="f464" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于使用什么内核:这是一个漫长而又传奇的问题！但是简单地说，一个众所周知的内核是“高斯”(指数平方)内核，</p><p id="5a56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">k(x，x') = exp (- d (x，x') * a)</p><p id="329f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中 d (x，x’)是 x 和 x’之间的平方欧几里德距离，a 是宽度参数。另一个内核是情商。(1)在前面的文档中，这是用于获得能量距离的内核。这两种核都给出了有效的整数概率度量来衡量 P 和 q 之间的距离。还有许多其他选项，对于什么对 GANs 最好还没有明确的共识。</p></div></div>    
</body>
</html>