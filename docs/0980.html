<html>
<head>
<title>Building a Bayesian deep learning classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建贝叶斯深度学习分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-bayesian-deep-learning-classifier-ece1845bc09?source=collection_archive---------5-----------------------#2017-07-17">https://towardsdatascience.com/building-a-bayesian-deep-learning-classifier-ece1845bc09?source=collection_archive---------5-----------------------#2017-07-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="4393" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇博客文章中，我将教你如何使用<a class="ae kl" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>和<a class="ae kl" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> tensorflow </a>来训练贝叶斯深度学习分类器。在深入具体的培训示例之前，我将介绍几个重要的高级概念:</p><ol class=""><li id="8c8c" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">贝叶斯深度学习是什么？</li><li id="5587" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">什么是不确定性？</li><li id="c777" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">为什么不确定性很重要？</li></ol><p id="bf01" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我将介绍两种在深度学习模型中包含不确定性的技术，并查看一个使用Keras在<a class="ae kl" href="https://www.cs.toronto.edu/%7Ekriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> cifar10 </a>数据集上的冻结<a class="ae kl" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> ResNet50 </a>编码器上训练完全连接的层的具体示例。通过这个例子，我还将讨论探索贝叶斯深度学习分类器的不确定性预测的方法，并提供未来改进模型的建议。</p><h1 id="e24c" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">感谢</h1><p id="1773" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">这篇文章基于两篇博客文章(<a class="ae kl" href="http://alexgkendall.com/computer_vision/bayesian_deep_learning_for_safe_ai/" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae kl" href="http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html" rel="noopener ugc nofollow" target="_blank">这里</a>)和剑桥大学机器学习小组关于贝叶斯深度学习的<a class="ae kl" href="https://arxiv.org/pdf/1703.04977.pdf" rel="noopener ugc nofollow" target="_blank">白皮书</a>。如果你在读完这篇文章后想了解更多关于贝叶斯深度学习的知识，我鼓励你查看所有这三个资源。感谢剑桥大学机器学习小组令人惊叹的博文和论文。</p><h1 id="a578" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">贝叶斯深度学习是什么？</h1><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi md"><img src="../Images/e4abf7d0fc13d434d5f3a4ac5b13d006.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*CHknJ3S2vw24Tq3L.jpg"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Visualizing a Bayesian deep learning model.</em></figcaption></figure><p id="d885" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">贝叶斯统计是统计领域中的一种理论，其中关于世界真实状态的证据是用信任度来表示的。贝叶斯统计和深度学习在实践中的结合意味着在你的深度学习模型预测中包含不确定性。早在<a class="ae kl" href="http://papers.nips.cc/paper/419-transforming-neural-net-output-levels-to-probability-distributions.pdf" rel="noopener ugc nofollow" target="_blank"> 1991 </a>就提出了在神经网络中包含不确定性的想法。简而言之，贝叶斯深度学习在典型神经网络模型中发现的每个权重和偏差参数上添加了先验分布。在过去，贝叶斯深度学习模型并不经常使用，因为它们需要更多的参数来优化，这可能使模型难以工作。然而，最近，贝叶斯深度学习变得更加流行，新技术正在开发中，以在模型中包括不确定性，同时使用与传统模型相同数量的参数。</p><h1 id="0e8d" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">什么是<a class="ae kl" href="https://en.wikipedia.org/wiki/Uncertainty" rel="noopener ugc nofollow" target="_blank">不确定性</a>？</h1><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/13dd4b11c4bf08f6a6f2d7757495a6a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/0*Te0lhvC2a2syJN_x.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">An example of ambiguity. What should the model predict?</em></figcaption></figure><p id="b2b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不确定性是一种知识有限的状态，无法准确描述现有状态、未来结果或一个以上的可能结果。当它涉及深度学习和分类时，不确定性也包括模糊性；人类定义和概念的不确定性，不是自然界的客观事实。</p><h1 id="79a3" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">不确定性的类型</h1><p id="a933" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">有几种不同类型的不确定性，在这篇文章中我将只讨论两种重要的不确定性。</p><h2 id="2864" class="mr lb iq bd lc ms mt dn lg mu mv dp lk jy mw mx lo kc my mz ls kg na nb lw nc bi translated">任意不确定性</h2><p id="62f8" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">随机不确定性衡量的是你无法从数据中理解的东西。它可以用观察所有解释变量的能力来解释。把随机不确定性想象成感知不确定性。实际上有两种类型的随机不确定性，异方差和同方差，但是我在这篇文章中只讨论异方差不确定性。在<a class="ae kl" href="http://alexgkendall.com/computer_vision/bayesian_deep_learning_for_safe_ai/" rel="noopener ugc nofollow" target="_blank">的这篇</a>博客文章中，对同性恋进行了更深入的探讨。</p><p id="33cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">立体图像中任意不确定性的具体例子有遮挡(相机看不到的场景部分)、缺乏视觉特征(如空白墙壁)或曝光过度/不足区域(眩光和阴影)。</p><div class="me mf mg mh gt ab cb"><figure class="nd mi ne nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><img src="../Images/5ed1928143aa604328c1f1b6864cd6e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/0*41TJf1loVwtDrs1t.png"/></div></figure><figure class="nd mi ne nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><img src="../Images/6288b99e2c17f5a33cdba972a699e544.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*oRiWyjsROVKKrEyDMin4hg.jpeg"/></div></figure><figure class="nd mi nn nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><img src="../Images/9f941054653d7e0117882cb1ac8868b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*0ubHUVR_vyvxNaoH.jpg"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk no di np nq">Examples of occlusions, lack of visual features and under/over exposure.</figcaption></figure></div><h2 id="3638" class="mr lb iq bd lc ms mt dn lg mu mv dp lk jy mw mx lo kc my mz ls kg na nb lw nc bi translated">认知不确定性</h2><p id="3b4d" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">认知不确定性衡量的是由于缺乏训练数据，模型不知道什么。它可以用无限的训练数据来解释。将认知不确定性视为模型不确定性。</p><p id="8aa8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">观察认知不确定性的一个简单方法是在25%的数据集上训练一个模型，在整个数据集上训练第二个模型。仅在25%的数据集上训练的模型将比在整个数据集上训练的模型具有更高的平均认知不确定性，因为它看到的例子更少。</p><p id="1174" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个有趣的认知不确定性的例子在现在著名的非热狗应用中被发现。从我自己使用该应用程序的经验来看，该模型表现非常好。但仔细观察后发现，这个网络似乎从未被训练过“不是热狗”的图片，这些图片中的物品上有番茄酱。因此，如果给模特看一张你腿上沾有番茄酱的照片，模特会被骗以为那是热狗。贝叶斯深度学习模型将预测这些情况下的高度认知不确定性。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nr ns l"/></div></figure><h1 id="b261" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">为什么不确定性很重要？</h1><p id="8229" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">在机器学习中，我们试图创建真实世界的近似表示。今天创建的流行深度学习模型产生点估计，但不是不确定性值。了解您的模型是过于自信还是过于自信有助于您对模型和数据集进行推理。上面解释的两种不确定性是重要的，原因各不相同。</p><p id="eb27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意:在分类问题中，softmax输出为每个类别提供了一个概率值，但这与不确定性不同。softmax概率是一个输入相对于其他类成为给定类的概率。因为概率是相对于其他类的，所以它无助于解释模型的总体置信度。</p><h2 id="77da" class="mr lb iq bd lc ms mt dn lg mu mv dp lk jy mw mx lo kc my mz ls kg na nb lw nc bi translated">为什么任意不确定性很重要？</h2><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/79d3c3b82b957d710902cec20558a02a.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/0*vCDoIjYaaB_b1_Yz.jpg"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Even for a human, driving with glare is difficult</em></figcaption></figure><p id="a90c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在观察空间的一部分比其他部分具有更高噪声水平的情况下，随机不确定性是很重要的。例如，任意的不确定性在第一起涉及无人驾驶汽车的死亡事故中发挥了作用。特斯拉表示，在这起事件中，汽车的自动驾驶仪未能识别明亮天空下的白色卡车。能够预测任意不确定性的图像分割分类器将认识到图像的这个特定区域难以解释，并预测高度不确定性。在特斯拉事件中，尽管汽车的雷达可以“看到”卡车，但雷达数据与图像分类器数据不一致，汽车的路径规划器最终忽略了雷达数据(已知雷达数据有噪声)。如果图像分类器在其预测中包含高度不确定性，则路径规划者将会知道忽略图像分类器预测，而使用雷达数据(这过于简单，但实际上将会发生。参见下面的卡尔曼滤波器)。</p><h2 id="bac7" class="mr lb iq bd lc ms mt dn lg mu mv dp lk jy mw mx lo kc my mz ls kg na nb lw nc bi translated">为什么认知不确定性很重要？</h2><p id="82b2" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">认知不确定性很重要，因为它识别模型从未被训练理解的情况，因为这些情况不在训练数据中。机器学习工程师希望我们的模型能够很好地概括与训练数据不同的情况；然而，在深度学习的安全关键应用中，希望是不够的。高认知不确定性是一个危险信号，表明模型更有可能做出不准确的预测，当这种情况发生在安全关键应用中时，该模型不应被信任。</p><p id="670a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">认知的不确定性也有助于探索你的数据集。例如，认知的不确定性可能有助于20世纪80年代发生的这个特定的神经网络灾难。在这种情况下，研究人员训练了一个神经网络来识别隐藏在树中的坦克和没有坦克的树。经过训练后，网络在训练集和测试集上表现得非常好。唯一的问题是，所有坦克的图像都是在阴天拍摄的，所有没有坦克的图像都是在晴天拍摄的。这个分类器实际上已经学会了辨别晴天和阴天。哎呦。</p><div class="me mf mg mh gt ab cb"><figure class="nd mi nu nf ng nh ni paragraph-image"><img src="../Images/22767a3efeb9b266ff065e206830ac1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/0*VAhCRVaAjSR3a7_8.jpg"/></figure><figure class="nd mi nu nf ng nh ni paragraph-image"><img src="../Images/dec0cae29c09d32d3b3c3273d81777cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/0*38BlNr4iYnfrkAYt.jpg"/><figcaption class="ml mm gj gh gi mn mo bd b be z dk nv di nw nq"><em class="mp">Tank &amp; cloudy vs no tank &amp; sunny</em></figcaption></figure></div><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="ab gu cl nx"><img src="../Images/a8c77a44a92216c6a0944588053f7180.png" data-original-src="https://miro.medium.com/v2/format:webp/1*m0T_vjg4mOJNIvel1JXGqQ.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Radar and lidar data merged into the Kalman filter. Image data could be incorporated as well.</em></figcaption></figure><p id="896a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">深度学习模型中的不确定性预测在机器人技术中也很重要。我目前就读于Udacity自动驾驶汽车纳米学位，并一直在学习汽车/机器人用于识别和跟踪物体的技术。自动驾驶汽车使用一种叫做<a class="ae kl" href="https://en.wikipedia.org/wiki/Kalman_filter" rel="noopener ugc nofollow" target="_blank">卡尔曼滤波器</a>的强大技术来追踪物体。卡尔曼滤波器组合一系列包含统计噪声的测量数据，并产生比任何单个测量更精确的估计。传统的深度学习模型无法对卡尔曼滤波器做出贡献，因为它们只预测结果，不包括不确定项。理论上，贝叶斯深度学习模型可以有助于卡尔曼滤波器跟踪。</p><h1 id="18b0" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">计算深度学习分类模型中的不确定性</h1><p id="b02a" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">任意不确定性和认知不确定性是不同的，因此，它们的计算方式也不同。</p><h2 id="2aee" class="mr lb iq bd lc ms mt dn lg mu mv dp lk jy mw mx lo kc my mz ls kg na nb lw nc bi translated">计算任意不确定性</h2><p id="606a" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">任意不确定性是输入数据的函数。因此，深度学习模型可以通过使用修改的损失函数来学习预测任意的不确定性。对于分类任务，贝叶斯深度学习模型将有两个输出，softmax值和输入方差，而不是仅预测softmax值。教导模型预测任意方差是无监督学习的一个示例，因为模型没有方差标签可供学习。下面是标准分类交叉熵损失函数和计算贝叶斯分类交叉熵损失的函数。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="ny ns l"/></div></figure><p id="52ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我创建的损失函数是基于这篇论文<a class="ae kl" href="https://arxiv.org/pdf/1703.04977.pdf" rel="noopener ugc nofollow" target="_blank">中的损失函数。在本文中，损失函数创建了一个均值为零的正态分布和预测方差。它通过从分布中采样来扭曲预测的logit值，并使用扭曲的预测来计算softmax分类交叉熵。损失函数运行T个蒙特卡罗样本，然后取T个样本的平均值作为损失。</a></p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nz"><img src="../Images/985e9b4d264835c8128dead71b8115f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Avvo2Vka-FA1wJ1M.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Figure 1: Softmax categorical cross entropy vs. logit difference for binary classification</em></figcaption></figure><p id="3ea1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在图1中，y轴是softmax分类交叉熵。x轴是“正确的”logit值和“错误的”logit值之间的差值。“右”表示该预测的正确类别。“错误”表示该预测的错误类别。我将使用术语“logit difference”来表示图1中的x轴。当“logit difference”在图1中为正时，softmax预测将是正确的。当“logit difference”为负时，预测将是不正确的。在解释任意损失函数时，我将继续使用术语“逻辑差”、“正确的”逻辑和“错误的”逻辑。</p><p id="9709" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图1有助于理解正态分布失真的结果。当使用正态分布对logit值(在二元分类中)进行扭曲时，这种扭曲实际上创建了一个正态分布，其原始预测“logit差异”和预测方差的平均值作为分布方差。将softmax交叉熵应用于失真的logit值与沿着图1中的线对“logit差异”值进行采样是相同的。</p><p id="ba22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对扭曲的逻辑取分类交叉熵应该理想地产生一些有趣的性质。</p><ol class=""><li id="cdd7" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">当预测的logit值远大于任何其他logit值时(图1的右半部分)，增加方差只会增加损失。这是真的，因为导数在图的右半部分是负的。即，与“logit difference”的相等减少相比，增加“logit difference”仅导致softmax分类交叉熵的稍微更小的减少。在这种情况下，最小损耗应该接近于0。</li><li id="aef5" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">当“错误的”logit远大于“正确的”logit(图的左半部分)且方差为~0时，损失应为~ <code class="fe oa ob oc od b">wrong_logit-right_logit</code>。你可以看到这在图1的右半部分。当“logit差”为-4时，softmax交叉熵为4。图中这一部分的斜率为~ -1，因此随着“logit差异”继续减小，这应该是正确的。</li><li id="593b" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">为了使模型能够学习任意的不确定性，当“错误的”logit值大于“正确的”logit值(图的左半部分)时，对于大于0的方差值，损失函数应该最小化。对于具有高度随机不确定性的图像(即，模型很难对该图像做出准确预测)，该特征鼓励模型通过增加其预测方差来在训练期间找到局部损失最小值。</li></ol><p id="71e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当“错误的”logit值大于“正确的”logit值时，我能够使用论文中建议的损失函数通过增加方差来减少损失，但是由于增加方差而减少的损失非常小(&lt;0.1). During training, my model had a hard time picking up on this slight local minimum and the aleatoric variance predictions from my model did not make sense. I believe this happens because the slope of Figure 1 on the left half of the graph is ~ -1. Sampling a normal distribution along a line with a slope of -1 will result in another normal distribution and the mean will be about the same as it was before but what we want is for the mean of the T samples to decrease as the variance increases.</p><p id="ff22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">To make the model easier to train, I wanted to create a more significant loss change as the variance increases. Just like in the paper, my loss function above distorts the logits for T Monte Carlo samples using a normal distribution with a mean of 0 and the predicted variance and then computes the categorical cross entropy for each sample. To get a more significant loss change as the variance increases, the loss function needed to weight the Monte Carlo samples where the loss decreased more than the samples where the loss increased. My solution is to use the <a class="ae kl" href="http://image-net.org/challenges/posters/JKU_EN_RGB_Schwarz_poster.pdf" rel="noopener ugc nofollow" target="_blank"> elu </a>激活函数，这是一个以0为中心的非线性函数)。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f5c4ef0de119f5c40642fe80451012f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/0*RlB8gfHtOUmEWZ-x.jpg"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">ELU activation function</em></figcaption></figure><p id="7d59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我将elu函数应用于分类交叉熵的变化，即原始未失真损失与失真损失的比较，<code class="fe oa ob oc od b">undistorted_loss - distorted_loss</code>。对于图1的左半部分，elu将正态分布的平均值从零移开。对于接近0的非常小的值，elu也是线性的，因此图1右半部分的平均值保持不变。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nz"><img src="../Images/d528840ebb1f9a23242bd5ea468ff95f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XReG7SYusCehyt83.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Figure 2: Average change in loss &amp; distorted average change in loss.</em></figcaption></figure><p id="76e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在图2中，<code class="fe oa ob oc od b">right &lt; wrong</code>对应于图1左半部分的一个点，而<code class="fe oa ob oc od b">wrong &lt; right</code>对应于图2右半部分的一个点。您可以看到,“错误”logit情况下的结果分布看起来类似于正态分布，而“正确”情况下的结果大多是接近零的小值。将<code class="fe oa ob oc od b">-elu</code>应用于损失变化后，<code class="fe oa ob oc od b">right &lt; wrong</code>的均值变得更大。在本例中，它从-0.16变为0.25。<code class="fe oa ob oc od b">wrong &lt; right</code>的意思保持不变。我将图2中下方图表的平均值称为“失真的平均损失变化”。随着图1右半部分方差的增加,“失真平均损耗变化”应保持在0附近，并且当图1右半部分方差增加时，应始终增加。</p><p id="3863" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我用原始未失真的分类交叉熵来衡量“失真的平均损失变化”。这样做是因为对于所有大于3的logit差异，错误logit情况下的失真平均损失变化大致相同(因为线的导数为0)。为了确保损失大于零，我添加了未失真的分类交叉熵。“损失的扭曲平均变化”总是随着方差的增加而减少，但是对于小于无穷大的方差值，损失函数应该最小化。为了确保最小化损失的方差小于无穷大，我添加了方差项的指数。如图3所示，方差的指数是方差超过2后的主要特征。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nz"><img src="../Images/0a355c7cd182b361da160b3fcbf97b20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lP80IsodFd0pzwJO.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Figure 3: Aleatoric variance vs loss for different ‘wrong’ logit values</em></figcaption></figure><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi of"><img src="../Images/3fe9844639f2e2d4dfac4d1886a0e7bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2XE2ViKOfaAXg3rv.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Figure 4: Minimum aleatoric variance and minimum loss for different ‘wrong’ logit values</em></figcaption></figure><p id="4e4a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些是针对二元分类示例计算上述损失函数的结果，在该示例中,“正确”的logit值保持为常数1.0，而“错误”的logit值会针对每一行发生变化。当“错误的”logit值小于1.0(因此小于“正确的”logit值)时，最小方差为0.0。随着错误的“logit”值增加，使损失最小化的方差也增加。</p><p id="4e02" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意:在生成此图时，我运行了10，000次蒙特卡洛模拟来创建平滑的线条。在训练模型时，我只运行了100次蒙特卡洛模拟，因为这应该足以得到一个合理的平均值。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi og"><img src="../Images/c6403e0249a034361f91402c37064e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*R6fBH4n978TqKFmK."/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Brain overload? Grab a time appropriate beverage before continuing.</em></figcaption></figure><h2 id="ae1a" class="mr lb iq bd lc ms mt dn lg mu mv dp lk jy mw mx lo kc my mz ls kg na nb lw nc bi translated">计算认知不确定性</h2><p id="d09f" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">对认知不确定性建模的一种方式是在测试时使用蒙特卡罗抽样(一种变分推理)。关于为什么辍学可以模拟不确定性的完整解释，请查看<a class="ae kl" href="http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html" rel="noopener ugc nofollow" target="_blank">这篇</a>博客和<a class="ae kl" href="https://arxiv.org/pdf/1703.04977.pdf" rel="noopener ugc nofollow" target="_blank">这篇</a>白皮书。实际上，蒙特卡洛漏失抽样意味着将漏失包括在模型中，并在测试时打开漏失多次运行模型，以创建结果分布。然后可以计算预测熵(预测分布中包含的平均信息量)。</p><p id="b32e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了理解如何使用dropout来计算认知不确定性，可以考虑将上面的猫狗图像垂直分成两半。</p><div class="me mf mg mh gt ab cb"><figure class="nd mi nu nf ng nh ni paragraph-image"><img src="../Images/492d9d466485243c019f9d67f6cec114.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/0*oa1PHmd9NpHdqQ-H.png"/></figure><figure class="nd mi nu nf ng nh ni paragraph-image"><img src="../Images/4d2348bc3beab8c809157f1919fc4395.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/0*NsUETUVG3EDVIbN6.png"/></figure></div><p id="e62e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你看到左半部分，你会预测狗。如果你看到右半边，你会预测猫。完美的五五分成。这个图像会有很高的认知不确定性，因为这个图像展示了你同时与猫类和狗类相关联的特征。</p><p id="9003" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是计算认知不确定性的两种方法。它们做完全相同的事情，但是第一个更简单，只使用numpy。第二种使用额外的Keras层(并获得GPU加速)来进行预测。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="ny ns l"/></div></figure><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="ny ns l"/></div></figure><p id="ef2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意:认知不确定性不用于训练模型。当评估测试/真实世界的例子时，它仅在测试时间(但是在训练阶段)被计算。这不同于随机的不确定性，随机的不确定性是作为训练过程的一部分被预测的。此外，根据我的经验，产生合理的认知不确定性预测比随机不确定性预测更容易。</p><h1 id="bdc8" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">训练贝叶斯深度学习分类器</h1><p id="d342" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">除了上面的代码之外，训练贝叶斯深度学习分类器来预测不确定性不需要比通常用于训练分类器更多的代码。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="ny ns l"/></div></figure><p id="dd53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个实验中，我使用Resnet50中的冻结卷积层和<a class="ae kl" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>的权重来编码图像。我最初试图在不冻结卷积层的情况下训练模型，但发现模型很快变得过度拟合。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="ny ns l"/></div></figure><p id="27fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我的模型的可训练部分是ResNet50输出之上的两组<code class="fe oa ob oc od b">BatchNormalization</code>、<code class="fe oa ob oc od b">Dropout</code>、<code class="fe oa ob oc od b">Dense</code>和<code class="fe oa ob oc od b">relu</code>层。使用单独的<code class="fe oa ob oc od b">Dense</code>层计算逻辑值和方差。注意，方差层应用了一个<code class="fe oa ob oc od b">softplus</code>激活函数来确保模型总是预测大于零的方差值。然后，针对任意损失函数重新组合logit和variance图层，并仅使用logit图层计算softmax。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="ny ns l"/></div></figure><p id="d6bc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我使用两个损失来训练模型，一个是任意的不确定性损失函数，另一个是标准的分类交叉熵函数。这允许创建logit的最后一个<code class="fe oa ob oc od b">Dense</code>层只学习如何产生更好的logit值，而创建方差的<code class="fe oa ob oc od b">Dense</code>层只学习预测方差。两个先前的<code class="fe oa ob oc od b">Dense</code>层将在这两个损失上训练。任意不确定性损失函数的权重小于分类交叉熵损失，因为任意不确定性损失包括分类交叉熵损失作为其一项。</p><p id="aed1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我用了100次蒙特卡罗模拟来计算贝叶斯损失函数。每个历元大约需要70秒。我发现，将蒙特卡洛模拟的次数从100次增加到1000次，每个训练周期会增加大约4分钟。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi oh"><img src="../Images/ce7cc7ceebbb8480d36a04d83b8cdc67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hbkc777vKnhDQc7G.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Example image with gamma value distortion. 1.0 is no distortion</em></figcaption></figure><p id="b586" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我通过随机应用0.5或2.0的伽玛值来降低或增加每张图像的亮度，向训练集添加了增强数据。在实践中，我发现cifar10数据集没有很多理论上表现出高度随机不确定性的图像。这大概是故意的。通过向训练集中的图像添加具有调整的灰度值的图像，我试图给模型更多的图像，这些图像应该具有高度的随机不确定性。</p><p id="43c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不幸的是，预测认知的不确定性需要相当长的时间。在我的Mac CPU上，完全连接的层预测训练集的所有50，000个类大约需要2-3秒，但预测认知不确定性需要5分钟以上。这并不奇怪，因为认知的不确定性需要对每张图像进行蒙特卡罗模拟。我运行了100次蒙特卡洛模拟，因此有理由预计预测认知不确定性所需的时间是随机不确定性的100倍。</p><p id="4dfa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我的<a class="ae kl" href="https://github.com/kyle-dorman/bayesian-neural-network-blogpost" rel="noopener ugc nofollow" target="_blank">项目</a>被设置为在未来轻松切换底层编码器网络和其他数据集的训练模型。如果您想更深入地训练自己的贝叶斯深度学习分类器，请随意使用它。</p><h1 id="1336" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">结果</h1><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi oh"><img src="../Images/67fc48395832881b123271ccc949c7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*n8jl34A47y-_ddit.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Example of each class in cifar10</em></figcaption></figure><p id="bece" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我的模型在测试数据集上的分类准确率是86.4%。这无论如何都不是一个惊人的分数。我能够得到高于93%的分数，但只是牺牲了随机不确定性的准确性。有几个不同的超参数我可以用来提高我的分数。我花了很少的时间调整两个损失函数的权重，我怀疑改变这些超参数可以大大提高我的模型精度。我也可以解冻Resnet50层，并训练这些。虽然在这个数据集上获得更好的准确性分数是有趣的，但贝叶斯深度学习是关于预测和不确定性估计的，所以我将在这篇文章的剩余部分评估我的模型的不确定性预测的有效性。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi oi"><img src="../Images/2e299b8fc6fb030cdd18078048684550.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ets-Bp3WZfJ3m4Js.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Figure 5: uncertainty mean and standard deviation for test set</em></figcaption></figure><p id="ef6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">任意的不确定性值往往比认知的不确定性小得多。这两个值不能在同一个图像上直接比较。但是，可以将它们与该模型针对该数据集中的其他图像预测的不确定性值进行比较。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi oj"><img src="../Images/bfcde49a906a79da94094b28417e235e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ryn54aG5YuO43Vn0.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Figure 6: Uncertainty to relative rank of ‘right’ logit value.</em></figcaption></figure><p id="d566" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了进一步探索不确定性，我根据正确logit的相对值将测试数据分成三组。在图5中，“第一个”包括所有正确的预测(即“右”标签的logit值是最大值)。“第二”，包括“右”标签是第二大logit值的所有情况。“休息”包括所有其他情况。86.4%的样本属于“第一”组，8.7%属于“第二”组，4.9%属于“其余”组。图5显示了这三组测试集的随机和认知不确定性的平均值和标准偏差。正如我所希望的，认知和任意的不确定性与“正确”逻辑的相对等级相关。这表明模型更有可能将不正确的标签识别为它不确定的情况。此外，当模型的预测正确时，模型预测大于零的不确定性。我希望这个模型能够展现出这种特性，因为即使它的预测是正确的，这个模型也可能是不确定的。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi oh"><img src="../Images/ca22cedd15e7dab97e166939a9757b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*P8ELgUcC_AqCMdbb.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Images with highest aleatoric uncertainty</em></figcaption></figure><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi oh"><img src="../Images/77789f65969ac3676f5e48393df1b86f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Se355XeNfwFXu0XL.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Images with the highest epistemic uncertainty</em></figcaption></figure><p id="26d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以上是随意性和认知不确定性最高的图像。虽然看这些图像很有趣，但我不太清楚为什么这些图像有很高的随意性或认知不确定性。这是训练图像分类器产生不确定性的一个缺点。整个图像的不确定性减少到单个值。理解图像分割模型中的不确定性通常要容易得多，因为比较图像中每个像素的结果更容易。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ok"><img src="../Images/7a60b61e171975c7219eb5afc22f850d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4YeBQkhBfaIv8NuU.jpg"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">“Illustrating the difference between aleatoric and epistemic uncertainty for semantic segmentation. You can notice that aleatoric uncertainty captures object boundaries where labels are noisy. The bottom row shows a failure case of the segmentation model, when the model is unfamiliar with the footpath, and the corresponding increased epistemic uncertainty.” </em><a class="ae kl" href="http://alexgkendall.com/computer_vision/bayesian_deep_learning_for_safe_ai/" rel="noopener ugc nofollow" target="_blank"><em class="mp">link</em></a></figcaption></figure><p id="038b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我的模型很好地理解了随机不确定性，那么我的模型应该为具有低对比度、高亮度/暗度或高遮挡的图像预测更大的随机不确定性值。为了测试这一理论，我将一系列伽马值应用于我的测试图像，以增加/减少增强图像的像素强度和预测结果。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi oh"><img src="../Images/bd4024d3355b9f8e4d68f7f387b18920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*z9OANsPXenoNpD5-.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk"><em class="mp">Figure 7: Left side: Images &amp; uncertainties with gamma values applied. Right side: Images &amp; uncertainties of original image.</em></figcaption></figure><p id="76c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该模型在增强图像上的准确度为5.5%。这意味着伽玛图像完全欺骗了我的模型。该模型没有被训练成在这些伽马失真上得分很高，所以这是意料之中的。图6显示了左侧的八个增强图像的预测不确定性和右侧的八个原始不确定性和图像。前四幅图像具有增强图像的最高预测随机不确定性，后四幅图像具有增强图像的最低随机不确定性。我很兴奋地看到，与原始图像相比，该模型预测了每个增强图像更高的任意性和认知不确定性！随机不确定性应该更大，因为模拟的不利照明条件使图像更难理解，而认知不确定性应该更大，因为模型没有在具有较大伽马失真的图像上训练。</p><h1 id="ff47" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">后续步骤</h1><p id="3460" class="pw-post-body-paragraph jn jo iq jp b jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk ij bi translated">这篇文章中详细描述的模型只探索了贝叶斯深度学习冰山的一角，并且展望未来，我相信我可以通过几种方式来改进模型的预测。例如，我可以继续使用损失权重，并解冻Resnet50卷积层，看看我是否可以在不失去上述不确定性特征的情况下获得更好的精度分数。我还可以尝试在一个数据集上训练一个模型，这个数据集有更多表现出高度随机不确定性的图像。一个候选是德国交通标志识别基准数据集，我在我的一个Udacity项目中使用过它。这个数据集专门用于使分类器“应对由于光照变化、部分遮挡、旋转、天气条件而导致的视觉外观的巨大变化”。对我来说听起来像是随机的不确定性！</p><p id="ad08" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除了尝试改进我的模型，我还可以进一步探索我的训练模型。一种方法是看我的模型如何处理对立的例子。为此，我可以使用像伊恩·古德费勒创建的<a class="ae kl" href="https://github.com/tensorflow/cleverhans" rel="noopener ugc nofollow" target="_blank"> CleverHans </a>这样的库。这个库使用一个对抗性的神经网络来帮助探索模型漏洞。看看CleverHans产生的对立例子是否也会导致高度的不确定性，这将是很有趣的。</p><p id="7468" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一个我很想探索的库是Edward，这是一个用于概率建模、推理和评论的Python库。<a class="ae kl" href="http://edwardlib.org/" rel="noopener ugc nofollow" target="_blank"> Edward </a>支持创建具有概率分布的网络图层，并使其易于执行变分推断。<a class="ae kl" href="https://alpha-i.co/blog/MNIST-for-ML-beginners-The-Bayesian-Way.html" rel="noopener ugc nofollow" target="_blank">这篇</a>博文使用Edward在MNIST数据集上训练贝叶斯深度学习分类器。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ol"><img src="../Images/85a0be89891aca7e90b682b9c1014916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xNK6WOcH2Wy97Yak.jpg"/></div></div></figure><p id="a1d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你已经做到了这一步，我非常感动和感激。希望这篇文章激发了你在下一个深度学习项目中加入不确定性。</p></div><div class="ab cl om on hu oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="ij ik il im in"><p id="9f38" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ot">原载于</em><a class="ae kl" href="https://gist.github.com/f93c1df82ed22ab4e1ec99fd93d0ea27" rel="noopener ugc nofollow" target="_blank"><em class="ot">gist.github.com</em></a><em class="ot">。</em></p></div></div>    
</body>
</html>