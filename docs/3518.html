<html>
<head>
<title>Coding Neural Network — Dropout</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">编码神经网络—辍学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/coding-neural-network-dropout-3095632d25ce?source=collection_archive---------3-----------------------#2018-05-20">https://towardsdatascience.com/coding-neural-network-dropout-3095632d25ce?source=collection_archive---------3-----------------------#2018-05-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/862c77cee35d8e619e02969a9e09d92f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*dEi_IkVB7IpkzZ-6H0Vpsg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk"><strong class="bd jy">Figure 1:</strong> Dropout</figcaption></figure><p id="599d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">辍学</strong>是一种正规化技术。在每次迭代中，我们随机关闭每层上的一些神经元(单元),并且在前向传播和反向传播中都不使用这些神经元。由于在每次迭代中被丢弃的单元将是随机的，学习算法将不知道在每次迭代中哪些神经元将被关闭；因此，强制学习算法分散权重，而不是专注于某些特定特征(单元)。此外，退出有助于通过以下方式改善泛化误差:</p><ul class=""><li id="38bf" class="kx ky iq kb b kc kd kg kh kk kz ko la ks lb kw lc ld le lf bi translated">因为我们在每次迭代中丢弃一些单元，这将导致更小的网络，这反过来意味着更简单的网络(正则化)。</li><li id="0a17" class="kx ky iq kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated">可以看作是装袋技术的近似。每一次迭代都可以被看作是不同的模型，因为我们在每一层随机放置不同的单元。这意味着误差将是来自所有不同模型(迭代)的误差的平均值。因此，平均来自不同模型的误差，特别是如果这些误差是不相关的，将减少总误差。在误差完全相关的最坏情况下，在所有模型中求平均值根本没有帮助；然而，我们知道在实践中，误差在某种程度上是不相关的。因此，它总是会改善泛化误差。</li></ul><p id="bd84" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们可以在每一层使用不同的概率；然而，输出层将总是具有<code class="fe ll lm ln lo b">keep_prob = 1</code>，而输入层具有高<code class="fe ll lm ln lo b">keep_prob</code>，例如 0.9 或 1。如果一个隐藏层有<code class="fe ll lm ln lo b">keep_prob = 0.8</code>，这意味着；在每次迭代中，每个单元有 80%的概率被包含，20%的概率被删除。</p><p id="ad16" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在计算机视觉问题中经常使用 Dropout，因为我们有很多特征，但没有很多数据。此外，相邻的特征(像素)通常不会增加很多信息。所以，模型总是会出现过拟合的情况。</p><p id="b16f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">为了说明 dropout 如何帮助我们减少泛化错误，我们将使用我们在以前的帖子中使用的相同数据集。该数据集有猫和非猫的图像。我们将尝试建立一个神经网络来分类图像是否有猫。每幅图像的 RGB 比例为 64 x 64 像素。让我们导入数据，看看形状以及来自训练集的猫图像样本。</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="abee" class="lx ly iq lo b gy lz ma l mb mc"># Import training data<br/>train_dataset = h5py.File("../data/train_catvnoncat.h5")<br/>X_train = np.array(train_dataset["train_set_x"])<br/>Y_train = np.array(train_dataset["train_set_y"])<br/><br/># Plot a sample image<br/>plt.imshow(X_train[50])<br/>plt.axis("off");<br/><br/># Import test data<br/>test_dataset = h5py.File("../data/test_catvnoncat.h5")<br/>X_test = np.array(test_dataset["test_set_x"])<br/>Y_test = np.array(test_dataset["test_set_y"])<br/><br/># Transform data<br/>X_train = X_train.reshape(209, -1).T<br/>X_train = X_train / 255<br/>Y_train = Y_train.reshape(-1, 209)<br/><br/>X_test = X_test.reshape(50, -1).T<br/>X_test = X_test / 255<br/>Y_test = Y_test.reshape(-1, 50)<br/><br/># print the new shape of both training and test datasets<br/>print("Training data dimensions:")<br/>print("X's dimension: {}, Y's dimension: {}".format(X_train.shape, Y_train.shape))<br/>print("Test data dimensions:")<br/>print("X's dimension: {}, Y's dimension: {}".format(X_test.shape, Y_test.shape))</span><span id="1f33" class="lx ly iq lo b gy md ma l mb mc">Training data dimensions:<br/>X's dimension: (12288, 209), Y's dimension: (1, 209)<br/>Test data dimensions:<br/>X's dimension: (12288, 50), Y's dimension: (1, 50)</span></pre><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div class="gh gi me"><img src="../Images/a7ac5e81ea9483b2edbd1c3417f123c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*n4kWtFMXmO9LgwFW0szo3A.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk"><strong class="bd jy">Figure 2:</strong> Sample image</figcaption></figure><p id="1e52" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在，我们将编写在前向传播和反向传播上应用压差所需的函数。请注意，我们将利用我们在以前的帖子中编写的函数，如<code class="fe ll lm ln lo b">initialize_parameters</code>。</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="5c9c" class="lx ly iq lo b gy lz ma l mb mc">def drop_out_matrices(layers_dims, m, keep_prob):<br/>    np.random.seed(1)<br/>    D = {}<br/>    L = len(layers_dims)<br/><br/>    for l in range(L):<br/>        # initialize the random values for the dropout matrix<br/>        D[str(l)] = np.random.rand(layers_dims[l], m)<br/>        # Convert it to 0/1 to shut down neurons corresponding to each element<br/>        D[str(l)] = D[str(l)] &lt; keep_prob[l]<br/>        assert(D[str(l)].shape == (layers_dims[l], m))<br/>    return D<br/><br/><br/>def L_model_forward(<br/>   X, parameters, D, keep_prob, hidden_layers_activation_fn="relu"):<br/>    A = X                           # since input matrix A0<br/>    A = np.multiply(A, D[str(0)])<br/>    A /= keep_prob[0]<br/>    caches = []                     # initialize the caches list<br/>    L = len(parameters) // 2        # number of layer in the network<br/><br/>    for l in range(1, L):<br/>        A_prev = A<br/>        A, cache = linear_activation_forward(<br/>            A_prev, parameters["W" + str(l)], parameters["b" + str(l)],<br/>            hidden_layers_activation_fn)<br/>        # shut down some units<br/>        A = np.multiply(A, D[str(l)])<br/>        # scale that value of units to keep expected value the same<br/>        A /= keep_prob[l]<br/>        caches.append(cache)<br/><br/>    AL, cache = linear_activation_forward(<br/>        A, parameters["W" + str(L)], parameters["b" + str(L)], "sigmoid")<br/>    AL = np.multiply(AL, D[str(L)])<br/>    AL /= keep_prob[L]<br/>    caches.append(cache)<br/>    assert(AL.shape == (1, X.shape[1]))<br/><br/>    return AL, caches<br/><br/><br/>def L_model_backward(<br/>   AL, Y, caches, D, keep_prob, hidden_layers_activation_fn="relu"):<br/>    Y = Y.reshape(AL.shape)<br/>    L = len(caches)<br/>    grads = {}<br/><br/>    # dA for output layer<br/>    dAL = np.divide(AL - Y, np.multiply(AL, 1 - AL))<br/>    dAL = np.multiply(dAL, D[str(L)])<br/>    dAL /= keep_prob[L]<br/><br/>    grads["dA" + str(L - 1)], grads["dW" + str(L)], grads[<br/>        "db" + str(L)] = linear_activation_backward(<br/>            dAL, caches[L - 1], "sigmoid")<br/>    grads["dA" + str(L - 1)] = np.multiply(<br/>        grads["dA" + str(L - 1)], D[str(L - 1)])<br/>    grads["dA" + str(L - 1)] /= keep_prob[L - 1]<br/><br/>    for l in range(L - 1, 0, -1):<br/>        current_cache = caches[l - 1]<br/>        grads["dA" + str(l - 1)], grads["dW" + str(l)], grads[<br/>            "db" + str(l)] = linear_activation_backward(<br/>                grads["dA" + str(l)], current_cache,<br/>                hidden_layers_activation_fn)<br/><br/>        grads["dA" + str(l - 1)] = np.multiply(<br/>            grads["dA" + str(l - 1)], D[str(l - 1)])<br/>        grads["dA" + str(l - 1)] /= keep_prob[l - 1]<br/><br/>    return grads<br/><br/><br/>def model_with_dropout(<br/>        X, Y, layers_dims, keep_prob, learning_rate=0.01, num_iterations=3000,<br/>        print_cost=True, hidden_layers_activation_fn="relu"):<br/>    # get number of examples<br/>    m = X.shape[1]<br/><br/>    # to get consistents output<br/>    np.random.seed(1)<br/><br/>    # initialize parameters<br/>    parameters = initialize_parameters(layers_dims)<br/><br/>    # intialize cost list<br/>    cost_list = []<br/><br/>    # implement gradient descent<br/>    for i in range(num_iterations):<br/>        # Initialize dropout matrices<br/>        D = drop_out_matrices(layers_dims, m, keep_prob)<br/><br/>        # compute forward propagation<br/>        AL, caches = L_model_forward(<br/>            X, parameters, D, keep_prob, hidden_layers_activation_fn)<br/><br/>        # compute regularized cost<br/>        cost = compute_cost(AL, Y)<br/><br/>        # compute gradients<br/>        grads = L_model_backward(<br/>            AL, Y, caches, D, keep_prob, hidden_layers_activation_fn)<br/><br/>        # update parameters<br/>        parameters = update_parameters(parameters, grads, learning_rate)<br/><br/>        # print cost<br/>        if (i + 1) % 100 == 0 and print_cost:<br/>            print(f"The cost after {i + 1} iterations : {cost:.4f}.")<br/>        # append cost<br/>        if i % 100 == 0:<br/>            cost_list.append(cost)<br/><br/>    # plot the cost curve<br/>    plt.plot(cost_list)<br/>    plt.xlabel("Iteration (per hundreds)")<br/>    plt.ylabel("Cost")<br/>    plt.title(f"Cost curve for the learning rate = {learning_rate}")<br/><br/>    return parameters</span></pre><p id="8beb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，我们准备好建立我们的神经网络。首先，我们将建立一个完全连接的网络，没有掉线。也就是说，<code class="fe ll lm ln lo b">keep_prob = 1</code>。接下来，我们将建立另一个网络。最后，我们将比较两个网络的泛化误差，并看看丢弃技术如何帮助我们改善泛化误差。</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="7da9" class="lx ly iq lo b gy lz ma l mb mc"># setup layers dimensions, number of examples, and keep probabilities list<br/>m = X_train.shape[0]<br/>keep_prob = [1, 1, 1, 1]<br/>layers_dims = [m, 10, 10, 1]<br/><br/># train NN with no dropout<br/>parameters = model_with_dropout(X_train, Y_train, layers_dims,      keep_prob=keep_prob, learning_rate=0.03, num_iterations=1000, hidden_layers_activation_fn="relu")<br/><br/># print the test accuracy<br/>print("The training accuracy rate: {}".format(accuracy(X_train, parameters, Y_train, "relu")[-7:]))<br/>print("The test accuracy rate: {}".format(accuracy(X_test, parameters, Y_test, "relu")[-7:]))</span><span id="b071" class="lx ly iq lo b gy md ma l mb mc">The cost after 100 iterations : 0.6555.<br/>The cost after 200 iterations : 0.6468.<br/>The cost after 300 iterations : 0.6447.<br/>The cost after 400 iterations : 0.6442.<br/>The cost after 500 iterations : 0.6440.<br/>The cost after 600 iterations : 0.6440.<br/>The cost after 700 iterations : 0.6440.<br/>The cost after 800 iterations : 0.6440.<br/>The cost after 900 iterations : 0.6440.<br/>The cost after 1000 iterations : 0.6440.<br/>The training accuracy rate: 65.55%.<br/>The test accuracy rate: 34.00%.</span></pre><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/e85e5e3e71c2f4a588d5ad482f92ca0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*gzLgnh7Fmz3CiGagLW4Mmw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk"><strong class="bd jy">Figure 3:</strong> Cost curve with no dropout</figcaption></figure><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="cdb8" class="lx ly iq lo b gy lz ma l mb mc"># setup keep probabilities list<br/>keep_prob = [1, 0.5, 0.5, 1]<br/><br/># train NN with no dropout<br/>parameters = model_with_dropout(X_train, Y_train, layers_dims,      keep_prob=keep_prob, learning_rate=0.03, num_iterations=1000, hidden_layers_activation_fn="relu")<br/><br/># print the test accuracy<br/>print("The training accuracy rate: {}".format(accuracy(X_train, parameters, Y_train, "relu")[-7:]))<br/>print("The test accuracy rate: {}".format(accuracy(X_test, parameters, Y_test, "relu")[-7:]))</span><span id="dae1" class="lx ly iq lo b gy md ma l mb mc">The cost after 100 iterations : 0.6555.<br/>The cost after 200 iterations : 0.6467.<br/>The cost after 300 iterations : 0.6445.<br/>The cost after 400 iterations : 0.6437.<br/>The cost after 500 iterations : 0.6412.<br/>The cost after 600 iterations : 0.6338.<br/>The cost after 700 iterations : 0.6108.<br/>The cost after 800 iterations : 0.5367.<br/>The cost after 900 iterations : 0.4322.<br/>The cost after 1000 iterations : 0.3114.<br/>The training accuracy rate: 74.16%.<br/>The test accuracy rate: 44.00%.</span></pre><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/a7cb99cc65a730756bd7a0a88c848644.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*8ojJ-qcm86mpMXLgyA2wGg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk"><strong class="bd jy">Figure 4:</strong> Cost curve with dropout</figcaption></figure><p id="312f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">以上结果表明，有丢包的网络测试准确率提高了 30%。请注意，这只是一个说明性的例子，以显示辍学技术的有效性。在这个例子中，我们选择了任意的概率；然而，我们可以调整每一层上的丢弃概率，以产生最佳的验证损失和准确性。</p><h1 id="e12c" class="mg ly iq bd mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc bi translated">结论</h1><p id="712d" class="pw-post-body-paragraph jz ka iq kb b kc nd ke kf kg ne ki kj kk nf km kn ko ng kq kr ks nh ku kv kw ij bi translated">Dropout 是一种非常有效的正则化技术，在<em class="ni">卷积神经网络</em>中被大量使用。以下是一些要点:</p><ul class=""><li id="0dfe" class="kx ky iq kb b kc kd kg kh kk kz ko la ks lb kw lc ld le lf bi translated">使用梯度检测时设置<code class="fe ll lm ln lo b">keep_prob = 1</code>；不然就不行了。</li><li id="cf31" class="kx ky iq kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated">辍学仅在培训期间使用。不要在测试/预测新示例时使用它。</li><li id="5a0a" class="kx ky iq kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><code class="fe ll lm ln lo b">keep_prob</code>越低→神经网络越简单。随着<code class="fe ll lm ln lo b">keep_prob</code>的减少，偏差增加，方差减少。因此，具有更多神经元的层预期具有较低的<code class="fe ll lm ln lo b">keep_prob</code>以避免过度拟合。</li><li id="cf18" class="kx ky iq kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated">在计算上，这是一种改善泛化误差和帮助解决过度拟合的廉价方法。</li><li id="e0b0" class="kx ky iq kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated">人们可以调整<code class="fe ll lm ln lo b">keep_prob</code>来获得手头任务的最佳结果。</li></ul><p id="75bb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">创建这篇文章的源代码可以在<a class="ae nj" href="https://github.com/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Dropout.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。帖子的灵感来自 deeplearning.ai 课程。</p><p id="895b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="ni">原载于 2018 年 5 月 20 日</em><a class="ae nj" href="https://imaddabbura.github.io/posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html" rel="noopener ugc nofollow" target="_blank"><em class="ni">imaddabbura . github . io</em></a><em class="ni">。</em></p></div></div>    
</body>
</html>