<html>
<head>
<title>Building client routing / semantic search in the wild</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在野外构建客户端路由/语义搜索</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-client-routing-semantic-search-in-the-wild-14db04687c7e?source=collection_archive---------16-----------------------#2018-11-10">https://towardsdatascience.com/building-client-routing-semantic-search-in-the-wild-14db04687c7e?source=collection_archive---------16-----------------------#2018-11-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/eb47bf0a6a4abe0ace0eb4c172629b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o-bBbOMYF2faGfKopPggKQ.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">Main objective</strong> — help our clients find what they want, ideally even <strong class="bd kc">before </strong>they type the whole query. Search also should generalize reasonably well (synonyms / forms of known words, Russian has rich morphology)</figcaption></figure><h1 id="4065" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">TLDR</h1><p id="74b1" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这是一份关于我们在语义工厂项目中的<a class="ae lz" href="http://profi.ru/" rel="noopener ugc nofollow" target="_blank"> Profi.ru </a> DS 部门(Profi.ru 是 CIS 地区领先的在线服务市场之一)在大约 2 个月的时间里所做工作的执行摘要。</p><p id="bbe7" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">这篇文章主要集中在比较新的 NLP 技术在应用商业环境中的适用性，如果你计划在你的公司/项目中做类似的事情，它可以作为一个指南。</p><p id="0c9b" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><strong class="ld ir">简而言之，我们有两个目标:</strong></p><ul class=""><li id="9862" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly mk ml mm mn bi translated">使 Profi.ru 上主搜索栏内的搜索/路由更好(<strong class="ld ir">监督任务</strong>)；</li><li id="de3c" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">这样做——开发无监督的方法，以有效地<strong class="ld ir">在第三方数据中搜索新服务</strong>(<strong class="ld ir">无监督任务</strong>)；</li></ul><p id="ab92" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">我们考虑了许多架构，但最终一个具有嵌入包层而不是普通嵌入的双 LSTM(或双 GRU)被证明是监督和非监督任务的最佳<strong class="ld ir">(见下图)。值得注意的是，使模型对部分输入/有误差的输入具有鲁棒性<strong class="ld ir">提高了其性能</strong>。此外，添加第二个目标(假“服务”检测)也略微提高了准确性<strong class="ld ir">。像 transformer 这样更复杂的模型在我们的领域中没有显示出真正的好处。</strong></strong></p><h1 id="8b6c" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">现有数据</h1><p id="9639" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们使用的<strong class="ld ir">内部数据</strong>来自几个主要来源:</p><ul class=""><li id="78b7" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly mk ml mm mn bi translated"><strong class="ld ir">实际客户端搜索</strong>(每 3-6 个月 30k 个唯一查询)+手动注释(这产生了我们的主开发集)；</li><li id="d78e" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">内部<strong class="ld ir">同义词库</strong> (30k 唯一同义词)<strong class="ld ir">；</strong></li><li id="faf3" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">内部营销<strong class="ld ir"> SEO 数据库</strong>带注释(300k)；</li></ul><p id="ddaa" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">在我们之前，公司里没有人建立 ML 模型，所以我们必须在获取数据和管理数据注释时随机应变。还应该注意的是，尽管查询的总数很大，但唯一的查询计数却非常低，这迫使我们在获取/扩充数据时要有创造性。此外，手头的数据集和注释的质量差异很大。正如所料，最好的监督数据集是人工注释的数据集和同义词数据库。我们还尝试探索转换管道(即查询—会话—顺序)并提取以前的分类器结果—预期性能不佳。</p><p id="8fc8" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">关于<strong class="ld ir">外部数据</strong>，我们使用了以下来源:</p><ul class=""><li id="c767" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly mk ml mm mn bi translated">搜索引擎数据(Google 和 Yandex 的 SEO 小组及其统计服务提供的数据)(<strong class="ld ir"> 11m </strong>查询)；</li><li id="e491" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">外部 SEO 服务(<strong class="ld ir"> 3m </strong>查询)；</li><li id="7045" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">我们竞争对手的公开数据(<strong class="ld ir"> 1.5m </strong>查询)；</li></ul><h1 id="f0c2" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">实现的目标</h1><p id="554b" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="ld ir">业务目标:</strong></p><ol class=""><li id="30d3" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly mt ml mm mn bi translated"><code class="fe mu mv mw mx b">88+%</code>(相对于具有弹性搜索的<code class="fe mu mv mw mx b">60%</code>)在客户端路由/意图分类上的准确性(~ <code class="fe mu mv mw mx b">5k</code>类)；</li><li id="da3d" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">搜索不知道输入质量(印刷错误/部分输入)；</li><li id="1492" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">分类器一般化，语言的形态学结构被开发；</li><li id="3ee9" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">为了安全起见——至少发现了<code class="fe mu mv mw mx b">1,000</code>个新服务+至少<code class="fe mu mv mw mx b">15,000</code>个同义词(相对于<code class="fe mu mv mw mx b">5,000</code> + ~ <code class="fe mu mv mw mx b">30,000</code>的当前状态)。我预计这个数字会翻一倍甚至三倍；</li></ol><p id="7b4a" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><strong class="ld ir">“科学”目标:</strong></p><ol class=""><li id="013c" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly mt ml mm mn bi translated">我们使用下游分类任务+ KNN 和服务同义词数据库彻底比较了许多现代句子嵌入技术；</li><li id="ad97" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">我们使用<strong class="ld ir">非监督</strong>方法，在基准测试中成功击败了弱监督(本质上，他们的分类器是一袋 ngrams)弹性搜索(详见下文);</li><li id="86f0" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">我们开发了一种构建应用 NLP 模型的新方法(一个普通的双 LSTM +嵌入包，本质上是快速文本符合 RNN)——这考虑到了俄语的形态学，并且推广得很好；</li><li id="cc74" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">我们证明了我们的最终嵌入技术(来自最佳分类器的瓶颈层)结合最先进的无监督算法(UMAP + HDBSCAN)可以在外部数据上产生恒星簇；</li><li id="f4a7" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">我们在实践中证明了以下方法的可能性、可行性和可用性:(1)知识提炼(2)文本数据扩充(原文如此！);</li><li id="0930" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">与生成更大的静态数据集相比，使用动态增强来训练基于文本的分类器大大减少了收敛时间(10 倍)(即，CNN 学习概括错误，显示大大减少增强的句子)；</li></ol><h1 id="965f" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">现在 NLP 里什么管用？</h1><p id="fba3" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">鸟瞰 NLP 景观:</p><figure class="mz na nb nc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi my"><img src="../Images/dda2819e1140d33123cc38613df986b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yEUSkCgVmkRepL4p.png"/></div></div></figure><p id="804e" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">此外，您可能知道 NLP 现在可能正在经历<a class="ae lz" href="https://thegradient.pub/nlp-imagenet/" rel="noopener ugc nofollow" target="_blank"> Imagenet 时刻</a>。</p><h1 id="6abd" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">大规模 UMAP 黑客攻击</h1><p id="99cd" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在构建集群时，我们偶然发现了一种方法/技巧，可以将 UMAP 应用于 1 亿多点(甚至 10 亿)大小的数据集。本质上是用 FAISS 构建一个 KNN 图，然后用你的 GPU 将主 UMAP 循环改写成 PyTorch。我们不需要这个，并放弃了这个概念(我们毕竟只有 1000-1500 万点)，但请按照这个<a class="ae lz" href="https://github.com/lmcinnes/umap/issues/125" rel="noopener ugc nofollow" target="_blank">线程</a>了解详情。</p><h1 id="4b43" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">什么最有效</h1><ul class=""><li id="c3da" class="mf mg iq ld b le lf li lj lm nd lq ne lu nf ly mk ml mm mn bi translated">对于监督分类，快速文本满足 RNN(双 LSTM) +精心选择的 n 元文法集；</li><li id="4bb6" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">实现——普通 python 为<a class="ae lz" href="https://t.me/snakers4/2137" rel="noopener ugc nofollow" target="_blank"> n-grams </a> + PyTorch 嵌入包层；</li><li id="133d" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">对于集群——该模型的瓶颈层+UMAP+hdb scan；</li></ul><h1 id="839c" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">最佳分类器基准</h1><p id="28d6" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="ld ir">手动注释开发集</strong></p><figure class="mz na nb nc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/11505956aa203cb9030bd7361cca2f8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FukR_6L2dfnFHQbWzoOkGw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Performance of the best model (n-gram bag + biLSTM) on manually annotated dev-set</figcaption></figure><p id="cce5" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><strong class="ld ir">手动注释开发集+每个查询 1-3 个错别字</strong></p><figure class="mz na nb nc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/8e838fa781a6f8453e8e2c96afc4d2fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ynrY53UQT55XJ9s3sVRriw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Performance of the best model (n-gram bag + biLSTM) on manually annotated dev-set, where we added 1 to 3 typos to each query</figcaption></figure><p id="9988" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><strong class="ld ir">手动注释开发集+部分输入</strong></p><figure class="mz na nb nc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/9b3bbdc4d9da83297f93ae698543280a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a9tjlA21N7JodWPWTLdotg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Performance of the best model (n-gram bag + biLSTM) on manually annotated dev-set, where we randomly cut the input sequence to simulate partial input</figcaption></figure><h1 id="7fbe" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">大规模语料库/ n-gram 选择</h1><p id="aabf" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们收集了最大的俄语语料库:</p><ul class=""><li id="12e4" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly mk ml mm mn bi translated">我们用<a class="ae lz" href="https://t.me/snakers4/2147" rel="noopener ugc nofollow" target="_blank"> 1TB 抓取</a>收集了一个<code class="fe mu mv mw mx b">100m</code>单词字典；</li><li id="150e" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">也用这个<a class="ae lz" href="https://t.me/snakers4/2148" rel="noopener ugc nofollow" target="_blank">黑</a>更快下载这类文件(隔夜)；</li><li id="1772" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">我们为我们的分类器选择了一组最佳的<code class="fe mu mv mw mx b">1m</code> n-grams，以进行最佳概括(来自俄语维基百科上训练的快速文本的<code class="fe mu mv mw mx b">500k</code>最流行的 n-grams+<code class="fe mu mv mw mx b">500k</code>我们领域数据上最流行的 n-grams)；</li></ul><p id="e8c2" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><strong class="ld ir">我们的 100 万词汇上的 100 万 n-grams 的压力测试:</strong></p><figure class="mz na nb nc gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/3ae2f129833d3e59c61c3b20b4c4425e.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/0*ylTEeTIy3rXJ3-LG.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Share of n-grams (3–7 grams) in a 100M word vocabulary covered by our n-gram selection</figcaption></figure><h1 id="97f0" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">文本扩充</h1><p id="be6a" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">简而言之:</p><ul class=""><li id="13aa" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly mk ml mm mn bi translated">拿一本有错误的大字典(例如 10-100 万个独特的单词)；</li><li id="fc3b" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">产生一个错误(丢弃一个字母，使用计算的概率交换一个字母，插入一个随机的字母，可能使用键盘布局等)；</li><li id="0fc8" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">检查新单词是否在词典中；</li></ul><p id="d60d" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">我们对像<a class="ae lz" href="https://tech.yandex.ru/speller/" rel="noopener ugc nofollow" target="_blank">这个</a>这样的服务进行了大量的强制查询(试图从本质上对它们的数据集进行逆向工程)，它们内部有一个非常小的字典(这个服务也是由一个具有 n 元语法特征的树分类器支持的)。看到它们只覆盖了我们在一些语料库中的 30 %- 50%的单词有点好笑。</p><p id="40eb" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">如果你有大量的领域词汇，我们的方法要优越得多。</p><h1 id="1078" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">最佳无监督/半监督结果</h1><p id="421d" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">KNN 被用作比较不同嵌入方法的基准。这更多的是一个例子，但是显然我们使用这些方法来聚集/搜索新的服务。</p><p id="8179" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">(向量大小)<strong class="ld ir">测试模型列表:</strong></p><ul class=""><li id="41ae" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly mk ml mm mn bi translated">(512)在 200 GB 普通爬行数据上训练的大规模假句子检测器；</li><li id="edb3" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">(300)假句子检测器，被训练成从服务中辨别来自维基百科的随机句子；</li><li id="0445" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">(300)从这里获得的快速文本，在 araneum 语料库上进行预训练；</li><li id="0dc2" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">(200)在我们的领域数据上训练的快速文本；</li><li id="9c9b" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">(300)在 200GB 的普通爬行数据上训练的快速文本；</li><li id="69a3" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">(300)用来自维基百科的服务/同义词/随机句子训练的三联体丢失的连体网络；</li><li id="da60" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">(200)嵌入包 RNN 的嵌入层的第一次迭代，句子被编码为整个嵌入包；</li><li id="b37b" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">(200)相同，但首先将句子拆分成单词，然后嵌入每个单词，然后取普通平均值；</li><li id="df8a" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">(300)同上，但用于最终模型；</li><li id="560e" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">(300)同上，但用于最终模型；</li><li id="612f" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">(250)最终模型的瓶颈层(250 个神经元)；</li><li id="fcf1" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">弱监督弹性搜索基线；</li></ul><figure class="mz na nb nc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/a07a9d65b828b007b1cdf2f90ff286a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fiG8ZgYLfZiYAkJX.png"/></div></div></figure><p id="b9b8" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">为了避免泄密，所有的随机句子都是随机抽样的。他们的单词长度与他们所比较的服务/同义词的长度相同。此外，还采取措施确保模型不只是通过分离词汇来学习(嵌入被冻结，维基百科被欠采样以确保每个维基百科句子中至少有一个领域词)。</p><h1 id="625d" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">集群可视化</h1><p id="b605" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">最佳嵌入技术+ UMAP 如何在一个外部语料库上工作的例子。</p><p id="462a" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><strong class="ld ir"> 3D </strong></p><figure class="mz na nb nc gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/4e4beacd80b03c72dc3e3345edc20872.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/0*AP0x4GfQ-8Vg2iPR.gif"/></div></figure><p id="9755" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><strong class="ld ir"> 2D </strong></p><figure class="mz na nb nc gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/a68fcc5f633fb64abff150619a38df48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/0*1RITNiJfNCOGIFK_.PNG"/></div></figure><h1 id="e301" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">集群探索“界面”</h1><p id="78e4" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">绿色—新单词/同义词。灰色背景-可能是新词。</p><p id="7221" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated">灰色文本-现有同义词。</p><figure class="mz na nb nc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nn"><img src="../Images/4a7ce010bb5209f3425031d750d2d2b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/0*PHcPJ48nLpR19s3I.jpg"/></div></div></figure><h1 id="010e" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">消融试验和哪些有效，我们尝试了哪些，哪些没有</h1><ol class=""><li id="5607" class="mf mg iq ld b le lf li lj lm nd lq ne lu nf ly mt ml mm mn bi translated">见以上图表；</li><li id="7285" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">快速文本嵌入的普通平均值/tf-idf 平均值——一个<strong class="ld ir">非常强大的基线</strong>；</li><li id="caa9" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">俄语的 fast-text &gt; word 2 vec；</li><li id="4aaf" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">通过伪句子检测的句子嵌入类工作，但是与其他方法相比相形见绌；</li><li id="c6e2" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">BPE(句子)显示没有改善我们的领域；</li><li id="b0f9" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">尽管谷歌撤回了论文，但 Char 级模型很难推广；</li><li id="dab0" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">我们尝试了多头 transformer(带有分类器和语言建模头)，但是在手边可用的注释上，它的性能与普通的基于 LSTM 的模型大致相同。当我们迁移到嵌入 bad 方法时，我们放弃了这一研究方向，因为变压器的实用性较低，并且具有 LM 头和嵌入包层是不切实际的；</li><li id="29d8" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">伯特(BERT)——似乎有些夸张，也有人声称变形金刚实际上训练了几个星期；</li><li id="96fe" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated"><strong class="ld ir">ELMO</strong>——在我看来，无论是在研究/生产还是教育环境中，使用像 AllenNLP 这样的库似乎都是适得其反的，原因我就不在这里说了；</li></ol><h1 id="b683" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">部署</h1><p id="bba9" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">使用完成:</p><ul class=""><li id="2dfe" class="mf mg iq ld b le ma li mb lm mh lq mi lu mj ly mk ml mm mn bi translated">带有简单 web 服务的 Docker 容器；</li><li id="5bf2" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">CPU-仅用于推断就足够了；</li><li id="2247" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">~ <code class="fe mu mv mw mx b">2.5 ms</code>对于 CPU 上的每个查询，批处理并不是真正必要的；</li><li id="e99e" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">~ <code class="fe mu mv mw mx b">1GB</code> RAM 内存占用量；</li><li id="b689" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">几乎没有依赖，除了<code class="fe mu mv mw mx b">PyTorch</code>、<code class="fe mu mv mw mx b">numpy</code>、<code class="fe mu mv mw mx b">pandas</code>(还有 web 服务器 ofc)。</li><li id="58c8" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">模仿像<a class="ae lz" href="https://t.me/snakers4/2137" rel="noopener ugc nofollow" target="_blank">这样的快速文本 n-gram 生成；</a></li><li id="8ac3" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mk ml mm mn bi translated">嵌入刚刚存储在字典中的包层+索引；</li></ul><h1 id="8d62" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">结论</strong></h1><p id="c15b" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们已经表明，您可以应用相对简单/现成的工具来解决真实业务中语义搜索框架内的监督和非监督挑战。</p><h1 id="1fef" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">进一步阅读</h1><ol class=""><li id="721c" class="mf mg iq ld b le lf li lj lm nd lq ne lu nf ly mt ml mm mn bi translated">更详细的俄语<a class="ae lz" href="http://resources.spark-in.me/profi_2018_11_search_prez.html" rel="noopener ugc nofollow" target="_blank">演示文稿</a>；</li><li id="89a0" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">解析<a class="ae lz" href="https://spark-in.me/post/parsing-common-crawl-in-four-simple-commands" rel="noopener ugc nofollow" target="_blank">普通爬虫</a>和<a class="ae lz" href="https://spark-in.me/post/parsing-wikipedia-in-four-commands-for-nlp" rel="noopener ugc nofollow" target="_blank">维基百科</a>；</li><li id="cf82" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated"><a class="ae lz" href="https://t.me/snakers4/2137" rel="noopener ugc nofollow" target="_blank">如何</a>在一个函数中模拟嵌入的快速文本包；</li><li id="dbd8" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">更多预处理<a class="ae lz" href="https://t.me/snakers4/2147" rel="noopener ugc nofollow" target="_blank">普通抓取</a>；</li><li id="de37" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">快速文本单词<a class="ae lz" href="https://fasttext.cc/docs/en/pretrained-vectors.html" rel="noopener ugc nofollow" target="_blank">向量</a>在维基百科上预先训练；</li><li id="993c" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">俄罗斯语言的快速文本和 Word2Vec <a class="ae lz" href="http://rusvectores.org/ru/models/" rel="noopener ugc nofollow" target="_blank">模型</a>；</li><li id="63eb" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">种子词嵌入论文:<a class="ae lz" href="http://arxiv.org/abs/1607.04606" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>，<a class="ae lz" href="http://arxiv.org/abs/1712.09405" rel="noopener ugc nofollow" target="_blank">快文</a>，进一步<a class="ae lz" href="http://arxiv.org/abs/1802.06893" rel="noopener ugc nofollow" target="_blank">调</a>；</li><li id="5484" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">当前一些基于 SOTA CNN 的方法:<a class="ae lz" href="http://arxiv.org/abs/1705.02364" rel="noopener ugc nofollow" target="_blank">推断</a>/CNN 的生成<a class="ae lz" href="https://blog.openai.com/language-unsupervised/" rel="noopener ugc nofollow" target="_blank">预训练</a>/<a class="ae lz" href="http://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">ul mfit</a>/深度语境化单词<a class="ae lz" href="http://arxiv.org/abs/1802.05365" rel="noopener ugc nofollow" target="_blank">表示</a>(Elmo)；</li><li id="9187" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">Imagenet 时刻在<a class="ae lz" href="https://thegradient.pub/nlp-imagenet/" rel="noopener ugc nofollow" target="_blank"> NLP </a>？</li><li id="c809" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">句子嵌入基线<a class="ae lz" href="https://openreview.net/pdf?id=SyK00v5xx" rel="noopener ugc nofollow" target="_blank"> 1 </a>、<a class="ae lz" href="http://nlp.town/blog/sentence-similarity/" rel="noopener ugc nofollow" target="_blank"> 2 </a>、<a class="ae lz" href="https://arxiv.org/abs/1806.06259" rel="noopener ugc nofollow" target="_blank"> 3 </a>、<a class="ae lz" href="http://www.offconvex.org/2018/09/18/alacarte/" rel="noopener ugc nofollow" target="_blank">4</a>；</li><li id="7352" class="mf mg iq ld b le mo li mp lm mq lq mr lu ms ly mt ml mm mn bi translated">假句子<a class="ae lz" href="https://arxiv.org/abs/1808.03840" rel="noopener ugc nofollow" target="_blank">检测</a>作为句子编码的训练任务；</li></ol></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><p id="2c27" class="pw-post-body-paragraph lb lc iq ld b le ma lg lh li mb lk ll lm mc lo lp lq md ls lt lu me lw lx ly ij bi translated"><em class="nv">原载于 2018 年 11 月 2 日</em><a class="ae lz" href="https://spark-in.me/post/profi-ru-semantic-search-project" rel="noopener ugc nofollow" target="_blank"><em class="nv">spark-in . me</em></a><em class="nv">。</em></p></div></div>    
</body>
</html>