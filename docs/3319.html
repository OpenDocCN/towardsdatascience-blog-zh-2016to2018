<html>
<head>
<title>Speeding up Convolutional Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">加速卷积神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speeding-up-convolutional-neural-networks-240beac5e30f?source=collection_archive---------2-----------------------#2018-05-02">https://towardsdatascience.com/speeding-up-convolutional-neural-networks-240beac5e30f?source=collection_archive---------2-----------------------#2018-05-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/c1fa0ba827164d3a9cf741fd19955e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*soJ0CXaNaS2XOVYLclpwjw.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Source: <a class="ae jd" href="http://yesofcorsa.com/wp-content/uploads/2017/04/High-Speed-Photo.jpg" rel="noopener ugc nofollow" target="_blank">http://yesofcorsa.com/wp-content/uploads/2017/04/High-Speed-Photo.jpg</a></figcaption></figure><div class=""/><p id="7256" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">加速卷积神经网络训练而不显著影响精度的方法综述。</p><p id="3e7e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有趣的是，完全连接的层是神经网络占用大量内存的主要原因，但速度很快，而卷积虽然参数数量很少，但却消耗了大部分计算能力。实际上，卷积是如此的计算饥渴，以至于它们是我们需要如此多的计算能力来训练和运行最先进的神经网络的主要原因。</p><blockquote class="lb"><p id="39a4" class="lc ld jg bd le lf lg lh li lj lk la dk translated">我们能设计出既快速又高效的卷积吗？</p></blockquote><p id="cbbc" class="pw-post-body-paragraph kd ke jg kf b kg lm ki kj kk ln km kn ko lo kq kr ks lp ku kv kw lq ky kz la ij bi translated">某种程度上——是的！</p><p id="9055" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有一些方法可以在不严重降低模型精度的情况下加速卷积。在这篇博文中，我们将考虑以下方法。</p><ul class=""><li id="3cb6" class="lr ls jg kf b kg kh kk kl ko lt ks lu kw lv la lw lx ly lz bi translated">卷积核的因子分解/分解</li><li id="f573" class="lr ls jg kf b kg ma kk mb ko mc ks md kw me la lw lx ly lz bi translated">瓶颈层</li><li id="a711" class="lr ls jg kf b kg ma kk mb ko mc ks md kw me la lw lx ly lz bi translated">更宽的回旋</li><li id="0909" class="lr ls jg kf b kg ma kk mb ko mc ks md kw me la lw lx ly lz bi translated">深度可分卷积</li></ul><p id="4099" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面，我将深入研究所有这些方法的实现和背后的原因。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="b206" class="mm mn jg bd mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj bi translated">简单因式分解</h1><p id="ca6d" class="pw-post-body-paragraph kd ke jg kf b kg nk ki kj kk nl km kn ko nm kq kr ks nn ku kv kw no ky kz la ij bi translated">让我们从NumPy中的以下示例开始</p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="2939" class="ny mn jg nu b gy nz oa l ob oc">&gt;&gt;&gt; from numpy.random import random<br/>&gt;&gt;&gt; random((3, 3)).shape == (random((3, 1)) * random((1, 3))).shape<br/>&gt;&gt;&gt; True</span></pre><p id="dde7" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可能会问，为什么我要给你看这个愚蠢的片段？答案是，它表明你可以写一个NxN矩阵，把卷积核想象成2个较小的矩阵/核的乘积，形状为Nx1和1xN。回想一下，卷积运算需要<code class="fe od oe of nu b">in_channels * n * n * out_channels </code>参数或权重。此外，请记住，每个重量/参数都需要激活。因此，参数数量的任何减少都将减少所需的操作数量和计算成本。</p><p id="0de1" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设卷积运算实际上是使用张量乘法来完成的，而张量乘法是多项式的，依赖于张量的大小，正确应用因式分解应该会产生明显的加速。</p><p id="1287" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在喀拉斯，它看起来像这样:</p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="dc03" class="ny mn jg nu b gy nz oa l ob oc"># k - kernel size, for example 3, 5, 7...<br/># n_filters - number of filters/channels<br/># Note that you shouldn't apply any activation<br/># or normalization between these 2 layers<br/>fact_conv1 = Conv(n_filters, (1, k))(inp)<br/>fact_conv1 = Conv(n_filters, (k, 1))(fact_conv1)</span></pre><p id="0cff" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不过，请注意，不建议使用最接近输入卷积层的因子。此外，分解3x3卷积甚至会损害网络性能。最好为更大的内核保留它们。</p><p id="50cb" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们深入这个主题之前，有一个更稳定的方法来分解大内核:只是堆叠较小的内核。例如，不使用5x5卷积，而是堆叠两个3x3卷积，或者如果您想要替换7x7内核，则堆叠3个卷积。有关更多信息，请参见[4]。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="e436" class="mm mn jg bd mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj bi translated">瓶颈层</h1><p id="15ed" class="pw-post-body-paragraph kd ke jg kf b kg nk ki kj kk nl km kn ko nm kq kr ks nn ku kv kw no ky kz la ij bi translated">瓶颈层背后的主要思想是通过减少输入通道的数量(也称为输入张量的深度)来减少内核大于1×1的卷积层中的输入张量的大小。</p><p id="d6f5" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是它的Keras代码:</p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="3d21" class="ny mn jg nu b gy nz oa l ob oc">from keras.layers import Conv2D</span><span id="13c4" class="ny mn jg nu b gy og oa l ob oc"># given that conv1 has shape (None, N, N, 128)</span><span id="b57e" class="ny mn jg nu b gy og oa l ob oc">conv2 = Conv2D(96, (1, 1), ...)(conv1) # squeeze<br/>conv3 = Conv2D(96, (3, 3), ...)(conv2) # map<br/>conv4 = Conv2D(128, (1, 1), ...)(conv3) # expand</span></pre><p id="8865" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">几乎所有的CNN，从革命性的概念1到现代的DenseNet，都在以这样或那样的方式使用瓶颈层。这种技术有助于保持参数的数量，从而降低计算成本。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="c6c2" class="mm mn jg bd mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj bi translated">更宽的回旋</h1><p id="1616" class="pw-post-body-paragraph kd ke jg kf b kg nk ki kj kk nl km kn ko nm kq kr ks nn ku kv kw no ky kz la ij bi translated">另一种加速卷积的简单方法是所谓的宽卷积层。你看，你的模型卷积层数越多，速度就越慢。然而，你需要大量卷积的表示能力。你是做什么的？你使用更少但更胖的层，其中脂肪意味着每层更多的果仁。为什么有效？因为对于GPU或其他大规模并行机器来说，处理单个大块数据比处理大量较小的数据更容易。更多信息可以在[6]中找到。</p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="7401" class="ny mn jg nu b gy nz oa l ob oc"># convert from<br/>conv = Conv2D(96, (3, 3), ...)(conv)<br/>conv = Conv2D(96, (3, 3), ...)(conv)<br/># to<br/>conv = Conv2D(128, (3, 3), ...)(conv)<br/># roughly, take the sqrt of the number of layers you want<br/># to merge and multipy the number to<br/># the number of filters/channels in the initial convolutions<br/># to get the number of filters/channels in the new layer</span></pre></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="eca2" class="mm mn jg bd mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj bi translated">深度可分卷积</h1><p id="7860" class="pw-post-body-paragraph kd ke jg kf b kg nk ki kj kk nl km kn ko nm kq kr ks nn ku kv kw no ky kz la ij bi translated">在深入研究这种方法之前，要知道它非常依赖于可分离卷积在给定框架中的实现方式。就我而言，TensorFlow可能会对这种方法进行一些特定的优化，而对于其他后端，如Caffe、CNTK或PyTorch，还不清楚。</p><figure class="np nq nr ns gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/2e1d49d653ceaa93352ab4fa6c6bd9a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odJXfzodb02HDnKy27yfpQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Vincent Vanhoucke, April 2014, “Learning Visual Representations at Scale”</figcaption></figure><p id="131a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个想法是，不是在图像的所有通道上联合卷积，而是在每个深度为<code class="fe od oe of nu b">channel_multiplier</code>的通道上运行单独的2D卷积。<code class="fe od oe of nu b">in_channels * channel_multiplier</code>个中间通道连接在一起，并使用1x1卷积映射到<code class="fe od oe of nu b">out_channels</code>。[5]这样一来，需要训练的参数就少得多。[2]</p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="f1d7" class="ny mn jg nu b gy nz oa l ob oc"># in Keras<br/>from keras.layers import SeparableConv2D<br/>...<br/>net = SeparableConv2D(32, (3, 3))(net)<br/>...<br/># it's almost 1:1 similar to the simple Keras Conv2D layer</span></pre><p id="8317" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事情没那么简单。请注意，可分卷积有时不是训练。在这种情况下，将深度倍增从1修改为4或8。还要注意，这些算法在小数据集上效率不是很高，比如CIFAR 10，此外还有MNIST。另一件要记住的事情是，不要在网络的早期阶段使用可分卷积。</p><figure class="np nq nr ns gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/5274f4511770b66d1d026fdcdba8da2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FjzcTRoe-R680V0hOwYo5A.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Source: V. Lebedev et al, Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition</figcaption></figure><h1 id="99d2" class="mm mn jg bd mo mp oj mr ms mt ok mv mw mx ol mz na nb om nd ne nf on nh ni nj bi translated">CP分解和高级方法</h1><p id="a0dc" class="pw-post-body-paragraph kd ke jg kf b kg nk ki kj kk nl km kn ko nm kq kr ks nn ku kv kw no ky kz la ij bi translated">上述因式分解方案在实践中工作良好，但是非常简单。它们可以工作，但是还没有达到极限。有许多工作，包括V. Lebedev等人的[3],向我们展示了不同的张量分解方案，这些方案大大减少了参数的数量，从而减少了所需的计算量。</p><p id="acc5" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">受[1]启发，下面是如何在Keras中进行CP分解的代码片段:</p><pre class="np nq nr ns gt nt nu nv nw aw nx bi"><span id="8aef" class="ny mn jg nu b gy nz oa l ob oc"># **kwargs - anything valid for Keras layers,<br/># like regularization, or activation function<br/># Though, add at your own risk</span><span id="72fa" class="ny mn jg nu b gy og oa l ob oc"># Take a look into how ExpandDimension and SqueezeDimension<br/># are implemented in the associated Colab Notebook<br/># at the end of the article</span><span id="2bb1" class="ny mn jg nu b gy og oa l ob oc">first = Conv2D(rank, kernel_size=(1, 1), **kwargs)(inp)<br/>expanded = ExpandDimension(axis=1)(first)<br/>mid1  = Conv3D(rank, kernel_size=(d, 1, 1), **kwargs)(exapanded)<br/>mid2  = Conv3D(rank, kernel_size=(1, d, 1), **kwargs)(mid1)<br/>squeezed = SqueezeDimension(axis=1)(mid2)<br/>last  = Conv2D(out,  kernel_size=(1, 1), **kwargs)(squeezed)</span></pre><p id="4c89" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">遗憾的是，它不起作用，但是它给了你在代码中应该是什么样子的直觉。顺便说一下，文章顶部的图片是CP分解如何工作的图形解释。</p><p id="992a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">应该注意诸如TensorTrain分解和Tucker这样的方案。对于PyTorch和NumPy，有一个名为<a class="ae jd" href="http://tensorly.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Tensorly </a>的很棒的库，它为你做所有的底层实现。在TensorFlow中没有与之接近的东西，但还是有一个TensorTrain(又名TT方案)的实现，这里是<a class="ae jd" href="https://github.com/Bihaqo/t3f" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="c5b8" class="mm mn jg bd mo mp oj mr ms mt ok mv mw mx ol mz na nb om nd ne nf on nh ni nj bi translated">收场白</h1><p id="9b87" class="pw-post-body-paragraph kd ke jg kf b kg nk ki kj kk nl km kn ko nm kq kr ks nn ku kv kw no ky kz la ij bi translated">完整的代码目前可以作为一个带有特斯拉K80 GPU加速器的<a class="ae jd" href="https://colab.research.google.com/drive/1i0Fwh-d8kF05o4QRfJG5dZt_P7G85MCS" rel="noopener ugc nofollow" target="_blank">合作笔记本</a>获得。给自己做一份拷贝，享受修改代码的乐趣。</p><p id="47eb" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你正在读这篇文章，我想感谢你，并希望上面写的对你有很大的帮助，就像对我一样。请在评论区让我知道你的想法。你的反馈对我很有价值。</p><p id="3ca0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另外，如果你喜欢这篇文章，别忘了鼓掌😏或者关注我更多类似的文章。</p><h1 id="a20d" class="mm mn jg bd mo mp oj mr ms mt ok mv mw mx ol mz na nb om nd ne nf on nh ni nj bi translated">参考</h1><p id="86c3" class="pw-post-body-paragraph kd ke jg kf b kg nk ki kj kk nl km kn ko nm kq kr ks nn ku kv kw no ky kz la ij bi translated">[1]<a class="ae jd" href="https://medium.com/@krishnatejakrothapalli/hi-rain-4e76039423e2" rel="noopener">https://medium . com/@ krishnate jakrothapalli/hi-rain-4e 76039423 e 2</a><br/>【2】f . Chollet，Xception:具有深度方向可分离卷积的深度学习，<a class="ae jd" href="https://arxiv.org/abs/1610.02357v2" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1610.02357v2</a><br/>【3】v . lebe dev等人，使用微调CP分解加速卷积神经网络，<a class="ae jd" href="https://arxiv.org/abs/1412.6553" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1412.6553</a><br/>【4】c . Szegedy等人，重新思考Inception架构</p></div></div>    
</body>
</html>