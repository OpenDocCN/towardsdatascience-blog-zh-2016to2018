<html>
<head>
<title>Multimodal Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多模态深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multimodal-deep-learning-ce7d1d994f4?source=collection_archive---------3-----------------------#2018-12-18">https://towardsdatascience.com/multimodal-deep-learning-ce7d1d994f4?source=collection_archive---------3-----------------------#2018-12-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4e26" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用深度学习的多模态融合</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/f92625290776a343b62bc8801c9a7cd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*n5utRuo-cen8keYwVs2PiQ.png"/></div></figure><p id="4f72" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">由于对深度学习的研究充满热情，我一直在寻找该领域未被探索的领域(尽管很难找到)。我之前做过<a class="ae lj" href="https://medium.com/@purvanshimehta/can-computers-learn-maths-through-deep-learning-7ef714ca2cb3" rel="noopener"> <em class="lk">数学应用题</em> </a> <em class="lk"> </em>等许多类似的题目。</p><p id="e963" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">使用深度神经网络作为黑盒的挑战激起了我的兴趣。我决定更深入地研究“多模态深度学习中的可解释性”这个话题。以下是一些结果。</p><h2 id="2cd1" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kw lu lv lw la lx ly lz le ma mb mc md bi translated">多模态数据</h2><p id="5693" class="pw-post-body-paragraph kn ko iq kp b kq me jr ks kt mf ju kv kw mg ky kz la mh lc ld le mi lg lh li ij bi translated">我们对世界的体验是多模态的——我们看到物体，听到声音，感受纹理，闻到气味，品尝味道。模态是指事情发生或经历的方式，当一个研究问题包括多个这样的模态时，它就被称为多模态。为了让人工智能在理解我们周围的世界方面取得进展，它需要能够一起解释这种多模态信号。</p><p id="1836" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">比如<strong class="kp ir"> <em class="lk">图片通常会关联标签和文字说明；文本包含图像，以更清楚地表达文章的主要思想。</em> </strong>不同的模态以非常不同的统计特性为特征。</p><h2 id="8964" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kw lu lv lw la lx ly lz le ma mb mc md bi translated">多模态深度学习</h2><p id="1357" class="pw-post-body-paragraph kn ko iq kp b kq me jr ks kt mf ju kv kw mg ky kz la mh lc ld le mi lg lh li ij bi translated">虽然组合不同模态或类型的信息以提高性能看起来是直观上吸引人的任务，但是在实践中，组合不同水平的噪声和模态之间的冲突是具有挑战性的。此外，模态对预测输出具有不同的定量影响。实践中最常见的方法是将不同输入的高级嵌入连接起来，然后应用 softmax。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi mj"><img src="../Images/f98ae278d05c3bf6861c446385663dba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mcMzbAxZfENUbjmorZH6pQ.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Example of Multimodal deep learning where different types of NN are used to extract features</figcaption></figure><blockquote class="ms mt mu"><p id="c5c2" class="kn ko lk kp b kq kr jr ks kt ku ju kv mv kx ky kz mw lb lc ld mx lf lg lh li ij bi translated">这种方法的问题在于，它会对所有子网络/模态给予同等的重视，这在现实生活中是极不可能的。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi my"><img src="../Images/acb710f2f58357e201a0c0083e90f749.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wW-eQSP8LdnOPOR2Uuk4fg.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">All Modalities have an equal contribution towards prediction</figcaption></figure><h2 id="fd9e" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kw lu lv lw la lx ly lz le ma mb mc md bi translated"><strong class="ak">网络的加权组合</strong></h2><p id="1223" class="pw-post-body-paragraph kn ko iq kp b kq me jr ks kt mf ju kv kw mg ky kz la mh lc ld le mi lg lh li ij bi translated">我们采用子网络的加权组合，使得每个输入模态可以对输出预测具有学习贡献(θ)。</p><p id="650f" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们的优化问题变成了-</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/bdfa4d0b08b27aecaa4e1ac4b48f47f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*QJn-Me1ICcv27jE6VCGMHQ.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Loss Function after Theta weight is given to each sub-network.</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi na"><img src="../Images/9680c04049fd69642a5bf08b28a684d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DM3aXEELNeUlv-6nGjc4-w.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">The output is predicted after attaching weights to the subnetworks.</figcaption></figure><h2 id="d98e" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kw lu lv lw la lx ly lz le ma mb mc md bi translated">但是这一切的用处！！</h2><p id="c3f1" class="pw-post-body-paragraph kn ko iq kp b kq me jr ks kt mf ju kv kw mg ky kz la mh lc ld le mi lg lh li ij bi translated">言归正传，我开始吹嘘成果了。</p><p id="156e" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> <em class="lk">准确性和可解释性</em> </strong></p><p id="4600" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们在两个真实的多模态数据集上取得了最先进的结果</p><p id="5229" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><em class="lk">多模态情感强度语料库(MOSI)数据集— </em>每毫秒视频注释音频特征的注释数据集 417。总共有 2199 个带注释的数据点，其中情绪强度被定义为从强负到强正，线性标度从 3 到+3。</p><p id="54bc" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">模式是-</p><ol class=""><li id="fbdd" class="nb nc iq kp b kq kr kt ku kw nd la ne le nf li ng nh ni nj bi translated">文本</li></ol><p id="59b3" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">2.声音的</p><p id="1181" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">3.演讲</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/96937c441a1855078ed485fc920b8ab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*rUKoXeDdUj0KAKbTJ1wlIw.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Amount of contribution of each modality on sentiment prediction</figcaption></figure><p id="f669" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><em class="lk">转录起始位点预测(TSS)数据集— </em> <strong class="kp ir">转录</strong>是基因表达的第一步，其中 DNA 的特定片段被复制成 RNA (mRNA)。转录起始位点是转录开始的位置。DNA 片段的不同部分具有影响其存在的不同特性。我们把 TSS 分成三部分-</p><ol class=""><li id="e062" class="nb nc iq kp b kq kr kt ku kw nd la ne le nf li ng nh ni nj bi translated">上游 DNA</li><li id="e05e" class="nb nc iq kp b kq nl kt nm kw nn la no le np li ng nh ni nj bi translated">下游 DNA</li><li id="81d3" class="nb nc iq kp b kq nl kt nm kw nn la no le np li ng nh ni nj bi translated">TSS 区域</li></ol><p id="fefd" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们取得了前所未有的 3%的进步，超过了之前的最高水平。具有 TATA 盒的下游 DNA 区域对该过程影响最大。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3cfaa9a900ee5823e3eacf63a90065d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*whxIjPxjCN-GECklYEJI_w.png"/></div></figure><p id="7284" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们还对合成生成的数据进行了实验，以验证我们的理论。</p><p id="0050" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在，我们正在起草一篇论文，准备提交给一本 ML 期刊。</p><p id="4c07" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果你有兴趣了解多模态学习的数学细节或范围，一般来说，在 purvanshi.mehta11@gmail.com 上 ping 我。欢迎对作品提出意见。</p></div></div>    
</body>
</html>