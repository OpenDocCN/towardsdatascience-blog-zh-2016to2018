<html>
<head>
<title>Web Scraping the List of Greatest Video Games from Wikipedia</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从维基百科上抓取最棒的视频游戏列表</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/web-scraping-the-list-of-greatest-video-games-from-wikipedia-pt-1-230c64d93e8c?source=collection_archive---------3-----------------------#2017-03-17">https://towardsdatascience.com/web-scraping-the-list-of-greatest-video-games-from-wikipedia-pt-1-230c64d93e8c?source=collection_archive---------3-----------------------#2017-03-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="d4f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嗨，欢迎来到我的博客。</p><p id="8301" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">博客和数据科学(以及两者的结合)是我生活中相对较新的尝试，但伟大的事情往往来自卑微的开始。</p><p id="1680" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我学习python及其相关库的短暂时间里，我已经被自己学到的东西震惊了。为了努力工作并展示我不断发展的技能，我希望我的每一篇博文都可以作为我不断进步的标志。</p><p id="b0f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">撇开序言不谈，让我们进入这篇文章的实质内容。</p><p id="1e9e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作为对自己的最初挑战，我想尝试使用python进行web抓取，从网页中提取数据，并以一种允许进一步检查和可视化的方式对其进行格式化。维基百科似乎非常适合这项任务，因为它非常容易访问，并且包含大量有趣的数据。</p><p id="c73d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作为一个终生的电子游戏迷，我认为看看这个被认为是有史以来最好的电子游戏列表会很有趣。我正在研究的表格根据“有史以来最好的游戏”名单中的提及次数对视频游戏进行排序。</p><p id="2a7a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该表列出了1978年至2013年间发布的100款不同游戏，结构如下:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/bdd3374d178b3bb1b471ca570f7ea3e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q6f9spwFsnFFlFq-NV2TDA.png"/></div></div></figure><h1 id="4c5b" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">宝宝的第一次刮网</h1><p id="b597" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">接近网络抓取时，我发现<a class="ae kl" href="https://www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/" rel="noopener ugc nofollow" target="_blank">这个来自分析网站Vidhya的指南</a>非常有价值。我鼓励任何第一次接触网络搜索的人使用这个资源来收集他们的方位。</p><p id="eff0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在python中，我使用了两个python库:Urllib2和BeautifulSoup。</p><p id="0d6e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://docs.python.org/2/library/urllib2.html" rel="noopener ugc nofollow" target="_blank"> Urllib2 </a>帮助python获取URL，而<a class="ae kl" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank"> BeautifulSoup </a>使用HTML和XML文件从网页中提取信息。</p><p id="573b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着分析Vidhya指南的编码，我能够利用以下代码检索网页的HTML数据:</p><pre class="kn ko kp kq gt mb mc md me aw mf bi"><span id="2357" class="mg kz iq mc b gy mh mi l mj mk">#import library to query webpage of interest<br/>import urllib2</span><span id="b9c0" class="mg kz iq mc b gy ml mi l mj mk">#specifying page of interest<br/>wiki = “<a class="ae kl" href="https://en.wikipedia.org/wiki/List_of_video_games_considered_the_best" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/List_of_video_games_considered_the_best</a>"</span><span id="df80" class="mg kz iq mc b gy ml mi l mj mk">#save the HTML of the site within the page variable<br/>page = urllib2.urlopen(wiki)</span><span id="c751" class="mg kz iq mc b gy ml mi l mj mk">#import library to parse HTML from page<br/>from bs4 import BeautifulSoup</span><span id="a5ec" class="mg kz iq mc b gy ml mi l mj mk">#parse data from "page" and save to new variable "soup"<br/>soup = BeautifulSoup(page)</span></pre><p id="7b90" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该指南提供了一些额外的演示，说明如何检查HTML的结构，以及如何使用各种HTML标签从“soup”文件中返回感兴趣的信息。出于本文的目的，我不会深入讨论这个问题，但它有助于更好地理解如何阅读上述步骤的HTML输出。</p><p id="1936" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了从感兴趣的页面中提取适当的表数据，我需要确定表“class”。在Chrome浏览器中，我通过(右键单击-&gt; inspect)检查了我感兴趣的表，并确定该表属于“wikitable sortable”类型。页面本身特别将其标注为“wikitable sortable jquery-table sorter”，但是，soup文件只将其识别为“wiki table sortable”在对这个次要组件进行故障排除之后，我能够以下面的方式提取表信息:</p><pre class="kn ko kp kq gt mb mc md me aw mf bi"><span id="b53d" class="mg kz iq mc b gy mh mi l mj mk">#pinpointing the location of the table and its contents<br/>first_table = soup.find(“table”, class_ = “wikitable sortable”)</span><span id="edb8" class="mg kz iq mc b gy ml mi l mj mk">#creating lists for each of the columns I know to be in my table.<br/>A=[]<br/>B=[]<br/>C=[]<br/>D=[]<br/>E=[]<br/>F=[]</span><span id="815f" class="mg kz iq mc b gy ml mi l mj mk">#utilizing HTML tags for rows &lt;tr&gt; and elements &lt;td&gt; to iterate through each row of data and append data elements to their appropriate lists:</span><span id="51fc" class="mg kz iq mc b gy ml mi l mj mk">for row in first_table.findAll(“tr”):<br/>    cells = row.findAll(‘td’)<br/>    if len(cells)==6: #Only extract table body not heading<br/>        A.append(cells[0].find(text=True))<br/>        B.append(cells[1].find(text=True))<br/>        C.append(cells[2].find(text=True))<br/>        D.append(cells[3].find(text=True))<br/>        E.append(cells[4].find(text=True))<br/>        F.append(cells[5].find(text=True))</span></pre><p id="1c88" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我检查输出时，我注意到数据是unicode格式的，这使得对其执行操作变得更加复杂。由于堆栈溢出，我利用列表理解将所有值从unicode转换为简单的python字符串值:</p><pre class="kn ko kp kq gt mb mc md me aw mf bi"><span id="b77e" class="mg kz iq mc b gy mh mi l mj mk">#convert all values from unicode to string<br/>A = [x.encode(‘UTF8’) for x in A]<br/>B = [x.encode(‘UTF8’) for x in B]<br/>C = [x.encode(‘UTF8’) for x in C]<br/>D = [x.encode(‘UTF8’) for x in D]<br/>E = [x.encode(‘UTF8’) for x in E]<br/>F = [x.encode(‘UTF8’) for x in F]</span></pre><p id="4c54" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我导入了pandas，并将每个数据列表连接到一个数据框架中。这样做的时候，我指定了出现在维基百科页面上的列名:</p><pre class="kn ko kp kq gt mb mc md me aw mf bi"><span id="4753" class="mg kz iq mc b gy mh mi l mj mk">#import pandas to convert list to data frame<br/>import pandas as pd<br/>df=pd.DataFrame(A,columns=[‘Year’])<br/>df[‘Game’]=B<br/>df[‘Genre’]=C<br/>df[‘Lists’]=D<br/>df[‘Original Platform’]=E<br/>df[‘References’]=F</span></pre><p id="2b46" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">万岁。打印dataframe得到了以下输出。我没有拿出所有的参考信息，但这在我的后续检查中不需要。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mm"><img src="../Images/e0cdd7a0d7675d8adffbab18c957178b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*trKaf9SwaU6dlzTDq4lqxg.png"/></div></div></figure><p id="df5d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要了解我是如何用Tableau Public可视化这些数据的，请查看我的另一篇博文。感谢阅读！</p></div></div>    
</body>
</html>