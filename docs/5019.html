<html>
<head>
<title>Review: Pre-Activation ResNet with Identity Mapping — Over 1000 Layers Reached (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:带有身份映射的预激活 ResNet 已达到 1000 多个图层(图像分类)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e?source=collection_archive---------5-----------------------#2018-09-22">https://towardsdatascience.com/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e?source=collection_archive---------5-----------------------#2018-09-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a508" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">预激活 ResNet:卷积前的批量 Norm 和 ReLU</h2></div><p id="8600" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">在这个故事中，我们回顾了微软改进的 ResNet [1]。<strong class="kh ir">通过身份映射，深度学习架构可以达到 1000 层以上</strong>，不会增加错误。</p><p id="d80d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 ResNet [2]的上一版本中，当 ResNet 从 101 层到 1202 层时，虽然 ResNet-1202 仍然可以收敛，但是错误率从 6.43%下降到 7.93%(这个结果可以在[2]中看到)。并且在[2]中被陈述为开放问题而没有任何解释。</p><p id="a5ce" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图显示了使用标识映射的 ResNet 的结果。在层数达到 1001 的情况下，先前的 ResNet [2]仅获得 7.61%的误差，而对于 CIFAR-10 数据集，具有身份映射[1]的新 ResNet 可以获得 4.92%的误差。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/605fa745ed86dc629c0809b82b1415c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V2FgD6udOE4xJuu_R7L6qA.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk"><strong class="bd ma">(a) Previous ResNet [2] (7.61%) (b) New ResNet with Identity Mapping [1] (4.92%) for CIFAR-10 Dataset</strong></figcaption></figure><p id="c520" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是<strong class="kh ir">为什么保持快捷连接路径干净</strong>(如图将 ReLU 层从快捷连接路径移动到 conv 层路径)会更好？本文对此进行了很好的解释。一系列的消融研究证实了这种身份映射的重要性。</p><p id="3393" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果是<strong class="kh ir">比 Inception-v3</strong>【3】还要好。(如果有兴趣，也请阅读我的<a class="ae mb" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener"> Inception-v3 评测</a>。)这么好的成绩，发表在我写这个故事的时候<strong class="kh ir"> 2016 ECCV </strong>论文上<strong class="kh ir">1000 多篇引用</strong>。(<a class="mc md ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----bb50a42af03e--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="22df" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">涵盖哪些内容</h1><ol class=""><li id="4a42" class="nd ne iq kh b ki nf kl ng ko nh ks ni kw nj la nk nl nm nn bi translated"><strong class="kh ir">身份映射重要性的解释</strong></li><li id="41da" class="nd ne iq kh b ki no kl np ko nq ks nr kw ns la nk nl nm nn bi translated"><strong class="kh ir">消融研究</strong></li><li id="86c5" class="nd ne iq kh b ki no kl np ko nq ks nr kw ns la nk nl nm nn bi translated"><strong class="kh ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="df91" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 1。身份映射重要性的解释</strong></h1><p id="d680" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nt kq kr ks nu ku kv kw nv ky kz la ij bi translated">前馈、反向传播和梯度更新似乎使深度学习成为一个秘密。我觉得这里的解释非常好。</p><h2 id="760e" class="nw mm iq bd mn nx ny dn mr nz oa dp mv ko ob oc mx ks od oe mz kw of og nb oh bi translated">1.1 前馈</h2><p id="e5e7" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nt kq kr ks nu ku kv kw nv ky kz la ij bi translated">在具有身份映射的 ResNet 中，在没有任何 conv 层 BN 和 ReLU 的情况下，保持从输入到输出的快捷连接路径的干净是至关重要的。</p><p id="b5b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">xl 是 l 层的输入，F(。)是代表 conv 层 BN 和 ReLU 的函数。那么我们可以这样表述:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/2f6d548989932e3a278ed96a43da621c.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*nAhwzaJ0RMGd2nqu50hS3Q.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk"><strong class="bd ma">One Particular Layer</strong></figcaption></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/cc70376b505a71b5f09995786281893d.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*_57CC9O1VHiZe0DdjLnpdw.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk"><strong class="bd ma">L layers from l-th layer</strong></figcaption></figure><p id="729a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到<strong class="kh ir">输入信号 xl 仍然保持在这里！</strong></p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="547d" class="nw mm iq bd mn nx ny dn mr nz oa dp mv ko ob oc mx ks od oe mz kw of og nb oh bi translated">1.2 反向传播</h2><p id="870e" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nt kq kr ks nu ku kv kw nv ky kz la ij bi translated">在反向传播过程中，我们可以得到分解成两个附加项的梯度:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/8342afb83123b3a971a71aa8b59606c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*uHUGqh8iJnH_e2WTHH18eA.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk"><strong class="bd ma">Gradient which decomposed into two additive terms</strong></figcaption></figure><p id="34f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在毯子里面，<strong class="kh ir">无论网络有多深</strong>，我们总能在左边的项得到“1”。右项不能总是-1，这使得梯度为零。因此，<strong class="kh ir">这个渐变并没有消失！！</strong></p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="e107" class="nw mm iq bd mn nx ny dn mr nz oa dp mv ko ob oc mx ks od oe mz kw of og nb oh bi translated">1.2 违反身份映射时的反向传播</h2><p id="4cfa" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nt kq kr ks nu ku kv kw nv ky kz la ij bi translated">另一方面，如果左边的项不等于 1 呢:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/4523e778f7affd50b65f522b581be5b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*qXDsS_I-q02zBgJvrTh8Dg.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk"><strong class="bd ma">One Particular Layer</strong></figcaption></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi om"><img src="../Images/04a7b29f16fec88acb7845a8d1313806.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*nrxm2PshU5xnjGAo4b1zDw.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk"><strong class="bd ma">L layers from l-th layer</strong></figcaption></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi on"><img src="../Images/92b4e299bae06a83a83c81862822fdb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*xw7SoGOQoludcyrQo8c0UQ.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk"><strong class="bd ma">Gradient which decomposed into two additive terms</strong></figcaption></figure><p id="2479" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，梯度的左项是λ的乘积。</p><p id="6570" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果<strong class="kh ir"> λ &gt; 1 </strong>，左项将呈指数大，出现<strong class="kh ir">梯度爆炸</strong>问题。我们应该记得，当梯度爆炸时，<strong class="kh ir">损失无法收敛</strong>。</p><p id="f6a0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果<strong class="kh ir"> λ &lt; 1 </strong>，左项将呈指数小，出现<strong class="kh ir">梯度消失</strong>问题。我们不能用大值更新梯度，<strong class="kh ir">损耗停留在平稳状态，最终以大损耗收敛</strong>。</p><p id="de87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，这就是为什么我们需要保持从输入到输出的捷径连接路径没有任何 conv 层，BN 和 ReLU。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="5460" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 2。消融研究</strong></h1><h2 id="3cfc" class="nw mm iq bd mn nx ny dn mr nz oa dp mv ko ob oc mx ks od oe mz kw of og nb oh bi translated"><strong class="ak"> 2.1 各种快捷连接方式</strong></h2><p id="0d4c" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nt kq kr ks nu ku kv kw nv ky kz la ij bi translated"><strong class="kh ir"> 110 层 ResNet (54 个两层剩余单元)</strong>与各种类型的快捷连接在<strong class="kh ir"> CIFAR-10 </strong>数据集上测试如下:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi oo"><img src="../Images/eff3bcada9fbaaa898aeda06ceb32062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9lIcsEfLaxvb3hNW-WNIuQ.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk"><strong class="bd ma">Performance of Various Types of Shortcut Connections</strong></figcaption></figure><p id="8a62" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">原文</strong>:即[2]中 ResNet 的上一版本，误差 6.61%。</p><p id="b771" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">常数缩放</strong> : λ=0.5，存在上述梯度消失问题，仔细选择偏置 bg 后，误差为 12.35%。</p><p id="774b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">排他门控&amp;仅快捷门控</strong>:两者都试图增加快捷路径的复杂性，同时仍然保持路径等于“1”。但是两者都不能得到更好的结果。</p><p id="8059" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 1×1 Conv 快捷方式</strong>:类似于之前 ResNet [2]中的选项 C。在以前的 ResNet 中，发现使用选项 c 更好。但现在发现，当有许多剩余单元(太深)时，情况并非如此。</p><p id="a178" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">辍学捷径</strong>:实际上是统计执行λ=0.5。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="d904" class="nw mm iq bd mn nx ny dn mr nz oa dp mv ko ob oc mx ks od oe mz kw of og nb oh bi translated">2.2 激活的各种用法</h2><p id="c088" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nt kq kr ks nu ku kv kw nv ky kz la ij bi translated">通过在 BN 和 ReLU 的位置附近玩耍，获得以下结果:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi op"><img src="../Images/6f5cb532e016db35c3ca1babca9536ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kxgT5JHFEB31auMdj1iwaA.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk"><strong class="bd ma">Performance of Various Usages of Activation</strong></figcaption></figure><p id="2651" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">添加</strong>后之前的 ResNet &amp; BN:两者都不能在违反身份映射的快捷连接上保持干净。</p><p id="b882" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">加法前的 ReLU</strong>:ReLU 后的残差函数必须是非负的，使得正向传播的信号是单调递增的，而残差函数最好也是负值。</p><p id="fa5e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> ReLU-only 预激活</strong> : ReLU 不配合 BN 使用，不能很好的享受 BN 的好处。</p><p id="d12e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">全预激活</strong>:快捷路径干净，ReLU 配合 BN 使用，成为最佳设置。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="b775" class="nw mm iq bd mn nx ny dn mr nz oa dp mv ko ob oc mx ks od oe mz kw of og nb oh bi translated">2.3 预激活的双重优势</h2><p id="408e" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nt kq kr ks nu ku kv kw nv ky kz la ij bi translated"><strong class="kh ir"> 2.3.1 优化的简易性</strong></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi oq"><img src="../Images/48e58a3e3a5b387e177b403eafc19bfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tZjqttFpjWzSqf9XZQK-5A.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk"><strong class="bd ma">Previous ResNet structure (Baseline) vs Pre-activation Unit</strong></figcaption></figure><p id="21cf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于 ReLU 层的错误位置，使用先前的 ResNet 结构(基线)在太深(1001)时具有更差的结果。<strong class="kh ir">当网络从 110 到 1001 越来越深的时候，使用预激活单元总能得到一个比较好的结果。</strong></p><p id="42c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 2.3.2 减少过拟合</strong></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi or"><img src="../Images/c0c19991e257e37b6d481f1e97a430e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z7Mx6XQ_knGc8eT-G_Zt5g.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk"><strong class="bd ma">Training Error vs Iterations</strong></figcaption></figure><p id="b951" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">预激活单元处于<strong class="kh ir">正则化</strong>状态，即<strong class="kh ir">收敛时的训练损失略高，但测试误差</strong>较低。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="e1db" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">3.<strong class="ak">与最先进方法的比较</strong></h1><h2 id="7bb5" class="nw mm iq bd mn nx ny dn mr nz oa dp mv ko ob oc mx ks od oe mz kw of og nb oh bi translated">3.1 西法尔-10 和西法尔-100</h2><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi os"><img src="../Images/d0d58228ba589ebd02938c48ca479217.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SrUvoGCBkYOuUfAJLZxRCw.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk"><strong class="bd ma">CIFAR-10 &amp; CIFAR-100 Results</strong></figcaption></figure><p id="79fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于 CIFAR-10，使用具有建议预激活单元的<strong class="kh ir">ResNet-1001(4.62%)，甚至优于使用以前版本 ResNet 的 ResNet-1202 (7.93%) </strong>，<strong class="kh ir">少 200 层</strong>。</p><p id="0239" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于 CIFAR-100，使用<strong class="kh ir"> ResNet-1001 和建议的预激活单元(22.71%)，甚至优于使用 ResNet 早期版本的 ResNet-1001 (27.82%) </strong>。</p><p id="cf21" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于 CIFAR-10 和 CIFAR-100 来说，<strong class="kh ir">带有建议预激活单元的 ResNet-1001 并不比 ResNet-164 有更大的误差，但是以前的 ResNet [2]有</strong>。</p><p id="1083" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 CIFAR-10 上，ResNet-1001 用 2 个 GPU 训练大约需要 27 个小时。</p><h2 id="b697" class="nw mm iq bd mn nx ny dn mr nz oa dp mv ko ob oc mx ks od oe mz kw of og nb oh bi translated">3.2 ILSVRC</h2><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ot"><img src="../Images/9724052b8b6aa483ba56011c03bf7d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HJiVwHvQYB5lRkdpFDLmNg.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk"><strong class="bd ma">ILSVRC Image Classification Results</strong></figcaption></figure><p id="ed48" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">仅在规模上有所增强，ILSVRC 2015 的获胜者 ResNet-152 (5.5%)上一版本由于 ReLU 位置错误，在往更深处走时，比 ResNet-200 (6.0%) <strong class="kh ir">上一版本<strong class="kh ir">性能差</strong>。</strong></p><p id="42db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并且所提出的具有预激活的 ResNet-200(5.3%)具有比先前的 ResNet-200 (6.0%)更好的结果。</p><p id="5e95" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着规模和纵横比的增加，提议的预激活 ResNet-200(4.8%)优于谷歌的 Inception-v3[3](5.6%)。</p><p id="0d71" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同时，谷歌也有一个 Inception-ResNet-v2，它有 4.9%的误差，有了预激活单元，误差有望进一步降低。</p><p id="f7b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 ILSVRC 上，ResNet-200 在 8 个 GPU 上训练大约需要 3 周时间。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="f120" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在回顾了 ResNet 和带有身份映射的 ResNet，以及 Inception-v1、Inception-v2 和 Inception-v3 之后，我还将回顾 Inception-v4。敬请关注！</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="56fd" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">参考</h1><ol class=""><li id="609c" class="nd ne iq kh b ki nf kl ng ko nh ks ni kw nj la nk nl nm nn bi translated">【2016 ECCV】【带有身份映射的 ResNet】<br/><a class="ae mb" href="https://arxiv.org/abs/1603.05027" rel="noopener ugc nofollow" target="_blank">深度剩余网络中的身份映射</a></li><li id="490b" class="nd ne iq kh b ki no kl np ko nq ks nr kw ns la nk nl nm nn bi translated">【2016 CVPR】【ResNet】<br/><a class="ae mb" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习</a></li><li id="aa93" class="nd ne iq kh b ki no kl np ko nq ks nr kw ns la nk nl nm nn bi translated">【2016 CVPR】【盗梦空间-v3】<br/><a class="ae mb" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">重新思考计算机视觉的盗梦空间架构</a></li></ol><h1 id="fdda" class="ml mm iq bd mn mo ou mq mr ms ov mu mv jw ow jx mx jz ox ka mz kc oy kd nb nc bi translated">我的评论</h1><ol class=""><li id="bf8d" class="nd ne iq kh b ki nf kl ng ko nh ks ni kw nj la nk nl nm nn bi translated"><a class="ae mb" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">回顾:ResNet——2015 年国际影像分类、定位、检测奖得主</a></li><li id="75da" class="nd ne iq kh b ki no kl np ko nq ks nr kw ns la nk nl nm nn bi translated"><a class="ae mb" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener">回顾:Inception-v3–ils vrc 2015 亚军(图像分类)</a></li><li id="11c6" class="nd ne iq kh b ki no kl np ko nq ks nr kw ns la nk nl nm nn bi translated"><a class="ae mb" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">回顾:批量归一化(Inception-v2/BN-Inception)——ILSVRC 2015 中第二个超越人类水平的性能(图像分类)</a></li><li id="4d8d" class="nd ne iq kh b ki no kl np ko nq ks nr kw ns la nk nl nm nn bi translated"><a class="ae mb" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener">回顾:谷歌网(Inception v1)——ILSVRC 2014(图像分类)获奖者</a></li></ol></div></div>    
</body>
</html>