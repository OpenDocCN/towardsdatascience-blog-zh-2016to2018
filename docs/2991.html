<html>
<head>
<title>Building &amp; Improving a K-Nearest Neighbors Algorithm in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中K近邻算法的构建与改进</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-improving-a-k-nearest-neighbors-algorithm-in-python-3b6b5320d2f8?source=collection_archive---------3-----------------------#2018-03-28">https://towardsdatascience.com/building-improving-a-k-nearest-neighbors-algorithm-in-python-3b6b5320d2f8?source=collection_archive---------3-----------------------#2018-03-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="078f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">** <strong class="jp ir">注意**:自从写了这篇文章，Medium就不再允许在Jupyter笔记本上嵌入代码。要查看本文的配套笔记本和代码，请访问我的GitHub上的</strong> <a class="ae kl" href="https://github.com/samueleverett01/K-Nearest-Neighbors-Image-Classifier/blob/master/Cosine-Similarity%20Model%20Analysis.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">此链接</strong> </a> <strong class="jp ir">。</strong></p><p id="379e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">K近邻算法(K-Nearest Neighbors algorithm，简称K-NN)是一种经典的机器学习工作马算法，这种算法在深度学习的今天往往被忽视。在本教程中，我们将在Scikit-Learn中构建一个K-NN算法，并在MNIST数据集上运行它。从那里，我们将建立我们自己的K-NN算法，希望开发一个比Scikit-Learn K-NN具有更好的准确性和分类速度的分类器。在这篇文章的最后，我给好奇的读者列出了一个书单，希望了解更多关于这些方法的知识。</p><h1 id="f895" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">k-最近邻分类模型</h1><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/24af3e5f23b648d0ed4404b503d68e59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*3gRZvgOB3-L4T5ow.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Lazy Programmer</figcaption></figure><p id="7882" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">K-最近邻算法是一种受监督的机器学习算法，它易于实现，并且具有进行稳健分类的能力。K-NN最大的优势之一就是它是一个<em class="lw">懒学习者</em>。这意味着该模型不需要训练，可以直接对数据进行分类，不像它的其他ML兄弟，如SVM，回归和多层感知。</p><h2 id="a15c" class="lx kn iq bd ko ly lz dn ks ma mb dp kw jy mc md la kc me mf le kg mg mh li mi bi translated">K-NN如何工作</h2><p id="f553" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">为了对某个给定的数据点<strong class="jp ir"> p </strong>进行分类，K-NN模型将首先使用某个<em class="lw">距离度量</em>将<strong class="jp ir"> p </strong>与其数据库中可用的所有其他点进行比较。距离度量是类似于<a class="ae kl" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">欧几里德距离、</a>一个简单的函数，它取两个点，并返回这两个点之间的距离。因此，可以假设它们之间距离较小的两个点比它们之间距离较大的两个点<em class="lw">更相似</em>。这是K-NN背后的中心思想。</p><p id="ef8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个过程将返回一个无序数组，其中数组中的每个条目保存了模型数据库中的<strong class="jp ir"> p </strong>和<strong class="jp ir"> n </strong>数据点之一之间的距离。所以返回的数组大小为<strong class="jp ir"> n </strong>。这就是K-最近邻的K部分的用武之地:<strong class="jp ir"> k </strong>是选择的某个任意值(通常在3-11之间)，它告诉模型<em class="lw">在对<strong class="jp ir"> p </strong>分类时，它应该考虑多少个</em><strong class="jp ir"><em class="lw"/></strong><em class="lw">与</em> <strong class="jp ir"> p </strong>最相似的点。然后，该模型将采用这些<strong class="jp ir"> k </strong>最相似的值，并使用投票技术来决定如何对<strong class="jp ir"> p </strong>进行分类，如下图所示。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/67c91363f0c8717e42c60e4ca577897f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*g1n1h_ZqSbz03FWz.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Lazy Programmer</figcaption></figure><p id="75f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图像中的K-NN模型的<strong class="jp ir"> k </strong>值为3，中间箭头指向的点就是<strong class="jp ir"> p </strong>，需要分类的点。如你所见，圆圈中的三点是最接近，或者说最类似于<strong class="jp ir"> p </strong>的三点。因此，使用简单的投票技术，<strong class="jp ir"> p </strong>将被归类为“白色”，因为白色占了<strong class="jp ir"> k </strong>最相似值的大多数。</p><p id="01a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相当酷！令人惊讶的是，这个简单的算法在某些情况下可以实现超人的结果，并且可以应用于各种各样的问题，正如我们接下来将看到的那样。</p><h1 id="39a0" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">在Scikit-Learn中实现K-NN算法对MNIST图像进行分类</h1><h2 id="de0c" class="lx kn iq bd ko ly lz dn ks ma mb dp kw jy mc md la kc me mf le kg mg mh li mi bi translated">数据:</h2><p id="1a3f" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">对于这个例子，我们将使用无处不在的MNIST数据集。MNIST数据集是机器学习中最常用的数据集之一，因为它易于实现，但却是证明模型的可靠方法。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/762cff314252f782672894b4952e048f.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/0*d9UxRMLhdHL7IH0D."/></div></figure><p id="361a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">MNIST是一个由7万个手写数字组成的数据集，数字从0到9。没有两个手写数字是相同的，有些很难正确分类。对MNIST进行分类的人类基准是大约97.5%的准确率，所以我们的目标是超过这个数字！</p><h2 id="db9b" class="lx kn iq bd ko ly lz dn ks ma mb dp kw jy mc md la kc me mf le kg mg mh li mi bi translated">算法:</h2><p id="f2f6" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">我们将从Scikit-Learn Python库中的<code class="fe mp mq mr ms b">KNeighborsClassifier()</code>开始。这个函数有很多参数，但是在这个例子中我们只需要关注其中的几个。具体来说，我们将只为<code class="fe mp mq mr ms b">n_neighbors</code>参数传递一个值(这是<strong class="jp ir"> k </strong>值)。<code class="fe mp mq mr ms b">weights</code>参数给出了模型使用的投票系统的类型，其中缺省值是<code class="fe mp mq mr ms b">uniform</code>，这意味着在分类<strong class="jp ir"> p </strong>时，每个<strong class="jp ir"> k </strong>点的权重相等。<code class="fe mp mq mr ms b">algorithm</code>参数也将保留其默认值<code class="fe mp mq mr ms b">auto</code>，因为我们希望Scikit-Learn找到用于分类MNIST数据本身的最佳算法。</p><p id="07f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面，我嵌入了一个Jupyter笔记本，它用Scikit-Learn构建了K-NN分类器。开始了。</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="dfee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">太棒了。我们使用Scikit-Learn建立了一个非常简单的K近邻模型，在MNIST数据集上取得了非凡的性能。</p><p id="4af2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">问题？对这些点进行分类花费了很长时间(对于两个数据集分别是8分钟和几乎4分钟)，讽刺的是K-NN仍然是最快的分类方法之一。一定有更快的方法…</p><h1 id="b0a9" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">构建更快的模型</h1><p id="6e49" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">大多数K-NN模型使用欧几里德距离或曼哈顿距离作为到达距离度量。这些指标很简单，在各种情况下都表现良好。</p><p id="b24d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个很少使用的距离度量是<a class="ae kl" href="http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/" rel="noopener ugc nofollow" target="_blank">余弦相似度</a>。余弦相似性通常不是最佳距离度量，因为它违反了<a class="ae kl" href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/linear-algebra-vector-triangle-inequality" rel="noopener ugc nofollow" target="_blank">三角形不等式</a>，并且对负数据无效。然而，余弦相似性是完美的MNIST。它快速、简单，并且比MNIST上的其他距离度量标准精度略高。但是，为了尽可能地获得最佳性能，我们必须编写自己的K-NN模型。在我们自己制作了一个K-NN模型之后，我们应该会得到比Scikit-Learn模型更好的性能，甚至更好的准确性。让我们看看下面的笔记本，在那里我们建立了自己的K-NN模型。</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="8bd6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如笔记本中所示，我们自己制作的K-NN模型在分类速度(相当大的差距)和准确性(在一个数据集上提高1%)方面都优于Scikit-Learn K-NN！现在，我们可以继续在实践中实现这个模型，因为我们已经开发了一个真正快速的算法。</p><h1 id="08b2" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">结论</h1><p id="e39a" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">这是很多，但我们学到了一些宝贵的经验。首先，我们学习了K-NN如何工作，以及如何轻松地实现它。但最重要的是，我们了解到，始终考虑你试图解决的问题和你可用于解决该问题的工具是很重要的。有时候，在解决问题时，最好花时间去试验——是的，建立你自己的模型。正如笔记本电脑所证明的那样，它可以带来巨大的回报:我们的第二个专有模型将使用速度提高了1.5-2倍，为使用该模型的实体节省了大量时间。</p><p id="d239" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你想了解更多，我鼓励你去查看一下这个GitHub库，在那里你会发现这两个模型之间更彻底的分析，以及一些关于我们更快的K-NN模型的更有趣的特性！</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="4412" class="km kn iq bd ko kp nc kr ks kt nd kv kw kx ne kz la lb nf ld le lf ng lh li lj bi translated">书单</h1><p id="c5b7" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">以下是K-NN、通用机器学习和深度学习的有用书籍列表，以及提供人工智能和数学讨论的伟大书籍(见哥德尔、埃舍尔、巴赫)</p><ol class=""><li id="f8f2" class="nh ni iq jp b jq jr ju jv jy nj kc nk kg nl kk nm nn no np bi translated"><a class="ae kl" href="https://amzn.to/2IsNcPz" rel="noopener ugc nofollow" target="_blank">统计学习的要素</a></li><li id="99b6" class="nh ni iq jp b jq nq ju nr jy ns kc nt kg nu kk nm nn no np bi translated"><a class="ae kl" href="https://amzn.to/2VGDhO4" rel="noopener ugc nofollow" target="_blank">深度学习</a></li><li id="f698" class="nh ni iq jp b jq nq ju nr jy ns kc nt kg nu kk nm nn no np bi translated"><a class="ae kl" href="https://amzn.to/32KRb3f" rel="noopener ugc nofollow" target="_blank">动手机器学习</a></li><li id="a5db" class="nh ni iq jp b jq nq ju nr jy ns kc nt kg nu kk nm nn no np bi translated"><a class="ae kl" href="https://amzn.to/2x797t3" rel="noopener ugc nofollow" target="_blank">用于数据分析的Python</a></li><li id="f270" class="nh ni iq jp b jq nq ju nr jy ns kc nt kg nu kk nm nn no np bi translated"><a class="ae kl" href="https://amzn.to/2VHJ2Lu" rel="noopener ugc nofollow" target="_blank">Python机器学习简介</a></li><li id="f095" class="nh ni iq jp b jq nq ju nr jy ns kc nt kg nu kk nm nn no np bi translated"><a class="ae kl" href="https://amzn.to/38jZlkb" rel="noopener ugc nofollow" target="_blank">最近邻法讲座</a></li><li id="2409" class="nh ni iq jp b jq nq ju nr jy ns kc nt kg nu kk nm nn no np bi translated"><a class="ae kl" href="https://amzn.to/2vzJctx" rel="noopener ugc nofollow" target="_blank">模式识别和机器学习</a></li><li id="bc83" class="nh ni iq jp b jq nq ju nr jy ns kc nt kg nu kk nm nn no np bi translated"><a class="ae kl" href="https://amzn.to/32KDSQ4" rel="noopener ugc nofollow" target="_blank">机器学习:概率视角</a></li><li id="3747" class="nh ni iq jp b jq nq ju nr jy ns kc nt kg nu kk nm nn no np bi translated"><a class="ae kl" href="https://amzn.to/2PL69AZ" rel="noopener ugc nofollow" target="_blank">哥德尔、埃舍尔、巴赫</a></li><li id="8e12" class="nh ni iq jp b jq nq ju nr jy ns kc nt kg nu kk nm nn no np bi translated"><a class="ae kl" href="https://amzn.to/2PJ3tDT" rel="noopener ugc nofollow" target="_blank">我是个奇怪的循环</a></li><li id="7934" class="nh ni iq jp b jq nq ju nr jy ns kc nt kg nu kk nm nn no np bi translated"><a class="ae kl" href="https://amzn.to/2Tj61dT" rel="noopener ugc nofollow" target="_blank">生活3.0 </a></li></ol><p id="54ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请在评论中留下您的任何意见、批评或问题！</p></div></div>    
</body>
</html>