<html>
<head>
<title>Support vector machines ( intuitive understanding ) — Part#2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机(直观理解)——第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machines-intuitive-understanding-part-2-1046dd449c59?source=collection_archive---------1-----------------------#2017-10-18">https://towardsdatascience.com/support-vector-machines-intuitive-understanding-part-2-1046dd449c59?source=collection_archive---------1-----------------------#2017-10-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="88d0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">最大差值分类的概念</h2></div><div class="kf kg gp gr kh ki"><a href="https://medium.com/towards-data-science/support-vector-machines-intuitive-understanding-part-1-3fb049df4ba1" rel="noopener follow" target="_blank"><div class="kj ab fo"><div class="kk ab kl cl cj km"><h2 class="bd ir gy z fp kn fr fs ko fu fw ip bi translated">支持向量机(直观理解)——第一部分</h2><div class="kp l"><h3 class="bd b gy z fp kn fr fs ko fu fw dk translated">网上关于这个话题的大部分材料都是用数学和很多细节来处理的，其中一个…</h3></div><div class="kq l"><p class="bd b dl z fp kn fr fs ko fu fw dk translated">medium.com</p></div></div><div class="kr l"><div class="ks l kt ku kv kr kw kx ki"/></div></div></a></div><p id="e60d" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这是关于这个主题的第一部分的延续。如果你愿意，请参考以上部分。</p><p id="07f6" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这里我们将讨论 SVM 固有的最大利润分类背后的直觉。问题陈述如下—</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/161b5e82d509fdbb9072cb6412b94c02.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*uBMd9gcHrWSXyjPWVsOpug.png"/></div></figure><p id="31e6" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">理想情况下，我们希望我们的分类线看起来像上图中的粗线。请注意，与虚线相比，粗线离其最近的数据点更远。那么，SVM 是如何做到这一点的呢？</p><p id="b912" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">首先，让我们复习一些基础知识来理解这一点。我们知道一个简单的线性方程由 a* <em class="mb"> x </em> +b* <em class="mb"> y </em> +c = 0 给出。我们将对 2D 使用这个简单的等式，这可以很容易地扩展到对<em class="mb"> n </em> D</p><p id="1a30" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">一些基本假设—</p><ul class=""><li id="50a3" class="mc md iq la b lb lc le lf lh me ll mf lp mg lt mh mi mj mk bi translated">我们将只考虑两个特性<em class="mb"> x1，x2 </em></li><li id="8e33" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">一个目标类<em class="mb"> y </em>，对于正类可以取+1，对于负类可以取-1。</li><li id="0679" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">设<em class="mb"> x1i，x2i(对于 i = 1 到 n，</em>组观察值<em class="mb"> ) </em>代表个体'<em class="mb"> n' </em>对这些特征中的每一个的个体观察值<em class="mb"> x1，x2 </em>。</li></ul><p id="5eb5" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">那么我们可以把我们的简单线性方程改写为 w1 *<em class="mb">x1</em>+w2 *<em class="mb">x2</em>+w0 = 0——只是代入:a = w1，b = w2，c = w0，x = <em class="mb"> x1 </em>，y =<em class="mb">x2</em>；其中 w1、w2 和 w0 是我们的优化算法将最终计算出的权重。</p><p id="f137" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">然后，对于任何观察说，(<em class="mb"> x1 </em> i，<em class="mb"> x2 </em> i) —</p><ul class=""><li id="f56e" class="mc md iq la b lb lc le lf lh me ll mf lp mg lt mh mi mj mk bi translated">w1 *<em class="mb">x1i</em>+w2 *<em class="mb">x2i</em>+w0<strong class="la ir">+T76】0、</strong> if ( <em class="mb"> x1 </em> i、<em class="mb"> x2 </em> i)位于直线的一侧(当(<em class="mb"> x1 </em> i、<em class="mb"> x2 </em> i)属于<em class="mb"> y </em> = +1 目标类时发生)</li></ul><p id="c6a0" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><em class="mb">或</em></p><ul class=""><li id="14c1" class="mc md iq la b lb lc le lf lh me ll mf lp mg lt mh mi mj mk bi translated">w1 *<em class="mb">x1i</em>+w2 *<em class="mb">x2i</em>+w0<strong class="la ir">&lt;<strong class="la ir">0</strong>，如果(<em class="mb"> x1 </em> i，<em class="mb"> x2 </em> i)位于直线的另一侧(当(<em class="mb"> x1 </em> i，<em class="mb"> x2 </em> i)属于<em class="mb"> y </em> = -1 目标时发生</strong></li></ul><p id="5897" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">将两个等式合并为一个:<em class="mb">y</em>*(w1 *<em class="mb">x1i</em>+w2 *<em class="mb">x2i</em>+w0)&gt;0，当<em class="mb"> y </em> =-1 或+1(仅在两边乘以目标值，<em class="mb"> y </em> (+1 / -1)时，该等式适用于上述两种情况，因此该等式适用于正负两种分类</p><p id="5de7" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">从几何学中，我们知道从点(<em class="mb"> x1 </em> i，<em class="mb"> x2 </em> i)到直线 w1* <em class="mb"> x1 </em> +w2*x2+w0 = 0 的垂直距离(姑且称之为'<em class="mb"> m </em>')由下式给出</p><p id="18d1" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><em class="mb">m</em>=(w1 *<em class="mb">x1i</em>+w2 * x2i+w0)<strong class="la ir">/</strong>sqrt(<em class="mb">w1</em>+<em class="mb">w2</em></p><p id="eb1d" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">请参考下面的链接，了解为什么会这样的简单推导</p><div class="kf kg gp gr kh ki"><a href="https://www.intmath.com/plane-analytic-geometry/perpendicular-distance-point-line.php" rel="noopener  ugc nofollow" target="_blank"><div class="kj ab fo"><div class="kk ab kl cl cj km"><h2 class="bd ir gy z fp kn fr fs ko fu fw ip bi translated">点到一条线的垂直距离</h2><div class="kp l"><h3 class="bd b gy z fp kn fr fs ko fu fw dk translated">展示了如何找到一个点到一条线的垂直距离，以及公式的证明。</h3></div><div class="kq l"><p class="bd b dl z fp kn fr fs ko fu fw dk translated">www.intmath.com</p></div></div><div class="kr l"><div class="mq l kt ku kv kr kw kx ki"/></div></div></a></div><p id="3b33" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">考虑到归一化权重(<em class="mb"> w1 </em> + <em class="mb"> w2 </em> = 1)，我们可以去掉上一个等式中的分母，它简化为以下等式</p><p id="2ab9" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">w1 *<em class="mb">x1i</em>+w2 *<em class="mb">x2i</em>+w0 =<em class="mb">m</em></p><p id="4b35" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">通过在任一侧乘以目标值，<em class="mb"> y </em> (+1 / -1)，该等式可以概括为对正类和负类进行分类。给定这一点，然后下面的内等式，<strong class="la ir">y *(w1 *<em class="mb">x1i</em>+w2 *<em class="mb">x2i</em>+w0)≥<em class="mb">m</em></strong><em class="mb">，</em>保证对于阴性/阳性目标类，每个观察值(<em class="mb"> x1 </em> i，<em class="mb"> x2 </em> i)位于/超出分类线两侧的距离“<em class="mb"> m </em>”。在这里，‘<em class="mb">m</em>’被称为边距。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/047ca13e3cddc693f7686cb3d79ec6d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*XvUKm2BC0sPL3ieM6XuFEQ.png"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Margin = m</figcaption></figure><p id="f405" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">那么现在下一个问题来了，是什么原因导致 SVM 将利润最大化？答案在于优化第一部分中讨论的成本/损失函数。</p><ul class=""><li id="2c85" class="mc md iq la b lb lc le lf lh me ll mf lp mg lt mh mi mj mk bi translated">正如我们从第 1 部分的图中注意到的，当“预测的<em class="mb">y</em>”≥1 时，铰链损失变为 0。</li><li id="8b81" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">如第 1 部分所示的“预测的<em class="mb"> y </em>”等于 w1* <em class="mb"> x1 </em> + w2*x2 + w0(输入值的加权平均值，X)。</li></ul><p id="b2e7" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">综合以上，当 w1* <em class="mb"> x1 </em> + w2*x2 + w0 ≥ 1 时，铰链损耗为 0。同样，通过在任一侧将此乘以 y (+1 / -1)的目标值，使得该等式可以概括为分类正类和负类。这样，我们得到:<em class="mb">y</em>*(w1 *<em class="mb">x1</em>+w2 * x2+w0)≥1。与之前的等式(粗体)相比，我们可以看到裕量 m 等于 1。根据我们使用的铰链损耗函数，我们可以将这个裕量更改为我们想要的值。优化算法将计算出权重，使得上述等式成立，以便最小化损失函数。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/7c9a1f346ad38837d5cecb0977df8418.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*qtWprL7lgmIjk8HKxupocw.png"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">margin value on x-axis vs loss on y-axis</figcaption></figure><p id="787c" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">从上面的图中，请注意—</p><ul class=""><li id="de7b" class="mc md iq la b lb lc le lf lh me ll mf lp mg lt mh mi mj mk bi translated">对于离分类线相当远的所有点，余量，<em class="mb"> m </em>大于 1。所以他们的损失= 0(蓝线)。因此，只有在分类线附近(<em class="mb"> m </em> ≤1)的点才真正影响权重。</li><li id="9879" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">对于更远的点(<em class="mb"> m </em> &gt; 1)，无论如何损失函数是 0，因此当优化算法遇到这样的更远的点时，不对权重进行调整。</li><li id="e26a" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">为此，(<em class="mb"> m </em> ≤1)的点称为<strong class="la ir"> <em class="mb">支持向量</em> </strong>，因为它们支持/影响分类线。为什么是矢量？—因为任何点都是数据空间中的向量。</li></ul><p id="7f49" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这里需要注意的重要一点是，第 1 部分中“预测 y”的概念只不过是边缘值的概念，<em class="mb"> m. </em>如果“<em class="mb">m”</em>高(=“预测 y”高)，那么该点离分类线更远，因此我们更确信观察值肯定属于所识别的类别。这正是最大间隔分类器背后的概念。</p><p id="5fb8" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">最后，通过控制我们在第 1 部分中讨论的成本函数中的正则化参数“C ”,我们间接操纵每个观察值的裕量<em class="mb">‘m’</em>(<em class="mb">x1</em>I，<em class="mb"> x2 </em> i ),从而调整缓冲/松弛量，我们同意，在对数据进行错误分类时。如果“C”选择得更高，那么我们允许更多的缓冲数据被错误分类，反之亦然。我们可以直观地理解这一点，考虑一个让‘C’= 0 的极限情况，这意味着我们不允许任何缓冲。然后，请注意，损失函数只是简化为优化成本=||w ||/2(我们在第 1 部分中讨论了这一点作为硬间隔分类器)，这意味着我们需要取得平衡以减少权重(使上述成本函数最小)，同时实现 1 的间隔(因为我们希望 w1* <em class="mb"> x1 </em> + w2*x2 + w0 ≥ 1)。组合的这种性质不允许任何错误分类的缓冲，因此被恰当地称为硬边界分类器。</p><p id="43ef" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在下一个也是最后一个部分，我们将看看如何使用内核技巧来完成特性转换。</p></div></div>    
</body>
</html>