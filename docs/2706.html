<html>
<head>
<title>The Search for Categorical Correlation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对范畴相关的探索</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9?source=collection_archive---------2-----------------------#2018-02-24">https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9?source=collection_archive---------2-----------------------#2018-02-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="0bd4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">本帖中出现的所有代码都是我的</em> <a class="ae kq" href="https://github.com/shakedzy" rel="noopener ugc nofollow" target="_blank"> <em class="kl"> GitHub页面</em> </a> <em class="kl">上的</em> <code class="fe km kn ko kp b"><a class="ae kq" href="https://github.com/shakedzy/dython" rel="noopener ugc nofollow" target="_blank">dython</a></code> <em class="kl">库的一部分。<br/>如有任何代码相关问题，请在本库的GitHub页面</em> <a class="ae kq" href="https://github.com/shakedzy/dython/issues" rel="noopener ugc nofollow" target="_blank"> <em class="kl">开题</em> </a> <em class="kl">。</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/8cb33217021355052eb096cefeb55bbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eqArgnH7PNT76mhB9pQjIg.jpeg"/></div></div></figure><p id="1112" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不久前，我在Kaggle 上偶然发现了一个蘑菇数据集，其中收集了20多种不同特征的食用和有毒蘑菇，并进行了分类。寻找可能指出吃随机蘑菇有多安全的模式的想法似乎是一个不错的挑战——我甚至发现自己在我后来出版的的<a class="ae kq" href="https://www.kaggle.com/shakedzy/alone-in-the-woods-using-theil-s-u-for-survival" rel="noopener ugc nofollow" target="_blank">内核后面创造了一个完整的故事情节——一个在森林里迷路的人。</a></p><p id="2b51" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在浏览其他用户的内核时，很容易看到随机森林和其他简单的方法无需太多努力就可以达到极高的准确性，所以我也认为没有理由这样做——我决定看看我是否可以自己找到哪些功能指向我可以安全食用的蘑菇，如果我需要的话。我意识到我实际上在寻找的是特征和蘑菇类型之间的相关性，但这是一个问题，因为特征都是分类的，在这种情况下相关性没有定义。</p><h2 id="a471" class="ld le iq bd lf lg lh dn li lj lk dp ll jy lm ln lo kc lp lq lr kg ls lt lu lv bi translated">什么是相关性？</h2><p id="eaf4" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">在我们讨论相关性不是什么之前，我们先来讨论它是什么。在人类语言中，相关性是对两个特征如何相关的度量；就像一年中的月份与日平均温度相关，一天中的时间与户外的光照量相关。从数学上来说，通常使用的相关性定义是数据样本的<a class="ae kq" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank"> Pearson's R </a>(产生范围为[-1，1]的值):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/9e8ca218916dd13bb5848ae072fba9b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*uOBToLtIFDfrNwnXLBv-7w.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Pearson’s R for data sample. Taken from Wikipedia</figcaption></figure><p id="1539" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是，从上面的等式可以看出，当数据是分类的时，皮尔逊的R没有被定义；让我们假设<em class="kl"> x </em>是一个颜色特征——如何从颜色的<em class="kl">平均值中减去<em class="kl">黄色</em>？我们需要别的东西。</em></p><p id="723d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">处理这种情况的一个常见选项是首先使用一次性编码，并将每个分类特征的每个可能选项分解为0或1特征。这将允许使用相关性，但它很容易变得过于复杂而无法分析。例如，one-hot编码将蘑菇数据集的22个分类特征转换为112个特征的数据集，当将相关表绘制为热图时，我们会得到如下结果:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/0860f11da3229cf5ebbb930302098ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*5zXKfXX0U2zw68ea_CXaeg.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Correlation of the mushrooms data-set, transformed using one-hot encoding</figcaption></figure><p id="9908" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这不是可以轻易用来获得新见解的东西。所以我们还需要别的东西。</p><h2 id="c8f6" class="ld le iq bd lf lg lh dn li lj lk dp ll jy lm ln lo kc lp lq lr kg ls lt lu lv bi translated">直言不讳</h2><p id="314a" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">我们需要的是<em class="kl">看起来</em>像相关，但对分类值有效的东西——或者更正式地说，我们在寻找两个分类特征之间关联的<em class="kl">度量。介绍:<a class="ae kq" href="https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V" rel="noopener ugc nofollow" target="_blank">克莱默V </a>。它是基于皮尔森卡方检验的一个名义上的变化，并且有一些内在的好处:</em></p><ol class=""><li id="8078" class="mh mi iq jp b jq jr ju jv jy mj kc mk kg ml kk mm mn mo mp bi translated">与相关类似，输出在[0，1]的范围内，其中0表示没有关联，1表示完全关联。(与相关性不同，不存在负值，因为不存在负相关这种东西。要么有，要么没有)</li><li id="1574" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">像相关性一样，克莱姆的V是对称的——它对交换<em class="kl"> x </em>和<em class="kl"> y </em>不敏感</li></ol><p id="b957" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更好的是——有人已经将其实现为一个<a class="ae kq" href="https://stackoverflow.com/a/46498792/5863503" rel="noopener ugc nofollow" target="_blank"> Python函数</a>。这是我编辑过的原文:</p><pre class="ks kt ku kv gt mv kp mw mx aw my bi"><span id="f4e6" class="ld le iq kp b gy mz na l nb nc"><strong class="kp ir">def </strong>cramers_v(x, y):<em class="kl"><br/>    </em>confusion_matrix = pd.crosstab(x,y)<br/>    chi2 = ss.chi2_contingency(confusion_matrix)[0]<br/>    n = confusion_matrix.sum().sum()<br/>    phi2 = chi2/n<br/>    r,k = confusion_matrix.shape<br/>    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))<br/>    rcorr = r-((r-1)**2)/(n-1)<br/>    kcorr = k-((k-1)**2)/(n-1)<br/>    <strong class="kp ir">return </strong>np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))</span></pre><p id="aeee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当应用于蘑菇数据集时，它看起来像这样:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nd"><img src="../Images/3d51ff2c3c1ab7f3478e6fcfee0431fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kUuEuJu3B1LNAXiFwUvDIw.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Cramer’s V calculated for the mushrooms data-set</figcaption></figure><p id="918b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那不是很漂亮吗？通过观察这张热图，我们可以看到<em class="kl">气味</em>与蘑菇的<em class="kl">类</em>(可食用/有毒)高度相关，并且<em class="kl">鳃附着</em>特征与其他三种特征高度相关。</p><h2 id="34d1" class="ld le iq bd lf lg lh dn li lj lk dp ll jy lm ln lo kc lp lq lr kg ls lt lu lv bi translated">对称的诅咒</h2><p id="9022" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">思考克莱姆V的输出，我意识到由于它的对称性，我正在失去有价值的信息。为了更好地证明这一点，请考虑以下数据集:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/b4f56e8eb80c78e4e1e5ebbfefba31c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*3Mx7I537OnQybSOMPvgqEw.png"/></div></figure><p id="f849" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，如果已知<em class="kl"> x </em>的值，仍然无法确定<em class="kl"> y </em>的值，但是如果已知<em class="kl"> y </em>的值——那么就保证了<em class="kl"> x </em>的值。当使用克莱姆的V时，由于它的对称性，这些有价值的信息丢失了，所以为了保存它，我们需要一个<em class="kl">非对称的</em>分类特征之间关联的度量。这正是泰尔的U。</p><p id="612c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kq" href="https://en.wikipedia.org/wiki/Uncertainty_coefficient" rel="noopener ugc nofollow" target="_blank">泰尔的U </a>，也称为不确定系数，是基于<em class="kl"> x </em>和<em class="kl"> y — </em>之间的<em class="kl">条件熵</em>或者用人类的语言来说，给定<em class="kl"> x </em>的值，<em class="kl"> y </em>有多少种可能的状态，以及它们出现的频率。就像克莱姆的V一样，输出值在[0，1]的范围内，与之前的解释相同——但与克莱姆的V不同，它是不对称的，意思是<em class="kl"> U(x，y)</em>≦<em class="kl">U(y，x) </em>(而<em class="kl"> V(x，y)=V(y，x) </em>，其中<em class="kl"> V </em>是克莱姆的V <em class="kl"> ) </em>。在上面这个简单的例子中使用Theil的U会让我们发现，知道<em class="kl"> y </em>就意味着我们知道<em class="kl"> x </em>，反之则不然。</p><p id="e6de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将公式实现为Python函数产生了这个<em class="kl">(关于</em> <code class="fe km kn ko kp b">conditional_entropy</code> <em class="kl">函数的完整代码可以在我的Github页面上找到——链接在帖子顶部)</em>:</p><pre class="ks kt ku kv gt mv kp mw mx aw my bi"><span id="69f8" class="ld le iq kp b gy mz na l nb nc"><strong class="kp ir">def </strong>theils_u(x, y):<em class="kl"><br/>    </em>s_xy = conditional_entropy(x,y)<br/>    x_counter = Counter(x)<br/>    total_occurrences = sum(x_counter.values())<br/>    p_x = list(map(<strong class="kp ir">lambda </strong>n: n/total_occurrences, x_counter.values()))<br/>    s_x = ss.entropy(p_x)<br/>    <strong class="kp ir">if </strong>s_x == 0:<br/>        <strong class="kp ir">return </strong>1<br/>    <strong class="kp ir">else</strong>:<br/>        <strong class="kp ir">return </strong>(s_x - s_xy) / s_x</span></pre><p id="2866" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将此应用于蘑菇数据集:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nd"><img src="../Images/319981b40faa9968277af81ddc2a63d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XPVvypiS7o6R0Fs4h8SvBA.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Theil’s U calculated for the mushrooms data-set</figcaption></figure><p id="0b60" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个新的计算让我们从克莱姆的V中看到了更多的关联——例如，我们现在看到，虽然知道<em class="kl">气味</em>比蘑菇的<em class="kl">种类</em>提供了很多信息，但反过来却不是这样。泰尔的U确实给了我们更多关于不同特征之间真实关系的信息。</p><h2 id="abdf" class="ld le iq bd lf lg lh dn li lj lk dp ll jy lm ln lo kc lp lq lr kg ls lt lu lv bi translated">当我们把东西混在一起时会发生什么？</h2><p id="fe73" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">因此，现在我们有了一种方法来衡量两个连续特征之间的相关性，以及两种方法来衡量两个分类特征之间的关联。但是一对连续特征和一个分类特征呢？为此，我们可以使用<a class="ae kq" href="https://en.wikipedia.org/wiki/Correlation_ratio" rel="noopener ugc nofollow" target="_blank">相关比率</a>(通常使用希腊字母<em class="kl"> eta </em>来标记)。数学上定义为每个类别均值的加权方差除以所有样本的方差；在人类的语言中，相关比回答了下面这个问题:<em class="kl">给定一个连续数，你能多好地知道它属于哪一类？</em>就像我们之前看到的两个系数一样，这里的输出也在[0，1]的范围内。</p><p id="9973" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Python中的实现如下所示:</p><pre class="ks kt ku kv gt mv kp mw mx aw my bi"><span id="0b55" class="ld le iq kp b gy mz na l nb nc"><strong class="kp ir">def </strong>correlation_ratio(categories, measurements):<br/>    fcat, _ = pd.factorize(categories)<br/>    cat_num = np.max(fcat)+1<br/>    y_avg_array = np.zeros(cat_num)<br/>    n_array = np.zeros(cat_num)<br/>    <strong class="kp ir">for </strong>i <strong class="kp ir">in </strong>range(0,cat_num):<br/>        cat_measures = measurements[np.argwhere(fcat == i).flatten()]<br/>        n_array[i] = len(cat_measures)<br/>        y_avg_array[i] = np.average(cat_measures)<br/>    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)<br/>    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))<br/>    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))<br/>    <strong class="kp ir">if </strong>numerator == 0:<br/>        eta = 0.0<br/>    <strong class="kp ir">else</strong>:<br/>        eta = np.sqrt(numerator/denominator)<br/>    <strong class="kp ir">return </strong>eta</span></pre><h2 id="e55f" class="ld le iq bd lf lg lh dn li lj lk dp ll jy lm ln lo kc lp lq lr kg ls lt lu lv bi translated">最后的话</h2><p id="9535" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">我相信我可以宣布，对分类特征的关联度量的研究是成功的，特别是当某些要求——例如对非对称度量的需要——在开始时没有被预期到。在探索包含分类特征的数据集时，这三个新指标非常有用，并帮助我对我探索的数据集获得更多见解。我只能希望这能像对我一样对你有用，如果没有的话，至少你现在知道如何识别可食用的蘑菇了。</p></div></div>    
</body>
</html>