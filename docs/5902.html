<html>
<head>
<title>Neural Network from scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中从头开始的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65?source=collection_archive---------1-----------------------#2018-11-15">https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65?source=collection_archive---------1-----------------------#2018-11-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5332" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">制作自己的机器学习库。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0f34ec9d15cbc4d6e74eae1f76b2930d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YtYVlwMAM7s7cwgI"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@cadop?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Mathew Schwartz</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4138" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将从头开始学习机器学习的数学和代码，用 Python，一个小型库来构建具有各种层(全连接、卷积等)的神经网络。).最终，我们将能够以模块化的方式创建网络:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/4e6f9e60336b16478567852337304728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*Ld6m0dsfSkJMnbXaoIAOVw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">3-layer neural network</figcaption></figure><p id="03df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我假设你已经有了一些关于神经网络的知识。这里的目的不是解释我们为什么制作这些模型，而是向<strong class="ky ir">展示如何进行适当的实现</strong>。</p><h1 id="91e1" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">一层层地</h1><p id="cac7" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">我们需要记住这里的大局:</p><ol class=""><li id="e085" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">我们将<strong class="ky ir">输入</strong>的数据输入神经网络。</li><li id="600f" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">数据从层<strong class="ky ir">流向层</strong>，直到我们有了<strong class="ky ir">输出</strong>。</li><li id="185c" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">一旦我们有了输出，我们就可以计算<strong class="ky ir">误差</strong>，它是一个<strong class="ky ir">标量</strong>。</li><li id="98ec" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">最后，我们可以通过减去相对于<strong class="ky ir"> </strong>参数本身的误差的<strong class="ky ir">导数</strong>来调整给定的参数(权重或偏差)。</li><li id="1280" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">我们重复这个过程。</li></ol><p id="9f58" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最重要的一步是第四步。我们希望能够拥有任意多的层，并且是任意类型的。但是，如果我们修改/添加/删除网络中的一层，网络的输出将会改变，这将改变误差，这将改变误差相对于参数的导数。我们需要能够计算导数，不管网络结构如何，不管激活函数如何，不管我们使用的损耗如何。</p><p id="3443" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了实现这一点，我们必须分别实现<strong class="ky ir">每一层</strong>。</p><h1 id="f7a5" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">每一层应该实现什么</h1><p id="80ab" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">我们可能创建的每一层(全连接、卷积、最大池、丢弃等。)至少有 2 个共同点:<strong class="ky ir">输入</strong>和<strong class="ky ir">输出</strong>数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/02336bfb05456a56ce72687a70671b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/0*3cUc4jsQlHsoovn9.png"/></div></figure><h2 id="2daf" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated"><strong class="ak">正向传播</strong></h2><p id="dc08" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">我们已经可以强调一个要点，那就是:<strong class="ky ir">一层的输出是下一层的输入</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/6c660249acd629ba82193347edad5380.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*ddDKxWSAYci2dHaG3O_DOg.png"/></div></figure><p id="ca61" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这被称为<strong class="ky ir">正向传播。本质上，我们给第一层输入数据，然后每一层的输出成为下一层的输入，直到我们到达网络的末端。通过比较网络的结果(Y)和期望的输出(假设 Y*)，我们可以计算 en 误差<strong class="ky ir"> E </strong>。目标是<strong class="ky ir">通过改变网络中的参数来最小化</strong>该误差。也就是反向传播<strong class="ky ir"> </strong> (backpropagation)。</strong></p><h2 id="a168" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">梯度下降</h2><p id="e386" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">这是一个快速的<strong class="ky ir">提醒</strong>，如果你需要学习更多关于梯度下降的知识，互联网上有大量的资源。</p><p id="8330" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基本上，我们想要改变网络中的某个参数(称之为<strong class="ky ir"> w </strong>，以便总误差<strong class="ky ir"> E 减小</strong>。有一个聪明的方法可以做到这一点(不是随机的)，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/aa1478c5b817253a4e8ebb9fef1994b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/0*MZYPbIwBn5SOEpG7.png"/></div></figure><p id="6aba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<strong class="ky ir"> α </strong>是我们设置的范围【0，1】内的参数，称为<strong class="ky ir">学习率</strong>。无论如何，这里重要的是<strong class="ky ir">∂e/∂w</strong>(e 相对于 w 的导数)。<strong class="ky ir">我们需要能够为网络的任何参数找到该表达式的值，而不管其架构如何。</strong></p><h2 id="6539" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">反向传播</h2><p id="dec0" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">假设我们给一个层误差相对于其输出 (∂E/∂Y)的<strong class="ky ir">导数，那么它必须能够提供误差相对于其输入</strong> (∂E/∂X).)的<strong class="ky ir">导数</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/81d24104b2fffd38f2435484fd283fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bL0jVGoQiH_TsrvT.png"/></div></div></figure><p id="d484" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">记住<code class="fe nv nw nx ny b">E</code>是一个<strong class="ky ir">标量</strong>(一个数)<strong class="ky ir">T22<code class="fe nv nw nx ny b">X</code>和<code class="fe nv nw nx ny b">Y</code>是<strong class="ky ir">矩阵</strong>。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/378beffbd00272ade51c1dfb5ff6b105.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/0*KYFYEbu_o9j1_a5w.png"/></div></figure><p id="a9f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们暂时忘掉∂E/∂X 吧。这里的诀窍是，如果我们可以访问∂E/∂Y，我们可以非常容易地计算∂E/∂W(如果该层有任何可训练的参数)<strong class="ky ir">而无需了解任何网络架构！</strong>我们简单地使用链式法则:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0b79b113a1d565482cfa7c7bd4c77d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/0*3xceKA-7b2oNbE4T.png"/></div></figure><p id="30a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">未知的是<strong class="ky ir"> ∂y_j/∂w </strong>，这完全取决于该层如何计算其输出。因此，如果每一层都可以访问∂E/∂Y，其中 y 是它自己的输出，那么我们可以更新我们的参数！</p><h2 id="f121" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">但是为什么是∂E/∂X 呢？</h2><p id="01ab" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">别忘了，一层的输出是下一层的输入。这意味着一层的∂E/∂X 是前一层的∂E/∂Y！就是这样！这只是传播错误的一种巧妙方式！同样，我们可以使用链式法则:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/31ef3ebc82955cab166d36f61a432274.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/0*Of8qDiWo31MK-MU0.png"/></div></figure><p id="77e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这<em class="lt">非常</em>重要，是理解反向传播的<em class="lt">关键</em>！在那之后，我们马上就能从头开始编写一个深度卷积神经网络了！</p><h2 id="f1e2" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">用于理解反向传播的图表</h2><p id="eb01" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">这就是我之前描述的。第 3 层将使用∂E/∂Y 更新其参数，然后将∂E/∂H2 传递给上一层，即它自己的“∂E/∂Y".”第二层也会这样做，以此类推。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/480363d0c4421a68a145e206542cb461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0QPRST83oBicKPE_R4biJA.png"/></div></div></figure><p id="5817" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这在这里可能看起来很抽象，但是当我们将它应用到一个特定类型的层时，它会变得非常清晰。说到<em class="lt">摘要</em>，现在是写我们第一个 python 类的好时机。</p><h2 id="b8c6" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">抽象基类:层</h2><p id="360c" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">所有其他层将继承的抽象类<em class="lt">层</em>处理简单的属性，即一个<strong class="ky ir">输入</strong>，一个<strong class="ky ir">输出</strong>，以及一个<strong class="ky ir">向前</strong>和<strong class="ky ir">向后</strong>方法。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">3-layer neural network</figcaption></figure><p id="c465" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如你所见，<code class="fe nv nw nx ny b">backward_propagation</code>中还有一个我没有提到的额外参数，那就是<code class="fe nv nw nx ny b">learning_rate</code>。这个参数应该类似于更新策略，或者像 Keras 中所说的优化器，但是为了简单起见，我们将简单地传递一个学习率，并使用梯度下降来更新我们的参数。</p><h1 id="b801" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">全连接层</h1><p id="8a51" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">现在让我们定义和实现第一种类型的层:全连接层或 FC 层。FC 层是最基本的层，因为每个输入神经元都连接到每个输出神经元。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/b696f8ff3f9204992c5f3745ebc06506.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*jOObSF4HAB5VL5wO6petqA.png"/></div></figure><h2 id="e81a" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">正向传播</h2><p id="b21f" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">每个输出神经元的值可以计算如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/5e240c48f571c825d1ad481b2d7409ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/0*Ec9fiXkNOoA5Z2v5.png"/></div></div></figure><p id="19f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用矩阵，我们可以使用<strong class="ky ir">点积</strong>一次性计算每个输出神经元的公式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/d7d255de5dbdb701ed1f4c8101efc250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qTBPwmzIjBOGKpXH.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/07fb37cab8807d55140b5a4fdf54249f.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/0*FsJQ82GmKlV2X22z.png"/></div></figure><p id="da5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们完成了向前传球。现在我们来做 FC 层的后向传递。</p></div><div class="ab cl oj ok hu ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="ij ik il im in"><p id="291e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，我还没有使用任何激活函数，因为我们将在一个单独的层中实现它！</p></div><div class="ab cl oj ok hu ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="ij ik il im in"><h2 id="f39e" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">反向传播</h2><p id="4ac1" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">如前所述，假设我们有一个矩阵，其中包含误差相对于该层输出的导数(∂E/∂Y).)我们需要:</p><ol class=""><li id="ac62" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">误差相对于参数的导数(∂E/∂W，∂E/∂B)</li><li id="85db" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">误差相对于输入的导数(∂E/∂X)</li></ol><p id="eebe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们来计算∂E/∂W.这个矩阵的大小应该和 w 本身一样:<code class="fe nv nw nx ny b">ixj</code>其中<code class="fe nv nw nx ny b">i</code>是输入神经元的数量<code class="fe nv nw nx ny b">j</code>是输出神经元的数量。我们需要<strong class="ky ir">一个梯度对应一个重量</strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/3995498d164fffda251f42f87820bf4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/0*PiVZ-czmfvBaAFSe.png"/></div></figure><p id="6f9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用前面陈述的链式法则，我们可以写出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/c2fe739959e91cf44eb92b51d8529ea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/0*cKYlxf87ZwkKtnrt.png"/></div></div></figure><p id="1041" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/398414108edac0fd4c1ed85c95361a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/0*sEeQVqIapym6O9VH.png"/></div></figure><p id="d2e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就这样，我们有了更新权重的第一个公式！现在让我们来计算∂E/∂B.</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/8403516b7e7039d4e7adf5eb87873085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/0*sJVXt05jf2cgd_Ys.png"/></div></figure><p id="5f0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，∂E/∂B 需要和 b 本身一样大，每个偏差一个梯度。我们可以再次使用链式法则:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/b8b29cc971a33fca4c5762df862b40ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/0*ud43sxuKpgPo9Rlo.png"/></div></figure><p id="4cd2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并得出结论，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/0652055ac27664375672f75511ba5cd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/0*sovSd27ja1_7R2yU.png"/></div></figure><p id="ecf2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经有了<strong class="ky ir"> ∂E/∂W </strong>和<strong class="ky ir"> ∂E/∂B </strong>、<strong class="ky ir"> </strong>我们只剩下<strong class="ky ir"> ∂E/∂X </strong>这是<strong class="ky ir">非常重要的</strong>，因为它将“充当”前一层的∂E/∂Y。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/2c0a47610f8f1c75cf31af8ed9c1da85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/0*nPzn6Jgv-P0wxUA7.png"/></div></figure><p id="f453" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">再次使用链式法则，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/aa11a1a89cf2d78862f342d42fb73180.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/0*x6uE01GkG3NKLNQp.png"/></div></figure><p id="a5b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们可以写出整个矩阵:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/ab44678687cdf52e5a012d3ed474aef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JWCzIdtJVTeQ_PG8.png"/></div></div></figure><p id="506e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！我们已经有了 FC 层所需的三个公式！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/4d35988278a21f430cdb439f49e316f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/0*HlI8qj8qZqGIWBrk.png"/></div></figure><h2 id="532b" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">对全连接层进行编码</h2><p id="66e8" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">我们现在可以编写一些 python 代码来实现这种数学！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><h1 id="250c" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">活化层</h1><p id="3607" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">到目前为止，我们所做的所有计算都是完全线性的。用那种模式学东西是没希望的。我们需要通过将非线性函数应用于某些层的输出来将<strong class="ky ir">非线性</strong>添加到模型中。</p><p id="e7c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们需要为这个新类型的层重做整个过程！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oz oe l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="https://gph.is/21pKLjE" rel="noopener ugc nofollow" target="_blank">https://gph.is/21pKLjE</a></figcaption></figure><p id="f037" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">别担心，这会快得多，因为没有可学习的参数。我们只需要计算<strong class="ky ir"> ∂E/∂X </strong>。</p><p id="aa9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将分别称<code class="fe nv nw nx ny b">f</code>和<code class="fe nv nw nx ny b">f'</code>为激活函数及其导数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/e986ff01305fd359d998588efc3c0f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*xl7UacJfCUAk_KqX3WI49Q.png"/></div></figure><h2 id="0df3" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">正向传播</h2><p id="8f51" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">正如您将看到的，这非常简单。对于给定的输入<code class="fe nv nw nx ny b">X</code>，输出只是应用于<code class="fe nv nw nx ny b">X</code>的每个元素的激活函数。也就是说<strong class="ky ir">输入</strong>和<strong class="ky ir">输出</strong>具有<strong class="ky ir">相同的尺寸</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/2ce87a6874dada213fa08b495360cae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/0*Aw9jCvpliMjaO00W.png"/></div></figure><h2 id="2b9b" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">反向传播</h2><p id="e89f" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">给定<strong class="ky ir"> ∂E/∂Y </strong>，我们要计算<strong class="ky ir"> ∂E/∂X </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/8d54d150bb10b95ae5b393e647b0bf2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9aXZPVNHXXpz1vd9.png"/></div></div></figure><p id="a7ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，这里我们使用了两个矩阵之间的<strong class="ky ir">元素乘法</strong>(而在上面的公式中，它是点积)。</p><h2 id="dd53" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">激活层编码</h2><p id="926b" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">激活层的代码非常简单。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="0044" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你也可以在一个单独的文件中写一些激活函数和它们的导数。这些将被用来创建一个<code class="fe nv nw nx ny b">ActivationLayer</code>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><h1 id="b02a" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">损失函数</h1><p id="ec1c" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">直到现在，对于一个给定的层，我们假设<strong class="ky ir"> ∂E/∂Y </strong>是<strong class="ky ir"> </strong>给定的(由下一层给出)。但是最后一层会发生什么呢？它是如何得到<strong class="ky ir"> ∂E/∂Y </strong>的？我们简单地手动给出，这取决于我们如何定义误差。</p><p id="36bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">网络的误差由<strong class="ky ir"> you </strong>定义，它衡量网络对于给定输入数据的好坏。定义误差的方法有很多种，其中最广为人知的一种叫做<strong class="ky ir">MSE——均方误差</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/d06d37f06948b5d24457c8fd82cdf1ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/0*CuLKvZnTvjT1d6KJ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Mean Squared Error</figcaption></figure><p id="4e5e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<code class="fe nv nw nx ny b">y*</code>和<code class="fe nv nw nx ny b">y</code>分别表示<strong class="ky ir">期望输出</strong>和<strong class="ky ir">实际输出</strong>。你可以把这种损失想象成最后一层，它把所有的输出神经元挤压成一个神经元。我们现在需要的是，像其他层一样，定义∂E/∂Y。除了现在，我们终于到了<code class="fe nv nw nx ny b">E</code>！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/e6c7acb537c5c23bc2c0f1964cd209bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/0*2ggVYLPNH-vIJir6.png"/></div></figure><p id="ccd4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这只是两个 python 函数，可以放在一个单独的文件中。创建网络时会用到它们。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><h1 id="bb0c" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">网络类</h1><p id="a8ce" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">快好了！我们将制作一个<code class="fe nv nw nx ny b">Network</code>类来创建神经网络，就像第一张图片一样简单！</p><p id="9e4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我几乎注释了代码的每一部分，如果你掌握了前面的步骤，理解起来应该不会太复杂。不过，如果你有任何问题，请留下评论，我会很乐意回答！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><h1 id="6c6f" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">构建神经网络</h1><p id="496a" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">终于！我们可以使用我们的类来创建一个神经网络，我们想要多少层就有多少层！我们将构建两个神经网络:一个简单的<strong class="ky ir">异或</strong>和一个<strong class="ky ir"> MNIST </strong>解算器。</p><h2 id="bdc0" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">求解异或</h2><p id="3d02" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">从 XOR 开始总是很重要的，因为这是一种简单的方法来判断网络是否正在学习任何东西。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="f1dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很多东西我觉得不需要强调。只是要小心训练数据，你应该总是有<strong class="ky ir">样本</strong>维度<strong class="ky ir">第一个</strong>。例如这里，输入形状是<strong class="ky ir"> (4，1，2) </strong>。</p><h2 id="fa3b" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">结果</h2><pre class="kg kh ki kj gt pf ny pg ph aw pi bi"><span id="e218" class="ng lv iq ny b gy pj pk l pl pm"><strong class="ny ir">$ python xor.py</strong> <br/>epoch 1/1000 error=0.322980<br/>epoch 2/1000 error=0.311174<br/>epoch 3/1000 error=0.307195<br/>...<br/>epoch 998/1000 error=0.000243<br/>epoch 999/1000 error=0.000242<br/>epoch 1000/1000 error=0.000242</span><span id="25fb" class="ng lv iq ny b gy pn pk l pl pm">[<br/>    array([[ 0.00077435]]),<br/>    array([[ 0.97760742]]),<br/>    array([[ 0.97847793]]),<br/>    array([[-0.00131305]])<br/>]</span></pre><p id="f83f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很明显这是有效的，太好了！我们现在可以解决更有趣的事情了，让我们解决 MNIST 吧！</p><h2 id="e634" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">解决 MNIST</h2><p id="d61a" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">我们没有实现卷积层，但这不是问题。我们需要做的只是重塑我们的数据，以便它可以适合一个完全连接的层。</p><p id="4d9f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lt"> MNIST 数据集由从 0 到 9 的数字图像组成，形状为 28x28x1。目标是预测图片上画的是什么数字。</em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><h2 id="274f" class="ng lv iq bd lw nh ni dn ma nj nk dp me lf nl nm mg lj nn no mi ln np nq mk nr bi translated">结果</h2><pre class="kg kh ki kj gt pf ny pg ph aw pi bi"><span id="60db" class="ng lv iq ny b gy pj pk l pl pm"><strong class="ny ir">$</strong> <strong class="ny ir">python example_mnist_fc.py</strong><br/>epoch 1/30   error=0.238658<br/>epoch 2/30   error=0.093187<br/>epoch 3/30   error=0.073039<br/>...<br/>epoch 28/30   error=0.011636<br/>epoch 29/30   error=0.011306<br/>epoch 30/30   error=0.010901</span><span id="3923" class="ng lv iq ny b gy pn pk l pl pm"><strong class="ny ir">predicted values : </strong><br/>[<br/>    array([[ 0.119,  0.084 , -0.081,  0.084, -0.068, 0.011,  0.057,  <strong class="ny ir">0.976</strong>, -0.042, -0.0462]]),<br/>    array([[ 0.071,  0.211,  <strong class="ny ir">0.501</strong> ,  0.058, -0.020, 0.175,  0.057 ,  0.037,  0.020,  0.107]]),<br/>    array([[ 1.197e-01,  <strong class="ny ir">8.794e-01</strong>, -4.410e-04, 4.407e-02, -4.213e-02,  5.300e-02, 5.581e-02,  8.255e-02, -1.182e-01, 9.888e-02]])<br/>]<br/><strong class="ny ir">true values : </strong><br/>[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]<br/> [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]<br/> [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]</span></pre><p id="9121" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是完美的工作！太神奇了:)</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="po oe l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="https://gph.is/2jzemp3" rel="noopener ugc nofollow" target="_blank">https://gph.is/2jzemp3</a></figcaption></figure><h1 id="91a0" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">GitHub 知识库和 Google Colab</h1><p id="425a" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">您可以在下面的 GitHub 资源库和 Google Colab 文件中找到本文使用的全部工作代码。它还包含其他层的代码，如<strong class="ky ir">卷积</strong>或<strong class="ky ir">展平</strong>。</p><div class="pp pq gp gr pr ps"><a href="https://github.com/OmarAflak/Medium-Python-Neural-Network" rel="noopener  ugc nofollow" target="_blank"><div class="pt ab fo"><div class="pu ab pv cl cj pw"><h2 class="bd ir gy z fp px fr fs py fu fw ip bi translated">OmarAflak/Medium-Python-神经网络</h2><div class="pz l"><h3 class="bd b gy z fp px fr fs py fu fw dk translated">通过在 GitHub 上创建一个帐户，为 OmarAflak/Medium-Python-Neural-Network 开发做出贡献。</h3></div><div class="qa l"><p class="bd b dl z fp px fr fs py fu fw dk translated">github.com</p></div></div><div class="qb l"><div class="qc l qd qe qf qb qg kp ps"/></div></div></a></div><div class="pp pq gp gr pr ps"><a href="https://colab.research.google.com/drive/10y6glU28-sa-OtkeL8BtAtRlOITGMnMw" rel="noopener  ugc nofollow" target="_blank"><div class="pt ab fo"><div class="pu ab pv cl cj pw"><h2 class="bd ir gy z fp px fr fs py fu fw ip bi translated">Python 中从头开始的神经网络</h2><div class="pz l"><h3 class="bd b gy z fp px fr fs py fu fw dk translated">请随时联系我</h3></div><div class="qa l"><p class="bd b dl z fp px fr fs py fu fw dk translated">colab.research.google.com</p></div></div><div class="qb l"><div class="qh l qd qe qf qb qg kp ps"/></div></div></a></div><p id="632d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我最近把那篇文章的内容放到了一个精美的动画视频里。你可以在 YouTube 上看看。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="qi oe l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Neural Network from Scratch | Mathematics &amp; Python Code — The Independent Code</figcaption></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="qi oe l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Convolutional Neural Network from Scratch | Mathematics &amp; Python Code — The Independent Code</figcaption></figure></div><div class="ab cl oj ok hu ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="ij ik il im in"><h1 id="cc4b" class="lu lv iq bd lw lx qj lz ma mb qk md me jw ql jx mg jz qm ka mi kc qn kd mk ml bi translated"><strong class="ak">如果你喜欢这篇文章——如果你按下鼓掌按钮，我会非常感激</strong>👏这对我帮助很大。和平！😎</h1></div></div>    
</body>
</html>