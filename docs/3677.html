<html>
<head>
<title>Support Vector Machine — Introduction to Machine Learning Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机——机器学习算法简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47?source=collection_archive---------0-----------------------#2018-06-07">https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47?source=collection_archive---------0-----------------------#2018-06-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7f24" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从零开始的 SVM 模式</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3777abcce6d987dab8e2b8b100d6dce8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FgHPTrzeFwHE5bbSRVrAxw.jpeg"/></div></div></figure><h2 id="9af1" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">介绍</h2><p id="0818" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv la lw lx ly le lz ma mb li mc md me mf ij bi translated">我猜现在你已经习惯了线性回归和 T2 逻辑回归算法。如果没有，我建议你在学习支持向量机之前先看看它们。支持向量机是另一种简单的算法，每个机器学习专家都应该拥有它。支持向量机是许多人的首选，因为它以较少的计算能力产生显著的准确性。支持向量机，缩写为 SVM，可用于回归和分类任务。但是，它被广泛用于分类目标。</p><h2 id="0982" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">什么是支持向量机？</h2><p id="ef85" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv la lw lx ly le lz ma mb li mc md me mf ij bi translated">支持向量机算法的目标是在 N 维空间(N-特征的数量)中找到一个超平面，该超平面清楚地分类数据点。</p><div class="kg kh ki kj gt ab cb"><figure class="mh kk mi mj mk ml mm paragraph-image"><img src="../Images/bbf5e5a73d9d3738471248739bbf7179.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*9jEWNXTAao7phK-5.png"/></figure><figure class="mh kk mn mj mk ml mm paragraph-image"><img src="../Images/8d1c69e4700c14c39c526b73bd941e4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*0o8xIA4k3gXUDCFU.png"/><figcaption class="mo mp gj gh gi mq mr bd b be z dk ms di mt mu">Possible hyperplanes</figcaption></figure></div><p id="9b32" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv la mx lx ly le my ma mb li mz md me mf ij bi translated">为了分离这两类数据点，有许多可能的超平面可供选择。我们的目标是找到一个具有最大余量的平面，即两类数据点之间的最大距离。最大化边缘距离提供了一些加强，以便可以更有把握地对未来的数据点进行分类。</p><h2 id="1336" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">超平面和支持向量</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/c3911a7ed2237a7e30f740e64152689c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZpkLQf2FNfzfH4HXeMw4MQ.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Hyperplanes in 2D and 3D feature space</figcaption></figure><p id="09a0" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv la mx lx ly le my ma mb li mz md me mf ij bi translated">超平面是帮助分类数据点的决策边界。落在超平面任一侧的数据点可以归属于不同的类别。此外，超平面的维数取决于特征的数量。如果输入特征的数量是 2，那么超平面只是一条线。如果输入特征的数量是 3，则超平面变成二维平面。当特征的数量超过 3 时，变得难以想象。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/d46523a057f6cb7b1a64735a7251b217.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ecA4Ls8kBYSM5nza.jpg"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Support Vectors</figcaption></figure><p id="2cf8" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv la mx lx ly le my ma mb li mz md me mf ij bi translated">支持向量是更接近超平面并影响超平面的位置和方向的数据点。使用这些支持向量，我们最大化分类器的余量。删除支持向量将改变超平面的位置。这些是帮助我们建设 SVM 的要点。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nc nd l"/></div></figure><h2 id="679d" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">大幅度直觉</h2><p id="94a9" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv la lw lx ly le lz ma mb li mc md me mf ij bi translated">在逻辑回归中，我们采用线性函数的输出，并使用 sigmoid 函数压缩[0，1]范围内的值。如果挤压值大于阈值(0.5)，我们将其指定为标签 1，否则将其指定为标签 0。在 SVM，我们获取线性函数的输出，如果该输出大于 1，我们将其识别为一个类，如果输出为-1，我们将其识别为另一个类。由于在 SVM 阈值被改变为 1 和-1，我们获得了这个值的加强范围([-1，1])，其作为余量。</p><h2 id="8bab" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">成本函数和梯度更新</h2><p id="7185" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv la lw lx ly le lz ma mb li mc md me mf ij bi translated">在 SVM 算法中，我们希望最大化数据点和超平面之间的间隔。有助于最大化裕量的损失函数是铰链损失。</p><div class="kg kh ki kj gt ab cb"><figure class="mh kk ne mj mk ml mm paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/9def072dedf6f84d81ccbcf64a082726.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*3xErahGeTFnbDiRuNXjAuA.png"/></div></figure><figure class="mh kk nf mj mk ml mm paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/502ccd71079f2c28780c88ebd8921854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*hHlytjVk6d7O2WWvG2Gdig.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk ng di nh mu">Hinge loss function (function on left can be represented as a function on the right)</figcaption></figure></div><p id="bd2a" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv la mx lx ly le my ma mb li mz md me mf ij bi translated">如果预测值和实际值符号相同，则成本为 0。如果不是，我们就计算损失值。我们还在代价函数中加入了一个正则化参数。正则化参数的目标是平衡余量最大化和损失。添加正则化参数后，成本函数如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/65eb599a4badb4b448a1b98aced968c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GQAd28bK8LKOL2kOOFY-tg.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Loss function for SVM</figcaption></figure><p id="fd4b" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv la mx lx ly le my ma mb li mz md me mf ij bi translated">现在我们有了损失函数，我们对权重求偏导数来求梯度。使用梯度，我们可以更新我们的权重。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/4db4a46686898461d8f73922dde94f85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WUphtYLfTOAoaXQXvImBeA.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Gradients</figcaption></figure><p id="f876" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv la mx lx ly le my ma mb li mz md me mf ij bi translated">当没有错误分类时，即我们的模型正确地预测了我们的数据点的类别时，我们只需要根据正则化参数更新梯度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/de60216f545cf80383abb10072156011.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*-nKEXrWos8Iuf-DWSv_srQ.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Gradient Update — No misclassification</figcaption></figure><p id="f5aa" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv la mx lx ly le my ma mb li mz md me mf ij bi translated">当存在错误分类时，即我们的模型在预测我们的数据点的类别时出错，我们将损失与正则化参数一起包括进来以执行梯度更新。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/ec94187820ddf938e8f6e7fbd6d7a62f.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*tnvMhAKaTUCO43diEvtTAQ.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Gradient Update — Misclassification</figcaption></figure><h2 id="1dd6" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">Python 中的 SVM 实现</h2><p id="81c6" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv la lw lx ly le lz ma mb li mc md me mf ij bi translated">我们将用来实现 SVM 算法的数据集是虹膜数据集。可以从这个<a class="ae mg" href="https://www.kaggle.com/jchen2186/machine-learning-with-iris-dataset/data" rel="noopener ugc nofollow" target="_blank">链接</a>下载。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nd l"/></div></figure><p id="bf2e" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv la mx lx ly le my ma mb li mz md me mf ij bi translated">由于 Iris 数据集有三个类，我们将删除其中一个类。这给我们留下了一个二元类分类问题。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nd l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/977471c059071be471a685b722fbb610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TJy6SQItr1a-_Zqg4SP9CQ.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Visualizing data points</figcaption></figure><p id="6dca" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv la mx lx ly le my ma mb li mz md me mf ij bi translated">另外，有四个特性可供我们使用。我们将只使用两个特征，即萼片长度和花瓣长度。我们把这两个特征画出来。从上图中，你可以推断出可以用一条直线来分隔数据点。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nd l"/></div></figure><p id="54a1" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv la mx lx ly le my ma mb li mz md me mf ij bi translated">我们提取所需的特征，并将其分成训练和测试数据。90%的数据用于训练，剩下的 10%用于测试。现在让我们使用 numpy 库来构建我们的 SVM 模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nd l"/></div></figure><p id="4fa4" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv la mx lx ly le my ma mb li mz md me mf ij bi translated">α(0.0001)是学习率，正则化参数λ被设置为 1/历元。因此，正则化值减少了历元数的增加。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nd l"/></div></figure><p id="97c1" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv la mx lx ly le my ma mb li mz md me mf ij bi translated">我们现在裁剪权重，因为测试数据只包含 10 个数据点。我们从测试数据中提取特征并预测值。我们获得预测值，并将其与实际值进行比较，打印出我们模型的准确性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/8908b738263ca065a4e01dfed52da43e.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*g055rcAbUMmIpjYMNiQm3A.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Accuracy of our SVM model</figcaption></figure><p id="bc53" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv la mx lx ly le my ma mb li mz md me mf ij bi translated">还有另一种简单的方法来实现 SVM 算法。我们可以使用 Scikit 学习库，只需调用相关函数来实现 SVM 模型。代码的行数明显减少太少行。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nd l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nd l"/></div></figure><h2 id="eaaf" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">结论</h2><p id="dfd2" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv la lw lx ly le lz ma mb li mc md me mf ij bi translated">支持向量机是一种优雅而强大的算法。明智地使用它:)</p></div></div>    
</body>
</html>