<html>
<head>
<title>Generating Large, Synthetic, Annotated, &amp; Photorealistic Datasets for Computer Vision</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为计算机视觉生成大型、合成、带注释和照片级真实感数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-large-synthetic-annotated-photorealistic-datasets-for-computer-vision-ffac6f50a29c?source=collection_archive---------16-----------------------#2018-09-25">https://towardsdatascience.com/generating-large-synthetic-annotated-photorealistic-datasets-for-computer-vision-ffac6f50a29c?source=collection_archive---------16-----------------------#2018-09-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e409df055736a76e4d5d6a4e61c10a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QeeQJCqJfHF45aUI0zvrCg.jpeg"/></div></div></figure><p id="9119" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我给大家介绍一下我们在 Greppy 一直在做的一个工具的 beta 版，叫 Greppy 元宇宙(<em class="kw">2020 年 2 月 18 日更新:</em> <a class="ae kx" href="https://synthesis.ai/" rel="noopener ugc nofollow" target="_blank"> <em class="kw">合成 AI </em> </a> <em class="kw">已经获得了这个软件，请通过</em><a class="ae kx" href="https://synthesis.ai/" rel="noopener ugc nofollow" target="_blank"><em class="kw">Synthesis . AI</em></a>联系他们！)，它通过快速轻松地生成大量用于机器学习的训练数据，来辅助计算机视觉对象识别/语义分割/实例分割。<em class="kw">(旁白:如果可以的话，合成 AI 也喜欢帮助你的项目——在 LinkedIn </em>  <em class="kw">上通过</em><a class="ae kx" href="https://synthesis.ai/contact/" rel="noopener ugc nofollow" target="_blank">【https://synthesis.ai/contact/】</a><em class="kw">或</em> <a class="ae kx" href="https://www.linkedin.com/company/synthesis-ai/" rel="noopener ugc nofollow" target="_blank"> <em class="kw">联系他们)。</em></a></p><p id="b962" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您过去做过图像识别，您会知道数据集的大小和准确性非常重要。你的所有场景也需要被注释，这可能意味着成千上万的图像。对于我们的小团队来说，这样的时间和精力是不可扩展的。</p><h2 id="e7cd" class="ky kz iq bd la lb lc dn ld le lf dp lg kj lh li lj kn lk ll lm kr ln lo lp lq bi translated">概观</h2><p id="3005" class="pw-post-body-paragraph jy jz iq ka b kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr lv kt ku kv ij bi translated">因此，我们发明了一种工具，使得创建大型的带注释的数据集更加容易。我们希望这可以对增强现实、自主导航和机器人技术有所帮助——通过生成识别和分割各种新对象所需的数据。</p><p id="d82e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们甚至开源了我们的<a class="ae kx" href="https://drive.google.com/file/d/1J4fnNh9IaXa6gkqbU93V2RmZStr8M9Bf/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"> VertuoPlus 豪华银数据集</a>，其中有 1000 个咖啡机的场景，所以你可以一起玩！是个<a class="ae kx" href="https://drive.google.com/file/d/1J4fnNh9IaXa6gkqbU93V2RmZStr8M9Bf/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"> 6.3 GB 下载</a>。</p><p id="1d2e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了展示它的功能，我将在 Greppy 这里为您展示一个真实的例子，我们需要用一个<a class="ae kx" href="https://click.intel.com/intelr-realsensetm-depth-camera-d435.html" rel="noopener ugc nofollow" target="_blank">英特尔实感 D435 </a>深度摄像头来识别我们的咖啡机及其按钮。将来会有更多关于<em class="kw">为什么</em>我们想认识我们的咖啡机，但可以说我们经常需要咖啡因。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lw"><img src="../Images/5a5c355c8f5eda045098ee76f038dc7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XL3b1-iN6aBuCnvUMi3tIg.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Screenshot of the Greppy Metaverse website</figcaption></figure><h2 id="1990" class="ky kz iq bd la lb lc dn ld le lf dp lg kj lh li lj kn lk ll lm kr ln lo lp lq bi translated">在过去的日子里，我们不得不用手来注释。</h2><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mf"><img src="../Images/43ba2687edaec29559d36e22ace00cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OIgrsG_R7tUYxbmf9NRaiw.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">VGG Image Annotator tool example, courtesy of <a class="ae kx" href="https://www.linkedin.com/in/waleedka/" rel="noopener ugc nofollow" target="_blank">Waleed Abdulla’s</a> “<a class="ae kx" href="https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46" rel="noopener ugc nofollow" target="_blank">Splash of Color</a>”</figcaption></figure><p id="1836" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于过去的大多数数据集，注释任务都是由(人类)手工完成的。正如你在左边看到的，这不是特别有趣的工作，而且和所有人类的事情一样，它容易出错。</p><p id="5bce" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">它也几乎不可能准确地注释其他重要信息，如物体姿态，物体法线和深度。</p><h2 id="00fc" class="ky kz iq bd la lb lc dn ld le lf dp lg kj lh li lj kn lk ll lm kr ln lo lp lq bi translated">合成数据:10 年前的想法</h2><p id="89c0" class="pw-post-body-paragraph jy jz iq ka b kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr lv kt ku kv ij bi translated">人工标记的一个有希望的替代方法是合成产生(理解:计算机生成)的数据。这是一个已经存在了十多年的想法(参见这个<a class="ae kx" href="https://github.com/unrealcv/synthetic-computer-vision" rel="noopener ugc nofollow" target="_blank"> GitHub repo 链接到许多这样的项目</a>)。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/dc2a18cec59b1df9617a93a71cbbc9bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*CMfY6KeJ_AvmDe6ykRWVjw.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">From <a class="ae kx" href="http://people.csail.mit.edu/jmarin/docs/cvpr10.pdf" rel="noopener ugc nofollow" target="_blank">Learning Appearance in Virtual Scenarios for Pedestrian Detection, 2010</a></figcaption></figure><p id="b1e6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">不过，我们在现有项目中遇到了一些问题，因为它们要么需要编程技能才能使用，要么不能输出逼真的图像。我们需要一些非编程团队成员可以用来帮助有效生成大量数据以识别新类型对象的东西。此外，我们的一些对象在没有光线追踪(<a class="ae kx" href="https://en.wikipedia.org/wiki/Ray_tracing_(graphics)" rel="noopener ugc nofollow" target="_blank">维基百科</a>)的情况下很难产生真实感，这是其他现有项目没有使用的技术。</p><h2 id="1273" class="ky kz iq bd la lb lc dn ld le lf dp lg kj lh li lj kn lk ll lm kr ln lo lp lq bi translated">用格雷皮·元宇宙制作大规模合成数据</h2><p id="6f0e" class="pw-post-body-paragraph jy jz iq ka b kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr lv kt ku kv ij bi translated">为了达到我们想要的目标数量，我们制作了 Greppy 元宇宙工具。例如，我们可以使用来自<a class="ae kx" href="https://3dwarehouse.sketchup.com/" rel="noopener ugc nofollow" target="_blank"> 3D 仓库</a>网站的预制 CAD 模型，并使用网络界面使它们更加逼真。或者，我们的艺术家可以创建一个定制的 3D 模型，但不必担心如何编码。</p><p id="7fee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们回到咖啡上。使用我们的工具，我们首先上传我们拥有的 Nespresso VertuoPlus 豪华银机的 2 个非真实感 CAD 模型。我们实际上上传了两个 CAD 模型，因为我们想在两种配置中识别机器。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/65c3aafcbd449f59982ce1e4060a346a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*l-P0PdovTPwLAmbMkd1X5Q.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Custom-made CAD models by our team.</figcaption></figure><p id="a43c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">CAD 模型上传后，我们从预先制作的照片级真实材料中进行选择，并应用到每个表面。Greppy 元宇宙的目标之一是建立一个开源的、真实感的材料库，任何人都可以使用(最好是在社区的帮助下！).作为一个旁注，3D 艺术家通常需要创建自定义材料。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/6a1d3ed76e70c1f8e3b1785d2e2455a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/1*OQ2Xm8fZ1CehwSGACqtZ0w.gif"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Select pre-made, photorealistic materials for CAD models.</figcaption></figure><p id="aa69" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了能够识别机器的不同部分，我们还需要标注我们关心机器的哪些部分。web 界面提供了实现这一点的工具，因此不了解 3D 建模软件的人可以帮助完成这一注释。不需要 3D 艺术家或程序员；-)</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/827059d291f89780ccda337f8725e4fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/1*e3t-nxRirK79jYRgr1HRpg.gif"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Easily label all the parts of interest for each object.</figcaption></figure><p id="00ab" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后……就这样了！我们自动生成多达数万个场景，这些场景在姿势、对象实例数量、相机角度和照明条件方面各不相同。它们都会被自动标注，并且精确到像素。在幕后，该工具用 GPU 旋转了一堆云实例，并在一个小“renderfarm”上渲染这些变化。</p><p id="01b1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以下是来自<a class="ae kx" href="https://drive.google.com/open?id=1J4fnNh9IaXa6gkqbU93V2RmZStr8M9Bf" rel="noopener ugc nofollow" target="_blank">开源 VertuoPlus 豪华银数据集</a>的 RGB 图像示例:</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/6e2f92432130381df0269a777be81d04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OOHwPVYjbpLLlBc16nfkGA.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">A lot of scene RGBs with various lighting conditions, camera angles, and arrangements of the object.</figcaption></figure><p id="65c2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于每个场景，我们输出一些东西:基于所选相机的单目或立体相机 RGB 图片，相机看到的深度，所有对象和对象部分的像素完美注释，相机和每个对象的姿势，最后，场景中对象的表面法线。</p><p id="ba2c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我再次强调<strong class="ka ir">任何场景都不需要手工标注！</strong></p><p id="0a80" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">单个场景的输出示例如下:</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mk"><img src="../Images/8ee5aa610f2f0e5ee6936062442dafa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0VKxbwUUqjXglyFlGJYvnQ.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Output examples from each scene</figcaption></figure><h2 id="b7ff" class="ky kz iq bd la lb lc dn ld le lf dp lg kj lh li lj kn lk ll lm kr ln lo lp lq bi translated">基于合成数据集的机器学习</h2><p id="7676" class="pw-post-body-paragraph jy jz iq ka b kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr lv kt ku kv ij bi translated">随着整个数据集的生成，很容易使用它来训练 Mask-RCNN 模型(关于 Mask-RCNN 的历史有一个很好的<a class="ae kx" href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4" rel="noopener ugc nofollow" target="_blank">帖子</a>)。在后续的帖子中，我们将使用 Mask-RCNN 的<a class="ae kx" href="https://github.com/matterport/Mask_RCNN" rel="noopener ugc nofollow" target="_blank"> Matterport 实现，开源我们用于从 Greppy 元宇宙数据集训练 3D 实例分割的代码。</a></p><p id="59c0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">与此同时，这里有一个小小的预览。这是来自<a class="ae kx" href="https://click.intel.com/intelr-realsensetm-depth-camera-d435.html" rel="noopener ugc nofollow" target="_blank">英特尔实感 D435 </a>相机的原始捕捉数据，左边是 RGB，右边是对齐的深度(总共构成 4 个通道的 RGB-D):</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/898c94134de9a011f0dd7cb0a0fffe9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZCNI1zORcCbeteIgKu01Jg.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Raw data capture from Intel RealSense D435. Yes, that’s coffee, tea, and vodka together ;-)</figcaption></figure><p id="4a8d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于这个 Mask-RCNN 模型，我们在具有大约 1000 个场景的开源数据集上进行训练。经过 30 个时期的模型训练后，我们可以在上面的 RGB-D 上看到运行推理。瞧！我们得到一个几乎 100%确定的输出掩码，只对合成数据进行训练。</p><p id="d6a0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当然，我们也会开源培训代码，这样你就可以自己验证了。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/9f3cf7719415102fe3b6e3c62a98184c.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*8ugYiu6wTHeCT0E9EuV16g.png"/></div></figure><p id="c6ea" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦我们能够识别图像中的哪些像素是感兴趣的对象，我们就可以使用英特尔实感框架来收集这些像素处咖啡机的深度(以米为单位)。知道 Nespresso 机器的精确像素和精确深度对任何 AR、导航规划和机器人操纵应用都非常有帮助。</p></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h2 id="7a3a" class="ky kz iq bd la lb lc dn ld le lf dp lg kj lh li lj kn lk ll lm kr ln lo lp lq bi translated">总结想法</h2><p id="0628" class="pw-post-body-paragraph jy jz iq ka b kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr lv kt ku kv ij bi translated">目前，Greppy 元宇宙还处于测试阶段，我们还有很多需要改进的地方，但我们对目前的结果非常满意。</p><p id="5edd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">同时，如果你有一个项目需要帮助，请联系 https://synthesis.ai/contact/<a class="ae kx" href="https://synthesis.ai/contact/" rel="noopener ugc nofollow" target="_blank">的合成人工智能</a>或 LinkedIn 的<a class="ae kx" href="https://www.linkedin.com/company/synthesis-ai/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="3a6e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">特别感谢<a class="ae kx" href="https://www.linkedin.com/in/waleedka/" rel="noopener ugc nofollow" target="_blank">瓦利德·阿卜杜拉</a>和<a class="ae kx" href="https://www.linkedin.com/in/jyip4849/" rel="noopener ugc nofollow" target="_blank">叶锦华</a>帮助改进这篇文章:)</p></div></div>    
</body>
</html>