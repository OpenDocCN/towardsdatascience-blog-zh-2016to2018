<html>
<head>
<title>Parameter Inference — Maximum Likelihood</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">参数推断—最大似然</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/parameter-inference-maximum-likelihood-2382ef895408?source=collection_archive---------1-----------------------#2017-04-03">https://towardsdatascience.com/parameter-inference-maximum-likelihood-2382ef895408?source=collection_archive---------1-----------------------#2017-04-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/d18ccc176b7c0e25189cf9f966db492f.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*ULb9lkajm38eGIfrtRX7ZQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">The never ending debate of Frequentists and Bayesians. (Image source: <a class="ae jy" href="https://xkcd.com/1132/" rel="noopener ugc nofollow" target="_blank">xkcd</a>)</figcaption></figure><p id="7c0b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi kx translated">他的文章深入探讨了理论机器学习最重要的概念之一，即。，参数推断。当我觉得需要的时候，我会试着专注于对概念的直觉理解，同时嵌入数学公式。如果你曾经在大学上过本科/研究生水平的机器学习课程，你一定遇到过参数推断。当我第一次遇到这种情况时，我绞尽脑汁了好一阵子，试图理解圆周率的概念(不是一个众所周知的参数推断的缩写)，这就是为什么我觉得有必要写一篇文章，让其他人更容易理解。我希望当我们完成这篇文章后，你能从中获取一些有价值的东西。我们开始吧。</p><p id="6d81" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">PI 可以自由地描述为确定参数的过程，这些参数控制着从实验中生成的数据集。例如，如果我们抛硬币十次，得到下面的数据集，</p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/91d393b159826f6603ff1e30d59e07e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*7n4g4m4X2t8cVxlZiEl-eA.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 1: 10 flips of a coin</figcaption></figure><p id="c10d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">产生这十次翻转的参数是什么？事实证明，参数推断的过程并非完全无关紧要，需要相当多的数学知识才能理解。如果你了解对数、微积分(准确地说是微分)和基本概率的基础知识，这篇文章应该不难理解。无论如何，让我们言归正传。PI 可以被认为是一个分步骤的过程，有三个主要步骤:</p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/2e5c3ee0548012cccec0a06964dec2ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*Ldi4VHZW0t3HOHu445knfQ.jpeg"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 2: Three steps to Parameter Inference.</figcaption></figure><p id="5a7d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们从最低的一步开始，然后努力到达最高的一步。每一步都比前一步稍微复杂一些，但同时，它为我们提供了一个更健壮的模型来描述数据。这篇文章将详细讨论最底层的步骤<em class="lm">最大似然估计</em>，后续文章将讨论另外两个步骤。<em class="lm">(附带提示:使用笔和纸可以轻松理解术语和公式，因为介质尚不支持数学符号)</em>。好吧，那么 MLE 是什么？</p><h1 id="e1d8" class="ln lo iq bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">最大似然估计</h1><p id="42c7" class="pw-post-body-paragraph jz ka iq kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw ij bi translated">如图所示，最大似然估计的整个过程围绕着确定使数据概率最大化的参数。回到抛硬币的例子，你认为第 11 次抛硬币是反面的概率是多少？有两个合乎逻辑的答案:</p><ol class=""><li id="01e1" class="mq mr iq kb b kc kd kg kh kk ms ko mt ks mu kw mv mw mx my bi translated">如果你说 0.5，这是一个合理的答案，因为翻转是相互独立的，下一次翻转是<em class="lm">尾</em>有 1/2 的机会，即 0.5。</li><li id="a9a4" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">另一个答案可能是 0.3。这个答案背后的基本原理是，我们可以预期硬币的翻转会继续到目前为止发生的方式(直到第十次翻转)。</li></ol><p id="2ef6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我暂时不会告诉你正确的答案。让我们一起努力找出哪一个是正确的。好的，那么我们需要找出<em class="lm">P_11</em>(<em class="lm">F_11</em>=<em class="lm">Tails</em>)，其中 P _ 11 是第 11 次翻转的概率，F _ 11 是第 11 次翻转。这产生了一般情况:<em class="lm"> i </em> th 翻转为<em class="lm"> Tails: </em></p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/2141f8f7e595d57cc1ae36ab74eee058.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*Xi7G1x760IshTCU-XOW6Eg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 3</figcaption></figure><p id="3df7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">其中<em class="lm"> F_i </em>表示第<em class="lm"> i </em>次翻转，而<em class="lm">θ_ I</em>是控制第<em class="lm"> i </em>次翻转的参数。为了表示概率分布取决于<em class="lm">θ_ I</em>，我们可以写为:</p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/400bc472695a8e8c01fab58ecfe0dabb.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*2J8lTJcHrHUbK-MYqjw45g.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 4</figcaption></figure><p id="b04e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">你看到这里的问题了吗？我们有依赖于参数<em class="lm"> theta_1 </em>到<em class="lm"> theta_10 </em> (i = 1 到 10) <em class="lm">的数据。这就提出了一个问题，那就是我们如何把它推广到。暂时还是坚持现有的，即<em class="lm"> theta_1 </em>到<em class="lm"> theta_10。</em></em></p><p id="bfeb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">翻转序列的所有随机性由参数<em class="lm"> theta_1 </em>至<em class="lm"> theta_10: </em>控制(建模)</p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/4fff100ab66613b42a7788f8f7ce6fe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*kQx4YgaFA0FQUCh5F6Uceg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 5</figcaption></figure><p id="08d6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们目前对<em class="lm"> theta_1 了解多少..theta_10 </em>？他们是否以某种方式与<em class="lm"> theta_11 </em>联系在一起？乍一看，似乎没有什么联系。现在我们可以开始 MLE 的过程了:我们需要找到<em class="lm"> theta_i 的</em>使得</p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/4fff100ab66613b42a7788f8f7ce6fe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*kQx4YgaFA0FQUCh5F6Uceg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 6</figcaption></figure><p id="05b0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">是尽可能高的。这是 MLE 背后的原则:</p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/80d06471f51e1ebf6327fa07154e945d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d5CXJKGkh1r0ywYa7vwJ4Q.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 7</figcaption></figure><p id="4ad7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">MLE 着眼于数据的概率(所谓的<em class="lm">可能性；</em> Img。5 &amp; 6】并且它试图找到使该序列的可能性/概率最大化的那些参数<em class="lm">θ_ 1</em>到<em class="lm">θ_ 10</em>。最后一次重申，我们要选择那些参数，在这些参数下，我们的观察变得最有可能。这就是这个过程被称为 MLE 的原因。</p><p id="fef6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在让我们尝试实际模拟 Img 中描述的概率。5 &amp; 6.为了能够计算出数据的最大似然估计，我们需要做出两个关键假设:</p><p id="84e0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">一、抛硬币互不影响，即互不相关<em class="lm"/>(第一次抛硬币的结果不影响第二次抛硬币的结果，以此类推)<em class="lm"> : </em></p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nm"><img src="../Images/ff910bb9db4dfeaf382f68a474dad213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GsOToZE0CzwQ8gBrf0lqsg.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 8: The independence assumption allows us to simplify the complex likelihood term into ten simpler factors.</figcaption></figure><p id="bd21" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">Img 中的第三个语句。8 只是一个简写符号，描述第二条语句中十项的乘积。</p><p id="97f1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">第一个假设允许我们大大简化似然项，但是请注意，速记符号中仍然存在<em class="lm"> theta_i </em>。这意味着我们仍然没有等式中的<em class="lm"> theta_11 </em>。</p><p id="f76d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">还有一个假设可以进一步解开这个等式。这是因为硬币(一般来说，实验设置)没有显著变化:</p><p id="b319" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">二。翻转在数量上是相同的，即它们是<em class="lm">同分布的:</em></p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/2d9f8447895095d91ecbf6938d1e0c88.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*6jp2fsqHM84SZcBAQSiq6A.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 9: Since the flips are taking place under similar circumstances, we can assume that the parameter governing the flips is one and same.</figcaption></figure><p id="6f37" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">第二个假设是翻转是同分布的，允许我们从<em class="lm">θ</em>中去掉下标<em class="lm"> i </em>。这意味着我们观察到的所有 10 次翻转本质上都是由同一个参数<em class="lm">θ决定的；不再有十个参数来控制十次不同的翻转，我们现在只有一个参数来控制整个硬币翻转的顺序，这也包括第 11 次翻转。从某种意义上来说，我们正在把前 10 次抛硬币和第 11 次抛硬币联系起来。我们很快就会看到，这将是推论的关键。</em></p><p id="9d37" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">注意:我们所做的两个假设在机器学习中使用得如此频繁，以至于它们作为一个实体有一个特殊的名称，<strong class="kb ir"> i.i.d .假设</strong>:</p><p id="286f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">总的来说，这 10 次翻转是独立的，并且分布相同。</strong></p><p id="caf1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这允许我们明确地写下我们试图优化的可能性。记住<em class="lm"> theta </em>被定义为翻转出现<em class="lm">尾部</em>的概率；我们的序列 w . r . t .<em class="lm">θ</em>的概率现在可以表述为:</p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi no"><img src="../Images/d6a3d76b5c2e7ec9d8c21bfac98a2ebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vEQyqeA7o7LrBLREjwt2eA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 10: First term is the probability of <em class="np">Heads, second for Tails, third and fourth for Heads, fifth for Tails and so on. Note that this formula corresponds to the sequence of our coin flip.</em></figcaption></figure><p id="65df" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">因此，通过这种非常广泛和非常直观的假设，我们将似然函数简化为一个相当简单的多项式；我们可以开始优化参数<em class="lm"> theta 的函数。</em></p><p id="a527" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">根据我们的模型假设:</p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/44df48aaf204b21a13731f58436886d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*a5E6oxJNN2SF87w-F1tZew.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 11</figcaption></figure><p id="d7c4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">上述简化的多项式可以解释为<em class="lm">θ</em>的函数，即<em class="lm">f(θ)</em>。现在我们想找出这个函数的最大值(最大似然)。对如何进行有什么想法吗？</p><p id="5552" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">没错！从这里开始都是基本的高中数学。我们取导数<em class="lm">df/d(θ)</em>，设为零，求解<em class="lm">θ。</em>然后验证<a class="ae jy" href="http://mathinsight.org/critical_points_monotone_increase_decrease_refresher" rel="noopener ugc nofollow" target="_blank">临界点</a>(函数斜率为零的点:极大值、极小值和鞍点。在这种情况下，我们只关心最大值)，通过将它们插入到<em class="lm"> f(theta) </em>的二阶导数中。</p><p id="eb58" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">原则上，这相当简单。除了一个警告:f(theta) 的二阶导数已经很难看了，因为我们需要应用<a class="ae jy" href="https://www.math.ucdavis.edu/~kouba/CalcOneDIRECTORY/productruledirectory/ProductRule.html" rel="noopener ugc nofollow" target="_blank">乘积法则</a> <em class="lm">两次</em>才能得到它。这是可行的，但要实现这一目标，需要大量单调的技术工作。有什么方法可以简化我们的计算吗？</p><p id="14fb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">数学的美妙之处在于它为我们提供了无数的途径来找到一个特定的解决方案。其中一些比另一些容易几倍。我们将应用一个更简单的途径来优化我们的似然函数:</p><blockquote class="nr ns nt"><p id="f877" class="jz ka lm kb b kc kd ke kf kg kh ki kj nu kl km kn nv kp kq kr nw kt ku kv kw ij bi translated">让我们稍微了解一下高中微积分，分析一下单调函数(如果你已经对这个概念很熟悉了，可以继续到本节末尾)。我们已经知道如何找出函数的最大值或最小值，并且实际的最大值和最小值被称为函数的临界点<em class="iq">。<em class="iq">数学中有一条规则，即</em>如果我们将一个单调函数应用于我们正在优化的另一个函数，单调函数的应用将保留原始函数的临界点<em class="iq">(维基百科文章</em>  <em class="iq">是了解更多关于单调函数的极好来源)。</em></em></p><p id="d753" class="jz ka lm kb b kc kd ke kf kg kh ki kj nu kl km kn nv kp kq kr nw kt ku kv kw ij bi translated"><em class="iq">如果 x1 &lt; x2 = &gt; f(x1) &lt; f(x2):这是一个</em>单调递增函数<em class="iq">。</em></p><p id="7101" class="jz ka lm kb b kc kd ke kf kg kh ki kj nu kl km kn nv kp kq kr nw kt ku kv kw ij bi translated"><em class="iq">如果 x1 &lt; x2 = &gt; f(x1) &gt; f(x2):这是一个</em>单调递减函数<em class="iq">。</em></p><p id="6060" class="jz ka lm kb b kc kd ke kf kg kh ki kj nu kl km kn nv kp kq kr nw kt ku kv kw ij bi translated"><a class="ae jy" href="http://catalog.flatworldknowledge.com/bookhub/reader/4372?e=fwk-redden-ch07_s03" rel="noopener ugc nofollow" target="_blank"> <em class="iq">对数函数</em> </a> <em class="iq">是单调递增函数的一个例子:</em></p></blockquote><figure class="lh li lj lk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nx"><img src="../Images/42a04d0ac7a436c7d2e5ed238c726acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ehJ78FmFQJH0Lp-RxS6v5w.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img 12: A monotonically increasing function — log(x)</figcaption></figure><blockquote class="nr ns nt"><p id="a439" class="jz ka lm kb b kc kd ke kf kg kh ki kj nu kl km kn nv kp kq kr nw kt ku kv kw ij bi translated"><em class="iq">因此，我们可以断定</em> log f(theta) <em class="iq">与</em> f(theta)具有相同的最大值。</p></blockquote><p id="f354" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">因此，如果我们将 log 应用于我们的似然函数，我们将得到与我们直接优化它所得到的相同的最大值。</p><p id="febd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">所以我们想找出:</p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi ny"><img src="../Images/e45869ae0d20692d454a9392dc15e193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B3P2xKu7fZBIm-qMVXhpZQ.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 13: arg max f(theta)</figcaption></figure><p id="5403" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><a class="ae jy" href="http://math.stackexchange.com/questions/312012/what-is-the-difference-between-arg-max-and-max" rel="noopener ugc nofollow" target="_blank"> Arg max </a>是指我们想知道这个函数最大化时<em class="lm">θ</em>的<em class="lm">值，而不是函数本身的<em class="lm">最大值</em>。这意味着我们<strong class="kb ir">而不是</strong>真正关心的是函数的实际最大值。相反，我们感兴趣的是函数具有最大值的<em class="lm">θ</em>的值(在继续之前，让它沉淀片刻)。</em></p><p id="7d42" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">取该函数的日志:</p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nz"><img src="../Images/5a3f576b931ae56d6c780af4e73bdc93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YXjdMk4r-pW3rHH6yGY-Wg.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 13: log arg max f(theta)</figcaption></figure><p id="aa4b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们用<em class="lm"> g(theta) </em>表示这个函数:对数似然。</p><p id="123c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对<em class="lm">g(θ)</em>求导</p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi oa"><img src="../Images/9a9806f2f6d4d06ff219d5278178a142.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wpiKmSTX-FimvVUl0rfA9A.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 14: derivative of g(theta)</figcaption></figure><p id="1591" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">其中|T|是尾部的数量(在我们的示例中为 3)，而|H|是头部的数量(在我们的示例中为 7)。我引入 T 和 H 是为了使解决方案具有普遍性。</p><p id="9ef1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">将<em class="lm">g’(θ)</em>的值设置为零(以找到<em class="lm">θ</em>的临界值):</p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi ob"><img src="../Images/1670c87ee99583be1a443538e6504b5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*etcdQfUP7x6B81iMVcSoZg.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 15: Critical value of theta — I</figcaption></figure><figure class="lh li lj lk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi oc"><img src="../Images/80e72170c23db9d7a376691f26246620.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i3Tl5NHvJ6ODl8YeTnZB6g.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 16: Critical value of theta — II</figcaption></figure><p id="b46a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">因此，任何硬币序列的最大似然估计(MLE ):</p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div class="gh gi od"><img src="../Images/484c0595583ac3fec1a2ce8ad4dfbef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*LEmKYhQzPSbppID40IIkoA.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 17: Theta MLE</figcaption></figure><p id="dedb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们已经得到了参数<em class="lm">θ</em>的最大似然估计，该参数控制着我们的硬币投掷数据集。回到原来的问题，第 11 次翻转是<em class="lm">尾</em>的概率是多少？</p><p id="bbed" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">设|T| = 3 和|H| = 7，我们得到的答案为 0.3。这证明了 30%是我们最初问题的合理答案。</p><p id="33d3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在我们需要在二阶导数中代入<em class="lm">θ</em>的这个临界值，来验证它确实是最大值。为了简洁起见，我在这里跳过了这个验证。但是请继续下去，说服自己这确实是最大值。</p><p id="6720" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这个最大值被称为<em class="lm">θ的<em class="lm"> MLE。</em>这个特殊的<em class="lm"> theta </em>是最有可能让我们观察到的序列。</em></p><p id="1a01" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">直觉上，这个 30%的值也是合理的，因为我们预计硬币会以开始的方式继续翻转。所以，我们发现当我们看数据时，MLE 解释了我们的直觉，这很好。</p><h1 id="a5fc" class="ln lo iq bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">MLE 的缺点？</h1><p id="95b3" class="pw-post-body-paragraph jz ka iq kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw ij bi translated">显然，正如我们在阶梯图中已经看到的，找到最大似然并不是参数估计的结束。我们还剩下<em class="lm">最大事后估计</em>和<em class="lm">完全贝叶斯分析。</em>那么，是什么促使我们不止步于 MLE 并完成流程呢？你能想出一个理由吗？</p><p id="ca1c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果序列如下所示会怎样:</p><figure class="lh li lj lk gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/515e4683f64c9dfe4db6fdba7963623b.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*TFDkfzT6GM_ZCMd9-SKVoQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 18: A sequence of two Heads</figcaption></figure><p id="18a6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">你认为第三次翻转是<em class="lm">反面</em>的概率是多少？</p><p id="bfac" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在即将到来的<a class="ae jy" href="https://medium.com/@rahulbohare/parameter-inference-maximum-aposteriori-estimate-49f3cd98267a" rel="noopener">帖子</a>中，我们将探索 MLE 的缺点，找出 MAP ( <em class="lm">最大后验估计</em>)和<em class="lm">完全贝叶斯分析</em>背后的直观原理，并且还将建立 ML 中的几个关键概念，即。共轭先验，归纳先验和一种允许我们计算不可解积分的技术。</p><p id="b330" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果你心中有任何概念对你来说没有直观的意义，并希望看到类似的帖子，请在评论中告诉我，我会尽力提出来。</p><h2 id="cfec" class="of lo iq bd lp og oh dn lt oi oj dp lx kk ok ol mb ko om on mf ks oo op mj oq bi translated">资源:</h2><ol class=""><li id="5bb9" class="mq mr iq kb b kc ml kg mm kk or ko os ks ot kw mv mw mx my bi translated">我在慕尼黑工业大学的研究生院教授的精彩的 ML 课程。你可以在这个 YouTube <a class="ae jy" href="https://www.youtube.com/channel/UCpyPuMAyBvERU8YMngGy11g" rel="noopener ugc nofollow" target="_blank">频道</a>观看所有讲座。</li></ol></div><div class="ab cl ou ov hu ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="ij ik il im in"><p id="31f4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果你喜欢这篇文章，请随意推荐和分享，这样其他人也可以从中受益。</p></div></div>    
</body>
</html>