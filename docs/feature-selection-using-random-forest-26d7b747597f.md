# 使用随机森林的特征选择

> 原文：<https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f?source=collection_archive---------1----------------------->

## 群体的智慧

![](img/fb9a46b613c59f1623bcd9c8776363e8.png)

Photo by [Noah Silliman](https://unsplash.com/@noahsilliman?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

随机森林是最流行的机器学习算法之一。它们之所以如此成功，是因为它们总体上提供了良好的预测性能、较低的过拟合和易于解释的能力。这种可解释性是由这样一个事实给出的，即很容易推导出每个变量在决策树中的重要性。换句话说，很容易计算出每个变量对决策的影响程度。

使用随机森林的特征选择属于嵌入式方法的范畴。嵌入式方法结合了过滤器和包装器方法的优点。它们是由算法实现的，这些算法有自己内置的特征选择方法。嵌入式方法的一些好处是:

*   它们非常精确。
*   他们概括得更好。
*   它们是可以解释的

**随机森林如何选择特征？**

随机森林由 4 到 1200 个决策树组成，每个决策树都是基于从数据集中随机提取的观察值和随机提取的特征构建的。不是每棵树都能看到所有的特征或所有的观察结果，这保证了树是不相关的，因此不容易过度拟合。每个树也是一系列基于单一或组合特征的是非问题。在每个节点处(这是在每个问题处)，这三个节点将数据集分成两个桶，每个桶存放的观察值彼此更相似，但与另一个桶中的观察值不同。因此，每个特征的重要性来源于每个桶的“纯净”程度。

**分类和回归的工作方式不同吗？**

对于*分类*，杂质的度量或者是*基尼杂质*或者是*信息增益/熵。*

对于*回归*，杂质*的度量是方差。*

因此，在训练树时，可以计算每个特征减少杂质的程度。一个特征减少杂质越多，该特征就越重要。在随机森林中，每个特征的杂质减少可以跨树进行平均，以确定变量的最终重要性。

> 为了给出更好的直觉，在树的顶部选择的特征通常比在树的末端节点选择的特征更重要，因为通常顶部分裂导致更大的信息增益。

***让我们来看一些关于如何使用随机森林选择要素的 Python 代码。***

在这里，我不会将随机森林应用于实际的数据集，但它可以很容易地应用于任何实际的数据集。

1.  *导入库*

```
import pandas as pd
from sklearn.ensemble import RandomForestClassfier
from sklearn.feature_selection import SelectFromModel
```

2.*在所有特征选择过程中，通过仅检查训练集来选择特征是一个好的实践。这是为了避免过度拟合。*

考虑到我们有一个训练和一个测试数据集。我们从训练集中选择特征，然后将变化转移到测试集中。

```
X_train,y_train,X_test,y_test = train_test_split(data,test_size=0.3)
```

3.*在这里，我将在一行代码中完成模型拟合和特征选择。*

*   首先，我指定了随机森林实例，指出了树的数量。
*   然后我用 sklearn 的`selectFromModel`对象自动选择特征。

```
sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))
sel.fit(X_train, y_train)
```

默认情况下，`SelectFromModel`将选择重要性大于所有特征的平均重要性的那些特征，但我们可以根据需要改变该阈值。

4.*为了查看哪些特征是重要的，我们可以在拟合的模型上使用* `get_support` *方法。*

```
sel.get_support()
```

它将返回一个由*布尔*值组成的数组。**重要性大于平均重要性的特征为真**，其余为假**。**

**5.*我们现在可以列出并统计所选的特性。***

```
selected_feat= X_train.columns[(sel.get_support())]
len(selected_feat)
```

**它将返回一个整数，表示随机森林选择的要素数量。**

**6.*获取所选特征的名称***

```
print(selected_feat)
```

**它将返回所选特征的名称。**

**7.我们还可以检查并绘制重要性的分布。**

```
pd.series(sel.estimator_,feature_importances_,.ravel()).hist()
```

**它将返回一个直方图，显示使用该特征选择技术选择的特征的分布。**

> **我们当然可以调整决策树的参数。我们选择特性的截止点有点武断。一种方法是选择前 10、20 个特征。或者，前 10%的人。为此，我们可以将 mutual info 与 sklearn 的 SelectKBest 或 SelectPercentile 结合使用。**

****随机森林的局限性有:****

*   **相关特征将被赋予相同或相似的重要性，但是与没有相关对应物而构建的相同树相比，整体重要性降低。**
*   **一般来说，随机森林和决策树优先选择基数高的特征(树偏向于这些类型的变量)。**

**通过使用树导出的特征重要性来选择特征是为机器学习选择良好特征的一种非常直接、快速且通常准确的方式。特别是，如果我们要构建树方法。**

**然而，正如我所说的，如果树是在没有相关对应物的情况下构建的，那么与它们的重要性相比，相关的特征将在树中显示出相似和较低的重要性。**

**在这种情况下，最好递归地选择特性，而不是像我们在这里所做的那样一起选择。**