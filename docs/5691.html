<html>
<head>
<title>Capsule Neural Networks – Part 2: What is a Capsule?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">胶囊神经网络–第 2 部分:什么是胶囊？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/capsule-neural-networks-part-2-what-is-a-capsule-846d5418929f?source=collection_archive---------6-----------------------#2018-11-03">https://towardsdatascience.com/capsule-neural-networks-part-2-what-is-a-capsule-846d5418929f?source=collection_archive---------6-----------------------#2018-11-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b675" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">胶囊神经网络中的这些“胶囊”是关于什么的？这篇文章将以简单的语言(和狗脸)给你完整的直觉和洞察力，以及后来深入理解它们的技术细节。</h2></div><blockquote class="kf kg kh"><p id="3d1a" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">这是胶囊网络解释系列的第二部分。<a class="ae lf" rel="noopener" target="_blank" href="/capsule-neural-networks-are-here-to-finally-recognize-spatial-relationships-693b7c99b12">帖子#1 在这里</a>，如果你还没看的话，看看吧。</p></blockquote><h1 id="d1d4" class="lg lh iq bd li lj lk ll lm ln lo lp lq jw lr jx ls jz lt ka lu kc lv kd lw lx bi translated">胶囊:直觉</h1><p id="ba92" class="pw-post-body-paragraph ki kj iq kl b km ly jr ko kp lz ju kr ma mb ku kv mc md ky kz me mf lc ld le ij bi translated">在经典的 CNN 中，第一层的每个神经元代表一个像素。然后，它将这些信息转发给下一层。接下来的卷积层将一群神经元组合在一起，这样单个神经元就可以代表一整帧(一群)神经元。因此，它可以学习表示一组看起来有点像<em class="kk">鼻子、</em>的像素，特别是如果我们的数据集中有许多这样的例子，当识别图像是否是狗时，神经网络将学习增加那个<em class="kk">鼻子神经元</em>特征的权重(重要性)。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mg"><img src="../Images/1156c1a15319540c16d546452c452177.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jdRxRtfc72coKq5bSwECiQ.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Abstraction 1 of how a regular Convolutional Neural Network would recognize sub-structures <strong class="bd mw">present </strong>in the big picture, regardless of their location</figcaption></figure><p id="28fa" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">然而，这种方法只关心特定位置周围的图片中对象的存在；但是它对物体的空间关系和方向不敏感。<br/> <strong class="kl ir">但不惧！胶囊来拯救我们了！<em class="kk">胶囊</em> </strong> s 是一个新概念，它可以包含<strong class="kl ir">更多关于每个【对象】的</strong> <em class="kk"> </em>信息。<br/> <br/>胶囊是一个<strong class="kl ir"> <em class="kk">向量</em> </strong>(一个具有大小和方向的元素)指定了对象的<strong class="kl ir"><em class="kk"/></strong>特征及其<strong class="kl ir"> <em class="kk">可能性</em> </strong>。这些特征可以是任何实例化参数，如“<em class="kk">姿态“</em>”(位置、大小、方向)、变形、速度、反照率(光反射)、色调、纹理等。</p><p id="0515" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">因此，例如，神经网络可以学习拥有一个代表“眼睛”的胶囊，它将包含关于它所看到的所有眼睛变化的信息，而不是针对不同眼睛变化的不同神经元。例如，除了关于看起来像“眼睛”的像素组的信息之外，胶囊还可以指定它的属性，如角度和大小，因此，如果我们使用这些角度或大小参数，它可以用相同的一般眼睛信息来表示各种眼睛。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/a81287739d18cb302b153a541c5f45d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*DE-aicjCBBzBIGOmJ3wxuQ.png"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Abstraction of how a Capsule Neural Network would recognize sub-structures present in the big picture, along with information about the location and direction of these elements.</figcaption></figure><p id="9e88" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">现在，就像神经网络有多层神经元一样，胶囊网络也可以有多层胶囊。所以可能有更高的胶囊代表它们下面的一组物体(胶囊)。例如，在第三层你可能有代表“眼睛”、“鼻子”、“嘴”的胶囊，在第四层你可能有代表“狗脸”的胶囊。因此，胶囊在识别一系列变化的特征时更加灵活和稳健。</p><p id="d05a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">但是等等……还有更多！</p><p id="84a9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">我们的智能胶囊可以利用这些信息进行更好的识别。直观上，<strong class="kl ir">胶囊网络可以问:<em class="kk">所有这些特征都是相似的旋转和大小吗？</em> </strong>如果不是，这个有问题的图像不太可能是狗脸(这个标签会得到较低的概率分数)。如果是的话，这增加了我们的信心，这具有重要的意义:我们刚刚创建了一个网络，它可以识别对象，即使它们是从原始输入转换而来的。下面再详细说说这两点。<br/><br/><strong class="kl ir">(1)capsnes 可以根据方向和大小的不一致性进行更好的分类以便识别。</strong>如果子元素(鼻子、眼睛和嘴巴)在方向和大小上彼此不一致(比如我们的毕加索！)<em class="kk">，</em> t <em class="kk">这时较高的胶囊会注意到并且会不太确定这是一张(常规的)狗脸。</em>对于中枢神经系统中的正常神经元，我们无法做到这一点；我们只有可能让一组像素看起来像没有方向信息的东西。通过比较胶囊每个特征的兼容性，我们可以发现它们是不一致的，并且(遗憾的是)排除了 Picasso 的可能性。<br/><br/><strong class="kl ir">②观点不变性。</strong>经典的 CNN 只能基于以相似的方向和大小存储的相似的狗脸检测器来识别狗脸。这是因为狗脸的特征存储在像素帧内的位置。例如，它可能有一个狗脸的表示，其中鼻子在像素[50，50]左右，嘴在像素[50，20]左右，眼睛在像素[20，70]和[70，70]左右。然后，它将只识别在图片中相似位置具有相似特征的图像。因此，对于“旋转 30°的狗脸”或“小狗脸”，它必须有一个单独的<em class="kk">表示。这些表示最终会映射到同一个类，但这仍然意味着 CNN 必须事先看到每种类型转换的足够多的例子，以创建它的内部表示，并在将来识别它。相比之下，胶囊网络可以具有“狗脸”的一般表示，并检查其每个特征(鼻子、嘴等)的变换(旋转、大小等)。它检查是否所有特征都以相同的量和方向</em>旋转或变换<em class="kk">，从而更加确信它确实是一张狗脸。神经网络可以直接检测到<em class="kk">这个子结构集合实际上等同于由相同数量转换的更高结构。</em>这意味着 capsnes<strong class="kl ir">将类</strong>一般化，而不是记住该类的每个视点变量<strong class="kl ir">，因此它对视点是不变的。</strong></em></p><p id="c1c2" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">这是个好消息！为什么？因为视点不变意味着:<strong class="kl ir"> (a)它对输入的方向和大小的变化更鲁棒(b)它将需要少得多的</strong> <strong class="kl ir">数据</strong>(这通常很难得到)<em class="kk"/><strong class="kl ir">内部表示</strong>，从而更有效地正确分类。这意味着(c)<strong class="kl ir">caps net 可以识别新的、<em class="kk">未见过的</em>类变体，而无需对它们进行训练！</strong></p><p id="33f1" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">从概念上讲，这是一个非常好的消息，因为<em class="kk">更像是</em> <strong class="kl ir"> <em class="kk">更像是我们人类在视觉上所做的，因此是一个重要的改进</em> </strong> <em class="kk">。当鼻子在 1.70 米，嘴在 1.67 米时，我们不是记住一张脸，而是存储嘴和鼻子之间的关系，从而可以检测出它们的任何变化。<br/> <br/>辛顿把这种<strong class="kl ir">称为胶囊的等方差</strong>。<strong class="kl ir"> </strong> <br/> <strong class="kl ir">等变</strong>是对一类<em class="kk">对象</em>的检测，这些对象可以<em class="kk">相互</em>变换(即，通过旋转、移位或任何变换)。除了识别物体本身及其变形之外，CapsNet<strong class="kl ir"/>equ variance<strong class="kl ir"/>还意味着它们可以检测出<strong class="kl ir"> <em class="kk">物体现在处于何种变形状态。</em> </strong>我们强制模型将特征变体学习到一个胶囊中，这样我们<em class="kk">可以</em>用更少的训练数据更有效地推断可能的变体。<br/>所以当物体移动，倾斜，或者大小不同时，但是<em class="kk">是同一个底层物体，</em>胶囊还是会以很高的概率检测到这个物体。这是可能的，因为胶囊在以长度为概率的向量中包含关于对象的信息，所以如果对象被变换，长度将不会改变，但是它朝向它所表示的变换维度的方向将会改变。对象的这种更健壮的表示也可以使其对对抗性攻击更健壮。简而言之，对抗性攻击是一种“愚弄”神经网络的方法，通过以人眼几乎察觉不到的方式调整图像中的像素，来确定一个对象<code class="fe my mz na nb b">[dog]</code>实际上是另一个东西<code class="fe my mz na nb b">[Trump]</code>，但在神经网络代表另一个对象的方向上刚好足够，直到网络认为它是另一个对象。具有该对象的更一般化、更健康和更健壮的表示，特别是具有对对象修改的视点不变性和弹性，可以帮助网络保持识别它是相同的对象，从而减轻这些攻击。</em></p><p id="a97e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">这对于当神经网络图像识别决定现实生活事件时很重要:像<strong class="kl ir">自动驾驶汽车检测</strong> <code class="fe my mz na nb b"><strong class="kl ir">[STOP]</strong></code> <strong class="kl ir">迹象。</strong>一个(受过良好教育的)罪犯可以在那个标志上贴一个几乎看不见的标签，然后“黑掉”汽车，将这个标志识别为一个<code class="fe my mz na nb b">[“Speed = 60”]</code>标志并继续行驶。但是，一个基于 CapsNets 而不是 CNN 的系统将更能抵御这种敌对攻击。我针对一种常见的对抗性攻击“FGSM”测试了该模型，在噪声= 50 的水平下，它的准确率下降到了 73%。它没有抗药性，但比正常的中枢神经网络表现更好。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><blockquote class="nj"><p id="0417" class="nk nl iq bd nm nn no np nq nr ns le dk translated">你大概知道了。</p><p id="4cc1" class="nk nl iq bd nm nn no np nq nr ns le dk translated">现在让我们开始讨论细节。</p></blockquote><h1 id="e7b7" class="lg lh iq bd li lj lk ll lm ln lo lp lq jw nt jx ls jz nu ka lu kc nv kd lw lx bi translated">胶囊:定义</h1><p id="89ac" class="pw-post-body-paragraph ki kj iq kl b km ly jr ko kp lz ju kr ma mb ku kv mc md ky kz me mf lc ld le ij bi translated">胶囊是一个抽象的概念，由一组神经元组成，其活动向量包含关于该对象的更多信息。有许多方法可以实现这一点。Hinton 等人选择了一种特殊的方法来实现这一点，这种方法允许使用“动态路由”。我们将在这里讨论这个实现。<br/> <br/> Sabour，Frosst &amp; Hinton (2017)用这个定义和概述打开了他们的论文:</p><blockquote class="kf kg kh"><p id="2b23" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">胶囊是一组神经元，其活动向量表示诸如对象或对象部分的特定类型的实体的实例化参数。我们用活动向量的长度来表示实体存在的概率，用它的方向来表示实例化参数。处于一个级别的活动胶囊通过变换矩阵对更高级别的胶囊的实例化参数进行预测。当多个预测一致时，更高水平的胶囊变得活跃。”</p></blockquote><h1 id="c691" class="lg lh iq bd li lj lk ll lm ln lo lp lq jw lr jx ls jz lt ka lu kc lv kd lw lx bi translated">胶囊代表什么？</h1><p id="f33e" class="pw-post-body-paragraph ki kj iq kl b km ly jr ko kp lz ju kr ma mb ku kv mc md ky kz me mf lc ld le ij bi translated">在他们的论文和所有可用的实现中，胶囊网络被用于 MNIST 手写的 0-9 位数数据集——经典的 ML 分类任务。现在让我们来看这个例子。</p><ul class=""><li id="cdb0" class="nw nx iq kl b km kn kp kq ma ny mc nz me oa le ob oc od oe bi translated"><strong class="kl ir">每个囊都是一组神经元。</strong></li><li id="3b3c" class="nw nx iq kl b km of kp og ma oh mc oi me oj le ob oc od oe bi translated"><strong class="kl ir">在 DigitCaps 层中，每个神经元代表</strong>一个<strong class="kl ir">维度</strong>，其中数字可以不同:比例和粗细、笔画粗细、倾斜、宽度、平移等。作者在每个胶囊中使用 16 个神经元来代表 16 个维度，其中一个数字可以不同。</li></ul><p id="7ec7" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">我们最想从胶囊中得到的是它的输出。<br/> <strong class="kl ir">胶囊的<em class="kk">输出</em>是一个矢量。</strong> <br/>向量本身的长度表达了“实体的存在”——意思是这个物体被[ <code class="fe my mz na nb b">5</code> ]的概率。向量的<strong class="kl ir"> <em class="kk">方向</em> </strong>被强制表示属性本身(宽度、厚度、倾斜...).它的大小被强制小于 1(因为大小它总是正的，所以在 0-1 之间),并表示成为该类的概率。它指向一个点，这个点代表它有多大，它的角度是什么。<br/> <br/> <br/>我们来勾画一个简单的例子。<br/>之前，我们让一个神经元接收表示子对象的单个标量，并输出单个标量。假设我们有一些“5”数字神经元靠近 CNN 的末端，接收来自一种特殊的 5:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ok"><img src="../Images/67d746a709cb5d49186b7b79bf698fd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*u_Kr-esxiCjMUwcQFn23jg.png"/></div></div></figure><p id="5090" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">现在，我们可以对更多的信息进行编码，如果不是一个神经元，而是一个有两个神经元的囊，也可以指示手指的角度。向量本身的长度表示该输入有多可能是该胶囊所表示的；与单个神经元输出的概率信息相同。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/28a5f18690f5ebe63e9c41d514a8f82d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*F2rk5BQyAugtwCohl9WlbQ.png"/></div></figure><p id="4b1c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated"><em class="kk">长度计算为矢量的大小。如果第一个向量是</em> <code class="fe my mz na nb b">v=(0,0.9)</code>，那么它的长度就是= <code class="fe my mz na nb b">sqrt√(0^2 + 0.9^2) = 0.9</code>。现在我们可以在胶囊中添加更多的神经元来捕捉更多的维度:尺度和厚度、宽度、笔画粗细、倾斜等。<br/> <br/> Hinton 等人(2017)在其论文中展示了在 MNIST 数据集上训练的 DigitCaps 图层胶囊中各种维度的结果，其中包含这些维度:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ol"><img src="../Images/10873a96989a785e459a5fd355734fb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-iSmw248maq06cstTAIyMQ.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">From the original paper: Sabour, Frosst and Hinton (2017), Google Brain [<a class="ae lf" href="https://arxiv.org/pdf/1710.09829.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1710.09829.pdf</a>]</figcaption></figure></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="f763" class="lg lh iq bd li lj om ll lm ln on lp lq jw oo jx ls jz op ka lu kc oq kd lw lx bi translated">胶囊是做什么的？</h1><p id="61e2" class="pw-post-body-paragraph ki kj iq kl b km ly jr ko kp lz ju kr ma mb ku kv mc md ky kz me mf lc ld le ij bi translated">胶囊学习正确的转换并优化其输出。这是怎么回事？首先，让我们回忆一下传统神经元在 CNN 中的作用:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi or"><img src="../Images/4fe7137672e5d2c5f69c0888e6cd717d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jBe31nQeWpllTd7gLdb7Qw.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Traditional Neuron part — image from <a class="ae lf" href="https://github.com/naturomics/CapsNet-Tensorflow" rel="noopener ugc nofollow" target="_blank">Naturomics</a></figcaption></figure><ol class=""><li id="e613" class="nw nx iq kl b km kn kp kq ma ny mc nz me oa le os oc od oe bi translated">接收标量作为输入(X1，X2，X3)，附加常数(1)表示偏置权重</li><li id="9888" class="nw nx iq kl b km of kp og ma oh mc oi me oj le os oc od oe bi translated">对输入标量执行加权求和(通过标量权重 w1、w2、w3 和 b)</li><li id="0764" class="nw nx iq kl b km of kp og ma oh mc oi me oj le os oc od oe bi translated">应用非线性激活函数<em class="kk">f()</em></li><li id="6341" class="nw nx iq kl b km of kp og ma oh mc oi me oj le os oc od oe bi translated">输出标量 h(取决于其学习的权重参数<em class="kk"> w，b. </em></li></ol><p id="9b46" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">我们自己的太空舱也以类似的方式运作，但有很大的不同。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ot"><img src="../Images/e30c6e13ba4ecb5b3b3b278b35a14da1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XTMOUbm-_D2YU_iBLq-KxQ.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Capsule Networks Neuron structure. Image from <a class="ae lf" href="https://github.com/naturomics/CapsNet-Tensorflow" rel="noopener ugc nofollow" target="_blank">Naturomics</a></figcaption></figure><p id="770a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated"><strong class="kl ir"> 1。输入一个矢量(<em class="kk"> u_i </em>)。</strong>传统神经元接收单个标量作为输入并输出单个标量，而胶囊接收输入向量并输出向量。这些输入向量<em class="kk"> u1、u2、u3 </em>来自前一层的胶囊。假设他们代表眼睛，鼻子/鼻子和狗嘴。它们的长度代表了它们正确识别所接收内容的概率。它们的方向表示胶囊的维度空间中底层对象的变化/状态。</p><p id="f793" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">请注意，第一层胶囊没有相同的输入，因为它不是由其他胶囊产生的。这个后面会讨论。</p><p id="d533" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">还要注意，没有<em class="kk">偏置</em>作为输入。该偏差可以包含在下一阶段“仿射变换”中，其中变换矩阵 Wij 可以包含该偏差和更复杂的运算。</p><p id="670d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated"><strong class="kl ir"> 2。仿射变换</strong>。</p><p id="a5f1" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">这一步是胶囊独有的。它将一个<strong class="kl ir">变换矩阵</strong> <em class="kk"> Wij </em>应用于前一层的矢量<em class="kk"> u1/u2/u3 </em>。例如，假设我们从一个大小为<strong class="kl ir"><em class="kk">m</em></strong>×<strong class="kl ir">×k</strong>的矩阵和一个大小为(<em class="kk"> k，d)</em>的输入<em class="kk">向量 ui </em>开始，我们将该向量转换为一个大小为<em class="kk"> (m，D) </em>的新矩阵<em class="kk">u</em>【̂<strong class="kl ir"/><em class="kk">Ji</em><strong class="kl ir"/>。((<em class="kk">m</em>×<em class="kk">k</em>)×(<em class="kk">k</em>×1)⟹<em class="kk">m</em>×1)。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/40297e473574b42e40831d734526d7f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*w09ddERg9iBhGdD0Ru0Mhw.png"/></div></figure><p id="e7ab" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">这个转换矩阵很重要；它可以表示丢失的<strong class="kl ir">空间关系</strong>以及输入的子对象(狗的鼻子、狗的右眼)和输出的高级对象(狗的脸)之间的其他关系。例如，矩阵 W1j 之一可以表示右眼在狗脸的右上部分的信息，狗脸应该大大约 20 倍，并且脸的角度应该与眼睛的角度相同。这个矩阵的目的是<em class="kk">将输入向量转换成代表下一级的预测输出向量的位置——人脸(更高级别的特征)。</em>在这个矩阵乘法之后，我们得到了面部的预测位置。这发生在每个矢量上。所以 u1 代表根据眼睛预测的狗脸位置，u2 代表根据鼻子预测的狗脸位置，u1 代表根据嘴预测的狗脸位置。这样，我们可以叠加这些预测，看看它们是否相关或相互干扰。如果他们都预测在同一个地方应该有一只狗，我们对 dog_face 的预测就更有把握了。基于每个输入叠加 dog_face 预测的示例:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ov"><img src="../Images/0a3aae1f86d2a9aac81ee3944550a96f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A2Hpb-hQ7Ds4Cdh_-zqhdQ.png"/></div></div></figure><p id="868a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">3.<strong class="kl ir">输入向量</strong> <em class="kk"> U^j|i </em>的加权和( S <em class="kk"> ums C_ij </em> <strong class="kl ir"/></p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ow"><img src="../Images/89d4bd9acd741b97a61cf537dade0cc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*rfsZNqOsaoHbTdzvp3ewVQ.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Weighted Sum of input vectors</figcaption></figure><ul class=""><li id="156c" class="nw nx iq kl b km kn kp kq ma ny mc nz me oa le ob oc od oe bi translated">C_ij 是“耦合系数”,我们使用动态路由算法(将在下面解释)找到它。它们的加权和∑ <em class="kk"> cij </em>被设计为和为 1。就目前而言，动态路由的一个简短直觉是，它是一种胶囊"<em class="kk">决定</em>"向何处发送其输出的方式。这是通过预测如果它和这个太空舱一起走，它的变形会落在太空的什么地方来实现的。我们从所有的胶囊到所有的胶囊，所以对于每一个下一个胶囊，我们有一个空间充满了前一层胶囊的假设点。这个胶囊将到达它的投影更靠近来自其他胶囊的其他点群的地方。</li><li id="2247" class="nw nx iq kl b km of kp og ma oh mc oi me oj le ob oc od oe bi translated">因此[口鼻囊](较低层次)可以投影哪个较高层次的囊会与其投影更一致—(在面部囊、躯干囊、腿囊之间；吻部的投影将更接近面部被膜上的投影)。因此，它将基于此拟合来优化其 C_ij 权重，最大化与 face_capsule 的拟合，最小化与 legs_capsule 的拟合。</li></ul><p id="33ba" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated"><strong class="kl ir"> 4。“挤压函数”:一种新的矢量非线性激活函数</strong></p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ox"><img src="../Images/da96a69f6cd273ec6c99748f2530a081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jbVtO_yWmTcn1QnpTK8E4w.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Squashing function breakdown</figcaption></figure><ol class=""><li id="c9f9" class="nw nx iq kl b km kn kp kq ma ny mc nz me oa le os oc od oe bi translated">传统上，神经元层映射到一个非线性“激活”函数的不同层，这个函数最常见的是<strong class="kl ir"> ReLU </strong>(简单到 f(x)=max(0，x)，只把所有的负值消去为 0)。</li><li id="40a2" class="nw nx iq kl b km of kp og ma oh mc oi me oj le os oc od oe bi translated">胶囊网络需要从向量转换到向量，Hinton 的设计要求输出向量的长度必须在 0-1 之间(或者更准确地说，0-单位长度),才能表示概率。然而，它必须保持其方向。它如何确保这一点？他们创造了一种新的激活功能，称为“挤压”功能。它将向量缩小到 1 或更小，同时保持它们的方向。具体来说，它将大约 3–4 以上的长向量缩小到大约 1(单位向量大小)，小于 1 的向量大幅缩小(在等式的左侧，它们被平方并被自身+ 1 除)。注意，这个向量的大小表示对象属于这个胶囊的概率。</li><li id="9e41" class="nw nx iq kl b km of kp og ma oh mc oi me oj le os oc od oe bi translated">(例如，对于||Sj||=1，这部分应该是 1/2=0.5，但是对于大小为||0.1||，0.1/1.1 = 0.09091 的向量</li><li id="0a39" class="nw nx iq kl b km of kp og ma oh mc oi me oj le os oc od oe bi translated">挤压函数可以计算为(从<a class="ae lf" href="https://github.com/gram-ai/capsule-networks/blob/master/capsule_network.py" rel="noopener ugc nofollow" target="_blank"> gram-ai 的 PyTorch 实现</a>开始，我们将在后面介绍):</li></ol><pre class="mh mi mj mk gt oy nb oz pa aw pb bi"><span id="2bc6" class="pc lh iq nb b gy pd pe l pf pg"># squashing function as we’ve seen before<br/>def squash(self, tensor, dim=-1):<br/>   squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)<br/>   scale = squared_norm / (1 + squared_norm)<br/>   return scale * tensor / torch.sqrt(squared_norm)</span></pre><ol class=""><li id="8abe" class="nw nx iq kl b km kn kp kq ma ny mc nz me oa le os oc od oe bi translated"><strong class="kl ir">输出:产生的矢量。</strong>成品。</li></ol><h1 id="064f" class="lg lh iq bd li lj lk ll lm ln lo lp lq jw lr jx ls jz lt ka lu kc lv kd lw lx bi translated">摘要</h1><p id="484d" class="pw-post-body-paragraph ki kj iq kl b km ly jr ko kp lz ju kr ma mb ku kv mc md ky kz me mf lc ld le ij bi translated">总之，胶囊的内部工作是:</p><ol class=""><li id="44c2" class="nw nx iq kl b km kn kp kq ma ny mc nz me oa le os oc od oe bi translated"><strong class="kl ir">接收输入向量</strong>(代表眼睛)</li><li id="ba02" class="nw nx iq kl b km of kp og ma oh mc oi me oj le os oc od oe bi translated"><strong class="kl ir">应用编码空间关系的“仿射变换”或变换矩阵(</strong>在<strong class="kl ir"> </strong>眼睛和 dog_face 之间，<strong class="kl ir"> </strong>投影脸部应该在的位置)</li><li id="06c7" class="nw nx iq kl b km of kp og ma oh mc oi me oj le os oc od oe bi translated"><strong class="kl ir">通过路由算法学习的 C 个权重应用加权和</strong></li><li id="8b79" class="nw nx iq kl b km of kp og ma oh mc oi me oj le os oc od oe bi translated"><strong class="kl ir">使用非线性“挤压”激活功能将其“挤压”至 0–1</strong></li><li id="f3f4" class="nw nx iq kl b km of kp og ma oh mc oi me oj le os oc od oe bi translated">得到了我们的新矢量，准备发送。</li></ol><p id="59b1" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">自然组学的这张表很好地总结了正常“神经元”和胶囊之间的主要差异:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ph"><img src="../Images/78e3b8f9d9741bb98b45ed5b74288305.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6SgEIRjVxx7mcD4n9sHSfQ.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Key differences between a traditional neuron and a capsule. Source: <a class="ae lf" href="https://github.com/naturomics/CapsNet-Tensorflow" rel="noopener ugc nofollow" target="_blank">Naturomics</a></figcaption></figure><p id="30c5" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">恭喜你。如果你一路走到了这里(即使只是略读了一些技术部分)，你就知道了关于“胶囊”你需要知道的一切！</p><p id="afa5" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">但是我们仍然需要知道:<strong class="kl ir"> <em class="kk">你用这些胶囊做什么？你如何将它们连接成一个神经网络？下一篇文章将解释胶囊之间动态路由信息的算法和架构</em> </strong> <em class="kk">。</em></p><p id="3029" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">如果你想看到下一个帖子以及未来更多的帖子，请鼓掌！</p><p id="d95e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">直到下一次，</p><p id="89af" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">托梅尔</p><p id="10a0" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated"><a class="ae lf" href="http://linkedin.com/in/tomere" rel="noopener ugc nofollow" target="_blank">linkedin.com/in/tomere</a></p></div></div>    
</body>
</html>