<html>
<head>
<title>Only Numpy: Implementing Mini VGG (VGG 7) and SoftMax Layer with Interactive Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">仅 Numpy:使用交互式代码实现迷你 VGG (VGG 7)和 SoftMax 层</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/only-numpy-implementing-mini-vgg-vgg-7-and-softmax-layer-with-interactive-code-8994719bcca8?source=collection_archive---------4-----------------------#2018-02-07">https://towardsdatascience.com/only-numpy-implementing-mini-vgg-vgg-7-and-softmax-layer-with-interactive-code-8994719bcca8?source=collection_archive---------4-----------------------#2018-02-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/828f2ca2d033549af76d6b100465ba43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gMvsvLiQ4ogKBE1CMc2_1w.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Picture from Pixel Bay</figcaption></figure><p id="87bf" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我想在卷积神经网络上练习我的反向传播技能。现在我想实现我自己的 VGG 网(来自原始论文“<a class="ae la" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank">用于大规模图像识别的非常深的卷积网络</a>”)，所以今天我决定结合这两个需求。</p><p id="fb07" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果你不了解卷积神经网络的反向传播过程，请查看我的卷积神经网络反向传播教程，这里<a class="ae la" href="https://becominghuman.ai/only-numpy-implementing-convolutional-neural-network-using-numpy-deriving-forward-feed-and-back-458a5250d6e4" rel="noopener ugc nofollow" target="_blank">这里</a>或者<a class="ae la" href="https://medium.com/@SeoJaeDuk/only-numpy-understanding-back-propagation-for-transpose-convolution-in-multi-layer-cnn-with-c0a07d191981" rel="noopener">这里</a>。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="519d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> Softmax 层及其衍生物</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi li"><img src="../Images/20ae17184715a2212e78cf821b20294a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ue0M7lJdBD06qgI_QIF6QQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Softmax Function <a class="ae la" href="http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/" rel="noopener ugc nofollow" target="_blank">photo from Peter</a></figcaption></figure><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ln"><img src="../Images/070bbf73bf97a020847cdf9cc33381c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R4uaiqeO517WJ3fb2yawLw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Derivative of Softmax <a class="ae la" href="http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/" rel="noopener ugc nofollow" target="_blank">photo from Peter</a></figcaption></figure><p id="58b5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在有很多关于 softmax 函数及其导数的好文章。所以这里就不深入了。不过我这里链接几个<a class="ae la" href="http://photo from Peter" rel="noopener ugc nofollow" target="_blank">，这里</a><a class="ae la" href="https://stats.stackexchange.com/questions/79454/softmax-layer-in-a-neural-network" rel="noopener ugc nofollow" target="_blank">这里</a>，这里<a class="ae la" href="https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression.md" rel="noopener ugc nofollow" target="_blank">这里</a>，这里<a class="ae la" href="https://sefiks.com/2017/12/17/a-gentle-introduction-to-cross-entropy-loss-function/comment-page-1/#comment-600" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="eeee" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">网络架构(示意图)</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lo"><img src="../Images/707660edc1126e60a2d2f357b1dec9e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c99AGOXqTjyY3BcsxwF3vw.jpeg"/></div></div></figure><p id="5c14" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">所以我们只有 7 层，因此名字是 VGG 7，而不是 VGG 16 或 19。原始实现之间也有两个主要区别。</p><p id="247b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">1.我们将使用平均池而不是最大池。如果你想知道为什么，<a class="ae la" href="https://www.quora.com/What-is-the-benefit-of-using-average-pooling-rather-than-max-pooling" rel="noopener ugc nofollow" target="_blank">请查看此链接</a>。<br/> 2。我们网络中的信道数量将比原来的网络少得多。为了便于比较，请参见下面的原始网络架构或这里的<a class="ae la" href="https://blog.heuritech.com/2016/02/29/a-brief-report-of-the-heuritech-deep-learning-meetup-5/vgg16/" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lp"><img src="../Images/c070def57ac2bec88db9baaa3369d1e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*100AYt-p1C6JNAtST-1z0Q.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae la" href="https://blog.heuritech.com/2016/02/29/a-brief-report-of-the-heuritech-deep-learning-meetup-5/vgg16/" rel="noopener ugc nofollow" target="_blank">Image from heuritech</a></figcaption></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="0eaa" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">数据准备和超参数声明</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lq"><img src="../Images/9bd3e4a802950412c155210186e0e606.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jkgVeEhObiWiPlMzJ1y_Dg.png"/></div></div></figure><p id="d237" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如上所述，我们现在不必过滤掉只包含 0 或 1 的图像。由于 SoftMax 图层，我们能够对 0 到 9 的每张图像进行分类。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="d703" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">正向进给操作</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lr"><img src="../Images/28f464f49a0ce896f3d458a358adec30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IxJbI6_Q4BB6eeQIu1eaxw.png"/></div></div></figure><p id="340e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是一个标准的前馈操作，激活函数 ReLU()用于卷积层，tanh()和 arctan()用于完全连接的层。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="2d87" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">反向传播</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ls"><img src="../Images/5ebe85465137c224b48a76949aac7de2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7XsJsuHWqa1b9ji9q2RvXw.png"/></div></div></figure><p id="d204" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">由于我们使用的是平均池而不是最大池，反向传播非常容易和简单。需要注意的一点是，我对卷积层和全连通层设置了不同的学习速率。(绿色方框区域)。</p><p id="3f9b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="lt">更新:请注意在代码中有一个错别字，我将调用以上反向传播作为断开的反向传播，因为我们正在用 w1 而不是 w2 更新 w2。</em></p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="baf6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">训练和结果(正确的反向传播)</strong></p><div class="lj lk ll lm gt ab cb"><figure class="lu jr lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/bb2702398ae170591303b8e8cc3e55c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*KOBGXMddKos7xSjX5srobg.png"/></div></figure><figure class="lu jr ma lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/0739abaa77486f155ec1bf4a4a88d0d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*jmpH5_bF_7hxD1PI0tjRSg.png"/></div></figure></div><p id="db19" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">简·扎瓦日基指出了我的错别字，所以我修改了它，并重新培训了网络。如右图所示，是一段时间内的成本值。</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mb"><img src="../Images/fbc5f6da2bef627e5936ab77b09048ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XSArdEcFLzlcHznYv4DT_w.png"/></div></div></figure><p id="08c4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">该网络在测试集图像上也做得更好，准确率为 84%。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="22ae" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">训练和结果(反向传播中断)</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/b05ccb6af1513a10d9df98ca318cd7e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*nWDDiqOBFfECZWag0xUfgA.png"/></div></figure><p id="6770" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因此，随着时间的推移，成本也稳定下降，但是该模型在测试集图像上表现不佳。</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div class="gh gi md"><img src="../Images/a6eb6e6f553852b2f1608eb19125057f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*ORHNX_zWFfdQ7228rCiE8g.png"/></div></figure><p id="d745" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在 50 个数字中，只能正确分类 39 个，准确率约为 78%。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="4699" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">交互代码</strong></p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/9cd4e04786f1c1e53f5e37a05b8fc690.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wd0NfNcB-aDsBktinweysg.png"/></div></div></figure><p id="0944" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="lt">为了交互代码，我搬到了 Google Colab！所以你需要一个谷歌帐户来查看代码，你也不能在谷歌实验室运行只读脚本，所以在你的操场上做一个副本。最后，我永远不会请求允许访问你在 Google Drive 上的文件，仅供参考。编码快乐！</em></p><p id="3333" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">要访问<a class="ae la" href="https://colab.research.google.com/drive/10zTS8F31ZW6NJAmgFAJMup8xHYZF489E" rel="noopener ugc nofollow" target="_blank">交互代码，请点击此链接。</a></p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="ecf9" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">遗言</strong></p><p id="9e2b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">VGG 网络是实践前馈操作和反向传播的非常好的网络，因为它们都是直接的。</p><p id="234b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 找我。</p><p id="ccc9" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">同时，在我的推特<a class="ae la" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>关注我，并访问<a class="ae la" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或我的<a class="ae la" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。如果你感兴趣的话，我还做了解耦神经网络<a class="ae la" href="https://becominghuman.ai/only-numpy-implementing-and-comparing-combination-of-google-brains-decoupled-neural-interfaces-6712e758c1af" rel="noopener ugc nofollow" target="_blank">的比较。</a></p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="0ee4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">参考</strong></p><ol class=""><li id="8bfd" class="mf mg iq ke b kf kg kj kk kn mh kr mi kv mj kz mk ml mm mn bi translated">Simonyan 和 a . zisser man(2014 年)。用于大规模图像识别的非常深的卷积网络。<em class="lt"> arXiv 预印本 arXiv:1409.1556 </em>。</li><li id="5930" class="mf mg iq ke b kf mo kj mp kn mq kr mr kv ms kz mk ml mm mn bi translated">罗兰茨，p .(未注明)。如何实现一个神经网络间奏曲 2？检索 2018 . 02 . 07，来自<a class="ae la" href="http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/" rel="noopener ugc nofollow" target="_blank">http://peterroelants . github . io/posts/neural _ network _ implementation _ intermezzo 02/</a></li><li id="3e54" class="mf mg iq ke b kf mo kj mp kn mq kr mr kv ms kz mk ml mm mn bi translated">R.(未注明)。rasbt/python-机器学习-图书。2018 . 02 . 07 检索，来自<a class="ae la" href="https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression.md" rel="noopener ugc nofollow" target="_blank">https://github . com/rasbt/python-machine-learning-book/blob/master/FAQ/soft max _ regression . MD</a></li><li id="fa8a" class="mf mg iq ke b kf mo kj mp kn mq kr mr kv ms kz mk ml mm mn bi translated">交叉熵损失函数简介。(2018 年 01 月 07 日)。检索 2018 . 02 . 07，来自<a class="ae la" href="https://sefiks.com/2017/12/17/a-gentle-introduction-to-cross-entropy-loss-function/comment-page-1/#comment-600" rel="noopener ugc nofollow" target="_blank">https://sefiks . com/2017/12/17/a-gentle-introduction-to-cross-entropy-loss-function/comment-page-1/# comment-600</a></li><li id="121c" class="mf mg iq ke b kf mo kj mp kn mq kr mr kv ms kz mk ml mm mn bi translated">Vgg16。(2016 年 2 月 26 日)。检索于 2018 年 2 月 7 日，来自<a class="ae la" href="https://blog.heuritech.com/2016/02/29/a-brief-report-of-the-heuritech-deep-learning-meetup-5/vgg16/" rel="noopener ugc nofollow" target="_blank">https://blog . heuritech . com/2016/02/29/a-brief-report-of-the-heuritech-deep-learning-meetup-5/vgg 16/</a></li><li id="8b0a" class="mf mg iq ke b kf mo kj mp kn mq kr mr kv ms kz mk ml mm mn bi translated">2018.【在线】。可用:<a class="ae la" href="https://www.quora.com/What-is-the-benefit-of-using-average-pooling-rather-than-max-pooling." rel="noopener ugc nofollow" target="_blank">https://www . quora . com/What-the-benefit-of-use-average-pooling-than-max-pooling</a>【访问时间:2018 年 2 月 7 日】。</li></ol></div></div>    
</body>
</html>