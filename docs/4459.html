<html>
<head>
<title>What The Heck Are VAE-GANs?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">VAE 根是什么鬼东西？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-the-heck-are-vae-gans-17b86023588a?source=collection_archive---------3-----------------------#2018-08-17">https://towardsdatascience.com/what-the-heck-are-vae-gans-17b86023588a?source=collection_archive---------3-----------------------#2018-08-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="e46b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">是的，你没看错标题。虽然我的几个朋友是纯素食者，但他们中没有一个人知道任何关于 VAE-甘的事情。VAE-甘代表可变自动编码器-生成对抗网络(这是一个很好的名字。)在我们开始之前，我必须承认我不是这方面的专家(我没有电气工程博士学位，只是说说而已)。但在阅读了几篇研究论文和伊恩·古德菲勒长达 30 分钟的甘斯简介后，我有一个简短(但简洁)的总结:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/244633bc46a2d4c55e05896c9c364f81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2IXeCfGWMw48EXmDdEgcLw.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Image reconstructed by VAE and VAE-GAN compared to their original input images</figcaption></figure><p id="950e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">变分自动编码器(VAEs) </strong></p><p id="0b38" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">解释可变自动编码器的最简单方式是通过图表。或者，你可以阅读<a class="lf lg ep" href="https://medium.com/u/cb2327c63f48?source=post_page-----17b86023588a--------------------------------" rel="noopener" target="_blank"> Irhum Shafkat </a>关于<a class="ae ko" rel="noopener" target="_blank" href="/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">的优秀文章，直观地理解变型自动编码器</a>。在这一点上，我假设你对无监督学习和生成模型有一个大致的概念。教科书上对 VAE 的定义是，它“提供了对潜在空间中观察结果的概率描述”简单地说，这意味着 vae 将潜在属性存储为概率分布。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi li"><img src="../Images/2c9f002dbeb2765703b26c0cd65f5bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jkkHAb_IP4cjFPldb-u0GQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">“<a class="ae ko" href="https://www.jeremyjordan.me/variational-autoencoders/" rel="noopener ugc nofollow" target="_blank">Variational autoencoders</a>”- Jeremy Jordan</figcaption></figure><p id="2596" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每个输入图像都具有通常可以描述为单个离散值的特征。变分自动编码器将这些值描述为概率分布。然后，解码器可以从输入向量的概率分布中随机采样。让我猜猜，你可能想知道什么是解码器，对不对？让我们后退一步，看看 VAE 的总体建筑。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lj"><img src="../Images/f6eac00551912b925ab5acf32c525db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6uuK7GpIbfTb-0chqFwXXw.png"/></div></div></figure><p id="911d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">变型自动编码器的典型设置无非是一个巧妙设计的深度神经网络，它由一对网络组成:编码器和解码器。<strong class="js iu">编码器</strong>可以更好地描述为变分推理网络，它负责将输入<em class="lh"> x </em>映射到后验分布<em class="lh"> q θ (z∣x) </em>。可能性<em class="lh"> p(x∣z) </em>然后由<strong class="js iu">解码器</strong>参数化，这是一个生成网络，它将潜在变量<em class="lh"> z </em>和参数作为输入，并将它们投影到数据分布<em class="lh"> p ϕ (x∣z).</em></p><p id="b56b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">VAEs 的一个主要缺点是它们产生的输出模糊。正如 Dosovitskiy &amp; Brox 所建议的，VAE 模型倾向于产生不现实的、模糊的样本。这与如何在 VAEs 中恢复数据分布和计算损失函数有关，我们将在下面进一步讨论。赵等人的一篇 2017 <a class="ae ko" href="https://arxiv.org/pdf/1702.08658.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。艾尔。建议修改 VAEs，不使用变分贝叶斯方法来提高输出质量。</p><p id="2149" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">生成敌对网络</strong></p><p id="7cea" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">字典对<em class="lh">对抗性</em>的定义是<em class="lh">涉及冲突或对立</em>或以此为特点。在我看来，这是对 GANs 的一个非常准确的描述。就像 VAEs 一样，GANs 属于一类用于无监督机器学习的生成算法。典型的 GANs 由两个神经网络组成，一个<strong class="js iu">生成型</strong>神经网络和一个<strong class="js iu">鉴别型</strong>神经网络。生成神经网络负责将噪声作为输入并生成样本。然后要求判别神经网络评估和区分来自训练数据的生成样本。与 VAEs 非常相似，生成网络将潜在变量和参数映射到数据分布。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lk"><img src="../Images/2f272be102987bd0fbd542ac3abed1dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fEvjrIl9ar9fj51J.png"/></div></div></figure><p id="8b2f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">生成器的主要目标是生成越来越“愚弄”判别神经网络的数据，即增加其错误率。这可以通过重复生成看起来来自训练数据分布的样本来完成。一个简单的形象化方法是警察和网络罪犯之间的“竞争”。网络罪犯(生成者)试图创建类似普通公民的在线身份，而警察(鉴别者)则试图区分假的和真的个人资料。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="ll lm l"/></div></figure><p id="928a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">变分自动编码器生成对抗网络(VAE-甘斯)</strong></p><p id="d215" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">好吧。既然我们已经介绍了 VAEs 和 gan，是时候讨论什么是 VAE-gan 了。术语 VAE-甘首先在 A. Larsen 等人的论文“<a class="ae ko" href="https://arxiv.org/abs/1512.09300" rel="noopener ugc nofollow" target="_blank"> <em class="lh">使用学习的相似性度量对像素之外的内容进行自动编码</em></a><em class="lh">”</em>中引入。艾尔。作者认为，变分自动编码器和生成对抗网络的结合优于传统的 VAEs。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ln"><img src="../Images/95e826076a1043333b3b9788e9926774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KEmfTtghsCDu6UTb.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">VAE-GAN architecture, the discriminator from GAN takes input from VAE’s decoder</figcaption></figure><p id="0d48" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">还记得 gan 被细分为发生器和鉴别器网络吗？作者建议可以使用 GAN 鉴别器代替 VAE 解码器来学习损失函数。这种修改背后的动机如上所述，vae 在重建阶段往往会产生模糊的输出。这种“模糊”在某种程度上与 VAE 损失函数的计算方式有关。我不会深入这个新的损失函数是如何计算的，但是你需要知道的就是这个方程组</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/6a833b5d41c850c26bac9d03bb11342a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*BljjQBkvZDtHFQS-CJw38Q.png"/></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lp"><img src="../Images/ad642a69f3e19c28bd69ffe49bf5d9f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tt-TJPCi1--zOENrRmmhcA.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk"><a class="ae ko" href="https://pravn.wordpress.com/category/vae-gan-vaegan/" rel="noopener ugc nofollow" target="_blank">Learned Losses in VAE-GAN</a></figcaption></figure><p id="f14d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在有很多的<em class="lh"> L </em>但是玩笑归玩笑，上面的等式假设鉴频器的第<em class="lh">L</em>层具有以高斯方式不同的输出。结果，计算第 1 层<em class="lh">和第 5 层</em>输出之间的均方误差(MSE)就给出了 VAE 损失函数。GAN 的最终输出<em class="lh">D(x)</em>可用于计算其自身的损失函数。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/2cdabb348a4cd726274b392d722ecda7.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/0*YMrlsDsZ9bfeMIWJ"/></div></figure><p id="5e89" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">脸书等顶级科技公司现在将生成模型加入了人工智能研究的清单。杰出的计算机科学家和人工智能梦想家 Yann Lecun 曾经说过<em class="lh">“在我看来，这(生成对抗网络)和现在提出的变体是过去 10 年中最有趣的想法。”</em></p><p id="5fe4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">除了 VAE GANs，许多其他的 GANs 变体已经被研究和实现。DCGANs，或深度卷积生成对抗网络，是在 Ian Goodfellow 介绍最初的 GANs 后不久引入的。我很高兴看到生成模型在未来的人工智能应用中找到它的角色，并潜在地改善我们的生活质量。</p><p id="9e4a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢您阅读我的文章。</p></div></div>    
</body>
</html>