<html>
<head>
<title>Intuition (and maths!) behind multivariate gradient descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">直觉(还有数学！)在多元梯度下降后面</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-bit-by-bit-multivariate-gradient-descent-e198fdd0df85?source=collection_archive---------3-----------------------#2017-12-19">https://towardsdatascience.com/machine-learning-bit-by-bit-multivariate-gradient-descent-e198fdd0df85?source=collection_archive---------3-----------------------#2017-12-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e960" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一点一点的机器学习:关于机器学习的小文章</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/046cb2c15b8eb2efd46ff68a54eb6b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqWWrQLIzKnWOI33dSjW9Q.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/photos/Sot0f3hQQ4Y?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Dominik Scythe</a> on <a class="ae kv" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="d862" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">又见面了！</p><p id="05af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">机器学习一点一滴</em>旨在分享我自己在机器学习方面的探索和实验。</p><p id="c4ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我的<a class="ae kv" href="https://hackernoon.com/machine-learning-bit-by-bit-univariate-gradient-descent-9155731a9e30" rel="noopener ugc nofollow" target="_blank">上一篇</a>中，我们讨论了:</p><ol class=""><li id="94bb" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">什么是梯度下降。</li><li id="a74e" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">它如何在线性回归中有用。</li><li id="2289" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">它实际上是如何处理一个简单的一元函数的。</li></ol><p id="8035" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将扩展我们对梯度下降的理解，并将其应用于一个<strong class="ky ir">多元函数</strong>。</p><p id="a7bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我看来，这为将梯度下降应用于更复杂的函数提供了一个平滑的过渡，并帮助您巩固梯度下降的知识，这在本系列的下一个主题—线性回归中是必不可少的。</p><p id="4b47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好吧，我们开始吧。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h2 id="1c89" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated"><strong class="ak">多元梯度下降—直觉</strong></h2><p id="dc1e" class="pw-post-body-paragraph kw kx iq ky b kz nh jr lb lc ni ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">首先，让我们谈谈直觉。对多元函数应用梯度下降实际上意味着什么？</p><p id="79e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将试着通过想象来解释这一点:</p><ol class=""><li id="c863" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">目标多元函数</li><li id="b635" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">梯度下降如何使用它</li></ol><p id="c0bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请记住，梯度下降是一种算法<em class="ls">找到一个函数</em>的最小值。因此，我们的目标是找到一个函数的最小值，这个函数有多个变量。</p><p id="d4ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我的上一篇文章中，我们以一元二次函数为例:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/78cd5748dcac5342b5d6c3cdc46eb60c.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*3ZE9kkoxCvNRE7wbNIaZbA@2x.png"/></div></div></figure><p id="efb3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里是我们今天要看的<em class="ls">二元(两个变量)二次函数</em>，<em class="ls"> J(θ1，θ2)，</em>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/87d98d6ffd4ff01b1d51cd28fd1708a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*8Vku8wBShwOIYmvUlPzLiQ@2x.png"/></div></figure><p id="b611" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">下面的图 1 </strong>以各种方式显示了<em class="ls"> J(θ1，θ2)</em>——左边的<strong class="ky ir"> 3D 图</strong>(<strong class="ky ir">图 1a </strong>)和中间的<strong class="ky ir">图 1b) </strong>以及右边的<strong class="ky ir">等高线图(图 1c) </strong>。等高线图是<em class="ls">在 2D 平面</em>上表示 3D 函数的一种方式。这就好像你从顶部俯视 3D 图形，并沿着 z 轴挤压它。<strong class="ky ir">图 1b </strong>，是<strong class="ky ir">图 1a </strong>的旋转版，应该能给你一些视觉上的直观。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/b7405cab94c9328b7460434ceafbdf57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mc-P5QwXIVpUJpnHtmwSBg@2x.png"/></div></div></figure><p id="7ce3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对该函数应用梯度下降时，我们的目标仍然保持不变，只是现在我们有两个参数<em class="ls"> θ1 </em>和<em class="ls"> θ2 </em>来优化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3d695692112c4d30c8b73094146da80c.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*p_wDEMnnA0J-4Cp04MR8Fg@2x.png"/></div></figure><p id="d5ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止一切顺利…</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h2 id="9e89" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">更新规则</h2><p id="d9e6" class="pw-post-body-paragraph kw kx iq ky b kz nh jr lb lc ni ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">梯度下降的另一个特点是，它是一个<em class="ls">迭代</em>算法。因此，它使用更新规则在每次迭代之后系统地和有效地更新参数值。</p><p id="95d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是单变量梯度下降的更新规则:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/be7d7f88cdf28aef55b0ec53970d9804.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*wDF75wCjfcg7tb0iaJ_50w@2x.png"/></div></figure><p id="27b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<em class="ls"> α </em>是<strong class="ky ir">学习率</strong>，而<em class="ls"> dJ(θ)/dθ </em>是 J(θ) 的<strong class="ky ir">导数——即在给定<em class="ls"> θ </em>处与<em class="ls"> J(θ) </em>相切的切线的斜率。</strong></p><p id="ee72" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们有了两个变量，我们需要为每个变量提供一个更新规则:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/7acaa00975950a024dbc5c1f4c6ae914.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*zV62uBk-Cl-qBG_88hZ53g@2x.png"/></div></figure><p id="dc9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些方程看起来几乎和一元函数的方程一样。这里唯一的变化是衍生术语，<em class="ls">θ2)/∂θ1 ∂j(θ1</em>和<em class="ls">θ2)/∂θ2 ∂j(θ1</em>。但不要被它们吓到。符号<em class="ls"> ∂ </em>而不是<em class="ls"> d </em>仅仅意味着它是<strong class="ky ir">偏导数</strong>。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h2 id="7dcc" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">偏导数</h2><p id="e8e7" class="pw-post-body-paragraph kw kx iq ky b kz nh jr lb lc ni ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">在偏导数中，就像在正态导数中一样，我们仍然对在给定的<em class="ls"> θ1 </em>或<em class="ls"> θ2 </em>处接触<em class="ls"> J(θ1，θ2) </em>的切线的<em class="ls">斜率感兴趣……但是这里的<em class="ls">或</em>是至关重要的。</em></p><p id="5d62" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本质上，当查看切线时，我们不能同时移动<em class="ls"> θ1 </em>和<em class="ls"> θ2 </em>。因此，我们一次只关注一个变量，同时保持其他变量不变。因此，得名<em class="ls">偏</em>。</p><p id="7052" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将借助一个图表来更好地解释这一点。让我们把<em class="ls"> θ1 </em>看成一个变量，保持<em class="ls"> θ2 </em>不变，换句话说，就是<em class="ls"> θ1 </em>的一个偏导数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/102d17a1d871563b43649a1ba8a10b90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_BHZriasUABcujg0SNDBNg@2x.png"/></div></div></figure><p id="4f3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">保持θ2 不变</em>视觉上翻译过来就是一个<em class="ls"> θ1 </em> - <em class="ls"> J(θ1，θ2) </em>平面(<strong class="ky ir">图 2 蓝色方块</strong>)以<em class="ls"> θ2 的特定值穿过图形。</em><strong class="ky ir">图 2 红线</strong>代表<em class="ls"> θ1 </em> - <em class="ls"> J(θ1，θ2) </em>平面与<em class="ls"> J(θ1，θ2) </em>图的交点，成为偏导数中感兴趣的函数。</p><p id="66fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，如果我们沿着红线提取蓝色平面，我们得到的是一个以θ1 为参数的古老的一元函数<em class="ls">，<em class="ls">在 2D 平面</em>，就像我们在上一篇文章中看到的一样！</em></p><p id="c0b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，当<em class="ls"> ∂θ1 </em>向零收缩时，我们可以如下计算更新函数中的偏导数项:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/00e801a8858bf3d8b153a7f07dad840a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DiDo3lxPcJLGa9gGHIl8Xg@2x.png"/></div></div></figure></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h2 id="2b9c" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated"><strong class="ak">偏导数公式的证明(可选)</strong></h2><p id="a1bc" class="pw-post-body-paragraph kw kx iq ky b kz nh jr lb lc ni ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">上面的等式利用了一个众所周知的偏导数公式，所以它省略了你如何<em class="ls">实际</em>计算偏导数以达到<em class="ls"> 2θ1 </em>的证明。如果你没有兴趣证明，请跳过这一节。</p><p id="72d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，这是给你的——你有点像我，有一种强迫性的冲动想看看幕后发生了什么…</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/13d533b6b2749a860b0477869f878d3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kk3EU69uRZkaznNn6FYjHg@2x.png"/></div></div></figure><p id="f60e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">瞧吧！</em></p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h2 id="f7b0" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">同步更新</h2><p id="d25c" class="pw-post-body-paragraph kw kx iq ky b kz nh jr lb lc ni ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">将相同的逻辑应用于<em class="ls"> θ2 </em>的部分推导，我们可以将更新规则简化如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/280efceac7cb29eb7b3e276a4170e3cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*5qRP8n8BAtsAcgLgsZbW8w@2x.png"/></div></figure><p id="f818" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后但同样重要的是，要提到的是<strong class="ky ir">同时更新</strong>的概念——也就是说，当梯度下降应用于多元函数时，对每个参数的<em class="ls">更新必须同时发生</em>，而不是顺序发生。</p><p id="667f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我发现了一个非常直观的描述:</p><blockquote class="nw nx ny"><p id="6c77" class="kw kx ls ky b kz la jr lb lc ld ju le nz lg lh li oa lk ll lm ob lo lp lq lr ij bi translated">一个简单的类比就是走路。你通常不会先走东西向，再走南北向。你走最短的方向，即同时在两个坐标上移动。(<a class="ae kv" href="https://math.stackexchange.com/questions/2419301/why-should-we-update-simultaneously-all-the-variables-in-gradient-descent/2419310" rel="noopener ugc nofollow" target="_blank"> StackExchange </a>)</p></blockquote><p id="b22c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这实际上意味着，在每次迭代中，我们必须将每个新计算的参数赋给一个临时变量，直到我们计算完所有的参数。使用我们的示例，它看起来像这样:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/55e5cf66b5dd949e62e07e50c32a407b.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*CFPufwaKxDkMpfBaFdtEIw@2x.png"/></div></figure><p id="3b1b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/35a1b5ea0c06e3d5eca7c34a810c3d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*P8gnVkWQsEJDCnYWEotk1w@2x.png"/></div></figure><p id="674f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">太好了，我们有了拼图的每一部分。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h2 id="c961" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated"><strong class="ak">动作中的梯度下降</strong></h2><p id="f10e" class="pw-post-body-paragraph kw kx iq ky b kz nh jr lb lc ni ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">时候到了！</p><p id="04a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在可以看到多元梯度下降在起作用，使用<em class="ls"> J(θ1，θ2) = θ1 + θ2 </em>。我们将使用学习率<em class="ls"> α = 0.2 </em>和起始值<em class="ls"> θ1 = 0.75 </em>和<em class="ls"> θ2 = 0.75 </em>。</p><p id="8159" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">图 3a </strong>显示了梯度下降如何接近等高线图上<em class="ls"> J(θ1，θ2) </em>的最小值。<strong class="ky ir">图 3b </strong>是<em class="ls"> J(θ1，θ2) </em>相对于迭代次数的曲线图，用于监控收敛。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/f13b4c145fd448c391b59c4d7978ad24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ans4GhP_GmoaFJYGcd0qsQ@2x.png"/></div></div></figure><p id="3b93" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在图图例中可以看到，从第七次迭代到第八次迭代，<em class="ls"> J(θ1，θ2) </em>减少<em class="ls"> 0.00056 </em>，小于<em class="ls"> 10^(-3) </em>的阈值，此时我们可以宣告收敛。</p><p id="5a5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们找到了满足我们目标的参数组合<em class="ls"> θ1 = 0.013 </em>和<em class="ls"> θ2 = 0.013 </em>。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h2 id="b763" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">总结</h2><p id="4040" class="pw-post-body-paragraph kw kx iq ky b kz nh jr lb lc ni ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">好的，这次我们看了梯度下降在多元函数中的应用。下一次，我们将最后看看梯度下降在线性回归中的应用。敬请期待！</p><p id="0d0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请发表任何反馈、问题或主题请求。我也希望👏如果你喜欢这篇文章，那么其他人也可以找到这篇文章。</p><p id="076d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">谢谢！</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h2 id="70e4" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">机器学习一点一滴系列</h2><ol class=""><li id="c22b" class="lt lu iq ky b kz nh lc ni lf of lj og ln oh lr ly lz ma mb bi translated"><a class="ae kv" href="https://hackernoon.com/machine-learning-bit-by-bit-univariate-gradient-descent-9155731a9e30" rel="noopener ugc nofollow" target="_blank">单变量梯度下降</a></li><li id="865d" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><a class="ae kv" href="https://medium.com/@misaogura/machine-learning-bit-by-bit-multivariate-gradient-descent-e198fdd0df85" rel="noopener">多元梯度下降</a></li></ol></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h2 id="e1ac" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">资源</h2><div class="oi oj gp gr ok ol"><a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ir gy z fp oq fr fs or fu fw ip bi translated">可汗学院</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">免费学习数学、艺术、计算机编程、经济学、物理学、化学、生物学、医学、金融…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">www.khanacademy.org</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz kp ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a href="https://jakevdp.github.io/PythonDataScienceHandbook/04.12-three-dimensional-plotting.html" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ir gy z fp oq fr fs or fu fw ip bi translated">Matplotlib 中的三维绘图</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">现在，根据这个参数化，我们必须确定嵌入条带的(x，y，z)位置。想想看，我们…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">jakevdp.github.io</p></div></div></div></a></div><div class="oi oj gp gr ok ol"><a href="http://louistiao.me/posts/numpy-mgrid-vs-meshgrid/" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ir gy z fp oq fr fs or fu fw ip bi translated">NumPy mgrid 与 meshgrid</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">meshgrid 函数对于创建坐标数组以在网格上对函数求值进行矢量化非常有用…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">louistiao.me</p></div></div></div></a></div><div class="oi oj gp gr ok ol"><a href="https://stackoverflow.com/questions/4700614/how-to-put-the-legend-out-of-the-plot/43439132#43439132" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ir gy z fp oq fr fs or fu fw ip bi translated">如何把传说从剧情中剔除</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">我有一系列的 20 个情节(不是次要情节)要在一个单一的数字。我希望图例在盒子外面…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">stackoverflow.com</p></div></div><div class="ou l"><div class="pa l ow ox oy ou oz kp ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a href="https://math.stackexchange.com/questions/2419301/why-should-we-update-simultaneously-all-the-variables-in-gradient-descent/2419310" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ir gy z fp oq fr fs or fu fw ip bi translated">为什么我们要同时更新梯度下降中的所有变量</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">在经典的梯度下降算法中，在每一步迭代中，我们同时更新所有的变量，即</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">math.stackexchange.com</p></div></div><div class="ou l"><div class="pb l ow ox oy ou oz kp ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a href="https://hackernoon.com/tagged/machine-learning" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ir gy z fp oq fr fs or fu fw ip bi translated">机器学习-黑客正午</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">在黑客正午阅读关于机器学习的文章。黑客如何开始他们的下午？</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">hackernoon.com</p></div></div><div class="ou l"><div class="pc l ow ox oy ou oz kp ol"/></div></div></a></div></div></div>    
</body>
</html>