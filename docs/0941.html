<html>
<head>
<title>Reinforcement Learning at O’Reilly Artificial Intelligence Conference NY 2017</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2017年纽约奥莱利人工智能大会上的强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-at-oreilly-artificial-intelligence-conference-ny-2017-404f62c99002?source=collection_archive---------8-----------------------#2017-07-11">https://towardsdatascience.com/reinforcement-learning-at-oreilly-artificial-intelligence-conference-ny-2017-404f62c99002?source=collection_archive---------8-----------------------#2017-07-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="a3ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">几周前，2017年奥莱利人工智能大会在纽约举行。这是一次令人惊叹的会议，来自学术界和工业界的演讲都非常精彩。这篇文章总结了我在那里做的关于<a class="ae kl" href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel="noopener ugc nofollow" target="_blank">强化学习</a>的一些演讲和一个教程，“机器学习领域关注的是软件代理应该如何在一个环境中采取行动，以便最大化一些累积回报的概念”。</p><h2 id="cdf1" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">与人协调的汽车</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/bf613133f65bb59848f132a5d8cdfbd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*wTYxZMizzOmMXiRxbP-PKQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Fantasia, Disney 1940</figcaption></figure><p id="be60" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">来自伯克利的Anca D. Dragan发表了题为“<a class="ae kl" href="https://www.oreilly.com/ideas/cars-that-coordinate-with-people" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">与人协调的汽车</strong> </a>”的主题演讲，她在演讲中介绍了论文“<a class="ae kl" href="https://people.eecs.berkeley.edu/~sseshia/pubdir/rss16.pdf" rel="noopener ugc nofollow" target="_blank">规划对人类行动产生影响的自动驾驶汽车</a>”的结果他们没有进行纯粹的避障，即试图避免妨碍其他移动物体，而是能够将驾驶员建模为遵循自己政策的其他代理。这意味着机器人知道其他汽车也会避免撞到障碍物，所以它可以预测其他车辆对它的行动会有什么反应。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/883b389a61becd8c27e471a07b5ab665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*wM6A9bK5qZw-JQ3plNkZfw.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Figure from “Planning for Autonomous Cars that Leverages Effects on Human Actions”</figcaption></figure><p id="1583" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自主车辆还能够采取行动，收集其他车辆的信息。例如，它可以开始慢慢合并人类前面的车道，直到有足够的证据表明司机没有侵略性，并将实际刹车以避免碰撞。</p><p id="dd76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关键的音符是如此之好，以至于我改变了我的时间表去看她的演讲，名为“<strong class="jp ir">逆奖励函数</strong>”，在我看来，这是大会中最好的演讲。她从迪士尼的电影《幻想曲》开始讲起。它是根据歌德在1797年写的诗《巫师的学徒》改编的。正如W <a class="ae kl" href="https://en.wikipedia.org/wiki/The_Sorcerer%27s_Apprentice" rel="noopener ugc nofollow" target="_blank"> ikipedia总结</a>它，</p><blockquote class="ls"><p id="edf3" class="lt lu iq bd lv lw lx ly lz ma mb kk dk translated">这首诗以一个老巫师离开他的工作室开始，留下他的徒弟做杂务。厌倦了用桶打水，这个学徒用一把扫帚附魔来为他做这件事——使用他还没有完全训练好的魔法。地板很快就被水淹没了，学徒意识到他无法停止扫帚，因为他不知道如何停止。</p></blockquote><p id="81bb" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">随着我们创造更多与人类直接互动的机器人，Dragan正在研究我们如何确保它们会做我们真正想要的事情，即使我们发出的命令并不十分精确。我曾经读过一个假设的故事，也说明了这个问题。不幸的是，我找不到参考文献，但它是这样的:</p><blockquote class="mh mi mj"><p id="8970" class="jn jo mk jp b jq jr js jt ju jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj kk ij bi translated">假设我们创造了一台超级智能机器，并要求它找到治疗疟疾的方法。我们设定的目标是尽量减少死于这种疾病的人数。机器人发现，解决问题的最快和最有保证的方法是侵入世界上所有的核武器，并发射它们杀死所有的人类，确保没有人会再次死于疟疾。机器能够实现它的目标，但显然不是以程序员想要的方式。</p></blockquote><p id="ea94" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">艾萨克·阿西莫夫也写了几个关于类似情况的好故事，并提出了机器人三定律作为解决问题的方法。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/df3766f3aed0fc19ea1776e8ead05829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ah7PwFCMGzG5huEN6EK5_w.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><a class="ae kl" href="https://xkcd.com/1613/" rel="noopener ugc nofollow" target="_blank">https://xkcd.com/1613/</a></figcaption></figure><p id="fc82" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Dragan和她的小组在这方面做了很多研究:“<a class="ae kl" href="https://arxiv.org/pdf/1705.09990.pdf" rel="noopener ugc nofollow" target="_blank">机器人应该听话吗？</a>、<a class="ae kl" href="https://arxiv.org/pdf/1705.04226.pdf" rel="noopener ugc nofollow" target="_blank">机器人用数学模型规划人的状态和动作</a>、<a class="ae kl" href="https://arxiv.org/pdf/1611.08219.pdf" rel="noopener ugc nofollow" target="_blank">关闸游戏</a>简而言之，他们的方法是让机器人考虑到人类指定的命令或政策并不完美，并通过不做与训练时看到的太不同的事情来避免风险。</p><p id="9442" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://www.oreilly.com/ideas/superhuman-ai-for-strategic-reasoning" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">战略推理的超人AI:在单挑无限注德州扑克</strong> </a> <strong class="jp ir">中击败顶级职业选手，作者Tuomas Sandholm(卡内基梅隆大学):</strong>他们能够在一场难度相当的比赛中击败顶级人类选手，但没有得到媒体的同等关注。这个游戏增加了额外的复杂性，因为玩家没有完整的信息。桑德霍尔姆评论了除了典型的<a class="ae kl" href="https://medium.com/@jbochi/how-not-to-sort-by-popularity-92745397a7ae" rel="noopener">探索与利用权衡</a>之外游戏必须考虑的第三个变量:可利用性。他们的代理人Liberatus试图将剥削最小化。它并不真正擅长探索糟糕的玩家，但可以用这种方法击败最好的人类。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mt"><img src="../Images/7cd854a2e27ef30265747e791cd579f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HP73tjm0vX0cisiLaQUF-g.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk"><a class="ae kl" href="https://www.cmu.edu/news/stories/archives/2017/january/AI-tough-poker-player.html" rel="noopener ugc nofollow" target="_blank">https://www.cmu.edu/news/stories/archives/2017/january/AI-tough-poker-player.html</a></figcaption></figure><p id="00f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">深度强化学习教程，作者Arthur Juliani。非常好的教程，代码易于理解和运行。朱利安尼提出了几种不同的强化学习方法，包括多武装匪徒理论、Q学习、政策梯度和行动者-批评家代理。看一看<a class="ae kl" href="https://github.com/awjuliani/oreilly-rl-tutorial" rel="noopener ugc nofollow" target="_blank">资源库</a>中的教程。</strong></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mu"><img src="../Images/88486a475b749ab677337180754ded9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*82_g4b7p9CAds8GpKp8-WA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Multi-Armed Bandit Dungeon environment</figcaption></figure><p id="1eb9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">使用OpenAI的Gym和Universe构建游戏机器人，作者Anmol Jagetia: </strong>不幸的是，他遇到了几个技术问题:一些例子崩溃了，运行的例子帧率很差，我们看不到代理在做什么，但他的教程笔记本看起来很有趣:【https://github.com/anmoljagetia/OReillyAI-Gamebots】T2</p><p id="d14c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">敬请关注更多内容。我会写另一篇关于我在那里看到的其他主题的帖子:推荐系统、张量流和自然语言理解。</p></div></div>    
</body>
</html>