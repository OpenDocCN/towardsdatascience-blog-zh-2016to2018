<html>
<head>
<title>faced: CPU Real Time face detection using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">faced:使用深度学习的 CPU 实时人脸检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/faced-cpu-real-time-face-detection-using-deep-learning-1488681c1602?source=collection_archive---------4-----------------------#2018-09-26">https://towardsdatascience.com/faced-cpu-real-time-face-detection-using-deep-learning-1488681c1602?source=collection_archive---------4-----------------------#2018-09-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3dbb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">在没有 GPU 的情况下，有可能实现具有实时性能的对象检测模型吗？</em> <code class="fe km kn ko kp b">faced</code>是一个概念证明，可以为 CPU 上运行于<strong class="jp ir">实时</strong> <strong class="jp ir">的单个类对象(在本例中为人脸)构建自定义对象检测模型。</strong></p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi kq"><img src="../Images/246f2410532e3fe2d6a4ebbe90b57571.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tdz9Pm3pTT75ix1yZDCSLg.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk"><a class="ae lg" href="https://www.kairos.com/blog/face-detection-explained" rel="noopener ugc nofollow" target="_blank">Face detection task</a></figcaption></figure><h1 id="1847" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">有什么问题？</h1><p id="bb8d" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">在许多情况下，需要进行单个类对象检测。这意味着我们想要检测图像中属于特定类别的所有对象的位置。例如，我们可以为人脸识别系统检测人脸，或者为行人跟踪检测人。</p><p id="fe14" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，大多数时候我们希望实时运行这些模型<strong class="jp ir">。</strong>为了实现这一点，我们有一个以速率<em class="kl"> x </em>提供样本的图像馈送，我们需要一个模型以小于速率<em class="kl"> x </em>运行每个样本。<strong class="jp ir">然后，一旦图像可用，我们就可以对其进行处理。</strong></p><p id="847e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如今，解决这一任务(以及计算机视觉中的许多其他任务)最容易获得和使用的解决方案是对先前训练的模型执行<a class="ae lg" href="https://en.wikipedia.org/wiki/Transfer_learning" rel="noopener ugc nofollow" target="_blank"><em class="kl"/></a>(在大型数据集上训练的一般标准模型，如在<a class="ae lg" href="https://www.tensorflow.org/hub/" rel="noopener ugc nofollow" target="_blank"> Tensorflow Hub </a>或<a class="ae lg" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" rel="noopener ugc nofollow" target="_blank"> TF 对象检测 API </a>中找到的数据集)</p><p id="b944" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有许多经过训练的对象检测架构(例如 FasterRCNN、SSD 或 YOLO)在运行于 GPU 上的实时性能<strong class="jp ir">内实现了令人印象深刻的准确性。</strong></p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/c47ed62db3571bffc37b6a3ccc92353e.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*vPSB9em5LqVFw10hIUqe9w.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Extracted from SSD paper <a class="ae lg" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><p id="006e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">GPU 很贵，但在训练阶段是必须的。然而，据推断<strong class="jp ir">用专用 GPU 来实现实时性能是不可行的。如果没有 GPU，所有通用对象检测模型(如上所述)都无法实时运行。</strong></p><blockquote class="ml"><p id="28eb" class="mm mn iq bd mo mp mq mr ms mt mu kk dk translated"><em class="mv">那么，如何重新审视单类对象的对象检测问题，以达到实时性能但在 CPU 上？</em></p></blockquote><h1 id="6812" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls mw lu lv lw mx ly lz ma my mc md me bi translated">主要观点:简单的任务需要较少的可学习的特征</h1><p id="9c5a" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">上面提到的所有架构都是为检测多个对象类而设计的(在<a class="ae lg" href="https://cocodataset.org/" rel="noopener ugc nofollow" target="_blank"> COCO </a>或<a class="ae lg" href="http://host.robots.ox.ac.uk/pascal/VOC//" rel="noopener ugc nofollow" target="_blank"> PASCAL VOC </a>数据集上训练)。为了能够将每个边界框分类到其适当的类别，这些架构需要大量的特征提取。这意味着大量的可学习参数，大量的过滤器，大量的层。换句话说，这个网络很大。</p><p id="7fe1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们定义更简单的任务(而不是多类包围盒分类)，那么我们可以认为网络需要学习更少的特征来执行任务。检测图像中的人脸显然比检测汽车、人、交通标志和狗(都在同一模型内)更简单。<strong class="jp ir">深度学习模型为了识别人脸(或任何单个类对象)所需的特征量将少于同时检测几十个类所需的特征量。执行第一项任务所需的信息少于后一项任务。</strong></p><blockquote class="ml"><p id="5bd5" class="mm mn iq bd mo mp mq mr ms mt mu kk dk translated"><strong class="ak">单一类别对象检测模型将需要较少的可学习特征。参数越少意味着网络会越小。较小的网络运行速度更快，因为它需要更少的计算。</strong></p><p id="ec0b" class="mm mn iq bd mo mp mq mr ms mt mu kk dk translated"><strong class="ak">那么，问题是:在保持准确性的情况下，我们可以在 CPU 上实现多小的实时性能？</strong></p></blockquote><p id="2160" class="pw-post-body-paragraph jn jo iq jp b jq mz js jt ju na jw jx jy nb ka kb kc nc ke kf kg nd ki kj kk ij bi translated">这是<code class="fe km kn ko kp b">faced</code>的主要概念:<strong class="jp ir">建立尽可能小的网络，在保持准确性的同时(希望)在 CPU 中实时运行。</strong></p><h1 id="7891" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">建筑</h1><p id="9e97" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated"><code class="fe km kn ko kp b">faced</code>是两个神经网络的集合，都是使用<a class="ae lg" href="http://tensorflow.org" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>实现的。</p><h2 id="f845" class="ne li iq bd lj nf ng dn ln nh ni dp lr jy nj nk lv kc nl nm lz kg nn no md np bi translated">主网络</h2><p id="47e4" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated"><code class="fe km kn ko kp b">faced</code>主建筑大量基于<a class="ae lg" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank"> YOLO </a>的建筑。基本上，它是一个完全卷积的网络(FCN ),通过一系列卷积层和池层(不涉及其他层类型)运行 288x288 输入图像。</p><p id="2a3a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">卷积层负责提取空间感知特征。汇集层增加了后续卷积层的感受域。</p><p id="a4eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该架构的输出是一个 9x9 的网格(相对于 YOLO 的 13x13 网格)。每个网格单元负责预测该单元内是否有人脸(相对于 YOLO，每个单元可以检测多达 5 个不同的对象)。</p><p id="7d75" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每个网格单元有 5 个关联值。第一个是包含面部中心的单元的概率<em class="kl"> p </em>。另外 4 个值是被检测人脸的(<em class="kl"> x_center，y_center，width，height) </em>(相对于单元格)。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi nq"><img src="../Images/fbe256714f30e414020803d1e0c67962.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-w9qXAOq8NsRDcN8E8-0vQ.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Main architecture</figcaption></figure><p id="3798" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">确切的架构定义如下:</p><ul class=""><li id="3778" class="nr ns iq jp b jq jr ju jv jy nt kc nu kg nv kk nw nx ny nz bi translated">2x[288 x288 图像上的 8 个过滤卷积层]</li><li id="7ab1" class="nr ns iq jp b jq oa ju ob jy oc kc od kg oe kk nw nx ny nz bi translated">最大池(288x288 到 144x144 功能图)</li><li id="b3e1" class="nr ns iq jp b jq oa ju ob jy oc kc od kg oe kk nw nx ny nz bi translated">2x[144 x144 特征图上的 16 个过滤卷积层]</li><li id="1ffe" class="nr ns iq jp b jq oa ju ob jy oc kc od kg oe kk nw nx ny nz bi translated">最大池(144x144 到 72x72 功能图)</li><li id="52a3" class="nr ns iq jp b jq oa ju ob jy oc kc od kg oe kk nw nx ny nz bi translated">2x[72x 72 特征图上的 32 个过滤卷积层]</li><li id="792d" class="nr ns iq jp b jq oa ju ob jy oc kc od kg oe kk nw nx ny nz bi translated">最大池(72x72 到 36x36 功能图)</li><li id="2944" class="nr ns iq jp b jq oa ju ob jy oc kc od kg oe kk nw nx ny nz bi translated">2x[36x 36 特征图上的 64 个过滤卷积层]</li><li id="35bc" class="nr ns iq jp b jq oa ju ob jy oc kc od kg oe kk nw nx ny nz bi translated">最大池(36x36 到 18x18 功能图)</li><li id="25d9" class="nr ns iq jp b jq oa ju ob jy oc kc od kg oe kk nw nx ny nz bi translated">2x[18x 18 特征图上的 128 过滤卷积层]</li><li id="fedf" class="nr ns iq jp b jq oa ju ob jy oc kc od kg oe kk nw nx ny nz bi translated">最大池(18x18 到 9x9 功能图)</li><li id="8960" class="nr ns iq jp b jq oa ju ob jy oc kc od kg oe kk nw nx ny nz bi translated">4x[9x 9 特征图上的 192 滤波卷积层]</li><li id="eab8" class="nr ns iq jp b jq oa ju ob jy oc kc od kg oe kk nw nx ny nz bi translated">5 在 9x9 特征图上过滤卷积层，以形成最终网格</li></ul><p id="71a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所有激活功能都是<code class="fe km kn ko kp b">leaky_relu</code>。</p><blockquote class="ml"><p id="f330" class="mm mn iq bd mo mp mq mr ms mt mu kk dk translated"><code class="fe km kn ko kp b">faced has </code>6993517 个参数。YOLOv2 有 51，000，657 个参数。它的大小是 YOLO 大小的 13%!</p></blockquote><h2 id="116d" class="ne li iq bd lj nf of dn ln nh og dp lr jy oh nk lv kc oi nm lz kg oj no md np bi translated">辅助网络</h2><p id="b7e1" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">(<em class="kl"> x_center，y_center，width，height) </em>主网络的输出不如预期的准确。因此，实现了一个小的 CNN 网络，将包含人脸的小图像(用主架构输出裁剪)作为输入，并输出人脸的地面真实边界框的回归。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ok"><img src="../Images/4c78bdf2e10d469265499b068940680a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZxfOG9eSKfja65EzhU7fZg.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">The network takes a crop containing a face and predicts the correct bounding box</figcaption></figure><p id="5c8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它唯一的任务是补充和改进主架构的输出坐标。</p><p id="0e50" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该网络的具体架构无关紧要。</p><h1 id="0c2f" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">资料组</h1><p id="7124" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">两个网络都是在<a class="ae lg" href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" rel="noopener ugc nofollow" target="_blank">宽脸数据集</a>上训练的。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ol"><img src="../Images/bad2da9127f37bea0b15215c736d7564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0J_Sl81iLs5XRCLIzsBQhQ.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">WIDER FACE multiple scenarios</figcaption></figure><blockquote class="om on oo"><p id="3642" class="jn jo kl jp b jq jr js jt ju jv jw jx op jz ka kb oq kd ke kf or kh ki kj kk ij bi translated">“更宽的人脸数据集是人脸检测基准数据集[…]。我们选择了<strong class="jp ir"> 32，203 </strong>张图像，并标记了<strong class="jp ir"> 393，703 </strong>张在比例、姿态和遮挡方面具有高度可变性的人脸，如样本图像中所描绘的那样。”</p></blockquote><h1 id="f068" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">培养</h1><p id="a7a6" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">训练是在 Nvidia Titan XP GPU 上进行的。培训时间大约花了 20 个小时。<a class="ae lg" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">使用批量标准化</a>技术来帮助收敛，并使用漏失(40%比率)作为正则化方法来避免过拟合。</p><h1 id="0574" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">推理和非最大抑制</h1><p id="2fb0" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">当使用<code class="fe km kn ko kp b">faced</code>进行推理时，首先将图像尺寸调整为 288x288，以便输入网络。该图像位于 FCN 下方，给出了上面描述的 9x9 网格输出。</p><p id="68c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每个单元格有一个包含图像的概率<em class="kl"> p </em>。通过可配置的阈值过滤单元格(即仅保留具有<em class="kl"> p &gt; t </em>的单元格)。对于那些保留的单元格，使用单元格的(<em class="kl"> x_center，y_center，width，height)定位人脸。</em></p><p id="706a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在某些情况下，多个细胞可以竞争同一张脸。让我们假设一个面部中心位于 4 个细胞相交的精确位置。这 4 个单元可能具有高的<em class="kl"> p </em>(在单元内包含面中心的概率)。如果我们保留所有的单元并投影每个单元的面坐标，那么我们将会看到相同的面周围有 4 个相似的边界框。这个问题是通过一种叫做<a class="ae lg" href="https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">非最大抑制</strong> </a>的技术解决的。结果如下图所示:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/12acab67bc6f5ff6603ad55a6dcde012.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*Mi6IPSd0gSdtCvKUBzZi9Q.jpeg"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Non max suppression example.</figcaption></figure><h1 id="4d7f" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">表演</h1><p id="763b" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated"><code class="fe km kn ko kp b">faced</code>在推论上能够达到以下速度:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ot"><img src="../Images/1b48c7cd6a34d76afb418f3378978bf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wkRfA8gUcfwDdFoa1XXGA.png"/></div></div></figure><p id="339e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑到 YOLOv2 在 i5 2015 MBP 上甚至不能达到 1FPS，这已经很不错了。</p><h1 id="aa15" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">结果</h1><p id="e9de" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">来看看成果吧！</p><div class="kr ks kt ku gt ab cb"><figure class="ou kv ov ow ox oy oz paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/f0f862b003092ca2408dbfbee03a4917.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*gwC5tknaFN8SOKRqVpbyYA.png"/></div></figure><figure class="ou kv pa ow ox oy oz paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/7511a8cd4e2809e19bb4750b70da42ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*A3ke5xblWCos0wWkYtraSw.png"/></div></figure></div><div class="ab cb"><figure class="ou kv pb ow ox oy oz paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/27e68ef06b869b9f49adb34774d1572e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*iCNwCWlaHqn3TT9vpEpI4Q.png"/></div></figure><figure class="ou kv pc ow ox oy oz paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/7bbaab8a587e14d3ec7157201186f751.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*afYSdEHidlC-UzmTAxa8_Q.png"/></div></figure></div><div class="ab cb"><figure class="ou kv pd ow ox oy oz paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/66263e325fff761701646338c4b0c3a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*hidCftzR20n1A6RTXKtNrQ.png"/></div></figure><figure class="ou kv pe ow ox oy oz paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/4d9b00f1402af8cf369a9231e8096d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1438/format:webp/1*uBEAhu5ngPMnEuX3rk5N3g.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk pf di pg ph">faced on images</figcaption></figure></div><p id="6509" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们来看一下<code class="fe km kn ko kp b">faced</code>和<a class="ae lg" href="https://docs.opencv.org/3.4/d7/d8b/tutorial_py_face_detection.html" rel="noopener ugc nofollow" target="_blank"> Haar Cascades </a>的对比，Haar Cascades 是一种不使用深度学习的计算机视觉传统方法。这两种方法在相似的速度性能下运行。<code class="fe km kn ko kp b">faced</code>显示出显著的更高的准确性。</p><div class="kr ks kt ku gt ab cb"><figure class="ou kv pi ow ox oy oz paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/5f63a71355e768e55d7a7ee5746bad3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*bQVE1o-cSwi4EZiFiYBd5A.gif"/></div></figure><figure class="ou kv pi ow ox oy oz paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/23555fee9c63b1d7d96e52ebb5c94478.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*_K2pmLW2rgQMe8YYd3-SWw.gif"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk pj di pk ph">Haar Cascade [left] vs faced [right]</figcaption></figure></div><h1 id="8930" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">如何使用 faced？</h1><p id="19e1" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated"><code class="fe km kn ko kp b">faced</code>是一个非常简单的程序，既可以嵌入 Python 代码，也可以作为命令行程序使用。</p><p id="bac4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请访问 github repo 获取更多说明:</p><div class="pl pm gp gr pn po"><a href="https://github.com/iitzco/faced" rel="noopener  ugc nofollow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd ir gy z fp pt fr fs pu fu fw ip bi translated">iitzco/faced</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">🚀 😏使用深度学习的近实时 CPU 人脸检测- iitzco/faced</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">github.com</p></div></div><div class="px l"><div class="py l pz qa qb px qc la po"/></div></div></a></div><p id="6332" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">喜欢这个项目？</em>在项目的回购上留下⭐！</p><h1 id="c4ed" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">结论</h1><p id="a43e" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated"><code class="fe km kn ko kp b">faced</code>是一个概念证明，在这些模型对您的问题来说是多余的并且涉及到性能问题的场景中，您不需要总是依赖通用训练模型。<strong class="jp ir">不要高估花时间设计专门针对你的问题的定制神经网络架构的能力。这些特定的网络将是比一般网络更好的解决方案。</strong></p></div></div>    
</body>
</html>