<html>
<head>
<title>Semantic Segmentation with Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于深度学习的语义分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/semantic-segmentation-with-deep-learning-a-guide-and-code-e52fc8958823?source=collection_archive---------1-----------------------#2018-09-19">https://towardsdatascience.com/semantic-segmentation-with-deep-learning-a-guide-and-code-e52fc8958823?source=collection_archive---------1-----------------------#2018-09-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7696" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">指南和代码</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/eed936533fcd65c551eb97d759c76189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*RZnBSB3QpkIwFUTRFaWDYg.gif"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Semantic Segmentation</figcaption></figure><blockquote class="ku kv kw"><p id="002b" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">想获得灵感？快来加入我的<a class="ae lu" href="https://www.superquotes.co/?utm_source=mediumtech&amp;utm_medium=web&amp;utm_campaign=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">超级行情快讯</strong> </a>。😎</p></blockquote><h1 id="a487" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">什么是语义切分？</h1><p id="1e05" class="pw-post-body-paragraph kx ky it la b lb mn ju ld le mo jx lg mp mq lj lk mr ms ln lo mt mu lr ls lt im bi translated">深度学习和计算机视觉社区的大多数人都明白什么是图像分类:我们希望我们的模型告诉我们图像中存在什么单个对象或场景。分类很粗，层次很高。</p><p id="0d47" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">许多人也熟悉对象检测，我们试图通过在它们周围绘制边界框，然后对框中的内容进行分类，来对图像中的多个对象进行<em class="kz">定位和分类。检测是中级的，我们有一些非常有用和详细的信息，但仍然有点粗糙，因为我们只画了边界框，并没有真正得到对象形状的准确想法。</em></p><p id="c90b" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">语义分割是这三者中信息量最大的，我们希望对图像中的<em class="kz">每一个像素进行分类，就像你在上面的 gif 中看到的那样！在过去的几年里，这完全是通过深度学习来完成的。</em></p><p id="96b3" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">在本指南中，您将了解语义分割模型的基本结构和工作原理，以及所有最新、最棒的最先进方法。</p><p id="05e3" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">如果您想亲自尝试这些模型，您可以查看我的语义分割套件，其中包含 TensorFlow 培训和本指南中许多模型的测试代码！</p><div class="mv mw gp gr mx my"><a href="https://github.com/GeorgeSeif/Semantic-Segmentation-Suite" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">GeorgeSeif/语义分段套件</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">TensorFlow 中的语义切分套件。轻松实现、训练和测试新的语义分割模型！…</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">github.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm ko my"/></div></div></a></div><h1 id="e965" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">基本结构</h1><p id="0a71" class="pw-post-body-paragraph kx ky it la b lb mn ju ld le mo jx lg mp mq lj lk mr ms ln lo mt mu lr ls lt im bi translated">我将要向您展示的语义分割模型的基本结构存在于所有最先进的方法中！这使得实现不同的应用程序变得非常容易，因为几乎所有的应用程序都有相同的底层主干、设置和流程。</p><p id="1780" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">U-Net 模型很好地说明了这种结构。模型的左侧代表<strong class="la iu">为图像分类而训练的任何特征提取网络。</strong>这包括像 VGGNet、ResNets、DenseNets、MobileNets 和 NASNets 这样的网络！在那里你真的可以使用任何你想要的东西。</p><p id="478b" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">选择用于特征提取的分类网络时，最重要的是要记住权衡。使用一个非常深的 ResNet152 会让你非常准确，但不会像 MobileNet 那样快。将这些网络应用于分类时出现的权衡也出现在将它们用于分割时。需要记住的重要一点是<strong class="la iu">在设计/选择您的细分网络时，这些主干将是主要驱动因素，这一点我再强调也不为过。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/38bca0a6dd3162b29b36beafe403ca07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*kDXHxHJjiM2NoAp0hlCWjA.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">U-Net model for segmentation</figcaption></figure><p id="05d9" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">一旦这些特征被提取出来，它们将在不同的尺度下被进一步处理。原因有两个。首先，你的模型很可能会遇到许多不同大小的物体；以不同的比例处理要素将赋予网络处理不同规模要素的能力。</p><p id="6cab" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">第二，当执行分段时，有一个折衷。如果你想要好的分类准确度，那么你肯定会想要在网络的后期处理那些高级特征，因为它们更有鉴别能力，包含更多有用的语义信息。另一方面，如果你只处理那些深度特征，由于分辨率低，你不会得到好的定位！</p><p id="6724" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">最近最先进的方法都遵循上面的特征提取结构，然后是多尺度处理。因此，许多都非常易于端到端的实施和培训。您选择使用哪一个将取决于您对准确性与速度/内存的需求，因为所有人都在尝试提出新的方法来解决这一权衡，同时保持效率。</p><p id="aaef" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">在接下来的最新进展中，我将把重点放在最新的方法上，因为在理解了上面的基本结构之后，这些方法对大多数读者来说是最有用的。我们将按照大致的时间顺序进行演练，这也大致对应于最新技术的发展。</p><h1 id="037b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">最先进的漫游</h1><h2 id="928d" class="no lw it bd lx np nq dn mb nr ns dp mf mp nt nu mh mr nv nw mj mt nx ny ml nz bi translated">全分辨率残差网络(FRRN)</h2><p id="de87" class="pw-post-body-paragraph kx ky it la b lb mn ju ld le mo jx lg mp mq lj lk mr ms ln lo mt mu lr ls lt im bi translated"><a class="ae lu" href="https://arxiv.org/pdf/1611.08323.pdf" rel="noopener ugc nofollow" target="_blank"> FRRN </a>模型是多尺度处理技术的一个非常明显的例子。它使用两个独立的流来实现这一点:剩余流和池流。</p><p id="23b7" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">我们希望处理这些语义特征以获得更高的分类精度，因此 FRRN 逐步处理和缩减采样池流中的特征映射。同时，它在残差流中以<em class="kz">全分辨率</em>处理特征图。所以池流处理高级语义信息(为了高分类精度)，残差流处理低级像素信息(为了高定位精度)！</p><p id="15cf" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">现在，由于我们正在训练一个端到端网络，我们不希望这两个流完全断开。因此，在每次最大池化后，FRRN 对来自两个流的特征地图进行一些联合处理，以组合它们的信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/d9de716bfd3408bb7f694f6ff3473ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*LlYK2Pjemx3kNC61yVV-yA.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">FRRN model structure</figcaption></figure><h2 id="2455" class="no lw it bd lx np nq dn mb nr ns dp mf mp nt nu mh mr nv nw mj mt nx ny ml nz bi translated">金字塔场景解析网络</h2><p id="b4a9" class="pw-post-body-paragraph kx ky it la b lb mn ju ld le mo jx lg mp mq lj lk mr ms ln lo mt mu lr ls lt im bi translated">FRRN 在直接执行多尺度处理方面做得很好。但是在每一个尺度上进行繁重的处理是相当计算密集的。此外，FRRN 做一些全分辨率的处理，非常慢的东西！</p><p id="317b" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">PSPNet 提出了一个聪明的方法来解决这个问题，通过使用<em class="kz">多尺度池。</em>它从标准特征提取网络(ResNet、DenseNet 等)开始，并对第三次下采样的特征进行进一步处理。</p><p id="ce68" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">为了获得多尺度信息，PSPNet 使用 4 种不同的窗口大小和步长应用 4 种不同的最大池操作。这有效地从 4 个不同的尺度捕获特征信息，而不需要对每个尺度进行大量的单独处理！我们简单地对每个图做一个轻量级的卷积，然后进行上采样，这样每个特征图都有相同的分辨率，然后把它们连接起来。</p><p id="8139" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">瞧啊。我们已经合并了多尺度特征地图，而没有对它们应用很多卷积！</p><p id="4ebf" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">所有这些都是在较低分辨率的特征图上完成的。最后，我们使用双线性插值将输出分割图放大到期望的大小。这种仅在所有处理完成后才进行升级的技术出现在许多最新的作品中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ob"><img src="../Images/b9ec12ebbf35bf09f5605a722e16a2f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*REgHs3PeemO3TIuyE46iRg.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">PSPNet model structure</figcaption></figure><h2 id="a28b" class="no lw it bd lx np nq dn mb nr ns dp mf mp nt nu mh mr nv nw mj mt nx ny ml nz bi translated">一百层提拉米苏</h2><p id="2d08" class="pw-post-body-paragraph kx ky it la b lb mn ju ld le mo jx lg mp mq lj lk mr ms ln lo mt mu lr ls lt im bi translated">如果深度学习带来了一种令人敬畏的趋势，那就是令人敬畏的研究论文名称！<a class="ae lu" href="https://arxiv.org/pdf/1611.09326.pdf" rel="noopener ugc nofollow" target="_blank">百层提拉米苏</a>(听起来很好吃！)使用了和我们之前看到的 U-Net 架构类似的结构。主要贡献是巧妙使用了类似于 DenseNet 分类模型的密集连接。</p><p id="5d92" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">这确实强调了计算机视觉的强大趋势，其中特征提取前端是在任何其他任务中表现良好的主要支柱。因此，寻找精度增益的第一个地方通常是您的特征提取前端。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/592bd8a7f5492a487b69b9afd3ef0b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*ioURm5w_miMj64DeLQ9H_g.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">FCDenseNet model structure</figcaption></figure><h2 id="71ea" class="no lw it bd lx np nq dn mb nr ns dp mf mp nt nu mh mr nv nw mj mt nx ny ml nz bi translated">重新思考阿特鲁卷积</h2><p id="bef3" class="pw-post-body-paragraph kx ky it la b lb mn ju ld le mo jx lg mp mq lj lk mr ms ln lo mt mu lr ls lt im bi translated">DeepLabV3 和<a class="ae lu" href="https://arxiv.org/pdf/1706.05587.pdf" rel="noopener ugc nofollow" target="_blank">是另一种进行多尺度处理的聪明方法，这次<strong class="la iu">没有增加参数</strong>。</a></p><p id="22eb" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">这个模型非常轻便。我们再次从特征提取前端开始，获取第四次下采样的特征用于进一步处理。这个分辨率非常低(比输入小 16 倍)，因此如果我们能在这里处理就太好了！棘手的是，在如此低的分辨率下，由于像素精度差，很难获得良好的定位。</p><p id="da4a" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">这就是 DeepLabV3 的主要贡献所在，它巧妙地使用了<em class="kz">阿特鲁</em>卷积。常规卷积只能处理非常局部的信息，因为权重总是彼此紧挨着。例如，在标准的 3×3 卷积中，一个权重与任何其他权重之间的距离只有一个步长/像素。</p><p id="47da" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">对于 atrous 卷积，我们将直接增加卷积权重之间的间距，而不会实际增加运算中权重的<em class="kz">数量</em>。因此，我们仍然使用一个总共有 9 个参数的 3x3，我们只是将相乘的权重间隔得更远！每个砝码之间的距离称为<em class="kz">膨胀率</em>。下面的模型图很好地说明了这个想法。</p><p id="aee3" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">当我们使用低膨胀率时，我们将处理非常局部/低尺度的信息。当我们使用高膨胀率时，我们处理更多的全局/高尺度信息。因此，DeepLabV3 模型混合了具有不同膨胀率的 atrous 卷积来捕捉多尺度信息。</p><p id="7cea" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">在 PSPNet 中解释的所有处理之后，在最后进行升级的技术也在这里完成。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi oh"><img src="../Images/c0619c13898a1a8ae04f8dbe0ae5f0b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tn-mWYhEva2S6rEZg4oZGA.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">DeepLabV3 model structure</figcaption></figure><h2 id="19cf" class="no lw it bd lx np nq dn mb nr ns dp mf mp nt nu mh mr nv nw mj mt nx ny ml nz bi translated">多路径细化网络</h2><p id="4d99" class="pw-post-body-paragraph kx ky it la b lb mn ju ld le mo jx lg mp mq lj lk mr ms ln lo mt mu lr ls lt im bi translated">我们之前已经看到 FRRN 是如何成功地将来自多个分辨率的信息直接组合在一起的。缺点是在如此高的分辨率下处理是计算密集型的，我们仍然需要处理和结合那些低分辨率的特征！</p><p id="9def" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated"><a class="ae lu" href="https://arxiv.org/pdf/1611.06612.pdf" rel="noopener ugc nofollow" target="_blank"> RefineNet </a>模型说我们不需要这样做。当我们通过特征提取网络运行输入图像时，我们自然会在每次下采样后获得多尺度特征图。</p><p id="0618" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">然后，RefineNet 以自下而上的方式处理这些多分辨率特征图，以组合多尺度信息。首先，独立处理每个特征图。然后，当我们放大时，我们将低分辨率的特征图与高分辨率的特征图结合起来，对它们一起做进一步的处理。因此，多尺度特征地图被独立地和一起处理。在下图中，整个过程从左向右移动。</p><p id="62ec" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">这里也采用了 PSPNet 和 DeepLabV3 中所述的在所有处理结束后进行升级的技术。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/b8fe7dfe986fc97a5223b794afa92898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*V6E6QIyB1BTOdbev4eDAqg.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">RefineNet model structure</figcaption></figure><h2 id="d718" class="no lw it bd lx np nq dn mb nr ns dp mf mp nt nu mh mr nv nw mj mt nx ny ml nz bi translated">大内核问题(GCN)</h2><p id="275a" class="pw-post-body-paragraph kx ky it la b lb mn ju ld le mo jx lg mp mq lj lk mr ms ln lo mt mu lr ls lt im bi translated">之前，我们看到了 DeepLabV3 模型如何使用具有不同膨胀率的 atrous 卷积来捕捉多尺度信息。棘手的是，我们一次只能处理一个秤，然后必须将它们组合起来。例如，速率为 16 的 atrous 卷积不能很好地处理局部信息，必须在以后与来自速率小得多的卷积的信息相结合，以便在语义分割中表现良好。</p><p id="666d" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">因此，在以前的方法中，多尺度处理首先单独发生，然后将结果组合在一起。如果能一次性获得多尺度信息，那就更有意义了。</p><p id="860e" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">为了做到这一点，<a class="ae lu" href="https://arxiv.org/pdf/1703.02719.pdf" rel="noopener ugc nofollow" target="_blank">全球卷积网络(GCN) </a>巧妙地提出使用大型一维核，而不是方形核。对于像 3x3、7x7 等这样的平方卷积，我们不能让它们太大而不影响速度和内存消耗。另一方面，一维内核的缩放效率更高，我们可以在不降低网络速度的情况下将它们做得很大。这张纸甚至被放大到了 15 号！</p><p id="de5b" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">你必须确保做的重要的事情是平衡水平和垂直回旋。此外，该论文确实使用了具有低滤波器数的小 3×3 卷积，以有效细化一维卷积可能遗漏的任何内容。</p><p id="e536" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">GCN 遵循与以前作品相同的风格，从特征提取前端处理每个尺度。由于一维卷积的效率，GCN 在所有尺度上执行处理，一直到全分辨率，而不是保持小规模，然后扩大规模。这允许在我们按比例放大时不断细化分割，而不是由于停留在较低分辨率而可能发生的瓶颈。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi oj"><img src="../Images/690e5e5b32859efac372d08a40796a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dj5OCBeY4svwaMb4N7fcJw.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">GCN model structure</figcaption></figure><h2 id="8ba9" class="no lw it bd lx np nq dn mb nr ns dp mf mp nt nu mh mr nv nw mj mt nx ny ml nz bi translated">DeepLabV3+</h2><p id="19fa" class="pw-post-body-paragraph kx ky it la b lb mn ju ld le mo jx lg mp mq lj lk mr ms ln lo mt mu lr ls lt im bi translated"><a class="ae lu" href="https://arxiv.org/pdf/1802.02611.pdf" rel="noopener ugc nofollow" target="_blank"> DeepLabV3+ </a>模型，顾名思义，是 DeepLabV3 的快速扩展，从它之前的进步中借用了一些概念上的想法。正如我们之前看到的，如果我们只是简单地等待在网络末端使用双线性插值来扩大规模，就会存在潜在的瓶颈。事实上，最初的 DeepLabV3 模型在结尾被 x16 升级了！</p><p id="25e6" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated">为了解决这个问题，DeepLabV3+建议在 DeepLabV3 之上添加一个中间解码器模块。通过 DeepLabV3 处理后，特征由 x4 进行上采样。然后，它们与来自特征提取前端的原始特征一起被进一步处理，然后被 x4 再次放大。这减轻了网络末端的负载，并提供了从特征提取前端到网络近端的快捷路径。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ok"><img src="../Images/92752beb33534f11f4ee6401cb1389d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MFchBd4c8ZEgE3qtbnTznw.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">DeepLabV3+ model structure</figcaption></figure><h1 id="ce20" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">CVPR 和 ECCV 2018</h1><p id="e74d" class="pw-post-body-paragraph kx ky it la b lb mn ju ld le mo jx lg mp mq lj lk mr ms ln lo mt mu lr ls lt im bi translated">我们在上一节中讨论的网络代表了你需要知道的进行语义分割的大部分技术！今年在计算机视觉会议上发布的许多东西都是微小的更新和准确性上的小波动，对开始并不是非常关键。为了彻底起见，我在这里为任何感兴趣的人提供了他们的贡献的快速回顾！</p><p id="0920" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated"><a class="ae lu" href="https://arxiv.org/pdf/1704.08545.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">图像级联网络(ICNet) </strong> </a> <strong class="la iu"> — </strong>使用深度监督并以不同的尺度运行输入图像，每个尺度通过其自己的子网并逐步组合结果</p><p id="5417" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated"><a class="ae lu" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Learning_a_Discriminative_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="la iu">【DFN】</strong></a><strong class="la iu">—</strong>使用深度监督并尝试分别处理线段的平滑和边缘部分</p><p id="f62a" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated"><a class="ae lu" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="la iu">DenseASPP</strong></a><strong class="la iu">—</strong>结合了密集连接和粗糙卷积</p><p id="e02b" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated"><a class="ae lu" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Context_Encoding_for_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">上下文编码</strong></a><strong class="la iu">——</strong>通过添加通道关注模块，利用全局上下文来提高准确性，该模块基于新设计的损失函数来触发对某些特征地图的关注。损失基于网络分支，该网络分支预测图像中存在哪些类别(即，更高级别的全局上下文)。</p><p id="d162" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated"><a class="ae lu" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Bilinski_Dense_Decoder_Shortcut_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">密集解码器快捷连接</strong> </a> <strong class="la iu"> — </strong>在解码阶段使用密集连接以获得更高的精度(以前仅在特征提取/编码期间使用)</p><p id="65b5" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated"><a class="ae lu" href="https://arxiv.org/pdf/1808.00897v1.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="la iu">【BiSeNet】</strong></a><strong class="la iu">—</strong>有两个分支:一个是深度分支，用于获取语义信息，另一个是对输入图像进行很少/很小的处理，以保留低级像素信息</p><p id="9ba8" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg mp li lj lk mr lm ln lo mt lq lr ls lt im bi translated"><a class="ae lu" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenli_Zhang_ExFuse_Enhancing_Feature_ECCV_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="la iu">ex fuse</strong></a><strong class="la iu">—</strong>使用深度监督，在处理之前，显式组合来自特征提取前端<em class="kz">的多尺度特征，以确保多尺度信息在各级一起处理。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ol"><img src="../Images/40c98fcf9a4a41a43139c0b5e561e447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w5jqI6cVQnZG8vSuWc4BNA.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">ICNet model structure</figcaption></figure><h1 id="476b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">TL；DR 或者如何做语义分割</h1><ul class=""><li id="6ede" class="om on it la b lb mn le mo mp oo mr op mt oq lt or os ot ou bi translated">注意分类网络权衡。您的分类网络是您处理要素的主要驱动力，您的大部分收益/损失将来自于此</li><li id="26f1" class="om on it la b lb ov le ow mp ox mr oy mt oz lt or os ot ou bi translated">在多个尺度上处理并将信息结合在一起</li><li id="8023" class="om on it la b lb ov le ow mp ox mr oy mt oz lt or os ot ou bi translated">多尺度池、atrous convs、大型一维 conv 都适用于语义分割</li><li id="6a83" class="om on it la b lb ov le ow mp ox mr oy mt oz lt or os ot ou bi translated">你不需要在高分辨率下做大量的处理。为了速度，在低分辨率下完成大部分，然后升级，如果必要的话，在最后做一些轻微的处理</li><li id="dc2a" class="om on it la b lb ov le ow mp ox mr oy mt oz lt or os ot ou bi translated">深度监督可以稍微提高你的准确性(尽管设置训练更加棘手)</li></ul></div><div class="ab cl pa pb hx pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="im in io ip iq"><h1 id="c5dc" class="lv lw it bd lx ly ph ma mb mc pi me mf jz pj ka mh kc pk kd mj kf pl kg ml mm bi translated">喜欢学习？</h1><p id="b7f2" class="pw-post-body-paragraph kx ky it la b lb mn ju ld le mo jx lg mp mq lj lk mr ms ln lo mt mu lr ls lt im bi translated">在 twitter 上关注我，我会在这里发布所有最新最棒的人工智能、技术和科学！也在<a class="ae lu" href="https://www.linkedin.com/in/georgeseif/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上和我联系吧！</p></div></div>    
</body>
</html>