<html>
<head>
<title>My Journey to Reinforcement Learning — Part 0: Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我的强化学习之旅——第0部分:简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/my-journey-to-reinforcement-learning-part-0-introduction-1e3aec1ee5bf?source=collection_archive---------3-----------------------#2018-04-06">https://towardsdatascience.com/my-journey-to-reinforcement-learning-part-0-introduction-1e3aec1ee5bf?source=collection_archive---------3-----------------------#2018-04-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/b1fb2bde061ab495f43d80b29b75368d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/1*6sE---UY4iiCDPA2kdYQkg.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Gif from <a class="ae jy" href="https://giphy.com/gifs/pancake-flipper-bsBrYepOYSebe/download" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="1a07" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">目前，我对强化学习知之甚少，我想改变这一点，所以这是我学习强化学习的第一步。作为第一步，我希望先介绍高层次的概述。</p><p id="3e3d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">请注意，这篇文章是为我未来的自己写的，我的学习过程可能会很慢，或者与你的不同。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="1e23" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">加州大学伯克利分校彼得·博迪克的强化学习教程</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi le"><img src="../Images/f689e79011a657b5f7f1ae784dcbfad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jXidfjJ6BA6Q8tUMpfP2qg.png"/></div></div></figure><p id="9ca7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从这个讲座中，我了解到，强化学习相对于有监督或无监督来说，更具有一般性。然而，似乎仍然有一个目标的概念，因此我假设将有一个特定的成本函数来衡量我们离实现目标有多近。下面是对强化学习的一个很好的总结。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ln"><img src="../Images/e22829da8d665044a7666c103ff503db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wAh-h95qgFaCp4dXODg6nA.png"/></div></div></figure><div class="lf lg lh li gt ab cb"><figure class="lo jr lp lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><img src="../Images/f124b0cface2ec745793c97f0492a6e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*LsvnWEaJBfMb5vYULHn7Lg.png"/></div></figure><figure class="lo jr lu lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><img src="../Images/1c001f8b11c5abf5dc0446de09652e72.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*gjiCSZXA3UxJjMqJfd8WUQ.png"/></div></figure><figure class="lo jr lv lq lr ls lt paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><img src="../Images/7e869552b670f3edce820950480808f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*TKklaXvUY_eCir7PnfPuyw.png"/></div></figure></div><p id="00e1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">右图</strong> →最优解(每步无奖励)<br/> <strong class="kb ir">中图</strong> →每步奖励为-0.1时的解<br/> <strong class="kb ir">左图</strong> →每步奖励为0.01时的解</p><p id="4361" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">上面，图片是一个完美的例子(对我来说),展示了强化学习是多么复杂。如果我们制造一个机器人，它的目标是得到最多的点，最优解将是最正确的图像。然而，根据不同的政策(这一次，每一步都有奖励)，机器人学习的解决方案是完全不同的。从这里开始，ppt解释了相当多的数学，所以我不会把它作为一个高层次的概述，但是ppt有一个非常好的总结页面。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi lw"><img src="../Images/12fb2488ea1ba5ea4006e5fc132e3de7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oAoAo-igg9iAPxVSpphlpQ.png"/></div></div></figure><p id="24f8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从上文中，我了解到当使用强化学习时，强化学习最具挑战性的部分实际上是设计特征、状态和奖励。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="f4c0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">机器学习国际会议(ICML 2007)教程</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi lx"><img src="../Images/5a9f461e6b0447902e87cb20c24a48d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AzX5FvOj6DAKIduK8dHFgw.png"/></div></div></figure><p id="d1a1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从之前的演示中，我们已经了解到，具有挑战性的部分是设计状态和奖励。底部的缩写代表<a class="ae jy" href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="noopener ugc nofollow" target="_blank">马尔可夫决策过程</a>和<a class="ae jy" href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process" rel="noopener ugc nofollow" target="_blank">部分可观测马尔可夫决策过程</a>。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ly"><img src="../Images/a56c5227d851d5bd56594024ee9527b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bbGk9ZfAtF0dx5ucE7UJFg.png"/></div></div></figure><p id="e311" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">上面显示了状态、行动和奖励的完美示例。我们可以看到，这个设置可以很容易地应用于任何游戏。(国际象棋，开始工艺甚至真实世界的设置。)</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi lz"><img src="../Images/a39fb6df3b462da2970a185d880142ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R5vevDucGaasLr1xEcof1w.png"/></div></div></figure><p id="2ea5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">不同类型的学习算法，从这里我了解到有不同的类似于分类算法集的算法集，SVM，NN，或者<a class="ae jy" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="noopener ugc nofollow" target="_blank"> k近邻</a>。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="9ea6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><a class="ae jy" href="https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/" rel="noopener ugc nofollow" target="_blank"> <strong class="kb ir">强化学习简易入门&amp;其实现</strong></a><strong class="kb ir">(analyticsvidhya)</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/4d8ffca35ec5bcbc0cd36d7a78b7f5cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/0*CIZYhOcpGx2M2AJq.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from this <a class="ae jy" href="https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="4cea" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当我们谷歌强化学习时，我们可以一遍又一遍地看到如上图。因此，与其看到一个代理人或环境，不如把它想象成一个婴儿学习如何走路的过程。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/b1fe6d304d8900e5c37d7ba885405792.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/0*Qtny7kW2jY0k__ia.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from this <a class="ae jy" href="https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="5f2b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="mb">这个例子的“问题陈述”是</em> <strong class="kb ir"> <em class="mb">行走</em> </strong> <em class="mb">，其中</em> <strong class="kb ir"> <em class="mb">这个孩子是一个代理人</em> </strong> <em class="mb">试图操纵</em> <strong class="kb ir"> <em class="mb">环境(它行走的表面)</em> </strong> <em class="mb">通过</em> <strong class="kb ir"> <em class="mb">采取行动(即行走)</em> </strong> <em class="mb">和他/她当他/她完成任务的一个</em><strong class="kb ir"><strong class="kb ir"><em class="mb"/></strong><em class="mb"/><strong class="kb ir"><em class="mb"/></strong><em class="mb">子模块(即采取几个步骤)时，孩子得到一个</em> <strong class="kb ir"> <em class="mb">奖励</em><em class="mb">而当他/她完成任务的一个</em></strong><em class="mb">子模块时，将不会得到任何巧克力</em> <strong class="kb ir"> <em class="mb">(也就是说这是一个强化学习问题的简化描述。</em>——<a class="ae jy" href="https://www.analyticsvidhya.com/blog/author/jalfaizy/" rel="noopener ugc nofollow" target="_blank">法赞·谢赫</a></strong></strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mc"><img src="../Images/e0fbbf58ca7d529c56d4dfb6f1ed7ef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*m5qRVsjjez1qpXm-.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from this <a class="ae jy" href="https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="09ae" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">作者实际上对这些算法的不同之处做了很长的解释，如果你想看的话，请点击这里。不过短短的一/两句话。</p><p id="e2db" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> <em class="mb">监督vs RL </em> </strong>:两者都映射了输入和输出之间的关系，但在RL中有一个奖励函数来衡量代理采取的行动，另外还有一个成本函数来衡量我们是否达到了最终目标。(例如赢得一盘棋→赢得比赛很重要，但赢得一盘棋有多种方式)<br/> <strong class="kb ir"> <em class="mb">无监督vs RL </em> </strong>:无监督学习(大多)是在底层数据中发现模式，并对其进行聚类。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="0cc3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="6f44" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">还有一个帖子，“<a class="ae jy" rel="noopener" target="_blank" href="/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287">各种强化学习算法介绍。第一部分(Q-Learning，SARSA，DQN，DDPG) </a>”是一篇了解不同类型学习算法的优秀文章。总的来说，互联网上有数以百万计的资源，所以任何想学习RL的人都不会有时间去寻找资源。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="3f23" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="8e92" class="md me iq kb b kc kd kg kh kk mf ko mg ks mh kw mi mj mk ml bi translated">(2018).cs . uwaterloo . ca 2018年4月6日检索，来自<a class="ae jy" href="https://cs.uwaterloo.ca/~ppoupart/ICML-07-tutorial-slides/icml07-brl-tutorial-part2-intro-ghavamzadeh.pdf" rel="noopener ugc nofollow" target="_blank">https://cs . uwaterloo . ca/~ ppou part/ICML-07-tutorial-slides/icml 07-brl-tutorial-part 2-intro-ghavamzadeh . pdf</a></li><li id="4935" class="md me iq kb b kc mm kg mn kk mo ko mp ks mq kw mi mj mk ml bi translated">2018年<em class="mb">People.eecs.berkeley.edu</em>。【在线】。可用:<a class="ae jy" href="https://people.eecs.berkeley.edu/~jordan/MLShortCourse/reinforcement-learning.ppt." rel="noopener ugc nofollow" target="_blank">https://people . eecs . Berkeley . edu/~ Jordan/ml short course/reinforcement-learning . PPT .</a>【访问时间:2018年4月6日】。</li><li id="b097" class="md me iq kb b kc mm kg mn kk mo ko mp ks mq kw mi mj mk ml bi translated">部分可观测马尔可夫决策过程。(2018).En.wikipedia.org。检索于2018年4月6日，来自<a class="ae jy" href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Partially _ observable _ Markov _ decision _ process</a></li><li id="094b" class="md me iq kb b kc mm kg mn kk mo ko mp ks mq kw mi mj mk ml bi translated">马尔可夫决策过程。(2018).En.wikipedia.org。于2018年4月6日检索，来自<a class="ae jy" href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Markov_decision_process</a></li><li id="d141" class="md me iq kb b kc mm kg mn kk mo ko mp ks mq kw mi mj mk ml bi translated">各种强化学习算法介绍。第一部分(Q-Learning，SARSA，DQN，DDPG)。(2018).走向数据科学。2018年4月6日检索，来自<a class="ae jy" rel="noopener" target="_blank" href="/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287">https://towards data science . com/introduction-to-variable-reinforcement-learning-algorithms-I-q-learning-sarsa-dqn-ddpg-72 a5 E0 CB 6287</a></li></ol></div></div>    
</body>
</html>