<html>
<head>
<title>Gradient Descent, Back-Prop and derivatives</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降、反向推进和导数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-back-prop-and-derivatives-5b286bb8d8b5?source=collection_archive---------3-----------------------#2017-08-16">https://towardsdatascience.com/gradient-descent-back-prop-and-derivatives-5b286bb8d8b5?source=collection_archive---------3-----------------------#2017-08-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn"><p id="5cca" class="jo jp iq bd jq jr js jt ju jv jw jx dk translated"><em class="jy">我收到了很多人关于梯度下降和反向传播的问题——这里是我对这些概念的</em> <strong class="ak"> <em class="jy">简化解释</em> </strong> <em class="jy">，给你一些关于神经网络是如何训练的直觉！</em></p></blockquote><p id="fceb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv jx ij bi translated">在训练神经网络时，我们使用梯度下降来降低我们的网络犯的错误总数。梯度下降是一种希尔下降算法。这座山和我们网络预测的“总错误数”一样高。</p><p id="90ae" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">我们从运行一个循环开始，在每个循环迭代中，我们的目标是减少我们的净错误总数。这就是我们如何计算网络的训练准确度分数。</p><p id="b1c1" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">在正向推进的最后，通过比较我们的预测结果和真实结果，我们知道我们的网络所产生的“总误差量”。(Y 减去 Yhat)。</p><p id="39a0" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">但是只有在反推步骤中，我们才能看到总误差中有多少是由每个神经元 T21 引起的。在每个神经元犯下的所有小错误之后，加起来成为整个网络的“总错误”。</p><p id="0a38" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">微分学是我们用来观察事物如何变化的方法:它们变化快吗？慢慢改变？他们到底有没有改变？</p><p id="8fbe" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">当我们对(山坡)进行微分以获得成本函数的导数时，我们确定了我们所处位置的梯度/斜率。我们得到的梯度告诉我们<em class="kw">我们在误差山</em>的每个方向上倾斜了多少。这帮助我们小心翼翼地走下错误之山，一步一步地到达谷底，在我们的循环中，我们找到救赎！</p><p id="36b9" class="pw-post-body-paragraph jz ka iq kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv jx ij bi translated">我刚刚谷歌了一下，发现了这个很棒的视频…制作得很好，很直观！</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="lh li l"/></div></figure></div></div>    
</body>
</html>