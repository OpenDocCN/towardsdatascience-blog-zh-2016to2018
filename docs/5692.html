<html>
<head>
<title>Understand how to transfer your paragraph to vector by doc2vec</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解如何通过 doc2vec 将您的段落转换为 vector</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understand-how-to-transfer-your-paragraph-to-vector-by-doc2vec-1e225ccf102?source=collection_archive---------7-----------------------#2018-11-03">https://towardsdatascience.com/understand-how-to-transfer-your-paragraph-to-vector-by-doc2vec-1e225ccf102?source=collection_archive---------7-----------------------#2018-11-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/8b2d180636fd6effa6ecfd4e4d0e8d31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BvHaTwdKgHIrjxNf"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">“person holding camera lens” by <a class="ae kf" href="https://unsplash.com/@pawelskor?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Paul Skorupskas</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4f33" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在之前的故事中，提到的<a class="ae kf" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a"> word2vec </a>是 Mikolov 等人(2013)介绍的。Mikolov 和 Le 发表了句子/文档向量转换。这是嵌入技术的又一个突破，我们可以用向量来表示一个句子或文档。米科洛夫等人称之为“段落向量”。</p><p id="9acf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看完这篇文章，你会明白:</p><ul class=""><li id="df6c" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">段落向量设计</li><li id="373e" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">体系结构</li><li id="7f06" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">履行</li><li id="b2c1" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">拿走</li></ul><h1 id="2775" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">段落向量设计</h1><p id="370a" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">doc2vec 的设计基于 word2vec。如果你不熟悉 word2vec(即 skip-gram 和 CBOW)，你可以看看这个<a class="ae kf" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a">的故事</a>。Doc2vec 还使用无监督学习方法来学习文档表示。每个文档的文本(即单词)输入可以是多种多样的，而输出是固定长度的向量。</p><p id="af67" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">段落向量和单词向量被初始化。段落向量在所有文档中是唯一，而单词向量在所有文档中是共享的，从而可以从不同的文档中学习单词向量。</p><p id="7650" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在训练阶段，单词向量将被训练，而段落将被丢弃。在预测阶段(即在线预测)，段落向量将被随机初始化，并通过词向量来计算。</p><h1 id="0aef" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">体系结构</h1><p id="b4ef" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">学习单词向量有两种算法。这两种方法都是受学习单词向量的启发，单词向量是 skip-gram 和连续词袋(CBOW)</p><p id="b69e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="mv">分布式段落向量记忆模型(PV-DM) </em> </strong></p><p id="0c19" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">段落向量和单词向量都是随机初始化的。每个段落向量被分配给单个文档，而单词向量在所有文档之间共享。平均或连接段落向量和单词向量，并传递到随机梯度下降，梯度通过反向传播获得。</p><figure class="mx my mz na gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mw"><img src="../Images/435dc19861a78ca991f940495d58b4ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*QeLhnXrW5_PP_7MM-UDMRw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Architecture of PV-DM (Mikolov et al., 2014)</figcaption></figure><p id="63ed" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种方法类似于 word2vec 中的连续词包(CBOW)方法。</p><p id="711c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="mv">【分布式文字包版本段落矢量(PV-DBOW) </em> </strong></p><p id="71c0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一种方法走的是不同的路。它不是预测下一个单词，而是使用段落向量对文档中的整个单词进行分类。在训练过程中，对单词列表进行采样，然后形成分类器，对单词是否属于文档进行分类，从而学习单词向量。</p><figure class="mx my mz na gt ju gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/d4807f4553f7adb57e144245338b9366.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*aranc4uU1DwZ9nULEq7Yfw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Architecture of PV-DBOW (Mikolov et al., 2014)</figcaption></figure><p id="01b7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种方法类似于 word2vec 中的跳格法。</p><h1 id="24c5" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">履行</h1><p id="81a9" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">首先，我们需要传递训练数据来构建词汇，并调用训练阶段来计算单词向量。</p><pre class="mx my mz na gt nc nd ne nf aw ng bi"><span id="c9cb" class="nh lt it nd b gy ni nj l nk nl">doc2vec_embs = Doc2VecEmbeddings()<br/>x_train_tokens = doc2vec_embs.build_vocab(documents=x_train)<br/>doc2vec_embs.train(x_train_tokens)</span></pre><p id="c867" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">之后，我们可以通过提供训练数据和测试数据对其进行编码。</p><pre class="mx my mz na gt nc nd ne nf aw ng bi"><span id="26e8" class="nh lt it nd b gy ni nj l nk nl">x_train_t = doc2vec_embs.encode(documents=x_train)<br/>x_test_t = doc2vec_embs.encode(documents=x_test)</span></pre><p id="d668" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们可以将向量传递给分类器</p><pre class="mx my mz na gt nc nd ne nf aw ng bi"><span id="7e99" class="nh lt it nd b gy ni nj l nk nl">from sklearn.linear_model import LogisticRegression</span><span id="bf9a" class="nh lt it nd b gy nm nj l nk nl">model = LogisticRegression(solver='newton-cg', max_iter=1000)<br/>model.fit(x_train_t, y_train)</span><span id="1645" class="nh lt it nd b gy nm nj l nk nl">y_pred = model.predict(x_test_t)</span><span id="baff" class="nh lt it nd b gy nm nj l nk nl">from sklearn.metrics import accuracy_score<br/>from sklearn.metrics import classification_report</span><span id="3cb6" class="nh lt it nd b gy nm nj l nk nl">print('Accuracy:%.2f%%' % (accuracy_score(y_test, y_pred)*100))<br/>print('Classification Report:')<br/>print(classification_report(y_test, y_pred))</span></pre><p id="d907" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果</p><pre class="mx my mz na gt nc nd ne nf aw ng bi"><span id="c5f2" class="nh lt it nd b gy ni nj l nk nl">Accuracy: 52.8%<br/>Average Precision: 0.66<br/>Average Recall: 0.53<br/>Average f1: 0.5</span></pre><h1 id="96cd" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">拿走</h1><p id="e31a" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">要访问所有代码，你可以访问我的 github repo。</p><ul class=""><li id="e322" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">与 word2vec 不同，doc2vec 动态计算句子/文档向量。换句话说，<strong class="ki iu">在预测时间</strong>内得到矢量需要时间。</li><li id="f5a2" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">根据 Mikolov 等人的实验，<strong class="ki iu"> PV-DM 始终优于 PV-DBOW </strong>。</li><li id="0079" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu">在 PV-DM 方法中，串联方式通常比求和/平均方式更好</strong>。</li></ul><h1 id="3f55" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">参考</h1><p id="dbe4" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">米克洛夫·托马斯，来自英国。2014.<a class="ae kf" href="https://arxiv.org/pdf/1405.4053.pdf" rel="noopener ugc nofollow" target="_blank">句子和文档的分布式表示</a></p><p id="40b3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://radimrehurek.com/gensim/models/doc2vec.html" rel="noopener ugc nofollow" target="_blank">gensim 中的 doc 2 vec</a></p></div></div>    
</body>
</html>