<html>
<head>
<title>[ Paper Summary ] H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">H-DenseUNet:用于从 CT 体积中分割肝脏和肿瘤的混合密集连接 UNet</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-summary-h-denseunet-hybrid-densely-connected-unet-for-liver-and-tumor-segmentation-from-a47597845d37?source=collection_archive---------8-----------------------#2018-07-31">https://towardsdatascience.com/paper-summary-h-denseunet-hybrid-densely-connected-unet-for-liver-and-tumor-segmentation-from-a47597845d37?source=collection_archive---------8-----------------------#2018-07-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/9d8c9aeca55ee92b21a5110941e89473.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/1*CVgPmD0X8h_rBtcoIGwssQ.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/search/CT-Volumes" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="102c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最近我在玩 3D 卷积运算，我想知道更多关于。</p><blockquote class="kx ky kz"><p id="4c66" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated"><strong class="kb ir">T3】</strong></p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="https://arxiv.org/pdf/1709.07330.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="ce4c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">摘要</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/25c31a9fc1b1656633c47c76817d7cac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qbnOaJJsA02KysgZNOULMA.png"/></div></div></figure><p id="8ccc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">分割肝脏和其他器官是一项具有挑战性的任务，2D 卷积运算没有充分利用第三维，而 3D 卷积运算具有高计算成本。为了克服这些问题，作者提出了一种新的混合密集连接 UNet 架构，其中他们使用 2D DenseUNet 来提取切片内特征，并使用 3D 计数器部分来分层聚集体积上下文。所有这些网络都是通过端到端的学习来训练的。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="4380" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">简介</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/fb07aab83f69e5e48c35ee42d5ef0940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9RitzKbiCF6MtDK2Gcjd6Q.png"/></div></div></figure><p id="2bcb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">肝癌是最常见的癌症之一，因此准确的诊断非常重要。这就是为什么分割正确的肝脏区域是重要的。但是肝脏和肝脏病变是由放射科医师在逐切片的基础上描绘的，这可能导致错误并消耗大量时间。在 CT 中分割肝脏区域是困难的，因为它缺乏其他器官之间的对比度。(见上图，这就是为什么许多放射科医生使用注射协议来增强对比度。)此外，分割肿瘤更具挑战性，因为形状不规则，没有清晰的边界线，并且许多 CT 扫描包括沿 z 轴方向变化很大的各向异性维度。已经提出了许多不同方法来自动化这个过程，例如使用手工制作的特征或完全卷积的神经网络。</p><p id="d1d1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">通常，放射科医师根据沿 z 轴的许多相邻切片来观察和分割肿瘤，然而 2D 基卷积方法没有考虑 z 轴。(一些研究人员已经应用了对于每个 xy、yz 和 xz 平面具有 3 个 2D 卷积神经网络并对它们进行平均的方法，然而，这些方法有其自身的局限性。)3D 卷积神经网络占用更多的 GPU 内存和训练时间。此外，现有的预训练 3D 卷积模型并不多。为了克服这些问题，作者提出了混合密集连接的 UNet。关键因素是增加网络深度和混合特征探索。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="9a87" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">相关工作</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lx"><img src="../Images/5a965cc4b64601674f919fe33447b65a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CnHl2CFBjfqRLaQvY294_g.png"/></div></div></figure><p id="b5eb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">开发了许多其他分割方法，例如使用手工制作的特征，如水平集方法(如纹理特征向量)，结合传统的机器学习算法，如支持向量机。但是在深度学习的热潮之后，卷积神经网络成为执行分割的主要方法。(使用 3D 条件随机场等方法)</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="dcbd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">方法</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ly"><img src="../Images/567a0337576c2c4949379e40f9f6a143.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*upabGHvSJDva8wVct21hNg.png"/></div></div></figure><p id="6cbd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">上图显示了作者方法的总体轮廓，一个有趣的事实是，实际上有 4 个网络，2D Res Net，2D Dense-UNet，3D Dense-UNet 和 HFF 层。</p><p id="fbad" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="la">用于切片内特征提取的深层 2D dense unet</em></p><p id="d64d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">2D 密集 UNet 是由密集连接的块组成的 UNet 架构。(而大致的框架框架见下图。).这个网络的两大优点是:第一，它通过密集连接最大化了层间的信息流；第二，它通过 UNet 型连接利用了高级功能和低级功能。(此外，2D 密集 UNet 的深度为 167，另外请注意，我不会详细说明输入数据或输出数据的确切维度。)上采样是通过双线性插值完成的，使用线性单位(ReLU)作为激活函数。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lz"><img src="../Images/10e451157fc02cc6313eaa610f495056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oZvX6tsnjOmeD18oGK95wg.png"/></div></div></figure><p id="dd0c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="la">用于混合特性探索的 H-dense unet</em></p><p id="3fc0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">2D 密集 UNet 能够从给定的 ct 扫描中提取高级特征，但是，它仍然没有充分利用 z 轴中存在的数据。为了克服这一点，作者提出了一种 H-DenseUNet 来联合融合和优化所学习的切片内和切片间特征，以便更好地分割肝脏肿瘤。在 2D 密集 UNet 的最后一层之后，输出的特征地图被转换成如下所示的体数据。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/411944b453df7c4e9fe478cc2b16e5e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*cM_05FnmJi8SR0qq0oOfrg.png"/></div></figure><p id="6ca0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">此特征地图是 3D Dense UNet 的输入数据，当用数学等式表示时，可以认为如下所示。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/2f3d41a4332d9bda3d903e2232e06679.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*KkRx3W2KFercQUaFzD6y6A.png"/></div></figure><p id="c742" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，从两个网络输出的数据相加并传递到 HFF 层。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/3af7f81082a3a75103e95d3c57b48b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*Prpvl9801IX8XS1zs2cknw.png"/></div></figure><p id="9bb0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">完整的网络架构如下所示。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi md"><img src="../Images/23384a291dc5ec883d1bd9643c55fced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-DU5Hm8r1VneqsMd_Sjvg.png"/></div></div></figure><p id="8c5e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="la">损失函数、训练和推理方案</em></p><p id="9119" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">作者使用加权交叉熵损失函数通过端到端的方式训练网络。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi me"><img src="../Images/b1559a42e6e433ad659e47fbd90fcf70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rh1W9YFmCAvc_oGmrD2v0Q.png"/></div></div></figure><p id="cdff" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">并且每个网络训练的顺序可以列举如下<br/> 1。Resnet →得到粗肝分割<br/> 2。2D DenseUNet →用 DenseNet 的权重(编码器)初始化权重，同时随机初始化解码器<br/> 3。在没有 UNet 连接的情况下开始 2D 密集 UNet 的训练，并且在一定时期之后进行 UNet 连接。<br/> 4。使用上述方案训练 2D 密集 UNet <br/> 5。在训练 2D UNet 之后，训练 3D 密集 UNet，同时固定 2D 密集 UNet 的参数。<br/> 6。在 3D 密集 UNet 被训练后，使用损失函数微调整个网络，如下所示。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mf"><img src="../Images/15724b8739a1dafaacaec8c6efc7d8b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ByuhtJrkTDMprXKW5OTpKA.png"/></div></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="2133" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">实验和结果</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mg"><img src="../Images/32f5354bb778693258fbccef164bf5dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w2Lw4c9fgxRaJ6Nv1OwJnQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from this <a class="ae jy" href="https://competitions.codalab.org/competitions/17094" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="3601" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">使用来自 MICCAI 2017 LiTS challenge 的数据，该数据包含分别用于训练和测试的 131 次和 70 次对比度增强的 3D 腹部 CT 扫描，作者在-200 和 250 之间截断了图像强度值。(删除了所有不相关的细节。).所有模型都通过 Dice per case 评分、Dice global 评分和均方根误差进行评估。工作使用的框架是 Keras，学习率为 0.01，学习率衰减。</p><p id="9a9a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="la">预训练模型的有效性/2D 和 3D DenseUNet 的比较</em></p><div class="ll lm ln lo gt ab cb"><figure class="mh jr mi mj mk ml mm paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/e354d8b58ea16b0c168c12edb0ce91b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*JC7q-cz7MjHJZV5hpKBGow.png"/></div></figure><figure class="mh jr mn mj mk ml mm paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/815a75d8733280e0db304290b40f4090.png" data-original-src="https://miro.medium.com/v2/resize:fit:1452/format:webp/1*fWAazG3qZ7lQMnmAEaJ1PA.png"/></div></figure></div><p id="e9bd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，当作者比较有和没有预训练权重的模型的训练损失以及骰子得分时，很明显需要预训练权重以获得更好的性能。此外，当比较 2D 密集 UNet 和 3D 密集 UNet 的性能时，很明显 2D 密集 UNet 的性能更好。最后，UNet connected 也为实现更好的性能发挥了作用。</p><p id="3f78" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="la">混合特征融合的有效性</em></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mo"><img src="../Images/a834bb45dc1ffa565df782b73b474969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iEkpjdoM36KdvrivxNRT6g.png"/></div></div></figure><p id="04df" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，作者比较了具有和不具有混合连接的网络，发现具有混合连接的网络产生更清晰的分段掩码。</p><p id="655f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="la">与其他方法的比较</em></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mp"><img src="../Images/58357f6b1f897756aa585d98355aad68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*98SZtaqP2PLB1IFViybgvQ.png"/></div></div></figure><p id="c08c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">甚至当与其他现有技术的网络相比时，我们可以观察到作者提出的方法在所有不同的测量中具有最高的性能分数。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="86ff" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">讨论</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mq"><img src="../Images/754ecf6dbad6700930f1678f67941c8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aeVKkNtxqFbp0z_0x_OEWg.png"/></div></div></figure><p id="cab0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">作者提出的方法能够在不同的测量标准下获得最佳性能。为了更好地理解性能增益，作者分析了每个患者肿瘤大小的有效性。当作者绘制肿瘤大小的散点图时，我们可以注意到一个很大的变化。如下所示，作者的方法提高了大肿瘤组和小肿瘤组的 dice 评分准确性。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mr"><img src="../Images/c01b9238a27c97458ef436264e53250e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1OdGgl_SVPMAyGw70IURzg.png"/></div></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="b781" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结论</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lx"><img src="../Images/8684ebc8ae2f23b088c335f97861afa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BituwkKcQ3n2G_pFOGJVCA.png"/></div></div></figure><p id="dc9c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">总之，本文的作者提出了一种新的混合架构，该架构可以有效地探测高级别代表性片内和片间特征，随后通过混合特征融合层优化这些特征。并且克服了 2D/3D 卷积神经网络的问题。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="1e23" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="568a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">网络的训练方式给我留下了特别深刻的印象。事实上，该网络没有从头到尾进行训练是一个败笔，但是作者提出了一个非常聪明的方法来训练一个复杂的网络。</p><p id="9b48" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有写作的列表，请<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">在这里查看我的网站</a>。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">与此同时，请在我的 twitter <a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>关注我，并访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我也实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文 pos </a> t</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="69ae" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="a374" class="ms mt iq kb b kc kd kg kh kk mu ko mv ks mw kw mx my mz na bi translated">(2018).Arxiv.org。检索于 2018 年 7 月 31 日，来自 https://arxiv.org/pdf/1709.07330.pdf<a class="ae jy" href="https://arxiv.org/pdf/1709.07330.pdf" rel="noopener ugc nofollow" target="_blank"/></li><li id="52e4" class="ms mt iq kb b kc nb kg nc kk nd ko ne ks nf kw mx my mz na bi translated">李，谢，陈，洪，齐，谢，窦，秦，傅，陈，恒，彭(2017)。H-DenseUNet:用于从 CT 体积中分割肝脏和肿瘤的混合密集连接 UNet。Arxiv.org。检索于 2018 年 7 月 31 日，来自 https://arxiv.org/abs/1709.07330<a class="ae jy" href="https://arxiv.org/abs/1709.07330" rel="noopener ugc nofollow" target="_blank"/></li><li id="686b" class="ms mt iq kb b kc nb kg nc kk nd ko ne ks nf kw mx my mz na bi translated">CodaLab —竞争。(2017).Competitions.codalab.org。检索于 2018 年 7 月 31 日，来自<a class="ae jy" href="https://competitions.codalab.org/competitions/17094" rel="noopener ugc nofollow" target="_blank">https://competitions.codalab.org/competitions/17094</a></li></ol></div></div>    
</body>
</html>