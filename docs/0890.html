<html>
<head>
<title>How to do text classification with CNNs, TensorFlow and word embedding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用CNN，TensorFlow，单词嵌入做文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-do-text-classification-using-tensorflow-word-embeddings-and-cnn-edae13b3e575?source=collection_archive---------0-----------------------#2017-07-06">https://towardsdatascience.com/how-to-do-text-classification-using-tensorflow-word-embeddings-and-cnn-edae13b3e575?source=collection_archive---------0-----------------------#2017-07-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="0ba5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设我给你一篇文章的标题“Twitter Bootstrap的惊人扁平版本”，并问你这篇文章出现在哪个出版物上:纽约时报、TechCrunch或GitHub。你的猜测是什么？一篇题为“最高法院审理党派选区重大案件”的文章怎么样？</p><p id="dfce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你猜到GitHub和纽约时报了吗？为什么？像Twitter和Major这样的词很可能出现在任何出版物中，但像Twitter Bootstrap和Supreme Court这样的词序列更有可能分别出现在GitHub和纽约时报中。我们能训练一个神经网络来学习这个吗？</p><blockquote class="kl km kn"><p id="7e21" class="jn jo ko jp b jq jr js jt ju jv jw jx kp jz ka kb kq kd ke kf kr kh ki kj kk ij bi translated">注意:评估者现在已经进入核心张量流。 <a class="ae ks" href="https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/machine_learning/deepdive/09_sequence/txtclsmodel/trainer" rel="noopener ugc nofollow" target="_blank"> <em class="iq">更新了使用tf.estimator而不是tf.contrib.learn.estimator的代码现已在GitHub</em></a><em class="iq">——以更新后的代码为起点。</em></p></blockquote><h1 id="2731" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">创建数据集</h1><p id="061d" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">机器学习就是从例子中学习。为了了解给定标题的文章的可能来源，我们需要大量文章标题及其来源的示例。尽管它存在严重的选择偏差(因为只包括HN书呆子成员感兴趣的文章)，但黑客新闻文章的<a class="ae ks" href="https://cloud.google.com/bigquery/public-data/hacker-news" rel="noopener ugc nofollow" target="_blank"> BigQuery公共数据集</a>是这一信息的合理来源。</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="c251" class="mf ku iq mb b gy mg mh l mi mj">query="""<br/>SELECT source, REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ') AS title FROM<br/>(SELECT<br/>  ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,<br/>  title<br/>FROM<br/>  `bigquery-public-data.hacker_news.stories`<br/>WHERE<br/>  REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')<br/>  AND LENGTH(title) &gt; 10<br/>)<br/>WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch')<br/>"""<br/>traindf = bq.Query(query + " AND MOD(ABS(FARM_FINGERPRINT(title)),4) &gt; 0").execute().result().to_dataframe()<br/>evaldf  = bq.Query(query + " AND MOD(ABS(FARM_FINGERPRINT(title)),4) = 0").execute().result().to_dataframe()</span></pre><p id="2457" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本质上，我从BigQuery中的黑客新闻故事数据集中提取URL和标题，并将其分离到一个训练和评估数据集中(完整代码请参见<a class="ae ks" href="https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/textclassification/txtcls.ipynb" rel="noopener ugc nofollow" target="_blank"> Datalab笔记本</a>)。可能的标签是github、纽约时报或techcrunch。以下是生成的数据集的外观:</p><figure class="lw lx ly lz gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mk"><img src="../Images/e209544f3f3387ccb515f016bcf0c693.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NVAegfGxvbU1TUtnF21WPQ.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Training dataset</figcaption></figure><p id="5788" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我将两只熊猫的数据帧写成CSV文件(总共有72，000个训练样本，大约平均分布在纽约时报、github和techcrunch上)。</p><h1 id="e35b" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">创造词汇</h1><p id="fb25" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">我的训练数据集由标签(“源”)和单个输入列(“标题”)组成。然而，标题不是数字，神经网络需要数字输入。因此，我们需要将文本输入列转换为数字。怎么会？</p><p id="590c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最简单的方法是对标题进行一次性编码。假设数据集中有72，000个唯一的标题，我们将得到72，000列。如果我们随后就此训练一个神经网络，这个神经网络基本上必须记住标题——没有进一步推广的可能。</p><p id="3e4f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了让网络通用化，我们需要将标题转换成数字，使得相似的标题以相似的数字结束。一种方法是找到标题中的单个单词，并将这些单词映射到唯一的数字。然后，有相同单词的标题在这部分序列中会有相似的数字。训练数据集中的唯一单词集被称为<em class="ko">词汇</em>。</p><p id="572d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设我们有四个标题:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="f659" class="mf ku iq mb b gy mg mh l mi mj">lines = ['Some title', <br/>         'A longer title', <br/>         'An even longer title', <br/>         'This is longer than doc length']</span></pre><p id="44c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为这些标题都有不同的长度，所以我会用一个虚拟单词来填充短标题，并截断很长的标题。这样，我就可以处理长度相同的标题了。</p><p id="ecfe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我可以使用下面的代码创建词汇表(这并不理想，因为词汇表处理器将所有内容都存储在内存中；对于更大的数据集和更复杂的预处理，比如合并停用词和不区分大小写，<a class="ae ks" href="https://github.com/tensorflow/transform" rel="noopener ugc nofollow" target="_blank"> tf.transform </a>是更好的解决方案——这是另一篇博文的主题):</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="180b" class="mf ku iq mb b gy mg mh l mi mj"><strong class="mb ir">import</strong> <strong class="mb ir">tensorflow</strong> <strong class="mb ir">as</strong> <strong class="mb ir">tf</strong><br/><strong class="mb ir">from</strong> <strong class="mb ir">tensorflow.contrib</strong> <strong class="mb ir">import</strong> lookup<br/><strong class="mb ir">from</strong> <strong class="mb ir">tensorflow.python.platform</strong> <strong class="mb ir">import</strong> gfile<br/><br/>MAX_DOCUMENT_LENGTH = 5  <br/>PADWORD = 'ZYXW'<br/><br/><em class="ko"># create vocabulary</em><br/>vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)<br/>vocab_processor.fit(lines)<br/><strong class="mb ir">with</strong> gfile.Open('vocab.tsv', 'wb') <strong class="mb ir">as</strong> f:<br/>    f.write("{}<strong class="mb ir">\n</strong>".format(PADWORD))<br/>    <strong class="mb ir">for</strong> word, index <strong class="mb ir">in</strong> vocab_processor.vocabulary_._mapping.iteritems():<br/>      f.write("{}<strong class="mb ir">\n</strong>".format(word))<br/>N_WORDS = len(vocab_processor.vocabulary_)</span></pre><p id="3f6b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的代码中，我会用一个填充词填充简短的标题，我希望它不会出现在实际的文本中。标题将被填充或截短至5个单词的长度。我传入训练数据集(上面示例中的“lines”)，然后写出结果词汇。词汇变成了:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="ba61" class="mf ku iq mb b gy mg mh l mi mj">ZYXW<br/>A<br/>even<br/>longer<br/>title<br/>This<br/>doc<br/>is<br/>Some<br/>An<br/>length<br/>than<br/>&lt;UNK&gt;</span></pre><p id="cdd3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，我添加了padword，词汇处理器在这组行中找到了所有独特的单词。最后，在评估/预测期间遇到的不在训练数据集中的单词将被替换为<unk>，因此这也是词汇表的一部分。</unk></p><p id="5fc9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">根据上面的词汇表，我们可以将任何标题转换成一组数字:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="5ad8" class="mf ku iq mb b gy mg mh l mi mj">table = lookup.index_table_from_file(<br/>  vocabulary_file='vocab.tsv', num_oov_buckets=1, vocab_size=None, default_value=-1)<br/>numbers = table.lookup(tf.constant(<strong class="mb ir">'Some title'</strong>.split()))<br/><strong class="mb ir">with</strong> tf.Session() <strong class="mb ir">as</strong> sess:<br/>  tf.tables_initializer().run()<br/>  <strong class="mb ir">print</strong> "{} --&gt; {}".format(lines[0], numbers.eval())</span></pre><p id="fe91" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的代码将查找单词“Some”和“title ”,并根据词汇返回索引[8，4]。当然，在实际的训练/预测图中，我们还需要确保填充/截断。让我们看看接下来该怎么做。</p><h1 id="8ecd" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">文字处理</h1><p id="1d57" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">首先，我们从行(每行是一个标题)开始，将标题拆分成单词:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="411b" class="mf ku iq mb b gy mg mh l mi mj"><em class="ko"># string operations</em><br/>titles = tf.constant(lines)<br/>words = tf.string_split(titles)</span></pre><p id="253b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这导致:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="b237" class="mf ku iq mb b gy mg mh l mi mj">titles= ['Some title' 'A longer title' 'An even longer title'<br/> 'This is longer than doc length']<br/>words= SparseTensorValue(indices=array([[0, 0],<br/>       [0, 1],<br/>       [1, 0],<br/>       [1, 1],<br/>       [1, 2],<br/>       [2, 0],<br/>       [2, 1],<br/>       [2, 2],<br/>       [2, 3],<br/>       [3, 0],<br/>       [3, 1],<br/>       [3, 2],<br/>       [3, 3],<br/>       [3, 4],<br/>       [3, 5]]), values=array(['Some', 'title', 'A', 'longer', 'title', 'An', 'even', 'longer',<br/>       'title', 'This', 'is', 'longer', 'than', 'doc', 'length'], dtype=object), dense_shape=array([4, 6]))</span></pre><p id="8892" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">TensorFlow的string_split()函数最终创建了一个SparseTensor。谈论一个过于有用的API。但是我不希望自动创建映射，所以我将把稀疏张量转换成密集张量，然后从我自己的词汇表中查找索引:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="6b21" class="mf ku iq mb b gy mg mh l mi mj"><em class="ko"># string operations</em><br/>titles = tf.constant(lines)<br/>words = tf.string_split(titles)<br/>densewords = tf.sparse_tensor_to_dense(words, default_value=PADWORD)<br/>numbers = table.lookup(densewords)</span></pre><p id="395a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，densewords和numbers与预期的一样(注意填充的PADWORD:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="6412" class="mf ku iq mb b gy mg mh l mi mj">dense= [['Some' 'title' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW']<br/> ['A' 'longer' 'title' 'ZYXW' 'ZYXW' 'ZYXW']<br/> ['An' 'even' 'longer' 'title' 'ZYXW' 'ZYXW']<br/> ['This' 'is' 'longer' 'than' 'doc' 'length']]<br/>numbers= [[ 8  4  0  0  0  0]<br/> [ 1  3  4  0  0  0]<br/> [ 9  2  3  4  0  0]<br/> [ 5  7  3 11  6 10]]</span></pre><p id="d577" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还要注意，数字矩阵具有数据集中最长标题的宽度。因为这个宽度会随着处理的每一批而变化，所以它并不理想。为了保持一致，让我们将其填充到MAX_DOCUMENT_LENGTH，然后将其截断:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="e8e8" class="mf ku iq mb b gy mg mh l mi mj">padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])<br/>padded = tf.pad(numbers, padding)<br/>sliced = tf.slice(padded, [0,0], [-1, MAX_DOCUMENT_LENGTH])</span></pre><p id="187c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这会创建一个batchsize x 5矩阵，其中较短的标题用零填充:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="5bf3" class="mf ku iq mb b gy mg mh l mi mj">padding= [[0 0]<br/> [0 5]] <br/>padded= [[ 8  4  0  0  0  0  0  0  0  0  0]<br/> [ 1  3  4  0  0  0  0  0  0  0  0]<br/> [ 9  2  3  4  0  0  0  0  0  0  0]<br/> [ 5  7  3 11  6 10  0  0  0  0  0]] <br/>sliced= [[ 8  4  0  0  0]<br/> [ 1  3  4  0  0]<br/> [ 9  2  3  4  0]<br/> [ 5  7  3 11  6]]</span></pre><p id="e412" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的例子中，我使用的MAX_DOCUMENT_LENGTH为5，这样我可以向您展示正在发生的事情。在真实数据集中，标题长于5个单词。所以，在我会用</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="fbf2" class="mf ku iq mb b gy mg mh l mi mj">MAX_DOCUMENT_LENGTH = 20</span></pre><p id="b97b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ko">切片</em>矩阵的形状将是batchsize x MAX_DOCUMENT_LENGTH，即batchsize x 20。</p><h1 id="2a51" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">把...嵌入</h1><p id="7bf5" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">既然我们的单词已经被数字取代，我们可以简单地进行一次性编码，但这将导致非常广泛的输入——在标题数据集中有数千个独特的单词。一个更好的方法是减少输入的维度——这是通过嵌入层来实现的(参见<a class="ae ks" href="https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/textclassification/txtcls1/trainer/model.py" rel="noopener ugc nofollow" target="_blank">完整代码</a>):</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="4646" class="mf ku iq mb b gy mg mh l mi mj">EMBEDDING_SIZE = 10<br/>embeds = tf.contrib.layers.embed_sequence(sliced, <br/>                 vocab_size=N_WORDS, embed_dim=EMBEDDING_SIZE)</span></pre><p id="b30e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦有了嵌入，我们现在就有了标题中每个单词的表示。嵌入的结果是一个batch SIZE x MAX_DOCUMENT_LENGTH x EMBEDDING_SIZE张量，因为一个标题由MAX _ DOCUMENT _ LENGTH个单词组成，每个单词现在用EMBEDDING _ SIZE个数字表示。(养成在TensorFlow代码的每一步计算张量形状的习惯——这将帮助你理解代码在做什么，维度意味着什么)。</p><p id="3893" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们愿意，我们可以简单地将嵌入的单词连接到一个深度神经网络中，训练它，然后开始我们的快乐之路。但是仅仅使用单词本身并没有利用单词序列具有特定含义的事实。毕竟，“最高法院”可以出现在许多场合，但是“最高法院”有更具体的含义。我们如何学习单词序列？</p><h1 id="916d" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">盘旋</h1><p id="3be7" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">学习序列的一种方法是不仅嵌入独特的单词，还嵌入二元模型(单词对)、三元模型(单词三元组)等。然而，对于相对较小的数据集，这开始变得类似于对数据集中的每个唯一单词进行一次性编码。</p><p id="8bb9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更好的方法是增加一个卷积层。卷积只是一种将移动窗口应用于输入数据并让神经网络学习应用于相邻单词的权重的方法。虽然在处理图像数据时更常见，但这是帮助任何神经网络了解附近输入之间的相关性的便捷方式:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="20ba" class="mf ku iq mb b gy mg mh l mi mj">WINDOW_SIZE = EMBEDDING_SIZE<br/>STRIDE = int(WINDOW_SIZE/2)<br/>conv = tf.contrib.layers.conv2d(embeds, 1, WINDOW_SIZE, <br/>                stride=STRIDE, padding='SAME') # (?, 4, 1)    <br/>conv = tf.nn.relu(conv) # (?, 4, 1)    <br/>words = tf.squeeze(conv, [2]) # (?, 4)</span></pre><p id="6b33" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回想一下，嵌入的结果是一个20 x 10的张量(我们暂且忽略batchsize这里的所有操作都是一次对一个标题进行的)。我现在将10x10窗口中的加权平均值应用于标题的嵌入表示，将窗口移动5个单词(步幅=5)，然后再次应用它。所以，我会有4个这样的卷积结果。然后，我对卷积结果应用非线性变换(relu)。</p><p id="5530" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我现在有四个结果。我可以简单地将它们通过密集层连接到输出层:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="6064" class="mf ku iq mb b gy mg mh l mi mj">n_classes = len(TARGETS)     <br/>logits = tf.contrib.layers.fully_connected(words, n_classes, <br/>                                    activation_fn=None)</span></pre><p id="b9da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你习惯于图像模型，你可能会感到惊讶，我用了卷积层，但没有最大池层。使用maxpool图层的原因是为了增加网络的空间不变性-直观地说，您希望找到一只猫，而不管这只猫在图像中的位置。然而，标题中的空间位置非常重要。很有可能《纽约时报》文章的标题与GitHub文章的标题有所不同。因此，我没有在这个任务中使用maxpool层。</p><p id="f895" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">给定logit，我们可以通过执行TARGETS[max(logit)]来找出源。在TensorFlow中，这是使用tf.gather完成的:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="f6b2" class="mf ku iq mb b gy mg mh l mi mj">predictions_dict = {      <br/>'source': tf.gather(TARGETS, tf.argmax(logits, 1)),      <br/>'class': tf.argmax(logits, 1),      <br/>'prob': tf.nn.softmax(logits)    <br/>}</span></pre><p id="95dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了完整起见，我还发送了实际的类索引和每个类的概率。</p><h1 id="a842" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">培训和部署</h1><p id="0643" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">代码都写好了(见<a class="ae ks" href="https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/textclassification" rel="noopener ugc nofollow" target="_blank">完整代码这里</a>)，我就可以在Cloud ML引擎上训练它了:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="c4b9" class="mf ku iq mb b gy mg mh l mi mj">OUTDIR=gs://${BUCKET}/txtcls1/trained_model<br/>JOBNAME=txtcls_$(date -u +%y%m%d_%H%M%S)<br/>echo $OUTDIR $REGION $JOBNAME<br/>gsutil -m rm -rf $OUTDIR<br/>gsutil cp txtcls1/trainer/*.py $OUTDIR<br/>gcloud ml-engine jobs submit training $JOBNAME \<br/>   --region=$REGION \<br/>   --module-name=trainer.task \<br/>   --package-path=$(pwd)/txtcls1/trainer \<br/>   --job-dir=$OUTDIR \<br/>   --staging-bucket=gs://$BUCKET \<br/>   --scale-tier=BASIC --runtime-version=1.2 \<br/>   -- \<br/>   --bucket=${BUCKET} \<br/>   --output_dir=${OUTDIR} \<br/>   --train_steps=36000</span></pre><p id="9eb5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据集非常小，所以训练不到五分钟就结束了，我在评估数据集上获得了73%的准确率。</p><p id="f6a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我可以将该模型作为微服务部署到云ML引擎:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="31f9" class="mf ku iq mb b gy mg mh l mi mj">MODEL_NAME="txtcls"<br/>MODEL_VERSION="v1"<br/>MODEL_LOCATION=$(gsutil ls \<br/>     gs://${BUCKET}/txtcls1/trained_model/export/Servo/ | tail -1)<br/>gcloud ml-engine models create ${MODEL_NAME} --regions $REGION<br/>gcloud ml-engine versions create ${MODEL_VERSION} --model \<br/>     ${MODEL_NAME} --origin ${MODEL_LOCATION}</span></pre><h1 id="80e9" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">预言；预测；预告</h1><p id="a290" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">为了让模型进行预测，我们可以向它发送一个JSON请求:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="ea0b" class="mf ku iq mb b gy mg mh l mi mj"><strong class="mb ir">from</strong> <strong class="mb ir">googleapiclient</strong> <strong class="mb ir">import</strong> discovery<br/><strong class="mb ir">from</strong> <strong class="mb ir">oauth2client.client</strong> <strong class="mb ir">import</strong> GoogleCredentials<br/><strong class="mb ir">import</strong> <strong class="mb ir">json</strong><br/><br/>credentials = GoogleCredentials.get_application_default()<br/>api = discovery.build('ml', 'v1beta1', credentials=credentials,<br/>            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1beta1_discovery.json')<br/><br/>request_data = {'instances':<br/>  [<br/>      {<br/>        'title': 'Supreme Court to Hear Major Case on Partisan Districts'<br/>      },<br/>      {<br/>        'title': 'Furan -- build and push Docker images from GitHub to target'<br/>      },<br/>      {<br/>        'title': 'Time Warner will spend $100M on Snapchat original shows and ads'<br/>      },<br/>  ]<br/>}<br/><br/>parent = 'projects/<strong class="mb ir">%s</strong>/models/<strong class="mb ir">%s</strong>/versions/<strong class="mb ir">%s</strong>' % (PROJECT, 'txtcls', 'v1')<br/>response = api.projects().predict(body=request_data, name=parent).execute()<br/><strong class="mb ir">print</strong> "response={0}".format(response)</span></pre><p id="7d45" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这会产生一个JSON响应:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="b24f" class="mf ku iq mb b gy mg mh l mi mj">response={u'predictions': [{u'source': u'nytimes', u'prob': [<strong class="mb ir">0.777</strong>5614857673645, 5.86951500736177e-05, 0.22237983345985413], u'class': 0}, {u'source': u'github', u'prob': [0.1087314561009407, <strong class="mb ir">0.890</strong>9648656845093, 0.0003036781563423574], u'class': 1}, {u'source': u'techcrunch', u'prob': [0.0021869686897844076, 1.563105769264439e-07, <strong class="mb ir">0.997</strong>8128671646118], u'class': 2}]}</span></pre><p id="8fef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">经过训练的模型预测，最高法院的文章有78%的可能性来自《纽约时报》。根据该服务，Docker的文章89%可能来自GitHub，而时代华纳的文章100%可能来自TechCrunch。那是3/3。</p><p id="dc24" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">资源:</strong>所有代码都在GitHub这里:<a class="ae ks" href="https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/textclassification" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/Google cloud platform/training-data-analyst/tree/master/blogs/text classification</a></p></div></div>    
</body>
</html>