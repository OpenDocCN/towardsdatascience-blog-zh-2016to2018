<html>
<head>
<title>Activation Functions in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的激活函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6?source=collection_archive---------0-----------------------#2017-09-06">https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6?source=collection_archive---------0-----------------------#2017-09-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d72c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Sigmoid，tanh，Softmax，ReLU，Leaky ReLU解释！！！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d0a587fad16490a877614dac21d4e0dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*GIPiAdQyOa8wUOkHaL-MJg.gif"/></div></div></figure><h2 id="9ec0" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><strong class="ak">什么是激活功能？</strong></h2><blockquote class="ln lo lp"><p id="6a72" class="lq lr ls lt b lu lv jr lw lx ly ju lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated">它只是一个东西函数，你用它来得到node的输出。它也被称为<strong class="lt ir">传递函数</strong>。</p></blockquote></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h2 id="0a1c" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><strong class="ak">为什么我们在神经网络中使用激活函数？</strong></h2><blockquote class="ln lo lp"><p id="e6ea" class="lq lr ls lt b lu lv jr lw lx ly ju lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated">它用于确定神经网络的输出，如是或否。它将结果值映射到0到1或-1到1等之间。(取决于功能)。</p></blockquote><p id="92dd" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">激活功能基本上可以分为两种类型</p><ol class=""><li id="8e97" class="mu mv iq lt b lu lv lx ly la mw le mx li my mm mz na nb nc bi translated">线性激活函数</li><li id="b095" class="mu mv iq lt b lu nd lx ne la nf le ng li nh mm mz na nb nc bi translated">非线性激活函数</li></ol><blockquote class="ln lo lp"><p id="9db5" class="lq lr ls lt b lu lv jr lw lx ly ju lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated">仅供参考:备忘单如下。</p></blockquote></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h2 id="b648" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><strong class="ak">线性或身份激活功能</strong></h2><p id="2cf7" class="pw-post-body-paragraph lq lr iq lt b lu ni jr lw lx nj ju lz la nk mc md le nl mg mh li nm mk ml mm ij bi translated">如你所见，这个函数是一条直线或线性的。因此，函数的输出将不会被限制在任何范围之间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/31ae84cbc89abf6864fcfe6ad633c11e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tldIgyDQWqm-sMwP7m3Bww.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Fig: Linear Activation Function</strong></figcaption></figure><p id="63b9" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated"><strong class="lt ir">方程式:</strong> f(x) = x</p><p id="93c5" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated"><strong class="lt ir">范围:</strong>(-无穷大到无穷大)</p><p id="cac1" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">它对输入神经网络的通常数据的复杂性或各种参数没有帮助。</p><h2 id="57a2" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><strong class="ak">非线性激活功能</strong></h2><p id="7a6a" class="pw-post-body-paragraph lq lr iq lt b lu ni jr lw lx nj ju lz la nk mc md le nl mg mh li nm mk ml mm ij bi translated">非线性激活函数是最常用的激活函数。非线性有助于使图表看起来像这样</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/eb0b280d024868ac90dccbbb662ff48b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*cxNqE_CMez7vUIkcLUH8PA.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Fig: Non-linear Activation Function</strong></figcaption></figure><p id="9b7c" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">这使得模型很容易概括或适应各种数据，并区分输出。</p><p id="9bce" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">需要理解非线性函数的主要术语有:</p><blockquote class="ln lo lp"><p id="9017" class="lq lr ls lt b lu lv jr lw lx ly ju lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated"><strong class="lt ir">导数或微分:</strong>y轴的变化与x轴的变化。它也被称为斜坡。</p><p id="a354" class="lq lr ls lt b lu lv jr lw lx ly ju lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated"><strong class="lt ir">单调函数:</strong>要么完全非增，要么完全非减的函数。</p></blockquote><p id="fe6e" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">非线性激活函数主要根据它们的<strong class="lt ir">范围或曲线</strong>来划分</p><h2 id="9dcb" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><strong class="ak"> 1。乙状结肠或逻辑激活功能</strong></h2><p id="3bf7" class="pw-post-body-paragraph lq lr iq lt b lu ni jr lw lx nj ju lz la nk mc md le nl mg mh li nm mk ml mm ij bi translated">Sigmoid函数曲线看起来像S形。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/4e2ca71344b0cfda33b168c32595462e.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*Xu7B5y9gp0iL5ooBj7LtWw.png"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Fig: Sigmoid Function</strong></figcaption></figure><p id="5bc7" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">我们之所以用sigmoid函数，主要是因为它存在于<strong class="lt ir"> (0到1)之间。</strong>因此，它特别适用于我们必须<strong class="lt ir">预测概率</strong>作为输出的模型。由于任何事情的概率只存在于<strong class="lt ir"> 0和1之间，</strong> sigmoid是正确的选择。</p><p id="f1a4" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">该函数是<strong class="lt ir">可微的</strong>。这意味着，我们可以在任意两点找到s形曲线的斜率。</p><p id="a515" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">函数是<strong class="lt ir">单调的</strong>，但函数的导数不是。</p><p id="6ccd" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">逻辑sigmoid函数会导致神经网络在训练时停滞不前。</p><p id="5190" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated"><strong class="lt ir"> softmax函数</strong>是一个更通用的逻辑激活函数，用于多类分类。</p><h2 id="4111" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><strong class="ak"> 2。双曲正切或双曲正切激活函数</strong></h2><p id="264b" class="pw-post-body-paragraph lq lr iq lt b lu ni jr lw lx nj ju lz la nk mc md le nl mg mh li nm mk ml mm ij bi translated">tanh也像逻辑s形但更好。双曲正切函数的范围是从(-1到1)。tanh也是s形的(s形)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/b2276edae65288c1d9598c38e5b40f53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*f9erByySVjTjohfFdNkJYQ.jpeg"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Fig: tanh v/s Logistic Sigmoid</strong></figcaption></figure><p id="1463" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">优点是负输入将被映射为强负输入，而零输入将被映射到双曲正切图中的零附近。</p><p id="cb93" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">函数是<strong class="lt ir">可微的</strong>。</p><p id="fef3" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">函数是<strong class="lt ir">单调的</strong>，而它的<strong class="lt ir">导数不是单调的</strong>。</p><p id="149d" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">双曲正切函数主要用于两类之间的分类。</p><blockquote class="ln lo lp"><p id="27b8" class="lq lr ls lt b lu lv jr lw lx ly ju lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated">双曲正切和逻辑sigmoid激活函数都用于前馈网络。</p></blockquote><h2 id="d8df" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><strong class="ak"> 3。ReLU(整流线性单元)激活功能</strong></h2><p id="1a02" class="pw-post-body-paragraph lq lr iq lt b lu ni jr lw lx nj ju lz la nk mc md le nl mg mh li nm mk ml mm ij bi translated">ReLU是目前世界上使用最多的激活函数。因为，它被用于几乎所有的卷积神经网络或深度学习。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/95fe53ba4d0b1b9c2242aaadef9ca98f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XxxiA0jJvPrHEJHD4z893g.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Fig: ReLU v/s Logistic Sigmoid</strong></figcaption></figure><p id="cec1" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">正如你所看到的，ReLU是半整流(从底部)。当z小于零时f(z)为零，当z大于或等于零时f(z)等于z。</p><p id="7dad" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated"><strong class="lt ir">范围:</strong>【0到无穷大】</p><p id="4891" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">函数及其导数<strong class="lt ir">都是</strong> <strong class="lt ir">单调</strong>。</p><p id="eb83" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">但问题是，所有的负值立即变为零，这降低了模型根据数据进行适当拟合或训练的能力。这意味着给予ReLU激活函数的任何负输入都会在图形中立即将值变成零，这反过来会通过不适当地映射负值来影响结果图形。</p><h2 id="f7ef" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><strong class="ak"> 4。泄漏的ReLU </strong></h2><p id="b572" class="pw-post-body-paragraph lq lr iq lt b lu ni jr lw lx nj ju lz la nk mc md le nl mg mh li nm mk ml mm ij bi translated">这是解决濒临死亡的ReLU问题的一次尝试</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/d2f5fd0e34210bda9a033bae98c260ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A_Bzn0CjUgOXtPCJKnKLqA.jpeg"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Fig : ReLU v/s Leaky ReLU</strong></figcaption></figure><p id="50f7" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">你能看到漏洞吗？😆</p><p id="5131" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">泄漏有助于增加ReLU功能的范围。通常情况下，<strong class="lt ir"> a </strong>的值在0.01左右。</p><p id="7cb0" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">当<strong class="lt ir"> a不为0.01 </strong>时，称为<strong class="lt ir">随机化ReLU </strong>。</p><p id="b0f8" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">因此，泄漏ReLU的<strong class="lt ir">范围</strong>为(-无穷大到无穷大)。</p><p id="a283" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">泄漏和随机化ReLU函数本质上都是单调的。同样，它们的导数在性质上也是单调的。</p><h2 id="a650" class="kr ks iq bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">为什么要用导数/微分？</h2><blockquote class="ny"><p id="494d" class="nz oa iq bd ob oc od oe of og oh mm dk translated">更新曲线时，根据坡度在<strong class="ak">和</strong>和<strong class="ak">中知道向哪个方向</strong>改变或更新曲线多少。这就是为什么我们在机器学习和深度学习的几乎每个部分都使用微分。</p></blockquote><figure class="oj ok ol om on kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/649cca7c60287b418c7dee8b340e9baa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p_hyqAtyI8pbt2kEl6siOQ.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Fig: Activation Function Cheetsheet</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/1ded89297c9a594f8f03881957377e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n1HFBpwv21FCAzGjmWt1sg.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk"><strong class="bd ns">Fig: Derivative of Activation Functions</strong></figcaption></figure></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/b42e0e7fafdf3abd4e6cf4edb511b8c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/1*B_EOC2l6EIKmgQRKJ4g_lg.gif"/></div></figure><p id="8b99" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">乐意帮忙。支持我。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://www.buymeacoffee.com/sagarsharma4244"><div class="gh gi oq"><img src="../Images/bb2fa791eaa3e12a545ee28a308080a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*rQv8JgstmK0juxP-Kb4IGg.jpeg"/></div></a><figcaption class="no np gj gh gi nq nr bd b be z dk">If you liked it</figcaption></figure><p id="1884" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">所以，跟着我上<a class="ae or" href="https://medium.com/@sagarsharma4244" rel="noopener">中</a>、<a class="ae or" href="https://www.linkedin.com/in/sagar-sharma-232a06148/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>看看类似的帖子。</p><p id="1020" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated">任何评论或者如果你有任何问题，<strong class="lt ir">写在评论里。</strong></p><p id="518c" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz la mb mc md le mf mg mh li mj mk ml mm ij bi translated"><strong class="lt ir">鼓掌吧！分享一下！跟我来。</strong></p></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h1 id="93df" class="os ks iq bd kt ot ou ov kw ow ox oy kz jw oz jx ld jz pa ka lh kc pb kd ll pc bi translated">你会喜欢的以前的故事:</h1><div class="pd pe gp gr pf pg"><a href="https://hackernoon.com/what-the-hell-is-tensor-in-tensorflow-e40dbf0253ee" rel="noopener  ugc nofollow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd ir gy z fp pl fr fs pm fu fw ip bi translated">“TensorFlow”里的“Tensor”是什么鬼？</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">我不知道…</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">hackernoon.com</p></div></div><div class="pp l"><div class="pq l pr ps pt pp pu kp pg"/></div></div></a></div><div class="pd pe gp gr pf pg"><a rel="noopener follow" target="_blank" href="/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd ir gy z fp pl fr fs pm fu fw ip bi translated">纪元与批量大小与迭代次数</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">了解您的代码…</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="pv l pr ps pt pp pu kp pg"/></div></div></a></div><div class="pd pe gp gr pf pg"><a rel="noopener follow" target="_blank" href="/monte-carlo-tree-search-158a917a8baa"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd ir gy z fp pl fr fs pm fu fw ip bi translated">蒙特卡罗树搜索</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">每个数据科学爱好者的MCTS</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="pw l pr ps pt pp pu kp pg"/></div></div></a></div><div class="pd pe gp gr pf pg"><a rel="noopener follow" target="_blank" href="/policy-networks-vs-value-networks-in-reinforcement-learning-da2776056ad2"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd ir gy z fp pl fr fs pm fu fw ip bi translated">强化学习中的政策网络与价值网络</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">在强化学习中，代理在他们的环境中采取随机决策，并学习选择正确的决策…</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="px l pr ps pt pp pu kp pg"/></div></div></a></div><div class="pd pe gp gr pf pg"><a rel="noopener follow" target="_blank" href="/tensorflow-image-recognition-python-api-e35f7d412a70"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd ir gy z fp pl fr fs pm fu fw ip bi translated">TensorFlow图像识别Python API教程</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">在带有Inception-v3的CPU上(以秒为单位)</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="py l pr ps pt pp pu kp pg"/></div></div></a></div><div class="pd pe gp gr pf pg"><a href="https://medium.com/@sagarsharma4244/how-to-send-emails-using-python-4293dacc57d9" rel="noopener follow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd ir gy z fp pl fr fs pm fu fw ip bi translated">如何使用Python发送电子邮件</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">使用Flask设计专业邮件！</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">medium.com</p></div></div><div class="pp l"><div class="pz l pr ps pt pp pu kp pg"/></div></div></a></div></div></div>    
</body>
</html>