<html>
<head>
<title>Comparing the performance of non-supervised vs supervised learning methods for NLP text classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">非监督与监督学习方法在自然语言处理文本分类中的性能比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comparing-the-performance-of-non-supervised-vs-supervised-learning-methods-for-nlp-text-805a9c019b82?source=collection_archive---------11-----------------------#2018-10-18">https://towardsdatascience.com/comparing-the-performance-of-non-supervised-vs-supervised-learning-methods-for-nlp-text-805a9c019b82?source=collection_archive---------11-----------------------#2018-10-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="7fff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将分类性能与 scikit-learn 文档中现有的 NLP 指标基线进行比较。Gal Arav M.Sc</p><h1 id="f65d" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">笔记</h1><p id="eddb" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">1.本文中使用的术语的定义包含在末尾。</p><p id="63c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.附带的 python 笔记本托管在 Google Colab:https://Colab . research . Google . com/github/gal-a/blog/blob/master/docs/notebooks/NLP/NLP _ TF-IDF _ clustering . ipynb</p><p id="e1a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.这是我在<strong class="jp ir">关于数据科学</strong>的第一篇文章，我期待任何反馈或问题，我的博客在:<a class="ae lo" href="http://gal-a.com/" rel="noopener ugc nofollow" target="_blank">http://gal-a.com/</a></p><h1 id="677f" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">输入数据</h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/f76af0bbe6cdf0a6dab5ef13c01609e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*TAQkjpme441WH6xwSQlwqg.jpeg"/></div></figure><p id="604b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输入数据是从 scikit learn library 中的 4 个互联网新闻组中提取的。下面显示的是这些新闻组中的<strong class="jp ir">热门话题</strong>的单词云，强调了每个新闻组中最受欢迎的话题。</p><h1 id="5004" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">无神论新闻组词云:</h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi lx"><img src="../Images/26c7764731c761ddc4f75778f5b6f3e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*PDyVIaT6LVm4zXkVOVJ7Tw.png"/></div></div></figure><h1 id="d0f7" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">基督教新闻组单词云:</h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/9a80e692fe5e23b2b38b12a33f6f49aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*B1jzgOKGj82myCg9pMfciw.png"/></div></figure><h1 id="66b1" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">医学新闻组单词云:</h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi md"><img src="../Images/fa6a1b4c591a0da3c3c03ea0642972d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*84TAPNKR1_6fg7O81PkRpQ.png"/></div></figure><h1 id="475d" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">图形新闻组单词云:</h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi md"><img src="../Images/d02ffb5112a0919f0cdc13dadeb3c40d.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*FG8dHnHFBLiiJVkxa6J8Jw.png"/></div></figure><h1 id="bf73" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">动机</h1><p id="a65e" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir">我们的目标是准确地对文本文档进行分类，并将无监督学习方法的性能指标与现有的有监督学习方法的性能指标进行比较。</strong></p><p id="ec53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将比较以下非监督方法的性能指标:K-means、NMF 和 LDA 方法与这些监督方法:多项式朴素贝叶斯和线性支持向量机(SVM)方法。</p><p id="b8e6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">scikit learn 的文档中详细介绍了<strong class="jp ir">监督方法</strong>的性能指标，我们将它们用作基准参考。</p><p id="7b93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了确保我们能够相互比较，<strong class="jp ir">我们将在这里分析的完全相同的测试数据集</strong>上评估性能指标:<br/><a class="ae lo" href="http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/tutorial/text _ analytics/working _ with _ text _ data . html</a></p><p id="54d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们知道有监督的方法会比无监督的方法更好，为什么还要为无监督的方法烦恼呢？这是因为在许多情况下，我们面临着没有完全标记的数据集的情况。标注大型数据集既昂贵又耗时，因此，在对文本标注项目投入资源之前，通过找到相似文档的聚类来获得我们研究的初始角度是有用的。例如，如果我们对文本数据的性质只有一个模糊的概念，通过非监督学习方法找到的“有趣”的类簇可以用来帮助我们决定在随后的监督学习项目中关注哪些类标签。半监督学习方法，即使用一个大的未标记数据集来扩充一个小的标记数据集，可以显著提高分类精度。</p><h1 id="e871" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">预处理</h1><p id="ede0" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在我们可以应用机器学习方法之前，我们需要“清理”文本，具体来说，我们的目标是:<strong class="jp ir">删除标点符号</strong>，<strong class="jp ir">删除琐碎的“停用词”</strong>，应用一些基本的语言学方法，如<strong class="jp ir">词干分析器</strong>或<strong class="jp ir">词法分析器</strong>算法，并限制<strong class="jp ir">词性(POS) </strong>。在这个简短的研究中，我们尝试只保留文本中的名词。</p><p id="960c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">斯特梅尔</strong>算法的工作原理是切断单词的结尾或开头，同时考虑到一系列常见的前缀和后缀，这些前缀和后缀可以在一个屈折的单词中找到。</p><p id="5016" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Lemmatizer </strong>算法更复杂，通过使用详细的字典来考虑单词的形态分析，算法可以浏览这些字典以将形式链接回其 lemma。</p><p id="857e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是从原句中提取的 3 个名词示例列表。这里有些问题需要更复杂的预处理，但它们足够精确，我们可以继续处理(如前所示，单词 clouds 很好地反映了 4 个新闻组中的每一个)。原文句子:</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="2e80" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，nltk 的缺省 POS 实现远非完美，还有各种其他的 POS 实现可以尝试(例如 HunPos、Stanford POS、Senna)。在这里我们看到，对于上面的句子，单词“bite”被错误地归类为名词，尽管它实际上是“bite”(一个动词)的过去分词。</p><p id="da49" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">机器学习模型需要输入数据的数字表示，因此我们需要在应用学习模型之前将文本转换为数字格式。用于将文本文档转换为数字表示的最流行的方法被称为<strong class="jp ir"> TF-IDF </strong>、<strong class="jp ir">‘词频-逆文档频率’</strong>，尽管它的名字很长，但它非常容易理解。<strong class="jp ir">简而言之，TF-IDF 是一个数字表，表示一个术语在一个特定文档中出现的频率，通过它在语料库中所有文档中出现的频率来标准化。</strong></p><p id="676c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意,“词频”概念通常被称为“词汇包”。当处理相关文本文档的大型语料库时，通常我们想要做的第一件事是创建这些文档中最流行的单词或短语的摘要。然后，可以通过术语频率或加权术语频率(当它们出现在 TF-IDF 矩阵中时，通过文档计数标准化)从术语的排序列表中创建“热门话题”</p><p id="5d0d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦预处理完成，绘制标记化文档长度的直方图是很有帮助的。</p><h1 id="af2f" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">标记化文档长度的直方图:</h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/7c79e32f74b0371dea0bd638a800f72e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*R2m987oE5CbHhZMphQFaLA.png"/></div></figure><p id="4de2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，最后一个箱代表接近 40 个计数的尖峰(在文档长度= 1200 处)，因为该值用于最大文档长度。</p><p id="8996" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是可以在附带的笔记本中运行的预处理步骤的摘要(使用 sklearn 库):</p><p id="ad8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(I)从文档构建计数矢量器并拟合文档</p><p id="1a8c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(ii)从文档中建立 TF(术语频率),这是词袋的稀疏版本</p><p id="7359" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">㈢分两步建立词汇袋:适应、转换</p><p id="914b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">㈣获得特征名称，并建立词汇袋的数据框架版本</p><p id="060b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(v)使用 TfidfTransformer 将单词包转换成 TF-IDF 矩阵(术语频率-逆文档频率)</p><p id="25cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(vi)找出最流行的词和最高权重</p><p id="727c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(vii)建立单词权重列表并对其进行排序</p><p id="4e4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(viii)计算所有文档与其自身的余弦相似度</p><p id="15a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(ix)计算文档的距离矩阵</p><h1 id="3a79" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">基于 K-均值聚类的 PCA 降维方法</h1><p id="f63d" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">K-means 是最著名的聚类算法。这是一种无监督的学习方法，因为学习不是基于标记的类。</p><p id="26cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">给定一组观察值(x1，x2，…，xn)，其中每个观察值是一个 d 维实向量，k-means 聚类旨在将 n 个观察值划分为 k (≤ n)个集合 S = {S1，S2，…，Sk}，以便最小化组内平方和(WCSS)(即最小化方差)。</p><p id="37a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它可以在语料库中的所有文档中生成最流行主题的良好分割。</p><p id="0e15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然 K-means 可以直接处理任意数量的特征，但它通常在负责减少特征数量的单独预处理步骤之后使用，例如流行的 PCA 方法(主成分分析)。sklearn.decomposition 中有一个很好的 PCA 实现。</p><p id="2324" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">用线性代数术语</strong>来说，PCA 可以通过数据协方差(或相关性)矩阵的特征值分解来完成。得到的特征向量对应于数据分散的不同方向。特征值代表这些特征向量的相对重要性。PCA 允许我们根据相应特征值的有序排序，丢弃相对不重要的特征向量。</p><p id="812b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">在统计学术语</strong>中，主成分解释的方差分数是该主成分的方差与总方差(所有单个主成分的方差之和)之比。为了估计需要多少分量来描述数据，我们可以检查作为分量数量(按特征值大小排序)的函数的累积解释方差比，并仅选择与解释方差的特定百分比(例如 90%)相对应的顶部分量。</p><p id="d98f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们的例子中，由于我们想要创建一个 2 维的可视化(见下文)，我们丢弃了除了对应于两个最大特征值的两个最重要的特征向量之外的所有特征向量，这两个剩余的特征作为 K-means 算法的输入——注意，仅基于两个特征的分析通常不包含 90%的解释方差！</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="me mf l"/></div></figure><h1 id="dd0c" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">K 均值模型与实际类别的 PCA 特征:</h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi gj"><img src="../Images/65c59b64bd4785b9cc1d9de836d2c85d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D92fRUsF1b6RSGLYaYR8KA.png"/></div></div></figure><ul class=""><li id="5b57" class="mh mi iq jp b jq jr ju jv jy mj kc mk kg ml kk mm mn mo mp bi translated">K-means 导致所有数据点的清晰分割(左侧)。</li><li id="9f2e" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">当然，K-Means 模型导致了一些分类错误，如右图所示(对于具有实际类别标签的相同数据点)，尽管我们仍然可以非常清楚地看到 4 个聚类。</li><li id="fe72" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">请注意，一般来说，由于我们通常不知道数据中的聚类数，我们需要使用一种技术来估计这一点，例如流行的<strong class="jp ir">肘方法</strong>:<br/><a class="ae lo" href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Determining _ the _ number _ of _ clusters _ in _ a _ data _ set</a></li><li id="6247" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">中心的维数是:(n 个簇，n 个特征)。</li><li id="d516" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">PCA 作为 K-均值之前的初始步骤运行，这产生了上面的二维表示(n_features = 2)。</li></ul><h1 id="5f24" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">NMF 和 LDA 聚类</h1><p id="5059" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">NMF(非负矩阵分解)和 LDA(潜在狄利克雷分配)聚类都使用“单词包”矩阵作为输入，即与每个文档相关联的术语频率。然后，两种算法将原始单词包矩阵分成两个独立的单词包:</p><ol class=""><li id="522b" class="mh mi iq jp b jq jr ju jv jy mj kc mk kg ml kk mv mn mo mp bi translated">文档到主题矩阵</li><li id="3956" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mv mn mo mp bi translated">主题矩阵的术语</li></ol><p id="28c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用最小误差方法，这两个导出矩阵的乘积尽可能接近等于原始项频率。</p><p id="299e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然 K-means 可以提供良好的整体分割，但使用替代聚类方法(如 NMF 和 LDA)的主要优势在于，这些方法提供了对每个主题聚类中排名靠前的文档的额外洞察。注意，所有这三种算法都要求用户指定主题的数量作为输入参数。</p><p id="cec2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，NMF 需要 TF-IDF 矩阵作为输入，而 LDA 只需要单词包(TF 表)作为输入。如何使用这些算法的例子可以在附带的 python 笔记本中找到。</p><h1 id="88a6" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">结果</h1><p id="a2d8" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">首先，为无监督学习方法计算<strong class="jp ir">调整后的 Rand 指数</strong>和<strong class="jp ir">调整后的互信息得分</strong>(参见本文末尾的定义)。这些是测量两个赋值的相似性的函数，忽略置换和机会归一化:</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="5e5e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基于调整后的 Rand 指数和调整后的互信息得分，NMF 和 LDA 方法都优于 K-means。</p><p id="a94a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这项研究中，LDA 算法取得了最好的结果(下面显示的 sklearn.metrics 表的列定义包含在“文本处理定义”部分的末尾)。</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="c5fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与监督方法的最佳结果(线性 SVM 获得的)相比:</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="f3d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑到我们使用的是无监督学习方法，这还不错！</p><h1 id="7bf1" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">摘要</h1><ul class=""><li id="65f2" class="mh mi iq jp b jq lj ju lk jy mw kc mx kg my kk mm mn mo mp bi translated"><strong class="jp ir">在没有“偷看”类别标签的情况下，我们能够使用 LDA 进行无监督学习，在平均精确度、召回率和 f1 分数方面获得略高于 0.8 的分数。相比之下，对于在实际类别标签上训练的监督学习方法，使用线性支持向量机(SVM)分类器，得分仅超过 0.9。</strong></li><li id="2322" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">总的来说，所有的算法在挑选“comp.graphics”新闻组的能力方面都比较好(例如，这个新闻组与其他新闻组最少混淆)。</li><li id="e1ca" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">正如我们所料，最大的困惑发生在类似的“另类无神论”和“社会宗教基督教”新闻组之间。</li></ul><p id="5198" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">附带的 python 笔记本使用 python 的 sklearn 库演示了 NLP(自然语言处理)环境中的以下技术:</p><ul class=""><li id="f2f3" class="mh mi iq jp b jq jr ju jv jy mj kc mk kg ml kk mm mn mo mp bi translated">使用 TF-IDF(术语频率-逆文档频率)将文本矢量化为数字矩阵</li><li id="ec6a" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">利用主成分分析进行维数约简</li><li id="d118" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">无监督分类:基于 PCA(TF-IDF 的简化版本)计算 K 均值聚类</li><li id="caa5" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">非监督分类:基于 TF-IDF 计算 NMF</li><li id="ccd4" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">无监督分类:基于 TF 计算 LDA(潜在导数分析)</li></ul><p id="927c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一步将是对这些算法的参数化进行网格搜索(这可能会导致更好的分类)，并在更大更大胆的数据集上释放它们。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="1f63" class="kl km iq bd kn ko ng kq kr ks nh ku kv kw ni ky kz la nj lc ld le nk lg lh li bi translated">文本处理定义(在本文中使用)</h1><h1 id="abdc" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">常规:</h1><p id="b5e7" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">文集:某一特定主题的书面文本的集合。</p><p id="5702" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">文档/样本/观察</strong>:语料库中的单词序列。</p><p id="3256" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">术语/单词/短语/特征/ n 元语法</strong>:来自给定文本(或语音)样本的 n 个项目的连续序列。n 元语法可以有不同的长度，例如一元语法、二元语法、三元语法。</p><p id="1988" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">术语簇</strong>:话题。相关术语的分类。</p><p id="b2d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">文档簇</strong>:文档中具有共同主题的主题混合。</p><h1 id="96cd" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">预处理:</h1><p id="8372" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir"> TF-IDF(术语频率-逆文档频率)</strong>:术语相对于整个语料库对文档的重要性的统计度量。实际上，这种方法通过一个术语在语料库中出现的频率来标准化它在文档中出现的频率。</p><p id="b07d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> TF-IDF 样本</strong>:语料库中的单个文档，TF-IDF 矩阵中的行。</p><p id="59d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> TF-IDF 特征</strong>:这些都是语料库中包含的唯一 n 元文法，即 TF-IDF 矩阵中的列。</p><p id="1d4e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> PCA </strong>(主成分分析):一种流行的降维统计方法，即降低特征空间的维度。它使用正交变换将一组可能相关的变量样本转换为一组称为主成分的线性不相关变量的值。</p><h1 id="4e48" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">监督学习方法:</h1><p id="86b4" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir">多项式朴素贝叶斯</strong>:基于应用贝叶斯定理的概率分类器家族，在特征之间具有强(朴素)独立性假设。多项式分布用于对每个特征建模，因为这适合于可以容易地转化为字数的文本数据。这是一种用于文本分类的流行的监督学习方法，尽管它有“天真”的假设，但表现良好。</p><p id="8e15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">支持向量机(SVM) </strong>:通过寻找最大化类间距离余量的超平面来执行分类。这是一种流行的用于文本分类的监督学习方法，尽管实现比上面的朴素贝叶斯方法慢。</p><h1 id="c2f0" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">监督学习绩效指标:</h1><p id="0c6d" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir">精度</strong>:tp/(TP+fp)的比值，其中 TP 为真阳性的数量，FP 为假阳性的数量。精确度直观上是分类器不将阴性样品标记为阳性的能力。</p><p id="fee0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">回想一下</strong>:比值 tp / (tp + fn)其中 tp 是真阳性的数量，fn 是假阴性的数量。召回直观上是分类器找到所有肯定样本的能力。</p><p id="0655" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> F1 得分</strong>:这可以解释为精度和召回率的加权平均值，其中 F1 得分在 1 时达到最佳值，在 0 时达到最差得分。在多类别和多标签的情况下，这是每个类别的 F1 分数的加权平均值。</p><p id="8570" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">F1 得分= 2 *(精确度*召回率)/(精确度+召回率)</p><p id="83ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">支持</strong>:目标类向量中每个类出现的次数。</p><h1 id="c878" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">无监督学习方法:</h1><p id="5216" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir"> K-means </strong>:一种流行的聚类方法，用于将一组样本划分为 K 个簇。</p><p id="9453" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> NMF(非负矩阵分解)</strong>:一种线性代数方法，将一个非负矩阵分解成两个与话题分布相关的非负矩阵 W 和 H。</p><p id="fe59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> LDA(潜在狄利克雷分配)</strong>:一种贝叶斯推理方法，通过使用狄利克雷分布建立每文档主题模型和每主题术语模型。</p><h1 id="35b2" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">无监督学习性能指标:</h1><p id="be77" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir">注意</strong>:在监督学习中，我们获得基础真实类向量和预测类向量之间的直接映射，而在非监督学习中，我们无法预测绝对类 id，而是获得相对类 id 的预测。例如，包含 3 个不同类别标签{0，1，2}的基本事实的样本可能导致标记为{0，1，2}或{0，2，1}或{1，0，2}或{1，2，0}或{2，0，1}或{2，1，0}的 6 个不同排列中的任何一个的预测分配，并且我们对相对于基本事实的“最佳”排列的度量的质量感兴趣。</p><p id="33f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">调整后的互信息得分</strong>:互信息是衡量两个赋值的一致性的函数，忽略排列。这种度量有两种不同的标准化版本，标准化互信息(NMI)和调整互信息(AMI)。NMI 经常在文献中使用，而 AMI 是最近才提出来的，并根据概率进行标准化。</p><p id="2c93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">调整后的 Rand 指数</strong>:调整后的 Rand 指数是一个函数，它测量两个赋值的相似性，忽略置换并进行机会归一化。</p><p id="c909" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有关这些指标的详细信息，请参见 sk learn:<a class="ae lo" href="http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/modules/clustering . html # clustering-performance-evaluation</a></p></div></div>    
</body>
</html>