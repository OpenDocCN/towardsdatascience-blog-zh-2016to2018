<html>
<head>
<title>How a simple algorithm classifies texts with moderate accuracy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个简单的算法如何以中等精度对文本进行分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-a-simple-algorithm-classifies-texts-with-moderate-accuracy-79f0cd9eb47?source=collection_archive---------1-----------------------#2017-12-15">https://towardsdatascience.com/how-a-simple-algorithm-classifies-texts-with-moderate-accuracy-79f0cd9eb47?source=collection_archive---------1-----------------------#2017-12-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e7c1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">涵盖的内容</strong>:数据探索、文本分类、不平衡数据、各种度量</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4c5b455e89e05e9cf1146618800cad84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xkHvKjFIs9CCNgFZ"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@dendrolago89?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Damiano Lingauri</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="b8d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管最近围绕NLP的深度学习取得了令人兴奋的进展，但我想展示简单的分类器如何能够达到中等精度。这是一件好事，因为训练这些分类器不需要大量的数据和计算，这是训练深度神经网络的情况。</p><p id="5149" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇博文中，我选择了一个叫做<a class="ae kv" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>的分类器。我将在<a class="ae kv" href="https://www.kaggle.com/snap/amazon-fine-food-reviews/data" rel="noopener ugc nofollow" target="_blank">亚马逊美食评论</a>上训练它，以预测给定的文本被归类为正面评论或负面评论。</p><h1 id="149b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak"> 1。数据描述</strong></h1><p id="6012" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">数据形状(行，列):(568454，10)</p><p id="f2cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">列名:描述</strong></p><ol class=""><li id="7550" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">Id:分配给每行的唯一编号</li></ol><p id="8101" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.产品Id:产品id号</p><p id="fa5b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.用户标识:用户标识号</p><p id="d062" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.ProfileName:用户名</p><p id="f5fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.HelpfulnessNumerator:认为评论有帮助的用户数量</p><p id="dde9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">6.HelpfulnessDenominator:投票总数(认为评论有帮助的用户数和认为没有帮助的用户数)</p><p id="c2c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">7.得分:评分在1到5之间(1 =最不喜欢，5 =最喜欢)</p><p id="b9b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">8.时间:审核的时间戳</p><p id="ec01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">9.总结:产品的简短回顾</p><p id="e549" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">10.文本:产品评论的文本</p><p id="d4eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将只使用两个特征:7:分数和10:文本作为预测器或自变量。</p><h1 id="6afb" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">2.数据探索</h1><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="7f41" class="nd lt iq mz b gy ne nf l ng nh">import panda as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import seaborn as sns; sns.set();</span><span id="2d6e" class="nd lt iq mz b gy ni nf l ng nh">data = '../../datasets/amazon_review/Reviews.csv'<br/>data = pd.read_csv(data)<em class="nj"><br/># Take proportion of each review score<br/></em>score_pct = np.bincount(data['Score']) <strong class="mz ir">/</strong> data.shape[0]<br/>scores = pd.DataFrame(np.random.rand(1, 5), <br/>                      index=['scores'],<br/>                      columns=pd.Index([1, 2, 3, 4, 5], name='Score'))<br/>scores.iloc[0] = score_pct[1:]<br/>scores.plot.bar()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/f6a90f8b6d013847147e6a39bdb586e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mx0YeWU9L7fQuJ6EOxX0Jg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Proportion of review from 1 to 5</figcaption></figure><p id="7efc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">超过60%的评论是最受欢迎的评论(得分5)，其次是中等受欢迎的评论(得分4)。总评论的70%由这两个评论分数组成。</p><p id="b207" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">来看一些评分3和评分4的评论。</p><p id="8899" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 10条评分为3的评论:</strong></p><p id="8ad7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些评论包括正面评论和负面评论。我将评分为3的评论归类为负面，原因如下:第一、第三和第十条评论不包含任何正面评论，即使有些评论有正面评论，但在同一食品的评论中包含负面评论可能会阻止其他人购买食品。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="2b76" class="nd lt iq mz b gy ne nf l ng nh">print(data[data['Score'] == 3]['Text'].values[:10])</span><span id="d63f" class="nd lt iq mz b gy ni nf l ng nh">[ "This seems a little more wholesome than some of the supermarket brands, but it is somewhat mushy and doesn't have quite as much flavor either.  It didn't pass muster with my kids, so I probably won't buy it again."<br/> 'The flavors are good.  However, I do not see any differce between this and Oaker Oats brand - they are both mushy.'<br/> 'This is the same stuff you can buy at the big box stores.  There is nothing healthy about it.  It is just carbs and sugars.  Save your money and get something that at least has some taste.'<br/> "we're used to spicy foods down here in south texas and these are not at all spicy.  doubt very much habanero is used at all.  could take it up a notch or two."<br/> 'Watch your prices with this.  While the assortment was good, and I did get this on a gold box purchase, the price for this was&lt;br /&gt;$3-4 less at Target.'<br/> "If you're impulsive like me, then $6 is ok. Don't get me wrong, the quality of these babies is very good and I have no complaints. But in retrospect, the price is a little ridiculous (esp. when you add on the shipping)."<br/> 'The taste was great, but the berries had melted.  May order again in winter. If you order in cold weather you should enjoy flavor.'<br/> 'While my dogs like all of the flavors that we have tried of this dog food, for some reason their itching increased when I tried the lamb and rice. I have some very itchy dogs and am giving them a limited ingredient dog food to try to help. The duck and sweet potato cut down on the itching significantly, but when we tried lamb and rice they started itching more once again. I like Natural Balance for the quality ingredients.'<br/> 'Awesome dog food. However, when given to my "Boston", who has severe reactions to some food ingredients; his itching increased to violent jumping out of bed at night, scratching. As soon as I changed to a different formula, the scratching stopped. So glad Natural Balance has other choices. I guess you have to try each, until you find what\'s best for your pet.'<br/> "not what I was expecting in terms of the company's reputation for excellent home delivery products"]</span></pre><p id="d46a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 5条评分为4的评论:</strong></p><p id="5873" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">得分为4的评论总体上是正面的，并且包括诸如“高度推荐”、“我很高兴”之类的词语。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="738e" class="nd lt iq mz b gy ne nf l ng nh">print(data[data['Score'] == 4]['Text'].values[:10])</span><span id="d73b" class="nd lt iq mz b gy ni nf l ng nh">[ 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\' "The Lion, The Witch, and The Wardrobe" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.'<br/> 'I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.'<br/> 'good flavor! these came securely packed... they were fresh and delicious! i love these Twizzlers!'<br/> 'I was so glad Amazon carried these batteries.  I have a hard time finding them elsewhere because they are such a unique size.  I need them for my garage door opener.&lt;br /&gt;Great deal for the price.'<br/> "McCann's Instant Oatmeal is great if you must have your oatmeal but can only scrape together two or three minutes to prepare it. There is no escaping the fact, however, that even the best instant oatmeal is nowhere near as good as even a store brand of oatmeal requiring stovetop preparation.  Still, the McCann's is as good as it gets for instant oatmeal. It's even better than the organic, all-natural brands I have tried.  All the varieties in the McCann's variety pack taste good.  It can be prepared in the microwave or by adding boiling water so it is convenient in the extreme when time is an issue.&lt;br /&gt;&lt;br /&gt;McCann's use of actual cane sugar instead of high fructose corn syrup helped me decide to buy this product.  Real sugar tastes better and is not as harmful as the other stuff. One thing I do not like, though, is McCann's use of thickeners.  Oats plus water plus heat should make a creamy, tasty oatmeal without the need for guar gum. But this is a convenience product.  Maybe the guar gum is why, after sitting in the bowl a while, the instant McCann's becomes too thick and gluey."]</span></pre><h1 id="6f3d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak"> 2。训练分类器和预测</strong></h1><p id="a59c" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我会将每一篇评论分为正面或负面，并训练一个分类器来预测这两个值。正如我们在上面看到的，分数为3的评论可以被视为负面的，所以如果分数超过3，我会将每个评论分为正面的，如果分数低于3，则分为负面的。由于数据量很大，我将使用4%的数据。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="02a0" class="nd lt iq mz b gy ne nf l ng nh"><em class="nj"># We shuffle the rows and extract 10% of the rows<br/></em>df_reduced = data.sample(frac=0.04, random_state=7)<br/>reduced_data_size = df_reduced.shape[0]</span><span id="2cbe" class="nd lt iq mz b gy ni nf l ng nh"># Encode positive as 1 and negative as 0<br/>reduced_labels  = np.array([df_reduced['Score'] &gt;= 4])[0][:].astype(int) <br/>reduced_texts = df_reduced['Text'].values</span></pre><p id="6542" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">样本数量从568454减少到22738</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="3547" class="nd lt iq mz b gy ne nf l ng nh">reduced_sentiment_pct = np.bincount(reduced_labels) / reduced_data_size # percentage value of each sentiment<br/>sentiment = pd.DataFrame(np.random.rand(1, 2), <br/>index=[‘count’],<br/>columns=pd.Index([“negative”, “positive”], name=’sentiment’))<br/>sentiment.iloc[0] = reduced_sentiment_pct<br/>sentiment<br/>sentiment.plot.bar(title="Ratio of each sentiment")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/c49c568a9144b104102e0f03f2af0709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ghGByp60quc2v-WrDuqkhg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Proportion of each sentiment value</figcaption></figure><p id="c259" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们在上面的柱状图中看到的，78%的评论是正面的。所以这是不平衡的数据，这意味着，<strong class="ky ir">“如果我们只是手动将每个样本(或每个样本)分类为“阳性”，我们将获得大约78%的分类准确率。”</strong>因此，78%的准确率是我们的基准之一。</p><p id="2ed3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将使用80%的数据作为训练集，20%作为测试集。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="b5fe" class="nd lt iq mz b gy ne nf l ng nh"><em class="nj"># we will use 80% of data as training set, 20% as test set</em></span><span id="0b90" class="nd lt iq mz b gy ni nf l ng nh">train_size = int(reduced_data_size <strong class="mz ir">*</strong> .8)</span><span id="f2cc" class="nd lt iq mz b gy ni nf l ng nh">text_train = reduced_texts[:train_size]</span><span id="782e" class="nd lt iq mz b gy ni nf l ng nh">y_train = reduced_labels[:train_size]</span><span id="0655" class="nd lt iq mz b gy ni nf l ng nh">text_test = reduced_texts[train_size:]</span><span id="784b" class="nd lt iq mz b gy ni nf l ng nh">y_test = reduced_labels[train_size:]</span></pre><p id="2a03" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练集:18190个样本<br/>测试集:4548个样本</p><p id="e0da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我会将文本转换成适当的格式(由数值组成)，以便可以对它们进行分类训练。我将使用两种方法:A:单词袋和B.TFIDF</p><p id="b42a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">答:一堆废话</strong></p><p id="c7ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">单词包使用标记化将文本转换成数字表示。</p><p id="3291" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用下面的单词包处理两个文档的示例:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/9ebbc1b03845075375c0107332a78944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*YB7gbpsylW5rJu7ZkL5Iww.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Process of bag-of-words. Inspired by <a class="ae kv" href="http://Process of bag-of-words. Inspired by Introduction to Machine Learning with Python" rel="noopener ugc nofollow" target="_blank">Introduction to Machine Learning with Python</a></figcaption></figure><p id="cca7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上述过程的代码</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="85fc" class="nd lt iq mz b gy ne nf l ng nh">from sklearn.feature_extraction.text import CountVectorizer<br/>vect = CountVectorizer().fit(text_train)<br/>X_train = vect.transform(text_train)<br/>print(repr(X_train))</span><span id="161e" class="nd lt iq mz b gy ni nf l ng nh">X_test = vect.transform(text_test)<br/>print(repr(X_test))</span></pre><p id="6063" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">X_train是:8190x26817稀疏矩阵</p><p id="793d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">X_test为:4548x26817稀疏矩阵</p><p id="6961" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">#稀疏矩阵是只包含非零元素的矩阵</p><p id="9441" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">拟合逻辑回归</strong></p><p id="5f53" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意:</p><ol class=""><li id="752f" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">我在下面的实例化中使用了class_weigtht="balanced"。<a class="ae kv" href="https://svds.com/learning-imbalanced-classes/" rel="noopener ugc nofollow" target="_blank">这具有惩罚少数类</a>(该数据集中的负类)上的错误的效果。</li><li id="eff1" class="mp mq iq ky b kz nn lc no lf np lj nq ln nr lr mu mv mw mx bi translated">我用的GridSearchCV是带“roc_auc”的，不是“分类精度”。这方面的理由写在下面一节。</li></ol><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="0144" class="nd lt iq mz b gy ne nf l ng nh">from sklearn.pipeline import make_pipeline<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.metrics import confusion_matrix<br/>from sklearn.linear_model import LogisticRegression</span><span id="7de7" class="nd lt iq mz b gy ni nf l ng nh">logreg = LogisticRegression(class_weight=”balanced”, random_state=0)<br/>param_grid = {‘C’: [0.01, 0.1, 1, 10, 100]}</span><span id="3d75" class="nd lt iq mz b gy ni nf l ng nh">grid = GridSearchCV(logreg, param_grid, scoring=”roc_auc”, cv=5)<br/>logreg_train = grid.fit(X_train, y_train)</span><span id="2ab6" class="nd lt iq mz b gy ni nf l ng nh">pred_logreg = logreg_train.predict(X_test)<br/>confusion = confusion_matrix(y_test, pred_logreg)<br/>print(confusion)<br/>print("Classification accuracy is: ", (confusion[0][0] + confusion[1][1]) / np.sum(confusion))</span></pre><p id="f856" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">混乱矩阵:<br/> [[ 826 206] <br/> [ 468 3048]]</p><p id="ddc5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">分类精度为:0.851802990325</p><p id="e250" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我打印了一个混淆矩阵，下面画出了它的一个概念。总的来说，我们希望增加真阴性和真阳性的数量。(这两个是矩阵的主对角线)同时最小化假阴性和假阳性的数量(这两个是非对角线)。在上面的混淆矩阵中，假阴性比假阳性多。这可能是由于给定的类权重的惩罚。分类精度计算为:(TN+TP) / (TN+TP+FP + FN)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/86ed06f5e2058c044392ef2809e7c21f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*GpAqhezaMuldOt5i4PvdkQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Concept of confusion matrix</figcaption></figure><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="a7f7" class="nd lt iq mz b gy ne nf l ng nh">from sklearn.metrics import roc_curve</span><span id="2848" class="nd lt iq mz b gy ni nf l ng nh">fpr, tpr, thresholds = roc_curve(y_test, grid.decision_function(X_test))<br/># find threshold closest to zero:<br/>close_zero = np.argmin(np.abs(thresholds))<br/>plt.plot(fpr[close_zero], tpr[close_zero], ‘o’, markersize=10, <br/> label=”threshold zero(default)”, fillstyle=”none”, c=’k’, mew=2)<br/>plt.plot([0,1], linestyle=’ — ‘, lw=2, color=’r’, label=’random’, alpha=0.8)<br/>plt.legend(loc=4)<br/>plt.plot(fpr, tpr, label=”ROC Curve”)<br/>plt.xlabel(“False Positive Rate”)<br/>plt.ylabel(“True Positive Rate (recall)”)<br/>plt.title(“roc_curve”);<br/>from sklearn.metrics import auc<br/>print(“AUC score is: “, auc(fpr, tpr));</span></pre><p id="d9f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">AUC分数是:0.901340273919</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/2be1773d902ce77da4a1efcb2cb45ba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aSosHFQqWPnOnHoZKwC-ZA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">ROC curve</figcaption></figure><p id="bac4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> ROC &amp; AUC </strong></p><p id="b33e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ROC:针对假阳性率和真阳性率绘制的曲线，它考虑了分类器的所有阈值。真阳性率也叫召回率。最佳点是左上角，在这里可以实现最低的FPR和最高的TPR。</p><p id="4452" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">AUC:ROC曲线下的面积。此AUC的范围(可能值)介于0(最差)和1(最佳)之间。随机预测总是产生0.5的AUC分数。<a class="ae kv" href="https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy" rel="noopener ugc nofollow" target="_blank"> AUC是从阳性类别中随机选择的样本比从阴性类别中随机选择的样本给出更高分数的概率</a>或阳性类别中的置信度。因此，AUC给出了关于预测的附加信息，即关于模型比较(例如，一些模型可能是随机分类器)和可以产生更高分类率的适当阈值的信息。这就是我在GridSearch中使用AUC作为度量标准的原因。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="dd28" class="nd lt iq mz b gy ne nf l ng nh">from sklearn.metrics import precision_recall_curve<br/>precision, recall, thresholds = precision_recall_curve(\<br/>                                                      y_test, logreg_train.decision_function(X_test))<br/>close_zero = np.argmin(np.abs(thresholds))<br/>plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10, <br/>         label="threhold zero", fillstyle="none", c="k", mew=2)<br/>plt.plot(precision, recall, label="precision recall curve")<br/>plt.xlabel("precision")<br/>plt.ylabel("recall")<br/>plt.title("Precision Recall Curve")<br/>plt.legend(loc="best");</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/9408e2e983e962d72b77e36f23a3ed20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vZGotzEaDvH6H8por0yXOQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Precision Recall Curve</figcaption></figure><p id="dc95" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">精确和召回</strong></p><p id="78d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">精确度和召回率由下面的公式决定。这两种价值观之间是有取舍的。当目标是减少假阳性的数量时，使用精度。在该数据中，目标可以是减少被预测为正面的负面评论的数量。设定这一目标的理由是，如果预测是肯定的，一些食品公司可能会出售大量不喜欢或不受欢迎的食品。因此，更高的假阳性率将导致不必要的更高的食品生产成本。另一方面，当目标是减少假阴性的数量时，使用召回。在这些数据中，这意味着目标是减少被错误分类为负面评价的正面评价的数量。我们可能需要更高的召回率，因为销售更多的大众食品会增加利润。因此，我们可以将<strong class="ky ir">精确度和召回率的权衡</strong>视为该数据集中<strong class="ky ir">成本最小化和利润最大化的权衡</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/d46bb1a64be5e47cd339fb4ac44816ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jry_YQ3hJ9kAeCLGA3jC1w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Formulas of precision and recall and tradeoff</figcaption></figure><p id="243e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> B: TFIDF(频率-逆文档频率)</strong></p><p id="c8c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">TFIDF对在特定评论(文档)中频繁出现但在作为整体的评论(语料库)中不频繁出现的词给予高权重。具有高权重的单词是每个评论的代表，而具有低权重的单词出现在许多评论(例如，吃、食物)中，并且不与特定评论相关联。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/2e1fcd7a0dffc95ec3e2356c41c5dc40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cZmKpNbY9iNTF-Y7WJvCjw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="http://www.tfidf.com/" rel="noopener ugc nofollow" target="_blank">A formula of tf-idf</a></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/e888f874575b4d7cc8aacb0f4017e234.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SG0dRrAQU9UEtWm_8DaAYw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">A process of tf-idf</figcaption></figure><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="b8c5" class="nd lt iq mz b gy ne nf l ng nh">logreg = LogisticRegression(class_weight="balanced", random_state=0)<br/>pipe = make_pipeline(TfidfVectorizer(norm=None, stop_words='english'), logreg)<br/>param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}</span><span id="b52c" class="nd lt iq mz b gy ni nf l ng nh">grid = GridSearchCV(pipe, param_grid, scoring="roc_auc", cv=5)<br/>logreg_train = grid.fit(text_train, y_train)</span><span id="582e" class="nd lt iq mz b gy ni nf l ng nh">fpr, tpr, thresholds = roc_curve(y_test, grid.decision_function(text_test))<br/>pred_logreg = logreg_train.predict(text_test)<br/>confusion = confusion_matrix(y_test, pred_logreg)<br/>print(confusion)<br/>print("Classification accuracy is: ", (confusion[0][0] + confusion[1][1]) / np.sum(confusion)) <br/>print("Test AUC score is: ", auc(fpr, tpr));</span></pre><p id="94df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">混淆矩阵</p><p id="f29f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[[806 226]<br/>【454 3062]]<br/>分类准确率为:0.850483729112 <br/> AUC得分为:0.899483666</p><p id="4107" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们想象一下哪25个单词对预测的影响最大。(点击下图放大)</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="8825" class="nd lt iq mz b gy ne nf l ng nh">mglearn.tools.visualize_coefficients(grid.best_estimator_.named_steps['logisticregression'].coef_, <br/>                                   feature_names, n_top_features=25)<br/>plt.title("tfidf-cofficient")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/8a17eefa0c1f09a0aa0da94dae0bf463.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PaHrsoMH2m-rWkkitOJlsw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Blue for word which has huge influence on making prediction on positive review and red for negative.</figcaption></figure><p id="8168" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于负面评论(红色部分)，会出现“失望”、“最差”等词语。对于正面评论(蓝色部分)，会出现“很好”、“最好”等词语。奇怪的是，还有一些红色的单词，如“ok”、“excited”，这些应该是正面的评论。这将在下面讨论。</p><p id="1caf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> N-gram </strong></p><p id="2c14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对单词或每个单词进行训练是很棘手的，因为一些单词组合在一起表示非常不同的意思。</p><p id="d3dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，“这不是一个好产品”，如果你用1-gram(unigram)来标记这个句子，你会得到6个不同的单词，其中包括“好”。如果你用3-gram，那么你会得到3个单词的所有组合，其中一个是“不好”。很明显，对“不是一个好”的训练可能导致更好的准确性，因为“不是一个好”更好地抓住了这句话的负面含义。</p><p id="8d0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3-gram(三元模型)用于训练下面的分类器。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="0b82" class="nd lt iq mz b gy ne nf l ng nh">pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression(class_weight="balanced", random_state=0))<br/>param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 10, 100], <br/>              'tfidfvectorizer__ngram_range': [(1,1), (1,2), (1,3)]}</span><span id="6bd0" class="nd lt iq mz b gy ni nf l ng nh">grid = GridSearchCV(pipe, param_grid, scoring="roc_auc", cv=5)<br/>logreg_train = grid.fit(text_train, y_train)<br/>pred_logreg = logreg_train.predict(text_test)<br/><br/>confusion = confusion_matrix(y_test, pred_logreg)<br/>print("confusion matrix \n", confusion</span><span id="d5f1" class="nd lt iq mz b gy ni nf l ng nh">print("Classification accuracy is: ", (confusion[0][0] + confusion[1][1]) / np.sum(confusion)) <br/>print("AUC score is: ", auc(fpr, tpr));<br/></span></pre><p id="da25" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">混淆矩阵<br/>[[810 222]<br/>【261 3255】]<br/>分类准确率为:0.893799472296 <br/> AUC评分为:0.932426383</p><p id="b8df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"># AUC增加了约3%</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="f526" class="nd lt iq mz b gy ne nf l ng nh">feature_names = np.array(grid.best_estimator_.named_steps["tfidfvectorizer"].get_feature_names())<br/>coef = grid.best_estimator_.named_steps["logisticregression"].coef_<br/>mask = np.array([len(feature.split(" ")) for feature in feature_names]) == 3<br/>mglearn.tools.visualize_coefficients(coef.ravel()[mask], <br/>                                   feature_names[mask], n_top_features=40)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/31aec0e840dd0e9faab8707d5e2c56f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhiCB3mOV3wAJtMed7qr0w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Trigram gram visualization</figcaption></figure><p id="9bd3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于负面评论，会出现“不值得”、“不如”这样的词。对于这些单词，每个单词不传达任何意思(例如，the，as)或相反的意思(例如，值得，不值得)。对于正面评价，有一个连词，“喜出望外”。“惊讶”只能被认为是消极或积极的，副词，“愉快地”确保这有一个积极的意义。</p><h1 id="e72d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">总结:</strong></h1><p id="9006" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">对于少量数据和少量计算资源，您可以使用分类器，如逻辑回归，并根据如何转换数据来达到中等精度。考虑到数据大小和有限的硬件，在某些情况下，您可能不需要使用像RNN、LSTM这样的模型。</p><h1 id="59f8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><a class="ae kv" href="https://github.com/Juice178/sentiment_analysis" rel="noopener ugc nofollow" target="_blank">完整代码在此</a></h1></div></div>    
</body>
</html>