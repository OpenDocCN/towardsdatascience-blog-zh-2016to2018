<html>
<head>
<title>Speech Classification Using Neural Networks: The Basics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用神经网络的语音分类:基础</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speech-classification-using-neural-networks-the-basics-e5b08d6928b7?source=collection_archive---------7-----------------------#2018-09-25">https://towardsdatascience.com/speech-classification-using-neural-networks-the-basics-e5b08d6928b7?source=collection_archive---------7-----------------------#2018-09-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/a006dce197538ea530e348a32b3d422b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKs6dvnYoeec1KEj83-b8Q.png"/></div></div></figure><p id="2cb8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最近，我开始研究一个语音分类问题，因为我对语音/音频处理知之甚少，所以我不得不回顾一下最基本的内容。在这篇文章中，我想回顾一下我学到的一些东西。为此，我想研究“语音 MNIST”数据集，即一组记录的口语数字。</p><p id="a4ce" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你可以在这里找到数据集<a class="ae kw" href="https://github.com/Jakobovski/free-spoken-digit-dataset" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="332d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">数据集包含以下内容:</p><ul class=""><li id="edd2" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated">3 个扬声器</li><li id="afc3" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">1，500 个录音(每个扬声器每个数字 50 个)</li></ul><p id="6a63" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如题所述，这是一个分类问题，我们得到一段录音，需要预测其中所说的数字。</p><p id="a551" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我想尝试不同的方法来解决这个问题，并逐步了解什么更好，为什么。</p><p id="260a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，我们来了解一下录音到底是什么。我将使用优秀的<code class="fe ll lm ln lo b">librosa</code>库:</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="2e53" class="lx ly iq lo b gy lz ma l mb mc">wav, sr = librosa.load(DATA_DIR + random_file)<br/>print 'sr:', sr<br/>print 'wav shape:', wav.shape</span><span id="4d35" class="lx ly iq lo b gy md ma l mb mc"># OUTPUT<br/>sr: 22050<br/>wav shape: (9609,)</span></pre><p id="c1ad" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe ll lm ln lo b">load</code>方法返回 2 个值，第一个是实际声波，第二个是‘采样率’。如你所知，“声音”是一种模拟信号，为了使其数字化，并能够用类似于<code class="fe ll lm ln lo b">numpy</code>阵列的东西来表示，我们必须对原始信号进行<a class="ae kw" href="https://en.wikipedia.org/wiki/Sampling_(signal_processing)" rel="noopener ugc nofollow" target="_blank">采样</a>。简而言之，采样只是从原始信号中“选择”有限数量的点，并丢弃其余的点。我们可以将这些选定的点存储在一个数组中，并对其执行不同的离散操作。<a class="ae kw" href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem" rel="noopener ugc nofollow" target="_blank">奈奎斯特-香农采样定理</a>表明，如果我们的采样速率足够高，我们就能够捕获信号中的所有信息，甚至完全恢复它。</p><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div class="gh gi me"><img src="../Images/83f2fdf7837e1c9f2adecd6dd8e3041f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*L-E98JQ6r9Q88rqhf2TXjA.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Signal sampling representation. The continuous (analog) signal is represented with a green colored line while the discrete samples are indicated by the blue vertical lines.</figcaption></figure><p id="5241" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">采样率非常重要，我们稍后会在不同的算法中用到它。它的标度是赫兹，即每秒的点数(样本数)。在我们的示例中，<code class="fe ll lm ln lo b">sr=22050 </code>我们每秒有 22050 个样本，我们的波形大小为 9609，我们可以使用以下公式计算音频长度:</p><p id="754e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe ll lm ln lo b">length = wav.shape[0]/float(sr) = 0.435 secs</code></p><p id="c29b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">实际上，我们的实际采样率不是 22050，<code class="fe ll lm ln lo b">librosa</code>隐式地对我们的文件进行重新采样，以获得更标准的 22050 SR。要获得原始采样率，我们可以使用<code class="fe ll lm ln lo b">sr=False</code>的<code class="fe ll lm ln lo b">load</code>方法:</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="f0d3" class="lx ly iq lo b gy lz ma l mb mc">wav, sr = librosa.load(DATA_DIR + random_file, sr=None)<br/>print 'sr:', sr<br/>print 'wav shape:', wav.shape<br/>print 'length:', sr/wav.shape[0], 'secs'</span><span id="310e" class="lx ly iq lo b gy md ma l mb mc"># OUTPUT<br/>sr: 8000<br/>wav shape: (3486,)<br/>length: 0.43575 secs</span></pre><p id="6ab3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">音频长度与预期保持一致。</p><p id="5091" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当我们处理具有不同采样率的文件时，重采样非常有用。我们暂时坚持这一点。</p><p id="3dab" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">实际声音是这样的:</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="ee8e" class="lx ly iq lo b gy lz ma l mb mc">plt.plot(wav)</span></pre><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/a23c8a2674d2f629d8f5abfa75aae93b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CZknjoZTA8GqcEt1FTiG0w.png"/></div></div></figure><p id="acee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如你所见，这是一个非常复杂的信号，很难从中找出规律。就算放大看，还是挺复杂的。</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="bf21" class="lx ly iq lo b gy lz ma l mb mc">plt.plot(wav[4000:4200])</span></pre><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mk"><img src="../Images/265bf559eb2d0b59f737005096714870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lAzsKwBSjGnaDVsDJXTFDQ.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">200 samples out of 9000 of the original file</figcaption></figure><p id="9e36" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，我们将尝试“原样”使用这些波形，并尝试建立一个神经网络来为我们预测口语数字。实际上，几乎从来没有这样做过。我这样做只是为了理解从原始文件到完整解决方案的不同步骤。</p><p id="abfe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但首先，我们需要准备数据集。正如我们前面看到的，我们有 1500 个由 3 个说话者说出的数字的录音。将数据分割成经典的 80-20 训练测试分割可能不是一个好主意，因为我们的训练数据将包含与测试数据中相同的扬声器，并且可能会提供非常好的结果，而对其他扬声器提供非常差的结果。测试我们算法的正确方法是在两个扬声器上训练它，在第三个扬声器上测试它。这样，我们将有一个非常小的训练和测试集，但对于本文的目的来说，这已经足够了。在现实生活中，我们必须在许多不同性别、种族、口音等的人身上测试我们的算法，以便真正了解我们的算法有多好。</p><p id="e08a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的数据看起来像这样:</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="4b2f" class="lx ly iq lo b gy lz ma l mb mc">X = []<br/>y = []<br/>pad = lambda a, i: a[0: i] if a.shape[0] &gt; i else np.hstack((a, np.zeros(i - a.shape[0])))<br/>for fname in os.listdir(DATA_DIR):<br/>    struct = fname.split('_')<br/>    digit = struct[0]<br/>    wav, sr = librosa.load(DATA_DIR + fname)<br/>    padded = pad(wav, 30000)<br/>    X.append(padded)<br/>    y.append(digit)</span><span id="a984" class="lx ly iq lo b gy md ma l mb mc">X = np.vstack(X)<br/>y = np.array(y)</span><span id="1e90" class="lx ly iq lo b gy md ma l mb mc">print 'X:', X.shape<br/>print 'y:', y.shape</span><span id="7e14" class="lx ly iq lo b gy md ma l mb mc"># OUTPUT<br/>X: (1500, 30000)<br/>y: (1500,)</span></pre><p id="94da" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将使用一个简单的 MLP 网络与一个单一的隐藏层开始:</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="f088" class="lx ly iq lo b gy lz ma l mb mc">ip = Input(shape=(X[0].shape))<br/>hidden = Dense(128, activation='relu')(ip)<br/>op = Dense(10, activation='softmax')(hidden)<br/>model = Model(input=ip, output=op)</span><span id="2003" class="lx ly iq lo b gy md ma l mb mc">model.summary()</span><span id="2e28" class="lx ly iq lo b gy md ma l mb mc"># OUTPUT<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_1 (InputLayer)         (None, 30000)             0         <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 128)               3840128   <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 10)                1290      <br/>=================================================================<br/>Total params: 3,841,418<br/>Trainable params: 3,841,418<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="9df1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们来训练它:</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="f465" class="lx ly iq lo b gy lz ma l mb mc">model.compile(loss='categorical_crossentropy',<br/>              optimizer='adam',<br/>              metrics=['accuracy'])</span><span id="174d" class="lx ly iq lo b gy md ma l mb mc">history = model.fit(train_X,<br/>          train_y,<br/>          epochs=10,<br/>          batch_size=32,<br/>          validation_data=(test_X, test_y))</span><span id="19ea" class="lx ly iq lo b gy md ma l mb mc">plt.plot(history.history['acc'], label='Train Accuracy')<br/>plt.plot(history.history['val_acc'], label='Validation Accuracy')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Accuracy')<br/>plt.legend()</span></pre><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/0d5099e93b2d49806fdf7ada83d7478c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dtmwTGh1TIYVUm69Z7-rkg.png"/></div></div></figure><p id="78a2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可以看到，我们在训练数据上的准确性还可以，但在测试数据上却很糟糕。</p><p id="405b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以尝试不同的网络架构来解决这个问题，但正如我前面所说，在实践中，raw waves 几乎从未使用过。所以我们将转向更标准的方式来表示声音文件:声谱图！</p><p id="2aaa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在讲光谱图之前，我们先来讲讲余弦波。</p><p id="ddf8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">余弦波看起来像这样:</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="58b5" class="lx ly iq lo b gy lz ma l mb mc">signal = np.cos(np.arange(0, 20, 0.2))<br/>plt.plot(signal)</span></pre><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mm"><img src="../Images/93e2aff8fade2c3c632b52bb057c5608.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jm0Iqmxd5PtCs0HtBVm6-Q.png"/></div></div></figure><p id="a3c9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你可以看到这是一个非常简单的信号，其中有一个清晰的模式。我们可以通过改变余弦波的振幅和频率来控制余弦波。</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="4c42" class="lx ly iq lo b gy lz ma l mb mc">signal = 2*np.cos(np.arange(0, 20, 0.2)*2)<br/>plt.plot(signal)</span></pre><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mn"><img src="../Images/f66f4e2e52892e1e6e1b1822bbeb592b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oThm2OCyz8wQZYgzmKdXUg.png"/></div></div></figure><p id="6bd6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过将不同频率的不同余弦波叠加在一起，我们可以实现非常复杂的波。</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="3262" class="lx ly iq lo b gy lz ma l mb mc">cos1 = np.cos(np.arange(0, 20, 0.2))<br/>cos2 = 2*np.cos(np.arange(0, 20, 0.2)*2)<br/>cos3 = 8*np.cos(np.arange(0, 20, 0.2)*4)<br/>signal = cos1 + cos2 + cos3<br/>plt.plot(signal)</span></pre><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mo"><img src="../Images/0c5dac3927a96010c58d87762c8f90b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ShBKGWhXGXc9sDYas9vs-A.png"/></div></div></figure><p id="b7f1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以(至少)用两种方式来表示上面的波。我们可以使用非常复杂的整个 100 大小的信号，或者，我们可以只存储该信号中使用的频率。这是一种更简单的数据表示方式，占用的空间也少得多。这里我们有 3 个不同振幅的不同频率。那三个频率到底是什么？这是一个很好的问题，答案是:这取决于采样率(我稍后将展示如何)。</p><p id="f7bc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们需要一种方法，给定一个原始波(时间的函数)，将返回给我们其中的频率。这就是傅立叶变换的作用！我不会深究傅立叶变换是如何工作的，所以让我们把它当作一个黑匣子，给我们一个声波的频率。如果你想更好的理解，我推荐<a class="ae kw" href="https://www.youtube.com/watch?v=spUNpyF58BY&amp;t=3s" rel="noopener ugc nofollow" target="_blank">这个</a>视频。</p><p id="6f89" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们需要确定我们的波的采样率是多少，或者类似地，我们的信号的长度是多少。为了方便起见，我们用一秒钟。我们有 100 品脱，所以我们的采样率是 100 赫兹。</p><p id="ccdb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们可以对信号进行傅立叶变换。我们将使用“快速傅立叶变换”算法来计算 O(nlogn)中的离散傅立叶变换。为此我们将使用<code class="fe ll lm ln lo b">numpy</code></p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="78bd" class="lx ly iq lo b gy lz ma l mb mc">fft = np.fft.fft(signal)[:50]<br/>fft = np.abs(fft)<br/>plt.plot(fft)</span></pre><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mo"><img src="../Images/795322538643b345c2c0fe899e420fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMFT4CYbhAc_S43NaZcH6g.png"/></div></div></figure><p id="020e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们在这里看到 3 种频率:4、7 和 14 位每秒，正如我们建立我们的信号(欢迎您检查)。</p><p id="62b0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里我们只使用返回值的前半部分，因为 FFT 的结果是对称的。</p><p id="79f3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">原来每一个声音(甚至是人的语音)都是由很多这样的不同频率的基本余弦波组成的。</p><p id="b1ca" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们有办法从任何声音信号中获取频率，但人类语音不是静态噪声，它会随着时间的推移而变化，因此为了正确地表示人类语音，我们将把我们的记录分成小窗口，并计算每个窗口中使用的频率。为此，我们可以使用<a class="ae kw" href="https://en.wikipedia.org/wiki/Short-time_Fourier_transform" rel="noopener ugc nofollow" target="_blank">短时傅立叶变换</a>。</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="1d20" class="lx ly iq lo b gy lz ma l mb mc">D = librosa.amplitude_to_db(np.abs(librosa.stft(wav)), ref=np.max)<br/>librosa.display.specshow(D, y_axis='linear')</span></pre><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mp"><img src="../Images/25e8c4f8438a8751cbf5dff95544716b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A0I_mSSuw7zdE7nBVY___Q.png"/></div></div></figure><p id="1018" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以在录音中间看到一些低频声音。这是典型的男性声音。这就是光谱图！它向我们展示了录音不同部分的不同频率。</p><p id="9910" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们看一下代码:</p><p id="43ee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe ll lm ln lo b">librosa.stft</code>为我们计算短时傅立叶变换。返回值是一个矩阵，其中 X 是窗口号，Y 是频率。STFT 值是复数。我们只需要使用它的实部来找到频率系数。</p><p id="b45d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe ll lm ln lo b">np.abs</code>取<code class="fe ll lm ln lo b">stft</code>的绝对值，如果是复数，则返回实部的绝对值。</p><p id="5cad" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe ll lm ln lo b">librosa.amplitude_to_db</code>将数值转换为分贝。</p><p id="b480" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">概括一下:</p><p id="4e1c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://en.wikipedia.org/wiki/Fourier_transform" rel="noopener ugc nofollow" target="_blank">傅立叶变换</a> —将时间幅度信号转换为频率幅度函数的过程</p><p id="84cd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform" rel="noopener ugc nofollow" target="_blank">离散傅里叶变换</a> (DST) —傅里叶变换<strong class="ka ir">离散</strong>信号</p><p id="8feb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://en.wikipedia.org/wiki/Fast_Fourier_transform" rel="noopener ugc nofollow" target="_blank">快速傅立叶变换</a>(FFT)——一种能够以 O(nlogn)而不是 O(n)计算傅立叶变换的算法</p><p id="d54d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://en.wikipedia.org/wiki/Short-time_Fourier_transform" rel="noopener ugc nofollow" target="_blank">短时傅立叶变换</a>(STFT)——将记录分成小窗口并计算每个窗口 DST 的算法</p><p id="ddd3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">很好，现在我们能够计算数据集中每个文件的光谱图，然后用它们来分类数字。光谱图的真正好处是它们像 2D 图像，所以我们可以对它们使用图像分类技术，特别是卷积神经网络！</p><pre class="lp lq lr ls gt lt lo lu lv aw lw bi"><span id="44d2" class="lx ly iq lo b gy lz ma l mb mc">ip = Input(shape=train_X_ex[0].shape)<br/>m = Conv2D(32, kernel_size=(4, 4), activation='relu', padding='same')(ip)<br/>m = MaxPooling2D(pool_size=(4, 4))(m)<br/>m = Dropout(0.2)(m)<br/>m = Conv2D(64, kernel_size=(4, 4), activation='relu')(ip)<br/>m = MaxPooling2D(pool_size=(4, 4))(m)<br/>m = Dropout(0.2)(m)<br/>m = Flatten()(m)<br/>m = Dense(32, activation='relu')(m)<br/>op = Dense(10, activation='softmax')(m)</span><span id="7154" class="lx ly iq lo b gy md ma l mb mc">model = Model(input=ip, output=op)</span><span id="90ab" class="lx ly iq lo b gy md ma l mb mc">model.summary()</span><span id="453a" class="lx ly iq lo b gy md ma l mb mc"># OUTPUT<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_2 (InputLayer)         (None, 1025, 40, 1)       0         <br/>_________________________________________________________________<br/>conv2d_4 (Conv2D)            (None, 1022, 37, 64)      1088      <br/>_________________________________________________________________<br/>max_pooling2d_4 (MaxPooling2 (None, 255, 9, 64)        0         <br/>_________________________________________________________________<br/>dropout_3 (Dropout)          (None, 255, 9, 64)        0         <br/>_________________________________________________________________<br/>flatten_2 (Flatten)          (None, 146880)            0         <br/>_________________________________________________________________<br/>dense_3 (Dense)              (None, 32)                4700192   <br/>_________________________________________________________________<br/>dense_4 (Dense)              (None, 10)                330       <br/>=================================================================<br/>Total params: 4,701,610<br/>Trainable params: 4,701,610<br/>Non-trainable params: 0<br/>____________________________</span></pre><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mq"><img src="../Images/fc1a3662e7c1b9366a5b78b2fe5cf007.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AgtWkx9EE_v4JO3u96jYDQ.png"/></div></div></figure><p id="2279" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好多了！在我们的测试集上，我们得到了大约 65%的准确率，这离完美还差得很远，但这比使用原始波形要好得多。借助更多的数据和对我们网络的更多微调，我们可能会得到更好的结果。</p><p id="d6d1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">结论</strong></p><p id="149d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们看到了一个非常基本的语音分类实现。为机器学习表示声音一点也不简单，有很多方法，也做了很多研究。傅立叶变换是信号处理的基础，几乎无处不在，是处理声音的基础。</p><p id="1562" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你可以在这里找到我的实验代码<a class="ae kw" href="https://github.com/shudima/notebooks/blob/master/Speech%20Classification.ipynb" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="ee13" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">下一个要探索的话题:</strong></p><ul class=""><li id="b414" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated">梅尔标度光谱图</li><li id="d6ae" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">梅尔频率倒谱系数</li></ul></div></div>    
</body>
</html>