<html>
<head>
<title>Random Initialization For Neural Networks : A Thing Of The Past</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络的随机初始化:过去的事情</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/random-initialization-for-neural-networks-a-thing-of-the-past-bfcdd806bf9e?source=collection_archive---------1-----------------------#2018-02-25">https://towardsdatascience.com/random-initialization-for-neural-networks-a-thing-of-the-past-bfcdd806bf9e?source=collection_archive---------1-----------------------#2018-02-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/fefacc60be095fdc02f3bbe65955e611.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_4dBjBNyNntW-Iuy."/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@nasa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">NASA</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="b985" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最近，神经网络已经成为我们几乎所有机器学习相关问题的<em class="lb">到</em>解决方案。这仅仅是因为神经网络有能力合成复杂的非线性，这种非线性几乎总是能神奇地给出以前不可能的精度。</p><p id="2343" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在业内，神经网络被视为黑匣子。这是因为当我们从一个密集层移动到下一个密集层时，它们使用给定数据的特征来公式化越来越复杂的特征。研究人员试图研究这些复杂的特征生成过程，但迄今为止还没有太大的进展。神经网络仍然是黑匣子。</p><p id="5c74" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一点正是让<em class="lb">深</em>学、<em class="lb">深</em>的原因。</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lc"><img src="../Images/837e881ac617c39a25f63481276566e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FPPOV8I_jo0j1PWrP1wnag.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">courtesy:<a class="ae kc" href="https://www.facebook.com/convolutionalmemes/" rel="noopener ugc nofollow" target="_blank"> Machine Learning Memes for Convolutional Teens,facebook</a></figcaption></figure><p id="97c0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一些研究人员也反对在自动驾驶汽车和无人机等非常重要的领域使用神经网络。他们说，与支持向量机或随机森林的决策框架相比，深度神经网络做出的决策是不合理的。如果明天出现任何问题，比如说，如果一辆自动驾驶汽车在去杂货店的路上跳下悬崖，如果是支持向量机在控制汽车的行为，那么问题背后的原因可以很容易地纠正和纠正，另一方面，由于神经网络的框架非常复杂，没有人能够预测汽车为什么跳下悬崖，为什么它会做出这个决定。</p><p id="f2e6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但总的来说，今天没有其他方法可以像神经网络一样准确地学习数据。神经网络是图像识别成为今天这个样子的原因。如今，复杂的卷积网络正在被制造出来，它们在识别物体方面变得越来越精确，甚至可以在这项任务中与人类竞争。</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/dd082ac7b8a3145ee64bd62aade050df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*UO4Ca9_f_na7Ycbd2RUCrw.png"/></div></figure><p id="3082" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在神经网络中，每两层之间存在权重。这些权重和先前层中的值的线性变换通过非线性激活函数来产生下一层的值。这个过程在前向传播期间逐层发生，通过反向传播，可以找出这些权重的最佳值，以便在给定输入的情况下产生精确的输出。</p><p id="8c5d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">到目前为止，机器学习工程师一直使用随机初始化的权重作为这个过程的起点。直到现在(即:2015 年)，还不知道这些权重的初始值在寻找深度神经网络成本函数的全局最小值中起到如此重要的作用</p><p id="65fd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我目前正在吴恩达的 coursera 上做深度学习专业化，专业化的第二个课程涉及这些深度神经网络的超参数调整。</p><p id="dccf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们开始前向、后向传播以找到最佳权重之前，让我们看看初始化层之间的权重的三种方式。</p><h2 id="1ae4" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">1:零初始化</h2><h2 id="531b" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">2:随机初始化</h2><h2 id="11f1" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">3: he-et-al 初始化</h2></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="e670" class="mi lj iq bd lk mj mk ml ln mm mn mo lq mp mq mr lt ms mt mu lw mv mw mx lz my bi translated">零初始化</h1><p id="e99c" class="pw-post-body-paragraph kd ke iq kf b kg mz ki kj kk na km kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">零初始化没有任何意义。神经网络不执行对称破坏。如果我们将所有的权重都设置为零，那么所有层的所有神经元都执行相同的计算，给出相同的输出，从而使整个深度网络变得无用。如果权重为零，整个深度网络的复杂性将与单个神经元的复杂性相同，并且预测将不会比随机更好。</p><blockquote class="ne nf ng"><p id="3b64" class="kd ke lb kf b kg kh ki kj kk kl km kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">w=np.zeros((层大小[l]，层大小[l-1])</p></blockquote><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/fdf93e21bcd120942dfc0924bc70fd9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*_wS_ul0act9fCT-b7SuONQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">courtesy:<a class="ae kc" href="https://www.facebook.com/convolutionalmemes/" rel="noopener ugc nofollow" target="_blank"> Machine Learning Memes for Convolutional Teens,facebook</a></figcaption></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="d733" class="mi lj iq bd lk mj mk ml ln mm mn mo lq mp mq mr lt ms mt mu lw mv mw mx lz my bi translated">随机初始化</h1><p id="6fb9" class="pw-post-body-paragraph kd ke iq kf b kg mz ki kj kk na km kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">这有助于对称性破缺的过程，并给出更好的精度。在这种方法中，权重被初始化为非常接近零，但是随机的。这有助于打破对称性，每个神经元不再执行相同的计算。</p><blockquote class="ne nf ng"><p id="4c0e" class="kd ke lb kf b kg kh ki kj kk kl km kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">w = NP . random . randn(layer _ size[l]，layer_size[l-1])*0.01</p></blockquote><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/7c206baf99c639f3a6d0c6f485b2d8fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*3xDSB2RxoKDPnrdsonV4aA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">courtesy:<a class="ae kc" href="https://www.facebook.com/convolutionalmemes/" rel="noopener ugc nofollow" target="_blank"> Machine Learning Memes for Convolutional Teens,facebook</a></figcaption></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="4c3c" class="mi lj iq bd lk mj mk ml ln mm mn mo lq mp mq mr lt ms mt mu lw mv mw mx lz my bi translated">He-et-al 初始化</h1><p id="904b" class="pw-post-body-paragraph kd ke iq kf b kg mz ki kj kk na km kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">这种初始化的方法通过何等人 2015 年提交的一篇论文而出名，类似于 Xavier 初始化，因子乘以 2。在这种方法中，记住前一层的大小来初始化权重，这有助于更快更有效地获得成本函数的全局最小值。权重仍然是随机的，但是范围根据前一层神经元的大小而不同。这提供了受控的初始化，因此更快和更有效的梯度下降。</p><blockquote class="ne nf ng"><p id="aa66" class="kd ke lb kf b kg kh ki kj kk kl km kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">w=np.random.randn(层大小[l]，层大小[l-1])* NP . sqrt(2/层大小[l-1])</p></blockquote></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><div class="ld le lf lg gt nm"><a href="http://deepdish.io/2015/02/24/network-initialization/" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">芝加哥大学深度网络深度学习的初始化</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">提供芝加哥式的深度学习。来自芝加哥大学的深度学习博客。</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">deepdish.io</p></div></div></div></a></div><p id="7ee8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">阅读这篇文章以获得更多关于这个主题的信息。</p><p id="0c5e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过在相同的数据上尝试所有三种初始化技术，我观察到以下情况:</p><h1 id="0434" class="mi lj iq bd lk mj nv ml ln mm nw mo lq mp nx mr lt ms ny mu lw mv nz mx lz my bi translated">零初始化:</h1><p id="fe81" class="pw-post-body-paragraph kd ke iq kf b kg mz ki kj kk na km kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">15000 次迭代后的成本:0.7</p><p id="9080" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">精确度:0.5</p><h1 id="1d16" class="mi lj iq bd lk mj nv ml ln mm nw mo lq mp nx mr lt ms ny mu lw mv nz mx lz my bi translated">随机初始化:</h1><p id="9702" class="pw-post-body-paragraph kd ke iq kf b kg mz ki kj kk na km kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">15000 次迭代后的成本:0.38</p><p id="d7a6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">精确度:0.83</p><h1 id="8681" class="mi lj iq bd lk mj nv ml ln mm nw mo lq mp nx mr lt ms ny mu lw mv nz mx lz my bi translated">何等人初始化:</h1><p id="0ac0" class="pw-post-body-paragraph kd ke iq kf b kg mz ki kj kk na km kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">15000 次迭代后的成本:0.07</p><p id="0dc8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">精确度:0.96</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="1a2c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这本身显示了权重的初始化如何影响神经网络的性能。</p><p id="ac50" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看看 facebook 上的这个 rad 页面。</p><div class="oa ob gp gr oc nm"><a href="https://www.facebook.com/convolutionalmemes/" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">卷积青少年的机器学习迷因</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">卷积青少年的机器学习迷因。20K 赞。在它变深之前，我就已经在做了</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">www.facebook.com</p></div></div><div class="od l"><div class="oe l of og oh od oi jw nm"/></div></div></a></div><p id="ca74" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一如既往，快乐学习。</p></div></div>    
</body>
</html>