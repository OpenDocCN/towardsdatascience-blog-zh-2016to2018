# 会推理的机器人

> 原文：<https://towardsdatascience.com/robots-that-reason-b33c9851dfa9?source=collection_archive---------19----------------------->

**基于模型强化学习的无机知识传统**

![](img/95b3cb660d84e82bd1652ec7d9769c8e.png)

*这篇文章探索了无机知识传统的概念，它能够使用基于模型的强化学习进行连续改进*

许多行为经济学家目前认为，人类进行战略决策时主要使用两种方法。一种是快速、直觉和无意识的——被称为系统 1 思维。另一种是缓慢的、有逻辑的、循序渐进的，需要努力思考，被称为系统 2 思维。

越来越多的证据现在支持 DeepMind 等人推广的无模型强化学习的说法。所有这些在功能上都与来自[卡尼曼和特沃斯基的](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow)双重过程理论的系统 1 思维相对应。在我看来，这一发展没有得到应有的重视。考虑到系统 1 思维可能是迄今为止几乎所有高级动物认知形式的基础。各种各样的 RL 算法现在可以在功能上复制系统 1 的思维，这很可能会导致[机器人控制机制](https://deepmind.com/blog/learning-playing/)的复兴，至少如此。如果有人能像训练狗一样训练一个家庭机器人，那他能训练它做什么呢？可能是一些非常漂亮的东西，各种各样奇怪的东西，嘿，老实说，一些彻头彻尾的邪恶的东西！毕竟是人类在进行训练，而不是天使。

然而，这种前景忽略了强化学习领域一个更重要的发展，即可以在功能上复制双重过程理论中的系统 2 思维的算法——人类特别擅长的基于缓慢深思熟虑的模型的推理。不希望把它说得太细，系统 2 算法的全面开花可能会使一些关于奇点临近的更大胆的说法显得不太牵强。也就是说，本文的目的不是要说服奇点的怀疑论者，而是指出基于模型的强化学习的一些最新发展，并探索它们的含义。

关于直觉系统 1 的决策，当我们经历时，那些经历会改变我们未来的决策。直到最近，这种随时间改变和适应直觉决定的能力一直是生物智能的专属领域。这也是我们认为的“一般智力”的基础。这是一个激烈争论的话题，一些人认为没有“一般智力”这样的东西，只有许多种专门的智力。直觉上，我们中的许多人可能会有不同的感觉，认识到一个共同的线索，这就是我们学习如何骑自行车和征服 Atari 视频游戏的能力。一个可能的共同思路是无模型强化学习，大量的证据支持这一观点。

赋予计算机无模型强化学习带来的一个有问题的结果是，代理人没有办法知道它的策略为什么有效，也没有办法就此进行交流或推理。和进化一样，无模型强化学习的手也是瞎的。它可以优化，而不知道它做了什么导致了理想的结果。这是一个无意识的过程，因为主体不能抽象地推理环境中的特定元素如何与它的动作相结合来实现期望的结果。就像一只狗可以成功地操纵它的主人，却不知道为什么当它坐在枕头上做出愚蠢的表情时，食物会像雨点般落在它身上。狗只是知道过去什么工作得好，而不是它为什么工作的机制。因此，生成这些解的路径经常隐藏在代理为达到其目标状态而采用的随机变化中。

这使得复制或交流通过无模型强化学习产生的技能变得困难。在固定的环境中，在固定的回报下，人们会期望多个无模型 RL 算法在给定足够训练的情况下最终收敛到单个目标状态。这种收敛性的证明已经在 [SARSA 和 Q Learning](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-43/lib/idauction2/.g/web/glossary/converge.html) 中得到验证。然而，在一个丰富的真实世界环境中，有一个连续的动作空间，可能很难或不可能复制最终导致代理达到所需目标状态的动作的精确组合，就像试图将一个雪球滚下山，并让它遵循前一个雪球滚入的完全相同的路径-但滑雪者和雪地摩托车一直穿过这些相同的轨道。

另一方面，基于模型的强化学习的优势在于，当代理学习事物时，它创建了事物在环境中交互方式的抽象。然后，可以将这些信息传递给其他代理，或者存储起来供以后在其他环境中使用，从而加快相关技能的获取。因此，基于模型的强化学习的发展很可能构成人工智能领域的下一波进步。DeepMind 已经朝着这个目标取得了一些令人印象深刻的进展，推出了用于[“关系深度强化学习”](https://deepmind.com/research/publications/relational-deep-reinforcement-learning/)的算法。这些将关系网络与强化学习结合起来，允许代理推测环境中的[对象如何与它的动作](https://hackernoon.com/deepmind-relational-networks-demystified-b593e408b643)交互以产生期望的结果。

在许多方面，这种向基于模型的强化学习的转变概括了人类获得性文化策略的进化。以现代农业为例——正如贾雷德·戴蒙德在他的书《第三只黑猩猩》中指出的，这很可能始于一个完全无意识的过程，发生在人类采集野生植物的过程中，在这个过程中，不知不觉地将种子放在它们的家养围栏附近。其中一些种子后来发芽了，形成了第一个原始花园。因此，当人类开始除草、收割和重新种植那些曾经是野生的植物时，一个无意识模式的策略可能会逐渐产生一个基于有意识模式的策略。通过关系型深度强化学习，可以认为计算机正在经历类似的“大跃进”，从无模型的强化学习策略过渡到使用模型对环境进行推理和推测的策略。

其结果是，人类文化，我们物种的集体知识，可以转移和改进，可能很快就会在计算机科学和机器人技术中产生必然结果。由深度 RL 代理开发的关系网络可以逐渐改进，然后传播到其他代理。从理论上讲，这允许多个主体以类似于人类科学知识随着时间的推移而增长的方式，慢慢地提高他们对给定环境的集体理解，从而产生了我们目前生活的文明。