<html>
<head>
<title>What metrics should be used for evaluating a model on an imbalanced data set? (precision + recall or ROC=TPR+FPR)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在不平衡数据集上评估模型应该使用什么指标？(精确度+召回率或ROC=TPR+FPR)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba?source=collection_archive---------0-----------------------#2017-09-05">https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba?source=collection_archive---------0-----------------------#2017-09-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="5f84" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我一直认为度量的主题有点令人困惑，特别是当数据集不平衡时(这在我们的常见问题中经常发生)。为了澄清一些事情，我决定用不同类型的指标测试几个简单的不平衡数据集的例子，看看哪个更正确地反映了模型性能——ROC曲线指标——TPR和FPR或精度或召回。</p><h1 id="ef39" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">定义</h1><p id="5c4e" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">所以让我们从提醒自己这些定义开始。</p><p id="13ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae lo" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" rel="noopener ugc nofollow" target="_blank"> ROC曲线</a>中我们看:<br/> <strong class="jp ir"> TPR(真阳性率)= #真阳性/ #阳性=召回= TP / (TP+FN) <br/> FPR(假阳性率)= #假阳性/ #阴性= FP / (FP+TN) </strong></p><p id="f48d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里我们将关注单点的TPR(真阳性率)和FPR(假阳性率)(这将表明由TPR和FPR通过各种概率阈值组成的<a class="ae lo" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" rel="noopener ugc nofollow" target="_blank"> ROC曲线</a>的总体表现)。</p><p id="94f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lo" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">精度和召回率</a>分别是:<br/> <strong class="jp ir">精度=#真阳性/ #预测阳性= TP/(TP+FP) <br/>召回= #真阳性/ #阳性= TP / (TP+FN) </strong></p><h1 id="7189" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">有什么区别？</h1><p id="5332" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">召回率和真阳性率(TPR)是完全一样的。所以区别在于精度和假阳性率。</p><p id="77dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这两种度量的主要区别在于，精度分母包含假阳性，而假阳性率分母包含真阴性。【precision衡量分类为阳性的样本实际为阳性的概率，而假阳性率衡量阴性样本中假阳性的比率。</p><h1 id="d2ac" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">如果有大量阴性样本，精确度可能会更高</h1><p id="cefc" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">如果阴性样本的数量非常大(也称为不平衡数据集)，假阳性率增加得更慢。因为真正的负值(在fpr分母中— (FP+TN))可能会非常高，使这个度量变小。<br/>但是，Precision不受大量阴性样本的影响，因为它测量预测为阳性的样本中真正阳性的样本数(TP+FP)。</p><p id="6e60" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">精度在正类中比在负类中更受关注，它实际上测量<strong class="jp ir">正确检测正值的概率</strong>，而FPR和TPR (ROC度量)测量<strong class="jp ir">区分类的能力</strong>。</p><h1 id="54ad" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">例子</h1><p id="169c" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">这些例子只是用直观的图像来说明这些指标的一种方式。</p><p id="e5d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了可视化的目的，这里有一个简单的函数，它画出了一个给定模型的决策区域。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="f242" class="ly km iq lu b gy lz ma l mb mc"><strong class="lu ir">def plot_model_boundaries</strong>(model, xmin=0, xmax=1.5, ymin=0, ymax=1.5, npoints=40):<br/>    xx = np.linspace(xmin, xmax, npoints)<br/>    yy = np.linspace(ymin, ymax, npoints)<br/>    xv, yv = np.meshgrid(xx, yy)<br/>    xv, yv = xv.flatten(), yv.flatten()<br/>    labels = model.predict(np.c_[xv,yv])<br/>    plt.scatter(xv[labels==1],yv[labels==1],color='r', alpha=0.02, marker='o', s=300)<br/>    plt.scatter(xv[labels==0],yv[labels==0],color='b', alpha=0.02, marker='o', s=300)<br/>    plt.ylim([xmin, xmax])<br/>    plt.xlim([ymin, ymax])</span></pre><p id="ea3d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我创建了一个非常简单的数据集(10个点)，并训练了一个线性SVM模型。下图是数据集和模型的完美决策区域。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="3621" class="ly km iq lu b gy lz ma l mb mc"><strong class="lu ir">from </strong>matplotlib <strong class="lu ir">import </strong>pyplot <strong class="lu ir">as </strong>plt<br/><strong class="lu ir">from </strong>sklearn.svm <strong class="lu ir">import </strong>LinearSVC<br/><strong class="lu ir">from </strong>sklearn.metrics <strong class="lu ir">import </strong>precision_score, recall_score, roc_curve</span><span id="1b39" class="ly km iq lu b gy md ma l mb mc">np.random.seed(1)<br/>x = np.concatenate([np.zeros((8, 2)), np.zeros((1,2)), np.zeros((1,2))]) + 0.8*np.random.rand(10,2)<br/>x[-1,0]+=1<br/>x[-1,-1]+=1<br/>y = np.concatenate([np.ones(9), np.zeros(1)])<br/>model = LinearSVC()<br/>model.fit(x,y)<br/>predicted_labels = model.predict(x)<br/>plot_model_boundaries(model, xmin=0, xmax=1.5, ymin=0, ymax=1.5)<br/>plt.scatter(x[y==0,0],x[y==0,1], color='b')<br/>plt.scatter(x[y==1,0],x[y==1,1], color='r');<br/>fpr, tpr, thresholds = roc_curve(y, predicted_labels)<br/>plt.title("precision = {}, recall = {}, fpr = {}, tpr = {}".format(<br/>    precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr[0], tpr[0]));</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi me"><img src="../Images/4a1cda0337d727bb09e420b0f73098e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*Hp33aG4y3Tns724D00VvpA.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">My simple data set and the model decision region</figcaption></figure><h1 id="269d" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">示例#1 —大多数阳性样本和—所有阳性样本都被检测到，但也有假阳性— ROC是更好的度量</h1><p id="2316" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">我们在测试数据集中有10个样本。9个样本为阳性，1个为阴性。</p><p id="4e59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们之前看到的——在完美模型的情况下，所有指标都是完美的，但现在我们来看一个天真的模型，它预测<strong class="jp ir">一切都是积极的。</strong></p><p id="08eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这种情况下我们的基本度量是:<strong class="jp ir"> TP = 9，FP = 1，TN = 0，FN = 0。</strong>然后我们可以计算高级指标:</p><p id="a7c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">精度</strong> = TP/(TP+FP) = 0.9，<strong class="jp ir">召回</strong> = TP/(TP+FN)= 1.0。在这种情况下，准确率和召回率都很高，但是我们有一个很差的分类器。</p><p id="7de2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> TPR </strong> = TP/(TP+FN) = 1.0，<strong class="jp ir"> FPR </strong> = FP/(FP+TN) = 1.0。<br/> <strong class="jp ir">因为FPR很高，我们可以识别出这不是一个好的分类器。</strong></p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="fb93" class="ly km iq lu b gy lz ma l mb mc">np.random.seed(1)<br/>x = np.concatenate([np.zeros((8, 2)), np.zeros((1,2)), np.zeros((1,2))]) + 1.2*np.random.rand(10,2)<br/>x[-1,0]+=1<br/>x[-1,-1]+=1<br/>y = np.concatenate([np.ones(9), np.zeros(1)])<br/>model = LinearSVC()<br/>model.fit(x,y)<br/>predicted_labels = model.predict(x)<br/>plot_model_boundaries(model, xmin=0, xmax=1.5, ymin=0, ymax=1.5)<br/>plt.scatter(x[y==0,0],x[y==0,1], color='b')<br/>plt.scatter(x[y==1,0],x[y==1,1], color='r');<br/>fpr, tpr, thresholds = roc_curve(y, predicted_labels)<br/>plt.title("precision = {}, recall = {}, fpr = {}, tpr = {}".format(<br/>    precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr[1], tpr[1]));<br/></span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi me"><img src="../Images/e7b79dbcc6f1e0084a6179ac3cca817d.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*SavpJdQ-zl4c6VLBNl8ejQ.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">The classifier predicts all the labels as positive</figcaption></figure><h1 id="90b9" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">示例# 1—具有相反标签的相同数据集—两个指标都为0，因为没有“检测”</h1><p id="8b85" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">现在，我们交换标签——9个样本是阴性，1个是阳性。我们有一个预测一切负面的模型。我们现在的基本度量是:<strong class="jp ir"> TP = 0，FP = 0，TN = 9，FN = 1。因为没有真阳性和假阳性，所以高级指标都是零。</strong></p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="f2d2" class="ly km iq lu b gy lz ma l mb mc">np.random.seed(1)<br/>x = np.concatenate([np.zeros((8, 2)), np.zeros((1,2)), np.zeros((1,2))]) + 1.2*np.random.rand(10,2)<br/>x[-1,0]+=1<br/>x[-1,-1]+=1<br/>y = np.concatenate([np.zeros(9), np.ones(1)])<br/>model = LinearSVC()<br/>model.fit(x,y)<br/>predicted_labels = model.predict(x)<br/>plot_model_boundaries(model, xmin=0, xmax=1.5, ymin=0, ymax=1.5)<br/>plt.scatter(x[y==0,0],x[y==0,1], color='b')<br/>plt.scatter(x[y==1,0],x[y==1,1], color='r');<br/>fpr, tpr, thresholds = roc_curve(y, predicted_labels)<br/>plt.title("precision = {}, recall = {}, fpr = {}, tpr = {}".format(<br/>    precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr[0], tpr[0]));</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi me"><img src="../Images/c8f691627d2b08fee916b0c7aaec999f.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*kEDWVhFB0w9h8B9wB5oWww.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">The classifier predicts all the labels as negatives so all the metrics are zeros</figcaption></figure><h1 id="d1e3" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">示例2 —大多数阳性样本—所有阳性样本都被检测到，但也有假阳性— ROC是一个更好的指标</h1><p id="efe4" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">该示例类似于示例#1，并且显示了基本相同的内容。<br/>在这种情况下，8个样本为阳性，2个为阴性。<br/>该模型预测9个样品为阳性(8个真阳性和1个阴性)，1个为阴性。<br/>基本度量是:<strong class="jp ir"> TP = 8，FP = 1，TN = 1，FN = 0。</strong></p><p id="b0cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">高级指标包括:</p><p id="c3be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">精度= TP/(TP+FP) = 8/9 = 0.89，召回率= TP/(TP+FN)= 1。<br/> </strong>准确率和召回率都很高，因为在正类上表现不错。</p><p id="b997" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> TPR = TP/(TP+FN) = 1，FPR = FP/(FP+TN) = 1/2 = 0.5。<br/> </strong>假阳性率是0.5，这是相当高的，因为我们在两个阴性中有1个假阳性——这是很多的！请注意，高FPR实际上与负面类别的低回忆是一回事。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="8516" class="ly km iq lu b gy lz ma l mb mc">x = np.concatenate([np.zeros((8, 2)), np.zeros((1,2)), np.zeros((1,2))]) + 0.9*np.random.rand(10,2)<br/>x[-1,0]+=1<br/>x[-1,-1]+=1<br/>y = np.concatenate([np.zeros(1), np.ones(8), np.zeros(1)])<br/>model = LinearSVC()<br/>model.fit(x,y)<br/>predicted_labels = model.predict(x)<br/>plot_model_boundaries(model, xmin=0, xmax=2, ymin=0, ymax=2)<br/>plt.scatter(x[y==0,0],x[y==0,1], color='b')<br/>plt.scatter(x[y==1,0],x[y==1,1], color='r');<br/>fpr, tpr, thresholds = roc_curve(y, predicted_labels)<br/>plt.title("precision = {:.2f}, recall = {}, fpr = {}, tpr = {}".format(<br/>    precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr[1], tpr[1]));</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/51a61feb3237e282fa7ca4fe6b0e51d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*Hu6cTPMo9wvMWr-HUZ_VUA.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">One negative sample is classified correctly as negative and the other is classified as positive — this caused a small decrease in precision and a relatively high value in FPR</figcaption></figure><h1 id="eb77" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">示例2 —相反的标签—在这种情况下，两个指标是相同的</h1><p id="303f" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">现在8个样本为阴性，2个为阳性。我们有一个模型，除了一个阳性样本(实际上是阳性的)之外，它预测了所有的阴性样本。<br/>基本度量是:TP = 1，FP = 0，TN = 8，FN = 1。</p><p id="4c17" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">高级指标包括:</p><p id="a981" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">精度= TP/(TP+FP) = 1，召回率= TP/(TP+FN)= 1/(1+1) = 0.5。<br/> TPR = TP/(TP+FN) = 1/2 = 0.5，FPR = FP/(FP+TN) = 0。</strong></p><p id="77b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这种情况下，两个指标给出了相同的信息。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="e911" class="ly km iq lu b gy lz ma l mb mc">x = np.concatenate([np.zeros((8, 2)), np.zeros((1,2)), np.zeros((1,2))]) + 0.9*np.random.rand(10,2)<br/>x[-1,0]+=1<br/>x[-1,-1]+=1<br/>y = np.concatenate([np.ones(1), np.zeros(8), np.ones(1)])<br/>model = LinearSVC()<br/>model.fit(x,y)<br/>predicted_labels = model.predict(x)<br/>plot_model_boundaries(model, xmin=0, xmax=2, ymin=0, ymax=2)<br/>plt.scatter(x[y==0,0],x[y==0,1], color=’b’)<br/>plt.scatter(x[y==1,0],x[y==1,1], color=’r’);<br/>fpr, tpr, thresholds = roc_curve(y, predicted_labels)<br/>plt.title(“precision = {:.2f}, recall = {}, fpr = {}, tpr = {}”.format(<br/> precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr[0], tpr[0]));</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/cdb5c65772e434cfa790b50130350c28.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*m8pJ3m26ns1ygfy629Xcaw.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">One positive sample is detected correctly and the other is not</figcaption></figure><h1 id="9e1a" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">示例#3 —检测到大多数阳性样本，而不是所有阳性样本—在这种情况下，两个指标相同</h1><p id="1a89" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">现在，9个样本呈阳性，1个呈阴性。该模型预测7个样本为阳性(全部为阳性)，3个为阴性。<br/>基本度量有:<strong class="jp ir"> TP = 7，FP = 0，TN = 1，FN = 2。</strong></p><p id="5a92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">高级指标包括:</strong></p><p id="2236" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">精度= TP/(TP+FP) = 1，召回率= TP/(TP+FN)= 7/9 = 0.78 <br/> </strong>精度和召回率都很高，因为在正类上表现不错。<br/><strong class="jp ir">TPR = TP/(TP+FN)= 7/9 = 0.78，FPR = FP/(FP+TN) = 0。</strong></p><p id="ce53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这种情况下，两个指标给出了相似的结果。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="8456" class="ly km iq lu b gy lz ma l mb mc">np.random.seed(1)<br/>x = np.concatenate([np.zeros((8, 2)), np.zeros((1,2)), np.zeros((1,2))]) + 0.9*np.random.rand(10,2)<br/>x[-1,0]+=1<br/>x[-1,-1]+=1<br/>x[-2,0]+=0.8<br/>x[-2,-1]+=0.8<br/>x[-3,0]+=0.8<br/>x[-3,-1]+=0.8<br/>y = np.concatenate([np.zeros(7), np.ones(3)])<br/>y = 1-y<br/>model = LinearSVC()<br/>model.fit(x,y)<br/>y = np.concatenate([np.zeros(9), np.ones(1)])<br/>y = 1-y<br/>predicted_labels = model.predict(x)<br/>plot_model_boundaries(model, xmin=0, xmax=2, ymin=0, ymax=2)<br/>plt.scatter(x[y==0,0],x[y==0,1], color='b')<br/>plt.scatter(x[y==1,0],x[y==1,1], color='r');<br/>fpr, tpr, thresholds = roc_curve(y, predicted_labels)<br/>plt.title("precision = {:.2f}, recall = {:.2f}, fpr = {:.2f}, tpr = {:.2f}".format(<br/>    precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr[0], tpr[0]));</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/e218150278fcfc750c5016ca18df39d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*KosDzatxL8UH--OGX05fwg.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Both metrics are the same in this case</figcaption></figure><h1 id="2861" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">示例3 —相反的标签—检测到大部分阴性样本，而不是所有阳性样本—精确度和召回率更高</h1><p id="bfed" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">现在我们交换标签——9个样本为阴性，1个为阳性。<br/>模型预测3个样本为阳性(实际上只有一个为阳性)，7个为阴性。<br/>基本度量是:<strong class="jp ir"> TP = 1，FP = 2，TN = 7，FN = 0。</strong></p><p id="9b7c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">高级指标包括:</p><p id="5aee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">精度= TP/(TP+FP) = 0.33，召回率= TP/(TP+FN)= 1。</strong></p><p id="c61c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> TPR = TP/(TP+FN) = 1，FPR = FP/(FP+TN) = 2/9 = 0.22。</strong></p><p id="f96c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这种情况下，精度很低，FPR也没有我们希望的那么高。由于不平衡的数据集，FPR不高，因为我们有很多真正的否定。在这种情况下，较差的检测能力最好地反映在精度上。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="1e07" class="ly km iq lu b gy lz ma l mb mc">np.random.seed(1)<br/>x = np.concatenate([np.zeros((8, 2)), np.zeros((1,2)), np.zeros((1,2))]) + 0.9*np.random.rand(10,2)<br/>x[-1,0]+=1<br/>x[-1,-1]+=1<br/>x[-2,0]+=0.8<br/>x[-2,-1]+=0.8<br/>x[-3,0]+=0.8<br/>x[-3,-1]+=0.8<br/>y = np.concatenate([np.zeros(7), np.ones(3)])<br/>model = LinearSVC()<br/>model.fit(x,y)<br/>y = np.concatenate([np.zeros(9), np.ones(1)])<br/>predicted_labels = model.predict(x)<br/>plot_model_boundaries(model, xmin=0, xmax=2, ymin=0, ymax=2)<br/>plt.scatter(x[y==0,0],x[y==0,1], color='b')<br/>plt.scatter(x[y==1,0],x[y==1,1], color='r');<br/>fpr, tpr, thresholds = roc_curve(y, predicted_labels)<br/>plt.title("precision = {:.2f}, recall = {}, fpr = {:.2f}, tpr = {}".format(<br/>    precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr[1], tpr[1]));</span></pre><figure class="lp lq lr ls gt mf gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/15d65576c49cee3cc8f04613491106a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*sb-zaHnU4XYvGOaQNqZghw.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">In this case the poor detection ability is reflected best in the precision while the FPR is relatively not that high because of the large amount of negative samples</figcaption></figure><h1 id="ed95" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">度量选择的最终直觉</h1><ol class=""><li id="6f14" class="mn mo iq jp b jq lj ju lk jy mp kc mq kg mr kk ms mt mu mv bi translated"><strong class="jp ir">使用精度和召回率关注小阳性类别— </strong>当阳性类别较小时，正确检测阳性样本的能力是我们的主要关注点(正确检测阴性样本对问题不太重要)时，我们应该使用精度和召回率。</li><li id="f6da" class="mn mo iq jp b jq mw ju mx jy my kc mz kg na kk ms mt mu mv bi translated"><strong class="jp ir">当两个类别的检测同等重要时使用ROC—</strong>当我们想要对两个类别的预测能力给予同等的权重时，我们应该查看ROC曲线。</li><li id="0cd0" class="mn mo iq jp b jq mw ju mx jy my kc mz kg na kk ms mt mu mv bi translated"><strong class="jp ir">当阳性类别占多数时使用ROC，或者交换标签并使用精度和召回率— </strong>当阳性类别较大时，我们可能应该使用ROC指标，因为精度和召回率将主要反映阳性类别的预测能力，而不是阴性类别的预测能力，后者由于样本数量较少自然更难检测。如果负面类别(在这种情况下是少数)更重要，我们可以交换标签并使用precision和recall(正如我们在上面的例子中看到的那样——交换标签可以改变一切)。</li></ol></div></div>    
</body>
</html>