<html>
<head>
<title>Self Learning AI-Agents III:Deep (Double) Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自我学习人工智能代理 III:深度(双)Q 学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-double-q-learning-7fca410b193a?source=collection_archive---------2-----------------------#2018-11-04">https://towardsdatascience.com/deep-double-q-learning-7fca410b193a?source=collection_archive---------2-----------------------#2018-11-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="83ee" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">让深度 Q 学习再次变得伟大。关于自学习人工智能代理的系列文章的第三部分。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/a0dcb20cda5593f747118ce79d4b1410.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/1*iw6qqyBkbcBPeU36d8OSlg.gif"/></div></figure></div><div class="ab cl kq kr hx ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="im in io ip iq"><h2 id="05f7" class="kx ky it bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">自学习人工智能代理系列—目录</h2><ul class=""><li id="423e" class="lt lu it lv b lw lx ly lz lg ma lk mb lo mc md me mf mg mh bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f">第一部分:马尔可夫决策过程</a></li><li id="35e1" class="lt lu it lv b lw mj ly mk lg ml lk mm lo mn md me mf mg mh bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47">第二部分:深度 Q 学习</a></li><li id="dbdb" class="lt lu it lv b lw mj ly mk lg ml lk mm lo mn md me mf mg mh bi translated">第三部分:深(双)问学习(<strong class="lv iu">本文</strong>)</li><li id="fa8f" class="lt lu it lv b lw mj ly mk lg ml lk mm lo mn md me mf mg mh bi translated">第四部分:持续行动空间的政策梯度</li><li id="ae79" class="lt lu it lv b lw mj ly mk lg ml lk mm lo mn md me mf mg mh bi translated">第五部分:决斗网络</li><li id="9477" class="lt lu it lv b lw mj ly mk lg ml lk mm lo mn md me mf mg mh bi translated">第六部分:异步演员-评论家代理</li><li id="f4c9" class="lt lu it lv b lw mj ly mk lg ml lk mm lo mn md me mf mg mh bi">…</li></ul></div><div class="ab cl kq kr hx ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="im in io ip iq"><h2 id="8546" class="kx ky it bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">如果你喜欢这篇文章，想分享你的想法，问问题或保持联系，请随时通过 LinkedIn 与我联系。</h2></div><div class="ab cl kq kr hx ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="im in io ip iq"><h2 id="e54b" class="kx ky it bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">0.介绍</h2><p id="dee8" class="pw-post-body-paragraph mo mp it lv b lw lx ju mq ly lz jx mr lg ms mt mu lk mv mw mx lo my mz na md im bi translated">在“自我学习 AI-Agents”系列的第二篇文章中，我向您介绍了作为一种算法的深度 Q-Learning，它可以用来教会 AI 在离散动作空间中的行为和解决任务。然而，这种方法并不是没有缺点，有可能导致人工智能代理的性能降低。</p><p id="0b9b" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">在下文中，我将介绍一个常见的问题<strong class="lv iu">深度 Q-Learning </strong>，并向您展示如何将普通实现扩展到我们所说的<strong class="lv iu">双重深度 Q-Learning </strong>，这通常会提高 AI 代理的性能。</p></div><div class="ab cl kq kr hx ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="im in io ip iq"><h2 id="14ba" class="kx ky it bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">怎么才能练成深度双 Q 学习？</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ff7f32a170f15e3555b49fc8e3344348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*RhJKi88ZNLRowQbrhCFXFQ.gif"/></div></figure><blockquote class="nh ni nj"><p id="2a1e" class="mo mp nk lv b lw nb ju mq ly nc jx mr nl nd mt mu nm ne mw mx nn nf mz na md im bi translated">OpenAI 的体操横竿问题的这个例子是用这里介绍的双 Q 学习算法解决的——以及上一篇文章中的一些技术。文档齐全的源代码可以在我的<a class="ae mi" href="https://github.com/artem-oppermann/Deep-Reinforcement-Learning/tree/master/src/double%20q%20learning" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>中找到。我选择了 CartPole 作为一个例子，因为这个问题的训练时间非常短，你可以很快地自己重现它。克隆存储库并执行<strong class="lv iu"> run_training.py </strong>来启动算法。</p></blockquote></div><div class="ab cl kq kr hx ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="im in io ip iq"><h2 id="bde5" class="kx ky it bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">1.动作值函数</h2><p id="362d" class="pw-post-body-paragraph mo mp it lv b lw lx ju mq ly lz jx mr lg ms mt mu lk mv mw mx lo my mz na md im bi translated">在本系列的前两部分中，我介绍了动作值函数<strong class="lv iu"> <em class="nk"> Q(s，a) </em> </strong>作为期望回报<strong class="lv iu"><em class="nk">G _ t</em></strong>AI 代理将通过从状态<strong class="lv iu"> <em class="nk"> s </em> </strong>开始，采取动作<strong class="lv iu"> <em class="nk"> a </em> </strong>，然后遵循某个策略<strong class="lv iu"> <em class="nk"> π </em>来获得。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi no"><img src="../Images/a896b1fc5543304bac185016ec4562c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*2RFSBs8JwXQeiru3q5NoPA.png"/></div></div><figcaption class="nt nu gj gh gi nv nw bd b be z dk">Eq. 1 Action value function <strong class="bd nx">Q(s,a)</strong>.</figcaption></figure><p id="5b17" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">等式的右边部分也被称为<em class="nk">时间差目标</em>(TD-目标)。TD-Target 是代理人在状态<strong class="lv iu"><em class="nk"/></strong>中为动作<strong class="lv iu"> <em class="nk"> a </em> </strong>获得的即时奖励与贴现值<strong class="lv iu"><em class="nk">Q(s ')</em></strong><em class="nk">a '</em>之和，贴现值是代理人将在下一个状态<strong class="lv iu"> <em class="nk"> s' </em> </strong>中采取的动作。</p><p id="db1c" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated"><strong class="lv iu"> <em class="nk"> Q(s，a) </em> </strong>告诉代理一个可能动作的值(或质量)<strong class="lv iu"><em class="nk"/></strong><strong class="lv iu"><em class="nk">s</em></strong>。给定一个状态<strong class="lv iu"> <em class="nk"> s </em> </strong>，动作值函数计算该状态下每个可能动作<strong class="lv iu"> <em class="nk"> a_i </em> </strong>的质量/值作为标量值。更高的质量意味着对于给定的目标更好的行动。对于一个人工智能代理，一个可能的目标是学习如何走路或如何与人类棋手下棋。</p><p id="b43c" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">遵循贪婪策略 w.r.t <strong class="lv iu"> <em class="nk"> Q(s，a) </em> </strong>，意味着采取导致<strong class="lv iu"> Q(s，a’)</strong>的最高值的动作<strong class="lv iu"><em class="nk"/></strong>导致<em class="nk">贝尔曼最优性方程，</em>给出了<strong class="lv iu"> <em class="nk"> Q(s，a) </em> </strong> ( <a class="ae mi" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f">参见第一篇</a><a class="ae mi" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f">贝尔曼方程也可用于递归计算任何给定动作或状态的所有值<strong class="lv iu"><em class="nk">【Q(s，a)】</em></strong>。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi ny"><img src="../Images/07daedd2f08b9b8acc6374b0ce649f44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*3aJGB_0QqUELtVeUHOj9eQ.png"/></div></div><figcaption class="nt nu gj gh gi nv nw bd b be z dk">Eq.2 <em class="nz">Bellmann Optimality Equation.</em></figcaption></figure><p id="b2ca" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">在系列文章的<a class="ae mi" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47">第二篇文章中，介绍了<em class="nk">时间差异学习</em>作为估计值<strong class="lv iu"> <em class="nk"> Q(s，a) </em> </strong>的更好方法。时间差异学习的目标是最小化 TD 目标和<strong class="lv iu"><em class="nk">【s，a】</em></strong>之间的距离，这表明<strong class="lv iu"> <em class="nk"> Q(s，a) </em> </strong>向其在给定环境中的真实值收敛。这被称为<em class="nk"> Q-Learning </em>。</a></p><h2 id="5d35" class="kx ky it bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">2.深度 Q-网络</h2><p id="610b" class="pw-post-body-paragraph mo mp it lv b lw lx ju mq ly lz jx mr lg ms mt mu lk mv mw mx lo my mz na md im bi translated">我们已经看到，神经网络方法被证明是估计<strong class="lv iu"> <em class="nk"> Q(s，a) </em> </strong>的更好方法。主要目标保持不变。它是<strong class="lv iu"> <em class="nk"> Q(s，a) </em> </strong>与 TD-Target 的距离(或<strong class="lv iu"> <em class="nk"> Q(s，a) </em> </strong>的时态距离)的最小化。这个目标可以表示为误差损失函数的最小化:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi oa"><img src="../Images/d4e0036c7a35f141ea08c1b1bc9f7a6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zwdaxLofeJlrWTTcRZBcxg.png"/></div></div><figcaption class="nt nu gj gh gi nv nw bd b be z dk">Eq. 3 Squared error loss function.</figcaption></figure><p id="7bc3" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">在<strong class="lv iu">深度 Q 学习</strong>TD-Target<strong class="lv iu"><em class="nk">y _ I</em></strong>和<strong class="lv iu"> <em class="nk"> Q(s，a) </em> </strong>分别由两个不同的神经网络估计，这两个网络通常称为 Target-和 Q-网络(图 4)。参数<strong class="lv iu"> <em class="nk"> θ(i-1) </em> </strong>(权重、偏差)属于目标网络，而<strong class="lv iu"> <em class="nk"> θ(i) </em> </strong>属于 Q 网络。</p><p id="7554" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">根据<em class="nk">行为策略</em> <strong class="lv iu"> <em class="nk"> (a|s)选择 AI 代理的动作。</em> </strong>另一边，<strong class="lv iu"> <em class="nk"> </em> </strong>贪婪目标策略<strong class="lv iu"> <em class="nk"> </em> π(a|s) </strong>只选择动作<strong class="lv iu"><em class="nk">【a’</em></strong>即最大化<strong class="lv iu"><em class="nk">【Q(s)</em></strong>，即用于计算 TD-Target。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/db63fd8f2274be512e825e31a3895f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*Vd1kcpLoQDnM5vrKnvzxbw.png"/></div><figcaption class="nt nu gj gh gi nv nw bd b be z dk">Fig. 1 Target,- and Q-Network. <strong class="bd nx">s</strong> being the current and <strong class="bd nx">s’</strong> the next state.</figcaption></figure><p id="5acb" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">误差损失函数的最小化可以通过深度学习中使用的常用梯度下降算法来实现。</p></div><div class="ab cl kq kr hx ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="im in io ip iq"><blockquote class="nh ni nj"><p id="044c" class="mo mp nk lv b lw nb ju mq ly nc jx mr nl nd mt mu nm ne mw mx nn nf mz na md im bi translated"><strong class="lv iu"> <em class="it">即将推出</em> : </strong> <em class="it">面向软件开发人员、数据分析师、学者和行业专家的高级深度学习教育，旨在加快向人工智能职业的过渡。</em></p><p id="a051" class="mo mp nk lv b lw nb ju mq ly nc jx mr nl nd mt mu nm ne mw mx nn nf mz na md im bi translated"><em class="it">更多详情请看:</em><a class="ae mi" href="https://www.deeplearning-academy.com/" rel="noopener ugc nofollow" target="_blank"><em class="it">www.deeplearning-academy.com</em></a></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi oc"><img src="../Images/e610ab27a85f76f7836d01406052d725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9WtNs-E4aUH5YRhBnPjouA.png"/></div></div><figcaption class="nt nu gj gh gi nv nw bd b be z dk"><a class="ae mi" href="https://www.deeplearning-academy.com/" rel="noopener ugc nofollow" target="_blank">www.deeplearning-academy.com</a></figcaption></figure></div><div class="ab cl kq kr hx ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="im in io ip iq"><h2 id="2e1e" class="kx ky it bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">3.深度 Q 学习的问题</h2><p id="e597" class="pw-post-body-paragraph mo mp it lv b lw lx ju mq ly lz jx mr lg ms mt mu lk mv mw mx lo my mz na md im bi translated"><strong class="lv iu">深度 Q 学习</strong>已知有时会学习不切实际的高行动值，因为它包括对估计行动值的最大化步骤，这往往倾向于高估而不是低估的值，这可以在 TD-Target<strong class="lv iu"><em class="nk">y _ I</em></strong>的计算中看到。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/cb248d5ee31be44ad696322a843d1766.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*t0PDLYCufDcxdz4mmk05dA.png"/></div></figure><p id="0456" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">在实践中，高估是否会对人工智能代理的性能产生负面影响，这仍然是一个或多或少有待解决的问题。过于乐观的价值估计本身并不一定是一个问题。如果所有的值都一致地更高，那么相对的动作偏好被保留，并且我们不期望得到的策略会更差。</p><p id="8401" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">然而，如果高估并不一致，也没有集中在我们希望了解更多的州，那么它们可能会对最终政策的质量产生负面影响。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/6c1adb389e1d01aecda81d873d96fb11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*vd0L1FbRZXgRPxpnVHisGw.png"/></div></figure><h2 id="f329" class="kx ky it bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">4.双重深度 Q 学习</h2><p id="166f" class="pw-post-body-paragraph mo mp it lv b lw lx ju mq ly lz jx mr lg ms mt mu lk mv mw mx lo my mz na md im bi translated"><strong class="lv iu">双 Q 学习</strong>的思路是通过将目标中的<em class="nk"> max </em>运算分解为<strong class="lv iu">动作选择</strong>和<strong class="lv iu">动作评估</strong>来减少高估。</p><p id="f4f8" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">在普通实现中，动作选择和动作评估是耦合的。我们使用目标网络来选择行动，同时评估行动的质量。这意味着什么？</p><p id="1eb3" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">目标网络为状态<strong class="lv iu"> <em class="nk"> s </em> </strong>中的每个可能动作<strong class="lv iu"> <em class="nk"> a_i </em> </strong>计算<strong class="lv iu"> <em class="nk"> Q(s，a_i) </em> </strong>。贪婪策略决定选择最高值<strong class="lv iu"> <em class="nk"> Q(s，a_i) </em> </strong>哪个动作<strong class="lv iu"> <em class="nk"> a_i </em> </strong>。这意味着目标网络<strong class="lv iu">选择</strong>动作<strong class="lv iu"> <em class="nk"> a_i </em> </strong>，同时<strong class="lv iu">通过计算<strong class="lv iu"> <em class="nk"> Q(s，a_i)来评估</em></strong>其质量。双 Q 学习试图将这两个过程相互分离。</strong></p><p id="720e" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">在<strong class="lv iu">双 Q 学习</strong>中，TD 目标看起来如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi of"><img src="../Images/a323baedc3adbe7fbf7d139424fa5bb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b7q1hltlNSqiPCAajzJAEg.png"/></div></div></figure><p id="495c" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">正如您所看到的，目标中的最大运算消失了。虽然具有参数<strong class="lv iu"> <em class="nk"> θ(i-1) </em> </strong>的目标网络评估动作的质量，但是动作本身由具有参数<strong class="lv iu"> <em class="nk"> θ(i)的 Q 网络确定。</em> </strong>该过程与深度 Q 学习的普通实现形成对比，在深度 Q 学习中，目标网络负责动作选择和评估。</p><p id="d207" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">新 TD-Target<strong class="lv iu"><em class="nk">y _ I</em></strong>的计算可以总结为以下步骤:</p><ul class=""><li id="f821" class="lt lu it lv b lw nb ly nc lg og lk oh lo oi md me mf mg mh bi translated">Q-网络使用下一个状态<strong class="lv iu"><em class="nk">【s’</em></strong>来计算质量<strong class="lv iu"><em class="nk">Q(s’，a) </em> </strong>对于每个可能的动作<strong class="lv iu"> <em class="nk"> a </em> </strong>处于状态<strong class="lv iu"><em class="nk">【s’</em></strong></li><li id="5431" class="lt lu it lv b lw mj ly mk lg ml lk mm lo mn md me mf mg mh bi translated"><em class="nk"> argmax </em>操作应用<em class="nk"> </em>于<strong class="lv iu"><em class="nk">Q(s’，a) </em> </strong>选择属于最高质量的动作<strong class="lv iu"><em class="nk"/></strong>*<strong class="lv iu">动作选择</strong></li><li id="b473" class="lt lu it lv b lw mj ly mk lg ml lk mm lo mn md me mf mg mh bi translated">选择属于动作<strong class="lv iu"><em class="nk">【a*】</em></strong>【a *】(由目标网络确定)<strong class="lv iu"/>的质量<strong class="lv iu"> <em class="nk">(由 Q 网络确定)用于目标的计算。(<strong class="lv iu">动作评价</strong>)</em></strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/0878c7c93b4331e3e8f1315362dd4245.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*GbhVtWQ6LteFA2O1_XSK7w.png"/></div><figcaption class="nt nu gj gh gi nv nw bd b be z dk">Fig 2. Path of the agent through different states.</figcaption></figure><p id="69b7" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated"><strong class="lv iu">双 Q 学习</strong>的过程可以再一次以图形的形式可视化，以便进一步理解(图 2)。一个 AI 智能体开始时处于状态<strong class="lv iu"> <em class="nk"> s </em> </strong>。基于一些先前的计算，他知道在该状态下可能的两个动作的品质<strong class="lv iu"> <em class="nk"> Q(s，a_1) </em> </strong>和<strong class="lv iu"> <em class="nk"> Q(s，a_2) </em> </strong>。他决定采取行动<strong class="lv iu"> <em class="nk"> a_1 </em> </strong>并结束于状态<strong class="lv iu"><em class="nk">s’</em></strong>。</p><p id="58c7" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">Q-网络计算质量<strong class="lv iu"><em class="nk">Q(s’，a _ 1’)</em></strong>和<strong class="lv iu"> <em class="nk"> Q(s，a _ 2’)</em></strong>用于这个新状态中可能的动作。动作<strong class="lv iu"><em class="nk">a _ 1’</em></strong>被挑选，因为根据 Q 网络，它产生最高质量。</p><p id="54fa" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">状态<strong class="lv iu"><em class="nk"/></strong>中动作<strong class="lv iu"> <em class="nk"> a_1 </em> </strong>的新动作值<strong class="lv iu"> <em class="nk"> Q(s，a1) </em> </strong>现在可以用图 2 中的等式来计算，其中<strong class="lv iu"><em class="nk">Q(s’，a _ 1’)</em></strong>是由目标网络确定的<strong class="lv iu"><em class="nk">a _ 1’</em></strong>的评估。</p><h2 id="1a34" class="kx ky it bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">4.实证结果</h2><p id="2215" class="pw-post-body-paragraph mo mp it lv b lw lx ju mq ly lz jx mr lg ms mt mu lk mv mw mx lo my mz na md im bi translated">在[1] <em class="nk">中，David Silver 等人</em>在几款<em class="nk"> Atari 2600 </em>游戏上测试了深度 Q 网络(DQNs)和深度双 Q 网络(Double DQNs)。在图 3 中示出了由这两种方法的 AI 代理实现的标准化分数以及可比较的人类表现。该图还包含双 DQN 的调谐版本，其中执行了一些超参数优化。然而，这个版本的 DQN 将不在这里讨论。</p><p id="2210" class="pw-post-body-paragraph mo mp it lv b lw nb ju mq ly nc jx mr lg nd mt mu lk ne mw mx lo nf mz na md im bi translated">可以清楚地注意到，这两个不同版本的双 dqn 在这方面比它的普通实现获得了更好的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/8b41219f0bd3311e13817615414fd5d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*YakW0tYklA7tdD_0eYWgDw.png"/></div><figcaption class="nt nu gj gh gi nv nw bd b be z dk">Fig. 3 Performances on Atari 2600 games.</figcaption></figure></div></div>    
</body>
</html>