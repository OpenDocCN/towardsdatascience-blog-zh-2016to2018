<html>
<head>
<title>PySpark ML and XGBoost full integration tested on the Kaggle Titanic dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Kaggle Titanic 数据集上测试 PySpark ML 和 XGBoost 完全集成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pyspark-and-xgboost-integration-tested-on-the-kaggle-titanic-dataset-4e75a568bdb?source=collection_archive---------1-----------------------#2018-07-08">https://towardsdatascience.com/pyspark-and-xgboost-integration-tested-on-the-kaggle-titanic-dataset-4e75a568bdb?source=collection_archive---------1-----------------------#2018-07-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/0379e1596367290fd57b21a50e334594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OiM9iXjO4R8Vq_wCSfQZIg.png"/></div></div></figure><p id="d927" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在本教程中，我们将讨论如何使用标准的机器学习管道来集成 PySpark 和 XGBoost。</p><p id="dafd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们将使用来自“泰坦尼克号:机器从灾难中学习”的数据，这是众多 Kaggle 比赛之一，在 T2 举行。</p><p id="bb97" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">本文的新版本包括 PySpark 和 XGBoost 1.7.0+之间的原生集成，可以在</strong> <a class="ae kz" href="https://medium.com/@bogdan.cojocar/pyspark-integration-with-the-native-python-package-of-xgboost-3ac6d9082776" rel="noopener"> <strong class="kd iu">这里</strong> </a> <strong class="kd iu">找到。</strong></p><p id="78c2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在开始之前，请了解您应该熟悉<a class="ae kz" href="https://spark.apache.org" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>和<a class="ae kz" href="http://xgboost.readthedocs.io/en/latest/get_started/" rel="noopener ugc nofollow" target="_blank"> Xgboost </a>和 Python。</p><p id="7c2f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">本教程中使用的代码可以在 github 上的 Jupyther 笔记本中找到。</p><p id="8109" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你对本教程的设置有任何问题，你可以查看另一篇解释如何在<a class="ae kz" href="https://medium.com/@bogdan.cojocar/pyspark-ml-and-xgboost-setup-using-a-docker-image-e2e6122cae0f" rel="noopener"> docker </a>中运行的文章。</p><h2 id="458d" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">步骤 1:下载或构建 XGBoost jars</h2><p id="98c2" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">python 代码需要两个 scala jars 依赖项才能工作。您可以直接从 maven 下载它们:</p><ul class=""><li id="f833" class="ly lz it kd b ke kf ki kj km ma kq mb ku mc ky md me mf mg bi translated"><a class="ae kz" href="https://mvnrepository.com/artifact/ml.dmlc/xgboost4j/0.72" rel="noopener ugc nofollow" target="_blank"> xgboost4j </a></li><li id="f4b0" class="ly lz it kd b ke mh ki mi km mj kq mk ku ml ky md me mf mg bi translated"><a class="ae kz" href="https://mvnrepository.com/artifact/ml.dmlc/xgboost4j-spark/0.72" rel="noopener ugc nofollow" target="_blank"> xgboost4j-spark </a></li></ul><p id="8dda" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你想自己建造它们，你可以从我以前的教程<a class="ae kz" href="https://medium.com/@bogdan.cojocar/how-to-make-xgboost-available-in-the-spark-notebook-de14e425c948" rel="noopener">中找到方法。</a></p><h2 id="3b7e" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">步骤 2:下载 XGBoost python 包装器</h2><p id="310b" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">你可以从<a class="ae kz" href="https://github.com/dmlc/xgboost/files/2161553/sparkxgb.zip" rel="noopener ugc nofollow" target="_blank">这里</a>下载 PySpark XGBoost 代码。这是我们将要编写的部分和 XGBoost scala 实现之间的接口。我们将在教程的后面看到如何将它集成到代码中。</p><h2 id="b349" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">第三步:开始一个新的 Jupyter 笔记本</h2><p id="3610" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">我们将开始一个新的笔记本，以便能够编写我们的代码:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="8404" class="la lb it mr b gy mv mw l mx my">jupyter notebook </span></pre><h2 id="7e39" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">步骤 4:将定制的 XGBoost jars 添加到 Spark 应用程序中</h2><p id="198b" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">在启动 Spark 之前，我们需要添加之前下载的 jar。我们可以使用<code class="fe mz na nb mr b">--jars</code>标志来做到这一点:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="c404" class="la lb it mr b gy mv mw l mx my">import os<br/>os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars xgboost4j-spark-0.72.jar,xgboost4j-0.72.jar pyspark-shell'</span></pre><h2 id="9cb2" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">步骤 5:将 PySpark 集成到 Jupyther 笔记本中</h2><p id="add9" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">使 PySpark 可用的最简单的方法是使用<code class="fe mz na nb mr b"><a class="ae kz" href="https://github.com/minrk/findspark" rel="noopener ugc nofollow" target="_blank">findspark</a></code>包:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="a54e" class="la lb it mr b gy mv mw l mx my">import findspark<br/>findspark.init()</span></pre><h2 id="acb3" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">步骤 6:开始 spark 会话</h2><p id="2f1a" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">我们现在准备开始 spark 会话。我们正在创建一个 spark 应用程序，它将在本地运行，并将使用与使用<code class="fe mz na nb mr b">local[*]</code>的内核一样多的线程:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="2874" class="la lb it mr b gy mv mw l mx my">spark = SparkSession\<br/>        .builder\<br/>        .appName("PySpark XGBOOST Titanic")\<br/>        .master("local[*]")\<br/>        .getOrCreate()</span></pre><h2 id="4147" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">步骤 7:添加 PySpark XGBoost 包装器代码</h2><p id="0ad3" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">现在我们有了 spark 会话，我们可以添加之前下载的包装器代码:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="82c0" class="la lb it mr b gy mv mw l mx my">spark.sparkContext.addPyFile("YOUR_PATH/sparkxgb.zip")</span></pre><h2 id="cefe" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">步骤 8:定义模式</h2><p id="55a8" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">接下来，我们定义从 csv 读取的数据的模式。这通常是一个比让 spark 推断模式更好的实践，因为它消耗更少的资源，并且我们完全控制字段。</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="06bb" class="la lb it mr b gy mv mw l mx my">schema = StructType(<br/>  [StructField("PassengerId", DoubleType()),<br/>    StructField("Survival", DoubleType()),<br/>    StructField("Pclass", DoubleType()),<br/>    StructField("Name", StringType()),<br/>    StructField("Sex", StringType()),<br/>    StructField("Age", DoubleType()),<br/>    StructField("SibSp", DoubleType()),<br/>    StructField("Parch", DoubleType()),<br/>    StructField("Ticket", StringType()),<br/>    StructField("Fare", DoubleType()),<br/>    StructField("Cabin", StringType()),<br/>    StructField("Embarked", StringType())<br/>  ])</span></pre><h2 id="1f9f" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">步骤 9:将 csv 数据读入数据帧</h2><p id="34df" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">我们将 csv 读入一个<code class="fe mz na nb mr b">DataFrame</code>，确保我们提到我们有一个头，并且我们还用 0:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="55ff" class="la lb it mr b gy mv mw l mx my">df_raw = spark\<br/>  .read\<br/>  .option("header", "true")\<br/>  .schema(schema)\<br/>  .csv("YOUR_PATH/train.csv")</span><span id="bd0a" class="la lb it mr b gy nc mw l mx my">df = df_raw.na.fill(0)</span></pre><h2 id="e351" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">步骤 10: C <strong class="ak">将标称值转换为数值</strong></h2><p id="4968" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">在浏览这一步的代码之前，让我们简单地浏览一下 Spark ML 的一些概念。他们引入了 ML pipelines 的概念，ML pipelines 是建立在<code class="fe mz na nb mr b">DataFrames</code>之上的一组高级 API，使得将多个算法合并到一个进程中变得更加容易。管道的主要元件是<code class="fe mz na nb mr b">Transformer</code>和<code class="fe mz na nb mr b">Estimator</code>。第一个可以表示一个可以将一个<code class="fe mz na nb mr b">DataFrame</code>转换成另一个<code class="fe mz na nb mr b">DataFrame</code>的算法，而后者是一个可以适合一个<code class="fe mz na nb mr b">DataFrame</code>来产生一个<code class="fe mz na nb mr b">Transformer</code>的算法。</p><p id="8484" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了将标称值转换成数值，我们需要为每列定义一个<code class="fe mz na nb mr b">Transformer</code>:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="c59a" class="la lb it mr b gy mv mw l mx my">sexIndexer = StringIndexer()\<br/>  .setInputCol("Sex")\<br/>  .setOutputCol("SexIndex")\<br/>  .setHandleInvalid("keep")<br/>    <br/>cabinIndexer = StringIndexer()\<br/>  .setInputCol("Cabin")\<br/>  .setOutputCol("CabinIndex")\<br/>  .setHandleInvalid("keep")<br/>    <br/>embarkedIndexer = StringIndexer()\<br/>  .setInputCol("Embarked")\<br/>  .setOutputCol("EmbarkedIndex")\<br/>  .setHandleInvalid("keep")</span></pre><p id="49fd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们使用<code class="fe mz na nb mr b">StringIndexer</code>来转换这些值。对于每个<code class="fe mz na nb mr b">Transformer</code>,我们将定义包含修改值的输入列和输出列。</p><h2 id="192a" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">步骤 11: A <strong class="ak">将列组装成特征向量</strong></h2><p id="6b96" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">我们将使用另一个<code class="fe mz na nb mr b">Transformer</code>将 XGBoost <code class="fe mz na nb mr b">Estimator</code>分类中使用的列组装成一个向量:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="242c" class="la lb it mr b gy mv mw l mx my">vectorAssembler = VectorAssembler()\<br/>  .setInputCols(["Pclass", "SexIndex", "Age", "SibSp", "Parch", "Fare", "CabinIndex", "EmbarkedIndex"])\<br/>  .setOutputCol("features")</span></pre><h2 id="08b1" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">步骤 12:定义 XGBoostEstimator</h2><p id="b5a5" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">在这一步中，我们将定义产生模型的<code class="fe mz na nb mr b">Estimator</code>。这里使用的大多数参数都是默认的:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="0511" class="la lb it mr b gy mv mw l mx my">xgboost = XGBoostEstimator(<br/>    featuresCol="features", <br/>    labelCol="Survival", <br/>    predictionCol="prediction"<br/>)</span></pre><p id="5191" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们只定义了<code class="fe mz na nb mr b">feature, label</code>(必须匹配来自<code class="fe mz na nb mr b">DataFrame</code>的列)和包含分类器输出的新的<code class="fe mz na nb mr b">prediction</code>列。</p><h2 id="a58c" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">步骤 13: B <strong class="ak">建立管道和分类器</strong></h2><p id="64b3" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">在我们创建了所有单独的步骤之后，我们可以定义实际的管道和操作顺序:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="b4af" class="la lb it mr b gy mv mw l mx my">pipeline = Pipeline().setStages([sexIndexer, cabinIndexer, embarkedIndexer, vectorAssembler, xgboost])</span></pre><p id="4648" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">输入<code class="fe mz na nb mr b">DataFrame</code>将被转换多次，最终将产生用我们的数据训练的模型。</p><h2 id="7272" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">步骤 14:训练模型并根据新的测试数据进行预测</h2><p id="9471" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">我们首先将数据分为训练和测试，然后我们用训练数据拟合模型，最后我们看看我们为每个乘客获得了什么预测:</p><pre class="mm mn mo mp gt mq mr ms mt aw mu bi"><span id="a536" class="la lb it mr b gy mv mw l mx my">trainDF, testDF = df.randomSplit([0.8, 0.2], seed=24)</span><span id="6fb0" class="la lb it mr b gy nc mw l mx my">model = pipeline.fit(trainDF)</span><span id="83f2" class="la lb it mr b gy nc mw l mx my">model.transform(testDF).select(col("PassengerId"), col("prediction")).show()</span></pre></div></div>    
</body>
</html>