<html>
<head>
<title>25 Lights</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">25盏灯</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/25-lights-part-ii-e021b66e449b?source=collection_archive---------5-----------------------#2017-12-08">https://towardsdatascience.com/25-lights-part-ii-e021b66e449b?source=collection_archive---------5-----------------------#2017-12-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ae67" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">第二部分:使用真实世界的数据</h2></div><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="kk kl l"/></div></figure><p id="1023" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">在第<a class="ae li" rel="noopener" target="_blank" href="/25-lights-c4cc2c3f1832">部分</a> I中，我展示了一个简单的全连接自动编码器，可以用随机位模式再现图像。autoencoder是一个很好的选择，可以用来说明网络训练时发生了什么，并表明它在理论上是可行的。然而，一个简单的全连接网络在现实生活中是行不通的。这是因为将图像展平成一维会丢失很多空间信息。此外，一个单独的隐藏层并不能处理真实照片在大小、位置、旋转、模糊等方面的所有变化。</p><h2 id="eec1" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kv ls lt lu kz lv lw lx ld ly lz ma mb bi translated"><strong class="ak">处理照片</strong></h2><p id="446b" class="pw-post-body-paragraph km kn iq ko b kp mc jr kr ks md ju ku kv me kx ky kz mf lb lc ld mg lf lg lh ij bi translated">我的目标是构建一个演示iPhone应用程序，它可以拍摄模式的照片，并显示检测到的位模式。这意味着训练集将不得不使用照片，而不是纯粹的计算机生成的图像。处理真实世界的照片需要比第一篇文章更复杂的方法。幸运的是，在图像识别方面有大量的研究可以借鉴。<a class="ae li" href="http://www.image-net.org/challenges/LSVRC/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>竞赛的获胜者发布他们的结果，复制获胜的网络是一个很好的开始。一个用于迁移学习的流行网络是VGG-16。在我们设计中，我们可以大量借鉴网络。VGG-16是一个深度卷积网络(DCNN)。如果你对CNN不熟悉，我发现这个<a class="ae li" href="https://www.youtube.com/watch?v=FmpDIaiMIeA" rel="noopener ugc nofollow" target="_blank">视频</a>是一个极好的教程。简短的解释是，CNN使用一堆卷积滤波器，这些滤波器擅长提取空间信息和空间关系。</p><p id="5613" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">下面是我用过的网络。它不同于VGG，因为它使用平均池代替最大池，具有更少的层和不同的损失函数。</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="mh kl l"/></div></figure><p id="d40f" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">对于笔记本的其余部分，请查看<a class="ae li" href="https://github.com/briandw/25Lights" rel="noopener ugc nofollow" target="_blank">完整的jupyter笔记本和支持工具</a>。我已经包括了macOS图像服务器、iOS图像记录器和最终的iOS识别应用程序。</p><h2 id="e1e4" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kv ls lt lu kz lv lw lx ld ly lz ma mb bi translated">创建培训和验证集</h2><p id="fee7" class="pw-post-body-paragraph km kn iq ko b kp mc jr kr ks md ju ku kv me kx ky kz mf lb lc ld mg lf lg lh ij bi translated">现在我们已经建立了我们的网络，我们需要更好的数据来训练。在我们的第一个例子中，我们使用了按需绘制的简单图像。这些照片太干净了，没有一张真正的照片会有的任何不规则性。由于所需的数量，使用真实照片是一个挑战。我最初尝试使用纯合成图像，但未能通过真实照片验证。然后，我创建了一个大约15k照片的训练集。这些照片是由一部指向电脑屏幕的iPhone拍摄的。手机通过本地网络连接检索位模式，然后将图片保存到磁盘。直到大约90k的图像被包含在训练集中，我才能够得到好的结果。为了提高训练的质量，对图像应用了随机仿射变换。应用此变换可以移动、缩放和倾斜图像。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mi"><img src="../Images/55921419efd2cb0c64dce766f4fc4278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cNjULs1OYlHaDsmnVXe2VQ.jpeg"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk">iOS app capturing images from the screen</figcaption></figure><p id="8752" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">收集完图像后，我保留了其中的10%作为验证集。一个好的验证集将使你知道训练进展如何，以及要达到你的目标还需要哪些额外的步骤。fast.ai的Rachel Thomas有一篇关于这个话题的优秀文章。</p><h2 id="b25d" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kv ls lt lu kz lv lw lx ld ly lz ma mb bi translated">过度拟合</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mt"><img src="../Images/b8198a5cdf3c51191439b8cd29121cb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m1TocH-TeB4TsbUgpSzKiQ.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk">This is what overfitting looks like</figcaption></figure><p id="a5a0" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我第一次尝试使用一个类似VGG的网络，并没有包括任何正规化。这显然是个错误。上图是过度拟合的经典案例。左边的<strong class="ko ir">损失</strong>图显示了网络在训练数据上的表现。右边的<strong class="ko ir"> val_loss </strong>显示了网络如何处理验证集。我喜欢认为神经网络是懒惰的骗子，它们会利用任何优势来获得正确的答案。一个大的网络和一个小的训练集必然会导致过度拟合。最初的训练集是10k张图像。在这种尺寸下，网络更容易“记住”每张图像的值，而不是学会以概括的方式看到这些点。</p><h2 id="8998" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kv ls lt lu kz lv lw lx ld ly lz ma mb bi translated">如何避免过度拟合</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mu"><img src="../Images/013b24cecb3267f7b369f43ba2ea7744.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3vvxmEcupDQlnX9VZmiKWg.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk">The same data as above, but with batch normalization and dropout</figcaption></figure><p id="b15f" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">避免过度拟合的一些技巧是:</p><ul class=""><li id="c31f" class="mv mw iq ko b kp kq ks kt kv mx kz my ld mz lh na nb nc nd bi translated">更大的训练集(更大的训练集、数据论证、更多样的输入源)</li><li id="d132" class="mv mw iq ko b kp ne ks nf kv ng kz nh ld ni lh na nb nc nd bi translated">正规化(批量正规化，退出)</li><li id="f989" class="mv mw iq ko b kp ne ks nf kv ng kz nh ld ni lh na nb nc nd bi translated">改变网络拓扑(使网络变小)</li></ul><p id="a597" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">增加漏失和批量归一化非常有效地防止了第一张图中看到的失控过拟合。然而，数据集仍然太小，无法在验证集上获得好的结果。早期训练仅导致0.4的验证损失和0.1以下的训练损失。这个差异被称为<a class="ae li" href="https://en.wikipedia.org/wiki/Bias–variance_tradeoff" rel="noopener ugc nofollow" target="_blank">差异</a>。要获得好的结果，需要低于0.02的验证损失。当使用更大、更多样的训练集时，可以获得更好的结果。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nj"><img src="../Images/edc3014a7f2f00555ea2eed18b233753.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-2bhIe7l1tmQo7jDNlIs-w.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk">Final training run</figcaption></figure><h2 id="e607" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kv ls lt lu kz lv lw lx ld ly lz ma mb bi translated">调试网络</h2><p id="27b5" class="pw-post-body-paragraph km kn iq ko b kp mc jr kr ks md ju ku kv me kx ky kz mf lb lc ld mg lf lg lh ij bi translated">一旦训练达到一个合理的表现水平，检查网络出错的情况是一个好主意。对这些情况进行简单的目视检查通常就足够了。</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="mh kl l"/></div></figure><p id="f459" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">第一件事是计算有多少图像有一个或多个不正确的位。大约92%的图像是完全正确的。那么剩下的8%是怎么回事呢？以下是一些错过的图片:</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nk"><img src="../Images/c42ba4a1479075853f024c0d308fa81b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MApHH-vmZ3dMraBR8PIwKA.png"/></div></div></figure><p id="026d" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">有一堆重影和模糊的照片，合理的东西要错过。我回去删除了所有有重影或其他主要问题的照片，然后从头开始重新运行培训。结果如下:</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nl"><img src="../Images/f8863abe06fca21aff8d4a4177db1302.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s8YvFbYR8Juv66HPQvSlmw.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk">Final loss of 0.04 and validation loss of 0.02</figcaption></figure><p id="3dd1" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">最终错误率为97%。如果有必要接近100%,就需要更多种类的图像，集中在当前网络做得不好的情况下。</p><h2 id="8ab1" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kv ls lt lu kz lv lw lx ld ly lz ma mb bi translated">iPhone实现和CoreML</h2><p id="9630" class="pw-post-body-paragraph km kn iq ko b kp mc jr kr ks md ju ku kv me kx ky kz mf lb lc ld mg lf lg lh ij bi translated">我打算写第三篇关于用训练好的模型制作一个iOS应用的文章。然而，它真的很简单，不值得一提。</p><p id="719c" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">一旦你训练好你的模型，安装<a class="ae li" href="https://pypi.python.org/pypi/coremltools" rel="noopener ugc nofollow" target="_blank"> CoreML工具</a>。你需要有一个Python 2.7环境，因为CoreML不支持Python 3。然后在您的笔记本中创建一个新的单元格，并运行以下命令。</p><pre class="kf kg kh ki gt nm nn no np aw nq bi"><span id="6ff3" class="lj lk iq nn b gy nr ns l nt nu">import coremltools</span><span id="dd85" class="lj lk iq nn b gy nv ns l nt nu">coreml_model = coremltools.converters.keras.convert(model)<br/>coreml_model.save(“Lights.mlmodel”)</span></pre><p id="4db8" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">一旦您导出了您的模型，创建一个新的项目并导入它。唯一棘手的部分是将图像转换成CoreML模型的输入。</p><p id="327c" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">要将图像转换为CoreML:</p><ol class=""><li id="ce22" class="mv mw iq ko b kp kq ks kt kv mx kz my ld mz lh nw nb nc nd bi translated">按照我们的模型输入的形状制作一个多层阵列。在这种情况下是1×128×128，32位浮点。</li><li id="f3e4" class="mv mw iq ko b kp ne ks nf kv ng kz nh ld ni lh nw nb nc nd bi translated">将图像渲染到8位灰度绘图环境中。</li><li id="71d7" class="mv mw iq ko b kp ne ks nf kv ng kz nh ld ni lh nw nb nc nd bi translated">提取字节并将其转换为浮点数，然后将其写入数组。</li><li id="f372" class="mv mw iq ko b kp ne ks nf kv ng kz nh ld ni lh nw nb nc nd bi translated">为模型创建一个输入对象，并从MLModels predictionFromFeatures方法获取预测。</li></ol><p id="c828" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">更多细节请看一下<a class="ae li" href="https://gist.github.com/briandw/3662437e975ace767caa9ef27d9d6712" rel="noopener ugc nofollow" target="_blank"> iOS CoreML代码</a>。</p><p id="9dd8" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">关于如何使用CoreML模型的完整教程，请查看<a class="ae li" href="https://www.raywenderlich.com/164213/coreml-and-vision-machine-learning-in-ios-11-tutorial" rel="noopener ugc nofollow" target="_blank"> CoreML和视觉机器学习</a>。</p><h2 id="ff0f" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kv ls lt lu kz lv lw lx ld ly lz ma mb bi translated">结论</h2><p id="93d8" class="pw-post-body-paragraph km kn iq ko b kp mc jr kr ks md ju ku kv me kx ky kz mf lb lc ld mg lf lg lh ij bi translated">用错误的方法解决这个问题太麻烦了！有很多图像识别系统的例子可以在不使用深度卷积网络的情况下完成这种任务。传统的CV技术可以用更小的二进制数更快地解决这个问题。然而，这是一个非常有趣的学习练习，帮助我找到了如何解决新的ML任务带来的问题。</p><p id="5b9c" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">当我把这个问题作为一个ML训练练习来考虑时，我不确定它是否可行。我遇到的所有例子都是分类任务，比如ImageNet。这些分类任务有一个最终的热门矢量输出，并使用了损失函数，如分类交叉熵。在对其他网络做了更多的阅读并对损失函数有了更好的理解后，我意识到像二进制交叉熵这样的东西应该是可行的。事后看来，我应该<a class="ae li" href="http://yeephycho.github.io/2017/09/16/Loss-Functions-In-Deep-Learning/" rel="noopener ugc nofollow" target="_blank">更早</a>开始详细阅读损失函数。</p><p id="1d18" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">所需的训练数据的大小和呼吸也有点令人兴奋。与其他训练集相比，超过85，000个示例并不算大，但仍然非常大。用<a class="ae li" href="https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b" rel="noopener">胶囊网络</a>再试一次会很有趣。胶囊网络应该具有更好的相对位置感，并且需要更少的例子。</p><p id="e8c0" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">把我学会使用的所有技术放在一起，看着它们工作是非常令人满意的。<a class="ae li" href="http://www.andrewng.org/" rel="noopener ugc nofollow" target="_blank">吴恩达的</a>和<a class="ae li" href="http://course.fast.ai/" rel="noopener ugc nofollow" target="_blank">杰瑞米·霍华德的</a>视频课程是很好的向导。在大多数ML课程材料中，有大量的时间花在正则化和管理你的训练数据和验证集上。我希望我已经证明了这是有充分理由的。感谢您阅读我的项目。我很想在下面的评论中听到关于这篇文章的任何评论或建议。</p></div></div>    
</body>
</html>