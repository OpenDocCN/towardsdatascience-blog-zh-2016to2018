<html>
<head>
<title>Building a Custom Mask RCNN model with Tensorflow Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">建立带有张量流对象检测的定制掩模 RCNN 模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-custom-mask-rcnn-model-with-tensorflow-object-detection-952f5b0c7ab4?source=collection_archive---------3-----------------------#2018-05-09">https://towardsdatascience.com/building-a-custom-mask-rcnn-model-with-tensorflow-object-detection-952f5b0c7ab4?source=collection_archive---------3-----------------------#2018-05-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="dbd8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">用数据做酷事！</em></p><p id="bdd5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您现在可以使用<a class="ae km" href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="noopener ugc nofollow" target="_blank"> Tensorflow 对象检测库</a>构建一个自定义的遮罩 RCNN 模型！掩模 RCNN 是一个实例分割模型，可以逐个像素地识别任何对象的位置。本文是我广受欢迎的<a class="ae km" rel="noopener" target="_blank" href="/using-tensorflow-object-detection-to-do-pixel-wise-classification-702bf2605182">帖子</a>的第二部分，在这里我解释了 Mask RCNN 模型的基础知识，并在视频上应用了一个预先训练好的 Mask 模型。</p><p id="d6bc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练掩模模型比训练对象检测模型稍微复杂一些，因为在训练时也需要对象掩模。我花了一些迭代来弄清楚这个过程，我在这里分享了关键的细节。我在一个玩具上训练了一个面具 RCNN 模型。参见下面的演示:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi kn"><img src="../Images/cf11cb8555e1f03a42daca9a1843674f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*_NEbboVQx7fEkbw-oUEGFw.gif"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Custom Mask RCNN Model on a toy</figcaption></figure><p id="1e7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以在我的<a class="ae km" href="https://github.com/priya-dwivedi/Deep-Learning/tree/master/Custom_Mask_RCNN" rel="noopener ugc nofollow" target="_blank"> Github repo </a>上找到代码。</p><p id="3bd3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你有一个有趣的项目需要帮助，请联系我在 priya.toronto3@gmail.com</p><p id="f89a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 1)收集数据并创建掩码</strong></p><p id="22d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">常规的对象检测模型要求您使用边界框来注释图像中的对象。然而，蒙版模型的输入是带有蒙版的 PNG 文件。请参见下面的示例:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi kz"><img src="../Images/79e4576cbb8012fd98ea678e9a094990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*j5NjITF-vat4IjlJoRsarA.png"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Object Mask — Toy</figcaption></figure><p id="2abd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">利用这个二进制掩模图像，模型可以提取边界框的坐标以及对象的像素位置。</p><p id="3020" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我用来创建蒙版的工具是<a class="ae km" href="https://github.com/abreheret/PixelAnnotationTool" rel="noopener ugc nofollow" target="_blank">像素注释工具</a>。这个工具的输出是 API 想要的格式的 PNG 文件。您可以在注释工具中打开图像，并使用画笔给玩具“上色”。将外部着色并标记为感兴趣区域之外也很重要。我花了大约 20 秒的时间来着色和保存每一张还不错的蒙版图片。如果你想让蒙版非常精确，那么在边缘使用精细的笔刷。通过我的实验，我观察到训练一个掩模 RCNN 模型比训练一个更快的 RCNN 模型需要更少的图像来达到相同的精度。</p><p id="f82c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 2。生成 TF 记录</strong></p><p id="0fb6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Tensorflow 对象检测模型的输入是一个 TFRecord 文件，您可以将其视为图像、边界框、遮罩等的压缩表示，以便在训练模型时将所有信息放在一个地方。创建这个文件最简单的方法是使用一个类似的脚本，该脚本可用于 pet 数据集的<a class="ae km" href="https://github.com/tensorflow/models/blob/master/research/object_detection/dataset_tools/create_pet_tf_record.py" rel="noopener ugc nofollow" target="_blank"> TFRecord，并针对我们的情况对其进行一点修改。我已经分享了我在 Github repo 上使用的脚本。</a></p><p id="e5ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您还需要创建一个 label.pbtxt 文件，用于将标签名称转换为数字 id。对我来说，这很简单</p><pre class="ko kp kq kr gt la lb lc ld aw le bi"><span id="8e9f" class="lf lg iq lb b gy lh li l lj lk">item {<br/> id: 1<br/> name: ‘toy’<br/>}</span></pre><p id="f534" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 3。选择模型超参数</strong></p><p id="2634" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在你可以选择你想要使用的遮罩模型。Tensorflow API 提供了<a class="ae km" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" rel="noopener ugc nofollow" target="_blank"> 4 个模型选项</a>。我选择了掩模 RCNN 盗梦 V2，这意味着盗梦 V2 被用作特征提取器。该模型在推理时间上是最快的，尽管它可能不具有最高的准确性。模型参数存储在配置文件中。我为相同类型的 coco 模型使用了<a class="ae km" href="https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config" rel="noopener ugc nofollow" target="_blank">配置文件</a>,并更新了类的数量和路径，使大多数模型参数保持不变。</p><p id="4a09" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 4。训练模型</strong></p><p id="113b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">锁定输入文件和参数后，您可以开始培训。我能够在几个小时内在 CPU 上训练这个模型。您可以同时在两个独立的终端上启动培训作业和评估作业。启动 tensorboard 监控性能。当我看到损失趋于平稳时，我停止了训练。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/489fc087ec1831352db512f37f0f61a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*nH4KtaO9WrEGc0mSjUBoqw.png"/></div></figure><p id="fd44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Tensorboard 中最酷的事情是，随着训练的进行，它允许你在测试集的样本图像上可视化预测。下面的 gif 显示了随着训练的进行，模型变得确定其遮罩和边界框预测。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/c9c38d2a89a74418a729eb4e2244e9b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/1*lOa5FtWt8HoYlWI77izr6Q.gif"/></div></figure><p id="430b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 5。在您的定制视频上测试模型</strong></p><p id="b0c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了测试模型，我们首先选择一个模型检查点(通常是最新的)并将其导出到一个冻结的推理图中。这个脚本也在我的 github 上。我在我的 Iphone 上录制的新视频中测试了这个模型。正如在我的<a class="ae km" href="https://medium.com/towards-data-science/is-google-tensorflow-object-detection-api-the-easiest-way-to-implement-image-recognition-a8bd1f500ea0" rel="noopener">上一篇</a>文章中，我使用 Python moviepy 库将视频解析成帧，然后在每一帧上运行 object detector，并将结果整理回视频中。</p><h1 id="3bfc" class="ln lg iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">后续步骤</h1><p id="eabc" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">未来的额外探索</p><ul class=""><li id="6713" class="mp mq iq jp b jq jr ju jv jy mr kc ms kg mt kk mu mv mw mx bi translated">我想将这个模型扩展到同一张图片中的多个类别的物体。TFRecord creator 脚本需要一些修改，这样它才能正确地为每个对象分配正确的标签和掩码</li><li id="b5a8" class="mp mq iq jp b jq my ju mz jy na kc nb kg nc kk mu mv mw mx bi translated">正如我提到的，我在这个项目中使用了最轻量级的模型。我很想看看该套件中速度较慢的其他型号在检测准确性方面的表现</li></ul><p id="c7ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">给我一个❤️，如果你喜欢这个职位:)希望你拉代码，并尝试自己。</p><p id="5012" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我有自己的深度学习咨询公司，喜欢研究有趣的问题。我已经帮助许多初创公司部署了基于人工智能的创新解决方案。请到 http://deeplearninganalytics.org/来看看我们吧。</p><p id="1dad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你也可以在 https://medium.com/@priya.dwivedi 的<a class="ae km" href="https://medium.com/@priya.dwivedi" rel="noopener">看到我的其他作品</a></p><p id="38c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你有一个我们可以合作的项目，请通过我的网站或 info@deeplearninganalytics.org 联系我</p><p id="4308" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">参考文献:</strong></p><ul class=""><li id="bc45" class="mp mq iq jp b jq jr ju jv jy mr kc ms kg mt kk mu mv mw mx bi translated"><a class="ae km" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank">蒙版 RCNN 纸</a></li><li id="7e76" class="mp mq iq jp b jq my ju mz jy na kc nb kg nc kk mu mv mw mx bi translated"><a class="ae km" href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="noopener ugc nofollow" target="_blank">谷歌 Tensorflow 物体检测 Github </a></li><li id="33c2" class="mp mq iq jp b jq my ju mz jy na kc nb kg nc kk mu mv mw mx bi translated"><a class="ae km" href="http://mscoco.org/home/" rel="noopener ugc nofollow" target="_blank"> COCO 数据集</a></li><li id="0de2" class="mp mq iq jp b jq my ju mz jy na kc nb kg nc kk mu mv mw mx bi translated"><a class="ae km" href="https://stackoverflow.com/questions/33947823/what-is-semantic-segmentation-compared-to-segmentation-and-scene-labeling" rel="noopener ugc nofollow" target="_blank">了解黑白实例切分和语义切分的区别</a></li><li id="3fc2" class="mp mq iq jp b jq my ju mz jy na kc nb kg nc kk mu mv mw mx bi translated">非常好的<a class="ae km" href="https://www.youtube.com/watch?v=UdZnhZrM2vQ&amp;t=111s" rel="noopener ugc nofollow" target="_blank">解释</a>屏蔽 RCNN</li></ul></div></div>    
</body>
</html>