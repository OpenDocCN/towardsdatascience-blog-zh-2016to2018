<html>
<head>
<title>Artificial Intelligence meets Art: Neural Transfer Style</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能遇见艺术:神经传递风格</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/artificial-intelligence-meets-art-neural-transfer-style-50e1c07aa7f7?source=collection_archive---------7-----------------------#2018-11-08">https://towardsdatascience.com/artificial-intelligence-meets-art-neural-transfer-style-50e1c07aa7f7?source=collection_archive---------7-----------------------#2018-11-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/0becfa773bde9af462af48fdbc03de3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p06Rgst1a_7mXl8-N_hXYQ.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Hokusai in Boston</figcaption></figure><h1 id="07dc" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">介绍</h1><p id="4db6" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">神经转移风格是人工智能在创造性背景下最令人惊叹的应用之一。在这个项目中，我们将看到如何将艺术绘画风格转移到选定的图像上，创造出令人惊叹的效果。Leon A. Gatys 等人在 2015 年的论文<a class="ae ly" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank"> <em class="lz">中构思了<strong class="lc ir">神经传递风格</strong>的概念，一种艺术风格</em> </a>的神经算法。在那之后，许多研究人员应用并改进了这种方法，增加了损失的元素，尝试了不同的优化器，并试验了用于此目的的不同神经网络。<br/>尽管如此，原始论文仍然是理解这一概念的最佳来源，VGG16 和 VGG19 网络是这方面最常用的模型。这种选择是不寻常的，考虑到两者都被最近的网络超越，在风格转移中实现的最高性能证明了这一点。</p><p id="20dc" class="pw-post-body-paragraph la lb iq lc b ld ma lf lg lh mb lj lk ll mc ln lo lp md lr ls lt me lv lw lx ij bi translated">完整代码可以查看这个<a class="ae ly" href="https://github.com/maurock/neural_transfer_style" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ir"> GitHub 库</strong> </a> <strong class="lc ir"> </strong>。</p><h1 id="4685" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">它是如何工作的？</h1><p id="6600" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这种技术的目标是将图像的样式(我们称之为“样式图像”)应用到目标图像，保留后者的内容。让我们定义这两个术语:</p><ul class=""><li id="d081" class="mf mg iq lc b ld ma lh mb ll mh lp mi lt mj lx mk ml mm mn bi translated"><strong class="lc ir">风格</strong>是图像中的纹理和视觉模式。一个例子是艺术家的笔触。</li><li id="4a27" class="mf mg iq lc b ld mo lh mp ll mq lp mr lt ms lx mk ml mm mn bi translated"><strong class="lc ir">内容</strong>是一幅图像的宏观结构。人、建筑物、物体都是图像内容的例子。</li></ul><p id="55c0" class="pw-post-body-paragraph la lb iq lc b ld ma lf lg lh mb lj lk ll mc ln lo lp md lr ls lt me lv lw lx ij bi translated">令人惊叹的效果如下所示:</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/4da9bac3d1d2ce1f495d9d539a411e9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*tBCPZpaGYX7gTobCMpAp_Q.gif"/></div></div></figure><blockquote class="my"><p id="7fef" class="mz na iq bd nb nc nd ne nf ng nh lx dk translated">你想看到更多的效果吗？在文章的最后检查他们！</p></blockquote><p id="3f91" class="pw-post-body-paragraph la lb iq lc b ld ni lf lg lh nj lj lk ll nk ln lo lp nl lr ls lt nm lv lw lx ij bi translated">让我们看看高级步骤:</p><ul class=""><li id="b00b" class="mf mg iq lc b ld ma lh mb ll mh lp mi lt mj lx mk ml mm mn bi translated">选择要样式化的图像</li><li id="9219" class="mf mg iq lc b ld mo lh mp ll mq lp mr lt ms lx mk ml mm mn bi translated">选择样式参考图像。通常，这是一幅风格奇特且易于辨认的画。</li><li id="e039" class="mf mg iq lc b ld mo lh mp ll mq lp mr lt ms lx mk ml mm mn bi translated">初始化预训练的深度神经网络，并获得中间层的特征表示。完成该步骤是为了实现内容图像和样式图像的表示。在内容图像中，最好的选择是获得最高层的特征表示，因为它们包含关于图像宏观结构的信息。对于样式参考影像，从不同比例的多个图层中获取要素制图表达。</li><li id="fe03" class="mf mg iq lc b ld mo lh mp ll mq lp mr lt ms lx mk ml mm mn bi translated">将最小化的损失函数定义为<em class="lz">内容损失</em>、<em class="lz">风格损失</em>和<em class="lz">变化损失</em>之和。每次迭代，优化器都会生成一幅图像。内容损失是生成图像和内容图像之间的差异(l2 归一化)，而样式损失是生成图像和样式之间的差异。我们稍后会看到这些变量是如何被数学定义的。</li><li id="b10b" class="mf mg iq lc b ld mo lh mp ll mq lp mr lt ms lx mk ml mm mn bi translated">重复最小化损失</li></ul><h1 id="b797" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">处理和取消处理图像</h1><p id="d0a8" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">首先，我们需要格式化我们的图像以供我们的网络使用。我们要用的 CNN 是预先训练好的 VGG19 convnet。当我们将图像处理成兼容的数组时，我们还需要对生成的图像进行解处理，从 BGR 格式切换到 RGB 格式。让我们构建两个辅助函数来实现这一点:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="b248" class="ns kd iq no b gy nt nu l nv nw"># Preprocessing image to make it compatible with the VGG19 model<br/><strong class="no ir">def</strong> <strong class="no ir">preprocess_image</strong>(image_path):<br/>    img = load_img(image_path, target_size=(resized_width, resized_height))<br/>    img = img_to_array(img)<br/>    img = np.expand_dims(img, axis=<strong class="no ir">0</strong>)<br/>    img = vgg19.preprocess_input(img)<br/>    <strong class="no ir">return</strong> img<br/><br/># Function to convert a tensor into an image<br/><strong class="no ir">def</strong> <strong class="no ir">deprocess_image</strong>(x):<br/>    x = x.reshape((resized_width, resized_height, <strong class="no ir">3</strong>))<br/><br/>    # Remove zero-center by mean pixel. Necessary when working with VGG model<br/>    x[:, :, <strong class="no ir">0</strong>] += <strong class="no ir">103.939</strong><br/>    x[:, :, <strong class="no ir">1</strong>] += <strong class="no ir">116.779</strong><br/>    x[:, :, <strong class="no ir">2</strong>] += <strong class="no ir">123.68</strong><br/><br/>    # Format BGR-&gt;RGB<br/>    x = x[:, :, ::-<strong class="no ir">1</strong>]<br/>    x = np.clip(x, <strong class="no ir">0</strong>, <strong class="no ir">255</strong>).astype('uint8')<br/>    <strong class="no ir">return</strong> x</span></pre><h1 id="f776" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">内容损失</h1><p id="db6d" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">内容损失将主输入图像的内容保留到样式中。由于卷积神经网络的较高层包含图像宏观结构的信息，我们将内容损失计算为输入图像的最高层的输出和生成图像的相同层之间的差异(l2 归一化)。<br/>内容损失定义为:</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/9417fb28773fcddc8dcebca4721a8a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*mx_6OSVh6QGnWhJuRJjZYw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Content loss</figcaption></figure><p id="67c6" class="pw-post-body-paragraph la lb iq lc b ld ma lf lg lh mb lj lk ll mc ln lo lp md lr ls lt me lv lw lx ij bi translated">在等式中，<em class="lz"> F </em>是内容图像的特征表示(当我们运行我们的输入图像时，网络输出的内容)，而<em class="lz"> P </em>是在特定隐藏层<em class="lz"> l </em>生成的图像的特征表示。<br/>实现如下:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="70c1" class="ns kd iq no b gy nt nu l nv nw"># The content loss maintains the features of the content image in the generated image.<br/><strong class="no ir">def</strong> <strong class="no ir">content_loss</strong>(layer_features):<br/>    base_image_features = layer_features[<strong class="no ir">0</strong>, :, :, :]<br/>    combination_features = layer_features[<strong class="no ir">2</strong>, :, :, :]<br/>    <strong class="no ir">return</strong> K.sum(K.square(combination_features - base_image_features))</span></pre><h1 id="e78c" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">风格丧失</h1><p id="fb9b" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">理解风格损失不像理解内容损失那么简单。目标是在新生成的图像中保留图像的样式(即，作为笔触的视觉图案)。在前一个例子中，我们比较了中间层的原始输出。这里，我们比较样式参考图像和生成图像的特定层的 Gram 矩阵之间的差异。<strong class="lc ir"> Gram 矩阵</strong>被定义为给定层的矢量化特征图之间的内积。矩阵的意义在于捕捉层特征之间的相关性。计算多个层的损失允许在样式图像和生成的图像之间保留不同层中内部相关的相似特征。<br/>单层的风格损失计算如下:</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/b2ba623ff3bbdded849f686be4634cf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Shg5dNFvlt7M8xH3AKyIpg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Style loss per layer</figcaption></figure><p id="9048" class="pw-post-body-paragraph la lb iq lc b ld ma lf lg lh mb lj lk ll mc ln lo lp md lr ls lt me lv lw lx ij bi translated">在等式中，<em class="lz"> A </em>是样式图像的 Gram 矩阵，<em class="lz"> G </em>是生成的图像的 Gram 矩阵，两者都与给定的层有关。<em class="lz"> N </em>和<em class="lz"> M </em>为样式图像的宽度和高度。<br/>在等式中，<em class="lz"> A </em>是风格图像的克矩阵，<em class="lz"> G </em>是生成的图像的克矩阵，两者都与给定的层有关。<em class="lz"> N </em>和<em class="lz"> M </em>为样式图像的宽度和高度。<br/>首先为每个单独的层计算样式损失，然后将其应用于被认为是对样式建模的每个层。让我们来实现它:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="8bd4" class="ns kd iq no b gy nt nu l nv nw"># The gram matrix of an image tensor is the inner product between the vectorized feature map in a layer.<br/># It is used to compute the style loss, minimizing the mean squared distance between the feature correlation map of the style image<br/># and the input image<br/><strong class="no ir">def</strong> <strong class="no ir">gram_matrix</strong>(x):<br/>    features = K.batch_flatten(K.permute_dimensions(x, (<strong class="no ir">2</strong>, <strong class="no ir">0</strong>, <strong class="no ir">1</strong>)))<br/>    gram = K.dot(features, K.transpose(features))<br/>    <strong class="no ir">return</strong> gram<br/><br/><br/># The style_loss_per_layer represents the loss between the style of the style reference image and the generated image.<br/># It depends on the gram matrices of feature maps from the style reference image and from the generated image.<br/><strong class="no ir">def</strong> <strong class="no ir">style_loss_per_layer</strong>(style, combination):<br/>    S = gram_matrix(style)<br/>    C = gram_matrix(combination)<br/>    channels = <strong class="no ir">3</strong><br/>    size = resized_width * resized_height<br/>    <strong class="no ir">return</strong> K.sum(K.square(S - C)) / (<strong class="no ir">4.</strong> * (channels ** <strong class="no ir">2</strong>) * (size ** <strong class="no ir">2</strong>))<br/><br/># The total_style_loss represents the total loss between the style of the style reference image and the generated image,<br/># taking into account all the layers considered for the style transfer, related to the style reference image.<br/><strong class="no ir">def</strong> <strong class="no ir">total_style_loss</strong>(feature_layers):<br/>    loss = K.variable(<strong class="no ir">0.</strong>)<br/>    <strong class="no ir">for</strong> layer_name <strong class="no ir">in</strong> feature_layers:<br/>        layer_features = outputs_dict[layer_name]<br/>        style_reference_features = layer_features[<strong class="no ir">1</strong>, :, :, :]<br/>        combination_features = layer_features[<strong class="no ir">2</strong>, :, :, :]<br/>        sl = style_loss_per_layer(style_reference_features, combination_features)<br/>        loss += (style_weight / len(feature_layers)) * sl<br/>    <strong class="no ir">return</strong> loss</span></pre><h1 id="6552" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">变异损失</h1><p id="3da7" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">最后，损失的最后一部分是变异损失。原始论文中没有包括这一要素，严格来说，它对于项目的成功并不是必要的。尽管如此，经验证明，添加该元素会产生更好的结果，因为它平滑了相邻像素之间的颜色变化。让我们把这个包括进去:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="b52d" class="ns kd iq no b gy nt nu l nv nw"># The total variation loss mantains the generated image loclaly coherent,<br/># smoothing the pixel variations among neighbour pixels.<br/><strong class="no ir">def</strong> <strong class="no ir">total_variation_loss</strong>(x):<br/>    a = K.square(x[:, :resized_width - <strong class="no ir">1</strong>, :resized_height - <strong class="no ir">1</strong>, :] - x[:, <strong class="no ir">1</strong>:, :resized_height - <strong class="no ir">1</strong>, :])<br/>    b = K.square(x[:, :resized_width - <strong class="no ir">1</strong>, :resized_height - <strong class="no ir">1</strong>, :] - x[:, :resized_width - <strong class="no ir">1</strong>, <strong class="no ir">1</strong>:, :])<br/>    <strong class="no ir">return</strong> K.sum(K.pow(a + b, <strong class="no ir">1.25</strong>))</span></pre><h1 id="1503" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">全损</h1><p id="8187" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">最后，将所有这些因素考虑在内，计算总损失。首先，我们需要提取我们选择的特定层的输出。为此，我们定义一个字典为&lt;<em class="lz">层名，层输出</em> &gt;:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="1ff7" class="ns kd iq no b gy nt nu l nv nw"># Get the outputs of each key layer, through unique names.<br/>outputs_dict = dict([(layer.name, layer.output) <strong class="no ir">for</strong> layer <strong class="no ir">in</strong> model.layers])</span></pre><p id="b831" class="pw-post-body-paragraph la lb iq lc b ld ma lf lg lh mb lj lk ll mc ln lo lp md lr ls lt me lv lw lx ij bi translated">然后，我们通过调用先前编码的函数来计算损失。每个分量都乘以特定的权重，我们可以调整权重以产生强烈或较轻的效果:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="ea70" class="ns kd iq no b gy nt nu l nv nw"><strong class="no ir">def</strong> <strong class="no ir">total_loss</strong>():<br/>    loss = K.variable(<strong class="no ir">0.</strong>)<br/><br/>    # contribution of content_loss<br/>    feature_layers_content = outputs_dict['block5_conv2']<br/>    loss += content_weight * content_loss(feature_layers_content)<br/><br/>    # contribution of style_loss<br/>    feature_layers_style = ['block1_conv1', 'block2_conv1',<br/>                            'block3_conv1', 'block4_conv1',<br/>                            'block5_conv1']<br/>    loss += total_style_loss(feature_layers_style) * style_weight<br/><br/>    # contribution of variation_loss<br/>    loss += total_variation_weight * total_variation_loss(combination_image)<br/>    <strong class="no ir">return</strong> loss</span></pre><h1 id="c907" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">设置神经网络</h1><p id="ff3f" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">VGG19 网络将一批三个图像作为输入:输入内容图像、样式参考图像和包含生成图像的符号张量。前两个是常量变量，使用 keras.backend 包定义为<em class="lz">变量</em>。第三个变量定义为<em class="lz">占位符</em>，因为它会随着优化器更新结果的时间而变化。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/a73d5c3461c5f304037056115aaa683e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*gASPVOwJJq-EoviO02Iw0w.jpeg"/></div></figure><p id="3ab7" class="pw-post-body-paragraph la lb iq lc b ld ma lf lg lh mb lj lk ll mc ln lo lp md lr ls lt me lv lw lx ij bi translated">一旦变量被初始化，我们就把它们加入一个张量，这个张量将在以后提供给网络。</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="9c6c" class="ns kd iq no b gy nt nu l nv nw"># Get tensor representations of our images<br/>base_image = K.variable(preprocess_image(base_image_path))<br/>style_reference_image = K.variable(preprocess_image(style_reference_image_path))<br/><br/># Placeholder for generated image<br/>combination_image = K.placeholder((<strong class="no ir">1</strong>, resized_width, resized_height, <strong class="no ir">3</strong>))<br/><br/># Combine the 3 images into a single Keras tensor<br/>input_tensor = K.concatenate([base_image,<br/>                              style_reference_image,<br/>                              combination_image], axis=<strong class="no ir">0</strong>)</span></pre><p id="4f3b" class="pw-post-body-paragraph la lb iq lc b ld ma lf lg lh mb lj lk ll mc ln lo lp md lr ls lt me lv lw lx ij bi translated">完成后，我们需要定义损耗、梯度和输出。原始论文使用算法 L-BFGS 作为优化器。这种算法的一个限制是它要求损失和梯度分别通过。因为单独计算它们效率极低，所以我们将实现一个赋值器类，它可以同时计算损失和梯度值，但分别返回它们。让我们这样做:</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="b96b" class="ns kd iq no b gy nt nu l nv nw">loss = total_loss()<br/><br/># Get the gradients of the generated image<br/>grads = K.gradients(loss, combination_image)<br/>outputs = [loss]<br/>outputs += grads<br/><br/>f_outputs = K.function([combination_image], outputs)<br/><br/># Evaluate the loss and the gradients respect to the generated image. It is called in the Evaluator, necessary to<br/># compute the gradients and the loss as two different functions (limitation of the L-BFGS algorithm) without<br/># excessive losses in performance<br/><strong class="no ir">def</strong> <strong class="no ir">eval_loss_and_grads</strong>(x):<br/>    x = x.reshape((<strong class="no ir">1</strong>, resized_width, resized_height, <strong class="no ir">3</strong>))<br/>    outs = f_outputs([x])<br/>    loss_value = outs[<strong class="no ir">0</strong>]<br/>    <strong class="no ir">if</strong> len(outs[<strong class="no ir">1</strong>:]) == <strong class="no ir">1</strong>:<br/>        grad_values = outs[<strong class="no ir">1</strong>].flatten().astype('float64')<br/>    <strong class="no ir">else</strong>:<br/>        grad_values = np.array(outs[<strong class="no ir">1</strong>:]).flatten().astype('float64')<br/>    <strong class="no ir">return</strong> loss_value, grad_values<br/><br/># Evaluator returns the loss and the gradient in two separate functions, but the calculation of the two variables<br/># are dependent. This reduces the computation time, since otherwise it would be calculated separately.<br/><strong class="no ir">class</strong> <strong class="no ir">Evaluator</strong>(object):<br/><br/>    <strong class="no ir">def</strong> <strong class="no ir">__init__</strong>(self):<br/>        self.loss_value = <strong class="no ir">None</strong><br/>        self.grads_values = <strong class="no ir">None</strong><br/><br/>    <strong class="no ir">def</strong> <strong class="no ir">loss</strong>(self, x):<br/>        <strong class="no ir">assert</strong> self.loss_value <strong class="no ir">is</strong> <strong class="no ir">None</strong><br/>        loss_value, grad_values = eval_loss_and_grads(x)<br/>        self.loss_value = loss_value<br/>        self.grad_values = grad_values<br/>        <strong class="no ir">return</strong> self.loss_value<br/><br/>    <strong class="no ir">def</strong> <strong class="no ir">grads</strong>(self, x):<br/>        <strong class="no ir">assert</strong> self.loss_value <strong class="no ir">is</strong> <strong class="no ir">not</strong> <strong class="no ir">None</strong><br/>        grad_values = np.copy(self.grad_values)<br/>        self.loss_value = <strong class="no ir">None</strong><br/>        self.grad_values = <strong class="no ir">None</strong><br/>        <strong class="no ir">return</strong> grad_values<br/><br/>evaluator = Evaluator()</span></pre><h1 id="b728" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">最后一档</h1><p id="3f2e" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">终于万事俱备了！最后一步是多次迭代优化器，直到我们达到期望的损失或期望的结果。我们将保存迭代的结果，以检查算法是否按预期工作。如果结果不令人满意，我们可以调整权重以改善生成的图像。</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="bd81" class="ns kd iq no b gy nt nu l nv nw"># The oprimizer is fmin_l_bfgs<br/><strong class="no ir">for</strong> i <strong class="no ir">in</strong> range(iterations):<br/>    print('Iteration: ', i)<br/>    x, min_val, info = fmin_l_bfgs_b(evaluator.loss,<br/>                                     x.flatten(),<br/>                                     fprime=evaluator.grads,<br/>                                     maxfun=<strong class="no ir">15</strong>)<br/><br/>    print('Current loss value:', min_val)<br/><br/>    # Save current generated image<br/>    img = deprocess_image(x.copy())<br/>    fname = 'img/new' + np.str(i) + '.png'<br/>    save(fname, img)</span></pre><p id="2a2a" class="pw-post-body-paragraph la lb iq lc b ld ma lf lg lh mb lj lk ll mc ln lo lp md lr ls lt me lv lw lx ij bi translated">要查看完整代码，请参考页面开头提供的 GitHub 链接。</p><h1 id="3e1f" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">惊人的结果</h1><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oa"><img src="../Images/543654a95d45d3361475770f5d3714a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*kdUTCYnmfliMngQ_Y9g6og.gif"/></div></div></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ob"><img src="../Images/2610bc33e817664db42274ae74b32dbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EInbyrqdhszURjRCYVeMIA.png"/></div></div></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oc"><img src="../Images/6a3bee3e5029daa618938b248828166d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BgS6uhsspFM-lq8E5okOtg.png"/></div></div></figure><blockquote class="od oe of"><p id="2b61" class="la lb lz lc b ld ma lf lg lh mb lj lk og mc ln lo oh md lr ls oi me lv lw lx ij bi translated">如果你想尝试特定的效果，绘画，或者你有任何建议，请留下评论！</p></blockquote><p id="a4c4" class="pw-post-body-paragraph la lb iq lc b ld ma lf lg lh mb lj lk ll mc ln lo lp md lr ls lt me lv lw lx ij bi translated"><em class="lz">如果你喜欢这篇文章，我希望你能点击鼓掌按钮</em>👏因此其他人可能会偶然发现它。对于任何意见或建议，不要犹豫留下评论！</p></div><div class="ab cl oj ok hu ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="ij ik il im in"><h2 id="f93c" class="ns kd iq bd ke oq or dn ki os ot dp km ll ou ov kq lp ow ox ku lt oy oz ky pa bi translated">我是一名数据科学专业的学生，热爱机器学习及其无尽的应用。你可以在 maurocomi.com<a class="ae ly" href="http://www.maurocomi.com" rel="noopener ugc nofollow" target="_blank">找到更多关于我和我的项目的信息。你也可以在</a><a class="ae ly" href="https://www.linkedin.com/in/mauro-comi/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>上找到我，或者直接给我发邮件。我总是乐于聊天，或者合作新的令人惊奇的项目。</h2></div></div>    
</body>
</html>