<html>
<head>
<title>Linear Regression using Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用梯度下降的线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931?source=collection_archive---------0-----------------------#2018-09-16">https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931?source=collection_archive---------0-----------------------#2018-09-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="b868" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本教程中，你可以学习梯度下降算法的工作原理，并从头开始用 python 实现它。首先我们看看什么是线性回归，然后我们定义损失函数。我们学习梯度下降算法如何工作，最后我们将在给定的数据集上实现它并进行预测。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/902d84d7f9cbcf2747c30416de3734c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*CjTBNFUEI_IokEOXJ00zKw.gif"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The values of m and c are updated at each iteration to get the optimal solution</figcaption></figure><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="lb lc l"/></div></figure><p id="b2fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是这段视频的书面版本。如果你喜欢，就看着它！</p><h1 id="063f" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">线性回归</h1><p id="82a1" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">在统计学中，线性回归是一种对因变量和一个或多个自变量之间的关系进行建模的线性方法。设<strong class="jp ir"> X </strong>为自变量，<strong class="jp ir"> Y </strong>为因变量。我们将定义这两个变量之间的线性关系如下:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/75af73122d6d56bfdb10241f4d2e2ee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*p3LTR6GB6g2MpRZzE5JIxw.png"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/31766e6ead40349744433ed91e24e18b.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/1*ETn5o9GRaF8ZK6wIHvGrJQ.gif"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Source: <a class="ae mj" href="http://www.nabla.hr/SlopeInterceptLineEqu.gif" rel="noopener ugc nofollow" target="_blank">http://www.nabla.hr/SlopeInterceptLineEqu.gif</a></figcaption></figure><p id="94d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是你在高中学过的一条线的方程。<strong class="jp ir"> m </strong>是直线的斜率，c<strong class="jp ir">是 y 轴截距。今天，我们将使用这个等式用给定的数据集来训练我们的模型，并针对任何给定的<strong class="jp ir"> X </strong>值来预测<strong class="jp ir"> Y </strong>的值。我们今天的挑战是确定<strong class="jp ir"> m </strong>和<strong class="jp ir"> c </strong>的值，使得对应于这些值的线是最佳拟合线或给出最小误差。</strong></p><h1 id="789a" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">损失函数</h1><p id="38e7" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">损失就是我们预测的<strong class="jp ir"> m </strong>和<strong class="jp ir"> c </strong>的误差。我们的目标是最小化这一误差，以获得最准确的 m<strong class="jp ir">和 c</strong>值。<br/>我们将使用均方差函数来计算损失。该功能有三个步骤:</p><ol class=""><li id="0321" class="mk ml iq jp b jq jr ju jv jy mm kc mn kg mo kk mp mq mr ms bi translated">对于给定的 x，求实际 y 值和预测 y 值的差(y = mx + c)。</li><li id="35a6" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">平方这个差值。</li><li id="6bed" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">求 x 中每个值的平方的平均值。</li></ol><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/528e9089b184e640e4b62b8e9931c180.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*_y5QA1yF4w6LDDRxfTt6GA.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Mean Squared Error Equation</figcaption></figure><p id="44c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里 yᵢ是实际值，ȳᵢ是预测值。让我们用ȳᵢ:的值来代替</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi my"><img src="../Images/fb19dcfa6ff051ef18be94d05a6bffbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*3cpC7oHy4IbH3o3Jc-ygVw.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Substituting the value of ȳᵢ</figcaption></figure><p id="63c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以我们求误差的平方，然后求平均值。因此得名均方误差。既然我们已经定义了损失函数，让我们进入有趣的部分——最小化它并找到<strong class="jp ir"> m </strong>和<strong class="jp ir"> c. </strong></p><h1 id="ee05" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">梯度下降算法</h1><p id="94ea" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">梯度下降是一种寻找函数最小值的迭代优化算法。这个函数就是我们的损失函数。</p><p id="12bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">了解梯度下降</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mz"><img src="../Images/f7c1b2269a156e0b7ae31d5ecfccf696.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N5WjbzwsCFse-KPjBWZZ6g.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Illustration of how the gradient descent algorithm works</figcaption></figure><p id="00dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想象一个山谷和一个没有方向感的人想要到达谷底。他走下斜坡，在斜坡陡的时候迈大步，在斜坡不那么陡的时候迈小步。他根据当前位置决定下一个位置，当他到达他的目标山谷底部时停下来。<br/>让我们试着对<strong class="jp ir"> m </strong>和<strong class="jp ir"> c </strong>应用梯度下降，一步一步接近它；</p><ol class=""><li id="7cad" class="mk ml iq jp b jq jr ju jv jy mm kc mn kg mo kk mp mq mr ms bi translated">最初设 m = 0，c = 0。设 L 为我们的学习率。这控制了<strong class="jp ir"> m </strong>的值随着每一步变化的程度。l 可以是一个小值，如 0.0001，以获得良好的精度。</li><li id="ae25" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">计算损失函数关于 m 的偏导数，将 x，y，m，c 的当前值代入其中，得到导数值<strong class="jp ir"> D </strong>。</li></ol><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi my"><img src="../Images/33f60908e52a7baae843dafee8741f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*FvYfCBrl2gX9K-KxSO1eIw.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Derivative with respect to <strong class="bd na">m</strong></figcaption></figure><p id="9a27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Dₘ是关于<strong class="jp ir"> m </strong>的偏导数的值。类似地，让我们找到关于<strong class="jp ir"> c </strong>，Dc 的偏导数:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/abd56d27356dd30e23b35b131a62822a.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*rj09w2TcBxnHPtQ0oq4ehA.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Derivative with respect to <strong class="bd na">c</strong></figcaption></figure><p id="ae84" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.现在，我们使用以下等式更新<strong class="jp ir"> m </strong>和<strong class="jp ir"> c </strong>的当前值:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/a29c511bbe8f8b2b8eaa6c0ca3f1a85e.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*JDcHqFK8jLcgQu1cj2XuVQ.jpeg"/></div></figure><p id="9fa2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4.我们重复这个过程，直到我们的损失函数是一个非常小的值或理想的 0(这意味着 0 误差或 100%的准确性)。我们现在剩下的<strong class="jp ir"> m </strong>和<strong class="jp ir"> c </strong>的值将是最佳值。</p><p id="71c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在回到我们的类比，<strong class="jp ir"> m </strong>可以被认为是人的当前位置。<strong class="jp ir"> D </strong>相当于斜坡的陡度<strong class="jp ir"> L </strong>可以是他移动的速度。现在，我们使用上面的等式计算出的新值 m 将是他的下一个位置，而 L×D 将是他将要走的步数。坡度越陡(<strong class="jp ir"> D </strong>越大)，他的步幅就越大，坡度越缓(<strong class="jp ir"> D </strong>越小)，他的步幅就越小。最后，他到达了谷底，这相当于我们的损失= 0。<br/>现在有了 m<strong class="jp ir">m</strong>和 c<strong class="jp ir">c</strong>的最佳值，我们的模型就可以进行预测了！</p><h1 id="ad89" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">实施模型</h1><p id="28bf" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">现在让我们把上面的一切都转换成代码，看看我们的模型在运行！</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nb lc l"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nc"><img src="../Images/e3af13f2c246dfda6069bcf301620742.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*72OmGTG9o9EF2mEqeQC6Rw.png"/></div></div></figure><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nb lc l"/></div></figure><pre class="km kn ko kp gt nd ne nf ng aw nh bi"><span id="87c7" class="ni lf iq ne b gy nj nk l nl nm">1.4796491688889395 0.10148121494753726</span></pre><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nb lc l"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nc"><img src="../Images/71d0f88724c89c0362c2c9023d58e650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kfzkh0G6EChPI53IQ0DuNw.png"/></div></div></figure></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><p id="562d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度下降是机器学习中最简单也是最广泛使用的算法之一，主要是因为它可以应用于任何函数来优化它。学习它是掌握机器学习的基础。</p><p id="df73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ld">在这里找到数据集和代码:</em><a class="ae mj" href="https://github.com/chasinginfinity/ml-from-scratch/tree/master/02%20Linear%20Regression%20using%20Gradient%20Descent" rel="noopener ugc nofollow" target="_blank">https://github . com/chasing infinity/ml-from-scratch/tree/master/02% 20 linear % 20 regression % 20 using % 20 gradient % 20 descending</a></p><blockquote class="nu nv nw"><p id="e66c" class="jn jo ld jp b jq jr js jt ju jv jw jx nx jz ka kb ny kd ke kf nz kh ki kj kk ij bi translated">有问题吗？需要帮助吗？联系我！</p></blockquote><p id="55ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">【adarsh1021@gmail.com】电子邮件:T4</p><p id="e5dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ld">领英:</em><a class="ae mj" href="https://www.linkedin.com/in/adarsh-menon-739573146/" rel="noopener ugc nofollow" target="_blank"><em class="ld">https://www.linkedin.com/in/adarsh-menon-739573146/</em></a></p><p id="6569" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ld">推特:</em><a class="ae mj" href="https://twitter.com/adarsh_menon_" rel="noopener ugc nofollow" target="_blank"><em class="ld">https://twitter.com/adarsh_menon_</em></a></p><p id="e313" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ld">insta gram:</em><a class="ae mj" href="https://www.instagram.com/adarsh_menon_/" rel="noopener ugc nofollow" target="_blank">【https://www.instagram.com/adarsh_menon_/】T21</a></p><p id="ef35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ld">参考文献:</em></p><div class="oa ob gp gr oc od"><a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd ir gy z fp oi fr fs oj fu fw ip bi translated">机器学习的梯度下降法</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">优化是机器学习的一大部分。几乎每个机器学习算法都有一个优化算法…</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">machinelearningmastery.com</p></div></div><div class="om l"><div class="on l oo op oq om or kv od"/></div></div></a></div><div class="oa ob gp gr oc od"><a rel="noopener follow" target="_blank" href="/gradient-descent-a-beginners-guide-fa0b5d0a1db8"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd ir gy z fp oi fr fs oj fu fw ip bi translated">梯度下降——初学者指南</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">简介:</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">towardsdatascience.com</p></div></div><div class="om l"><div class="os l oo op oq om or kv od"/></div></div></a></div></div></div>    
</body>
</html>