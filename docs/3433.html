<html>
<head>
<title>Tuning Hyperparameters (part II): Random Search on Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">调谐超参数(二):火花的随机搜索</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameters-part-ii-random-search-on-spark-77667e68b606?source=collection_archive---------4-----------------------#2018-05-11">https://towardsdatascience.com/hyperparameters-part-ii-random-search-on-spark-77667e68b606?source=collection_archive---------4-----------------------#2018-05-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/1e2e49c7d3e55f34eb8d54a15e3ffdc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*nfcKTpcyaaPM8yoHCPzgUA.png"/></div></figure><p id="2fa5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在本系列的第二部分中，我将继续我的超参数优化策略，这一次我想从 Spark 的角度更仔细地看看<a class="ae ks" href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jw ir"/></a>。该系列的每个部分都可以单独阅读，请随意查看<a class="ae ks" href="https://medium.com/machine-learning-rambling/tuning-hyperparameters-part-i-successivehalving-c6c602865619" rel="noopener">第一部分</a>。</p><h1 id="c9ac" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">随机搜索和分布式机器学习框架</h1><p id="f14d" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">不管你的算法设计得多好，数学可能有多美，如果客户需要在大量数据上相对较短的训练时间，你最好找到一种方法来提供！</p><p id="0181" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">幸运的是，分布式计算是随机搜索果冻的花生酱。让我们提醒一下为什么<strong class="jw ir">随机搜索</strong>比<strong class="jw ir">网格搜索</strong>效率高？</p><p id="3d52" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">最大的敌人是维度的诅咒。对于附加的超参数，以及它们各自选择的值，会成倍增加搜索时间。与其固定搜索空间的值，还不如对其进行采样。为了理解这一点，让我们看一下图 1。摘自<a class="ae ks" href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank">原文</a>。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/ec2eec2a2e72ab14163baa342f1bd06c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/0*bZx5WJYaKhOOfoqW.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Figure 1: Grid Search vs Random Search</figcaption></figure><p id="e3ee" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">正如我们所见，在搜索中经常出现的情况，一些超参数比其他的更具决定性。在<strong class="jw ir">网格搜索</strong>的情况下，尽管采样了 9 次试验，实际上我们只试验了一个重要参数的 3 个不同值。在<strong class="jw ir">随机搜索</strong>的情况下，9 次试验将测试 9 个不同的决定性参数值。</p><p id="a9ad" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因为超参数配置的每个样本都是彼此独立绘制的，所以我们可以看到并行化是多么容易！于是，火花来了！</p><h1 id="eb7f" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">火花</h1><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/594cb9f843d6e804a9e9c801830f46a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/0*5QUptgUicJtJQT13.png"/></div></figure><p id="1abf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><a class="ae ks" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="jw ir"> Spark </strong> </a>是一个流行的开源框架，用于在集群上进行分布式计算，提供了一个广泛的库，用于操作数据库、流、分布式图形处理，最重要的是本讨论中的机器学习，即 Spark MLlib。</p><p id="7dd8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">由于微软 Azure 机器学习团队的工作，Spark MLlib 最近获得了巨大的推动，该团队发布了<a class="ae ks" href="https://github.com/Azure/mmlspark" rel="noopener ugc nofollow" target="_blank"> <strong class="jw ir"> MMLSpark </strong> </a>。从实用机器学习的角度来看，MMLSpark 最显著的功能是访问极端梯度增强库<strong class="jw ir"> Lighgbm </strong>，这是大多数数据科学概念证明的快速制胜方法。</p><p id="b106" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">既然每个数据科学家最喜欢的库都可以在集群上训练，我们只差一个合适的超参数调整框架了。遗憾的是，最初的 Spark MLlib 只有一个网格搜索的实现。MMLSpark 提供了带<strong class="jw ir">随机搜索</strong>的超调，但遗憾的是采样只有<strong class="jw ir">统一</strong>。</p><h1 id="9c16" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">实际上…</h1><p id="fa6a" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">均匀采样是一个很好的步骤，但对于许多超参数来说不是最佳的。在 Lightgbm 等极端梯度推进算法的情况下，学习率和正则化超参数浮现在脑海中。这些参数应该在对数尺度上采样，而不是在一个间隔内均匀采样。</p><p id="807f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">那么我们如何黑 Spark MLlib 来满足我们的需求呢？</p><p id="befc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">下面让我们先来看看<strong class="jw ir"> Spark </strong>中的关键成分。</p><figure class="lx ly lz ma gt jr"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="3f6d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">正如我们所看到的，超参数值的网格被定义为 ParamMap 类型的数组，来自 ParamGridBuilder 类的一个实例。因此，为了保持与 Spark 的 CrossValidator 兼容，让我们继续并重新定义<strong class="jw ir"> build() </strong>和<strong class="jw ir"> addGrid </strong>方法。</p><p id="ba64" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们不是在网格中添加一个超参数值列表，而是定义一个分布，稍后我们将从该分布中抽取配置样本。</p><p id="c483" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><a class="ae ks" href="https://github.com/scalanlp/breeze" rel="noopener ugc nofollow" target="_blank"><strong class="jw ir">Breeze</strong></a><strong class="jw ir"/>是一个流行的用于数值处理的 scala 库，在 breeze.stats.distributions 中有各种各样的分布。<br/>例如，在逻辑回归的情况下，我们可能希望定义以下采样空间:</p><figure class="lx ly lz ma gt jr"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="09be" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">一方面，我们希望从分布中取样，另一方面，在一组分类选择的情况下，我们应该能够设置一组选择。</p><p id="4d18" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们可以提出以下解决方案，</p><figure class="lx ly lz ma gt jr"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="d237" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在让我们对 LightGBM 进行最后一次测试，</p><figure class="lx ly lz ma gt jr"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="2a81" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们走吧！</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/a5b1175a4662fefd96584c5ee2817b13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/0*EP2cJDAPig_ltlfN.png"/></div></figure><p id="4dcc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">更多例子的代码可以在<a class="ae ks" href="https://github.com/benoitdescamps/Hyperparameters-tuning" rel="noopener ugc nofollow" target="_blank"> <strong class="jw ir">这里</strong> </a>找到。</p><h1 id="68bd" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">我写了更多精彩的东西！</h1><p id="534f" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">@ <a class="ae ks" href="https://medium.com/machine-learning-rambling/tuning-hyperparameters-part-i-successivehalving-c6c602865619" rel="noopener">调整超参数(第一部分):成功减半</a></p><p id="4ca6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">@<a class="ae ks" href="https://www.kdnuggets.com/2018/01/custom-optimizer-tensorflow.html" rel="noopener ugc nofollow" target="_blank">tensor flow 中的自定义优化器</a></p><p id="e9b5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">@<a class="ae ks" href="https://medium.com/bigdatarepublic/regression-prediction-intervals-with-xgboost-428e0a018b" rel="noopener">XG boost 回归预测区间</a></p><h1 id="da39" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">参考资料:</h1><ol class=""><li id="c4bc" class="mj mk iq jw b jx lr kb ls kf ml kj mm kn mn kr mo mp mq mr bi translated">J.Bergstra 和 Y. Bengio，超参数优化的随机搜索，2011 年</li><li id="abf9" class="mj mk iq jw b jx ms kb mt kf mu kj mv kn mw kr mo mp mq mr bi translated">https://spark.apache.org/<a class="ae ks" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">星火</a></li><li id="4f17" class="mj mk iq jw b jx ms kb mt kf mu kj mv kn mw kr mo mp mq mr bi translated">https://github.com/Azure/mmlspark</li><li id="1243" class="mj mk iq jw b jx ms kb mt kf mu kj mv kn mw kr mo mp mq mr bi translated">微风，<a class="ae ks" href="https://github.com/scalanlp/breeze" rel="noopener ugc nofollow" target="_blank">https://github.com/scalanlp/breeze</a></li><li id="bf2f" class="mj mk iq jw b jx ms kb mt kf mu kj mv kn mw kr mo mp mq mr bi translated">J.Bergstra，R. Bardenet，Y. Bengio，B. Kégl，超参数优化算法</li><li id="75f4" class="mj mk iq jw b jx ms kb mt kf mu kj mv kn mw kr mo mp mq mr bi translated">github:<a class="ae ks" href="https://github.com/benoitdescamps/Hyperparameters-tuning" rel="noopener ugc nofollow" target="_blank">https://github.com/benoitdescamps/Hyperparameters-tuning</a></li></ol></div></div>    
</body>
</html>