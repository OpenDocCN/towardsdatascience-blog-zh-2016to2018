<html>
<head>
<title>Bishop’s PRML book: review and insights, chapters 4–6</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">毕晓普的PRML书:回顾和见解，第4-6章</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bishops-prml-book-review-and-insights-chapters-4-6-eb2a2a33c939?source=collection_archive---------4-----------------------#2017-11-29">https://towardsdatascience.com/bishops-prml-book-review-and-insights-chapters-4-6-eb2a2a33c939?source=collection_archive---------4-----------------------#2017-11-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jn jo jp jq gh gi paragraph-image"><div class="ab gu cl jr"><img src="../Images/debf4cebeb467731c373341c41749f2f.png" data-original-src="https://miro.medium.com/v2/format:webp/1*3KFyJHoWxxR9hajZzSgg7Q.jpeg"/></div></figure><p id="82ad" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">大家好！在<a class="ae ks" href="https://techburst.io/bishops-prml-book-review-and-insights-chapters-1-3-528bb5cfaade" rel="noopener ugc nofollow" target="_blank">的上一篇文章</a>中，我发表了一份关于Bishop的《模式识别和机器学习》一书前三章的简短简历，在这本书里，我将继续写下一章。</p><p id="1b35" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于更注重实践、正在寻找如何提高理论背景的数据科学家，对于那些想要快速总结一些基础知识的人，或者对于刚刚起步的初学者来说，这可能会很有趣。</p><h1 id="a0a4" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">4.分类的线性模型</h1><p id="e95e" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">在第三章中，我们发现了线性回归的一个推广形式为<em class="lw"> f(w，x)= w0+w1 * x1</em>到<em class="lw">w0+w1 *фI(x)+…+фn(x)，</em>，其中<em class="lw"> ф(x) </em>称为基函数。我想，这里几乎每个人都知道，要应用这个模型进行分类，我们需要把我们的<em class="lw"> w_i * x </em>传递给一个激活函数:<em class="lw"> y(x) = f(w_i * x)。</em></p><p id="1a23" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">首先，我们想知道，如何做N类分类，其中N &gt; 2。例如，在<a class="ae ks" href="http://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>中，我们可以选择以“一对一”或“一对一”模式训练二进制分类器，但这些方法存在一些模糊区域，我们不知道如何做出决策，并且构建这些模型需要更多时间:</p><div class="lx ly lz ma gt ab cb"><figure class="mb jq mc md me mf mg paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><img src="../Images/06113f9a0c7cd1927ad72a25fdd9913c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*YHyzUpDr_NecIOo-PESOaA.jpeg"/></div></figure><figure class="mb jq ml md me mf mg paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><img src="../Images/b14fd0d4eccab0f8455a7f1252fa6765.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*b1vLz4Im3aM7BU9I9KsEkg.jpeg"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk mq di mr ms">Ambiguity regions for “one vs one” or “one vs all” approaches</figcaption></figure></div><p id="b4f1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">相反，我们有三种主要策略来构建判别函数:</p><ul class=""><li id="62e9" class="mt mu iq jw b jx jy kb kc kf mv kj mw kn mx kr my mz na nb bi translated"><strong class="jw ir">最小二乘法</strong> —直接求解矩阵方程，但对异常值不太好。</li><li id="abe2" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated"><strong class="jw ir"> Fisher线性判别式</strong> —我们希望找到一种投影，使类别分离最大化，例如，类别均值。使用类间和类内方差和类的均值，我们建立Fisher判别式，我们稍后使用它来建立方程，类似于最小二乘法。</li><li id="3296" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated"><strong class="jw ir">感知器</strong>——非常类似于逻辑回归，我将在下面描述，或者你可以在这里阅读。</li></ul><p id="6348" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">逻辑回归的推导非常简单，通过最大似然法，我们得到了我们最喜欢的二元交叉熵:</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nh"><img src="../Images/d7dc5eef1b02ad49d2ac6be38ffe614c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*xJQl4Ce_C4ndjr2SQX5hGA.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">cross-entropy loss</figcaption></figure><p id="5b71" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于逻辑回归，还展示了迭代算法，基于Hessian ( <strong class="jw ir"> Newton-Raphson </strong>)以最小化损失，并针对不同应用进行扩展，其中softmax或逻辑函数不适合(如二元变量)——<strong class="jw ir">概率单位回归</strong>。例如，让我们考虑2类问题，其中我们可以有一些给出阈值<em class="lw">θ</em>的激活函数，并且我们基于if <em class="lw"> w_i * x </em>高于或低于<em class="lw">θ</em>来分配类。主要思想是θ是有噪声的，例如从一些PDF中提取的，并且该PDF的参数将被估计。</p><p id="4e60" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">本章继续介绍<strong class="jw ir">拉普拉斯近似法</strong>，其目的是在一组连续变量上找到PDF的高斯近似法。我们主要在贝叶斯推理应用中需要它，在这种情况下使用高斯分布很方便:</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ni"><img src="../Images/5da301a8c5c91521d138cd262019457d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MgfGo195Nyi7ma3D013ERg.jpeg"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">visualization of PDF (yellow) and Laplace approximation (red)</figcaption></figure><p id="791c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">本章最后是模型比较和贝叶斯逻辑回归与地图，高斯近似和预测分布。值得一提的是<strong class="jw ir">贝叶斯信息准则</strong> (BIC)。有了<em class="lw"> {M_i} </em>模型、<em class="lw"> {W_i} </em>参数和数据<em class="lw"> D </em>对于每个模型，我们可以定义似然<em class="lw"> p(D|W_i，M_i) </em>，并从中定义模型证据<em class="lw"> p(D|M_i) </em>。对于后者我们计算<em class="lw">ln(p(D))~ ln(p(D | W _ map))—M * ln(N)</em>，其中<em class="lw"> M </em> —参数个数，<em class="lw"> N </em> —数据点个数。正如我们所看到的，BIC惩罚模型有太多的参数。</p><h1 id="93bb" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">5.神经网络</h1><p id="2127" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">我想读者已经对NNs有了很多了解，我将只提到一些有趣的时刻。</p><p id="4122" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">首先，这里引入神经网络作为具有基函数的模型，基函数是预先固定的，但是它们必须是自适应的。有趣的是，<strong class="jw ir">跳过连接</strong>中使用的<a class="ae ks" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">resnet</a>在本书中有显示:</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nj"><img src="../Images/7cc6dabec063e61d48cbfe497ff17f46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KrL_gq2OkGdLZ8ZPpdPAWg.jpeg"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">skip connections in neural networks</figcaption></figure><p id="1e94" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这本书的很大一部分致力于反向传播和衍生品。它展示了如何:</p><ul class=""><li id="2ce0" class="mt mu iq jw b jx jy kb kc kf mv kj mw kn mx kr my mz na nb bi translated">用BP计算<strong class="jw ir">雅可比矩阵</strong></li><li id="b1cc" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">用BP及其逆运算计算<strong class="jw ir"> Hessian矩阵</strong>(对角线，外积近似)</li></ul><p id="bf33" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">神经网络的正则化也在这里讨论。首先，<a class="ae ks" href="https://en.wikipedia.org/wiki/Lasso_(statistics)#Elastic_net" rel="noopener ugc nofollow" target="_blank">弹性</a>正则化项被提出，因为具有正则<strong class="jw ir">权值衰减</strong>的神经网络是<strong class="jw ir">不变的线性变换</strong>。</p><p id="41b9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们都知道，例如，对于计算机视觉，我们做了很多数据增强，但通常我们认为这是对初始数据集的放大。使用<strong class="jw ir">切线传播</strong>也可以更“正确”地完成。主要思想是，我们将这些变换形式化为一些流形M上的向量，并且我们对它们的方向导数进行反向传播:</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/1f09183f61cc376ac0b1335f777a0195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*k5rXqWs1cVPdfODPZEXnAw.jpeg"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">tangent of x on a manifold M</figcaption></figure><p id="d23b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">NNs的另一个很酷的应用是<strong class="jw ir">密度近似</strong>，其中从数据<em class="lw"> {X_i} </em>我们预测经验分布的均值和方差为高斯混合。</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nl"><img src="../Images/2cc2e9187414d4410d22222a179c809c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HszVJSqThV4PAROW-10jRw.jpeg"/></div></div></figure><p id="be9d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">本章以贝叶斯神经网络结束。我们设定目标分布和权重的先验，我们可以用拉普拉斯近似后验分布。</p><h1 id="448f" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">6.核心方法</h1><p id="d63e" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">大多数线性模型可以用等价的<strong class="jw ir">“对偶表示”</strong>来表示，其中预测是基于在数据点评估的核函数的线性组合来完成的。</p><p id="0547" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对偶表示可以从损失函数中得到。例如，正则化均方损失<em class="lw"> J(w) </em>后，我们通过导数计算最佳参数值，在表达式中，我们将得到类似于<em class="lw"> w^t * ф_i(x)的结果。</em>我们可以称之为a_n，并重新表示最小二乘<em class="lw"> J(a) </em>，因为我们为这个<em class="lw"> a </em>求解，并得到一个具有Gram矩阵<em class="lw"> K，</em>的解，其中<em class="lw">K _ nm =ф_ I(x _ n)*ф_ I(x _ m)= K(x _ n，x _ m)——</em>这是我们的核。</p><p id="a741" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们可以将核构造为多项式、高斯函数或逻辑函数:</p><div class="lx ly lz ma gt ab cb"><figure class="mb jq nm md me mf mg paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><img src="../Images/755d4ca77fe6cab9e7ec105e2a202742.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*8MPxRg1b40N5CJrx5gVkzA.jpeg"/></div></figure><figure class="mb jq nn md me mf mg paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><img src="../Images/0031706783d9d004fd2d4a299b8bd88d.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*boUoNpODZilxe1h9UsH0Eg.jpeg"/></div></figure><figure class="mb jq nn md me mf mg paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><img src="../Images/19ae3ade7157cd4d42c70425fc84a2ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*TQTAdj7SRyTlknK3Ux40iQ.jpeg"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk no di np ms">polys, Gaussians and logistic functions as kernels</figcaption></figure></div><p id="0e2c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">有很多不同的方法来构建内核:从内核的加法和乘法组合开始，到像Fisher内核这样的专用版本，它测量x和x '之间的相似性。</p><p id="315c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">另一个有趣的算法是<strong class="jw ir">径向基函数网络</strong>。当输入噪声过大时，它适用于插值问题。内核(根据NNs的激活函数)与<strong class="jw ir"> Nadaraya-Watson模型</strong>中的相同。在这个模型中，我们希望将期望E(Y|X)建模为某个函数y(X), Naradaya和Watson建议将y(X)估计为某个加权平均值，并且核应该起到加权函数的作用。</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/d971ab84562c2568b8c55d3b398bfbe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*xaoxMMkD_giG4i6l4UikFw.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Nadaraya-Watson estimator</figcaption></figure><p id="88c9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在这一章的最后有一类非常重要的算法，特别是对于时间序列——<strong class="jw ir">高斯过程</strong>。如果在贝叶斯线性或逻辑回归中我们设置了权重分布的先验，为什么在<em class="lw"> f(w，x) = w_0 + w_1*x_1 </em>到<em class="lw"> w_0 + w_1 * ф_i(x) + … + ф_n(x) </em>中我们不能直接定义函数<em class="lw"> f </em>的概率分布？事实上我们可以。高斯过程的优点是它们可以通过期望和协方差函数来定义，后者可以用核来表示，这就是它们与核方法相关的原因。下图是不同的高斯过程，取决于不同的协方差函数。</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nr"><img src="../Images/b4ddeb34732c62110d3da2bb9af9b7fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hDbViZfzNGlv_0Sy.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">different Gaussian processes</figcaption></figure><p id="b69e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">必须学习协方差函数的超参数。为了将高斯过程应用于分类问题，我们有三种主要策略:</p><ul class=""><li id="2e2b" class="mt mu iq jw b jx jy kb kc kf mv kj mw kn mx kr my mz na nb bi translated">变分推理</li><li id="9318" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">期望传播</li><li id="9408" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">拉普拉斯近似</li></ul><p id="177c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">附言<br/>关注我还可以在<a class="ae ks" href="https://www.facebook.com/rachnogstyle.blog" rel="noopener ugc nofollow" target="_blank">脸书</a>看到太短的人工智能文章，在<a class="ae ks" href="http://instagram.com/rachnogstyle" rel="noopener ugc nofollow" target="_blank"> Instagram </a>看到个人资料，在<a class="ae ks" href="https://www.linkedin.com/in/alexandr-honchar-4423b962/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>！</p></div></div>    
</body>
</html>