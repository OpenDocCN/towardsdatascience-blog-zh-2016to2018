<html>
<head>
<title>A Feature Selection Tool for Machine Learning in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中用于机器学习的特征选择工具</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0?source=collection_archive---------0-----------------------#2018-06-22">https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0?source=collection_archive---------0-----------------------#2018-06-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/52ea115605d3d99ddb1a4c52c710cbaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rqeNwjiakRS-SqsW9wCgrA.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="ef5d" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">使用 FeatureSelector 实现高效的机器学习工作流</h2></div><p id="a8d7" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">特征选择，即在数据集中寻找和选择最有用的特征的过程，是机器学习管道的关键步骤。不必要的特征降低了训练速度，降低了模型的可解释性，最重要的是，降低了测试集的泛化性能。</p><p id="69ca" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我发现自己在机器学习问题上一遍又一遍地应用特别的特征选择方法，这让我感到沮丧，于是我用 Python <a class="ae lm" href="https://github.com/WillKoehrsen/feature-selector" rel="noopener ugc nofollow" target="_blank">编写了一个特征选择类，可以在 GitHub </a>上找到。<code class="fe ln lo lp lq b">FeatureSelector</code>包括一些最常见的特征选择方法:</p><ol class=""><li id="1f03" class="lr ls jb ks b kt ku kw kx kz lt ld lu lh lv ll lw lx ly lz bi translated"><strong class="ks jc">缺失值百分比高的特征</strong></li><li id="93ce" class="lr ls jb ks b kt ma kw mb kz mc ld md lh me ll lw lx ly lz bi translated"><strong class="ks jc">共线(高度相关)特征</strong></li><li id="5217" class="lr ls jb ks b kt ma kw mb kz mc ld md lh me ll lw lx ly lz bi translated"><strong class="ks jc">在基于树的模型中具有零重要性的特征</strong></li><li id="c0d2" class="lr ls jb ks b kt ma kw mb kz mc ld md lh me ll lw lx ly lz bi translated"><strong class="ks jc">重要性低的特征</strong></li><li id="f416" class="lr ls jb ks b kt ma kw mb kz mc ld md lh me ll lw lx ly lz bi translated"><strong class="ks jc">具有单一唯一值的特征</strong></li></ol><p id="4c1d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在本文中，我们将在一个示例机器学习数据集上使用<code class="fe ln lo lp lq b">FeatureSelector</code>。我们将看到它如何允许我们快速实现这些方法，从而实现更高效的工作流。</p><p id="f422" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">完整的代码可以在 GitHub 上获得，我鼓励任何贡献。功能选择器是一项正在进行的工作，并将根据社区需求继续改进！</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h2 id="a514" class="mm mn jb bd mo mp mq dn mr ms mt dp mu kz mv mw mx ld my mz na lh nb nc nd ne bi translated">示例数据集</h2><p id="fe65" class="pw-post-body-paragraph kq kr jb ks b kt nf kc kv kw ng kf ky kz nh lb lc ld ni lf lg lh nj lj lk ll ij bi translated">对于这个例子，我们将使用 Kaggle 上的<a class="ae lm" href="https://www.kaggle.com/c/home-credit-default-risk" rel="noopener ugc nofollow" target="_blank">家庭信用违约风险机器学习竞赛</a>的数据样本。(参赛入门，见<a class="ae lm" rel="noopener" target="_blank" href="/machine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426">本文</a>)。整个数据集可以从下载<a class="ae lm" href="https://www.kaggle.com/c/home-credit-default-risk/data" rel="noopener ugc nofollow" target="_blank">，这里我们将使用一个示例来说明。</a></p><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/02545c83b290f70e82d7e01b360eafb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W0qSMsheaWsXJBJ7i2pH4g.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Example data. TARGET is the label for classification</figcaption></figure><p id="b2f3" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">竞争是一个监督分类问题，这是一个很好的数据集，因为它有许多缺失值、许多高度相关(共线)的要素和许多对机器学习模型没有帮助的不相关要素。</p><h2 id="1c53" class="mm mn jb bd mo mp mq dn mr ms mt dp mu kz mv mw mx ld my mz na lh nb nc nd ne bi translated">创建实例</h2><p id="f354" class="pw-post-body-paragraph kq kr jb ks b kt nf kc kv kw ng kf ky kz nh lb lc ld ni lf lg lh nj lj lk ll ij bi translated">为了创建一个<code class="fe ln lo lp lq b">FeatureSelector</code>类的实例，我们需要传递一个行中有观察值、列中有特性的结构化数据集。我们可以使用一些只有特征的方法，但是基于重要性的方法也需要训练标签。由于我们有一个监督分类任务，我们将使用一组特征和一组标签。</p><p id="756c" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">(确保在与<code class="fe ln lo lp lq b">feature_selector.py</code>相同的目录下运行此程序)</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="2ebf" class="mm mn jb lq b gy nx ny l nz oa">from feature_selector import FeatureSelector</span><span id="7988" class="mm mn jb lq b gy ob ny l nz oa"># Features are in train and labels are in train_labels<br/>fs = FeatureSelector(data = train, labels = train_labels)</span></pre><h2 id="3bfa" class="mm mn jb bd mo mp mq dn mr ms mt dp mu kz mv mw mx ld my mz na lh nb nc nd ne bi translated">方法</h2><p id="5a0a" class="pw-post-body-paragraph kq kr jb ks b kt nf kc kv kw ng kf ky kz nh lb lc ld ni lf lg lh nj lj lk ll ij bi translated">要素选择器有五种方法来查找要移除的要素。我们可以访问任何已识别的特征，并手动将其从数据中删除，或使用特征选择器中的<code class="fe ln lo lp lq b">remove</code>功能。</p><p id="c1fa" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在这里，我们将介绍每种识别方法，并展示如何同时运行所有 5 种方法。<code class="fe ln lo lp lq b">FeatureSelector</code>另外还有几个绘图功能，因为视觉检查数据是机器学习的一个重要组成部分。</p><h1 id="21f7" class="oc mn jb bd mo od oe of mr og oh oi mu kh oj ki mx kk ok kl na kn ol ko nd om bi translated">缺少值</h1><p id="6135" class="pw-post-body-paragraph kq kr jb ks b kt nf kc kv kw ng kf ky kz nh lb lc ld ni lf lg lh nj lj lk ll ij bi translated">查找要移除的要素的第一种方法很简单:查找缺失值比例高于指定阈值的要素。下面的调用识别缺失值超过 60%的特征(输出<strong class="ks jc">粗体</strong>)。</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="14c9" class="mm mn jb lq b gy nx ny l nz oa">fs.identify_missing(missing_threshold = 0.6)</span><span id="77e3" class="mm mn jb lq b gy ob ny l nz oa"><strong class="lq jc">17 features with greater than 0.60 missing values.</strong></span></pre><p id="c659" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们可以看到数据帧中每一列缺失值的比例:</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="5026" class="mm mn jb lq b gy nx ny l nz oa">fs.missing_stats.head()</span></pre><figure class="nl nm nn no gt is gh gi paragraph-image"><div class="gh gi on"><img src="../Images/ea23c948ec8deb48a3d6fe326a912b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*fpLJQBGZWhQXPFG5FyA1kg.png"/></div></figure><p id="f3e6" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">为了查看被识别为要删除的特性，我们访问了<code class="fe ln lo lp lq b">FeatureSelector</code>的<code class="fe ln lo lp lq b">ops</code>属性，这是一个 Python 字典，特性在值中作为列表。</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="cdef" class="mm mn jb lq b gy nx ny l nz oa">missing_features = fs.ops['missing']<br/>missing_features[:5]</span><span id="c942" class="mm mn jb lq b gy ob ny l nz oa"><strong class="lq jc">['OWN_CAR_AGE',<br/> 'YEARS_BUILD_AVG',<br/> 'COMMONAREA_AVG',<br/> 'FLOORSMIN_AVG',<br/> 'LIVINGAPARTMENTS_AVG']</strong></span></pre><p id="60ca" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">最后，我们绘制了所有特征中缺失值的分布图:</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="4f98" class="mm mn jb lq b gy nx ny l nz oa">fs.plot_missing()</span></pre><figure class="nl nm nn no gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/382b33435252ab5600e28d8ed91b89be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*0WBIKN83twXyWfyx9LG7Qg.png"/></div></figure><h1 id="d60e" class="oc mn jb bd mo od oe of mr og oh oi mu kh oj ki mx kk ok kl na kn ol ko nd om bi translated">共线特征</h1><p id="2fd5" class="pw-post-body-paragraph kq kr jb ks b kt nf kc kv kw ng kf ky kz nh lb lc ld ni lf lg lh nj lj lk ll ij bi translated"><a class="ae lm" href="https://www.quora.com/Why-is-multicollinearity-bad-in-laymans-terms-In-feature-selection-for-a-regression-model-intended-for-use-in-prediction-why-is-it-a-bad-thing-to-have-multicollinearity-or-highly-correlated-independent-variables" rel="noopener ugc nofollow" target="_blank">共线特征</a>是彼此高度相关的特征。在机器学习中，由于高方差和较低的模型可解释性，这些会导致测试集上的泛化性能下降。</p><p id="1fda" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><code class="fe ln lo lp lq b">identify_collinear</code>方法基于指定的<a class="ae lm" href="http://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/" rel="noopener ugc nofollow" target="_blank">相关系数</a>值寻找共线特征。对于每对相关的特征，它识别要移除的特征之一(因为我们只需要移除一个):</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="1161" class="mm mn jb lq b gy nx ny l nz oa">fs.identify_collinear(correlation_threshold = 0.98)</span><span id="dbbe" class="mm mn jb lq b gy ob ny l nz oa"><strong class="lq jc">21 features with a correlation magnitude greater than 0.98.</strong></span></pre><p id="9be5" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们可以用相关性做一个简洁的可视化，这就是热图。这将显示至少有一个相关性高于阈值的所有要素:</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="8df4" class="mm mn jb lq b gy nx ny l nz oa">fs.plot_collinear()</span></pre><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/4a6c1c15a06af867865c4460d86a3316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_gK6g3YWylcgfL5Bz8JMUg.png"/></div></div></figure><p id="1249" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">和以前一样，我们可以访问将被移除的相关要素的完整列表，或者在数据帧中查看高度相关的要素对。</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="9726" class="mm mn jb lq b gy nx ny l nz oa"># list of collinear features to remove<br/>collinear_features = fs.ops['collinear']</span><span id="a8ee" class="mm mn jb lq b gy ob ny l nz oa"># dataframe of collinear features<br/>fs.record_collinear.head()</span></pre><figure class="nl nm nn no gt is gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/d3b9456781aeb553b9f3d5319b398e64.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*unCzyN2BgucGodbioUz-Kw.png"/></div></figure><p id="621c" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">如果我们想要研究我们的数据集，我们也可以通过向调用传递<code class="fe ln lo lp lq b">plot_all = True</code>来绘制数据中所有相关性的图表:</p><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi or"><img src="../Images/981d8e657b1f3c9ea52387de12b15014.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fcLsRYskgzWxVoxj4npfvg.png"/></div></div></figure><h1 id="b2aa" class="oc mn jb bd mo od oe of mr og oh oi mu kh oj ki mx kk ok kl na kn ol ko nd om bi translated">零重要性特征</h1><p id="aa73" class="pw-post-body-paragraph kq kr jb ks b kt nf kc kv kw ng kf ky kz nh lb lc ld ni lf lg lh nj lj lk ll ij bi translated">前两种方法可以应用于任何结构化数据集，并且是<strong class="ks jc">确定性的</strong> —对于给定的阈值，每次结果都是相同的。下一种方法仅设计用于监督机器学习问题，其中我们有用于训练模型的标签，并且是不确定的。<code class="fe ln lo lp lq b">identify_zero_importance </code>函数根据梯度推进机器(GBM)学习模型查找重要性为零的特征。</p><p id="06e6" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">利用基于树的机器学习模型，<a class="ae lm" href="https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/" rel="noopener ugc nofollow" target="_blank">比如 boosting 集成，我们可以发现特征的重要性。</a>重要性的绝对值不如相对值重要，相对值可用于确定任务的最相关特征。我们还可以通过移除零重要性特征来使用特征重要性进行特征选择。在基于树的模型中，重要性为零的<a class="ae lm" href="https://www.salford-systems.com/blog/dan-steinberg/what-is-the-variable-importance-measure" rel="noopener ugc nofollow" target="_blank">特征不用于分割任何节点</a>，因此我们可以在不影响模型性能的情况下移除它们。</p><p id="e121" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><code class="fe ln lo lp lq b">FeatureSelector</code>使用梯度推进机器从<a class="ae lm" href="http://lightgbm.readthedocs.io" rel="noopener ugc nofollow" target="_blank"> LightGBM 库</a>中找到特征重要性。为了减少方差，在 GBM 的 10 次训练运行中对特征重要性进行平均。此外，使用验证集的早期停止来训练模型(有一个选项可以关闭它)，以防止过度拟合训练数据。</p><p id="f780" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">下面的代码调用方法并提取零重要性特征:</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="0149" class="mm mn jb lq b gy nx ny l nz oa"># Pass in the appropriate parameters<br/>fs.identify_zero_importance(task = 'classification', <br/>                            eval_metric = 'auc', <br/>                            n_iterations = 10, <br/>                             early_stopping = True)</span><span id="d609" class="mm mn jb lq b gy ob ny l nz oa"># list of zero importance features<br/>zero_importance_features = fs.ops['zero_importance']</span><span id="7dd2" class="mm mn jb lq b gy ob ny l nz oa"><strong class="lq jc">63 features with zero importance after one-hot encoding.</strong></span></pre><p id="45dc" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们传入的参数如下:</p><ul class=""><li id="4094" class="lr ls jb ks b kt ku kw kx kz lt ld lu lh lv ll os lx ly lz bi translated"><code class="fe ln lo lp lq b">task</code>:对应我们问题的“分类”或“回归”</li><li id="dc9d" class="lr ls jb ks b kt ma kw mb kz mc ld md lh me ll os lx ly lz bi translated"><code class="fe ln lo lp lq b">eval_metric</code>:用于提前停止的度量(如果提前停止被禁用，则不需要)</li><li id="d19a" class="lr ls jb ks b kt ma kw mb kz mc ld md lh me ll os lx ly lz bi translated"><code class="fe ln lo lp lq b">n_iterations</code>:平均特征重要性的训练运行次数</li><li id="4e99" class="lr ls jb ks b kt ma kw mb kz mc ld md lh me ll os lx ly lz bi translated"><code class="fe ln lo lp lq b">early_stopping</code>:是否使用提前停止来训练模型</li></ul><p id="3d73" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这次我们用<code class="fe ln lo lp lq b">plot_feature_importances</code>得到两个图:</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="1037" class="mm mn jb lq b gy nx ny l nz oa"># plot the feature importances<br/>fs.plot_feature_importances(threshold = 0.99, plot_n = 12)</span><span id="8913" class="mm mn jb lq b gy ob ny l nz oa"><strong class="lq jc">124 features required for 0.99 of cumulative importance</strong></span></pre><div class="nl nm nn no gt ab cb"><figure class="ot is ou ov ow ox oy paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/dcd8170f3fe523c6ab5dff8825601fe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*hWCOAEWkH4z5BKKqkFAd1g.png"/></div></figure><figure class="ot is oz ov ow ox oy paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/99291b99a46ec20495d17bf477538d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*HJk89EkbcmriiWbxpV6Uew.png"/></div></figure></div><p id="74ac" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">左边是<code class="fe ln lo lp lq b">plot_n</code>个最重要的特征(根据归一化重要性绘制，总和为 1)。右边是累积重要性与特征数量的关系。垂直线画在累积重要性的<code class="fe ln lo lp lq b">threshold</code>处，在本例中为 99%。</p><p id="d885" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">对于基于重要性的方法，需要记住两点:</p><ul class=""><li id="f09a" class="lr ls jb ks b kt ku kw kx kz lt ld lu lh lv ll os lx ly lz bi translated">训练梯度推进机器是随机的，这意味着<em class="pa">特征的重要性将在每次模型运行时发生变化</em></li></ul><p id="4a4a" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这应该不会有很大的影响(最重要的特性不会突然变得最不重要)，但是会改变一些特性的顺序。它还会影响所识别的零重要性特征的数量。如果特性的重要性每次都发生变化，不要感到惊讶！</p><ul class=""><li id="f9ef" class="lr ls jb ks b kt ku kw kx kz lt ld lu lh lv ll os lx ly lz bi translated">为了训练机器学习模型，首先对特征进行<em class="pa">一键编码</em>。这意味着某些被标识为重要性为 0 的要素可能是在建模过程中添加的一次性编码要素。</li></ul><p id="0ae5" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">当我们到达特征移除阶段时，有一个选项来移除任何添加的独热编码特征。然而，如果我们在特征选择之后进行机器学习，我们无论如何都要对特征进行一次性编码！</p><h1 id="1622" class="oc mn jb bd mo od oe of mr og oh oi mu kh oj ki mx kk ok kl na kn ol ko nd om bi translated">低重要性特征</h1><p id="af97" class="pw-post-body-paragraph kq kr jb ks b kt nf kc kv kw ng kf ky kz nh lb lc ld ni lf lg lh nj lj lk ll ij bi translated">下一种方法建立在零重要性函数的基础上，使用模型中的特征重要性进行进一步选择。函数<code class="fe ln lo lp lq b">identify_low_importance</code>查找对指定的总重要性没有贡献的最低重要性特征。</p><p id="ade4" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">例如，下面的调用查找实现 99%的总重要性不需要的最不重要的功能:</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="a322" class="mm mn jb lq b gy nx ny l nz oa">fs.identify_low_importance(cumulative_importance = 0.99)</span><span id="c78d" class="mm mn jb lq b gy ob ny l nz oa"><strong class="lq jc">123 features required for cumulative importance of 0.99 after one hot encoding.<br/>116 features do not contribute to cumulative importance of 0.99.</strong></span></pre><p id="5fe6" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">基于累积重要性的图和该信息，梯度推进机器认为许多特征与学习无关。同样，这种方法的结果会在每次训练中发生变化。</p><p id="0d2e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">要查看数据框架中所有特征的重要性:</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="bcba" class="mm mn jb lq b gy nx ny l nz oa">fs.feature_importances.head(10)</span></pre><figure class="nl nm nn no gt is gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/b907f436e4f4f198987b11d0fb0814f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*d1uRrw212LAmpjlszj7CFg.png"/></div></figure><p id="c599" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><code class="fe ln lo lp lq b">low_importance</code>方法借鉴了<a class="ae lm" rel="noopener" target="_blank" href="/pca-using-python-scikit-learn-e653f8989e60">的一种方法，使用主成分分析(PCA) </a>，其中通常只保留需要保留一定百分比方差(如 95%)的 PC。占总重要性的百分比是基于同样的想法。</p><p id="cfb1" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">基于特征重要性的方法实际上仅在我们要使用基于树的模型进行预测时才适用。除了随机性之外，基于重要性的方法是一种黑盒方法，因为我们并不真正知道为什么模型认为特征是不相关的。如果使用这些方法，运行几次，看看结果如何变化，也许可以创建多个带有不同参数的数据集进行测试！</p><h1 id="f20b" class="oc mn jb bd mo od oe of mr og oh oi mu kh oj ki mx kk ok kl na kn ol ko nd om bi translated">单一独特价值特征</h1><p id="cbec" class="pw-post-body-paragraph kq kr jb ks b kt nf kc kv kw ng kf ky kz nh lb lc ld ni lf lg lh nj lj lk ll ij bi translated">最后一个方法相当简单:<a class="ae lm" href="https://github.com/Featuretools/featuretools/blob/master/featuretools/selection/selection.py" rel="noopener ugc nofollow" target="_blank">查找任何具有唯一值的列。</a>只有一个唯一值的特征不能用于机器学习，因为这个<a class="ae lm" href="https://www.r-bloggers.com/near-zero-variance-predictors-should-we-remove-them/" rel="noopener ugc nofollow" target="_blank">特征的方差为零</a>。例如，基于树的模型永远不能对只有一个值的特征进行分割(因为没有组来划分观察值)。</p><p id="591a" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">与其他方法不同，这里没有可供选择的参数:</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="63ff" class="mm mn jb lq b gy nx ny l nz oa">fs.identify_single_unique()</span><span id="cd7b" class="mm mn jb lq b gy ob ny l nz oa"><strong class="lq jc">4 features with a single unique value.</strong></span></pre><p id="cc26" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们可以绘制每个类别中唯一值数量的直方图:</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="798a" class="mm mn jb lq b gy nx ny l nz oa">fs.plot_unique()</span></pre><figure class="nl nm nn no gt is gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/7ecec9f5a0ac270b36279368a453697e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*F3BV5mUWG-GLP8gnS62Z6w.png"/></div></figure><p id="1711" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">需要记住的一点是，默认情况下，<code class="fe ln lo lp lq b">NaNs</code>在<a class="ae lm" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.nunique.html" rel="noopener ugc nofollow" target="_blank">计算 Pandas 中的唯一值之前被删除。</a></p><h1 id="0bd2" class="oc mn jb bd mo od oe of mr og oh oi mu kh oj ki mx kk ok kl na kn ol ko nd om bi translated">移除功能</h1><p id="fe68" class="pw-post-body-paragraph kq kr jb ks b kt nf kc kv kw ng kf ky kz nh lb lc ld ni lf lg lh nj lj lk ll ij bi translated">一旦我们确定了要丢弃的特性，我们有两个移除它们的选项。所有要删除的特征都存储在<code class="fe ln lo lp lq b">FeatureSelector</code>的<code class="fe ln lo lp lq b">ops</code>字典中，我们可以使用列表手动删除特征。另一种选择是使用<code class="fe ln lo lp lq b">remove</code>内置函数。</p><p id="26f0" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">对于这个方法，我们传入<code class="fe ln lo lp lq b">methods</code>来用来移除特性。如果我们想使用所有实现的方法，我们只需传入<code class="fe ln lo lp lq b">methods = 'all'</code>。</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="676e" class="mm mn jb lq b gy nx ny l nz oa"># Remove the features from all methods (returns a df)<br/>train_removed = fs.remove(methods = 'all')</span><span id="3b36" class="mm mn jb lq b gy ob ny l nz oa"><strong class="lq jc">['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance'] methods have been run<br/><br/>Removed 140 features.</strong></span></pre><p id="8ecf" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">该方法返回一个移除了特征的数据帧。要移除在机器学习过程中创建的一次性编码特征:</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="d6fc" class="mm mn jb lq b gy nx ny l nz oa">train_removed_all = fs.remove(methods = 'all', keep_one_hot=False)</span><span id="2c07" class="mm mn jb lq b gy ob ny l nz oa"><strong class="lq jc">Removed 187 features including one-hot features.</strong></span></pre><p id="049c" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在继续操作之前检查将要删除的功能可能是个好主意！原始数据集存储在<code class="fe ln lo lp lq b">FeatureSelector</code>的<code class="fe ln lo lp lq b">data</code>属性中作为备份！</p><h1 id="4842" class="oc mn jb bd mo od oe of mr og oh oi mu kh oj ki mx kk ok kl na kn ol ko nd om bi translated">同时运行所有方法</h1><p id="c6d8" class="pw-post-body-paragraph kq kr jb ks b kt nf kc kv kw ng kf ky kz nh lb lc ld ni lf lg lh nj lj lk ll ij bi translated">我们可以通过<code class="fe ln lo lp lq b">identify_all</code>使用所有的方法，而不是单独使用这些方法。这需要每个方法的参数字典:</p><pre class="nl nm nn no gt nt lq nu nv aw nw bi"><span id="3a71" class="mm mn jb lq b gy nx ny l nz oa">fs.identify_all(selection_params = {'missing_threshold': 0.6,    <br/>                                    'correlation_threshold': 0.98, <br/>                                    'task': 'classification',    <br/>                                    'eval_metric': 'auc', <br/>                                    'cumulative_importance': 0.99})</span><span id="8a74" class="mm mn jb lq b gy ob ny l nz oa"><strong class="lq jc">151 total features out of 255 identified for removal after one-hot encoding.</strong></span></pre><p id="3960" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">请注意，由于我们重新运行了模型，总特征的数量将会发生变化。然后可以调用<code class="fe ln lo lp lq b">remove</code>函数来丢弃这些特征。</p><h1 id="2c62" class="oc mn jb bd mo od oe of mr og oh oi mu kh oj ki mx kk ok kl na kn ol ko nd om bi translated">结论</h1><p id="a73a" class="pw-post-body-paragraph kq kr jb ks b kt nf kc kv kw ng kf ky kz nh lb lc ld ni lf lg lh nj lj lk ll ij bi translated">特征选择器类实现了几个常见的<a class="ae lm" href="https://machinelearningmastery.com/an-introduction-to-feature-selection/" rel="noopener ugc nofollow" target="_blank">操作，用于在训练机器学习模型之前移除特征</a>。它提供了用于识别要移除的特征以及可视化的功能。方法可以单独运行，也可以同时运行，以实现高效的工作流程。</p><p id="45c8" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><code class="fe ln lo lp lq b">missing</code>、<code class="fe ln lo lp lq b">collinear</code>和<code class="fe ln lo lp lq b">single_unique</code>方法是确定性的，而基于特征重要性的方法会随着每次运行而改变。特征选择，很像机器学习的<a class="ae lm" href="https://hips.seas.harvard.edu/blog/2012/12/24/the-empirical-science-of-machine-learning-evaluating-rbms/" rel="noopener ugc nofollow" target="_blank">领域，很大程度上是经验性的</a>，需要测试多个组合来找到最佳答案。最佳实践是在管道中尝试几种配置，特性选择器提供了一种快速评估特性选择参数的方法。</p><p id="9b67" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">一如既往，我欢迎反馈和建设性的批评。我想强调的是，我正在寻求关于<code class="fe ln lo lp lq b">FeatureSelector</code>的帮助。任何人都可以<a class="ae lm" href="https://github.com/WillKoehrsen/feature-selector" rel="noopener ugc nofollow" target="_blank">在 GitHub </a>上做出贡献，我感谢那些刚刚使用这个工具的人的建议！也可以通过 Twitter <a class="ae lm" href="http://twitter.com/@koehrsen_will" rel="noopener ugc nofollow" target="_blank"> @koehrsen_will </a>联系到我。</p></div></div>    
</body>
</html>