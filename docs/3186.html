<html>
<head>
<title>Learning Backpropagation from Geoffrey Hinton</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">向杰弗里·辛顿学习反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-backpropagation-from-geoffrey-hinton-619027613f0?source=collection_archive---------6-----------------------#2018-04-17">https://towardsdatascience.com/learning-backpropagation-from-geoffrey-hinton-619027613f0?source=collection_archive---------6-----------------------#2018-04-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jn jo jp jq"><div class="bz fp l di"><div class="jr js l"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Image from this <a class="ae jx" href="https://gph.is/28YKpua" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="aefe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">机器学习掌握的所有路径都要经过反向传播。</p><p id="fd30" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">自从开始我的<a class="ae jx" href="https://medium.com/@rgotesman1/learning-machine-learning-part-0-the-setup-7ad1fc053d8f" rel="noopener">机器学习</a>之旅以来，我最近第一次发现自己被难住了。我一直在稳步完成吴恩达广受欢迎的 ML 课程。</p><p id="0802" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">线性回归。检查。逻辑回归。检查。梯度下降。检查检查检查。</p><p id="772d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后我们到了神经网络和用来训练它们的算法:反向传播。尽管安德鲁尽了最大努力，我还是不明白这项技术是如何工作的。尽管 Andrew 向我们保证这没什么，你可以在没有更深入理解的情况下使用神经网络，而且他自己也这样做了很多年，但我决心更好地理解这个概念。</p><p id="4de4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了做到这一点，我求助于大师 Geoffrey Hinton 和他合著的 1986 年《自然》<a class="ae jx" href="http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>,其中首次提出了反向传播(几乎 15000 次引用！).我会鼓励每个人读报纸。它很短，只有 4 页，在详细研究之后，我对反向传播有了更好的理解，现在我将试着传授给大家。</p><blockquote class="kw"><p id="4598" class="kx ky iq bd kz la lb lc ld le lf kv dk translated">本质上，反向传播只是链式法则的巧妙应用。</p></blockquote><p id="a784" class="pw-post-body-paragraph jy jz iq ka b kb lg kd ke kf lh kh ki kj li kl km kn lj kp kq kr lk kt ku kv ij bi translated">链式法则是任何本科课程中教授的导数的一个基本性质。它指出，如果你有 3 个函数<em class="ll"> f </em>、<em class="ll"> g </em>和<em class="ll"> h </em>，其中<em class="ll"> f </em>是<em class="ll"> g </em>的函数，而<em class="ll"> g </em>是<em class="ll"> h </em>的函数，那么<em class="ll"> f </em>关于<em class="ll"> h </em>的导数等于<em class="ll"> f </em>关于的导数的乘积</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/4f3a86e5a0d83c478c93602b6f6669a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*liFT6yx2iyAfQ8BXKuHSng.png"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">The chain rule</figcaption></figure><p id="45c2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在让我们用这个来弄清楚反向传播是如何工作的。</p><p id="02bf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">假设我们正在使用下面的简单神经网络。这个网络有三层:蓝色的输入层，绿色的隐藏层和红色的输出层。前一层中的每个单元都连接到下一层中的每个单元。每个连接都有一个权重，每当一个单元向另一个单元发送信息时，该信息就会乘以该连接的权重。一个单元从上一层获得与其连接的所有输入的总和，对该总和执行逻辑函数，并将该值向前传递。</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/fe1feacfdd1fbd6a5fa835774f587b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*WZKw_W4GLbTsg_yYxjZOtw.png"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">A 3–4–3 Neural Network</figcaption></figure><p id="8f8d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们想象我们被给予了<em class="ll"> m </em>个训练的例子:</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/15455c02ea4f21cb06364942d8735484.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*RkRtJbQVEDfAMib27Kk2CA.png"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">i-th input output pair</figcaption></figure><p id="1c1c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<em class="ll"> x </em>和<em class="ll"> y </em>为三维向量，<em class="ll"> i </em>为第 I 个训练样本。对于输入<em class="ll"> x </em>，我们将我们的神经网络的预测/输出称为<em class="ll"> g </em>，它也是一个三维向量，每个向量元素对应一个输出单元。因此，对于每个培训示例，我们都有:</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lv"><img src="../Images/1b09912e706ac79ed56878fdfea78c75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UPCOUu6trB1G7xCbCnIfJg.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Vector form of training Input, output and neural network prediction</figcaption></figure><p id="1dc3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">给定输入<em class="ll"> x </em>，我们希望找到导致<em class="ll"> g </em>等于(或至少非常相似)y 的权重值。</p><p id="f67f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为此，我们将使用一个误差函数，我们定义为:</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/4184d5485a4aa274246b138b6af8d73e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*Ofoy4OO_vTS_hjIOOa7-SQ.png"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Total error for the Neural Network above</figcaption></figure><p id="2620" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了计算总误差，我们采用训练集中的每个案例<em class="ll"> i </em>，并针对红色输出层中的每个单元，计算该单元的预测和真实输出之间的平方误差。如果我们对每种情况都这样做，我们将得到总误差。</p><p id="1b1b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于网络预测的值<em class="ll"> g </em>依赖于网络的权重，我们可以看到误差会随着权重的变化而变化，我们希望找到使<em class="ll"> E </em>最小的权重。</p><p id="0fe5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以使用梯度下降来实现。但是梯度下降要求我们找到<em class="ll"> E </em>相对于每个权重的导数。这就是反向传播让我们实现的目标。</p><p id="3330" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们现在将考虑单一情况，而不是 3 个输出单元，假设我们有任意数量的输出单元<em class="ll"> n </em>。对于这种情况，我们的错误现在变成了:</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/cae06161c2ced7178e8806ff1b8c83dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*Xx9t2WNPU84LrBMPnW4KAQ.png"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Error for a single case</figcaption></figure><p id="904e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了方便起见，我们去掉了下标，因为它是常数。</p><p id="6510" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们现在可以问自己，当一个输出单位的预测变化时，这个误差是如何变化的。这是简单的导数:</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mc"><img src="../Images/ad4ab967c39b309991229ad004e8537a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ELyAlgRXv2YF9dadFbWTw.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">The derivative of the total error with respect to each output unit (Note that we used the chain rule to the derivative of the squared term.)</figcaption></figure><p id="f638" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有趣的是，似乎随着一个输出单位的预测值的变化，误差将以等于预测值和真实值之间的“误差”或差异的速率变化。酷！</p><p id="5474" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，我们可能会问自己，当这些输出单元的总输入发生变化时，误差会如何变化。同样，这只是一个导数。让我们用<em class="ll"> z </em>来表示一个输出单元的总输入。那么我们有兴趣发现:</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div class="gh gi md"><img src="../Images/5a1b9e610366f200f7405d275b7eae49.png" data-original-src="https://miro.medium.com/v2/resize:fit:136/format:webp/1*n3U0XUevU6XTNY4JgKuXVQ.png"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Derivative of E with respect to the total input for output unit j</figcaption></figure><p id="4246" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是由于<em class="ll"> g </em>是<em class="ll"> z </em>的函数，通过应用链式法则，我们可以将其重写为:</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div class="gh gi me"><img src="../Images/114e005b334c19bca4c1ea7e24a520e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*yd-avShwmQ8FYNhmS11G2w.png"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Applying the chain rule!!</figcaption></figure><p id="4ad6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">还记得我们说过，每个单元在传递输入之前，都将逻辑函数应用于它的输入。这意味着<em class="ll"> g </em>是一个逻辑函数，<em class="ll"> z </em>是它的输入，所以我们可以写:</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mf"><img src="../Images/84440128cb7d5bb1ba4408c9d6c1cb2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lhaWQwzkxEHWLopOwNEmLg.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Logistic function and it’s derivative</figcaption></figure><p id="c764" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以现在我们可以说:</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/141f8ad31e14683f1cb373f33d52dc34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*UotmwOymo0y7KN3Mw25tcA.png"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">The derivative of the total error with respect to the total input to an output unit</figcaption></figure><p id="93b0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们已经计算出，当一个输出单元的总输入发生变化时，误差是如何变化的。太神奇了！</p><p id="b93b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们现在可以找到误差对某些重量的导数。这就是我们梯度下降所需要的。</p><p id="50d8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们将绿色单元的预测/输出称为<em class="ll">g’</em>，并且我们将绿色层中的单元<em class="ll"> k </em>和红色/输出层中的单元<em class="ll"> j </em>之间的连接的权重称为:</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/2d257dbbb06c33845d9f5e0b27a17d3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:150/format:webp/1*DBoaRUm4s4GMqhHWz3a-2Q.png"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Weight of connection between unit k in green and unit j in red layers</figcaption></figure><p id="d01c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">考虑下面黄色输出单元的总输入<em class="ll"> z </em>。我们可以通过计算每个绿色单元的输出，乘以连接绿色单元和黄色单元的红色箭头的重量，然后将它们相加来计算输入。</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mi"><img src="../Images/68b08412c87d097a33bde5f5d28dc622.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*G69wPf7F8mlgMzodFpI2Ww.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">The red arrows represent the connections that are added up to get the total input of the yellow unit</figcaption></figure><p id="3592" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">而不是 4，如果我们有任意数量的绿色单元<em class="ll"> n </em>(这个<em class="ll"> n </em>与我们之前定义的不同)我们就可以说:</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/85325d0c6996aef518d8bd4cfb2104b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*wdsUSlNXam5ZCIweK9CHaw.png"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Total input to output j</figcaption></figure><p id="d0e9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，我们似乎可以将<em class="ll"> z </em>写成连接到它的重量的函数和连接到它的单元的输出的函数。</p><p id="73e5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">链式法则的时间到了。</p><p id="56fd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们自问，当连接到输出单元的砝码发生变化时，误差会如何变化。这可以写成:</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/9bc090f94e94c9797753b5b775f61257.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*Dmvp51wrfov-fFJhqjiwwA.png"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">The derivative of the total error with respect to weights connecting to an output unit</figcaption></figure><p id="480e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们刚刚算出了误差相对于连接到输出层的权重的导数。这正是我们需要让梯度下降工作。</p><p id="5b89" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是我们还没完。我们仍然需要计算出误差相对于连接第一层和第二层的权重的导数。</p><p id="6fb6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">谢天谢地，链式法则也还没有完成。</p><p id="e508" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们计算当第 k 个绿色单元的输出变化时，误差如何变化:</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ml"><img src="../Images/0d7b01fe035eb1309e05d8dce93be98f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*60iQ0VZmXPsJcnbDrk5GVA.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">The derivative of error with respect to the output of unit k in the green layer</figcaption></figure><p id="0420" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于有来自单位<em class="ll"> k </em>的<em class="ll"> j </em>重量，我们认为:</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mm"><img src="../Images/965f22c309f623f1bac1487ceee811c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8FPikppW3Gc_xX0N2fPmPQ.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">The derivative of total error with respect to the output of unit k in the green layer</figcaption></figure><p id="a513" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你仔细想想，我们又回到了起点。我们得到了误差对某个单位输出的导数。我们现在可以完全忽略红色输出层，将绿色层视为网络的最后一层，并重复上述所有步骤来计算<em class="ll"> E </em>相对于传入权重的导数。</p><figure class="ln lo lp lq gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mn"><img src="../Images/1c846af9a591b2e1c2565102e6b50f46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MSbZ28DzF5neVHKZ."/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Image from this <a class="ae jx" href="https://unsplash.com/photos/PuZeaoMB2A8" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="1563" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你会注意到我们计算的一阶导数等于预测值和真实输出值之间的“误差”。同样，最后一个导数也有这个误差项，乘以一些其他的项。该算法被称为反向传播，因为这种误差的一种形式从最后一层反向传播到第一层，并用于计算<em class="ll"> E </em>相对于网络中每个权重的导数。</p><p id="a21c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦计算出这些导数，我们就可以在梯度下降中使用它们来最小化<em class="ll"> E </em>并训练我们的神经网络。</p><p id="617a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">希望这篇文章能让你更好地理解反向传播是如何工作的。如果你有任何问题，请告诉我，我会尽力回答你。谢谢！</p></div></div>    
</body>
</html>