<html>
<head>
<title>Machine Learning, Fast and Slow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习，快与慢</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-fast-and-slow-eb16b732fb5f?source=collection_archive---------6-----------------------#2016-09-12">https://towardsdatascience.com/machine-learning-fast-and-slow-eb16b732fb5f?source=collection_archive---------6-----------------------#2016-09-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="853e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">模型的记忆和概括</h2></div><p id="a386" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设你经营一家名为米其林人的餐厅点评网站。你从用户那里收集了大量评论，并想分析它们是否<strong class="kh ir">连贯</strong>。</p><p id="2077" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一位刚刚在意大利餐厅用餐的用户写道:</p><blockquote class="lb lc ld"><p id="b810" class="kf kg le kh b ki kj jr kk kl km ju kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">我喜欢这种调料。</p></blockquote><p id="53fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解决这个问题的一个常见方法是在单词级别查看消息。</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="79bc" class="lr ls iq ln b gy lt lu l lv lw">["i", "like", "the", "dressing", "."]</span></pre><h2 id="2ee5" class="lr ls iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated">一般化</h2><p id="1739" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">你可以从理解每个单词的意思开始，通过在大量文本数据上训练一个模型来生成<strong class="kh ir">单词表示</strong>。<a class="ae mt" href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/" rel="noopener ugc nofollow" target="_blank">有很多技术可以做到这一点</a>，但本质上，一个单词最终将由一个数字向量来表示。</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="4932" class="lr ls iq ln b gy lt lu l lv lw">like = [.03, -1.45, ..., 0.45, 0.74]</span></pre><p id="26fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为这些单词表示是在一大组文本(比如整个互联网)中学习的，所以它们是可概括的。换句话说，你可以把单词“like”的意思编码出来，并把它们放在各种其他的上下文中(<em class="le">例如</em> SMS 文本、短文写作等)。)而且总体来说，它们会非常合适。</p><p id="533e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您在向量空间中可视化这些表示，您将看到相似的单词将聚集在一起。</p><figure class="li lj lk ll gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mu"><img src="../Images/276626af74a05f6da9c47404487d3c63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Q1avtrQ96wy7l0wex05tQ.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Word representations (hence, words) that are similar should be close in vector space. Credit: <a class="ae mt" href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/" rel="noopener ugc nofollow" target="_blank">Colah</a> and <a class="ae mt" href="http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf" rel="noopener ugc nofollow" target="_blank">Turian</a>.</figcaption></figure><p id="86ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设另一个人写道:</p><blockquote class="lb lc ld"><p id="85e5" class="kf kg le kh b ki kj jr kk kl km ju kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">我喜欢这种调料。</p></blockquote><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="af4a" class="lr ls iq ln b gy lt lu l lv lw">["i", "admire", "the", "dressing", "."]</span></pre><p id="c686" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，“钦佩”和“喜欢”被认为是同义词。因此，各自的单词表示应该相似。</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="8986" class="lr ls iq ln b gy lt lu l lv lw">word_similarity("like", "admire") = high</span></pre><p id="86ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">太好了！假设单词表示或多或少是自动学习的，这种学习单词之间关系的方法是可扩展的和有用的。</p><p id="3f16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">词语表征的概括能力可以扩展到连贯。定义连贯性的一种方法是看一对词在同一语境中是否搭配得很好。一个简单的计算方法是比较一个句子中两个单词的相似度。</p><p id="2b5e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，从上面的例子中，你可以分离出单词“dressing ”,并检查它是否与单词“like”和“appraise”兼容。</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="3929" class="lr ls iq ln b gy lt lu l lv lw">word_similarity("like", "dressing") = somewhat high</span><span id="a2c6" class="lr ls iq ln b gy ng lu l lv lw">word_similarity("admire", "dressing") = somewhat low</span></pre><p id="594e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这也有道理，“喜欢”比“佩服”更相似。</p><p id="95cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，从上面看不出“钦佩”和“穿着”是否一致。尽管人们赞美一件“衣服”并不是完全不可思议的，但更有可能的是，用户要么是有创意，要么是弄错了。</p><p id="dfa9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一般化的弱点是，它经常在一般情况之外失败，<em class="le">即平均而言</em>。尽管“表示”这个词有很多含义，但它认为将“钦佩”和“穿着”放在句子中是合理的，因为“喜欢”与“钦佩”和“穿着”都很接近。在当前的设置下，即使是从未出现过的单词也只会显示出些许相似。</p><h2 id="36cb" class="lr ls iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated">记住</h2><p id="b601" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">显示连贯性的一个简单方法是计数。通过统计上面的词对，可以清楚地看出它们以前是否存在过。</p><p id="8f06" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在谷歌 Ngram 语料库上快速扫描显示<a class="ae mt" href="https://books.google.com/ngrams/graph?content=admire+the+dressing%2C+like+the+dressing&amp;year_start=1800&amp;year_end=2000&amp;corpus=15&amp;smoothing=3&amp;share=&amp;direct_url=t1%3B%2Clike%20the%20dressing%3B%2Cc0" rel="noopener ugc nofollow" target="_blank">“赞美着装”在</a>之前从未发生过。</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="d2de" class="lr ls iq ln b gy lt lu l lv lw">counts_in_the_same_sentence("like", "dressing") = some</span><span id="2f9c" class="lr ls iq ln b gy ng lu l lv lw">counts_in_the_same_sentence("admire", "dressing") = none</span></pre><p id="16b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">计数基本上没有普遍性，因为你必须记住每一个可能的组合，这需要大量的记忆。但是这一信息是对从单词表示中得出的概括的补充。</p><p id="06b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从视觉上看，这两种方法的分布非常明显。使用计数方法，绝大多数单词对将为零，而少数单词对将具有指数级的更高计数。相似性一词更为正态分布，尽管通常是右偏的，所以高相似性聚集在一起。</p><figure class="li lj lk ll gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nh"><img src="../Images/857b334b2688e276ff40238070641ebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l8wjxSeeEpAQ3FSh7WgLLw.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Counts will approximately be an exponential distribution and word similarities will approximately be a normal distribution. Credit: <a class="ae mt" href="http://perfdynamics.blogspot.com/2012/02/on-accuracy-of-exponentials-and.html" rel="noopener ugc nofollow" target="_blank">Gunther</a>.</figcaption></figure><p id="2143" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">直观地说，这两种方法在其分布斜率最大的地方具有最强的区分能力。计数在低计数时表现良好，而单词表示在高相似度时表现良好。</p></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><p id="996b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将不同的模型混合在一起并不是什么新鲜事。<a class="ae mt" href="http://mlwave.com/kaggle-ensembling-guide/" rel="noopener ugc nofollow" target="_blank">如果你想赢得单人游戏比赛，组装几乎是必不可少的。</a></p><p id="0e05" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae mt" href="https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html" rel="noopener ugc nofollow" target="_blank">谷歌最近建立了一个“广度”和“深度”模型</a>，将记忆和归纳模型动态地结合起来。这只是将模型混合在一起的一种方式，但这是向用户展示他们喜欢什么(记忆)和他们可能喜欢什么(概括)的好方法。</p></div></div>    
</body>
</html>