<html>
<head>
<title>How Recurrent Neural Networks work</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络如何工作</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7?source=collection_archive---------0-----------------------#2017-12-02">https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7?source=collection_archive---------0-----------------------#2017-12-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/4bb578fdb99b9516fe664f30e94facff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t1v_LaNkdJAwjWMCyuEtlg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><a class="ae jd" href="https://pixabay.com" rel="noopener ugc nofollow" target="_blank">https://pixabay.com</a></figcaption></figure><div class=""/><p id="c328" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你肯定遇到过翻译自然语言的软件(Google Translate)或者把你的语音转换成文本的软件(Apple Siri ),可能一开始你很好奇它是如何工作的。</p><p id="2150" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在过去的几年里，这些系统背后的科学已经有了相当大的进步。例如，在 2016 年末，谷歌<a class="ae jd" href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html" rel="noopener ugc nofollow" target="_blank">在他们的谷歌翻译背后引入了一个新系统</a>，该系统使用了最先进的机器学习技术。改进是显著的，你可以<a class="ae jd" href="https://translate.google.com/" rel="noopener ugc nofollow" target="_blank">亲自测试一下</a>。</p><p id="2a2c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一个惊人的例子是<a class="ae jd" href="http://research.baidu.com/deep-voice-production-quality-text-speech-system-constructed-entirely-deep-neural-networks/" rel="noopener ugc nofollow" target="_blank">百度最近的文本转语音</a>:</p><figure class="lb lc ld le gt is"><div class="bz fp l di"><div class="lf lg l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Credits to Dhruv Pathasarathy for the amazing demo.</figcaption></figure><p id="0452" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么以上都有什么共同点呢？他们处理<strong class="kf jh">序列数据</strong>来做出预测。好吧，但是这和众所周知的猫图像识别器有什么不同呢？</p><p id="b005" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">想象一下你想说如果照片里有只猫。你可以使用多张有猫和没有猫的照片来训练一个<strong class="kf jh">前馈神经网络</strong>(典型的是 CNN-卷积神经网络)。</p><blockquote class="lh li lj"><p id="9624" class="kd ke lk kf b kg kh ki kj kk kl km kn ll kp kq kr lm kt ku kv ln kx ky kz la ij bi translated">在这个网络中，信息只在一个方向上移动，即从输入节点向前，通过隐藏节点(如果有的话)到达输出节点。网络中没有循环或环路。— <a class="ae jd" href="https://en.wikipedia.org/wiki/Feedforward_neural_network" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></blockquote><p id="b4a8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些网络主要用于模式识别，如下所示:</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/ef1846027f9e54dba52bf070614991df.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*8wOnrZ2UHFItcTdDNSZnTQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Feedforward neural network</figcaption></figure><p id="3c5d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">相反，为了成功处理序列数据，你需要使用<strong class="kf jh">递归(反馈)神经网络</strong>。它能够“记忆”部分输入，并使用它们进行准确的预测。这些网络是语音识别、翻译等的核心。因此，让我们深入进行更详细的解释。</p><h2 id="281e" class="lp lq jg bd lr ls lt dn lu lv lw dp lx ko ly lz ma ks mb mc md kw me mf mg mh bi translated">什么是递归神经网络？</h2><p id="23fb" class="pw-post-body-paragraph kd ke jg kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">训练典型的神经网络包括以下步骤:</p><ol class=""><li id="01a6" class="mn mo jg kf b kg kh kk kl ko mp ks mq kw mr la ms mt mu mv bi translated">从数据集中输入一个示例。</li><li id="f5ae" class="mn mo jg kf b kg mw kk mx ko my ks mz kw na la ms mt mu mv bi translated">该网络将采用该示例，并使用随机初始化的变量(称为权重和偏差)对其进行一些复杂的计算。</li><li id="7518" class="mn mo jg kf b kg mw kk mx ko my ks mz kw na la ms mt mu mv bi translated">将产生一个预测结果。</li><li id="59bc" class="mn mo jg kf b kg mw kk mx ko my ks mz kw na la ms mt mu mv bi translated">将该结果与预期值进行比较会给我们一个错误。</li><li id="14ec" class="mn mo jg kf b kg mw kk mx ko my ks mz kw na la ms mt mu mv bi translated">通过相同的路径将误差传播回来将调整变量。</li><li id="4adf" class="mn mo jg kf b kg mw kk mx ko my ks mz kw na la ms mt mu mv bi translated">重复步骤 1-5，直到我们有信心说我们的变量是定义良好的。</li><li id="736c" class="mn mo jg kf b kg mw kk mx ko my ks mz kw na la ms mt mu mv bi translated">预测是通过将这些变量应用到一个新的看不见的输入中来实现的。</li></ol><p id="d623" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，这是对神经网络的一个非常天真的解释，但是，至少，给出了一个很好的概述，并且可能对这个领域的新手有用。</p><p id="7a91" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">递归神经网络的工作方式类似，但为了清楚地了解这种差异，我们将通过最简单的模型，使用基于前一个单词预测序列中的下一个单词的任务。</p><p id="6568" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们需要使用大型数据集来训练网络。出于目的，我们可以选择任何大型文本(列夫·托尔斯泰的《战争与和平》是一个不错的选择)。当完成训练后，我们可以输入句子“拿破仑是……的皇帝”，并期望根据书中的知识做出合理的预测。</p><p id="a51c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，我们如何开始？如上所述，我们一次输入一个例子，产生一个结果，这两个结果都是单词。前馈网络的不同之处在于，在评估结果之前，我们还需要了解以前的输入。因此，您可以将 RNNs 视为多个前馈神经网络，从一个网络向另一个网络传递信息。</p><p id="5296" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们检查以下模式:</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nb"><img src="../Images/14ec1fbf5fde2839e8db6a649f92f780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4KwIUHWL3sTyguTahIxmJw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Recurrent neural network</figcaption></figure><p id="07aa" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里<em class="lk"> x_1，x_2，x_3，…，x_t </em>表示来自文本的输入单词，<em class="lk"> y_1，y_2，y_3，…，y_t </em>表示预测的下一个单词，<em class="lk"> h_0，h_1，h_2，h_3，…，h_t </em>保存前一个输入单词的信息。</p><p id="0271" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于纯文本不能用于神经网络，我们需要将单词编码成向量。最好的方法是使用<strong class="kf jh">单词嵌入</strong> ( <a class="ae jd" href="https://www.tensorflow.org/tutorials/word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec </a>或<a class="ae jd" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe </a>)但是出于本文的目的，我们将使用独热编码向量。这些是(<em class="lk"> V，1) </em>向量(<em class="lk"> V </em>是我们词汇表中的字数)，其中所有的值都是 0，除了在第<em class="lk"> i 个</em>位置的那个。例如，如果我们的词汇是<em class="lk">苹果、杏、香蕉、…、国王、…斑马</em>，单词是<em class="lk">香蕉</em>，那么向量就是<em class="lk">【0，0，1，…，0，…，0】</em>。</p><p id="c93c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通常，词汇表包含所有英语单词。这就是为什么有必要使用单词嵌入。</p><p id="72f8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们定义训练所需的等式:</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nc"><img src="../Images/f79848f8b134c4d89cf9ecf2167042e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bJonSV6knypXR9StvEBYZw.png"/></div></div></figure><ul class=""><li id="632e" class="mn mo jg kf b kg kh kk kl ko mp ks mq kw mr la nd mt mu mv bi translated">1)—保存序列中前面单词的信息。可以看到，<em class="lk"> h_t </em>是用之前的<em class="lk"> h_(t-1) </em>向量和当前的词向量<em class="lk"> x_t </em>计算出来的。我们还将非线性激活函数<em class="lk"> f </em>(通常为<a class="ae jd" href="https://theclevermachine.wordpress.com/tag/tanh-function/" rel="noopener ugc nofollow" target="_blank"> tanh </a>或<a class="ae jd" href="https://ipfs.io/ipfs/QmXoypizjW3WknFiJnKLwHCnL72vedxjQkDDP1mXWo6uco/wiki/Sigmoid_function.html" rel="noopener ugc nofollow" target="_blank"> sigmoid </a>)应用于最终求和。假设<em class="lk"> h_0 </em>是一个零向量是可以接受的。</li><li id="2dec" class="mn mo jg kf b kg mw kk mx ko my ks mz kw na la nd mt mu mv bi translated">2) —计算给定时间步长<em class="lk"> t </em>的预测字向量。我们使用<a class="ae jd" href="https://www.youtube.com/watch?v=mlaLLQofmR8" rel="noopener ugc nofollow" target="_blank"> softmax 函数</a>产生一个<em class="lk"> (V，1) </em>向量，所有元素的总和为 1。这种概率分布为我们提供了词汇表中最有可能的下一个单词的索引。</li><li id="bb03" class="mn mo jg kf b kg mw kk mx ko my ks mz kw na la nd mt mu mv bi translated">3)-在每个时间步<em class="lk"> t </em>使用<a class="ae jd" href="https://www.youtube.com/watch?v=tRsSi_sqXjI" rel="noopener ugc nofollow" target="_blank">交叉熵</a>损失函数来计算预测字和实际字之间的误差。</li></ul><p id="ea8a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你想知道这些<em class="lk"> W 的</em>是什么，它们中的每一个都代表了网络在某一阶段的权重。如上所述，权重是用随机元素初始化的矩阵，使用来自损失函数的误差进行调整。我们使用更新权重的反向传播算法来进行这种调整。我将在以后的文章中解释这个过程，但是，如果你对它是如何工作的感到好奇，迈克尔·尼尔森的书是必读的。</p><p id="affa" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们获得了正确的权重，预测句子“拿破仑是……的皇帝”中的下一个单词就非常简单了。在 RNN 的不同时间步长插入每个字将产生<em class="lk">h1、H2、H3、H4</em>。我们可以利用<em class="lk"> h_4 </em>和<em class="lk">x _ 5</em>(“of”这个词的向量)推导出<em class="lk"> y_5 </em>。如果我们的训练是成功的，我们应该期望在<em class="lk"> y_5 </em>中最大数字的索引与我们的词汇表中单词“France”的索引相同。</p><h2 id="df17" class="lp lq jg bd lr ls lt dn lu lv lw dp lx ko ly lz ma ks mb mc md kw me mf mg mh bi translated">标准 RNN 的问题</h2><p id="66f0" class="pw-post-body-paragraph kd ke jg kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">不幸的是，如果你实现了上面的步骤，你不会对结果感到高兴。这是因为最简单的 RNN 模型有一个主要缺点，称为<strong class="kf jh">消失梯度问题，</strong>这使得它不精确。</p><p id="c217" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之，问题来自于这样一个事实，即在训练期间的每个时间步，我们都使用相同的权重来计算<em class="lk"> y_t </em>。该乘法也在反向传播期间完成。我们越往后退，误差信号就变得越大或越小。这意味着<strong class="kf jh">网络在记忆序列</strong>中距离较远的单词时会遇到困难，并且只根据最近的单词做出预测。</p><p id="3ce6" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是为什么更强大的型号像 T4 LSTM T5 和 T6 GRU T7 会出现。解决了上述问题，它们已经成为实现递归神经网络的公认方法。</p><h2 id="6c65" class="lp lq jg bd lr ls lt dn lu lv lw dp lx ko ly lz ma ks mb mc md kw me mf mg mh bi translated">奖金</h2><p id="c4b5" class="pw-post-body-paragraph kd ke jg kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">最后，我想与所有让我更好地了解 RNNs 的资源分享我的列表:</p><p id="93ce" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">热身:</p><ul class=""><li id="56f5" class="mn mo jg kf b kg kh kk kl ko mp ks mq kw mr la nd mt mu mv bi translated"><a class="ae jd" href="https://blog.paperspace.com/recurrent-neural-networks-part-1-2/" rel="noopener ugc nofollow" target="_blank"> Paperspace 博客—递归神经网络</a></li><li id="1681" class="mn mo jg kf b kg mw kk mx ko my ks mz kw na la nd mt mu mv bi translated"><a class="ae jd" href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" rel="noopener ugc nofollow" target="_blank">野生 ML—RNNs 简介</a></li></ul><p id="a258" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更深入:</p><ul class=""><li id="82d0" class="mn mo jg kf b kg kh kk kl ko mp ks mq kw mr la nd mt mu mv bi translated"><a class="ae jd" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank"> Andrej Karpathy 博客——递归神经网络的不合理有效性</a></li><li id="3484" class="mn mo jg kf b kg mw kk mx ko my ks mz kw na la nd mt mu mv bi translated"><a class="ae jd" href="https://www.youtube.com/watch?v=Keqep_PKrY8&amp;t=1080s" rel="noopener ugc nofollow" target="_blank">斯坦福 CS224n —第八讲:递归神经网络和语言模型</a>(建议贯穿整个<a class="ae jd" href="http://web.stanford.edu/class/cs224n/" rel="noopener ugc nofollow" target="_blank">课程+练习</a>)</li></ul><p id="5de8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">高级(抓住细节):</p><ul class=""><li id="4c38" class="mn mo jg kf b kg kh kk kl ko mp ks mq kw mr la nd mt mu mv bi translated"><a class="ae jd" href="https://www.tensorflow.org/tutorials/recurrent" rel="noopener ugc nofollow" target="_blank"> Tensorflow —递归神经网络</a></li><li id="3238" class="mn mo jg kf b kg mw kk mx ko my ks mz kw na la nd mt mu mv bi translated"><a class="ae jd" href="https://arxiv.org/pdf/1506.00019.pdf" rel="noopener ugc nofollow" target="_blank"> arXiv 论文——对用于序列学习的递归神经网络的评论</a></li></ul><p id="7d21" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望这篇文章能让你对递归神经网络有一个很好的理解，并能对你激动人心的深度学习之旅有所贡献。</p><blockquote class="ne"><p id="6195" class="nf ng jg bd nh ni nj nk nl nm nn la dk translated">还有哪些 AI 内容？<a class="ae jd" href="https://www.linkedin.com/in/simeonkostadinov/" rel="noopener ugc nofollow" target="_blank">在 LinkedIn 上关注我</a>获取每日更新。</p></blockquote><h2 id="2d62" class="lp lq jg bd lr ls no dn lu lv np dp lx ko nq lz ma ks nr mc md kw ns mf mg mh bi translated">感谢您的阅读。如果你喜欢这篇文章，给它一些掌声👏。希望你有一个伟大的一天！</h2></div></div>    
</body>
</html>