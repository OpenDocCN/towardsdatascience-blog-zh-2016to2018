<html>
<head>
<title>Uncover the structure of tree ensembles in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭示 Python 中树集合的结构</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/uncover-the-structure-of-tree-ensembles-in-python-a01f72ea54a2?source=collection_archive---------6-----------------------#2018-06-07">https://towardsdatascience.com/uncover-the-structure-of-tree-ensembles-in-python-a01f72ea54a2?source=collection_archive---------6-----------------------#2018-06-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/73f970c6e45683738ea8638593ed8107.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2bx3gO9yJeFaeAyb4b4kEw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/photos/zhws9e2tzfA?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Ghost Presenter</a> on <a class="ae kc" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="fb96" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">集成方法将多个基本估计量结合起来，以产生更稳健的模型，更好地概括新数据。<em class="lb"> Bagging </em>和<em class="lb"> Boosting </em>是集成方法的两个主要类别，它们的区别在于它们组合来自基础估计量的预测的方式。</p><p id="c4ae" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Bagging 方法独立地建立估计量，然后平均它们的预测值。通过在构建每个基础估计量的方式中引入随机性，他们旨在减少最终模型的方差。换句话说，减少过度拟合。</p><p id="74f0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Boosting 方法在序列中建立基本估计量，其中每个后续估计量从其前一个估计量的错误中学习。与减少方差相反，这些方法的目标是减少所得估计量的偏差。</p><p id="2730" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每一类都包含几个算法。在这里，我们将看看两个广泛应用的算法，每个类别一个；<em class="lb">随机森林</em>作为装袋的例子<em class="lb"> Ada Boost </em>作为 boosting 方法的例子，用决策树作为基本估计器。我们将在分类的上下文中讨论这些方法，尽管这两种方法也在回归任务中使用。</p><p id="0a53" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章的目的是展示如何通过可视化的树来挖掘集合的几个潜在估计量。</p><p id="631d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">随机森林</strong></p><p id="9176" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随机森林，顾名思义，是一种建立在决策树上的装袋方法。每棵树都在数据的引导样本上进行训练，并且可以选择使用原始输入特征的子集。这两个随机因素确保了每棵树都是不同的。如果我们假设每棵树都超过了部分数据，我们期望它们的平均值会减少这种影响。</p><p id="3118" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">控制随机森林结构的主要参数是估计器的数量(<em class="lb"> n_estimators </em>)，即构建的决策树的总数和用于在每个节点选择分裂的输入属性的最大数量(<em class="lb"> max_features </em>)。</p><p id="d16d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一般来说，估计量越大，结果越好。然而，大量的估计器也导致长的训练时间。也很有可能在一定数量的估计量之后，精确度的增加将趋于平稳。人们应该在准确性和性能之间寻求平衡。交叉验证是一种可以用来调整这些参数的原则性方法。</p><p id="217a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦选择了 n_estimators= <em class="lb"> N </em>，就以上述方式构建了<em class="lb"> N </em>个决策树，并且通过对这些预测进行平均来获得最终预测。为了深入了解每个单独的预测，我们可以将构建的每个树可视化。</p><p id="d63d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们展示了如何在 Jupyter 笔记本中绘制决策树。这里，我们将更进一步，看看如何绘制一个系综的所有<em class="lb"> N </em>棵树。</p><p id="d3fc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是为 sklearn 葡萄酒数据集训练随机森林分类器的脚本。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="74f9" class="ll lm iq lh b gy ln lo l lp lq">from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.datasets import load_wine</span><span id="c093" class="ll lm iq lh b gy lr lo l lp lq"># load dataset<br/>data = load_wine()</span><span id="9af0" class="ll lm iq lh b gy lr lo l lp lq"># feature matrix<br/>X = data.data</span><span id="789b" class="ll lm iq lh b gy lr lo l lp lq"># target vector<br/>y = data.target</span><span id="e91d" class="ll lm iq lh b gy lr lo l lp lq"># class labels<br/>labels = data.feature_names</span><span id="0db2" class="ll lm iq lh b gy lr lo l lp lq">estimator = RandomForestClassifier().fit(X, y)</span></pre><p id="4bbc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">分类器对象有一个属性<em class="lb"> estimators_ </em>，它是一个由<em class="lb"> N </em>棵决策树组成的列表。这个列表的长度给出了树的数量。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="f276" class="ll lm iq lh b gy ln lo l lp lq">print(“Number of trees “ + str(len(trees)))</span><span id="2cec" class="ll lm iq lh b gy lr lo l lp lq">Number of trees 50</span></pre><p id="61e7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们打印 trees 对象，我们将得到每个估计量的摘要。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="e651" class="ll lm iq lh b gy ln lo l lp lq">display(estimator.estimators_)</span></pre><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/47a858381283ebe0ad56acfff74574ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*dAqz2fzUuiNXkuWaepH-jw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Sample of estimators_</figcaption></figure><p id="3cea" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以遍历列表中的每个元素，得到每棵树的节点数。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="0f52" class="ll lm iq lh b gy ln lo l lp lq">print("Number of nodes per tree is ")<br/><br/>i_tree = 0<br/>for tree_in_forest in trees:<br/>print(tree_in_forest.tree_.node_count)<br/>i_tree += 1</span><span id="cc91" class="ll lm iq lh b gy lr lo l lp lq">Number of nodes per tree is <br/>23<br/>23<br/>9<br/>19<br/>27<br/>27<br/>21<br/>23<br/>21<br/>25</span></pre><p id="d9d3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，如果我们迭代 trees 对象，我们可以使用<em class="lb"> export_graphviz </em>函数来绘制每棵树。因为树的数量通常很大，所以在笔记本中画出每一棵树并不方便。相反，我们可以将每个文件保存在工作目录中的. dot 文件中。如果您还想将每个文件保存为. png 格式，请使用<em class="lb"> check_call </em>功能。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="21c2" class="ll lm iq lh b gy ln lo l lp lq">from sklearn.tree import export_graphviz<br/>from subprocess import check_call</span><span id="b3e7" class="ll lm iq lh b gy lr lo l lp lq">cnt = 1<br/>for item in trees:<br/>     export_graphviz(item, out_file=str(cnt)+’_tree.dot’,feature_names= labels, class_names=labels, rounded=True, precision=4, node_ids=True, proportion=True<br/>, filled=True)<br/>    <br/>     check_call([‘dot’,’-Tpng’,str(cnt)+’_tree.dot’,’-o’,str(cnt)+’_tree.png’])</span><span id="264f" class="ll lm iq lh b gy lr lo l lp lq">      cnt += 1</span></pre><p id="8ece" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦迭代完成，我们将在我们的目录中得到一个文件编号列表。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/b84f8460b2bf64bdad3fff7c6605ae7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*N4D2p0Lmy7VCrhXIe6SsdA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">List of files in directory</figcaption></figure><p id="3291" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是两种经过训练的树。</p><div class="lc ld le lf gt ab cb"><figure class="lu jr lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/46615357ebd92728f66c1fc55fe62448.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*Ow_4Yjjf3UKat8N2mqyyhw.png"/></div></figure><figure class="lu jr ma lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/9f6165de60c016a073c3cf0162f23991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*zfPNB1SPfhvwrNN5wUsxWw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk mb di mc md">Two of the Decision Trees of the trained Random Forest model</figcaption></figure></div><p id="ddae" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到，每棵树可以有不同的深度，以及不同的节点结构和分裂。这正是在不同的数据子集上构建每棵树，并基于特征子集进行分割的结果。在这一点上，我们应该提到，bagging 方法通常在复杂的底层模型(即深树)中工作得更好，它们减少了过拟合效应。</p><p id="ad37" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> Ada 增强</strong></p><p id="c278" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与 bagging 相反，boosting 方法的原理是训练一系列“弱”学习器，例如深度小的决策树。在每次迭代中，通过为数据的每个实例分配权重来构建树。首先，给定<em class="lb"> k 个</em>训练样本，每个训练样本被赋予等于<em class="lb"> 1/k </em>的权重。在每次迭代中，没有被正确分类的样本得到更高的权重，迫使模型在这些情况下更好地训练。最后，通过加权多数投票将预测结合起来。</p><p id="0d18" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Ada Boost 的主要参数是<em class="lb"> base_estimator </em>和<em class="lb">n _ estimator</em>。估计量的数量相当于随机森林所描述的参数。基本估计量是基础模型的模型类型。在 sklearn learn 中，默认的基础估计器是决策树桩(具有<em class="lb"> max_depth </em> = 1 的决策树)。</p><p id="391c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与 RandomForestClassifier 类似，AdaBoostClassifier 也有一个<em class="lb"> estimators_ </em>属性。因此，我们可以训练一个 AdaBoost 分类器，并以上述方式绘制所有的树。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="2055" class="ll lm iq lh b gy ln lo l lp lq">from sklearn.ensemble import AdaBoostClassifier</span><span id="d112" class="ll lm iq lh b gy lr lo l lp lq">estimator = AdaBoostClassifier().fit(X, y)<br/><br/>trees = estimator.estimators_<br/><br/>print("Number of trees " + str(len(trees)))</span><span id="848f" class="ll lm iq lh b gy lr lo l lp lq">Number of trees 50</span></pre><p id="7b85" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">sklearn 中 AdaBoost 的默认估计数是 50。使用与上面完全相同的循环，我们可以得到 50 棵决策树中每一棵的图像。</p><div class="lc ld le lf gt ab cb"><figure class="lu jr me lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/ec4ca16fe08e6cdbecd041e51db05c4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*7b9JLJrizkflXRIHnYFG-Q.png"/></div></figure><figure class="lu jr mf lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/4cc53a2d4331eafb6267ecb86c0571b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*HeOnW0Xl7Zt5Fwm1GGB7pw.png"/></div></figure><figure class="lu jr mg lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/658cf2a4a806250dd640dc8903ac1a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*zHLpXM5b808ivvt2orA5BA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk mh di mi md">Three first Decision Trees of the trained AdaBoost model</figcaption></figure></div><p id="aa24" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有前三个决策树，每个在顶部节点有不同的分割。这一次，不同的分裂是由于每个样本在每次迭代中得到的不同权重。每棵树都专注于正确预测前一棵树未能正确分类的内容。</p><p id="165b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们看到了一种可以应用于 bagging 和 boosting 两个分类器家族的方法，以便可视化底层模型，这些模型的投票构成了我们模型的最终预测。通过这种可视化，我们展示了如何通过装袋中的随机化和 boosting 中的权重分配从相同的原始数据和输入要素构建 N 个不同的树。再一次，我发现这是一个实用的方法，可以更好地理解集合方法在幕后是如何工作的，并且在我们的项目中使用这种模型时，可以减少黑箱。</p></div></div>    
</body>
</html>