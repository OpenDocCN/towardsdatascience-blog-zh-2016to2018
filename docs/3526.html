<html>
<head>
<title>A simple 2D CNN for MNIST digit recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于 MNIST 数字识别的简单 2D CNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-simple-2d-cnn-for-mnist-digit-recognition-a998dbc1e79a?source=collection_archive---------1-----------------------#2018-05-21">https://towardsdatascience.com/a-simple-2d-cnn-for-mnist-digit-recognition-a998dbc1e79a?source=collection_archive---------1-----------------------#2018-05-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/204e2d28920af8d59a99e1fb14db48d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cPAmSB9nziZPI73VC5HAHg.png"/></div></div></figure><div class=""/><p id="7a06" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">卷积神经网络(CNN)是当前用于图像分类任务的最新架构。无论是面部识别、自动驾驶汽车还是物体检测，CNN 都在到处使用。在这篇文章中，我们使用 keras 和 tensorflow backend 为著名的 MNIST 数字识别任务设计了一个简单的二维卷积神经网络(CNN)模型。整个工作流程可以是:</p><ol class=""><li id="a387" class="kw kx jb ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">准备数据</li><li id="3cb3" class="kw kx jb ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">模型的建立和编译</li><li id="2257" class="kw kx jb ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">训练和评估模型</li><li id="d2af" class="kw kx jb ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">将模型保存到磁盘以供重用</li></ol><div class="ip iq gp gr ir lk"><a href="https://github.com/sambit9238/Deep-Learning/blob/master/cnn_mnist.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ll ab fo"><div class="lm ab ln cl cj lo"><h2 class="bd jc gy z fp lp fr fs lq fu fw ja bi translated">sambit 9238/深度学习</h2><div class="lr l"><h3 class="bd b gy z fp lp fr fs lq fu fw dk translated">深度学习——深度学习技术在自然语言处理、计算机视觉等领域的实现。</h3></div><div class="ls l"><p class="bd b dl z fp lp fr fs lq fu fw dk translated">github.com</p></div></div><div class="lt l"><div class="lu l lv lw lx lt ly ix lk"/></div></div></a></div><p id="9299" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">准备数据</strong></p><p id="0423" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里使用的数据集是上面提到的 MNIST 数据集。<strong class="ka jc"> MNIST 数据库</strong>(修改后的<a class="ae lz" href="https://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology" rel="noopener ugc nofollow" target="_blank">国家标准技术研究院</a>数据库)是一个手写数字(0 到 9)的大型数据库。该数据库包含 60，000 个训练图像和 10，000 个测试图像，每个图像的大小为 28×28。第一步是加载数据集，这可以通过 keras api 轻松完成。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="74e7" class="mj mk jb mf b gy ml mm l mn mo">import keras<br/>from keras.datasets import mnist<br/>#load mnist dataset<br/>(X_train, y_train), (X_test, y_test) = mnist.load_data() #everytime loading data won't be so easy :)</span></pre><p id="b487" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里，X_train 包含 60，000 个训练图像数据，每个数据的大小为 28×28，y_train 包含它们相应的标签。类似地，X_test 包含 10，000 个测试图像的数据，每个维度为 28x28，y_test 包含它们相应的标签。让我们可视化一些来自训练的数据，以更好地了解深度学习模型的目的。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="84eb" class="mj mk jb mf b gy ml mm l mn mo">import matplotlib.pyplot as plt<br/>fig = plt.figure()<br/>for i in range(9):<br/>  plt.subplot(3,3,i+1)<br/>  plt.tight_layout()<br/>  plt.imshow(X_train[i], cmap='gray', interpolation='none')<br/>  plt.title("Digit: {}".format(y_train[i]))<br/>  plt.xticks([])<br/>  plt.yticks([])<br/>fig</span></pre><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mp"><img src="../Images/35cb1f6ef87b3279bfe0771ab11e2d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kv6HShVW39A5qMOIHNzKNA.png"/></div></div></figure><p id="3a96" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如这里可以看到的，在左上角,“5”的图像存储为 X _ train[0 ], y _ train[0]包含标签“5”。我们的深度学习模型应该能够只获取手写图像，并预测实际书写的数字。</p><p id="fcfe" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，为了准备数据，我们需要对图像进行一些处理，如调整图像大小、归一化像素值等。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="4296" class="mj mk jb mf b gy ml mm l mn mo">#reshaping<br/>#this assumes our data format<br/>#For 3D data, "channels_last" assumes (conv_dim1, conv_dim2, conv_dim3, channels) while <br/>#"channels_first" assumes (channels, conv_dim1, conv_dim2, conv_dim3).<br/>if k.image_data_format() == 'channels_first':<br/>    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)<br/>    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)<br/>    input_shape = (1, img_rows, img_cols)<br/>else:<br/>    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)<br/>    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)<br/>    input_shape = (img_rows, img_cols, 1)<br/>#more reshaping<br/>X_train = X_train.astype('float32')<br/>X_test = X_test.astype('float32')<br/>X_train /= 255<br/>X_test /= 255<br/>print('X_train shape:', X_train.shape) #X_train shape: (60000, 28, 28, 1)</span></pre><p id="128e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在对图像信息进行必要的处理后，标签数据即 y_train 和 y_test 需要转换成分类格式，如标签<strong class="ka jc">‘3’</strong>应转换成向量<strong class="ka jc">【0，0，0，1，0，0，0，0，0】</strong>用于建模。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="54c6" class="mj mk jb mf b gy ml mm l mn mo">import keras<br/>#set number of categories<br/>num_category = 10<br/># convert class vectors to binary class matrices<br/>y_train = keras.utils.to_categorical(y_train, num_category)<br/>y_test = keras.utils.to_categorical(y_test, num_category)</span></pre><p id="2258" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">模型的建立和编译</strong></p><p id="f36b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">数据准备好输入模型后，我们需要定义模型的架构，并使用必要的<a class="ae lz" href="https://keras.io/optimizers/" rel="noopener ugc nofollow" target="_blank">优化函数</a>、<a class="ae lz" href="https://keras.io/losses/" rel="noopener ugc nofollow" target="_blank">损失函数</a>和<a class="ae lz" href="https://keras.io/metrics/" rel="noopener ugc nofollow" target="_blank">性能指标</a>对其进行编译。</p><p id="1fb7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此处遵循的架构是 2 个卷积层，然后分别是池层、全连接层和 softmax 层。对于不同类型的特征提取，在每个卷积层使用多个滤波器。一个直观的解释是，如果第一个过滤器有助于检测图像中的直线，第二个过滤器将有助于检测圆等等。对每一层的技术执行的解释将是即将到来的文章的一部分。为了更好地理解每一层，可以参考<a class="ae lz" href="http://cs231n.github.io/convolutional-networks/" rel="noopener ugc nofollow" target="_blank"><em class="mq"/></a></p><p id="df1e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在最大池和全连接层之后，在我们的模型中引入了<a class="ae lz" href="http://jmlr.org/papers/v15/srivastava14a.html" rel="noopener ugc nofollow" target="_blank">丢弃</a>作为正则化，以减少过拟合问题。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="692a" class="mj mk jb mf b gy ml mm l mn mo">##model building<br/>model = Sequential()<br/>#convolutional layer with rectified linear unit activation<br/>model.add(Conv2D(32, kernel_size=(3, 3),<br/>                 activation='relu',<br/>                 input_shape=input_shape))<br/>#32 convolution filters used each of size 3x3<br/>#again<br/>model.add(Conv2D(64, (3, 3), activation='relu'))<br/>#64 convolution filters used each of size 3x3<br/>#choose the best features via pooling<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>#randomly turn neurons on and off to improve convergence<br/>model.add(Dropout(0.25))<br/>#flatten since too many dimensions, we only want a classification output<br/>model.add(Flatten())<br/>#fully connected to get all relevant data<br/>model.add(Dense(128, activation='relu'))<br/>#one more dropout for convergence' sake :) <br/>model.add(Dropout(0.5))<br/>#output a softmax to squash the matrix into output probabilities<br/>model.add(Dense(num_category, activation='softmax'))</span></pre><p id="36ba" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">定义了模型的架构之后，就需要编译模型了。这里，我们使用分类交叉熵损失函数，因为它是一个多类分类问题。由于所有标签都具有相似的重量，因此我们更倾向于将准确性作为性能指标。称为 AdaDelta 的流行梯度下降技术用于优化模型参数。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="df62" class="mj mk jb mf b gy ml mm l mn mo">#Adaptive learning rate (adaDelta) is a popular form of gradient descent rivaled only by adam and adagrad<br/>#categorical ce since we have multiple classes (10) <br/>model.compile(loss=keras.losses.categorical_crossentropy,<br/>              optimizer=keras.optimizers.Adadelta(),<br/>              metrics=['accuracy'])</span></pre><p id="12df" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">训练和评估模型</strong></p><p id="27db" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在模型架构被定义和编译之后，模型需要用训练数据来训练，以便能够识别手写数字。因此，我们将用 X_train 和 y_train 来拟合模型。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="3da4" class="mj mk jb mf b gy ml mm l mn mo">batch_size = 128<br/>num_epoch = 10<br/>#model training<br/>model_log = model.fit(X_train, y_train,<br/>          batch_size=batch_size,<br/>          epochs=num_epoch,<br/>          verbose=1,<br/>          validation_data=(X_test, y_test))</span></pre><p id="0b3c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里，一个时期意味着所有训练样本的一次向前和一次向后传递。批量意味着一次向前/向后传递中训练样本的数量。培训输出是:</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mr"><img src="../Images/057e3d99b11cbb67267c0273576092d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-_NJ6kCaS26yr728E4viBQ.png"/></div></div></figure><p id="f2f4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在需要根据性能来评估训练好的模型。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="0d2c" class="mj mk jb mf b gy ml mm l mn mo">score = model.evaluate(X_test, y_test, verbose=0)<br/>print('Test loss:', score[0]) #Test loss: 0.0296396646054<br/>print('Test accuracy:', score[1]) #Test accuracy: 0.9904</span></pre><p id="358d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">测试准确度 99%以上意味着模型在预测方面训练有素。如果我们可视化整个训练日志，那么随着更多的历元数，训练和测试数据的模型的损失和准确性收敛，从而使模型稳定。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="68c4" class="mj mk jb mf b gy ml mm l mn mo">import os<br/># plotting the metrics<br/>fig = plt.figure()<br/>plt.subplot(2,1,1)<br/>plt.plot(model_log.history['acc'])<br/>plt.plot(model_log.history['val_acc'])<br/>plt.title('model accuracy')<br/>plt.ylabel('accuracy')<br/>plt.xlabel('epoch')<br/>plt.legend(['train', 'test'], loc='lower right')</span><span id="f76e" class="mj mk jb mf b gy ms mm l mn mo">plt.subplot(2,1,2)<br/>plt.plot(model_log.history['loss'])<br/>plt.plot(model_log.history['val_loss'])<br/>plt.title('model loss')<br/>plt.ylabel('loss')<br/>plt.xlabel('epoch')<br/>plt.legend(['train', 'test'], loc='upper right')</span><span id="16c7" class="mj mk jb mf b gy ms mm l mn mo">plt.tight_layout()</span><span id="e491" class="mj mk jb mf b gy ms mm l mn mo">fig</span></pre><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mt"><img src="../Images/af2d1ede74e8718d19b87f17ef8bb47c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VSMiY-Hs_YLEygYTiUDF9Q.png"/></div></div></figure><p id="713f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">将模型保存到磁盘以便重复使用</strong></p><p id="38fb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，训练好的模型需要序列化。模型的架构或结构将存储在 json 文件中，权重将以 hdf5 文件格式存储。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="37fb" class="mj mk jb mf b gy ml mm l mn mo">#Save the model<br/># serialize model to JSON<br/>model_digit_json = model.to_json()<br/>with open("model_digit.json", "w") as json_file:<br/>    json_file.write(model_digit_json)<br/># serialize weights to HDF5<br/>model.save_weights("model_digit.h5")<br/>print("Saved model to disk")</span></pre><p id="c4a6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，保存的模型可以在以后重用，或者很容易移植到其他环境中。在接下来的文章中，我们将看到如何在生产中部署这个经过训练的模型。</p><p id="f1d7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">享受深度学习！</p><p id="9104" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">参考资料:</p><div class="ip iq gp gr ir lk"><a href="https://keras.io/getting-started/sequential-model-guide/" rel="noopener  ugc nofollow" target="_blank"><div class="ll ab fo"><div class="lm ab ln cl cj lo"><h2 class="bd jc gy z fp lp fr fs lq fu fw ja bi translated">顺序模型指南- Keras 文件</h2><div class="lr l"><h3 class="bd b gy z fp lp fr fs lq fu fw dk translated">Keras 顺序模型入门</h3></div><div class="ls l"><p class="bd b dl z fp lp fr fs lq fu fw dk translated">keras.io</p></div></div><div class="lt l"><div class="mu l lv lw lx lt ly ix lk"/></div></div></a></div><div class="ip iq gp gr ir lk"><a href="http://cs231n.github.io/convolutional-networks/" rel="noopener  ugc nofollow" target="_blank"><div class="ll ab fo"><div class="lm ab ln cl cj lo"><h2 class="bd jc gy z fp lp fr fs lq fu fw ja bi translated">用于视觉识别的 CS231n 卷积神经网络</h2><div class="lr l"><h3 class="bd b gy z fp lp fr fs lq fu fw dk translated">斯坦福 CS231n 课程材料和笔记:视觉识别的卷积神经网络。</h3></div><div class="ls l"><p class="bd b dl z fp lp fr fs lq fu fw dk translated">cs231n.github.io</p></div></div><div class="lt l"><div class="mv l lv lw lx lt ly ix lk"/></div></div></a></div></div></div>    
</body>
</html>