<html>
<head>
<title>Review: Faster R-CNN (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:更快的 R-CNN(目标检测)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=collection_archive---------1-----------------------#2018-09-14">https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=collection_archive---------1-----------------------#2018-09-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="4937" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个故事中，<strong class="jp ir">更快的 R-CNN</strong>【1–2】被回顾。在之前的快速 R-CNN [3]和 R-CNN [4]中，区域建议是通过选择性搜索(SS) [5]而不是使用卷积神经网络(CNN)来生成的。</p><p id="99aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在更快的 R-CNN[1–2]，<strong class="jp ir">中，区域提案生成和异议检测任务都由同一个 conv 网络完成。</strong>采用这样的设计，物体检测速度更快。</p><p id="bba1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了更好地了解深度学习对象检测，作为一系列的对象检测方法，如果有足够的时间，最好按照顺序阅读 R-CNN、Fast R-CNN 和 Fast R-CNN，以了解对象检测的演变，特别是为什么区域提议网络(RPN)存在于该方法中。如果有兴趣，我建议看看我对他们的评论。</p><p id="1182" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于更快的 R-CNN 是一种最先进的方法，当我写这个故事时，它被发表为<strong class="jp ir"> 2015 NIPS </strong>论文和<strong class="jp ir"> 2017 TPAMI </strong>论文，分别有超过<strong class="jp ir"> 4000 和 800 篇引用</strong><strong class="jp ir"/>。(<a class="kl km ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----f5685cb30202--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl kn ko hu kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ij ik il im in"><h1 id="9ccc" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">涵盖哪些内容</h1><ol class=""><li id="7437" class="ls lt iq jp b jq lu ju lv jy lw kc lx kg ly kk lz ma mb mc bi translated"><strong class="jp ir">地区提案网络</strong></li><li id="c2fe" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated"><strong class="jp ir">检测网络</strong></li><li id="b25f" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated"><strong class="jp ir">四步交替训练</strong></li><li id="9d56" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated"><strong class="jp ir">消融研究</strong></li><li id="9bdb" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated"><strong class="jp ir">检测结果</strong></li></ol></div><div class="ab cl kn ko hu kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ij ik il im in"><h1 id="da42" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">1.区域提案网络</h1><p id="4b4f" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy mi ka kb kc mj ke kf kg mk ki kj kk ij bi translated">简而言之，R-CNN [4]和快速 R-CNN [3]首先通过选择性搜索(SS) [5]生成区域建议，然后使用基于 CNN 的网络对对象类别进行分类并检测边界框。(主要区别在于 R-CNN 在像素级将区域建议输入到 CNN 中用于检测，而快速 R-CNN 在特征映射级输入区域建议。)<strong class="jp ir">因此，在 R-CNN [4]和快速 R-CNN [3]中，区域提议方法/网络(即 SS)和检测网络是解耦的。</strong></p><p id="b119" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">脱钩不是一个好主意。比如说 SS 有假阴性的时候，这个误差会直接伤害到检测网络。最好将它们耦合在一起，使它们相互关联。</p><p id="5f39" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">在更快的 R-CNN[1–2]中，使用 SS [5]的 RPN 被使用 CNN 的 RPN 取代。并且该 CNN 与检测网络共享。这个 CNN 可以是论文中的 ZFNet 或者 VGGNet。因此，整个网络如下:</strong></p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/d9114298e41c447200e8d7e207e3947c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*e6dx5qzUKWwasIVGSuCyDA.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">Faster R-CNN</strong></figcaption></figure><ol class=""><li id="7062" class="ls lt iq jp b jq jr ju jv jy my kc mz kg na kk lz ma mb mc bi translated">首先，图片经过 conv 层和特征地图提取。</li><li id="f22a" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated">然后在 RPN 中为特征图上的每个位置使用一个<strong class="jp ir">滑动窗口</strong>。</li><li id="104a" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated">对于每个位置，使用<strong class="jp ir"> k (k=9)个锚定框</strong>(<strong class="jp ir">128、256 和 512 的 3 个比例，以及 1:1、1:2、2:1 </strong>的 3 个纵横比)来生成区域提议。</li><li id="fc2a" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated">一个<strong class="jp ir"> <em class="nb"> cls </em> </strong>层输出<em class="nb"> 2k </em>分数<strong class="jp ir">对于<em class="nb"> k </em>盒子是否有对象</strong>。</li><li id="0958" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated">一个<strong class="jp ir"> <em class="nb"> reg </em> </strong>层输出<em class="nb">k</em>盒子的<strong class="jp ir">坐标</strong>(盒子中心坐标，宽度和高度)4k 。</li><li id="bcdf" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated">一张大小为<em class="nb">W</em>T22】×T24】H 的特征图，总共有<em class="nb"> WHk </em>个主播。</li></ol><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/acaadb65c48c089b12e27d0f12e16ea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*wB3ctS9WGNmw6pP_kjLjgg.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">The Output of RPN</strong></figcaption></figure><p id="6702" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">128、256 和 512 三种比例以及 1:1、1:2 和 2:1 三种纵横比的平均建议尺寸为:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nd"><img src="../Images/20d554480bd7a337137a12084ace9870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xbaX4MqolDTnfiMTJpX-Qg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">Average Proposal Sizes</strong></figcaption></figure><p id="2141" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">损失函数是:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/b76b1f494228c9e91ae52fcdf02788f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*ifiqMHGbDsyIE7Zf06dAdA.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">RPN Loss Function</strong></figcaption></figure><p id="350d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一项是 2 类(有无对象)上的分类损失。第二项是仅当存在对象(即 p_i* =1)时边界框的回归损失。</p><p id="2403" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，<strong class="jp ir"> RPN 网络就是预检查哪个位置包含对象</strong>。并且<strong class="jp ir">相应的位置和边界框将传递到检测网络</strong>用于检测对象类别并返回该对象的边界框。</p><p id="048d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于区域可以彼此高度重叠，所以使用非最大抑制(NMS)来将提议的数量从大约 6000 减少到 N (N=300)。</p></div><div class="ab cl kn ko hu kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ij ik il im in"><h1 id="4e3c" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak"> 2。检测网络</strong></h1><p id="a91c" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy mi ka kb kc mj ke kf kg mk ki kj kk ij bi translated">除了 RPN，其余部分与快速 R-CNN 类似。首先执行 ROI 合并。然后汇集的区域通过 CNN 和两个 FC 分支，用于类 softmax 和边界框回归器。(如果有兴趣，请阅读<a class="ae nj" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">我对 Fast R-CNN </a>的评论。)</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="ab gu cl nk"><img src="../Images/3e4598d2c5600647cc4839eacacb94de.png" data-original-src="https://miro.medium.com/v2/format:webp/1*67iVyCzqapfB5Nyci_zynw.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">Fast R-CNN Detection Network</strong></figcaption></figure></div><div class="ab cl kn ko hu kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ij ik il im in"><h1 id="1c27" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">3.<strong class="ak">四步交替训练</strong></h1><p id="f525" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy mi ka kb kc mj ke kf kg mk ki kj kk ij bi translated">由于 conv 层被共享以提取最终具有不同输出的特征图，因此，训练过程非常不同:</p><ol class=""><li id="5fec" class="ls lt iq jp b jq jr ju jv jy my kc mz kg na kk lz ma mb mc bi translated">用 imagenet 预训练模型训练(微调)RPN。</li><li id="5c05" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated">使用 imagenet 预训练模型训练(微调)单独的检测网络。(Conv 图层尚未共享)</li><li id="c399" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated">使用检测器网络来初始化 PRN 训练，修复共享的 conv 层，仅微调 RPN 的独特层。</li><li id="38c1" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated">保持 conv 层固定，微调探测器网络的独特层。</li></ol></div><div class="ab cl kn ko hu kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ij ik il im in"><h1 id="e817" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak"> 4。消融研究</strong></h1><h2 id="c147" class="nl kv iq bd kw nm nn dn la no np dp le jy nq nr li kc ns nt lm kg nu nv lq nw bi translated">4.1.区域提案</h2><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nx"><img src="../Images/ca0cc053a5b1addf50cfd8985548fe7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_WiGCG90bcLVMJl4-q3c4A.png"/></div></div></figure><p id="d1a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所述，利用非共享 conv 层(在交替训练中只有前 2 步)，获得了 58.7%的 mAP。<strong class="jp ir">共享 conv 图层，获得 59.9%的地图。并且它优于现有技术的 SS 和 EB。</strong></p><h2 id="3dd0" class="nl kv iq bd kw nm nn dn la no np dp le jy nq nr li kc ns nt lm kg nu nv lq nw bi translated">4.2 比例和比率</h2><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c9eef49709fb2416d9f0dc968e494c02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*DZofSYET9wmabxKUIFpgdg.png"/></div></figure><p id="e065" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">在 3 个尺度和 3 个比例的情况下，获得了 69.9%的地图，与 3 个尺度和 1 个比例的地图相比只有很小的改进。</strong>但是仍然使用 3 个刻度和 3 个比率。</p><h2 id="b529" class="nl kv iq bd kw nm nn dn la no np dp le jy nq nr li kc ns nt lm kg nu nv lq nw bi translated">损失函数中的 4.3 λ</h2><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/f3558cb222d9135c06c644a4c1106958.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*GbRAOPLLob1XWOoiS4LI2A.png"/></div></figure><p id="783c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> λ = 10 达到最佳效果。</strong></p></div><div class="ab cl kn ko hu kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ij ik il im in"><h1 id="ae26" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak"> 5。检测结果</strong></h1><h2 id="a0aa" class="nl kv iq bd kw nm nn dn la no np dp le jy nq nr li kc ns nt lm kg nu nv lq nw bi translated">5.1 帕斯卡 VOC 2007</h2><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi oa"><img src="../Images/c18675dfbe8be9a6ef96b0f8de062cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vhtYJ4sc36Hc5Fxd0IZNJg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">Detailed Results</strong></figcaption></figure><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi ob"><img src="../Images/b9380811816b6c26af878cc61b7a8772.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AbTLCWNxDvXGjr8PjMa6sg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">Overall Results</strong></figcaption></figure><p id="e564" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过使用 COCO、VOC 2007 (trainval)和 VOC 2012 (trainval)数据集的训练数据，获得了 78.8%的 mAP。</p><h2 id="13e9" class="nl kv iq bd kw nm nn dn la no np dp le jy nq nr li kc ns nt lm kg nu nv lq nw bi translated">5.2 帕斯卡 VOC 2012</h2><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi oc"><img src="../Images/80d1081832d8ac7106d5eeefcce79a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PlkPYAPH0TcU-9FFvRQAFg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">Detailed Results</strong></figcaption></figure><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi od"><img src="../Images/816b24628d7f507cb7849939bff30498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4fty0BsVFvkYs460wPXYiw.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">Overall Results</strong></figcaption></figure><p id="6f2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">利用使用 COCO、VOC 2007 (trainval+test)和 VOC 2012 (trainval)数据集的训练数据，获得了 75.9%的 mAP。</p><h2 id="9031" class="nl kv iq bd kw nm nn dn la no np dp le jy nq nr li kc ns nt lm kg nu nv lq nw bi translated">5.3 可可女士</h2><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi oe"><img src="../Images/f31ca8a188ab908b660f2f4e701d6e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N8OmCMdSQJ1fHyq1Z6cBwQ.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">Overall Results</strong></figcaption></figure><p id="a024" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用 COCO 训练集进行训练，用 IoU @ 0.5 获得 42.1%的 mAP。当 IoU 从 0.5 到 0.95，步长为 0.05 时，可以获得 21.5%的 mAP。</p><h2 id="858d" class="nl kv iq bd kw nm nn dn la no np dp le jy nq nr li kc ns nt lm kg nu nv lq nw bi translated">5.4 检测时间</h2><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi of"><img src="../Images/3cc946688c8ca9cfab0cce5ceff34ac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v2xKdUizHcO-n2kUkOpGbQ.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">Detection Time</strong></figcaption></figure><p id="142d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用 SS 做 RPN，VGGNet 做检测网络:0.5 fps / 1830ms <br/>用<strong class="jp ir"> VGGNet 做 RPN 和检测网络</strong> : <strong class="jp ir"> 5fps / 198ms </strong> <br/>用<strong class="jp ir"> ZFNet 做 RPN 和检测网络</strong> : <strong class="jp ir"> 17fps / 59ms <br/>比 SS 快很多。</strong></p><h2 id="7672" class="nl kv iq bd kw nm nn dn la no np dp le jy nq nr li kc ns nt lm kg nu nv lq nw bi translated">5.5.一些例子</h2><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi og"><img src="../Images/f3ade6820959f8e1f1d73244b0b53088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IyeVVQKWDf02Jlt2-Cexag.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">VOC 2007</strong></figcaption></figure></div><div class="ab cl kn ko hu kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ij ik il im in"><h1 id="1cb6" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">参考</h1><ol class=""><li id="b435" class="ls lt iq jp b jq lu ju lv jy lw kc lx kg ly kk lz ma mb mc bi translated">[2015 NIPS][更快的 R-CNN] <br/> <a class="ae nj" href="https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" rel="noopener ugc nofollow" target="_blank">更快的 R-CNN:利用区域提议网络实现实时对象检测</a></li><li id="af18" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated">【2017 TPAMI】【更快的 R-CNN】<br/><a class="ae nj" href="https://ieeexplore.ieee.org/document/7485869/" rel="noopener ugc nofollow" target="_blank">更快的 R-CNN:利用区域提议网络实现实时对象检测</a></li><li id="2abd" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated">【2015 ICCV】【快速 R-CNN】<br/><a class="ae nj" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" rel="noopener ugc nofollow" target="_blank">快速 R-CNN </a></li><li id="274e" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated">【2014 CVPR】【R-CNN】<br/><a class="ae nj" href="https://arxiv.org/pdf/1311.2524" rel="noopener ugc nofollow" target="_blank">丰富的特征层次，用于精确的对象检测和语义分割</a></li><li id="7ffa" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated">【2013 IJCV】【选择性搜索】<br/> <a class="ae nj" href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" rel="noopener ugc nofollow" target="_blank">选择性搜索对象识别</a></li></ol><h1 id="0656" class="ku kv iq bd kw kx oh kz la lb oi ld le lf oj lh li lj ok ll lm ln ol lp lq lr bi translated">我的评论</h1><ol class=""><li id="ea6e" class="ls lt iq jp b jq lu ju lv jy lw kc lx kg ly kk lz ma mb mc bi translated"><a class="ae nj" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">回顾:快速 R-CNN(物体检测)</a></li><li id="08c4" class="ls lt iq jp b jq md ju me jy mf kc mg kg mh kk lz ma mb mc bi translated"><a class="ae nj" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">回顾:R-CNN(物体检测)</a></li></ol></div></div>    
</body>
</html>