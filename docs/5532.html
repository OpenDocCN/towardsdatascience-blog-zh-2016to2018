<html>
<head>
<title>Debugging a Machine Learning model written in TensorFlow and Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">调试一个用 TensorFlow 和 Keras 编写的机器学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/debugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736?source=collection_archive---------3-----------------------#2018-10-24">https://towardsdatascience.com/debugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736?source=collection_archive---------3-----------------------#2018-10-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="66dc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">可能出错的事情，以及如果出错如何诊断。</h2></div><p id="cb9a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，在我调试 TensorFlow 模型的过程中，您可以看到我的身后。我做了很多傻事，所以请不要评价。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/cd03c8cf39bb38fa32f5de22b4a86015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gVy_qoJwg4tzxp_7X5OoXA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Cheat sheet. The numbers refer to sections in this article (https://bit.ly/2PXpzRh)</figcaption></figure><h1 id="093b" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">1ML 模型的目标</h1><p id="464f" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">你可以在 GitHub 上看到<a class="ae mo" href="https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/lightning/ltgpred" rel="noopener ugc nofollow" target="_blank">最终(工作)模型</a>。我正在建立一个模型来预测 30 分钟后的闪电，并计划在美国气象学会上向<a class="ae mo" href="https://ams.confex.com/ams/2019Annual/meetingapp.cgi/Paper/354046" rel="noopener ugc nofollow" target="_blank">展示它。基本思想是在红外和全球闪电测绘仪(GLM) GOES-16 数据的每个像素周围创建 64x64 图像补片，如果照明图像实际上在 30 分钟后出现在像素周围的 16x16 图像补片内，则将像素标记为“has_ltg=1”。</a></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/74799e465182c502e3bfb5955070fa9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gbmRGyGtqfDFYfS0FfA4jg.jpeg"/></div></div></figure><p id="90ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据当前的红外和 GLM 数据，以这种方式训练的模型可以用于实时预测 30 分钟后的闪电。</p><h2 id="df64" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">1a。输入函数</h2><p id="10b0" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">我写了一个 convnet 模型，大量借鉴了为 TPU 编写的<a class="ae mo" href="https://github.com/tensorflow/tpu/tree/master/models/official/resnet" rel="noopener ugc nofollow" target="_blank"> ResNet 模型的训练循环，并修改了输入函数(读取我的数据，而不是 JPEG)和模型(一个简单的卷积网络，而不是 ResNet)。</a></p><p id="c092" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将输入转换为张量的代码的关键位:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="bb52" class="mp ls iq nc b gy ng nh l ni nj">parsed = tf.parse_single_example(<br/>    example_data, {<br/>        <strong class="nc ir">'ref'</strong>: tf.VarLenFeature(tf.float32),<br/>        <strong class="nc ir">'ltg'</strong>: tf.VarLenFeature(tf.float32),<br/>        <strong class="nc ir">'has_ltg'</strong>: tf.FixedLenFeature([], tf.int64, 1),<br/>    })<br/>parsed[<strong class="nc ir">'ref'</strong>] = _sparse_to_dense(parsed[<strong class="nc ir">'ref'</strong>], height * width)<br/>parsed[<strong class="nc ir">'ltg'</strong>] = _sparse_to_dense(parsed[<strong class="nc ir">'ltg'</strong>], height * width)<br/>label = tf.cast(tf.reshape(parsed[<strong class="nc ir">'has_ltg'</strong>], shape=[]), dtype=tf.int32)</span></pre><p id="6d10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本质上，每个 TensorFlow 记录(由 Apache 射束管道创建)由 ref、ltg 和 has_ltg 字段组成。ref 和 ltg 是可变长度数组，它们被整形为密集的 64x64 矩阵(使用 tf.sparse_tensor_to_dense)。标签只是 0 或 1。</p><p id="1970" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我将两张解析后的图像叠加在一起:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="f611" class="mp ls iq nc b gy ng nh l ni nj">stacked = tf.concat([parsed[<strong class="nc ir">'ref'</strong>], parsed[<strong class="nc ir">'ltg'</strong>]], axis=1)<br/><strong class="nc ir">img = </strong>tf.reshape(stacked, [height, width, n_channels])</span></pre><p id="328f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此时，我有一个张量是[？64，64，2]，即一批 2 通道图像。make_input_fn 中输入管道的其余部分(列出文件、通过并行交错读取文件、预处理数据、使形状静态化以及预取)本质上只是从 ResNet 代码复制粘贴而来的。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/29009fc95fe0d310ab556bfff7ce71ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*piI6vHZ4ye9-S7YaDYdeTw.jpeg"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Stacking the two 4096-length arrays into a 3D tensor of shape (64, 64, 2)</figcaption></figure><h2 id="d2ab" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">1b。运行代码</h2><p id="5c3b" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">我通过在本地以小批量运行训练器几个步骤来开发代码:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="2d95" class="mp ls iq nc b gy ng nh l ni nj">gcloud ml-engine local train \<br/>    --module-name=trainer.train_cnn --package-path=${PWD}/ltgpred/trainer \<br/>    -- \<br/>    --train_steps=10 --num_eval_records=512 --train_batch_size=16 \<br/>    --job-dir=$OUTDIR --train_data_path=${DATADIR}/train* --eval_data_path=${DATADIR}/eval*</span></pre><p id="d48f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我使用配有更大 GPU 的大型机器在 Cloud ML Engine 上的更大数据集上运行了它:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="6c31" class="mp ls iq nc b gy ng nh l ni nj">gcloud ml-engine jobs submit training $JOBNAME \<br/>    --module-name=trainer.train_cnn --package-path=${PWD}/ltgpred/trainer --job-dir=$OUTDIR \<br/>    --region=${REGION} --scale-tier=CUSTOM --config=largemachine.yaml \<br/>    --python-version=3.5 --runtime-version=1.8 \<br/>    -- \<br/>    --train_data_path=${DATADIR}/train* --eval_data_path=${DATADIR}/eval* \<br/>    --train_steps=5000 --train_batch_size=256 \<br/>    --num_eval_records=128000 </span></pre><p id="e802" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这很有用。我的大部分调试和开发都是通过本地运行来完成的。这样，我可以在不连接的情况下工作，不需要在我的机器上安装 GPU。</p><h1 id="b349" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">2 模型不学习</h1><p id="4936" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">一旦我写好代码并运行它，我发现这个模型很快就产生了令人怀疑的相同损失:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/658026a978648f8329a4a6e87f68fa9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*V4tQnalvBgFTRES8LCt_Pw.png"/></div></figure><p id="052d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并达到了一个精度指标，这个指标从公元 1000 年开始就一直保持不变:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="c9cf" class="mp ls iq nc b gy ng nh l ni nj">'rmse': 0.49927762, 'accuracy': 0.6623125,</span></pre><h2 id="a2e1" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">2a。非常嫌疑犯</h2><p id="024c" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">当机器学习模型不会学习时，有几个常见的疑点。我试着改变初始化。默认情况下，TensorFlow 使用 zeros _ initializer[编辑:事实证明我不需要这样做— tf.layers.conv2d 继承自<a class="ae mo" href="https://keras.io/layers/convolutional/#conv2d" rel="noopener ugc nofollow" target="_blank"> Keras 的 Conv2D </a>，它使用与 Xavier 相同的 glorot _ uniform。如何使用 Xavier(它使用小的初始值)，并为可重复性设置随机种子？</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="1cc6" class="mp ls iq nc b gy ng nh l ni nj">xavier = tf.contrib.layers.xavier_initializer(seed=13)<br/><em class="nm">c1 = tf.layers.conv2d(<br/>       convout,<br/>       filters=nfilters,<br/>       kernel_size=ksize,<br/>       </em><strong class="nc ir"><em class="nm">kernel_initializer=xavier,</em></strong><em class="nm"><br/>       strides=1,<br/>       padding='same',<br/>       activation=tf.nn.relu)</em></span></pre><p id="1e71" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将梯度从复杂的 AdamOptimizer 改为可靠的备用 GradientDescentOptimizer 如何？</p><p id="344d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">把学习率从 0.01 降到 1e-6 怎么样？</p><p id="4f95" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些都没用，但这是我第一次尝试。</p><h2 id="225a" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">2b。TensorFlow 打印</h2><p id="9937" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">我的输入函数可能每次都返回相同的数据吗？这就解释了为什么模型被卡住了。我如何知道输入函数正在读入什么？</p><p id="4781" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">验证输入函数是否正确的一个简单方法是简单地打印出读取的值。然而，你不能只打印一个张量:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="d40b" class="mp ls iq nc b gy ng nh l ni nj">print(img)</span></pre><p id="b69e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这将只是打印出张量的元数据，而不是它的值。相反，您需要在程序执行时评估张量的值:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="3ee7" class="mp ls iq nc b gy ng nh l ni nj">print(img.eval(sess=...))</span></pre><p id="ab76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">即使这样也不行，因为 Estimator API 没有给你一个会话句柄。解决办法就是用 tf。打印:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="a0ca" class="mp ls iq nc b gy ng nh l ni nj">img = tf.Print(img, [img], "image values=")</span></pre><p id="9b2e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这样做的目的是在原始 img 节点和新 img 节点之间插入打印节点，以便在再次使用图像节点之前打印值:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/058922cae1a7a87b85b8190dfdbdeef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*TSHGAe_1K9CB5qxPmtCwAw.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">tf.Print() allows you to insert a printing node in the TensorFlow graph so that you can print out the values of a Tensor as the program executes.</figcaption></figure><h2 id="09ee" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">2c。张量的打印统计</h2><p id="3299" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">但是，一旦我这样做了，我只得到前 3 或 4 个值(默认情况下，tf。打印不打印整个张量)它们大部分为零。它们是零，是因为卫星图像往往有很多零，还是因为我的输入管道有一个 bug？简单地打印图像并不是一个好主意。所以，我决定打印出输入的统计数据:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="723a" class="mp ls iq nc b gy ng nh l ni nj">numltg = tf.reduce_sum(labels)<br/>ref = tf.slice(img, [0, 0, 0, 0], [-1, -1, -1, 1])<br/>meanref = tf.reduce_mean(ref, [1, 2])<br/>ltg = tf.slice(img, [0, 0, 0, 1], [-1, -1, -1, 1])<br/>meanltg = tf.reduce_mean(ltg, [1, 2])</span><span id="7758" class="mp ls iq nc b gy no nh l ni nj">ylogits = tf.Print(ylogits, <br/>             [numltg, meanref, meanltg, ylogits], "...")</span></pre><p id="7601" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TensorFlow 中的 reduce_*函数允许您沿轴求和。因此，第一个函数 reduce_sum(labels)计算批次中标签的总和。由于标签是 0 或 1，这个总和告诉我该批中照明示例的数量。</p><p id="4ba5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我还想打印反射率和闪电输入图像补丁的平均值。为此，我想对高度和宽度求和，但保持每个示例和通道独立——这就是为什么在 reduce_mean 调用中看到[1，2]。第一个 tf.slice 获取第一个通道，第二个 slice 获取第二个通道(-TF . slice 中的 1 告诉 TensorFlow 获取那个维度中的所有元素)。</p><p id="258e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还要注意，我已经在 ylogits 的位置插入了 Print 节点，可以打印整个张量列表。这很重要——您必须将 Print()放入图中实际使用的节点。如果我做了:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="3489" class="mp ls iq nc b gy ng nh l ni nj">numltg = tf.Print(numltg, <br/>             [numltg, meanref, meanltg], "...")</span></pre><p id="d7ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">整个分支将被优化掉，因为我的模型实际上并没有在任何地方使用 numltg！</p><p id="9bc4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我运行代码，我发现每一批都有一个好的(和不同的)闪光点组合，每一批的平均值看起来有点相似，但每一批都不同。</p><h2 id="9d7c" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">2d。调整洗牌</h2><p id="7fec" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">虽然这不是我们试图解决的问题，但这种批量方式的相似性是很奇怪的。怀着这种好奇心查看代码，我发现我已经硬编码了一个洗牌大小:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="319c" class="mp ls iq nc b gy ng nh l ni nj">dataset = dataset.shuffle(1024)</span></pre><p id="5a4b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将此更改为</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="916f" class="mp ls iq nc b gy ng nh l ni nj">dataset = dataset.shuffle(batch_size * 50)</span></pre><p id="4fde" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解决了批处理问题。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/e654e313dfa5f58fcbf989e2575f5f90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wkw-soxPpNL7XmWZUkqlkg.jpeg"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Because successive patches are highly correlated, it is important to shuffle in a large buffer</figcaption></figure><p id="6a93" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">问题是，因为数据是从滑动窗口创建的，所以连续的例子往往高度相关，所以我需要在足够大的缓冲区内移动，以便从卫星图像的不同部分(或者更好的是，不同的图像)获得例子。在 ResNet 的案例中，每个训练示例都是完全不同的图像，因此这不是他们关心的问题。复制粘贴再次来袭！</p><h2 id="eae7" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">2e。ReLu 削波和饱和</h2><p id="c782" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">但是回到最初的问题。为什么准确度和 RMSE 卡住了？我在这里运气不错。我不得不插入 tf。Print()在某个地方，所以我进入了一个我知道我需要的节点——在我的模型函数(ylogits)的输出节点上。我还碰巧打印出了 ylogits，瞧……每次输入都是不同的值，ylogits 开始是随机的，但很快就变成了零。</p><p id="cff1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为什么 ylogits 为零？仔细查看 ylogits 的计算，我注意到我写了:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="f975" class="mp ls iq nc b gy ng nh l ni nj">ylogits = tf.layers.dense(<br/>  convout, 1, activation=tf.nn.relu, kernel_initializer=xavier)</span></pre><p id="5120" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">哎呀！通过在输出密集层上设置 ReLu 激活函数，我确保了 ylogits 永远不会为负。与此同时，闪电比非闪电更罕见，因此优化器将 ylogits 推到了它所能采取的最小可能值。也就是零。因为 ReLu 在零度以下饱和，所以东西有可能卡在那里。</p><p id="4409" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">分类网络的倒数第二层类似于回归网络的最后一层。它必须是:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="fd16" class="mp ls iq nc b gy ng nh l ni nj">ylogits = tf.layers.dense(<br/>  convout, 1, <strong class="nc ir">activation=None</strong>, kernel_initializer=xavier)</span></pre><p id="9200" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">傻，傻，小虫。发现并修复。咻！</p><h1 id="6abf" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">3.NaN 损失</h1><p id="2f22" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">现在，当我运行它，我没有得到一个卡住的准确性指标。更糟。我得了……”南在训练中失利如果有什么会让一个 ML 从业者感到恐惧的话，那就是 NaN 的损失。哦，好吧，如果我能解决这个问题，我会写一篇博文。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a3d18aacbd5a4d8764f8b45948e7a364.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*hT9HEVn4MhWULHGrlTzMLg.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">NaNs are spooky</figcaption></figure><p id="fe78" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与不训练的模型一样，NaN 的损失有几个常见的疑点。尝试自己计算交叉熵损失就是其中之一。但我没有那样做。我计算的损失是:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="b296" class="mp ls iq nc b gy ng nh l ni nj">loss = tf.reduce_mean(<br/>    tf.nn.sigmoid_cross_entropy_with_logits(<br/>        logits=tf.reshape(ylogits, [-1]),<br/>        labels=tf.cast(labels, dtype=tf.float32)))</span></pre><p id="a219" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这应该是数值稳定的。另一个问题是学习率太高。我切换回 AdamOptimizer，尝试设置一个较低的学习率(1e-6)。不去。</p><p id="c579" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一个问题是输入数据本身可能包含 nan。这应该是不可能的—使用 TFRecords 的好处之一是 TFRecordWriter 不会接受 NaN 值。为了确保万无一失，我回到输入管道，将 np.nan_to_num 添加到将数组插入 TFRecord 的代码段中:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="a81c" class="mp ls iq nc b gy ng nh l ni nj"><strong class="nc ir">def </strong>_array_feature(value):<em class="nm"><br/>  </em>value = np.nan_to_num(value.flatten())<br/>  <strong class="nc ir">return </strong>tf.train.Feature(float_list=tf.train.FloatList(value=value))</span></pre><p id="55d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还是不行。</p><h2 id="848b" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">3a。更简单的深度学习模型</h2><p id="44d8" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">也许有太多的重量？我使模型中的层数可配置，并尝试了不同的层数和更小的内核大小:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="c881" class="mp ls iq nc b gy ng nh l ni nj"><em class="nm">convout = img</em><strong class="nc ir"><br/>for </strong>layer <strong class="nc ir">in </strong>range(nlayers):<br/>  nfilters = (nfil // (layer+1))<em class="nm"><br/>  </em>nfilters = 1 <strong class="nc ir">if </strong>nfilters &lt; 1 <strong class="nc ir">else </strong>nfilters<br/>  <em class="nm"># convolution<br/>  </em>c1 = tf.layers.conv2d(<br/>      convout,<br/>      filters=nfilters,<br/>      kernel_size=ksize,<br/>      kernel_initializer=xavier,<br/>      strides=1,<br/>      padding=<strong class="nc ir">'same'</strong>,<br/>      activation=tf.nn.relu)<br/>  <em class="nm"># maxpool<br/>  </em>convout = tf.layers.max_pooling2d(c1, pool_size=2, strides=2, padding=<strong class="nc ir">'same'</strong>)<br/>  <strong class="nc ir">print</strong>(<strong class="nc ir">'Shape of output of {}th layer = {} {}'</strong>.format(<br/>      layer + 1, convout.shape, convout))<br/><br/>outlen = convout.shape[1] * convout.shape[2] * convout.shape[3]<br/>p2flat = tf.reshape(convout, [-1, outlen])  <em class="nm"># flattened<br/></em><strong class="nc ir">print</strong>(<strong class="nc ir">'Shape of flattened conv layers output = {}'</strong>.format(p2flat.shape))</span></pre><p id="96eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还是不行。南的损失依然存在。</p><h2 id="977f" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">3b。回到线性</h2><p id="8ca5" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">如果我完全去掉深度学习模型会怎么样？还记得我对输入图像的调试统计吗？如果我们试图训练一个模型，仅基于这些工程特征来预测照明，会怎么样？</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="675f" class="mp ls iq nc b gy ng nh l ni nj">halfsize = params[<strong class="nc ir">'predsize'</strong>]<br/>qtrsize = halfsize // 2<br/>ref_smbox = tf.slice(img, [0, qtrsize, qtrsize, 0], [-1, halfsize, halfsize, 1])<br/>ltg_smbox = tf.slice(img, [0, qtrsize, qtrsize, 1], [-1, halfsize, halfsize, 1])<br/>ref_bigbox = tf.slice(img, [0, 0, 0, 0], [-1, -1, -1, 1])<br/>ltg_bigbox = tf.slice(img, [0, 0, 0, 1], [-1, -1, -1, 1])<br/>engfeat = tf.concat([<br/>  tf.reduce_max(ref_bigbox, [1, 2]), <em class="nm"># [?, 64, 64, 1] -&gt; [?, 1]<br/>  </em>tf.reduce_max(ref_smbox, [1, 2]),<br/>  tf.reduce_mean(ref_bigbox, [1, 2]),<br/>  tf.reduce_mean(ref_smbox, [1, 2]),<br/>  tf.reduce_mean(ltg_bigbox, [1, 2]),<br/>  tf.reduce_mean(ltg_smbox, [1, 2])<br/>], axis=1)<em class="nm"><br/><br/></em>ylogits = tf.layers.dense(<br/>  engfeat, 1, activation=None, kernel_initializer=xavier)</span></pre><p id="42bd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我决定创建两组统计数据，一组在 64x64 的盒子中，另一组在 16x16 的盒子中，并用这 6 个输入特征创建一个逻辑回归模型。</p><p id="eac0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不去。还是南。这非常非常奇怪。一个线性模型应该永远不会过时。从数学上来说，这太疯狂了。</p><p id="d718" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是就在它结束之前，这个模型达到了 75%的准确率。这太有希望了。但是 NaN 的事越来越让人讨厌了。有趣的是，就在它与 loss = NaN“背离”之前，该模型根本没有背离，亏损一直在下降:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/035527cd9bd37cff258e1208b8ee4eb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*yvtjihszZ3zBuNhcUtLZ6Q.png"/></div></figure><h2 id="506e" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">3c。验证损失的输入</h2><p id="e6e1" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">四处寻找是否有另一种方法来计算损失，我发现现在有一个方便的函数:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="0e03" class="mp ls iq nc b gy ng nh l ni nj">loss = tf.losses.sigmoid_cross_entropy(labels,<br/>  tf.reshape(ylogits, [-1]))</span></pre><p id="739f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，这可能不能解决任何问题，但是最好使用这个函数，而不是自己调用 reduce_mean。但是在这里，让我们添加断言来缩小问题的范围:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="2828" class="mp ls iq nc b gy ng nh l ni nj"><strong class="nc ir">with </strong>tf.control_dependencies([<br/>  tf.Assert(tf.is_numeric_tensor(ylogits),[ylogits]),<br/>  tf.assert_non_negative(labels, [labels]),<br/>  tf.assert_less_equal(labels, 1, [labels])<br/>]):<br/>  loss = tf.losses.sigmoid_cross_entropy(labels,<br/>    tf.reshape(ylogits, [-1]))</span></pre><p id="2729" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本质上，我断言 ylogits 是数值型的，每个标签介于 0 和 1 之间。只有满足这些条件，我才能计算损失。否则，程序应该抛出一个错误。</p><h2 id="0a78" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">3d。剪辑渐变</h2><p id="d61d" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">断言不会触发，但是程序仍然以 NaN 丢失结束。在这一点上，似乎很清楚问题不在于输入数据(因为我在那里做了一个 nan_to_num)或我们的模型计算(因为断言不会触发)本身。它可能在反向传播中，可能在梯度计算中？</p><p id="df6d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，也许一个不寻常(但正确)的例子会导致意想不到的高梯度幅度。让我们通过剪切梯度来限制这种不寻常示例的影响:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="baa9" class="mp ls iq nc b gy ng nh l ni nj">optimizer = tf.train.AdamOptimizer(learning_rate=0.001)<br/>optimizer = <strong class="nc ir">tf.contrib.estimator.clip_gradients_by_norm(<br/>  optimizer, 5)</strong><br/>train_op = optimizer.minimize(loss, tf.train.get_global_step())</span></pre><p id="5c64" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那也没用。</p><h2 id="58b0" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">3e。L2 损失</h2><p id="7ca4" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">如果有许多非常相似的输入，权重本身可能会爆炸。随着时间的推移，一个输入节点可能会产生非常高的正幅度，而下一个输入节点可能会产生非常高的负幅度。使网络避免这种情况的一种方法是通过在损失函数中增加一个额外项来惩罚高幅度权重:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="f35a" class="mp ls iq nc b gy ng nh l ni nj">l2loss = tf.add_n(<br/>  [tf.nn.l2_loss(v) <strong class="nc ir">for </strong>v <strong class="nc ir">in </strong>tf.trainable_variables()])<br/>loss = loss + 0.001 * l2loss</span></pre><p id="4c34" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这将获得所有可训练变量(权重和偏差),并根据这些可训练变量的值对损失进行惩罚。理想情况下，我们只惩罚体重(而不是偏见)，但这只是为了尝试。</p><h2 id="153c" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">3f。亚当·艾司隆</h2><p id="fd35" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">再多逛逛，我意识到 AdamOptimizer 的<a class="ae mo" href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer" rel="noopener ugc nofollow" target="_blank">文档解释默认的ε值 1e-8 可能有问题。本质上，小的ε值会导致不稳定，即使ε的目的是防止被零除。这回避了为什么这是默认值的问题，但是让我们试试推荐的更大的值。</a></p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="2101" class="mp ls iq nc b gy ng nh l ni nj">optimizer = tf.train.AdamOptimizer(<br/>    learning_rate=params[<strong class="nc ir">'</strong>learning_rate<strong class="nc ir">'</strong>], <strong class="nc ir">epsilon=0.1)</strong></span></pre><p id="7d11" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就把 NaN 推得更远了，但还是失败了。</p><h2 id="bb55" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">3g。不同的硬件</h2><p id="e637" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">另一个原因可能是 CUDA 错误之类的。让我们尝试在不同的硬件上进行训练(用 P100 代替特斯拉 K80 ),看看问题是否仍然存在。</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="065c" class="mp ls iq nc b gy ng nh l ni nj">trainingInput:<br/>  scaleTier: CUSTOM<br/>  masterType: complex_model_m_p100</span></pre><p id="0f4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还是不行。</p><h1 id="4d5a" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">4.重写为 Keras</h1><p id="0cce" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">有时，当留下一个难以解决的 bug 时，最好尝试用一种完全不同的方式重写模型。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/2ee1bac5372be075a817216ac544f8b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*MinLAxfTxoxsyAls62lzUw.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Let’s start again</figcaption></figure><p id="a1cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，我决定用 Keras 重写模型。这也给了我学习 Keras 的机会，这是我一直想做的事情。用柠檬做柠檬汁之类的。</p><h2 id="55fa" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">4a。CNN 与 Batchnorm</h2><p id="401d" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">在 Keras 中，用 batchnorm 创建 CNN(这将有助于保持范围内的渐变)非常容易。(谢谢你，弗朗索瓦)。简单到我到处都放了 batchnorm。</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="9ecc" class="mp ls iq nc b gy ng nh l ni nj">img = keras.Input(shape=[height, width, 2])<br/>cnn = keras.layers.BatchNormalization()(img)<br/><strong class="nc ir">for </strong>layer <strong class="nc ir">in </strong>range(nlayers):<br/>  nfilters = nfil * (layer + 1)<br/>  cnn = keras.layers.Conv2D(nfilters, (ksize, ksize), padding=<strong class="nc ir">'same'</strong>)(cnn)<br/>  cnn = keras.layers.Activation(<strong class="nc ir">'elu'</strong>)(cnn)<br/>  cnn = keras.layers.BatchNormalization()(cnn)<br/>  cnn = keras.layers.MaxPooling2D(pool_size=(2, 2))(cnn)<br/>  cnn = keras.layers.Dropout(dprob)(cnn)<br/>cnn = keras.layers.Flatten()(cnn)<br/>ltgprob = keras.layers.Dense(10, activation='sigmoid')(cnn)</span></pre><p id="9669" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还要注意，最后一层直接增加了一个 s 形激活。没有必要在逻辑上浪费时间，因为优化为我们解决了数字问题:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="332a" class="mp ls iq nc b gy ng nh l ni nj">optimizer = tf.keras.optimizers.Adam(lr=params[<strong class="nc ir">'learning_rate'</strong>])<br/>model.compile(optimizer=optimizer,<br/>              loss=<strong class="nc ir">'binary_crossentropy'</strong>,<br/>              metrics=[<strong class="nc ir">'accuracy'</strong>, <strong class="nc ir">'mse'</strong>])</span></pre><p id="a532" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不错！</p><p id="740b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是(你现在知道了)，我还是得了 NaNs！</p><p id="06fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">啊。好吧，回到起点。让我们在喀拉斯做所有在 TensorFlow 做过的事情。</p><h2 id="81c6" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">4b。Keras 中的特征工程</h2><p id="481a" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">第一步是忘掉所有这些深度学习的东西，建立一个线性模型。你是怎么做到的？我有一个图像。我需要做特征工程，送到密集层。在 Keras 中，这意味着我必须编写自己的层来完成功能工程:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="47d0" class="mp ls iq nc b gy ng nh l ni nj"><strong class="nc ir">def </strong>create_feateng_model(params):<br/>  <em class="nm"># input is a 2-channel image<br/>  </em>height = width = 2 * params[<strong class="nc ir">’predsize’</strong>]<br/>  img = keras.Input(shape=[height, width, 2])<br/><br/>  engfeat = keras.layers.Lambda(<br/>    <strong class="nc ir">lambda </strong>x: engineered_features(x, height//2))(img)<br/><br/>  ltgprob = keras.layers.Dense(1, activation=<strong class="nc ir">’sigmoid’</strong>)(engfeat)<br/><br/>  <em class="nm"># create a model<br/>  </em>model = keras.Model(img, ltgprob)</span></pre><p id="f5df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">engineered_features 与之前的 TensorFlow 函数完全相同！关键思想是，要将 TensorFlow 函数包装到 Keras 层中，可以使用 Lambda 层并调用 TensorFlow 函数。</p><h2 id="478a" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">4c。打印图层</h2><p id="3abd" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">但是我想打印出图层，以确保流过的数字是正确的。我该怎么做？tf。Print()不行，因为，嗯，我没有张量。我有 Keras 层。</p><p id="6841" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">嗯，tf。Print()是一个张量流函数，因此，使用相同的 Lambda 层技巧:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="8b31" class="mp ls iq nc b gy ng nh l ni nj"><strong class="nc ir">def </strong>print_layer(layer, message, first_n=3, summarize=1024):<br/>  <strong class="nc ir">return </strong>keras.layers.Lambda((<br/>    <strong class="nc ir">lambda </strong>x: tf.Print(x, [x],<br/>                      message=message,<br/>                      first_n=first_n,<br/>                      summarize=summarize)))(layer)</span></pre><p id="54c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后可以调用它作为:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="ad45" class="mp ls iq nc b gy ng nh l ni nj">engfeat = print_layer(engfeat, <strong class="nc ir">"engfeat="</strong>)</span></pre><h2 id="18f5" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">4d。剪裁渐变</h2><p id="4f57" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">在 Keras 中裁剪渐变？轻松点。每个优化器都支持 clipnorm。</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="dbd1" class="mp ls iq nc b gy ng nh l ni nj">optimizer = tf.keras.optimizers.Adam(lr=params[<strong class="nc ir">'learning_rate'</strong>],<br/>                                     clipnorm=1.)</span></pre><p id="7f8c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">嘿，我喜欢 Keras 这个东西——它给了我一个漂亮、简单的 API。此外，它与 TensorFlow 很好地互操作，在我需要的时候给我低级别的控制。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi np"><img src="../Images/31d5fd930ae3dc1d8d56caa58691d6a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*tfdzRkLiDOVNix11DtyP-w.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">NaN is still there, slurping my milkshake</figcaption></figure><p id="3fb5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">哦，对了。我仍然有南的问题</p><h1 id="7525" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">5.揭露数据</h1><p id="f08c" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">最后一件事，我有点忽略了。NaN 问题也可能由未缩放的数据引起。但是我的反射率和闪电数据都在范围[0，1]内。所以，我根本不需要缩放。</p><p id="2f67" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管如此，我还是闲着没事。不如我把图像数据归一化(减去均值，除以方差)，看看有没有帮助。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi np"><img src="../Images/51b5102496374ccfdc8d4ca8e37605be.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*ysKOeFFXu-_ghdI15YFApA.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Yes, I’m clutching at straws now</figcaption></figure><p id="8df7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了计算方差，我需要遍历整个数据集，所以这是 Beam 的工作。我可以用张量流变换来做这个，但是现在，让我用 Beam 来破解它。</p><h2 id="b844" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">5a。混洗波束中的训练数据</h2><p id="61b1" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">既然我正在重写我的管道，我也可以一劳永逸地解决洗牌的问题(见第 2d 节)。增加洗牌缓冲区大小是一个黑客。我真的不想按照创建图像补丁的顺序写入数据。让我们随机排列 Apache Beam 中图像补丁的顺序:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="b0da" class="mp ls iq nc b gy ng nh l ni nj"><em class="nm"># shuffle the examples so that each small batch doesn't contain<br/># highly correlated records<br/></em>examples = (examples<br/>    | <strong class="nc ir">'{}_reshuffleA'</strong>.format(step) &gt;&gt; beam.Map(<br/>        <strong class="nc ir">lambda </strong>t: (random.randint(1, 1000), t))<br/>    | <strong class="nc ir">'{}_reshuffleB'</strong>.format(step) &gt;&gt; beam.GroupByKey()<br/>    | <strong class="nc ir">'{}_reshuffleC'</strong>.format(step) &gt;&gt; beam.FlatMap(<strong class="nc ir">lambda </strong>t: t[1]))</span></pre><p id="0732" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本质上，我给每个记录分配一个随机键(1 到 1000 之间),按随机键分组，删除键并写出记录。现在，连续的图像补片不会一个接一个地跟随。</p><h2 id="b0c0" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">5b。计算阿帕奇波束的方差</h2><p id="df72" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">如何计算阿帕奇波束的方差？我像往常一样，在 StackOverflow 上搜索一些我可以复制粘贴的东西。不幸的是，我只找到了一个没有答案的问题。哦，好吧，认真编写一个定制的合并器:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="b988" class="mp ls iq nc b gy ng nh l ni nj">import apache_beam as beam<br/>import numpy as np</span><span id="eb4a" class="mp ls iq nc b gy no nh l ni nj">class MeanStddev(beam.CombineFn):<br/>  def create_accumulator(self):<br/>    return (0.0, 0.0, 0) # x, x^2, count</span><span id="5a36" class="mp ls iq nc b gy no nh l ni nj">def add_input(self, sum_count, input):<br/>    (sum, sumsq, count) = sum_count<br/>    return sum + input, sumsq + input*input, count + 1</span><span id="a1c4" class="mp ls iq nc b gy no nh l ni nj">def merge_accumulators(self, accumulators):<br/>    sums, sumsqs, counts = zip(*accumulators)<br/>    return sum(sums), sum(sumsqs), sum(counts)</span><span id="6bda" class="mp ls iq nc b gy no nh l ni nj">def extract_output(self, sum_count):<br/>    (sum, sumsq, count) = sum_count<br/>    if count:<br/>      mean = sum / count<br/>      variance = (sumsq / count) - mean*mean<br/>      # -ve value could happen due to rounding<br/>      stddev = np.sqrt(variance) if variance &gt; 0 else 0<br/>      return {<br/>        'mean': mean,<br/>        'variance': variance,<br/>        'stddev': stddev,<br/>        'count': count<br/>      }<br/>    else:<br/>      return {<br/>        'mean': float('NaN'),<br/>        'variance': float('NaN'),<br/>        'stddev': float('NaN'),<br/>        'count': 0<br/>      }<br/>    <br/>    <br/>[1.3, 3.0, 4.2] | beam.CombineGlobally(MeanStddev())</span></pre><p id="246c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意这里的工作流程——我可以在一系列数字上快速测试它，以确保它正常工作。</p><p id="551b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我去给 StackOverflow 添加了<a class="ae mo" href="https://stackoverflow.com/a/52958771/3645038" rel="noopener ugc nofollow" target="_blank">答案</a>。也许我需要的只是一些好的因果报应…</p><h2 id="b120" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">5b。写出平均值、方差</h2><p id="0152" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">然后，我可以转到我的管道代码并添加:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="daf8" class="mp ls iq nc b gy ng nh l ni nj"><strong class="nc ir">if </strong>step == <strong class="nc ir">'train'</strong>:<br/>  _ = (examples<br/>    | <strong class="nc ir">'get_values' </strong>&gt;&gt; beam.FlatMap(<br/>        <strong class="nc ir">lambda </strong>x : [(f, x[f]) <strong class="nc ir">for </strong>f <strong class="nc ir">in </strong>[<strong class="nc ir">'ref'</strong>, <strong class="nc ir">'ltg'</strong>]])<br/>    | <strong class="nc ir">'compute_stats' </strong>&gt;&gt; beam.CombinePerKey(MeanStddev())<br/>    | <strong class="nc ir">'write_stats' </strong>&gt;&gt; beam.io.Write(beam.io.WriteToText(<br/>        os.path.join(options[<strong class="nc ir">'outdir'</strong>], <strong class="nc ir">'stats'</strong>), num_shards=1))<br/>  )</span></pre><p id="ff27" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本质上，我提取 example['ref']和 example['ltg']并创建元组，然后按键对其进行分组。然后，我可以计算整个数据集上这两幅图像中每个像素的平均值和标准差。</p><p id="f242" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我运行了管道，我就可以打印结果统计数据:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="47ea" class="mp ls iq nc b gy ng nh l ni nj">gsutil cat gs://$BUCKET/lightning/preproc/stats*</span><span id="84b3" class="mp ls iq nc b gy no nh l ni nj">('ltg', {'count': 1028242, 'variance': 0.0770683210620995, 'stddev': 0.2776118172234379, 'mean': 0.08414945119923131})</span><span id="f235" class="mp ls iq nc b gy no nh l ni nj">('ref', {'count': 1028242, 'variance': <strong class="nc ir">masked</strong>, 'stddev': 0, 'mean': masked})</span></pre><p id="3b9b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">蒙面？@#$@#$@#是什么意思？原来<a class="ae mo" href="https://currents.soest.hawaii.edu/ocn760_4/_static/masked_arrays.html" rel="noopener ugc nofollow" target="_blank">掩码数组</a>是一个特殊的数字东西。被屏蔽的值不是 NaN，所以如果用 Numpy 处理它们，nan_to_num()不会对它做任何事情。另一方面，它看起来是数值，所以我所有的张量流断言都不成立。带有屏蔽值的数值运算会产生屏蔽值。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/6570b3c03caebd446152d4c5bc01b184.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*1x8jmSSe2Jtww9gb.jpg"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Masked? What the @#@$#@ does masked mean?</figcaption></figure><p id="33fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">屏蔽值就像 NaN 一样——它们会从房间的另一端啜饮你的奶昔，但是常规的 numpy 和 TensorFlow 库方法对屏蔽一无所知。</p><h2 id="a465" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">5c。更改屏蔽值</h2><p id="0051" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">由于屏蔽只发生在反射率网格中(我自己通过积累闪电创建闪电网格)，我必须在读取后将屏蔽值转换成一个好的数字</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="3a61" class="mp ls iq nc b gy ng nh l ni nj">ref = goesio.read_ir_data(ir_blob_path, griddef)<br/>ref = np.ma.filled(ref, 0) <em class="nm"># mask -&gt; 0</em></span></pre><p id="b2e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，当我重新运行管道时，我得到了合理的值:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="f440" class="mp ls iq nc b gy ng nh l ni nj">('ref', {'count': 1028242, 'variance': 0.07368491739234752, 'stddev': 0.27144965903892293, 'mean': 0.3200035849321707})</span><span id="cac0" class="mp ls iq nc b gy no nh l ni nj">('ltg', {'count': 1028242, 'variance': 0.0770683210620995, 'stddev': 0.2776118172234379, 'mean': 0.08414945119923131})</span></pre><p id="2279" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些都是小数字。不应该有任何缩放的理由。因此，让我们在解决掩蔽问题的情况下进行训练。</p><p id="6ccb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一次，没有楠。相反，程序崩溃，日志显示:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="6479" class="mp ls iq nc b gy ng nh l ni nj">Filling up shuffle buffer (this may take a while): 251889 of 1280000<br/>The replica master 0 ran out-of-memory and exited with a non-zero status of 9(SIGKILL)</span></pre><h2 id="c805" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">5d。减小混洗缓冲区大小</h2><p id="d079" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">副本零是输入管道(读取数据发生在 CPU 上)。为什么它要一次将 1280000 条记录全部读入 shuffle 缓冲区？</p><p id="ead4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，输入数据已经被我的光束/数据流很好地混洗了，我甚至不需要那么大的混洗缓冲区(以前是 5000，见 2d 部分):</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="bdb5" class="mp ls iq nc b gy ng nh l ni nj">dataset = dataset.shuffle(batch_size * 50) <em class="nm"># shuffle by a bit</em></span></pre><p id="8732" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">12 分钟后，训练以 83%的准确率结束。！！).哪儿都没有 NaNs。</p><p id="442b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">吼吼！</p><h2 id="efc0" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">5e。连接模型</h2><p id="0771" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">请记住，我们只做了一个由工程特征组成的线性模型。让我们并行添加回 convnet。想法是连接两个密集层，这样我的模型架构可以包含 CNN 层和特征工程:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/1aa3f011203cc6581715f7f7a97b7318.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J7JdA7-KzeM7NI5fZib9WQ.jpeg"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">A combined model would be great</figcaption></figure><p id="f13a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是如何在 Keras 中做到这一点的，其中加入了一些 batchnorm 和 dropouts，因为它们很容易添加:</p><pre class="lc ld le lf gt nb nc nd ne aw nf bi"><span id="1b6f" class="mp ls iq nc b gy ng nh l ni nj">cnn = keras.layers.BatchNormalization()(img)<br/><strong class="nc ir">for </strong>layer <strong class="nc ir">in </strong>range(nlayers):<br/>  nfilters = nfil * (layer + 1)<br/>  cnn = keras.layers.Conv2D(nfilters, (ksize, ksize), padding=<strong class="nc ir">'same'</strong>)(cnn)<br/>  cnn = keras.layers.Activation(<strong class="nc ir">'elu'</strong>)(cnn)<br/>  cnn = keras.layers.BatchNormalization()(cnn)<br/>  cnn = keras.layers.MaxPooling2D(pool_size=(2, 2))(cnn)<br/>cnn = keras.layers.Flatten()(cnn)<br/>cnn = keras.layers.Dropout(dprob)(cnn)<br/>cnn = keras.layers.Dense(10, activation=<strong class="nc ir">'relu'</strong>)(cnn)<br/><br/><em class="nm"># feature engineering part of model<br/></em>engfeat = keras.layers.Lambda(<br/>  <strong class="nc ir">lambda </strong>x: engineered_features(x, height//2))(img)<br/><br/><em class="nm"># concatenate the two parts<br/></em>both = keras.layers.concatenate([cnn, engfeat])<br/>ltgprob = keras.layers.Dense(1, activation=<strong class="nc ir">'sigmoid'</strong>)(both)</span></pre><p id="e884" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我有 85%的准确率。这仍然是一个小数据集(只有 60 天的卫星数据)，这可能是深度学习路径没有增加那么多价值的原因。所以，我会回去生成更多的数据，训练更长的时间，在 TPU 上训练等等。</p><p id="fe8d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是这些事情可以等。现在，我要去庆祝了。</p></div></div>    
</body>
</html>