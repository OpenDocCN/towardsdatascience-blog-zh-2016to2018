<html>
<head>
<title>Neural Networks: Error-Prediction Layers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络:误差预测层</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-error-prediction-layers-fd8181dc33cd?source=collection_archive---------4-----------------------#2017-07-24">https://towardsdatascience.com/neural-networks-error-prediction-layers-fd8181dc33cd?source=collection_archive---------4-----------------------#2017-07-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="76cc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">早在 2005 年，杰夫·霍金斯就写过“<a class="ae kl" href="https://www.amazon.com/Intelligence-Understanding-Creation-Intelligent-Machines/dp/0805078533" rel="noopener ugc nofollow" target="_blank">论智力</a>”——关于人类神经科学中的一个奇特发现，<em class="km">尚未被深度学习利用</em>。它值得仔细看看。</p><p id="8948" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">人类、海豚和猴子拥有不同于其他生物的大脑:在我们的额叶，我们有许多<strong class="jp ir">相邻的多种<em class="km">类型</em>神经元的堆叠</strong>——就像一张覆盖着盘子的早餐桌上，每个盘子上都有自己的一堆煎饼，点缀着配料。这些神经元以一种特殊的方式发挥作用:</p><ul class=""><li id="3597" class="kn ko iq jp b jq jr ju jv jy kp kc kq kg kr kk ks kt ku kv bi translated">最下面的<em class="km">层</em>被赋予<strong class="jp ir">预测下一时刻</strong>的任务；每当它知道未来时，多巴胺就会激增，并加强它的联系。</li><li id="0233" class="kn ko iq jp b jq kw ju kx jy ky kc kz kg la kk ks kt ku kv bi translated">下一个层<em class="km">的任务是<strong class="jp ir">预测下层何时出错</strong>；每当它知道下层</em>的未来<em class="km">时，它就会感到兴奋，并加强它的学习。</em></li><li id="a822" class="kn ko iq jp b jq kw ju kx jy ky kc kz kg la kk ks kt ku kv bi translated"><em class="km">进一步的</em>层也被赋予预测其下层何时<strong class="jp ir">出错</strong>的任务；他们学会成为错误检测者。</li></ul><p id="4e70" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">人类在我们的“煎饼堆”中有六层，我们有一个有数百万个盘子的“早餐桌”，以形成我们更高层次的推理和自我反思的智能。这个模型与今天的人工神经网络完全不同。</p><p id="cd4a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">但是，深度神经网络工作得很好……</strong></p><p id="6020" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">是的，他们有。甘斯和 LSTM 网络公司也是如此。这些方法很快就找到了将数据压缩成特征的好方法。然而，我们的大脑做的不止这些。</p><p id="51a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们比较一下:一个循环神经网络，和人脑。rnn 接收环境的当前状态(其“输入”是屏幕上的像素)并在该环境中产生一个动作(“输出”在可能动作的空间中，而“输入”在可能像素排列的空间<em class="km"/>)。然而，人类的额叶有一个最底层的“煎饼”，它取系统的当前状态(即“输入”)，并试图<strong class="jp ir">预测系统的未来状态</strong>(“输出”是与“输入”在同一空间上的<em class="km">，<strong class="jp ir">损失函数是两个</strong>之差)。人脑不会像 RNN 那样试图“选择游戏中的最佳走法”！我们实际上是在试图<strong class="jp ir">预测接下来</strong>会发生什么。</em></p><p id="2ea9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，一个 RNN 可能有许多“层”神经元，但它只代表单一“层”的<em class="km">功能</em>。像素输入→动作输出。我们自己的“煎饼”栈实际上就像多个神经网络的<em class="km">栈</em>一样运行。每个“煎饼”接受状态输入→状态预测输出。更高层的“煎饼”起着<strong class="jp ir">错误检测器</strong>的作用，因为它们观察到的<em class="km">状态</em>是它们更低层的基本事实与其预测之间的<strong class="jp ir">差异</strong>的映射。<strong class="jp ir">较高层只看到较低层做出<em class="km">错误预测</em> </strong>的像素。</p><p id="4070" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们希望神经网络的行为像额叶一样，它需要映射像素输入→像素<em class="km">预测</em>输出，然后将该输出与随后时刻的地面真实像素进行比较，以查看<em class="km">哪里的预测是错误的</em>。那将是最低级的“煎饼”。然后，被<em class="km">误预测的像素</em>将成为下一个“煎饼”的<strong class="jp ir">输入</strong>，并且该“煎饼”试图预测下一个时刻的误差将在<strong class="jp ir">的哪里。第二个“煎饼”将是一个新的神经网络。</strong></p><p id="383c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将需要六个这样的神经网络，一个堆叠在另一个之上——让事情变得更复杂的是，更高层的“煎饼”也将<em class="km">接收来自多个特征检测器神经元的信号。我们的第六个“煎饼”将接收第五个“煎饼”的<strong class="jp ir">误差</strong>作为输入，以及来自第一个、第二个、第三个和第四个煎饼的一些<strong class="jp ir">特征检测器信号</strong>！这与现有的人工神经网络不同，我认为这种差异很重要。</em></p><p id="19d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">有什么帮助？</strong></p><p id="5545" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">目前，研究人员期望单个神经网络每次都能正确处理问题。大脑不是这样工作的。在我们六个‘煎饼’中，第一个‘煎饼’神经网络出错<em class="km">往往</em>。如果我们将一个人工神经网络训练成最低的“煎饼”，我们将需要尽早降低学习速度，并在过度拟合之前<strong class="jp ir">停止。我们的网络仍然会得到许多错误的答案。</strong></p><p id="1f62" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我们需要第二个“煎饼”，第二个神经网络，接收下层的错误。那一层也会很早就停止。很可能<em class="km"/>仍然会<strong class="jp ir">错误地判断最低层何时会出错</strong>。只有当许多这样的“煎饼”堆叠在一起时，错误率才会显著下降。</p><p id="bc5a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于注重数字的人来说:当前的 NLP 网络在大约 4%的时间里是错误的。同时，假设一个有六个“煎饼”的“额叶”有一个最低的“煎饼”有 50%的时间是错误的。就其本身而言，这个“煎饼”比我们目前的网络要糟糕得多。然而，它的错误实例被传递给第二个“煎饼”。这个煎饼只寻找 50%的错误，我们可以假设它纠正了 50%的错误。到目前为止，这两个“煎饼”加在一起，有 75%的正确率。有了这些“煎饼”中的六个“煎饼”，每个“煎饼”只纠正一半剩余的错误，组合的准确度是 98.4375%！因此，即使每个错误检测器都“有故障”,错误检测器的堆栈也能很快胜过端到端网络。</p><p id="d550" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">通过种植煎饼预测误差</strong></p><p id="9e0b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">人类有更强的理性和反思能力，我们的盘子里也有更多的“煎饼”!海豚有四个，猿和猴子更少。我预计，如果一台机器有比我们更多的<em class="km"/>【煎饼】，每个“煎饼”都试图预测它下面的“煎饼”的误差，那么这台机器将会比我们更有能力。这为机器智能开辟了一个新的方向，<em class="km">在前进中学习</em>。</p><p id="4891" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">边学边做:机器将从一个单一的深度神经网络开始，并被赋予预测下一时刻的任务。当它的成功率超过某个阈值时，在顶部添加一个新的深度神经网络。这个新网络将接收下层网络的错误预测，并负责预测下一个错误将发生在哪里。当网络的成功率超过阈值时，添加一个新的深度神经网络。继续这个过程，以逐步改善网络的组合。</p><p id="d9d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过这种“煎饼”范式，网络通过在所有旧网络之上生长另一个深度神经网络“煎饼”<em class="km">来响应<strong class="jp ir">新信息</strong>。学习永远不会停止。“煎饼”堆得越来越高。当与专家混合的各种神经网络相结合时，这个概念变得更加重要。</em></p><p id="622d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">专家混合</strong></p><p id="b756" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在混合专家神经网络中，神经元像树莓一样“聚集”成密集连接的束。而且，就像树莓酱一样，有一些长距离的连接将所有的树莓“粘合”在一起。目前，专家模型混合就此打住。当一个输入进入果酱堆的底部时，它只激活几个“树莓”，每个“树莓”执行一点点特征检测。这些特征激活了果酱堆中较高的一些“覆盆子”，在那里检测到更高水平的特征。</p><p id="5d85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">沿着果酱堆向上移动，这些专家覆盆子能够发现不同输入的特征；对于输入的每个子集，不同的一组专家开始工作。专家混合网络的行为就像许多稀疏网络的联合，每个覆盆子都是其中一些网络的 T2 交集。</p><p id="cd9e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(在稀疏网络的拥挤维恩图中，每个网络都有特征检测器，这些特征检测器<em class="km">与任何其他网络有一点点</em>重叠；因为有如此多的网络在一起，组合图允许大多数稀疏网络与其他人共享大多数特征检测器。例如，稀疏网络#1 可能利用特征检测器“树莓”A、B 和 C。同时，稀疏网络#2 使用 A、D 和 e。网络#3 需要检测特征 C、D 和 e 的“树莓”。因此，您可以将这三个稀疏网络组合成检测所有特征的专家混合物:A、B、C、D 和 e。每个稀疏网络仅在少数地方与其他稀疏网络重叠。但是，综合起来看，所有的专家都有重叠之处。)</p><p id="076a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">所有浇头</strong></p><p id="c0d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回到构成我们额叶的“煎饼”。为了与我们自己的大脑相匹配，煎饼的比喻需要更加精细:我们的神经元显示出从一个盘子到另一个盘子的跨越堆叠的链接。这就像一个覆盖着盘子的早餐桌，每个盘子上堆放着六个“煎饼”……并且<em class="km">覆盆子果酱涂抹在所有的盘子上</em>，沿着盘子的侧面滴落，并且每个盘子都接触到其他的盘子！我们的大脑很混乱。</p><p id="9c3a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们想要一个像我们一样学习和成长的人工神经网络，它将需要多个错误预测深度神经网络的“煎饼”，其中每个深度神经网络都由专家的分层混合物组成。每个“煎饼”网络接收其下方“煎饼”的<strong class="jp ir">错误</strong>、<em class="km">以及由相邻“煎饼”检测到的一些<strong class="jp ir">特征</strong>作为输入。这与当前的深度神经网络架构大相径庭。而且，值得一试。</em></p></div></div>    
</body>
</html>