<html>
<head>
<title>Transforming text to Sentence Embeddings via some thoughts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过一些想法将文本转化为句子嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c?source=collection_archive---------11-----------------------#2018-09-29">https://towardsdatascience.com/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c?source=collection_archive---------11-----------------------#2018-09-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/eb0bb124c893339e8aedeb5158a78ff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JsGeQNbrk9FDyM8v"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">“man standing on mountain peak” by <a class="ae kf" href="https://unsplash.com/@sammieeev?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sammie Vasquez</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ff45" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">学习一个通用的分布式嵌入层是 NLP 问题中的一个重要步骤。从 Mikolov 等人(2013)开始，引入了许多监督学习和非监督学习方法来检索高质量的文本表示。</p><p id="5f0f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Kiros 等人在 2015 年引入了 skip-thinks，其目标是提供句子级向量。看完这篇文章，你会明白:</p><ul class=""><li id="df6c" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">跳跃式设计</li><li id="373e" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">体系结构</li><li id="7f06" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">履行</li><li id="4f19" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">拿走</li></ul><h1 id="2219" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">跳跃式设计</h1><p id="48e9" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">如果您不熟悉构建单词嵌入的跳格方法，您可以查看这个<a class="ae kf" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a">博客</a>了解更多细节。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mv"><img src="../Images/41713ab6bcc0702845bb21b6d08a05f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CeYMkWWXqgf_zAbl"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">“women's white tank top on top of the hill” by <a class="ae kf" href="https://unsplash.com/@cristina_gottardi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Cristina Gottardi</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="6b88" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Skip-thoughts 将 skip-grams 模型从单词嵌入扩展到句子嵌入。跳跃思维不是通过周围的词来预测上下文，而是通过周围的句子来预测目标句子。典型的例子是用上一句和下一句来预测当前句。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi na"><img src="../Images/42dc0176377a610ad6b1c45ac5740f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jKZr6M_UOUizE3ej74ohwg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Kiros et al. (2015)</figcaption></figure><p id="0fda" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面的例子中，</p><ul class=""><li id="86c8" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">上一句:我回到了家。</li><li id="679f" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">我能看见台阶上的猫。</li><li id="5169" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">下一句:这很奇怪。</li></ul><p id="c004" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第二，单词嵌入提供了更好的单词表示，而不考虑句子中的顺序。递归神经网络(RNN)可以处理句子中的单词顺序，但 skip-thoughts 也会考虑这个问题。</p><p id="f444" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作者证明了该模型能够捕获句子的语义和句法，从而可以将高质量的嵌入结果转移到下游问题。</p><h1 id="cbdd" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">体系结构</h1><p id="4c01" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">该模型遵循编码器-解码器框架架构来计算矢量。在神经网络层，它可以是 GRU(门控循环单元)，LSTM(长短期记忆)或注意机制。</p><p id="d33c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nb">编码器</em> </strong>:输入的是每句话的词向量序列，传递给 RNN (GRU 或 LSTM)。经过训练的模型，将用于下游任务，如分类问题。</p><p id="23c2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nb">解码器</em> </strong>:解码器的架构与编码器类似，只是引入了一些矩阵。此外，有两个解码器而不是一个。第一个是为了消耗上一个句子而决定的，而第二个是为了消耗下一个句子。不同之处在于它们彼此之间不共享解码器权重(隐藏状态),但是它们共享词汇向量。解码器帮助训练编码器，之后就不再使用了。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nc"><img src="../Images/fd9a7dc4ddf2ae838ae1ec93d60fa15b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QROmyA1elZqFSk1H"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">“gray concrete post on seashore under gray sky” by <a class="ae kf" href="https://unsplash.com/@perminder_klair?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Perminder Klair</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4d80" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">其思想是使用预先训练的嵌入(例如 word2vec)作为输入(即 x 或特征)和输出(即 y 或标签)是嵌入层的另一个维度。以便它可以“预测”每个单词的嵌入，即使它从未在训练数据中出现过。</strong></p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nd"><img src="../Images/5e818e7757dcec5c0ab01e3995d9304b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fiD7fT6-fQoYaLphlU1IvA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Kiros et al. (2015). Showing the nearest neighbors of words after vocabulary expansion. For example, “chorepgraph” is OOV and the the nearest neighbors of words are “choreography“, “choreographs”.</figcaption></figure><p id="34b0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">文中没有详细的解释，所以我进一步研究了源代码，以了解它是如何工作的。你也可以参考<a class="ae kf" href="https://github.com/ryankiros/skip-thoughts/blob/master/training/tools.py" rel="noopener ugc nofollow" target="_blank">源代码</a> (train_regresso 函数)来浏览它。</p><p id="6274" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据 Kiros 等人的设计，有两种方法，即单跳和双跳。Uni-skip 指使用单向神经网络来构建句子嵌入，而 bi-skip 使用双向 RNN (LSTM 或 GRU)来构建句子嵌入。单跳(或双跳)的输出是句子的 2400 维(如果使用组合跳过方法，则是 4800 维)。我们可以通过给定下游任务(例如分类)输出来训练模型。</p><h1 id="748a" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">履行</h1><p id="c090" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">最初的作者是 Kiros 等人，他们通过 Theano 实现了跳过思想。有 Pytorch，Tensorflow 和 Keras 版本可用。我将使用 Pytorch 版本来演示我们如何将数据转换成句子向量。对于其他人，您可以查看参考资料部分提到的 githubs。</p><p id="b439" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">给定预先训练好的模型、词汇文件和输入(句子)，你将得到 2400 维向量的输出(对于 uni-skip)。</p><h1 id="888b" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">拿走</h1><p id="735d" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">要访问所有代码，你可以访问我的<a class="ae kf" href="https://github.com/makcedward/nlp/blob/master/sample/nlp-skip_thoughts.ipynb" rel="noopener ugc nofollow" target="_blank"> github </a> repo。</p><ul class=""><li id="1fdd" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">skip-thoughts 和 skip-gram 都是<strong class="ki iu">无监督学习</strong>。</li><li id="50a8" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">Skip-thoughts 的目标是学习一个<strong class="ki iu">句子级向量</strong>而不是单词向量。</li><li id="21d9" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">由于它是固定的句子表示，如果你的“句子”很长，它将消耗大量的内存。它不应该用于段落或文档嵌入。</li></ul><h1 id="1c1d" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">参考</h1><ul class=""><li id="4794" class="le lf it ki b kj mq kn mr kr ne kv nf kz ng ld lj lk ll lm bi translated">Ryan Kiros、Yukun Zhu、Ruslan Salakhutdinov、Richard S. Zemel、Antonio Torralba、Raquel Urtasun 和 Sanja Fidler。<a class="ae kf" href="http://arxiv.org/abs/1506.06726" rel="noopener ugc nofollow" target="_blank">跳过思维向量</a></li><li id="7ed9" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">s<a class="ae kf" href="https://github.com/ryankiros/skip-thoughts" rel="noopener ugc nofollow" target="_blank">kip-thinks the ano github</a></li><li id="4b98" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae kf" href="https://github.com/Cadene/skip-thoughts.torch/tree/master/pytorch" rel="noopener ugc nofollow" target="_blank">Skip-thought py torch github</a></li><li id="4aa3" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae kf" href="https://github.com/tensorflow/models/tree/master/research/skip_thoughts" rel="noopener ugc nofollow" target="_blank">Skip-Thoughts tensor flow github</a></li><li id="b9e9" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae kf" href="https://github.com/phdowling/skip-thought-keras" rel="noopener ugc nofollow" target="_blank">Skip-Thoughts Keras github</a></li></ul></div></div>    
</body>
</html>