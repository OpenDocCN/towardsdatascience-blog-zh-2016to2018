<html>
<head>
<title>Welcome to Deep Reinforcement Learning Part 1 : DQN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">欢迎来到深度强化学习第一部分:DQN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b?source=collection_archive---------0-----------------------#2017-10-20">https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b?source=collection_archive---------0-----------------------#2017-10-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/1981720f0b6fdd2f8c1482826ac9c946.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JBhnpbD_S4vu0y7mYqJ4oQ.jpeg"/></div></div></figure><p id="c2ee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">近年来，许多人工智能实验室正在致力于研究深度强化学习(DRL ),它有望成为未来的核心技术。我也在庆应义塾大学从事 DRL 研究。在接下来的几篇文章中，我将写下我对最近 DRL 的调查。所有这些都假设读者具备强化学习的基础知识。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="da8f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇文章中，我介绍了 DeepMind 提出的第一个深度强化学习方法——深度 Q 网络(DQN)。该论文于 2015 年在《自然》杂志上发表后，许多研究机构加入了这一领域，因为由于 DQN 使用的技术，深度神经网络可以让 RL 直接处理图像等高维状态。让我们看看 DQN 取得了多大的成就。</p><h1 id="8b90" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">街机学习环境</h1><p id="c881" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated"><a class="ae mg" href="https://arxiv.org/pdf/1207.4708.pdf" rel="noopener ugc nofollow" target="_blank">街机学习环境:总代理评估平台</a>于 2013 年发布，提出了 AI 的学习环境。ALE 有很多游戏最初是为一款古典游戏主机 Atari 2600 设计的。大概是 Pong，SpaceInvaders，PacMan，Breakout 这些游戏中的一部分大家都非常了解。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mh"><img src="../Images/1bb2ca038cb73fc199d1a7e65d3fe12e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eqITX5uG7SOPrFBDFBa79w.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Atari games available in ALE</figcaption></figure><p id="555e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个模拟器的任务是提供一个平台，在这个平台上，AI 可以玩很多游戏，而不需要任何特定的功能设计。在 DQN 出版之前，RL 代理需要手工设计的功能作为输入。比如 RL 代理玩太空入侵者的时候，入侵者位置是显式提取的。但是这些信息在玩突围的时候完全没用。因此，代理商不能依赖这种功能来玩 ALE 中的所有游戏。</p><h1 id="96d3" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">深度神经网络(DNN)</h1><p id="fe33" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated"><a class="ae mg" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> AlexNet </a>利用 DNN 在 ILSVRC 2012 图像分类竞赛中取得了不可思议的成绩。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mq"><img src="../Images/7866db40a9a87d5eba797aee4e2eab5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5kzs7LltWpqRS5eS_NDZjQ.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">AlexNet</figcaption></figure><p id="900d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">DNN 最伟大的事情是通过反向传播提取特征表示。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mr"><img src="../Images/10f42a8bbcae3a0a40cd39c28fb4a680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YMqID5wSSACPSTE4AHNZcw.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">learned weights of a convolutional layer in AlexNet</figcaption></figure><p id="3994" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于这种能力，分类器不再需要手工设计的特征。经过适当的多次反向传播后，DNN 知道颜色或形状等哪些信息对完成任务很重要。</p><h1 id="fa82" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">将 DNN 带入 RL</h1><p id="2d44" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">人们很自然地认为 DNN 使 RL 代理能够将图像与价值联系起来。然而事情并不容易。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ms"><img src="../Images/64c71e07346b1210b3e579bed97f2cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1O0TOXae9K3aeVEG2gQikg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Comparison between naive DQN and linear model (with DQN techniques) from Nature</figcaption></figure><p id="850d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">朴素 DQN 具有 3 个卷积层和 2 个全连接层，以直接从图像估计 Q 值。另一方面，线性模型只有一个完全连接的层，一些学习技术将在下一节讨论。两个模型都以 Q 学习方式学习 Q 值。如上表所示，朴素 DQN 的结果非常差，甚至比线性模型还要差，因为 DNN 很容易在在线强化学习中过度拟合。</p><h1 id="1362" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">深度 Q 网络</h1><p id="ad77" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">在两篇论文中介绍了 DQN，2013 年在 NIPS 上<a class="ae mg" href="https://arxiv.org/pdf/1312.5602.pdf" rel="noopener ugc nofollow" target="_blank">用深度强化学习玩雅达利</a>，2015 年在 Nature 上<a class="ae mg" href="http://www.davidqiu.com:8888/research/nature14236.pdf" rel="noopener ugc nofollow" target="_blank">通过深度强化学习进行人类级别的控制</a>。有趣的是，在 2013 年至 2015 年期间，关于 DRN 的论文很少。我猜原因是人们不能在没有自然版本信息的情况下复制 DQN 实现。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/1c59ac307c5e9a8318f7ffb9e5b04a55.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/0*35Fy2ZkW6zTJLGO-."/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">DQN agent playing Breakout</figcaption></figure><p id="512d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">DQN 主要通过四种技巧来克服不稳定学习。</p><ul class=""><li id="c11c" class="mu mv iq ka b kb kc kf kg kj mw kn mx kr my kv mz na nb nc bi translated"><strong class="ka ir">经历回放</strong></li><li id="2585" class="mu mv iq ka b kb nd kf ne kj nf kn ng kr nh kv mz na nb nc bi translated"><strong class="ka ir">目标网络</strong></li><li id="bd44" class="mu mv iq ka b kb nd kf ne kj nf kn ng kr nh kv mz na nb nc bi translated"><strong class="ka ir">剪裁奖励</strong></li><li id="d688" class="mu mv iq ka b kb nd kf ne kj nf kn ng kr nh kv mz na nb nc bi translated"><strong class="ka ir">跳过帧</strong></li></ul><p id="1434" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我逐一解释每种技术。</p><h1 id="c41e" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">体验回放</h1><p id="760f" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">经验重放最初是在 1993 年的<a class="ae mg" href="https://pdfs.semanticscholar.org/54c4/cf3a8168c1b70f91cf78a3dc98b671935492.pdf" rel="noopener ugc nofollow" target="_blank">使用神经网络的机器人强化学习</a>中提出的。DNN 很容易过度适应当前的剧集。一旦 DNN 过度满足，就很难产生各种体验。为了解决这个问题，经验重放存储了包括状态转换、奖励和行动在内的经验，这些经验是执行 Q 学习的必要数据，并进行小批量更新神经网络。这种技术有以下优点。</p><ul class=""><li id="0ee2" class="mu mv iq ka b kb kc kf kg kj mw kn mx kr my kv mz na nb nc bi translated">降低更新 DNN 的经验之间的相关性</li><li id="ed77" class="mu mv iq ka b kb nd kf ne kj nf kn ng kr nh kv mz na nb nc bi translated">通过小批量提高学习速度</li><li id="4963" class="mu mv iq ka b kb nd kf ne kj nf kn ng kr nh kv mz na nb nc bi translated">重用过去的转换以避免灾难性的遗忘</li></ul><h1 id="6728" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">目标网络</h1><p id="804f" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">在 TD 误差计算中，目标函数随 DNN 频繁变化。目标函数不稳定，训练困难。因此，目标网络技术确定目标函数的参数，并且每隔数千步用最新的网络替换它们。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/2f867995f1dd60997572c785b8140a77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gqg5g7PxlpHv35MchecWiA.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">target Q function in the red rectangular is fixed</figcaption></figure><h1 id="1ddd" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">剪裁奖励</h1><p id="06b6" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">每个游戏都有不同的分数尺度。例如，在乒乓球比赛中，玩家赢得比赛可以得到 1 分。否则，玩家得到-1 点。但在《太空入侵者》中，玩家击败入侵者获得 10~30 分。这种差异会使训练不稳定。因此，修剪奖励技术修剪分数，所有积极的奖励设置为+1，所有消极的奖励设置为-1。</p><h1 id="d66e" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">跳过帧</h1><p id="6db6" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">ALE 每秒能够渲染 60 幅图像。但实际上人们不会在一秒钟内采取这么多行动。AI 不需要计算每一帧的 Q 值。因此，跳帧技术是 DQN 每 4 帧计算 Q 值，并使用过去的 4 帧作为输入。这样降低了计算成本，积累了更多的经验。</p><h1 id="721e" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">表演</h1><p id="e025" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">所有上述技术使 DQN 能够实现稳定的训练。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/4bcac35ea49a0961c55818e4b4ecb61c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mo7vMyOHEj5TVr3mRjGVpg.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">DQN overwhelms naive DQN</figcaption></figure><p id="7967" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在 Nature 版本中，它显示了经验重放和目标网络对稳定性的贡献有多大。</p><figure class="mi mj mk ml gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/fbe74f55dbb1f93b904fb835637dffd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V81as-UDBknULyI9onPC0A.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Performance with and without Experience Replay and Target Network</figcaption></figure><p id="f7bb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">经验回放在 DQN 非常重要。目标网络也提高了其性能。</p><h1 id="2e90" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">结论</h1><p id="dbf1" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">DQN 已经用以上 4 种技术在很多雅达利游戏中实现了人类水平的控制。然而，仍然有一些游戏 DQN 不能玩。在这个系列中，我将介绍与它们斗争的论文。</p><p id="2ab7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我提供 DQN 的 TensorFlow 实现。</p><ul class=""><li id="4578" class="mu mv iq ka b kb kc kf kg kj mw kn mx kr my kv mz na nb nc bi translated">欢迎学习深度强化学习第二部分:张量流中的 DQN(即将推出)</li></ul></div></div>    
</body>
</html>