<html>
<head>
<title>fastText++: Batteries Included</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">fastText++包括电池</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fasttext-batteries-included-fa23f46d52e4?source=collection_archive---------11-----------------------#2018-08-25">https://towardsdatascience.com/fasttext-batteries-included-fa23f46d52e4?source=collection_archive---------11-----------------------#2018-08-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="689b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我的<a class="ae kl" href="https://medium.com/@nishansubedi/fasttext-under-the-hood-11efc57b2b3" rel="noopener">上一篇文章</a>中，我概述了 fastText 是如何实现的，并阐明了一些设计决策。fastText 的主要创新是用子词信息丰富了 word2vec 模型。然而，我们不需要局限于仅仅学习单词。fastText 可以对任何符号序列进行建模，只要序列的顺序对它有一定的意义。例如用户穿越一个站点的旅程。我将更详细地讨论这一点，并指出为此使用 fastText word2vec 的一些限制。最后，我将通过<a class="ae kl" href="https://arxiv.org/pdf/1607.01869.pdf" rel="noopener ugc nofollow" target="_blank">利用 word2vec 模型从语料库中提取语义结构</a>以及在<a class="ae kl" href="https://github.com/nishansubedi/fastText/releases/tag/v0.2.0" rel="noopener ugc nofollow" target="_blank"> fastText </a>中提供这些的实现，介绍一些有助于解决这些问题的巧妙研究。这目前在我的 fastText 分支中，但是如果我得到足够多的请求，我可能会合并到上游。我也很感激你对它的任何反馈。</p><p id="d4a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">fastText 主要是通过添加一包字符 n-grams 来扩展经典的 word2vec 模型。虽然这对于文本来说很好，其中单词的 n 元语法仍然有一些意义，但对于主要由 id 组成的语料库来说没有意义，因为这些单词的 n 元语法没有内在意义。</p><p id="4b81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Word2vec 通过理解单词出现的上下文来捕捉单词的意思。类似地，如果我们在一系列事件上训练 word2vec，比如用户在网站上的旅程，我们可以学习这些事件的潜在质量以及它们彼此之间的关系。通过对一系列事件(用户执行的搜索、他们点击的项目等)训练 word2vec 模型，我们有效地学习了如何使用户在一个会话中选择的项目彼此相似。这是雅虎<a class="ae kl" href="https://arxiv.org/pdf/1607.01869.pdf" rel="noopener ugc nofollow" target="_blank">可扩展语义匹配论文</a>的总体思路。用 word2vec 的话说，这些跨站点的用户交互(搜索查询、点击等..)可以解释为一句话。所以每次互动都变成了一个词。</p><p id="51a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个序列的 word2vec 公式允许我们在给定用户参与上下文(CBOW)或基于用户交互的上下文(Skip-gram)的情况下，对用户交互的概率进行建模。该模型可用于各种推荐、候选人选择以及搜索、广告和推荐问题的特征化。</p><p id="13d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以这种方式公式化序列有一些很好的特性:</p><ol class=""><li id="d4d4" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">它也允许我们在我们的词汇表中考虑单词时有很大的灵活性。它可以是搜索会话查询、商品 id、市场、位置。</li><li id="75e2" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">我们可以为多种类型的对象创建一个共享的嵌入空间，给我们一个可能难以定义的跨多种事物的关系(例如，一个查询与一个地理区域的相似性)。</li><li id="7963" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">我们可以利用当前问题空间之外的上下文(例如，利用搜索结果生成广告，或生成推荐)。</li></ol><p id="be0f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，传统的 word2vec 模型不允许我们表达这些丰富的语义关系。我们来看 word2vec 模型公式。跳格目标是最大化平均对数概率:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi la"><img src="../Images/51c6950616d22940ed7c47e68c159f76.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*UfI7imeSXui2x266"/></div><figcaption class="li lj gj gh gi lk ll bd b be z dk">where w^I and w^O are the input and output vector representations of w.</figcaption></figure><p id="6339" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用<a class="ae kl" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank">负采样损失(NEG) </a> (2.2)，概率可以被分解为上下文和目标词的正和负对的和，其中负采样被应用以获得负词。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/3b64281b7d005e60782feffc63b547d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/0*zbqaMTKAJxdyOgjL"/></div><figcaption class="li lj gj gh gi lk ll bd b be z dk">Where S is the set of negative samples drawn from the entire vocabulary</figcaption></figure><p id="c9ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个公式来自<a class="ae kl" href="http://www.jmlr.org/papers/volume13/gutmann12a/gutmann12a.pdf" rel="noopener ugc nofollow" target="_blank">噪声对比估计(NCE) </a>，它假定一个好的模型应该能够通过逻辑回归来区分数据和噪声。进一步简化，学习任务变成从噪声分布图<code class="fe ln lo lp lq b">w_s</code>中区分目标单词<code class="fe ln lo lp lq b">w_o</code>。</p><p id="136f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看一个例子。考虑以下用户旅程:</p><pre class="lb lc ld le gt lr lq ls lt aw lu bi"><span id="b4ea" class="lv lw iq lq b gy lx ly l lz ma">user u -&gt; search query 'watch' -&gt; saw items a, b, c -&gt; clicked c -&gt; purchased c</span></pre><p id="edc1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将用户的旅程建模为一系列动作，包括搜索查询和参与动作的 fastText 的输入将是:</p><pre class="lb lc ld le gt lr lq ls lt aw lu bi"><span id="edd3" class="lv lw iq lq b gy lx ly l lz ma">query_watch item_c item_c</span></pre><p id="ec00" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，让我们看看使用 word2vec 来模拟用户旅程的一些限制。我们将继续讨论这个例子。</p><h2 id="2379" class="lv lw iq bd mb mc md dn me mf mg dp mh jy mi mj mk kc ml mm mn kg mo mp mq mr bi translated">1.模拟隐含否定</h2><p id="9fe0" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">word2vec 模型不允许我们捕捉的一个重要关系是隐含的负面信号。这些对于模拟用户的旅程尤其重要。例如，我们知道，如果搜索结果中的一个项目被点击，它左边的项目最有可能被用户看到，因此不相关。正如我们在上面的 NEG 公式中看到的，word2vec 不允许我们显式地对负数建模，所有的负数都是从整个分布中随机抽取的。</p><p id="1a94" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这种形式中，改变损失函数的信号是简单的。对于引入的跳格损失，我们可以使用自定义负数扩展预测概率，如下所示:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/b39620bfee24bc702c51b0fdbeed7549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/0*nkjU1xEO1R8_FOXn"/></div><figcaption class="li lj gj gh gi lk ll bd b be z dk">Where C is the set of custom negatives for a particular positive token</figcaption></figure><p id="9444" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这只是 NEG 公式的扩展，其中学习任务是将目标单词<code class="fe ln lo lp lq b">w_o</code>与噪声分布的绘制<code class="fe ln lo lp lq b">w_s </code>以及所有隐含的否定<code class="fe ln lo lp lq b">w_c</code>区分开来。</p><p id="62a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了能够指定这一点，我们需要扩展 fastText 的词汇表，即它能够理解输入数据中的标记。fastText 的输入格式是一个文本文件，每个单词由一个空格字符分隔，每个句子由一个换行符分隔。对于监督学习，fastText 使用前缀<code class="fe ln lo lp lq b">__label__</code>作为标签来标记单词。前缀可以通过<code class="fe ln lo lp lq b">-label</code>标志定制。</p><p id="2ec8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae kl" href="https://github.com/nishansubedi/fastText/releases/tag/v0.2.0" rel="noopener ugc nofollow" target="_blank"> v0.2.0 </a>中，我扩展了这个词汇表以支持前缀<code class="fe ln lo lp lq b">__neg__</code>。所有带有前缀<code class="fe ln lo lp lq b">__neg__</code>的标记都是上下文特定的否定，可以添加到训练文件中。任何带有<code class="fe ln lo lp lq b">__neg__</code>前缀的单词都作为否定词添加到它前面的最后一个肯定性单词，并且是损失函数中集合<code class="fe ln lo lp lq b">C</code>的一部分。前缀可以通过<code class="fe ln lo lp lq b">-negPrefix</code>标志定制。标志<code class="fe ln lo lp lq b">-ignoreCNegs</code>可用于禁用自定义底片。如果禁用，这些令牌仍将被解析，但将在训练期间被丢弃。</p><p id="0669" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上述示例输入将是，带有隐含否定的:</p><pre class="lb lc ld le gt lr lq ls lt aw lu bi"><span id="544c" class="lv lw iq lq b gy lx ly l lz ma">query_watch item_c __neg__item_a __neg__item_b item_c</span></pre><h2 id="54d2" class="lv lw iq bd mb mc md dn me mf mg dp mh jy mi mj mk kc ml mm mn kg mo mp mq mr bi translated">2.应用全球背景</h2><p id="1eb3" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">在为用户的旅程建模时，很难找到正确的归属。可能有多种因素导致最终结果，例如，对于一个电子商务网站，可能有多个用户查看的项目(多个推荐、内容页面等)，以及他们采取的行动(点击多个项目、发出不同的搜索、浏览多个类别等)最终导致他们做出最终决定。对于多点触摸属性模型，我们希望与整个用户动作序列共享最终动作的意图。这可以被建模为在该特定会话中应用所有动作的全局上下文。将全球背景加入到我们迄今为止累积的损失中:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi my"><img src="../Images/9f4fec319633ee3500bf3ec177a20b0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zNJ78c2IknzvSlpT"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk">where G is the set of global contexts for that particular sequence</figcaption></figure><p id="c87b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://github.com/nishansubedi/fastText/releases/tag/v0.2.0" rel="noopener ugc nofollow" target="_blank"> v0.2.0 </a>词汇表也支持前缀<code class="fe ln lo lp lq b">__global__</code>。前缀为<code class="fe ln lo lp lq b">__global__</code>的所有标记都是全局上下文标记，并且是损失函数中集合<code class="fe ln lo lp lq b">G</code>的一部分。在模型训练期间，具有该前缀的单词被附加到该特定行的每个目标单词。前缀可以通过<code class="fe ln lo lp lq b">-globalPrefix</code>标志定制。标志<code class="fe ln lo lp lq b">-ignoreGContext</code>可用于禁用全局上下文令牌。如果禁用，这些令牌仍将被解析，但将在训练过程中被丢弃。</p><p id="544f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用用户和购买的项目作为全局上下文，上面的示例变成:</p><pre class="lb lc ld le gt lr lq ls lt aw lu bi"><span id="2936" class="lv lw iq lq b gy lx ly l lz ma">__global__user_u __global__item_c query_watch item_c __neg__item_a __neg__item_b item_c</span></pre><h2 id="e613" class="lv lw iq bd mb mc md dn me mf mg dp mh jy mi mj mk kc ml mm mn kg mo mp mq mr bi translated">3.编码重要性</h2><p id="6f80" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">基于模型被训练用于的特定任务，不同的动作可以具有不同的相对重要性。在电子商务中，添加到购物车是比点击更清楚的用户购买意图的例子。因此，为了模拟购买意图，能够赋予购物车添加比点击更重要的意义是很有用的。</p><p id="e8bf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae kl" href="https://github.com/nishansubedi/fastText/releases/tag/v0.2.0" rel="noopener ugc nofollow" target="_blank"> v0.2.0 </a>中，您可以为每个带有后缀<code class="fe ln lo lp lq b">:WEIGHT</code>的目标单词指定权重。权重的取值范围可以从<code class="fe ln lo lp lq b">1</code>到<code class="fe ln lo lp lq b">255</code>。如果指定权重，请确保您的学习率仍然合理，否则您可能会在训练过程中遇到爆炸梯度。通过平均权重缩小学习权重是一个很好的启发。</p><p id="d147" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在每个特定项目上的停留时间也可以表示为指定给给定标记的权重。加权模式ηt = log(1 + t)在可扩展语义论文中指定<a class="ae kl" href="https://arxiv.org/pdf/1607.01869.pdf" rel="noopener ugc nofollow" target="_blank">。我发现停留时间通常与购买率线性相关，直到某一点，所以设定最大值是一个好方法。这是使用<code class="fe ln lo lp lq b">-parseWeights</code>参数指定的。这样，如果我们给点击一个权重<code class="fe ln lo lp lq b">1</code>，购买一个权重<code class="fe ln lo lp lq b">25</code>，训练示例看起来像这样:</a></p><pre class="lb lc ld le gt lr lq ls lt aw lu bi"><span id="50dd" class="lv lw iq lq b gy lx ly l lz ma">__global__user_u __global__item_c query_watch item_c:1 __neg__item_a __neg__item_b item_c:25</span></pre><h2 id="cda8" class="lv lw iq bd mb mc md dn me mf mg dp mh jy mi mj mk kc ml mm mn kg mo mp mq mr bi translated">4.控制词汇量</h2><p id="9aaf" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">在同一个向量空间中嵌入多个对象真的很有用；它允许我们捕获不同实体之间的关系，比如特定查询与某个区域的关系。然而，这带来了一个困难。通常，语料库中的项目总数因实体而异，因此对不同的语料库使用相同的<a class="ae kl" href="https://medium.com/@nishansubedi/fasttext-under-the-hood-11efc57b2b3" rel="noopener">阈值策略</a>(读取数据)可能不会给我们带来理想的结果。例如，如果我们将区域的嵌入和查询一起编码，区域的数量与用户发出的搜索查询数量级不同。<a class="ae kl" href="https://github.com/nishansubedi/fastText/releases/tag/v0.2.0" rel="noopener ugc nofollow" target="_blank"> v0.2.0 </a>提供了一些不同的选项来帮助解决这个问题。</p><p id="5a56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用<code class="fe ln lo lp lq b">-maxVocabSize</code>参数可以指定控制词汇表大小的。这使得训练可以扩展到训练模型的机器的极限。</p><p id="14f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">符合与<code class="fe ln lo lp lq b">-minCountGlobal</code>分开的全局上下文令牌的令牌的最小计数。</p><p id="7f8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过使用<code class="fe ln lo lp lq b">__cc__</code>前缀识别特定令牌并通过<code class="fe ln lo lp lq b">-minCountCustom</code>标志控制计数，为任意令牌定制最小计数。</p><p id="b9ae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们遇到具有多个前缀的同一个令牌，那么这个特定令牌的类型由下面的优先顺序决定:<code class="fe ln lo lp lq b">customCountWord &gt; globalContext &gt; word</code>，也就是说，如果存在由<code class="fe ln lo lp lq b">-minCountCustom</code>指定的计数，那么它就是优先的，以此类推。</p><p id="1d94" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除了这些之外，fastText <a class="ae kl" href="https://github.com/nishansubedi/fastText/releases/tag/v0.2.0" rel="noopener ugc nofollow" target="_blank"> v0.2.0 </a>还有很多新增功能。下面是 v0.2.0 中跳过 gram 模型训练的选项列表，新标志以粗体显示。</p><pre class="lb lc ld le gt lr lq ls lt aw lu bi"><span id="4e01" class="lv lw iq lq b gy lx ly l lz ma">$ fasttext skipgram</span><span id="891a" class="lv lw iq lq b gy nd ly l lz ma">Empty input or output path.</span><span id="bc2d" class="lv lw iq lq b gy nd ly l lz ma">The following arguments are mandatory:</span><span id="fdac" class="lv lw iq lq b gy nd ly l lz ma">-input              training file path<br/>-output             output file path</span><span id="7874" class="lv lw iq lq b gy nd ly l lz ma">The following arguments are optional:</span><span id="e194" class="lv lw iq lq b gy nd ly l lz ma">-verbose            verbosity level [2]</span><span id="0ab6" class="lv lw iq lq b gy nd ly l lz ma">The following arguments for the dictionary are optional:</span><span id="743e" class="lv lw iq lq b gy nd ly l lz ma"><strong class="lq ir">-minCount           minimal number of word occurences [5]</strong></span><span id="f3db" class="lv lw iq lq b gy nd ly l lz ma"><strong class="lq ir">-minCountLabel      minimal number of label occurences [0]</strong></span><span id="b25f" class="lv lw iq lq b gy nd ly l lz ma"><strong class="lq ir">-minCountGlobal     minimal number of global context occurences [1]</strong></span><span id="36ae" class="lv lw iq lq b gy nd ly l lz ma"><strong class="lq ir">-minCountCustom     minimal number of custom token [5]</strong></span><span id="643f" class="lv lw iq lq b gy nd ly l lz ma">-wordNgrams         max length of word ngram [1]<br/>-bucket             number of buckets [2000000]<br/>-minn               min length of char ngram [3]<br/>-maxn               max length of char ngram [6]</span><span id="0be2" class="lv lw iq lq b gy nd ly l lz ma"><strong class="lq ir">-maxVocabSize       max tokens in vocabulary. Pruning happens at 0.75 of this: [30000000]</strong></span><span id="322b" class="lv lw iq lq b gy nd ly l lz ma">-t                  sampling threshold [0.0001]<br/>-label              labels prefix [__label__]</span><span id="66bf" class="lv lw iq lq b gy nd ly l lz ma"><strong class="lq ir">-negPrefix          negative token prefix [__neg__] negative tokens are associated with preceeding positive token</strong></span><span id="d7c6" class="lv lw iq lq b gy nd ly l lz ma"><strong class="lq ir">-globalPrefix       global context token prefix [__global__] global context is associated with all tokens in the line</strong></span><span id="3422" class="lv lw iq lq b gy nd ly l lz ma"><strong class="lq ir">-ccPrefix           token prefix for custom counts [__cc__]</strong></span><span id="75db" class="lv lw iq lq b gy nd ly l lz ma"><strong class="lq ir">-splitPrefix        prefix for tokens to split [__split__]  prefix stripped off when creating token</strong></span><span id="bc46" class="lv lw iq lq b gy nd ly l lz ma"><strong class="lq ir">-splitChar          char to split text on [_] these tokens are considered words and not ngrams. Using splits and ngrams together is not supported</strong></span><span id="e349" class="lv lw iq lq b gy nd ly l lz ma"><strong class="lq ir">-ignoreCNegs        ignore negative tokens. Negatives tokens have [__neg__] preceding them [false]</strong></span><span id="ea08" class="lv lw iq lq b gy nd ly l lz ma"><strong class="lq ir">-ignoreGContext     ignore global context tokens. Global cotext tokens have [__global__] preceding them [false]</strong></span><span id="675b" class="lv lw iq lq b gy nd ly l lz ma"><strong class="lq ir">-ignoreSplits       ignore split prefix. Only the original token is used, with prefix [__split__] stripped off [false]</strong></span><span id="d741" class="lv lw iq lq b gy nd ly l lz ma"><strong class="lq ir">-parseWeights       parse weights from word tokens, does not apply to neg, global or split. Weight is unsigned int in range [1-255], eg. word:3 [false]</strong></span><span id="6730" class="lv lw iq lq b gy nd ly l lz ma">The following arguments for training are optional:</span><span id="aa30" class="lv lw iq lq b gy nd ly l lz ma">-lr                 learning rate [0.05]<br/>-lrUpdateRate       change the rate of updates for the learning rate [100]<br/>-dim                size of word vectors [100]<br/>-ws                 size of the context window [5]<br/>-epoch              number of epochs [5]<br/>-neg                number of negatives sampled [5]<br/>-loss               loss function {ns, hs, softmax} [ns]<br/>-thread             number of threads [12]<br/>-pretrainedVectors  pretrained word vectors for supervised learning []<br/>-noSubsampling      disable subsampling [false]<br/>-saveOutput         whether output params and vector should be saved [false]</span><span id="fa5e" class="lv lw iq lq b gy nd ly l lz ma">The following arguments for quantization are optional:</span><span id="9eeb" class="lv lw iq lq b gy nd ly l lz ma">-cutoff             number of words and ngrams to retain [0]<br/>-retrain            whether embeddings are finetuned if a cutoff is applied [false]<br/>-qnorm              whether the norm is quantized separately [false]<br/>-qout               whether the classifier is quantized [false]<br/>-dsub               size of each sub-vector [2]</span></pre><h2 id="c5e1" class="lv lw iq bd mb mc md dn me mf mg dp mh jy mi mj mk kc ml mm mn kg mo mp mq mr bi translated">承认</h2><p id="9f5b" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">非常感谢<a class="ae kl" href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Xiaoting?q=Xiaoting%20Zhao" rel="noopener ugc nofollow" target="_blank">赵晓婷</a>的合作，以及对 v0.2.0 的帮助，<a class="ae kl" href="https://medium.com/@mihajlo.grbovic" rel="noopener"> Mihajlo Grbovic </a>对语义嵌入的澄清，<a class="ne nf ep" href="https://medium.com/u/9199bc6998f5?source=post_page-----fa23f46d52e4--------------------------------" rel="noopener" target="_blank">Giovanni Fernandez-kin cade</a>对本文的反馈。</p></div></div>    
</body>
</html>