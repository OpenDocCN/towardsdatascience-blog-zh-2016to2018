<html>
<head>
<title>Applied Deep Learning - Part 2: Real World Case Studies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">应用深度学习-第2部分:真实世界案例研究</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585?source=collection_archive---------0-----------------------#2017-09-01">https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585?source=collection_archive---------0-----------------------#2017-09-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="e950" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">概观</h1><p id="a0b1" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">欢迎来到应用深度学习系列的第2部分。第1部分是对人工神经网络的实际介绍，包括理论和应用，有很多代码示例和可视化。现在是最酷的部分，深度学习对真实世界数据集的端到端应用。我们将涵盖3个最常见的问题作为案例研究:二元分类，多类分类和回归。</p><ol class=""><li id="f78c" class="lk ll iq kn b ko lm ks ln kw lo la lp le lq li lr ls lt lu bi translated"><a class="ae lj" href="#581e" rel="noopener ugc nofollow">案例分析:二元分类<br/> </a> 1.1)数据可视化&amp;预处理<br/> 1.2) Logistic回归模型<br/> 1.3) ANN模型<br/> 1.4)深层ANN可视化</li><li id="a11c" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li lr ls lt lu bi translated"><a class="ae lj" href="#3b7d" rel="noopener ugc nofollow">案例分析:多类分类<br/> </a> 2.1)数据可视化&amp;预处理<br/> 2.2) Softmax回归模型<br/> 2.3) ANN模型<br/> 2.4)交叉验证</li><li id="ed8e" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li lr ls lt lu bi translated"><a class="ae lj" href="#cf02" rel="noopener ugc nofollow">案例分析:回归<br/> </a> 3.1)数据可视化&amp;预处理<br/> 3.2)线性回归模型<br/> 3.3) ANN模型</li><li id="dd99" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li lr ls lt lu bi translated"><a class="ae lj" href="#af92" rel="noopener ugc nofollow">结论</a></li></ol><p id="751e" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">这篇文章的代码可以从<a class="ae lj" href="https://github.com/ardendertat/Applied-Deep-Learning-with-Keras/blob/master/notebooks/Part%202%20-%20Case%20Studies.ipynb" rel="noopener ugc nofollow" target="_blank">这里得到</a>作为一个Jupyter笔记本，你可以随意下载并亲自试用。</p><h1 id="51e8" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">1.案例研究:二元分类</h1><p id="c59b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们将在Kaggle上使用人力资源分析数据集。我们试图根据各种特征来预测员工是否会离开，例如他们参与的项目数量、在公司呆的时间、上次绩效评估、工资等。数据集大约有15，000行和9列。我们试图预测的列称为“左”。这是一个具有0/1值的二进制列。标签1表示该员工已经离职。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/2eac799ebf6ee9f6a2f9ff1c892cbbf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cg6bLhWp6LVbjDnW_Ol0tQ.png"/></div></div></figure><h2 id="e676" class="mp jo iq bd jp mq mr dn jt ms mt dp jx kw mu mv kb la mw mx kf le my mz kj na bi translated">1.1)数据可视化和预处理</h2><p id="0571" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">首先，让我们在直接构建模型之前执行一些数据可视化和预处理。这一部分是至关重要的，因为我们需要知道我们正在处理什么类型的特性。对于每个ML任务，我们至少需要回答以下问题:</p><ul class=""><li id="f95f" class="lk ll iq kn b ko lm ks ln kw lo la lp le lq li nb ls lt lu bi translated">我们有什么类型的特征:实值的，分类的，还是两者都有？</li><li id="b438" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li nb ls lt lu bi translated">是否有任何功能需要规范化？</li><li id="58d4" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li nb ls lt lu bi translated">我们有空值吗？</li><li id="96a6" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li nb ls lt lu bi translated">标签分布是怎样的，阶层不平衡吗？</li><li id="9a9c" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li nb ls lt lu bi translated">特征之间有关联吗？</li></ul><p id="ffbf" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">朱庇特笔记本包含了详细的分析。综上所述，既有真实特征，也有分类特征。没有空值，但有些要素需要规范化。76%的例子被标为0，意味着员工没有离开。</p><p id="71fe" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">让我们检查特征与标签的相关性(名为“left”的列)。我们将使用<em class="nc"> seaborn </em>包来绘制相关图。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nf"><img src="../Images/34e2211dc889e7fdfe8a946bfd7ae985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HT9SLHAtM1Niun4mjP71UA@2x.png"/></div></div></figure><p id="7e1d" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">在该图中，正值表示与标签的相关性，负值表示与标签的反向相关性。当然“左”和它本身有很好的相关性，你可以忽略它。除此之外，只有一个特征有很强的信号，那就是“满意度”,与员工是否已经离职成反比。这很有道理。</p><p id="b21a" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">现在，让我们来看看所有特性之间的成对相关性。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ng"><img src="../Images/692cf4d4c3ee9c13b4151bc4c9d8ab0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iy3c6AHEzHUzA04048yHCw@2x.png"/></div></div></figure><p id="5caa" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">我们看到“平均_月_小时”与“数字_项目”正相关，这也是有道理的。一个人参与的项目越多，需要投入的工作时间就越多。</p><p id="3eba" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">现在我们来看看特征值的分布。通过检查特征的直方图，我们可以看到哪些特征需要归一化。这背后的动机是什么？规范化是什么意思，为什么需要规范化？如果实值特征被缩放到预定义的范围内，例如[0，1]，大多数ML算法执行得更好。这对深度神经网络尤为重要。如果输入特征由大值组成，深度网络真的很难学习。原因在于，随着数据流经各层，伴随着所有的乘法和加法运算，数据会很快变大，这会使非线性饱和，从而对优化过程产生负面影响。我们将在另一篇文章中看到这方面的详细演示，因为现在我们需要注意特征值是小数字。</p><p id="73fb" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">查看特征直方图，我们需要对其中的3个特征进行归一化:average_monthly_hours、number_project和time_spend_company。所有其他特征都在[0，1]内，所以我们可以不去管它们。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nh"><img src="../Images/5ab5d7a51e30af8b2af20f6cf40924d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yiI2yYFKoukhqNCImrgCfA@2x.png"/></div></div></figure><p id="2931" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">Scikit-learn有几种归一化方法，我们将使用的是<em class="nc">标准缩放器</em>。它单独缩放要素，使其具有零均值和单位方差，因此它们都属于标准的<em class="nc">正态(0，1) </em>分布。请注意，这不会改变特征值的顺序，只是改变了比例。这是一个简单却极其重要的技巧。</p><p id="712b" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">我们加载的数据在一个<em class="nc">熊猫数据帧中。Pandas是一个非常流行的处理表格数据的软件包，尤其是在像jupyter笔记本这样的交互式环境中。DataFrame是pandas最常用的数据结构，它充当了我们数据的容器，并公开了几个内置函数，使我们的生活更加轻松(查看笔记本了解更多详细信息)。在下面的代码片段中，df <em class="nc"> </em>是我们数据的数据帧。</em></p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="23bf" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">Scikit-learn API设计得非常好，包含4个非常常用的方法。预测器是像逻辑回归这样的ML模型，转换器是像标准定标器这样的数据操纵器。</p><ul class=""><li id="3072" class="lk ll iq kn b ko lm ks ln kw lo la lp le lq li nb ls lt lu bi translated"><em class="nc"> fit </em> : For预测器对给定输入进行训练。For transformers计算统计数据，如稍后要使用的输入的平均值和标准偏差。</li><li id="fc36" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li nb ls lt lu bi translated"><em class="nc">transform</em>:For transformers使用fit函数学习到的统计数据处理输入数据。我们在fit之后运行transform方法，因为存在依赖关系。预测者不支持这种方法。</li><li id="53cf" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li nb ls lt lu bi translated"><em class="nc"> fit_transform </em>:在一次调用中高效地执行fit + transform。对于转换器，计算输入的统计信息并执行转换。这是变压器常用的方法。对于预测器，训练模型并对给定输入执行预测。</li><li id="5537" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li nb ls lt lu bi translated"><em class="nc">预测:</em>顾名思义，for predictors使用用fit方法训练的模型来执行预测任务。非常常用于预测。变形金刚不支持这种方法。</li></ul><p id="bde5" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">既然我们已经将实值特征调整到了理想的范围内，那么让我们来处理分类特征。我们需要将分类数据转换成<em class="nc">独热</em>表示。例如，薪金列包含3个唯一的字符串值:低、中、高。一键转换后，我们将有3个新的二进制列:薪水_低，薪水_中和薪水_高。对于给定的示例，它们中只有一个具有值1，其他的都是0。然后我们将删除原来的薪水栏，因为我们不再需要它了。</p><p id="1a31" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">一键转换由熊猫的<em class="nc"> get_dummies </em>执行。我们也可以在scikit-learn中使用<em class="nc">onehotencode</em>，它们都可以完成工作。因为我们的数据已经在熊猫的数据框架中，所以得到虚拟模型更容易。它还自动执行特征的重命名。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/6106d9e4527bcf3169a9c9ce5cc071ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*FMDW4HBu3ilcYbCgNxUhmQ@2x.png"/></div></figure><p id="cad0" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">现在是创建训练和测试数据的最后一部分。该模型将在训练集上执行学习，并在保留的测试集上进行评估。Scikit-learn有一个方便的<em class="nc"> train_test_split </em>函数。我们只需要指定测试集的一部分，在我们的例子中是30%。但是首先我们使用数据帧的<em class="nc"> values </em>属性将数据从pandas数据帧转换为numpy数组。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><h2 id="0f3b" class="mp jo iq bd jp mq mr dn jt ms mt dp jx kw mu mv kb la mw mx kf le my mz kj na bi translated">1.2)逻辑回归模型</h2><p id="4b2a" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">既然我们已经完成了数据预处理和训练/测试集生成，那么有趣的部分来了，训练模型。我们首先从一个简单的模型开始，逻辑回归(LR)。然后，我们将训练深度人工神经网络，并将结果与LR进行比较。</p><p id="2189" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">看完第一篇文章，构建模型应该很熟悉了。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="58ea" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">我们得到了79%的训练准确率。这实际上很糟糕，因为上面我们看到76%的标签是0。所以不管输入是什么，最简单的分类器总是输出0，它能得到76%的准确率，我们也没有比这更好的了。这意味着我们的数据不是线性可分的，就像我们在第一篇文章中看到的例子一样，我们需要一个更复杂的模型。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nj"><img src="../Images/d466bb894f1579af5a308f5cf4ec846c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JibpR-U7ubytFsX_Kru-2A@2x.png"/></div></div></figure><p id="a6c9" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">上图描述了训练损失和准确性。但更重要的是，我们对测试集的指标感兴趣。训练集中的度量标准可能会产生误导，因为模型已经在其上进行了训练，我们希望检查模型在保留的测试集上的表现。测试准确率为78%，略低于训练准确率。ML模型的测试精度几乎总是低于训练精度，因为在训练过程中测试数据对模型是不可见的。查看分类报告，我们看到属于类别1的示例中只有60%被正确分类。相当糟糕的表现。混淆矩阵显示了许多错误分类的例子，看起来也不乐观。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nk"><img src="../Images/18ae0fd1cfbddf2bb8071ee651ba41bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zsxILJqJbpAe-Pzp3Ud-Hg@2x.png"/></div></div></figure><h2 id="9c7e" class="mp jo iq bd jp mq mr dn jt ms mt dp jx kw mu mv kb la mw mx kf le my mz kj na bi translated">1.3)人工神经网络模型</h2><p id="f72c" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在让我们建立一个深度神经网络进行二分类。这个模型将更加强大，能够模拟非线性关系。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="3c98" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">模型构建过程也是非常熟悉的。我们有2个64和16节点的隐藏层，带有双曲正切函数。输出层使用sigmoid激活，因为这是一个二元分类问题。我们使用Adam优化器，学习率设置为0.01。</p><p id="e84d" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">这次我们达到了97.7%的训练准确率，相当不错。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nj"><img src="../Images/6c62b8a39d2fb10ebaae206add57f12a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RTsSpETDVjdhK5JUxrIQoQ@2x.png"/></div></div></figure><p id="74ba" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">让我们比较一下LR和ANN模型。人工神经网络模型更优越，具有更低的损耗和更高的精度。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nl"><img src="../Images/4a80584c3ad663ed92032edab148fe4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BTAKM5PVbdkCJdbNsdEvqA@2x.png"/></div></div></figure><p id="c8ab" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">为了完整起见，这是测试集上人工神经网络模型的分类报告和混淆矩阵。与LR模型的78%相比，我们实现了97%的准确率。我们仍然对4500个例子中的147个进行了错误分类。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nm"><img src="../Images/b7514e0f1669378912b9a05fe6b18348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oHm6r0ieSNQcsIe1exXZ6g@2x.png"/></div></div></figure><p id="a5b2" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">我们可以通过以下方法进一步提高人工神经网络的性能:</p><ul class=""><li id="9b28" class="lk ll iq kn b ko lm ks ln kw lo la lp le lq li nb ls lt lu bi translated">为模型定型更长时间(增加历元数)。</li><li id="d649" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li nb ls lt lu bi translated">超参数调优:改变学习率，使用不同于Adam的优化器(比如RMSprop)，使用不同于tanh的另一个激活函数(可以是relu)。</li><li id="58f4" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li nb ls lt lu bi translated">增加每层的节点数量:我们可以增加到128–64–1，而不是64–16–1。</li><li id="eb21" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li nb ls lt lu bi translated">增加层数:我们可以做128–64–32–16–1层。</li></ul><p id="f580" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">不过，一个重要的警告是，随着我们使模型更强大，训练损失可能会减少，准确性会增加。但是我们会遇到过度拟合的风险。这意味着与简单模型相比，复杂模型在测试集上的表现更差，即使复杂模型的训练指标更好。我们将在另一篇文章中更多地讨论过度拟合，但记住这一点非常重要。这就是为什么我们不会为层数和每层节点数而疯狂。完成工作的最简单的模型就足够了。</p><h2 id="2073" class="mp jo iq bd jp mq mr dn jt ms mt dp jx kw mu mv kb la mw mx kf le my mz kj na bi translated">1.4)深层人工神经网络的可视化</h2><p id="2e5b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在上一篇文章中，我们了解到人工神经网络的每一层都执行输入从一个向量空间到另一个向量空间的非线性转换。通过这样做，我们将输入数据投射到一个新的空间，在这个空间中，类可以通过一个复杂的决策边界相互分离。</p><p id="3d03" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">让我们直观地演示一下。我们在上面做的初始数据预处理之后的输入数据是20维的。为了形象化，让我们把它投射到2D。请记住，一个层中有k个节点意味着该层转换其输入，从而输出是一个k维向量。我们上面训练的ANN有两个隐藏层，分别是64和16个节点。然后，我们需要一个有两个节点的新层，以便将我们的数据投影到2D空间。因此，我们在输出节点之前添加了这个新层。其余的完全没动过。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="8448" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">这是我们的输入数据从20D到2D空间的投影结果。决策边界对应于ANN的最后一层。人工神经网络能够很好地区分这些类别，尽管有一些分类错误。在2D，许多数据点重叠，因此我们无法看到所有数据点，作为参考，该模型在4500个数据点中错误分类了约160个点(96%的准确率)。我们不关心这个模型的准确性，我们感兴趣的是高维输入到2D的投影。这是一个巧妙的小技巧，直观地展示了人工神经网络执行的投影的结果。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nn"><img src="../Images/b3703591159baa4e930b37db681732b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qxOu_WMksF7iAZTKJ0QJ1w@2x.png"/></div></div></figure><p id="7268" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">一种更有原则的可视化方法是使用<em class="nc"> t-SNE </em>，这是一种用于可视化高维数据的降维技术。详情请点击<a class="ae lj" href="https://lvdmaaten.github.io/tsne/" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="3b7d" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">2.案例研究:多类分类</h1><p id="6687" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们现在将在著名的<a class="ae lj" href="https://archive.ics.uci.edu/ml/datasets/iris" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a>上执行多类分类。它包含3类花，每类50个例子。总共有4个功能。所以它很小，但是很受欢迎。数据如下所示。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi no"><img src="../Images/013a94774f68ed7e0e7bd12adda49f8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rj5oEd4ak6t3nNqMj1UJbQ@2x.png"/></div></div></figure><h2 id="98fc" class="mp jo iq bd jp mq mr dn jt ms mt dp jx kw mu mv kb la mw mx kf le my mz kj na bi translated">2.1)数据可视化和预处理</h2><p id="9af4" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这部分现在更容易了，因为我们只有4个实值特征。同样，我们将特征归一化到[0，1]之间。特征值很小，我们可以不进行任何规范化，但是这样做是一个好习惯，而且这样做也没有坏处。</p><p id="3520" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">如果我们有少量的特征，Pairplot 是一个很酷的可视化技术。我们可以看到按类(数据集中的“标签”列)着色的要素的成对分布。我们使用<em class="nc"> seaborn </em>包，对于一个信息丰富的情节来说非常简单。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi np"><img src="../Images/6fa7006ba4d200a21eb999dd0bf649fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d_OLKiExayH3ZvDe4IKScQ@2x.png"/></div></div></figure><p id="8edf" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">我们可以看到“setosa”类很容易分开，但“versicolor”和“virginica”更容易混合。</p><h2 id="bfb8" class="mp jo iq bd jp mq mr dn jt ms mt dp jx kw mu mv kb la mw mx kf le my mz kj na bi translated">2.2) Softmax回归模型</h2><p id="29fa" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们现在将训练Softmax回归(SR)模型来预测标签。前一篇文章包含了详细的解释，构建模型也与上面非常相似。主要区别在于，我们使用<em class="nc"> softmax </em>激活和<em class="nc">分类_交叉熵</em>作为损失。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="c48c" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">SR模型已经达到了97%的训练准确率和最小的损失，已经很不错了。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nj"><img src="../Images/93345ccf2e9181c9b2d2a954ab73d741.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-wSv9B8e8PzAgVD5SjJ24A@2x.png"/></div></div></figure><h2 id="74f3" class="mp jo iq bd jp mq mr dn jt ms mt dp jx kw mu mv kb la mw mx kf le my mz kj na bi translated">2.3)人工神经网络模型</h2><p id="aaeb" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在让我们建立我们的人工神经网络模型。我们添加2个隐藏层，分别有32个和16个节点。注意我们也改变了这些层的激活函数为<em class="nc"> relu </em>而不是tanh。我们将在另一个教程中探讨各种激活函数及其差异，但relu可以说是最受欢迎的一个。那为什么我们没有使用它？嗯，只是因为tanh激活后决策边界图看起来更漂亮。说真的，没别的原因。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="8ad6" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">这次我们获得了100%的训练准确率。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nj"><img src="../Images/576ebf0df7d9bc767cdf132077968e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nNOTfolv_DYzXH8G6MReQA@2x.png"/></div></div></figure><p id="14b4" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">另外，随机共振和人工神经网络模型的测试准确率都是100%。这是一个非常小的数据集，所以这些结果并不奇怪。不是所有的问题都需要深度神经网络才能得到好的结果。对于这个问题来说，这可能有点过了，因为线性模型也能很好地工作。</p><h2 id="5f39" class="mp jo iq bd jp mq mr dn jt ms mt dp jx kw mu mv kb la mw mx kf le my mz kj na bi translated">2.4)交叉验证</h2><p id="6270" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">对于像我们目前这样的小样本情况，进行交叉验证以获得更好的准确性估计尤为重要。通过<em class="nc"> k折交叉验证</em>，我们将数据集分成k个不相交的部分，使用k-1个部分进行训练，另一个部分进行测试。这样，每个例子都会出现在训练集和测试集中。然后，我们对所有k次运行中的模型性能进行平均，并获得模型准确性的更好的低方差估计。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nq"><img src="../Images/f8f067cf540e7927bd298f25a3099b3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z6NOTHj9rllT_SghGxA3rg@2x.png"/></div></div></figure><p id="7f1d" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">通常在训练深度学习模型时，我们不会执行k-fold交叉验证。因为训练需要很长时间，而且从头开始训练模型k次是不可行的。但由于我们的数据集很小，所以它是一个很好的候选对象。</p><p id="d47e" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">这是两个模型的5倍交叉验证准确度的曲线图。深度模型表现稍好，具有更高的准确性和更低的方差。在图中，精度有时似乎超过100%，但这是平滑曲线的人工产物。我们得到的最大准确度是100%。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nr"><img src="../Images/6dfd08900330e1b27ebf3c1a57b67353.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ptJyVjZ-OoLf4cIxXCIBVg@2x.png"/></div></div></figure><h1 id="cf02" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">3.案例研究:回归</h1><p id="a979" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们现在将研究一个回归问题，预测一个实值输出而不是离散的类成员。我们将使用华盛顿州卡格尔市金县的<a class="ae lj" href="https://www.kaggle.com/harlfoxem/housesalesprediction" rel="noopener ugc nofollow" target="_blank">房屋销售数据集</a>。大约有21，000行20个特征。我们试图预测的值是一个标记为“价格”的浮点数。</p><h2 id="335d" class="mp jo iq bd jp mq mr dn jt ms mt dp jx kw mu mv kb la mw mx kf le my mz kj na bi translated">3.1)数据可视化和预处理</h2><p id="09fe" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">首先让我们看看特性分布</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ns"><img src="../Images/d966fae89a2a2d76997efee2c1dedd50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OUgF1Y-xgTWmP66fmtwpeA@2x.png"/></div></div></figure><p id="0e95" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">现在你知道该怎么做了，我们需要做特征标准化和分类。例如，与squarefoot相关的要素肯定需要进行规范化，因为值的范围以千计，并且像zipcode这样的要素需要进行分类。</p><p id="767f" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">我们还需要做一种新的预处理，<em class="nc"> bucketization </em>。例如，包含房屋建造年份(yr _ built)的要素的范围从1900年到2015年。我们当然可以把它分类，每一年都属于一个不同的类别，但这样的话，它会相当稀疏。如果我们在不丢失太多信息的情况下取消这个特性，我们会得到更多的信号。例如，如果我们使用10年时段，则[1950，1959]之间的年份将折叠在一起。知道这栋房子建于20世纪50年代而不是1958年就足够了。</p><p id="8b8c" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">受益于分桶的其他特征是房屋的纬度和经度。确切的坐标没那么重要，我们可以把坐标四舍五入到最近的公里。这样，特征值将更加密集和信息丰富。在存储桶化中使用哪个范围没有硬性规定，它们主要是通过反复试验来决定的。</p><p id="d184" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">我们需要做的最后一个转换是关于房子的价格，我们试图预测的价值。目前，它的价值在7.5万美元到770万美元之间。试图在如此大的规模和变化范围内进行预测的模型将非常不稳定。所以我们也把它标准化了。有关详细信息，请随时查看代码。</p><p id="d5ef" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">在所有的转换之后，我们从20个特性增加到165个。让我们检查一下每个特性与价格的相关性。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nt"><img src="../Images/8ba2ae656271db6fc3da74ca841c4e60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*YchUnn2IQBMT850ElvCLTQ@2x.png"/></div></div></figure><p id="68b5" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">最相关的特征是平方英尺，这是意料之中的，越大的房子通常越贵。看看这个列表，这些特性是有意义的。一些邮政编码与价格高度相关，例如98039对应于麦地那，那是比尔盖茨居住的地方，也是美国最昂贵的社区之一。还有另一个邮政编码98004，它与贝尔维尤更相关。那里有许多高层建筑和技术办公室，这使得价格最近大幅上涨。我以前住在那个街区，但后来它变得太无聊和昂贵，所以我搬家了:)</p><h2 id="b8fc" class="mp jo iq bd jp mq mr dn jt ms mt dp jx kw mu mv kb la mw mx kf le my mz kj na bi translated">3.2)线性回归模型</h2><p id="09a3" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这是我们第一次建立回归模型。就像逻辑回归是我们在分类问题中首先尝试的最简单的模型一样，线性回归是我们在回归问题中开始的模型。</p><p id="f9e4" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">记住逻辑回归的方程式是<em class="nc"> y=f(xW) </em>其中<em class="nc"> f </em>是sigmoid函数。线性回归简单地说就是<em class="nc"> y=xW，</em>没有激活函数。为了简单起见，我再次省略了偏差项。随着偏差的增加，它们分别变为<em class="nc"> y=f(xW+b) </em>和<em class="nc"> y=xW+b </em>。</p><p id="0ae5" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">这里提醒一下我们如何建立逻辑回归(LR)模型</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="7077" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">这是线性回归模型(LinR)的代码</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="2e90" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">有3个主要区别:</p><ul class=""><li id="c090" class="lk ll iq kn b ko lm ks ln kw lo la lp le lq li nb ls lt lu bi translated">LR使用sigmoid激活函数，而LinR没有激活。</li><li id="7122" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li nb ls lt lu bi translated">LR使用binary_crossentropy损失函数，LinR使用mean_squared_error。</li><li id="671c" class="lk ll iq kn b ko lv ks lw kw lx la ly le lz li nb ls lt lu bi translated">LR也报告精度，但是精度不是回归问题的适用度量，因为输出是浮点数而不是类成员。</li></ul><p id="c6ff" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">最重要的变化是损失函数<em class="nc">均方误差</em> (MSE)。MSE是用于回归的标准损失函数。公式很简单:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nu"><img src="../Images/befeacfde5562262952a181c1960f69b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*5B-_uIK9ULfzx5sV-aBmiA@2x.png"/></div></div></figure><p id="a11d" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">其中<em class="nc"> y </em>为真值，<em class="nc"> ŷ </em>为预测值，<em class="nc"> n </em>为样本数。</p><p id="b28f" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">我们还向fit函数传递了一个新的<em class="nc"> validation_split </em>参数。它指定了在训练过程中用作保留验证集的训练数据部分，在我们的例子中是20%。使用验证集，我们可以看到我们是否在训练中过度适应。但是不要混淆验证集和测试集。测试集是完全独立的，在训练过程中根本不会暴露给模型。</p><p id="d050" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">损失在最初几个时期减少，然后稳定下来。我们可能<em class="nc">配置不足</em>意味着我们的型号没有足够的容量，我们需要一个更复杂的型号。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nj"><img src="../Images/fb7e9c668d00898b63f9db2b59747a62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iD2T2pCPjSIjEtFXJ2JFlw@2x.png"/></div></div></figure><p id="14bf" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">这些是权重最高的前10个特征。分类邮政编码特征占主导地位。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/bd2ca825dcb00676b21bfac42bc6352f.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*7ysMgg1Ccz1ItTCNMabpIg@2x.png"/></div></figure><h2 id="58d0" class="mp jo iq bd jp mq mr dn jt ms mt dp jx kw mu mv kb la mw mx kf le my mz kj na bi translated">3.3)人工神经网络模型</h2><p id="6b4f" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">最后，让我们为回归建立一个人工神经网络。在前面的例子中，从线性模型到深度模型只涉及添加具有非线性激活函数的新层。这次也一样。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="cf8e" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">我们添加了relu激活的新层。损失图现在看起来很有趣。训练误差损失似乎仍在减少，但是验证误差在第5个时期之后开始增加。我们显然<em class="nc">过度适应</em>。人工神经网络正在记忆训练数据，这降低了它对验证集进行归纳的能力。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nw"><img src="../Images/82ccd29322a59fee266bf0007ca4d2c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BGtqvQS1M4OfYfURuPIavg@2x.png"/></div></div></figure><p id="a46c" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">解决办法？有几种方法可以解决深度神经网络中的过拟合问题。大多数方法依赖于约束模型的容量。这可以通过例如限制重量、分享重量或者甚至在训练损耗达到稳定水平之前停止训练来实现。为了简洁起见，我们将使用最后一个，其余的将在另一篇文章中讨论。这意味着一旦确认损失停止改善，我们将简单地停止训练。这叫做<em class="nc">提前停止</em>，用Keras很容易实现。我们只需要修改对fit函数的调用，如下所示。如果两个时期的确认损失没有改善，我们停止训练。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="b756" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">验证损失在第5个时期停止改善，模型再等待2个时期，并在第7个时期完成训练。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nx"><img src="../Images/e36c7b82a5f1f01165ba7c66df015f7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZovHeahUK6PrV9CZewmMFA@2x.png"/></div></div></figure><p id="00d4" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">LinR模型的训练损失是0.158，ANN的损失是0.086。我们比线性模型提高了83%，相当不错。并且比较在测试集上的损失，LinR模型得到0.191，相比于ANN的0.127，提高了32%。下图比较了LinR与ANN的训练损失。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nl"><img src="../Images/3280276318e8286163af2596b68a302c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S2BgOl-ms49TcN1USJPx2w@2x.png"/></div></div></figure><p id="d6f3" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">让我们做一个最后的比较，模型预测价格和实际价格之间的美元价值差异。最天真的模型总是预测训练集的平均价格(540，000美元)，在测试集上相差229，000美元，非常糟糕。线性回归模型的误差为87K，而深度人工神经网络的误差为68K。人工神经网络比线性神经网络好21%。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/4d2d108553e3bee212ff94ee37573682.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*7aSTu0aBLlTJpE4BkydjlQ@2x.png"/></div></figure><h1 id="af92" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">4)结论</h1><p id="dbb1" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我希望这是一篇信息丰富的文章。我试图用现实生活中的数据集演示深度学习在3个常见机器学习问题上的一步一步的应用。</p><p id="d9e5" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">详细讨论了数据预处理和可视化。尽管它们不是解决一个ML问题的有趣部分，并且经常被忽视，但是它们是极其重要的。就像第1部分一样，我们首先用一个简单的模型解决问题，然后使用深度神经网络来获得更好的结果。</p><p id="4541" class="pw-post-body-paragraph kl km iq kn b ko lm kq kr ks ln ku kv kw ma ky kz la mb lc ld le mc lg lh li ij bi translated">如果你想自己动手，这篇文章的全部代码可以在这里找到。如果你有任何反馈，请随时通过<a class="ae lj" href="https://twitter.com/ardendertat" rel="noopener ugc nofollow" target="_blank">推特</a>联系我。</p></div></div>    
</body>
</html>