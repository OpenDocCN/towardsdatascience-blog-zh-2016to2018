# 变分自动编码器的解惑:综述

> 原文：<https://towardsdatascience.com/disentanglement-with-variational-autoencoder-a-review-653a891b69bd?source=collection_archive---------5----------------------->

可解释的因子分解表示的学习在机器学习中已经存在了很长时间。但是随着最近像变分自动编码器(VAE)这样的深度生成模型的进步，对学习这种解开的表示的兴趣出现了爆炸。由于任何生成模型的目标本质上都是捕捉潜在的数据生成因素，因此**解开的表示将意味着单个潜在单元对单个生成因素的变化敏感。**

![](img/457e6cb560b5af06a7655272c6d496c2.png)

由于香草 VAE 鼓励生成因子上的后验分布 *q(z|x)* 更接近各向同性高斯 *N(0，I)* ，它促进了潜在生成因子的解开。这是因为各向同性高斯的协方差∑等于单位矩阵 I，这意味着所有维度都是独立的。在 ELBO 中，这是由第二个术语推动的:

![](img/1bd8e032becace98a7ff012112c72df7.png)

然而，有效解纠缠所需的学习压力可能还不够，因为在 VAE，我们还想对输入信号进行适当的自动编码(或重建)，而重建损失(第一项)与第二项相比可能太大。受此启发，[[β-VAE](https://openreview.net/pdf?id=Sy2fzU9gl)通过赋予第二项β > 1 权重，对潜在瓶颈产生了更强的约束。因此，他们的目标函数是这样的:

![](img/555031732581fe491fe0420af954c768.png)

由于增加了第二项的权重，重建精度开始下降。这给许多研究者带来了重要的研究问题:“**如何在不损失重构能力的情况下实现更好的解纠缠？**“ELBO 的[ [手术极大地帮助了寻找这个答案的道路，其中第二个术语被分解为:](http://approximateinference.org/accepted/HoffmanJohnson2016.pdf)

![](img/839d502c297489d098adc8684e25ab91.png)

这里，第一项是索引码互信息(MI ),第二项是对先验的边际 KL。这种分解提供了一种观点，即实际上第二项对学习解纠缠表示更重要，惩罚 MI(比常规 ELBO 更多)可能是重建不佳的原因。此外，[ [InfoGAN](https://arxiv.org/pdf/1606.03657.pdf) ](不是基于 VAE 的模型)最大化了相同的 MI 以实现更好的解缠。

有了这个理论基础，本文[ [link](https://arxiv.org/pdf/1711.00848.pdf) ]在正则 ELBO 上增加了(-1) *λ* 加权 *KL(q(z)||p(z))* 。但是，由于 *KL(q(z)||p(z))* 已经存在于 ELBO 中，它们实际上是最小化( *λ* + 1)加权 *KL(q(z)||p(z))* 以鼓励解纠缠。注意[ [adversarialAE](https://arxiv.org/pdf/1511.05644.pdf) ]也使用对抗性损失最小化这个 KL(不是 KL(q(z|x)||p(z)))。

更深入地说，【 [TC-βVAE](https://arxiv.org/pdf/1802.04942.pdf) 】进一步将这个边际 KL 分解为总相关性(TC) *【第一项】*和维度 KL *【第二项】*:

![](img/7ecdd523c21b03c8f899d775e173cc0d.png)

通过这种分解，他们认为 TC (Watanabe 1960)是一种流行的多随机变量相关性度量，是学习解纠缠表示的最重要的术语，因此用一些β权重惩罚 TC，因此他们的总体目标看起来像:

![](img/cc2eecbf3c00fb71ffd56e1085b63c78.png)

与此同时，[[dfactoring](https://arxiv.org/pdf/1802.05983.pdf)]论文也承认了 TC 对于解纠缠的重要性，并在 ELBO 中增加了这个术语的权重(- *λ)* 。同样，由于 TC 已经存在于 ELBO 中，他们实际上是最小化( *λ* + 1)加权 TC 以鼓励解开。

然而，根本的挑战在于对 *q(z)* (聚合后验分布)的估计，这取决于整个数据集(不仅仅是一个小批量)。这导致所有这些工作在估计 *q(z)* 或任何涉及它的项时采取不同的方法。例如，[[dfactoring](https://arxiv.org/pdf/1802.05983.pdf)]使用了带有独立鉴别器的密度比技巧。

总的来说，我相信，在不久的将来，解开与 VAE 的纠葛会变得更加有趣。(更新)针对这一点，在[ [IBP-VAE](https://arxiv.org/pdf/1909.01839.pdf) ]上，我们认为，随着生成因素复杂性的增加，这些讨论过的方法中的一些方法的解开能力下降，并提出 VAE 与非参数潜在因素模型(IBP-VAE)，潜在密度可以随着数据复杂性的增加而增加，表明解开能力提高。