<html>
<head>
<title>Practical Reinforcement Learning — 02 Getting started with Q-learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实用强化学习—02 Q-Learning 入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/practical-reinforcement-learning-02-getting-started-with-q-learning-582f63e4acd9?source=collection_archive---------0-----------------------#2017-04-04">https://towardsdatascience.com/practical-reinforcement-learning-02-getting-started-with-q-learning-582f63e4acd9?source=collection_archive---------0-----------------------#2017-04-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6ad1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">最简单的 Q 学习入门。<a class="ae kf" href="https://notebooks.azure.com/shreyasgite/libraries/100daysofAI" rel="noopener ugc nofollow" target="_blank">浏览器中的代码</a>，未安装:)</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/256596773765d73897248b8ac04389f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2RK1d3jNoA0nrmoJn_r39Q.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Smart Cab — GridWorld</figcaption></figure><p id="0820" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这一次，我们将教会我们的自动驾驶汽车开车送我们回家(橙色节点)。我们必须小心，因为一些街道正在建设中(灰色节点)，我们不希望我们的车撞上它。</p><p id="7c77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如你所见，我们有从 0 到 8 的街道。这给了我们 9 个独特的<strong class="ky ir">州</strong>(街道)。在任何给定的时间，我们的汽车(代理)可以处于这 9 种状态之一。状态 8 是我们的目标，也称为<strong class="ky ir">终端状态</strong>。</p><p id="00e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的车可以左、右、上、下行驶。换句话说，我们的代理可以采取四种不同的<strong class="ky ir">动作</strong>。我们把它写成:<em class="ls">A∈A {上，下，左，右} </em></p><p id="6165" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代理因达到终止状态而获得<strong class="ky ir">奖励 10，除此之外没有奖励。</strong></p><h2 id="2a8b" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">Q 学习和 Q 表</h2><p id="5cb6" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">现在我们将创建一个矩阵，“Q”也称为 Q-table，这将是我们代理的大脑。矩阵 Q 被初始化为零，因为代理开始时什么都不知道(<em class="ls">就像约翰·斯诺；)</em>)。当它学习时，它用状态-动作对的新值更新 Q 表。下面是计算 Q[状态，动作]的公式</p><blockquote class="mr ms mt"><p id="c468" class="kw kx ls ky b kz la jr lb lc ld ju le mu lg lh li mv lk ll lm mw lo lp lq lr ij bi translated"><em class="iq"> Q[s，a] = Q[s，a] + alpha*(R + gamma*Max[Q(s '，A)] - Q[s，a]) </em></p></blockquote><p id="b564" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">哪里；</p><ul class=""><li id="0bfd" class="mx my iq ky b kz la lc ld lf mz lj na ln nb lr nc nd ne nf bi translated"><strong class="ky ir"> alpha </strong>是<strong class="ky ir">学习率，</strong></li><li id="1c0a" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><strong class="ky ir">伽玛</strong>是<strong class="ky ir">贴现因子</strong>。它量化了我们对未来奖励的重视程度。在未来的奖励中估算噪音也很方便。伽玛从 0 到 1 不等。如果 Gamma 接近于零，代理人将倾向于只考虑直接的回报。如果 Gamma 更接近 1，代理人将考虑更大权重的未来奖励，愿意延迟奖励。</li><li id="5296" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated">Max[Q(s '，A)]给出下一状态中所有可能动作的最大 Q 值<strong class="ky ir">。</strong></li></ul><p id="2f73" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代理探索不同的“状态-动作”组合，直到它达到目标或落入洞中。我们将把每一次探索称为<strong class="ky ir">集</strong>。每次代理到达目标或被终止，我们开始下一集。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/4b3c540f742d233ecdaa52d19231ecce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f8PhlUiGYVv_FJcgyFNGfw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Q-table</figcaption></figure><blockquote class="mr ms mt"><p id="768f" class="kw kx ls ky b kz la jr lb lc ld ju le mu lg lh li mv lk ll lm mw lo lp lq lr ij bi translated">下面用一些简单的数学来演示 Q-table 是如何更新的:<br/>取学习率(alpha)为 0.5 &amp;折现因子(gamma)为 0.9 <br/> Q[s，a] = Q[s，a] + alpha*(R + gamma*Max[Q(s '，A)] — Q[s，a]) <br/> <strong class="ky ir">早期剧集</strong> <br/> Q[3，L] = Q[3，L]+0.5*(10+0.9*Max[Q(8，U) 同样 Q[6，D] = 5 <br/> <strong class="ky ir">下一集</strong> <br/> Q[2，L] = Q[2，L]+0.5*(0+0.9*Max[Q(6，U)，Q(6，D)，Q(6，R)，Q(6，L)]-Q(2，L)) <br/> Q[2，L] = 0 + 0.5 * (0 + 0.9 * Max [0，5，0，0] -0) <br/> Q[2</p></blockquote><h2 id="fa03" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">勘探与开采—ε(<strong class="ak"><em class="nl">ε</em></strong>)</h2><p id="1daa" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">当代理开始学习时，我们希望它采取随机行动来探索更多的路径。但是随着代理变得更好，Q 函数收敛到更一致的 Q 值。现在，我们希望我们的代理利用具有最高 Q 值的路径，即采取贪婪的行动。这就是艾司隆的用武之地。</p><p id="6474" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">智能体对概率</em> <strong class="ky ir"> <em class="ls"> ε </em> </strong> <em class="ls">采取随机行动，对概率采取贪婪行动(1-</em><strong class="ky ir"><em class="ls">ε</em></strong><em class="ls">)。</em></p><p id="822c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Google DeepMind 用了一个衰减的ε-贪婪动作选择。其中ε <em class="ls"> </em>随时间从 1 到 0.1 衰减<em class="ls"/>——开始时，系统完全随机地移动以最大化地探索状态空间，然后它稳定到固定的探索速率。</p><h2 id="7217" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">q 学习伪代码</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/35c84168a5725d9d215f9519b54308e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CG93ImeHOgRf2WFbbrF3jQ.png"/></div></div></figure><h2 id="4fa4" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">是时候动手 Q 学习了</h2><p id="5fc0" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">现在继续用 OpenAI 健身房练习 Q-learning。你可以在你的<a class="ae kf" href="https://notebooks.azure.com/shreyasgite/libraries/100daysofAI" rel="noopener ugc nofollow" target="_blank">浏览器中使用 azure 笔记本</a>来完成。或者克隆<a class="ae kf" href="https://github.com/shreyasgite/practical-reinforcement-learning" rel="noopener ugc nofollow" target="_blank">这个 git 回购</a>。</p><h2 id="a809" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">更多资源</h2><p id="494e" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated"><a class="ae kf" href="http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf" rel="noopener ugc nofollow" target="_blank">强化学习:简介</a> —第六章:时差学习<br/> <a class="ae kf" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA" rel="noopener ugc nofollow" target="_blank">大卫·西尔弗的强化学习课程第四讲</a> —无模型预测<br/> <a class="ae kf" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4" rel="noopener ugc nofollow" target="_blank">大卫·西尔弗的强化学习课程第五讲</a> —无模型控制</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><p id="2f47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望这篇教程对刚接触 Q-learning 和 TD-reinforcement 学习的人有所帮助！</p><p id="cee3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你想关注我关于强化学习的文章，请在 Medium <a class="nt nu ep" href="https://medium.com/u/47216364aba9?source=post_page-----582f63e4acd9--------------------------------" rel="noopener" target="_blank"> Shreyas Gite </a>或 twitter <a class="ae kf" href="https://twitter.com/shreyasgite" rel="noopener ugc nofollow" target="_blank"> @shreyasgite </a>上关注我。<br/>如有任何问题或建议，请随时给我写信:)</p><p id="fa18" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">更多来自我的实用强化学习系列:</em> </strong></p><ol class=""><li id="4de5" class="mx my iq ky b kz la lc ld lf mz lj na ln nb lr nv nd ne nf bi translated"><a class="ae kf" href="https://medium.com/@shreyas.gite/reinforcement-learning-is-awesome-100daysofai-day-01-intro-1ddebba6d435" rel="noopener">强化学习简介</a></li><li id="4adc" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nv nd ne nf bi translated"><strong class="ky ir">Q-learning 入门</strong></li></ol></div></div>    
</body>
</html>