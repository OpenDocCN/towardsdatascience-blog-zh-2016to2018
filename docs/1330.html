<html>
<head>
<title>All the news</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">所有的新闻</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-the-news-17fa34b52b9d?source=collection_archive---------3-----------------------#2017-08-24">https://towardsdatascience.com/all-the-news-17fa34b52b9d?source=collection_archive---------3-----------------------#2017-08-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5cbf" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用KMeans对143，000篇文章进行聚类。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b658cdf658a1239e0a23bdf2b7dc3bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lJQ8C5VwHONBN_J_wIuqkA.png"/></div></div></figure><p id="dd3a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我最近策划了这个数据集，探索构成我们新闻的类别的一些算法近似，这是我在不同时间阅读和创作的东西。如果你有成千上万篇来自各种渠道的文章，这些文章似乎或多或少地代表了我们的国家新闻格局，你把它们变成结构化数据，你用枪指着这些数据的头，强迫它们分组，这些分组会是什么？</p><p id="71a4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我决定，简单性和有效性的最佳平衡是使用无监督聚类方法，让数据自己排序，无论多么粗糙(类别，无论它们来自什么算法，几乎总是粗糙的，因为没有理由媒体不能无限小地分类)。出于各种原因(本地内存限制、能力、来自更有学问的人的建议)，我选择通过KMeans运行一个单词包——换句话说，如果每个单词都成为它自己的维度，每篇文章都成为单个数据点，那么会形成什么样的文章簇呢？如果你渴望跳到“那又怎样”和/或不关心代码，向下滚动直到你看到粗体字母告诉你不要这样做。代码在这里，如果有人想同行审查这一点，告诉我，如果/哪里我搞砸了，和/或给我建议。</p><p id="c88e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因为KMeans是非确定性的，所以结果会有所不同；聚类在运行之间会有一点点变化，但是，现在已经做了很多，我可以证明它们不会有很大的变化。这里的结果或多或少是数据所固有的。</p><h1 id="c69b" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">简要概述</h1><p id="1757" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">数据看起来是这样的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/05682d797705577d7af26c01ed5d01a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*2YnG7zkXqIT28WnosDYibQ.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">The number of articles in the dataset</figcaption></figure><p id="5d53" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这些出版物是基于完全不科学的笛卡尔内省过程选择的，我审视了自己，得出了一个粗略的总结，我认为这是我们国家新闻景观的合理样本。抓取这些文章的方法大致如下:</p><ol class=""><li id="6793" class="mq mr iq kt b ku kv kx ky la ms le mt li mu lm mv mw mx my bi translated">从无价的archive.org上获取过去一年半中每份出版物的存档主页或RSS订阅源的链接(如此珍贵，以至于我后来给了他们钱以表示感谢(如果有人在阅读，感谢存在))。</li><li id="b44a" class="mq mr iq kt b ku mz kx na la nb le nc li nd lm mv mw mx my bi translated">使用一个由BeautifulSoup拼凑而成的非常非常粗糙的web scraper从存档的主页上的每个链接中抓取每一篇文章。</li><li id="787a" class="mq mr iq kt b ku mz kx na la nb le nc li nd lm mv mw mx my bi translated">无趣的数据。</li><li id="99fc" class="mq mr iq kt b ku mz kx na la nb le nc li nd lm mv mw mx my bi translated">清理不干净的数据。</li><li id="c08b" class="mq mr iq kt b ku mz kx na la nb le nc li nd lm mv mw mx my bi translated">等等。</li></ol><p id="3179" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">也就是说，这里的数据包含了主要从2016年初到2017年7月的这些出版物的文章，并且是在主页或RSS提要上出现的文章——即，这不是像2016年8月13日那样对整个领域进行贪得无厌的真空处理。</p><h1 id="913e" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">词干分析</h1><p id="fb06" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">这是数据的一部分:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/0fb344adb9096dcd1a2ae4f0a6fbc45e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iYnEdEbqzXApkLALJTZUKg.png"/></div></div></figure><p id="e5f8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我做了一个判断调用(你可以判断这个判断)从语料库中删除专有名词。这种想法是，A)它们太多了，B)它们没有告诉我们太多关于写作的内容和方式，C)当试图归结这些类别的本质时，它们是另一种形式的噪音。稍后您将会看到，Python的NLTK是一个很棒的包，但是它并不完美，在试图清除掉一些专有名词之后，它们仍然留在了corupus中。</p><p id="76fa" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，我决定删除数字。这里有一行是关于NLTK的标记器有时会在单词末尾留下句点的倾向，我认为这是一个bug。</p><p id="9993" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">堵塞过程看起来像:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="b591" class="nk lp iq ng b gy nl nm l nn no">import nltk<br/>from nltk import pos_tag<br/>from nltk.stem import PorterStemmer<br/>from nltk import word_tokenize<br/>from collections import Counter<br/>import time</span><span id="d00f" class="nk lp iq ng b gy np nm l nn no">stemmer = PorterStemmer()<br/>tokenizer = nltk.data.load(‘tokenizers/punkt/english.pickle’)</span><span id="29cc" class="nk lp iq ng b gy np nm l nn no">progress = 0 #for keeping track of where the function is</span><span id="13c2" class="nk lp iq ng b gy np nm l nn no">def stem(x):<br/>    end = time.time()<br/>    dirty = word_tokenize(x)<br/>    tokens = []<br/>    for word in dirty:<br/>        if word.strip(‘.’) == ‘’: #this deals with the bug<br/>           pass<br/>        elif re.search(r’\d{1,}’, word): #getting rid of digits<br/>           pass<br/>       else:<br/>           tokens.append(word.strip(‘.’))<br/>   global start<br/>   global progress<br/>   tokens = pos_tag(tokens) #<br/>   progress += 1<br/>   stems = ‘ ‘.join(stemmer.stem(key.lower()) for key, value in  tokens if value != ‘NNP’) #getting rid of proper nouns<br/> <br/>   end = time.time()</span><span id="5e04" class="nk lp iq ng b gy np nm l nn no">   sys.stdout.write(‘\r {} percent, {} position, {} per second ‘.format(str(float(progress / len(articles))), <br/> str(progress), (1 / (end — start)))) #lets us see how much time is left </span><span id="cf0a" class="nk lp iq ng b gy np nm l nn no">   start = time.time()<br/>   return stems</span><span id="a043" class="nk lp iq ng b gy np nm l nn no">start = time.time()<br/>articles['stems'] = articles.content.apply(lambda x: stem(x))</span></pre><p id="9600" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">结果是一篇<a class="ae ln" href="https://www.nytimes.com/2017/01/08/world/europe/queen-elizabeth-health.html?mcubz=3&amp;_r=0" rel="noopener ugc nofollow" target="_blank">文章</a>由此而来:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="b99f" class="nk lp iq ng b gy nl nm l nn no">Queen Elizabeth II made her first public appearance in almost a month on Sunday, allaying concerns about her health after she missed Christmas and New Year’s Day church services because of what Buckingham Palace described as a persistent cold. The queen, who will turn 91 in April, attended services at St. Mary Magdalene Church in Sandringham...</span></pre><p id="8898" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对此:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="4921" class="nk lp iq ng b gy nl nm l nn no">made her first public appear in almost a month on , allay concern about her health after she miss and s church servic becaus of what describ as a persist cold the queen , who will turn in , attend servic at in...</span></pre><p id="a789" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">词干处理大大减少了语料库的规模。“实现”和“实现”不是被认为是不同的词，并被赋予各自的维度，而是被简化为它们共有的词干，“现实”。这减少了噪音，因此算法不会对出版物使用过去时态而不是现在时态的决定产生影响，或者不会将复数名词视为词汇的不同部分而不是单数名词，等等。</p><h1 id="201e" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">创造词汇</h1><p id="f553" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">现在进行人数统计，统计整个语料库中的每一个词干，然后可以将其转化为数据帧，用于文档术语矩阵和词汇表。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="ef9d" class="nk lp iq ng b gy nl nm l nn no">from collections import Counter<br/>all_words = Counter()<br/>start = time.time()<br/>progress = 0<br/>def count_everything(x):<br/>    global start<br/>    global all_words<br/>    global progress<br/>    x = x.split(‘ ‘)<br/>    for word in x:<br/>        all_words[word] += 1<br/>    progress += 1<br/>    end = time.time()<br/> sys.stdout.write(‘\r {} percent, {} position, {} per second ‘.format((str(float(progress / len(articles)))), <br/> (progress), (1 / (end — start))))<br/>    start = time.time()</span><span id="23ba" class="nk lp iq ng b gy np nm l nn no">for item in articles.stems:<br/>    count_everything(item)</span></pre><p id="74aa" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然后将其传送到新的数据帧:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="e59f" class="nk lp iq ng b gy nl nm l nn no">allwordsdf = pd.DataFrame(columns = [‘words’, ‘count’])<br/>allwordsdf[‘count’] = pd.Series(list(all_words.values()))<br/>allwordsdf[‘words’] = pd.Series(list(all_words.keys()))<br/>allwordsdf.index = allwordsdf[‘words’]</span></pre><p id="1854" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在数据帧的开头给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/16eb549ed157f47cd034d8920f8707b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*D1jYAgY_0_VbWcC8Ospqfg.png"/></div></figure><p id="4e98" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">Republican的加入是一个很好的例子，说明词性标注者并没有完全去除专有名词。但忽略这一点，语料库现在是一个数据框架，词汇表中的每个术语都是索引中的项目，这在不久的将来会很有用。</p><p id="744f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当处理来自因特网的文本数据时的一个挑战是，非单词，如字符和符号的组合(例如，“@username”，“#hashtags”，用类似“well 1…”的省略号连接的单词)，以相对频率出现。我决定只保留NLTK的完整英语语料库中的单词，而不是找到并清理每一个单词。语言学家们还在争论这个语料库有多完整，但是有236，736个单词，这是一个相当大的数字。我们将首先通过对整个英语语料库进行词干分析，然后将该语料库与我们自己的语料库进行比较，来完成我们的数据框架的最终精简:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="5dd3" class="nk lp iq ng b gy nl nm l nn no">from nltk.corpus import words</span><span id="ba3b" class="nk lp iq ng b gy np nm l nn no">nltkstems = [stemmer.stem(word) for word in words.words()] #stem the #words in the NLTK corpus so that they’re equivalent to the words in #the allwordsdf dataframe</span><span id="586b" class="nk lp iq ng b gy np nm l nn no">nltkwords = pd.DataFrame() #make a new dataframe with the stemmed #NLTK words</span><span id="3bbf" class="nk lp iq ng b gy np nm l nn no">nltkwords[‘words’] = nltkstems</span><span id="f322" class="nk lp iq ng b gy np nm l nn no">allwordsdf = allwordsdf[allwordsdf[‘words’].isin(nltkwords[‘words’])] #keep only #those in the stemmed NLTK corpus</span></pre><p id="64c2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这将总词汇量从89216减少到34527。它处理了词汇表中的每一个细节，我花了几个星期才考虑这个解决方案。</p><h1 id="6b5c" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">将单词矢量化</h1><p id="cf47" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">粗略地说，TfIdf(术语频率-逆文档频率)矢量器为每篇文章中的每个词给出一个值，该值由该词在整个语料库中的频率加权。逆向文档频率是从单词在整个数据集中的频率得出的分母。以“perspicacious”这个词为例，由于它在英语中有许多更好的替代词，这个词是一个我们很幸运几乎看不到的扯淡词。由于这种稀缺性，它的反向文档频率，或妖魔化，是低的。如果它在一篇文章中出现15次，那么它的Tf值或分子将会很高。因此，它的TfIdf值将是一个大分子，而不是一个小恶魔，从而产生一个大数字。因此，在我们数千维的空间中，文章在“洞察力”维度上具有价值。(当然，这还不包括向量的归一化和寻找TfIdf值所涉及的其他任务。)</p><p id="786f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当使用这种类型的矢量器时，包含非索引词(算法忽略的词的列表)并不重要，因为不常用的词被赋予较低的值。但它仍然是有用的，因为至少它降低了内存的使用，减少了我们的空间已经非常高的维度。此外，创建一个单词库可以确保出现在一篇文章中的非常罕见的单词不会自己聚集在一起。我选择了第40个分位数以上的单词。乍一看，这似乎很高，直到你看到这个分位数包含的内容:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="3ede" class="nk lp iq ng b gy nl nm l nn no">allwordsdf[allwordsdf[‘count’] == allwordsdf[‘count’].quantile(.4)][:10]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ec0f06cdbd30448ceccb7b8bbb5cf372.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*phPf_y9pF1ieaOgL9dAkxg.png"/></div></figure><p id="2d34" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，第40个分位数包括在整个语料库中只出现9次的单词——非常低，因此不倾向于提供信息。为什么不是第50或60分位数？因为必须在某个地方选择一个数字，它可能就是这个。</p><p id="4b0c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">创建停用词，矢量词汇和矢量。写停用词和词汇表可能是多余的；我把两者都加进来是为了更好的衡量，因为我们以后需要这个词汇列表。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="7d2b" class="nk lp iq ng b gy nl nm l nn no">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="7d44" class="nk lp iq ng b gy np nm l nn no">stopwords = list(allwordsdf[(allwordsdf[‘count’] &gt;= allwordsdf[‘count’].quantile(.995)) | (allwordsdf[‘count’] &lt;= allwordsdf[‘count’].quantile(.4))][‘words’])</span><span id="b0d5" class="nk lp iq ng b gy np nm l nn no">vecvocab = list(allwordsdf[(allwordsdf[‘count’] &lt; allwordsdf[‘count’].quantile(.995)) &amp; (allwordsdf[‘count’] &gt; allwordsdf[‘count’].quantile(.4))][‘words’])</span><span id="dcac" class="nk lp iq ng b gy np nm l nn no">vec = TfidfVectorizer(stop_words = stopwords, vocabulary = vecvocab, tokenizer=None)</span></pre><p id="92d1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在要转换数据帧:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="32bb" class="nk lp iq ng b gy nl nm l nn no">vec_matrix = vec.fit_transform(articles[‘stems’])</span></pre><p id="c84c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这就产生了一个矩阵形状的<code class="fe ns nt nu ng b">(142570, 20193)</code>，也就是大约20，000个单词。</p><h1 id="c269" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">降维</h1><p id="ed56" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">把我们的20，193维矩阵降多少维很难回答。Sklearn的<a class="ae ln" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html" rel="noopener ugc nofollow" target="_blank">官方推荐</a>声明，“对于潜在语义分析(我们在这里做的)，推荐值为100。”我用所有20，193个维度对这些数据进行了聚类，用100个维度进行了聚类，用3个维度进行了聚类，每一次，这些聚类看起来都与有多少个维度无关。这最终归结为减少处理时间，因为创造这个包的人的智慧规定了100个维度，所以它是100个维度。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="412e" class="nk lp iq ng b gy nl nm l nn no">from sklearn.decomposition import TruncatedSVD</span><span id="f8f6" class="nk lp iq ng b gy np nm l nn no">pca = TruncatedSVD(n_components=100)</span><span id="9d62" class="nk lp iq ng b gy np nm l nn no">vec_matrix_pca = pca.fit_transform(vec_matrix)</span></pre><h1 id="6f3e" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">使聚集</h1><p id="db0d" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">更难回答的是给数据分配多少个聚类。对于KMeans，数据过于紧密地聚集在一起，不适合分层聚类或任何自己查找聚类的算法。如上所述，我选择了十作为起点。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="7eba" class="nk lp iq ng b gy nl nm l nn no">from sklearn.cluster import KMeans</span><span id="aab3" class="nk lp iq ng b gy np nm l nn no">clf10 = KMeans(n_clusters=10, verbose = 0)</span><span id="f44c" class="nk lp iq ng b gy np nm l nn no">clf10.fit(vec_matrix_pca)</span></pre><p id="f79a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，将我们刚刚创建的标签分配给原始数据帧，以便进行分组、可视化和分析:</p><p id="bfa1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><code class="fe ns nt nu ng b">articles[‘labels’] = clf10.labels_</code></p><p id="3a6e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们可以看看每个出版物的文章被分配到每个标签的百分比:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="27d6" class="nk lp iq ng b gy nl nm l nn no">labelsdf = articles.groupby([‘publication’, ‘labels’]).count()<br/>pubslist = list(articles[‘publication’].unique())<br/>labelsdf[‘percentage’] = 0</span><span id="024a" class="nk lp iq ng b gy np nm l nn no">for index, row in labelsdf.iterrows():<br/>    for pub in pubslist:<br/>        for label in range(10):<br/>            try:<br/>                labelsdf.loc[(pub, label), ‘percentage’] = labelsdf.loc[(pub, label), ‘id’] / labelsdf.loc[(pub), ‘id’].sum()<br/>            except:<br/>                pass</span><span id="026f" class="nk lp iq ng b gy np nm l nn no">labelsdf = labelsdf[['publication', 'labels', 'percentage']]</span></pre><p id="6944" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">等于</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/bf1300eafb9267f71d3e712dd893048e.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*_nsUcF8o5P52ScJY0O466g.png"/></div></figure><p id="c644" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">诸如此类。</p><h1 id="0302" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">如果你向下滚动，这里是停止的地方</h1><p id="8f1f" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">这些图表是在RStudio中用Plotly制作的，plot ly是一个有时质量不佳但仍然有用的数据软件包。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="8a37" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这些是聚类，每个聚类中的前200个单词及其TfIdf值。单词越大，确定是什么组成的集群就越重要。它们不会被政治联盟、基调或其他任何东西所瓦解；当数据聚集在一起时，会按类别分解。记住，我们选了十个；我们可以选择8或20个并得到不同的集群，但这些是最突出的。一个是关于俄罗斯的所有事情-丑闻，另一个是关于外交政策，另一个是关于教育等等。随意探索。(Plotly允许用户放大，但字不会变大；我已经通知了当局来解决这个问题。)</p><p id="fe55" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面是出版物是如何按组分类的。(同样，对于隐私獾用户来说，<a class="ae ln" href="https://plot.ly/~snapcrack/12" rel="noopener ugc nofollow" target="_blank">这个</a>就是。)</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="19a3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">还有最后一个viz，如果它大小引起了问题，我很抱歉，但是我认为它很重要。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="6cfd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">它的名字(“articlestest”)应该表明我在创建它时处于什么准备阶段，它与最终的数据集略有不同——除了其他所有东西，它还包括来自the Verge的长篇文章，我选择了去掉它。它是将所有数据缩减为三维的可视化，而不是20，000多个维度(或上面主成分分析中使用的100个维度)。添加边缘会给人一种与移除边缘时截然不同的形状，我完全不确定这是为什么。但是如果你观察它，你可以看到每个集群在三维空间中形成的位置，你可以双击右边的每个出版物名称，只保留那个出版物。如果你点击它们，你会看到几乎每一个都是无形的数据点云，除了路透社，它形成了一个几乎完美的l。</p><h1 id="77e0" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">推论性的结论</h1><p id="8e63" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">不同的人可能会对这些数据和图表有不同的解读，但我是这样解读的:当被迫分组时，出版物被归类为路透社和其他。</p><p id="db59" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我不认为这是对数据的过度提取。数据科学的一个主要哲学基础是，潜在的真理和联系是由表面现象引发的，一个人说话的语气或穿着的风格比我们愿意承认的方式泄露了更多关于我们的信息。这些出版物中的一个以与其他出版物如此不同的形式出现，完全基于它覆盖的主题，这将被合理地认为是考虑它与数据集的其他成员根本不同的原因。</p><p id="7e3e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">诸如此类的结论应该经得起纯粹反思的考验——考虑到阅读这些出版物的体验，这些结论有意义吗？</p><p id="bf79" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在Medium的几个门下，<a class="ny nz ep" href="https://medium.com/u/2faf1ea1b2fb?source=post_page-----17fa34b52b9d--------------------------------" rel="noopener" target="_blank">兰詹·罗伊</a> <a class="ae ln" href="https://medium.com/@ranjanxroy/about-that-vice-charlottesville-documentary-3db02e92fcb7" rel="noopener">写了一篇文章</a>关于Vice的夏洛茨维尔分部看似不可思议的病毒式传播。他提醒我们，Vice现在是另一个默多克资助的暴行机器。我不同意罗伊的观点，即视频的点击率如此容易地归因于HBO的阴谋诡计(因此暗示病毒传播可以被预测地设计，但它不能)，但就我们的目的而言，他以一个有益的反思结束了这篇文章:</p><blockquote class="oa ob oc"><p id="cc6f" class="kr ks od kt b ku kv jr kw kx ky ju kz oe lb lc ld of lf lg lh og lj lk ll lm ij bi translated">我们越是接受耸人听闻的现实，我们就越糟。只要我们让我们的蜥蜴脑控制我们对世界的感知，我们就没有机会。如果我们没有意识到我们所观看的背后的动机，疯狂的人总是会赢。</p><p id="a338" class="kr ks od kt b ku kv jr kw kx ky ju kz oe lb lc ld of lf lg lh og lj lk ll lm ij bi translated">这是价值十亿美元的媒体，由拥有福克斯的同一个人出资。它完善了让你下去的艺术。记住，在一个我们每天花费116分钟消费广告资助的社交媒体的世界里，<strong class="kt ir">最响亮的声音总是会赢得</strong>。[强调他的]</p></blockquote><p id="c202" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">除了路透社之外，该数据集中的所有出版物都有一些共同点。它们完全由广告和/或订阅资助(Vox和BuzzFeed也有风险投资，但它们是基于广告的模式)，它们的存在依赖于点击。相比之下，路透社的新闻产品仅仅是一家大型信息集团的公众形象。或许更重要的是，它是一家新闻通讯社，其报道包括我们金融世界的事务，因此被赋予了不同于其他媒体的使命——可以说比《纽约时报》更重要，它必须覆盖所有新闻，而不会陷入人物驱动的真人秀，而该数据集的每个其他公民似乎都非常喜欢这样做。在所有这些事件中，它的声音倾向于保持最适度的室内音量，如果路透社能够激起愤怒的话，没有一个全球性事件会激起比生活更大的愤怒。也许这是隶属于财经出版社，宏观分析世界的产物；非金融媒体的叙述未能对伦敦银行间同业拆借利率(LIBOR)的变化和一个疯子的政策提议给予同等的重视，尽管按理说应该如此。这里的所有其他出版物似乎都带有乌托邦的暗示，它们内容的潜台词通常是，如果我们在食谱中混合了正确的成分，一个完美的世界就会实现，而你感到愤怒的事情实际上是我们和天堂之间的事情。在我作为读者的经历中，我从来没有感觉到路透社有任何类似的东西。</p><p id="c380" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这不应被解释为断言《纽约时报》和《布莱巴特》因此是同一批中风患者。我今天在《泰晤士报》上读到了一篇设计精美的文章，内容是关于生物发光在深海生物中是多么普遍。不言而喻，在布莱巴特找到这样一份文件的可能性几乎为零，这是我对政治光谱的这一领域感到非常悲哀的事情之一，也是对其截然相反的反对者的一份谈话要点备忘录。但这才是重点:给一个算法看你写的关于深海生物的故事数量，它就会显示你是谁。在更精细的分辨率下，我们可能会发现《纽约时报》和《福克斯新闻频道》之间的鸿沟，或者《NPR时报》和《纽约邮报》之间的鸿沟。看到上面的第三个聚类了吗，所有的单词都用较低的TfIdf值进行了压缩，没有突出的内容。它实际上是其他主题的整个丛林，你可以在那个集群上运行算法，并获得新的分组和区别——这些<em class="od">集群中的一个</em>也将是不同类型故事的压缩，你可以在机器学习的分形中一遍又一遍地这样做。这里的区别不是唯一的，但从数据的空中视角来看，这是第一个。</p><p id="fadc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="od">作为一个创造内容的人，我在法律上有义务经营一份个人时事通讯。可以在这里</em>  <em class="od">报名参加</em> <a class="ae ln" href="https://tinyletter.com/asthompson" rel="noopener ugc nofollow" target="_blank"> <em class="od">。</em></a></p></div></div>    
</body>
</html>