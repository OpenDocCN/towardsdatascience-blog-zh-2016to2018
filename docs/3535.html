<html>
<head>
<title>[ Paper Summary ] The Effectiveness of Data Augmentation in Image Classification using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">【论文摘要】使用深度学习的图像分类中数据增强的有效性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-summary-the-effectiveness-of-data-augmentation-in-image-classification-using-deep-9160dc87806b?source=collection_archive---------10-----------------------#2018-05-21">https://towardsdatascience.com/paper-summary-the-effectiveness-of-data-augmentation-in-image-classification-using-deep-9160dc87806b?source=collection_archive---------10-----------------------#2018-05-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/18a2fe51ea3b1711741115e0cee282ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fv2wXjQTzkMwgivP-jXR6g.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Image from this <a class="ae kc" href="https://pixabay.com/en/statistics-analysis-graph-data-3411311/" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="7c22" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我想知道更多关于数据扩充在多大程度上有助于提高测试/验证数据的准确性。我发现了这篇很棒的论文，想做一个论文总结。</p><blockquote class="lb lc ld"><p id="aca2" class="kd ke le kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated"><strong class="kf ir"> **请注意这篇帖子是为了我未来的自己复习这篇论文上的材料，而不是从头再看一遍。</strong></p></blockquote></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><figure class="lp lq lr ls gt jr"><div class="bz fp l di"><div class="lt lu l"/></div></figure></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="1b72" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">摘要</strong></p><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lv"><img src="../Images/e1f239b77cc27dfd54228070603fd711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4RKHIisrfhFPHN2JbFHmGA.png"/></div></div></figure><p id="edaf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文旨在通过使用传统的数据增强技术(如裁剪、旋转)或使用 GAN ( <a class="ae kc" href="https://arxiv.org/abs/1703.10593" rel="noopener ugc nofollow" target="_blank"> CycleGAN </a>)来研究数据增强的有效性。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="3d33" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">简介</strong></p><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/d42d1d42b4cd02e2e765d334d714e8c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*gJfqSTyyZfiTWG3vnE55wA.png"/></div></figure><p id="641d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本节中，作者指出神经网络可以从大量数据中受益匪浅。他们举了一个例子来说明 Google corpus 的发布是如何让文本到语音以及基于文本的模型受益的。作者提出的一个有趣的观点是，对于大量的非结构化数据，任务变成了寻找模式。然而，我们可以采用另一种方法，其中我们有一小组结构化数据并执行增强。最后，作者介绍了他们将要进行实验的数据集，即<a class="ae kc" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST </a>和<a class="ae kc" href="https://tiny-imagenet.herokuapp.com/" rel="noopener ugc nofollow" target="_blank">微小图像网</a>数据</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="49d3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">相关工作</strong></p><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lx"><img src="../Images/d6988fca5b7956e72e4a7dad651d80fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hC-JVIN15WZPXBsmvOncog.png"/></div></div></figure><p id="45d2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本节中，作者回顾了一些用于防止过度拟合的流行方法。方法，如增加一个正则项，辍学，批量规范化和转移学习介绍。此外，作者给出了数据增强技术的简单描述，如几何或颜色增强。(大多是<a class="ae kc" href="https://www.mathworks.com/discovery/affine-transformation.html" rel="noopener ugc nofollow" target="_blank">仿射变换</a>。).以及如何训练 GAN 的基本描述。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="0a89" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">方法</strong></p><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ly"><img src="../Images/a54e7b1f6a1cc0cc6351e27110326cce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rjwrMDtXxDiOpScH8byu-A.png"/></div></div></figure><p id="4db3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是超级有趣的地方，作者将采取两种不同的方法。</p><p id="353c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">a)在训练分类器之前增加数据(使用 GAN 或仿射)<br/> b)通过使用分类器网络的前置神经网络实时增加数据。(基本上，他们正在创建一个从扩充网络到分类器网络的数据管道)</p><p id="5870" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作者将使用传统转换或 CycleGAN(风格转换)来执行数据扩充。(如下图)。</p><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lz"><img src="../Images/216f88f64c6cbc0b5ab71aa05b59e8c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OkQ8LAiIsq7Qwd80XVkmpg.png"/></div></div></figure><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ma"><img src="../Images/e10b6ed81cc50c28bc4718f89a92547a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mARhQmms6TIioV-56Dr7dw.png"/></div></div></figure><p id="2239" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，对于增强网络，他们创建了一个小型的 5 CNN 网络，使用用于训练网络的各种损失函数来执行增强。(1.内容丢失，2。风格丧失，3。不亏)</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="5246" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">数据集和特征</strong></p><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mb"><img src="../Images/3d0c0d1e5560dfa0df0346e2f0adafa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MrBPFiKBARwAM-5YM0e4iw.png"/></div></div></figure><p id="2801" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作者用三个数据集进行了实验，其中两个数据集来自微图像网，第三个数据集来自 MNIST 数据集。第一数据集仅由狗/猫图像组成，第二数据集由狗/金鱼的图像组成。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="b297" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">实验</strong></p><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mc"><img src="../Images/25c477b39bc63c500969129442b439a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VpifuKcYXlEyDyFI6GmnoA.png"/></div></div></figure><p id="164a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个实验中使用了两个网络，首先是分类网络(SmallNet)和增强网络(Augmentation Network)。两个网络网络架构如下所示。</p><div class="lp lq lr ls gt ab cb"><figure class="md jr me mf mg mh mi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/d66cd5e72d8196f779f52db3cec97ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*TgaHlC3_oocYrbih1dwmgQ.png"/></div></figure><figure class="md jr mj mf mg mh mi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/5ed2a3361cb761b562632ee950dcd482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*4lB2HIfCCjVpnojDUjTn_g.png"/></div></figure></div><p id="a95a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">增强网络的输入是通过连接来自相同类别的两个图像(在它们的通道维度上)来创建附加图像。(这是数据扩充部分。)增强网络仅在训练期间使用，而不在测试期间使用，整个过程如下所示。</p><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mk"><img src="../Images/1ddc0e9b90f02a1b635c648cebcfdb0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zrJhFIwWb-QZX_9gJwf6sw.png"/></div></div></figure><p id="63d7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后要考虑的是损失函数，在图像被放大后，作者介绍了三种损失函数。(实际上是两个，因为最终损失函数根本不是损失函数。)</p><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/277880f441620a46bba72d527e3cc224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*BlGhvmOMhCw0e1_mCnZOIQ.png"/></div></figure><p id="ac46" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一个损失是增强图像和目标图像之间的 L2 损失，其中 D 项是增强图像和目标图像的长度</p><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mm"><img src="../Images/adf7fa85ff0e781165d4845c20b8a75c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZLdI49CUaL88pEo8oSIorQ.png"/></div></div></figure><p id="29d3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二种损失是增强图像和目标图像之间的<a class="ae kc" href="https://en.wikipedia.org/wiki/Gramian_matrix" rel="noopener ugc nofollow" target="_blank"> Gram 矩阵</a>的 L2 损失函数。如上所述，第三损失函数没有损失函数。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="63ea" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">结果</strong></p><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mn"><img src="../Images/979b7bbcf6def87278f0c7ff0209c5b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DifkM0wsJmo504qZU-MKWg.png"/></div></div></figure><p id="647b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于所有数据集，他们进行了不同类型的增强，并得到了以下结果。</p><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mo"><img src="../Images/8b48fa6da4e61fd8de1e58cdc6eab864.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wVxme12nekKik-2P3axogg.png"/></div></div></figure><p id="8c56" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">乍一看，我们可以假设无损失功能的神经增强与任何其他不同的方法相比表现最佳。(控制方法是当它们将相同的图像提供给增强网络时。)下面可以看到增强网络生成的一些图像。</p><div class="lp lq lr ls gt ab cb"><figure class="md jr mp mf mg mh mi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/ddee1435a7d38f04327023700ba3befe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*LOywL_iKLvMAbV11w_RG3A.png"/></div></figure><figure class="md jr mq mf mg mh mi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/2da8d29552294cc85156396c8f1be66d.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*o2m67o5tgmlYzx4oEJxSIg.png"/></div></figure></div><p id="c633" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作者表示，增强网络似乎从两幅图像中提取了一些关键特征，同时平滑了背景像素。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="2bdf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">结论和未来工作</strong></p><figure class="lp lq lr ls gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mr"><img src="../Images/ad45ee00a6cf0566e7674e46d8f300fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d1_ilLC9jQdHPFVhbdbEgg.png"/></div></div></figure><p id="8f16" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作者指出，使用更复杂的网络来执行分类和扩充是值得的。且还陈述了传统图像增强方法本身是有效的，同时与 GAN 的或神经增强相比消耗更少的时间。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="9e93" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">最后的话</strong></p><p id="31be" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这确实是一项有趣的工作，但是在这一点上，我看不到使用 GAN/神经增强来执行数据增强的巨大好处。但是这种方法显示了许多有希望的迹象。</p><p id="5195" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有写作的列表，请<a class="ae kc" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">在这里查看我的网站</a>。</p><p id="ccc9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同时，在我的 twitter 上关注我<a class="ae kc" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae kc" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae kc" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae kc" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文</a> t。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><p id="8605" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">参考</strong></p><ol class=""><li id="eb94" class="ms mt iq kf b kg kh kk kl ko mu ks mv kw mw la mx my mz na bi translated">朱，j .，帕克，t .，伊索拉，p .，，埃夫罗斯，A. (2017)。使用循环一致对抗网络的不成对图像到图像翻译。Arxiv.org。检索于 2018 年 5 月 21 日，来自<a class="ae kc" href="https://arxiv.org/abs/1703.10593" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1703.10593</a></li><li id="187d" class="ms mt iq kf b kg nb kk nc ko nd ks ne kw nf la mx my mz na bi translated">佩雷斯，l .，，王，J. (2017)。使用深度学习的图像分类中数据扩充的有效性。Arxiv.org。检索于 2018 年 5 月 21 日，来自<a class="ae kc" href="https://arxiv.org/abs/1712.04621" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1712.04621</a></li><li id="dac6" class="ms mt iq kf b kg nb kk nc ko nd ks ne kw nf la mx my mz na bi translated">MNIST 手写数字数据库，Yann LeCun，Corinna Cortes 和 Chris Burges。(2018).Yann.lecun.com。检索于 2018 年 5 月 21 日，来自<a class="ae kc" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">http://yann.lecun.com/exdb/mnist/</a></li><li id="27c4" class="ms mt iq kf b kg nb kk nc ko nd ks ne kw nf la mx my mz na bi translated">微型图像网络视觉识别挑战。(2018).Tiny-imagenet.herokuapp.com。检索于 2018 年 5 月 21 日，来自<a class="ae kc" href="https://tiny-imagenet.herokuapp.com/" rel="noopener ugc nofollow" target="_blank">https://tiny-imagenet.herokuapp.com/</a></li><li id="9bf5" class="ms mt iq kf b kg nb kk nc ko nd ks ne kw nf la mx my mz na bi translated">格拉米矩阵。(2018).En.wikipedia.org。检索于 2018 年 5 月 21 日，来自<a class="ae kc" href="https://en.wikipedia.org/wiki/Gramian_matrix" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Gramian_matrix</a></li><li id="21ca" class="ms mt iq kf b kg nb kk nc ko nd ks ne kw nf la mx my mz na bi translated">仿射变换。(2018).Mathworks.com。检索于 2018 年 5 月 21 日，来自<a class="ae kc" href="https://www.mathworks.com/discovery/affine-transformation.html" rel="noopener ugc nofollow" target="_blank">https://www . mathworks . com/discovery/affine-transformation . html</a></li></ol></div></div>    
</body>
</html>