<html>
<head>
<title>Dimensionality Reduction: ways and intuitions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维:方法和直觉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dimensionality-reduction-ways-and-intuitions-1b5e97592d8e?source=collection_archive---------8-----------------------#2018-11-24">https://towardsdatascience.com/dimensionality-reduction-ways-and-intuitions-1b5e97592d8e?source=collection_archive---------8-----------------------#2018-11-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/50a405e4cf6850f8a14cea0949798a04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BuZJQ0OgWpNXJloxMUMDDg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo of Jupiter by NASA 360 on <a class="ae kc" href="https://www.facebook.com/FollowNASA360/" rel="noopener ugc nofollow" target="_blank">Facebook</a></figcaption></figure><p id="9cf4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在大数据应用变得无处不在后，维数灾难变得比预期的更严重。因此，对于这种高维空间，可视化和分析变得更加困难。此外，我们来自 2D 和 3D 世界的洞察力和直觉(例如，距离)不再适用。另一个问题是，当预测器的数量很高时，预测模型更有可能具有高方差，这使得模型易于过度拟合。<br/> ّThis 的文章是关于降维技术的，这是我作为数据科学硕士生第一学期学到的。对于每一种技术，我将尝试提供直觉、标准和用例，并支持 Python 中的插图。为了简单起见，我不会深入这些技术的底层数学，但我会回顾一下我第一次研究它们时想到的问题。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="d20e" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">主成分分析法:</h1><p id="3492" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">主成分分析是降维技术中最流行和最广泛使用的方法之一。PCA 是一种无监督的方法，这意味着它不需要对您的数据执行任何标记。首先，您应该知道数据集中的每个要素都有一些可变性(即方差)，这表明您的数据围绕平均值分布了多少。PCA 是做什么的？它得到一些线性独立的分量(即，每个分量之间的相关性为零)，而每个分量是原始特征的线性组合，同时尽可能多地保留数据的总方差。因此，如果您的目标是在执行降维的同时保持数据的方差，那么 PCA 是一个不错的选择。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="ead8" class="mu lj iq mq b gy mv mw l mx my">import numpy as np<br/>from sklearn.decomposition import PCA<br/><br/><em class="mz"># Generate random Data of size (n x 5).<br/></em>X = np.random.uniform(size=(20, 5))<br/><br/><em class="mz"># Number of component wanted. X after reduction will be (n x 3).<br/></em>pca = PCA(n_components=3)<br/>X_reduced = pca.fit_transform(X)<br/><br/><em class="mz"># Portion preserved from the total variance. <br/># If 1, means we didn't lose any variance.<br/></em>print("Portion preserved from the total variance", np.sum(pca.explained_variance_ratio_))</span></pre><p id="253f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你也可以指定想要的最小变化量，它会为你决定组件的数量。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="40ad" class="mu lj iq mq b gy mv mw l mx my">pca = PCA(n_components=0.90)<br/>X_reduced = pca.fit_transform(X)</span><span id="455d" class="mu lj iq mq b gy na mw l mx my">print("Number of components =", pca.components_)</span></pre><h1 id="f8c4" class="li lj iq bd lk ll nb ln lo lp nc lr ls lt nd lv lw lx ne lz ma mb nf md me mf bi translated">线性判别分析— LDA:</h1><p id="26d7" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">LDA 是一种受监督的方法，这意味着您的数据需要在降维优化函数中进行标记。为什么使用标签？LDA 旨在找到最佳方向，即投影，以减少类内可变性并增加类间可变性。换句话说，LDA 最大化了类别之间的可分性。所以在高维分类问题中，可以先对数据进行 LDA，在保持可分性的同时降低维数，然后在降维后的数据上安全地建立预测模型。需要注意的是，如果你的类的数量是 C，那么你不能把维数减少到超过 C-1 维。为什么？在优化过程中，我们处理 C 个质心点，这些质心点最多位于 C-1 维空间中，因此我们可以得到的最大投影是 C-1。</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/587d055d13642929a4949bd1db93b8d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*9FXuEYwpuotc1naz8EPQYg.png"/></div></figure><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="c38e" class="mu lj iq mq b gy mv mw l mx my"><strong class="mq ir">import </strong>numpy <strong class="mq ir">as </strong>np<br/><strong class="mq ir">from </strong>sklearn.discriminant_analysis <strong class="mq ir">import </strong>LinearDiscriminantAnalysis<br/><br/><em class="mz"># Generate random Data of size (n x 5), with labels (n x 1).<br/></em>X = np.random.uniform(size=(200, 5))<br/>y = np.random.randint(low=0, high=5, size=(200, ))<br/><br/><em class="mz"># Number of component wanted. X after reduction will be (n x 3). <br/># Number of classes C = 5, so max number of n_components = 4.<br/></em>lda = LinearDiscriminantAnalysis(n_components=3)<br/>X_reduced = lda.fit_transform(X, y)</span></pre><h1 id="726a" class="li lj iq bd lk ll nb ln lo lp nc lr ls lt nd lv lw lx ne lz ma mb nf md me mf bi translated">因子分析— FA:</h1><p id="bfaa" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">在 FA 中，有带假设的建模，有建立这个模型的标准。FA 认为在我们观察到的特征背后有隐藏的因素。每个因素都会在一定程度上影响这些特征。这种假设从一个角度来看待特性，即每个特性都有自己的可变性，可以表示为一组三种类型；普通方差、特定方差和误差方差。公共方差是几个特征之间共享的方差(即同一组因素的结果)。当特征高度相关时，这种普通方差很高。特定方差是只对特定特征有贡献的因素的影响。误差方差是在观察期间由误差源产生的方差(例如，人为误差、测量误差)。)</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/dde24c4388f9076fa00cbeb0536b433a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*TChDlrl8MdkgKOsSfQNDTQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Factor 1&amp;2 are the source of the common variance between features. While the others are the source of the unique variance for each feature which is divided into specific variance and error variance.</figcaption></figure><p id="10c3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">FA job 是什么？找出最大化原始特征之间已解释的公共方差的因素。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="dbf9" class="mu lj iq mq b gy mv mw l mx my"><strong class="mq ir">import </strong>numpy <strong class="mq ir">as </strong>np<br/><strong class="mq ir">from </strong>sklearn.decomposition <strong class="mq ir">import </strong>FactorAnalysis<br/><br/><em class="mz"># Generate random Data of size (n x 5).<br/></em>X = np.random.uniform(low=0, high=100, size=(20, 5))</span><span id="93d6" class="mu lj iq mq b gy na mw l mx my"><em class="mz"># Number of factors wanted. The resulted factors are (n x 3).<br/></em>fa = FactorAnalysis(n_components=3)<br/>factors = fa.fit_transform(X)</span></pre><h1 id="f2c0" class="li lj iq bd lk ll nb ln lo lp nc lr ls lt nd lv lw lx ne lz ma mb nf md me mf bi translated">独立分量分析— ICA:</h1><p id="3290" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">对 ICA 最好的描述不是降维技术。最好将其描述为卷积/混合信号的分离/解混技术。ICA 的一个典型用例是“鸡尾酒会问题”，其中有独立的声源(例如扬声器)和几个用于录音的麦克风。使用麦克风的录音，可以通过 ICA 单独获得对原始声音的估计。<br/> ICA 假设源是非高斯独立的，并且依赖于 CLT，独立随机变量之和的分布趋于高斯。在每一步，ICA 改变基向量-投影方向-并测量所获得的源的非高斯性，并且在每一步，它使基向量更接近非高斯性(主要使用梯度下降)。在一些停止准则之后，它达到原始独立源的估计。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="d877" class="mu lj iq mq b gy mv mw l mx my"><strong class="mq ir">import </strong>numpy <strong class="mq ir">as </strong>np<br/><strong class="mq ir">from </strong>sklearn.decomposition <strong class="mq ir">import </strong>FastICA<br/><br/><em class="mz"># Generate random Data of size (n x 5).<br/></em>X = np.random.uniform(low=0, high=100, size=(20, 5))<br/><br/><em class="mz"># Number of sources wanted. The resulted sources are (n x 3).<br/></em>ica = FastICA(n_components=3)<br/>sources = ica.fit_transform(X)</span></pre><h1 id="4db6" class="li lj iq bd lk ll nb ln lo lp nc lr ls lt nd lv lw lx ne lz ma mb nf md me mf bi translated">多维标度— MDS:</h1><p id="3d91" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">如果您想在缩减后保留数据点之间的距离，那么 MDS 应该是您的选择。如果两个数据点在原始数据集中很接近，它们在缩减的数据集中也会很接近，反之亦然。这种方法只需要物体(点)的成对距离矩阵。所以，如果你只有这个距离矩阵，MDS 可以用来恢复你想要的维数的原始物体。</p><p id="b7d3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">MDS 可用于在 2D 或 3D 空间中可视化您的对象，以尽可能保留原始对象之间在更高维度中的距离。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="aec1" class="mu lj iq mq b gy mv mw l mx my">import numpy as np<br/><strong class="mq ir">from </strong>sklearn.manifold <strong class="mq ir">import </strong>MDS<br/><br/><em class="mz"># Generate random Data of size (n x 5).<br/></em>X = np.random.uniform(size=(20, 5))<br/><br/><em class="mz"># Number of component wanted = 3. X after reduction will be (n x 3).<br/></em>mds = MDS(n_components=3)<br/>X_reduced = mds.fit_transform(X)</span></pre><p id="52f3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要恢复数据的 2D 表示，请执行以下操作:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="1064" class="mu lj iq mq b gy mv mw l mx my"><em class="mz"># The recovered X will be of size (n x 2).<br/></em>mds = MDS(n_components=2, dissimilarity=<strong class="mq ir">'precomputed'</strong>)</span><span id="4339" class="mu lj iq mq b gy na mw l mx my"># D is the pair-wise distance matrix.<br/>X_recovered = mds.fit_transform(D)</span></pre></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="c750" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">总结:</h1><p id="3a55" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">PCA 不是唯一的降维方法，还有其他的。每一种都有其用途和目标。选择哪一个取决于应用程序。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="7d63" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">资源:</h1><ul class=""><li id="7194" class="ni nj iq kf b kg mg kk mh ko nk ks nl kw nm la nn no np nq bi translated">英格·科赫。<em class="mz">多元高维数据分析。</em>澳洲阿德莱德大学。</li><li id="2582" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">走向数据科学:<a class="ae kc" rel="noopener" target="_blank" href="/is-lda-a-dimensionality-reduction-technique-or-a-classifier-algorithm-eeed4de9953a"><em class="mz">LDA 是降维技术还是分类器算法？</em>T3】</a></li><li id="aaa3" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">stack exchange:<a class="ae kc" href="https://stats.stackexchange.com/questions/22884/how-does-linear-discriminant-analysis-reduce-the-dimensions" rel="noopener ugc nofollow" target="_blank">T5】线性判别分析如何降维？ </a></li><li id="af65" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated"><a class="ae kc" href="https://stats.idre.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/" rel="noopener ugc nofollow" target="_blank"> <em class="mz">因子分析实用入门</em>。</a>数字研究与教育研究所。</li><li id="2b2a" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">Aapo Hyvä rinen 和 Erkki Oja。<a class="ae kc" href="http://mlsp.cs.cmu.edu/courses/fall2012/lectures/ICA_Hyvarinen.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mz">独立成分分析:算法与应用</em> </a> <em class="mz">。</em>赫尔辛基理工大学。</li></ul></div></div>    
</body>
</html>