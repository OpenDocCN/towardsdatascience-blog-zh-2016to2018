<html>
<head>
<title>Overview of GANs (Generative Adversarial Networks) - Part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">生成性对抗网络概述——第一部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overview-of-gans-generative-adversarial-networks-part-i-ac78ec775e31?source=collection_archive---------5-----------------------#2017-10-25">https://towardsdatascience.com/overview-of-gans-generative-adversarial-networks-part-i-ac78ec775e31?source=collection_archive---------5-----------------------#2017-10-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="dc1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">看看我的</em> <a class="ae km" href="https://www.youtube.com/watch?v=3z8VSpBL6Vg&amp;list=PLSgGvve8UweFoMyAEFlFiE--JtWect5-T" rel="noopener ugc nofollow" target="_blank"> <em class="kl"> YouTube 上甘斯</em> </a> <em class="kl">的视频换个视角。本文原载于</em><a class="ae km" href="https://blog.zakjost.com/post/gans_overview_1/" rel="noopener ugc nofollow" target="_blank"><em class="kl"/></a></p><p id="bfe6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本系列文章的目的是提供 GAN 研究的概述并解释其贡献的性质。我自己是这个领域的新手，所以这肯定是不完整的，但希望它可以为其他新手提供一些快速的背景。</p><p id="9de7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于第一部分，我们将在高层次上介绍 GANs，并总结原始论文。如果您已经熟悉了基础知识，请随意跳到第二部分。假设你熟悉神经网络的基础知识。</p><h1 id="6fea" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">背景</h1><p id="3e6a" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated"><em class="kl">创成式</em>是什么意思？在高层次上，生成模型意味着您已经绘制了数据本身的概率分布。在图像的情况下，这意味着你有每一个可能的像素值组合的概率。这也意味着您可以通过从该分布中采样来生成新的数据点(即选择具有大概率的组合)。如果你在计算机视觉领域，这意味着你的模型可以从头开始创建新的图像。例如，这是一个生成的面。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/8d740c18e3931ecdc6ae502939bfc1a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*dmBYCaKMUGPIQuJwU7JNSw.png"/></div></figure><p id="cb1c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以防这还没有完全被理解:这不是一个真实的人，这是一张电脑发明的脸。GAN 可以做到这一点，因为它被给予了大量的面部图像来学习，这导致了概率分布。这个图像是从分布中提取的一个点。有了创成式模型，你可以创造出以前不存在的新东西。音频、文本、图像…等等。非常非常酷的东西。</p><h1 id="a2f6" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">原甘</h1><p id="b6d5" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">Ian Goodfellow 等人的<a class="ae km" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">原始论文</a>概述了基本方法，构建了理论基础并给出了一些示例基准。</p><p id="ff5a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">GANs 没有发明生成模型，而是提供了一种有趣而方便的学习方法。它们被称为“对抗性的”，因为问题的结构是两个实体相互竞争，而这两个实体都是机器学习模型。</p><p id="49b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最好用一个例子来解释。假设您想要构建一个面部图像生成器。你首先向一个系统输入一堆随机数，它将它们相加相乘，然后应用一些奇特的功能。最后，它输出每个像素的亮度值。这是你的<em class="kl">生成模型</em>——你给它噪音，它生成数据。现在，假设你这样做 10 次，得到 10 个不同的假图像。</p><p id="7719" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，你抓取 10 张真实面孔的图像。然后，你把假图像和真图像都输入到一个不同的叫做<em class="kl">鉴别器的模型中。它的工作是为每个输入图像输出一个数字，告诉你图像是真实的概率。开始时，生成的样本只是噪声，所以您可能认为这很容易，但鉴别器也一样糟糕，因为它也没有学到任何东西。</em></p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ly"><img src="../Images/2aaa899ffe5704e29350358c0d0ac8a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E1f3tapp4ImJ6pwqm5iOIQ.png"/></div></div></figure><p id="32a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于假图像上的每一个错误，鉴别者会受到惩罚，而生成者会得到奖励。基于对真实图像的正确分类，鉴别器也受到惩罚或奖励。这就是为什么它们被称为对抗性的——鉴别者的损失就是生产者的收益。久而久之，竞争导致相互提高。</p><p id="65d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，使用“网络”一词，因为作者使用神经网络来模拟发生器和鉴别器。这太棒了，因为它提供了一个简单的框架，使用惩罚/奖励来调整网络参数，使它们能够学习:熟悉的反向传播。</p><h1 id="bac7" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">理论基础</h1><p id="c2c8" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">我不会再现论文中所有血淋淋的细节，但值得一提的是，它们展示了两者:</p><ul class=""><li id="2544" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">优化目标 V(D，G)导致发电机概率分布与真实概率分布精确匹配。这意味着你的假例子是完美的，无法与真实的例子区分开来。</li></ul><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mm"><img src="../Images/1fbef6eebacc17b97ec41cd4a0e1d633.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yT6FLWSUUR8RrB3GWr_1xw.png"/></div></div></figure><ul class=""><li id="92ea" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">作者的梯度上升/下降训练算法收敛到这个最优值。所以你不仅知道你需要做什么，而且知道怎么做。</li></ul><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/1a4a85bb5af82a00870e94e6ac57ae30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*WnsJz9ZKS3dV3Nw36RtefQ.png"/></div></figure><p id="2f00" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了建立直觉，在上面的优化目标 V(D，G)中，D(x)项是鉴别器对以下问题的回答:输入 x 来自真实数据集的概率是多少？如果你把 G(z)代入这个函数，当你给它假数据时，它就是鉴别器的猜测。如果你分别考虑 D 和 G，你会发现 G 希望 V(D，G)小，而 D 希望这个大。这激发了算法中的梯度上升/下降技术。【E 的意思是“期望”，只是一个平均值。下标显示了你正在平均的概率分布，或者是真实的数据，或者是生成器变成假图像的噪声】。</p><p id="0819" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，他们提供的证明并不直接适用，因为我们是通过优化神经网络的<em class="kl">参数</em>来间接优化这些概率分布的，但很高兴知道该基金会有理论保证。</p><h1 id="60e0" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">结果</h1><p id="8fdd" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">值得注意的是，很难量化假数据的质量。如何判断假脸一代的进步？除此之外，当涉及到生成逼真的图像时，它们具有最先进的性能，这引起了很多关注。图像通常看起来没有其他方法模糊。</p><p id="8faa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尽管自最初的论文(将在第二部分中讨论)以来已经取得了巨大的进步，这里还是有一些例子:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mo"><img src="../Images/334f77779ba07ee7018481a6387137d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QwQ0_7lA6G2J1HdZVgMpzw.png"/></div></div></figure><h1 id="8e63" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">问题</h1><p id="d9b8" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">原始 GAN 实施的最大问题是:</p><ul class=""><li id="01a8" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">训练困难。有时模型永远不会学到任何东西或收敛到局部极小值。</li><li id="0e63" class="md me iq jp b jq mp ju mq jy mr kc ms kg mt kk mi mj mk ml bi translated">“模式崩溃”，即生成器本质上一遍又一遍地输出同样的东西。</li></ul><p id="0fcb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些问题通过对架构的改进得到了解决，并将在以后的文章中提出。</p><h1 id="0157" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">结论</h1><p id="12f1" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">最终，这个框架允许我们以<em class="kl">无监督</em>的方式使用神经网络的正常监督学习方法。这是因为我们的标签很容易生成，因为我们知道哪些数据来自训练集，哪些数据是生成的。值得注意的是，在上面的手写数字图像中，数字标签本身并没有在训练中使用。尽管如此，生成器和鉴别器都能够学习数据的有用表示，正如生成器模拟数据的能力所证明的那样。</p><p id="f637" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在第二部分中，我们将讨论如何解决许多训练问题，以及在真实图像生成方面做出巨大的改进。</p></div></div>    
</body>
</html>