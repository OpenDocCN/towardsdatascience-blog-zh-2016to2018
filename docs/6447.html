<html>
<head>
<title>The Naïve Bayes Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朴素贝叶斯分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-naive-bayes-classifier-e92ea9f47523?source=collection_archive---------7-----------------------#2018-12-14">https://towardsdatascience.com/the-naive-bayes-classifier-e92ea9f47523?source=collection_archive---------7-----------------------#2018-12-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="7b31" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">约瑟夫·卡坦扎里特</p><p id="87ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko"/><a class="ae kp" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank"><em class="ko">朴素贝叶斯分类器</em> </a> <em class="ko">或许是用来构建、训练和预测的最简单的机器学习分类器。这篇文章将展示它是如何工作的以及为什么工作。</em> <strong class="js iu"> <em class="ko">第一部分</em> </strong> <em class="ko">揭示了广为人知的</em> <a class="ae kp" href="https://simple.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank"> <em class="ko">贝叶斯法则</em> </a> <em class="ko">只是简单陈述了关于</em> <a class="ae kp" href="https://en.wikipedia.org/wiki/Joint_probability_distribution" rel="noopener ugc nofollow" target="_blank"> <em class="ko">联合</em></a><em class="ko"/><a class="ae kp" href="https://en.wikipedia.org/wiki/Conditional_probability" rel="noopener ugc nofollow" target="_blank"><em class="ko">条件</em> </a> <em class="ko">的概率。但它的平淡掩盖了惊人的力量，正如我们将在</em> <strong class="js iu"> <em class="ko">第 2 和第 3 部分、</em> </strong> <em class="ko">中看到的，在那里我们组装了机械的</em> <a class="ae kp" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank"> <em class="ko">朴素贝叶斯分类器</em> </a> <em class="ko">。</em> <strong class="js iu"> <em class="ko">第四部分</em> </strong> <em class="ko">是一个简要的讨论，而</em> <strong class="js iu"> <em class="ko">第五部分和第六部分</em> </strong> <em class="ko">列举了</em> <a class="ae kp" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank"> <em class="ko">朴素贝叶斯分类器</em> </a> <em class="ko">的优缺点。</em> <strong class="js iu"> <em class="ko">第 7 部分</em> </strong> <em class="ko">是总结，</em> <strong class="js iu"> <em class="ko">第 8 部分</em> </strong> <em class="ko">列举了几个我觉得有用的参考文献。欢迎建设性的意见、批评和建议！</em></p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi kq"><img src="../Images/864dd173ae4229137f5269c916e2506b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NpQf9G3ZdnMXLT-QT3sErQ.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Reverend Thomas Bayes</figcaption></figure><h1 id="cae5" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">1.前奏:贝叶斯法则</h1><p id="ac8d" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">给定两个事件 A 和 B，<a class="ae kp" href="https://en.wikipedia.org/wiki/Joint_probability_distribution" rel="noopener ugc nofollow" target="_blank">联合概率</a> P(A，B)是 A 和 B 一起发生的概率。它可以用两种方式之一来写:</p><p id="9eb0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第一种方式:</strong></p><blockquote class="mj"><p id="0079" class="mk ml it bd mm mn mo mp mq mr ms kn dk translated"><em class="mt"> P(A，B) = P(A|B) * P(B) </em></p></blockquote><p id="b113" class="pw-post-body-paragraph jq jr it js b jt mu jv jw jx mv jz ka kb mw kd ke kf mx kh ki kj my kl km kn im bi translated">这里 P(A|B)是一个<a class="ae kp" href="https://en.wikipedia.org/wiki/Conditional_probability" rel="noopener ugc nofollow" target="_blank">条件概率</a>:假设 B 已经发生，A 发生的概率。这就说 A 和 B 一起发生的概率是(假设 B 已经发生，A 发生的概率)乘以(B 已经发生的概率)。</p><p id="32ee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第二种方式:</strong></p><p id="c30a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将前一等式中的 A 和 B 互换，得到一个等价的表达式:</p><blockquote class="mj"><p id="9486" class="mk ml it bd mm mn mo mp mq mr ms kn dk translated"><em class="mt"> P(B，A) = P(B|A) * P(A) </em></p></blockquote><p id="d541" class="pw-post-body-paragraph jq jr it js b jt mu jv jw jx mv jz ka kb mw kd ke kf mx kh ki kj my kl km kn im bi translated">注意，P(B，A)和 P(A，B)是相等的，因为联合概率不依赖于顺序。相等的两个方程的右边产生著名的<a class="ae kp" href="https://simple.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯法则</a>:</p><blockquote class="mj"><p id="0eaf" class="mk ml it bd mm mn mo mp mq mr ms kn dk translated">P(A|B) * P(B) <em class="mt"> = </em> P(B|A) * P(A)</p></blockquote><h1 id="faf9" class="lg lh it bd li lj lk ll lm ln lo lp lq lr mz lt lu lv na lx ly lz nb mb mc md bi translated">2.如何构建贝叶斯分类器</h1><p id="1019" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">假设我们有一个由 N 个数据点组成的数据集，每个数据点有 M 个特征，被标记为一组 K 个类中的一个。给定特征值，有一个可以预测类别标签的分类器就好了，对吧？你想一下，条件类概率 P(C|D)就是那张票！我们只需要添加一个<em class="ko">决策规则</em>来告诉我们如何选择类标签。</p><p id="e625" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">条件类概率来自<a class="ae kp" href="https://simple.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯法则</a>的应用:</p><blockquote class="mj"><p id="04ee" class="mk ml it bd mm mn mo mp mq mr ms kn dk translated"><em class="mt">P(C | D)= P(C)* P(D | C)/P(D)</em></p></blockquote><p id="bc29" class="pw-post-body-paragraph jq jr it js b jt mu jv jw jx mv jz ka kb mw kd ke kf mx kh ki kj my kl km kn im bi translated">其中:</p><p id="e8e9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">c 是一个类，D 是一个数据点，由 M 个特征中的每一个的值组成。</p><p id="943d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae kp" href="https://simple.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯规则</a>表达式中的术语从左至右为:</p><ul class=""><li id="03f1" class="nc nd it js b jt ju jx jy kb ne kf nf kj ng kn nh ni nj nk bi translated"><a class="ae kp" href="https://en.wikipedia.org/wiki/Posterior_probability" rel="noopener ugc nofollow" target="_blank">后验</a> P(C|D)是数据点 D 在类别 C 中的概率</li><li id="d5b3" class="nc nd it js b jt nl jx nm kb nn kf no kj np kn nh ni nj nk bi translated"><a class="ae kp" href="https://en.wikipedia.org/wiki/Prior_probability" rel="noopener ugc nofollow" target="_blank">先验</a> P(C)是 C 类在数据集中出现的概率，即具有 C 类的数据点的比例</li><li id="5d3c" class="nc nd it js b jt nl jx nm kb nn kf no kj np kn nh ni nj nk bi translated"><a class="ae kp" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">可能性</a> P(D|C)是给定类为 C 的情况下，我们观察到数据点 D 的相对概率</li><li id="9f0f" class="nc nd it js b jt nl jx nm kb nn kf no kj np kn nh ni nj nk bi translated"><a class="ae kp" href="https://en.wikipedia.org/wiki/Bayesian_inference" rel="noopener ugc nofollow" target="_blank">证据</a> P(D)是数据点 D 在自然界出现的概率</li></ul><p id="cfe0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">实际上，P(D)通常很难或不可能计算。不要担心！因为 P(D)独立于 C 类，就我们而言，它只是一个比例常数，我们并不关心。</p><p id="8c48" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">不知道 P(D)的唯一后果是后验概率将是一个<em class="ko">相对</em>概率，而不是一个<em class="ko">真实</em>概率。但这也没什么，posterio  r 的<em class="ko">相对</em>值足以进行分类预测。</p><p id="4262" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了完成我们的分类器的规范，我们采用了<a class="ae kp" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank">映射</a>(最大后验概率)<em class="ko">决策规则</em>，该规则将标签分配给具有最高<a class="ae kp" href="https://en.wikipedia.org/wiki/Posterior_probability" rel="noopener ugc nofollow" target="_blank">后验概率</a>的类。</p><p id="c6fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了应用分类器，我们<br/> 1。计算每个类的<a class="ae kp" href="https://en.wikipedia.org/wiki/Posterior_probability" rel="noopener ugc nofollow" target="_blank"> posterio </a> r，然后<br/> 2。应用<a class="ae kp" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank">映射</a>决策规则来预测分类标签</p><h1 id="533f" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">3.朴素贝叶斯分类器</h1><p id="a8e1" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">采用“<a class="ae kp" href="https://simple.wikipedia.org/wiki/Na%C3%AFve" rel="noopener ugc nofollow" target="_blank">天真的</a>”假设，即特征值对于给定类别的成员是独立的，可以极大地简化<a class="ae kp" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">可能性</a>的计算。假设数据点 D 的 M 个特征值具有值(x1，x2，..，xM)。然后在“天真”的假设下，<a class="ae kp" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">可能性</a>成为每个特征的独立<a class="ae kp" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">可能性</a>的乘积:</p><blockquote class="mj"><p id="d780" class="mk ml it bd mm mn mo mp mq mr ms kn dk translated"><em class="mt"> P(D|C) = P(x1，x2，..，xM|C) = ∏{i=1 到 M}P(xi|C) </em></p></blockquote><p id="9996" class="pw-post-body-paragraph jq jr it js b jt mu jv jw jx mv jz ka kb mw kd ke kf mx kh ki kj my kl km kn im bi translated">独立的<a class="ae kp" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">可能性</a>易于计算离散特征:P(xi|C)是 C 类中所有数据点的比例，这些数据点在具有特征的<em class="ko">中具有 xi 值。</em></p><p id="e864" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过这种简化，<a class="ae kp" href="https://simple.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯法则</a>变成了</p><blockquote class="mj"><p id="557b" class="mk ml it bd mm mn mo mp mq mr ms kn dk translated"><em class="mt"> P(C|D) ∝ P(C) * ∏{i=1 到 M}P(xi|C) </em></p></blockquote><p id="54a4" class="pw-post-body-paragraph jq jr it js b jt mu jv jw jx mv jz ka kb mw kd ke kf mx kh ki kj my kl km kn im bi translated">上述公式受到<a class="ae kp" href="https://en.wikipedia.org/wiki/Arithmetic_underflow" rel="noopener ugc nofollow" target="_blank">下溢误差</a>的影响，因为它通常是非常小的数的乘积。取对数将小数字的乘积转换为普通大小数字的和，同时保持正数的顺序(即，如果 A &gt; B，则 logA &gt; logB)。有了这个补救措施，我们就有了一个实用的计算版本的<a class="ae kp" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯分类器</a>，它很容易用代码实现:</p><blockquote class="mj"><p id="00b3" class="mk ml it bd mm mn mo mp mq mr ms kn dk translated"><em class="mt"> logP(C|D) = logP(C) + ∑{i=1 到 M}logP(xi|C) </em></p></blockquote><p id="9b3a" class="pw-post-body-paragraph jq jr it js b jt mu jv jw jx mv jz ka kb mw kd ke kf mx kh ki kj my kl km kn im bi translated">我们忽略了右边比例常数的对数；由于我们只对<a class="ae kp" href="https://en.wikipedia.org/wiki/Posterior_probability" rel="noopener ugc nofollow" target="_blank">后验</a>的相对值感兴趣，这不会影响课堂作业。</p><p id="5750" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于具有离散值的特征(如单词),求和中带有项的<em class="ko">是 C 类数据点比例的对数，其带有</em>特征值的<em class="ko">等于 xi。万一没有呢？那项变成 log(0)，方程爆炸，我们无法做出预测。为了确保这种情况不会发生，我们可以在每个<a class="ae kp" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">可能性</a>因子的分子和分母上加 1。这是拉普拉斯平滑的变体。</em></p><h1 id="34a6" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">4.讨论</h1><p id="7310" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">如果有连续值特征，如身高和体重，<a class="ae kp" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯</a>可用于回归。在这种情况下，训练数据被划分到它的类中。在每个类中，每个特征的值都被假设为<a class="ae kp" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">正态分布</a>；<a class="ae kp" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Gaussian_naive_Bayes" rel="noopener ugc nofollow" target="_blank">高斯</a>的参数是特征值的均值和方差。或者，可以通过宁滨值对连续要素进行离散化，但这样做会丢弃信息，并且结果可能对宁滨方案很敏感。</p><p id="3ef8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每当“天真”假设成立时，<a class="ae kp" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯分类器</a>是最优的(意味着它是最准确的可能分类器)，甚至在某些情况下<a class="ae kp" href="http://web.cs.ucdavis.edu/~vemuri/classes/ecs271/Bayesian.pdf" rel="noopener ugc nofollow" target="_blank">不成立</a>。在大多数情况下，假设不成立，结果是<a class="ae kp" href="https://en.wikipedia.org/wiki/Posterior_probability" rel="noopener ugc nofollow" target="_blank">后验</a>概率不准确。令人惊讶的是，使用<a class="ae kp" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank">图</a>决策规则进行的分类通常相当准确，即使概率并不准确！</p><p id="1ccd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当训练数据代表母体时，朴素贝叶斯应该工作得最好，以便<a class="ae kp" href="https://en.wikipedia.org/wiki/Prior_probability" rel="noopener ugc nofollow" target="_blank">先验</a>是准确的。</p><h1 id="ba6e" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">5.朴素贝叶斯分类器的优势</h1><ul class=""><li id="9d60" class="nc nd it js b jt me jx mf kb nq kf nr kj ns kn nh ni nj nk bi translated">对于少量训练数据的问题，它可以获得比其他分类器更好的结果，因为它具有较低的过拟合倾向。那是<a class="ae kp" href="https://en.wikipedia.org/wiki/Occam%27s_razor" rel="noopener ugc nofollow" target="_blank">奥卡姆剃刀</a>在起作用！</li><li id="7795" class="nc nd it js b jt nl jx nm kb nn kf no kj np kn nh ni nj nk bi translated">训练很快，包括计算<a class="ae kp" href="https://en.wikipedia.org/wiki/Prior_probability" rel="noopener ugc nofollow" target="_blank">先验</a>和<a class="ae kp" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">可能性</a></li><li id="e44e" class="nc nd it js b jt nl jx nm kb nn kf no kj np kn nh ni nj nk bi translated"><strong class="js iu">对新数据点的预测</strong>很快。首先计算每个类的<a class="ae kp" href="https://en.wikipedia.org/wiki/Posterior_probability" rel="noopener ugc nofollow" target="_blank">后验</a>。然后应用<a class="ae kp" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank">映射</a> <em class="ko">决策规则</em>:标签是具有最大<a class="ae kp" href="https://en.wikipedia.org/wiki/Posterior_probability" rel="noopener ugc nofollow" target="_blank">后验</a>的类。</li><li id="d8b5" class="nc nd it js b jt nl jx nm kb nn kf no kj np kn nh ni nj nk bi translated">RAM 内存的占用是适度的，因为这些操作不需要一次将整个数据集保存在 RAM 中。</li><li id="c1e7" class="nc nd it js b jt nl jx nm kb nn kf no kj np kn nh ni nj nk bi translated"><strong class="js iu"> CPU 使用率</strong>适中:没有梯度或迭代参数更新需要计算，因为预测和训练仅使用解析公式。</li><li id="29b2" class="nc nd it js b jt nl jx nm kb nn kf no kj np kn nh ni nj nk bi translated">与特征数量和数据点数量成线性比例，并且易于用新的训练数据进行更新。</li><li id="9dfb" class="nc nd it js b jt nl jx nm kb nn kf no kj np kn nh ni nj nk bi translated">因为它的线性缩放、快速执行、较小的内存需求和较低的 CPU 使用率，可以为其他方法计算量太大的大量问题(许多行和列)提供可行的解决方案。</li><li id="93cb" class="nc nd it js b jt nl jx nm kb nn kf no kj np kn nh ni nj nk bi translated"><a class="ae kp" href="https://qr.ae/TUtuL0" rel="noopener ugc nofollow" target="_blank">轻松处理缺失的特征值</a> —在没有该特征的情况下通过重新训练和预测！</li></ul><h1 id="6e2a" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">6.朴素贝叶斯分类器的缺点</h1><ul class=""><li id="853e" class="nc nd it js b jt me jx mf kb nq kf nr kj ns kn nh ni nj nk bi translated">无法合并功能交互。</li><li id="aec3" class="nc nd it js b jt nl jx nm kb nn kf no kj np kn nh ni nj nk bi translated">对于回归问题，即连续的实值数据，可能没有好的方法来计算<a class="ae kp" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">的可能性</a>。对数据进行宁滨并将离散类分配给箱是次优的，因为它丢弃了信息。假设每个特性都是正态分布的是可行的，但是如果特性不是正态分布的，可能会影响性能。另一方面，有了每个类中足够的训练数据，您可以直接估计<a class="ae kp" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">可能性</a>密度，允许对新数据进行精确的<a class="ae kp" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">可能性</a>计算。</li><li id="0aa1" class="nc nd it js b jt nl jx nm kb nn kf no kj np kn nh ni nj nk bi translated">性能对倾斜的数据很敏感，也就是说，当训练数据不代表总体中的类别分布时。在这种情况下，<a class="ae kp" href="https://en.wikipedia.org/wiki/Prior_probability" rel="noopener ugc nofollow" target="_blank">之前的</a>估计将是不正确的。</li></ul><h1 id="ef55" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">7.摘要</h1><p id="d1ac" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">在这篇文章中，你已经学会了如何从头开始构建一个朴素贝叶斯分类器。你现在被授权运用<a class="ae kp" href="https://en.wikipedia.org/wiki/Occam%27s_razor" rel="noopener ugc nofollow" target="_blank">奥卡姆剃刀</a>从观测数据中推断<a class="ae kp" href="https://en.wikipedia.org/wiki/Informal_inferential_reasoning" rel="noopener ugc nofollow" target="_blank">宇宙的真相！我希望你已经看到了一丝曙光，为什么贝叶斯规则是有史以来最重要的实用数学发现之一。现在，向前去发现…</a></p><h1 id="2b83" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">8.参考</h1><p id="6fc3" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">a.<a class="ae kp" href="https://web.stanford.edu/class/cs124/lec/naivebayes.pdf" rel="noopener ugc nofollow" target="_blank">文本分类应用(斯坦福 CS124) </a></p><p id="8c2a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">b.<a class="ae kp" href="https://dzone.com/articles/naive-bayes-tutorial-naive-bayes-classifier-in-pyt" rel="noopener ugc nofollow" target="_blank">一个不错的教程，有一个工作示例和代码</a></p><p id="f8cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">c.<a class="ae kp" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank">Na</a>T22】μT24】ve Bayes 分类器，维基百科</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/c8557f4e3f0bdc5becffc18a0a6bc79f.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*eC3B5wLW8dmUTFxIA0dtxg.jpeg"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Bayes’ Rule</figcaption></figure></div></div>    
</body>
</html>