<html>
<head>
<title>Implementation of Multi-Variate Linear Regression in Python using Gradient Descent Optimization from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始使用梯度下降优化在 Python 中实现多变量线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementation-of-multi-variate-linear-regression-in-python-using-gradient-descent-optimization-b02f386425b9?source=collection_archive---------5-----------------------#2018-12-26">https://towardsdatascience.com/implementation-of-multi-variate-linear-regression-in-python-using-gradient-descent-optimization-b02f386425b9?source=collection_archive---------5-----------------------#2018-12-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="869f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习、实施和调整…</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ee64750c4671c38e41424aa54091cddb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0YerV0ALeYxCehORHoblOw.jpeg"/></div></div></figure><p id="f8fc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">机器学习的大多数实际应用涉及目标结果所依赖的多个特征。类似地，在回归分析问题中，存在目标结果依赖于许多特征的情况。多元线性回归是解决这类问题的一种可能的方法。在本文中，我将讨论多变量(多特征)线性回归，它的 Python 实现，在一个实际问题上的应用和性能分析。</p><p id="a910" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于它是一种“线性”回归技术，在构建假设时，将只采用每个特征的<strong class="kw iu">线性</strong>项。设 x_1，x_2，… x_n 是目标结果所依赖的特征。然后，多元线性回归的假设:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lq"><img src="../Images/b66d5e039a37cbaa0ff5d01900c4b6a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CySZafLwRhQI2SEljpaLlg.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">where <strong class="bd lv">theta_0, theta_1, theta_2, theta_3,…., theta_n </strong>are the parameters</figcaption></figure><p id="8ed6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">此外，上述假设也可以根据向量代数重新构建:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/c6729ed2ecf89a40006bc23fa3b67d92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_eZxSV8XLI_N2iTG6gESdA.png"/></div></div></figure><p id="2287" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">还有一个与依赖于参数θ_ 0，θ_ 1，θ_ 2，…，θ_ n 的假设相关的成本函数(或损失函数)</p><p id="e87f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里的成本函数与多项式回归的情况相同[1]。</p><p id="3b7c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，这些参数θ_ 0，θ_ 1，θ_ 2，…，θ_ n 必须采用这样的值，对于这些值，成本函数(或简单地成本)达到其可能的最小值。换句话说，必须找出成本函数的最小值。</p><p id="5de3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这种情况下，分批梯度下降可用作优化策略。</p><p id="6866" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">使用批量梯度下降实现多元线性回归:</strong></p><p id="0ce2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">实现是通过创建 3 个模块来完成的，每个模块用于在培训过程中执行不同的操作。</p><p id="82ab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">=&gt; hypothesis():是计算并输出目标变量的假设值的函数，给定 theta (theta_0，theta_1，theta_2，theta_3，…，theta_n)，矩阵中的特征，维数为[m X (n+1)]的 X 其中 m 是样本数，n 是特征数。假设()的实现如下所示:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="3889" class="mc md it ly b gy me mf l mg mh">def <strong class="ly iu">hypothesis</strong>(theta, X, n):<br/>    h = np.ones((X.shape[0],1))<br/>    theta = theta.reshape(1,n+1)<br/>    for i in range(0,X.shape[0]):<br/>        h[i] = float(np.matmul(theta, X[i]))<br/>    h = h.reshape(X.shape[0])<br/>    return h</span></pre><p id="4504" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">=&gt;BGD():它是执行批量梯度下降算法的函数，将当前的θ值(theta_0，theta_1，…，theta_n)、学习速率(alpha)、迭代次数(num_iters)、所有样本的假设值列表(h)、特征集(X)、目标变量集(y)和特征数(n)作为输入，并输出优化的 theta (theta_0，theta_1，theta_2，theta_3，…，theta_n)和包含所有迭代的成本函数值的成本历史或成本 BGD()的实现如下所示:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="21e5" class="mc md it ly b gy me mf l mg mh">def <strong class="ly iu">BGD</strong>(theta, alpha, num_iters, h, X, y, n):<br/>    cost = np.ones(num_iters)<br/>    for i in range(0,num_iters):<br/>        theta[0] = theta[0] - (alpha/X.shape[0]) * sum(h - y)<br/>        for j in range(1,n+1):<br/>            theta[j] = theta[j] - (alpha/X.shape[0]) * <br/>                                   sum((h-y) * X.transpose()[j])<br/>        h = hypothesis(theta, X, n)<br/>        cost[i] = (1/X.shape[0]) * 0.5 * sum(np.square(h - y))<br/>    theta = theta.reshape(1,n+1)<br/>    return theta, cost</span></pre><p id="90f1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">=&gt;linear_regression():它是主函数，以特征矩阵(X)、目标变量向量(y)、学习速率(alpha)和迭代次数(num_iters)作为输入，输出最终优化的 theta，即[ <strong class="kw iu"> theta_0，theta_1，theta_2，theta_3，…]的值。成本函数在批量梯度下降后几乎达到最小值的θ_ n</strong><strong class="kw iu"/>和存储每次迭代的成本值的<strong class="kw iu">成本</strong>。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="f87e" class="mc md it ly b gy me mf l mg mh">def <strong class="ly iu">linear_regression</strong>(X, y, alpha, num_iters):<br/>    n = X.shape[1]<br/>    one_column = np.ones((X.shape[0],1))<br/>    X = np.concatenate((one_column, X), axis = 1)<br/>    # initializing the parameter vector...<br/>    theta = np.zeros(n+1)<br/>    # hypothesis calculation....<br/>    h = hypothesis(theta, X, n)<br/>    # returning the optimized parameters by Gradient Descent...<br/>    theta, cost = BGD(theta,alpha,num_iters,h,X,y,n)<br/>    return theta, cost</span></pre></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><p id="8684" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，让我们继续讨论多元线性回归在实际数据集上的应用。</p><p id="88b8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们考虑俄勒冈州波特兰市的房价数据。它包含房子的大小(平方英尺)和卧室的数量作为特征，房子的价格作为目标变量。该数据集可从以下网址获得:</p><div class="mp mq gp gr mr ms"><a href="https://github.com/navoneel1092283/multivariate_regression.git" rel="noopener  ugc nofollow" target="_blank"><div class="mt ab fo"><div class="mu ab mv cl cj mw"><h2 class="bd iu gy z fp mx fr fs my fu fw is bi translated">navoneel 1092283/多元回归</h2><div class="mz l"><h3 class="bd b gy z fp mx fr fs my fu fw dk translated">通过在 GitHub 上创建帐户，为 navoneel 1092283/multivariate _ regression 开发做出贡献。</h3></div><div class="na l"><p class="bd b dl z fp mx fr fs my fu fw dk translated">github.com</p></div></div><div class="nb l"><div class="nc l nd ne nf nb ng ks ms"/></div></div></a></div><p id="38a6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">问题陈述</strong>:<em class="nh">给定房子的大小和卧室数量，分析预测房子可能的价格"</em></p><p id="7549" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">数据读入 Numpy 数组</strong>:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="7be0" class="mc md it ly b gy me mf l mg mh">data = np.loadtxt('data2.txt', delimiter=',')<br/>X_train = data[:,[0,1]] #feature set<br/>y_train = data[:,2] #label set</span></pre><p id="ceac" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">特征标准化或特征缩放</strong>:</p><p id="57c5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这包括缩放特征以实现快速高效的计算。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e6e6a77ddd43662c2810c5b89cb8d4b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*0kO2HpKGl_B1UVkNMSmueQ.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Standard Feature Scaling Strategy</figcaption></figure><p id="4385" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中 u 是平均值，sigma 是标准偏差:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/30ac67150516a5bf32398d9bce30331e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DqHqxjFBQEB0P08Cp5y2Aw.png"/></div></div></figure><p id="139d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">特征缩放的实现:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="0ad1" class="mc md it ly b gy me mf l mg mh">mean = np.ones(X_train.shape[1])<br/>std = np.ones(X_train.shape[1])</span><span id="faef" class="mc md it ly b gy nk mf l mg mh">for i in range(0, X_train.shape[1]):<br/>    mean[i] = np.mean(X_train.transpose()[i])<br/>    std[i] = np.std(X_train.transpose()[i])<br/>    for j in range(0, X_train.shape[0]):<br/>        X_train[j][i] = (X_train[j][i] - mean[i])/std[i]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/493d30ce168f75928c203bbb0085b2c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*yRDpf9zh7ZWB4YthrPMV8w.png"/></div></figure><p id="f2c8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，</p><ol class=""><li id="1fa1" class="nm nn it kw b kx ky la lb ld no lh np ll nq lp nr ns nt nu bi translated">特征“房子的大小(平方。英尺)或 F1: 2000.6808</li><li id="a043" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">特征“床位数”的平均值或 F2: 3.1702</li><li id="2797" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">F1 的标准偏差:7.86202619e+02</li><li id="3eac" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">F2 的标准偏差:7.52842809e-01</li></ol><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="237e" class="mc md it ly b gy me mf l mg mh"># calling the principal function with <strong class="ly iu">learning_rate = 0.0001 </strong>and <br/># <strong class="ly iu">num_iters = 300000</strong><br/>theta, cost = linear_regression(X_train, y_train,<br/>                                               0.0001, 300000)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/f4860cc8df4e628953ad77a1a387c075.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*xvgbJD4dQ_odVEAegaR4og.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">theta after Multi-Variate Linear Regression</figcaption></figure><p id="0177" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在分批梯度下降迭代过程中，成本得到了降低。成本的降低借助于曲线显示出来。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="d0df" class="mc md it ly b gy me mf l mg mh">import matplotlib.pyplot as plt<br/>cost = list(cost)<br/>n_iterations = [x for x in range(1,300001)]<br/>plt.plot(n_iterations, cost)<br/>plt.xlabel('No. of iterations')<br/>plt.ylabel('Cost')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/cf513066a98ea50b5f54032d41d0b10e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*e4KnjsP2DY3GerMcz2Mujg.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Line Curve Representation of Cost Minimization using BGD in Multi-Variate Linear Regression</figcaption></figure><p id="8526" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">使用三维散点图并排显示特征和目标变量实际值和预测值:</strong></p><p id="4a64" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">= &gt;实际目标变量可视化:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="2333" class="mc md it ly b gy me mf l mg mh">from matplotlib import pyplot<br/>from mpl_toolkits.mplot3d import Axes3D</span><span id="4fb8" class="mc md it ly b gy nk mf l mg mh">sequence_containing_x_vals = list(X_train.transpose()[0])<br/>sequence_containing_y_vals = list(X_train.transpose()[1])<br/>sequence_containing_z_vals = list(y_train)</span><span id="3958" class="mc md it ly b gy nk mf l mg mh">fig = pyplot.figure()<br/>ax = Axes3D(fig)</span><span id="33a5" class="mc md it ly b gy nk mf l mg mh">ax.scatter(sequence_containing_x_vals, sequence_containing_y_vals,<br/>           sequence_containing_z_vals)<br/>ax.set_xlabel('Living Room Area', fontsize=10)<br/>ax.set_ylabel('Number of Bed Rooms', fontsize=10)<br/>ax.set_zlabel('Actual Housing Price', fontsize=10)</span></pre><p id="01d1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">= &gt;预测目标变量可视化:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="993d" class="mc md it ly b gy me mf l mg mh"># Getting the predictions...<br/>X_train = np.concatenate((np.ones((X_train.shape[0],1)), X_train)<br/>                         ,axis = 1)<br/>predictions = hypothesis(theta, X_train, X_train.shape[1] - 1)</span><span id="13b7" class="mc md it ly b gy nk mf l mg mh">from matplotlib import pyplot<br/>from mpl_toolkits.mplot3d import Axes3D</span><span id="1568" class="mc md it ly b gy nk mf l mg mh">sequence_containing_x_vals = list(X_train.transpose()[1])<br/>sequence_containing_y_vals = list(X_train.transpose()[2])<br/>sequence_containing_z_vals = list(predictions)</span><span id="6689" class="mc md it ly b gy nk mf l mg mh">fig = pyplot.figure()<br/>ax = Axes3D(fig)</span><span id="b776" class="mc md it ly b gy nk mf l mg mh">ax.scatter(sequence_containing_x_vals, sequence_containing_y_vals,<br/>           sequence_containing_z_vals)<br/>ax.set_xlabel('Living Room Area', fontsize=10)<br/>ax.set_ylabel('Number of Bed Rooms', fontsize=10)<br/>ax.set_zlabel('Housing Price Predictions', fontsize=10)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/a0dd2ecdda17e29e96d79106f5b0ee0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9-aUaYM4sE0z6UFH9zbk_Q.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk"><strong class="bd lv">Actual Housing Price Vs Predicted Housing Price</strong></figcaption></figure><p id="cb3f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">性能分析:</strong></p><ol class=""><li id="fa66" class="nm nn it kw b kx ky la lb ld no lh np ll nq lp nr ns nt nu bi translated">平均绝对误差:51502.7803(美元)</li><li id="cc0b" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">均方差:4086560101.2158(美元平方)</li><li id="446a" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">均方根误差:63926.2082(美元)</li><li id="a62b" class="nm nn it kw b kx nv la nw ld nx lh ny ll nz lp nr ns nt nu bi translated">r 平方得分:0.7329</li></ol><p id="ebdd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">需要注意的一点是，平均绝对误差、均方误差和均方根误差不是无单位的。为了使它们无单位，在训练模型之前，可以用缩放特征的相同方式缩放目标标注。除此之外，还获得了 0.7329 的下降 R 平方分数。</p><p id="bbf5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就是使用梯度下降法在 Python 中实现多元线性回归的全部内容。</p><p id="87e9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">参考文献</strong></p><p id="e358" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[1]<a class="ae od" rel="noopener" target="_blank" href="/implementation-of-uni-variate-linear-regression-in-python-using-gradient-descent-optimization-from-3491a13ca2b0">https://towards data science . com/implementation-of-uni-variable-linear-regression-in-python-using-gradient-descent-optimization-from-3491 a13c a2 b 0</a></p></div></div>    
</body>
</html>