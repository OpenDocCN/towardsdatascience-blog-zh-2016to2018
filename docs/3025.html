<html>
<head>
<title>Hierarchical Clustering on Categorical Data in R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">R 中分类数据的层次聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hierarchical-clustering-on-categorical-data-in-r-a27e578f2995?source=collection_archive---------0-----------------------#2018-04-01">https://towardsdatascience.com/hierarchical-clustering-on-categorical-data-in-r-a27e578f2995?source=collection_archive---------0-----------------------#2018-04-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="b5d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated">这是我第一次尝试在真实数据上执行客户聚类，这是一次宝贵的经历。虽然网上关于使用数值变量进行聚类的文章和博客文章很多，但我花了一些时间来寻找分类数据的解决方案，如果你想到这一点，这确实不那么简单。分类数据聚类的方法仍在开发中——我将在另一篇文章中尝试这两种方法。</p><p id="dde6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一方面，我也遇到过这样的观点，对分类数据进行聚类可能不会产生合理的结果——这在一定程度上是正确的(在 CrossValidated 上有一个<a class="ae ku" href="https://stats.stackexchange.com/questions/218604/with-categorical-data-can-there-be-clusters-without-the-variables-being-related" rel="noopener ugc nofollow" target="_blank">惊人的讨论)。在某一点上，我想“我在做什么，为什么不把它全部分成几组呢？”但是群组分析并不总是明智的，特别是在您获得更多级别的分类变量的情况下，您可以轻松浏览 5-7 个群组，这可能很容易，但是如果您有 22 个变量，每个变量有 5 个级别(比如，这是一个离散分数为 1，2，3，4，5 的客户调查)，并且您需要了解您有哪些独特的客户群，您将有 22×5 个群组。没人想这么做。聚类似乎是有用的。所以这篇文章是关于分享我希望在开始研究集群时遇到的事情。</a></p><p id="d13c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">聚类过程本身包含 3 个不同的步骤:</p><ol class=""><li id="2831" class="kv kw iq jp b jq jr ju jv jy kx kc ky kg kz kk la lb lc ld bi translated">计算相异度矩阵——可以说是聚类中最重要的决定，你所有的后续步骤都将基于你所做的相异度矩阵。</li><li id="52d3" class="kv kw iq jp b jq le ju lf jy lg kc lh kg li kk la lb lc ld bi translated">选择聚类方法</li><li id="13f8" class="kv kw iq jp b jq le ju lf jy lg kc lh kg li kk la lb lc ld bi translated">评估集群</li></ol><p id="9202" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章是初级水平的，涵盖了基础知识和 r 中的实现。</p></div><div class="ab cl lj lk hu ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="ij ik il im in"><p id="2f0f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di"> D </span> issimilarity Matrix <br/>可以说，这是你聚类的主干。相异矩阵是一种数学表达式，表示数据集中的点彼此之间的差异或距离，因此您可以稍后将最接近的点分组在一起或分离最远的点，这是聚类的核心思想。</p><p id="4f70" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是数据类型差异非常重要的步骤，因为相异矩阵是基于各个数据点之间的距离。虽然很容易想象数字数据点之间的距离(例如，还记得<a class="ae ku" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">欧几里德距离</a>？)，分类数据(R 中的因子)似乎不那么明显。</p><p id="6adf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了计算这种情况下的相异度矩阵，你需要一个叫做高尔距离的东西。我不会进入它的数学，但我在这里提供了一个<a class="ae ku" href="http://venus.unive.it/romanaz/modstat_ba/gowdis.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>和<a class="ae ku" href="https://www.rdocumentation.org/packages/cluster/versions/2.0.6/topics/daisy" rel="noopener ugc nofollow" target="_blank">链接</a>。为此，我更喜欢使用<code class="fe lq lr ls lt b"><strong class="jp ir">cluster </strong>package</code>中的<code class="fe lq lr ls lt b">daisy()</code>和<code class="fe lq lr ls lt b">metric = c("gower")</code>。</p><pre class="lu lv lw lx gt ly lt lz ma aw mb bi"><span id="8589" class="mc md iq lt b gy me mf l mg mh">#----- Dummy Data -----#<br/># the data will be sterile clean in order to not get distracted with other issues that might arise, but I will also write about some difficulties I had, outside the code</span><span id="a8aa" class="mc md iq lt b gy mi mf l mg mh">library(dplyr)</span><span id="2b60" class="mc md iq lt b gy mi mf l mg mh"># ensuring reproducibility for sampling<br/>set.seed(40)</span><span id="58ac" class="mc md iq lt b gy mi mf l mg mh"># generating random variable set<br/># specifying ordered factors, strings will be converted to factors when using data.frame()</span><span id="e682" class="mc md iq lt b gy mi mf l mg mh"># customer ids come first, we will generate 200 customer ids from 1 to 200<br/>id.s &lt;- c(1:200) %&gt;%<br/>        factor()<br/>budget.s &lt;- sample(c("small", "med", "large"), 200, replace = T) %&gt;%<br/>            factor(levels=c("small", "med", "large"), <br/>            ordered = TRUE)</span><span id="41f8" class="mc md iq lt b gy mi mf l mg mh">origins.s &lt;- sample(c("x", "y", "z"), 200, replace = T, <br/>             prob = c(0.7, 0.15, 0.15))</span><span id="95bb" class="mc md iq lt b gy mi mf l mg mh">area.s &lt;- sample(c("area1", "area2", "area3", "area4"), 200, <br/>          replace = T,<br/>          prob = c(0.3, 0.1, 0.5, 0.2))</span><span id="27e9" class="mc md iq lt b gy mi mf l mg mh">source.s &lt;- sample(c("facebook", "email", "link", "app"), 200,   <br/>            replace = T,<br/>            prob = c(0.1,0.2, 0.3, 0.4))</span><span id="c195" class="mc md iq lt b gy mi mf l mg mh">## day of week - probabilities are mocking the demand curve<br/>dow.s &lt;- sample(c("mon", "tue", "wed", "thu", "fri", "sat", "sun"), 200, replace = T,<br/>         prob = c(0.1, 0.1, 0.2, 0.2, 0.1, 0.1, 0.2)) %&gt;%<br/>         factor(levels=c("mon", "tue", "wed", "thu", "fri", "sat", "sun"), <br/>        ordered = TRUE)</span><span id="836a" class="mc md iq lt b gy mi mf l mg mh"># dish <br/>dish.s &lt;- sample(c("delicious", "the one you don't like", "pizza"), 200, replace = T)<br/>            <br/># by default, data.frame() will convert all the strings to factors<br/>synthetic.customers &lt;- data.frame(id.s, budget.s, origins.s, area.s, source.s, dow.s, dish.s)</span><span id="be5b" class="mc md iq lt b gy mi mf l mg mh">#----- Dissimilarity Matrix -----#</span><span id="66d0" class="mc md iq lt b gy mi mf l mg mh">library(cluster) <br/># to perform different types of hierarchical clustering<br/># package functions used: daisy(), diana(), clusplot()</span><span id="086e" class="mc md iq lt b gy mi mf l mg mh">gower.dist &lt;- daisy(synthetic.customers[ ,2:7], metric = c("gower"))</span><span id="53dc" class="mc md iq lt b gy mi mf l mg mh"># class(gower.dist) <br/>## dissimilarity , dist</span></pre><p id="354f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用相异矩阵完成。这对于 200 次观察来说是非常快的，但是如果你有一个大的数据集，计算起来可能会非常昂贵。</p><p id="5d9a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实际上，您很可能必须首先清理数据集，执行从字符串到因子的必要转换，并注意丢失的值。在我自己的例子中，数据集包含丢失值的行，每次都很好地聚集在一起，使我认为我找到了宝藏，直到我看了这些值(哼！).</p><p id="e9c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di"> C </span>聚类算法<br/>你可能听说过有<em class="mj"> k-means </em>和<em class="mj">分层</em>聚类。在本文中，我主要关注后者，因为它是一种更具探索性的类型，并且可以采用不同的方法:您可以选择遵循<em class="mj">聚集</em>(自下而上)或<em class="mj">分裂</em>(自上而下)的聚类方式。</p><figure class="lu lv lw lx gt ml gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/335471245b1914f4c1157d64cfecefd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*qrfDH1woi77HSuzOq7ymmA.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk"><em class="ms">Credits: </em><a class="ae ku" href="http://uc-r.github.io/hc_clustering" rel="noopener ugc nofollow" target="_blank"><em class="ms">UC Business Analytics R Programming Guide</em></a></figcaption></figure><p id="c6b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">凝聚聚类将从<em class="mj"> n 个</em>聚类开始，其中<em class="mj"> n </em>是观察值的数量，假设它们中的每一个都是自己单独的聚类。然后，算法将尝试找到最相似的数据点，并将它们分组，因此它们开始形成聚类。</p><p id="a650" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相比之下，分裂聚类将反其道而行之——假设所有的<em class="mj"> n </em>个数据点是一个大的聚类，并将最不相似的分成不同的组。</p><p id="644d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你正在考虑使用其中的哪一个，尝试所有的选项总是值得的，但总的来说，<em class="mj">凝聚聚类在发现小簇方面更好，</em>并且被大多数软件使用；<em class="mj">分裂聚类——发现更大的聚类</em>。</p><p id="104c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我个人喜欢先看看树状图——聚类的图形表示，以决定我将坚持哪种方法。正如你将在下面看到的，一些树状图将会非常平衡，而其他的看起来会很混乱。</p><pre class="lu lv lw lx gt ly lt lz ma aw mb bi"><span id="5343" class="mc md iq lt b gy me mf l mg mh"># The main input for the code below is dissimilarity (distance matrix)<br/># After dissimilarity matrix was calculated, the further steps will be the same for all data types<br/># I prefer to look at the dendrogram and fine the most appealing one first - in this case, I was looking for a more balanced one - to further continue with assessment</span><span id="ec02" class="mc md iq lt b gy mi mf l mg mh">#------------ DIVISIVE CLUSTERING ------------#<br/>divisive.clust &lt;- diana(as.matrix(gower.dist), <br/>                  diss = TRUE, keep.diss = TRUE)<br/>plot(divisive.clust, main = "Divisive")</span></pre><figure class="lu lv lw lx gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mt"><img src="../Images/5fe124f2ec07a687f5ff18c5e7713794.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zPX8MlW1CTBokovn0uDANg.png"/></div></div></figure><pre class="lu lv lw lx gt ly lt lz ma aw mb bi"><span id="f7ca" class="mc md iq lt b gy me mf l mg mh">#------------ AGGLOMERATIVE CLUSTERING ------------#<br/># I am looking for the most balanced approach<br/># Complete linkages is the approach that best fits this demand - I will leave only this one here, don't want to get it cluttered</span><span id="0a66" class="mc md iq lt b gy mi mf l mg mh"># complete<br/>aggl.clust.c &lt;- hclust(gower.dist, method = "complete")<br/>plot(aggl.clust.c,<br/>     main = "Agglomerative, complete linkages")<br/></span></pre><figure class="lu lv lw lx gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mt"><img src="../Images/d333fb50c99ee84f630a6121a582e257.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B0MZ8iCdGLURkK5F-EDd7A.png"/></div></div></figure><p id="e057" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di"> A </span>选择聚类<br/>在这里，您将在不同的聚类算法和不同数量的聚类之间做出选择。正如评估经常发生的那样，可能的方式不止一种，辅以<strong class="jp ir"> <em class="mj">你自己的判断</em> </strong>。它是粗体和斜体，因为你自己的判断<strong class="jp ir"> <em class="mj">很重要</em> </strong> —聚类的数量应该有实际意义，数据分组的方式也应该有意义。使用分类变量时，您可能最终会得到无意义的聚类，因为它们的值的组合是有限的-它们是离散的，组合的数量也是如此。可能，您也不希望集群数量非常少——它们很可能太过笼统。最后，一切都取决于你的目标和你分析的目的。</p><p id="6f18" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从概念上讲，当创建聚类时，您会对不同的数据点组感兴趣，这样，在聚类内它们之间的距离(<em class="mj">或紧密度</em>)最小，而组之间的距离(<em class="mj">间隔</em>)尽可能大。这直观上很容易理解:点与点之间的距离是从相异度矩阵导出的它们相异度的度量。因此，聚类的评估是围绕紧密性和分离性的评估而建立的。</p><p id="f4b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里我将采用两种方法，并说明其中一种可能会产生无意义的结果:</p><ul class=""><li id="a4ae" class="kv kw iq jp b jq jr ju jv jy kx kc ky kg kz kk my lb lc ld bi translated"><em class="mj">肘法</em>:当聚类的紧密度，或者说组内的相似性对你的分析最重要的时候，就开始使用肘法。</li><li id="6a13" class="kv kw iq jp b jq le ju lf jy lg kc lh kg li kk my lb lc ld bi translated"><em class="mj">剪影法</em>:作为数据一致性的一种度量，<a class="ae ku" href="http://www.sthda.com/english/articles/29-cluster-validation-essentials/97-cluster-validation-statistics-must-know-methods/#silhouette-coefficient" rel="noopener ugc nofollow" target="_blank">剪影图显示一个聚类中的每个点与相邻聚类中的点的接近程度。</a></li></ul><p id="7e61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在实践中，它们很可能会提供不同的结果，这些结果在某一点上可能会令人困惑-不同数量的聚类将对应于最紧密/最明显分离的聚类，因此判断和理解数据实际上是什么将是做出最终决策的重要部分。</p><p id="4176" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还有一堆测量值，你可以根据自己的情况进行分析。我将它们添加到代码本身。</p><pre class="lu lv lw lx gt ly lt lz ma aw mb bi"><span id="2dae" class="mc md iq lt b gy me mf l mg mh"># Cluster stats comes out as list while it is more convenient to look at it as a table<br/># This code below will produce a dataframe with observations in columns and variables in row<br/># Not quite tidy data, which will require a tweak for plotting, but I prefer this view as an output here as I find it more comprehensive </span><span id="e303" class="mc md iq lt b gy mi mf l mg mh">library(fpc)</span><span id="6e7a" class="mc md iq lt b gy mi mf l mg mh">cstats.table &lt;- function(dist, tree, k) {<br/>clust.assess &lt;- c("cluster.number","n","within.cluster.ss","average.within","average.between",<br/>                  "wb.ratio","dunn2","avg.silwidth")<br/>clust.size &lt;- c("cluster.size")<br/>stats.names &lt;- c()<br/>row.clust &lt;- c()</span><span id="77f4" class="mc md iq lt b gy mi mf l mg mh">output.stats &lt;- matrix(ncol = k, nrow = length(clust.assess))<br/>cluster.sizes &lt;- matrix(ncol = k, nrow = k)</span><span id="37e3" class="mc md iq lt b gy mi mf l mg mh">for(i in c(1:k)){<br/>  row.clust[i] &lt;- paste("Cluster-", i, " size")<br/>}</span><span id="7df6" class="mc md iq lt b gy mi mf l mg mh">for(i in c(2:k)){<br/>  stats.names[i] &lt;- paste("Test", i-1)<br/>  <br/>  for(j in seq_along(clust.assess)){<br/>    output.stats[j, i] &lt;- unlist(cluster.stats(d = dist, clustering = cutree(tree, k = i))[clust.assess])[j]<br/>    <br/>  }<br/>  <br/>  for(d in 1:k) {<br/>    cluster.sizes[d, i] &lt;- unlist(cluster.stats(d = dist, clustering = cutree(tree, k = i))[clust.size])[d]<br/>    dim(cluster.sizes[d, i]) &lt;- c(length(cluster.sizes[i]), 1)<br/>    cluster.sizes[d, i]<br/>    <br/>  }<br/>}</span><span id="8ec8" class="mc md iq lt b gy mi mf l mg mh">output.stats.df &lt;- data.frame(output.stats)</span><span id="1c3f" class="mc md iq lt b gy mi mf l mg mh">cluster.sizes &lt;- data.frame(cluster.sizes)<br/>cluster.sizes[is.na(cluster.sizes)] &lt;- 0</span><span id="d0bc" class="mc md iq lt b gy mi mf l mg mh">rows.all &lt;- c(clust.assess, row.clust)<br/># rownames(output.stats.df) &lt;- clust.assess<br/>output &lt;- rbind(output.stats.df, cluster.sizes)[ ,-1]<br/>colnames(output) &lt;- stats.names[2:k]<br/>rownames(output) &lt;- rows.all</span><span id="39c3" class="mc md iq lt b gy mi mf l mg mh">is.num &lt;- sapply(output, is.numeric)<br/>output[is.num] &lt;- lapply(output[is.num], round, 2)</span><span id="8d30" class="mc md iq lt b gy mi mf l mg mh">output<br/>}</span><span id="858a" class="mc md iq lt b gy mi mf l mg mh"># I am capping the maximum amout of clusters by 7<br/># I want to choose a reasonable number, based on which I will be able to see basic differences between customer groups as a result</span><span id="3869" class="mc md iq lt b gy mi mf l mg mh">stats.df.divisive &lt;- cstats.table(gower.dist, divisive.clust, 7)<br/>stats.df.divisive</span></pre><figure class="lu lv lw lx gt ml gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/621b4f11e6f7bafc620cfc50f2a05415.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*u8x9FocOgG2oFTwQvwMvIA.png"/></div></figure><p id="a8c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看，average.within，即聚类内观察值之间的平均距离，正在缩小，聚类 SS 内也是如此。平均轮廓宽度有点不直接，但相反的关系仍然存在。</p><p id="dc59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看看星团的大小是多么不成比例。我不会仓促地处理星团中无与伦比的观测数据。其中一个原因是，数据集可能不平衡，一些观察组将在分析中超过所有其他组-这不好，很可能导致偏差。</p><pre class="lu lv lw lx gt ly lt lz ma aw mb bi"><span id="a9ef" class="mc md iq lt b gy me mf l mg mh">stats.df.aggl &lt;-cstats.table(gower.dist, aggl.clust.c, 7) #complete linkages looks like the most balanced approach<br/>stats.df.aggl</span></pre><figure class="lu lv lw lx gt ml gh gi paragraph-image"><div class="gh gi na"><img src="../Images/c6182e5d19702b3c26ed416e76dd1623.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*o4rSsYq8z6ddjWwAYLGwdg.png"/></div></figure><p id="8058" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，在每组的观察数量上，更加平衡的聚集完全联系层次聚类是如何进行比较的。</p><pre class="lu lv lw lx gt ly lt lz ma aw mb bi"><span id="8574" class="mc md iq lt b gy me mf l mg mh"># --------- Choosing the number of clusters ---------#</span><span id="329c" class="mc md iq lt b gy mi mf l mg mh"># Using "Elbow" and "Silhouette" methods to identify the best number of clusters<br/># to better picture the trend, I will go for more than 7 clusters.</span><span id="37d7" class="mc md iq lt b gy mi mf l mg mh">library(ggplot2)</span><span id="9ff4" class="mc md iq lt b gy mi mf l mg mh"><strong class="lt ir"># Elbow<br/># Divisive clustering</strong><br/>ggplot(data = data.frame(t(cstats.table(gower.dist, divisive.clust, 15))), <br/>  aes(x=cluster.number, y=within.cluster.ss)) + <br/>  geom_point()+<br/>  geom_line()+<br/>  ggtitle("Divisive clustering") +<br/>  labs(x = "Num.of clusters", y = "Within clusters sum of squares (SS)") +<br/>  theme(plot.title = element_text(hjust = 0.5))</span></pre><figure class="lu lv lw lx gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nb"><img src="../Images/16b5915292674fea55a90d6355a68035.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ZMTeIyQwdbOCfYSWA42kw.png"/></div></div></figure><p id="b561" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，我们制作了“肘”图。它显示了平方和(作为观察值接近程度的一种度量:平方和越低，分类内的观察值越接近)随着分类数的不同而变化。理想情况下，我们应该在肘部看到一个与众不同的“弯曲”,在那里分裂星系团只会使 SS 略微下降。在下图的情况下，我会选择 7 左右。虽然在这种情况下，一个聚类将只包含 2 个观察值，但让我们看看凝聚聚类会发生什么。</p><pre class="lu lv lw lx gt ly lt lz ma aw mb bi"><span id="33cb" class="mc md iq lt b gy me mf l mg mh"># Agglomerative clustering,provides a more ambiguous picture<br/>ggplot(data = data.frame(t(cstats.table(gower.dist, aggl.clust.c, 15))), <br/>  aes(x=cluster.number, y=within.cluster.ss)) + <br/>  geom_point()+<br/>  geom_line()+<br/>  ggtitle("Agglomerative clustering") +<br/>  labs(x = "Num.of clusters", y = "Within clusters sum of squares (SS)") +<br/>  theme(plot.title = element_text(hjust = 0.5))</span></pre><figure class="lu lv lw lx gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nb"><img src="../Images/3562e069579c0649e00dfb77bd8e25e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dbkfvskblos9x8zd_XS-Tw.png"/></div></div></figure><p id="0cdf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">凝聚型“肘”看起来与分裂型相似，只是凝聚型看起来更平滑，而“弯”不那么突兀。与分裂聚类类似，我会选择 7 个聚类，但在这两种方法之间进行选择，我更喜欢凝聚方法产生的聚类的大小——我想要大小相当的东西。</p><pre class="lu lv lw lx gt ly lt lz ma aw mb bi"><span id="b572" class="mc md iq lt b gy me mf l mg mh"><strong class="lt ir"># Silhouette</strong></span><span id="7107" class="mc md iq lt b gy mi mf l mg mh">ggplot(data = data.frame(t(cstats.table(gower.dist, divisive.clust, 15))), <br/>  aes(x=cluster.number, y=avg.silwidth)) + <br/>  geom_point()+<br/>  geom_line()+<br/>  ggtitle("Divisive clustering") +<br/>  labs(x = "Num.of clusters", y = "Average silhouette width") +<br/>  theme(plot.title = element_text(hjust = 0.5))</span></pre><figure class="lu lv lw lx gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nb"><img src="../Images/8f021d6dcba4829dd2eeddef5ad96b37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3FwCZ7CL9kz9iyr6AVvzow.png"/></div></div></figure><p id="a7d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当谈到轮廓评估时，规则是您应该选择使轮廓系数最大化的数字，因为您希望聚类足够独特(远)以被认为是独立的。</p><p id="276c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">轮廓系数的范围在-1 和 1 之间，1 表示类内一致性良好，-1 表示不太好。</p><p id="c272" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从上面的图中可以看出，您不会选择 5 个集群，而会选择 9 个。</p><p id="6e30" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作为比较，对于“简单”的情况，轮廓图可能看起来像下图。我们还不完全是，但几乎是。</p><figure class="lu lv lw lx gt ml gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/590d434f33fdd1ab79b1d1cce2b88e46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*YbphXOTwnYPllANuj6i_GQ.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk"><em class="ms">Credits: </em><a class="ae ku" href="http://data-sailors.com/2016/10/17/let-the-machine-find-optimal-number-of-clusters-from-your-data/" rel="noopener ugc nofollow" target="_blank"><em class="ms">Data sailors</em></a></figcaption></figure><pre class="lu lv lw lx gt ly lt lz ma aw mb bi"><span id="9dba" class="mc md iq lt b gy me mf l mg mh">ggplot(data = data.frame(t(cstats.table(gower.dist, aggl.clust.c, 15))), <br/>  aes(x=cluster.number, y=avg.silwidth)) + <br/>  geom_point()+<br/>  geom_line()+<br/>  ggtitle("Agglomerative clustering") +<br/>  labs(x = "Num.of clusters", y = "Average silhouette width") +<br/>  theme(plot.title = element_text(hjust = 0.5))</span></pre><figure class="lu lv lw lx gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nb"><img src="../Images/5f374123fa19354eaebcb8379d8e65ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4JGrdBvEOLj_qvmZibnLXQ.png"/></div></div></figure><p id="09fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的剪影宽度图是在说“你打破的数据集越多，聚类就变得越有特色”。最终，您将得到单个数据点——您不希望这样，如果您尝试使用更大的 k 作为聚类数，您将会看到它。例如，在<em class="mj"> k=30，</em>我得到了下面的图:</p><figure class="lu lv lw lx gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nb"><img src="../Images/d0f78818aada9b9d1e3f1b1f98dd7ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ap8SYc0BjOmae7abhRkp6A.png"/></div></div></figure><p id="78a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以:你分割得越多，效果越好，但是我们不能分割到单独的数据点(记住我们在上面的图中有 30 个集群，只有 200 个数据点)。</p><p id="8bc7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">综上所述，在我看来，这种情况下的凝聚聚类看起来更加平衡——聚类大小或多或少是可比的(看看分裂部分中只有 2 个观察值的那个聚类！)，我会选择通过这种方法获得的 7 个聚类。让我们看看它们的样子，并检查里面有什么。</p><p id="6b49" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该数据集由 6 个需要在 2D 或 3D 中可视化的变量组成，所以是时候接受挑战了！分类数据的性质也造成了一些限制，因此使用一些预定义的解决方案可能会变得棘手。我想 a)了解观察值如何聚集，b)了解观察值如何跨类别分布，因此我创建了 a)彩色树状图，b)每个聚类内每个变量的观察值计数热图。</p><pre class="lu lv lw lx gt ly lt lz ma aw mb bi"><span id="2441" class="mc md iq lt b gy me mf l mg mh">library("ggplot2")<br/>library("reshape2")<br/>library("purrr")<br/>library("dplyr")</span><span id="3d61" class="mc md iq lt b gy mi mf l mg mh"><em class="mj"># let's start with a dendrogram</em><br/>library("dendextend")</span><span id="fce9" class="mc md iq lt b gy mi mf l mg mh">dendro &lt;- as.dendrogram(aggl.clust.c)</span><span id="af3c" class="mc md iq lt b gy mi mf l mg mh">dendro.col &lt;- dendro %&gt;%<br/>  set("branches_k_color", k = 7, value =   c("darkslategray", "darkslategray4", "darkslategray3", "gold3", "darkcyan", "cyan3", "gold3")) %&gt;%<br/>  set("branches_lwd", 0.6) %&gt;%<br/>  set("labels_colors", <br/>      value = c("darkslategray")) %&gt;% <br/>  set("labels_cex", 0.5)</span><span id="e63b" class="mc md iq lt b gy mi mf l mg mh">ggd1 &lt;- as.ggdend(dendro.col)</span><span id="a755" class="mc md iq lt b gy mi mf l mg mh">ggplot(ggd1, theme = theme_minimal()) +<br/>  labs(x = "Num. observations", y = "Height", title = "Dendrogram, k = 7")</span></pre><figure class="lu lv lw lx gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nd"><img src="../Images/451d0de90b3e7f664c593e242212b044.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dbn77hdeZIa0HYlmTlogKw.png"/></div></div></figure><pre class="lu lv lw lx gt ly lt lz ma aw mb bi"><span id="a199" class="mc md iq lt b gy me mf l mg mh"><em class="mj"># Radial plot looks less cluttered (and cooler)</em><br/>ggplot(ggd1, labels = T) + <br/>  scale_y_reverse(expand = c(0.2, 0)) +<br/>  coord_polar(theta="x")</span></pre><figure class="lu lv lw lx gt ml gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/427e1801be59fef208dd72417b0fef68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*1knGSh-w4kSbYYS55GjEMA.png"/></div></figure><pre class="lu lv lw lx gt ly lt lz ma aw mb bi"><span id="8702" class="mc md iq lt b gy me mf l mg mh"><em class="mj"># Time for the heatmap<br/># the 1st step here is to have 1 variable per row<br/># factors have to be converted to characters in order not to be dropped</em></span><span id="cfcb" class="mc md iq lt b gy mi mf l mg mh">clust.num &lt;- cutree(aggl.clust.c, k = 7)<br/>synthetic.customers.cl &lt;- cbind(synthetic.customers, clust.num)</span><span id="083c" class="mc md iq lt b gy mi mf l mg mh">cust.long &lt;- melt(data.frame(lapply(synthetic.customers.cl, as.character), stringsAsFactors=FALSE), <br/>                  id = c("id.s", "clust.num"), factorsAsStrings=T)</span><span id="5e85" class="mc md iq lt b gy mi mf l mg mh">cust.long.q &lt;- cust.long %&gt;%<br/>  group_by(clust.num, variable, value) %&gt;%<br/>  mutate(count = n_distinct(id.s)) %&gt;%<br/>  distinct(clust.num, variable, value, count)</span><span id="1ffa" class="mc md iq lt b gy mi mf l mg mh"><em class="mj"># heatmap.c will be suitable in case you want to go for absolute counts - but it doesn't tell much to my taste</em></span><span id="8123" class="mc md iq lt b gy mi mf l mg mh">heatmap.c &lt;- ggplot(cust.long.q, aes(x = clust.num, y =        factor(value, levels = c("x","y","z",                                                                   "mon", "tue", "wed", "thu", "fri","sat","sun",                                                       "delicious", "the one you don't like", "pizza",                                                             "facebook", "email", "link", "app",                                                             "area1", "area2", "area3", "area4",                                                             "small", "med", "large"), ordered = T))) +<br/>  <br/>  geom_tile(aes(fill = count))+<br/>  scale_fill_gradient2(low = "darkslategray1", mid = "yellow", high = "turquoise4")</span><span id="62a9" class="mc md iq lt b gy mi mf l mg mh"><em class="mj"># calculating the percent of each factor level in the absolute count of cluster members</em><br/>cust.long.p &lt;- cust.long.q %&gt;%<br/>  group_by(clust.num, variable) %&gt;%<br/>  mutate(perc = count / sum(count)) %&gt;%<br/>  arrange(clust.num)</span><span id="bc38" class="mc md iq lt b gy mi mf l mg mh">heatmap.p &lt;- ggplot(cust.long.p, aes(x = clust.num, y = factor(value, levels = c("x","y","z",<br/>      "mon", "tue", "wed", "thu", "fri","sat", "sun",                                                                     "delicious", "the one you don't like", "pizza",                                             "facebook", "email", "link", "app",                                             "area1", "area2", "area3", "area4",                                           "small", "med", "large"), ordered = T))) +<br/>  <br/>geom_tile(aes(fill = perc), alpha = 0.85)+<br/>  labs(title = "Distribution of characteristics across clusters", x = "Cluster number", y = NULL) +<br/>  geom_hline(yintercept = 3.5) + <br/>  geom_hline(yintercept = 10.5) + <br/>  geom_hline(yintercept = 13.5) + <br/>  geom_hline(yintercept = 17.5) + <br/>  geom_hline(yintercept = 21.5) + <br/>  scale_fill_gradient2(low = "darkslategray1", mid = "yellow", high = "turquoise4")</span><span id="ab6f" class="mc md iq lt b gy mi mf l mg mh">heatmap.p</span></pre><figure class="lu lv lw lx gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nf"><img src="../Images/386998bfa085732a4debfce582c3ebad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tT0kMFJhcOHqPjEZWaUO4Q.png"/></div></div></figure><p id="92be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有了热图，您可以看到初始因素(我们已经开始使用的变量)中每个因素级别有多少观察值。较深的蓝色对应于一个集群中相对较多的观测值。在此图中，您还可以看到，一周中的每一天/购物篮大小在每个箱中都有几乎相同数量的客户，这可能意味着这些对于分析来说不是决定性的，并且<em class="mj">可能会被省略。</em></p></div><div class="ab cl lj lk hu ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="ij ik il im in"><p id="dbc8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di"> C </span>丢失音符<br/>在这篇文章中，我们已经经历了相异矩阵计算，尝试了凝聚和分裂的层次聚类方法，并且用“肘”和“剪影”方法看了一下聚类评估。</p><p id="de13" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我们经历了相异度矩阵计算，尝试了凝聚和分裂的层次聚类方法，并对聚类评估进行了研究。</p><p id="0fee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">分裂式和凝聚式层次聚类是开始探索的好地方，但是如果您的目标是成为一名聚类大师，请不要止步于此——还有更多的方法和技术出现在那里。与数值数据聚类相比，主要区别在于相异矩阵的计算。从评估的角度来看，并不是所有的标准聚类评估方法都会产生可靠和合理的结果——剪影方法可能会被淘汰。</p><p id="eb74" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我做这个练习已经有一段时间了。到目前为止，我看到了我的方法背后的一些缺点，我欢迎任何反馈。我的分析中的一个普遍缺陷在于聚类本身之外— <em class="mj">我的数据集在很多方面都是不平衡的</em>，这个问题仍然没有得到解决。我可以看到它对聚类的影响:有 70%的客户属于一个因素级别(在这种情况下是国籍)，这个组控制了大多数产生的聚类，使得很难找出其他因素级别之间的差异。平衡数据集和比较聚类结果是我接下来要尝试的，我会写一篇单独的文章来讨论这个问题。</p><p id="6fd5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，如果你想克隆，这里有 github 的链接:<a class="ae ku" href="https://github.com/khunreus/cluster-categorical" rel="noopener ugc nofollow" target="_blank">https://github.com/khunreus/cluster-categorical</a></p><p id="d20f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">希望你喜欢它！</p></div><div class="ab cl lj lk hu ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="ij ik il im in"><p id="7c95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mj">以下是我发现有用的资源:</em></p><p id="154d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">分层聚类教程(数据准备、聚类、可视化)，总的来说，这个博客可能对那些对 R:<a class="ae ku" href="http://uc-r.github.io/hc_clustering" rel="noopener ugc nofollow" target="_blank">http://uc-r.github.io/hc_clustering</a>和<a class="ae ku" href="https://uc-r.github.io/kmeans_clustering" rel="noopener ugc nofollow" target="_blank">https://uc-r.github.io/kmeans_clustering</a>的商业分析感兴趣的人有用</p><p id="41e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">聚类验证:<a class="ae ku" href="http://www.sthda.com/english/articles/29-cluster-validation-essentials/97-cluster-validation-statistics-must-know-methods/" rel="noopener ugc nofollow" target="_blank">http://www . sth da . com/English/articles/29-cluster-validation-essentials/97-cluster-validation-statistics-must-know-methods/</a></p><p id="a00b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">文档分类示例(分层和 k-means):<a class="ae ku" href="https://eight2late.wordpress.com/2015/07/22/a-gentle-introduction-to-cluster-analysis-using-r/" rel="noopener ugc nofollow" target="_blank">https://eight 2 late . WordPress . com/2015/07/22/a-gentle-introduction-to-cluster-analysis-using-r/</a></p><p id="84b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">denextend 包相当有趣，允许比较不同方法之间的集群结构:<a class="ae ku" href="https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html#the-set-function" rel="noopener ugc nofollow" target="_blank">https://cran . r-project . org/web/packages/dend extend/vignettes/introduction . html # the-set-function</a></p><p id="46fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不仅有树状图，还有聚类图:<a class="ae ku" href="https://www.r-statistics.com/2010/06/clustergram-visualization-and-diagnostics-for-cluster-analysis-r-code/" rel="noopener ugc nofollow" target="_blank">https://www . r-statistics . com/2010/06/cluster gram-visualization-and-diagnostics-for-cluster-analysis-r-code/</a></p><p id="8f5f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">结合聚类热图和树状图:<a class="ae ku" href="https://jcoliver.github.io/learn-r/008-ggplot-dendrograms-and-heatmaps.html" rel="noopener ugc nofollow" target="_blank">https://JC Oliver . github . io/learn-r/008-gg plot-dendrograms-and-heat maps . html</a></p><p id="3a79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我个人有兴趣尝试一下在<a class="ae ku" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5025633/" rel="noopener ugc nofollow" target="_blank">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5025633/</a>引入的方法，他们的 GitHub 库:<a class="ae ku" href="https://github.com/khunreus/EnsCat" rel="noopener ugc nofollow" target="_blank">https://github.com/khunreus/EnsCat</a></p></div></div>    
</body>
</html>