<html>
<head>
<title>Andrew Ng’s Machine Learning Course in Python (Neural Networks)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">吴恩达的 Python(神经网络)机器学习课程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-neural-networks-e526b41fdcd9?source=collection_archive---------5-----------------------#2018-12-27">https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-neural-networks-e526b41fdcd9?source=collection_archive---------5-----------------------#2018-12-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/89cc757928aed2bdb0a09a8d44d4dede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_nZHZK5eR0ExqOqhy7cC2A.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Machine Learning — Andrew Ng</figcaption></figure><p id="875d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">本文将着眼于吴恩达的机器学习课程中关于神经网络的编程作业 3 和 4。这也是我们在课程中遇到的第一个复杂的非线性算法。我不知道你是怎么想的，但是对我来说，这个任务绝对有一个陡峭的学习曲线。神经网络形成深度学习的基础，深度学习具有广泛的应用，例如计算机视觉或自然语言处理。因此，获得最基本的权利是很重要的，用 python 编写这些赋值代码是确保这一点的一种方法。</p></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="b19a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi lk translated"><span class="l ll lm ln bm lo lp lq lr ls di"> B </span>在进入神经网络之前，让我们完成逻辑回归的最后一部分——多类逻辑回归。</p><p id="2995" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这一系列练习利用了由 5000 个训练示例组成的手写数字数据集，其中每个示例都是数字的 20 像素乘 20 像素灰度图像。</p><p id="cc20" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">加载数据集</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="eeb3" class="mc md it ly b gy me mf l mg mh">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from scipy.io import loadmat</span><span id="d963" class="mc md it ly b gy mi mf l mg mh"># Use loadmat to load matlab files<br/>mat=loadmat("ex3data1.mat")<br/>X=mat["X"]<br/>y=mat["y"]</span></pre><p id="23eb" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">因为数据集是在。mat 格式而不是通常的格式。txt 格式，我不得不使用 scipy loadmat 函数来完成这项工作。官方文档可以在<a class="ae mj" href="https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.io.loadmat.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。自装载垫装载以来。mat 文件作为一个以变量名为关键字的字典，给 X 和 y 赋值就像用变量的关键字访问字典一样简单。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="8882" class="mc md it ly b gy me mf l mg mh">X.shape, y.shape</span></pre><p id="19ea" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了更好地理解数据集，拥有数据的形状可以告诉我们数据的维度。X 具有对应于 5000 个训练样本的形状<code class="fe mk ml mm ly b">5000,400</code>，每个训练样本具有来自其 20×20 像素的 400 个特征。y 有一个<code class="fe mk ml mm ly b">5000,1</code>的形状，其中每个训练样本有一个从 1 到 10 的标签(在这个数据集中‘0’数字被标记为‘10’)。</p><p id="fd53" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">可视化数据</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="e1bc" class="mc md it ly b gy me mf l mg mh">import matplotlib.image as mpimg<br/>fig, axis = plt.subplots(10,10,figsize=(8,8))<br/>for i in range(10):<br/>    for j in range(10):<br/>        axis[i,j].imshow(X[np.random.randint(0,5001),:].reshape(20,20,order="F"), cmap="hot") #reshape back to 20 pixel by 20 pixel<br/>        axis[i,j].axis("off")</span></pre><p id="1ce7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">上面的代码块构建了 100 个子情节，并使用<a class="ae mj" href="https://matplotlib.org/users/image_tutorial.html" rel="noopener ugc nofollow" target="_blank"> plt.imshow </a>从 5000 个训练示例中随机可视化 100 个。请注意，我们必须将训练样本重新整形回 20 X 20 像素，然后才能将其可视化，并将<code class="fe mk ml mm ly b">order="F"</code>作为参数添加到整形函数中，以确保图像的方向是垂直的。</p><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/fe3d2574c987fd52b4dd7cc5d179ea4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*ODboas36xuFeYVySCiadmQ.png"/></div></figure><p id="1efc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">计算成本函数和梯度</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="e19f" class="mc md it ly b gy me mf l mg mh">def sigmoid(z):<br/>    """<br/>    return the sigmoid of z<br/>    """<br/>    <br/>    return 1/ (1 + np.exp(-z))</span><span id="8f6d" class="mc md it ly b gy mi mf l mg mh">def lrCostFunction(theta, X, y, Lambda):<br/>    """<br/>    Takes in numpy array of theta, X, y, and float lambda to compute the regularized logistic cost function <br/>    """<br/>    <br/>    m=len(y)<br/>    predictions = sigmoid(X @ theta)<br/>    error = (-y * np.log(predictions)) - ((1-y)*np.log(1-predictions))<br/>    cost = 1/m * sum(error)<br/>    regCost= cost + Lambda/(2*m) * sum(theta[1:]**2)<br/>    <br/>    # compute gradient<br/>    j_0= 1/m * (X.transpose() @ (predictions - y))[0]<br/>    j_1 = 1/m * (X.transpose() @ (predictions - y))[1:] + (Lambda/m)* theta[1:]<br/>    grad= np.vstack((j_0[:,np.newaxis],j_1))<br/>    return regCost[0], grad</span></pre><p id="ba54" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这类似于我们在<a class="ae mj" rel="noopener" target="_blank" href="/andrew-ngs-machine-learning-course-in-python-regularized-logistic-regression-lasso-regression-721f311130fb">逻辑回归</a>任务中使用的成本函数。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="5438" class="mc md it ly b gy me mf l mg mh">theta_t = np.array([-2,-1,1,2]).reshape(4,1)<br/>X_t =np.array([np.linspace(0.1,1.5,15)]).reshape(3,5).T<br/>X_t = np.hstack((np.ones((5,1)), X_t))<br/>y_t = np.array([1,0,1,0,1]).reshape(5,1)<br/>J, grad = lrCostFunction(theta_t, X_t, y_t, 3)<br/>print("Cost:",J,"Expected cost: 2.534819")<br/>print("Gradients:\n",grad,"\nExpected gradients:\n 0.146561\n -0.548558\n 0.724722\n 1.398003")</span></pre><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/53134380592361a85926942c06303ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*OdeI1jhZeexzhpbQKKMhGA.png"/></div></figure><p id="624a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在是分类任务。由于我们有不止一个类，我们将不得不使用一对一的分类方法来训练多个逻辑回归分类器(每个类一个分类器)。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="3e6a" class="mc md it ly b gy me mf l mg mh">def gradientDescent(X,y,theta,alpha,num_iters,Lambda):<br/>    """<br/>    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps<br/>    with learning rate of alpha<br/>    <br/>    return theta and the list of the cost of theta during each iteration<br/>    """<br/>    <br/>    m=len(y)<br/>    J_history =[]<br/>    <br/>    for i in range(num_iters):<br/>        cost, grad = lrCostFunction(theta,X,y,Lambda)<br/>        theta = theta - (alpha * grad)<br/>        J_history.append(cost)<br/>    <br/>    return theta , J_history<br/></span><span id="0704" class="mc md it ly b gy mi mf l mg mh">def oneVsAll(X, y, num_labels, Lambda):<br/>    """<br/>    Takes in numpy array of X,y, int num_labels and float lambda to train multiple logistic regression classifiers<br/>    depending on the number of num_labels using gradient descent. <br/>    <br/>    Returns a matrix of theta, where the i-th row corresponds to the classifier for label i<br/>    """<br/>    m, n = X.shape[0], X.shape[1]<br/>    initial_theta = np.zeros((n+1,1))<br/>    all_theta = []<br/>    all_J=[]<br/>    # add intercept terms<br/>    <br/>    X = np.hstack((np.ones((m,1)),X))<br/>    <br/>    for i in range(1,num_labels+1):<br/>        theta , J_history = gradientDescent(X,np.where(y==i,1,0),initial_theta,1,300,Lambda)<br/>        all_theta.extend(theta)<br/>        all_J.extend(J_history)<br/>    return np.array(all_theta).reshape(num_labels,n+1), all_J</span></pre><p id="3cb9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><code class="fe mk ml mm ly b">gradientDescent</code>函数是我们之前实现的常用优化函数。至于<code class="fe mk ml mm ly b">oneVsAll</code>，它遍历所有的类，并使用梯度下降为每个类训练一组θ(在赋值中使用了<code class="fe mk ml mm ly b">fmincg</code>函数)。<code class="fe mk ml mm ly b">all_theta</code>然后在一个列表中捕获所有优化的 theta，并作为一个 numpy 数组返回，重新整形为一个 theta 矩阵，其中第 I 行对应于标签 I 的分类器。<code class="fe mk ml mm ly b">np.where</code>在这里派上用场，为每个类获取一个 1/0 的 y 向量，以在每次迭代中执行我们的二元分类任务。</p><p id="f654" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">绘制成本函数只是为了确保梯度下降按预期工作</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="b098" class="mc md it ly b gy me mf l mg mh">plt.plot(all_J[0:300])<br/>plt.xlabel("Iteration")<br/>plt.ylabel("$J(\Theta)$")<br/>plt.title("Cost function using Gradient Descent")</span></pre><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mo"><img src="../Images/9e8eb256c20d6f169091df2692468dc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*lIQyZ6d0jCkIW4KSXXeQAA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">alpha = 1, num_iters = 300</figcaption></figure><p id="c517" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了进行预测，计算每个类别的 x(i)的概率，并且该预测是具有最高概率的类别</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="9619" class="mc md it ly b gy me mf l mg mh">def predictOneVsAll(all_theta, X):<br/>    """<br/>    Using all_theta, compute the probability of X(i) for each class and predict the label<br/>    <br/>    return a vector of prediction<br/>    """<br/>    m= X.shape[0]<br/>    X = np.hstack((np.ones((m,1)),X))<br/>    <br/>    predictions = X @ all_theta.T<br/>    return np.argmax(predictions,axis=1)+1</span><span id="e2e4" class="mc md it ly b gy mi mf l mg mh">pred = predictOneVsAll(all_theta, X)<br/>print("Training Set Accuracy:",sum(pred[:,np.newaxis]==y)[0]/5000*100,"%")</span></pre><p id="26c0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">打印报表打印:<code class="fe mk ml mm ly b">Training Set Accuracy: 91.46 %</code></p></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="3ed5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">最后，神经网络的时间到了。对于相同的数据集，我们旨在使用更复杂的算法(如神经网络)来实现更高的准确性。对于练习的第一部分，优化的θ值被给我们，我们应该实现前馈传播以获得预测和模型精度。</p><p id="a54a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">优化 theta 的加载</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="072f" class="mc md it ly b gy me mf l mg mh">mat2=loadmat("ex3weights.mat")<br/>Theta1=mat2["Theta1"] # Theta1 has size 25 x 401<br/>Theta2=mat2["Theta2"] # Theta2 has size 10 x 26</span></pre><p id="5c2e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">使用前馈传播进行预测</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="5f6b" class="mc md it ly b gy me mf l mg mh">def predict(Theta1, Theta2, X):<br/>    """<br/>    Predict the label of an input given a trained neural network<br/>    """<br/>    m= X.shape[0]<br/>    X = np.hstack((np.ones((m,1)),X))<br/>    <br/>    a1 = sigmoid(X @ Theta1.T)<br/>    a1 = np.hstack((np.ones((m,1)), a1)) # hidden layer<br/>    a2 = sigmoid(a1 @ Theta2.T) # output layer<br/>    <br/>    return np.argmax(a2,axis=1)+1</span><span id="1ea9" class="mc md it ly b gy mi mf l mg mh">pred2 = predict(Theta1, Theta2, X)<br/>print("Training Set Accuracy:",sum(pred2[:,np.newaxis]==y)[0]/5000*100,"%")</span></pre><p id="0cd6" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">打印报表打印:<code class="fe mk ml mm ly b">Training Set Accuracy: 97.52 %</code>。与多类逻辑回归相比，准确度高得多！</p></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="a3ce" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在作业 4 中，我们从零开始实现一个神经网络。我们从计算成本函数和θ的梯度开始。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="79b4" class="mc md it ly b gy me mf l mg mh">def sigmoidGradient(z):<br/>    """<br/>    computes the gradient of the sigmoid function<br/>    """<br/>    sigmoid = 1/(1 + np.exp(-z))<br/>    <br/>    return sigmoid *(1-sigmoid)<br/></span><span id="5e8b" class="mc md it ly b gy mi mf l mg mh">def nnCostFunction(nn_params,input_layer_size, hidden_layer_size, num_labels,X, y,Lambda):<br/>    """<br/>    nn_params contains the parameters unrolled into a vector<br/>    <br/>    compute the cost and gradient of the neural network<br/>    """<br/>    # Reshape nn_params back into the parameters Theta1 and Theta2<br/>    Theta1 = nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)<br/>    Theta2 = nn_params[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)<br/>    <br/>    m = X.shape[0]<br/>    J=0<br/>    X = np.hstack((np.ones((m,1)),X))<br/>    y10 = np.zeros((m,num_labels))<br/>    <br/>    a1 = sigmoid(X @ Theta1.T)<br/>    a1 = np.hstack((np.ones((m,1)), a1)) # hidden layer<br/>    a2 = sigmoid(a1 @ Theta2.T) # output layer<br/>    <br/>    for i in range(1,num_labels+1):<br/>        y10[:,i-1][:,np.newaxis] = np.where(y==i,1,0)<br/>    for j in range(num_labels):<br/>        J = J + sum(-y10[:,j] * np.log(a2[:,j]) - (1-y10[:,j])*np.log(1-a2[:,j]))<br/>    <br/>    cost = 1/m* J<br/>    reg_J = cost + Lambda/(2*m) * (np.sum(Theta1[:,1:]**2) + np.sum(Theta2[:,1:]**2))<br/>    <br/>    # Implement the backpropagation algorithm to compute the gradients<br/>    <br/>    grad1 = np.zeros((Theta1.shape))<br/>    grad2 = np.zeros((Theta2.shape))<br/>    <br/>    for i in range(m):<br/>        xi= X[i,:] # 1 X 401<br/>        a1i = a1[i,:] # 1 X 26<br/>        a2i =a2[i,:] # 1 X 10<br/>        d2 = a2i - y10[i,:]<br/>        d1 = Theta2.T @ d2.T * sigmoidGradient(np.hstack((1,xi @ Theta1.T)))<br/>        grad1= grad1 + d1[1:][:,np.newaxis] @ xi[:,np.newaxis].T<br/>        grad2 = grad2 + d2.T[:,np.newaxis] @ a1i[:,np.newaxis].T<br/>        <br/>    grad1 = 1/m * grad1<br/>    grad2 = 1/m*grad2<br/>    <br/>    grad1_reg = grad1 + (Lambda/m) * np.hstack((np.zeros((Theta1.shape[0],1)),Theta1[:,1:]))<br/>    grad2_reg = grad2 + (Lambda/m) * np.hstack((np.zeros((Theta2.shape[0],1)),Theta2[:,1:]))<br/>    <br/>    return cost, grad1, grad2,reg_J, grad1_reg,grad2_reg</span></pre><p id="27f9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">该赋值一步一步地遍历整个过程，首先计算成本，然后是正则化成本、梯度，最后是正则化梯度。如果您想继续，我修改了代码，只要您使用正确的索引，它就会返回中间步骤的值。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="d27f" class="mc md it ly b gy me mf l mg mh">input_layer_size  = 400<br/>hidden_layer_size = 25<br/>num_labels = 10<br/>nn_params = np.append(Theta1.flatten(),Theta2.flatten())<br/>J,reg_J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, 1)[0:4:3]<br/>print("Cost at parameters (non-regularized):",J,"\nCost at parameters (Regularized):",reg_J)</span></pre><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mp"><img src="../Images/47d089bfa21e489348442c8a5293e1b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c80n2Ur1LiNagQO0WJeOOA.png"/></div></div></figure><p id="514b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><code class="fe mk ml mm ly b">flatten()</code>这里的函数将数组折叠成一维，而<code class="fe mk ml mm ly b">np.append</code>将参数“展开”成一个向量。</p><p id="704f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">讲座中讨论了初始θ的对称性问题。为了打破这种对称性，需要随机初始化。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="0a91" class="mc md it ly b gy me mf l mg mh">def randInitializeWeights(L_in, L_out):<br/>    """<br/>    randomly initializes the weights of a layer with L_in incoming connections and L_out outgoing connections.<br/>    """<br/>    <br/>    epi = (6**1/2) / (L_in + L_out)**1/2<br/>    <br/>    W = np.random.rand(L_out,L_in +1) *(2*epi) -epi<br/>    <br/>    return W</span><span id="9ba1" class="mc md it ly b gy mi mf l mg mh">initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)<br/>initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)<br/>initial_nn_params = np.append(initial_Theta1.flatten(),initial_Theta2.flatten())</span></pre><p id="3e54" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">最后，轮到我们使用前馈传播和反向传播来优化θ值。我使用的优化算法还是以前的梯度下降法。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="95aa" class="mc md it ly b gy me mf l mg mh">def gradientDescentnn(X,y,initial_nn_params,alpha,num_iters,Lambda,input_layer_size, hidden_layer_size, num_labels):<br/>    """<br/>    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps<br/>    with learning rate of alpha<br/>    <br/>    return theta and the list of the cost of theta during each iteration<br/>    """<br/>    Theta1 = initial_nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)<br/>    Theta2 = initial_nn_params[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)<br/>    <br/>    m=len(y)<br/>    J_history =[]<br/>    <br/>    for i in range(num_iters):<br/>        nn_params = np.append(Theta1.flatten(),Theta2.flatten())<br/>        cost, grad1, grad2 = nnCostFunction(nn_params,input_layer_size, hidden_layer_size, num_labels,X, y,Lambda)[3:]<br/>        Theta1 = Theta1 - (alpha * grad1)<br/>        Theta2 = Theta2 - (alpha * grad2)<br/>        J_history.append(cost)<br/>    <br/>    nn_paramsFinal = np.append(Theta1.flatten(),Theta2.flatten())<br/>    return nn_paramsFinal , J_history</span><span id="dbd6" class="mc md it ly b gy mi mf l mg mh">nnTheta, nnJ_history = gradientDescentnn(X,y,initial_nn_params,0.8,800,1,input_layer_size, hidden_layer_size, num_labels)<br/>Theta1 = nnTheta[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)<br/>Theta2 = nnTheta[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)</span></pre><p id="1da2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对执行代码的人的警告。根据您的计算能力，计算将花费相当多的时间，如果您正在优化 alpha 和 num_iters 值，时间甚至会更长。我对 alpha 使用 0.8，对 num_iters 使用 800，但是我相信通过更多的调整可以得到更好的精确度。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="dede" class="mc md it ly b gy me mf l mg mh">pred3 = predict(Theta1, Theta2, X)<br/>print("Training Set Accuracy:",sum(pred3[:,np.newaxis]==y)[0]/5000*100,"%")</span></pre><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mq"><img src="../Images/d469f24642705cc59fd4e18983bf7b0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*aLpB3lXsulujvlNTnoYYZg.png"/></div></div></figure></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="b64b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">至此，我已经完成了这个系列的一半。Jupyter 笔记本会上传到我的 GitHub 上(<a class="ae mj" href="https://github.com/Benlau93/Machine-Learning-by-Andrew-Ng-in-Python" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/Ben lau 93/Machine-Learning-by-Andrew-Ng-in-Python</a>)。</p><p id="5d56" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于本系列中的其他 python 实现，</p><ul class=""><li id="4ab9" class="mr ms it kh b ki kj km kn kq mt ku mu ky mv lc mw mx my mz bi translated"><a class="ae mj" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-linear-regression-dd04fba8e137" rel="noopener">线性回归</a></li><li id="71e4" class="mr ms it kh b ki na km nb kq nc ku nd ky ne lc mw mx my mz bi translated"><a class="ae mj" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-logistic-regression-c0ae25509feb" rel="noopener">逻辑回归</a></li><li id="acb3" class="mr ms it kh b ki na km nb kq nc ku nd ky ne lc mw mx my mz bi translated"><a class="ae mj" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-regularized-logistic-regression-lasso-regression-721f311130fb" rel="noopener">正则化逻辑回归</a></li><li id="cfca" class="mr ms it kh b ki na km nb kq nc ku nd ky ne lc mw mx my mz bi translated"><a class="ae mj" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-support-vector-machines-435fc34b7bf9" rel="noopener">支持向量机</a></li><li id="5945" class="mr ms it kh b ki na km nb kq nc ku nd ky ne lc mw mx my mz bi translated"><a class="ae mj" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-kmeans-clustering-pca-b7ba6fafa74" rel="noopener">无监督学习</a></li><li id="dd60" class="mr ms it kh b ki na km nb kq nc ku nd ky ne lc mw mx my mz bi translated"><a class="ae mj" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-anomaly-detection-1233d23dba95" rel="noopener">异常检测</a></li></ul><p id="a72e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">感谢您的阅读。</p></div></div>    
</body>
</html>