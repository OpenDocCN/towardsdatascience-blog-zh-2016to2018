<html>
<head>
<title>How to run spark batch jobs in AWS EMR using Apache Livy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用 Apache Livy 在 AWS EMR 中运行 spark 批处理作业</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-do-better-deployments-of-spark-batch-jobs-to-aws-emr-using-apache-livy-adc2417f0d8b?source=collection_archive---------7-----------------------#2018-11-30">https://towardsdatascience.com/how-to-do-better-deployments-of-spark-batch-jobs-to-aws-emr-using-apache-livy-adc2417f0d8b?source=collection_archive---------7-----------------------#2018-11-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/c0d341142899e1d62d34e971b5b5e101.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zDbk1KL2XHCoroHytJRG-g.jpeg"/></div></div></figure><p id="3cb1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本文中，我们将讨论在<a class="ae kw" href="https://livy.incubator.apache.org" rel="noopener ugc nofollow" target="_blank"> Apache Livy </a>的帮助下，使用 rest 接口在<a class="ae kw" href="https://aws.amazon.com/emr/" rel="noopener ugc nofollow" target="_blank"> AWS EMR </a>上运行 spark 作业。</p><p id="a60b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将完成以下步骤:</p><ul class=""><li id="e58f" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated">创建一个简单的批处理作业，从 Cassandra 读取数据并将结果写入 S3 的 parquet</li><li id="e0a3" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">建造罐子并把它存放在 S3</li><li id="56c4" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">提交作业并等待它通过 livy 完成</li></ul><p id="9c0d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ll">什么是阿帕奇李维？</em></p><blockquote class="lm ln lo"><p id="6292" class="jy jz ll ka b kb kc kd ke kf kg kh ki lp kk kl km lq ko kp kq lr ks kt ku kv ij bi translated">Apache Livy 是一个通过 REST 接口与 Spark 集群轻松交互的服务。它支持 Spark 作业或 Spark 代码片段的轻松提交、同步或异步结果检索，以及 Spark 上下文管理，所有这些都通过一个简单的 REST 接口或 RPC 客户端库实现。Apache Livy 还简化了 Spark 和应用服务器之间的交互，从而支持将 Spark 用于交互式 web/移动应用程序。</p></blockquote><p id="1a84" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ll">你为什么会使用它？</em></p><p id="6e62" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Apache livy 让我们的生活更轻松。我们不需要使用 EMR 步骤或 ssh 进入集群并运行 spark submit。我们只是使用了一个很好的 REST 接口。</p><p id="2213" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们看看它的实际效果。</p><h2 id="5483" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kj mb mc md kn me mf mg kr mh mi mj mk bi translated">步骤 1:构建简单的批处理作业</h2><p id="29b5" class="pw-post-body-paragraph jy jz iq ka b kb ml kd ke kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv ij bi translated">批处理作业将是一个 scala 应用程序。</p><p id="26b7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我们的应用程序中，我们首先构建 spark 会话，并确保我们可以连接到 Cassandra 集群:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="d7e7" class="ls lt iq mv b gy mz na l nb nc">val sparkConf = new SparkConf()</span><span id="35b1" class="ls lt iq mv b gy nd na l nb nc">sparkConf.set("spark.cassandra.connection.host", "YOUR_CASSANDRA_HOST")<br/>sparkConf.set("spark.cassandra.connection.port", "YOUR_CASSANDRA_PORT")</span><span id="de0f" class="ls lt iq mv b gy nd na l nb nc">val spark: SparkSession = SparkSession.builder()<br/>      .appName("RunBatchJob")<br/>      .config(sparkConf)<br/>      .getOrCreate()</span></pre><p id="88ac" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们还需要确保能够访问我们要写入数据的 S3 存储桶:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="eeb3" class="ls lt iq mv b gy mz na l nb nc">spark.sparkContext.hadoopConfiguration.set("fs.s3a.acl.default", "BucketOwnerFullControl")</span></pre><p id="008d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以从数据库中读取输入数据:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="b811" class="ls lt iq mv b gy mz na l nb nc">val input = spark.read<br/>      .format("org.apache.spark.sql.cassandra")<br/>      .options(Map("table" -&gt; "my_table", "keyspace" -&gt; "my_schema"))<br/>      .load()</span></pre><p id="cae9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Cassandra 桌子的结构非常简单:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="c698" class="ls lt iq mv b gy mz na l nb nc">CREATE TABLE my_schema.my_table (<br/>    id1 text PRIMARY KEY,<br/>    id2 text,<br/>    id3 text,<br/>    id4 text<br/>);</span></pre><p id="9918" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，我们只需将数据帧写入 S3 的一个拼花文件，同时删除重复的行:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="52c1" class="ls lt iq mv b gy mz na l nb nc">input<br/>      .dropDuplicates<br/>      .write<br/>      .mode("append")<br/>      .parquet("s3a://your_bucket/your_preffix/")</span></pre><h2 id="910b" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kj mb mc md kn me mf mg kr mh mi mj mk bi translated">步骤 2:将 jar 部署到 S3</h2><p id="5f9f" class="pw-post-body-paragraph jy jz iq ka b kb ml kd ke kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv ij bi translated">好，现在我们有了一个应用程序。我们需要把它装进一个<a class="ae kw" href="https://stackoverflow.com/questions/19150811/what-is-a-fat-jar" rel="noopener ugc nofollow" target="_blank">大罐子</a>里，然后复制到 S3。这样，远程访问 jar(从 livy)就更容易了。</p><p id="41d3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用<a class="ae kw" href="https://gradle.org" rel="noopener ugc nofollow" target="_blank"> gradle </a>管理我们的应用程序的依赖关系。在教程的最后，我会发布 github repo 的链接，在那里你可以找到 gradle 文件的完整代码和细节。我不会在这里讨论细节，因为这超出了本教程的范围。</p><p id="dc31" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要创建 fat jar，我们需要在应用程序的根目录下运行:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="8325" class="ls lt iq mv b gy mz na l nb nc">gradle shadowJar</span></pre><p id="1e58" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">生成后，我们可以使用<a class="ae kw" href="https://aws.amazon.com/cli/" rel="noopener ugc nofollow" target="_blank"> aws cli </a>将其复制到 S3:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="9805" class="ls lt iq mv b gy mz na l nb nc">aws s3 cp build/libs/spark_batch_job-1.0-SNAPSHOT-shadow.jar s3://your_bucket/your_prefix/</span></pre><p id="aee3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦这样做了，我们终于可以从使用 livy 开始我们的 spark 工作了。</p><h2 id="9ae5" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kj mb mc md kn me mf mg kr mh mi mj mk bi translated">步骤 3:通过 Livy 提交作业</h2><p id="2ade" class="pw-post-body-paragraph jy jz iq ka b kb ml kd ke kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv ij bi translated">我们将使用一个简单的 python 脚本来运行我们的命令。主要功能非常简单:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="6c62" class="ls lt iq mv b gy mz na l nb nc">def run_spark_job(master_dns):<br/>    response = spark_submit(master_dns)<br/>    track_statement_progress(master_dns, response)</span></pre><p id="5788" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">它将首先提交作业，然后等待它完成。<em class="ll"> track_statement_progress </em>步骤对于检测我们的作业是否成功运行非常有用。<em class="ll"> master_dns </em>是 EMR 集群的地址。让我们更深入地研究我们各自的方法。</p><p id="63fc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ll"> spark_submit </em>功能:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="fb83" class="ls lt iq mv b gy mz na l nb nc">host = '<a class="ae kw" rel="noopener ugc nofollow" target="_blank" href="/'">http://'</a> + master_dns + ':8999'</span><span id="2208" class="ls lt iq mv b gy nd na l nb nc">data = {'className': "com.app.RunBatchJob", "conf":{"spark.hadoop.fs.s3a.impl":"org.apache.hadoop.fs.s3a.S3AFileSystem"}, 'file': "s3a://your_bucket/spark_batch_job-1.0-SNAPSHOT-shadow.jar"}</span><span id="d5b2" class="ls lt iq mv b gy nd na l nb nc">headers = {'Content-Type': 'application/json'}</span><span id="f384" class="ls lt iq mv b gy nd na l nb nc">response = requests.post(host + '/batches', data=json.dumps(data), headers=headers)</span></pre><p id="d49c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这只是使用请求库完成的一个<em class="ll"> post </em>。在 EMR 上，livy 服务器运行在端口<strong class="ka ir"> 8999 上。</strong>我们请求中的数据本质上是我们将提供给<em class="ll"> spark-submit </em>命令的参数。我们需要使用<strong class="ka ir"> /batches </strong>端点。这将告诉 livy 我们将提交一个批处理作业。</p><p id="dd30" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以及<em class="ll">跟踪声明进度</em>函数。这将每 10 秒轮询一次 livy 服务器，并检查应用程序的状态是否为<em class="ll">成功。</em>如果是，则作业已成功完成:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="65eb" class="ls lt iq mv b gy mz na l nb nc">statement_status = ''<br/>host = '<a class="ae kw" rel="noopener ugc nofollow" target="_blank" href="/'">http://'</a> + master_dns + ':8999'<br/>session_url = host + response_headers['location'].split('/statements', 1)[0]</span><span id="a471" class="ls lt iq mv b gy nd na l nb nc">while statement_status != 'success':<br/>        statement_url = host + response_headers['location']<br/>        statement_response = requests.get(statement_url, headers={'Content-Type': 'application/json'})<br/>        statement_status = statement_response.json()['state']<br/>        logging.info('Statement status: ' + statement_status)</span><span id="ca87" class="ls lt iq mv b gy nd na l nb nc">        lines = requests.get(session_url + '/log', headers={'Content-Type': 'application/json'}).json()['log']<br/>        for line in lines:<br/>            logging.info(line)</span><span id="d29a" class="ls lt iq mv b gy nd na l nb nc">        if statement_status == 'dead':<br/>            raise ValueError('Exception in the app caused it to be dead: ' + statement_status)</span><span id="02c4" class="ls lt iq mv b gy nd na l nb nc">        if 'progress' in statement_response.json():<br/>            logging.info('Progress: ' + str(statement_response.json()['progress']))<br/>        time.sleep(10)</span></pre><p id="ea02" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是一个较长的函数，所以我将尝试一步一步地解释它。对于运行时间较长的作业，livy 会更改 url，因此我们需要存储最新的一个:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="1ff3" class="ls lt iq mv b gy mz na l nb nc">statement_url = host + response_headers['location']</span></pre><p id="e5dc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，我们获取集群的当前状态并记录下来:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="0510" class="ls lt iq mv b gy mz na l nb nc">statement_response = requests.get(statement_url, headers={'Content-Type': 'application/json'})<br/>statement_status = statement_response.json()['state']<br/>logging.info('Statement status: ' + statement_status)</span></pre><p id="1f4c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们也正在获取应用程序的日志并显示它们。这对调试错误很有用:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="eb97" class="ls lt iq mv b gy mz na l nb nc">lines = requests.get(session_url + '/log', headers={'Content-Type': 'application/json'}).json()['log']<br/>for line in lines:<br/>    logging.info(line)</span></pre><p id="b090" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果我们有一个严重的异常，那么应用程序将会死亡。我们需要标记它并抛出一个异常:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="fca2" class="ls lt iq mv b gy mz na l nb nc">if statement_status == 'dead':<br/>    raise ValueError('Exception in the app caused it to be dead: ' + statement_status)</span></pre><p id="f571" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后一步，我们记录进度并等待 10 秒钟，直到我们再次检查应用程序的状态:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="92e2" class="ls lt iq mv b gy mz na l nb nc">if 'progress' in statement_response.json():<br/>    logging.info('Progress: ' + str(statement_response.json()['progress']))<br/>time.sleep(10)</span></pre><p id="e03a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这就是全部。代码在 github 上，可以从<a class="ae kw" href="https://github.com/BogdanCojocar/medium-articles/tree/master/livy_batch_emr" rel="noopener ugc nofollow" target="_blank">这里</a>访问。</p></div></div>    
</body>
</html>