<html>
<head>
<title>Decision Trees and Random Forests for Classification and Regression pt.2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于分类和回归的决策树和随机森林第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-trees-and-random-forests-for-classification-and-regression-pt-2-2b1fcd03e342?source=collection_archive---------2-----------------------#2017-11-06">https://towardsdatascience.com/decision-trees-and-random-forests-for-classification-and-regression-pt-2-2b1fcd03e342?source=collection_archive---------2-----------------------#2017-11-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/e7ec38cbcc997c94003ea8b71e798991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2u09gqaZqk0Dfk7GjV0uCw.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Forest from the trees, mountains from the dust.</figcaption></figure><h1 id="858a" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">亮点:</strong></h1><p id="1a2b" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在本文中，我们将了解以下内容:</p><ul class=""><li id="8924" class="mb mc it lf b lg md lk me lo mf ls mg lw mh ma mi mj mk ml bi translated">用于鲁棒学习的自举聚合</li><li id="89f6" class="mb mc it lf b lg mm lk mn lo mo ls mp lw mq ma mi mj mk ml bi translated">变量选择的随机森林</li><li id="b1a3" class="mb mc it lf b lg mm lk mn lo mo ls mp lw mq ma mi mj mk ml bi translated">随机森林用于快速和稳健的回归、分类和特征选择分析</li></ul><p id="bb10" class="pw-post-body-paragraph ld le it lf b lg md li lj lk me lm ln lo mr lq lr ls ms lu lv lw mt ly lz ma im bi translated"><a class="ae mu" rel="noopener" target="_blank" href="/decision-trees-and-random-forests-for-classification-and-regression-pt-1-dbb65a458df">点击这里链接到第一部分。</a></p><p id="385d" class="pw-post-body-paragraph ld le it lf b lg md li lj lk me lm ln lo mr lq lr ls ms lu lv lw mt ly lz ma im bi translated">链接到Neptune.ai关于随机森林算法在哪里会失败的文章:<a class="ae mu" href="https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why" rel="noopener ugc nofollow" target="_blank">https://Neptune . ai/blog/Random-Forest-regression-when-do-it-fail-and-why</a></p><p id="213d" class="pw-post-body-paragraph ld le it lf b lg md li lj lk me lm ln lo mr lq lr ls ms lu lv lw mt ly lz ma im bi translated">链接到我的其他文章:</p><ol class=""><li id="fcb0" class="mb mc it lf b lg md lk me lo mf ls mg lw mh ma mv mj mk ml bi translated"><a class="ae mu" rel="noopener" target="_blank" href="/custom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a">tensor flow中的自定义损失函数</a></li><li id="ab7a" class="mb mc it lf b lg mm lk mn lo mo ls mp lw mq ma mv mj mk ml bi translated"><a class="ae mu" rel="noopener" target="_blank" href="/the-softmax-function-neural-net-outputs-as-probabilities-and-ensemble-classifiers-9bd94d75932"> Softmax分类</a></li><li id="0266" class="mb mc it lf b lg mm lk mn lo mo ls mp lw mq ma mv mj mk ml bi translated"><a class="ae mu" rel="noopener" target="_blank" href="/analyzing-climate-patterns-with-self-organizing-maps-soms-8d4ef322705b">气候分析</a></li><li id="6296" class="mb mc it lf b lg mm lk mn lo mo ls mp lw mq ma mv mj mk ml bi translated"><a class="ae mu" href="https://medium.com/@hhl60492/black-swans-and-hockey-riots-extreme-value-analysis-and-generalized-extreme-value-distributions-d4b4b84cd374" rel="noopener">曲棍球骚乱和极端值</a></li></ol><h1 id="b4b0" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">简介:</strong></h1><p id="d78c" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在我的上一篇文章中，我们讨论了决策树以及它们如何用于分类和回归。在这篇文章中，我们将继续我们停止的地方，并介绍集合决策树模型或所谓的<strong class="lf iu">随机森林</strong>。我们将看到决策树的优势与引导聚合的结合如何使随机森林成为<strong class="lf iu">非常健壮而简单的</strong>学习模型，与监督学习问题的单一决策树相比，不容易过度拟合。为了感受随机森林的威力，下面我们用它们从公开的飞行路线数据中识别美国政府驾驶的间谍飞机。</p><h1 id="ed72" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">引导聚合:</h1><p id="6c70" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">Bootstrap Aggregation或bagging是一种强大的技术，可以减少模型方差(过拟合)并改善有限样本(即少量观察值)或不稳定数据集的学习结果。Bagging的工作原理是获取原始数据集并创建<em class="mw"> M </em>个子集，每个子集有<em class="mw"> n </em>个样本。从原始数据集中用替换对<em class="mw"> n个</em>个体样本进行<strong class="lf iu">均匀采样。下图说明了这一点。</strong></p><figure class="my mz na nb gt ju gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/6eac726b7693f83d7653521afb3a9543.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*K4g0T7pjILwT6vA-EqkQ1Q.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Subsetting for bootstrap aggregation.</figcaption></figure><p id="36b2" class="pw-post-body-paragraph ld le it lf b lg md li lj lk me lm ln lo mr lq lr ls ms lu lv lw mt ly lz ma im bi translated">在上图中，保留了对应于每个数据点的标签。换句话说，每个数据元组<em class="mw">(</em><strong class="lf iu"><em class="mw">x</em></strong><em class="mw">，</em><strong class="lf iu"><em class="mw">y</em></strong><em class="mw">)</em><strong class="lf iu"><em class="mw">ᵢ</em></strong><em class="mw"/>被采样和子集化，其中每个<strong class="lf iu"><em class="mw">【xᵢ</em></strong>是输入的向量，<strong class="lf iu"> <em class="mw"> Yᵢ </em> </strong>是向量理论上，随着bootstrap样本数<em class="mw"> M </em>接近无穷大，bagging被证明收敛于某个非bagged函数估计量的均值，该估计量利用了来自原始数据集的所有可能样本(直观上，这是有意义的)。在随机梯度学习的情况下，例如在神经网络或逻辑回归中，以随机顺序从多个(重复的)数据样本中学习往往会提高学习性能，因为梯度估计往往更多地被“推来推去”,希望克服局部极值。同样，在<a class="ae mu" href="https://www.researchgate.net/publication/45130375_Bagging_Boosting_and_Ensemble_Methods" rel="noopener ugc nofollow" target="_blank"> Bühlmann的文章</a>中显示的一个有趣的结果表明，bagging倾向于<em class="mw">将</em> <em class="mw">偏差</em>添加到bagged估计量中，以减少方差为代价。在大多数实际应用中，与方差的减少相比，这种偏差的增加是很小的。一般来说，<a class="ae mu" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff" rel="noopener ugc nofollow" target="_blank">偏差-方差权衡</a>是统计学习的一个非常重要的方面，在挑选监督学习模型时，这是一个熟练的数据魔术师应该很清楚的事情。</p><p id="229f" class="pw-post-body-paragraph ld le it lf b lg md li lj lk me lm ln lo mr lq lr ls ms lu lv lw mt ly lz ma im bi translated">接下来，为每个<em class="mw"> M </em>引导样本创建<em class="mw"> k </em>个个体学习模型(称为<strong class="lf iu">集合</strong>)。然后，以某种方式，如投票或简单平均法，对每个单独学习模型的输出进行汇总或平均。下图对此进行了说明。</p><figure class="my mz na nb gt ju gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/502095606facd394cf7349195357e8f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*Qmx6upsk3bxIbdJr8y2aDA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Bootstrapping and learning ensembles.</figcaption></figure><p id="abb2" class="pw-post-body-paragraph ld le it lf b lg md li lj lk me lm ln lo mr lq lr ls ms lu lv lw mt ly lz ma im bi translated">一般来说，使用集成模型打包是一种稳健的方法，通过利用引导样本和聚合学习集成的输出(均值、中值、其他更复杂的方法)来减少学习模型的方差和过度拟合。Bagging和集成是通用的，可以应用于任何监督模型，从神经网络到SVM到决策树，以及非监督聚类模型(将在另一篇文章中讨论)。在实践中，<em class="mw"> M </em>被选择为至少50，而<em class="mw"> n </em>是原始数据集大小的80%。</p><h1 id="1d56" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">随机森林:</h1><p id="1380" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">随机森林是一个由<em class="mw"> k </em>个未训练的决策树(只有一个根节点的树)和<em class="mw"> M </em>个引导样本(<em class="mw"> k </em>和<em class="mw"> M </em>不必相同)组成的集合，这些样本使用<a class="ae mu" href="https://en.wikipedia.org/wiki/Random_subspace_method" rel="noopener ugc nofollow" target="_blank">随机子空间方法</a>或特征打包方法的变体进行训练。注意，训练随机森林的方法不像对一堆单独的决策树应用bagging，然后简单地聚合输出那样简单。训练随机森林的过程如下:</p><ol class=""><li id="0ebf" class="mb mc it lf b lg md lk me lo mf ls mg lw mh ma mv mj mk ml bi translated">在当前节点<em class="mw">，</em>从可用特征<em class="mw"> D </em>中随机选择<em class="mw"> p </em>特征。特征数量<em class="mw"> p </em>通常远小于特征总数<em class="mw"> D </em>。</li><li id="26e3" class="mb mc it lf b lg mm lk mn lo mo ls mp lw mq ma mv mj mk ml bi translated">使用指定的分裂度量(Gini杂质、信息增益等)计算树<em class="mw"> k </em>的最佳分裂点。)并将当前节点分割成子节点，并从此节点开始减少特征<em class="mw"> D </em>的数量。</li><li id="c061" class="mb mc it lf b lg mm lk mn lo mo ls mp lw mq ma mv mj mk ml bi translated">重复步骤1到2，直到达到最大树深度<em class="mw"> l </em>或者分裂度量达到某个极值。</li><li id="7e61" class="mb mc it lf b lg mm lk mn lo mo ls mp lw mq ma mv mj mk ml bi translated">对森林中的每棵树重复步骤1到3。</li><li id="448a" class="mb mc it lf b lg mm lk mn lo mo ls mp lw mq ma mv mj mk ml bi translated">投票或合计森林中每棵树的产量。</li></ol><p id="2a5f" class="pw-post-body-paragraph ld le it lf b lg md li lj lk me lm ln lo mr lq lr ls ms lu lv lw mt ly lz ma im bi translated">与单决策树相比，随机森林通过在每个分裂点选择多个特征变量而不是单个特征变量来分裂。直观地说，决策树的变量选择特性可以通过使用这种特征打包过程得到极大的改善。<strong class="lf iu">通常，树的数量<em class="mw"> k </em>很大，对于具有许多特征的大型数据集，数量级为数百到数千。</strong></p><h1 id="f915" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">变量选择:</h1><p id="be82" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">随机森林的变量选择非常简单。使用<a class="ae mu" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn的RandomForestClassifier </a>，让我们加载我们最喜欢的数据集(有高档葡萄酒的数据集)，看看我们的随机森林分类器认为葡萄酒分类最重要的特征是什么。滚动Jupyter笔记本以查看功能信息图和作为功能数量函数的F1平均分数(您可能应该<a class="ae mu" href="https://gist.github.com/hhl60492/9e01be15b291e8ceecb37d1c08f97521" rel="noopener ugc nofollow" target="_blank">从我的github </a>下载Jupyter笔记本文件/要点以更好地查看这些图)。</p><figure class="my mz na nb gt ju"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="69c8" class="pw-post-body-paragraph ld le it lf b lg md li lj lk me lm ln lo mr lq lr ls ms lu lv lw mt ly lz ma im bi translated">有趣的是，当<em class="mw"> n_trees </em> = 1000时，<em class="mw">脯氨酸</em>含量仍然是最具信息量的特征。在我的Core i7笔记本电脑上用1000棵树进行训练需要几秒钟，这比大多数深度神经网络模型快得多。对于较小的树，<em class="mw">颜色强度</em>倾向于出现在顶部。当<em class="mw"> n_trees </em> = 1000时，<em class="mw">颜色_强度</em>仍能与<em class="mw">脯氨酸</em>含量相当接近。<strong class="lf iu">一般来说，您应该扫描超参数<em class="mw"> n_trees </em>并评估诊断图，以更好地了解数据集以及哪些特征是重要的。</strong>一旦绘制了每个特征的相对信息/重要性图，高于某个阈值的特征可以用于另一个学习模型，如深度神经网络，低于阈值的特征可以忽略。跨越特征数量的F1分数应该在具有有限少数学习者的非常受限的集合模型上完成。笔记本中的F1扫描图显示了向模型添加要素或变量的效果，即添加更多输入要素对分类准确性的影响(注意F1得分和AUROCs仅针对分类问题定义，回归问题需要不同的误差测量，如均方差)。</p><h1 id="6625" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">总而言之:</h1><p id="fcae" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">随机森林可用于稳健分类、回归和特征选择分析。希望你能看到，在你期望得到像样的结果之前，你还需要先运用一些技巧。</p><p id="e3d5" class="pw-post-body-paragraph ld le it lf b lg md li lj lk me lm ln lo mr lq lr ls ms lu lv lw mt ly lz ma im bi translated">关于监督学习问题，对你的简单模型进行基准测试总是一个好主意，比如随机森林与复杂的深度神经网络或概率模型。如果你有任何关于运行代码的问题，或者关于一般生活的奥秘，请不要犹豫问我。</p></div></div>    
</body>
</html>