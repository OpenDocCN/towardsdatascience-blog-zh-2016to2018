<html>
<head>
<title>Multi Label Text Classification with Scikit-Learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Scikit-Learn进行多标签文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5?source=collection_archive---------0-----------------------#2018-04-21">https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5?source=collection_archive---------0-----------------------#2018-04-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/6a6c0325ac93d892c490912d95d26028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iROLS5mMuuUbKH9ysT-ULQ.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo credit: Pexels</figcaption></figure><p id="a26c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae la" rel="noopener" target="_blank" href="/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f">多类分类</a>指两个以上类的分类任务；每个标签都是互斥的。分类假设每个样本被分配给一个且仅一个标签。</p><p id="d020" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">另一方面，<a class="ae la" href="https://en.wikipedia.org/wiki/Multi-label_classification" rel="noopener ugc nofollow" target="_blank">多标签分类</a>给每个样本分配一组目标标签。这可以被认为是预测数据点的属性，这些属性并不相互排斥，例如Tim Horton经常被归类为面包店和咖啡店。多标签文本分类在现实世界中有很多应用，比如对Yelp上的企业进行分类，或者将电影分为一个或多个类型。</p><h1 id="602e" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">问题定式化</h1><p id="a534" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">任何在网上成为辱骂或骚扰目标的人都会知道，当你注销或关掉手机时，这种情况不会消失。谷歌的研究人员正致力于研究在线有毒评论的工具。在这篇文章中，我们将建立一个多标签模型，能够检测不同类型的毒性，如严重中毒，威胁，淫秽，侮辱，等等。我们将使用监督分类器和文本表示。毒性评论可能是关于毒性、严重毒性、淫秽、威胁、侮辱或身份仇恨中的任何一种，也可能不是以上任何一种。数据集可以在<a class="ae la" href="https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline/data" rel="noopener ugc nofollow" target="_blank">卡格尔</a>找到。</p><p id="2036" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">(<em class="me">来自数据源的免责声明:数据集包含可能被视为亵渎、粗俗或冒犯的文本。)</em></p><h1 id="5444" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">探索</h1><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="5a9d" class="mo lc iq mk b gy mp mq l mr ms">%matplotlib inline<br/>import re<br/>import matplotlib<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.naive_bayes import MultinomialNB<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.multiclass import OneVsRestClassifier<br/>from nltk.corpus import stopwords<br/>stop_words = set(stopwords.words('english'))<br/>from sklearn.svm import LinearSVC<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.pipeline import Pipeline<br/>import seaborn as sns</span><span id="7eb2" class="mo lc iq mk b gy mt mq l mr ms">df = pd.read_csv("train 2.csv", encoding = "ISO-8859-1")<br/>df.head()</span></pre><p id="e96d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">每个类别中的评论数量</strong></p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="9ddc" class="mo lc iq mk b gy mp mq l mr ms">df_toxic = df.drop(['id', 'comment_text'], axis=1)<br/>counts = []<br/>categories = list(df_toxic.columns.values)<br/>for i in categories:<br/>    counts.append((i, df_toxic[i].sum()))<br/>df_stats = pd.DataFrame(counts, columns=['category', 'number_of_comments'])<br/>df_stats</span></pre><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/f0f398c1d7bfaa1f141efa2f4bd2aa58.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*4LSNqH9MFfax8PAwmv5yow.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1</figcaption></figure><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="a7ce" class="mo lc iq mk b gy mp mq l mr ms">df_stats.plot(x='category', y='number_of_comments', kind='bar', legend=False, grid=True, figsize=(8, 5))<br/>plt.title("Number of comments per category")<br/>plt.ylabel('# of Occurrences', fontsize=12)<br/>plt.xlabel('category', fontsize=12)</span></pre><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/3c1c6ebe7321018a35e27ec1be869378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*-7gj7qzpfJNZpUqtt8iEtA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2</figcaption></figure><h1 id="124a" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">多标签</h1><p id="75bd" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated"><strong class="ke ir">有多少评论有多重标签？</strong></p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="a90e" class="mo lc iq mk b gy mp mq l mr ms">rowsums = df.iloc[:,2:].sum(axis=1)<br/>x=rowsums.value_counts()</span><span id="9712" class="mo lc iq mk b gy mt mq l mr ms">#plot<br/>plt.figure(figsize=(8,5))<br/>ax = sns.barplot(x.index, x.values)<br/>plt.title("Multiple categories per comment")<br/>plt.ylabel('# of Occurrences', fontsize=12)<br/>plt.xlabel('# of categories', fontsize=12)</span></pre><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/b54c2e06780ae9ffe893cdd0c4d50bba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*Zn54E8q9d36klf1AmTgOxg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3</figcaption></figure><p id="f567" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">绝大多数评论文字都没有标注。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="8109" class="mo lc iq mk b gy mp mq l mr ms">print('Percentage of comments that are not labelled:')<br/>print(len(df[(df['toxic']==0) &amp; (df['severe_toxic']==0) &amp; (df['obscene']==0) &amp; (df['threat']== 0) &amp; (df['insult']==0) &amp; (df['identity_hate']==0)]) / len(df))</span></pre><p id="7850" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="me">未标注评论的百分比:<br/>0.8983211235124177</em></strong></p><p id="3e1b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">评论文本字数分布</strong>。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="d0b5" class="mo lc iq mk b gy mp mq l mr ms">lens = df.comment_text.str.len()<br/>lens.hist(bins = np.arange(0,5000,50))</span></pre><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/6708d8b4e07ebcad3b23b3054b78a990.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*p1G3zpFBrmWeN1yLVI3ZfQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4</figcaption></figure><p id="4202" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">大多数评论文本长度在500个字符以内，有些异常值长达5000个字符。</p><p id="bd76" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">注释文本列中没有缺失的注释。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="18bf" class="mo lc iq mk b gy mp mq l mr ms">print('Number of missing comments in comment text:')<br/>df['comment_text'].isnull().sum()</span></pre><p id="70fd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="me">注释文本中缺失的注释数:</em> </strong></p><p id="fa5a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="me"> 0 </em> </strong></p><p id="d407" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">先偷看一下第一条评论，文字需要清理。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="45b2" class="mo lc iq mk b gy mp mq l mr ms">df['comment_text'][0]</span></pre><p id="e75c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">"<strong class="ke ir"> <em class="me">解释\ r为什么在我的用户名Hardcore Metallica Fan下所做的编辑被恢复？他们不是故意破坏，只是我在纽约娃娃FAC投票后关闭了一些煤气。请不要把模板从对话页面上删除，因为我现在退休了。89.205.38.27 </em> </strong>”</p><h1 id="e42e" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据预处理</h1><p id="ec3a" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">创建一个函数来清理文本</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="cd3a" class="mo lc iq mk b gy mp mq l mr ms">def clean_text(text):<br/>    text = text.lower()<br/>    text = re.sub(r"what's", "what is ", text)<br/>    text = re.sub(r"\'s", " ", text)<br/>    text = re.sub(r"\'ve", " have ", text)<br/>    text = re.sub(r"can't", "can not ", text)<br/>    text = re.sub(r"n't", " not ", text)<br/>    text = re.sub(r"i'm", "i am ", text)<br/>    text = re.sub(r"\'re", " are ", text)<br/>    text = re.sub(r"\'d", " would ", text)<br/>    text = re.sub(r"\'ll", " will ", text)<br/>    text = re.sub(r"\'scuse", " excuse ", text)<br/>    text = re.sub('\W', ' ', text)<br/>    text = re.sub('\s+', ' ', text)<br/>    text = text.strip(' ')<br/>    return text</span></pre><p id="4921" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">清理comment_text列:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="8701" class="mo lc iq mk b gy mp mq l mr ms">df['comment_text'] = df['comment_text'].map(lambda com : clean_text(com))</span><span id="56a8" class="mo lc iq mk b gy mt mq l mr ms">df['comment_text'][0]</span></pre><p id="89d6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="me">解释为什么我的用户名hardcore metallica fan下所做的编辑被恢复，他们不是故意破坏，只是在我在纽约娃娃fac投票后关闭了一些气体，请不要从对话页面删除模板，因为我现在退休了89 205 38 27 </em> </strong></p><p id="5430" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">好多了！</p><p id="e698" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">将数据拆分为定型集和测试集:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="f070" class="mo lc iq mk b gy mp mq l mr ms">categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']</span><span id="ecae" class="mo lc iq mk b gy mt mq l mr ms">train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)</span><span id="870c" class="mo lc iq mk b gy mt mq l mr ms">X_train = train.comment_text<br/>X_test = test.comment_text<br/>print(X_train.shape)<br/>print(X_test.shape)</span></pre><p id="918c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="me"> (106912，)<br/> (52659，)</em> </strong></p><h1 id="ecdb" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">分类器训练</h1><h2 id="8127" class="mo lc iq bd ld my mz dn lh na nb dp ll kn nc nd lp kr ne nf lt kv ng nh lx ni bi translated">管道</h2><p id="79f7" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">Scikit-learn提供了一个管道实用程序来帮助自动化机器学习工作流。管道在机器学习系统中非常常见，因为有大量数据要操作，并且有许多数据转换要应用。因此，我们将利用流水线来训练每个分类器。</p><h2 id="1993" class="mo lc iq bd ld my mz dn lh na nb dp ll kn nc nd lp kr ne nf lt kv ng nh lx ni bi translated">OneVsRest多标签策略</h2><p id="552e" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">多标签算法接受多个标签上的二进制掩码。每个预测的结果将是一个由0和1组成的数组，用于标记哪些类标签适用于每个行输入样本。</p><h2 id="bff4" class="mo lc iq bd ld my mz dn lh na nb dp ll kn nc nd lp kr ne nf lt kv ng nh lx ni bi translated">朴素贝叶斯</h2><p id="4499" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">OneVsRest策略可用于多标签学习，例如，使用分类器来预测多个标签。朴素贝叶斯支持多类，但我们处于多标签场景中，因此，我们将朴素贝叶斯包装在OneVsRestClassifier中。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="1a4e" class="mo lc iq mk b gy mp mq l mr ms"># Define a pipeline combining a text feature extractor with multi lable classifier<br/>NB_pipeline = Pipeline([<br/>                ('tfidf', TfidfVectorizer(stop_words=stop_words)),<br/>                ('clf', OneVsRestClassifier(MultinomialNB(<br/>                    fit_prior=True, class_prior=None))),<br/>            ])</span><span id="2fa7" class="mo lc iq mk b gy mt mq l mr ms">for category in categories:<br/>    print('... Processing {}'.format(category))<br/>    # train the model using X_dtm &amp; y<br/>    NB_pipeline.fit(X_train, train[category])<br/>    # compute the testing accuracy<br/>    prediction = NB_pipeline.predict(X_test)<br/>    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))</span></pre><p id="6af7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="me"> …处理有毒<br/>测试精度为0.9191401279933155 <br/> …处理严重_有毒<br/>测试精度为0.9900112041626312 <br/> …处理淫秽<br/>测试精度为0.95148027877747584<br/>…处理威胁<br/>测试精度为0.9971135033</em></strong></p><h2 id="db42" class="mo lc iq bd ld my mz dn lh na nb dp ll kn nc nd lp kr ne nf lt kv ng nh lx ni bi translated">线性SVC</h2><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="8644" class="mo lc iq mk b gy mp mq l mr ms">SVC_pipeline = Pipeline([<br/>                ('tfidf', TfidfVectorizer(stop_words=stop_words)),<br/>                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),<br/>            ])</span><span id="67c6" class="mo lc iq mk b gy mt mq l mr ms">for category in categories:<br/>    print('... Processing {}'.format(category))<br/>    # train the model using X_dtm &amp; y<br/>    SVC_pipeline.fit(X_train, train[category])<br/>    # compute the testing accuracy<br/>    prediction = SVC_pipeline.predict(X_test)<br/>    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))</span></pre><p id="5f31" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="me"> …处理有毒<br/>测试精度为0.9599498661197516 <br/> …处理严重_有毒<br/>测试精度为0.9906948479842003 <br/> …处理淫秽<br/>测试精度为0.9789019920621356 <br/> …处理威胁<br/>测试精度为0.997417334</em></strong></p><h2 id="b16f" class="mo lc iq bd ld my mz dn lh na nb dp ll kn nc nd lp kr ne nf lt kv ng nh lx ni bi translated">逻辑回归</h2><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="7539" class="mo lc iq mk b gy mp mq l mr ms">LogReg_pipeline = Pipeline([<br/>                ('tfidf', TfidfVectorizer(stop_words=stop_words)),<br/>                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),<br/>            ])</span><span id="8b4b" class="mo lc iq mk b gy mt mq l mr ms">for category in categories:<br/>    print('... Processing {}'.format(category))<br/>    # train the model using X_dtm &amp; y<br/>    LogReg_pipeline.fit(X_train, train[category])<br/>    # compute the testing accuracy<br/>    prediction = LogReg_pipeline.predict(X_test)<br/>    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))</span></pre><p id="3625" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="me"> …处理有毒的<br/>测试精度为0.9548415275641391 <br/> …处理严重_有毒的<br/>测试精度为0.9910556600011394 <br/> …处理淫秽的<br/>测试精度为0.9761104464573956 <br/> …处理威胁的<br/>测试精度为0.999779395</em></strong></p><p id="3597" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">三个分类器产生了相似的结果。我们已经为有毒评论多标签文本分类问题创建了一个强大的基线。</p><p id="1e0f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这篇文章的完整代码可以在Github上找到。我期待听到任何反馈或意见。</p></div></div>    
</body>
</html>