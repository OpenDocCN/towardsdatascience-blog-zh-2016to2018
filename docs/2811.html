<html>
<head>
<title>Capsule Neural Networks: The Next Neural Networks? Part 1: CNNs and their problems.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">胶囊神经网络:下一代神经网络？第一部分:CNN及其问题。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/capsule-neural-networks-are-here-to-finally-recognize-spatial-relationships-693b7c99b12?source=collection_archive---------3-----------------------#2018-03-09">https://towardsdatascience.com/capsule-neural-networks-are-here-to-finally-recognize-spatial-relationships-693b7c99b12?source=collection_archive---------3-----------------------#2018-03-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ffbd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">可选(“常规”)神经网络是机器学习领域的最新热点，但它们也有缺陷。胶囊神经网络是Hinton的最新发展，它帮助我们解决了其中的一些问题。</h2></div><blockquote class="kn ko kp"><p id="9256" class="kq kr ks kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是<strong class="kt ir">系列</strong>帖子的一部分，旨在教授胶囊神经网络。这第一篇文章将解释“正常”(卷积)神经网络及其问题。</p></blockquote><p id="0d67" class="pw-post-body-paragraph kq kr iq kt b ku kv jr kw kx ky ju kz ln lb lc ld lo lf lg lh lp lj lk ll lm ij bi translated">神经网络可能是机器学习中最热门的领域。近年来，有许多新的发展，改善神经网络和建设，使他们更容易。然而，它们大多是渐进式的，例如增加更多的层或稍微改进激活功能，但没有引入一种<em class="ks">新的</em>类型的架构或主题。<br/> <br/> <a class="ae lq" href="https://research.google.com/pubs/GeoffreyHinton.html" rel="noopener ugc nofollow" target="_blank"> Geoffery Hinton </a>是许多高度利用的深度学习算法的创始人之一，包括神经网络的许多发展——难怪，有神经科学和人工智能背景。<br/><br/>2017年10月下旬，Geoffrey Hinton、Sara Sabour和Nicholas Frosst在Google Brain下发表了一篇名为“<a class="ae lq" href="https://arxiv.org/abs/1710.09829" rel="noopener ugc nofollow" target="_blank">胶囊间动态路由</a>”的研究论文，介绍了神经网络的一项真正创新。这是令人兴奋的，因为这种发展已经被期待了很久，将可能刺激更多的研究和围绕它的进展，并被认为使神经网络比现在更好。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="8382" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">基线:卷积神经网络</h1><p id="a1d0" class="pw-post-body-paragraph kq kr iq kt b ku mq jr kw kx mr ju kz ln ms lc ld lo mt lg lh lp mu lk ll lm ij bi translated">卷积神经网络(CNN)是非常灵活的机器学习模型，它最初是受我们大脑工作原理的启发。</p><blockquote class="kn ko kp"><p id="ec7d" class="kq kr ks kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">神经网络利用“神经元”层将原始数据处理成模式和对象。</p></blockquote><p id="c86a" class="pw-post-body-paragraph kq kr iq kt b ku kv jr kw kx ky ju kz ln lb lc ld lo lf lg lh lp lj lk ll lm ij bi translated">神经网络的主要构件是“卷积”层(因此得名)。它是做什么的？它从上一层获取原始信息，理解其中的模式，并将其向前发送到下一层，以理解更大的画面。<br/> <br/>如果你是神经网络新手，想了解它，我推荐:</p><ol class=""><li id="cf30" class="mv mw iq kt b ku kv kx ky ln mx lo my lp mz lm na nb nc nd bi translated">观看由<a class="ae lq" href="https://www.youtube.com/watch?v=aircAruvnKk" rel="noopener ugc nofollow" target="_blank"> 3Blue1Brown </a>制作的动画视频。</li><li id="282b" class="mv mw iq kt b ku ne kx nf ln ng lo nh lp ni lm na nb nc nd bi translated">想要更详细的文字/视觉指南，你可以看看这个<a class="ae lq" href="https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/" rel="noopener ugc nofollow" target="_blank">初学者博客</a></li><li id="3839" class="mv mw iq kt b ku ne kx nf ln ng lo nh lp ni lm na nb nc nd bi translated">如果你能处理更多的数学和细节，你可以阅读斯坦福大学CS231的指南。<br/> <br/> <em class="ks">以防你没有做到以上任何一点，并打算继续，这里有一个手动波浪形的简要概述。</em></li></ol><h2 id="b145" class="nj lz iq bd ma nk nl dn me nm nn dp mi ln no np mk lo nq nr mm lp ns nt mo nu bi translated"><strong class="ak">卷积神经网络背后的直觉</strong></h2><p id="6d20" class="pw-post-body-paragraph kq kr iq kt b ku mq jr kw kx mr ju kz ln ms lc ld lo mt lg lh lp mu lk ll lm ij bi translated">让我们从头开始。<br/>神经网络接收原始输入数据。假设这是一只狗的涂鸦。当你看到一只狗，你的大脑会自动检测出它是一只狗。但是对于计算机来说，图像实际上只是代表颜色通道中颜色强度的一组数字。假设它只是一个黑色的白色涂鸦，那么我们可以用一个数组来表示它，其中每个单元格代表像素从黑到白的亮度。</p><figure class="nw nx ny nz gt oa gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/ce5829c702fe36ff380a73067f8420e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*bOO9L-2xNldzIC9OMQYPKA.png"/></div><figcaption class="od oe gj gh gi of og bd b be z dk">Example dog face for a Neural Network to recognize. Source: <a class="ae lq" href="https://www.thesun.co.uk/living/3009674/dog-called-picasso-because-of-his-wonky-face-is-looking-for-a-new-home-and-offers-are-already-flooding-in/" rel="noopener ugc nofollow" target="_blank">The Sun, image: lovable dog rescue</a></figcaption></figure><p id="6580" class="pw-post-body-paragraph kq kr iq kt b ku kv jr kw kx ky ju kz ln lb lc ld lo lf lg lh lp lj lk ll lm ij bi translated">我们的目标是什么？我们的目标是让网络从视觉上理解图片中的内容(对它来说，图片只是一个数字序列)。一种方法是自下而上的:首先查看小组像素，理解它们(如小线条和曲线:狗耳朵的曲线，瞳孔的小圆圈，腿的小线条)，然后建立这些线条和曲线的更大组(如:耳朵，鼻子，鼻子，眼睛)，理解这些形状的更大组(如:脸，腿，尾巴)，最后理解狗的整个图片。<br/> <br/>它是通过许多层将信息按顺序从一层传递到另一层来实现的。<br/>如果这对你来说是新的，请看我对卷积网络结构的总结:<a class="ae lq" href="https://quip.com/a4MBABJJB7oN" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">了解卷积神经网络</strong> </a> <strong class="kt ir">。</strong> <br/> <br/>如果你没读过，但对你来说还是新的，下面是一个更短的摘要:</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h2 id="ee93" class="nj lz iq bd ma nk nl dn me nm nn dp mi ln no np mk lo nq nr mm lp ns nt mo nu bi translated"><a class="ae lq" href="https://quip.com/a4MBABJJB7oN" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">了解卷积神经网络</strong> </a> <strong class="ak">。</strong></h2><ol class=""><li id="7f2c" class="mv mw iq kt b ku mq kx mr ln oh lo oi lp oj lm na nb nc nd bi translated"><strong class="kt ir">卷积层</strong>。第一个卷积层将图像空间映射到一个更低的空间，总结每组中发生的事情，比如说5x5像素，这是一条垂直线吗？水平线？什么形状的曲线？<em class="ks">这发生在逐元素乘法中，然后将滤波器中的所有值与原始滤波器值相加，并求和为单个数字。</em></li><li id="8f8b" class="mv mw iq kt b ku ne kx nf ln ng lo nh lp ni lm na nb nc nd bi translated">这就引出了<strong class="kt ir"> <em class="ks">神经元，或者说</em> </strong>卷积<strong class="kt ir"> <em class="ks">滤波器</em> </strong> <em class="ks">。每个过滤器/神经元被设计成对一种特定的形式(一条垂直线？一条水平线？等等……)。</em>来自第1层的像素组到达这些神经元，并根据该切片与神经元寻找的内容的相似程度来照亮与其结构匹配的神经元。</li><li id="2326" class="mv mw iq kt b ku ne kx nf ln ng lo nh lp ni lm na nb nc nd bi translated"><strong class="kt ir">激活(通常是“ReLU”)层</strong> —在每个卷积层之后，我们应用一个非线性层(或激活层)，这将非线性引入系统，使其能够发现数据中的非线性关系。ReLU非常简单:将任何负输入设为0，或者如果是正的，保持不变。<em class="ks"> ReLU(x) = max(0，x)。</em></li><li id="dab8" class="mv mw iq kt b ku ne kx nf ln ng lo nh lp ni lm na nb nc nd bi translated"><strong class="kt ir">汇集层</strong>。这允许减少“不必要的”信息，总结我们对一个地区的了解，并继续提炼信息。例如，这可能是“MaxPooling ”,其中计算机将只取传递的该补丁的最高值，以便计算机知道“在这些5x5像素周围，最主要的值是255。我不知道具体在哪一个像素上，但确切的位置并不重要，因为它就在附近。→ <strong class="kt ir"> <em class="ks">注意:这个不好。我们在这里丢失信息。胶囊网这里没有这个操作，这是一个进步。</em>T19】</strong></li><li id="d23a" class="mv mw iq kt b ku ne kx nf ln ng lo nh lp ni lm na nb nc nd bi translated"><strong class="kt ir">辍学层。</strong>该层通过将激活设置为零来“删除”该层中的一组随机激活。这使得网络更加健壮(有点像你吃泥土增强你的免疫系统，网络对小的变化更加免疫)并减少过度适应。这仅在训练网络时使用。</li><li id="1214" class="mv mw iq kt b ku ne kx nf ln ng lo nh lp ni lm na nb nc nd bi translated"><strong class="kt ir">最后一个完全连接的层。</strong>对于一个分类问题，我们希望每个最终神经元代表最终的类。它查看前一层的输出(我们记得它应该代表高级功能的激活图),并确定哪些功能与特定的类最相关。</li><li id="f932" class="mv mw iq kt b ku ne kx nf ln ng lo nh lp ni lm na nb nc nd bi translated"><strong class="kt ir"> SoftMax — </strong>有时添加这一层是为了以另一种方式表示每个类的输出，我们稍后可以在损失函数中传递这些输出。Softmax表示各种类别的概率分布。</li></ol><p id="3753" class="pw-post-body-paragraph kq kr iq kt b ku kv jr kw kx ky ju kz ln lb lc ld lo lf lg lh lp lj lk ll lm ij bi translated">通常，有更多层提供非线性和维度保持(如在边缘周围填充0 ),这有助于提高网络的鲁棒性和控制过拟合。但是这些是你需要理解接下来会发生什么的基础。<br/> <br/> <strong class="kt ir">现在重要的是，这些层只是按顺序连接。这与胶囊网络的结构相反。</strong></p><figure class="nw nx ny nz gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ol om di on bf oo"><div class="gh gi ok"><img src="../Images/50bd944a4c9c97c0f538bbb390332a91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oD7Jm__BQDBeMDoNFQMYTQ.png"/></div></div><figcaption class="od oe gj gh gi of og bd b be z dk">Neural Network Structure, <a class="ae lq" href="https://pdfs.semanticscholar.org/d8b9/eadcadb12d4d4e19201b4c5519470e21114c.pdf" rel="noopener ugc nofollow" target="_blank">From a Google article be Szegedy, Toshev &amp; Erhan presenting Neural Networks</a></figcaption></figure></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="2b04" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated"><span class="l kf kg kh bm ki kj kk kl km di"> W </span>卷积神经网络的问题在哪里？</h1><p id="28ab" class="pw-post-body-paragraph kq kr iq kt b ku mq jr kw kx mr ju kz ln ms lc ld lo mt lg lh lp mu lk ll lm ij bi translated">如果你对此感兴趣， <a class="ae lq" href="https://www.youtube.com/watch?v=rTawFwUvnLE&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank"> <em class="ks">观看韩丁的讲座</em> </a> <em class="ks">解释</em>到底<em class="ks">他们有什么问题。下面你会看到胶囊网络改进的几个关键点。</em></p><p id="05ca" class="pw-post-body-paragraph kq kr iq kt b ku kv jr kw kx ky ju kz ln lb lc ld lo lf lg lh lp lj lk ll lm ij bi translated">Hinton说，他们的子结构层次太少(网络是由神经元组成的层组成的，就是这样)；我们需要将每一层中的神经元分组到“胶囊”中，就像迷你列一样，进行大量的内部计算，然后<em class="ks">输出一个汇总结果。</em></p><h1 id="eba5" class="ly lz iq bd ma mb op md me mf oq mh mi jw or jx mk jz os ka mm kc ot kd mo mp bi translated">问题#1:共享会丢失信息</h1><p id="81fc" class="pw-post-body-paragraph kq kr iq kt b ku mq jr kw kx mr ju kz ln ms lc ld lo mt lg lh lp mu lk ll lm ij bi translated">CNN使用“汇集”或等效的方法来“总结”在较小区域发生的事情，并理解图像中越来越大的块。这是一个让CNN运行良好的解决方案，但是它丢失了有价值的信息。 <br/> <br/>胶囊网络会计算出一个<strong class="kt ir"><em class="ks"/></strong>(平移和旋转)较小特征之间的关系来组成一个较大的特征。<br/>这种信息的丢失导致空间信息的丢失。</p><h2 id="7f70" class="nj lz iq bd ma nk nl dn me nm nn dp mi ln no np mk lo nq nr mm lp ns nt mo nu bi translated">问题# 2:CNN没有考虑图像各部分之间的空间关系。因此，它们对方向也过于敏感。</h2><blockquote class="ou"><p id="544a" class="ov ow iq bd ox oy oz pa pb pc pd lm dk translated"><em class="pe">二次采样(和合并)会丢失鼻子和嘴等高级部分之间的精确空间关系。身份识别需要精确的空间关系。</em></p></blockquote><p id="a355" class="pw-post-body-paragraph kq kr iq kt b ku pf jr kw kx pg ju kz ln ph lc ld lo pi lg lh lp pj lk ll lm ij bi translated"><a class="ae lq" href="https://www.youtube.com/watch?v=TFIMqt0yT2I&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank"> <em class="ks">(韩丁，2012 </em> </a> <em class="ks">，在他的讲座中)。</em><br/><br/><strong class="kt ir">CNN不考虑底层对象之间的空间关系。</strong>通过这些平坦的神经元层根据它们看到的物体而发光，它们识别出这些物体的存在。但是随后它们被传递到其他激活和汇集层，并传递到下一层神经元(过滤器)，而不识别我们在该单层中识别的这些对象之间的关系<em class="ks">。</em> <br/>他们只是说明自己的存在。<br/> <br/> <strong class="kt ir">所以一个<em class="ks">(简单化)</em>神经网络会毫不犹豫地将这两只狗，巴勃罗和毕加索，归类为“柯基-比特-牛头梗混血儿”的同样好的代表。</strong></p><figure class="nw nx ny nz gt oa gh gi paragraph-image"><div role="button" tabindex="0" class="ol om di on bf oo"><div class="gh gi pk"><img src="../Images/ef722e09c37f43dd215e0d02d5f867ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JODQCqawdAvLIwMCANkZ1A.png"/></div></div><figcaption class="od oe gj gh gi of og bd b be z dk"><em class="pe">Normal (Convolutional) Neural Network will recognize both of these lovely dogs as the same type of dog face, because it does not care about WHERE the elements of the dog’s face are located relatively to each other in space. Picasso (the dog on the left) will luckily not be discriminated against by the model, but we really want a model to realize this is not a regular example of a </em>corgi-pit-bull-terrier mix <em class="pe">dog. Image source: </em><a class="ae lq" href="https://www.thesun.co.uk/living/3009674/dog-called-picasso-because-of-his-wonky-face-is-looking-for-a-new-home-and-offers-are-already-flooding-in/" rel="noopener ugc nofollow" target="_blank"><em class="pe">The Sun. Image: Lovable Dog Rescue</em></a></figcaption></figure><p id="44e4" class="pw-post-body-paragraph kq kr iq kt b ku kv jr kw kx ky ju kz ln lb lc ld lo lf lg lh lp lj lk ll lm ij bi translated">网络会将这两只狗归类为“柯基-比特-牛头梗混合”狗的类似良好表示，因为它们在面部合成卷积层回答相同的条件，例如:</p><pre class="nw nx ny nz gt pl pm pn po aw pp bi"><span id="b857" class="nj lz iq pm b gy pq pr l ps pt">if: (2 eyes &amp; pitbullmix_snout <br/>     + pitbullmix_wet_nose &amp; mouth)<br/>then: pitbullmix_face</span></pre><p id="ce8a" class="pw-post-body-paragraph kq kr iq kt b ku kv jr kw kx ky ju kz ln lb lc ld lo lf lg lh lp lj lk ll lm ij bi translated">错误地激活pitbullmix_face的神经元，而不是类似于:</p><pre class="nw nx ny nz gt pl pm pn po aw pp bi"><span id="aba3" class="nj lz iq pm b gy pq pr l ps pt">if: 2 eyes <br/>&amp; BELOW: pitbullmix_snout <br/>     &amp; pitbullmix_wet_nose <br/>&amp; BELOW: mouth <br/>then: pitbullmix_face</span></pre><p id="f911" class="pw-post-body-paragraph kq kr iq kt b ku kv jr kw kx ky ju kz ln lb lc ld lo lf lg lh lp lj lk ll lm ij bi translated"><strong class="kt ir">相反，胶囊网络代表方向以及内容，并连接神经元胶囊之间，以推断空间关系并保留姿势信息。</strong> <br/> <br/>缺少分组的胶囊表示、姿态计算，以及胶囊之间的重叠检查导致下一个问题。</p><h2 id="7edb" class="nj lz iq bd ma nk nl dn me nm nn dp mi ln no np mk lo nq nr mm lp ns nt mo nu bi translated">问题# 3:CNN无法将对几何关系的理解转移到新的观点上。</h2><p id="9448" class="pw-post-body-paragraph kq kr iq kt b ku mq jr kw kx mr ju kz ln ms lc ld lo mt lg lh lp mu lk ll lm ij bi translated">这使得它们对原始图像本身更敏感，以便将图像分类为同一类别。<br/><br/>CNN在解决与他们接受过训练的数据相似的问题方面非常棒。它可以对图像或其中与它以前见过的东西非常接近的物体进行分类。但是如果物体稍微旋转，从稍微不同的角度拍摄，特别是在3D中，倾斜或在另一个方向上，而不是CNN所看到的-网络不会很好地识别它。<br/> <br/>一种解决方案是<strong class="kt ir">人工创建图像或组的倾斜表示</strong>并将它们添加到“训练”集中。然而，这仍然缺乏一个更坚固的结构。</p><h2 id="1344" class="nj lz iq bd ma nk nl dn me nm nn dp mi ln no np mk lo nq nr mm lp ns nt mo nu bi translated">编码视点不变的空间关系姿态</h2><p id="4dda" class="pw-post-body-paragraph kq kr iq kt b ku mq jr kw kx mr ju kz ln ms lc ld lo mt lg lh lp mu lk ll lm ij bi translated">那么我们如何对3D对象之间的空间关系进行编码呢？辛顿从一个已经解决了这个问题的领域获得了灵感:3D计算机图形学。<br/>在三维图形中，一个<a class="ae lq" href="https://en.wikipedia.org/wiki/Pose_(computer_vision)" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir"/></a><strong class="kt ir"/><a class="ae lq" href="http://digitalrune.github.io/DigitalRune-Documentation/html/d995ee69-0650-4993-babd-1cdb1fd8fd7a.htm" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir">矩阵</strong> </a>是一种表示物体之间关系的特殊技术。姿态本质上是代表<em class="ks">平移</em>加上<em class="ks">旋转的矩阵。现在我们明白了。我们可以使用子对象之间的姿势关系来保留空间关系信息；测量物体之间的相对旋转和平移作为4D姿态矩阵。<br/>这将是理解胶囊间动态路由的关键。</em></p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><p id="47cd" class="pw-post-body-paragraph kq kr iq kt b ku kv jr kw kx ky ju kz ln lb lc ld lo lf lg lh lp lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="ks">既然我们已经知道了神经网络及其空间识别问题的基础知识，我们可以继续了解最近开发的解决方案:胶囊神经网络。这将是我下一篇文章的主题。</em> </strong> <em class="ks">敬请期待！</em></p></div></div>    
</body>
</html>