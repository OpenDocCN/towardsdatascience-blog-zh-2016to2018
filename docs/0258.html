<html>
<head>
<title>Parameter Inference — Maximum Aposteriori</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">参数推断—最大后验概率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/parameter-inference-maximum-aposteriori-estimate-49f3cd98267a?source=collection_archive---------1-----------------------#2017-04-06">https://towardsdatascience.com/parameter-inference-maximum-aposteriori-estimate-49f3cd98267a?source=collection_archive---------1-----------------------#2017-04-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/7b1ab0bb6b97cb23c632171871c85a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*0LpRQhApzpL4xh5ZcVPwfA.jpeg"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">I dare you to not laugh at this comic after going through the post. :D</figcaption></figure><p id="f9e0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在<a class="ae kw" href="https://medium.com/towards-data-science/parameter-inference-maximum-likelihood-2382ef895408" rel="noopener">之前的帖子</a>中，我们讨论了最大似然估计背后的动机以及如何计算它。我们还学习了一些通过引用单调函数来计算函数对数似然的技巧，以及它们如何使估计函数临界点的整个过程更容易，因为它们保留了这些临界点。</p><p id="4318" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在MLE帖子的最后，我试图通过问一个简单的问题来激发使用MAP(最大后验概率)的原因:</p><blockquote class="kx ky kz"><p id="9d08" class="jy jz la ka b kb kc kd ke kf kg kh ki lb kk kl km lc ko kp kq ld ks kt ku kv ij bi translated">如果序列如下所示会怎样:</p></blockquote><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi le"><img src="../Images/515e4683f64c9dfe4db6fdba7963623b.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*TFDkfzT6GM_ZCMd9-SKVoQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 1: A sequence of two Heads</figcaption></figure><blockquote class="kx ky kz"><p id="18a6" class="jy jz la ka b kb kc kd ke kf kg kh ki lb kk kl km lc ko kp kq ld ks kt ku kv ij bi translated">你认为第三次抛硬币是<em class="iq">反面</em>的概率是多少？</p></blockquote><p id="fd16" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">很明显，在这种情况下，</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/b6cc5dbce4f1d6cee6cc5f0a157395ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/1*3hZi9Emh8DihIi3ITXiQDw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 2</figcaption></figure><p id="b8e4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为什么？将#tails (0)和#heads (2)放入<em class="la"> theta_MLE </em>的等式中，</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/484c0595583ac3fec1a2ce8ad4dfbef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*LEmKYhQzPSbppID40IIkoA.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 3</figcaption></figure><p id="4b5e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们得出了结果。这个结果告诉我们，下一次掷硬币是<em class="la">反面</em>的概率是0(也就是说，它预测没有一次掷硬币会出现<em class="la">反面= &gt; </em>硬币总是会出现<em class="la">正面</em>)，很明显事实并非如此(除非是硬币被重装载的极端情况)。现在，这在参数估计过程中提出了一个大问题，因为它没有给我们下一次翻转的准确概率。我们知道，即使是公平的硬币也有25%的几率连续出现两个<em class="la">头像</em>(0.5 x 0.5 = 0.25)。所以，硬币也不是不可能是公平的。</p><h1 id="703b" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">后面的</h1><p id="c660" class="pw-post-body-paragraph jy jz iq ka b kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr mn kt ku kv ij bi translated">虽然我们知道MLE是机器学习中的一个强大工具，但它也有缺陷(正如我们刚刚看到的)，这些缺陷发生在我们可用的数据量有限的时候。最大似然估计的问题在于它是一个<strong class="ka ir">点估计，</strong>也就是说，我们被允许在<em class="la">计算一个特定值</em>的最大似然估计，这导致<strong class="ka ir">过度拟合</strong>(对于那些以前没有听说过这个术语的人，我建议你参考<a class="ae kw" href="https://www.quora.com/What-is-an-intuitive-explanation-of-overfitting" rel="noopener ugc nofollow" target="_blank">Quora上的这个</a>答案)。因为这是一个点估计，所以它与数据过度拟合(连续2个<em class="la">正面对</em>)，并且它没有考虑硬币可能仍然公平的可能性(或者可能只是稍微偏向<em class="la">正面</em>)。显而易见的问题是:我们如何解决这个问题？</p><p id="57d4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通常，我们对世界上正在发生的与数学无关的过程有着<strong class="ka ir">先验信念</strong>。让我们举一个简单的例子:假设你和你的朋友在一个“猜硬币”的游戏中打赌。现在，你可能会认为你的朋友已经稍微操纵了硬币，使其偏向他们，也许会使它有点偏向<em class="la">的头像；</em> 55% <em class="la">正面</em>和45% <em class="la">反面</em>(没有人会愚蠢到让它极端偏向某个特定的翻转，因为这很容易被察觉)。你对一个随机过程的这些假设被称为先验。用专业术语来说:</p><blockquote class="kx ky kz"><p id="149a" class="jy jz la ka b kb kc kd ke kf kg kh ki lb kk kl km lc ko kp kq ld ks kt ku kv ij bi translated">一个不确定量的<strong class="ka ir">先验概率分布</strong>，通常简称为<a class="ae kw" href="https://en.wikipedia.org/wiki/Prior_probability" rel="noopener ugc nofollow" target="_blank"><strong class="ka ir"/></a>，是在考虑一些证据之前，表达一个人对这个量的信念的概率分布。</p></blockquote><p id="95c7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有没有什么方法可以将这些先验的信念用数学方法整合到我们的模型中？</p><p id="4b97" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">是的:我们可以让参数<em class="la"> theta </em>本身成为一个随机变量，这也证明了我们一直在使用的符号是正确的；p(F = F |<em class="la">theta</em>)[注意，考虑到这个符号已经暗示了<em class="la"> theta </em>是一个随机变量，这个假设是多么完美]。</p><p id="1a27" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们可以在<em class="la">θ</em>上进行分布，并加入这样的概念，即使在极端情况下(HH)，硬币仍然是公平的。正如我们将看到的，这个概念也有助于我们防止过度拟合。</p><p id="038f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于任何<em class="la"> x </em>，我们希望能够在看到数据后表达我们的参数<em class="la">θ</em>的分布，我们通过在观察到的数据序列上调节<em class="la">θ</em>来做到这一点，就像这样:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/4ee1e115a28b11c12f3fea593ef8a558.png" data-original-src="https://miro.medium.com/v2/resize:fit:280/format:webp/1*t6PYPLRpLVoiC8CKF8fJuQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 4: Posterior notation</figcaption></figure><p id="2bbf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中D是一个随机变量，它捕获了手头的观察序列，即我们的数据。现在<em class="la">θ</em>是一个随机变量，它可以取特定的标量值<em class="la"> x </em>。因为我们讨论的是抛硬币，所以我们知道上面的等式只对[0，1]中的<em class="la"> x </em>有意义(由于<em class="la"> x </em>是一个概率，所以它必须在0和1之间)。</p><p id="3b07" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我知道这很难接受。因此，让我们后退一步，尝试理解MLE中发生了什么，以及它与我们正在进行的MAP估计相比如何:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mp"><img src="../Images/be4db7b15ffabeb8c045b6c132da8938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PCcbDIUHjCGl0DlqD3RluA.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 5: MLE - Likelihood is the probability of data <strong class="bd mu">given</strong> the parameter, and we were maximizing this w.r.t. <em class="mv">theta.</em></figcaption></figure><p id="3dbc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们感兴趣的是:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mw"><img src="../Images/e44d38c7263197d86281e2444fb4fc5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7gM2WmUtOmPCoS9GtJlStQ.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 6: MAP — Maximizing the probability of theta <strong class="bd mu">after </strong>we have observed our data.</figcaption></figure><p id="81bc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在还不完全清楚地图术语的直观含义，但是当我们完成这篇文章的时候就会明白了。</p><p id="fa4b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">而<em class="la">一般为</em>，</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mx"><img src="../Images/20a15caec5508e3a23b403d3199d15ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KZr1zrhzEsKvyDbn3U7uMQ.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 7: MLE != MAP</figcaption></figure><p id="de55" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以应用古老的贝叶斯法则来使地图公式变得更加神秘:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/6881c35685959538a465fa3b4c508b88.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*tF1zp93WolW8ZVOb5kbPig.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 8: Bayes’ rule on MAP (Posterior)</figcaption></figure><p id="f043" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">分子包括:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/637b7262c1dfdca971125bc71ac03762.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/1*uz19CUqaqmRLbVy2Ch9_DA.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 9: Numerator I — Likelihood</figcaption></figure><p id="fd1c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们从MLE中知道这部分，唯一的区别是它现在用<em class="la">θ</em>=<em class="la">x .</em>固定，它被称为似然。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/e8efa8875e359e19cba5ced6bbb86b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:198/format:webp/1*j4VPFNecLIVIIOmKfllg9g.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 10: Numerator II — Prior</figcaption></figure><p id="3db2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是我们在观察任何数据之前对<em class="la">θ</em>值的先验信念。它在之前被称为<em class="la">，它防止过度拟合。关于前科的一点小动机:</em></p><blockquote class="kx ky kz"><p id="106a" class="jy jz la ka b kb kc kd ke kf kg kh ki lb kk kl km lc ko kp kq ld ks kt ku kv ij bi translated">在很多情况下，我们有理由相信，θ的某些值比其他值更有可能。例如，在我们掷硬币的例子中，我们期望这个值在0.5左右，像0或1这样的极端值不太可能出现。这是Prior捕获的内容，它将防止极端结果(过度拟合)。</p></blockquote><p id="506e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">分母p(D)称为<em class="la">证据</em>，是一个归一化常数，在我们的例子中并不重要。没有分母，右边的表达式就不再是<em class="la">概率</em>，因此范围不会从0到1。“归一化常数”允许我们得到一个事件发生的概率，而不仅仅是这个事件与另一个事件相比的相对可能性。查看<a class="ae kw" href="http://stats.stackexchange.com/questions/129666/why-normalizing-factor-is-required-in-bayes-theorem" rel="noopener ugc nofollow" target="_blank">关于堆栈交换的讨论</a>。</p><p id="3e7e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们应该从所有这些讨论中获得三个术语:</p><ol class=""><li id="a292" class="nb nc iq ka b kb kc kf kg kj nd kn ne kr nf kv ng nh ni nj bi translated">在先的；在前的</li><li id="0474" class="nb nc iq ka b kb nk kf nl kj nm kn nn kr no kv ng nh ni nj bi translated">可能性，以及</li><li id="289f" class="nb nc iq ka b kb nk kf nl kj nm kn nn kr no kv ng nh ni nj bi translated">在后面的</li></ol><p id="95c2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">它们是由贝叶斯法则联系在一起的。因此，先验是我们在观察到任何数据之前对<em class="la"> theta </em>看起来如何的信念，后验是我们在观察到一些数据之后对<em class="la"> theta </em>看起来如何的<em class="la">的<em class="la">更新的</em>信念。</em></p><p id="bda9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们已经建立了可能性(我们在上一篇文章中一直使用)和后验概率(我们现在感兴趣的)之间的联系。而这两者之间的联系是<em class="la">先验</em>分布，先验分布是<em class="la">我们的</em>模型的一部分。所以，是我们来选择优先。<em class="la">先验的要点是，我们必须在不看观察的情况下选择它，</em>这为(可能是主观的)模型假设留下了空间。除了它需要是一个有效的概率分布之外，先验不需要满足任何特定的规则:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/122986be45857954c3b2f844713e1fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*dBqM-flB8ZAaFpg6Dxirzw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 11: Prior is just a probability distribution.</figcaption></figure><p id="ee7e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们看几个前科的例子:</p><ol class=""><li id="06db" class="nb nc iq ka b kb kc kf kg kj nd kn ne kr nf kv ng nh ni nj bi translated">我们假设存在除0(HH序列的最大似然)之外的其他可能性。现在这个先验是非常弱的，因为它没有给出太多关于这些可能性的信息。</li><li id="cd96" class="nb nc iq ka b kb nk kf nl kj nm kn nn kr no kv ng nh ni nj bi translated">我们还可以假设参数<em class="la">θ</em>最有可能在0.4和0.5之间的区域，这大约是0.5的真实概率(尽管MLE解告诉我们它是0)。由于我们将假设限制在某个区域，这是一个相当强的<em class="la">先验的例子。</em></li></ol><p id="2697" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这些主观假设也被称为<strong class="ka ir">归纳偏差</strong>。<em class="la">通过引入这些主观假设，我们使我们的分析/模型偏向某些解决方案。</em>这是我们写下模型时必须意识到的事情。</p><h1 id="2fd6" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">地图估计</h1><p id="7d61" class="pw-post-body-paragraph jy jz iq ka b kb mj kd ke kf mk kh ki kj ml kl km kn mm kp kq kr mn kt ku kv ij bi translated">下图说明了我们对<em class="la">θ</em>的先验的几种选择:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/2283ff90cab6097ae7d7777bd7b36590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*0mY-XiN_KfiF-tC2bTRXIQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 12: Prior choices (x-axis represents probability for theta. y-axis represents probability density)</figcaption></figure><p id="7ef3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第一张图显示了所有可能参数的均匀分布。它没有对模型施加任何归纳偏差，因此没有比其他方案更倾向于<em class="la">θ</em>的任何方案。换句话说，这是我们的最大似然估计。因此，我们可以说<strong class="ka ir"> MLE是当先验是一致的</strong>时映射的一个特例。</p><p id="7b1c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其他三个选项对<em class="la">θ</em>施加了一些感应偏置。具体来说，我们可以看到它们都以0.5为中心，宽度不同(如果您可以推断出后三个模型对先验假设的严格性的降序，则为一个cookie。；)).这意味着后三种方法都假设真实解在0.5左右的小区间内，区间宽度不同。</p><p id="9a84" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好了，现在我们对我们的先验应该是什么样子有了一些想法。在选择之前的模型的背后，我们能有更多的动机吗？一个可能的动机是，我们可以选择一个先验，这样可以简化进一步的计算(它们不会变得非常广泛)。这是为我们的特定模型(在我们的例子中:抛硬币)选择适当先验的驱动因素之一。</p><p id="fa14" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于先验只是另一种概率分布，我们希望选择这种分布，使我们随后的计算更容易。事实证明，每当我们寻找一个直接对应于概率的参数的先验时(就像在我们的例子中，<em class="la">θ</em>对应于抛硬币时<em class="la">尾</em>出现的概率)，通常存在这样的先验，它们非常适合。</p><p id="9aeb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我们的例子中，这个先验将是一个<strong class="ka ir"> Beta分布</strong>:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nr"><img src="../Images/1808440d89d89ffe1985d43b2ba4cf9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*qbjwnDo86UYO1U6PmVQ4Cg.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 13: <a class="ae kw" href="http://mathinsight.org/probability_density_function_idea" rel="noopener ugc nofollow" target="_blank">pdf</a> of a beta distibution</figcaption></figure><p id="300a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">贝塔分布可以理解为代表一个概率的<em class="la">分布</em><em class="la">——也就是说，它代表一个概率的所有可能值，当我们不知道那个概率是什么的时候。更直观的理解，见<a class="ae kw" href="http://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution" rel="noopener ugc nofollow" target="_blank">本</a>。</em></p><p id="ca25" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好吧，这看起来非常拜占庭式和令人沮丧的乍一看，但留在我这里。让我们仔细看看；我们开始注意到一些我们以前已经见过的术语:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/cc670fba3e304ec24ea7cde1047a1c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*8Tn4isCOaJsmvld2JnKXQw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 14: We have seen an extremely similar form in MLE.</figcaption></figure><p id="7fdf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们已经看到了Img中的组件。14在MLE中，除了指数中的# <em class="la">头</em>和# <em class="la">尾</em>被替换为a-1和b-1。</p><p id="546e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">除了<em class="la">θ</em>项，我们还有归一化常数，它有<a class="ae kw" href="https://en.wikipedia.org/wiki/Gamma_function" rel="noopener ugc nofollow" target="_blank">伽马函数</a>。伽玛函数只不过是阶乘函数的扩展，它的参数下移了1:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d475d75c6c2da545d86731184de3f096.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*4AvCzWcDTQYR23k_uiSMPg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 15: Gamma function</figcaption></figure><p id="f7ed" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">伽马函数的特殊之处在于它允许我们计算任意实数值的阶乘(伽马函数1.5是很好的定义，而1.5！未定义)。</p><p id="4e9f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">事实上，我们以前在某些地方看到过这种情况，这并不是巧合。我们已经选择了一个<em class="la">共轭先验</em>。每当我们有一个先验，它具有与可能性相似的函数形式，并且它们很好地配合在一起，我们就说所谓的<strong class="ka ir">共轭先验，</strong>贝塔分布是我们掷硬币实验的可能性的共轭先验。解释如下:在没有看到任何数据的情况下，我们对掷硬币实验的结果有一种感觉。在我们的实验中，如果我们要掷a+b-2硬币，那么我们可以选择优先次序，这样掷硬币中的a-1会出现<em class="la">尾</em>，剩下的b-1会出现<em class="la">头。</em></p><p id="c507" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以，如果我们假设硬币是无偏的，我们会选择a和b相等。我们对这个假设越确定，我们就越会选择a和b。</p><p id="5054" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下图说明了Beta分布中a和b的几种选择:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/354a8f107eabe182fb898e82484b52d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*-cQlkPEkiDlpyXSSWE3WdQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 16: Four different Beta distributions (x-axis represents the probability for theta. y-axis represents density)</figcaption></figure><p id="7cff" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你想尝试不同的测试版，请点击这里查看iPython笔记本<a class="ae kw" href="https://github.com/dovahkiin2/hello-world/blob/master/Beta%20distributions.ipynb" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="8ba5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">四幅图的解释:</p><p id="8a53" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">a.选择a和b为1(相等)意味着我们对实验没有任何假设。这是贝塔分布的一个重要特征:当a = b = 1时，它推广为均匀分布。</p><p id="1fe2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">b.选择一个&lt; b represents that we are less certain about <em class="la">#反面</em>比选择一个<em class="la">#正面</em>。该图通过显示<em class="la">尾部</em>的概率更可能小于0.5，清楚地说明了这一点。</p><p id="b4cb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">c.相比正面，我们对反面是T21的把握是正面的两倍。从图中我们可以看到，翻牌是<em class="la">反面</em>的概率偏向大于0.5。</p><p id="530c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">d.选择a和b为非整数有利于极端解决方案。不幸的是，为a和b选择非整数并没有任何可行的物理解释，但是它给了我们在建模先验分布时很大的灵活性。</p><p id="94f4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从某种意义上说，我们对先前的假设越确定，我们就越能选择a &amp; b，这从Img的‘b’和‘c’图中可以看出。16.</p><p id="d9eb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">既然我们已经通过各种论证论证了β分布是我们掷硬币实验的合适先验，那么让我们把我们所知道的一切代入我们在Img中使用的贝叶斯公式。8:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/6881c35685959538a465fa3b4c508b88.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*tF1zp93WolW8ZVOb5kbPig.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 17: Bayes formula for Posterior</figcaption></figure><p id="d6ae" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们知道:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/b9993aa047540758f387e17cc5ed5811.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*40U-okBfirrqPyrLN1wuGQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 18: MLE part in our MAP solution</figcaption></figure><p id="329e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在你可能会想，我们在之前得到的<a class="ae kw" href="https://medium.com/towards-data-science/parameter-inference-maximum-likelihood-2382ef895408" rel="noopener">的可能性中有<em class="la">θ</em>:</a></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/44df48aaf204b21a13731f58436886d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*a5E6oxJNN2SF87w-F1tZew.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 19 : Likelihood</figcaption></figure><p id="23e1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们现在有<em class="la"> x </em>而不是<em class="la">θ</em>的原因是因为我们已经假设<em class="la">θ</em>本身是一个随机变量，可以取一个特定的值<em class="la"> x. </em></p><p id="dfb1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因为我们已经决定先验是一个Beta分布，它采用以下形式:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/0831259e736d98d9ca7a5aebd21b5d8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*RRYI9hgpc0IK6KCZeyaRdg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 20: Prior</figcaption></figure><p id="2ed3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">将这些值代入贝叶斯法则:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ny"><img src="../Images/7e1213e47ec5b0643ab0676c1e780e0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y7CMK4IBrD4L3NUj6ejYEw.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 21: MAP formulation</figcaption></figure><p id="24d5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第二个陈述通过省略p(D)和贝塔分布的归一化常数引入了比例符号，因为它们都是常数w.r.t. <em class="la"> x. </em>(此外，省略常数不会改变最大值的位置，只会改变其值。)</p><p id="c247" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">考虑上图中的第二个语句。我们可以看到为什么β分布或共轭先验通常是强大的，因为这一项实际上看起来像可能性，除了它有这些校正项a-1和b-1。所以不管|T|和|H|有多极端(比如我们只抛了两次硬币)，我们都有这些修正项a &amp; b。</p><p id="8fa8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们在这里所做的基本上是将结果从极端边际结果中抽离出来，就像我们在数据太少的情况下得到的最大似然结果一样。共轭先验之所以是先验的绝佳选择，是因为它们确实有这种效果。</p><p id="bf3b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好了，现在我们只剩下以下术语:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/ec0057fee07d732d9a277a04ff0f1ee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*BV23xRtCQiKMyJtl5ggP8A.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 22: MAP with proportionality</figcaption></figure><p id="ea5f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们如何找到把比例符号变成等号的乘法常数？</p><blockquote class="kx ky kz"><p id="64d8" class="jy jz la ka b kb kc kd ke kf kg kh ki lb kk kl km lc ko kp kq ld ks kt ku kv ij bi translated">让我们绕一个弯子，看看如何得出用常规方法无法求解(或至少很难求解)的特定积分的解:</p></blockquote><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi oa"><img src="../Images/876db5e9ad93ebb5a169a3f83debf472.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i4fvDAJlmXdiBP_WfW_3MQ.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 23: A not-so-easy integral</figcaption></figure><blockquote class="kx ky kz"><p id="da6c" class="jy jz la ka b kb kc kd ke kf kg kh ki lb kk kl km lc ko kp kq ld ks kt ku kv ij bi translated">你认为你能解析地解出这个纸上积分吗？我不是数学专业的，但对我来说，答案是肯定的和否定的。是的，你可以，但如果你能想出解决方案，用传统的微积分规则得出结果将需要相当长的时间。不，嗯，因为类似的推理需要太长时间。</p><p id="1c3e" class="jy jz la ka b kb kc kd ke kf kg kh ki lb kk kl km lc ko kp kq ld ks kt ku kv ij bi translated">如果你非常了解你的概率分布，你会知道下面的积分是:</p></blockquote><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ob"><img src="../Images/5a28b65009f8616d394f8c54c5841ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7xB_4fpxHpoBo4Mn6c-BYw.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 24: Standard gaussian pdf</figcaption></figure><blockquote class="kx ky kz"><p id="762a" class="jy jz la ka b kb kc kd ke kf kg kh ki lb kk kl km lc ko kp kq ld ks kt ku kv ij bi translated">均值和单位方差为0的标准高斯分布的pdf，即X ~ N(0，1)，它等于1。如果你想温习高斯语，我认为ikipedia的文章相当可靠。</p><p id="14f2" class="jy jz la ka b kb kc kd ke kf kg kh ki lb kk kl km lc ko kp kq ld ks kt ku kv ij bi translated">如果上面的积分等于1，那么只需要通过两边乘以sq.rt(2*pi)来重新排列项，就可以得出我们刚才所说的实际积分的答案(Img。23).，我们看到:</p></blockquote><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi oc"><img src="../Images/59e66dfdb928f59c5629fc0e14b67c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VHthA7La9usCk0DYV8preQ.jpeg"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 25 : Answer to the not-so-easy integral</figcaption></figure><blockquote class="kx ky kz"><p id="5b16" class="jy jz la ka b kb kc kd ke kf kg kh ki lb kk kl km lc ko kp kq ld ks kt ku kv ij bi translated">因此，从某种意义上说，我们通过逆向工程找到了一个相当困难的积分的解。</p></blockquote><p id="2ae3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们有了一个非常方便的技巧，让我们看看是否可以应用它来找出我们的MAP估计的比例常数。</p><p id="6000" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们知道我们的先验分布总和为1:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi od"><img src="../Images/be87462c1aeb5656e2498f3bf67bce83.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*YW66YDwLzWEZBxBVncobkw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 26: The only constraint on prior</figcaption></figure><p id="50ae" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以及Img中的RHS。MAP的22表达式与pdf成比例，pdf积分为1。注意到和我们的酷魔术惊人的相似了吗？</p><p id="81c5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，通过逆向工程，后验概率也必须是贝塔分布，这里唯一起作用的常数是相应贝塔分布的归一化常数:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/70121babd4898573ce01f593157ea956.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*-j5v67KIy8F4b6SguGvY2g.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 27: Constant of Proportionality for our MAP</figcaption></figure><p id="f17e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">没有其他常数使我们的MAP估计的RHS积分为1。但是因为它<em class="la">必须</em>积分为1，这是唯一的工作常数。</p><p id="7dad" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">每当我们应该解决一个困难的积分，我们仔细检查它，并试图找到它是否看起来像一个pdf。如果是这样，我们可以很容易地逆向工程这个pdf的归一化常数，然后可以很容易地解决积分。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi of"><img src="../Images/47c8fc1fa031365d2f3d49dce20a0be9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oyRIVHU1lRLM-GAefzRzgQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 28: Note on the reverse engineering trick</figcaption></figure><p id="4e67" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这种逆向工程技巧的一个直接且非常好的结果是，我们可以很容易地确定我们的后验概率的分布。这使我们知道，我们的后验需要是一个贝塔分布，我们也知道参数:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi og"><img src="../Images/e3a68729ac363f23a3c06548b463a28d.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*UDq1eMDp5otBtSDE77vG_g.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 29: Posterior — Beta distribution</figcaption></figure><p id="52b5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们可以找到我们的参数<em class="la"> theta </em>的最大值，就像我们在MLE <a class="ae kw" href="https://medium.com/towards-data-science/parameter-inference-maximum-likelihood-2382ef895408" rel="noopener"> post </a>中所做的一样(我跳过了实际的计算；这是对数在Img方程中的一个简单应用。21找出最大值)，我们得出以下结果:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/31550a22e98f7a90ca271eaf874634ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*2P2bhiJ-7ZLrRyEB1ZMSNA.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Img. 30: MAP Solution</figcaption></figure><p id="2d77" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">将这个MAP估计与ML估计(|T|/|H|+|T|)进行比较，我们看到它们非常相似。项|T|/|H|+|T|再次出现在MAP estimate中，只不过它被我们先前的信念所修正。</p><p id="fc8d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="la">根据我们先前的想法，在看到(i.i.d .)数据后，theta_MAP是theta的最佳猜测。</em></p><p id="b9fd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">称为最大后验估计(MAP)。</strong></p><p id="8678" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">注意这里最重要的一点:后验工作<em class="la">非常</em>直观。如果我们只有很少的数据(就像在我们的第二个实验中，我们只有两次抛硬币，两次都是<em class="la">头</em>)，先验的影响会更强。另一方面，对于固定的a &amp; b，如果我们进行足够多的抛硬币(例如，一百万次)，那么a &amp; b的影响几乎可以忽略不计，并且我们非常接近MLE。这非常直观，因为在低数据范围内，先验防止过拟合，而如果我们从随机过程中获得大量可靠信息，我们就不再需要先验信息了。我们可以从数据本身获得所有信息。换句话说，当我们在数据中游泳时，MLE占主导地位。</p></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><p id="0b83" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个过程结束了吗？我们已经融入了我们对世界运作方式和随机过程展开方式的假设。还有什么要完成的？现在我们可以坐下来看LOTR三部曲了。</p><p id="0d0b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">或者我们可以吗？事实证明，当我们将我们先前的信念结合到模型中时，我们隐含地倾向于某些解决方案(通过假设在抛硬币过程中的beta分布，我们使MAP估计偏向特定的解决方案；看起来像Beta pdf的表单)。那么所有其他也能影响一个过程的前科呢，尽管是以一种非常微妙的方式？有没有什么方法可以把它们也合并到我们的模型中？正如你已经知道的，答案是肯定的，我们将在下一篇文章中深入讨论它，当我们谈论我们模型的完全贝叶斯分析时。</p></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><p id="c747" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我意识到这篇文章有时并不容易理解，其中涉及到很多数学问题。我希望我让你更容易和直观地理解地图估计的过程。如果你发现任何具体的部分太深奥，随时留下评论，我会尽快处理。</p></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><p id="2c52" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">资源</strong>:我在慕尼黑工业大学的研究生院教授的ML课程。在这里随意观看讲座<a class="ae kw" href="https://www.youtube.com/channel/UCpyPuMAyBvERU8YMngGy11g" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><p id="845e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你觉得这篇文章有趣，请推荐并分享它，这样其他人也可以从中受益。</p></div></div>    
</body>
</html>