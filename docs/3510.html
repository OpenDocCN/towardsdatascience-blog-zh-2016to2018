<html>
<head>
<title>Part of Speech Tagging with Hidden Markov Chain Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于隐马尔可夫链模型的词性标注</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/part-of-speech-tagging-with-hidden-markov-chain-models-e9fccc835c0e?source=collection_archive---------2-----------------------#2018-05-19">https://towardsdatascience.com/part-of-speech-tagging-with-hidden-markov-chain-models-e9fccc835c0e?source=collection_archive---------2-----------------------#2018-05-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/eb6ecc81ad1645076ec13333647fedd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*miouY1Focv9qP_jWYixQrg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo Credit: Pixabay</figcaption></figure><p id="e6dc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae la" href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" rel="noopener ugc nofollow" target="_blank">词性标注</a> (POS)是用名词、动词、形容词、副词等词性对句子进行标注的过程。</p><p id="72b5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae la" href="https://en.wikipedia.org/wiki/Hidden_Markov_model" rel="noopener ugc nofollow" target="_blank">隐马尔可夫模型</a> (HMM)是一个简单的概念，可以解释大多数复杂的实时过程，如语音识别和语音生成、机器翻译、生物信息学的基因识别、计算机视觉的人类手势识别等等。</p><p id="0a66" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在本帖中，我们将使用<a class="ae la" href="http://pomegranate.readthedocs.io/" rel="noopener ugc nofollow" target="_blank">石榴</a>库为词性标注构建一个隐马尔可夫模型。我们不会深入统计词性标注器的细节。不过，如果你有兴趣，这里有<a class="ae la" href="http://www.coli.uni-saarland.de/~thorsten/publications/Brants-ANLP00.pdf" rel="noopener ugc nofollow" target="_blank">纸</a>。</p><p id="c44e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">数据是布朗文集<a class="ae la" href="https://en.wikipedia.org/wiki/Brown_Corpus" rel="noopener ugc nofollow" target="_blank">的副本</a>，可以在这里找到<a class="ae la" href="https://github.com/susanli2016/Text-Mining-with-Python" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="a3fa" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据</h1><p id="84f7" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">导入库并读取数据。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9abb" class="mn lc iq mj b gy mo mp l mq mr">import matplotlib.pyplot as plt<br/>import numpy as np</span><span id="99bd" class="mn lc iq mj b gy ms mp l mq mr">from IPython.core.display import HTML<br/>from itertools import chain<br/>from collections import Counter, defaultdict<br/>from pomegranate import State, HiddenMarkovModel, DiscreteDistribution<br/>import random</span><span id="901f" class="mn lc iq mj b gy ms mp l mq mr">Sentence = namedtuple("Sentence", "words tags")</span><span id="c6a9" class="mn lc iq mj b gy ms mp l mq mr">def read_data(filename):<br/>    """Read tagged sentence data"""<br/>    with open(filename, 'r') as f:<br/>        sentence_lines = [l.split("\n") for l in f.read().split("\n\n")]<br/>    return OrderedDict(((s[0], Sentence(*zip(*[l.strip().split("\t")<br/>                        for l in s[1:]]))) for s in sentence_lines if s[0]))</span><span id="5c1a" class="mn lc iq mj b gy ms mp l mq mr">def read_tags(filename):<br/>    """Read a list of word tag classes"""<br/>    with open(filename, 'r') as f:<br/>        tags = f.read().split("\n")<br/>    return frozenset(tags)</span><span id="b877" class="mn lc iq mj b gy ms mp l mq mr">Sentence = namedtuple("Sentence", "words tags")</span><span id="6a83" class="mn lc iq mj b gy ms mp l mq mr">def read_data(filename):<br/>    """Read tagged sentence data"""<br/>    with open(filename, 'r') as f:<br/>        sentence_lines = [l.split("\n") for l in f.read().split("\n\n")]<br/>    return OrderedDict(((s[0], Sentence(*zip(*[l.strip().split("\t")<br/>                        for l in s[1:]]))) for s in sentence_lines if s[0]))</span><span id="0634" class="mn lc iq mj b gy ms mp l mq mr">def read_tags(filename):<br/>    """Read a list of word tag classes"""<br/>    with open(filename, 'r') as f:<br/>        tags = f.read().split("\n")<br/>    return frozenset(tags)</span><span id="b936" class="mn lc iq mj b gy ms mp l mq mr">class Subset(namedtuple("BaseSet", "sentences keys vocab X tagset Y N stream")):<br/>    def __new__(cls, sentences, keys):<br/>        word_sequences = tuple([sentences[k].words for k in keys])<br/>        tag_sequences = tuple([sentences[k].tags for k in keys])<br/>        wordset = frozenset(chain(*word_sequences))<br/>        tagset = frozenset(chain(*tag_sequences))<br/>        N = sum(1 for _ in chain(*(sentences[k].words for k in keys)))<br/>        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))<br/>        return super().__new__(cls, {k: sentences[k] for k in keys}, keys, wordset, word_sequences,<br/>                               tagset, tag_sequences, N, stream.__iter__)</span><span id="84c4" class="mn lc iq mj b gy ms mp l mq mr">def __len__(self):<br/>        return len(self.sentences)</span><span id="6976" class="mn lc iq mj b gy ms mp l mq mr">def __iter__(self):<br/>        return iter(self.sentences.items())</span><span id="0811" class="mn lc iq mj b gy ms mp l mq mr">class Dataset(namedtuple("_Dataset", "sentences keys vocab X tagset Y training_set testing_set N stream")):<br/>    def __new__(cls, tagfile, datafile, train_test_split=0.8, seed=112890):<br/>        tagset = read_tags(tagfile)<br/>        sentences = read_data(datafile)<br/>        keys = tuple(sentences.keys())<br/>        wordset = frozenset(chain(*[s.words for s in sentences.values()]))<br/>        word_sequences = tuple([sentences[k].words for k in keys])<br/>        tag_sequences = tuple([sentences[k].tags for k in keys])<br/>        N = sum(1 for _ in chain(*(s.words for s in sentences.values())))<br/>        <br/>        # split data into train/test sets<br/>        _keys = list(keys)<br/>        if seed is not None: random.seed(seed)<br/>        random.shuffle(_keys)<br/>        split = int(train_test_split * len(_keys))<br/>        training_data = Subset(sentences, _keys[:split])<br/>        testing_data = Subset(sentences, _keys[split:])<br/>        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))<br/>        return super().__new__(cls, dict(sentences), keys, wordset, word_sequences, tagset,<br/>                               tag_sequences, training_data, testing_data, N, stream.__iter__)</span><span id="8c02" class="mn lc iq mj b gy ms mp l mq mr">def __len__(self):<br/>        return len(self.sentences)</span><span id="cdc8" class="mn lc iq mj b gy ms mp l mq mr">def __iter__(self):<br/>        return iter(self.sentences.items())</span><span id="6418" class="mn lc iq mj b gy ms mp l mq mr">data = Dataset("tags-universal.txt", "brown-universal.txt", train_test_split=0.8)</span><span id="9993" class="mn lc iq mj b gy ms mp l mq mr">print("There are {} sentences in the corpus.".format(len(data)))<br/>print("There are {} sentences in the training set.".format(len(data.training_set)))<br/>print("There are {} sentences in the testing set.".format(len(data.testing_set)))</span></pre><p id="aab9" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">语料库中有 57340 个句子。</em>T15】</strong></p><p id="e7a5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集中有 45872 个句子。</em>T19】</strong></p><p id="694d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集中有 11468 个句子。</em> </strong></p><p id="3fbb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">看一眼数据。句子</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="dbcc" class="mn lc iq mj b gy mo mp l mq mr">key = 'b100-38532'<br/>print("Sentence: {}".format(key))<br/>print("words:\n\t{!s}".format(data.sentences[key].words))<br/>print("tags:\n\t{!s}".format(data.sentences[key].tags))</span></pre><p id="3c07" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">句子:b100–38532</em></strong></p><p id="56d2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">词语:</em> </strong></p><p id="6a62" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mt">(‘也许’，‘它’，‘曾经’，‘对’，‘对’，’, ';')</em> </strong></p><p id="6bb6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">标签:</em> </strong></p><p id="933b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mt">(‘ADV’，‘PRON’，‘动词’，‘ADJ’，’, '.')</em> </strong></p><p id="2fe2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">统计语料库中的独特元素。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="da14" class="mn lc iq mj b gy mo mp l mq mr">print("There are a total of {} samples of {} unique words in the corpus."<br/>      .format(data.N, len(data.vocab)))<br/>print("There are {} samples of {} unique words in the training set."<br/>      .format(data.training_set.N, len(data.training_set.vocab)))<br/>print("There are {} samples of {} unique words in the testing set."<br/>      .format(data.testing_set.N, len(data.testing_set.vocab)))<br/>print("There are {} words in the test set that are missing in the training set."<br/>      .format(len(data.testing_set.vocab - data.training_set.vocab)))</span></pre><p id="4f68" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">语料库中共有 1161192 个样本 56057 个独特词。</em>T47】</strong></p><p id="e9c4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练集中有 50536 个唯一词的 928458 个样本。</em>T51】</strong></p><p id="30f6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">测试集中有 25112 个唯一词的 232734 个样本。T55】</p><p id="2728" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试集中有 5521 个词是训练集中缺失的。</em>T59】</strong></p><p id="a02b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">使用数据集访问单词。x 和带有数据集的标签。Y</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="d009" class="mn lc iq mj b gy mo mp l mq mr">for i in range(2):    <br/>    print("Sentence {}:".format(i + 1), data.X[i])<br/>    print()<br/>    print("Labels {}:".format(i + 1), data.Y[i])<br/>    print()</span></pre><p id="c698" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">句子 1:('先生'，'波德格'，'曾'，'谢过'，'他'，'庄重地'，'和'，'现在'，'他'，'造了'，'用了'，'的'，'了'，'建议'，'。')</em> </strong></p><p id="d169" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">标签 1:('名词'，'名词'，'动词'，'动词'，' PRON '，'等)、'康吉'、' ADV '、' PRON '、'动词'、'名词'、' ADP '、' DET '、'名词'、'等)</em> </strong></p><p id="f083" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">第二句:('但是'，'那里'，'似乎'，'要'，'成为'，'一些'，'不同'，'的'，'意见'，'作为'，'要'，'如何'，'远'，'板'，'应该'，'去'，'，'和'，'谁的'，'建议'，'它'，'应该'，'跟随'，')</em> </strong></p><p id="b9f3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">标签 2: ('CONJ '，' PRT '，'动词'，' PRT '，'动词'，' DET '，'名词'，' ADP '，' ADP '，' ADV '，' ADV '，' DET '，'名词'，'动词'，'动词'，'、' CONJ '、' DET '、'名词'、' PRON '、'动词'、'动词'、')</em> </strong></p><p id="3703" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">了解 data.stream()的工作原理。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5066" class="mn lc iq mj b gy mo mp l mq mr">print("\nStream (word, tag) pairs:\n")<br/>for i, pair in enumerate(data.stream()):<br/>    print("\t", pair)<br/>    if i &gt; 3: break</span></pre><p id="9462" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">流(词、标记)对:(“先生”、“名词”)</em> </strong></p><p id="a807" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mt">(‘波德格’，‘名词’)</em></strong></p><p id="9940" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mt">(‘曾’，‘动词’)</em></strong></p><p id="6bcc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mt">(‘谢’，‘动词’)</em></strong></p><p id="acb4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"><em class="mt">(‘他’，‘PRON’)</em></strong></p><h1 id="d0d7" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">构建一个最频繁的类标记器(MFC) </strong></h1><p id="185e" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">对计数</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="0a57" class="mn lc iq mj b gy mo mp l mq mr">def pair_counts(tags, words):<br/>    d = defaultdict(lambda: defaultdict(int))<br/>    for tag, word in zip(tags, words):<br/>        d[tag][word] += 1<br/>        <br/>    return d<br/>tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]<br/>words = [word for i, (word, tag) in enumerate(data.training_set.stream())]</span></pre><p id="8570" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">MFC 标记器</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="7953" class="mn lc iq mj b gy mo mp l mq mr">FakeState = namedtuple('FakeState', 'name')</span><span id="e8c6" class="mn lc iq mj b gy ms mp l mq mr">class MFCTagger:<br/>    missing = FakeState(name = '&lt;MISSING&gt;')<br/>    <br/>    def __init__(self, table):<br/>        self.table = defaultdict(lambda: MFCTagger.missing)<br/>        self.table.update({word: FakeState(name=tag) for word, tag in table.items()})<br/>        <br/>    def viterbi(self, seq):<br/>        """This method simplifies predictions by matching the Pomegranate viterbi() interface"""<br/>        return 0., list(enumerate(["&lt;start&gt;"] + [self.table[w] for w in seq] + ["&lt;end&gt;"]))<br/>    <br/>tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]<br/>words = [word for i, (word, tag) in enumerate(data.training_set.stream())]</span><span id="1560" class="mn lc iq mj b gy ms mp l mq mr">word_counts = pair_counts(words, tags)<br/>mfc_table = dict((word, max(tags.keys(), key=lambda key: tags[key])) for word, tags in word_counts.items())</span><span id="eb64" class="mn lc iq mj b gy ms mp l mq mr">mfc_model = MFCTagger(mfc_table)</span></pre><p id="5c91" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">用模型做预测。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="ad67" class="mn lc iq mj b gy mo mp l mq mr">def replace_unknown(sequence):<br/>    <br/>    return [w if w in data.training_set.vocab else 'nan' for w in sequence]</span><span id="8d2f" class="mn lc iq mj b gy ms mp l mq mr">def simplify_decoding(X, model):<br/>    <br/>    _, state_path = model.viterbi(replace_unknown(X))<br/>    return [state[1].name for state in state_path[1:-1]]</span></pre><p id="8d6a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">MFC Tagger 解码序列示例</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="2cf2" class="mn lc iq mj b gy mo mp l mq mr">for key in data.testing_set.keys[:2]:<br/>    print("Sentence Key: {}\n".format(key))<br/>    print("Predicted labels:\n-----------------")<br/>    print(simplify_decoding(data.sentences[key].words, mfc_model))<br/>    print()<br/>    print("Actual labels:\n--------------")<br/>    print(data.sentences[key].tags)<br/>    print("\n")</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mu"><img src="../Images/513e1224b140345b77fd141792bb11f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OD96YAnaxIzCcgqkyMzRCQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1</figcaption></figure><p id="a16a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">看起来不错。</p><p id="1873" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">评估模型准确性。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="adf8" class="mn lc iq mj b gy mo mp l mq mr">def accuracy(X, Y, model):<br/>    <br/>    correct = total_predictions = 0<br/>    for observations, actual_tags in zip(X, Y):<br/>        <br/>        # The model.viterbi call in simplify_decoding will return None if the HMM<br/>        # raises an error (for example, if a test sentence contains a word that<br/>        # is out of vocabulary for the training set). Any exception counts the<br/>        # full sentence as an error (which makes this a conservative estimate).<br/>        try:<br/>            most_likely_tags = simplify_decoding(observations, model)<br/>            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))<br/>        except:<br/>            pass<br/>        total_predictions += len(observations)<br/>    return correct / total_predictions</span></pre><p id="39e0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">评估 MFC 标记的准确性。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9509" class="mn lc iq mj b gy mo mp l mq mr">mfc_training_acc = accuracy(data.training_set.X, data.training_set.Y, mfc_model)<br/>print("training accuracy mfc_model: {:.2f}%".format(100 * mfc_training_acc))</span><span id="57d8" class="mn lc iq mj b gy ms mp l mq mr">mfc_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, mfc_model)<br/>print("testing accuracy mfc_model: {:.2f}%".format(100 * mfc_testing_acc))</span></pre><p id="e5b3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练准确率 mfc_model: 95.72% </em> </strong></p><p id="3330" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试准确率 mfc_model: 93.01% </em> </strong></p><p id="85a4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">很好，让我们看看我们是否能做得更好！</p><h1 id="47a5" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">构建一个隐马尔可夫模型(HMM)标记器</h1><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="7bcd" class="mn lc iq mj b gy mo mp l mq mr">def unigram_counts(sequences):</span><span id="2f99" class="mn lc iq mj b gy ms mp l mq mr">return Counter(sequences)</span><span id="3250" class="mn lc iq mj b gy ms mp l mq mr">tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]<br/>tag_unigrams = unigram_counts(tags)</span></pre><p id="b916" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">二元组计数</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4961" class="mn lc iq mj b gy mo mp l mq mr">def bigram_counts(sequences):</span><span id="e55a" class="mn lc iq mj b gy ms mp l mq mr">d = Counter(sequences)<br/>    return d</span><span id="6a6d" class="mn lc iq mj b gy ms mp l mq mr">tags = [tag for i, (word, tag) in enumerate(data.stream())]<br/>o = [(tags[i],tags[i+1]) for i in range(0,len(tags)-2,2)]<br/>tag_bigrams = bigram_counts(o)</span></pre><p id="fb1c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">序列开始计数</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="3469" class="mn lc iq mj b gy mo mp l mq mr">def starting_counts(sequences):<br/>    <br/>    d = Counter(sequences)<br/>    return d</span><span id="d090" class="mn lc iq mj b gy ms mp l mq mr">tags = [tag for i, (word, tag) in enumerate(data.stream())]<br/>starts_tag = [i[0] for i in data.Y]<br/>tag_starts = starting_counts(starts_tag)</span></pre><p id="88da" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">序列结束计数。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="d08a" class="mn lc iq mj b gy mo mp l mq mr">def ending_counts(sequences):<br/>    <br/>    d = Counter(sequences)<br/>    return d</span><span id="7724" class="mn lc iq mj b gy ms mp l mq mr">end_tag = [i[len(i)-1] for i in data.Y]<br/>tag_ends = ending_counts(end_tag)</span></pre><p id="3646" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">嗯 Tagger</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="ad94" class="mn lc iq mj b gy mo mp l mq mr">basic_model = HiddenMarkovModel(name="base-hmm-tagger")</span><span id="2da1" class="mn lc iq mj b gy ms mp l mq mr">tags = [tag for i, (word, tag) in enumerate(data.stream())]<br/>words = [word for i, (word, tag) in enumerate(data.stream())]</span><span id="0771" class="mn lc iq mj b gy ms mp l mq mr">tags_count=unigram_counts(tags)<br/>tag_words_count=pair_counts(tags,words)</span><span id="e052" class="mn lc iq mj b gy ms mp l mq mr">starting_tag_list=[i[0] for i in data.Y]<br/>ending_tag_list=[i[-1] for i in data.Y]</span><span id="d338" class="mn lc iq mj b gy ms mp l mq mr">starting_tag_count=starting_counts(starting_tag_list)#the number of times a tag occured at the start<br/>ending_tag_count=ending_counts(ending_tag_list)      #the number of times a tag occured at the end</span><span id="19d2" class="mn lc iq mj b gy ms mp l mq mr">to_pass_states = []<br/>for tag, words_dict in tag_words_count.items():<br/>    total = float(sum(words_dict.values()))<br/>    distribution = {word: count/total for word, count in words_dict.items()}<br/>    tag_emissions = DiscreteDistribution(distribution)<br/>    tag_state = State(tag_emissions, name=tag)<br/>    to_pass_states.append(tag_state)</span><span id="2c84" class="mn lc iq mj b gy ms mp l mq mr">basic_model.add_states()</span><span id="31c4" class="mn lc iq mj b gy ms mp l mq mr">start_prob={}</span><span id="0103" class="mn lc iq mj b gy ms mp l mq mr">for tag in tags:<br/>    start_prob[tag]=starting_tag_count[tag]/tags_count[tag]</span><span id="4ab1" class="mn lc iq mj b gy ms mp l mq mr">for tag_state in to_pass_states :<br/>    basic_model.add_transition(basic_model.start,tag_state,start_prob[tag_state.name])</span><span id="023f" class="mn lc iq mj b gy ms mp l mq mr">end_prob={}</span><span id="663e" class="mn lc iq mj b gy ms mp l mq mr">for tag in tags:<br/>    end_prob[tag]=ending_tag_count[tag]/tags_count[tag]<br/>for tag_state in to_pass_states :<br/>    basic_model.add_transition(tag_state,basic_model.end,end_prob[tag_state.name])</span><span id="643f" class="mn lc iq mj b gy ms mp l mq mr">transition_prob_pair={}</span><span id="8386" class="mn lc iq mj b gy ms mp l mq mr">for key in tag_bigrams.keys():<br/>    transition_prob_pair[key]=tag_bigrams.get(key)/tags_count[key[0]]<br/>for tag_state in to_pass_states :<br/>    for next_tag_state in to_pass_states :<br/>        basic_model.add_transition(tag_state,next_tag_state,transition_prob_pair[(tag_state.name,next_tag_state.name)])</span><span id="feaa" class="mn lc iq mj b gy ms mp l mq mr">basic_model.bake()</span></pre><p id="36b9" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">HMM 标记的解码序列示例。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="c87b" class="mn lc iq mj b gy mo mp l mq mr">for key in data.testing_set.keys[:2]:<br/>    print("Sentence Key: {}\n".format(key))<br/>    print("Predicted labels:\n-----------------")<br/>    print(simplify_decoding(data.sentences[key].words, basic_model))<br/>    print()<br/>    print("Actual labels:\n--------------")<br/>    print(data.sentences[key].tags)<br/>    print("\n")</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mv"><img src="../Images/32449902fce51a4d19c4b2dbecec17d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UUQ6ZJhZcVIWtGQUK_uV8w.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2</figcaption></figure><p id="a747" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">评估 HMM 标记器的准确性。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="04ba" class="mn lc iq mj b gy mo mp l mq mr">hmm_training_acc = accuracy(data.training_set.X, data.training_set.Y, basic_model)<br/>print("training accuracy basic hmm model: {:.2f}%".format(100 * hmm_training_acc))</span><span id="3262" class="mn lc iq mj b gy ms mp l mq mr">hmm_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, basic_model)<br/>print("testing accuracy basic hmm model: {:.2f}%".format(100 * hmm_testing_acc))</span></pre><p id="7dda" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">训练准确率基本 hmm 模型:97.49% </em> </strong></p><p id="e78d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试准确率基本 hmm 模型:96.09% </em> </strong></p><p id="b478" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们的 HMM 标记确实改善了结果，</p><p id="6c99" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在我们已经完成了模型的构建。源代码可以在<a class="ae la" href="https://github.com/susanli2016/Text-Mining-with-Python/blob/master/Hidden%20Markov%20Models%20for%20POS.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。我期待听到反馈或问题。</p></div></div>    
</body>
</html>