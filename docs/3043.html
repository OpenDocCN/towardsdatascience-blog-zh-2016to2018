<html>
<head>
<title>Building a Soft Decision Tree in TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在TensorFlow中构建软决策树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-decision-tree-in-tensorflow-742438cb483e?source=collection_archive---------2-----------------------#2018-04-03">https://towardsdatascience.com/building-a-decision-tree-in-tensorflow-742438cb483e?source=collection_archive---------2-----------------------#2018-04-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/14aaed7bcdb6b2cad5e9e14780d26e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nCxmsbkMIFuOkve_lndh5w.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">20X magnification of the somatosensory cortex (by <a class="ae kc" href="https://www.flickr.com/photos/cudmore/5427064" rel="noopener ugc nofollow" target="_blank">Robert Cudmore</a> — <a class="ae kc" href="https://creativecommons.org/licenses/by-sa/2.0/" rel="noopener ugc nofollow" target="_blank">CC BY-SA 2.0</a>)</figcaption></figure><h1 id="b46a" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">介绍</h1><p id="d98c" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">深度学习产品在行业内引起了越来越多的兴趣。虽然计算量很大，但在处理自然语言或图像处理等非结构化数据的分类方面，它们已被证明是有效的。</p><p id="7471" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">高价值数据科学产品的目标不仅在于其高价值交付，还在于其被业务接受。因此，可解释性仍然是一个重要因素。</p><p id="c126" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">Google Brain最近发布了一篇<a class="ae kc" href="https://arxiv.org/abs/1711.09784" rel="noopener ugc nofollow" target="_blank">论文</a>，提议实现软决策树。</p><p id="504f" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">在这篇博客中，我从他们的论文中摘录了一些要点，并举例说明了神经网络是如何模仿树形结构的。</p><h1 id="92bd" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">目标</h1><ol class=""><li id="d200" class="me mf iq ld b le lf li lj lm mg lq mh lu mi ly mj mk ml mm bi translated">什么是数据科学中的决策树？</li><li id="1815" class="me mf iq ld b le mn li mo lm mp lq mq lu mr ly mj mk ml mm bi translated">神经网络怎么会有树状结构？</li><li id="dac5" class="me mf iq ld b le mn li mo lm mp lq mq lu mr ly mj mk ml mm bi translated">在TensorFlow中实现软决策树</li></ol><h1 id="5fb8" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">软决策树</h1><h2 id="83b2" class="ms ke iq bd kf mt mu dn kj mv mw dp kn lm mx my kr lq mz na kv lu nb nc kz nd bi translated">树</h2><p id="3a3e" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了不拘泥于形式，在数据科学中，树由三个主要成分组成:</p><ul class=""><li id="2de5" class="me mf iq ld b le lz li ma lm ne lq nf lu ng ly nh mk ml mm bi translated">分割条件</li><li id="5580" class="me mf iq ld b le mn li mo lm mp lq mq lu mr ly nh mk ml mm bi translated">修剪条件</li><li id="ed18" class="me mf iq ld b le mn li mo lm mp lq mq lu mr ly nh mk ml mm bi translated">离开</li></ul><p id="af4b" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">分割条件将数据分割到叶子上。修剪条件决定了分区停止的时间。最后，也是最重要的，树叶包含预测的目标值，该值已被拟合到分区内的数据。</p><p id="bb2e" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">如图1所示，每个分割条件都是特性的函数。</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/ab8fbcee988f299d70cd145a6cafbdd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DcDO-FVnxf226Amp."/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><em class="nn">Figure 1: Basic Tree. Each data-point travels through the tree until one of the leafs. The path is determined by the split conditions, which are functions of the features. The leafs determine the prediction target.</em></figcaption></figure><p id="3b4a" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">每个内部节点中的分裂和叶子的目标输出由损失函数的优化来确定。为每个数据点分配一个预测成本。</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f55e50b1f8ee00f86ed1e946de6a66f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/0*VPG1D39JYJ2ON455."/></div></figure><p id="9603" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">在哪里</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/ae5b33c08fc7eda3cb7d869cf20717cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:142/0*p2p1pYUEO8SbmBwd."/></div></figure><p id="456d" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">是数据点x的目标</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/7c46471de47704490b7f4b13f602e561.png" data-original-src="https://miro.medium.com/v2/resize:fit:208/0*CC-M9rp6R4t3teYJ."/></div></figure><p id="f8d5" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">是叶的目标类的概率。我们还表示了叶选择器:</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/eea2a4ddce915ee97a8e37080b53cdc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/0*ma8_t28bc4IoxLsq."/></div></figure><p id="c485" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">这个符号将在下一节中阐明。</p><h2 id="5cbe" class="ms ke iq bd kf mt mu dn kj mv mw dp kn lm mx my kr lq mz na kv lu nb nc kz nd bi translated">神经网络</h2><p id="9452" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">神经网络由一组对其输入特征的平滑操作组成。所谓平滑，我的意思是每个操作在其变量中是连续可微的。</p><p id="48c0" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">可微性是一个至关重要的因素，因为最佳变量是使用损失和操作的梯度计算的。起初这似乎是个问题，如果我们想把树设计成树的每个内部节点，基于分割条件分配它的一个孩子的每个数据点。</p><p id="7ab8" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">因此，我们用神经网络构建树的第一步是通过允许数据在每次分裂时以一定的概率向左和向右移动来消除分裂条件。</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/2f3b42ab4096d3b42f04a4045974a5e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/0*nXLAHkjuvOte-YGh."/></div></figure><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/c8b25f01f69e43905585d581257a49a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/0*2UI5JkP4s9rdEJFf.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><em class="nn">Figure 2: The hard split-condition of the tree is replaced by split-probability.</em></figcaption></figure><p id="0181" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">因此，每个数据点在树中没有唯一的路径。它们现在属于树的每一片叶子，具有一定的概率，即路径概率。</p><p id="7abd" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">让我们表示从树根到第n个(内部)节点的路径概率，</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/13abfecc23b5caa875eb6700805c22d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:184/0*kuuF1HFXnkIggt4k."/></div></figure><p id="7fc9" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">我们可以回顾一下我们之前写的损失函数，对于每个数据点<strong class="ld ir"> x </strong>:</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/ee06f6505f6d9a6e6c649145d0d6762f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/0*gW5hx-rXfZRHKWFq."/></div></figure><p id="2306" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">这只不过是对分类交叉熵的加权，</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e2a3162c3e09f5998d34093ca128fa7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/0*7UHFOR1MDepWXbby."/></div></figure><p id="0164" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">以<strong class="ld ir"> <em class="nx"> x </em> </strong>的概率属于一片树叶，这就是路径概率。</p><h2 id="cb56" class="ms ke iq bd kf mt mu dn kj mv mw dp kn lm mx my kr lq mz na kv lu nb nc kz nd bi translated">正规化</h2><p id="d8f0" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们现在已经重写了分割条件。叶输出可以保持概率，例如softmax。</p><p id="0803" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">最后一个问题仍然存在，但是，优化将收敛到一个适当的局部最小值？</p><p id="dc31" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">内部节点权重的更新取决于损失的梯度，因此也取决于路径概率</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/7a50ac2885ea8f363c143809d200d26a.png" data-original-src="https://miro.medium.com/v2/resize:fit:184/0*d1Y804AkY82YZ6uV."/></div></figure><p id="6922" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">让我们仔细看看它在一些内部节点j:</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/56a6e25c8cc116819300fb0007656998.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/0*SHasIDwQZ94xaZXc."/></div></div></figure><p id="15e3" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">我们立即看到了这里的一个问题。如果某片叶子的概率</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/143985621a117a65624b0dd214db8d76.png" data-original-src="https://miro.medium.com/v2/resize:fit:136/format:webp/0*hi7GHXLryAAc5A4z.png"/></div></figure><p id="d0d0" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">对于大量数据来说很小，那么权重在下一步中几乎不被更新。更有甚者，在这次更新之后，这种可能性仍然很小。因此，我们的模型可能会收敛并陷入这些无用的局部极小值。</p><p id="7903" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">一棵性能树不仅有最大纯度的叶子，而且在叶子上尽可能均匀地分布数据。如果我们的模型陷入这些局部极小值，我们就只能得到一些高度密集的叶子和许多稀疏的叶子。在这种情况下，我们失去了树的用处，我们最终得到一个逻辑回归模型的小集合。</p><p id="8d80" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">谷歌大脑的团队提出了一个解决这个问题的方法，</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oa"><img src="../Images/ae6fdeb31b902580f464480f8684d48f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sxq9OQidcTETjB51."/></div></div></figure><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/31284d3ab9e19903a835d2f9bb52a5f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/0*XzJEUFVQyNQNO3Ui."/></div></figure><p id="49bd" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">这种正则化有利于均匀的二进制分裂，并且不利于内部节点级别上的早期净化。衰减率gamma在树的早期内部节点中增加了重点。</p><p id="7cfc" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">现在一切都设置好了，我们可以讨论TensorFlow实现了。</p><h1 id="9ded" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">TensorFlow实现</h1><p id="dc99" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">TensorFlow是Google Brain的一个流行的开源软件库，主要用于深度学习模型的开发。用户在静态计算图上定义操作，并根据需要运行图的不同部分。</p><p id="4bb2" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">让我们用一些基本公式来总结不同的成分:</p><ul class=""><li id="86d6" class="me mf iq ld b le lz li ma lm ne lq nf lu ng ly nh mk ml mm bi translated"><strong class="ld ir">拆分条件</strong></li></ul><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/226fbc863b05ca9b41a9710993a3f47b.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/0*QskVt8y0GrwlqGu0."/></div></figure><ul class=""><li id="7037" class="me mf iq ld b le lz li ma lm ne lq nf lu ng ly nh mk ml mm bi translated">修剪条件:让我们坚持一个简单的最大深度</li><li id="a506" class="me mf iq ld b le mn li mo lm mp lq mq lu mr ly nh mk ml mm bi translated"><strong class="ld ir">叶片输出</strong></li></ul><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi od"><img src="../Images/e2680fa661f7497c99ba096e412f6049.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/0*jECvAoODTvD_l_la."/></div></figure><ul class=""><li id="7f81" class="me mf iq ld b le lz li ma lm ne lq nf lu ng ly nh mk ml mm bi translated">每个数据点x的<strong class="ld ir">损失</strong></li></ul><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/bd7f79b6ac96f16daae673bb7c896b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*M4XXj9gBS0VKVeu9."/></div></div></figure><p id="1bea" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">我们将工作分成三类:</p><ul class=""><li id="460d" class="me mf iq ld b le lz li ma lm ne lq nf lu ng ly nh mk ml mm bi translated"><strong class="ld ir"> TreeProperties </strong>:包含模型的所有超参数相关属性。</li><li id="5e7d" class="me mf iq ld b le mn li mo lm mp lq mq lu mr ly nh mk ml mm bi translated"><strong class="ld ir">节点</strong>:定义所有节点级的操作，如内部节点转发如下所示的概率，叶子节点转发softmax:</li></ul><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi of"><img src="../Images/92517d49f0d72091509f75b2425c70be.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/format:webp/0*H_oOgjM6L3GwdQcm.png"/></div></figure><ul class=""><li id="f48a" class="me mf iq ld b le lz li ma lm ne lq nf lu ng ly nh mk ml mm bi translated"><strong class="ld ir">软决策树</strong>:为每个数据点建立所有的内节点、叶子并输出预测目标。</li></ul><p id="12b9" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">让我们仔细看看！</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="9c4e" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">如第3.2.2节所示，超参数<strong class="ld ir"> self.decay_penalty </strong>和<strong class="ld ir">self . regularization _ penalty</strong>分别对应于衰减率γ和权重λ。</p><p id="202b" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">下一节课更有趣。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="3e64" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">在实例化时，我们定义节点分裂概率的权重和偏差。在构建节点时，我们首先转发节点的输出，要么是softmax预测概率，要么是内部节点分裂的概率。</p><p id="355a" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">损失也在节点级别上更新。内部节点仅通过前面描述的正则化来增加损失。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="3356" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">最后，所有的节点被组合成一棵树。目标的输出预测和节点的概率分布组合成<strong class="ld ir">自输出</strong>和<strong class="ld ir">自叶分布</strong>。</p><h1 id="ac28" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">对MNIST进行测试</h1><p id="1aeb" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">最后，让我们对MNIST进行一些测试。使用以下超参数构建树:</p><pre class="nj nk nl nm gt oi oj ok ol aw om bi"><span id="f9a8" class="ms ke iq oj b gy on oo l op oq">max_depth = 7<br/>regularisation_penality=10.<br/>decay_penality=0.9</span></pre><p id="59c6" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">并且选择训练批量为32，验证批量为256。我们得出以下结果:</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi or"><img src="../Images/e378e0114d88ce721da8275ef22ce882.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CWu-IJuQXoADoGY9."/></div></div></figure><p id="5001" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">还不错。当然没有CNN那么有竞争力，但是我们看到了很多有前途的想法！</p><p id="79f4" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">请务必查看<a class="ae kc" href="https://github.com/benoitdescamps/Neural-Tree" rel="noopener ugc nofollow" target="_blank">的完整实现</a>。</p><h1 id="b188" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">我写了更多精彩的东西！</h1><p id="534f" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">@ <a class="ae kc" rel="noopener" target="_blank" href="/tuning-hyperparameters-part-i-successivehalving-c6c602865619">调整超参数(第一部分):successful halvin</a>g</p><p id="4ca6" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">@<a class="ae kc" href="https://www.kdnuggets.com/2018/01/custom-optimizer-tensorflow.html" rel="noopener ugc nofollow" target="_blank">tensor flow中的自定义优化器</a></p><p id="e9b5" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">@<a class="ae kc" href="https://medium.com/bigdatarepublic/regression-prediction-intervals-with-xgboost-428e0a018b" rel="noopener">XGBOOST回归预测区间</a></p></div></div>    
</body>
</html>