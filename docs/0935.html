<html>
<head>
<title>Memory, attention, sequences</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">记忆，注意力，顺序</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/memory-attention-sequences-37456d271992?source=collection_archive---------2-----------------------#2017-07-11">https://towardsdatascience.com/memory-attention-sequences-37456d271992?source=collection_archive---------2-----------------------#2017-07-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="9438" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们已经看到<a class="ae kl" href="https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba" rel="noopener">分类神经网络</a>的兴起和成功。神经网络的下一个重大步骤是理解来自观察和与现实世界互动的复杂时空数据。我们之前谈过在这个领域运作的<a class="ae kl" href="https://medium.com/towards-data-science/a-new-kind-of-deep-neural-networks-749bcde19108" rel="noopener">新一波神经网络</a>。</p><p id="73e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是我们如何利用这些网络来学习现实世界中的复杂任务呢？比如，我该如何告诉我的高级真空清洁机器人:“Roomby:你忘了用吸尘器清理客厅红色沙发下的污点了！”并得到适当的回应？</p><p id="59d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要做到这一点，我们需要用注意机制来解析时空信息，这样我们才能理解复杂指令以及它们与我们环境的关系。</p><p id="bf73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们考虑几个示例应用:文本或视频的摘要。考虑这段文字:</p><blockquote class="km"><p id="61d9" class="kn ko iq bd kp kq kr ks kt ku kv kk dk translated">一个穿白色裙子的女人向一个穿蓝色裙子的女人走来。她切了几片苹果。然后她给了穿蓝衣服的女人一片。</p></blockquote><p id="02ff" class="pw-post-body-paragraph jn jo iq jp b jq kw js jt ju kx jw jx jy ky ka kb kc kz ke kf kg la ki kj kk ij bi translated">为了回答这个问题:“谁提供了一片苹果？”我们需要关注“苹果”、“苹果所有者”、“给予”等词汇和概念。故事的其余部分是不相关的。故事的这些部分需要我们<em class="lb">的关注。</em></p><p id="5282" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">类似的情况也发生在视频摘要中，一个长视频可以被摘要为一小组帧序列，在这些帧序列中，重要的动作被执行，并且再次需要我们的<em class="lb">注意</em>。想象你在找车钥匙，或者你的鞋子，你会关注视频的不同部分和场景。对于每个行动和每个目标，我们需要把注意力集中在重要的数据上，而忽略其他的。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/c5f87651052467881c5598df9e0db012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wI6VUU1lh2iw2tayk32zuw.jpeg"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Example of video summarization</figcaption></figure><p id="0517" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">仔细想想，摘要和一组集中的数据对于每个时间序列都很重要，无论是文档的翻译，还是视频中的动作识别，或者是任务的句子描述和环境中的执行的组合。</p><blockquote class="km"><p id="5a5a" class="kn ko iq bd kp kq kr ks kt ku kv kk dk translated">所有这些任务都需要把数据减少到焦点集中，并注意这个集中以便提供一个答案或行动</p></blockquote><p id="64d2" class="pw-post-body-paragraph jn jo iq jp b jq kw js jt ju kx jw jx jy ky ka kb kc kz ke kf kg la ki kj kk ij bi translated">注意力是擅长理解序列的神经网络最重要的组成部分之一，无论是视频序列，现实生活中的动作序列，还是输入序列，如语音、文本或任何其他数据。无怪乎<a class="ae kl" href="https://en.wikipedia.org/wiki/Attention" rel="noopener ugc nofollow" target="_blank">我们的大脑在很多层面上实施注意力</a>，为的是只选择重要的信息进行处理，排除掉手头任务不需要的铺天盖地的背景信息。</p><p id="fadb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经网络中注意力的一个伟大回顾是<a class="ae kl" href="https://blog.heuritech.com/2016/01/20/attention-mechanism/" rel="noopener ugc nofollow" target="_blank">这里给出</a>。我在这里报告一些重要的图表作为参考:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/10fee7a0c27c3db8e080c6e67829d603.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*ol7jlD1cGbUHawotKphD-w.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">An attention model is a method that takes n arguments y_1 … y_n and a context c. It returns a vector z which is the summary of the y_i focusing on the information linked to context c. More formally, it returns a weighted arithmetic mean of the y_i and the weights are chosen according the relevance of each y_i given the context c.</figcaption></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/4eb90ed0546a0eae18cc48d63200796f.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*jDXSSdfrA2HAw7zGTYy97g.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Implementation of the attention model. Notice that m_i = tanh(W1 c + W2 y_i), meaning that both y_i and c are linearly combined.</figcaption></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lu"><img src="../Images/cfc5b8859cda81b5839d2c2790243ba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SJH08SpTUzDm8kBDnZC9TA.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Attention model with dot-products used to define relevance of inputs vs context.</figcaption></figure><p id="704d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面最后两个数字都实现了“软”注意力。硬注意是通过随机选取概率为 s_i 的输入 y_i 中的一个来实现的。这是比软注意的平均化更粗略的选择。软注意的使用是首选的，因为它可以通过反向传播来训练。</p><p id="4a89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意力和记忆系统也在这里用很好的形象化描述<a class="ae kl" href="http://distill.pub/2016/augmented-rnns/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="b878" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如他们在<a class="ae kl" href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>中提到的，注意力是有代价的，但实际上这种代价可以通过分级注意力模块来最小化，比如这里实现的<a class="ae kl" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="5c48" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">现在看看 attention 如何能够<a class="ae kl" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">为翻译</a>实现整个 RNN:</h1><p id="e676" class="pw-post-body-paragraph jn jo iq jp b jq mt js jt ju mu jw jx jy mv ka kb kc mw ke kf kg mx ki kj kk ij bi translated">它可以通过堆叠多层注意力模块来实现这一点，并采用如下架构:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi my"><img src="../Images/22f7796a3be4ec02ac91ba4974795d15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*au1TiIuAinffEPyowy1Cbg.jpeg"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Sequence to sequence system: an encoder takes in an input sequence x, and produces an embedding z. A decoder produces an output sequence y, by taking as input the embedding z and the previous output y of t-1.</figcaption></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/2c3cd3ea0ec4844bed052f3309f031c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*dH-bVcxSDR6DTn-Ub9LoeA.jpeg"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Module used for attention <a class="ae kl" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">here</a>. Q = query, K = key, V = values. Q and K are multiplied together and scaled to compute a “similarity metric”. This metric produces a weight that modulates the values V.</figcaption></figure><p id="ad36" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在多头注意力中，可以并行使用多个注意力模块:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi na"><img src="../Images/207df99547ecbee66a16f89f07a8e901.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*4NT8DPqX6D7UodWv_5dtmQ.jpeg"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Multiple attention heads are used in parallel to focus on different parts of a sequence in parallel. Here V,q,K are projected with neural network layers to another space, so they can be scaled and mixed.</figcaption></figure><p id="a4a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">整个基于注意力的网络被称为“变压器”网络:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nb"><img src="../Images/019448ba364f38fe12e571278812b3d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2NQu3LppHOaNgdi9KI4gwg.jpeg"/></div></div></figure><p id="0716" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 RNN，时间被编码在序列中，因为输入和输出一次流一个。在前馈神经网络中，需要表示时间来保存<em class="lb">位置编码</em>。在这些注意力驱动的网络中，时间被编码为一个附加的额外输入，一个正弦波。它基本上是一个添加到输入和输出中的信号，用来表示时间的流逝。注意这里的脑电波与<a class="ae kl" href="https://en.wikipedia.org/wiki/Neural_oscillation" rel="noopener ugc nofollow" target="_blank">神经振荡</a>的生物相似性。</p><blockquote class="km"><p id="4a1b" class="kn ko iq bd kp kq kr ks kt ku kv kk dk translated">但是为什么我们要使用基于注意力的神经网络，而不是我们到目前为止一直在使用的 RNN/LSTM 呢？因为它们使用的计算量少得多！</p></blockquote><p id="26a9" class="pw-post-body-paragraph jn jo iq jp b jq kw js jt ju kx jw jx jy ky ka kb kc kz ke kf kg la ki kj kk ij bi translated">如果您阅读论文的表 2，您将会看到这些网络可以节省 2-3 个数量级的操作！这是一些严重的节约！</p><p id="732b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我相信这种基于注意力网络将会在神经网络的许多应用中慢慢取代 RNN。</p><p id="45d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">在这里</a>你可以找到关于变压器架构和数据流的精彩解释！</p><h1 id="d962" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">记忆</h1><p id="a4cc" class="pw-post-body-paragraph jn jo iq jp b jq mt js jt ju mu jw jx jy mv ka kb kc mw ke kf kg mx ki kj kk ij bi translated">一项重要且有趣的工作是<a class="ae kl" href="https://arxiv.org/abs/1610.06258" rel="noopener ugc nofollow" target="_blank">快速加权</a>。这项工作实现了一种神经联想记忆——这是一种短期记忆，位于神经权重(长期)和递归权重(基于输入活动的非常快速的权重)之间。快速权重实现了一种类似于上面看到的神经注意力机制的记忆，其中我们将当前输入与一组存储的先前输入进行比较。这基本上就是上面看到的“点积注意力模型”中发生的事情。</p><p id="9bf9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在快速加权中，输入<em class="lb"> x(t) </em>是用于与下图中先前存储的值<em class="lb"> h </em>进行比较的<em class="lb">上下文</em>。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nc"><img src="../Images/b9d3a197395215f7e21a87c983db2c96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K8SQv7yC_AEXEDYNkTJ04w.jpeg"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Fast associative memory implemented in <a class="ae kl" href="https://arxiv.org/abs/1610.06258" rel="noopener ugc nofollow" target="_blank">Fast Weights</a></figcaption></figure><p id="073c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你读了<a class="ae kl" href="https://arxiv.org/abs/1610.06258" rel="noopener ugc nofollow" target="_blank">的论文</a>你会发现这种神经网络联想记忆再次胜过 RNN 和 LSTM 网络，同样注意力也能。</p><p id="13ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我认为，这再次证明，目前由 RNN 执行的许多任务可以被像这样计算成本更低(更不用说使用更少的内存带宽和参数)的算法所取代。</p><p id="0f11" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还请再看:<a class="ae kl" href="https://medium.com/@sanyamagarwal/understanding-attentive-recurrent-comparators-ea1b741da5c3" rel="noopener">注意力递归比较器</a>，也是结合注意力和递归层来了解一个学习单元的细节。</p><h1 id="aa51" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">关于作者</h1><p id="7e75" class="pw-post-body-paragraph jn jo iq jp b jq mt js jt ju mu jw jx jy mv ka kb kc mw ke kf kg mx ki kj kk ij bi translated">我在硬件和软件方面都有将近 20 年的神经网络经验(一个罕见的组合)。在这里看关于我:<a class="ae kl" href="https://medium.com/@culurciello/" rel="noopener">媒介</a>、<a class="ae kl" href="https://e-lab.github.io/html/contact-eugenio-culurciello.html" rel="noopener ugc nofollow" target="_blank">网页</a>、<a class="ae kl" href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ" rel="noopener ugc nofollow" target="_blank">学者</a>、<a class="ae kl" href="https://www.linkedin.com/in/eugenioculurciello/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>等等…</p><h1 id="b80d" class="lv lw iq bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">捐款</h1><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nd"><img src="../Images/bb6a9f917e37eb5d49e0181cc20b788c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gCbUDLiPi6SzCmfkhuqrCg.jpeg"/></div></div></figure><p id="d4e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你觉得这篇文章有用，请考虑捐赠来支持更多的教程和博客。任何贡献都能有所作为！</p></div></div>    
</body>
</html>