<html>
<head>
<title>An Introduction to Clustering Algorithms in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中聚类算法的介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-clustering-algorithms-in-python-123438574097?source=collection_archive---------1-----------------------#2018-05-29">https://towardsdatascience.com/an-introduction-to-clustering-algorithms-in-python-123438574097?source=collection_archive---------1-----------------------#2018-05-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="a78a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在数据科学中，我们经常会思考如何利用数据对新的数据点进行预测。这被称为“监督学习”然而，有时我们不想“做预测”,而是想将数据分成不同的类别。这被称为“无监督学习”</p><p id="ba0a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了说明这种差异，假设我们在一家大型披萨连锁店工作，我们的任务是在订单管理软件中创建一个功能，为客户预测送货时间。为了实现这一点，我们得到了一个数据集，其中包含过去几次交付的交付时间、行驶距离、星期几、时间、现有员工和销售量。根据这些数据，我们可以预测未来的交货时间。这是监督学习的一个很好的例子。</p><p id="e169" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们假设比萨饼连锁店想要向顾客发送有针对性的优惠券。它想把顾客分成 4 类:大家庭、小家庭、单身人士和大学生。我们获得了之前的订购数据(例如订单规模、价格、频率等)，我们的任务是将每个客户归入四个类别之一。这将是“无监督学习”的一个例子，因为我们不做预测；我们只是把顾客分成几组。</p><p id="c91d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">聚类是最常用的无监督学习形式之一。在本文中，我们将探讨两种最常见的聚类形式:k-means 和层次聚类。</p><p id="593c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">了解 K-Means 聚类算法</strong></p><p id="1a54" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看 k-means 聚类是如何工作的。首先，让我给你介绍我的好朋友，blobby 即 Python 的<a class="ae kl" href="http://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> sci-kit 学习库</a>中的<a class="ae kl" href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html" rel="noopener ugc nofollow" target="_blank"> make_blobs </a>函数。我们将使用 make _ blobs 创建四个随机集群来帮助我们完成任务。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="80ef" class="kv kw iq kr b gy kx ky l kz la"># import statements<br/>from sklearn.datasets import make_blobs<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="ebad" class="kv kw iq kr b gy lb ky l kz la"># create blobs<br/>data = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.6, random_state=50)</span><span id="4d14" class="kv kw iq kr b gy lb ky l kz la"># create np array for data points<br/>points = data[0]</span><span id="86ed" class="kv kw iq kr b gy lb ky l kz la"># create scatter plot<br/>plt.scatter(data[0][:,0], data[0][:,1], c=data[1], cmap='viridis')<br/>plt.xlim(-15,15)<br/>plt.ylim(-15,15)</span></pre><p id="3097" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以在下面看到我们的“斑点”:</p><figure class="km kn ko kp gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi lc"><img src="../Images/228cd7134812c3ed4bd60b19107a1979.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vwBYTWsoS622bChMfWJOZg.jpeg"/></div></div></figure><p id="6bf2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们有四个彩色集群，但是上面的两个集群和下面的两个集群有一些重叠。k-means 聚类的第一步是选择随机质心。因为我们的 k=4，我们需要 4 个随机质心。下面是它在我的实现中从头开始的样子。</p><figure class="km kn ko kp gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi lk"><img src="../Images/48123cd1cd8d75c165e5d6b34a418367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8JwQLh2MbojLnfigXLm1wQ.jpeg"/></div></div></figure><p id="5ce8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们取每个点，找到最近的质心。度量距离的方法有很多种，我用的是<a class="ae kl" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">欧氏距离</a>，可以用 Python 中的<a class="ae kl" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html" rel="noopener ugc nofollow" target="_blank"> np.linalg.norm </a>来度量。</p><figure class="km kn ko kp gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi ll"><img src="../Images/3215834ff57eb9896eab56ee5a3b6639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gNvJrDX1I39_TBeanTn5tQ.jpeg"/></div></div></figure><p id="0155" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们有了 4 个集群，我们找到了集群的新的质心。</p><figure class="km kn ko kp gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi lm"><img src="../Images/50dbc67860a4907b03a016a10b5bf7fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bOT_8MU8imj9eGvtC9hD9A.jpeg"/></div></div></figure><p id="8ade" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我们再次将每个点匹配到最近的质心，重复这个过程，直到我们不能再改进聚类。在这种情况下，当过程结束时，我得到了下面的结果。</p><figure class="km kn ko kp gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi ln"><img src="../Images/43cadb315625617b26b7d7194e70bc69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cx_zK9sDqYi7amU9-hdnpw.jpeg"/></div></div></figure><p id="2349" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，这些集群与我最初的集群有些不同。这是随机初始化陷阱的结果。本质上，我们的起始质心可以决定我们的聚类在 k 均值聚类中的位置。</p><p id="c70a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这不是我们想要的结果，但是解决这个问题的一个方法是使用<a class="ae kl" href="http://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf" rel="noopener ugc nofollow" target="_blank"> k-means ++算法，</a>它提供了更好的初始播种，以便找到最佳的聚类。幸运的是，这是在我们将在 Python 中使用的 k-means 实现中自动完成的。</p><p id="f1d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">在 Python 中实现 K-Means 聚类</strong></p><p id="443c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要在 Python 中运行 k-means，我们需要从 sci-kit learn 中导入<a class="ae kl" href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank"> KMeans。</a></p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="e51b" class="kv kw iq kr b gy kx ky l kz la"># import KMeans<br/>from sklearn.cluster import KMeans</span></pre><p id="63c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，在文档中，k-means ++是缺省的，所以我们不需要为了运行这个改进的方法而做任何改变。现在，让我们在 blobss 上运行 k-means(这些 blob 被放入一个名为“points”的 numpy 数组中)。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="1e11" class="kv kw iq kr b gy kx ky l kz la"># create kmeans object<br/>kmeans = KMeans(n_clusters=4)</span><span id="557f" class="kv kw iq kr b gy lb ky l kz la"># fit kmeans object to data<br/>kmeans.fit(points)</span><span id="6eee" class="kv kw iq kr b gy lb ky l kz la"># print location of clusters learned by kmeans object<br/>print(kmeans.cluster_centers_)</span><span id="1005" class="kv kw iq kr b gy lb ky l kz la"># save new clusters for chart<br/>y_km = kmeans.fit_predict(points)</span></pre><p id="179c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们可以通过在 matplotlib 中运行以下代码来查看结果。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="357a" class="kv kw iq kr b gy kx ky l kz la">plt.scatter(points[y_km ==0,0], points[y_km == 0,1], s=100, c='red')<br/>plt.scatter(points[y_km ==1,0], points[y_km == 1,1], s=100, c='black')<br/>plt.scatter(points[y_km ==2,0], points[y_km == 2,1], s=100, c='blue')<br/>plt.scatter(points[y_km ==3,0], points[y_km == 3,1], s=100, c='cyan')</span></pre><p id="7d87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">瞧啊。我们有 4 个集群。请注意，k-means++算法比我在示例中运行的普通 ole' k-means 做得更好，因为它几乎完美地捕捉到了我们创建的初始聚类的边界。</p><figure class="km kn ko kp gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi lo"><img src="../Images/8ec4ddc5157a84c7e0503ebe7c61ace0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vhvxj6d09cumLM3rk933yg.jpeg"/></div></div></figure><p id="c096" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">K-means 是最常用的聚类形式，因为它速度快且简单。另一种非常常见的聚类方法是层次聚类。</p><p id="650e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">实现凝聚层次聚类</strong></p><p id="82d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">凝聚层次聚类在一个关键方面不同于 k-means。我们不是选择一些聚类，从随机的质心开始，而是从数据集中的每个点作为一个“聚类”开始然后我们找到最近的两个点，并把它们组合成一个簇。然后，我们找到下一个最近的点，这些成为一个集群。我们重复这个过程，直到我们只有一个巨大的星团。</p><p id="3883" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个过程中，我们创建了一个树状图。这是我们的“历史”你可以看到我们下面的数据点的树状图，以了解正在发生的事情。</p><figure class="km kn ko kp gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi lp"><img src="../Images/d2d11ced475616da539050732d86744b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p3Z5TOp0egX3ItAXw0JZlQ.jpeg"/></div></div></figure><p id="5713" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">树状图标出了每个聚类和距离。我们可以使用树状图找到我们选择的任何数字的聚类。在上面的树状图中，很容易看到第一个集群(蓝色)、第二个集群(红色)和第三个集群(绿色)的起点。这里只有前 3 个是用颜色编码的，但是如果你看树状图的红色一边，你也可以发现第 4 个集群的起点。该树状图一直运行，直到每个点都是它自己的单独聚类。</p><p id="3a31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看凝聚层次聚类在 Python 中是如何工作的。首先，让我们从<a class="ae kl" href="https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html" rel="noopener ugc nofollow" target="_blank"> scipy.cluster.hierarchy </a>和<a class="ae kl" href="http://scikit-learn.org/stable/modules/clustering.html" rel="noopener ugc nofollow" target="_blank"> sklearn.clustering </a>中导入必要的库。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="8ca0" class="kv kw iq kr b gy kx ky l kz la"># import hierarchical clustering libraries<br/>import scipy.cluster.hierarchy as sch<br/>from sklearn.cluster import AgglomerativeClustering</span></pre><p id="c22d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们创建我们的树状图(上面我已经向您展示过了)，确定我们需要多少个聚类，并保存这些聚类中的数据点以绘制它们的图表。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="ef96" class="kv kw iq kr b gy kx ky l kz la"># create dendrogram<br/>dendrogram = sch.dendrogram(sch.linkage(points, method='ward'))</span><span id="71d2" class="kv kw iq kr b gy lb ky l kz la"># create clusters<br/>hc = AgglomerativeClustering(n_clusters=4, affinity = 'euclidean', linkage = 'ward')</span><span id="dba6" class="kv kw iq kr b gy lb ky l kz la"># save clusters for chart<br/>y_hc = hc.fit_predict(points)</span></pre><p id="03c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们将像使用 k-means 算法一样，使用 matplotlib 查看我们的聚类。</p><pre class="km kn ko kp gt kq kr ks kt aw ku bi"><span id="be91" class="kv kw iq kr b gy kx ky l kz la">plt.scatter(points[y_hc ==0,0], points[y_hc == 0,1], s=100, c='red')<br/>plt.scatter(points[y_hc==1,0], points[y_hc == 1,1], s=100, c='black')<br/>plt.scatter(points[y_hc ==2,0], points[y_hc == 2,1], s=100, c='blue')<br/>plt.scatter(points[y_hc ==3,0], points[y_hc == 3,1], s=100, c='cyan')</span></pre><p id="8978" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">结果如下:</p><figure class="km kn ko kp gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi lq"><img src="../Images/c9b5936878345e2d9dae97bd5233c5ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I7Ze7FaLE_XkrHw9hnDTzQ.jpeg"/></div></div></figure><p id="8d09" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这种情况下，k-means 和层次聚类之间的结果非常相似。然而，情况并非总是如此。一般来说，凝聚层次聚类的优点是它往往会产生更准确的结果。缺点是，与 k-means 相比，层次聚类更难实现，并且消耗更多的时间/资源。</p><p id="9bb3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">延伸阅读</strong></p><p id="eb37" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你想了解更多关于聚类的知识，我强烈推荐 George Seif 的文章“数据科学家需要了解的 5 种聚类算法”</p><p id="97d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">其他资源</strong></p><ol class=""><li id="ba6f" class="lr ls iq jp b jq jr ju jv jy lt kc lu kg lv kk lw lx ly lz bi translated">G.詹姆斯，d .威滕等人。艾尔。<em class="ma">统计学习简介</em>，第十章:无监督学习，<a class="ae kl" href="http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf" rel="noopener ugc nofollow" target="_blank">链接</a> (PDF)</li><li id="09a8" class="lr ls iq jp b jq mb ju mc jy md kc me kg mf kk lw lx ly lz bi translated">安德里亚·特雷维尼奥，<em class="ma">K 均值聚类介绍</em>，<a class="ae kl" href="https://www.datascience.com/blog/k-means-clustering" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="ea11" class="lr ls iq jp b jq mb ju mc jy md kc me kg mf kk lw lx ly lz bi translated">基里尔·叶列缅科，<em class="ma">机器学习 A-Z </em> (Udemy 课程)，<a class="ae kl" href="https://www.udemy.com/machinelearning/learn/v4/overview" rel="noopener ugc nofollow" target="_blank">链接</a></li></ol></div></div>    
</body>
</html>