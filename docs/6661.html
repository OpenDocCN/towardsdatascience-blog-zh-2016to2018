<html>
<head>
<title>Convolutional Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/convolutional-neural-network-17fb77e76c05?source=collection_archive---------1-----------------------#2018-12-25">https://towardsdatascience.com/convolutional-neural-network-17fb77e76c05?source=collection_archive---------1-----------------------#2018-12-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8146" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">卷积神经网络导论</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8a5755826e783e52734abc032440d0d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ij3uWzuUqax2u7p-73_w2A.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="https://unsplash.com/photos/YvkH8R1zoQM" rel="noopener ugc nofollow" target="_blank"><strong class="bd kw">Fig 1. Toon Discovers how machines see</strong></a></figcaption></figure><p id="902b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在本文中，我们将了解什么是卷积神经网络，简称 ConvNets。ConvNets 是将深度学习中的图像工作提升到下一个水平的超级英雄。对于 ConvNets，输入是一幅图像，或者更具体地说，是一个 3D 矩阵。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="824c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们先来看看 ConvNet 是什么样子的！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ma"><img src="../Images/465b94f9c72e20fc3609853c1038c82b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rB2I1953RXE-LMytMv26yg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">[Fig 2.] </strong><a class="ae kv" href="http://file.scirp.org/Html/4-7800353_65406.htm" rel="noopener ugc nofollow" target="_blank"><strong class="bd kw">Convolutional Neural Network</strong></a></figcaption></figure><p id="c53a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">简单来说，<br/>一个 ConvNet 通常有 3 种类型的层:<br/> 1) <strong class="kz ir">卷积层</strong> ( <em class="mb"> CONV </em> ) <br/> 2) <strong class="kz ir">池层</strong> ( <em class="mb">池</em> ) <br/> 3) <strong class="kz ir">全连接层</strong> ( <em class="mb"> FC </em></p><p id="d8fa" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们更详细地看看每一层:</p><h1 id="6eb8" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">卷积层</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/89cb13a8f9c57d1ceb2e4fa383929f5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gSlM_2hk3hqxzELgu0B2Ag.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 3. Convolutional Layer</strong></figcaption></figure><p id="c7f1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">卷积层是 CNN 的第一层。<br/>作为<strong class="kz ir"> <em class="mb">输入</em> </strong>得到一个维度为<em class="mb">【h1 * w1 * D1】</em>的矩阵，就是上图中的蓝色矩阵。</p><p id="83f0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">接下来，我们有<strong class="kz ir"> <em class="mb">内核</em> </strong>(滤镜)。<br/>仁？<br/>内核是一个尺寸为<em class="mb">【H2 * w2 * D1】，</em>的矩阵，它是上图中(内核层中)多个长方体(内核)中的<em class="mb">一个</em>黄色长方体。<br/>对于每个卷积层，有多个内核堆叠在彼此的顶部，这就是形成图 2 中的黄色三维矩阵的原因，其维度为<em class="mb">【H2 * w2 * D2】</em>，其中<em class="mb"> d2 </em>是内核的数量。</p><p id="2de8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对于每个内核，我们都有其各自的<strong class="kz ir">偏差</strong>，这是一个标量。</p><p id="5dda" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">然后，我们有一个<strong class="kz ir"> <em class="mb">输出</em> </strong>给这一层，图 2 中的绿色矩阵，它有尺寸<em class="mb">【H3 * w3 * D2】。</em></p><p id="ee00" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们从上面揭示一些显而易见的事情。<br/> 1)输入和<em class="mb">一个</em>内核<strong class="kz ir">的深度(<em class="mb"> d1 </em>)(或通道数)相同</strong>。<br/> 2)输出<strong class="kz ir">的深度(<em class="mb"> d2 </em>)等于</strong>内核的数量(即橙色三维矩阵的深度)。</p><p id="30f0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">好，我们有输入，内核和输出。现在让我们看看 2D 输入和 2D 内核会发生什么，即<em class="mb"> d1=1 </em>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/c444d022ad171fbe6a0b57593327555f.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/1*tSb-EHybFdhvvm_wfjcYeA.gif"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 4. </strong><a class="ae kv" rel="noopener" target="_blank" href="/types-of-convolutions-in-deep-learning-717013397f4d"><strong class="bd kw">Calculation of output using 2D Convolution</strong></a></figcaption></figure><p id="0445" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对于图像上内核的每个位置，内核上的每个数字与输入矩阵上的相应数字相乘(<em class="mb">蓝色矩阵</em>)，然后对它们求和，得到输出矩阵中相应位置的值(<em class="mb">绿色矩阵</em>)。</p><p id="a22d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">使用<em class="mb"> d1 &gt; 1 </em>时，每个通道发生相同的情况，然后将它们相加，再与相应滤波器的偏置相加，这就形成了输出矩阵相应位置的值。让我们想象一下！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/39a98fbf0d99e8bdc7696238858a2db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*QgiVWSD6GscHh9nt55EfXg.gif"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 5. </strong><a class="ae kv" href="http://cs231n.github.io/assets/conv-demo/index.html" rel="noopener ugc nofollow" target="_blank"><strong class="bd kw">An example of how Inputs are mapped to Outputs</strong></a></figcaption></figure><p id="a0ba" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这是构成输出层的<em class="mb"> d2 </em>矩阵之一。</p><p id="81ef" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对所有的<em class="mb"> d2 </em>内核重复整个过程，这些内核形成输出层中的<em class="mb"> d2 </em>通道。</p><h1 id="aa34" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">汇集层</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/e2a50bb987e445a5b539cac20f503e59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wTu-73e3QjibbAplN4DIQA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 6. Max Pooling Layer</strong></figcaption></figure><p id="0e49" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">共有两种类型的池:<br/> 1)最大池<br/> 2)平均池</p><p id="0c0b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">汇集层的主要目的是减少输入张量的参数数量，从而<br/> -有助于减少过拟合<br/> -从输入张量中提取代表性特征<br/> -减少计算，从而提高效率</p><p id="acd9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">池层的输入是张量。</p><p id="c728" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在 Max Pooling 的情况下，其示例在图 6 的<em class="mb">中示出，</em>大小为<code class="fe my mz na nb b">n*n</code> <em class="mb">(在上述示例中为 2×2)</em>的核在矩阵中移动，并且对于每个位置，取<strong class="kz ir">最大值</strong>并放入输出矩阵的相应位置。</p><p id="4e1c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在平均汇集<em class="mb">、</em>的情况下，大小为<code class="fe my mz na nb b">n*n</code>的核在矩阵中移动，并且对于每个位置，对所有值取<strong class="kz ir">平均值，并放入输出矩阵的相应位置。</strong></p><p id="9548" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对输入张量中的每个通道重复这一过程。所以我们得到了输出张量。<br/>所以，要注意的一点是，<strong class="kz ir">池化在图像的高度和宽度上对图像进行缩减采样，但是通道的数量(深度)保持不变。</strong></p><h1 id="cbb5" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">全连接层</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/2f06f34df79a570b155cdd46ab17ce30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*au63ByDxkZYaRZpLdcfK1Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 4. Fully Connected Network</strong></figcaption></figure><p id="eb3b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">全连接层简单地说就是，<a class="ae kv" href="https://en.wikipedia.org/wiki/Feedforward_neural_network" rel="noopener ugc nofollow" target="_blank"><em class="mb"/></a><em class="mb">前馈神经网络。</em>完全连接的层形成网络中的最后几层。</p><p id="e1dc" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">到全连接层的<strong class="kz ir">输入</strong>是来自<em class="mb">最终</em>池化或卷积层的输出，它被<strong class="kz ir"> <em class="mb">展平</em> </strong>然后馈入全连接层。</p><blockquote class="nd ne nf"><p id="1344" class="kx ky mb kz b la lb jr lc ld le ju lf ng lh li lj nh ll lm ln ni lp lq lr ls ij bi translated">变平了？<br/>最终(和任何)池和卷积层的输出是一个三维矩阵，展平就是将其所有值展开成一个向量。<br/>我们来形象化一下！</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/7dc8ea42ae93c6b9f136ca9b74b8d85a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xY34n_U-IQdCXTcd2zXYQw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 5. Flattening</strong></figcaption></figure><p id="26b2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这个展平的向量然后连接到几个完全连接的层，这些层与人工神经网络相同，执行相同的数学运算！</p><p id="ef76" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对于人工神经网络的每一层，进行以下计算</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/54e29b00a99ab78814cc60c314158715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*c0Yr3t470_Kpphl5a7VTWw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 6. ANN Calculation for each layer</strong></figcaption></figure><p id="d0d6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">其中，<br/> <strong class="kz ir"> x </strong> — <em class="mb">为输入向量，维数为</em><strong class="kz ir"><em class="mb">【p _ l，1】</em></strong><br/><strong class="kz ir">W</strong>—<em class="mb">为权重矩阵，维数为</em><strong class="kz ir"><em class="mb">【p _ l，n _ l】</em></strong><em class="mb">其中，</em><strong class="kz ir"><em class="mb">p _ l</em></strong><em class="mb">为上一层神经元个数<br/> </em> <strong class="kz ir"> b </strong> <em class="mb"> —是维度为</em><strong class="kz ir"><em class="mb">【p _ l，1】</em></strong><em class="mb"><br/></em><strong class="kz ir">g</strong><em class="mb">—是激活函数，通常是</em><strong class="kz ir"><em class="mb">ReLU</em></strong><em class="mb">。</em></p><p id="b949" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对每一层重复这种计算。</p><p id="d66e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在通过完全连接的层之后，最终层使用<strong class="kz ir"><em class="mb">【soft max】激活函数</em> </strong> <em class="mb">(而不是 ReLU) </em>，该函数用于获得输入在特定类中的概率(<em class="mb">分类</em>)。</p><p id="1da4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">最后，我们得到了图像中物体属于不同类别的概率！！</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="81d0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这就是卷积神经网络的工作原理！！<br/>输入图像被归类为标签！！</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="a1dd" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在，让我们想象如何从输入张量计算输出张量的维数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/a29b0c91121c7505bfec35591c523dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9rUwBI56GsKnHJTTjRj9tg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kw">Fig 7. Output Dimension Calculations from Input Dimensions</strong></figcaption></figure><p id="68c7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">其中，<br/><strong class="kz ir"><em class="mb">W1</em></strong>—<em class="mb">是输入张量<br/></em><strong class="kz ir"><em class="mb">F</em></strong>—<em class="mb">是内核<br/></em><strong class="kz ir"><em class="mb">P</em></strong><em class="mb">—是填充<br/> </em> <strong class="kz ir"> <em class="mb"/></strong></p><blockquote class="nd ne nf"><p id="6a76" class="kx ky mb kz b la lb jr lc ld le ju lf ng lh li lj nh ll lm ln ni lp lq lr ls ij bi translated"><strong class="kz ir">垫高？通常，输入矩阵用零填充，这样内核可以在矩阵上均匀移动，结果矩阵具有期望的维数。因此，P=1 意味着输入矩阵的所有边都用 1 层零填充，如图 4 所示，输入(蓝色)矩阵周围有虚线。</strong></p><p id="4e27" class="kx ky mb kz b la lb jr lc ld le ju lf ng lh li lj nh ll lm ln ni lp lq lr ls ij bi translated"><strong class="kz ir">跨步？</strong> <br/>内核在矩阵上一次移动一个像素(图 4)。因此，这被称为具有 1 的步幅。我们可以将步距增加到 2，这样内核在矩阵上一次移动 2 个像素。这将反过来影响输出张量的维度，并有助于减少过拟合。</p></blockquote><p id="6cf8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> <em class="mb">输出张量的通道数会发生什么变化？</em> </strong> <br/>嗯，在卷积层的情况下，等于内核的数量。<br/>在合并层的情况下，输入张量和输出张量中的通道保持相同！</p></div></div>    
</body>
</html>