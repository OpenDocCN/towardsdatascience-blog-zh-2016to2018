<html>
<head>
<title>Text Summarization with Amazon Reviews</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带有亚马逊评论的文本摘要</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-summarization-with-amazon-reviews-41801c2210b?source=collection_archive---------0-----------------------#2017-05-09">https://towardsdatascience.com/text-summarization-with-amazon-reviews-41801c2210b?source=collection_archive---------0-----------------------#2017-05-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/816c31be666ee279af402251f056c6a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*fe0C7wegpFdNptuYBkQzNQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">With many products comes many reviews for training.</figcaption></figure><p id="ce38" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本文中，我们将使用亚马逊的美食评论来构建一个可以总结文本的模型。具体来说，我们将使用评论的描述作为我们的输入数据，使用评论的标题作为我们的目标数据。要下载数据集并了解更多信息，您可以在<a class="ae kw" href="https://www.kaggle.com/snap/amazon-fine-food-reviews" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上找到它。如果你决定建立一个像我这样的模型，你会发现它能够生成一些非常好的摘要:</p><p id="586b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kx">描述(1):咖啡味道好极了，价格也这么便宜！我强烈推荐这个给大家！</em></p><p id="8d9a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kx">总结(1):大咖</em></p><p id="8468" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kx">描述(2):这是我买过的最难吃的奶酪！我再也不会买它了，希望你也不要买！</em></p><p id="bdee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kx">汇总(二):omg毛总额</em></p><p id="ac28" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">代码是用Python写的，TensorFlow 1.1将是我们的深度学习库。如果您还没有使用TensorFlow 1.1，您将会看到构建seq2seq模型与以前的版本有很大的不同。为了帮助生成一些很棒的摘要，我们将在编码层使用双向RNN，在解码层使用注意力。我们将建立的模型类似于潘新和刘泳的模型，来自“用于文本摘要的具有注意力的序列到序列模型”(<a class="ae kw" href="https://github.com/tensorflow/models/tree/master/textsum" rel="noopener ugc nofollow" target="_blank"> GitHub </a>)。此外，相当多的信用应该给予Jaemin Cho教程(<a class="ae kw" href="https://github.com/j-min/tf_tutorial_plus/tree/master/RNN_seq2seq/contrib_seq2seq" rel="noopener ugc nofollow" target="_blank"> GitHub </a>)。这是我使用TensorFlow 1.1的第一个项目，他的教程确实帮助我将代码整理得井井有条。</p><p id="0519" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kx">注意:和我的其他文章一样，我将只展示我作品的主要部分，并且我将删除大部分的评论以保持简短。你可以在我的</em><a class="ae kw" href="https://github.com/Currie32/Text-Summarization-with-Amazon-Reviews" rel="noopener ugc nofollow" target="_blank"><em class="kx">Github</em></a><em class="kx">上看到整个项目。</em></p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><h1 id="24b0" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">准备数据</h1><p id="3506" class="pw-post-body-paragraph jy jz iq ka b kb md kd ke kf me kh ki kj mf kl km kn mg kp kq kr mh kt ku kv ij bi translated">首先，我将解释我是如何清理文本的:</p><ul class=""><li id="aaf6" class="mi mj iq ka b kb kc kf kg kj mk kn ml kr mm kv mn mo mp mq bi translated">转换成小写。</li><li id="5304" class="mi mj iq ka b kb mr kf ms kj mt kn mu kr mv kv mn mo mp mq bi translated">用更长的形式代替收缩。</li><li id="60fb" class="mi mj iq ka b kb mr kf ms kj mt kn mu kr mv kv mn mo mp mq bi translated">删除任何不需要的字符(这一步需要在替换缩写后完成，因为省略号将被删除。注意连字符前的反斜杠。如果没有这个斜杠，所有在连字符前后的字符之间的字符都将被删除。这可能会产生一些不必要的效果。举个例子，输入“a-d”会去掉a，b，c，d)。</li><li id="79cb" class="mi mj iq ka b kb mr kf ms kj mt kn mu kr mv kv mn mo mp mq bi translated">停用词只会从描述中删除。它们与训练模型不太相关，因此通过移除它们，我们能够更快地训练模型，因为数据更少。它们将保留在摘要中，因为它们相当短，我希望它们听起来更像自然短语。</li></ul><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="10be" class="nf lg iq nb b gy ng nh l ni nj">def clean_text(text, remove_stopwords = True):<br/>    <br/>    # Convert words to lower case<br/>    text = text.lower()<br/>    <br/>    # Replace contractions with their longer forms <br/>    if True:<br/>        text = text.split()<br/>        new_text = []<br/>        for word in text:<br/>            if word in contractions:<br/>                new_text.append(contractions[word])<br/>            else:<br/>                new_text.append(word)<br/>        text = " ".join(new_text)<br/>    <br/>    # Format words and remove unwanted characters<br/>    text = re.sub(r'https?:\/\/.*[\r\n]*', '', text,  <br/>                  flags=re.MULTILINE)<br/>    text = re.sub(r'\&lt;a href', ' ', text)<br/>    text = re.sub(r'&amp;amp;', '', text) <br/>    text = re.sub(r'[_"\-;%()|+&amp;=*%.,!?:#$@\[\]/]', ' ', text)<br/>    text = re.sub(r'&lt;br /&gt;', ' ', text)<br/>    text = re.sub(r'\'', ' ', text)<br/>    <br/>    # Optionally, remove stop words<br/>    if remove_stopwords:<br/>        text = text.split()<br/>        stops = set(stopwords.words("english"))<br/>        text = [w for w in text if not w in stops]<br/>        text = " ".join(text)</span><span id="2805" class="nf lg iq nb b gy nk nh l ni nj">return text</span></pre><p id="b0b8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将使用预先训练的词向量来帮助提高我们的模型的性能。过去，我曾为此使用过GloVe，但我发现了另一组单词嵌入，名为<a class="ae kw" href="https://github.com/commonsense/conceptnet-numberbatch" rel="noopener ugc nofollow" target="_blank">concept net number batch</a>(CN)。基于其创造者的工作，它似乎优于GloVe，这是有道理的，因为CN是包括GloVe在内的嵌入的集合。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="9a54" class="nf lg iq nb b gy ng nh l ni nj">embeddings_index = {}<br/>with open('/Users/Dave/Desktop/Programming/numberbatch-en-17.02.txt', encoding='utf-8') as f:<br/>    for line in f:<br/>        values = line.split(' ')<br/>        word = values[0]<br/>        embedding = np.asarray(values[1:], dtype='float32')<br/>        embeddings_index[word] = embedding</span></pre><p id="42fa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将把我们的词汇限制在CN中或者在我们的数据集中出现超过20次的单词。这将允许我们对每个单词都有非常好的嵌入，因为模型可以更好地理解单词在它们出现更多次时是如何相关的。</p><p id="a16f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在构建您的<code class="fe nl nm nn nb b">word_embedding_matrix</code>时，将它的“dtype”NP . zeros设置为float32是非常重要的。默认值是float64，但这不适用于TensorFlow，因为它希望值限制为_32。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="1718" class="nf lg iq nb b gy ng nh l ni nj">embedding_dim = 300<br/>nb_words = len(vocab_to_int)</span><span id="e9b0" class="nf lg iq nb b gy nk nh l ni nj">word_embedding_matrix = np.zeros((nb_words, embedding_dim), <br/>                                 dtype=np.float32)<br/>for word, i in vocab_to_int.items():<br/>    if word in embeddings_index:<br/>        word_embedding_matrix[i] = embeddings_index[word]<br/>    else:<br/>        # If word not in CN, create a random embedding for it<br/>        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))<br/>        embeddings_index[word] = new_embedding<br/>        word_embedding_matrix[i] = new_embedding</span></pre><p id="ca76" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了帮助更快地训练模型，我们将根据描述的长度从最短到最长对评论进行排序。这将有助于每一批具有相似长度的描述，这将导致更少的填充，从而更少的计算。我们可以根据摘要的长度进行二次排序，但是这将导致大量的循环来组织数据。另外，我怀疑它会减少多少额外的填充，因为摘要相当短。</p><p id="4a09" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于描述或摘要中的UNK令牌的数量，一些评论将不包括在内。如果描述中有超过1个UNK或摘要中有任何UNKs，则不会使用该评论。这样做是为了确保我们用有意义的数据构建模型。只有不到0.7%的词是UNKs，所以不会有很多评论被删除。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="b1de" class="nf lg iq nb b gy ng nh l ni nj">for length in range(min(lengths_texts.counts), max_text_length): <br/>   for count, words in enumerate(int_summaries):<br/>      if (len(int_summaries[count]) &gt;= min_length and<br/>          len(int_summaries[count]) &lt;= max_summary_length and<br/>          len(int_texts[count]) &gt;= min_length and<br/>          unk_counter(int_summaries[count]) &lt;= unk_summary_limit and<br/>          unk_counter(int_texts[count]) &lt;= unk_text_limit and<br/>          length == len(int_texts[count])<br/>         ):<br/>          sorted_summaries.append(int_summaries[count])<br/>          sorted_texts.append(int_texts[count])</span></pre></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><h1 id="c1f7" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">构建模型</h1><p id="9571" class="pw-post-body-paragraph jy jz iq ka b kb md kd ke kf me kh ki kj mf kl km kn mg kp kq kr mh kt ku kv ij bi translated">我们需要为这个模型做一些占位符。大多数都是不言自明的，但是有几个需要明确的是，<code class="fe nl nm nn nb b">summary_length</code>和<code class="fe nl nm nn nb b">text_length</code>是一个批处理中每个句子的长度，<code class="fe nl nm nn nb b">max_summary_length</code>是一个批处理中摘要的最大长度。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="929e" class="nf lg iq nb b gy ng nh l ni nj">def model_inputs():<br/>    <br/>    input_data = tf.placeholder(tf.int32,[None, None], name='input')<br/>    targets = tf.placeholder(tf.int32, [None, None], name='targets')<br/>    lr = tf.placeholder(tf.float32, name='learning_rate')<br/>    keep_prob = tf.placeholder(tf.float32, name='keep_prob')<br/>    summary_length = tf.placeholder(tf.int32, (None,), <br/>                                    name='summary_length')<br/>    max_summary_length = tf.reduce_max(summary_length, <br/>                                       name='max_dec_len')<br/>    text_length = tf.placeholder(tf.int32, (None,),       <br/>                                 name='text_length')</span><span id="1362" class="nf lg iq nb b gy nk nh l ni nj">return input_data, targets, lr, keep_prob, summary_length, <br/>       max_summary_length, text_length</span></pre><p id="7f2d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了构建我们的编码层，我们将使用LSTMs的双向RNN。如果您使用过TensorFlow的早期版本，您会注意到这种布局与通常创建两个层的方式不同。通常，只需将两个LSTMs包装在tf.contrib.rnn.MultiRNNCell中，但这种方法不再可行。这是我发现的一个方法，尽管代码更多，但也同样有效。</p><p id="410f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一些需要注意的事项:</p><ul class=""><li id="3d80" class="mi mj iq ka b kb kc kf kg kj mk kn ml kr mm kv mn mo mp mq bi translated">你需要使用<code class="fe nl nm nn nb b">tf.variable_scope</code>，这样你的变量可以在每一层重用。如果你真的不知道我在说什么，那就去看看TensorFlow的<a class="ae kw" href="https://www.tensorflow.org/tutorials/word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec教程</a>。</li><li id="31b7" class="mi mj iq ka b kb mr kf ms kj mt kn mu kr mv kv mn mo mp mq bi translated">因为我们使用的是双向RNN，所以需要将输出连接起来</li><li id="a048" class="mi mj iq ka b kb mr kf ms kj mt kn mu kr mv kv mn mo mp mq bi translated">初始化器与潘和刘模型中的初始化器相同。</li></ul><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="1037" class="nf lg iq nb b gy ng nh l ni nj">def encoding_layer(rnn_size, sequence_length, num_layers, <br/>                   rnn_inputs, keep_prob):<br/>    <br/>    for layer in range(num_layers):<br/>        with tf.variable_scope('encoder_{}'.format(layer)):<br/>            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,                                             <br/>                  initializer=tf.random_uniform_initializer(-0.1, <br/>                                                            0.1,       <br/>                                                            seed=2))<br/>            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, <br/>                                      input_keep_prob = keep_prob)</span><span id="3f6c" class="nf lg iq nb b gy nk nh l ni nj">            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,    <br/>                  initializer=tf.random_uniform_initializer(-0.1,      <br/>                                                            0.1, <br/>                                                            seed=2))<br/>            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, <br/>                                      input_keep_prob = keep_prob)</span><span id="66d5" class="nf lg iq nb b gy nk nh l ni nj">            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn( <br/>                                       cell_fw, <br/>                                       cell_bw,                     <br/>                                       rnn_inputs,<br/>                                       sequence_length,<br/>                                       dtype=tf.float32)<br/><br/>    enc_output = tf.concat(enc_output,2)<br/>    <br/>    return enc_output, enc_state</span></pre><p id="35ce" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了创建我们的训练和推理解码层，TF 1.1中有一些新的函数..为你分解事情:</p><ul class=""><li id="b9bc" class="mi mj iq ka b kb kc kf kg kj mk kn ml kr mm kv mn mo mp mq bi translated"><code class="fe nl nm nn nb b">TrainingHelper</code>从编码层读取整数序列。</li><li id="4023" class="mi mj iq ka b kb mr kf ms kj mt kn mu kr mv kv mn mo mp mq bi translated"><code class="fe nl nm nn nb b">BasicDecoder</code>用解码单元和输出层处理序列，输出层是全连接层。<code class="fe nl nm nn nb b">initial_state</code>来自我们的<code class="fe nl nm nn nb b">DynamicAttentionWrapperState</code>，你很快就会看到。</li><li id="943e" class="mi mj iq ka b kb mr kf ms kj mt kn mu kr mv kv mn mo mp mq bi translated"><code class="fe nl nm nn nb b">dynamic_decode</code>创建将用于培训的输出。</li></ul><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="952c" class="nf lg iq nb b gy ng nh l ni nj">def training_decoding_layer(dec_embed_input, summary_length, <br/>                            dec_cell, initial_state, output_layer, <br/>                            vocab_size, max_summary_length):<br/><br/>    training_helper = tf.contrib.seq2seq.TrainingHelper( <br/>                         inputs=dec_embed_input,<br/>                         sequence_length=summary_length,<br/>                         time_major=False)</span><span id="77dc" class="nf lg iq nb b gy nk nh l ni nj">    training_decoder = tf.contrib.seq2seq.BasicDecoder(<br/>                          dec_cell,<br/>                          training_helper,<br/>                          initial_state,<br/>                          output_layer)</span><span id="28aa" class="nf lg iq nb b gy nk nh l ni nj">    training_logits, _ = tf.contrib.seq2seq.dynamic_decode(<br/>                            training_decoder,<br/>                            output_time_major=False,<br/>                            impute_finished=True,<br/>                            maximum_iterations=max_summary_length)<br/>    return training_logits</span></pre><p id="f9b0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如你所见，这与训练层非常相似。主要区别是<code class="fe nl nm nn nb b">GreedyEmbeddingHelper</code>，它使用输出的argmax(被视为logits)并通过一个嵌入层传递结果以获得下一个输入。虽然是要求<code class="fe nl nm nn nb b">start_tokens</code>，但是我们只有一个，<code class="fe nl nm nn nb b">&lt;GO&gt;</code>。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="0382" class="nf lg iq nb b gy ng nh l ni nj">def inference_decoding_layer(embeddings, start_token, end_token,        <br/>                             dec_cell, initial_state, output_layer,<br/>                             max_summary_length, batch_size):<br/><br/>    <br/>    start_tokens = tf.tile(tf.constant([start_token],  <br/>                                       dtype=tf.int32),  <br/>                                       [batch_size], <br/>                                       name='start_tokens')<br/>    <br/>    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(  <br/>                           embeddings,<br/>                           start_tokens,<br/>                           end_token)<br/>                <br/>    inference_decoder = tf.contrib.seq2seq.BasicDecoder(<br/>                            dec_cell,<br/>                            inference_helper,<br/>                            initial_state,<br/>                            output_layer)<br/>                <br/>    inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(<br/>                              inference_decoder,<br/>                              output_time_major=False,<br/>                              impute_finished=True,<br/>                              maximum_iterations=max_summary_length)<br/>    <br/>    return inference_logits</span></pre><p id="bfb5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">虽然解码层可能看起来有点复杂，但它可以分为三个部分:解码细胞、注意力和获取我们的逻辑。</p><p id="b466" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">解码单元格:</p><ul class=""><li id="aa9a" class="mi mj iq ka b kb kc kf kg kj mk kn ml kr mm kv mn mo mp mq bi translated">只是一个辍学的双层LSTM。</li></ul><p id="b4b3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">注意:</p><ul class=""><li id="89a0" class="mi mj iq ka b kb kc kf kg kj mk kn ml kr mm kv mn mo mp mq bi translated">我用Bhadanau代替Luong作为我的注意力风格。使用它有助于模型更快地训练，并可以产生更好的结果(这里有一篇<a class="ae kw" href="https://arxiv.org/abs/1703.03906v2" rel="noopener ugc nofollow" target="_blank">好论文</a>，它比较了这两者以及seq2seq模型的许多方面)。</li><li id="18af" class="mi mj iq ka b kb mr kf ms kj mt kn mu kr mv kv mn mo mp mq bi translated"><code class="fe nl nm nn nb b">DynamicAttentionWrapper</code>将注意力机制应用于我们的解码单元。</li><li id="42bb" class="mi mj iq ka b kb mr kf ms kj mt kn mu kr mv kv mn mo mp mq bi translated"><code class="fe nl nm nn nb b">DynamicAttentionWrapperState</code>创建我们用于训练和推理层的初始状态。由于我们在解码层使用双向RNN，我们只能使用前向或后向状态。我选择向前，因为这是潘和刘选择的状态。</li></ul><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="b7d3" class="nf lg iq nb b gy ng nh l ni nj">def decoding_layer(dec_embed_input, embeddings, enc_output,  <br/>                   enc_state, vocab_size, text_length, <br/>                   summary_length, max_summary_length, rnn_size, <br/>                   vocab_to_int, keep_prob, batch_size, num_layers):<br/><br/>    for layer in range(num_layers):<br/>        with tf.variable_scope('decoder_{}'.format(layer)):<br/>            lstm = tf.contrib.rnn.LSTMCell(rnn_size,<br/>                  initializer=tf.random_uniform_initializer(-0.1, <br/>                                                            0.1,     <br/>                                                            seed=2))<br/>            dec_cell = tf.contrib.rnn.DropoutWrapper(<br/>                           lstm, <br/>                           input_keep_prob = keep_prob)<br/>    <br/>    output_layer = Dense(vocab_size,<br/>           kernel_initializer = tf.truncated_normal_initializer(  <br/>                                    mean=0.0, <br/>                                    stddev=0.1))<br/>    <br/>    attn_mech = tf.contrib.seq2seq.BahdanauAttention(<br/>                      rnn_size,<br/>                      enc_output,<br/>                      text_length,<br/>                      normalize=False,<br/>                      name='BahdanauAttention')</span><span id="87b0" class="nf lg iq nb b gy nk nh l ni nj">    dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell,<br/>                                                          attn_mech,<br/>                                                          rnn_size)<br/>            <br/>    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(<br/>                        enc_state[0],<br/>                        _zero_state_tensors(rnn_size, <br/>                                            batch_size, <br/>                                            tf.float32)) <br/>    <br/>    with tf.variable_scope("decode"):<br/>        training_logits = training_decoding_layer(<br/>                              dec_embed_input, <br/>                              summary_length, <br/>                              dec_cell, <br/>                              initial_state,<br/>                              output_layer,<br/>                              vocab_size, <br/>                              max_summary_length)<br/>    <br/>    with tf.variable_scope("decode", reuse=True):<br/>        inference_logits = inference_decoding_layer(<br/>                              embeddings,  <br/>                              vocab_to_int['&lt;GO&gt;'], <br/>                              vocab_to_int['&lt;EOS&gt;'],<br/>                              dec_cell, <br/>                              initial_state, <br/>                              output_layer,<br/>                              max_summary_length,<br/>                              batch_size)</span><span id="b24c" class="nf lg iq nb b gy nk nh l ni nj">    return training_logits, inference_logits</span></pre><p id="70aa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将使用我们之前创建的<code class="fe nl nm nn nb b">word_embedding_matrix</code>作为我们的嵌入。编码和解码序列都将使用这些嵌入。</p><p id="2b9e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个函数的其余部分主要是收集前面函数的输出，以便它们可以用于训练模型和生成新的摘要。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="2854" class="nf lg iq nb b gy ng nh l ni nj">def seq2seq_model(input_data, target_data, keep_prob, text_length,  <br/>                  summary_length, max_summary_length, <br/>                  vocab_size, rnn_size, num_layers, vocab_to_int,  <br/>                  batch_size):<br/>    <br/>    embeddings = word_embedding_matrix<br/>    <br/>    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)<br/>    enc_output, enc_state = encoding_layer(rnn_size, <br/>                                           text_length, <br/>                                           num_layers,  <br/>                                           enc_embed_input,  <br/>                                           keep_prob)<br/>    <br/>    dec_input = process_encoding_input(target_data, <br/>                                       vocab_to_int, <br/>                                       batch_size)<br/>    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)<br/>    <br/>    training_logits, inference_logits  = decoding_layer(<br/>         dec_embed_input, <br/>         embeddings,<br/>         enc_output,<br/>         enc_state, <br/>         vocab_size, <br/>         text_length, <br/>         summary_length, <br/>         max_summary_length,<br/>         rnn_size, <br/>         vocab_to_int, <br/>         keep_prob, <br/>         batch_size,<br/>         num_layers)<br/>    <br/>    return training_logits, inference_logits</span></pre><p id="42a8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">创建批处理是非常典型的。这里我唯一想指出的是创建了<code class="fe nl nm nn nb b">pad_summaries_lengths</code>和<code class="fe nl nm nn nb b">pad_texts_lengths</code>。这些包含一批内的摘要/文本的长度，并将用作输入值:<code class="fe nl nm nn nb b">summary_length</code>和<code class="fe nl nm nn nb b">text_length</code>。我知道寻找这些输入值似乎是一种奇怪的方法，但这是我能找到的最好的/唯一的解决方案。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="7f0e" class="nf lg iq nb b gy ng nh l ni nj">def get_batches(summaries, texts, batch_size):</span><span id="a93e" class="nf lg iq nb b gy nk nh l ni nj">    for batch_i in range(0, len(texts)//batch_size):<br/>        start_i = batch_i * batch_size<br/>        summaries_batch = summaries[start_i:start_i + batch_size]<br/>        texts_batch = texts[start_i:start_i + batch_size]<br/>        pad_summaries_batch = np.array(pad_sentence_batch(  <br/>                                  summaries_batch))<br/>        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))<br/>        <br/>        pad_summaries_lengths = []<br/>        for summary in pad_summaries_batch:<br/>            pad_summaries_lengths.append(len(summary))<br/>        <br/>        pad_texts_lengths = []<br/>        for text in pad_texts_batch:<br/>            pad_texts_lengths.append(len(text))<br/>        <br/>        yield (pad_summaries_batch, <br/>               pad_texts_batch, <br/>               pad_summaries_lengths, <br/>               pad_texts_lengths)</span></pre><p id="dab0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这些是我用来训练这个模型的超参数。我不认为它们有什么太令人兴奋的，它们很标准。《100个纪元》可能已经引起了你的注意。我使用这个较大的值，以便我的模型得到充分的训练，并且只在早期停止(当损失停止减少时)时停止训练。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="3450" class="nf lg iq nb b gy ng nh l ni nj">epochs = 100<br/>batch_size = 64<br/>rnn_size = 256<br/>num_layers = 2<br/>learning_rate = 0.01<br/>keep_probability = 0.75</span></pre><p id="99b8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我将跳过构建图表和如何训练模型。有一些很好的东西，比如我如何合并学习率衰减和早期停止，但我认为不值得在这里占用空间(如果你感兴趣，请查看我的<a class="ae kw" href="https://github.com/Currie32/Text-Summarization-with-Amazon-Reviews" rel="noopener ugc nofollow" target="_blank"> GitHub </a>)。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><h1 id="8d8b" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">生成您自己的摘要</h1><p id="7c95" class="pw-post-body-paragraph jy jz iq ka b kb md kd ke kf me kh ki kj mf kl km kn mg kp kq kr mh kt ku kv ij bi translated">您可以创建自己的描述，也可以使用数据集中的描述作为输入数据。使用你自己的描述会更有趣一些，因为你可以很有创意地去看看模型创建了什么样的摘要。下面的函数将使用我之前描述的<code class="fe nl nm nn nb b">clean_text</code>函数为您的模型准备描述。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="1748" class="nf lg iq nb b gy ng nh l ni nj">def text_to_seq(text):</span><span id="ecf3" class="nf lg iq nb b gy nk nh l ni nj">    text = clean_text(text)<br/>    return [vocab_to_int.get(word, vocab_to_int['&lt;UNK&gt;']) for word in text.split()]</span></pre><p id="f2ed" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们需要加载相当多的张量来生成新的摘要。尽管如此，这并不太难。运行会话时，<code class="fe nl nm nn nb b">input_data</code>和<code class="fe nl nm nn nb b">text_length</code>需要乘以<code class="fe nl nm nn nb b">batch_size</code>以匹配模型的输入参数。您可以将<code class="fe nl nm nn nb b">summary_length</code>设置为您喜欢的任何值，但是我决定将其设置为随机的，以保持事情的精彩。要记住的主要事情是将其保持在模型训练的摘要长度范围内，即2–13。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="bc36" class="nf lg iq nb b gy ng nh l ni nj">input_sentence = "<em class="kx">This is the worst cheese that I have ever bought! I will never buy it again and I hope you won’t either!</em>"<br/>text = text_to_seq(input_sentence)</span><span id="f880" class="nf lg iq nb b gy nk nh l ni nj">#random = np.random.randint(0,len(clean_texts))<br/>#input_sentence = clean_texts[random]<br/>#text = text_to_seq(clean_texts[random])</span><span id="8e3c" class="nf lg iq nb b gy nk nh l ni nj">checkpoint = "./best_model.ckpt"</span><span id="2a4d" class="nf lg iq nb b gy nk nh l ni nj">loaded_graph = tf.Graph()<br/>with tf.Session(graph=loaded_graph) as sess:<br/><br/>    loader = tf.train.import_meta_graph(checkpoint + '.meta')<br/>    loader.restore(sess, checkpoint)</span><span id="c33c" class="nf lg iq nb b gy nk nh l ni nj">    input_data = loaded_graph.get_tensor_by_name('input:0')<br/>    logits = loaded_graph.get_tensor_by_name('predictions:0')<br/>    text_length = loaded_graph.get_tensor_by_name('text_length:0')<br/>    summary_length =  <br/>         loaded_graph.get_tensor_by_name('summary_length:0')<br/>    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')<br/>    <br/>    answer_logits = sess.run(logits, {<br/>         input_data: [text]*batch_size, <br/>         summary_length: [np.random.randint(4,8)], <br/>         text_length: [len(text)]*batch_size,<br/>         keep_prob: 1.0})[0]</span><span id="0896" class="nf lg iq nb b gy nk nh l ni nj"># Remove the padding from the tweet<br/>pad = vocab_to_int["&lt;PAD&gt;"]</span><span id="e4dd" class="nf lg iq nb b gy nk nh l ni nj">print('\nOriginal Text:', input_sentence)</span><span id="85e1" class="nf lg iq nb b gy nk nh l ni nj">print('Text')<br/>print('Word Ids:    {}'.format([i for i in text if i != pad]))<br/>print('Input Words: {}'.format([int_to_vocab[i] for <br/>                                 i in text if i != pad]))</span><span id="c54c" class="nf lg iq nb b gy nk nh l ni nj">print('\nSummary')<br/>print('Word Ids: {}'.format([i for i in answer_logits if i != pad]))<br/>print('Response Words: {}'.format([int_to_vocab[i] for i in <br/>                                  answer_logits if i != pad]))</span></pre><p id="b66c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后要注意的一点是，我只用50，000条评论的子集来训练我的模型。由于我使用的是MacBook Pro，如果我使用所有的数据，我可能需要几天时间来训练这个模型。只要有可能，我就会使用FloydHub上的GPU来训练我的模型。如果您有时间，您可以将数据和ConceptNet Numberbatch上传到FloydHub，然后用所有评论训练模型。让我知道怎么走！</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="1e4e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个项目到此为止！我希望你觉得它很有趣，并且学到了一些东西。尽管这个模型执行得相当好，并且只有一个数据子集，但是尝试扩展这个体系结构来提高生成的摘要的质量是很好的。<a class="ae kw" href="https://www.aclweb.org/anthology/C/C16/C16-1229.pdf" rel="noopener ugc nofollow" target="_blank">这篇论文</a>同时使用了RNNs和CNN来预测情绪，也许在这里也可以。如果有人尝试这样做，如果你能在下面的评论区贴一个链接就太好了，因为我会很高兴看到它。</p><p id="8621" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">感谢您的阅读，如果您有任何问题或意见，请告诉我！干杯！</p></div></div>    
</body>
</html>