<html>
<head>
<title>Auto-Regressive Generative Models (PixelRNN, PixelCNN++)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自回归生成模型(PixelRNN，PixelCNN++)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173?source=collection_archive---------2-----------------------#2017-12-20">https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173?source=collection_archive---------2-----------------------#2017-12-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="091d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作者:<a class="kl km ep" href="https://medium.com/u/d55b19771e38?source=post_page-----32d192911173--------------------------------" rel="noopener" target="_blank">哈什夏尔马，</a>T2</p><p id="0f7b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">生成模型是无监督学习的子集，其中给定一些训练数据，我们从相同的分布中生成新的样本/数据。有两种方法来模拟这种分布，其中最有效和最流行的是自回归模型、自动编码器和GANs。</p><p id="8c12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">生成对手网络(GANs)和自回归模型之间的基本区别在于，GANs学习隐式数据分布，而后者学习由模型结构强加的先验支配的显式分布。发行版可以是任何东西，例如类别标签、汽车或猫的图像。更简单地说，先验意味着一个量的概率分布。</p><p id="9634" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自回归模型优于GANs的一些优点是:</p><p id="cc85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1.<strong class="jp ir">提供了一种计算可能性的方法</strong>:这些模型具有返回显式概率密度的优势(与GANs不同)，使得它可以直接应用于压缩、概率规划和探索等领域</p><p id="c3a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.<strong class="jp ir">训练比GAN更稳定</strong>:训练GAN需要找到纳什均衡。由于目前没有这样做的算法，与PixelRNN或PixelCNN相比，训练GAN是不稳定的。</p><p id="24c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.<strong class="jp ir">对离散和连续数据都有效</strong>:对于GAN来说生成离散数据很难，比如文本。</p><p id="b715" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">众所周知，GANs能产生更高质量的图像，训练速度也更快。人们正在努力将这两种类型的优势整合到一个单一的模型中，但这仍然是一个开放的研究领域。在这篇博客中，我们将只关注自回归部分，其他的留待以后讨论。</p><p id="b693" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">无监督学习中最重要和众所周知的问题之一是对自然图像的分布建模，这也是我们选择写这篇博客的原因。</p><p id="205a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了解决这个问题，我们需要一个易于处理和扩展的模型。PixelRNN、PixelCNN是满足这两个条件的自回归模型的一部分。</p><p id="14e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些类型的模型优选地用于图像补全。同样的原因是因为它在这类问题上比其他生成模型表现得更好。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi kn"><img src="../Images/1cadae8febbc3157bf9a920c39b7e632.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*o7PUa9K5rPGeFIcx."/></div></div></figure><h1 id="dc9e" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">像素RNN</h1><p id="76b8" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">对这种网络建模的有效方法是使用概率密度模型(如高斯或正态分布)来量化图像的像素，作为条件分布的产物。这种方法将建模问题转化为序列问题，其中下一个像素值由所有先前生成的像素值确定。</p><p id="f377" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了处理像素值和分布之间的非线性和长期依赖关系，我们需要一个像递归神经网络(RNN)这样的表达序列模型。rnn已经被证明在处理序列问题上非常有效。</p><p id="381a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">条件独立性</strong></p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/4f69312f0a58a385141a328ea4b8010a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*rkHKg19TkG4PA4Jn."/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">FIGURE 2 : Pixels of an nxn image</figcaption></figure><p id="c216" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">网络在每行中一次一个像素地扫描一行图像。随后，它预测可能的像素值的条件分布。图像像素的分布被写成条件分布的乘积，并且这些值在图像的所有像素之间共享。</p><p id="d8d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里的目的是给(n×n)图像的每个像素分配一个概率p(x)。这可以通过将像素xi的概率写成:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/b47b8d899f27b12f06263dd9d4b46f7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*zP9ArkB0vE9YDcrpQ1UvyA.png"/></div></figure><p id="38e6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是给定所有先前生成的像素的概率的第I个像素的概率。逐行逐像素地进行生成。此外，每个像素xi由所有三个颜色通道红色、绿色和蓝色(RGB)共同确定。第I个像素的条件概率变成:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/d084e9f4208b3063cf9152d73d71049d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*rIvwxZm-slYidb1uMt6X6g.png"/></div></figure><p id="5658" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，每种颜色都取决于其他颜色以及先前生成的像素。</p><p id="9b04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于我们现在可以知道我们的像素值的条件概率，为了获得适当的像素值，我们使用256路softmax层。该层的输出可以取0-255之间的任何值，即我们的像素值可以在0-255之间变化。</p><p id="e32f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">模型建筑:</strong></p><p id="10d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有四种不同的架构可供使用，即:</p><p id="f62d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">行LSTM、对角线BiLSTM、全卷积网络和多尺度网络。</p><p id="6e93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该网络由多达12层的二维LSTMs组成。使用的两种类型的LSTM层是，</p><ol class=""><li id="7a2f" class="mj mk iq jp b jq jr ju jv jy ml kc mm kg mn kk mo mp mq mr bi translated"><strong class="jp ir">行LSTM </strong>:第一层是使用a类掩码的7x7卷积，接着是使用B类掩码的3x1卷积的输入到状态层，以及未被掩码的3x1状态到状态卷积层。然后，特征映射通过由ReLU和b型掩码组成的几个1x1卷积层，该架构的最后一层是256路softmax层。</li><li id="8e78" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">对角线BiLSTM :其架构与Row LSTM的唯一区别在于状态输入层和状态层。它有一个到具有掩码类型B的状态层的1x1卷积输入和一个到没有掩码的状态层的1x2卷积输入。</li></ol><p id="6d0b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">排LSTM </strong></p><p id="c0c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">隐藏状态(I，j) =隐藏状态(i-1，j-1)+隐藏状态(i-1，j+1)+隐藏状态(i-1，j)+ p(i，j)</p><p id="45a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这从上到下逐行处理图像，同时计算整行的特征。它捕捉像素上方的一个相当三角形的区域。然而，它不能捕获整个可用区域。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/b65b85ec9d97bb86653d7ce9a1dc62cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/0*0huT_eYvMlXgP-c0."/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">FIGURE 3 : input-to-state and state-to-state mapping for Row LSTM</figcaption></figure><p id="958b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">对角线长度</strong></p><p id="331c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">pixel(i，j) = pixel(i，j-1) + pixel(i-1，j)。</p><p id="8e92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这一层的感受野包括整个可用区域。处理过程沿对角线进行。它从顶角开始，在向两个方向移动的同时到达对面的角。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi my"><img src="../Images/3b4a19d6218d245ca871a8db7403eca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/0*gAQ3lvKxpPV84grs."/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">FIGURE 4 : input-to-state and state-to-state mapping for Diagonal BiLSTM</figcaption></figure><p id="ea17" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这些网络中还使用剩余连接(或跳过连接)来提高收敛速度，并通过网络更直接地传播信号。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/cb3af1d10f273b2bba11e4ac92f3c60f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/0*qPCwWmLYsV88VZc7."/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">FIGURE 5 : Residual block for PixelRNNs. ‘h’ refers to the number of parameters.</figcaption></figure><p id="b0df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">蒙面卷积:</strong></p><p id="d087" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每层中每个输入位置的特征被分成三个部分，每个部分对应一种颜色(RGB)。为了计算G通道的值，我们需要R通道的值以及所有先前像素的值。类似地，B通道需要R和G通道的信息。为了限制网络遵守这些约束，我们对卷积应用掩码。</p><p id="fbae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用两种类型的面具:</p><ol class=""><li id="02ac" class="mj mk iq jp b jq jr ju jv jy ml kc mm kg mn kk mo mp mq mr bi translated"><strong class="jp ir">类型A </strong>:此遮罩仅应用于第一个卷积层，并限制连接到当前像素中已经预测的那些颜色。</li><li id="1d33" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">类型B :该蒙版应用于其他图层，并允许连接到当前像素中的预测颜色。</li></ol><p id="df96" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">掩码是网络的重要组成部分，它维护网络中的信道数量。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi na"><img src="../Images/5c2b35928bcd088e5ea8c704d74e859e.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*UbiGmm8uyZ-GKhv5."/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">FIGURE 6 : Connectivity inside a masked convolution</figcaption></figure><p id="1aa7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">损失函数和评估指标</strong></p><p id="49e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，负对数似然(NLL)用作损失和评估度量，因为网络从值0-255预测(分类)像素值。</p><h1 id="dc30" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">像素CNN</h1><p id="7a7b" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">PixelRNN的主要缺点是训练非常慢，因为每个状态都需要顺序计算。这可以通过使用卷积层和增加感受野来克服。PixelCNN使用标准卷积层来捕捉有界感受野，并同时计算所有像素位置的特征。它使用多个卷积层来保持空间分辨率。但是，没有使用池层。在卷积中采用掩码来限制模型违反条件依赖。</p><p id="7750" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一个图层是使用遮罩A的7x7卷积，其余图层使用3x3卷积和遮罩b。然后，要素地图经过由ReLU激活和1x1卷积组成的两个图层。该架构的最后一层是256路softmax层。</p><p id="2d7e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与PixelRNN相比，PixelCNN大大减少了训练时间。然而，图像生成仍然是顺序的，因为每个像素需要作为输入返回给网络以计算下一个像素。PixelCNN的主要缺点是它的性能比PixelRNN差。另一个缺点是在感受野中存在盲点。CNN对感受野的捕捉以三角形方式进行。它导致几个像素被排除在感受野之外，如下图所示。由于卷积网络捕捉有界感受域(不同于BiLSTM)并一次计算所有像素的特征，这些像素不依赖于所有先前的像素，这是不希望的。卷积层不能完全处理感受野，从而导致像素值的轻微误算。遗漏的像素构成了盲点。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/18aee39b7525c3cdd949eeb06375f1d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/0*_L6b_WvZqrANTD5B."/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">FIGURE 7 : Blind spot in a PixelCNN and its solution in Gated PixelCNN</figcaption></figure><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/3d476d9c78ed4f883feda5b2ab0043c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/0*yGJ9C1UEIQtarSV1."/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">FIGURE 8 : input-to-state and state-to-state mapping for PixelCNN</figcaption></figure><h1 id="feed" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">门控像素CNN</h1><p id="9d3f" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">它改进了PixelCNN的架构，同时匹配PixelRNN的对数似然性。</p><p id="98fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PixelRNN优于PixelCNN的基本原因是它使用了LSTM层。LSTMs的感受野包含网络中的所有相邻像素，同时它在PixelCNN中随着深度而增长。该方法中引入的主要改进或变化是使用以下激活代替ReLU:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/24ea8f8c6a22a0539ce7da08238a8805.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*r8IHR6lGe9ThNBCRfvUoYQ.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">σ is the sigmoid non-linearity, k is the number of the layer, ⊙ is the element-wise product and ∗ is the convolution operator.</figcaption></figure><p id="2696" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该模型的另一个主要改进是使用CNN通过使用堆栈来完全利用可用的感受野。PixelCNN中的感受野是有限的，这导致从条件分布的计算中遗漏了几个像素，从而产生了CNN不能处理的盲点。该模型使用两层堆栈来处理以消除上述问题:</p><ol class=""><li id="d5b6" class="mj mk iq jp b jq jr ju jv jy ml kc mm kg mn kk mo mp mq mr bi translated"><strong class="jp ir">水平堆栈</strong>:以当前行为条件，将前一层的输出和垂直堆栈的输出作为输入。</li><li id="a169" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated"><strong class="jp ir">垂直堆栈</strong>:以当前像素以上的所有行为条件。它没有任何掩蔽。它的输出被送入水平堆栈，感受野以矩形方式增长。</li></ol><p id="a7bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">结果表明，门控PixelCNN (3.03位/dim)比PixelCNN (3.14位/dim)高出0.11位/dim，其性能与PixelRNN (3.00位/dim)相当，而花费的训练时间不到一半。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi kn"><img src="../Images/e36c5da2a2e1e5c2f53b4fe6d98677fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jC_MCY-wXrfRTSyG."/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk">FIGURE 9 : A single layer in Gated PixelCNN architecture</figcaption></figure><h1 id="15e0" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">像素CNN ++</h1><p id="924d" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">OpenAI的这个模型做了一些修改，以提高PixelCNN的性能，同时保持其计算效率。值得注意的修改包括:</p><ol class=""><li id="95de" class="mj mk iq jp b jq jr ju jv jy ml kc mm kg mn kk mo mp mq mr bi translated"><strong class="jp ir">离散逻辑混合似然</strong>:soft max层，用于计算像素的条件分布，尽管效率在内存方面非常昂贵。此外，它使梯度稀疏在训练初期。为了应对这一点，我们假设一个潜在的颜色强度类似于在变化自动编码器中使用的，具有连续分布。它被舍入到最接近的8位表示，以给出像素值。强度的分布是逻辑的，因此可以容易地确定像素值。这种方法是内存有效的，输出是较低的维度，提供了更密集的梯度，从而解决了这两个问题。</li><li id="d3a0" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated"><strong class="jp ir">对整个像素进行调节</strong> : PixelCNN根据颜色(RGB)将模型分解为3个子像素，但这会使模型变得复杂。像素的颜色通道之间的依赖性相对简单，并且不需要深度模型来训练。因此，最好对整个像素而不是单独的颜色进行调节，然后在预测像素的所有3个通道上输出联合分布。</li><li id="2d85" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated"><strong class="jp ir">下采样</strong> : PixelCNN无法计算长程相关性。至于为什么比不上PixelRNN的性能，这也是PixelCNN的缺点之一。为了克服这一点，我们通过使用步长为2的卷积对层进行缩减采样。下采样减少了输入大小，从而提高了感受野的相对大小，这导致了一些信息损失，但可以通过添加额外的捷径连接来补偿。</li><li id="88e3" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated"><strong class="jp ir">快捷连接</strong>:模拟U-net的编解码结构。层2和3被下采样，然后层5和6被上采样。存在从编码器到解码器的剩余连接，以提供本地化信息。</li><li id="c9d6" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated"><strong class="jp ir"> Dropout </strong>:由于PixelCNN和PixelCNN++的模型都非常强大，如果不进行正则化，它们很可能会溢出数据。因此，我们在第一次卷积后对剩余路径应用下降。</li></ol><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi ne"><img src="../Images/4ab95db2f327baee4d52c257476f8d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vFy5avF1quHE4cX0."/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk">FIGURE 10 : Convolutional architecture with residual connections</figcaption></figure><p id="4349" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PixelCNN++的性能远远超过PixelRNN和PixelCNN。当在CIFAR-10上训练时，最佳测试对数似然是2.92比特/像素，相比之下，PixelRNN是3.0比特/像素，门控PixelCNN是3.03比特/像素。此外，如果没有使用任何一个修改，性能下降，模型学习缓慢或者在某些情况下不能完全学习。关于这些实验的更多细节在论文中给出。</p><p id="5104" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">结论</strong></p><p id="40ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实验表明，PixelCNN++可以用作变分自动编码器(VAE)中的解码器，这在正在进行的研究中具有应用，如可以在诸如PixelGAN的几篇研究论文中看到的。PixelCNN迫使编码器学习更高级别的特征，因为网络本身可以处理较低维度的特征。将VAEs、GANs和自回归模型一起使用是一个活跃的研究领域。这些模型以及强化技术可以改进最先进的技术，并引领无监督/半监督学习领域的研究，以匹配监督学习中正在进行的研究水平。</p><p id="e77f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">参考文献</strong></p><ol class=""><li id="b2a4" class="mj mk iq jp b jq jr ju jv jy ml kc mm kg mn kk mo mp mq mr bi translated"><a class="ae nf" href="https://allenlu2007.wordpress.com/2017/08/19/pixelrnn-vs-gan-for-probabilistic-generative-model/" rel="noopener ugc nofollow" target="_blank">https://allenlu 2007 . WordPress . com/2017/08/19/pixel rnn-vs-gan-for-probabilical-generative-model/</a></li><li id="4ec7" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">【http://sergeiturukin.com/2017/02/22/pixelcnn.html T4】</li><li id="7811" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">像素递归神经网络(范德奥尔德等人)2016(<a class="ae nf" href="https://arxiv.org/pdf/1601.06759.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1601.06759.pdf</a>)</li><li id="b622" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">使用PixelCNN解码器的条件图像生成(范德奥尔德等人)2016(<a class="ae nf" href="https://arxiv.org/pdf/1606.05328.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1606.05328.pdf</a>)</li><li id="b0c2" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">PixelCNN++用离散逻辑混合似然和其他修改改进PixelCNN。(萨利曼斯等人)2017。(<a class="ae nf" href="https://arxiv.org/pdf/1701.05517.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1701.05517.pdf</a>)</li><li id="0358" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated"><a class="ae nf" rel="noopener" target="_blank" href="/summary-of-pixelrnn-by-google-deepmind-7-min-read-938d9871d6d9">https://towards data science . com/summary-of-pixel rnn-by-Google-deep mind-7-min-read-938d 9871 D6 d 9</a></li><li id="0e4f" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated"><a class="ae nf" href="https://www.commonlounge.com/discussion/99e291af08e2427b9d961d41bb12c83b" rel="noopener ugc nofollow" target="_blank">https://www . common lounge . com/discussion/99 e 291 af 08 e 2427 b 9d 961d 41 bb 12 c 83 b</a></li><li id="f15e" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">极大极小博弈理论(<a class="ae nf" href="https://en.wikipedia.org/wiki/Minimax" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Minimax</a>)</li></ol><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi ng"><img src="../Images/c693394277ec69b04df310a83b734141.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XwjURC5g8qr-0IPND6TAjQ.png"/></div></div></figure><p id="e9ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">代码</strong></p><ol class=""><li id="4ec1" class="mj mk iq jp b jq jr ju jv jy ml kc mm kg mn kk mo mp mq mr bi translated">pixel cnn++:<a class="ae nf" href="https://github.com/openai/pixel-cnn" rel="noopener ugc nofollow" target="_blank">https://github.com/openai/pixel-cnn</a>(tensor flow)</li><li id="2a23" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">https://github.com/carpedm20/pixel-rnn-tensorflow(张量流)</li></ol></div></div>    
</body>
</html>