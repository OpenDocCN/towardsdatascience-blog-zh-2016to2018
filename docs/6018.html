<html>
<head>
<title>Review: YOLOv2 &amp; YOLO9000 — You Only Look Once (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习:YOLOv2 &amp; YOLO9000 —你只看一次(物体检测)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=collection_archive---------4-----------------------#2018-11-21">https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=collection_archive---------4-----------------------#2018-11-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3e68" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>这个故事里，<strong class="jp ir"> YOLOv2，你只看一次版本 2，</strong>是评论。YOLOv2 比 YOLOv1   有<strong class="jp ir">多项改进。YOLO9000 还被提议使用单词树</strong>检测超过 9000 个对象类别。</p><p id="058b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面(我相信)是作者非常著名的 YOLOv2 视频:</p><figure class="kw kx ky kz gt la"><div class="bz fp l di"><div class="lb lc l"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="ak">YOLOv2</strong></figcaption></figure><p id="9a9f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">在 67 FPS 时，YOLOv2 在 PASCAL VOC 2007 上获得了 76.8%的 mAP</strong>。<strong class="jp ir">40 FPS，YOLOv2 获得 78.6% mAP </strong>比<a class="ae ku" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202"> <em class="kv">更快 R-CNN </em> </a>使用<a class="ae ku" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><em class="kv">ResNet</em></a><em class="kv"/>和 SSD。取得如此好的成绩，YOLOv2 发表在<strong class="jp ir"> 2017 CVPR </strong>上，并获得超过<strong class="jp ir"> 1000 次引用</strong>。(<a class="lh li ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----7883d2b02a65--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/f46c8473c76bd8a70905904a68196f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*CHPbDpMh5oeQlxJt.gif"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd lm">YOLOv2</strong></figcaption></figure><p id="5879" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">论文题目是“<strong class="jp ir"> YOLO9000:更好更快更强</strong>”。<br/>那么，让我们看看怎样才能<strong class="jp ir">更好</strong>，<strong class="jp ir">更快</strong>，<strong class="jp ir">更强</strong>！！！</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="63eb" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">涵盖哪些内容</h1><ol class=""><li id="0422" class="ms mt iq jp b jq mu ju mv jy mw kc mx kg my kk mz na nb nc bi translated"><strong class="jp ir"> YOLOv2 比</strong><a class="ae ku" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89"><strong class="jp ir"><em class="kv">yolo v1</em></strong></a><strong class="jp ir">(更好)</strong></li><li id="de2a" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated"><strong class="jp ir"> YOLOv2 使用 Darknet-19(更快)</strong></li><li id="047c" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated"><strong class="jp ir"> YOLO9000 by WordTree(更强)</strong></li></ol></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="4059" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated"><strong class="ak"> 1。YOLOv2 的改进超过了</strong><a class="ae ku" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89"><strong class="ak"><em class="ni">yolo v1</em></strong></a><strong class="ak">(更好)</strong></h1><h2 id="7ebf" class="nj lv iq bd lw nk nl dn ma nm nn dp me jy no np mi kc nq nr mm kg ns nt mq nu bi translated">1.1.<a class="ae ku" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">批量归一化(BN) </a></h2><ul class=""><li id="b0a2" class="ms mt iq jp b jq mu ju mv jy mw kc mx kg my kk nv na nb nc bi translated"><a class="ae ku" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> <em class="kv"> BN </em> </a>用于 YOLOv2 中的所有卷积层。</li><li id="b491" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">地图提高 2%。</li></ul><h2 id="eac7" class="nj lv iq bd lw nk nl dn ma nm nn dp me jy no np mi kc nq nr mm kg ns nt mq nu bi translated">1.2.高分辨率分类器</h2><ul class=""><li id="e274" class="ms mt iq jp b jq mu ju mv jy mw kc mx kg my kk nv na nb nc bi translated">经过 224×224 图像训练后，YOLOv2 还使用 448×448 图像在 ImageNet 上对分类网络进行 10 个时期的微调。</li><li id="c892" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">地图增加 4%。</li></ul><h2 id="a298" class="nj lv iq bd lw nk nl dn ma nm nn dp me jy no np mi kc nq nr mm kg ns nt mq nu bi translated">1.3.带有锚盒的回旋</h2><ul class=""><li id="b9d2" class="ms mt iq jp b jq mu ju mv jy mw kc mx kg my kk nv na nb nc bi translated">YOLOv2 移除所有完全连接的层，并使用锚定框来预测边界框。</li><li id="6395" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">移除一个池层以提高输出的分辨率。</li><li id="9afb" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">现在使用 416×416 的图像来训练检测网络。</li><li id="93ab" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">并且获得 13×13 的特征图输出，即 32×下采样。</li><li id="0903" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">在没有锚盒的情况下，中间模型获得了 69.5%的 mAP 和 81%的召回率。</li><li id="6dbe" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">使用锚盒，获得了 69.2%的 mAP 和 88%的召回率。虽然地图略有下降，但召回率大幅上升。</li></ul><h2 id="eca5" class="nj lv iq bd lw nk nl dn ma nm nn dp me jy no np mi kc nq nr mm kg ns nt mq nu bi translated">1.4.维度群</h2><ul class=""><li id="f200" class="ms mt iq jp b jq mu ju mv jy mw kc mx kg my kk nv na nb nc bi translated">锚定框的大小和比例是预先定义的，没有获得任何事先信息，就像《更快的 R-CNN  中的<a class="ae ku" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202"> <em class="kv">一样。</em></a></li><li id="984b" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">使用基于标准欧几里德距离的 k-means 聚类不够好，因为较大的盒子比较小的盒子产生更多的误差</li><li id="b66d" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">YOLOv2 使用 k-means 聚类，可获得良好的 IOU 分数:</li></ul><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/c5025f3f52be3bdb3be34edbf6fc18d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*4UeShDFUuddbOOMAh7KTdg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd lm">Cluster IOU</strong></figcaption></figure><ul class=""><li id="14b2" class="ms mt iq jp b jq jr ju jv jy nx kc ny kg nz kk nv na nb nc bi translated">k = 5 是在模型复杂性和高召回率之间取得良好平衡的最佳值。</li></ul><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oa"><img src="../Images/aa639910c6f614fa737446ddb1fc3bce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7-FYvtgT3tebpIAXoiuilw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd lm">Different IOU Clustering Approaches with DIfferent Number of Anchor Boxes</strong></figcaption></figure><ul class=""><li id="b50f" class="ms mt iq jp b jq jr ju jv jy nx kc ny kg nz kk nv na nb nc bi translated">具有 5 个锚框的基于 IOU 的聚类(61.0%)具有与具有 9 个锚框的<a class="ae ku" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202"> <em class="kv">更快 R-CNN </em> </a>中的相似的结果(60.9%)。</li><li id="d526" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">基于 IOU 的 9 锚盒聚类得到 67.2%。</li></ul><h2 id="cce6" class="nj lv iq bd lw nk nl dn ma nm nn dp me jy no np mi kc nq nr mm kg ns nt mq nu bi translated">1.5.直接位置预测</h2><ul class=""><li id="2bfe" class="ms mt iq jp b jq mu ju mv jy mw kc mx kg my kk nv na nb nc bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89"> <em class="kv"> YOLOv1 </em> </a>对位置预测没有约束，这使得模型在早期迭代时不稳定。预测的边界框可能远离原始网格位置。</li><li id="a958" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">YOLOv2 使用逻辑激活σ来限定位置，这使得值落在 0 到 1 之间:</li></ul><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi of"><img src="../Images/0bc54f142fc2c5ea8eefbebd532bef70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r_LqJNwlbapcFbSc8lytRg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd lm">Constrained Bounding Box Prediction</strong></figcaption></figure><ul class=""><li id="8ac3" class="ms mt iq jp b jq jr ju jv jy nx kc ny kg nz kk nv na nb nc bi translated">(cx，cy)是网格的位置。</li><li id="6f1e" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">(bx，by)是包围盒的位置:(cx，cy) +由σ(tx)和σ(ty)包围的 delta。</li><li id="61da" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">(pw，ph)是从聚类中得到的锚盒先验。</li><li id="3067" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">(bw，bh)是边界框尺寸:(pw，ph)乘以(tw，th)。</li><li id="c6dd" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">地图比锚盒版本增加了 5%。</li></ul><h2 id="49af" class="nj lv iq bd lw nk nl dn ma nm nn dp me jy no np mi kc nq nr mm kg ns nt mq nu bi translated">1.6.精细特征</h2><ul class=""><li id="31f9" class="ms mt iq jp b jq mu ju mv jy mw kc mx kg my kk nv na nb nc bi translated">13×13 特征图输出足以检测大物体。</li><li id="3a28" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">为了更好地检测小目标，将前一层的 26×26×512 特征图映射为 13×13×2048 特征图，然后与原始的 13×13 特征图连接进行检测。</li><li id="0612" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">mAP 增加了 1%。</li></ul><h2 id="1e55" class="nj lv iq bd lw nk nl dn ma nm nn dp me jy no np mi kc nq nr mm kg ns nt mq nu bi translated">1.7.多尺度训练</h2><ul class=""><li id="06e4" class="ms mt iq jp b jq mu ju mv jy mw kc mx kg my kk nv na nb nc bi translated">对于每 10 个批次，随机选择新的图像尺寸。</li><li id="8198" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">图像尺寸为{320，352，…，608}。</li><li id="f457" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">调整网络大小并继续训练。</li></ul><h2 id="f14a" class="nj lv iq bd lw nk nl dn ma nm nn dp me jy no np mi kc nq nr mm kg ns nt mq nu bi translated">1.8.增量改进总结</h2><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi og"><img src="../Images/e9ff35996185fca31ec846eb229efffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2JHsliX3aGSP5QZznsa8pg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd lm">Incremental Improvements</strong></figcaption></figure><ul class=""><li id="7a11" class="ms mt iq jp b jq jr ju jv jy nx kc ny kg nz kk nv na nb nc bi translated">较小尺寸的图像网络运行速度更快。</li><li id="a317" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">对于低分辨率的 YOLOv2，在 90 FPS 的情况下，mAP 和<a class="ae ku" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener"> <em class="kv">快速 R-CNN </em> </a>一样好，非常适合更小的 GPU。</li><li id="a899" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">对于高分辨率 YOLOv2，以实时速度获得 76.8%的 mAP。</li></ul><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oh"><img src="../Images/cb2693563e0194e13357dbcee9c2b6f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DWN1vM5yUtqt1gIjTdiUeg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd lm">PASCAL VOC 2007 Dataset</strong></figcaption></figure><p id="436c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于 MS COCO 数据集，以下方法中只有 YOLOv2 可以获得实时性能:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oi"><img src="../Images/c28cebc66f65c304e21bdfbbfb4e5c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fh5NMQA06ok_RkrB3uQpDw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd lm">Microsoft COCO Dataset</strong></figcaption></figure><h1 id="a4c9" class="lu lv iq bd lw lx oj lz ma mb ok md me mf ol mh mi mj om ml mm mn on mp mq mr bi translated"><strong class="ak"> 2。YOLOv2 使用 Darknet-19(更快)</strong></h1><p id="574f" class="pw-post-body-paragraph jn jo iq jp b jq mu js jt ju mv jw jx jy oo ka kb kc op ke kf kg oq ki kj kk ij bi translated">除了以上修改，网络架构也是影响 mAP 的一个因素。</p><p id="c5df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 YOLOv2 中使用 Darknet-19 分类网络进行特征提取:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi or"><img src="../Images/6c63b84ff662a2d3b0bebe8895e41938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*iPHGuCWfCOTjrEW187fSZQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd lm">Darknet-19 Classification Network</strong></figcaption></figure><p id="723b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，Darknet-19 有许多 1×1 卷积来减少参数的数量。(如果对原因感兴趣，请访问我在<a class="ae ku" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener"> <em class="kv"> GoogLeNet </em> </a>上的评论。)</p><p id="3567" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Darknet-19 可以在准确性和模型复杂性之间获得良好的平衡:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi os"><img src="../Images/32de8e87b1286ce318c299377e83c4a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XNpkCCTVfEDfmGzk5VkL1g.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd lm">1000-Class ImageNet Classification Results</strong></figcaption></figure><p id="093c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于 Top-1 和 Top-5 错误接近于<a class="ae ku" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <em class="kv"> ResNet-50 </em> </a>，Darknet-19 具有更低的模型复杂度(FLOP)，因此具有更快的检测速度(FPS)。</p><h1 id="0b3e" class="lu lv iq bd lw lx oj lz ma mb ok md me mf ol mh mi mj om ml mm mn on mp mq mr bi translated">3.<strong class="ak"> YOLO9000 by WordTree(更强)</strong></h1><p id="68ad" class="pw-post-body-paragraph jn jo iq jp b jq mu js jt ju mv jw jx jy oo ka kb kc op ke kf kg oq ki kj kk ij bi translated">有多个数据集用于分类和检测。作者有一个想法，“他们能结合在一起吗？”</p><h2 id="3645" class="nj lv iq bd lw nk nl dn ma nm nn dp me jy no np mi kc nq nr mm kg ns nt mq nu bi translated"><strong class="ak"> 3.1。联合收割机</strong></h2><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi ot"><img src="../Images/3eabe6e1d0d60e2239ed73ad4bb86af8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1rpDaEiL-4NuTBlk9p0oAg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd lm">COCO and ImageNet</strong></figcaption></figure><ul class=""><li id="8d73" class="ms mt iq jp b jq jr ju jv jy nx kc ny kg nz kk nv na nb nc bi translated"><strong class="jp ir">微软 COCO </strong> : 100k 图片，80 个类，检测标签，类比较笼统像“狗”或者“船”。</li><li id="abdb" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">ImageNet:1300 万张图片，22k 个类别，分类标签，类别更具体，如“诺福克梗”，“约克夏梗”，或“贝德灵顿梗”。</li><li id="d8ec" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">像“狗”和“诺福克梗”这样的类别并不互相排斥。</li></ul><p id="af1d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们不能通过添加更多的类来直接组合它们，如下所示:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi ou"><img src="../Images/1148f57e7594ba78365c9b9ae60309cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Js9qWV9taiuZHzTgA65OxQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd lm">We CANNOT combine like this</strong></figcaption></figure><p id="bd5e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了合并，使用了单词树:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi ov"><img src="../Images/226e90748e969da9831d3abec5e4ca43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YiX61mdylOzZYlBFXl9HjA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd lm">WordTree</strong></figcaption></figure><p id="9b41" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所示，WordTree 有一个层次结构树，将类和子类联系在一起。简而言之，基于树的单词树是在基于图的单词网的基础上，通过将 ImageNet 中的名词可视化，并在单词网中选择较短的路径来构建的。</p><p id="ff06" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如:“诺福克梗”也有“狗”和“哺乳动物”的标签。</p><p id="2cf0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果网络看到“狗”的图像，但不确定它是哪种类型，它仍然会基于条件概率以高置信度预测为“狗”。</p><p id="7c4e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后有 9418 个班。</p><h2 id="87fb" class="nj lv iq bd lw nk nl dn ma nm nn dp me jy no np mi kc nq nr mm kg ns nt mq nu bi translated">3.2.YOLO9000 培训</h2><ul class=""><li id="444b" class="ms mt iq jp b jq mu ju mv jy mw kc mx kg my kk nv na nb nc bi translated">使用 3 个先验而不是 5 个先验来限制输出大小。</li><li id="030d" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">对于检测图像，损失通常反向传播。</li><li id="1449" class="ms mt iq jp b jq nd ju ne jy nf kc ng kg nh kk nv na nb nc bi translated">对于分类图像，只有分类损失在标签的相应级别或以上被反向传播。</li></ul><h2 id="7631" class="nj lv iq bd lw nk nl dn ma nm nn dp me jy no np mi kc nq nr mm kg ns nt mq nu bi translated">3.3.YOLO9000 结果</h2><ul class=""><li id="1968" class="ms mt iq jp b jq mu ju mv jy mw kc mx kg my kk nv na nb nc bi translated">获得了 19.7%的 mAP。</li></ul><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi ow"><img src="../Images/66323e392a42e99048b7c685e27e4bf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RCmvddJgGc6wyY_bjDhRSA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd lm">YOLO9000 Results</strong></figcaption></figure><p id="4c72" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最初，COCO 女士没有这么多的基本事实类，而 ImageNet 有类，但没有基本事实边界框。</p><p id="fa39" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在 YOLO9000 提供了一种将它们结合在一起的方法。</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="0010" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">参考</h1><p id="e003" class="pw-post-body-paragraph jn jo iq jp b jq mu js jt ju mv jw jx jy oo ka kb kc op ke kf kg oq ki kj kk ij bi translated">【2017 CVPR】【yolov 2 &amp; yolo 9000】<br/><a class="ae ku" href="https://arxiv.org/abs/1612.08242" rel="noopener ugc nofollow" target="_blank">yolo 9000:更好更快更强</a></p><h1 id="b48b" class="lu lv iq bd lw lx oj lz ma mb ok md me mf ol mh mi mj om ml mm mn on mp mq mr bi translated">我的相关评论</h1><p id="5578" class="pw-post-body-paragraph jn jo iq jp b jq mu js jt ju mv jw jx jy oo ka kb kc op ke kf kg oq ki kj kk ij bi translated">[ <a class="ae ku" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener"> R-CNN </a> ] [ <a class="ae ku" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快速 R-CNN </a> ] [ <a class="ae ku" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快 R-CNN</a>][<a class="ae ku" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae ku" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">yolov 1</a>][<a class="ae ku" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener">VGGNet</a>][<a class="ae ku" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">ResNet</a>][<a class="ae ku" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener">Google net/Inception-v1</a>][<a class="ae ku" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">Inception-v2/BN-Inception</a>]</p></div></div>    
</body>
</html>