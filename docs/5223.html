<html>
<head>
<title>Review: FCN — Fully Convolutional Network (Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:FCN —完全卷积网络(语义分段)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=collection_archive---------4-----------------------#2018-10-05">https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=collection_archive---------4-----------------------#2018-10-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="cc23" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>这个故事中，<strong class="jp ir">完全卷积网络(FCN)对语义分割</strong>进行了简要回顾。与分类和检测任务相比，分割是一项更加困难的任务。</p><ul class=""><li id="fd8e" class="ku kv iq jp b jq jr ju jv jy kw kc kx kg ky kk kz la lb lc bi translated"><strong class="jp ir">图像分类</strong>:对图像中的物体进行分类(识别出<strong class="jp ir">物体类别</strong>)。</li><li id="cb0d" class="ku kv iq jp b jq ld ju le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jp ir">物体检测</strong>:用包围物体的包围盒对图像内的物体进行分类和检测。这意味着我们还需要知道每个对象的<strong class="jp ir">类、位置和大小。</strong></li><li id="aab8" class="ku kv iq jp b jq ld ju le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jp ir">语义分割</strong>:对图像中的每个像素进行<strong class="jp ir">对象分类。这意味着每个像素都有一个标签。</strong></li></ul><p id="66e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">语义分割的一个例子如下:</p><figure class="li lj lk ll gt lm"><div class="bz fp l di"><div class="ln lo l"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><strong class="ak">An example of Semantic Segmentation</strong></figcaption></figure><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lt"><img src="../Images/ebcfbd9bf98a26ebee66870ec1d5c254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jrqv1qX2J1AGMA0g-RU7fg.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><strong class="bd ma">Original Image (Leftmost), Ground Truth Label Map (2nd Left), Predicted Label Map (2nd Right), Overlap Image and Predicted Label (Rightmost)</strong></figcaption></figure><p id="1544" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我写这个故事的时候，它已经发表在<strong class="jp ir">2015 CVPR</strong>【1】和<strong class="jp ir">2017 TPAMI</strong>【2】上，引用次数超过 6000 次。因此，它也是使用 FCN 进行语义分割的最基本的论文之一。(<a class="mb mc ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----eb8c9b50d2d1--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="b7a8" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated">涵盖哪些内容</h1><ol class=""><li id="f07f" class="ku kv iq jp b jq ni ju nj jy nk kc nl kg nm kk nn la lb lc bi translated"><strong class="jp ir">从图像分类到语义分割</strong></li><li id="2c67" class="ku kv iq jp b jq ld ju le jy lf kc lg kg lh kk nn la lb lc bi translated"><strong class="jp ir">通过去卷积进行上采样</strong></li><li id="07d1" class="ku kv iq jp b jq ld ju le jy lf kc lg kg lh kk nn la lb lc bi translated"><strong class="jp ir">定影输出</strong></li><li id="1923" class="ku kv iq jp b jq ld ju le jy lf kc lg kg lh kk nn la lb lc bi translated"><strong class="jp ir">结果</strong></li></ol></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="bcc2" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated"><strong class="ak"> 1。从图像分类到语义分割</strong></h1><p id="ce21" class="pw-post-body-paragraph jn jo iq jp b jq ni js jt ju nj jw jx jy no ka kb kc np ke kf kg nq ki kj kk ij bi translated">在分类中，传统上，输入图像被缩小并经过卷积层和全连接(FC)层，并输出输入图像的一个预测标签，如下:</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi nr"><img src="../Images/adea75e72177db0f4592f4e7f775fc03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ErOVnlmtWYFGpnpvKJImKw.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><strong class="bd ma">Classification</strong></figcaption></figure><p id="4487" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设我们将 FC 层变成 1×1 卷积层:</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ns"><img src="../Images/35201804712db5eced59c373154b04eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*11XDuwNHHRE7EB_fu_TzCg.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">All layers are convolutional layers</figcaption></figure><p id="4662" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果图像没有缩小，输出将不会是单个标签。相反，输出的大小小于输入图像(由于最大池化):</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi nt"><img src="../Images/93564ad48e45ef0065e56ec85689298d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T-tYcj11_qySDHTRhlF9pQ.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><strong class="bd ma">All layers are convolutional layers</strong></figcaption></figure><p id="9939" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们对上面的输出进行上采样，那么我们可以如下计算逐像素输出(标签映射):</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi nu"><img src="../Images/5b51eb8797657727e635e7906f6ed398.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LtSSJ9QP0Y9qWG9nz9sb2w.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><strong class="bd ma">Upsampling at the last step</strong></figcaption></figure><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/ef12f688898a6338abefba1028b56033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*NXNGhfSyzQcKzoOSt-Z0Ng.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><strong class="bd ma">Feature Map / Filter Number Along Layers</strong></figcaption></figure></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="a227" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated"><strong class="ak"> 2。通过去卷积进行上采样</strong></h1><p id="2e12" class="pw-post-body-paragraph jn jo iq jp b jq ni js jt ju nj jw jx jy no ka kb kc np ke kf kg nq ki kj kk ij bi translated">卷积是使输出变小的过程。因此，反卷积这个名称来自于我们希望通过上采样来获得更大的输出。(但名字，反卷积，被曲解为卷积的逆过程，其实不是。)又叫，<strong class="jp ir">上卷积，转置卷积。</strong>当使用分数步距时，也称为<strong class="jp ir">分数步距卷积</strong>。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/0d7d11b5da945d4daca4118874f9761c.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/0*NBKHZlXvqOg3R6_z.gif"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><strong class="bd ma">Upsampling Via Deconvolution (Blue: Input, Green: Output)</strong></figcaption></figure></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="684b" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated">3.融合输出</h1><p id="e7c2" class="pw-post-body-paragraph jn jo iq jp b jq ni js jt ju nj jw jx jy no ka kb kc np ke kf kg nq ki kj kk ij bi translated">经过如下 conv7 后，输出尺寸较小，则进行 32 倍上采样，使输出具有与输入图像相同的尺寸。但也让输出标签图粗糙。它叫做<strong class="jp ir"> FCN-32s </strong>:</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi nx"><img src="../Images/b867fa434885bfe478a254503e044749.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ajovnrcLYRuwlTrM5j-Qng.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><strong class="bd ma">FCN-32s</strong></figcaption></figure><p id="a57e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是因为，<strong class="jp ir">深入时可以获得深层特征，深入时也会丢失空间位置信息。</strong>这意味着来自较浅层的输出具有更多的位置信息。如果我们把两者结合起来，我们可以提高结果。</p><p id="e517" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了合并，我们<strong class="jp ir">融合输出(通过元素相加)</strong>:</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ny"><img src="../Images/bd1b12826c7354a16d08c3a8cc6ca3e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lUnNaKAjL-Mq10v3tIBtJg.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><strong class="bd ma">Fusing for FCN-16s and FCN-8s</strong></figcaption></figure><p id="2105" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">FCN-16s</strong>:pool 5 的输出经过 2 倍上采样，与 pool4 融合，进行 16 倍上采样。与上图中<strong class="jp ir"> FCN-8s </strong>类似的操作。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/86040f3a1b2655912d28b28906565c8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*tcYqvV0KHjK2ANBGe8GpjQ.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><strong class="bd ma">Comparison with different FCNs</strong></figcaption></figure><p id="04c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">由于丢失了位置信息，FCN-32s 的结果非常粗略</strong>，而 FCN-8s 的结果最好。</p><p id="8a82" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种融合操作实际上就像 AlexNet、VGGNet 和 GoogLeNet 中使用的 boosting / ensemble 技术一样，他们通过多个模型添加结果，使预测更加准确。但在这种情况下，它是针对每个像素完成的，并且它们是从模型内不同层的结果添加的。</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="7b34" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated">4.结果</h1><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oa"><img src="../Images/6d8e5dbea529778509fcd9a4f731a227.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2obgSShyzzBKuds_XxPCoA.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><strong class="bd ma">Pascal VOC 2011 dataset </strong>(Left), <strong class="bd ma">NYUDv2 Dataset (Middle), SIFT Flow Dataset (Right)</strong></figcaption></figure><ul class=""><li id="eb12" class="ku kv iq jp b jq jr ju jv jy kw kc kx kg ky kk kz la lb lc bi translated">FCN-8s 是 2011 年帕斯卡 VOC 的最佳产品。</li><li id="c764" class="ku kv iq jp b jq ld ju le jy lf kc lg kg lh kk kz la lb lc bi translated">FCN-16s 是 NYUDv2 中最好的。</li><li id="ddc3" class="ku kv iq jp b jq ld ju le jy lf kc lg kg lh kk kz la lb lc bi translated">FCN-16s 是最好的筛流。</li></ul><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/0ac4ddda9cb2e6d5e3527a3ec460b9d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*fU9El2B2qELtKsD9FsHB-w.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><strong class="bd ma">Visualized Results Compared with [Ref 15]</strong></figcaption></figure><p id="872d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第四行显示了一个失败案例:网络将船上的救生衣视为人。</p><p id="955c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">希望以后能多复习一下语义切分的深度学习技术。</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="faa9" class="mk ml iq bd mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bi translated">参考</h1><ol class=""><li id="7b1a" class="ku kv iq jp b jq ni ju nj jy nk kc nl kg nm kk nn la lb lc bi translated">【2015 CVPR】【FCN】<br/><a class="ae oc" href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" rel="noopener ugc nofollow" target="_blank">用于语义分割的全卷积网络</a></li><li id="f5f7" class="ku kv iq jp b jq ld ju le jy lf kc lg kg lh kk nn la lb lc bi translated">【2017 TPAMI】【FCN】<br/><a class="ae oc" href="https://ieeexplore.ieee.org/document/7478072" rel="noopener ugc nofollow" target="_blank">用于语义分割的全卷积网络</a></li></ol></div></div>    
</body>
</html>