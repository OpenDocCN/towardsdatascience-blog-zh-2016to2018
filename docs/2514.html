<html>
<head>
<title>Word2Vec and FastText Word Embedding with Gensim</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Gensim嵌入Word2Vec和FastText单词</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c?source=collection_archive---------1-----------------------#2018-02-04">https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c?source=collection_archive---------1-----------------------#2018-02-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/82400626d1dd911b97eed1a617cd711d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r_R38HJ9NkbPyb0Y7PkHRw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">(<a class="ae kf" href="https://iwcollege-cdn-pull-zone-theisleofwightco1.netdna-ssl.com/wp-content/uploads/2017/05/2DGerAvyRM.jpg" rel="noopener ugc nofollow" target="_blank">https://iwcollege-cdn-pull-zone-theisleofwightco1.netdna-ssl.com/wp-content/uploads/2017/05/2DGerAvyRM.jpg</a>)</figcaption></figure><p id="c89c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在自然语言处理(NLP)中，我们经常将单词映射成包含数值的向量，以便机器能够理解。单词嵌入是一种映射类型，它允许具有相似含义的单词具有相似的表示。本文将介绍两种最先进的单词嵌入方法，<strong class="ki iu"> Word2Vec </strong>和<strong class="ki iu"> FastText </strong>以及它们在Gensim中的实现。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="403d" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">传统方法</h1><p id="58d7" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">传统的表示单词的方式是one-hot vector，本质上是一个只有一个目标元素为1，其他为0的向量。向量的长度等于语料库中唯一词汇的总大小。按照惯例，这些独特的单词是按字母顺序编码的。也就是说，您应该预期以“a”开头的单词的单热向量具有较低的索引，而以“z”开头的单词的单热向量具有较高的索引。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mo"><img src="../Images/fa32a0f2badc1af79c09383c0a3a62f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zFzLwr9kMiuT5FH9TPwd-Q.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae kf" href="https://cdn-images-1.medium.com/max/1600/1*ULfyiWPKgWceCqyZeDTl0g.png" rel="noopener">https://cdn-images-1.medium.com/max/1600/1*ULfyiWPKgWceCqyZeDTl0g.png</a>ㄨ</figcaption></figure><p id="9f7a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管这种单词表示简单且易于实现，但还是有几个问题。首先，你不能推断两个词之间的任何关系，因为它们只有一个热表示。例如，单词“忍受”和“容忍”，虽然有相似的意思，他们的目标“1”是远离对方。此外，稀疏性是另一个问题，因为向量中有许多冗余的“0”。这意味着我们浪费了很多空间。我们需要一种更好的文字表达来解决这些问题。</p><h1 id="350f" class="ll lm it bd ln lo mt lq lr ls mu lu lv lw mv ly lz ma mw mc md me mx mg mh mi bi translated">Word2Vec</h1><p id="3683" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">Word2Vec是解决这些问题的有效方法，它利用了目标单词的上下文。本质上，我们希望使用周围的单词来用神经网络表示目标单词，神经网络的隐藏层对单词表示进行编码。</p><p id="a871" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Word2Vec有两种类型，Skip-gram和连续单词包(CBOW)。我将在下面的段落中简要描述这两种方法是如何工作的。</p><h2 id="1b26" class="my lm it bd ln mz na dn lr nb nc dp lv kr nd ne lz kv nf ng md kz nh ni mh nj bi translated">跳跃图</h2><p id="b3d7" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">对于skip-gram，输入是目标单词，而输出是目标单词周围的单词。例如，在句子“<em class="nk">我有一只可爱的狗</em>”中，输入将是“<em class="nk"> a </em>”，而输出是“<em class="nk"> I </em>”、“<em class="nk">有</em>”、“<em class="nk">可爱的</em>”和“<em class="nk">狗</em>”，假设窗口大小为5。所有输入和输出数据具有相同的维数，并且是一位热编码的。网络包含1个隐藏层，其尺寸等于嵌入尺寸，小于输入/输出向量尺寸。在输出层的末端，应用softmax激活函数，以便输出向量的每个元素描述特定单词在上下文中出现的可能性。下图显示了网络结构。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/c78b85bf92425427ceb1e2ecb06b1810.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*TbjQNQLuyEW-cgsofyDioQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Skip-gram (<a class="ae kf" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" rel="noopener ugc nofollow" target="_blank">https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/</a>)</figcaption></figure><p id="329f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目标单词的单词嵌入可以通过在将该单词的独热表示馈送到网络中之后提取隐藏层来获得。</p><p id="49c0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用skip-gram，表示维度从词汇大小(V)减少到隐藏层的长度(N)。此外，向量在描述单词之间的关系方面更“有意义”。两个关联词相减得到的向量有时表达的是性别或动词时态等有意义的概念，如下图所示(降维)。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nm"><img src="../Images/638495623eceb3515a9a9f5724fa4c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jpnKO5X0Ii8PVdQYFO2z1Q.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Visualize Word Vectors (<a class="ae kf" href="https://www.tensorflow.org/images/linear-relationships.png" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/images/linear-relationships.png</a>)</figcaption></figure><h2 id="b47a" class="my lm it bd ln mz na dn lr nb nc dp lv kr nd ne lz kv nf ng md kz nh ni mh nj bi translated">CBOW</h2><p id="dd1a" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">连续单词包(CBOW)与skip-gram非常相似，只是它交换了输入和输出。这个想法是，给定一个上下文，我们想知道哪个单词最有可能出现在其中。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/f0ff4d11f13b227ce648365cd9520dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*UdLFo8hgsX0a1NKKuf_n9Q.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">CBOW (<a class="ae kf" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" rel="noopener ugc nofollow" target="_blank">https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/</a>)</figcaption></figure><p id="8c0f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Skip-gram和CBOW之间最大的区别在于<strong class="ki iu">生成单词向量的方式</strong>。对于CBOW，将所有以目标词为目标的样本输入到网络中，并对提取的隐含层取平均值。比如，假设我们只有两句话，“他是个不错的家伙”和“她是个睿智的女王”。为了计算单词“a”的单词表示，我们需要将这两个示例“他是好人”和“她是明智的女王”输入到神经网络中，并取隐藏层中值的平均值。Skip-gram只输入一个且只有一个目标单词one-hot向量作为输入。</p><p id="04b5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">据说跳格法在生僻字中表现更好。尽管如此，Skip-gram和CBOW的性能大体相似。</p><h2 id="d56f" class="my lm it bd ln mz na dn lr nb nc dp lv kr nd ne lz kv nf ng md kz nh ni mh nj bi translated">履行</h2><p id="fa1b" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">我将向您展示如何使用Gensim(一个强大的NLP工具包)和TED Talk数据集来执行单词嵌入。</p><p id="4ae3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们使用urllib下载数据集，从文件中提取字幕。</p><figure class="mp mq mr ms gt ju"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="fefb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来看看<em class="nk"> input_text </em>变量存储了什么，部分如下图所示。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nq"><img src="../Images/b11d144d76d32ca7ca04aa425ed99323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C_3BoAOhjrNUNSbFPA2wmw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">input_text</figcaption></figure><p id="32f6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">显然，有一些多余的信息对我们理解演讲没有帮助，例如括号中描述声音的词和演讲者的名字。我们用正则表达式去掉这些词。</p><figure class="mp mq mr ms gt ju"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="3c9a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，<em class="nk"> sentences_ted </em>已经被转换成一个二维数组，每个元素都是一个单词。让我们打印出第一个和第二个元素。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/deb0ba4e9a1b2786e255a752e8659d57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cJSEWuP5vWygtlrC-arrwA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">sentences_ted</figcaption></figure><p id="f60b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是准备好输入Gensim中定义的Word2Vec模型的表单。Word2Vec模型用下面一行代码就可以轻松训练出来。</p><figure class="mp mq mr ms gt ju"><div class="bz fp l di"><div class="no np l"/></div></figure><ul class=""><li id="ee25" class="ns nt it ki b kj kk kn ko kr nu kv nv kz nw ld nx ny nz oa bi translated"><em class="nk">句子</em>:拆分句子列表。</li><li id="1121" class="ns nt it ki b kj ob kn oc kr od kv oe kz of ld nx ny nz oa bi translated"><em class="nk">大小</em>:嵌入向量的维数</li><li id="c5b6" class="ns nt it ki b kj ob kn oc kr od kv oe kz of ld nx ny nz oa bi translated"><em class="nk">窗口</em>:您正在查看的上下文单词数</li><li id="7293" class="ns nt it ki b kj ob kn oc kr od kv oe kz of ld nx ny nz oa bi translated"><em class="nk"> min_count </em>:告诉模型忽略总计数小于此数的单词。</li><li id="964f" class="ns nt it ki b kj ob kn oc kr od kv oe kz of ld nx ny nz oa bi translated"><em class="nk"> workers </em>:正在使用的线程数</li><li id="0680" class="ns nt it ki b kj ob kn oc kr od kv oe kz of ld nx ny nz oa bi translated"><em class="nk"> sg </em>:是使用skip-gram还是CBOW</li></ul><p id="5362" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们来试试哪些词和“人”这个词最相似。</p><figure class="mp mq mr ms gt ju"><div class="bz fp l di"><div class="no np l"/></div></figure><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi og"><img src="../Images/39cbdf5b082f1fd633af1c044cadce60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*M7yB5uXdKGZznfguXzoQEA.png"/></div></figure><p id="fa5a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">似乎与男人/女人/孩子相关的词与“男人”最为相似。</p><p id="6e2c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然Word2Vec成功地处理了一个热向量带来的问题，但是它有一些限制。最大的挑战是它不能代表没有出现在训练数据集中的单词。即使使用包含更多词汇的更大的训练集，一些很少使用的罕见单词也永远无法映射到向量。</p><h1 id="aa3d" class="ll lm it bd ln lo mt lq lr ls mu lu lv lw mv ly lz ma mw mc md me mx mg mh mi bi translated">快速文本</h1><p id="bd53" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">FastText是脸书在2016年提出的Word2Vec的扩展。FastText不是将单个单词输入到神经网络中，而是将单词分成几个n-grams(子单词)。例如，<em class="nk"> apple </em>这个词的三元组是<em class="nk"> app、ppl </em>和<em class="nk"> ple </em>(忽略单词边界的开始和结束)。<em class="nk">苹果</em>的单词嵌入向量将是所有这些n元文法的总和。在训练神经网络之后，给定训练数据集，我们将对所有n元语法进行单词嵌入。罕见的单词现在可以正确地表示，因为它们的一些n元语法很可能也出现在其他单词中。在下一节中，我将向您展示如何在Gensim中使用FastText。</p><h2 id="7bd9" class="my lm it bd ln mz na dn lr nb nc dp lv kr nd ne lz kv nf ng md kz nh ni mh nj bi translated">履行</h2><p id="d5fa" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">类似于Word2Vec，我们只需要一行来指定训练单词嵌入的模型。</p><figure class="mp mq mr ms gt ju"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="5d2b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们用<em class="nk">肠胃炎</em>这个词试试，这个词很少用，也没有出现在训练数据集中。</p><figure class="mp mq mr ms gt ju"><div class="bz fp l di"><div class="no np l"/></div></figure><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/3308099ec552aa6ff0e85ec490b836ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*gFFoZTsvoWHr8SQcs-J0ZQ.png"/></div></figure><p id="8d83" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">即使单词<em class="nk">肠胃炎</em>不存在于训练数据集中，它仍然能够找出这个单词与一些医学术语密切相关。如果我们在前面定义的Word2Vec中尝试这样做，它会弹出错误，因为这样的单词在训练数据集中不存在。虽然训练一个FastText模型需要更长的时间(n-grams的数量&gt;单词的数量)，但它的性能比Word2Vec更好，并允许适当地表示罕见的单词。</p><h1 id="d67a" class="ll lm it bd ln lo mt lq lr ls mu lu lv lw mv ly lz ma mw mc md me mx mg mh mi bi translated">结论</h1><p id="9723" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">您已经学习了什么是Word2Vec和FastText，以及它们在Gensim toolkit中的实现。如果你有任何问题，欢迎在下面留言。如果你喜欢这篇文章，确保你在twitter上关注我，这样你就不会错过任何伟大的机器学习/深度学习博客文章！</p></div></div>    
</body>
</html>