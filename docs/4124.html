<html>
<head>
<title>[ Paper Summary ] Convolutional Networks with Adaptive Computation Graphs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">【论文摘要】具有自适应计算图的卷积网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-summary-convolutional-networks-with-adaptive-computation-graphs-d3dcad10f565?source=collection_archive---------11-----------------------#2018-07-20">https://towardsdatascience.com/paper-summary-convolutional-networks-with-adaptive-computation-graphs-d3dcad10f565?source=collection_archive---------11-----------------------#2018-07-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/321994ab8f75441c9fcc5ff9650e568d.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/1*rCCTEzuedxZaUltDzKsmjw.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/glitch-datamosh-wheredidmypostgo-HFGiruj1v3vJC" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="d8f2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我总是对不同类型的网络体系结构感兴趣，本文介绍了非常特殊的网络体系结构。</p><blockquote class="kx ky kz"><p id="3f8a" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated"><strong class="kb ir">请注意，这篇帖子是为了我未来的自己回顾和复习这篇论文上的材料，而不是从头再看一遍论文。</strong></p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="https://arxiv.org/pdf/1711.11503.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="2f94" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">摘要</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/44928ba5fc97a18908e96a968f6efa92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*8nSlhi0Mrdv3TGDZhPx5Vg.png"/></div></figure><p id="394a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">神经网络通常有一个固定的结构，但它真的必须如此吗？在某一层之后，网络可能已经非常确信它正在图像中看到一只狗或一只猫，但是由于固定的结构，它必须使用所有的层，这可能损害它的性能。本文提出了一种新的网络算法——具有自适应计算图的卷积网络。与具有较少参数的 ResNet 34 相比，它在 Imagenet 数据上实现了更好的性能，并且对对立的例子更健壮。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="e76f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">简介</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ls"><img src="../Images/176e73e8359da269f80f4f78082b4d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l1B77YITlJQo_bTdU6-UEw.png"/></div></div></figure><p id="3a21" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">卷积神经网络不仅用于分割，还用于各种其他领域，众所周知，网络越深入，网络的性能就越好。关于这些网络的一个共同事实是，它们都是与输入图像无关的固定模型。然而，已经证明有些层对网络性能的贡献不是很大，现在要问的问题是…</p><p id="6d54" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们真的需要一个固定的卷积网络结构吗？或者，我们可以根据输入动态地组装一个网络图吗？</p><p id="256e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">本文提出了具有自适应计算图的卷积网络。而且大致思路可以看上面。一个挑战是，每个选通单元需要对是否使用下一层做出离散的决定，并且直接通过反向传播训练网络是困难的。因此，作者基于最近的工作，即离散随机节点的可微近似。这种架构的结果是网络的诞生，它能够为不同的高级类别生成不同的计算图。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="2d41" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">相关工作</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi lx"><img src="../Images/ad3d95012a2b1aa11b3214ae135e7ce0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KmsaepzJdQjrLFwfYj4n-w.png"/></div></div></figure><p id="e093" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">作者在这篇论文上的工作实际上涉及多个领域，如神经网络组成(通过构建计算图)、神经网络的自适应计算时间(通过动态计算时间)、带有随机噪声的正则化(通过丢弃某些层)和注意机制。(通过选择特定的重要层来组装计算图。)</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="7305" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">适应性</strong> / <strong class="kb ir">适应性计算图</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ly"><img src="../Images/5f7114c7f45c70aca4ffa6a3711fee12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oAEFqM5TAKP9lFdcPkFaAw.png"/></div></div></figure><p id="893d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">首先，每个卷积层可以用数学方法表示如下…</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/b50cdcb57d38959d633994b403c500a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*gz0LrwhtQt7EWqYELQ_ozg.png"/></div></figure><p id="ee25" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">秒 resnet 可以表示如下…</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/d6449a915c7c6b87ed8dd296e39bc9f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*mQpjgmdvHf_HhNiYFl99nA.png"/></div></figure><p id="9f8d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">并且自适应 resnet 可以表达如下…</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mb"><img src="../Images/afce0a6b50c141df0d844868a6c1b460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*HCOw85cbI-nZ6tEpOlULJA.png"/></div></div></figure><p id="d6cb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">一个重要的注意事项是，上述公式看起来类似于高速公路网络(如下所示),但注意自适应图网络不必执行每一层。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/a6281469ab016434b7ccbf0ed25bb3f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*Na9WFy0BppxBWrcb6q8GzA.png"/></div></figure><p id="ea25" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在让我们来谈谈这个网络的核心，也就是门控单元。这个单元非常重要，因为它必须理解输入数据，进行离散决策，并且易于执行。因此，作者提出了具有两个组件的门单元，第一个组件估计下一层应该被执行的概率，第二个组件获取估计的概率并从中抽取离散样本。(大致思路见上图。)</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi md"><img src="../Images/bae17c547a6c8aeadb454a1de26c6b26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*W4N93yODQpuG-iEg2A6bAg.png"/></div></figure><p id="8c97" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，在被传递到门单元之前，特征图通过在通道尺寸上的全局平均汇集而被缩小。(所以现在 dim 是 1*1*C)。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi gj"><img src="../Images/d56a6b3a29fffc89a42f9b2b2fc54e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z374RNleQvjGRGPhqBHq3g.png"/></div></div></figure><p id="d2a3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">接下来，该特征图被馈送到一个小全连接网络中，以产生一个二维向量，每个向量分别代表计算和跳过下一层的对数概率。之后，他们使用 Gumbel-Max 技巧，特别是其最近的连续松弛，来执行离散操作。(是否跳层。)所以作者在这里做了一件聪明的事。(为了清楚起见，上面的 beta 项被设置为 alpha。)</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi me"><img src="../Images/2ce44fbf7a7501945a2985b7c560d205.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*AWI8TaC1e6P5XE20iwhcHg.png"/></div></figure><p id="cbc8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">g 是一个遵循 Gumbel 分布的随机变量，在前馈操作中，它们使用上述函数。但是，在 back prop 期间，它们使用 softmax 函数。(特别是一个可微分的函数)。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mf"><img src="../Images/b7a0794f3f7b1554cbe00ffd4e243e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h3-eAf1leocou6XvmKgOyA.png"/></div></div></figure><p id="16c6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">注意，如上所述，根据 T 项，softmax 函数可以被视为 argmax 函数。值得注意的是，作者只能将方程 7 用于前馈操作和反馈操作，但通过实验，他们找到了最佳配置。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="a5b7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">训练学员</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mg"><img src="../Images/76bf3448fdb1ee4dcd445d4b07f2463b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YzAgoVf4SvwFehuZZZq23w.png"/></div></div></figure><p id="0029" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">由于新颖的结构，作者不得不引入额外的损失函数。有三件事需要考虑，1)网络可能学会使用所有的层，2)一些层可能消失，3)减少批量大小。作者首先使用传统的多类逻辑损失函数，此外，他们还引入了目标率损失(见下文)函数。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/39d41437a9e90a298e8d80a312fc289d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*6NcMCXZZFO6tWJ5NON5UNw.png"/></div></figure><p id="eab8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">其中 z 表示层 l 在一个小批中执行的次数，t 是目标速率。因此，结合这两个损失，我们获得如下。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mi"><img src="../Images/e136be7e3c7f9033182cd2c275eada0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FXkMMaoZnpfRlHx6r2syyQ.png"/></div></div></figure><p id="9b7f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">此外，为了成功训练，网络被初始化(最初)为偏向于门的打开，并且门控单元的学习速率被降低。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="aa9e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">实验</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mj"><img src="../Images/b4f42886ebbb893abcd26625b8cf6aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hpi6MKiVhuVoz4TdwvxmaQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">* notes Adanets in which all layers are executed and their output is scaled by the expected execution rate</figcaption></figure><p id="0569" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">作者从 Resnet 110 创建了 Adanet 以及创建了 wide Adanet，并且他们使用了权重衰减为 5e-4 的动量优化器，具有 350 个时期和 256 个小批量。如上表所示，我们可以看到，在减少计算时间的同时，改进算法能够优于不同的 Resnet 算法。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/b249b41788240f250aafe88c682ca6d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*M3zYJx51WJItZbTHdufg5g.png"/></div></figure><p id="e48f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">为了研究网络如何分配计算，作者绘制了不同类别图像的执行速率。从上面我们可以看到，下采样层是至关重要的，wide Adanet 显示了类之间更多的变化，这表明更多的容量可能有助于各个层专注于数据的某些子集，并且大多数类间变化来自网络的后续层。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ml"><img src="../Images/10a333589a05243fe2b7875f7ebf3e31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JpvPw2F8hNXlIHke4K_6VQ.png"/></div></div></figure><p id="34af" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">接下来，作者在 Imagenet 数据集上测试了 Adanet，如上所述，Adanet 能够降低计算成本。值得注意的是，Adanet 的性能优于 Resnet 34，Resnet 34 的参数更小，但计算代价更大。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/b038bd7347cd99bd5858c916ab42eb4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*YCNADfVetCrLFXv2FW0Zpw.png"/></div></figure><p id="0887" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">绘制执行速率图时，作者能够观察到更大范围的执行速率(第 9 层和第 10 层很少用于某些类)，下采样层/最后一层至关重要，在后面的层中，人造物体与动物的执行速率有显著差异。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mn"><img src="../Images/211ba6e2b53a227857e0160aacc69630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CYl2D6yDHccPyCR7_K3I7g.png"/></div></div></figure><p id="771a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当我们绘制前 30 个时期不同层的执行率时，我们可以直接观察到这些层迅速分成关键层和不太关键的层。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mo"><img src="../Images/ca542626bccfdc81b6c56629eb369238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a6IGi_5NlF1g46yqYN8pww.png"/></div></div></figure><p id="6889" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当我们绘制一个直方图，显示不同类别使用了多少层时，我们会得到类似上面的结果。平均执行 10.81 层，标准偏差为 1.11。然而，如上所述，鸟的图像比消费品的图像少用一层。(超级有趣)。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="123d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">对抗攻击的鲁棒性</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mp"><img src="../Images/434fc212077a249eac65f0a1ea0dd7ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hYD66C9VUJV1SUNXu4YDlw.png"/></div></div></figure><p id="f8da" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">为了知道特殊化层如何执行对抗攻击的效果，作者使用快速梯度符号攻击来创建对抗示例，并将其提供给网络。(作者还对创建的对抗实例进行了 JPEG 压缩防御。).如上所述，自适应算法对常规结果更健壮。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mq"><img src="../Images/fc21cbc33dcd657e6b1e0157bed3b4fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G7upKyqmGCccszLZYgJX0w.png"/></div></div></figure><p id="f725" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">接下来，为了观察对立的例子是否影响不同层次的执行率，作者绘制了常规鸟图像和对立的鸟图像的执行率条形图。我们可以看到执行率并没有受到太大的影响。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="b5e5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结论</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mr"><img src="../Images/76872ad90c46ad2feb19731368c8f56c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q8J9IYXDH-ObMdOYt30JTg.png"/></div></div></figure><p id="8771" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">总之，本文作者介绍了一种新的网络体系结构，称为 Adanets。其具有根据输入数据学习要执行哪些层的能力。通过多次实验，作者发现这种类型的体系结构不仅性能优于常规的 resnets，而且对恶意攻击也更具鲁棒性。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="6e12" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">遗言</strong></p><p id="f7dd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">非常聪明和新颖的建筑设计，我特别惊讶地知道网络是如何区分人造物品和动物的。</p><p id="f833" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有写作的列表，请<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">在这里查看我的网站</a>。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的 twitter 上关注我<a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文</a> t。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="d592" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="4617" class="ms mt iq kb b kc kd kg kh kk mu ko mv ks mw kw mx my mz na bi translated">(2018).Arxiv.org。检索于 2018 年 7 月 19 日，来自<a class="ae jy" href="https://arxiv.org/pdf/1711.11503.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1711.11503.pdf</a></li><li id="418e" class="ms mt iq kb b kc nb kg nc kk nd ko ne ks nf kw mx my mz na bi translated">Veit，a .，&amp; Belongie，S. (2017 年)具有自适应计算图的卷积网络。Arxiv.org。检索于 2018 年 7 月 19 日，来自<a class="ae jy" href="https://arxiv.org/abs/1711.11503" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1711.11503</a></li></ol></div></div>    
</body>
</html>