# 我训练假新闻检测 AI 准确率> 95%，差点疯了

> 原文：<https://towardsdatascience.com/i-trained-fake-news-detection-ai-with-95-accuracy-and-almost-went-crazy-d10589aa57c?source=collection_archive---------0----------------------->

![](img/d8ba2af6856bee7ec195fc22ccd24e1b.png)

Fake news is still a real problem

*TL；dr——我们用机器学习和自然语言处理制作了一个准确率超过 95%的假新闻检测器，你可以在这里下载*[](https://goo.gl/2cvBmp)**。在现实世界中，准确率可能会更低，尤其是随着时间的推移和文章写作方式的改变。**

*随着自然语言处理和机器学习的如此多的进步，我想也许，仅仅是也许，我可以制作一个模型，可以标记新闻内容为假，也许可以减轻假新闻泛滥的毁灭性后果。*

*可以说，制作自己的机器学习模型最困难的部分是收集训练数据。我花了很多天收集 2017/2018 赛季每个 [NBA 球员的照片，以训练一个面部识别模型](/5-things-i-learned-training-an-ai-model-on-every-nba-player-32d906b28688)。我一点也不知道我会陷入一个痛苦的、长达数月的过程中，暴露出一些真正黑暗和令人不安的事情，这些事情仍然被作为新闻和真实信息传播。*

# *定义假新闻*

*我的第一个障碍出乎意料。在对假新闻做了一些研究后，我很快发现误传可以分为许多不同的类别。有些文章是明显虚假的，有些文章提供了真实的事件，但随后做出了一些错误的解释，有些文章是伪科学的，有些文章实际上只是伪装成新闻的观点，有些文章是讽刺性的，还有一些文章主要是由推特和其他人的引用组成的。我在谷歌上搜索了一下，发现有些人试图将网站分成“讽刺”、“虚假”、“误导”等类别。*

*我认为这是最好的起点，所以我继续前进，开始访问这些领域，试图寻找一些例子。几乎立刻我就发现了一个问题。一些被标记为“虚假”或“误导”的网站有时会有真实的文章。所以我知道，如果不进行健全性检查，就无法清除它们。*

*然后，我开始问自己，我的模型是否应该考虑讽刺和观点作品，如果应该，它们应该被视为假的，真实的，还是被归入自己的类别？*

# *情感分析*

*在盯着假新闻网站看了大约一周后，我开始怀疑自己是否已经把问题过于复杂化了。也许我只是需要在情感分析上使用一些现有的机器学习模型，看看是否有一种模式？我决定构建一个快速的小工具，使用 web scraper 来抓取文章标题、描述、作者和内容，并将结果发布到情感分析模型。我使用了 [Textbox](https://goo.gl/2cvBmp) ，这很方便，因为它在我的机器上本地运行，并快速返回结果。*

*[文本框](https://goo.gl/2cvBmp)返回一个情感分数，你可以将其解释为*正面*或*负面*。然后，我构建了一个蹩脚的小算法，为我提取的不同类型文本(标题、内容、作者等)的情感添加权重。)然后全部加在一起，看看能不能得出一个全局的分数。*

*起初它工作得很好，但是在我尝试了大约第 7 或第 8 篇文章之后，它开始下降。长话短说，它与我想要建立的假新闻检测系统相去甚远。*

*失败。*

# ***自然语言处理***

*这是我的朋友大卫·赫南德兹推荐的，实际上是在文本本身上训练一个模型。为了做到这一点，我们需要很多很多不同类别的例子，我们希望模型能够预测。*

*由于我很难理解假新闻的模式，我们决定试着去搜索那些已知的假的、真实的、讽刺的领域。看看我们能否快速建立一个数据集。*

*在运行原油刮刀几天后，我们有了一个我们认为足够大的数据集来训练一个模型。*

*结果糟透了。深入研究训练数据，我们意识到，这些领域从来没有像我们希望的那样被划分成整齐的小类别。其中一些是假新闻混合真实新闻，另一些只是来自其他网站的博客帖子，还有一些只是 90%的文本都是特朗普推文的文章。所以我们意识到我们必须从训练数据开始。*

*这是事情变糟的时候。*

*那是一个周六，我开始了漫长的过程，手动阅读每一篇文章，然后决定它属于哪一类，然后笨拙地将文本复制并粘贴到一个越来越笨重的电子表格中。有一些黑暗的，令人厌恶的，种族主义的，真正堕落的东西，我读了，起初我试图忽略。但是在阅读了数百篇这样的文章后，他们开始影响我。随着我的视力模糊，我对颜色的理解变得一塌糊涂，我开始变得非常沮丧。文明是怎么沦落到如此地步的？为什么人们不能进行批判性思考？我们真的还有希望吗？这种情况持续了几天，因为我在努力为这个模型找到足够多的例子。*

*我发现自己在自己对假新闻的解读中随波逐流，当我看到我不同意的文章时会感到愤怒，并努力克制只选择我认为正确的文章的冲动。到底什么是对的或错的？*

*但是最后，我找到了我一直在寻找的神奇数量的例子，并且如释重负地把它们通过电子邮件发给了大卫。*

*第二天，当我急切地等待结果时，他又开始了训练。*

*我们达到了大约 70%的准确率。起初我认为这很棒，但是在对野外的文章做了一些抽查后，我意识到这对任何人都没有用。*

*失败。*

# *Fakebox*

*回到绘图板。我做错了什么？大卫提出，也许简化问题是提高准确度的关键。所以我认真思考了我想解决的问题是什么。然后它击中了我；也许答案不是检测假新闻，而是检测真新闻。真正的新闻更容易归类。它实事求是，切中要害，几乎没有解释。有很多可靠的来源可以得到它。*

*所以我回到网上，重新开始收集训练数据。我决定把所有的东西分成两类:真实和不真实。Notreal 包括讽刺文章、观点文章、假新闻，以及其他所有不以纯粹事实的方式撰写的文章，这些文章也符合美联社的标准。*

*我花了几周时间做这件事，每天花几个小时从你能想象到的各种网站获取最新内容，从 [*《洋葱》*](http://theonion.com) 到 [*【路透社】*](http://reuters.com) 。我把成千上万真实和非真实内容的例子放入一个巨大的电子表格中，每天我都会添加数百个。最终，我决定我有足够的例子再试一次。所以我把电子表格发给大卫，不耐烦地等待结果。*

*当我看到准确率达到 95%以上时，我高兴得差点跳起来。这意味着我们在文章的写作方式中发现了一种模式，可以发现真实新闻和你应该半信半疑的东西之间的差异。*

*成功(算是)！*

# *停止假新闻*

*这个练习的全部目的是阻止错误信息的传播，所以我很高兴与你们分享这个模型。我们称之为 [Fakebox](https://goo.gl/8eTTJE) ，它非常容易使用。*

*![](img/0b9ec1dbd810847b0f3c9655301a4257.png)*

*粘贴您不确定的文章内容，然后单击分析。*

*![](img/0b9ec1dbd810847b0f3c9655301a4257.png)*

*用一个好的 RESTful API 将其集成到任何环境中。它是一个 [Docker](http://docker.com) 容器，所以你可以在任何你喜欢的地方部署和扩展它。随心所欲地快速制作无限量的内容，并自动标记可能需要关注的内容。*

***记住，它告诉你的是，如果一篇文章的写作方式与一篇真实的新闻文章**相似，那么如果分数真的很低，这可能意味着这篇文章是假的，是一篇观点文章，是讽刺文章，或者不是一篇直截了当、只讲事实的新闻文章。*

*总之，我们训练了一个机器学习模型，它分析一篇文章的写作方式，并告诉你它是否类似于一篇用很少或没有偏见的词、强烈的形容词、观点或丰富多彩的语言写的文章。如果一篇文章太短，或者主要是由他人的引用(或推文)组成，这可能会很困难。这不是解决假新闻的最终办法。但是希望它能帮助你发现那些需要半信半疑的文章。*

*请欣赏！*