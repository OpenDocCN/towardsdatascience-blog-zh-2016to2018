<html>
<head>
<title>Using Scrapy to Build your Own Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Scrapy 构建自己的数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-scrapy-to-build-your-own-dataset-64ea2d7d4673?source=collection_archive---------0-----------------------#2017-09-26">https://towardsdatascience.com/using-scrapy-to-build-your-own-dataset-64ea2d7d4673?source=collection_archive---------0-----------------------#2017-09-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jn jo jp jq"><div class="bz fp l di"><div class="jr js l"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Web Scraping (Scrapy) using Python</figcaption></figure><p id="a45f" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">当我刚开始在工业界工作时，我很快意识到的一件事是，有时你必须收集、组织和清理你自己的数据。在本教程中，我们将从一个名为<a class="ae kv" href="https://fundrazr.com/" rel="noopener ugc nofollow" target="_blank"> FundRazr </a>的众筹网站收集数据。像许多网站一样，该网站有自己的结构、形式，并有大量可访问的有用数据，但很难从该网站获得数据，因为它没有结构化的 API。因此，我们将从网站上抓取非结构化的网站数据，并以有序的形式构建我们自己的数据集。</p><p id="b3aa" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">为了抓取网站，我们将使用<a class="ae kv" href="https://scrapy.org/" rel="noopener ugc nofollow" target="_blank"> Scrapy </a>。简而言之，Scrapy 是一个框架，旨在更容易地构建 web 抓取工具，并减轻维护它们的痛苦。基本上，它允许您专注于使用 CSS 选择器和选择 XPath 表达式的数据提取，而不是蜘蛛应该如何工作的复杂内部。这篇博客文章超越了来自 scrapy 文档的伟大的<a class="ae kv" href="https://doc.scrapy.org/en/latest/intro/tutorial.html" rel="noopener ugc nofollow" target="_blank">官方教程，希望如果你需要更难的东西，你可以自己做。就这样，让我们开始吧。如果你迷路了，我建议在一个单独的标签中打开</a><a class="ae kv" href="https://www.youtube.com/watch?v=O_j3OTXw2_E" rel="noopener ugc nofollow" target="_blank">视频</a>。</p><h2 id="0739" class="kw kx iq bd ky kz la dn lb lc ld dp le ki lf lg lh km li lj lk kq ll lm ln lo bi translated">入门(先决条件)</h2><p id="55f8" class="pw-post-body-paragraph jx jy iq jz b ka lp kc kd ke lq kg kh ki lr kk kl km ls ko kp kq lt ks kt ku ij bi translated">如果你已经有了 anaconda 和 google chrome(或者 Firefox)，跳到创建一个新的 Scrapy 项目。</p><p id="701e" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated"><strong class="jz ir"> 1。在你的操作系统上安装 Anaconda (Python)。您可以从官方网站下载 anaconda 并自行安装，也可以遵循下面的 anaconda 安装教程。</strong></p><figure class="lu lv lw lx gt jq"><div class="bz fp l di"><div class="ly js l"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Installing Anaconda</figcaption></figure><p id="fabc" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated"><strong class="jz ir"> 2。</strong>安装 Scrapy (anaconda 自带，只是以防万一)。您也可以在终端(mac/linux)或命令行(windows)上安装。您可以键入以下内容:</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="936a" class="kw kx iq ma b gy me mf l mg mh">conda install -c conda-forge scrapy</span></pre><p id="66d9" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">3.确保你有谷歌浏览器或火狐浏览器。在本教程中，我使用的是谷歌浏览器。如果你没有谷歌浏览器，你可以使用这个<a class="ae kv" href="https://support.google.com/chrome/answer/95346?co=GENIE.Platform%3DDesktop&amp;hl=en" rel="noopener ugc nofollow" target="_blank">链接</a>在这里安装。</p><h2 id="c24c" class="kw kx iq bd ky kz la dn lb lc ld dp le ki lf lg lh km li lj lk kq ll lm ln lo bi translated">创建一个新的 Scrapy 项目</h2><p id="96d0" class="pw-post-body-paragraph jx jy iq jz b ka lp kc kd ke lq kg kh ki lr kk kl km ls ko kp kq lt ks kt ku ij bi translated">1.打开终端(mac/linux)或命令行(windows)。导航到所需的文件夹(如果需要帮助，请参见下图)并键入</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="cdf1" class="kw kx iq ma b gy me mf l mg mh">scrapy startproject fundrazr</span></pre><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mi"><img src="../Images/4a1fabe87256e23d0a9e4f9944fcc66e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NSTOVwlIonixPtqM_YOx9w.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">scrapy startproject fundrazr</figcaption></figure><p id="38de" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">这将创建一个包含以下内容的 fundrazr 目录:</p><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mp"><img src="../Images/d088ab47b8f4ab2a5a1445f58b565067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZedpgQ0cl7IPjRywCXwYbw.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">fundrazr project directory</figcaption></figure><h2 id="9a2a" class="kw kx iq bd ky kz la dn lb lc ld dp le ki lf lg lh km li lj lk kq ll lm ln lo bi translated">在 Google Chrome(或 Firefox)上使用 Inspect 寻找好的开始网址</h2><p id="787b" class="pw-post-body-paragraph jx jy iq jz b ka lp kc kd ke lq kg kh ki lr kk kl km ls ko kp kq lt ks kt ku ij bi translated">在蜘蛛框架中，<strong class="jz ir"> start_urls </strong>是一个 URL 列表，当没有指定特定的 URL 时，蜘蛛将从该列表开始爬行。我们将使用<strong class="jz ir"> start_urls </strong>列表中的每个元素作为获取单个活动链接的手段。</p><p id="9515" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">1.下图显示，根据您选择的类别，您会得到不同的起始 url。黑色突出显示的部分是可能要刮除的 fundrazrs 类别。</p><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mq"><img src="../Images/66f269fef0d73a09f1176077a14c0690.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ePsVj6nMeSSns2SSvUXvA.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Finding a good first start_url</figcaption></figure><p id="e379" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">对于本教程，列表中的第一个<strong class="jz ir"> start_urls </strong>是:</p><p id="1bf3" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated"><a class="ae kv" href="https://fundrazr.com/find?category=Health" rel="noopener ugc nofollow" target="_blank">https://fundrazr.com/find?category=Health</a></p><p id="41ec" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">2.这一部分是关于获取额外的元素放入<strong class="jz ir"> start_urls </strong>列表。我们正在寻找如何转到下一页，这样我们就可以获得更多的 URL 放入<strong class="jz ir"> start_urls </strong>。</p><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mr"><img src="../Images/a06c5b1a65a409e4df3ba3ba506adbe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZzB7x0mx3jj5_GVbJZfKeg.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Getting additional elements to put in <strong class="bd ms">start_urls</strong> list by inspecting Next button</figcaption></figure><p id="296e" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">第二个开始网址是:<a class="ae kv" href="https://fundrazr.com/find?category=Health&amp;page=2" rel="noopener ugc nofollow" target="_blank">https://fundrazr.com/find?category=Health&amp;page = 2</a></p><p id="319e" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">下面的代码将在本教程后面的蜘蛛代码中使用。它所做的就是创建一个 start_urls 列表。变量 npages 就是我们希望从多少个附加页面(在第一页之后)获得活动链接。</p><figure class="lu lv lw lx gt jq"><div class="bz fp l di"><div class="ly js l"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Code to generate additional start urls based on current structure of website</figcaption></figure><h2 id="0c75" class="kw kx iq bd ky kz la dn lb lc ld dp le ki lf lg lh km li lj lk kq ll lm ln lo bi translated">Scrapy 外壳用于查找个人活动链接</h2><p id="70d5" class="pw-post-body-paragraph jx jy iq jz b ka lp kc kd ke lq kg kh ki lr kk kl km ls ko kp kq lt ks kt ku ij bi translated">学习如何用 Scrapy 提取数据的最好方法是使用 Scrapy shell。我们将使用 XPaths，它可以用来从 HTML 文档中选择元素。</p><p id="50bf" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">我们将尝试获取 xpaths 的第一件事是单独的活动链接。首先，我们检查一下 HTML 中的活动的大致位置。</p><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mt"><img src="../Images/1e5550938bebd6d9be9b2a550e276281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n5MWPew1dsD-f_FqU0xMTQ.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Finding links to individual campaigns</figcaption></figure><p id="d0e0" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">我们将使用 XPath 提取下面红色矩形中包含的部分。</p><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mu"><img src="../Images/17f6fc093ca6c405c7ee1e0e082d003b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XOzIItj5lS-wAiCeJn03FQ.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">The enclosed part is a partial url we will isolate</figcaption></figure><p id="c189" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">在终端类型(mac/linux)中:</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="ccf1" class="kw kx iq ma b gy me mf l mg mh">scrapy shell '<a class="ae kv" href="https://fundrazr.com/find?category=Health'" rel="noopener ugc nofollow" target="_blank">https://fundrazr.com/find?category=Health'</a></span></pre><p id="7519" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">在命令行中键入(windows):</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="46cf" class="kw kx iq ma b gy me mf l mg mh">scrapy shell “<a class="ae kv" href="https://fundrazr.com/find?category=Health" rel="noopener ugc nofollow" target="_blank">https://fundrazr.com/find?category=Health</a>"</span></pre><p id="8089" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">在 scrapy shell 中键入以下内容(为了帮助理解代码，请查看视频):</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="c9b7" class="kw kx iq ma b gy me mf l mg mh">response.xpath("//h2[contains(<a class="ae kv" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>, 'title headline-font')]/a[contains(<a class="ae kv" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>, 'campaign-link')]//@href").extract()</span></pre><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mv"><img src="../Images/4f431825d048ea0d20abb6ece124ff66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gsFvVmUq85f1Xg5L7mHyAA.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">There is a good chance you will get different partial urls as websites update over time</figcaption></figure><p id="d9e2" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">下面的代码用于获取给定起始 url 的所有活动链接(稍后在第一个蜘蛛部分会有更多)</p><figure class="lu lv lw lx gt jq"><div class="bz fp l di"><div class="ly js l"/></div></figure><p id="0508" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">通过键入<strong class="jz ir"> exit() </strong>退出 Scrapy Shell。</p><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/17c0647d608673d6096ea391f7a24ba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*62ksPbArt2iM732MQnN4wg.png"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">exit scrapy shell</figcaption></figure><h2 id="d082" class="kw kx iq bd ky kz la dn lb lc ld dp le ki lf lg lh km li lj lk kq ll lm ln lo bi translated">检查单个活动</h2><p id="2ac4" class="pw-post-body-paragraph jx jy iq jz b ka lp kc kd ke lq kg kh ki lr kk kl km ls ko kp kq lt ks kt ku ij bi translated">虽然我们应该先了解各个活动链接的结构，但本节将介绍各个活动的位置。</p><ol class=""><li id="f981" class="mx my iq jz b ka kb ke kf ki mz km na kq nb ku nc nd ne nf bi translated">接下来，我们转到一个单独的活动页面(见下面的链接)进行搜集(我应该注意到，有些活动很难查看)</li></ol><p id="7506" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated"><a class="ae kv" href="https://fundrazr.com/savemyarm" rel="noopener ugc nofollow" target="_blank">https://fundrazr.com/savemyarm</a></p><p id="c62e" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">2.使用与前面相同的检查过程，我们检查页面上的标题</p><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ng"><img src="../Images/6255de9ed0d383fe026c0e409eca4bf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Os8G6pRp2ZC3iM8-rgCBWQ.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Inspect Campaign Title</figcaption></figure><p id="ed51" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">3.现在，我们将再次使用 scrapy shell，但这一次是一个单独的活动。我们这样做是因为我们想了解各个活动是如何格式化的(包括了解如何从网页中提取标题)。</p><p id="9226" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">在终端类型(mac/linux)中:</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="d191" class="kw kx iq ma b gy me mf l mg mh">scrapy shell '<a class="ae kv" href="https://fundrazr.com/savemyarm'" rel="noopener ugc nofollow" target="_blank">https://fundrazr.com/savemyarm</a>'</span></pre><p id="86e5" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">在命令行中键入(windows):</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="f5ce" class="kw kx iq ma b gy me mf l mg mh">scrapy shell “<a class="ae kv" href="https://fundrazr.com/savemyarm" rel="noopener ugc nofollow" target="_blank">https://fundrazr.com/savemyarm</a>"</span></pre><p id="e7f9" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">获取活动标题的代码是</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="d3ad" class="kw kx iq ma b gy me mf l mg mh">response.xpath("//div[contains(<a class="ae kv" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>, 'campaign-title')]/descendant::text()").extract()[0]</span></pre><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nh"><img src="../Images/458e2afa105a89619a598a462f85f3dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6eftqCEKlegtcRq7cGdYQ.png"/></div></div></figure><p id="0e3a" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">4.我们可以对页面的其他部分做同样的事情。</p><p id="cc2b" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">筹集的金额:</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="e9d5" class="kw kx iq ma b gy me mf l mg mh">response.xpath("//span[contains(<a class="ae kv" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>,'stat')]/span[contains(<a class="ae kv" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>, 'amount-raised')]/descendant::text()").extract()</span></pre><p id="962b" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">目标:</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="76ad" class="kw kx iq ma b gy me mf l mg mh">response.xpath("//div[contains(<a class="ae kv" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>, 'stats-primary with-goal')]//span[contains(<a class="ae kv" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>, 'stats-label hidden-phone')]/text()").extract()</span></pre><p id="6934" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">货币类型:</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="533b" class="kw kx iq ma b gy me mf l mg mh">response.xpath("//div[contains(<a class="ae kv" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>, 'stats-primary with-goal')]/@title").extract()</span></pre><p id="45fe" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">活动结束日期:</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="cdc8" class="kw kx iq ma b gy me mf l mg mh">response.xpath("//div[contains(<a class="ae kv" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>, 'campaign-stats')]//span[contains(<a class="ae kv" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>,'stats-label hidden-phone')]/span[<a class="ae kv" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>='nowrap']/text()").extract()</span></pre><p id="5391" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">贡献者人数:</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="ee77" class="kw kx iq ma b gy me mf l mg mh">response.xpath("//div[contains(<a class="ae kv" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>, 'stats-secondary with-goal')]//span[contains(<a class="ae kv" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>, 'donation-count stat')]/text()").extract()</span></pre><p id="70a3" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">故事:</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="aef1" class="kw kx iq ma b gy me mf l mg mh">response.xpath("//div[contains(<a class="ae kv" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>, 'full-story')]/descendant::text()").extract()</span></pre><p id="3b5e" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">网址:</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="a254" class="kw kx iq ma b gy me mf l mg mh">response.xpath("//meta[<a class="ae kv" href="http://twitter.com/property" rel="noopener ugc nofollow" target="_blank">@property</a>='og:url']/@content").extract()</span></pre><p id="eb69" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">5.键入以下命令退出 scrapy shell:</p><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="e2fb" class="kw kx iq ma b gy me mf l mg mh">exit()</span></pre><h2 id="33d5" class="kw kx iq bd ky kz la dn lb lc ld dp le ki lf lg lh km li lj lk kq ll lm ln lo bi translated">项目</h2><p id="3e89" class="pw-post-body-paragraph jx jy iq jz b ka lp kc kd ke lq kg kh ki lr kk kl km ls ko kp kq lt ks kt ku ij bi translated">抓取的主要目标是从非结构化的源(通常是网页)中提取结构化数据。Scrapy 蜘蛛可以将提取的数据作为 Python 字典返回。虽然方便和熟悉，但 Python 字典缺乏结构:很容易在字段名中打错字或返回不一致的数据，尤其是在有许多蜘蛛的大型项目中(几乎逐字逐句地复制自伟大的 scrapy 官方文档！).</p><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ni"><img src="../Images/b9c92d4828e2ae4fce7fd99c53c8193e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0XU-5Axl8h9aDZFkXhGyLQ.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">File we will be modifying</figcaption></figure><p id="9088" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">items.py 的代码在这里是<a class="ae kv" href="https://github.com/mGalarnyk/Python_Tutorials/raw/master/Scrapy/fundrazr/fundrazr/items.py" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="58f4" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">将其保存在 fundrazr/fundrazr 目录下(覆盖原来的 items.py 文件)。</p><p id="03e4" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">本教程中使用的 item 类(基本上是我们在输出数据之前存储数据的方式)如下所示。</p><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/8a658067bf11f7e8cbf6cb500d30e824.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*itTpe8dWR8CXIPzymjoHOA.png"/></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">items.py code</figcaption></figure><h2 id="221e" class="kw kx iq bd ky kz la dn lb lc ld dp le ki lf lg lh km li lj lk kq ll lm ln lo bi translated">蜘蛛</h2><p id="fe3b" class="pw-post-body-paragraph jx jy iq jz b ka lp kc kd ke lq kg kh ki lr kk kl km ls ko kp kq lt ks kt ku ij bi translated">蜘蛛是你定义的类，Scrapy 用它从一个网站(或一组网站)抓取信息。我们蜘蛛的代码如下。</p><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nk"><img src="../Images/2ea23637779b30e1fd2260b7aed9b45b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhPZAv8PvzoPV2YhdUdAOQ.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Fundrazr Scrapy Code</figcaption></figure><p id="5dd3" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">在这里下载<a class="ae kv" href="https://raw.githubusercontent.com/mGalarnyk/Python_Tutorials/master/Scrapy/fundrazr/fundrazr/spiders/fundrazr_scrape.py" rel="noopener ugc nofollow" target="_blank">的代码</a>。</p><p id="ae84" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">将其保存在 fundrazr/spiders 目录下的一个名为<strong class="jz ir">的文件中。</strong></p><p id="5b64" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">当前项目现在应该有以下内容:</p><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nl"><img src="../Images/23098c7ce8f0bb11744b8f0bd381c0b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VI0lowOHVKAifkZuaU5uoA.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">File we will be creating/adding</figcaption></figure><h2 id="105e" class="kw kx iq bd ky kz la dn lb lc ld dp le ki lf lg lh km li lj lk kq ll lm ln lo bi translated">运行蜘蛛</h2><ol class=""><li id="ce54" class="mx my iq jz b ka lp ke lq ki nm km nn kq no ku nc nd ne nf bi translated">转到 fundrazr/fundrazr 目录并键入:</li></ol><pre class="lu lv lw lx gt lz ma mb mc aw md bi"><span id="33f9" class="kw kx iq ma b gy me mf l mg mh">scrapy crawl my_scraper -o MonthDay_Year.csv</span></pre><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi np"><img src="../Images/01d9ebd1c1d113406c04ff006848361f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*84JJg6FgMOKfcJXNivkJTQ.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">scrapy crawl my_scraper -o MonthDay_Year.csv</figcaption></figure><p id="1a93" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">2.数据应该输出到 fundrazr/fundrazr 目录中。</p><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nq"><img src="../Images/f479a5b02a30a4cda55f17786181bf9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v7eePNbptxkHWyJg3LKz4Q.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Data Output Location</figcaption></figure><h2 id="04cc" class="kw kx iq bd ky kz la dn lb lc ld dp le ki lf lg lh km li lj lk kq ll lm ln lo bi translated">我们的数据</h2><ol class=""><li id="53fb" class="mx my iq jz b ka lp ke lq ki nm km nn kq no ku nc nd ne nf bi translated">本教程中输出的数据应该大致如下图所示。随着网站的不断更新，单个广告活动也会有所不同。此外，在 excel 解释 csv 文件时，每个活动之间可能会有空格。</li></ol><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nr"><img src="../Images/b09634ee0eddb32764f9902f56477bf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kbi0QlS60C3ErTLJnGZDeQ.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">The data should <strong class="bd ms">roughly </strong>be in this format.</figcaption></figure><p id="cdb3" class="pw-post-body-paragraph jx jy iq jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku ij bi translated">2.如果你想下载一个更大的文件(它是通过将 npages = 2 改为 npages = 450 并添加 download_delay <strong class="jz ir"> = </strong> 2)你可以通过从我的<a class="ae kv" href="https://github.com/mGalarnyk/Python_Tutorials/tree/master/Scrapy/fundrazr/fundrazr" rel="noopener ugc nofollow" target="_blank"> github </a>下载文件来下载一个更大的文件，其中大约有 6000 个战役。该文件名为 MiniMorningScrape.csv(这是一个大文件)。</p><figure class="lu lv lw lx gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ns"><img src="../Images/48a23ec0143257cc0b2dc514ad2ed63f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bVE-wShga0SRq5OFyKam2g.png"/></div></div><figcaption class="jt ju gj gh gi jv jw bd b be z dk">Roughly 6000 Campaigns Scraped</figcaption></figure><h2 id="c31b" class="kw kx iq bd ky kz la dn lb lc ld dp le ki lf lg lh km li lj lk kq ll lm ln lo bi translated">结束语</h2><p id="7f34" class="pw-post-body-paragraph jx jy iq jz b ka lp kc kd ke lq kg kh ki lr kk kl km ls ko kp kq lt ks kt ku ij bi translated">创建数据集可能需要大量的工作，这通常是学习数据科学时被忽略的一部分。有一点我们没有注意到，虽然我们收集了大量数据，但我们仍然没有清理足够的数据来进行分析。不过这是另一篇博文的内容。如果你对本教程有任何问题或想法，欢迎通过<a class="ae kv" href="https://www.youtube.com/watch?v=O_j3OTXw2_E" rel="noopener ugc nofollow" target="_blank"> YouTube </a>或<a class="ae kv" href="https://twitter.com/GalarnykMichael" rel="noopener ugc nofollow" target="_blank"> Twitter </a>发表评论。如果你想学习如何使用 Pandas、Matplotlib 或 Seaborn 库，请考虑参加我的<a class="ae kv" href="https://www.linkedin.com/learning/python-for-data-visualization/value-of-data-visualization" rel="noopener ugc nofollow" target="_blank">数据可视化 LinkedIn 学习课程</a>。</p></div></div>    
</body>
</html>