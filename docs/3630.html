<html>
<head>
<title>A Beginner’s Guide on Sentiment Analysis with RNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RNN 情感分析初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e?source=collection_archive---------4-----------------------#2018-06-02">https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e?source=collection_archive---------4-----------------------#2018-06-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d722667bf3d8757c5594a4d0f46d7bac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TPRgQHHw_0Xg97_XKJy48A.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo Credit: Unsplash</figcaption></figure><p id="a790" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae la" href="https://en.wikipedia.org/wiki/Sentiment_analysis" rel="noopener ugc nofollow" target="_blank">情感分析</a>大概是<a class="ae la" href="https://en.wikipedia.org/wiki/Natural-language_processing" rel="noopener ugc nofollow" target="_blank">自然语言处理</a>中最常见的应用之一。客户服务工具情绪分析变得有多重要，我就不用强调了。所以在这里，我们将使用<a class="ae la" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络</a>在<a class="ae la" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank"> IMDB 数据集</a>中训练一个分类器电影评论。如果你想深入深度学习进行情感分析，这是一篇很好的<a class="ae la" href="https://arxiv.org/ftp/arxiv/papers/1801/1801.07883.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><h1 id="a8a7" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据</h1><p id="52e8" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">我们将使用递归神经网络，特别是<a class="ae la" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">lstm</a>，在<a class="ae la" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>中执行情感分析。方便的是，Keras 有一个内置的 IMDb 电影评论数据集，我们可以使用。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="2c4b" class="mn lc iq mj b gy mo mp l mq mr">from keras.datasets import imdb</span></pre><p id="c897" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">设置训练和测试数据中的词汇大小和负载。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="6e2c" class="mn lc iq mj b gy mo mp l mq mr">vocabulary_size = 5000</span><span id="db30" class="mn lc iq mj b gy ms mp l mq mr">(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)<br/>print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))</span></pre><p id="5b07" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">加载了 25000 个训练样本的数据集，25000 个测试样本</em> </strong></p><p id="44f0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">检查样本评论及其标签。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="940b" class="mn lc iq mj b gy mo mp l mq mr">print('---review---')<br/>print(X_train[6])<br/>print('---label---')<br/>print(y_train[6])</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mu"><img src="../Images/9aa1a6cb6d53acdfc6009a21168f401f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CiWuVTBsDfG6Wo9siICG0Q.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1</figcaption></figure><p id="b515" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">注意，评论被存储为一个整数序列。这些是预先分配给单个单词的单词 id，标签是一个整数(0 表示负，1 表示正)。</p><p id="4e97" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们可以用<code class="fe mv mw mx mj b">imdb.get_word_index()</code>返回的字典把复习映射回原来的单词。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1181" class="mn lc iq mj b gy mo mp l mq mr">word2id = imdb.get_word_index()<br/>id2word = {i: word for word, i in word2id.items()}<br/>print('---review with words---')<br/>print([id2word.get(i, ' ') for i in X_train[6]])<br/>print('---label---')<br/>print(y_train[6])</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi my"><img src="../Images/b790dddd9d9056a48403ee442e5131c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3aFMiuQp2xUpYHxKu8qxXQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2</figcaption></figure><p id="ca4d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">最大评论长度和最小评论长度。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="e145" class="mn lc iq mj b gy mo mp l mq mr">print('Maximum review length: {}'.format(<br/>len(max((X_train + X_test), key=len))))</span></pre><p id="3727" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">最大评论长度:2697 </em> </strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="7474" class="mn lc iq mj b gy mo mp l mq mr">print('Minimum review length: {}'.format(<br/>len(min((X_test + X_test), key=len))))</span></pre><p id="9da9" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">最小审核长度:14 </em> </strong></p><h1 id="ae28" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">焊盘序列</strong></h1><p id="9acc" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">为了将这些数据输入到我们的 RNN 中，所有输入文档必须具有相同的长度。我们将通过截断较长的评论并用空值(0)填充较短的评论，将最大评论长度限制为 max_words。我们可以使用 Keras 中的 pad_sequences()函数来实现这一点。现在，将 max_words 设置为 500。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="f20e" class="mn lc iq mj b gy mo mp l mq mr">from keras.preprocessing import sequence</span><span id="893f" class="mn lc iq mj b gy ms mp l mq mr">max_words = 500<br/>X_train = sequence.pad_sequences(X_train, maxlen=max_words)<br/>X_test = sequence.pad_sequences(X_test, maxlen=max_words)</span></pre><h1 id="4bed" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">设计一个用于情感分析的 RNN 模型</strong></h1><p id="214b" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">我们开始在下面的代码单元中构建我们的模型架构。我们从 Keras 中导入了一些你可能需要的层，但是你可以随意使用任何你喜欢的层/转换。</p><p id="f686" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">记住我们的输入是最大长度= max_words 的单词序列(技术上是整数单词 id)，我们的输出是二元情感标签(0 或 1)。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="6ef4" class="mn lc iq mj b gy mo mp l mq mr">from keras import Sequential<br/>from keras.layers import Embedding, LSTM, Dense, Dropout</span><span id="99a7" class="mn lc iq mj b gy ms mp l mq mr">embedding_size=32<br/>model=Sequential()<br/>model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))<br/>model.add(LSTM(100))<br/>model.add(Dense(1, activation='sigmoid'))</span><span id="a3cb" class="mn lc iq mj b gy ms mp l mq mr">print(model.summary())</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mz"><img src="../Images/29dbe891a9a163a75bd36e9dc76bf309.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ZYMVNcq1ckk3ptfYzAW8A.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3</figcaption></figure><p id="8ee6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">总之，我们的模型是一个简单的 RNN 模型，具有 1 个嵌入层、1 个 LSTM 层和 1 个致密层。总共需要训练 213，301 个参数。</p><h1 id="8749" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">训练和评估我们的模型</strong></h1><p id="613d" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">我们首先需要通过指定我们在训练时想要使用的损失函数和优化器，以及我们想要测量的任何评估指标来编译我们的模型。指定适当的参数，包括至少一个度量“准确性”。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="fd77" class="mn lc iq mj b gy mo mp l mq mr">model.compile(loss='binary_crossentropy', <br/>             optimizer='adam', <br/>             metrics=['accuracy'])</span></pre><p id="4ff0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">一旦编译完成，我们就可以开始培训过程了。我们必须指定两个重要的训练参数——批量大小和训练时期的数量，它们与我们的模型架构一起决定了总的训练时间。</p><p id="7e7a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">训练可能需要一段时间，所以喝杯咖啡，或者去跑步更好！</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="7323" class="mn lc iq mj b gy mo mp l mq mr">batch_size = 64<br/>num_epochs = 3</span><span id="c48c" class="mn lc iq mj b gy ms mp l mq mr">X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]<br/>X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]</span><span id="70ca" class="mn lc iq mj b gy ms mp l mq mr">model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi na"><img src="../Images/9a1c24667c061ffacef18a0fb3a5c935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KAxtWSwhtp_nVeoUY4MKFA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4</figcaption></figure><p id="3fb0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">一旦我们训练了我们的模型，就该看看它在看不见的测试数据上表现如何了。</p><p id="ba01" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果我们通过度量=['accuracy']，scores[1]将对应于准确性</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="0a94" class="mn lc iq mj b gy mo mp l mq mr">scores = model.evaluate(X_test, y_test, verbose=0)<br/>print('Test accuracy:', scores[1])</span></pre><p id="0038" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="mt">测试精度:0.86964 </em> </strong></p><h1 id="3a96" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">摘要</h1><p id="15fb" class="pw-post-body-paragraph kc kd iq ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">有几种方法可以用来建立我们的模型。我们可以通过试验不同的架构、层和参数来继续尝试和提高我们模型的准确性。如果不花太多时间训练，我们能有多好？我们如何防止过度拟合？</p><p id="88c3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">源代码可以在<a class="ae la" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Sentiment%20Analysis%20with%20RNN.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>找到。期待听到反馈或问题。</p><p id="b479" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">参考:<a class="ae la" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank"> Udacity — NLP </a></p></div></div>    
</body>
</html>