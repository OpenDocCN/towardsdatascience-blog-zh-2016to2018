# 十二月版

> 原文：<https://towardsdatascience.com/december-edition-80d8992a0fc?source=collection_archive---------15----------------------->

## **我们今年阅读量最大的帖子**

![](img/d47b027e0aab09ddab019db46cb2525a.png)

[Join us as an Editorial Associate](/join-us-as-an-editorial-associate-of-towards-data-science-766cdd74d13e)

我们希望你有一个伟大的一年。在 2018 年最后一期月刊中，我们为您带来了本年度从[到数据科学](https://towardsdatascience.com/)的最热门帖子。如果您最近加入了我们的行列，或者已经阅读了我们的文章很长一段时间，我们将为您带来我们最新的编辑精选:

1.帮助您解开关键概念的谜团，了解更多关于数据科学的知识

2.鼓励您与我们一起发布或加入我们的数据科学社区

3.帮助您在工作场所、下一次黑客马拉松或大学中应用数据科学

[James Le](/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies-dde4edffae11) 发布 2018 年，概述面向初学者的机器学习 10 大算法。2 月 [Nicklas Donges](/the-random-forest-algorithm-d457d499ffcd) 带我们踏上了一段随机森林和决策树的旅程，而 [Susan Li](/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f) 演示了如何使用 Python 中的 SciKit-Learn 库进行多类文本分类。今年 3 月， [Jonny Brooks-Bartlett 的](/why-so-many-data-scientists-are-leaving-their-jobs-a1f0329d7ea4)讨论了“现实与期望”，提到了为什么这么多数据科学家离职。

[Eugenio culrciello](/the-fall-of-rnn-lstm-2d1594c74ce0)在四月为我们带来了递归神经网络的兴衰。5 月，[詹姆斯·洛伊](/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6)用一篇精彩的介绍使用 Python 构建神经网络的文章引起了希拉里·梅森的转发，并且在[西蒙·格林曼的](/who-is-going-to-make-money-in-ai-part-i-77a2f30b8cef)文章中比较了最佳人工智能芯片和人工智能平台云解决方案。最近在 7 月， [Michael Galarnyk 的](/how-to-build-a-data-science-portfolio-5f566517c79c)关于如何构建一个伟大的数据科学组合的文章激励了我和数据科学界的许多人。

2018 年还剩下四周，我们希望这些文章真的像一瓶香槟一样“流行”,因为无论你在世界的哪个角落，都欢迎你的新年——我们要求你再次或第一次阅读这些文章。一定要把它们收藏起来，以备将来参考，并在朋友和同事之间分享这些文章。分享就是关爱！

[Wendy Wong](https://medium.com/u/68b80db0d4ab?source=post_page-----80d8992a0fc--------------------------------) ，TDS 编辑。

## [这就是这么多数据科学家离职的原因](/why-so-many-data-scientists-are-leaving-their-jobs-a1f0329d7ea4)

由 [Jonny Brooks-Bartlett](https://medium.com/u/c6ab8048de41?source=post_page-----80d8992a0fc--------------------------------) — 8 分钟阅读

**是的，**我是数据科学家，**是的，**你确实没看错标题，但总得有人说出来。我们读到了很多关于数据科学是 21 世纪最性感的工作，以及作为数据科学家可以赚到诱人的钱的故事，这似乎是绝对的梦想工作。

## [机器学习新手的 10 大算法之旅](/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies-dde4edffae11)

由詹姆斯·勒 — 11 分钟读完

在机器学习中，有一个叫做“没有免费的午餐”的定理。简而言之，它指出没有一种算法对每个问题都是最好的，并且它特别适用于监督学习(即预测建模)。

## [数据科学家需要了解的 5 种聚类算法](/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)

由乔治·赛义夫 — 11 分钟读完

聚类是一种涉及数据点分组的机器学习技术。给定一组数据点，我们可以使用聚类算法将每个数据点分类到特定的组中。

## [如何用 Python 从零开始构建自己的神经网络](/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6)

由詹姆斯·洛伊 — 6 分钟读完

作为我个人更好地理解深度学习之旅的一部分，我决定在没有 TensorFlow 这样的深度学习库的情况下，从头构建一个神经网络。我认为，理解神经网络的内部工作对任何有抱负的数据科学家都很重要。

## [随机森林算法](/the-random-forest-algorithm-d457d499ffcd)

由[尼克拉斯·东格斯](https://medium.com/u/8a23f092a330?source=post_page-----80d8992a0fc--------------------------------) — 8 分钟阅读

随机森林是一种灵活、易于使用的机器学习算法，即使没有超参数调整，在大多数情况下也能产生很好的结果。它也是最常用的算法之一，因为它简单，而且可以用于分类和回归任务。

## [RNN/LSTM 的陷落](/the-fall-of-rnn-lstm-2d1594c74ce0)

通过[Eugenio culrciello](https://medium.com/u/e53b1a2a902f?source=post_page-----80d8992a0fc--------------------------------)—9 分钟阅读

我们爱上了循环神经网络(RNN)、长短期记忆(LSTM)及其所有变体。现在是时候放下它们了！

## [如何构建数据科学产品组合](/how-to-build-a-data-science-portfolio-5f566517c79c)

由迈克尔·加拉尼克 — 17 分钟读完

数据科学怎么找工作？了解足够的统计学、机器学习、编程等知识以便能够找到工作是很困难的。我最近发现的一件事是，相当多的人可能拥有找工作所需的技能，但没有作品集。

## [使用 Scikit-Learn 进行多类文本分类](/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f)

由[苏珊李](https://medium.com/u/731d8566944a?source=post_page-----80d8992a0fc--------------------------------) — 11 分钟读完

文本分类在商业领域有很多应用。例如，新闻故事通常是按主题组织的；内容或产品通常按类别进行标记；用户可以根据他们在网上谈论产品或品牌的方式进行分类…

## [超参数调整 Python 中的随机森林](/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)

购买[威廉·科尔森](https://medium.com/u/e2f299e30cb9?source=post_page-----80d8992a0fc--------------------------------) — 12 分钟阅读

因此，我们建立了一个随机森林模型来解决我们的机器学习问题(也许通过遵循这个[端到端指南](/random-forest-in-python-24d0893d51c0))，但我们对结果并不太满意。我们有什么选择？

我们也感谢最近加入我们的所有伟大的新作家，[瓦伦蒂诺·康斯坦蒂努](https://medium.com/u/ac11031a57de?source=post_page-----80d8992a0fc--------------------------------)，[杰森·张硕](https://medium.com/u/aa63116d2ddf?source=post_page-----80d8992a0fc--------------------------------)，[奥斯卡·奈格](https://medium.com/u/c510ccc9027c?source=post_page-----80d8992a0fc--------------------------------)，[艾琳娜·尼西奥蒂](https://medium.com/u/98ae8c5c242e?source=post_page-----80d8992a0fc--------------------------------)，[约翰·布朗林](https://medium.com/u/2e882c5596c3?source=post_page-----80d8992a0fc--------------------------------)，[约翰·哈特奎斯特](https://medium.com/u/598c5512bf7f?source=post_page-----80d8992a0fc--------------------------------)，[梅兰妮·曾](https://medium.com/u/158b346f0342?source=post_page-----80d8992a0fc--------------------------------)，[亚当·拉德兹谢夫斯基](https://medium.com/u/21c4f2259907?source=post_page-----80d8992a0fc--------------------------------)，[艾丹·莫里森](https://medium.com/u/2f083c4e4af8?source=post_page-----80d8992a0fc--------------------------------)，[罗曼·博蒙特](https://medium.com/u/3781afe97e98?source=post_page-----80d8992a0fc--------------------------------) [](https://medium.com/u/3781afe97e98?source=post_page-----80d8992a0fc--------------------------------) [萨拉·沃尔夫](https://medium.com/u/1d66e5a14198?source=post_page-----80d8992a0fc--------------------------------)，[潘卡杰·马图尔](https://medium.com/u/1538b22688e3?source=post_page-----80d8992a0fc--------------------------------)，[萨比亚萨奇·萨胡](https://medium.com/u/5834a79f9d20?source=post_page-----80d8992a0fc--------------------------------)，[阮·范德梅尔韦](https://medium.com/u/54a794971b51?source=post_page-----80d8992a0fc--------------------------------)，[伊拉·科恩](https://medium.com/u/201be7d7fdcb?source=post_page-----80d8992a0fc--------------------------------)，[MJ·巴赫马尼](https://medium.com/u/151eeecff327?source=post_page-----80d8992a0fc--------------------------------)，[乌里·梅尔哈夫](https://medium.com/u/2dc5253a1da3?source=post_page-----80d8992a0fc--------------------------------)，[阿米蒂·拉蒂](https://medium.com/u/592798cbd74b?source=post_page-----80d8992a0fc--------------------------------)，[梁兆兵](https://medium.com/u/23b962bce579?source=post_page-----80d8992a0fc--------------------------------)，[罗伯特·桑多](https://medium.com/u/3b43a321fadd?source=post_page-----80d8992a0fc--------------------------------)， 我们邀请你看看他们的简介，看看他们的工作。