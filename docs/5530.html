<html>
<head>
<title>Calculating Gradient Descent Manually</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">手动计算梯度下降</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/calculating-gradient-descent-manually-6d9bee09aa0b?source=collection_archive---------1-----------------------#2018-10-24">https://towardsdatascience.com/calculating-gradient-descent-manually-6d9bee09aa0b?source=collection_archive---------1-----------------------#2018-10-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d53f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一步一步:神经网络背后的数学</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f49d09015716911b443fc27ec36fd535.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHWSlOR5B_PouhRDirZgAQ.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Title image: <a class="ae kv" href="https://c1.staticflickr.com/2/1834/42271822770_6d2a1d533f_b.jpg" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="574c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是我们的问题。我们有一个只有一层(为了简单起见)和一个损失函数的神经网络。这一层是一个简单的全连接层，只有一个神经元，许多权重<em class="ls"> w₁，w₂，w₃ </em> …，一个偏置<em class="ls"> b </em>，以及一个 ReLU 激活。我们的损失函数是常用的均方误差(MSE)。知道了我们的网络和损失函数，我们如何调整权重和偏差来最小化损失？</p><p id="337f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9">第一部分</a>中，我们了解到，我们必须找到损失(或成本)函数的斜率，以便将其最小化。我们发现我们的成本函数是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lt"><img src="../Images/2a6a7e17495d3b8a7ea7ea8d431ebf73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qb-m0gLn8X-aWnrJMqiNhA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 1: Cost function</figcaption></figure><p id="efc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在第二部分中，我们学习了如何求偏导数。这很重要，因为这个函数中有不止一个参数(变量)可以调整。我们需要找到成本函数相对于权重和偏差的导数，偏导数就起作用了。</p><p id="10aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-d002440227fb">第三部分</a>中，我们学习了如何求向量方程的导数。成本函数中的权重和偏差都是向量，因此学习如何计算包含向量的函数的导数是非常重要的。</p><p id="90c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们终于有了找到成本函数的导数(斜率)所需的所有工具！</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="1a57" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">神经元的梯度</h1><p id="17b5" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">我们需要逐步解决这个问题。让我们首先找到单个神经元相对于权重和偏差的梯度。</p><p id="9217" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们神经元的功能(伴随着激活)是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/efc5caffd7bec4efdc4a5339e264327f.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*UhqKOion6KFQ5GiOD7Q_EA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 2: Our neuron function</figcaption></figure><p id="be26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中它将<strong class="ky ir"> x </strong>作为输入，将其与权重<strong class="ky ir"> w </strong>相乘，并添加偏差<em class="ls"> b </em>。</p><p id="b6b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个函数实际上是其他函数的组合。如果我们让<em class="ls">f(</em><strong class="ky ir">x</strong><em class="ls">)=</em><strong class="ky ir">w∙x</strong><em class="ls">+b，</em>，<em class="ls"> g(x)=max(0，x) </em>，那么我们的函数就是<em class="ls">神经元(</em><strong class="ky ir">x</strong><em class="ls">)= g(f(</em><strong class="ky ir">x</strong><em class="ls">)</em>。我们可以用向量链法则来求这个函数合成的导数！</p><p id="5747" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们神经元的衍生物很简单:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/5d1fdf0b59fdee41ea160831e465f845.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*7bRypN6n1viYKfnpfoxPNg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 3: Derivative of our neuron function by the vector chain rule</figcaption></figure><p id="0eb6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<em class="ls">z = f(</em><strong class="ky ir">x</strong><em class="ls">)=</em><strong class="ky ir">w∙x</strong><em class="ls">+b</em>。</p><p id="2362" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个导数有两个部分:相对于<strong class="ky ir"> w </strong>的<em class="ls"> z </em>的偏导数，以及相对于<em class="ls"> z </em>的<em class="ls">神经元(z) </em>的偏导数。</p><h2 id="1764" class="na mc iq bd md nb nc dn mh nd ne dp ml lf nf ng mn lj nh ni mp ln nj nk mr nl bi translated">z 相对于<strong class="ak"> w </strong>的偏导数是多少？</h2><p id="fe36" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated"><em class="ls"> z </em>有两部分:<strong class="ky ir"> w∙x </strong>和<em class="ls"> +b </em>。先来看<strong class="ky ir"> w∙x </strong>。</p><p id="66f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> w∙x </strong>，或者说点积，实际上就是向量中每个元素的元素乘法的总和。换句话说:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/3034ec3f6363fbfcb37f9d95ea492846.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*p_75yunNKwum5kyzCusEQA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 4: Expanded version of <strong class="bd nn">w∙x</strong>, or the dot product</figcaption></figure><p id="f043" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这又是一个函数的组合，所以我们可以写成<strong class="ky ir">v</strong>=<strong class="ky ir">w</strong>⊗<strong class="ky ir">x</strong>和<em class="ls"> u=sum( </em> <strong class="ky ir"> v </strong> <em class="ls">)。</em>我们试图找出<em class="ls"> u </em>相对于<strong class="ky ir"> w </strong>的导数。在第三部分中，我们已经学习了这两个函数——逐元素的乘法和求和。它们的衍生物是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/bb889ef94e0cedd94ab581d566cecb13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*QNln41De03seD_xqsw6Jww.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/402431fcb61a52b3b0ae16e7db895f67.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*LRadecDMRJI0_UGNjSMNFg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 5: Derivative of u with respect to v and derivative of v with respect to w; where u=sum(<strong class="bd nn">w</strong>⊗<strong class="bd nn">x</strong>)</figcaption></figure><p id="2f5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(如果您不记得它们是如何推导出来的，请回头查看)</p><p id="29ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，由矢量链法则可知:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/76e3df8ebc9e294e1b7868f674cf4dba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*DipVJMX-iN7CY6EhJBMztw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 6: Derivative of u with respect to w</figcaption></figure><p id="b47d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！现在，让我们找出 z= <em class="ls"> u+b </em>的导数，其中 u= <strong class="ky ir"> w∙x </strong>相对于权重<strong class="ky ir"> w </strong>和偏差<em class="ls"> b. </em>记住，函数相对于不在该函数中的变量的导数为零，因此:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/73e468ff16b2f0d2553cbc67a68f936b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*x6Pmjwv5AhCadmkloHTvHQ.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/c896e19232afbe150335c4844b371428.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l7T0RsIx9C9lW4-zrnTI5Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 7: Derivative of z with respect to the weights and biases, where z=sum(<strong class="bd nn">w</strong>⊗<strong class="bd nn">x</strong>)+b</figcaption></figure><p id="71b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！这两个是 u 关于权重和偏差的导数。</p><h2 id="9d42" class="na mc iq bd md nb nc dn mh nd ne dp ml lf nf ng mn lj nh ni mp ln nj nk mr nl bi translated">神经元(z)对 z 的偏导数是多少？</h2><p id="c373" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated"><em class="ls">神经元(z)=max(0，z)=max(0，sum(</em><strong class="ky ir">w</strong>⊗<strong class="ky ir">x</strong><em class="ls">)+b)。</em></p><p id="d3e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">max(0，z)函数只是将所有负值视为 0。因此，该图看起来像这样:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/5cc13e99ad3484b31bbc8cbbb8dd2b5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*l801tAGF1QL2oOWEHBGbuQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 8: Max(0,z) // <a class="ae kv" href="https://www.desmos.com/calculator" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="c624" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查看该图，我们可以立即看到导数是分段函数:对于所有小于或等于 0 的 z 值，导数为 0，对于所有大于 0 的 z 值，导数为 1，或者:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/e2967b1f9922fb70aedfa7f06190a9d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*KH_1ia2IZ2UTplbn1n98nQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 9: Derivative of max(0,z)</figcaption></figure><p id="d00e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们有了这两部分，我们可以将它们相乘，得到神经元的导数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/5d1fdf0b59fdee41ea160831e465f845.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*7bRypN6n1viYKfnpfoxPNg.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4d1171e87ca80841e168085bfff530c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*HiGfaZNHAgHv-Ej04eu6qw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 10: Derivative with respect to the weights of our neuron: <em class="nw">max(0, sum(</em><strong class="bd nn">w</strong>⊗<strong class="bd nn">x</strong><em class="nw">)+b)</em></figcaption></figure><p id="250f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并将 z= <strong class="ky ir"> w∙x </strong> <em class="ls"> +b </em>代入:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/180d81255c960bec509d3f28d1151ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*A9kJfk6xMAC-MZfmTpkqew.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 11: Substituting <strong class="bd nn">w∙x</strong><em class="nw">+b for z</em></figcaption></figure><p id="8667" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">瞧啊。我们得到了神经元相对于其重量的导数！类似地，我们可以对偏差使用相同的步骤:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/10af0cd1671846bba8009430fd38117a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*jLcDETNi1LyDm7NgK4Yv4w.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 12: Derivative of our neuron with respect to bias</figcaption></figure><p id="1fa3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就对了。我们现在有了神经网络中一个神经元的梯度！</p><h1 id="fce5" class="mb mc iq bd md me nz mg mh mi oa mk ml jw ob jx mn jz oc ka mp kc od kd mr ms bi translated">损失函数的梯度</h1><p id="e0b2" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">在<a class="ae kv" href="https://medium.com/@reina.wang/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9" rel="noopener">第 1 部分</a>中定义的我们的损失函数是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/114d2ba6f0fd1460ef24960b048821d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1A0o9UOeA4oqebLjloheyA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 13: Loss Function</figcaption></figure><p id="cdb0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以立即确定这是一个函数的组合，它需要链式法则。我们将把中间变量定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/c751d7a7824f61ae84c8d206f85fd864.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*yOkXpMKJXmLJ8Rf_FB3WZQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 14: Intermediate variables for loss function</figcaption></figure><p id="6c63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">*注意，这里的<em class="ls"> u </em>和<em class="ls"> v </em>与上一节使用的<em class="ls"> u </em>和<em class="ls"> v </em>不同。</p><p id="3f0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们首先计算相对于重量<strong class="ky ir"> w </strong>的梯度。</p><h2 id="c80a" class="na mc iq bd md nb nc dn mh nd ne dp ml lf nf ng mn lj nh ni mp ln nj nk mr nl bi translated">相对于重量的梯度</h2><p id="07b8" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated"><em class="ls"> u </em>简单来说就是我们的神经元函数，我们之前已经解决了。因此:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/6ad3afe63601d98353fafc85baf707ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*0pCdA3EuwqDiOwvcyA9I8g.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 15: Derivative of u=<em class="nw">max(0, sum(</em><strong class="bd nn">w</strong>⊗<strong class="bd nn">x</strong><em class="nw">)+b) with respect to the weights</em></figcaption></figure><p id="1fc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls"> v(y，u) </em>简单来说就是<em class="ls"> y-u </em>。因此，我们可以利用分配性质，代入<em class="ls"> u </em>的导数，求出它的导数(相对于<strong class="ky ir"> w </strong>):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/988b6ca393295389b248c1a05b011add.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fe4YGoTe_tZGO2bl9sIdZQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 16: Derivative of v=y-u with respect to the weights</figcaption></figure><p id="248f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们需要找到整个成本函数相对于<strong class="ky ir"> w </strong>的导数。利用链式法则，我们知道:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/420ce2ed0455d5e48f877ba5657a3512.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*ofiLXnx_QrifZlrjHfL-EQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 17: Derivative of cost function</figcaption></figure><p id="fbe9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们先找到等式的第一部分，即<em class="ls"> C(v) </em>相对于<em class="ls"> v </em>的偏导数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/26b431e843830ccacc25e06dd34aa8fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ULVXvQc5v_y-wbAS3fSSCw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 18: Derivative of cost function with respect to v</figcaption></figure><p id="164a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上图(图片<strong class="ky ir"> 16 </strong>，我们知道<em class="ls"> v </em>相对于<strong class="ky ir"> w </strong>的导数。为了求 C(v)的偏导数，我们将两个导数相乘:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/19bcb22edd626d1d60556f3809bc7478.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*aRoKnd4k8QlhrtlGXKwsFQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 19: Derivative of cost function with respect to <strong class="bd nn">w</strong></figcaption></figure><p id="d525" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在用<em class="ls"> y-u </em>代替<em class="ls"> v </em>，用<em class="ls"> max(0，</em> <strong class="ky ir"> w∙x </strong> + <em class="ls"> b) </em>代替<em class="ls"> u </em>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/56ce098605a1d4dfab300eb9bedf752d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EtNBTb3DLlgsiHk1ahTSxg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 20: Derivative of cost function with respect to <strong class="bd nn">w</strong></figcaption></figure><p id="9c2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于<em class="ls"> max </em>函数在我们分段函数的第二行，这里<strong class="ky ir"> w∙x </strong> + <em class="ls"> b </em>大于 0，所以<em class="ls"> max </em>函数将总是简单地输出<strong class="ky ir"> w∙x </strong> + <em class="ls"> b </em>的值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/665382aaea5fdcdb77ae31db7b632f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tupZ8gQcqgtSlG1OeGQPBA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 21: Derivative of cost function with respect to <strong class="bd nn">w</strong></figcaption></figure><p id="f3bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们可以将求和移到分段函数中，并稍微整理一下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/6a82c91cc4b7f6e43774aa9f8b882d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*C_D3yPPGYLayq0wvGS4-bg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 22: Derivative of cost function with respect to <strong class="bd nn">w</strong></figcaption></figure><p id="94ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！我们对重量求导！然而，这意味着什么呢？</p><p id="3e45" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> w∙x </strong> + <em class="ls"> b-y </em>可以解释为误差项——神经网络的预测输出与实际输出之间的差异。如果我们称这个误差项为 ei，我们的最终导数为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/ca10a5013324cd166dbe981a5a17af00.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*h8cZDvxLw7DG8cUf4nmmSg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 23: Derivative of cost function with respect to <strong class="bd nn">w </strong>represented with an error term</figcaption></figure><p id="d99b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，误差越大，导数越高。换句话说，导数代表斜率，或者说，为了使误差最小化，我们必须移动多少重量。如果我们的神经网络刚刚开始训练，并且具有非常低的精度，则误差将会很高，因此导数也将会很大。因此，我们将不得不迈出一大步，以尽量减少我们的错误。</p><p id="66cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可能会注意到，这个梯度指向更高的成本，这意味着我们不能将梯度添加到我们当前的权重中，这只会增加误差，并使我们远离局部最小值。因此，我们必须用导数减去当前重量，以便更接近最小化损失函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/40c28297e65ecaaf396889df824058cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*_k7FnT-aNX2eL4YHzXIPjg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 24: Gradient descent function</figcaption></figure><p id="9a57" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里η代表学习率，我们作为程序员可以设定。学习率越大，步幅越大。然而，设置太大的学习率可能会导致采取太大的步骤，并跳出局部最小值。更多信息，请查看<a class="ae kv" rel="noopener" target="_blank" href="/the-beginners-guide-to-gradient-descent-c23534f808fd">这篇关于梯度下降的文章</a>和<a class="ae kv" rel="noopener" target="_blank" href="/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163">这篇关于设置学习率的文章</a>。</p><h2 id="0264" class="na mc iq bd md nb nc dn mh nd ne dp ml lf nf ng mn lj nh ni mp ln nj nk mr nl bi translated">相对于偏差的梯度</h2><p id="7acc" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">同样，我们有中间变量:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/c751d7a7824f61ae84c8d206f85fd864.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*yOkXpMKJXmLJ8Rf_FB3WZQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 25: Intermediate variables for loss function</figcaption></figure><p id="4845" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还有<em class="ls"> u </em>相对于之前计算的偏差的导数值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/51bf295068990907217defef76993fb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*xezbbgr3AFRqE75AAGIcbg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 26: Derivative of u with respect to the bias.</figcaption></figure><p id="e70e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，我们可以利用分配性质，代入<em class="ls"> u </em>的导数，求出<em class="ls"> v </em>相对于<em class="ls"> b </em>的导数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/6a8643ab46036f3af722e04119186384.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vf6pNe2BjMIQQetfbwqAgw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 27: Derivative of v with respect to the bias</figcaption></figure><p id="01d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，我们可以使用向量链规则来找到<em class="ls"> C </em>的导数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/9bdf542b771c32650698fbed23ecca9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*x4ZTx67cpOF9FaNvUvqDkg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 28: Derivative of cost function with respect to the bias</figcaption></figure><p id="da04" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls"> C </em>相对于<em class="ls"> v </em>的导数与我们为重量计算的导数相同:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/26b431e843830ccacc25e06dd34aa8fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ULVXvQc5v_y-wbAS3fSSCw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 29: Derivative of cost function with respect to v</figcaption></figure><p id="1749" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将两者相乘得到<em class="ls"> C </em>相对于<em class="ls"> b </em>的导数，用<em class="ls"> y-u </em>代替<em class="ls"> v </em>，用<em class="ls"> max(0，</em> <strong class="ky ir"> w∙x </strong> + <em class="ls"> b) </em>代替<em class="ls"> u </em>，得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/f5cbfe2c24d1088bc1923301c6e1bf26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PbOTn0Lx8oeE615g39d2cQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 30: Derivative of cost function with respect to the bias</figcaption></figure><p id="c0f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，因为第二行明确声明了<strong class="ky ir"> w∙x </strong> + <em class="ls"> b </em> &gt; 0，所以<em class="ls"> max </em>函数将始终只是<strong class="ky ir"> w∙x </strong> + <em class="ls"> b. </em>的值</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/f8af3c170e3f3c6f208163f9199962b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*sgPizLRj-5iR3FRey-Y1AQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 31: Derivative of cost function with respect to the bias</figcaption></figure><p id="d625" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就像之前一样，我们可以代入一个误差项，e = <strong class="ky ir"> w∙x </strong> + <em class="ls"> b-y </em>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/a9307dc35c38b8b688c916803a547bd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*ZFyh4iSjvkjQ3pb0TxHn0g.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">image 32: Derivative of cost function with respect to the bias, represented with an error term</figcaption></figure><p id="1182" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就像关于权重的导数一样，这个梯度的大小也与误差成比例:误差越大，我们向局部最小值前进的步伐就越大。它还指向更高成本的方向，这意味着我们必须从当前值中减去梯度，以更接近局部最小值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/a92602c064cb54d8e8127d17a715e389.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*I3ebCHVf0SDUz0xikB3FMg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 33: Gradient descent function for the bias</figcaption></figure></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="769c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">恭喜你写完这篇文章！这很可能不是一个容易的阅读，但你一直坚持到最后，并成功地手动梯度下降！</p><p id="2a86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我在本系列第 1 部分中所说的，如果不理解每行代码背后的底层数学和计算，我们就无法真正理解“创建神经网络”的真正含义，也无法理解支持我们编写的每个函数的复杂性。</p><p id="8757" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这些方程和我的解释是有意义的，并帮助你更好地理解这些计算。如果你有任何问题或建议，不要犹豫，在下面留下评论！</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="5045" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您还没有，请在此阅读第 1、2 和 3 部分:</p><ul class=""><li id="06f9" class="ow ox iq ky b kz la lc ld lf oy lj oz ln pa lr pb pc pd pe bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9">第一部分:简介</a></li><li id="161e" class="ow ox iq ky b kz pf lc pg lf ph lj pi ln pj lr pb pc pd pe bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-ac15e178bbd">第二部分:偏导数</a></li><li id="8498" class="ow ox iq ky b kz pf lc pg lf ph lj pi ln pj lr pb pc pd pe bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-d002440227fb">第三部分:向量微积分</a></li></ul><p id="2076" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此处下载原论文<a class="ae kv" href="https://arxiv.org/abs/1802.01528" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="606c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你喜欢这篇文章，别忘了留下一些掌声！如果您有任何问题或建议，请在下面留下您的评论:)</p></div></div>    
</body>
</html>