<html>
<head>
<title>Neural Networks from Scratch. Easy vs hard</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络从无到有。容易还是困难</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-from-scratch-easy-vs-hard-b26ddc2e89c7?source=collection_archive---------4-----------------------#2018-08-17">https://towardsdatascience.com/neural-networks-from-scratch-easy-vs-hard-b26ddc2e89c7?source=collection_archive---------4-----------------------#2018-08-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/47a0498e38a57758ee66a1d5878d8990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t0x18Vw4cQYI9QK5_QbDQQ.png"/></div></div></figure><p id="3a60" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于那些不了解人工神经网络工作原理的人来说，人工神经网络就像魔法一样神奇。当你可以成为魔术师的时候，为什么要做一个观众！。我想讨论一下开始使用标准的机器学习库(<code class="fe kz la lb lc b">keras</code>)是多么容易，如果我们试图从零开始实现一个神经网络(<code class="fe kz la lb lc b">numpy</code>)以及一点点数学知识，它会变得多么有趣。</p><h2 id="d2c0" class="ld le it bd lf lg lh dn li lj lk dp ll km lm ln lo kq lp lq lr ku ls lt lu lv bi translated">有什么新鲜事！</h2><p id="bfa8" class="pw-post-body-paragraph kb kc it kd b ke lw kg kh ki lx kk kl km ly ko kp kq lz ks kt ku ma kw kx ky im bi translated">嗯，已经有几篇关于如何从头开始开发神经网络的文章了。但是，在大多数让初学者记住的文章中，实现了一个简单的网络，没有任何关于成本或激活功能的讨论。当手头的问题改变时，由于输入值的范围、激活函数的类型、误差函数，网络不能正常工作。所以，我们再深入挖掘一下。</p><p id="509a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这是我们将要做的。准备一些数据集，创建一个合适的网络体系结构，先用简单的方法实施该体系结构，然后用困难的方法实施。</p><h2 id="fc75" class="ld le it bd lf lg lh dn li lj lk dp ll km lm ln lo kq lp lq lr ku ls lt lu lv bi translated">数据准备</h2><p id="d906" class="pw-post-body-paragraph kb kc it kd b ke lw kg kh ki lx kk kl km ly ko kp kq lz ks kt ku ma kw kx ky im bi translated">让我们使用来自 sklearn 的数字数据集。它是一个多类分类。</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="09e3" class="ld le it lc b gy mj mk l ml mm">import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from sklearn.datasets import load_digits<br/>from sklearn.model_selection import train_test_split</span><span id="fbcb" class="ld le it lc b gy mn mk l ml mm">dig = load_digits()<br/>plt.gray()<br/>plt.matshow(dig.images[25])</span></pre><figure class="mb mc md me gt ju gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/256d659acd81378e69fba96fd8c6ab1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*aoaglFBz4lkjJO8KjjDmBQ.png"/></div></figure><p id="df95" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">看起来像迷你版的 mnist。应该没问题。<a class="ae mp" href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits" rel="noopener ugc nofollow" target="_blank">该数据集</a>包含<code class="fe kz la lb lc b">data</code>中的输入和<code class="fe kz la lb lc b">target</code>变量中的输出。<code class="fe kz la lb lc b">target</code>取值范围从 0 到 9。它必须被转换成一位热码表示，其中数组包含所有的零，除了索引值为 1。即，值 4 将被表示为[0，0，0，0，1，0，0，0，0]。<code class="fe kz la lb lc b">pandas</code>在单一功能中完成。由于误差函数的性质，这种多类分类问题必须被转换成用于训练 NN 模型的一个热点表示。</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="b448" class="ld le it lc b gy mj mk l ml mm">onehot_target = pd.get_dummies(dig.target)</span></pre><p id="a7d7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">就是这样！。数据集准备好了。现在，让我们把它分成训练和测试数据集。<code class="fe kz la lb lc b">train_test_split</code>也随机化实例</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="4d38" class="ld le it lc b gy mj mk l ml mm">x_train, x_val, y_train, y_val = train_test_split(dig.data, onehot_target, test_size=0.1, random_state=20)</span><span id="32d3" class="ld le it lc b gy mn mk l ml mm"># shape of x_train :(1617, 64)<br/># shape of y_train :(1617, 10)</span></pre><p id="f7b0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">训练集中有 1617 个实例。每个输入实例包含 64 个值的数组，相应的输出包含 10 个值的数组。</p><figure class="mb mc md me gt ju"><div class="bz fp l di"><div class="mq mr l"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Dataset preparation</figcaption></figure></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="c674" class="nd le it bd lf ne nf ng li nh ni nj ll nk nl nm lo nn no np lr nq nr ns lu nt bi translated">人工神经网络体系结构；</h1><p id="db13" class="pw-post-body-paragraph kb kc it kd b ke lw kg kh ki lx kk kl km ly ko kp kq lz ks kt ku ma kw kx ky im bi translated">让我们构建一个四层网络(输入，两个隐藏层，输出)。</p><p id="8548" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="nu">输入层- 64 个神经元(输入图像数组)<br/>隐藏层 1 - 128 个神经元(任意)<br/>隐藏层 2 - 128 个神经元(任意)<br/>输出层- 10 个神经元(输出单热数组)</em></p><figure class="mb mc md me gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/be5a932bf876a8f6da8268f362e1a77c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o7VCg1WILHZMVoPAALKWYg.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">ANN architecture</figcaption></figure></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="234c" class="nd le it bd lf ne nf ng li nh ni nj ll nk nl nm lo nn no np lr nq nr ns lu nt bi translated">keras——简单的方法:</h1><p id="9e59" class="pw-post-body-paragraph kb kc it kd b ke lw kg kh ki lx kk kl km ly ko kp kq lz ks kt ku ma kw kx ky im bi translated"><code class="fe kz la lb lc b">Keras</code>是一个非常棒的入门库。又快又简单。</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="78ce" class="ld le it lc b gy mj mk l ml mm">from keras.layers import Dense<br/>from keras.models import Sequential<br/>from keras.optimizers import RMSprop, Adadelta, Adam</span><span id="3f05" class="ld le it lc b gy mn mk l ml mm">model = Sequential()<br/>model.add(Dense(128, input_dim=x_train.shape[1], activation='sigmoid'))<br/>model.add(Dense(128, activation='sigmoid'))<br/>model.add(Dense(10, activation='softmax'))</span></pre><p id="900b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在我们的例子中，我们将建立一个完全连接的神经网络，这意味着当前层中的所有神经元都连接到下一层中的所有神经元。因此，我们将模型定义为<code class="fe kz la lb lc b">sequential</code>，并添加了三个连续的层。需要注意的是，没有必要为输入添加单独的层。当定义第一个隐藏层时，输入层将自动初始化。</p><p id="0444" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">该模型添加了具有 128 个神经元的第一个隐藏层，其中<code class="fe kz la lb lc b">input_dim</code>也指定了输入层的大小。第二个隐藏层也有同样的 128 个神经元。最后，输出层有 10 个神经元。</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="cbca" class="ld le it lc b gy mj mk l ml mm">model.summary()</span></pre><figure class="mb mc md me gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nw"><img src="../Images/111929d9fd53c73d0d6c048d7974e69a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAmsIpiCYD1Z7o3XOmty6A.png"/></div></div></figure><p id="4f6d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">模型不完整，没有指定成本函数和梯度下降优化。</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="5b36" class="ld le it lc b gy mj mk l ml mm">model.compile(optimizer=Adadelta(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])<br/>model.fit(x_train, y_train, epochs=50, batch_size=64)</span></pre><p id="53c6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">好吧。这是做这件事最简单的方法。总的来说，</p><figure class="mb mc md me gt ju"><div class="bz fp l di"><div class="mq mr l"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">ANN using keras</figcaption></figure></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="cb50" class="nd le it bd lf ne nf ng li nh ni nj ll nk nl nm lo nn no np lr nq nr ns lu nt bi translated">安白手起家——艰难之路</h1><p id="9d60" class="pw-post-body-paragraph kb kc it kd b ke lw kg kh ki lx kk kl km ly ko kp kq lz ks kt ku ma kw kx ky im bi translated">准备好。。这是一条艰难的路。该算法通过两个主要过程来训练模型。前馈和反向传播。前馈预测具有某些权重的给定输入的输出，反向传播通过调整权重来训练模型。因此，必须首先初始化权重。</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="f360" class="ld le it lc b gy mj mk l ml mm">import numpy as np</span><span id="94a6" class="ld le it lc b gy mn mk l ml mm">class MyNN:<br/>    def __init__(self, x, y):<br/>        self.input = x<br/>        self.output = y<br/>        neurons = 128       # neurons for hidden layers<br/>        self.lr = 0.5       # user defined learning rate<br/>        ip_dim = x.shape[1] # input layer size 64<br/>        op_dim = y.shape[1] # output layer size 10<br/>        self.w1 = np.random.randn(ip_dim, neurons) # weights<br/>        self.b1 = np.zeros((1, neurons))           # biases<br/>        self.w2 = np.random.randn(neurons, neurons)<br/>        self.b2 = np.zeros((1, neurons))<br/>        self.w3 = np.random.randn(neurons, op_dim)<br/>        self.b3 = np.zeros((1, op_dim))</span></pre><p id="49bf" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">从一组好的权重开始肯定会快速提供局部最小值。除了缓慢地达到局部最小值之外，坏的权重集有时可能永远不会收敛。为了打破对称性，初始权重应该被随机化。这些值不应该为零，而应该更接近于零，这样输出就不会激增。最好有正值也有负值，这样除了大小，方向也不同。所以用正态分布。将偏置向量初始化为零是可以的。</p><figure class="mb mc md me gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/b3ccb57480613fd9669be6f969c27ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*loA-nREDNnc91W934u0ziQ.png"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">source: <a class="ae mp" href="https://www.facebook.com/convolutionalmemes/" rel="noopener ugc nofollow" target="_blank">Machine learning memes for convolutional teens, facebook</a></figcaption></figure><h2 id="1b7e" class="ld le it bd lf lg lh dn li lj lk dp ll km lm ln lo kq lp lq lr ku ls lt lu lv bi translated">前馈</h2><figure class="mb mc md me gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ny"><img src="../Images/9b8332e73172ccf4b30c92faa0be79f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ikQ1d01SBTu2FEf4O0EEQ.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Forward pass</figcaption></figure><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="0c97" class="ld le it lc b gy mj mk l ml mm">def sigmoid(s):<br/>    return 1/(1 + np.exp(-s))</span><span id="7a07" class="ld le it lc b gy mn mk l ml mm"># for numerical stability, values are normalised<br/>def softmax(s):<br/>    exps = np.exp(s - np.max(s, axis=1, keepdims=True))<br/>    return exps/np.sum(exps, axis=1, keepdims=True)</span><span id="dda2" class="ld le it lc b gy mn mk l ml mm">def feedforward(self):<br/>    z1 = np.dot(self.x, self.w1) + self.b1<br/>    self.a1 = sigmoid(z1)<br/>    z2 = np.dot(self.a1, self.w2) + self.b2<br/>    self.a2 = sigmoid(z2)<br/>    z3 = np.dot(self.a2, self.w3) + self.b3<br/>    self.a3 = softmax(z3)</span></pre><p id="6690" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">前馈过程非常简单。<code class="fe kz la lb lc b">(input x weights) + bias</code>计算<code class="fe kz la lb lc b">z</code>并将其传递到包含特定激活函数的层中。这些激活函数产生输出<code class="fe kz la lb lc b">a</code>。当前层的输出将是下一层的输入，依此类推。如您所见，第一和第二隐藏层包含作为激活函数的<em class="nu"> sigmoid </em>函数，输出层包含作为激活函数的<em class="nu"> softmax </em>。softmax 产生的最终结果<code class="fe kz la lb lc b">a3</code>是神经网络的输出。</p><p id="6495" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">应用于该层的函数类型有很大的不同。Sigmoid 函数将输入压缩到(0，1)，Softmax 也做同样的事情。但是，Softmax 确保输出之和等于 1。在我们的输出中，我们想要测量每个类的输入的概率。例如，如果一个图像有 0.9 的概率是数字 5，那么在其他类中分配 0.1 的概率是明智的，这是由 softmax 完成的。</p><h2 id="f715" class="ld le it bd lf lg lh dn li lj lk dp ll km lm ln lo kq lp lq lr ku ls lt lu lv bi translated">反向传播</h2><figure class="mb mc md me gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nz"><img src="../Images/6ff626311696ef537642d7223716dc83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YdYeDl_UoQp_E5OWs97OOQ.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Backward pass</figcaption></figure><p id="e2bb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">好吧。Back-prop 可能看起来很棘手，因为你要处理多个层，多个权重，损失函数，梯度。不要担心，我们会用数学、直觉和代码实现来尝试它。从根本上来说，反向推进根据前馈输出和真实值计算误差。通过计算每层中的梯度，该误差被反向传播到所有的权重矩阵，并且这些权重被更新。听起来很简单对吧。嗯。让我们看看。</p><p id="2782" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">损失函数:</strong></p><p id="24e4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">成本函数和损失函数可以互换使用。严格地说，损失函数是针对单个实例计算的，而成本函数是针对整个网络的。成本函数是一种衡量网络与真实值相比表现如何的方法。实际上，我们永远不会在训练中的任何地方使用成本函数，但是我们必须计算成本函数 w.r.t .权重和偏差的梯度以进行更新。所以，计算损失只是为了看看，我们在每个时期做得有多好。</p><p id="5d91" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最常用的损失函数是均方误差。由于我们正在处理多类分类问题，输出将是一个概率分布。我们要把它和我们的真实值进行比较，这也是一个概率分布，找到误差。对于这种情况，强烈建议使用交叉熵作为代价函数。为什么因为，交叉熵函数能够计算两个概率分布之间的误差。</p><p id="334b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">连锁规则:</strong></p><p id="25a6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们考虑成本函数为 c，从前馈，我们知道</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="8427" class="ld le it lc b gy mj mk l ml mm">z = xw + b                 -&gt; z = function(w)<br/>a = sig(z) or softmax(z)   -&gt; a = function(z)<br/>c = -(y*log(a3))           -&gt; c = function(a)</span></pre><p id="07db" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，根据输出，我们所要做的就是找出误差，以及每个权重对输出的影响程度。换句话说，求成本函数 w.r.t w3 的导数。是的，反向传播就是用链式法则计算导数。</p><p id="2ce5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">外层:</strong></p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="93bb" class="ld le it lc b gy mj mk l ml mm">dc     dc    da3   dz3<br/>---  = --- . --- . ---<br/>dw3    da3   dz3   dw3</span><span id="2b36" class="ld le it lc b gy mn mk l ml mm">z3 = a2w3 + b3<br/>a3 = softmax(z3)</span><span id="7f96" class="ld le it lc b gy mn mk l ml mm">dz3/dw3 = a2<br/>da3/dz3 = softmax derivative(z3)<br/>dc/da3  = cost function derivative(a3) = -y/a3</span></pre><p id="8b7a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">令人惊讶的是，由于交叉熵经常与 softmax 激活函数一起使用，我们实际上不必计算这两个导数。因为，这些导数的某些部分相互抵消，正如这里<a class="ae mp" href="http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function" rel="noopener ugc nofollow" target="_blank">清楚解释的</a>。由此可见，<code class="fe kz la lb lc b">predicted value — real value</code>是他们产物的结果。</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="54b3" class="ld le it lc b gy mj mk l ml mm">Let, a3_delta be the product of these terms as it will be needed in the upcoming chain rules.<br/>           dc    da3<br/>a3_delta = --- . --- <br/>           da3   dz3</span><span id="f164" class="ld le it lc b gy mn mk l ml mm">Thus, a3_delta = a3-y (the error to be propagated)</span><span id="415f" class="ld le it lc b gy mn mk l ml mm">dc     <br/>---  = (a3 - y) . a2<br/>dw3    <br/>w3 = w3 - dc/dw3</span><span id="8540" class="ld le it lc b gy mn mk l ml mm">For changes in biases,<br/>dc     dc    da3   dz3<br/>---  = --- . --- . ---<br/>db3    da3   dz3   db3</span><span id="50ff" class="ld le it lc b gy mn mk l ml mm">dz3/db3 = 1. Rest is already calculated<br/>b3 = b3 - dc/db3 =&gt; b3 = b3 - a3_delta</span></pre><p id="4bbd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">隐藏图层:</strong></p><p id="234d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在隐藏层中，成本函数相对于隐藏层的偏导数也将遵循链式规则，因为成本函数是外层的函数。</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="b60b" class="ld le it lc b gy mj mk l ml mm">z2 = a1w2 + b2<br/>a2 = sigmoid(z2)</span><span id="0b02" class="ld le it lc b gy mn mk l ml mm">dc     dc    da2   dz2<br/>---  = --- . --- . ---<br/>dw2    da2   dz2   dw2</span><span id="6aaa" class="ld le it lc b gy mn mk l ml mm">dz2/dw2 = a1<br/>da2/dz2 = sigmoid_derv(z2)</span><span id="8282" class="ld le it lc b gy mn mk l ml mm">dc     dc    da3   dz3<br/>---  = --- . --- . --- =&gt; dc/da2 = a3_delta.w3<br/>da2    da3   dz3   da2</span><span id="a1ba" class="ld le it lc b gy mn mk l ml mm">w2 = w2 - dc/dw2<br/>and set a2_delta = dc/da2 . da2/dz2</span><span id="9eb4" class="ld le it lc b gy mn mk l ml mm">dc     dc    da2   dz2<br/>---  = --- . --- . ---<br/>db2    da2   dz2   db2</span><span id="7fb1" class="ld le it lc b gy mn mk l ml mm">dz2/db2 = 1<br/>b2 = b2 - dc/db2 =&gt; b2 = b2 - a2_delta</span></pre><p id="6229" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">成本函数 w.r.t w1 的导数也是如此</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="2c19" class="ld le it lc b gy mj mk l ml mm">z1 = x.w1 + b1<br/>a1 = sigmoid(z1)<br/>c  = a1.w2 + b2</span><span id="3031" class="ld le it lc b gy mn mk l ml mm">dc     dc    da1   dz1<br/>---  = --- . --- . ---<br/>dw1    da1   dz1   dw1</span><span id="f2f4" class="ld le it lc b gy mn mk l ml mm">dz1/dw1 = x<br/>da1/dz1 = sigmoid_derv(z1)</span><span id="d3a7" class="ld le it lc b gy mn mk l ml mm">dc     dc    da2   dz2<br/>---  = --- . --- . --- =&gt; dc/da1 = a2_delta.w2<br/>da1    da2   dz2   da1</span><span id="7830" class="ld le it lc b gy mn mk l ml mm">w1 = w1 - dc/dw1<br/>and set a1_delta = dc/da1 . da1/dz1</span><span id="1c3f" class="ld le it lc b gy mn mk l ml mm">dc     dc    da1   dz1<br/>---  = --- . --- . ---<br/>db1    da1   dz1   db1</span><span id="485f" class="ld le it lc b gy mn mk l ml mm">dz1/db1 = 1<br/>b1 = b1 - dc/db1 =&gt; b1 = b1 - a1_delta</span></pre><p id="fdf0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">收集所有上述方程，并把它放在一个单一的镜头将提供一个更好的概览。与向前传球相比，直觉上也是有道理的。</p><figure class="mb mc md me gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oa"><img src="../Images/96ece7ae7b0a643ef75a090e8e4a8193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ykRnl7_YLvDmsDGWip3-kA.png"/></div></div></figure><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="d1cb" class="ld le it lc b gy mj mk l ml mm">Feed forward equations:<br/>z1 = x.w1+b1<br/>a1 = sigmoid(z1)</span><span id="8d26" class="ld le it lc b gy mn mk l ml mm">z2 = a1.w2+b2<br/>a2 = sigmoid(z2)</span><span id="743c" class="ld le it lc b gy mn mk l ml mm">z3 = a2.w3+b3<br/>a3 = softmax(z3)</span><span id="e41a" class="ld le it lc b gy mn mk l ml mm">Back propagation equations:</span><span id="f9b4" class="ld le it lc b gy mn mk l ml mm">There is no z3_delta and softmax_derv(a3), as explained before.<br/>a3_delta = a3-y    </span><span id="ef73" class="ld le it lc b gy mn mk l ml mm">z2_delta = a3_delta.w3.T<br/>a2_delta = z2_delta.sigmoid_derv(a2)</span><span id="2e5e" class="ld le it lc b gy mn mk l ml mm">z1_delta = a2_delta.w2.T<br/>a1_delta = z1_delta.sigmoid_derv(a1)</span></pre><p id="f273" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">注:</strong>sigmoid 函数的导数可通过两种方式实现，具体取决于输入。这总是有一个困惑。如果输入已经是 sigmoid 的输出，即<code class="fe kz la lb lc b">a</code>，则</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="8903" class="ld le it lc b gy mj mk l ml mm">def sigmoid_derv(x):<br/>    return x * (1 - x)</span></pre><p id="0241" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果输入为<code class="fe kz la lb lc b">z</code>且未通过 softmax 激活，则</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="ad17" class="ld le it lc b gy mj mk l ml mm">def sigmoid_derv(x):<br/>    return sigmoid(x) * (1 - sigmoid(x))</span></pre><p id="733f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在链式法则计算中，使用了<code class="fe kz la lb lc b">z</code>的 sigmoid 导数。但是在实现中，使用了<code class="fe kz la lb lc b">a</code>的 sigmoid 导数。因此，使用前一个等式。希望，这已经够清楚了。现在有了 numpy 实现。</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="d04e" class="ld le it lc b gy mj mk l ml mm">def sigmoid_derv(s):<br/>    return s * (1 - s)</span><span id="6417" class="ld le it lc b gy mn mk l ml mm">def cross_entropy(pred, real):<br/>    n_samples = real.shape[0]<br/>    res = pred - real<br/>    return res/n_samples</span><span id="b9a4" class="ld le it lc b gy mn mk l ml mm">def error(pred, real):<br/>    n_samples = real.shape[0]<br/>    logp = - np.log(pred[np.arange(n_samples), real.argmax(axis=1)])<br/>    loss = np.sum(logp)/n_samples<br/>    return loss</span><span id="ffef" class="ld le it lc b gy mn mk l ml mm">def backprop(self):<br/>    loss = error(self.a3, self.y)<br/>    print('Error :', loss)<br/>    a3_delta = cross_entropy(self.a3, self.y) # w3<br/>    z2_delta = np.dot(a3_delta, self.w3.T)<br/>    a2_delta = z2_delta * sigmoid_derv(self.a2) # w2<br/>    z1_delta = np.dot(a2_delta, self.w2.T)<br/>    a1_delta = z1_delta * sigmoid_derv(self.a1) # w1</span><span id="2ae2" class="ld le it lc b gy mn mk l ml mm">    self.w3 -= self.lr * np.dot(self.a2.T, a3_delta)<br/>    self.b3 -= self.lr * np.sum(a3_delta, axis=0, keepdims=True)<br/>    self.w2 -= self.lr * np.dot(self.a1.T, a2_delta)<br/>    self.b2 -= self.lr * np.sum(a2_delta, axis=0)<br/>    self.w1 -= self.lr * np.dot(self.x.T, a1_delta)<br/>    self.b1 -= self.lr * np.sum(a1_delta, axis=0)</span></pre><figure class="mb mc md me gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/9e29fe1897e6fbbfa117779bd64495d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*VxbAZY6PyVD53nfUwwrA1A.jpeg"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Source: <a class="ae mp" href="http://picbear.online/neuralnetmemes" rel="noopener ugc nofollow" target="_blank">neuralnetmemes, picbear</a></figcaption></figure><p id="5a3b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">预测阶段:</strong></p><p id="3592" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一旦模型经过足够的训练，预测就简单了。查询输入必须传递给前馈网络并预测输出。</p><pre class="mb mc md me gt mf lc mg mh aw mi bi"><span id="bf10" class="ld le it lc b gy mj mk l ml mm">def predict(self, data):<br/>    self.x = data<br/>    self.feedforward()<br/>    return self.a3.argmax()</span></pre><p id="9094" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">总的来说，</p><figure class="mb mc md me gt ju"><div class="bz fp l di"><div class="mq mr l"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">ANN using numpy</figcaption></figure><h1 id="9349" class="nd le it bd lf ne oc ng li nh od nj ll nk oe nm lo nn of np lr nq og ns lu nt bi translated">最后的想法</h1><ul class=""><li id="151f" class="oh oi it kd b ke lw ki lx km oj kq ok ku ol ky om on oo op bi translated">调整参数，如学习率，纪元，初始权重，激活函数，看看系统如何反应</li><li id="33a7" class="oh oi it kd b ke oq ki or km os kq ot ku ou ky om on oo op bi translated">如果您的模型不起作用，即误差直线上升或权重都是 NaNs，或者只是预测所有输入都属于同一个类别:<br/> -检查每个函数是否标准化<br/> -用单个类进行训练，看看它是如何工作的<br/> -检查矩阵的维数及其转置<br/> -验证所用产品的类型，点积还是<a class="ae mp" href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)" rel="noopener ugc nofollow" target="_blank">哈达玛积</a></li></ul></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><p id="f8b6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">感谢您阅读帖子。如果你喜欢，请鼓掌。如果你发现了错误或有疑问，请在评论中告诉我。</p><p id="4880" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">欢迎通过<a class="ae mp" href="https://github.com/chmodsss" rel="noopener ugc nofollow" target="_blank"> Github </a>、<a class="ae mp" href="https://twitter.com/chmodsss" rel="noopener ugc nofollow" target="_blank"> Twitter </a>和<a class="ae mp" href="https://www.linkedin.com/in/sivasuryas" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我。干杯！。</p></div></div>    
</body>
</html>