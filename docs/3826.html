<html>
<head>
<title>Neural Machine Translation with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 实现神经机器翻译</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd?source=collection_archive---------0-----------------------#2018-06-23">https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd?source=collection_archive---------0-----------------------#2018-06-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/0803d99bc97b342b003b79b4191cb3ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MfgBaSPc2G4hExYIOqRwOQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo credit: eLearning Industry</figcaption></figure><p id="97af" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">机器翻译，有时简称为<strong class="ke ir"> MT </strong>是一项非常具有挑战性的任务，研究如何使用软件将文本或语音从一种语言翻译成另一种语言。传统上，它涉及使用高度复杂的语言知识开发的大型统计模型。</p><p id="4896" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在这里，我们将使用深度神经网络来解决机器翻译的问题。我们将发现如何开发一个神经机器翻译模型来将英语翻译成法语。我们的模型将接受英语文本作为输入，并返回法语翻译。更准确地说，我们将练习构建 4 个模型，它们是:</p><ul class=""><li id="ba84" class="lb lc iq ke b kf kg kj kk kn ld kr le kv lf kz lg lh li lj bi translated">简单的 RNN。</li><li id="e273" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz lg lh li lj bi translated">嵌入的 RNN。</li><li id="a3f1" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz lg lh li lj bi translated">双向 RNN。</li><li id="7080" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz lg lh li lj bi translated">编码器-解码器模型。</li></ul><p id="65a5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">训练和评估深度神经网络是一项计算密集型任务。我使用了<a class="ae la" href="https://aws.amazon.com/ec2/" rel="noopener ugc nofollow" target="_blank"> AWS EC2 </a>实例来运行所有代码。如果您打算继续下去，您应该能够访问 GPU 实例。</p><h1 id="48a7" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">导入库</h1><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="b03a" class="mw lq iq ms b gy mx my l mz na">import collections</span><span id="2afe" class="mw lq iq ms b gy nb my l mz na">import helper<br/>import numpy as np<br/>import project_tests as tests</span><span id="54c3" class="mw lq iq ms b gy nb my l mz na">from keras.preprocessing.text import Tokenizer<br/>from keras.preprocessing.sequence import pad_sequences<br/>from keras.models import Model<br/>from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional<br/>from keras.layers.embeddings import Embedding<br/>from keras.optimizers import Adam<br/>from keras.losses import sparse_categorical_crossentropy</span></pre><p id="0200" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我用<code class="fe nc nd ne ms b"><a class="ae la" href="https://github.com/susanli2016/NLP-with-Python/blob/master/helper.py" rel="noopener ugc nofollow" target="_blank">help.py</a></code>加载数据，<code class="fe nc nd ne ms b"><a class="ae la" href="https://github.com/susanli2016/NLP-with-Python/blob/master/project_tests.py" rel="noopener ugc nofollow" target="_blank">project_test.py</a></code>用于测试我们的功能。</p><h1 id="52ac" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">数据</h1><p id="2453" class="pw-post-body-paragraph kc kd iq ke b kf nf kh ki kj ng kl km kn nh kp kq kr ni kt ku kv nj kx ky kz ij bi translated">数据集包含相对较少的词汇，可以在<a class="ae la" href="https://github.com/susanli2016/NLP-with-Python/tree/master/data" rel="noopener ugc nofollow" target="_blank">这里</a>找到。<code class="fe nc nd ne ms b">small_vocab_en</code>文件包含<code class="fe nc nd ne ms b">small_vocab_fr</code>文件中的英语句子及其法语翻译。</p><p id="515c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">加载数据</strong></p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="19f4" class="mw lq iq ms b gy mx my l mz na">english_sentences = helper.load_data('data/small_vocab_en')<br/>french_sentences = helper.load_data('data/small_vocab_fr')</span><span id="818d" class="mw lq iq ms b gy nb my l mz na">print('Dataset Loaded')</span></pre><p id="4db2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nk">数据集加载</em> </strong></p><p id="6be0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">例句</strong></p><p id="7024" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><code class="fe nc nd ne ms b">small_vocab_en</code>中的每一行都包含一个英文句子，而<code class="fe nc nd ne ms b">small_vocab_fr</code>中的每一行都有相应的翻译。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="8392" class="mw lq iq ms b gy mx my l mz na">for sample_i in range(2):<br/>    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))<br/>    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nl"><img src="../Images/88767b2839146edbb33a60dbfc225aa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*njzJa8HihVK7MCZbhFrybg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1</figcaption></figure><p id="1ab9" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">词汇</strong></p><p id="32d2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">问题的复杂程度是由词汇的复杂程度决定的。更复杂的词汇是更复杂的问题。让我们看看我们将要处理的数据集的复杂性。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="9b69" class="mw lq iq ms b gy mx my l mz na">english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])<br/>french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])</span><span id="3ce6" class="mw lq iq ms b gy nb my l mz na">print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))<br/>print('{} unique English words.'.format(len(english_words_counter)))<br/>print('10 Most common words in the English dataset:')<br/>print('"' + '" "'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '"')<br/>print()<br/>print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))<br/>print('{} unique French words.'.format(len(french_words_counter)))<br/>print('10 Most common words in the French dataset:')<br/>print('"' + '" "'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '"')</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/be272c12fd54ab7212c2f85572d42cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E6bHmtNhRfIS6pFcwXRSLg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2</figcaption></figure><h1 id="b3aa" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">预处理</h1><p id="5b02" class="pw-post-body-paragraph kc kd iq ke b kf nf kh ki kj ng kl km kn nh kp kq kr ni kt ku kv nj kx ky kz ij bi translated">我们将使用以下预处理方法将文本转换为整数序列:</p><ol class=""><li id="cc8e" class="lb lc iq ke b kf kg kj kk kn ld kr le kv lf kz nn lh li lj bi translated">将单词标记为 id</li><li id="249f" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz nn lh li lj bi translated">添加填充以使所有序列长度相同。</li></ol><p id="68f4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">符号化</strong></p><p id="86f3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">使用 Keras 的<code class="fe nc nd ne ms b"><a class="ae la" href="https://keras.io/preprocessing/text/#tokenizer" rel="noopener ugc nofollow" target="_blank">Tokenizer</a></code>函数将每个句子转换成一系列单词 id。使用此函数来标记<code class="fe nc nd ne ms b">english_sentences</code>和<code class="fe nc nd ne ms b">french_sentences</code>。</p><p id="057c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">函数<code class="fe nc nd ne ms b">tokenize</code>返回标记化的输入和标记化的类。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="a41f" class="mw lq iq ms b gy mx my l mz na">def tokenize(x):<br/>    x_tk = Tokenizer(char_level = False)<br/>    x_tk.fit_on_texts(x)<br/>    return x_tk.texts_to_sequences(x), x_tk</span><span id="a614" class="mw lq iq ms b gy nb my l mz na">text_sentences = [<br/>    'The quick brown fox jumps over the lazy dog .',<br/>    'By Jove , my quick study of lexicography won a prize .',<br/>    'This is a short sentence .']<br/>text_tokenized, text_tokenizer = tokenize(text_sentences)<br/>print(text_tokenizer.word_index)<br/>print()<br/>for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):<br/>    print('Sequence {} in x'.format(sample_i + 1))<br/>    print('  Input:  {}'.format(sent))<br/>    print('  Output: {}'.format(token_sent))</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/eba9b203111b5b044b8049092d32d7c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y3AaeOdHcidca5C3QjCD4Q.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3</figcaption></figure><p id="9c6a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">填充</strong></p><p id="bcc8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">通过使用 Keras 的<code class="fe nc nd ne ms b"><a class="ae la" href="https://keras.io/preprocessing/sequence/#pad_sequences" rel="noopener ugc nofollow" target="_blank">pad_sequences</a></code>函数向每个序列的<strong class="ke ir">末端</strong>添加填充，确保所有英文序列和所有法文序列长度相同。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="5a40" class="mw lq iq ms b gy mx my l mz na">def pad(x, length=None):<br/>    if length is None:<br/>        length = max([len(sentence) for sentence in x])<br/>    return pad_sequences(x, maxlen = length, padding = 'post')</span><span id="ecac" class="mw lq iq ms b gy nb my l mz na">tests.test_pad(pad)</span><span id="e7f1" class="mw lq iq ms b gy nb my l mz na"># Pad Tokenized output<br/>test_pad = pad(text_tokenized)<br/>for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):<br/>    print('Sequence {} in x'.format(sample_i + 1))<br/>    print('  Input:  {}'.format(np.array(token_sent)))<br/>    print('  Output: {}'.format(pad_sent))</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/9288eaf7660100ad4cfb9303adbb6569.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*V7jGsBA25nPHCBWwwmXv-w.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4</figcaption></figure><p id="58dd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">预处理管道</strong></p><p id="4ef0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">实现预处理功能</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="7ac0" class="mw lq iq ms b gy mx my l mz na">def preprocess(x, y):<br/>    preprocess_x, x_tk = tokenize(x)<br/>    preprocess_y, y_tk = tokenize(y)</span><span id="fe67" class="mw lq iq ms b gy nb my l mz na">preprocess_x = pad(preprocess_x)<br/>    preprocess_y = pad(preprocess_y)</span><span id="2a01" class="mw lq iq ms b gy nb my l mz na"># Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions<br/>    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)</span><span id="8f49" class="mw lq iq ms b gy nb my l mz na">return preprocess_x, preprocess_y, x_tk, y_tk</span><span id="4780" class="mw lq iq ms b gy nb my l mz na">preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\<br/>    preprocess(english_sentences, french_sentences)<br/>    <br/>max_english_sequence_length = preproc_english_sentences.shape[1]<br/>max_french_sequence_length = preproc_french_sentences.shape[1]<br/>english_vocab_size = len(english_tokenizer.word_index)<br/>french_vocab_size = len(french_tokenizer.word_index)</span><span id="632c" class="mw lq iq ms b gy nb my l mz na">print('Data Preprocessed')<br/>print("Max English sentence length:", max_english_sequence_length)<br/>print("Max French sentence length:", max_french_sequence_length)<br/>print("English vocabulary size:", english_vocab_size)<br/>print("French vocabulary size:", french_vocab_size)</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/1015f2dfd57876da7214467b4251e4c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*LNKPKLFtAIg8riMlPiuwYA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 5</figcaption></figure><h1 id="dde2" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">模型</h1><p id="2b62" class="pw-post-body-paragraph kc kd iq ke b kf nf kh ki kj ng kl km kn nh kp kq kr ni kt ku kv nj kx ky kz ij bi translated">在本节中，我们将实验各种神经网络架构。我们将从训练四个相对简单的架构开始。</p><ul class=""><li id="20c3" class="lb lc iq ke b kf kg kj kk kn ld kr le kv lf kz lg lh li lj bi translated">模型 1 是一个简单的 RNN</li><li id="362a" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz lg lh li lj bi translated">模型 2 是具有嵌入的 RNN</li><li id="ae96" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz lg lh li lj bi translated">模型 3 是双向 RNN</li><li id="4f86" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz lg lh li lj bi translated">模型 4 是一个编码器-解码器 RNN</li></ul><p id="5a16" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在对四个简单的架构进行实验之后，我们将构建一个更深层次的模型，该模型旨在超越所有四个模型。</p><p id="68e5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">id 返回正文</strong></p><p id="7a40" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">神经网络将把输入翻译成单词 ids，这不是我们想要的最终形式。我们想要法语翻译。函数<code class="fe nc nd ne ms b">logits_to_text</code>将在从神经网络到法语翻译的逻辑之间架起一座桥梁。我们将使用这个函数来更好地理解神经网络的输出。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="5739" class="mw lq iq ms b gy mx my l mz na">def logits_to_text(logits, tokenizer):<br/>    index_to_words = {id: word for word, id in tokenizer.word_index.items()}<br/>    index_to_words[0] = '&lt;PAD&gt;'</span><span id="00bb" class="mw lq iq ms b gy nb my l mz na">return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])</span><span id="7d99" class="mw lq iq ms b gy nb my l mz na">print('`logits_to_text` function loaded.')</span></pre><p id="3ed5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nk"> `logits_to_text `函数已加载。</em> </strong></p><p id="0496" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">车型一:RNN </strong></p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/959518aca7dce092adf3a79b52062ffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x1R8CyV3pTPOsjXvSOO5sg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 6</figcaption></figure><p id="2ce1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们正在创建一个基本的 RNN 模型，它是将英语翻译成法语的序列数据的良好基线。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="4cbb" class="mw lq iq ms b gy mx my l mz na">def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):<br/>    learning_rate = 1e-3<br/>    input_seq = Input(input_shape[1:])<br/>    rnn = GRU(64, return_sequences = True)(input_seq)<br/>    logits = TimeDistributed(Dense(french_vocab_size))(rnn)<br/>    model = Model(input_seq, Activation('softmax')(logits))<br/>    model.compile(loss = sparse_categorical_crossentropy, <br/>                 optimizer = Adam(learning_rate), <br/>                 metrics = ['accuracy'])<br/>    <br/>    return model<br/>tests.test_simple_model(simple_model)</span><span id="139d" class="mw lq iq ms b gy nb my l mz na">tmp_x = pad(preproc_english_sentences, max_french_sequence_length)<br/>tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))</span><span id="d3e6" class="mw lq iq ms b gy nb my l mz na"># Train the neural network<br/>simple_rnn_model = simple_model(<br/>    tmp_x.shape,<br/>    max_french_sequence_length,<br/>    english_vocab_size,<br/>    french_vocab_size)<br/>simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)</span><span id="29fb" class="mw lq iq ms b gy nb my l mz na"># Print prediction(s)<br/>print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/8146f137966e313a00120e917eaeb116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ETjq8QPsMiIkD9OPNRCYA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 7</figcaption></figure><p id="61eb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">基本的 RNN 模型的验证精度终止于 0.6039。</p><p id="77c5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">模式二:嵌入</strong></p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/ad7bc419a43b058108daa6dcdbca4870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q-grgm0n_P3blVLpVQOKbg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 8</figcaption></figure><p id="5523" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">嵌入是在 n 维空间中接近相似单词的单词的向量表示，其中 n 表示嵌入向量的大小。我们将使用嵌入创建一个 RNN 模型。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="df6a" class="mw lq iq ms b gy mx my l mz na">from keras.models import Sequential<br/>def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):<br/>    learning_rate = 1e-3<br/>    rnn = GRU(64, return_sequences=True, activation="tanh")<br/>    <br/>    embedding = Embedding(french_vocab_size, 64, input_length=input_shape[1]) <br/>    logits = TimeDistributed(Dense(french_vocab_size, activation="softmax"))<br/>    <br/>    model = Sequential()<br/>    #em can only be used in first layer --&gt; Keras Documentation<br/>    model.add(embedding)<br/>    model.add(rnn)<br/>    model.add(logits)<br/>    model.compile(loss=sparse_categorical_crossentropy,<br/>                  optimizer=Adam(learning_rate),<br/>                  metrics=['accuracy'])<br/>    <br/>    return model<br/>tests.test_embed_model(embed_model)</span><span id="4a58" class="mw lq iq ms b gy nb my l mz na">tmp_x = pad(preproc_english_sentences, max_french_sequence_length)<br/>tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))</span><span id="6528" class="mw lq iq ms b gy nb my l mz na">embeded_model = embed_model(<br/>    tmp_x.shape,<br/>    max_french_sequence_length,<br/>    english_vocab_size,<br/>    french_vocab_size)</span><span id="a8e8" class="mw lq iq ms b gy nb my l mz na">embeded_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)</span><span id="09b1" class="mw lq iq ms b gy nb my l mz na">print(logits_to_text(embeded_model.predict(tmp_x[:1])[0], french_tokenizer))</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/8eeb77a7de45f05ab27f49ccce51cd1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wscri-RL8VSyX9F0EU8C8g.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 9</figcaption></figure><p id="f396" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">嵌入模型的验证精度终止于 0.8401。</p><p id="a7a0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">模型 3:双向 RNNs </strong></p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/aecfbfb1c17cfa3fe1d1437e32230c38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1HqR8be4idB0AefyDSRZAQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 10</figcaption></figure><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="885a" class="mw lq iq ms b gy mx my l mz na">def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):<br/>   <br/>    learning_rate = 1e-3<br/>    model = Sequential()<br/>    model.add(Bidirectional(GRU(128, return_sequences = True, dropout = 0.1), <br/>                           input_shape = input_shape[1:]))<br/>    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))<br/>    model.compile(loss = sparse_categorical_crossentropy, <br/>                 optimizer = Adam(learning_rate), <br/>                 metrics = ['accuracy'])<br/>    return model<br/>tests.test_bd_model(bd_model)</span><span id="ed84" class="mw lq iq ms b gy nb my l mz na">tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])<br/>tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))</span><span id="3a78" class="mw lq iq ms b gy nb my l mz na">bidi_model = bd_model(<br/>    tmp_x.shape,<br/>    preproc_french_sentences.shape[1],<br/>    len(english_tokenizer.word_index)+1,<br/>    len(french_tokenizer.word_index)+1)</span><span id="b2b8" class="mw lq iq ms b gy nb my l mz na">bidi_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)</span><span id="33fd" class="mw lq iq ms b gy nb my l mz na"># Print prediction(s)<br/>print(logits_to_text(bidi_model.predict(tmp_x[:1])[0], french_tokenizer))</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/b8dfa9eb203bc0758785a291675308b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*vLiPKHB3plyezkcDfCccBw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 11</figcaption></figure><p id="e650" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">双向 RNN 模型的验证精度终止于 0.5992。</p><p id="7a24" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">模型 4:编码器-解码器</strong></p><p id="2c0a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">编码器创建句子的矩阵表示。解码器将该矩阵作为输入，并预测翻译作为输出。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="c7cd" class="mw lq iq ms b gy mx my l mz na">def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):<br/>  <br/>    learning_rate = 1e-3<br/>    model = Sequential()<br/>    model.add(GRU(128, input_shape = input_shape[1:], return_sequences = False))<br/>    model.add(RepeatVector(output_sequence_length))<br/>    model.add(GRU(128, return_sequences = True))<br/>    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))<br/>    <br/>    model.compile(loss = sparse_categorical_crossentropy, <br/>                 optimizer = Adam(learning_rate), <br/>                 metrics = ['accuracy'])<br/>    return model<br/>tests.test_encdec_model(encdec_model)</span><span id="6097" class="mw lq iq ms b gy nb my l mz na">tmp_x = pad(preproc_english_sentences)<br/>tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[1], 1))</span><span id="49c2" class="mw lq iq ms b gy nb my l mz na">encodeco_model = encdec_model(<br/>    tmp_x.shape,<br/>    preproc_french_sentences.shape[1],<br/>    len(english_tokenizer.word_index)+1,<br/>    len(french_tokenizer.word_index)+1)</span><span id="6891" class="mw lq iq ms b gy nb my l mz na">encodeco_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)</span><span id="700e" class="mw lq iq ms b gy nb my l mz na">print(logits_to_text(encodeco_model.predict(tmp_x[:1])[0], french_tokenizer))</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nx"><img src="../Images/ff2532957dc6b131d42beaf50d709bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*5ea_Unpt0IdP0nt95278rw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 12</figcaption></figure><p id="1d2b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">编码器-解码器模型的验证精度终止于 0.6406。</p><p id="a995" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">型号 5:定制</strong></p><p id="207a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">创建一个将嵌入和双向 RNN 合并到一个模型中的 model_final。</p><p id="76be" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在这个阶段，我们需要做一些实验，例如将 GPU 参数更改为 256，将学习率更改为 0.005，为超过(或少于)20 个时期训练我们的模型等。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="0a05" class="mw lq iq ms b gy mx my l mz na">def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):<br/>  <br/>    model = Sequential()<br/>    model.add(Embedding(input_dim=english_vocab_size,output_dim=128,input_length=input_shape[1]))<br/>    model.add(Bidirectional(GRU(256,return_sequences=False)))<br/>    model.add(RepeatVector(output_sequence_length))<br/>    model.add(Bidirectional(GRU(256,return_sequences=True)))<br/>    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))<br/>    learning_rate = 0.005<br/>    <br/>    model.compile(loss = sparse_categorical_crossentropy, <br/>                 optimizer = Adam(learning_rate), <br/>                 metrics = ['accuracy'])<br/>    <br/>    return model<br/>tests.test_model_final(model_final)</span><span id="3dcd" class="mw lq iq ms b gy nb my l mz na">print('Final Model Loaded')</span></pre><p id="d569" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="nk">最终模型载入</em> </strong></p><h1 id="820b" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">预言；预测；预告</h1><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="f9b3" class="mw lq iq ms b gy mx my l mz na">def final_predictions(x, y, x_tk, y_tk):<br/>    tmp_X = pad(preproc_english_sentences)<br/>    model = model_final(tmp_X.shape,<br/>                        preproc_french_sentences.shape[1],<br/>                        len(english_tokenizer.word_index)+1,<br/>                        len(french_tokenizer.word_index)+1)<br/>    <br/>    model.fit(tmp_X, preproc_french_sentences, batch_size = 1024, epochs = 17, validation_split = 0.2)<br/> <br/>    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}<br/>    y_id_to_word[0] = '&lt;PAD&gt;'</span><span id="77b7" class="mw lq iq ms b gy nb my l mz na">sentence = 'he saw a old yellow truck'<br/>    sentence = [x_tk.word_index[word] for word in sentence.split()]<br/>    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')<br/>    sentences = np.array([sentence[0], x[0]])<br/>    predictions = model.predict(sentences, len(sentences))</span><span id="9b67" class="mw lq iq ms b gy nb my l mz na">print('Sample 1:')<br/>    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))<br/>    print('Il a vu un vieux camion jaune')<br/>    print('Sample 2:')<br/>    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))<br/>    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))</span><span id="7cf2" class="mw lq iq ms b gy nb my l mz na">final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/37a1e3ab2f2a55e402a0657212a9164f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*nPcxoagKHWGdlTMHBH-k5g.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 13</figcaption></figure><p id="127c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们得到了两个句子的完美翻译和 0.9776 的验证准确度分数！</p><p id="2d87" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">源代码可以在<a class="ae la" href="https://github.com/susanli2016/NLP-with-Python/blob/master/machine_translation.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>找到。我期待听到反馈或问题。</p></div></div>    
</body>
</html>