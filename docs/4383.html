<html>
<head>
<title>Linear Regression in the Wild</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">野外线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-in-the-wild-335723a687e8?source=collection_archive---------11-----------------------#2018-08-12">https://towardsdatascience.com/linear-regression-in-the-wild-335723a687e8?source=collection_archive---------11-----------------------#2018-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/5c0e806762e0f83c12aee0a8b8526b26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YByzBGQgkOnLN_C58RaF4g.png"/></div></div></figure><p id="af9c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在一次数据科学家职位的面试中，我接到了一个家庭作业，我想和你分享一下。</p><p id="dd5a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">面试官给我发了一个 CSV 文件，里面有实测量<em class="kz"> x </em>和<em class="kz"> y </em>的样本，其中<em class="kz"> y </em>是一个响应变量，可以写成<em class="kz"> x </em>的显函数。众所周知，在标准偏差的意义上，用于测量<em class="kz"> x </em>的技术比用于测量<em class="kz"> y </em>的技术好两倍。</p><p id="92b6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">任务:将<em class="kz"> y </em>建模为<em class="kz"> x </em>的函数。</p><p id="538a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这是我需要的所有进口货:</p><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="efb1" class="lj lk it lf b gy ll lm l ln lo">import pandas as pd<br/>import numpy as np<br/>from sklearn.linear_model import LinearRegression<br/>from scipy.stats import probplot</span><span id="8d97" class="lj lk it lf b gy lp lm l ln lo">import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="7d61" class="lj lk it lf b gy lp lm l ln lo">data = pd.read_csv('data.csv', names=['x', 'y'])<br/>data.head()</span></pre><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/49a462f84a014f6c2a5367b58f5a90cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*G_2S3laz7-NVn8dRFGqkOA.png"/></div></figure><p id="9570" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们将数据可视化，看看是否容易用肉眼捕捉到模式:</p><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="d922" class="lj lk it lf b gy ll lm l ln lo">data.plot.scatter('x', 'y', title='data')</span></pre><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lr"><img src="../Images/333dc50d5525ce11eb7d46a660f356f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lLDc9IrPuIh3g0gCD_CSWg.png"/></div></div></figure><p id="4a4a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这看起来很像线性回归的情况。首先，我将手动移除异常值:</p><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="277a" class="lj lk it lf b gy ll lm l ln lo">data = data[data['x'] &lt; 600]<br/>data.plot.scatter('x', 'y', title='data without outliers')</span></pre><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ls"><img src="../Images/fdeba22e0dd5fd2d94d0f2f9c24ad033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2PuOVQJJX_mFYkouosqDQ.png"/></div></div></figure><p id="34a9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我将使用<code class="fe lt lu lv lf b">LinearRegression</code>来拟合最佳线条:</p><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="7356" class="lj lk it lf b gy ll lm l ln lo">lr = LinearRegression().fit(data[['x']], data['y'])<br/>data.plot.scatter('x', 'y', title='linear regression')<br/>lr_predicted_y = lr.predict(data[['x']])<br/>plt.plot(data['x'], lr_predicted_y)</span></pre><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lw"><img src="../Images/30699ba28ecafbb0ffb4c91ec71bd43b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oDGJZl-4e5rlDoCVNQC04Q.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">fitting a line through the data</figcaption></figure><p id="fc00" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">视觉上看起来很有说服力，但我会验证线性回归假设，以确保我使用的是正确的模型。</p><p id="bfa1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你对线性回归假设不熟悉，你可以在文章<a class="ae mb" href="https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/" rel="noopener ugc nofollow" target="_blank">用假设深入回归分析，绘制&amp;解</a>中阅读。</p><p id="80e7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首先，我们将绘制残差图:</p><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="6998" class="lj lk it lf b gy ll lm l ln lo">residuals = lr_predicted_y - data['y']<br/>plt.scatter(x=lr_predicted_y, y=residuals)<br/>plt.title('residuals')</span></pre><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mc"><img src="../Images/6dc69eb9cf9ef0a0e8e1f0f2d0407566.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pBUZzWy9wNVjin3nTGsO0Q.png"/></div></div></figure><ul class=""><li id="c74a" class="md me it kd b ke kf ki kj km mf kq mg ku mh ky mi mj mk ml bi translated">残差中似乎没有自相关。</li><li id="9273" class="md me it kd b ke mm ki mn km mo kq mp ku mq ky mi mj mk ml bi translated">异方差在这里看起来也不是问题，因为方差看起来几乎是恒定的(除了图的左边部分，但是没有太多的数据，所以我会忽略它)。</li><li id="2d5c" class="md me it kd b ke mm ki mn km mo kq mp ku mq ky mi mj mk ml bi translated">多重共线性在这里不相关，因为只有一个因变量。</li><li id="7af4" class="md me it kd b ke mm ki mn km mo kq mp ku mq ky mi mj mk ml bi translated">残差应呈正态分布:我将使用 QQ-plot 验证:</li></ul><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="fa7a" class="lj lk it lf b gy ll lm l ln lo">probplot(residuals, plot=plt)</span></pre><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mr"><img src="../Images/3f04272d44d96dda0edc8c8b8c35cb8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*brCSUl3ho2eWroHNjEPG3Q.png"/></div></div></figure><p id="e8b2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这看起来相当正常…</p><p id="2b8e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我的结论是，假设线性关系，x 和 y 之间的关系最好建模为</p><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="cce8" class="lj lk it lf b gy ll lm l ln lo">print 'y = %f + %f*x'  % (lr.intercept_, lr.coef_)</span><span id="d8d1" class="lj lk it lf b gy lp lm l ln lo">&gt;&gt;&gt; y = 70.023655 + 2.973585*x</span></pre></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="d873" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">给定<em class="kz"> x </em>(两者都有测量误差)，或者换句话说，线的系数，我们得到了计算<em class="kz"> y </em>所需参数的一致估计值。</p><p id="35b6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">到目前为止，我所做的只是简单的线性回归。关于这个任务有趣的事情是<em class="kz"> x </em>有测量误差(这在真实世界用例中是典型的)。</p><p id="6f37" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果我们想要在给定精确的<em class="kz"> x </em>值(没有测量误差)的情况下估计计算<em class="kz"> y </em>所需的参数，我们需要使用不同的方法。使用简单的线性回归而不考虑随机噪声的<em class="kz"> x </em>导致线斜率略小于真实的线斜率(描述没有测量误差的<em class="kz"> x </em>的线)。你可以阅读这个维基页面来了解原因。</p><p id="6e93" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我将使用<a class="ae mb" href="https://en.wikipedia.org/wiki/Deming_regression" rel="noopener ugc nofollow" target="_blank">戴明回归</a>，这是一种当两个变量<em class="kz"> x </em>和<em class="kz"> y </em>的误差被假设为独立且正态分布，并且它们的方差比(表示为<em class="kz"> δ </em>)已知时可以使用的方法。这种方法非常适合我们的环境</p><blockquote class="mz na nb"><p id="ef5e" class="kb kc kz kd b ke kf kg kh ki kj kk kl nc kn ko kp nd kr ks kt ne kv kw kx ky im bi translated">在标准偏差的意义上，用于测量 x 的技术比用于测量 y 的技术好两倍。</p></blockquote><p id="29a4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以在我们的设置中，<em class="kz"> δ </em>是 2 的平方。</p><p id="d3e6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">使用在<a class="ae mb" href="https://en.wikipedia.org/wiki/Deming_regression#Solution" rel="noopener ugc nofollow" target="_blank">维基页面</a>中找到的公式，我们得到</p><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="0648" class="lj lk it lf b gy ll lm l ln lo">cov = data.cov()<br/>mean_x = data['x'].mean()<br/>mean_y = data['y'].mean()<br/>s_xx = cov['x']['x']<br/>s_yy = cov['y']['y']<br/>s_xy = cov['x']['y']<br/>delta = 2 ** 2</span><span id="0d08" class="lj lk it lf b gy lp lm l ln lo">slope = (s_yy  - delta * s_xx + np.sqrt((s_yy - delta * s_xx) ** 2 + 4 * delta * s_xy ** 2)) / (2 * s_xy)<br/>intercept = mean_y - slope  * mean_x</span></pre><p id="9331" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">使用戴明回归，将<em class="kz"> x </em>和<em class="kz"> y </em>之间的关系建模为</p><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="bbe7" class="lj lk it lf b gy ll lm l ln lo">print 'y = %f + %f*x'  % (intercept, slope)</span><span id="a37a" class="lj lk it lf b gy lp lm l ln lo">&gt;&gt;&gt; y = 19.575797 + 3.391855*x</span></pre><p id="014a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们绘制两个模型:</p><pre class="la lb lc ld gt le lf lg lh aw li bi"><span id="8a53" class="lj lk it lf b gy ll lm l ln lo">data.plot.scatter('x', 'y', title='linear regression with &amp; without accounting for $x$ error measurements')<br/>plt.plot(data['x'], lr_predicted_y, label='ignoring errors in $x$')<br/>X = [data['x'].min(), data['x'].max()]<br/>plt.plot(X, map(lambda x: intercept + slope * x, X), label='accounting for errors in $x$')<br/>plt.legend(loc='best')</span></pre><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nf"><img src="../Images/52eabc834aedd0d44eb71b4f97923d82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8qfWziKoUdMEGu2deZ0HYw.png"/></div></div></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="5fe9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们拟合了两个模型:一个是简单的线性回归模型，另一个是考虑到<em class="kz"> x </em>中的测量误差的线性回归模型。</p><p id="3f13" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果我们的目的是给定一个具有测量误差的新的<em class="kz"> x </em>来计算<em class="kz"> y </em>(由与训练模型时使用的测量误差相同的分布产生)，则更简单的方法可能就足够了。</p><p id="8eae" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果我们想在一个没有测量误差的世界里，将<em class="kz"> y </em>的真实关系描述为<em class="kz"> x </em>的函数，我们应该使用第二个模型。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="5a30" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这是一个很好的面试问题，因为我学到了一种新的模式，这是非常整洁的:)</p><p id="34b6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">尽管这并不是如标题所示的<em class="kz">线性回归的真实例子</em>(好吧，我撒谎了)，这篇文章确实展示了许多人没有注意到的一个重要概念:在许多情况下，因变量是以不准确的方式测量的，这可能需要加以考虑(取决于应用)。</p><p id="c045" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">小心退步！</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="99fb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">这个帖子最初是我在 www.anotherdatum.com</em><a class="ae mb" href="http://anotherdatum.com/linear-regression-in-the-wild.html" rel="noopener ugc nofollow" target="_blank"><em class="kz"/></a><em class="kz">发的。</em></p></div></div>    
</body>
</html>