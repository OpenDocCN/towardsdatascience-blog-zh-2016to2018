<html>
<head>
<title>Neural Networks: an Alternative to ReLU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络:ReLU 的替代方案</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-an-alternative-to-relu-2e75ddaef95c?source=collection_archive---------11-----------------------#2018-09-26">https://towardsdatascience.com/neural-networks-an-alternative-to-relu-2e75ddaef95c?source=collection_archive---------11-----------------------#2018-09-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/4c3a4d447813e56e11c601653f972877.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*HkFB9jTgyPUGfN1eGPLLEw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">ReLU activation, two neurons</figcaption></figure><p id="d2d7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">上面是两个神经元(<em class="kw">紫色</em>和<em class="kw">橙色</em>)的激活(<em class="kw">粉色</em>)图，使用了一个常用的激活函数:整流线性单元，或 ReLU。当每个神经元的总输入增加时，ReLU 也会增加其激活——前提是输入超过某个阈值。这导致峰值出现在输入空间的<em class="kw">一角</em>，如上图右上角所示。添加更多的 ReLU 神经元将增加立方体的维度，然而<em class="kw">模式</em>仍然存在:神经元的激活总是在一个远处的角落达到峰值，沿着相邻的边有倾斜的侧翼。因为峰值只能出现在一个<em class="kw">单个拐角</em>，所以这些神经网络的表达能力是有限的。我提供一个替代方案:<strong class="ka ir"> <em class="kw">反射线性单元</em> </strong>。(RefLU？)</p><p id="30a8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">反射线性单元</strong></p><p id="60bc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw">整流</em>线性单元仍有几个优点。它们易于计算，非常适合专门的硬件架构，如谷歌的 TPU。它们是非线性的，形成了清晰的行为边界，这是单一直线所不能做到的。而且，它们创造了一个持久的梯度(至少在它们的右侧)，这将它们与激活功能区分开来，激活功能<em class="kw">逐渐变细</em>——例如，s 形函数和逻辑函数。随着这些激活逐渐减少，它们的斜率几乎是水平的，因此梯度下降几乎不会试图修改神经元，学习进展缓慢。ReLUs 的右半部分具有恒定的正斜率，因此它在所有这些点接收到来自梯度下降的相同强度的信号<em class="kw">。</em></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi kx"><img src="../Images/67adf5efa2d63efdd955982c23bf4b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*waJsVWdSmsUhu2tJa8XYog.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Rectified Linear Units, with various synapse weights causing different tilts</figcaption></figure><p id="5cd8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然而，经过整流的线性单元<em class="kw">可能会遭受“死亡神经元”的困扰:当它们接收到强烈的负信号时，它们的倾斜度会下降到零，之后没有任何东西能够刺激神经元。没有新的梯度信号通过它们，因为它们是平的，所以神经元的损失永远不会被纠正。经常被引用的斯坦福课程笔记解释说“你的网络中有多达 40%可能是‘死的’。”</em></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi lh"><img src="../Images/c49829c005226ef002f9725c25557e39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C7vfwjDTYpbrDRMzJCHgGw.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Reflected Linear Units, with various peak positions and tilts</figcaption></figure><p id="773a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw">反射的</em>线性单位分享了 ReLUs 的好处:它们易于计算，是非线性的，并且不会逐渐变细。此外，他们不受“死亡”的影响。无论任何一半的斜率是正还是负，RefLU 仍然起作用。只有双方奇迹般地更新到<em class="kw">恰好</em>零斜率，那一个神经元才会死亡。RefLUs 提供了其他好处，我可以在一些细节之后解释这些好处…</p><p id="ed67" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">倾斜线条</strong></p><p id="0a37" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">RefLU 由在中间某处相交的两条线段组成。你可能会把这些线条想象成一束激光，从一面看不见的镜子上反射回来。天真的是，他们都在倾斜，在他们的接触点形成一个高峰。此 RefLU 的变量比 ReLU 多；虽然 ReLU 仅指定其右半部分的<em class="kw">斜率</em>，但 RefLU 必须指定左右半部分的<em class="kw">斜率，以及它们相交的<em class="kw">高度</em>和<em class="kw">距离</em>。</em></p><p id="7dd7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这四个变量都接收梯度下降信号，以小步长更新，试图匹配数据要求的激活的<em class="kw">曲线。虽然陡峭的拱形可能不完全符合数据所需的形状，但是 RefLU 激活的总和可以在输入空间内的任何位置形成组合峰值。那是远远优于 ReLUs 的，ReLUs 只能在单个<em class="kw">角</em>形成一个峰值。而且，虽然大多数表面可以由分散在各处的峰的集合来近似，允许多个 RefLUs 构建他们需要的景观，但是 ReLUs 的角峰只能创建更陡的角。反射线性单位可以采用数据所需的形状。</em></p><p id="b2a1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">稀疏激活</strong></p><p id="a6c0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过将反射的线性单元初始化为倾斜的峰值，网络在学习阶段的行为被修改。除了 RefLU 之外，只有 sinc 和高斯分布包括其激活函数的右下侧。在操作过程中，这意味着变得<em class="kw">过于兴奋的神经元</em>实际上会关闭！这导致了一种完全不同的激活模式:当许多神经元在一个簇中放电时，它们会在下一层的一些神经元中重叠太多，导致这些神经元中的一些变得安静，而不是所有的都变成不和谐的一致。</p><p id="13c0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过让一些兴奋簇中的神经元安静下来，网络的活动朝着稀疏的方向调整。没有癫痫发作的“是”。每个图像或声音都是用最少的活跃神经元进行分类的，只对最相关的信息进行编码。这在网络中留下了更多的“空间”来学习多种任务和丰富的分类，稀疏性将相似分类的输入推得更近。通过这种方式，用 RefLU 激活功能构建的网络将表现得更像人脑，具有自己稀疏而有意义的活动。</p><p id="62e5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">否定之肯定</strong></p><p id="3fe3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，RefLU 不是一个简化的 sinc 函数，sinc 和高斯函数在其两侧降至零，而 RefLU 实际上降至负值区域。这使得 RefLU 成为<em class="kw">显著不同的</em>激活功能。通常，对于一个神经元来说，要从积极的兴奋性信号转换为消极的抑制性信号，其<em class="kw">突触权重</em>必须从积极的变为消极的。在中间，神经元的权重为零，“杀死”它。并且，当权重<em class="kw">接近</em>零时，整个激活功能被压扁。</p><p id="fba5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">相反，RefLUs 可以为某些输入电平<em class="kw">产生强负输出，而为其他电平</em>产生强正输出。功能没有被压扁。它可以起到双重作用，同时兴奋和抑制更多的神经元<em class="kw"/>，分享 arctan 激活功能的这种强度。虽然 arctan 仅在输入电平为<em class="kw">低时产生负信号，但</em> RefLUs 是<em class="kw">唯一能够为<em class="kw">高和低输入变为负的</em>。</em></p><p id="a9c0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">RefLUs 不需要<em class="kw">在两端</em>变为负；梯度下降可以从向下的斜坡向向上的斜坡平滑地倾斜任一侧，在这种情况下，神经元作为现有激活函数的近似来操作。</p><p id="4649" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我希望这一分析能够激发研究人员将 RefLU 与它的哥哥 ReLU 进行比较，看看这些潜在的好处在哪里提高了性能。特别是，网络可能受益最大，它必须学习和联系许多任务，解释神经元活动，并避免死亡的神经元。在这些概念的基础上，可能还有其他更高级的激活功能；重要的一点是，我们还没有挖掘出神经网络新花样的源泉。我们应该继续假设和实验，因为智力是我们面临的最大问题。我希望有些人会受到启发继续寻找！</p></div></div>    
</body>
</html>