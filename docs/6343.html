<html>
<head>
<title>Feel discouraged on the sparse data in your hand? Give Factorization Machine a shot (2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对手中稀疏的数据感到气馁？给因式分解机一个机会(2)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feel-discouraged-on-the-sparse-data-in-your-hand-give-factorization-machine-a-shot-2-b2e54d670cf8?source=collection_archive---------11-----------------------#2018-12-08">https://towardsdatascience.com/feel-discouraged-on-the-sparse-data-in-your-hand-give-factorization-machine-a-shot-2-b2e54d670cf8?source=collection_archive---------11-----------------------#2018-12-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="e57b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">打好矩阵分解的基础，你对矩阵分解概念<strong class="js iu"><em class="ko">衍生出的一系列高级模型<strong class="js iu"> <em class="ko">的探索就会顺利很多，比如 LDA、LSI、PLSA、张量分解等等</em> </strong>。</em></strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/6be8cbe7fe2800d0e1c61d7e6812750e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*w823C9MN7IjeGqVG3Lz2bg.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The models derived from the concept of Matrix Factorization</figcaption></figure><p id="aee0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<a class="ae lb" rel="noopener" target="_blank" href="/feel-discouraged-by-sparse-data-in-your-hand-give-factorization-machine-a-shot-1-7094628aa4ff">上一节</a>中，我们讲过<strong class="js iu"> <em class="ko">因式分解机</em> </strong>背后的基础理论。今天让我们揭示模型背后的技术细节和数学。</p><p id="71c0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本文将讨论两个问题:</p><ol class=""><li id="4419" class="lc ld it js b jt ju jx jy kb le kf lf kj lg kn lh li lj lk bi translated">如何估算<strong class="js iu"> <em class="ko">因式分解机</em> </strong>的系数？随机梯度下降(SGD)？交替最小二乘法(ALS)？</li><li id="7dc8" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn lh li lj lk bi translated">为什么计算复杂度是线性的？</li></ol><p id="a7e2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在陷入系数估计的深渊之前，让我们先回顾一下<strong class="js iu"> <em class="ko">因式分解机</em> </strong> 的模型<a class="ae lb" rel="noopener" target="_blank" href="/feel-discouraged-by-sparse-data-in-your-hand-give-factorization-machine-a-shot-1-7094628aa4ff">方程。</a></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/cfab4a2c72be8737712cab4e87488957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*5Gnk50KK2XmO4f_Gl62-1w.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The model equation of Factorization Machine</figcaption></figure><p id="3791" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="ko">我们的目标</em> </strong>是找到最优化的系数，使得变量和真实反应之间的关系可以用最准确的方式描述。</p><p id="d9ad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">机器<strong class="js iu"> <em class="ko">通过最小化</em> </strong> <strong class="js iu"> <em class="ko">损失函数</em> </strong>来学习模型的系数，该损失函数描述真实响应(地面真实值)与设计模型的估计值之间的距离。为了使概念形象化，系数更新的数学表达式如下。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/dafcbab855342ab72675bc3a08959d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pQbQJekEzFggdglYMUjf4w.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The goal of machine learning / model training</figcaption></figure><p id="4b87" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ѳ*是一组系数。标为蓝色的项目是<a class="ae lb" href="https://machinelearningmastery.com/vector-norms-machine-learning/" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="ko">L2-诺姆</em> </strong> </a> <strong class="js iu"> <em class="ko">，惩罚模型的复杂性，避免</em> </strong> <a class="ae lb" href="https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> <em class="ko">过拟合</em> </strong> </a> <strong class="js iu"> <em class="ko">。</em> </strong>为了简化数学推导，下图中省略了 L2 范数。</p><p id="2a72" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一个需要回答的问题就这样呈现在我们面前。<strong class="js iu"> <em class="ko">模型的损失函数是什么？</em> </strong></p><ol class=""><li id="9cca" class="lc ld it js b jt ju jx jy kb le kf lf kj lg kn lh li lj lk bi translated"><strong class="js iu"> <em class="ko">回归:</em> </strong>我们通常用最小二乘误差来处理回归问题。</li></ol><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/3dad048bb7f8b995bc7a94e7208679a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*kaHZvl7-IlQ0VvL-Ioyv7Q.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The equation of least square error</figcaption></figure><p id="c151" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">2<em class="ko">。二进制分类</em> </strong> : y = [-1，+1]</p><p id="f0c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">铰链损失或对数损失作为损失函数应用于二元分类。</p><p id="93a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="ko">铰链损耗:</em> </strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lt"><img src="../Images/3b6915b6ce36ea1833286532e5494410.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6FbSpMINuWx6uR3I-NT8iA.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The equation of hinge loss</figcaption></figure><p id="44d9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(y = 1)和(y = -1)下的铰链损耗如下图所示。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ly"><img src="../Images/e011cbaa34f9e50d84c2d32008740d44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AwkbzEvb9TimpmG9DmGzjA.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The hinge loss plot for y = 1 and y = -1</figcaption></figure><p id="479a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="ko">日志丢失:</em> </strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/1a48fa7823b1e8750c59b78b99418063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*-sfbXTex7y-RmkFFQLdUYw.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The equation of log loss</figcaption></figure><p id="f413" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">σ(x)是 sigmoid 函数。</p><p id="dadf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面是铰链损耗和对数损耗的对比图。随着估计值逐渐接近真实值，损耗变得越来越小。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ly"><img src="../Images/5b368f821e25fa37403fef63cbc41d42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B3pvqE9MJkgUwSad-vjncw.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">the plot of hinge loss and log loss (y = 1)</figcaption></figure><p id="3b2d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在阐明损失函数之后，<strong class="js iu">如何最小化它成为我们研究的第二个关注点。</strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ma"><img src="../Images/187f5936c977f2e8da7c3bc7122c38cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pknUkRf31JMedMjN7j0l3w.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">how to reach the bottom of valley within the shortest time?</figcaption></figure><p id="701f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">随机<a class="ae lb" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>因其在系数优化方面的出色表现而被引入。<strong class="js iu"> <em class="ko">关于方法的一个很棒的比喻</em> </strong>到达谷底的最快方法(<em class="ko">损失函数的最小值</em>)就是沿着最陡的方向迈步(<em class="ko">迭代梯度下降法</em>)(<em class="ko">梯度的负值</em>)。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mb"><img src="../Images/24239c80ac3f03bd624efa4adcfe7bcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mWbfc-qDLn0fljvNKI6C9A.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The equation of gradient descent</figcaption></figure><p id="1b5a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个想法是以数学的方式实现的。<strong class="js iu"> <em class="ko">损失函数的导数</em> </strong>用红色标记圈起来代表步长方向。<strong class="js iu"> <em class="ko"> η </em> </strong>是要走的步幅。<strong class="js iu"> w </strong>是要更新的权重集。</p><p id="e379" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同时，<strong class="js iu">因式分解机中系数的导数/梯度成为第三个焦点</strong>。</p><ol class=""><li id="cbf9" class="lc ld it js b jt ju jx jy kb le kf lf kj lg kn lh li lj lk bi translated"><strong class="js iu"> <em class="ko">对原木损耗进行分类的导数:</em> </strong></li></ol><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mc"><img src="../Images/5e208a64b689b3fc0c2ee3ecbf9bf8be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nbOZtN0_BW8aFTWyrZ2x5w.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The derivative of log loss for classification</figcaption></figure><p id="2f23" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可能想知道如何得到它。下面用 4 个步骤来说明演绎过程。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi md"><img src="../Images/4e2e59364b85c3baf0c6a03ffa3ff522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nEQXATcY1PsqB3EaQuWryw.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">step 1</figcaption></figure><p id="7d70" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">顺着逻辑，<strong class="js iu"> <em class="ko"> sigmoid 函数σ(x) </em> </strong>的导数是什么？</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi me"><img src="../Images/383802bd8b69349d76f97690078dbd2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xXzFw2kA4p3G43g4w05Tmg.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">step 2</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/a1c78c58f34e2f17d18146ad9f8fc688.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*2sDFnPWwwKLEC7dCvQVxPQ.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">step 3</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mg"><img src="../Images/43adb12d8517ba016fb57885272492aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WLB-Hp4JXjGXK6eLwd2V3g.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">step 4</figcaption></figure><p id="61a5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 2。<em class="ko">回归损失函数的导数:</em> </strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mh"><img src="../Images/7f0127dae803fcb1d8012ac7812a6d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YSGI0-R47mBysS3CViFMHQ.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The derivative of loss function for regression</figcaption></figure><p id="8ae7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">因此，到目前为止，在</strong>上方用红色标记圈起来的估计响应的导数被置于聚光灯下。</p><p id="b3ea" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">FM 模型的一个重要性质，<strong class="js iu">多重线性，</strong>可以对回答这个问题做出很大的贡献。让我们重新表述方程来形象化这个概念。</p><ol class=""><li id="6cc1" class="lc ld it js b jt ju jx jy kb le kf lf kj lg kn lh li lj lk bi translated"><strong class="js iu"> <em class="ko">关于 FM 对</em> W0 </strong>的偏导数:把<strong class="js iu"> W0 </strong>放到一边，重新组织公式。</li></ol><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mi"><img src="../Images/af6c781b08ad2277c30464344f32e40d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AhnfmE_YiQYyjtceakeJTg.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The model equation of Factorization Machine</figcaption></figure><p id="dd56" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> W0 </strong>的 FM 的梯度/偏导数为 1。</p><p id="4d39" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.<strong class="js iu"> <em class="ko">至于 FM 对</em> W </strong> <em class="ko"> l: </em>把<em class="ko"> </em> <strong class="js iu"> W </strong> <em class="ko"> l </em>放在一边重新组织公式。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mj"><img src="../Images/5b35e5db1dbdd407d49c1a824d2d334c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H9s4JxSB-LkhdD8nsZe2rg.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The model equation of Factorization Machine</figcaption></figure><p id="05d5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">FM 对<strong class="js iu">W</strong>T14】l 的梯度/偏导数为<strong class="js iu">X</strong>l .</p><p id="2118" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.<em class="ko"> </em> <strong class="js iu"> <em class="ko">至于 FM 对</em><em class="ko"/></strong><em class="ko">lm</em>的偏导数:把<strong class="js iu"><em class="ko"/></strong><em class="ko">lm</em>放到一边，重新组织公式。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mk"><img src="../Images/4a76b52792677cc99f3b058f23dcb82f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YmdKdYyCqjhZ-gSqIf-dIg.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The model equation of Factorization Machine</figcaption></figure><p id="d197" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">FM 对<strong class="js iu"><em class="ko">V</em></strong><em class="ko">lm</em>的梯度/偏导数就是上式中标记为红色的项目。</p><p id="58a2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">综上所述，FM 的导数为:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ml"><img src="../Images/c9a9573cd1f151b2fe10ea4470f20d23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qGzDrCX--_PCcMdgj28nmQ.jpeg"/></div></div></figure><p id="f3d9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">估计响应的导数集在随机梯度下降系数学习过程中起着重要作用。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ly"><img src="../Images/3b349122709956e0d199095ae670eab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l-m3k1Gsg0A6H4EY3xZ9Ow.jpeg"/></div></div></figure><p id="e816" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">随机梯度下降训练因式分解机怎么样？</strong></p><p id="5988" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过回顾算法中待估计的所有参数，收集了其中的 3 组参数。</p><p id="870f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> a. W </strong> 0:全局偏差</p><p id="5aec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> b. W </strong> i:第 I 个变量的强度</p><p id="00e2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> c. </strong> &lt; <strong class="js iu"> V </strong> i，<strong class="js iu"> V </strong> j &gt;:第 I 个和第 j 个变量之间的相互作用</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/b829595fd12d7c85219d334cf0df0d7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*l75B6dJc7IEQHjv3xEVMlQ.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Iterate until stopping criterion is met</figcaption></figure><p id="bf2e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">迭代直到满足停止标准。</p><p id="3565" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">到目前为止，接下来是关于计算次数的讨论。特征向量带来了参数数量的增加，从而增加了计算量。<strong class="js iu"> <em class="ko">线性时间复杂度</em> </strong>大大缓解了这个问题。</p><p id="2868" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们快速反思一下模型方程(度= 2)。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/95d3e86095e8da075f61a8c8b9343d9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*ipC5Fs3RdFV_skUasA1E1A.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The model equation of Factorization Machine</figcaption></figure><p id="46d9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">复杂度的直接计算在<em class="ko"> O (k n ) </em>中，因为必须计算所有成对的相互作用。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mo"><img src="../Images/47355409ba94aecb00ba4cac63b2a567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sPREPBKE59jhLVKawK4B2A.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Time complexity of machine factorization</figcaption></figure><p id="5196" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我把时间复杂度的方程拿到显微镜下，进行详细的推导。将复杂性公式分为三部分:</p><p id="1c53" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> a. n + (n-1): </strong></p><p id="4016" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在上述模型方程中，自变量权重的计算时间用黄色标出。</p><p id="cf48" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">b . {[k+(k-1)+2]*[(n-1)* n/2]+[(n-1)* n/2-1]}:</strong></p><p id="2305" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">模型方程中用红色表示的交互项的计算时间。</p><p id="dacd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> c. 2: </strong></p><p id="c5d2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3 个项目的 2 次添加</p><p id="e5ea" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">模型方程中交互项的复杂性可能最吸引你的眼球。下面的推论会清楚地梳理你的想法。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mp"><img src="../Images/9f63d8824cae095d5d08b1e8f7018f29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HvR7n2Wbh48bri6SmkY3eQ.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The deduction of complexity for interaction items in model equation</figcaption></figure><p id="5998" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">把其他项目的复杂程度加起来，我们就得到了整个计算结果。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mp"><img src="../Images/e69c4109232747e9af4414ed6a071967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m8l8KsoiePsG8lQIlt4EKw.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The deduction of complexity for the FM model</figcaption></figure><p id="bca3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是正如我们之前提到的，通过重构交互项，复杂度下降到<strong class="js iu"> <em class="ko">线性</em> </strong>运行时间。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/2a2c59b930dcbbc713d04c597d117475.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*pjxjlJLAVeD0Nfa4Wm-4dA.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">The reformulation of interaction items</figcaption></figure><p id="a12e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">方程现在在 k 和 n 中只有线性复杂度<strong class="js iu"> <em class="ko"> O(kn) </em> </strong>。我会留给读者一个问题。遵循上面类似的逻辑，如何得到基于方程重构后的线性复杂度？稍后我会在评论区附上答案。欢迎你在这里给我留下你的评论或想法。</p></div></div>    
</body>
</html>