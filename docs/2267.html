<html>
<head>
<title>A deeper understanding of NNets (Part 1) — CNNs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">加深对网络英语的理解(上)——CNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-deeper-understanding-of-nnets-part-1-cnns-263a6e3ac61?source=collection_archive---------0-----------------------#2018-01-01">https://towardsdatascience.com/a-deeper-understanding-of-nnets-part-1-cnns-263a6e3ac61?source=collection_archive---------0-----------------------#2018-01-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="4c7b" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">介绍</h1><p id="b5ad" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">深度学习和人工智能是2016年的热门词汇；到了2017年底，它们变得更加频繁，也更加混乱。所以让我们试着一次理解一件事。我们将研究深度学习的核心，即神经网络(NNets)。Nets的大多数变体很难理解，底层架构组件使它们听起来(理论上)和看起来(图形上)都一样。</p><p id="f3ea" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">感谢阿西莫夫研究所的Fjodor van Veen，我们有了最流行的NNet架构变体的公平代表。请参考他的<a class="ae lo" href="http://www.asimovinstitute.org/neural-network-zoo/" rel="noopener ugc nofollow" target="_blank">博客</a>。为了提高我们对网络的理解，我们将每周学习和实现一个架构。以下是我们将在未来几周讨论的架构。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/0063c909276d21851caa9980bd278095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mNr9ZDI7thP2aiJC.png"/></div></div></figure><h2 id="58fa" class="mb jo iq bd jp mc md dn jt me mf dp jx kw mg mh kb la mi mj kf le mk ml kj mm bi translated">第一周</h2><p id="9442" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">本周的架构是<strong class="kn ir">卷积神经网络或CNN </strong>。但是在开始CNN之前，我们将首先对<em class="mn">感知器</em>进行一次小小的深入探究。NNet是称为感知器的几个单元/单元的集合，感知器是二元线性分类器。让我们快速看一下，以了解相同的内容。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/fa22d87f55976d0e22f531d08be06b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/0*CcNwT07zNmckNoG5.png"/></div></figure><p id="eeaa" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">输入<code class="fe mp mq mr ms b">x1</code>和<code class="fe mp mq mr ms b">x2</code>乘以各自的权重w1和w2，并使用函数<code class="fe mp mq mr ms b">f</code>求和，因此得到<code class="fe mp mq mr ms b">f = x1*w1 + x2*w2 + b</code>(偏置项，可选添加)。这个函数<code class="fe mp mq mr ms b">f</code>可以是任何其他的运算，但是对于感知器来说，它通常是求和。该功能<code class="fe mp mq mr ms b">f</code>随后通过允许所需分类的激活进行评估。Sigmoid函数是用于二元分类的最常见的激活函数。关于感知机的更多细节，我推荐这篇<a class="ae lo" href="https://appliedgo.net/perceptron/" rel="noopener ugc nofollow" target="_blank">文章</a>。</p><p id="89c7" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">现在，如果我们堆叠多个输入，并使用函数<code class="fe mp mq mr ms b">f</code>将它们与堆叠在另一层中的多个单元连接，这形成了多个完全连接的感知机，来自这些单元(<em class="mn">隐藏层</em>)的输出成为最终单元的输入，最终单元再次使用函数<code class="fe mp mq mr ms b">f</code>和激活来导出最终分类。如下图所示，这是最简单的神经网络。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/097f2de0b4ed57e0beed29e920ef2b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/0*8n1WB3FaVX6IaRdM.jpeg"/></div></figure><p id="cab1" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">由于被称为“<em class="mn">通用近似函数</em>的nnet的独特能力，nnet的拓扑或架构变体是多样的。这本身就是一个巨大的话题，最好由迈克尔·尼尔森<em class="mn"/><a class="ae lo" href="http://neuralnetworksanddeeplearning.com/chap4.html" rel="noopener ugc nofollow" target="_blank">在这里</a>讲述。读完这篇文章后，我们可以相信这样一个事实:无论多么复杂，NNet都可以表现为任何函数。上述网络也被称为前馈神经网络或FFNN，因为信息流是单向的而不是循环的。现在我们知道了感知器和FFNN的基础知识，我们可以想象数百个输入连接到几个这样的隐藏层，会形成一个复杂的网络，流行称为深度神经网络或深度前馈网络。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/72752055f38730ca272fe3fbdac8e2a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/0*pnX45xI8nQvUV4WX.jpg"/></div></figure><h2 id="301e" class="mb jo iq bd jp mc md dn jt me mf dp jx kw mg mh kb la mi mj kf le mk ml kj mm bi translated">深度神经网络和CNN到底有什么不同？让我们找出答案。</h2><p id="b753" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">CNN通过像ImageNet这样的竞赛而变得流行，最近它们也被用于自然语言处理和语音识别。要记住的一个关键点是，许多其他变体，如RNN、LSTM、GRU等，都是基于与CNN相似的架构，但在架构上有所不同。我们将在后面详细讨论这些差异。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mv"><img src="../Images/bb37c3dbc558ed8c1958bb8e8a66ff18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dxGd8MmKOKvudjhR.png"/></div></div></figure><p id="9398" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">使用3种类型的层来形成CNN，即“<strong class="kn ir">卷积</strong>”、“<strong class="kn ir">汇集</strong>”和“<strong class="kn ir">密集或完全连接</strong>”。我们之前的网络是“密集”层网络的典型示例，因为所有层都是完全连接的。要了解更多关于需要切换到卷积和池层的信息，请阅读<em class="mn"> Andrej Karpathy的</em>精彩讲解<a class="ae lo" href="https://cs231n.github.io/convolutional-networks/" rel="noopener ugc nofollow" target="_blank">这里</a>。继续我们对层的讨论，让我们看看卷积层。</p><p id="c423" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">(对于下面的讨论，我们将使用图像分类作为理解CNN的任务，稍后转移到NLP和视频任务)</p><p id="2571" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">卷积层</strong>:考虑一个5X5像素的图像，有<code class="fe mp mq mr ms b">1 as white</code>和<code class="fe mp mq mr ms b">o as black</code>，这个图像被识别为一个5X5尺寸的单色图像。现在想象一个具有随机<code class="fe mp mq mr ms b">1s and 0s</code>的3×3矩阵，这个矩阵被允许与图像子集进行矩阵乘法，这个乘法被记录在一个新的矩阵中，因为我们的3×3矩阵在每次迭代中移动一个像素。下面是这一过程的视觉效果。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/04075b2d72351d7b581200f97c6e5df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/0*8pZppiZInoI9NQZK.gif"/></div></figure><p id="6144" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">上面考虑的3X3矩阵被称为“<em class="mn">滤波器</em>，它的任务是从图像中提取特征，它通过使用“<strong class="kn ir">优化算法</strong>来决定3X3矩阵中的特定<code class="fe mp mq mr ms b">1s and 0s</code>。我们允许几个这样的过滤器在一个神经网络的卷积层中提取几个特征。3×3矩阵的单个步骤被称为“<em class="mn">步距</em></p><p id="93e4" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">下面提供了使用两个3通道滤波器产生两个卷积输出的3通道(RGB)图像的详细视图。感谢安德烈·卡帕西！</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mx"><img src="../Images/78aab5df355c9f02fdf8aaec319e9d21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*K8Dz7WT5Yh1Sazsh.png"/></div></div></figure><p id="d765" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这些滤波器<code class="fe mp mq mr ms b">W0 and W1</code>是“卷积”,<code class="fe mp mq mr ms b">output</code>是提取的特征，由所有这些滤波器组成的层是卷积层。</p><p id="4f1b" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">池层</strong>:该层用于使用不同的函数降低输入的维度。一般来说，卷积层之后经常使用"<em class="mn"> MAX Pooling </em>"层。池化使用2X2矩阵，并以与卷积层相同的方式对图像进行操作，但这一次它缩小了图像本身。以下是使用“<em class="mn">最大池化</em>或“<em class="mn">平均池化</em>”来池化图像的两种方法</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi my"><img src="../Images/2375522f16ca7710f70d7afcab914bdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/0*9_zro3kc-ccq64LX.JPG"/></div></figure><p id="471d" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">密集层</strong>:该层是激活层和前一层之间的完全连接层。这类似于我们之前讨论的简单的“神经网络”。</p><p id="dce5" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">注意</strong>:归一化层也用在CNN架构中，但它们将单独讨论。此外，池层不是首选，因为它会导致信息丢失。通常的做法是在卷积层中使用更大的步幅。</p><p id="b254" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">ILSVRC 2014年的亚军VGGNet ，是一个受欢迎的CNN，它通过使用16层网络而不是ILSVRC 2012年获奖者<em class="mn"> AlexNet </em>的8层网络，帮助世界了解网络深度的重要性。一个即插即用模型“VGG-16”可在keras中使用，我们将使用相同的来查看一个获奖的CNN架构。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/648af4d0df79a4f979e4e5a2b2eb3a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*29VWVUMR77hFuFjg.jpg"/></div></figure><p id="1e10" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">在<em class="mn"> Keras </em>中加载模型后，我们可以看到每一层的<em class="mn">输出形状</em>以了解张量维度，以及<em class="mn"> Param # </em>以查看如何计算参数以获得卷积特征。"<em class="mn"> Param # </em>"是所有特征的每个回旋特征的总权重更新。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi na"><img src="../Images/2bd06a891e56170cb3c8f8dfd56bcc16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4UwRcHl1Uk1mAPu3.png"/></div></div></figure><p id="d7b0" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">现在，我们已经熟悉了CNN的架构，了解了它的层次和工作原理，我们可以进一步了解它在NLP和视频处理中的应用。这将在下周的文章中讨论，同时介绍RNNs以及CNN和RNNs之间的主要区别。同时，免费阅读自2012年以来赢得ImageNet竞赛的所有CNN模特，<a class="ae lo" href="https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html" rel="noopener ugc nofollow" target="_blank">此处</a>，感谢Adit Deshpande！</p><h2 id="f291" class="mb jo iq bd jp mc md dn jt me mf dp jx kw mg mh kb la mi mj kf le mk ml kj mm bi translated">未来的工作</h2><ul class=""><li id="0bd3" class="nb nc iq kn b ko kp ks kt kw nd la ne le nf li ng nh ni nj bi translated">一旦我们讨论了所有的架构，我们将遵循相同的顺序，并使用jupyter笔记本来实现它们。<em class="mn">一旦我们完成实施</em>，所有的代码链接都将可用。</li></ul><p id="1b6d" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">更新者上的类似帖子</strong></p><ul class=""><li id="0f74" class="nb nc iq kn b ko lj ks lk kw nk la nl le nm li ng nh ni nj bi translated">合成成分</li><li id="9446" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">阿达德尔塔</li><li id="fdd2" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">阿达格拉德</li><li id="1fe4" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">亚当</li><li id="ba05" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">涅斯特罗夫</li><li id="cc3a" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">RMSPROP</li><li id="0cb7" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">签名于</li><li id="f4b6" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">共轭梯度</li><li id="49de" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">黑森自由报</li><li id="8b25" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">LBFGS</li><li id="c16b" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">线梯度下降</li></ul><p id="6e60" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">类似开机自检激活功能</strong></p><ul class=""><li id="b7d3" class="nb nc iq kn b ko lj ks lk kw nk la nl le nm li ng nh ni nj bi translated">ELU</li><li id="bddd" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">硬乙状结肠</li><li id="443a" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">哈尔坦</li><li id="ad75" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">身份</li><li id="276e" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">LEAKYRELU</li><li id="b5a0" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">理性坦</li><li id="29f5" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">RELU</li><li id="6a10" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">RRELU</li><li id="d788" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">乙状结肠的</li><li id="a20e" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">SOFTMAX</li><li id="8218" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">SOFTPLUS</li><li id="cc83" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">软设计</li><li id="4b3b" class="nb nc iq kn b ko nn ks no kw np la nq le nr li ng nh ni nj bi translated">双曲正切</li></ul><p id="a887" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">感谢您的阅读，希望有所帮助</strong></p></div></div>    
</body>
</html>