<html>
<head>
<title>The use of KNN for missing values</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对缺失值使用 KNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-use-of-knn-for-missing-values-cf33d935c637?source=collection_archive---------0-----------------------#2017-01-31">https://towardsdatascience.com/the-use-of-knn-for-missing-values-cf33d935c637?source=collection_archive---------0-----------------------#2017-01-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/36b38c12d3142d2c3370526d4d74a665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L_kzed3FfjLJV0oBjIF-eA.jpeg"/></div></div></figure><div class=""/><h1 id="6c71" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">更新</strong></h1><p id="0db8" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">代码的主要更新，以提高计算加权汉明距离的性能。该方法从逐点距离计算改为矩阵计算，从而节省了多个数量级的时间。</p><h1 id="3176" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">访问代码:</h1><p id="5f88" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><a class="ae lu" href="https://gist.github.com/YohanObadia/b310793cd22a4427faaadd9c381a5850" rel="noopener ugc nofollow" target="_blank"> Github 链接</a></p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="c1df" class="jy jz jb bd ka kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv bi translated">问题:</h1><p id="7fd5" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在处理真实世界的数据时，您经常会在数据集中遇到缺失值。你如何处理它们对你的分析和你将得出的结论至关重要。</p><p id="8414" class="pw-post-body-paragraph kw kx jb ky b kz mh lb lc ld mi lf lg lh mj lj lk ll mk ln lo lp ml lr ls lt ij bi translated">缺失值有三种常见类型:</p><ol class=""><li id="12f2" class="mm mn jb ky b kz mh ld mi lh mo ll mp lp mq lt mr ms mt mu bi translated"><strong class="ky jc">完全随机缺失(MCAR): </strong>当缺失数据为 MCAR 时，数据的有/无完全独立于感兴趣的可观测变量和参数。在这种情况下，对数据进行的分析是无偏的。实际上，这是极不可能的。</li><li id="3a5c" class="mm mn jb ky b kz mv ld mw lh mx ll my lp mz lt mr ms mt mu bi translated"><strong class="ky jc">随机缺失(MAR): </strong>当缺失数据为<strong class="ky jc">非</strong>随机，但可以与一个变量完全相关，且有完整信息时。一个例子是，男性不太可能填写抑郁调查，但在考虑男性因素后，这与他们的抑郁水平无关。这种缺失数据会导致您的分析出现偏差，尤其是当您的数据因为某个类别中的许多缺失值而不平衡时。</li><li id="5bd0" class="mm mn jb ky b kz mv ld mw lh mx ll my lp mz lt mr ms mt mu bi translated"><strong class="ky jc">非随机缺失(MNAR): </strong>当缺失值既不是 MCAR 也不是马尔时。在前面的示例中，如果人们倾向于根据自己的抑郁水平不回答调查，就会出现这种情况。</li></ol><p id="406a" class="pw-post-body-paragraph kw kx jb ky b kz mh lb lc ld mi lf lg lh mj lj lk ll mk ln lo lp ml lr ls lt ij bi translated">在本教程中，我们将使用一种称为 k-最近邻(KNN)的非参数算法来替换丢失的值。该算法适用于前面三种情况中的任何一种，只要具有缺失值的变量与其他变量之间存在关系。</p><h1 id="8fa4" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">为什么用 KNN？</h1><p id="f5fa" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">KNN 是一种算法，用于在多维空间中将一个点与其最近的 k 个邻居进行匹配。它可以用于连续、离散、有序和分类的数据，这使得它在处理各种缺失数据时特别有用。</p><p id="a16d" class="pw-post-body-paragraph kw kx jb ky b kz mh lb lc ld mi lf lg lh mj lj lk ll mk ln lo lp ml lr ls lt ij bi translated">对缺失值使用 KNN 背后的假设是，基于其他变量，点的值可以由与其最接近的点的值来近似。</p><p id="28ba" class="pw-post-body-paragraph kw kx jb ky b kz mh lb lc ld mi lf lg lh mj lj lk ll mk ln lo lp ml lr ls lt ij bi translated">让我们保留之前的例子，并添加另一个变量，人的收入。现在，我们有三个变量，性别(T0)、收入(T2)、抑郁程度(T4)和缺失值(T5)。然后，我们假设收入相似、性别相同的人往往会有相同程度的抑郁。对于给定的缺失值，我们将查看该人的性别、收入，寻找其最近的 k 个邻居，并获得他们的抑郁水平。然后我们可以估计出我们想要的人的抑郁程度。</p><h1 id="0d8f" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">KNN 参数校准</h1><p id="c36d" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">使用 KNN 时，您必须考虑许多参数。您将在下面找到本文提供的函数所允许的内容:</p><ul class=""><li id="f2fc" class="mm mn jb ky b kz mh ld mi lh mo ll mp lp mq lt na ms mt mu bi translated"><strong class="ky jc">要寻找的邻居数量</strong>。采用低 k 值<strong class="ky jc">会增加噪声的影响，并且结果不太具有普遍性。另一方面，采用高 k 值会模糊局部效果，而这正是我们想要的。也建议二进制类取一个<strong class="ky jc">奇 k </strong>避免平局。</strong></li><li id="4323" class="mm mn jb ky b kz mv ld mw lh mx ll my lp mz lt na ms mt mu bi translated"><strong class="ky jc">聚合方法</strong>使用。在这里，我们允许数字变量的算术平均值、中值和众数以及分类变量的众数。</li><li id="c649" class="mm mn jb ky b kz mv ld mw lh mx ll my lp mz lt na ms mt mu bi translated"><strong class="ky jc">归一化数据</strong>是一种方法，当计算某种类型的距离时，如<strong class="ky jc">欧几里德</strong>距离，该方法允许在识别邻居时给予每个属性相同的影响。当比例没有意义和/或比例不一致(如厘米和米)时，应该对数据进行规范化。它意味着对数据的先验知识，以知道哪一个更重要。当同时提供了数字变量和分类变量时，该算法会自动对数据进行规范化。</li><li id="5334" class="mm mn jb ky b kz mv ld mw lh mx ll my lp mz lt na ms mt mu bi translated"><strong class="ky jc">数字属性距离:</strong>在各种可用的距离度量中，我们将关注主要的度量，欧几里德和曼哈顿。如果输入变量的类型相似(例如，所有测量的宽度和高度)，欧氏距离是一种很好的距离测量方法。如果输入变量的类型不相似(如年龄、身高等)，曼哈顿距离是一种很好的度量方法。</li><li id="21b0" class="mm mn jb ky b kz mv ld mw lh mx ll my lp mz lt na ms mt mu bi translated"><strong class="ky jc">分类属性距离:</strong>没有先验变换，适用距离与频率和相似度有关。这里我们允许使用两种距离:汉明距离和加权汉明距离。<br/> <br/> - <strong class="ky jc">汉明距离:</strong>取所有的分类属性，对于每个属性，如果两点之间的值不相同，则计 1。那么汉明距离就是值不同的属性的数量。<br/> <br/> - <strong class="ky jc">加权海明距离:</strong>如果值不同也返回 1，但如果匹配则返回属性中值的频率，值越频繁则增加距离。当多个属性是分类属性时，应用<strong class="ky jc">调和平均值</strong>。结果保持在 0 和 1 之间，但是与<strong class="ky jc">算术平均值</strong>相比，平均值向较低值移动。</li><li id="bb74" class="mm mn jb ky b kz mv ld mw lh mx ll my lp mz lt na ms mt mu bi translated"><strong class="ky jc">二元属性距离:</strong>这些属性通常是通过转换成虚拟变量的分类变量获得的。如前所述，对于连续变量，<strong class="ky jc">欧几里德距离</strong>也可应用于此。然而，也可以使用另一个基于相异度的度量，即<strong class="ky jc"> Jaccard 距离</strong>。</li></ul><p id="ebff" class="pw-post-body-paragraph kw kx jb ky b kz mh lb lc ld mi lf lg lh mj lj lk ll mk ln lo lp ml lr ls lt ij bi translated">为了确定用于您的数据的最佳距离度量，您可以使用上面给出的提示，但最重要的是，您必须进行实验，以找到最佳改进您的模型的方法。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="b3d4" class="jy jz jb bd ka kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv bi translated">在大型数据集上的测试</h1><p id="a20e" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">目标:预测哪位乘客在泰坦尼克号上幸存。你可以在<a class="ae lu" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上下载数据。为了能够快速进行多项测试，我还下载了完整的数据集，包括每位乘客的名单以及他们是否幸存。</p><p id="a764" class="pw-post-body-paragraph kw kx jb ky b kz mh lb lc ld mi lf lg lh mj lj lk ll mk ln lo lp ml lr ls lt ij bi translated"><strong class="ky jc">程序:</strong></p><ul class=""><li id="1740" class="mm mn jb ky b kz mh ld mi lh mo ll mp lp mq lt na ms mt mu bi translated">为了构建一个简单的模型，我删除了“姓名”、“客舱”和“机票”这三列。</li><li id="5c86" class="mm mn jb ky b kz mv ld mw lh mx ll my lp mz lt na ms mt mu bi translated">然后，我发现两列缺少值，“年龄”和“上船”。第一个有很多缺失值，而第二个只有几个。对于这两列，我应用了两种方法:<br/> 1-对数字列使用全局平均值，对分类列使用全局模式。<br/> 2-应用 knn_impute 函数。</li><li id="a7d0" class="mm mn jb ky b kz mv ld mw lh mx ll my lp mz lt na ms mt mu bi translated">构建一个简单的随机森林模型</li></ul><p id="ad51" class="pw-post-body-paragraph kw kx jb ky b kz mh lb lc ld mi lf lg lh mj lj lk ll mk ln lo lp ml lr ls lt ij bi translated"><strong class="ky jc">简单逼近结果:<br/> </strong>灵敏度= 66%；特异性= 79%；精度= 63%</p><p id="3087" class="pw-post-body-paragraph kw kx jb ky b kz mh lb lc ld mi lf lg lh mj lj lk ll mk ln lo lp ml lr ls lt ij bi translated"><strong class="ky jc"> KNN 用最佳模型插补结果:<br/> </strong>灵敏度= 69%；特异性= 80%；精度= 66%</p><p id="d230" class="pw-post-body-paragraph kw kx jb ky b kz mh lb lc ld mi lf lg lh mj lj lk ll mk ln lo lp ml lr ls lt ij bi translated"><strong class="ky jc">代码示例:</strong></p><figure class="nb nc nd ne gt is"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="5fcb" class="pw-post-body-paragraph kw kx jb ky b kz mh lb lc ld mi lf lg lh mj lj lk ll mk ln lo lp ml lr ls lt ij bi translated">对于这个数据集来说，这两种方法之间的结果差异不是很大，但是在 Kaggle 比赛中，人们可以花很多时间来获得这几个额外的百分比。</p><p id="f041" class="pw-post-body-paragraph kw kx jb ky b kz mh lb lc ld mi lf lg lh mj lj lk ll mk ln lo lp ml lr ls lt ij bi translated">如果你在其他数据集上测试这个函数，我期待着听到你的反馈，关于这个函数在帮助你改善结果方面有多大的帮助！</p><h1 id="4d01" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">资源:</h1><ul class=""><li id="6107" class="mm mn jb ky b kz la ld le lh nh ll ni lp nj lt na ms mt mu bi translated"><a class="ae lu" href="https://en.wikipedia.org/wiki/Missing_data" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Missing_data</a></li><li id="0f2b" class="mm mn jb ky b kz mv ld mw lh mx ll my lp mz lt na ms mt mu bi translated">【http://citeseerx.ist.psu.edu/viewdoc/download? T2】doi = 10 . 1 . 1 . 140 . 8831&amp;rep = re P1&amp;type = pdf</li><li id="0167" class="mm mn jb ky b kz mv ld mw lh mx ll my lp mz lt na ms mt mu bi translated"><a class="ae lu" href="http://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">http://machine learning mastery . com/k-nearest-neighbors-for-machine-learning/</a></li><li id="1bb9" class="mm mn jb ky b kz mv ld mw lh mx ll my lp mz lt na ms mt mu bi translated"><a class="ae lu" href="http://jamesxli.blogspot.co.il/2013/03/on-weighted-hamming-distance.html" rel="noopener ugc nofollow" target="_blank">http://jamesxli . blogspot . co . il/2013/03/on-weighted-hamming-distance . html</a></li></ul></div></div>    
</body>
</html>