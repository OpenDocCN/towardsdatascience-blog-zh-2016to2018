<html>
<head>
<title>Simple Reinforcement Learning: Temporal Difference Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单强化学习:时间差异学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-reinforcement-learning-temporal-difference-learning-53d1b3263d79?source=collection_archive---------18-----------------------#2018-10-29">https://towardsdatascience.com/simple-reinforcement-learning-temporal-difference-learning-53d1b3263d79?source=collection_archive---------18-----------------------#2018-10-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jn jo jp jq gh gi paragraph-image"><div class="ab gu cl jr"><img src="../Images/b33c49ccb3538dce98b0c6059a646276.png" data-original-src="https://miro.medium.com/v2/0*jXdY1VhbTXpWLbvA"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Exploration Image: Credit to Photographer <a class="ae jy" href="https://www.pexels.com/@valentinantonucci" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/@valentinantonucci</a></figcaption></figure><p id="f911" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">所以最近我读了很多关于强化学习的书，还看了大卫·西尔弗的<a class="ae jy" href="https://www.youtube.com/watch?v=2pWv7GOvuf0" rel="noopener ugc nofollow" target="_blank"> <em class="kx">强化学习简介</em> </a>视频系列，顺便说一下，这些视频非常棒，我强烈推荐它们！从传统的统计学和机器学习背景来看，就研究生院和工作项目而言，这些主题对我来说有些新。因此，就我个人的学习而言，并与感兴趣的人分享我的学习，我想我会通过一个中等的帖子来存档，同时试图使这些概念尽可能简单易懂。</p><p id="a282" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">为什么我们要使用强化学习而不是其他监督学习方法呢？为了回答这个问题，我想出了这个适时的例子:</p><blockquote class="ky kz la"><p id="dd8c" class="jz ka kx kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated">假设你是 NFL 的超级粉丝，你想预测即将到来的赛季的胜利数。在赛季初，你可能会看到一些变量，如:过去赛季的胜利，受伤球员的数量，天气预报，第一年首发球员的数量等。然后你拟合一个模型，预测一个 9 胜赛季。你有一个很好的团队，但一个未经证实的新四分卫(与去年相比只少了 1 场比赛)。在前 5 场比赛后，你的球队战绩为 5-0，你的新四分卫已经被誉为未来的名人堂成员。</p></blockquote><p id="bb9a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这个例子中，使用监督学习模型，你必须等到赛季结束后才能学习和做出任何改变。然而，通过强化学习，你可以在学习过程中获得信息，帮助你修正最初的预测。这个应用程序在任何时候都可以很好地工作，只要有一个时间或时间步长组件，您就可以不断地接收信息，并对您的目标或估计进行调整。</p><p id="9732" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在让我们看一个使用随机漫步(图 1)作为环境的例子。这是萨顿和巴尔托在《T4 强化学习:导论》一书中找到的一个例子。你可以在他们的网站上免费下载数字第二版。基本思想是，你总是从状态“D”开始，以 50%的概率随机向左或向右移动，直到到达终点或结束状态“A”或“G”。如果你在状态“A”结束，你得到的奖励是 0，但是如果你在状态“G”结束，奖励是 1。从“B”到“F”状态没有奖励。</p><figure class="le lf lg lh gt jq gh gi paragraph-image"><div class="ab gu cl jr"><img src="../Images/f89141a82ee0203f02b0030e4f6a354e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*IQrvO3_oozzHUvhNUz0i7w.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Figure 1: Random Walk Example [1]</figcaption></figure><p id="877c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这个例子中使用强化学习的目的是看看我们是否可以通过无模型方法准确预测这些状态中的每一个的值。基础真值仅仅是每个状态相对于结束于状态‘G’获得奖励的概率。因此，我在下面添加了一个表格，标明了我们要评估的每个州的标签。让我们讨论一下我们的模型，现在我们知道了我们的数据/环境和目标值，时间差异是什么。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="8778" class="ln lo iq lj b gy lp lq l lr ls">-----------------------<br/>| state | probability |<br/>-----------------------<br/>| a | 0.0 | <br/>| b | 0.167 |<br/>| c | 0.333 |<br/>| d | 0.50 |<br/>| e | 0.667 |<br/>| f | 0.833 |<br/>| g | 1.0 |<br/>-----------------------</span></pre><p id="382c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="kx">简单介绍</em></p><p id="a63e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">所以为了讨论这些算法，我将尝试用一种简单的方式来解释它们。请随意参考大卫·西尔弗讲座或萨顿和巴尔托的书，以获得更多的深度。时间差异是一个主体在事先没有环境知识的情况下通过片段从环境中学习。这意味着时间差异采用无模型或无监督的学习方法。你可以把它看作是从尝试和错误中学习。</p><p id="68a2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="kx">让我们学习希腊语</em></p><p id="717c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">你会在这篇文章中注意到一些符号，我们将讨论 3 种算法:TD(0)，TD(1)和 TD( <em class="kx"> λ </em>)。我将展示每一个方程，我们将解释它们，但让我们快速定义至少一些超参数的符号(希腊字母有时令人生畏)。</p><ol class=""><li id="9c0d" class="lt lu iq kb b kc kd kg kh kk lv ko lw ks lx kw ly lz ma mb bi translated"><strong class="kb ir"> Gamma (γ): </strong>贴现率。介于 0 和 1 之间的值。价值越高，折扣越少。</li><li id="2b83" class="lt lu iq kb b kc mc kg md kk me ko mf ks mg kw ly lz ma mb bi translated"><strong class="kb ir">λ(λ):</strong>信用分配变量。介于 0 和 1 之间的值。值越高，您可以分配给更后面的状态和操作的信用就越多。</li><li id="107d" class="lt lu iq kb b kc mc kg md kk me ko mf ks mg kw ly lz ma mb bi translated"><strong class="kb ir"> Alpha (α): </strong>学习率。我们应该接受多大的误差，从而调整我们的估计。介于 0 和 1 之间的值。较高的值调整积极，接受更多的误差，而较小的值调整保守，但可能更保守地向实际值移动。</li><li id="9fbd" class="lt lu iq kb b kc mc kg md kk me ko mf ks mg kw ly lz ma mb bi translated"><strong class="kb ir">增量(δ): </strong>数值的变化或差异。</li></ol><p id="f7bb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">所以我们开始的第一个算法是 TD(1)。在一集结束时，TD(1)以与蒙特卡罗相同的方式更新我们的值。所以回到我们的随机行走，随机向左或向右，直到在“A”或“G”着陆。一旦情节结束，则对先前的状态进行更新。正如我们上面提到的，如果λ值越高，信用就可以分配得越多，在这种情况下，λ等于 1 就是极限。这是一个重要的区别，因为 TD(1)和 MC 只在偶发环境中工作，这意味着它们需要一个“终点线”来进行更新。</p><p id="d8fd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在让我们看看算法，试着理解一下。Gt(图 2)是我们这一集看到的所有奖励的贴现总和。因此，当我们在我们的环境中旅行时，我们跟踪所有的奖励，并以折扣(γ)将它们加在一起。所以让我们表现得好像我们在大声朗读这个:在给定的时间点(时间，t+1)的即时回报(R)加上未来回报(Rt+2)的折扣(γ)等等。你可以看到，我们在未来对γ^T-1.的折现(γ)更大因此，如果γ=0.2，你在第 6 步贴现回报，你的贴现值γ变成γ⁶–1，等于 0.00032。仅 6 个时间步长后明显变小。</p><figure class="le lf lg lh gt jq gh gi paragraph-image"><div class="ab gu cl jr"><img src="../Images/37b2f9f0e679d217e8837d68f1277ee2.png" data-original-src="https://miro.medium.com/v2/format:webp/1*si4J_RMrOmAz4h5xj01NEQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Figure 2: Sum of Discounted Rewards</figcaption></figure><p id="b288" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在，我们将对我们的价值评估 V(S)进行更新。<strong class="kb ir">重要的是要知道，当你开始时，你真的没有一个好的开始估计。你用随机值或全零初始化，然后对估计值进行更新</strong>。对于我们的随机游走序列，我们将‘B’和‘F’之间所有状态的值初始化为零<code class="fe mh mi mj lj b">[0,0,0,0,0,0,1]</code>。我们不考虑终端状态，因为它们是已知的。记住我们只是试图预测非终态的值，因为我们知道终态的值。</p><p id="31eb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们将使用我们在本集看到的上述 Gt 的折扣奖励总和，并从先前的估计中减去它。这被称为 TD 误差。我们的最新估计减去先前的估计。然后，我们乘以α项，以调整我们希望更新误差量。最后，我们只需将之前的估计值 V(St)与调整后的 TD 误差相加，即可完成更新(图 3)。</p><figure class="le lf lg lh gt jq gh gi paragraph-image"><div class="ab gu cl jr"><img src="../Images/a13895c6abae3cf384bb80c0ecd02925.png" data-original-src="https://miro.medium.com/v2/format:webp/1*MOwsykJjWu73i1UH1mI7gw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Figure 3: TD(1) Update Value toward Action Return</figcaption></figure><p id="57be" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">所以这是第一集。我们做了 1 次随机行走，积累了奖励。然后，我们在每一个时间步取这些奖励，并将其与我们最初的估计值(全部为零)进行比较。我们权衡差异并调整我们先前的估计。然后重新开始。你刚学了 TD(1)或者 MC 更新！</p><p id="5c4b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">既然我们已经解释了 TD(1)，那么 TD(0)就更容易理解了。让我们看看图 3 的括号内，因为这是唯一的区别。我们不使用折扣奖励的累积和(Gt)我们只看即时奖励(Rt+1)，加上仅领先 (V(St+1))一步的<strong class="kb ir">的估计值的折扣(图 4)。</strong></p><figure class="le lf lg lh gt jq gh gi paragraph-image"><div class="ab gu cl jr"><img src="../Images/c4084e79dadeac83e786208131071b7b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*a3ED6KnKHDxYihItYsK6QQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Figure 4: TD(0) Update Value toward Estimated Return</figcaption></figure><p id="a2e7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这是 TD(0)和 TD(1)更新之间的唯一区别。请注意，我们刚刚将图 3 中的 Gt 替换为提前一步的估计。</p><p id="f717" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当我们使用估计来更新估计时，我们称之为自举。这种类型的技术比 TD(1)或 MC 有更高的偏差，因为你是从估计值中进行估计，而不是从观看整个剧集中进行估计。然而，这往往具有较低的方差。TD(0)的另一个好处是，它可以学习没有终端状态的环境，而 TD(1)则不能。</p><p id="7c12" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">假设我们想要在一集结束之前进行值更新(TD(1))并且使用多于 1 步的超前(TD(0))来进行我们的估计。这就是 TD(λ)发挥作用的地方。要知道 TD(λ)有两种实现:前视和后视。前瞻视图查看前面的所有 n 步，并使用λ从本质上衰减那些未来的估计。在这篇文章中，我们将继续使用 TD(λ)的后向视图，但是已经证明前向视图和后向视图是等价的，Sutton 表明如果感兴趣的话，这里的<a class="ae jy" href="http://incompleteideas.net/book/ebook/node76.html" rel="noopener ugc nofollow" target="_blank">就是</a>。</p><p id="1963" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">TD(λ)的后向视图在每一步更新值。所以在每集的每一步之后，你都要更新之前的所有步骤。问题是你如何恰当地权衡或分配前面所有步骤的功劳？答案是使用一种叫做资格追踪(ET)的东西。ET 基本上记录了进入给定状态的频率和最近次数(图 4)。它将信用分配给相对于我们的最终状态频繁访问和最近访问的状态。lambda (λ)和 gamma (γ)项用于贴现这些轨迹。</p><figure class="le lf lg lh gt jq gh gi paragraph-image"><div class="ab gu cl jr"><img src="../Images/8c909255501a616d9d0b68d0aa43e44a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*QXS4R7er5ie2N2pX76MOdQ.png"/></div></figure><figure class="le lf lg lh gt jq gh gi paragraph-image"><div class="ab gu cl jr"><img src="../Images/b301693a6987d758071a8e4acf53ed27.png" data-original-src="https://miro.medium.com/v2/format:webp/1*O4IM-50zekkD6awIRk8Gjg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Figure 4: Eligibility Traces</figcaption></figure><p id="0960" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果你直觉地思考这个问题，我们知道状态‘F’将会被频繁地访问，并且在最近结束于我们的最终状态‘G ’,因此将会收到许多更新。因此，ET 将根据 TD 误差(图 5，αδtEt(s))的比例向状态“F”分配更多信用。然而，相对于终止于状态‘G’而言，状态‘B’不会被频繁地访问，因此该状态的值不会被频繁地更新，并且将保持接近于其初始化时的 0。从这里开始，我们不断更新我们先前的估计。</p><figure class="le lf lg lh gt jq gh gi paragraph-image"><div class="ab gu cl jr"><img src="../Images/afd7fa1ad40cdc77848370c89afa5d43.png" data-original-src="https://miro.medium.com/v2/format:webp/1*dDWA843istUJvtW4MeYZVg.png"/></div></figure><figure class="le lf lg lh gt jq gh gi paragraph-image"><div class="ab gu cl jr"><img src="../Images/9bb576ebca5194aae4b9a110a16d4e60.png" data-original-src="https://miro.medium.com/v2/format:webp/1*He8i_VJFfsYhU-jxJnS2dg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Figure 5: Update with Respect to TD Error (δt) and Eligibility (Et(s))</figcaption></figure><p id="9255" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">所以我希望这是一种简单的学习方式。它的要点是我们做一个初始估计，探索一个空间，并根据我们的探索努力更新我们先前的估计。强化学习的困难部分似乎是在哪里应用它，环境是什么，我如何正确地设置我的奖励，等等，但至少现在你理解了状态空间的探索和用无监督的无模型方法进行估计。</p><p id="2ffb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">1.理查德·萨顿《学习用时间差异的方法预测》<em class="kx">机器学习</em>，第 3 卷第 1 期，1988 年，第 9–44 页。，doi:10.1007/bf00115009。</p><p id="6e10" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">2.西尔弗，大卫，导演。<em class="kx">大卫·西尔弗的 RL 课程——第四讲:无模型预测</em>。<em class="kx"> YouTube </em>，YouTube，2015 年 5 月 13 日，<a class="ae jy" href="http://www.youtube.com/watch?v=PnHCvfgC_ZA." rel="noopener ugc nofollow" target="_blank">www.youtube.com/watch?v=PnHCvfgC_ZA.</a></p><p id="76a5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">3.RL 创始人理查德·萨顿的免费下载，<a class="ae jy" href="https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view" rel="noopener ugc nofollow" target="_blank">强化学习:简介</a></p><p id="8022" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">4.这允许你编写 LaTex 并保存为图像。可能有更好的，但这是我用的。<a class="ae jy" href="https://www.codecogs.com/latex/eqneditor.php" rel="noopener ugc nofollow" target="_blank">在线乳胶编辑</a></p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><p id="647b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="kx">原载于 2018 年 10 月 29 日</em><a class="ae jy" href="https://medium.com/@violante.andre/simple-reinforcement-learning-temporal-difference-learning-e883ea0d65b0" rel="noopener"><em class="kx">【medium.com</em></a><em class="kx">。</em></p></div></div>    
</body>
</html>