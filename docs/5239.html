<html>
<head>
<title>Entropy is a measure of uncertainty</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">熵是不确定性的度量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/entropy-is-a-measure-of-uncertainty-e2c000301c2c?source=collection_archive---------0-----------------------#2018-10-06">https://towardsdatascience.com/entropy-is-a-measure-of-uncertainty-e2c000301c2c?source=collection_archive---------0-----------------------#2018-10-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a05c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">八个性质，几个例子和一个定理</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0d842b30354d10f1f6da224e84bfe568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uYHdWYGan1kfOaiu7EF03Q.png"/></div></div></figure><p id="e6c8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">假设你正在医生办公室的候诊室里与三个病人交谈。他们三人都刚刚完成了一项医学测试，经过一些处理后，得出两种可能的结果之一:疾病要么存在，要么不存在。让我们假设我们正在与好奇和面向数据的个人打交道。他们已经提前研究了他们特定风险状况的概率，现在急于找出结果。</p><p id="fd28" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">患者 A 知道，从统计学上来说，他有 95%的可能性患有所述疾病。对于患者 B 来说，被确诊患病的概率是 30%。相比之下，病人 C 面临着 50/50 的可能性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/010e19ae834430efcabe78872bf37e45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*dmTs2OeXV05RU-9c3Dljow.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Uncertainty in the waiting room</figcaption></figure><p id="117f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我想着重谈一个简单的问题。在其他条件相同的情况下，三个病人中哪一个面临最大程度的不确定性？</p><p id="31d7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我认为答案是明确的:病人 c。他不仅正在经历“许多不确定性”。他正在经历的是在这种情况下最大程度的不确定性:一个戏剧性的医学版的掷硬币游戏。</p><p id="6441" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">与病人 a 相比，当然，整体情况看起来相当严峻，但至少这个病人对他的医疗前景没有多少不确定性。</p><p id="6156" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">从直觉上讲，我们能说病人 B 什么呢？也许她的情况属于“中间某处”？</p><p id="affe" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这就是熵产生的原因。将一种情况描述为“中间的某个地方”对于候诊室的谈话来说可能已经足够好了，但是对于机器学习的目的来说，这种描述肯定太粗糙了。</p><h2 id="0454" class="ls lt iq bd lu lv lw dn lx ly lz dp ma la mb mc md le me mf mg li mh mi mj mk bi translated">测量不确定性</h2><p id="18af" class="pw-post-body-paragraph kr ks iq kt b ku ml jr kw kx mm ju kz la mn lc ld le mo lg lh li mp lk ll lm ij bi translated">熵允许我们对生活中最紧迫的问题之一做出精确的陈述和进行计算:不知道事情会如何发展。</p><p id="5ba9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">换句话说，熵是对不确定性的一种度量。</p><p id="9adb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">(它也是一种信息的度量，但是，我个人更倾向于不确定性的解释。可能只是我，但当我不再试图将我对信息的先入为主的观念强加于方程式时，事情似乎变得清晰多了。)</p><p id="dcdf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在某种程度上，说熵是“<em class="mq">不确定性的</em>度量”是一种轻描淡写。给定某些假设(并预示着下面提到的一个重要结果)，熵是不确定性的<em class="mq">度量。</em></p><p id="1de7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">顺便说一下，当我使用熵这个术语时，我指的是<strong class="kt ir">香农熵</strong>。还有很多其他熵，但我认为可以肯定的是，香农熵是自然语言处理和机器学习中使用最频繁的熵。</p><p id="8764" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">事不宜迟，这里是一个事件的熵公式<em class="mq"> X </em>有<em class="mq"> n </em>个可能的结果和概率<em class="mq"> p_1，…，p_n </em>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/476fb814498c53e1494723d1a1b09d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*a50ZrZrpo_Ny7MGG.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Shannon entropy</figcaption></figure><h1 id="281a" class="mt lt iq bd lu mu mv mw lx mx my mz ma jw na jx md jz nb ka mg kc nc kd mj nd bi translated">基本属性</h1><p id="9df4" class="pw-post-body-paragraph kr ks iq kt b ku ml jr kw kx mm ju kz la mn lc ld le mo lg lh li mp lk ll lm ij bi translated">如果你像我第一次看到这个公式时一样，你可能会问自己这样的问题:为什么是对数？为什么这是一个很好的不确定性的衡量标准？当然，为什么是字母<em class="mq"> H </em>？(显然，使用英文字母<em class="mq"> H </em>是从<a class="ae mr" href="https://math.stackexchange.com/questions/84719/why-is-h-used-for-entropy" rel="noopener ugc nofollow" target="_blank">希腊文大写字母 Eta </a>演变而来，尽管历史看起来相当复杂。)</p><p id="b766" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">随着时间的推移，我学到的一件事是，一个好的起点——在这里和许多其他情况下——是问两个问题:(1)我试图理解的数学构造具有哪些理想的属性？以及(2)它们是具有所有这些期望特性的竞争结构吗？</p><p id="5f12" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">简而言之，作为不确定性度量的香农熵的答案是:(1)许多和(2)没有。</p><p id="3b7b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们从愿望清单开始。</p><h2 id="5f87" class="ls lt iq bd lu lv lw dn lx ly lz dp ma la mb mc md le me mf mg li mh mi mj mk bi translated">基本性质 1:均匀分布具有最大的不确定性</h2><p id="9844" class="pw-post-body-paragraph kr ks iq kt b ku ml jr kw kx mm ju kz la mn lc ld le mo lg lh li mp lk ll lm ij bi translated">如果你的目标是最小化不确定性，远离均匀概率分布。</p><p id="b01a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">快速提醒:概率分布是一个函数，它为每一个可能的结果分配一个概率，使得概率总和为 1。当所有结果具有相同的概率时，分布是均匀的。例如，公平硬币(50%的尾部，50%的尾部)和公平骰子(六个面中的每一个面的概率为 1/6)遵循均匀分布。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/14699064f3bd0f2fa7278d73fe2edf99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*sYmGmOiSB9XL-rPoqdbnRw.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Uniform distributions have maximum entropy for a given number of outcomes.</figcaption></figure><p id="7d38" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于均匀分布，不确定性的良好度量达到其最高值。熵满足标准。给定<em class="mq"> n </em>种可能的结果，最大熵由等概率结果最大化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/0deeb7a30350974505ac271f2f9da7e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/0*bEhsYxBf_CP4qTI6.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Equiprobable outcomes</figcaption></figure><p id="6219" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是应用于伯努利试验的熵函数图(具有两种可能结果和概率的事件<em class="mq"> p </em>和<em class="mq"> 1-p </em>):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/be61ce4947b9ccd03ed76545f23b5a6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*8JwLMUrBgGWM7rWr.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">In the case of Bernoulli trials, entropy reaches its maximum value for p=0.5</figcaption></figure><h2 id="1562" class="ls lt iq bd lu lv lw dn lx ly lz dp ma la mb mc md le me mf mg li mh mi mj mk bi translated">基本性质 2:不确定性对于独立事件是可加的</h2><p id="f94d" class="pw-post-body-paragraph kr ks iq kt b ku ml jr kw kx mm ju kz la mn lc ld le mo lg lh li mp lk ll lm ij bi translated">设<em class="mq"> A </em>和<em class="mq"> B </em>为独立事件。换句话说，知道事件<em class="mq"> A </em>的结果并不能告诉我们任何关于事件<em class="mq"> B </em>的结果。</p><p id="e506" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">与这两个事件相关的不确定性——这是我们愿望清单上的另一个项目——应该是各个不确定性的总和:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/33a566a55c30c4c4dc5d88923e8c4bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/0*9VDWro34ADgoajyb.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Uncertainty is additive for independent events.</figcaption></figure><p id="9ac6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们用抛两枚硬币的例子来更具体地说明这一点。我们可以同时掷两枚硬币，或者先掷一枚硬币，然后再掷另一枚。另一种思考方式是，我们可以同时或分别报告两次抛硬币的结果。这两种情况下的不确定性是一样的。</p><p id="2ab7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了使这一点更加具体，考虑两个特殊的硬币。第一枚硬币正面朝上(<em class="mq"> H </em>)的概率为 80%，反面朝上(<em class="mq"> T </em>)的概率为 20%。另一枚硬币的概率是 60%和 40%。如果我们同时抛两枚硬币，有四种可能的结果:<em class="mq"> HH </em>、<em class="mq"> HT </em>、<em class="mq"> TH </em>和<em class="mq"> TT </em>。对应的概率由<em class="mq">【0.48，0.32，0.12，0.08】</em>给出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/e0a57ef2135e46cbeaadb2cf5f91fd3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*5bfcQeKVRypkSt3YBU-BHA.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">The joint entropy (green) for the two independent events is equal to the sum of the individual events (red and blue).</figcaption></figure><p id="ae0f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">将这些数字代入熵公式，我们看到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/909822594e833a5fa50e83739b702143.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1zlK66Cwi8T2q82L.png"/></div></div></figure><p id="8edd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">就像承诺的那样。</p><h2 id="2c92" class="ls lt iq bd lu lv lw dn lx ly lz dp ma la mb mc md le me mf mg li mh mi mj mk bi translated">基本属性 3:添加概率为零的结果没有效果</h2><p id="4751" class="pw-post-body-paragraph kr ks iq kt b ku ml jr kw kx mm ju kz la mn lc ld le mo lg lh li mp lk ll lm ij bi translated">假设(A)每当结果#1 出现时你就赢了，并且(B)你可以在两个概率分布中选择，<em class="mq"> A </em>和<em class="mq"> B </em>。分配<em class="mq"> A </em>有两种结果:比如说 80%和 20%。分布 B 有三种结果，概率分别为 80%、20%和 0%。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/47ebcd639bf256cc64a230a585169da0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*zCwXVLPDEybOYKZ4YDVnUw.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Adding a third outcome with zero probability doesn’t make a difference.</figcaption></figure><p id="4d5c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">给选项<em class="mq"> A </em>和<em class="mq"> B </em>，你会选哪个？此时一个合适的反应是耸耸肩或者翻白眼。包含第三种结果既不增加也不减少与游戏相关的不确定性。<em class="mq"> A </em>还是<em class="mq"> B </em>，谁在乎。没关系。</p><p id="7d14" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">熵公式同意这种评估:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/9bcdf396b988f61b3e95ce6332125ecc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SVvx3qTzKCBrAvfK.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Adding a zero-probability outcome has not effect on entropy.</figcaption></figure><p id="fbc3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">换句话说，添加一个概率为零的结果对不确定性的测量没有影响。</p><h2 id="b5a3" class="ls lt iq bd lu lv lw dn lx ly lz dp ma la mb mc md le me mf mg li mh mi mj mk bi translated">基本性质 4:不确定性的度量在其所有论证中是连续的</h2><p id="710e" class="pw-post-body-paragraph kr ks iq kt b ku ml jr kw kx mm ju kz la mn lc ld le mo lg lh li mp lk ll lm ij bi translated">最后一个基本属性是连续性。</p><p id="bd61" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">众所周知，对连续函数的直观解释是没有“间隙”或“洞”。更准确地说，输出中任意小的变化(在我们的例子中是不确定性)应该可以通过输入中足够小的变化(概率)来实现。</p><p id="e90b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对数函数在为其定义的每一点上都是连续的。在子集上连续的有限个函数的和与积也是如此。由此可见，熵函数在其概率论证中是连续的。</p><h1 id="b3b3" class="mt lt iq bd lu mu mv mw lx mx my mz ma jw na jx md jz nb ka mg kc nc kd mj nd bi translated">唯一性定理</h1><p id="36dd" class="pw-post-body-paragraph kr ks iq kt b ku ml jr kw kx mm ju kz la mn lc ld le mo lg lh li mp lk ll lm ij bi translated"><a class="ae mr" href="https://books.google.de/books/about/Mathematical_Foundations_of_Information.html?id=0uvKF-LT_tMC&amp;redir_esc=y" rel="noopener ugc nofollow" target="_blank">钦钦(1957) </a>表明，满足上述四个基本性质的唯一函数族具有以下形式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/7216ebbd240b0bb21a13f9869766269f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7WTvAHK5AFKX-Gnu.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Functions that satisfy the four basic properties</figcaption></figure><p id="fa99" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">其中λ是正常数。钦钦将此称为<strong class="kt ir">唯一性定理</strong>。设λ = 1 并使用二进制对数，我们得到香农熵。</p><p id="aa31" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">再次重申，使用熵是因为它具有令人满意的性质，并且是满足基本愿望清单(性质 1-4)上所有项目的家族函数中的自然选择。(以后我可能会在单独的文章中讨论这个证明。)</p><h1 id="9dbe" class="mt lt iq bd lu mu mv mw lx mx my mz ma jw na jx md jz nb ka mg kc nc kd mj nd bi translated">其他属性</h1><p id="c792" class="pw-post-body-paragraph kr ks iq kt b ku ml jr kw kx mm ju kz la mn lc ld le mo lg lh li mp lk ll lm ij bi translated">除了钦钦唯一性定理中使用的四个基本性质之外，熵还有许多其他性质。让我在此仅提及其中一些。</p><h2 id="4a50" class="ls lt iq bd lu lv lw dn lx ly lz dp ma la mb mc md le me mf mg li mh mi mj mk bi translated">性质 5:具有更多结果的均匀分布具有更多的不确定性</h2><p id="bef6" class="pw-post-body-paragraph kr ks iq kt b ku ml jr kw kx mm ju kz la mn lc ld le mo lg lh li mp lk ll lm ij bi translated">假设你可以在一枚漂亮的硬币和一个漂亮的骰子之间做出选择:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/14699064f3bd0f2fa7278d73fe2edf99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*sYmGmOiSB9XL-rPoqdbnRw.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Fair coin or fair die?</figcaption></figure><p id="a61f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">假设硬币正面朝上或者骰子正面朝上，你就赢了。</p><p id="066e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这两个选项你会选哪个？如果你是利润最大化者，A 和<em class="mq"> B </em>如果你喜欢更多的变化和不确定性。</p><p id="19ae" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">随着等概率结果数量的增加，我们对不确定性的度量也应该增加。</p><p id="0b43" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">而这正是熵的作用:H(1/6，1/6，1/6，1/6，1/6，1/6，1/6)，H(0.5，0.5)。</p><p id="e03c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">一般来说，如果我们让<em class="mq"> L(k) </em>是具有<em class="mq"> k </em>个可能结果的均匀分布的熵，我们有</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/67e52234b555b014f04652a3d39fa27e.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/0*vUT1_dOFGNMms4fk.png"/></div></figure><p id="0208" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于<em class="mq"> m &gt; n </em>。</p><h2 id="a86f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma la mb mc md le me mf mg li mh mi mj mk bi translated">属性 6:事件具有非负的不确定性</h2><p id="56de" class="pw-post-body-paragraph kr ks iq kt b ku ml jr kw kx mm ju kz la mn lc ld le mo lg lh li mp lk ll lm ij bi translated">你知道什么是负面不确定性吗？我也不知道。</p><p id="7f7d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">一个用户友好的不确定性度量应该总是返回一个非负的量，不管输入是什么。</p><p id="1415" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是熵满足的另一个标准。让我们再来看看这个公式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/2c983e9a742be9b60f769a002c6c73eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/0*DkWdyGidNSfdT1Nu.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Shannon entropy</figcaption></figure><p id="00b5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">根据定义，概率的范围在 0 和 1 之间，因此是非负的。概率的对数是非正的。概率的对数乘以一个概率不会改变符号。非正乘积之和为非正。最后，非正值的负值是非负值。因此，对于每个可能的输入，熵都不是负的。</p><h2 id="fe71" class="ls lt iq bd lu lv lw dn lx ly lz dp ma la mb mc md le me mf mg li mh mi mj mk bi translated">特性 7:具有特定结果的事件没有不确定性</h2><p id="9dfd" class="pw-post-body-paragraph kr ks iq kt b ku ml jr kw kx mm ju kz la mn lc ld le mo lg lh li mp lk ll lm ij bi translated">假设你拥有一枚神奇的硬币。不管你怎么抛硬币，它总是正面朝上。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/29b73bb14b788d2940228c927da29c57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*81paMOCPyN8D7nfNbGv52Q.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">A magical coin</figcaption></figure><p id="ccd0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">你如何量化魔法或任何其他情况下某个结果肯定会发生的不确定性？嗯，没有。所以自然的答案——我想，你会同意——是 0。</p><p id="5037" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">熵是否认同这种直觉？当然了。</p><p id="3e50" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">假设结果肯定会发生。由此得出<em class="mq"> p_i </em>，结果<em class="mq"> i </em>的概率等于 1。<em class="mq"> H(X)，</em>由此，简化为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/2e418af8d31c72f40409e305bb61983d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yHq88Rtv1qPydTro.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">The entropy for events with a certain outcome is zero.</figcaption></figure><h2 id="11b5" class="ls lt iq bd lu lv lw dn lx ly lz dp ma la mb mc md le me mf mg li mh mi mj mk bi translated">属性 8:翻转参数没有效果</h2><p id="9ea4" class="pw-post-body-paragraph kr ks iq kt b ku ml jr kw kx mm ju kz la mn lc ld le mo lg lh li mp lk ll lm ij bi translated">这是另一个明显可取的属性。考虑两种情况。第一种情况，正面和反面的概率分别是 80%和 20%。在第二种情况下，概率正好相反:正面 20%，反面 80%。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/8a92dc5bb25fdea00719097462bf8e93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Ec5q_ZzsbtCpUOxVmbfRkA.png"/></div></figure><p id="2945" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">两次抛硬币的不确定性相同，熵相同:<em class="mq"> H(0.8，0.2) = H(0.2，0.8) </em>。</p><p id="2c7e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">更一般地说，对于两种结果的情况，我们有:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/6f303494dc96bedf7fa866a444399bdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/0*XoMOqQNmHioYSvMQ.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Flipping arguments</figcaption></figure><p id="744e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这个事实适用于任何数量的结果。我们可以按照我们喜欢的任何顺序排列论点(即分布的概率)。熵函数的结果总是相同的。</p><h1 id="6e9a" class="mt lt iq bd lu mu mv mw lx mx my mz ma jw na jx md jz nb ka mg kc nc kd mj nd bi translated">摘要</h1><p id="885a" class="pw-post-body-paragraph kr ks iq kt b ku ml jr kw kx mm ju kz la mn lc ld le mo lg lh li mp lk ll lm ij bi translated">概括地说，香农熵是不确定性的度量。</p><p id="9bfe" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">它被广泛使用是因为它满足某些标准(也因为生活充满了不确定性)。唯一性定理告诉我们，只有一个函数族具有我们提到的所有四个基本性质。香农熵是这个家族中的自然选择。</p><p id="b2de" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">除了其他事实之外，熵对于均匀分布是最大的(属性#1)，对于独立事件是可加的(#2)，对于非零概率的结果数量增加(#3 和#5)，连续的(#4)，非负的(#6)，对于某些结果为零(#7)和排列不变的(#8)。</p></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="7499" class="mt lt iq bd lu mu nv mw lx mx nw mz ma jw nx jx md jz ny ka mg kc nz kd mj nd bi translated">感谢您的阅读！如果你喜欢这篇文章，请点击“鼓掌”按钮，跟随我了解更多关于自然语言处理和机器学习的知识。</h1><h1 id="2280" class="mt lt iq bd lu mu mv mw lx mx my mz ma jw na jx md jz nb ka mg kc nc kd mj nd bi translated">此外，让我知道如果你有一个项目在这两个领域的交叉点，你想讨论。</h1></div></div>    
</body>
</html>