<html>
<head>
<title>Sentence Embedding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">句子嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentence-embedding-3053db22ea77?source=collection_archive---------0-----------------------#2017-02-05">https://towardsdatascience.com/sentence-embedding-3053db22ea77?source=collection_archive---------0-----------------------#2017-02-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="b5dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">文献综述:</strong></p><p id="fd8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，让我们从单词嵌入开始，这些是单词在 n 维向量空间中的表示，以便语义相似(例如“船”-船只)或语义相关(例如“船”-水)的单词根据训练方法变得更接近。培训方法大致分为两部分:</p><ol class=""><li id="2bd3" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">使用文档作为上下文。(<a class="ae ku" href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" rel="noopener ugc nofollow" target="_blank"> LSA </a>，话题模特)。他们捕捉语义关联。</li><li id="0652" class="kl km iq jp b jq kv ju kw jy kx kc ky kg kz kk kq kr ks kt bi translated">使用单词作为上下文。(神经语言模型，分布式语义模型)。他们捕捉语义的相似性。</li></ol><p id="569d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在平行线上，我们希望开发一些东西，可以捕捉句子之间的语义相似性或相关性，然后是段落，然后是文档。</p><p id="43ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在暂时假设我们有单词向量。</p><p id="4699" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">获得句子表示的一种方法是将句子中包含的所有单词向量的表示相加，这被称为单词质心。并且两个句子之间的相似度可以通过 c <strong class="jp ir">熵距离</strong>来计算。同样的事情可以扩展到段落和文档。但是这种方法忽略了很多信息，比如序列，并且可能给出错误的结果。比如:</p><ol class=""><li id="2eea" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">你要去那里教书，而不是玩。</li><li id="b596" class="kl km iq jp b jq kv ju kw jy kx kc ky kg kz kk kq kr ks kt bi translated">你要去那里玩而不是教书</li></ol><p id="2dca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些将有相同的最终表现，但你可以看到，意义是完全不同的。</p><p id="a01a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，通过引入编辑距离方法对其进行了一点微调，这被称为<strong class="jp ir">字移动器的距离</strong>。它来自于发表在 em NLP’14 上的论文“<a class="ae ku" href="http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf" rel="noopener ugc nofollow" target="_blank">从单词嵌入到文档距离</a>”。这里我们取句子 1 到句子 2 中每个单词的最小距离，并将它们相加。比如:</p><ol class=""><li id="7d21" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">奥巴马在伊利诺伊州对媒体讲话</li><li id="4912" class="kl km iq jp b jq kv ju kw jy kx kc ky kg kz kk kq kr ks kt bi translated">总统在芝加哥迎接媒体</li></ol><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi la"><img src="../Images/286ce4f0c6f520eb96d4812258198edd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JICAZM0gRjD9kSyMZtm99Q.png"/></div></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Image taken from original paper</figcaption></figure><ul class=""><li id="6493" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk lq kr ks kt bi translated">奥巴马匹配总统</li><li id="e360" class="kl km iq jp b jq kv ju kw jy kx kc ky kg kz kk lq kr ks kt bi translated">向问候者说话</li><li id="ded2" class="kl km iq jp b jq kv ju kw jy kx kc ky kg kz kk lq kr ks kt bi translated">媒体报道</li><li id="7820" class="kl km iq jp b jq kv ju kw jy kx kc ky kg kz kk lq kr ks kt bi translated">伊利诺伊到芝加哥</li></ul><p id="35dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种方法正在产生令人鼓舞的结果。论文<a class="ae ku" href="http://nlp.cs.aueb.gr/pubs/BioNLP_2016_BioIR.pdf" rel="noopener ugc nofollow" target="_blank">在问答</a>中使用单词嵌入的质心和单词移动器的距离进行生物医学文献检索，使用质心距离进行初始剪枝，然后使用 WMD 得到更好的结果。(注意 WMD 慢。是<code class="fe lr ls lt lu b">O(n*m)</code>，其中<code class="fe lr ls lt lu b">n</code>是<code class="fe lr ls lt lu b">sentence1</code>的长度，<code class="fe lr ls lt lu b">m</code>是<code class="fe lr ls lt lu b">sentence2</code>的长度</p><p id="de69" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是我们看到，在这两种方法中，我们没有使用来自序列的信息。所以在这个领域也有很多研究。</p><p id="e3a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">论文中，<a class="ae ku" href="https://arxiv.org/abs/1408.5882" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">卷积神经网络用于句子分类</strong></a><strong class="jp ir"/>其中曾提出 CNN 用于同样的事情。它在句子中使用了填充，使它们具有相同的维度- &gt;使用单词嵌入来映射这些填充句子中的单词- &gt;应用 CNN - &gt;使用最大超时池- &gt;馈送到全连接层- &gt;获取表示。他们写道，使用 word2vec 初始化单词嵌入，然后通过反向传播进行微调，得到了比“不调整”和“随机初始化”更好的结果。</p><p id="a4c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后是微软研究院 2015 年发表的关于<strong class="jp ir">深度语义相似度模型</strong>的论文，本文使用不同类型的词向量，这里是基于 n-grams。例:对于 hello 这个词，他们会加上起始和结束标记，并分解成 n 个字母组，即 hello - &gt; #hello# - &gt; {#he，hel，ell，llo，lo#}。并以向量形式表示它，向量的大小为(27*26*27(如果不使用哈希))(请注意，它与词汇大小<strong class="jp ir">无关，并概括了看不见的单词，对拼写错误具有鲁棒性)。他们将这些向量的序列(用于表示句子)放入他们的模型，该模型给他们一个语义向量，并且基于<strong class="jp ir">已知的</strong>这些句子的相似性或不相似性，他们被训练(弱监督)。在该模型中，他们使用了具有最大池的 CNN。<strong class="jp ir">CNN 提取局部特征，max pooling 层从中生成全局特征。</strong>我已经在 short science<a class="ae ku" href="http://www.shortscience.org/paper?bibtexKey=conf/www/ShenHGDM14#nishnik" rel="noopener ugc nofollow" target="_blank">这里</a>写了一篇关于这篇论文的小总结。</strong></p><p id="d399" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，在训练期间，句子与具有相似性(肯定)和不相似性(否定)的句子一起提供。该模型可以被训练用于最大化(文件，阳性的余弦相似性)-文件，阴性的余弦相似性)。他们使用了随机梯度下降法。</p><p id="5978" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是我们不能在这里使用 rnn 吗？在这里，它们将拥有所有文档的内存，并在查询时返回所需的文档。根据 MetaMind 的这篇论文，<strong class="jp ir"> </strong> <a class="ae ku" href="https://arxiv.org/abs/1603.01417" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">用于视觉和文本问题回答的动态记忆网络</strong> </a>，其中他们使用 GRU 将数据编码为事实，并将问题编码为向量，然后使用问题向量并循环事实以生成上下文向量并更新记忆。基于记忆，它能够回答。我们可以用同样的方法来寻找文档的相似性。我已经在 short science 这里<a class="ae ku" href="http://www.shortscience.org/paper?bibtexKey=journals/corr/1603.01417#nishnik" rel="noopener ugc nofollow" target="_blank">写了一篇关于这篇论文的小总结。</a></p><p id="6ecf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文中提出的进一步的观点是:使用<strong class="jp ir">双向</strong> GRUs(在本文中有更好的解释，<strong class="jp ir"> " </strong> <a class="ae ku" href="https://arxiv.org/abs/1611.03382" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">具有再次读取和复制机制的有效摘要</strong> </a>")，以及使用句子嵌入而不是单词嵌入。</p><p id="43e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后还有一篇论文在这个领域有很强的基础，<a class="ae ku" href="https://arxiv.org/abs/1405.4053" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">句子和文档的分布式表示</strong> s </a>，它来自谷歌，发表于 2014 年。它提出以与单词向量相同的方式学习段落向量，并将其用于无数任务，包括情感分析和信息检索。</p><p id="ad61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如有任何疑问，请发邮件至 nishantiam@gmail.com<a class="ae ku" href="mailto:nishantiam@gmail.com" rel="noopener ugc nofollow" target="_blank">给我。</a></p></div></div>    
</body>
</html>