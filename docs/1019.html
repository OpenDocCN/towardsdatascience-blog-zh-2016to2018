<html>
<head>
<title>SELU — Make FNNs Great Again (SNN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SELU——让FNNs再次伟大(SNN)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9?source=collection_archive---------1-----------------------#2017-07-21">https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9?source=collection_archive---------1-----------------------#2017-07-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c294" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上个月，我看到了一篇最近的文章(发表于2017年6月22日),文章提出了一个新概念，叫做自归一化网络(SNN)。在这篇文章中，我将回顾它们的不同之处，并展示一些对比。<br/>文章链接——<a class="ae kl" href="https://arxiv.org/pdf/1706.02515.pdf" rel="noopener ugc nofollow" target="_blank">Klambauer等人</a> <br/>本帖代码摘自<a class="ae kl" href="https://github.com/bioinf-jku/SNNs" rel="noopener ugc nofollow" target="_blank">bio info-jku的github </a>。</p></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><h1 id="1bfc" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">这个想法</h1><p id="8a9f" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">在我们进入什么是SNN之前，让我们谈谈创建它们的动机。作者在文章的摘要中提到了一个很好的观点；虽然神经网络在许多领域获得了成功，但似乎主要阶段属于卷积网络和递归网络(LSTM，GRU)，而前馈神经网络(FNNs)被留在初学者教程部分。<br/>还要注意的是，在Kaggle取得胜利结果的FNN最多只有4层。</p><p id="00b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当使用非常深的架构时，网络变得容易出现梯度问题，这正是批量归一化成为标准的原因——这是作者将FNNs的薄弱环节放在训练中对归一化的敏感性上的地方。<br/>snn是一种替代使用外部标准化技术(如batch norm)的方法，标准化发生在激活函数内的<strong class="jp ir">。<br/>为了清楚起见，激活函数建议(SELU-比例指数线性单位)输出归一化值，而不是归一化激活函数的输出。<br/>为了使SNNs工作，它们需要两样东西，一个定制的权重初始化方法和SELU激活函数。</strong></p><h1 id="af1c" class="kt ku iq bd kv kw lw ky kz la lx lc ld le ly lg lh li lz lk ll lm ma lo lp lq bi translated">认识SELU</h1><p id="85d5" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">在我们解释它之前，让我们看一下它是关于什么的。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/61a235981765ddd7aef2a5b48debf1a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*Q_lez8e2mP7MdSZf-O5bKw.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Figure 1 The scaled exponential linear unit, taken from the article</figcaption></figure><p id="2c93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">SELU是某种ELU，但有一点扭曲。<br/> α和λ是两个固定的参数，这意味着我们不会通过它们进行反向传播，它们也不是需要做出决策的超参数。<br/> α和λ是从输入中得到的——我不会深入讨论这个，但是你可以在文章中看到你自己的数学计算(有93页附录:O，数学计算)。<br/>对于标准比例输入(平均值为0，标准偏差为1)，值为α=1.6732~，λ=1.0507~。<br/>让我们绘制图表，看看这些值是什么样子。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mn"><img src="../Images/46155e28407e62d791ba5b83e373674b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m0e8lZU_Zrkh4ESfQkY2Pw.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Figure 2 SELU plotted for α=1.6732~, λ=1.0507~</figcaption></figure><p id="11ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看起来很像leaky ReLU，但等着看它的魔力吧。</p></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><h1 id="8028" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">重量初始化</h1><p id="369e" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">SELU不能让它单独工作，所以一个定制的权重初始化技术正在使用。<br/> SNNs用零均值初始化权重，并使用1/(输入大小)的平方根的标准偏差。<br/>代码如下所示(摘自开篇提到的github)</p><pre class="mc md me mf gt ms mt mu mv aw mw bi"><span id="b826" class="mx ku iq mt b gy my mz l na nb"># Standard layer<br/>tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=np.sqrt(<br/>1 / n_input))</span><span id="cfca" class="mx ku iq mt b gy nc mz l na nb"># Convolution layer<br/>tf.Variable(tf.random_normal([5, 5, 1, 32], stddev=np.sqrt(1/25)))</span></pre><p id="0b59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们已经了解了初始化和激活方法，让我们开始工作吧。</p></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><h1 id="f046" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">表演</h1><p id="cf11" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">让我们看看SNNs如何使用指定的初始化和SELU激活函数在MNIST和CIFAR-10数据集上运行。<br/>首先，让我们看看在2层SNN(两个隐藏层都是784个节点，MNIST)上使用TensorBoard是否真的保持了输出的标准化。<br/>绘制第1层的激活函数输出和第2层的权重。<br/>github代码中没有<code class="fe nd ne nf mt b">layer1_act</code>的绘图，我是为了这个直方图才添加的。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/31176d789ae9b18d811df39e63f69d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*BHpTnjHJtOIJsu8gCEF1mQ.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Figure 3 SELU’s output after the first layer in the MLP from the github</figcaption></figure><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/af0a1c5499002b47c4c847389f4d8f87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*VJAdiFNEel1DEsvAP6FtSw.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Figure 4 Second layer’s weights on the second layer.</figcaption></figure><p id="a936" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不出所料，第一层的激活和第二层产生的权重几乎都是完美的零均值(我在运行中得到0.000201)。<br/>相信我，直方图在第一层权重上几乎是一样的。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/f0bb91e149b219073ab18259b7f636a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*AFvTGX3dV9eu9CNEto4duw.png"/></div></figure><p id="9cc3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更重要的是，snn似乎能够表现得更好，正如你从提到的github的图中看到的那样，比较了3个具有相同架构的卷积网络，只是它们的激活功能和初始化不同。<br/> SELU vs ELU vs雷鲁。</p><p id="2752" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">似乎SELU收敛得更好，在测试集上得到更好的精度。</p><p id="8366" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，使用SELU +上述初始化，我们在CNN网络上获得了更高的精度和更快的收敛速度，因此不要犹豫，在非纯FNN的架构上尝试它，因为它似乎也能够提高其他架构的性能。</p></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><h1 id="aad0" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">结论和进一步阅读</h1><p id="ce2a" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">看起来snn确实可以在神经网络的世界中找到自己的位置，也许可以在更短的时间内提高一点额外的准确性——但我们必须等待，看看它们自己会产生什么结果，更重要的是整合到联合架构中(就像上面的conv-snn)。也许我们会在竞赛获奖的建筑中遇到他们，谁知道呢。</p><p id="61eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有些东西我没有在这里讨论，但在这篇文章中提到了，比如提议的“alpha dropout ”,这是一种符合SNNs概念的dropout技术，也在提到的github中实现，所以非常欢迎您深入研究。</p><p id="6327" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">SNNs是不是一个东西，我真的不知道，但他们是另一个工具添加到您的工具包。希望你喜欢这篇文章并学到一些新东西:)</p></div></div>    
</body>
</html>