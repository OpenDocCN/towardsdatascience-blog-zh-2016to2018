<html>
<head>
<title>Empirical Analysis on Email Classification Using the Enron Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于安然数据集的电子邮件分类实证分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/empirical-analysis-on-email-classification-using-the-enron-dataset-19054d558697?source=collection_archive---------4-----------------------#2018-11-11">https://towardsdatascience.com/empirical-analysis-on-email-classification-using-the-enron-dataset-19054d558697?source=collection_archive---------4-----------------------#2018-11-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/519e85d35a9f867350c7e228baa4d136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8eJhfKzFb_0yg61H4Bq5EA.jpeg"/></div></div></figure><p id="d208" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本文中，我们将首先评估 3 种机器学习算法的性能，即逻辑回归、支持向量分类和随机森林分类器。在后一部分，我们将比较机器学习算法的性能与使用递归神经网络模型(LSTM)获得的结果。我们将在整个评估过程中使用<a class="ae kw" href="http://www2.aueb.gr/users/ion/data/enron-spam/" rel="noopener ugc nofollow" target="_blank">安然数据集</a>。</p><p id="2aee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 1。关于数据集</strong></p><p id="42db" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">安然数据集包括主要由安然公司高级管理层发送的电子邮件。在这个实验中，我们使用了专门为垃圾邮件和火腿分类制作的数据集的处理版本。该数据集包含 30207 封电子邮件，其中 16545 封被标记为垃圾邮件，13662 封被标记为垃圾邮件。</p><p id="ed5a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2。</strong> <strong class="ka ir">方法论</strong></p><p id="bac2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2.1 数据预处理</strong></p><p id="42f3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2.1.1 机器学习</strong></p><p id="e134" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于机器学习算法，我使用了单词袋方法。这是一种从文本中提取特征以用于机器学习算法的方法[1]。在这种情况下，我们首先对电子邮件进行标记，并为所有电子邮件中的所有单词创建一个字典。然后，我们从其中选取最常见的 3000 个单词作为我们的最终词典。然后我们遍历每封邮件，记录邮件中每个令牌在字典中对应的令牌的出现频率。这样，所有电子邮件的最大序列长度变为 3000。</p><p id="4f2c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2.1.2 递归神经网络</strong></p><p id="867e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">RNN 模型使用了一种略有不同的单词包方法来预处理电子邮件。我们首先对电子邮件进行标记，并制作所有电子邮件中所有单词的字典，就像我们对机器学习模型所做的那样。那么所有的单词将从 1 开始被索引。然后，电子邮件集合将被迭代，对于电子邮件中的每个单词，将参考字典将该单词映射到相应的索引。这种方法确保了电子邮件中单词的顺序不变。但是我们现在有一个问题，每个电子邮件有不同的序列长度。为了减轻这一点，我们将得到最长的电子邮件的长度，并将所有电子邮件填充到该长度。填充将通过在每个序列的开头添加“0”来完成，以便它不会影响训练过程。在这个场景中，所有电子邮件的序列长度是 3425。</p><p id="b0fb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2.2 机器学习算法</strong></p><p id="91af" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">安然数据集将用于训练机器学习模型，以将电子邮件分类为垃圾邮件或 ham。所有利用的机器学习算法都取自 scikit learn 库。所有算法的参数调整都是手动完成的。这是因为数据集的大小是 30207，运行网格搜索太耗费时间了</p><p id="5cbc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2.2.1 拆分数据</strong></p><p id="d428" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于所有算法，数据被分成三组作为训练集、验证集和测试集。验证集用于调整参数。在参数调整过程之后，使用测试集来获得最终结果。训练集由 80%的数据组成，20%用于验证集。然后这 20%被进一步平均分配，50%用于验证，另外 50%用于测试。</p><p id="20d6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2.2.2 支持向量分类</strong></p><p id="3508" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">支持向量分类算法通过创建一个特征空间来模拟这种情况，这是一个有限维的向量空间，其中的每一维都代表特定对象的一个“特征”[2]。</p><p id="da28" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，在没有任何优化的情况下进行观察。以下是初步结果:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi kx"><img src="../Images/181b4adba0f29266225663da17813d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r64S8ynGVyMEwQvTbdorvw.jpeg"/></div></div></figure><p id="d79a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在使用线性核获得初始结果之后，调整 C 参数以获得最终结果。下面显示的是收到的验证和测试集的最终结果。这些是观察期间获得的最高验证分数。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lc"><img src="../Images/c921a53998e97831fdf2f487ce4fa295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*50yBssX7-KTBl52Im3PBFg.jpeg"/></div></div></figure><p id="6591" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2.2.3 随机森林分类器</strong></p><p id="1b6d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">随机森林分类器从随机选择的训练集子集创建一组决策树。然后它聚集来自不同决策树的投票来决定测试对象的最终类[3]。</p><p id="799b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最初，观测是在没有任何优化的情况下进行的，就像 SVC 算法一样。以下是初步结果:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ld"><img src="../Images/068591c427b35da8584fe8a8123713c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2BwfL-YZO7XtlPA7HDxJ8A.jpeg"/></div></div></figure><p id="bf84" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过改变可能对最终结果产生最大影响的不同参数来改进初始结果。探索的参数是:n_estimators、max_features 和 Max_depth。下面显示的是最终结果:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi le"><img src="../Images/d90ba60dbbfbad1c847ad2e3232542ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jC0f_Cquu3GUVHDUSgcnlw.jpeg"/></div></div></figure><p id="8745" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2.2.4 逻辑回归</strong></p><p id="8fb2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">逻辑回归用于描述数据，并解释一个因变量与一个或多个名义变量、序数变量、区间变量或比率水平自变量之间的关系[4]。</p><p id="0976" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该算法使用默认设置运行，因为没有要优化的参数。下面显示的是测试集获得的结果。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lf"><img src="../Images/c81ed63552fc49c2128209c56da5cce8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FSzJMYBjQewB9OurIfuN8g.jpeg"/></div></div></figure><p id="d5a1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2.2.5 总结</strong></p><p id="747a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在比较这些车型时，我们将主要关注 f1 的得分。根据观察，尽管对 svc 算法进行了优化，但仍无法实现显著的改进。但是随机森林分类器在调整参数值后显示出相当显著的改进。尽管这两种算法都给出了很好的结果，但是逻辑回归算法的最终结果是最高的。如您所见，逻辑回归算法和 svc 算法的最终结果相差不远，但我们必须让逻辑回归算法胜出，因为这是我们能够获得的最高结果。</p><p id="c4dc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将进一步探讨这个主题，看看我们是否可以通过使用不同的方法来改善最终结果。即递归神经网络。我们将用于这一观察的递归神经网络的类型是 LSTM 模型。</p><p id="111e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2.3 LSTM 车型</strong></p><p id="ff89" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">LSTM 代表长期短期记忆网络，这是一种特殊的 RNN，能够学习长期依赖关系。这些是专门设计来避免大多数模型中存在的长期依赖问题的。LSTMs 长时间记忆信息，有助于建立长期依赖性模型[5]。</p><p id="0eb4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">采用张量流框架建立 LSTM 模型。LSTM 模型由几层组成。<strong class="ka ir"> </strong>当将数据输入模型时，我们将首先通过嵌入层传递数据。嵌入层的功能是将代表字典中单词的每个索引表示为一个范围内指定大小的随机数值向量。这种方法有助于对单词之间的关系进行建模。那么嵌入层的输出将通过 LSTM 层发送。我们使用 Tensorflow 提供的“基本 LSTM 池”来完成这项任务。LSTM 层将在其输出中添加一个漏失，以随机抑制神经网络中的一些神经元，从而降低过拟合的可能性[6]。然后，隐藏图层将消耗 LSTM 图层的输出，并生成最终输出。</p><p id="6a01" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当计算成本函数时，logit(没有通过激活函数发送的最终结果)将被传递给它。使用的成本函数是“softmax 交叉熵与 logits”。然后，优化函数将用于降低成本。所利用的优化功能是 Adam 优化器。</p><p id="9f5f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过激活函数传递获得的 logit 值进行预测。为此使用了 Sigmoid 函数，因为相关的任务是二元分类。</p><p id="7409" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2.3.1 拆分数据</strong></p><p id="c41a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">数据被分成 3 组，作为之前在机器学习方法中提到的训练、验证和测试集。在获得初始结果后，验证集用于调整超参数以提高最终得分。然后使用测试集进行最终评估。训练、验证和测试数据的百分比与用于机器学习方法的百分比相同。</p><p id="916b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2.3.2 结果</p><p id="c03c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面显示了使用验证数据集获得的初步结果:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lg"><img src="../Images/1d2cca54e49ccef435f6dff071648316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0l9gRWd6Xp1UpcC9LK7-2g.jpeg"/></div></div></figure><p id="0e64" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以下结果显示了调整超参数后的最终输出</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lh"><img src="../Images/3681d0bc439e0b5a39e57a366f673b66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OjFIkDD6tjCHw399POUkvw.jpeg"/></div></div></figure><p id="ecd3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在使用上述模型获得令人满意的结果之后。通过叠加更多的隐藏层来进行观察，以查看结果是否可以进一步改进。</p><p id="8ec5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面显示的是使用两个隐藏层得到的结果。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi li"><img src="../Images/890755b159b343e6633c2df8b09513a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ohZ4eHFeOXRGj3AlrfWnwg.jpeg"/></div></div></figure><p id="76d0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2.3.3 总结</strong></p><p id="e261" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了评估该模型，我们将主要考虑 f1 分数，正如我们对机器学习算法所做的那样。考虑到优化参数后获得的结果，我们可以看到分数略有提高。</p><p id="6e3b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当试图通过添加更多的隐藏层来进一步改善结果时，正如你所看到的，它给出了一个低得多的输出。其原因是，即使在理论上，神经网络应该对堆叠层给出更好的分数，在这种情况下，我们缺乏数据。我们拥有的数据量不足以训练多个隐藏层。</p><p id="4852" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 3。谁做得更好？</strong></p><p id="9e82" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如上所述，逻辑回归算法给出了比其他两种机器学习算法更好的结果。所以现在我们来了个问题，考虑到逻辑回归算法和 LSTM 模型，谁做得更好。下图是逻辑回归算法和 LSTM 模型的最终结果对比。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lj"><img src="../Images/f7511396659c3a4133b5b560591b0cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9IhwkM2kqO_g91b7DU0EOw.jpeg"/></div></div></figure><p id="4815" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">查看表格，我们可以清楚地看到，LSTM 的<strong class="ka ir"/><strong class="ka ir"/>f1 得分优于逻辑回归算法。基本上所有的分数 LSTM 模型都优于逻辑回归算法。这让 LSTM 模式轻而易举地胜出。</p><p id="689d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 4。结论</strong></p><p id="8d6a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">考虑到所获得的结果，LSTM 模型优于所有其他考虑的机器学习算法。其原因是 LSTMs 模拟长期依赖性的能力。安然是一个文本数据集，因此，能够记住整个电子邮件中单词之间的依赖关系增加了更好地猜测它是垃圾邮件还是火腿电子邮件的机会。由于 LSTM 设计擅长这项任务，它能够给出比其他机器学习算法更好的结果。</p><p id="62df" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">总之，我们可以说，LSTM 真正能够捕捉数据中的长期依赖性，并利用其在架构中的存储方面胜过其他机器学习算法。</p><p id="7de7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 5。参考文献</strong></p><p id="aec9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[1]<a class="ae kw" href="https://medium.com/greyatom/an-introduction-to-bag-of-words-in-nlp-ac967d43b428" rel="noopener">https://medium . com/grey atom/an-introduction-to-bag-of-words-in-NLP-AC 967d 43 b 428</a></p><p id="4cca" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[2]https://mindmajix.com/support-vector-machine-algorithm<a class="ae kw" href="https://mindmajix.com/support-vector-machine-algorithm" rel="noopener ugc nofollow" target="_blank"/></p><p id="5341" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[3]<a class="ae kw" href="https://medium.com/machine-learning-101/chapter-5-random-forest-classifier-56dc7425c3e1" rel="noopener">https://medium . com/machine-learning-101/chapter-5-random-forest-classifier-56dc 7425 c3e 1</a></p><p id="dc87" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[4]<a class="ae kw" href="http://www.statisticssolutions.com/what-is-logistic-regression/" rel="noopener ugc nofollow" target="_blank">http://www . statistics solutions . com/what-is-logistic-regression/</a></p><p id="f048" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><p id="841b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[6]<a class="ae kw" href="https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning-And-why-is-it-claimed-to-be-an-effective-trick-to-improve-your-network" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/How-the-dropout-method-work-in-deep-learning-And-why-it-be-a-effective-trick-to-improve-your-network</a></p><p id="b2b1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">注</strong>:本次评估使用的完整代码可以通过<a class="ae kw" href="https://github.com/suleka96/Email-Classification" rel="noopener ugc nofollow" target="_blank">这个链接</a>找到。请随意摆弄代码，亲自尝试一下。编码快乐！</p></div></div>    
</body>
</html>