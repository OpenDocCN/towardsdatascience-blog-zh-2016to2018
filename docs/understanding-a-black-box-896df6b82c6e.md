# 理解黑盒

> 原文：<https://towardsdatascience.com/understanding-a-black-box-896df6b82c6e?source=collection_archive---------7----------------------->

![](img/2fb478e60ec603fd613fa7dab3a774bc.png)

模型可解释性方法的概述…以及为什么它很重要。

在我们深入研究一些流行且相当强大的方法来打开黑盒机器学习模型(如深度学习模型)之前，让我们首先明确一下为什么它如此重要。

你看，有许多领域将受益于可理解的模型，如自动驾驶汽车，或广告定位，甚至有更多的领域需要这种可解释性，如信誉分配，银行，医疗保健，人力资源。能够审计这些关键领域的模型是非常重要的。

理解一个模型的最重要的特征让我们深入了解它的内部工作，并给出改进它的性能和消除偏差的方向。

除此之外，有时它有助于调试模型(经常发生)。然而，与预测一起提供解释的最重要的原因是，可解释的 ML 模型对于获得最终用户的信任是必要的(以医疗应用为例)。

我希望现在你也相信可理解的机器学习是非常重要的，所以让我们深入到具体的例子中来解决这个问题。

# 简单方法

人们能想到的最简单的方法是稍微改变输入数据，以观察底层黑盒是如何反应的。对于视觉数据，使用部分遮挡的图像是最简单的方法。对于文本——替换单词，对于数字/分类数据——改变变量。就这么简单！

这种方法的最大好处是——它是模型不可知的，您甚至可以检查其他人的模型，而无需直接访问它。

即使听起来很容易，好处也是巨大的。我多次使用这种方法来调试机器学习即服务解决方案和在我自己的机器上训练的神经网络，发现训练的模型选择不相关的特征来决定图像的类别，从而节省了工作时间。真正的 80/20 法则在起作用。

# GradCAM

**梯度加权类激活图**——一种更高级更专业的方法。这种方法的限制是您需要访问模型的内部，并且它应该可以处理图像。为了让您对该方法有一个简单的直觉，给定一个数据样本(图像)，它将输出图像区域的热图，其中神经网络具有最多和最大的激活，因此图像中的特征与该类的相关性最大。

从本质上讲，与之前的模型相比，您对该模型的重要特性有了更细粒度的理解。

[这里](http://gradcam.cloudcv.org/)是 GradCAM 可解释性方法的一个很好的演示。
要了解 GradCAM 如何工作，请查看[“Grad-CAM:通过基于梯度的定位从深度网络进行可视化解释”](https://arxiv.org/pdf/1610.02391.pdf)的论文。

# 石灰

也许你听说过这个。如果没有，首先看看这个简短的介绍。

2016 年发表《我为什么要相信你？:解释任何分类器的预测”的论文，该论文介绍了 **LIME —局部可解释的模型不可知解释**。让我们从名字中推导出它的功能！

**局部可解释性***——*你要知道机器学习模型的复杂度越高，模型的可解释性越差。也就是说，逻辑回归和决策树比随机森林和神经网络更容易解释。LIME 方法的假设是，像随机森林或神经网络这样的非线性复杂模型可以是线性的，并且在局部是简单的，即在整个决策边界的小块上。回想一下，我们说过简单的模型是可以解释的。

**模型无关** *—* 这部分比较容易。LIME 对被解释的模型没有任何假设。

关于 LIME 最好的事情是它也可以作为 PyPI 包使用。你已经准备好出发了！要了解更多信息，[这里是](https://github.com/marcotcr/lime)他们的 GitHub repo 基准和一些教程笔记本，[这里是](https://arxiv.org/pdf/1602.04938.pdf)他们论文的链接。[仅供参考，LIME 也被 Fast Forward Labs(现在是 Cloudera 的一部分)用于演示模型可解释性的重要性](https://blog.fastforwardlabs.com/2017/08/02/interpretability.html)。

# SHAP

**SHapley 附加解释** —一种理解黑盒模型的最新解决方案。在某种程度上，它非常类似于石灰。两者都是强大的统一解决方案，与模型无关，并且相对容易上手。

但是 SHAP 的特别之处在于它内部使用石灰。事实上，SHAP 背后有太多的可解释模型，它选择最适合手头问题的模型，用正确的工具给你所需的解释。

此外，如果我们分解这个解决方案的能力，我们实际上发现，SHAP 用 Shapley 值解释了任何机器学习模型的输出。这意味着 SHAP 为每个预测的每个特征分配一个值(即特征属性)；该值越高，该特征对特定预测的贡献就越大。这也意味着这些值的总和应该接近原始模型预测。

检查他们的 [GitHub repo](https://github.com/slundberg/shap) ，就像 LIME 一样，他们有一些教程，也可以通过 pip 安装 SHAP。此外，要了解更多细节，请查看他们的[压印线预打印纸](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf)。

# 最终注释

机器学习模型的可解释性和可解释性是人工智能社区的一个热门话题，具有巨大的影响。为了让人工智能产品和服务进入新的高度监管的市场，必须了解这些新算法是如何做出决策的。

此外，知道机器学习模型的原因在调试它，甚至改进它方面提供了突出的优势。

在设计深度学习/机器学习系统时，最好考虑到可解释性，这样总是很容易检查模型，并在关键情况下抑制其决策。

如果你已经做到了，谢谢你！我鼓励你在这个新领域进行自己的研究，并在评论区与我们分享你的发现。

另外，如果你喜欢这篇文章，别忘了鼓掌😏或者关注我，获取更多关于机器学习和深度学习的各种主题的文章。

P.S .抱歉，这次没有 Colab 笔记本，因为已经有很多关于这个主题的非常好的教程让你开始。