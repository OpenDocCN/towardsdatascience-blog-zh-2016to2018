<html>
<head>
<title>Sentiment analysis using RNNs(LSTM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用RNNs的情感分析(LSTM)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentiment-analysis-using-rnns-lstm-60871fa6aeba?source=collection_archive---------0-----------------------#2017-06-21">https://towardsdatascience.com/sentiment-analysis-using-rnns-lstm-60871fa6aeba?source=collection_archive---------0-----------------------#2017-06-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="d72b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里，我们使用评论的例子来预测情绪(尽管它可以更普遍地应用于其他领域，例如对推文、评论、客户反馈等的情绪分析)。这里的整体思想是，电影评论是由单词序列和单词顺序组成的，这些单词编码了许多对预测情感有用的信息。第一步是将单词映射到单词嵌入(参见帖子<a class="ae ko" href="https://medium.com/towards-data-science/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b" rel="noopener"> 1 </a>和<a class="ae ko" href="https://medium.com/towards-data-science/word2vec-skip-gram-model-part-2-implementation-in-tf-7efdf6f58a27" rel="noopener"> 2 </a>了解更多关于单词嵌入的上下文)。步骤2是RNN，它接收向量序列作为输入，并考虑向量的顺序来生成预测。</p><p id="a4fe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该网络的架构如下所示。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/bf5f78426cd78c8aaba6abb397acbacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*27JmK8VBdphpSCWNb4MhNA.png"/></div></figure><blockquote class="kx ky kz"><p id="ddd4" class="jq jr la js b jt ju jv jw jx jy jz ka lb kc kd ke lc kg kh ki ld kk kl km kn im bi translated">这里，我们将把单词传递给一个嵌入层。您实际上可以用word2vec训练一个嵌入，并在这里使用它。但是只要有一个嵌入层并让网络自己学习嵌入表就足够了。</p></blockquote><p id="4ba4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从嵌入层，新的表现将被传递到LSTM细胞。这将增加网络的循环连接，这样我们就可以在数据中包含单词序列的信息。最后，LSTM细胞将进入一个sigmoid输出层。我们使用sigmoid是因为我们试图预测这篇文章是否有积极或消极的情绪。输出层将只是一个具有s形激活函数的单一单元。</p><p id="8ada" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">除了最后一个，我们不关心sigmoid输出，其他的可以忽略。我们将根据上一步的输出和训练标签来计算成本。</p><h1 id="25c0" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">架构摘要:</h1><p id="ebef" class="pw-post-body-paragraph jq jr it js b jt mc jv jw jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn im bi translated">将固定长度的评论编码为整数，然后转换为嵌入向量，以循环方式传递给LSTM层，并挑选最后的预测作为输出情感。</p><h1 id="23c2" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">陷阱:</h1><p id="8ae5" class="pw-post-body-paragraph jq jr it js b jt mc jv jw jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn im bi translated">在我的实验中，我无法解释的一件事是，当我将单词编码为整数时，如果我随机为单词分配唯一的整数，我得到的最佳准确度是50–55%(基本上，该模型并不比随机猜测好多少)。但是，如果对单词进行编码，使得最高频率的单词得到最低的数字，那么在3-5个时期内，模型精度为80%。我的猜测是，这对于训练嵌入层是必要的，但是在任何地方都找不到原因的解释。</p><h1 id="3815" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">代码:</h1><p id="8a55" class="pw-post-body-paragraph jq jr it js b jt mc jv jw jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn im bi translated"><a class="ae ko" href="https://github.com/mchablani/deep-learning/blob/master/sentiment-rnn/Sentiment_RNN.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/mchablani/deep-learning/blob/master/情操-rnn/情操_RNN.ipynb </a></p><h2 id="d16f" class="mh lf it bd lg mi mj dn lk mk ml dp lo kb mm mn ls kf mo mp lw kj mq mr ma ms bi translated">数据预处理:</h2><p id="53d5" class="pw-post-body-paragraph jq jr it js b jt mc jv jw jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn im bi translated">将评论中的所有单词用整数编码。现在每个评论都是一个有序的整数数组。让每个评论固定大小(比如200)，这样较短的评论前面会填充0，而较长的评论会被截断到200。因为我们用0填充，所以单词到int的映射从1开始。标签被编码为“1”和“0”，分别代表“正”和“负”。</p><h2 id="5f41" class="mh lf it bd lg mi mj dn lk mk ml dp lo kb mm mn ls kf mo mp lw kj mq mr ma ms bi translated">构建图表</h2><pre class="kq kr ks kt gt mt mu mv mw aw mx bi"><span id="af21" class="mh lf it mu b gy my mz l na nb">lstm_size = 256<br/>lstm_layers = 2<br/>batch_size = 500<br/>learning_rate = 0.001<br/>embed_size = 300</span><span id="3292" class="mh lf it mu b gy nc mz l na nb">n_words = len(vocab_to_int) + 1 # Add 1 for 0 added to vocab</span><span id="9423" class="mh lf it mu b gy nc mz l na nb"># Create the graph object<br/>tf.reset_default_graph()<br/>with tf.name_scope('inputs'):<br/>    inputs_ = tf.placeholder(tf.int32, [None, None], name="inputs")<br/>    labels_ = tf.placeholder(tf.int32, [None, None], name="labels")<br/>    keep_prob = tf.placeholder(tf.float32, name="keep_prob")</span><span id="eec3" class="mh lf it mu b gy nc mz l na nb"># Sizeof embedding vectors (number of units in the embedding layer)<br/>with tf.name_scope("Embeddings"):<br/>    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))<br/>    embed = tf.nn.embedding_lookup(embedding, inputs_)</span><span id="8e83" class="mh lf it mu b gy nc mz l na nb">def lstm_cell():<br/>    # Your basic LSTM cell<br/>    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size, reuse=tf.get_variable_scope().reuse)<br/>    # Add dropout to the cell<br/>    return tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)</span><span id="a9de" class="mh lf it mu b gy nc mz l na nb">with tf.name_scope("RNN_layers"):<br/>    # Stack up multiple LSTM layers, for deep learning<br/>    cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(lstm_layers)])<br/>    <br/>    # Getting an initial state of all zeros<br/>    initial_state = cell.zero_state(batch_size, tf.float32)</span><span id="e89e" class="mh lf it mu b gy nc mz l na nb">with tf.name_scope("RNN_forward"):<br/>    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)</span><span id="7f45" class="mh lf it mu b gy nc mz l na nb">with tf.name_scope('predictions'):<br/>    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)<br/>    tf.summary.histogram('predictions', predictions)<br/>with tf.name_scope('cost'):<br/>    cost = tf.losses.mean_squared_error(labels_, predictions)<br/>    tf.summary.scalar('cost', cost)</span><span id="9381" class="mh lf it mu b gy nc mz l na nb">with tf.name_scope('train'):<br/>    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</span></pre><h2 id="fcd3" class="mh lf it bd lg mi mj dn lk mk ml dp lo kb mm mn ls kf mo mp lw kj mq mr ma ms bi translated">配料和培训</h2><pre class="kq kr ks kt gt mt mu mv mw aw mx bi"><span id="8218" class="mh lf it mu b gy my mz l na nb">def get_batches(x, y, batch_size=100):<br/>    <br/>    n_batches = len(x)//batch_size<br/>    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]<br/>    for ii in range(0, len(x), batch_size):<br/>        yield x[ii:ii+batch_size], y[ii:ii+batch_size]</span><span id="bb77" class="mh lf it mu b gy nc mz l na nb">epochs = 10</span><span id="071e" class="mh lf it mu b gy nc mz l na nb"># with graph.as_default():<br/>saver = tf.train.Saver()</span><span id="332a" class="mh lf it mu b gy nc mz l na nb">with tf.Session() as sess:<br/>    sess.run(tf.global_variables_initializer())<br/>    train_writer = tf.summary.FileWriter('./logs/tb/train', sess.graph)<br/>    test_writer = tf.summary.FileWriter('./logs/tb/test', sess.graph)<br/>    iteration = 1<br/>    for e in range(epochs):<br/>        state = sess.run(initial_state)<br/>        <br/>        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):<br/>            feed = {inputs_: x,<br/>                    labels_: y[:, None],<br/>                    keep_prob: 0.5,<br/>                    initial_state: state}<br/>            summary, loss, state, _ = sess.run([merged, cost, final_state, optimizer], feed_dict=feed)<br/>#             loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)</span><span id="9ec6" class="mh lf it mu b gy nc mz l na nb">train_writer.add_summary(summary, iteration)<br/>        <br/>            if iteration%5==0:<br/>                print("Epoch: {}/{}".format(e, epochs),<br/>                      "Iteration: {}".format(iteration),<br/>                      "Train loss: {:.3f}".format(loss))</span><span id="9798" class="mh lf it mu b gy nc mz l na nb">if iteration%25==0:<br/>                val_acc = []<br/>                val_state = sess.run(cell.zero_state(batch_size, tf.float32))<br/>                for x, y in get_batches(val_x, val_y, batch_size):<br/>                    feed = {inputs_: x,<br/>                            labels_: y[:, None],<br/>                            keep_prob: 1,<br/>                            initial_state: val_state}<br/>#                     batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)<br/>                    summary, batch_acc, val_state = sess.run([merged, accuracy, final_state], feed_dict=feed)<br/>                    val_acc.append(batch_acc)<br/>                print("Val acc: {:.3f}".format(np.mean(val_acc)))<br/>            iteration +=1<br/>            test_writer.add_summary(summary, iteration)<br/>            saver.save(sess, "checkpoints/sentiment_manish.ckpt")<br/>    saver.save(sess, "checkpoints/sentiment_manish.ckpt")</span></pre><p id="fb6d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">学分:来自课堂讲稿:<a class="ae ko" href="https://classroom.udacity.com/nanodegrees/nd101/syllabus" rel="noopener ugc nofollow" target="_blank">https://classroom.udacity.com/nanodegrees/nd101/syllabus</a></p></div></div>    
</body>
</html>