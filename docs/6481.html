<html>
<head>
<title>Text Generation Using Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用递归神经网络的文本生成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-generation-using-rnns-fdb03a010b9f?source=collection_archive---------12-----------------------#2018-12-15">https://towardsdatascience.com/text-generation-using-rnns-fdb03a010b9f?source=collection_archive---------12-----------------------#2018-12-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1b19" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从《爱丽丝梦游仙境》生成文本</h2></div><h1 id="be95" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">介绍</h1><p id="c56d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">文本生成是数据科学和机器学习中的一个热门问题，它适合于递归神经网络。该报告使用 TensorFlow 构建了一个 RNN 文本生成器，并在 Python3 中构建了一个高级 API。该报告的灵感来自于@karpathy ( <a class="ae lt" href="https://gist.github.com/karpathy/d4dee566867f8291f086" rel="noopener ugc nofollow" target="_blank"> min-char-rnn </a>)和 Aurélien Géron ( <a class="ae lt" href="https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291" rel="noopener ugc nofollow" target="_blank">用 Scikit-Learn 和 TensorFlow </a>进行动手机器学习)。这是 CST463 中的一个课堂项目——加州州立大学蒙特雷湾分校的高级机器学习，由 Glenn Bruns 博士指导。</p><h1 id="e613" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">模块</h1><p id="bb4e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><code class="fe lu lv lw lx b">Dataset</code>、<code class="fe lu lv lw lx b">RNNTextGenerator</code>和<code class="fe lu lv lw lx b">ModelSelector</code>是三个主要模块。有关文档，请在<a class="ae lt" href="https://github.com/donaldong/rnn-text-gen" rel="noopener ugc nofollow" target="_blank"> Github </a>上查看该项目。</p><h1 id="b634" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">资料组</h1><p id="af4e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在<a class="ae lt" href="https://github.com/donaldong/rnn-text-gen/blob/master/src/dataset.py" rel="noopener ugc nofollow" target="_blank"> src/dataset.py </a>中定义</p><p id="3113" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">创建包含独热编码文本数据的文本数据集。它批量生产编码标签序列。我们将文本数据分成批次用于训练 RNN，并且我们随机抽取文本样本(具有给定长度)来评估我们的模型的性能。</p><h1 id="bb74" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">RNNTextGenerator</h1><p id="e97f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在<a class="ae lt" href="https://github.com/donaldong/rnn-text-gen/blob/master/src/text_generator.py" rel="noopener ugc nofollow" target="_blank"> src/text_generator.py </a>中定义</p><p id="3fcf" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">使用张量流 RNN 单元创建递归神经网络(执行<code class="fe lu lv lw lx b">inputs</code>的动态展开)。它有一个输出投影图层，可生成每个字符类的最终概率。它通过基于当前序列的最后一个字符的概率分布对下一个字符进行采样来生成文本。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/7e9455aeb710713ec9f101072cc99701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fvdmox8-qMkwwAbEOEHahA.png"/></div></div></figure><p id="02e3" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">我们使用<code class="fe lu lv lw lx b">Alice in Wonderland</code>作为文本。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="mp mq l"/></div></figure><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mr"><img src="../Images/c5f72d7f6d4b8ce983f2bd45efbb1330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f_TZj5O4r0UnSOm1gqTXIw.png"/></div></div></figure><p id="d1ec" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">我们可以看到精度和损耗并没有收敛。每个<code class="fe lu lv lw lx b">fit</code>调用将运行一个时期(我们将<code class="fe lu lv lw lx b">epoch=1</code>传递给了构造函数)。</p><p id="de42" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">我们希望在 30 秒内继续用更多的纪元来拟合模型，并且我们对模型如何随着时间的推移而改进感兴趣。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="mp mq l"/></div></figure><pre class="me mf mg mh gt ms lx mt mu aw mv bi"><span id="53e7" class="mw kg iq lx b gy mx my l mz na">test acc: 0.535040020942688, test loss: 1.592130422592163<br/>Yes, but much towe this NOT, aft open') said 'Letger comimpie hone,<br/>'U VERY up in aborious, it with the adge,<br/>-----------------------<br/>test acc: 0.5360000133514404, test loss: 1.5756181478500366<br/>Yes, but the Habembood cacs must, was the lang donant as perpen my his huril about harriered me shreep), whre<br/>-----------------------<br/>test acc: 0.5430399775505066, test loss: 1.5500394105911255<br/>Yes, but to Alice.<br/><br/>So then<br/>hep datee went orling,' said Alice,' said had geined,' she you not!' said the<br/>nil<br/>-----------------------<br/>test acc: 0.5302400588989258, test loss: 1.6037070751190186<br/>Yes, but the could mould cobl over howner? Now that oplo-thing to her orverewn.'<br/><br/>'Why, to<br/>herself!<br/><br/>The Qu-e<br/>-----------------------<br/>test acc: 0.539199948310852, test loss: 1.6024020910263062<br/>Yes, but the Queen home way the at the placsarnd of the sat a it,' the QUED INK Affece herge how eacushing ov<br/>-----------------------<br/>test acc: 0.5388799905776978, test loss: 1.538255214691162<br/>Yes, but did they rossan that may the sing,' sayeing, round queer thatled thing's she went on, I'm she can wa<br/>-----------------------<br/>test acc: 0.5424000024795532, test loss: 1.5805484056472778<br/>Yes, but first's word.' She was byself, if our it was en!' seemes you se<br/>leasions of the doous rige out oldes<br/>-----------------------<br/>test acc: 0.5455999970436096, test loss: 1.5576822757720947<br/>Yes, but the other--<br/> Whereet heam who her not<br/>one and it didn't knowling e s<br/>-----------------------</span></pre><p id="6e6e" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">我们可以看到，随着时间的推移，生成的文本确实变得更好了。句子中有更多的实词。测试精度和损耗仍在提高。</p><p id="2edd" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">现在，我们将尝试不同的超参数。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="7169" class="kf kg iq bd kh ki ni kk kl km nj ko kp jw nk jx kr jz nl ka kt kc nm kd kv kw bi translated">模型选择器</h1><p id="4d18" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在<a class="ae lt" href="https://github.com/donaldong/rnn-text-gen/blob/master/src/model_selector.py" rel="noopener ugc nofollow" target="_blank"> src/model_selector.py </a>中定义</p><p id="7212" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">执行随机搜索并按准确度对模型进行排序。它选择最好的排名模型，并允许长时间的搜索(几个小时/几天)。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="8bbb" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">我们为模型选择器定义了一个大的搜索空间，并给它 24 小时的搜索时间。我们希望它能为<code class="fe lu lv lw lx b">Alice in Wonderland</code>文本发现一个优秀的模式。</p><p id="ebbe" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">选择器看到的最好的模型使用带有<code class="fe lu lv lw lx b">320</code>神经元的<code class="fe lu lv lw lx b">GRU Cell</code>和<code class="fe lu lv lw lx b">leaky_relu</code>作为激活函数。它使用一个学习速率为<code class="fe lu lv lw lx b">0.006</code>的<code class="fe lu lv lw lx b">AdamOptimizer</code>。它在<code class="fe lu lv lw lx b">45</code>个时期中被训练，每批有<code class="fe lu lv lw lx b">91</code>个序列(batch_size)。在 5 个随机采样的测试序列中，它的平均精度为<code class="fe lu lv lw lx b">0.6245</code>，损失<code class="fe lu lv lw lx b">1.25</code>。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="mp mq l"/></div></figure><pre class="me mf mg mh gt ms lx mt mu aw mv bi"><span id="856b" class="mw kg iq lx b gy mx my l mz na">Yes, but to talk about this of ketched him in the puppy juging said to the tell messave off, things very unfusts, and that put on hesserriely I’ll make the pu</span></pre><p id="3311" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">我们认为<code class="fe lu lv lw lx b">epoch</code>是模型的超参数，因为大量的历元会使神经网络过度拟合，在测试序列上表现不佳。因此，选择器中的最佳模型是我们将使用的最终模型。</p><p id="bb8c" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">最好的文本生成器部署为<a class="ae lt" href="https://www.ddong.me/alice-text-gen" rel="noopener ugc nofollow" target="_blank">一个简单的 web 应用程序</a> ( <code class="fe lu lv lw lx b">https://www.ddong.me/alice-text-gen</code>)用于教育目的。</p></div></div>    
</body>
</html>