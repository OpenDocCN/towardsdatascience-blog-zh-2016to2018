<html>
<head>
<title>NLP — Building a Question Answering model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP —构建问题回答模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-building-a-question-answering-model-ed0529a68c54?source=collection_archive---------0-----------------------#2018-03-29">https://towardsdatascience.com/nlp-building-a-question-answering-model-ed0529a68c54?source=collection_archive---------0-----------------------#2018-03-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="83a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">用数据做酷事！</em></p><p id="fd2d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我最近在斯坦福大学通过深度学习(CS224N)完成了一门关于NLP的课程，我很喜欢这种体验。学到了一大堆新东西。在我的最后一个项目中，我设计了一个基于<a class="ae km" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank">斯坦福问答数据集(SQuAD) </a>的问答模型。在这篇博客中，我想介绍问答模型的主要组成部分。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi kn"><img src="../Images/83ce1ddde6cd5a312374b5ed991f4836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/1*XFKZ_MA_EL54Cw5SZBtZDg.gif"/></div></figure><p id="4d8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以在我的<a class="ae km" href="https://github.com/priya-dwivedi/cs224n-Squad-Project" rel="noopener ugc nofollow" target="_blank"> Github </a> repo上找到完整的代码。</p><p id="c2fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我最近还为这个模型添加了一个<a class="ae km" href="http://deeplearninganalytics.org/demos" rel="noopener ugc nofollow" target="_blank">网络演示</a>，你可以放入任何段落并提出相关问题。查看<a class="ae km" href="http://deeplearninganalytics.org/demos" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="9e02" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">小队数据集</strong></p><p id="6e06" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">S<strong class="jp ir">S</strong>tanford<strong class="jp ir">Qu</strong>estion<strong class="jp ir">A</strong>nswering<strong class="jp ir">D</strong>ataset(<a class="ae km" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank">SQuAD</a>)是一个新的阅读理解数据集，由一组维基百科文章上的众包工作者提出的问题组成，其中每个问题的答案都是相应阅读文章的一段文字，或<em class="kl"> span </em>。SQuAD拥有500+篇文章上的100，000+问答对，比以前的阅读理解数据集大得多。</p><p id="4602" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在SQuAD数据集上有了快速的进步，一些最新的模型在回答问题的任务中达到了人类水平的准确性！</p><p id="0856" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上下文、问题和答案的示例</p><p id="2e4e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">背景——阿波罗计划从1961年运行到1972年，并得到了从1962年到1966年与它同时运行的双人双子座计划的支持。双子座任务开发了一些太空旅行技术，这些技术对于阿波罗任务的成功是必要的。阿波罗使用土星家族火箭作为发射工具。阿波罗/土星飞行器也被用于阿波罗应用计划，该计划包括天空实验室，这是一个在1973-74年支持三次载人任务的空间站，以及阿波罗-联盟测试项目，这是1975年与苏联的联合地球轨道任务。</p><p id="397d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">问:在1973-1974年间，哪个空间站支持了三次载人任务？</p><p id="af8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">答案——天空实验室</p><p id="6796" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">小队的主要特征:</p><p id="8516" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">I)它是一个封闭的数据集，意味着问题的答案总是上下文的一部分，也是上下文的连续跨度</p><p id="fdca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ii)因此找到答案的问题可以简化为找到对应于答案的上下文的开始索引和结束索引</p><p id="5837" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">iii) 75%的答案长度小于等于4个单词</p><h1 id="f805" class="kv kw iq bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated"><strong class="ak">机器理解模型—关键部件</strong></h1><p id="2215" class="pw-post-body-paragraph jn jo iq jp b jq lt js jt ju lu jw jx jy lv ka kb kc lw ke kf kg lx ki kj kk ij bi translated"><strong class="jp ir"> i)嵌入层</strong></p><p id="0d01" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型的训练数据集由上下文和相应的问题组成。这两者都可以被分解成单独的单词，然后使用像<a class="ae km" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a>向量这样的预训练向量将这些单词转换成单词嵌入。要了解更多关于单词嵌入的知识，请查看我的文章。单词嵌入在捕捉单词周围的上下文方面比对每个单词使用一个热点向量要好得多。对于这个问题，我使用了100维手套词嵌入，并且在训练过程中没有调整它们，因为我们没有足够的数据。</p><p id="432a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> ii)编码器层</strong></p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/6cdfaa16b9e6e8e2f0d49fb186547844.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*p3pQ3OJOEEBU-e8FEYVcnw.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">RNN Encoder</figcaption></figure><p id="2dd5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们在模型中添加的下一层是基于RNN的编码器层。我们希望上下文中的每个单词都知道它前面和后面的单词。双向的GRU/LSTM可以帮助做到这一点。RNN的输出是向前和向后方向的一系列隐藏向量，我们将它们连接起来。类似地，我们可以使用相同的RNN编码器来创建问题隐藏向量。</p><p id="9d21" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> iii)关注层</strong></p><p id="ffe7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">到目前为止，我们有一个上下文的隐藏向量和一个问题的隐藏向量。要找到答案，我们需要将二者放在一起看。这就是注意力的来源。它是问题回答系统中的关键部分，因为它帮助我们决定，给定问题，我应该“注意”上下文中的哪些单词。让我们从最简单的注意力模型开始:</p><p id="aeef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">点产品注意事项</em></p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi md"><img src="../Images/bffd85836581d1f0d8d286858b7c3b61.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*oQ7V3p8Q5cXEmeOH6H8PSg.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">Basic Attention Visualisation from CS224N</figcaption></figure><p id="5fcb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">点积注意力将是，对于每个上下文向量c i，我们乘以每个问题向量q j以获得向量e i(上图中的注意力分数)。然后我们对e i取一个softmax，得到α i(上图注意力分布)。Softmax保证所有e i之和为1。最后我们计算a i，作为注意力分布α i和相应问题向量(上图中注意力输出)的乘积。点积注意力也在下面的等式中描述</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi me"><img src="../Images/e8c9bfeab022861619435f686c8dcd9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*OWYx924xnJQ8bhQKEQBaJA.png"/></div></figure><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/412770a9f64bc98a6d93eca9f3311088.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*IYf7LThw32TizyrjaNuTnA.png"/></div></figure><p id="10ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上述注意已经在Github代码中实现为基线注意。</p><p id="e6de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">更复杂的注意——BiDAF注意</em></p><p id="54e6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以用上面描述的基本注意力层运行小队模型，但是性能不会很好。更复杂的注意力导致更好的表现。</p><p id="73e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们描述一下<a class="ae km" href="https://arxiv.org/abs/1611.01603" rel="noopener ugc nofollow" target="_blank"> BiDAF论文中的关注点。</a>主要思想是注意力应该双向流动——从上下文到问题，从问题到上下文。</p><p id="4da8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们首先计算相似性矩阵S ∈ R N×M，它包含每对(ci，qj)上下文和问题隐藏状态的相似性得分Sij。sij = wT sim[ci；QJ；ci♀QJ]∈R这里，ci♀QJ是元素式乘积，wsim ∈ R 6h是权重向量。在下面的等式中描述:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/2b8ed01ddb9a0ad64fc93101ee379c55.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*Nmi5ZM6IP6Yk_EGtqY1CDw.png"/></div></figure><p id="79d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们执行上下文到问题(C2Q)注意。(这个和上面说的点积注意差不多)。我们取S的行方式softmax来获得注意力分布α i，我们用它来取问题隐藏状态q j的加权和，产生C2Q注意力输出αI</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/118588f971152274d87cfeede2ffe581.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*kcckGB2g7rHT_lbKV85wUg.png"/></div></figure><p id="b08f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们执行问题到上下文(Q2C)注意。对于每个上下文位置i ∈ {1，.。。，N}，我们取相似性矩阵的对应行的最大值，m i = max j Sij ∈ R。然后，我们取结果向量m ∈ R N上的softmax这给出了上下文位置上的注意力分布β ∈ R N。然后我们用β取上下文隐藏状态c i的加权和——这就是Q2C注意力输出c素数。参见下面的等式</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/4edcc397e9f31e0d097c7012c97aaed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*VVOiMKLn1MLav3blYyPrYw.png"/></div></figure><p id="a8da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，对于每个上下文位置c i，我们组合来自C2Q注意力和Q2C注意力的输出，如下式所述</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/39c0a5482cff9d72988c83d80fd4bebb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*PXdIGEJrrn3VuTlQRodAaQ.png"/></div></figure><p id="5a01" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您觉得这一部分令人困惑，不要担心。注意力是一个复杂的话题。尝试用一杯茶阅读BiDAF文件:)</p><p id="0589" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">iv)输出层</p><p id="ee04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">快到了。模型的最后一层是softmax输出层，帮助我们决定答案范围的开始和结束索引。我们组合上下文隐藏状态和来自前一层的注意力向量来创建混合的重复。这些混合的重复成为全连接层的输入，该全连接层使用softmax来创建具有开始索引概率的p_start向量和具有结束索引概率的p_end向量。因为我们知道大多数答案的开始和结束索引最多相差15个单词，所以我们可以寻找使p_start*p_end最大化的开始和结束索引。</p><p id="15f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的损失函数是起点和终点的交叉熵损失之和。并且使用Adam优化器将其最小化。</p><p id="70b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我构建的最终模型比上面描述的要复杂一点，在测试集上F1得分为75。还不错！</p><h2 id="a3da" class="mk kw iq bd kx ml mm dn lb mn mo dp lf jy mp mq lj kc mr ms ln kg mt mu lr mv bi translated">后续步骤</h2><p id="eabc" class="pw-post-body-paragraph jn jo iq jp b jq lt js jt ju lu jw jx jy lv ka kb kc lw ke kf kg lx ki kj kk ij bi translated">关于未来探索的几个额外想法:</p><ul class=""><li id="6713" class="mw mx iq jp b jq jr ju jv jy my kc mz kg na kk nb nc nd ne bi translated">我一直在用一个基于CNN的编码器来代替RNN编码器，因为CNN比RNNs快得多，并且更容易在GPU上并行化</li><li id="c3df" class="mw mx iq jp b jq nf ju ng jy nh kc ni kg nj kk nb nc nd ne bi translated">其他注意机制，如在<a class="ae km" href="https://arxiv.org/abs/1611.01604" rel="noopener ugc nofollow" target="_blank">论文</a>中描述的动态共同注意</li></ul><p id="c7ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">给我一个❤️，如果你喜欢这个职位:)希望你拉代码，并尝试自己。</p><p id="5012" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">其他著述</strong>:【http://deeplearninganalytics.org/blog】T4</p><p id="4b32" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PS:我有自己的深度学习咨询公司，喜欢研究有趣的问题。我已经帮助几家初创公司部署了基于人工智能的创新解决方案。请到http://deeplearninganalytics.org/来看看我们吧。</p><p id="941d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你有一个我们可以合作的项目，请通过我的网站或priya.toronto3@gmail.com联系我</p><p id="1862" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">参考文献:</strong></p><ul class=""><li id="7e76" class="mw mx iq jp b jq jr ju jv jy my kc mz kg na kk nb nc nd ne bi translated"><a class="ae km" href="http://web.stanford.edu/class/cs224n/" rel="noopener ugc nofollow" target="_blank"> CS 224N </a></li><li id="e578" class="mw mx iq jp b jq nf ju ng jy nh kc ni kg nj kk nb nc nd ne bi translated"><a class="ae km" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank">小队数据集</a></li><li id="e01b" class="mw mx iq jp b jq nf ju ng jy nh kc ni kg nj kk nb nc nd ne bi translated"><a class="ae km" href="https://arxiv.org/abs/1611.01603" rel="noopener ugc nofollow" target="_blank"> BiDAF型号</a></li></ul></div></div>    
</body>
</html>