<html>
<head>
<title>Dimensionality Reduction For Dummies — Part 3: Connect The Dots</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">虚拟模型的降维第 3 部分:连接点</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dimensionality-reduction-for-dummies-part-3-f25729f74c0a?source=collection_archive---------4-----------------------#2018-11-01">https://towardsdatascience.com/dimensionality-reduction-for-dummies-part-3-f25729f74c0a?source=collection_archive---------4-----------------------#2018-11-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1f87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是一切都到位的地方。</p><p id="d704" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回到<a class="ae kl" rel="noopener" target="_blank" href="/https-medium-com-abdullatif-h-dimensionality-reduction-for-dummies-part-1-a8c9ec7b7e79">第 1 部分</a> &amp; <a class="ae kl" rel="noopener" target="_blank" href="/dimensionality-reduction-for-dummies-part-2-3b1e3490bdc9">第 2 部分</a>，我们看到我们的最终目标是<strong class="jp ir">找到这些定义最大方差方向的向量</strong>。鉴于我们目前所了解的情况，这个过程现在才有意义。</p><p id="c683" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我假设你知道线性变换，特征向量和特征值的基础知识，因为没有必要重复已经完全建立的东西。但是如果你需要一些洞察力，请看这个很棒的教程。</p><p id="3725" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们开始吧。</p><h1 id="1909" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">你想要什么?</h1><p id="79ed" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">你不能解决一个你没有明确定义的问题，这就是为什么我们应该考虑<strong class="jp ir">我们通过寻找最大方差的方向到底意味着什么。开始问自己什么是方差是很自然的。</strong></p><p id="b20e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们之前看到的，沿<em class="lp"> x </em>轴的方差是该轴表示的特征的方差，即数据集的<em class="lp"> x </em>坐标的方差。为了将这个概念推广到空间中的任何线，一个简单的观察是适当的:</p><blockquote class="lq"><p id="98af" class="lr ls iq bd lt lu lv lw lx ly lz kk dk translated">数据点的 x 坐标仅仅是它们在 x 轴上的投影。</p></blockquote><p id="97af" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">利用这一事实，我们可以通过<strong class="jp ir">将我们的点投影到代表直线的单位向量上，然后计算这些投影的方差，从而很容易地找到沿着任何直线的方差。</strong></p><p id="54b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果<strong class="jp ir"> <em class="lp"> A </em> </strong>是我们的(<em class="lp"> n </em> x <em class="lp"> d </em> ) <strong class="jp ir">均值居中的</strong>数据矩阵，其中<strong class="jp ir"> <em class="lp"> n </em> </strong>是实例(点数)<strong class="jp ir"> <em class="lp"> d </em> </strong>是维数(特征)，而<strong class="jp ir"> <em class="lp"> v </em> </strong>是我们线的单位向量，那么</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/547445dbf3c2aeaecbce0f323660b5e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*K30kK8xnfj6xIpfQoSaMWA.jpeg"/></div></figure><p id="09a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<em class="lp"> p </em>是一个(<em class="lp"> n </em> x 1)列向量，其中每个元素是每个点的投影长度。</p><p id="542f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们有了列向量中的长度，我们可以使用 Floor #5 来求它们的方差:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/5a31be2b4878e6a7bb64e08995a54f71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*R2WBVmIlvuGmhdL8DuA44g.jpeg"/></div></figure><p id="efe2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">突然，弹出了我们数据的协方差矩阵，<strong class="jp ir"> <em class="lp"> C </em> </strong>！这个非常奇怪的外观将为我们提供很大的帮助。给定我们对沿任意直线的方差的定义，我们可以通过说<strong class="jp ir">我们想要找到向量<em class="lp"> v </em>使得<em class="lp"> σ </em>是最大值来重新表述我们的问题。从上面的定义可以明显看出，这个向量仅仅依赖于<strong class="jp ir"> <em class="lp"> C </em> </strong>的值，这迫使我们仔细研究这个奇怪的矩阵。</strong></p><h1 id="9fb3" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">奇怪的协方差</h1><p id="90ec" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在上一部分中，我们了解到一个(<em class="lp"> n </em> x <em class="lp"> d </em>)数据矩阵有一个(<em class="lp"> d </em> x <em class="lp"> d </em>)协方差矩阵，即一个 2D 数据集有一个 2x2 协方差矩阵。你应该知道，<strong class="jp ir">一个矩阵可以看做是一个线性变换，通过剪切或者旋转将向量映射到其他向量。对于我们的协方差矩阵来说，这没有什么不同。</strong></p><p id="b329" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">了解线性变换矩阵的最佳方法是查看其特征向量</strong>——这些向量不会被矩阵旋转或剪切，而只会缩放与其特征值相等的量。如果<em class="lp"> v </em>是特征向量，λ是其特征值，则:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/478cbcdaa9d6a33dbc0999c1c5156a1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:188/format:webp/1*2rqzZMEt1K89MfoNInWdAQ.jpeg"/></div></figure><p id="c7ae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，协方差矩阵的特征向量具有特殊的性质。协方差矩阵是对称的，具有<strong class="jp ir">正交特征向量</strong>——单位长度且相互正交的向量<a class="ae kl" href="https://yutsumura.com/orthogonality-of-eigenvectors-of-a-symmetric-matrix-corresponding-to-distinct-eigenvalues/" rel="noopener ugc nofollow" target="_blank">证明</a>。这产生了以下恒等式:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/b6f1735f26c324cdb241710c02cd8b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*WHa3FlR1Fe1GqLjyADDlgA.jpeg"/></div></figure><p id="4a6e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">特征向量的优雅来自一个简单的事实:<strong class="jp ir">空间中的任何向量都可以表示为一个矩阵的特征向量的线性组合。</strong>这种优雅背后的原因是，我们现在可以<strong class="jp ir">根据特征向量和特征值，计算由矩阵<em class="lp"> C、</em>表示的线性变换对任何向量<em class="lp"> u </em>的影响。</strong>这将复杂的矩阵乘法程序<strong class="jp ir"> <em class="lp"> C.u </em> </strong>转化为简单的矢量缩放<em class="lp">。</em>要了解这是如何做到的，请注意,( 2 x 2)协方差矩阵有两个对应于每个维度的正交本征向量。<strong class="jp ir">任何向量<em class="lp"> u </em>都可以用两个特征向量<em class="lp"> v </em> 1 和<em class="lp"> v </em> 2 表示为:</strong></p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/0e00a4fc7f1336064b876d208b68dfdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*1ZUCLZ1HOsgR96tEVBApQQ.jpeg"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Where k1<strong class="bd mv"> </strong>and k2 are scalars.</figcaption></figure><p id="9ac3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以要找出协方差矩阵对任何向量的影响<strong class="jp ir"><em class="lp"/></strong>:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/261679650a357e65785a37168eaf4220.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*KHBKjh6K296ELilyOSNY2A.jpeg"/></div></figure><p id="6702" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在可以把我们想要最大化的量，<strong class="jp ir"> <em class="lp"> (u^T)Cu </em> </strong>)写成:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/65b4f8f6aa19587a8cadf7918b1489bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*uvZT_w0CJdx081oem6YqqA.jpeg"/></div></figure><p id="aa08" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在最后一步中我们使用了标准正交恒等式。</p><h1 id="ac80" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">大惊喜</h1><p id="e4a1" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">找到最大化<strong class="jp ir"><em class="lp">【σ】</em>=<em class="lp">(u^t)cu</em></strong>)的<strong class="jp ir"> <em class="lp"> u </em> </strong>就是找到<strong class="jp ir"> <em class="lp"> k </em> 1 </strong>和<strong class="jp ir"><em class="lp">k</em>2</strong>——定义<strong class="jp ir"> <em class="lp"> u </em> </strong>的特征向量的系数。但是由于我们只关心定义最大方差线的<em class="lp">单位向量</em>，<strong class="jp ir"> <em class="lp"> k1 +k2 = </em> 1 </strong>。你现在能猜出如何最大化<strong class="jp ir"> <em class="lp"> σ </em>吗？</strong></p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/80ff7cfbf0ab12e871eed544042d8705.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*l3KtsvyWUJjJBu3BosT3yA.jpeg"/></div></figure><p id="835f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关键是要注意，如果<strong class="jp ir"> λ1 &gt; λ2 </strong>，那么最大化<strong class="jp ir">T35】σT37】的唯一方法，给定约束<em class="lp"> k1 </em> + <em class="lp"> k2 = </em> 1，<strong class="jp ir">就是将 1 赋给<em class="lp"> k </em> 1，0 赋给<em class="lp"> k </em> 2，这样最大的特征值λ1 支配总和。</strong></strong></p><p id="e4c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这让我们大吃一惊:</p><blockquote class="lq"><p id="d4bf" class="lr ls iq bd lt lu lv lw lx ly lz kk dk translated">最大方差的方向是具有最大绝对特征值的协方差矩阵的特征向量。</p></blockquote><p id="6641" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">因为如果<em class="lp"> k </em> 1=1 并且<em class="lp"> k </em> 2=0，那么<strong class="jp ir">T53】uT55】变成:</strong></p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/399f8e2f33308386b26fa51a50be17cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*mRj-jrAJT17IXkwWEz3PNg.png"/></div></figure><p id="67fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">即<em class="lp"> u，</em>方差最大的方向，<em class="lp"> </em>是具有最大特征值的特征向量<em class="lp"> v </em> 1 本身。</strong>对此的一个直接结论是，<em class="lp">第二大的</em>方差方向对应于具有<em class="lp">第二大的</em>特征值的特征向量，以此类推。</p><h1 id="4ffe" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">另一个大惊喜</h1><p id="e4dc" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">更有趣的是，如果你看我们的问题陈述:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a02ac968fee57dc7a08c23fc6f823c07.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*kCkYF5ECFA2z-xRqZOu9pw.jpeg"/></div></figure><p id="cea6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">并结合<strong class="jp ir"> <em class="lp"> v </em> </strong>为特征向量的事实，则:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/1e2c81e97f92c6ab110ff9859ca93e7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*bLzydrB2ZrWVjPIYHspgsw.jpeg"/></div></figure><p id="6026" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">即</p><blockquote class="lq"><p id="536f" class="lr ls iq bd lt lu lv lw lx ly lz kk dk translated">特征向量方向上的方差是它们的特征值。</p></blockquote><h1 id="6bc8" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx nc kz la lb nd ld le lf ne lh li lj bi translated">现在怎么办？</h1><p id="1dce" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">好吧。我们终于找到了我们都想要的向量。现在怎么办？</p><p id="62c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您还记得第 1 部分中的内容，我们通过将点投影到方差最大的线上，将 2D 数据集简化为一条 1D 线。现在我们已经有了我们需要的所有量，即数据矩阵<strong class="jp ir"> <em class="lp"> A </em> </strong>、协方差矩阵<strong class="jp ir"> <em class="lp"> C </em> </strong>及其定义我们的线的特征向量<strong class="jp ir"> <em class="lp"> v </em> 1 </strong>、<strong class="jp ir"> <em class="lp"> v </em> 2 </strong>，我们可以执行投影:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/04accac8a84bbbbd7a6d91c557ef5aeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*bKDfxKAt01Y_FMosDK7sXw.jpeg"/></div></figure><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/2712ada7580fc0f7e175135758a25d9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*ENuE-TtubSCFhdcuxeo6zg.jpeg"/></div></figure><p id="14a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">推广到更大的维度只是用额外的组件来扩充我们的矩阵。如果我们在 3D 中，协方差矩阵有 3 个特征向量，<strong class="jp ir"> <em class="lp"> v </em> 1，<em class="lp"> v </em> 2，<em class="lp"> v </em> 3 </strong>，从最大到最小<strong class="jp ir"> λ </strong>排序。但是为了更灵活地组织我们的问题，我们使用矩阵乘法的性质。我们的问题现在可以表述为:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/7f0ea8828a6bac43ebcab0caac237ab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*aKMTFTKrU_8rW1XmvZcIGA.jpeg"/></div></figure><p id="0c0e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<strong class="jp ir"> <em class="lp"> E </em> </strong>是方差(特征值)的对角矩阵，而<strong class="jp ir"> <em class="lp"> V </em> </strong>是特征向量的标准正交矩阵，按列排列。取任意数量的具有最大特征值的<strong class="jp ir">主成分</strong>(特征向量),并将我们的数据投射到它们上面以降低维数。如果我们选择<em class="lp"> v </em> 1 和<em class="lp"> v </em> 2，最大方差的平面现在由它们定义，将这些点投影到平面上相当于投影到<em class="lp"> v </em> 1 和<em class="lp"> v </em> 2 上，然后组合这些分量:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/07602fed666540923263dce31fcff3f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*9okmetbqilyMG3Seh8znxA.jpeg"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">This is just an extension to projection on a line (p = Av), where v is now a matrix V of column-wise vectors.</figcaption></figure><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/80a8211363fa8461a9021f95ec149a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/1*af59rzCxIku9oOrRoM0flQ.gif"/></div></figure><h1 id="48b6" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">一般程序</h1><p id="4179" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">求协方差矩阵的特征向量的过程称为协方差矩阵的<strong class="jp ir">特征分解</strong>，是求解 PCA 的方法之一。这个名字来源于这样一个事实，如果我们把我们的问题陈述重新组织成:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/cf51ef5bf65e6ea2639632fcd953b412.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*ctafVNfYB1sEpzJ6yrutPA.jpeg"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Using the identity V^T = V^-1 for orthonormal matrices</figcaption></figure><p id="61c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后矩阵 C 被分解成它的特征向量 V 和特征值 e。</p><p id="4963" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们应该选择多少个特征向量来投影我们的数据？经验法则是选择第一个<em class="lp"> n 个</em>特征向量，使得它们的方差(特征值)之和大于总数的 95%。</p><p id="c03a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在可以总结使用 PCA 降低维数的过程:</p><ol class=""><li id="c6ac" class="nl nm iq jp b jq jr ju jv jy nn kc no kg np kk nq nr ns nt bi translated">平均值-通过从平均值中减去每个要素来确定数据的中心。</li><li id="0ca8" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">求中心数据 C =(A^T)A.的协方差矩阵</li><li id="69a6" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">对 c 应用特征分解。</li><li id="332c" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">按照特征值降序排列特征向量。</li><li id="719c" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">选择解释总方差 95%的第一个<em class="lp"> n 个</em>特征向量。</li><li id="2ce4" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">创建一个矩阵<em class="lp"> V </em>，其中每一列都是<em class="lp"> n </em>个选择向量中的一个。</li><li id="8a42" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">将数据投影到由<em class="lp"> V </em>定义的子空间上:</li></ol><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/8a88a16e47b5408fd9ab0141fa9a63a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*X3-OybGVkYFFpB3f4xuYjg.png"/></div></figure><h1 id="611f" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">下一步是什么</h1><p id="f330" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">另一个非常有趣的解决 PCA 的方法是<strong class="jp ir">奇异值分解(SVD)。</strong>它类似于特征分解，但在实践中更通用、更广泛。但是这已经足够了，所以我把它留给<a class="ae kl" rel="noopener" target="_blank" href="/svd-8c2f72e264f">下一部分</a>。</p></div></div>    
</body>
</html>