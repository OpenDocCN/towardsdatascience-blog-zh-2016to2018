<html>
<head>
<title>Natural Language Processing in Artificial Intelligence is almost human-level accurate. Worse yet, it gets smart!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能中的自然语言处理几乎是人类级别的精确。更糟糕的是，它变聪明了！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-processing-in-artificial-intelligence-is-almost-human-level-accurate-fbdaffed6392?source=collection_archive---------3-----------------------#2017-11-02">https://towardsdatascience.com/natural-language-processing-in-artificial-intelligence-is-almost-human-level-accurate-fbdaffed6392?source=collection_archive---------3-----------------------#2017-11-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="9b15" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">深度学习时代之前</h1><p id="2dcc" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">早在深度学习时代之前的日子里——那时神经网络更像是一种可怕、神秘的数学好奇心，而不是一种强大的机器学习或人工智能工具——在<em class="lj">自然语言处理(NLP) </em>领域，经典数据挖掘算法有许多令人惊讶的相对成功的应用。似乎像T2的垃圾邮件过滤和T4的词性标注这样的问题可以用简单易懂的模型来解决。</p><p id="b3f7" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated">但并不是每个问题都能这样解决。简单的模型无法充分捕捉语言的微妙之处，如上下文、习语或讽刺(尽管人类也经常在这方面失败)。基于整体摘要的算法(例如<a class="ae lk" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">词袋</a>)被证明不足以捕捉文本数据的顺序性质，而<a class="ae lk" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"> n元语法</a>努力为一般上下文建模，并严重遭受<a class="ae lk" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">维数灾难</a>。即使是基于<a class="ae lk" href="https://en.wikipedia.org/wiki/Hidden_Markov_model" rel="noopener ugc nofollow" target="_blank">嗯</a>的模型也很难克服这些问题，因为它们具有马尔可夫性(比如，它们的无记忆性)。当然，这些方法也用于处理更复杂的NLP任务，但不是很成功。</p><h1 id="c14c" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">首次突破— Word2Vec</h1><p id="facc" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">NLP领域在单词的语义丰富表示形式方面经历了第一次重大飞跃，这是通过应用神经网络实现的..在此之前，最常见的表示方法是所谓的<em class="lj"> one-hot </em>编码，其中每个单词都被转换为唯一的二进制向量，只有一个非零条目。这种方法很大程度上受到稀疏性的影响，并且根本没有考虑特定单词的含义。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/b718779f7d309cd0ebb9f48f70cac4b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*t4uOAiQLgbAVWwPd.png"/></div></div></figure><p id="51c3" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated"><strong class="kn ir"> <em class="lj">图1: </em> </strong> <em class="lj"> Word2Vec将单词投影到二维空间上的表示法。</em></p><p id="d624" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated">相反，想象从一串单词开始，删除中间的一个，并让神经网络预测给定中间单词的上下文(<a class="ae lk" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" rel="noopener ugc nofollow" target="_blank"> skip-gram </a>)。或者，要求它基于上下文预测中心单词(即，连续的单词包，<a class="ae lk" href="http://iksinc.wordpress.com/tag/continuous-bag-of-words-cbow/" rel="noopener ugc nofollow" target="_blank"> CBOW </a>)。当然，这样的模型是没有用的，但事实证明，作为一个<em class="lj">副作用</em>，它产生了一个惊人强大的向量表示，保留了单词的语义结构。</p><h1 id="3d63" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">进一步的改进</h1><p id="36ce" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">尽管新的强大的<em class="lj"> Word2Vec </em>表示提高了许多经典算法的性能，但仍然需要一种能够捕获文本中顺序依赖关系的解决方案(<em class="lj">长期和短期</em>)。这个问题的第一个概念是所谓的香草递归神经网络(RNNs)。传统的rnn利用数据的时间特性，在使用存储在<em class="lj">隐藏状态</em>中的关于先前单词的信息的同时，将单词顺序地馈送给网络。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/3a96010f29e3f5cf267985a9aa6e5f33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Yw70zD6RL6gjbMSr.png"/></div></div></figure><p id="899c" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated"><strong class="kn ir"> <em class="lj">图2: </em> </strong> <em class="lj">一个递归神经网络。图片来自一位优秀的Colah在LSTMs </em> 上的 <a class="ae lk" href="http://http//colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"> <em class="lj">帖子</em></a></p><p id="6270" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated">事实证明，这些网络很好地处理了局部依赖性，但由于<a class="ae lk" href="http://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失梯度</a>而难以训练。为了解决这个问题，计算机科学家&amp;机器学习研究人员开发了一种新的网络拓扑结构，称为长短期记忆(<a class="ae lk" href="http://http//colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"> LSTM </a>)。LSTM通过在网络中引入被称为存储单元的特殊单元来解决这个问题。这种复杂的机制允许在不显著增加参数数量的情况下找到更长的模式。许多流行的架构也是LSTM的变体，如<a class="ae lk" href="http://arxiv.org/abs/1609.07959" rel="noopener ugc nofollow" target="_blank"> mLSTM </a>或<a class="ae lk" href="http://arxiv.org/abs/1406.1078" rel="noopener ugc nofollow" target="_blank"> GRU </a>，由于<em class="lj">存储单元</em>更新机制的智能简化，这些架构显著减少了所需的参数数量。</p><p id="d77f" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated">在<em class="lj">计算机视觉</em>中<strong class="kn ir">卷积神经网络</strong>取得惊人成功之后，它们被纳入NLP只是时间问题。今天，1D卷积是许多成功应用的热门构建模块，包括<a class="ae lk" href="http://arxiv.org/abs/1511.07122" rel="noopener ugc nofollow" target="_blank">语义分割</a>，快速<a class="ae lk" href="http://arxiv.org/abs/1610.10099" rel="noopener ugc nofollow" target="_blank">机器翻译</a>，以及通用<a class="ae lk" href="http://s3.amazonaws.com/fairseq/papers/convolutional-sequence-to-sequence-learning.pdf/" rel="noopener ugc nofollow" target="_blank">序列到序列学习</a>框架——它胜过递归网络，并且由于更容易并行化，可以更快地训练一个数量级。</p><blockquote class="mc md me"><p id="1328" class="kl km lj kn b ko ll kq kr ks lm ku kv mf ln ky kz mg lo lc ld mh lp lg lh li ij bi translated"><em class="iq">👀</em> <strong class="kn ir">卷积神经网络</strong>，最初用于解决计算机视觉问题，并在该领域保持最先进水平。了解更多关于<a class="ae lk" href="http://sigmoidal.io/dl-computer-vision-beyond-classification/" rel="noopener ugc nofollow" target="_blank">他们的应用和功能</a>。</p></blockquote><h1 id="fd8f" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">典型的NLP问题</h1><p id="bbe1" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">有各种各样的语言任务，虽然简单并且是人类的第二天性，但对机器来说却非常困难。这种混淆主要是由于语言上的细微差别，如反语和习语。让我们来看看研究人员正在努力解决的NLP的一些领域(大致按照它们的复杂性排序)。</p><p id="0622" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated">最常见也可能是最简单的是<strong class="kn ir">情感分析</strong>。本质上，这决定了说话者/作者对某个特定话题(或总体)的态度或情绪反应。可能的情绪有积极的、中立的和消极的。查看<a class="ae lk" href="http://casa.disi.unitn.it/~moschitt/since2013/2015_SIGIR_Severyn_TwitterSentimentAnalysis.pdf" rel="noopener ugc nofollow" target="_blank">这篇伟大的文章</a>关于使用深度卷积神经网络来衡量推文中的情绪。另一个有趣的实验表明，一个深度循环网络可以偶然地学习情绪<a class="ae lk" href="http://blog.openai.com/unsupervised-sentiment-neuron/" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mi"><img src="../Images/6e4068c350b540a9ffd42991a3e65f89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qOvbHZS9mOG-NNP1.png"/></div></div></figure><p id="7b88" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated"><strong class="kn ir"> <em class="lj">图3 </em> </strong> : <em class="lj">激活用于生成文本下一个字符的网络中的神经元。很明显，它学会了这种情绪，尽管它是在完全无人监督的环境中接受训练的。</em></p><p id="4a84" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated">前一种情况的自然推广是<strong class="kn ir">文档分类</strong>，我们解决了一个普通的分类问题，而不是给每篇文章分配三个可能的标志中的一个。根据对算法的综合<a class="ae lk" href="http://arxiv.org/pdf/1703.01898.pdf" rel="noopener ugc nofollow" target="_blank">比较，可以肯定地说深度学习是文本分类的必由之路。</a></p><p id="977b" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated">现在，我们开始真正的交易:<strong class="kn ir">机器翻译</strong>。很长一段时间以来，机器翻译一直是一个严峻的挑战。重要的是要理解，这是一个完全不同的任务，与前两个我们已经讨论过。对于这个任务，我们需要一个模型来预测单词序列，而不是标签。机器翻译清楚地表明了深度学习有什么值得大惊小怪的，因为当涉及到序列数据时，它已经是一个令人难以置信的突破。在这篇<a class="ae lk" href="http://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa" rel="noopener">博文</a>中，你可以读到更多关于如何——是的，你猜对了——<strong class="kn ir">递归神经网络</strong>处理翻译的内容，在这篇中，你可以了解它们如何实现最先进的结果。</p><p id="e5cb" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated">假设你需要一个自动的<strong class="kn ir">文本摘要</strong>模型，你希望它只提取文本中最重要的部分，同时保留所有的意思。这需要一种算法，它可以理解整个文本，同时专注于承载大部分意思的特定部分。这个问题被<a class="ae lk" href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/" rel="noopener ugc nofollow" target="_blank">注意机制</a>巧妙地解决了，它可以作为模块引入到<a class="ae lk" href="http://arxiv.org/pdf/1602.06023.pdf" rel="noopener ugc nofollow" target="_blank">端到端解决方案</a>中。</p><p id="9ce5" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated">最后，还有<strong class="kn ir">问答</strong>，这是你能得到的最接近人工智能的东西。对于这项任务，模型不仅需要理解一个问题，还需要对感兴趣的文本有全面的理解，并确切地知道在哪里寻找答案。对于一个问题回答解决方案的详细解释(当然是使用深度学习)，请查看<a class="ae lk" href="http://einstein.ai/research/state-of-the-art-deep-learning-model-for-question-answering" rel="noopener ugc nofollow" target="_blank">这篇文章</a>。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mj"><img src="../Images/fcec619b3fdb14ce653f0640efd5c292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H-0kDCegs7tP6t18.png"/></div></div></figure><p id="d40a" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated"><strong class="kn ir"> <em class="lj">图4: </em> </strong> <em class="lj">漂亮</em> <a class="ae lk" href="http://distill.pub/2016/augmented-rnns/" rel="noopener ugc nofollow" target="_blank"> <em class="lj">可视化</em> </a> <em class="lj">一个被训练成将英语翻译成法语的递归神经网络中的注意机制。</em></p><p id="68fb" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated">由于深度学习为各种类型的数据(例如，文本和图像)提供了向量表示，因此您可以建立模型来专攻不同的领域。研究人员就是这样想出了<a class="ae lk" href="https://arxiv.org/abs/1705.03865" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir">视觉问答</strong> </a>。任务很“琐碎”:只需回答一个关于某个图像的问题。听起来像是7岁小孩的工作，对吧？尽管如此，深度模型是第一个在没有人类监督的情况下产生合理结果的模型。该模型的结果和描述见本文。</p><h1 id="eb2f" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">概述</h1><p id="5029" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在你知道了。由于计算问题，深度学习最近才出现在NLP中，我们需要学习更多关于深度神经网络的知识，以了解它们的能力。但一旦我们做到了，它就永远改变了游戏。</p><p id="ee42" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw ln ky kz la lo lc ld le lp lg lh li ij bi translated"><strong class="kn ir"> <em class="lj">转帖:</em> </strong> <a class="ae lk" href="https://sigmoidal.io/boosting-your-solutions-with-nlp/" rel="noopener ugc nofollow" target="_blank">用NLP提升你的解决方案，sigmoid id . io</a></p></div></div>    
</body>
</html>