<html>
<head>
<title>GAN — Introduction and Implementation — PART1: Implement a simple GAN in TF for MNIST handwritten digit generation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GAN —简介和实现—第1部分:在TF中实现一个简单的GAN，用于MNIST手写数字生成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gan-introduction-and-implementation-part1-implement-a-simple-gan-in-tf-for-mnist-handwritten-de00a759ae5c?source=collection_archive---------2-----------------------#2017-06-27">https://towardsdatascience.com/gan-introduction-and-implementation-part1-implement-a-simple-gan-in-tf-for-mnist-handwritten-de00a759ae5c?source=collection_archive---------2-----------------------#2017-06-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="186a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">GANs背后的想法是，你有两个网络，一个生成器GG和一个鉴别器DD，彼此竞争。生成器生成假数据传递给鉴别器。鉴别器也能看到真实数据，并预测它收到的数据是真是假。生成器被训练来欺骗鉴别器，它想要输出看起来尽可能接近真实数据的数据。鉴别器被训练来辨别哪些数据是真的，哪些是假的。最终发生的情况是，生成器学习生成与鉴别器的真实数据无法区分的数据。</p><blockquote class="kp kq kr"><p id="5993" class="jq jr ko js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated">这是平衡状态，期望值是鉴别器发出的真实和虚假数据的概率都是0.5。</p></blockquote><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi kv"><img src="../Images/5093689fa90965cdc85240c1d930269b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M2Er7hbryb2y0RP1UOz5Rw.png"/></div></div></figure><p id="026d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">GAN的一般结构如上图所示，使用MNIST图像作为数据。潜在样本是一个随机向量，生成器用它来构造假图像。当生成器通过训练学习时，它会想出如何将这些随机向量映射到可识别的图像，以欺骗鉴别器。</p><p id="bd07" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">鉴别器的输出是sigmoid函数，其中0表示伪图像，1表示真实图像。如果你只对生成新图像感兴趣，你可以在训练后扔掉鉴别器。</p><h2 id="7a4e" class="lh li it bd lj lk ll dn lm ln lo dp lp kb lq lr ls kf lt lu lv kj lw lx ly lz bi translated">实施:</h2><p id="8669" class="pw-post-body-paragraph jq jr it js b jt ma jv jw jx mb jz ka kb mc kd ke kf md kh ki kj me kl km kn im bi translated"><a class="ae mf" href="https://github.com/mchablani/deep-learning/blob/master/gan_mnist/Intro_to_GANs_Exercises.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/mchablani/deep-learning/blob/master/gan _ mnist/Intro _ to _ GANs _ exercises . ipynb</a></p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mg"><img src="../Images/f4c8da9978ed9685a07c15abbfee6224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*J_Aul_wedIFTwqYW.png"/></div></div></figure><blockquote class="kp kq kr"><p id="ebb3" class="jq jr ko js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated">我们使用一个泄漏的ReLU来允许梯度不受阻碍地流过这个层。TensorFlow没有为leaky ReLUs提供操作，你可以只从一个线性全连接层中取出输出，然后把它们传递给<code class="fe mh mi mj mk b">tf.maximum</code>。通常，参数<code class="fe mh mi mj mk b">alpha</code>设置负值的输出幅度。因此，负输入(<code class="fe mh mi mj mk b">x</code>)值的输出为<code class="fe mh mi mj mk b">alpha*x</code>，正<code class="fe mh mi mj mk b">x</code>的输出为<code class="fe mh mi mj mk b">x</code>:</p><p id="4c01" class="jq jr ko js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated">对于发电机，我们将对其进行培训，同时在培训期间和培训结束后从其<em class="it">取样</em>。鉴别器需要在伪输入图像和真实输入图像之间共享变量。因此，我们可以使用<code class="fe mh mi mj mk b">tf.variable_scope</code>的<code class="fe mh mi mj mk b">reuse</code>关键字来告诉TensorFlow重用变量，而不是在我们再次构建图时创建新的变量。</p><p id="6e7d" class="jq jr ko js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated">已经发现，对于发电机输出，发电机与tanh一起表现最佳。这意味着我们必须将MNIST图像重新缩放到-1和1之间，而不是0和1之间。</p></blockquote><pre class="kw kx ky kz gt ml mk mm mn aw mo bi"><span id="4506" class="lh li it mk b gy mp mq l mr ms"><strong class="mk iu">def</strong> generator(z, out_dim, n_units=128, reuse=<strong class="mk iu">False</strong>,  alpha=0.01):    <br/>    <strong class="mk iu">with</strong> tf.variable_scope('generator', reuse=reuse):<br/>        <em class="ko"># Hidden layer</em><br/>        h1 = tf.layers.dense(z, n_units, activation=<strong class="mk iu">None</strong>)<br/>        <em class="ko"># Leaky ReLU</em><br/>        h1 = tf.maximum(h1, alpha*h1)<br/>        <br/>        <em class="ko"># Logits and tanh output</em><br/>        logits = tf.layers.dense(h1, out_dim, activation=<strong class="mk iu">None</strong>)<br/>        out = tf.nn.tanh(logits)<br/>        <br/>        <strong class="mk iu">return</strong> out, logits</span></pre><p id="6e8c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">鉴频器网络与发生器网络几乎完全相同，只是我们使用的是sigmoid输出层。</p><pre class="kw kx ky kz gt ml mk mm mn aw mo bi"><span id="6771" class="lh li it mk b gy mp mq l mr ms"><strong class="mk iu">def</strong> discriminator(x, n_units=128, reuse=<strong class="mk iu">False</strong>, alpha=0.01):<br/>    <strong class="mk iu">with</strong> tf.variable_scope('discriminator', reuse=reuse):<br/>        <em class="ko"># Hidden layer</em><br/>        h1 = tf.layers.dense(x, n_units, activation=<strong class="mk iu">None</strong>)<br/>        <em class="ko"># Leaky ReLU</em><br/>        h1 = tf.maximum(h1, alpha*h1)<br/>        <br/>        logits = tf.layers.dense(h1, 1, activation=<strong class="mk iu">None</strong>)<br/>        out = tf.nn.sigmoid(logits)<br/>        <br/>        <strong class="mk iu">return</strong> out, logits</span><span id="7ee8" class="lh li it mk b gy mt mq l mr ms"><em class="ko"># </em>Hyperparameters<em class="ko"><br/># Size of input image to discriminator</em><br/>input_size = 784 <em class="ko"># 28x28 MNIST images flattened</em><br/><em class="ko"># Size of latent vector to generator</em><br/>z_size = 100<br/><em class="ko"># Sizes of hidden layers in generator and discriminator</em><br/>g_hidden_size = 128<br/>d_hidden_size = 128<br/><em class="ko"># Leak factor for leaky ReLU</em><br/>alpha = 0.01<br/><em class="ko"># Label smoothing </em><br/>smooth = 0.1</span></pre><h1 id="625d" class="mu li it bd lj mv mw mx lm my mz na lp nb nc nd ls ne nf ng lv nh ni nj ly nk bi translated">构建网络</h1><pre class="kw kx ky kz gt ml mk mm mn aw mo bi"><span id="08fa" class="lh li it mk b gy mp mq l mr ms">tf.reset_default_graph() <br/><em class="ko"># Create our input placeholders</em> <br/>input_real, input_z = model_inputs(input_size, z_size)  <br/><em class="ko"># Generator network here</em> <br/>g_model, g_logits = generator(input_z, input_size, g_hidden_size, reuse=<strong class="mk iu">False</strong>,  alpha=alpha) <br/><em class="ko"># g_model is the generator output</em>  </span><span id="710f" class="lh li it mk b gy mt mq l mr ms"><em class="ko"># Disriminator network here</em> <br/>d_model_real, d_logits_real = discriminator(input_real, d_hidden_size, reuse=<strong class="mk iu">False</strong>, alpha=alpha) <br/>d_model_fake, d_logits_fake = discriminator(g_model, d_hidden_size, reuse=<strong class="mk iu">True</strong>, alpha=alpha)</span></pre><h1 id="677e" class="mu li it bd lj mv mw mx lm my mz na lp nb nc nd ls ne nf ng lv nh ni nj ly nk bi translated">鉴频器和发电机损耗</h1><blockquote class="kp kq kr"><p id="eb0c" class="jq jr ko js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated">对于鉴别器，总损失是真实和伪造图像的损失之和，<code class="fe mh mi mj mk b">d_loss = d_loss_real + d_loss_fake</code>。</p><p id="7896" class="jq jr ko js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated">对于真实的图像逻辑，我们将使用从上面单元格中的鉴别器获得的<code class="fe mh mi mj mk b">d_logits_real</code>。对于标签，我们希望它们都是1，因为这些都是真实的图像。为了帮助鉴别器更好地归纳，标签从1.0减少到0.9，例如，使用参数<code class="fe mh mi mj mk b">smooth</code>。这被称为标签平滑，通常与分类器一起使用以提高性能。在TensorFlow中，它看起来有点像<code class="fe mh mi mj mk b">labels = tf.ones_like(tensor) * (1 - smooth)</code></p><p id="6074" class="jq jr ko js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated">伪数据的鉴别器损耗是相似的。逻辑是<code class="fe mh mi mj mk b">d_logits_fake</code>，它是我们通过将生成器输出传递给鉴别器得到的。这些假逻辑与全零标签一起使用。请记住，我们希望鉴频器为真实图像输出1，为虚假图像输出0，因此我们需要设置损耗来反映这一点。</p><p id="0068" class="jq jr ko js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated">最后，发电机损耗使用<code class="fe mh mi mj mk b">d_logits_fake</code>，假图像逻辑。但是，现在的标签都是一。生成器试图欺骗鉴别器，所以它希望鉴别器输出假图像。</p></blockquote><pre class="kw kx ky kz gt ml mk mm mn aw mo bi"><span id="a7dc" class="lh li it mk b gy mp mq l mr ms"><em class="ko"># Calculate losses</em><br/>d_labels_real = tf.ones_like(d_logits_real) * (1 - smooth)<br/>d_labels_fake = tf.zeros_like(d_logits_fake)<br/><br/>d_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(labels=d_labels_real, logits=d_logits_real)<br/>d_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(labels=d_labels_fake, logits=d_logits_fake)<br/><br/>d_loss = tf.reduce_mean(d_loss_real + d_loss_fake)<br/><br/>g_loss = tf.reduce_mean(<br/>    tf.nn.sigmoid_cross_entropy_with_logits(<br/>        labels=tf.ones_like(d_logits_fake), <br/>        logits=d_logits_fake))</span></pre><h1 id="744d" class="mu li it bd lj mv mw mx lm my mz na lp nb nc nd ls ne nf ng lv nh ni nj ly nk bi translated">优化者</h1><p id="22a3" class="pw-post-body-paragraph jq jr it js b jt ma jv jw jx mb jz ka kb mc kd ke kf md kh ki kj me kl km kn im bi translated">我们希望分别更新生成器和鉴别器变量。</p><p id="001d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请注意，当最小化d_loss时，我们希望优化器只更新鉴别器变量以及生成器的类似变量。</p><pre class="kw kx ky kz gt ml mk mm mn aw mo bi"><span id="ae99" class="lh li it mk b gy mp mq l mr ms"><em class="ko"># Optimizers</em><br/>learning_rate = 0.002<br/><br/><em class="ko"># Get the trainable_variables, split into G and D parts</em><br/>t_vars = tf.trainable_variables()<br/>g_vars = [var <strong class="mk iu">for</strong> var <strong class="mk iu">in</strong> t_vars <strong class="mk iu">if</strong> var.name.startswith("generator")]<br/>d_vars = [var <strong class="mk iu">for</strong> var <strong class="mk iu">in</strong> t_vars <strong class="mk iu">if</strong> var.name.startswith("discriminator")]<br/><br/>d_train_opt = tf.train.AdamOptimizer().minimize(d_loss, var_list=d_vars)<br/>g_train_opt = tf.train.AdamOptimizer().minimize(g_loss, var_list=g_vars)</span></pre><h1 id="158e" class="mu li it bd lj mv mw mx lm my mz na lp nb nc nd ls ne nf ng lv nh ni nj ly nk bi translated">培养</h1><pre class="kw kx ky kz gt ml mk mm mn aw mo bi"><span id="7005" class="lh li it mk b gy mp mq l mr ms">batch_size = 100<br/>epochs = 100<br/>samples = []<br/>losses = []<br/>saver = tf.train.Saver(var_list = g_vars)<br/><strong class="mk iu">with</strong> tf.Session() <strong class="mk iu">as</strong> sess:<br/>    sess.run(tf.global_variables_initializer())<br/>    <strong class="mk iu">for</strong> e <strong class="mk iu">in</strong> range(epochs):<br/>        <strong class="mk iu">for</strong> ii <strong class="mk iu">in</strong> range(mnist.train.num_examples//batch_size):<br/>            batch = mnist.train.next_batch(batch_size)<br/>            <br/>            <em class="ko"># Get images, reshape and rescale to pass to D</em><br/>            batch_images = batch[0].reshape((batch_size, 784))<br/>            batch_images = batch_images*2 - 1<br/>            <br/>            <em class="ko"># Sample random noise for G</em><br/>            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))<br/>            <br/>            <em class="ko"># Run optimizers</em><br/>            _ = sess.run(d_train_opt, feed_dict={input_real: batch_images, input_z: batch_z})<br/>            _ = sess.run(g_train_opt, feed_dict={input_z: batch_z})<br/>        <br/>        <em class="ko"># At the end of each epoch, get the losses and print them out</em><br/>        train_loss_d = sess.run(d_loss, {input_z: batch_z, input_real: batch_images})<br/>        train_loss_g = g_loss.eval({input_z: batch_z})<br/>            <br/>        print("Epoch <strong class="mk iu">{}</strong>/<strong class="mk iu">{}</strong>...".format(e+1, epochs),<br/>              "Discriminator Loss: <strong class="mk iu">{:.4f}</strong>...".format(train_loss_d),<br/>              "Generator Loss: <strong class="mk iu">{:.4f}</strong>".format(train_loss_g))    <br/>        <em class="ko"># Save losses to view after training</em><br/>        losses.append((train_loss_d, train_loss_g))<br/>        <br/>        <em class="ko"># Sample from generator as we're training for viewing afterwards</em><br/>        sample_z = np.random.uniform(-1, 1, size=(16, z_size))<br/>        gen_samples = sess.run(<br/>                       generator(input_z, input_size, reuse=<strong class="mk iu">True</strong>),<br/>                       feed_dict={input_z: sample_z})<br/>        samples.append(gen_samples)<br/>        saver.save(sess, './checkpoints/generator.ckpt')<br/><br/><em class="ko"># Save training generator samples</em><br/><strong class="mk iu">with</strong> open('train_samples.pkl', 'wb') <strong class="mk iu">as</strong> f:<br/>    pkl.dump(samples, f)</span></pre><p id="2fbf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">学分:来自课堂讲稿:<a class="ae mf" href="https://classroom.udacity.com/nanodegrees/nd101/syllabus" rel="noopener ugc nofollow" target="_blank">https://classroom.udacity.com/nanodegrees/nd101/syllabus</a></p></div></div>    
</body>
</html>