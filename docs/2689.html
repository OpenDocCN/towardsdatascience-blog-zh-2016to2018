<html>
<head>
<title>Deploy a Python model (more efficiently) over Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Spark 上部署 Python 模型(更高效)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deploy-a-python-model-more-efficiently-over-spark-497fc03e0a8d?source=collection_archive---------0-----------------------#2018-02-22">https://towardsdatascience.com/deploy-a-python-model-more-efficiently-over-spark-497fc03e0a8d?source=collection_archive---------0-----------------------#2018-02-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/28b49ee6a4708bdd79b7aaeb99159a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*POBZLlpD0hRFdl6c8mTAEQ.jpeg"/></div></div></figure><p id="d135" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">更新:我写了关于</strong> <a class="ae kw" rel="noopener" target="_blank" href="/a-different-way-to-deploy-a-python-model-over-spark-2da4d625f73e"> <strong class="ka ir">部署 Scikit 的另一种方式</strong></a><strong class="ka ir">——通过 Spark 学习模型，避免了这种方法的一些问题。</strong></p><p id="008e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我把这个贴出来，希望其他人可以避免一些我必须经历的痛苦。有时候我想在 scikit-learn 中训练一个模型，但是在 Spark 中为一个大型数据集中的所有记录建模结果。是的，我知道我可以使用 Spark MLlib，但我发现 scikit-learn 有一套更强大的产品，我理解 scikit 中的代码-learn 比我理解 MLlib 中的代码更好，我只是更熟悉 scikit-learn。然而，通过 PySpark UDF 调用 scikit-learn 的“predict”方法会产生一些问题:</p><ul class=""><li id="b7a9" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated">对于 Spark 数据帧的每一条记录，它都会导致对模型对象进行清理和拆包的开销。</li><li id="c7e5" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">它没有利用 scikit-learn 的优化，这主要是由于对 NumPy 数组的矢量化函数调用。</li></ul><p id="11e4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我发现可以通过对 spark 数据帧进行分组来减轻一些开销，以便一次在多个记录上调用我的 Python 对象的方法。你可以这样做:</p><ol class=""><li id="a71d" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv ll ld le lf bi translated">在 Spark 数据框中设置三列:</li></ol><p id="cfb0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> *一个唯一的 id。这可以是任何东西。只要它是独一无二的，你就可以去。</strong></p><p id="5297" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所有你的预测者。您可以使用 Structype 或 MLLib 的 VectorAssembler 将所有预测器放入一列。</p><p id="1a72" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> *一组列。</strong>你可以调用 row_number()按你想要的组数取模运算。我发现创建足够多的组，每个组包含 50-100k 条记录，这样做通常很有效。</p><p id="c191" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2.调用 Spark SQL 函数“create_map ”,将您的唯一 id 和预测值列合并成一列，其中每条记录都是一个键值存储。</p><p id="7501" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">3.按 groups 列分组，并在 key-value 列上调用 Spark SQL 函数“collect_list”。这将把你的数据集合到字典列表中。</p><p id="4c0a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">4.传播你的 scikit-learn 模型。</p><p id="5ada" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">5.创建一个 UDF，将字典列表解包为一个键列表(您的唯一 id)和一个列表列表(您的预测值)。然后，您可以将列表列表直接输入到广播的 scikit-learn 模型的“predict”方法中。然后用您的键列表压缩该函数调用的结果，并转换成一个字典。udf 将返回一个 MapType，根据您的键所采用的格式以及您希望从 scikit-learn 函数调用返回的格式来适当地设置键和值的类型。</p><p id="8793" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">6.对 udf 的结果调用 explode，并包含两个别名—一个用于键，一个用于结果。然后，您将拥有一个新的数据框，其大小与原始(预分组)数据框相同，其中一列是您的结果，另一列是可用于将结果与原始数据连接起来的键。</p><p id="5e04" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里有一个例子:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="18ce" class="lv lw iq lr b gy lx ly l lz ma">"""<br/>assumes the following already exist within the environment:<br/>`model`: a scikit-learn class that predicts probabilities for a two-class (0.0, 1.0) model<br/>`sdf`: a spark dataframe with at least two columns: "unique_id" and "feature_list"<br/>"""</span><span id="e6d2" class="lv lw iq lr b gy mb ly l lz ma"><strong class="lr ir">import</strong> <!-- -->pyspark.sql.functions as f<br/><strong class="lr ir">import</strong> <!-- -->pyspark.sql.types as t<br/><strong class="lr ir">import</strong> <!-- -->pyspark.sql.window.Window as w<br/><strong class="lr ir">from</strong> <!-- -->pyspark.context <strong class="lr ir">import</strong> <!-- -->SparkContext</span><span id="36a1" class="lv lw iq lr b gy mb ly l lz ma">sc <strong class="lr ir">=</strong> <!-- -->SparkContext.getOrCreate()</span><span id="8457" class="lv lw iq lr b gy mb ly l lz ma"># broadcast model<br/>model_broadcast <strong class="lr ir">=</strong> <!-- -->sc.broadcast(model)</span><span id="a958" class="lv lw iq lr b gy mb ly l lz ma"># udf to predict on the cluster<br/><strong class="lr ir">def</strong> <!-- -->predict_new(feature_map):</span><span id="5744" class="lv lw iq lr b gy mb ly l lz ma">    ids, features <strong class="lr ir">=</strong> <!-- -->zip(<strong class="lr ir">*</strong>[<br/>        (k,  v) <strong class="lr ir">for</strong> <!-- -->d <strong class="lr ir">in</strong> <!-- -->feature_map <strong class="lr ir">for</strong> <!-- -->k, v <strong class="lr ir">in</strong> <!-- -->d.items()<br/>    ])</span><span id="0664" class="lv lw iq lr b gy mb ly l lz ma">    ind <strong class="lr ir">=</strong> <!-- -->model_broadcast.value.classes_.tolist().index(1.0)</span><span id="371f" class="lv lw iq lr b gy mb ly l lz ma">    probs <strong class="lr ir">=</strong> <!-- -->[<br/>        float(v) <strong class="lr ir">for</strong> <!-- -->v <strong class="lr ir">in</strong> <br/>        <!-- -->model_broadcast.value.predict_proba(features)[:, ind]<br/>    ]</span><span id="4aa3" class="lv lw iq lr b gy mb ly l lz ma"><strong class="lr ir">    return</strong> <!-- -->dict(zip(ids, probs))</span><span id="327e" class="lv lw iq lr b gy mb ly l lz ma">predict_new_udf <strong class="lr ir">=</strong> <!-- -->f.udf(<br/>    predict_new, <br/>    t.MapType(t.LongType(), t.FloatType()<br/>)</span><span id="1abd" class="lv lw iq lr b gy mb ly l lz ma"># set the number of prediction groups to create<br/>nparts <strong class="lr ir">=</strong> <!-- -->5000</span><span id="525f" class="lv lw iq lr b gy mb ly l lz ma"># put everything together<br/>outcome_sdf <strong class="lr ir">=</strong> <!-- -->(<br/>    sdf<br/>    .select(<br/>        f.create_map(<br/>            f.col('unique_id'),     <br/>            f.col('feature_list')<br/>        ).alias('feature_map'), <br/>        (<br/>            f.row_number().over(<br/>                w.partitionBy(f.lit(1)).orderBy(f.lit(1))<br/>            ) <strong class="lr ir">% </strong>nparts<br/>        ).alias('grouper')<br/>    )<br/>    .groupby(f.col('grouper'))<br/>    .agg(<br/>        f.collect_list(f.col('feature_map')).alias('feature_map')<br/>    )<br/>    .select(<br/>        predict_new_udf(f.col('feature_map')).alias('results')<br/>    )<br/>    .select(<br/>        f.explode(f.col('results'))<br/>        .alias('unique_id', 'probability_estimate')<br/>    )<br/>)</span></pre><p id="54bc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我希望能够展示一些图表，将上述方法的性能与在 PySpark UDF 中简单包装对模型的 predict 方法的调用的性能进行比较，但是我不能:我想出了这个方法，因为我无法让这个简单的方法完成。我有一个超过 1 亿条记录的数据集。我训练了一个朴素贝叶斯(使用 scikit-learn 的 MultinomialNB)分类器，根据散列的术语-文档矩阵(使用 scikit-learn 的 HashingVectorizer)来区分两个类——0 和 1。起初，我只是简单地广播了训练好的模型，然后编写了一个 UDF，它将一个预处理过的字符串作为参数。UDF 通过哈希矢量器运行字符串，然后将这些结果输入模型预测方法。然后，我运行脚本并监控 YARN。两天后，这个过程大约完成了 10%。然后，我按照我在这里概述的方法重新编写了这个过程:UDF 接受一个字符串列表作为参数，这个列表由哈希矢量器处理，结果提供给模型的 predict 方法。整个过程在 30 分钟内完成。</p><p id="ed58" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后一点:不同的 scikit-learn 模型在内存中有很大的不同。朴素贝叶斯模型只需要为每个参数保留几个值。随机森林需要保留森林中的每一棵树。因为您需要将模型传播给每个执行者，所以您会很容易地发现，根据大量数据训练的模型需要大量内存。</p></div></div>    
</body>
</html>