<html>
<head>
<title>Getting Started With Apache Spark, Python and PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark、Python 和 PySpark 入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/working-with-apache-spark-python-and-pyspark-128a82668e67?source=collection_archive---------4-----------------------#2018-11-07">https://towardsdatascience.com/working-with-apache-spark-python-and-pyspark-128a82668e67?source=collection_archive---------4-----------------------#2018-11-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/c5c8b07a38ddc2ed6903adc6976c5332.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*bJsYVXM8Z3uG-mSxGsp0Zg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image Source: www.<a class="ae jy" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">spark.apache.org</a></figcaption></figure><p id="0680" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">本文是 Apache Spark 单节点安装的快速指南，以及如何使用 Spark python 库 PySpark。</p><h1 id="fcb1" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">1.环境</h1><ul class=""><li id="fcc1" class="lv lw iq kb b kc lx kg ly kk lz ko ma ks mb kw mc md me mf bi translated">Hadoop 版本:3.1.0</li><li id="eab3" class="lv lw iq kb b kc mg kg mh kk mi ko mj ks mk kw mc md me mf bi translated">阿帕奇卡夫卡版本:1.1.1</li><li id="ccaf" class="lv lw iq kb b kc mg kg mh kk mi ko mj ks mk kw mc md me mf bi translated">操作系统:Ubuntu 16.04</li><li id="de1c" class="lv lw iq kb b kc mg kg mh kk mi ko mj ks mk kw mc md me mf bi translated">Java 版本:Java 8</li></ul><h1 id="6476" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">2.先决条件</h1><p id="2797" class="pw-post-body-paragraph jz ka iq kb b kc lx ke kf kg ly ki kj kk ml km kn ko mm kq kr ks mn ku kv kw ij bi translated">Apache Spark 需要 Java。要确保安装了 Java，首先更新操作系统，然后尝试安装它:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="47ea" class="mx ky iq mt b gy my mz l na nb">sudo apt-get update</span><span id="fe40" class="mx ky iq mt b gy nc mz l na nb">sudo apt-get –y upgrade</span><span id="726b" class="mx ky iq mt b gy nc mz l na nb">sudo add-apt-repository -y ppa:webupd8team/java</span><span id="4ffe" class="mx ky iq mt b gy nc mz l na nb">sudo apt-get install oracle-java8-installer</span></pre><h1 id="ac33" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">3.安装 Apache Spark</h1><h1 id="0366" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">3.1.下载并安装 Spark</h1><p id="44fc" class="pw-post-body-paragraph jz ka iq kb b kc lx ke kf kg ly ki kj kk ml km kn ko mm kq kr ks mn ku kv kw ij bi translated">首先，我们需要为 apache Spark 创建一个目录。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="2b8a" class="mx ky iq mt b gy my mz l na nb">sudo mkdir /opt/spark</span></pre><p id="4937" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">然后，我们需要下载 apache spark 二进制包。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="da25" class="mx ky iq mt b gy my mz l na nb"><strong class="mt ir">wget “</strong>http://www-eu.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz<strong class="mt ir">”</strong></span></pre><p id="d40e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">接下来，我们需要将 apache spark 文件解压缩到/opt/spark 目录中</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="1e54" class="mx ky iq mt b gy my mz l na nb">sudo tar -xzvf spark-2.3.1-bin-hadoop2.7.tgz --directory=/opt/spark -- strip 1</span></pre><h1 id="dc10" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">3.2.配置 Apache Spark</h1><p id="2164" class="pw-post-body-paragraph jz ka iq kb b kc lx ke kf kg ly ki kj kk ml km kn ko mm kq kr ks mn ku kv kw ij bi translated">当 Spark 启动作业时，它会将其 jar 文件传输到 HDFS，这样任何正在工作的机器都可以使用这些文件。这些文件对于较小的作业来说是一笔很大的开销，所以我把它们打包，复制到 HDFS，并告诉 Spark 它不再需要复制它们了。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="4a75" class="mx ky iq mt b gy my mz l na nb">jar cv0f ~/spark-libs.jar -C /opt/spark/jars/ .</span><span id="1a55" class="mx ky iq mt b gy nc mz l na nb">hdfs dfs -mkdir /spark-libs</span><span id="5509" class="mx ky iq mt b gy nc mz l na nb">hdfs dfs -put ~/spark-libs.jar /spark-libs/</span></pre><p id="b935" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">复制文件后，我们必须告诉 spark 忽略从 Spark 默认配置文件中复制 jar 文件:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="f550" class="mx ky iq mt b gy my mz l na nb">sudo gedit /opt/spark/conf/spark-defaults.conf</span></pre><p id="3a6c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">添加以下几行:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="61da" class="mx ky iq mt b gy my mz l na nb">spark.master spark://localhost:7077</span><span id="7160" class="mx ky iq mt b gy nc mz l na nb">spark.yarn.preserve.staging.files true</span><span id="8d5b" class="mx ky iq mt b gy nc mz l na nb">spark.yarn.archive hdfs:///spark-libs/spark-libs.jar</span></pre><p id="7579" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在本文中，我们将配置 Apache Spark 在单个节点上运行，因此它将只是 localhost:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="e234" class="mx ky iq mt b gy my mz l na nb">sudo gedit /opt/spark/conf/slaves</span></pre><p id="6041" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">确保它只包含值 localhost</p><p id="a45f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在运行服务之前，我们必须打开。使用 gedit 的 bashrc 文件</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="83a9" class="mx ky iq mt b gy my mz l na nb">sudo gedit ~/.bashrc</span></pre><p id="82e8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">并添加以下几行</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="9654" class="mx ky iq mt b gy my mz l na nb">export SPARK_HOME=/opt/spark</span><span id="6e30" class="mx ky iq mt b gy nc mz l na nb">export SPARK_CONF_DIR=/opt/spark/conf</span><span id="8866" class="mx ky iq mt b gy nc mz l na nb">export SPARK_MASTER_HOST=localhost</span></pre><p id="90e8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在，我们必须运行 Apache Spark 服务:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="39cc" class="mx ky iq mt b gy my mz l na nb">sudo /opt/spark/sbin/start-master.sh</span><span id="c323" class="mx ky iq mt b gy nc mz l na nb">sudo /opt/spark/sbin/start-slaves.sh</span></pre><h1 id="d56e" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">4.安装 Python</h1><h1 id="e930" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">4.1.获取最新的 Python 版本</h1><p id="09da" class="pw-post-body-paragraph jz ka iq kb b kc lx ke kf kg ly ki kj kk ml km kn ko mm kq kr ks mn ku kv kw ij bi translated">Ubuntu 16.04 预装了 Python 3 和 Python 2。为了确保我们的版本是最新的，我们必须用 apt-get 更新和升级系统(在先决条件一节中提到):</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="ee1a" class="mx ky iq mt b gy my mz l na nb">sudo apt-get update</span><span id="c991" class="mx ky iq mt b gy nc mz l na nb">sudo apt-get -y upgrade</span></pre><p id="4832" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们可以通过键入以下命令来检查系统中安装的 Python 3 的版本:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="8338" class="mx ky iq mt b gy my mz l na nb">python3 –V</span></pre><p id="533f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">它必须返回 python 版本(例如:Python 3.5.2)</p><h1 id="0de9" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">4.2.安装 Python 工具</h1><p id="8104" class="pw-post-body-paragraph jz ka iq kb b kc lx ke kf kg ly ki kj kk ml km kn ko mm kq kr ks mn ku kv kw ij bi translated">要管理 Python 的软件包，我们必须安装 pip 实用程序:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="40a3" class="mx ky iq mt b gy my mz l na nb">sudo apt-get install -y python3-pip</span></pre><p id="1a28" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">还需要安装一些软件包和开发工具，以确保我们的编程环境有一个健壮的设置。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="2d1b" class="mx ky iq mt b gy my mz l na nb">sudo apt-get install build-essential libssl-dev libffi-dev python-dev</span></pre><h1 id="f687" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">4.3.营造环境</h1><p id="dbd4" class="pw-post-body-paragraph jz ka iq kb b kc lx ke kf kg ly ki kj kk ml km kn ko mm kq kr ks mn ku kv kw ij bi translated">我们需要首先安装 venv 模块，它允许我们创建虚拟环境:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="c911" class="mx ky iq mt b gy my mz l na nb">sudo apt-get install -y python3-venv</span></pre><p id="27f7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">接下来，我们必须为我们的环境创建一个目录</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="053c" class="mx ky iq mt b gy my mz l na nb">mkdir testenv</span></pre><p id="87f4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在，我们必须转到这个目录并创建环境(所有环境文件都将创建在一个名为 my_env 的目录中):</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="99a8" class="mx ky iq mt b gy my mz l na nb">cd testenv</span><span id="ca9a" class="mx ky iq mt b gy nc mz l na nb">python3 -m venv my_env</span></pre><p id="a6df" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们完成了检查使用 ls my_env 创建的环境文件</p><p id="69be" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要使用此环境，您需要激活它:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="20d1" class="mx ky iq mt b gy my mz l na nb">source my_env/bin/activate</span></pre><h1 id="b457" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">5.使用 PySpark</h1><h1 id="775c" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">5.1.配置</h1><p id="2b79" class="pw-post-body-paragraph jz ka iq kb b kc lx ke kf kg ly ki kj kk ml km kn ko mm kq kr ks mn ku kv kw ij bi translated">首先我们需要打开。bashrc 文件</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="87d3" class="mx ky iq mt b gy my mz l na nb">sudo gedit ~/.bashrc</span></pre><p id="2984" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">并添加以下几行:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="4c5e" class="mx ky iq mt b gy my mz l na nb">export PYTHONPATH=/usr/lib/python3.5</span><span id="0ee1" class="mx ky iq mt b gy nc mz l na nb">export PYSPARK_SUBMIT_ARGS=” -- master local[*] pyspark-shell”</span><span id="e1c4" class="mx ky iq mt b gy nc mz l na nb">export PYSPARK_PYTHON=/usr/bin/python3.5</span></pre><h1 id="79a1" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">5.2.FindSpark 库</h1><p id="b88c" class="pw-post-body-paragraph jz ka iq kb b kc lx ke kf kg ly ki kj kk ml km kn ko mm kq kr ks mn ku kv kw ij bi translated">如果我们在机器上安装了 Apache Spark，我们就不需要在开发环境中安装 pyspark 库。我们需要安装 findspark 库，它负责定位随 apache Spark 一起安装的 pyspark 库。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="7a0d" class="mx ky iq mt b gy my mz l na nb">pip3 install findspark</span></pre><p id="dde3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在每个 python 脚本文件中，我们必须添加以下行:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="161e" class="mx ky iq mt b gy my mz l na nb">import findspark</span><span id="12f9" class="mx ky iq mt b gy nc mz l na nb">findspark.init()</span></pre><h1 id="eea5" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">5.3.PySpark 示例</h1><h1 id="e105" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">5.3.1.阅读 HDFS 的作品</h1><p id="267e" class="pw-post-body-paragraph jz ka iq kb b kc lx ke kf kg ly ki kj kk ml km kn ko mm kq kr ks mn ku kv kw ij bi translated">下面的脚本将读取存储在 hdfs 中的文件</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="2981" class="mx ky iq mt b gy my mz l na nb">import findspark</span><span id="96a8" class="mx ky iq mt b gy nc mz l na nb">findspark.init()</span><span id="e57e" class="mx ky iq mt b gy nc mz l na nb">from pyspark.sql import SparkSession</span><span id="bbca" class="mx ky iq mt b gy nc mz l na nb">sparkSession = SparkSession.builder.appName(“example-pyspark-hdfs”).getOrCreate()</span><span id="c73a" class="mx ky iq mt b gy nc mz l na nb">df_load = sparkSession.read.csv(‘hdfs://localhost:9000/myfiles/myfilename’)</span><span id="2830" class="mx ky iq mt b gy nc mz l na nb">df_load.show()</span></pre><h1 id="836e" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">5.3.2.阅读阿帕奇·卡夫卡《消费者》</h1><p id="40bb" class="pw-post-body-paragraph jz ka iq kb b kc lx ke kf kg ly ki kj kk ml km kn ko mm kq kr ks mn ku kv kw ij bi translated">我们首先必须将 spark-streaming-Kafka-0–8-assembly _ 2.11–2 . 3 . 1 . jar 库添加到我们的 Apache spark jars 目录/opt/spark/jars 中。我们可以从 mvn 存储库下载:</p><p id="9ba1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">-<a class="ae jy" href="https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-8-assembly_2.11/2.3.1" rel="noopener ugc nofollow" target="_blank">https://mvn repository . com/artifact/org . Apache . spark/spark-streaming-Kafka-0-8-assembly _ 2.11/2 . 3 . 1</a></p><p id="3db8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">以下代码从 Kafka 主题消费者那里读取消息，并逐行打印出来:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="cb63" class="mx ky iq mt b gy my mz l na nb">import findspark</span><span id="f286" class="mx ky iq mt b gy nc mz l na nb">findspark.init()</span><span id="50dd" class="mx ky iq mt b gy nc mz l na nb">from kafka import KafkaConsumer</span><span id="4a63" class="mx ky iq mt b gy nc mz l na nb">from pyspark import SparkContext</span><span id="74d9" class="mx ky iq mt b gy nc mz l na nb">from pyspark.streaming import StreamingContext</span><span id="cf78" class="mx ky iq mt b gy nc mz l na nb">from pyspark.streaming.kafka import KafkaUtils</span><span id="c33d" class="mx ky iq mt b gy nc mz l na nb">KAFKA_TOPIC = ‘KafkaTopicName’</span><span id="998c" class="mx ky iq mt b gy nc mz l na nb">KAFKA_BROKERS = ‘localhost:9092’</span><span id="8593" class="mx ky iq mt b gy nc mz l na nb">ZOOKEEPER = ‘localhost:2181’</span><span id="abba" class="mx ky iq mt b gy nc mz l na nb">sc = SparkContext(‘local[*]’,’test’)</span><span id="cbaa" class="mx ky iq mt b gy nc mz l na nb">ssc = StreamingContext(sc, 60)</span><span id="1cb0" class="mx ky iq mt b gy nc mz l na nb">kafkaStream = KafkaUtils.createStream(ssc, ZOOKEEPER, ‘spark-streaming’, {KAFKA_TOPIC:1})</span><span id="6272" class="mx ky iq mt b gy nc mz l na nb">lines = kafkaStream.map(lambda x: x[1])</span><span id="1dea" class="mx ky iq mt b gy nc mz l na nb">lines.pprint()</span><span id="6acb" class="mx ky iq mt b gy nc mz l na nb">ssc.start()</span><span id="928a" class="mx ky iq mt b gy nc mz l na nb">ssc.awaitTermination()</span></pre><h1 id="6141" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">6.文献学</h1><p id="715a" class="pw-post-body-paragraph jz ka iq kb b kc lx ke kf kg ly ki kj kk ml km kn ko mm kq kr ks mn ku kv kw ij bi translated">[1] M. Litwintschik，“Hadoop 3 单节点安装指南”，2018 年 3 月 19 日。【在线】。可用:<a class="ae jy" href="http://tech.marksblogg.com/hadoop-3-single-node-install-guide.html." rel="noopener ugc nofollow" target="_blank">http://tech . marksblogg . com/Hadoop-3-single-node-install-guide . html</a>【2018 年 6 月 1 日访问】。</p><p id="de64" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">[2] L. Tagiaferri，“如何在 Ubuntu 16.04 上安装 Python 3 并设置本地编程环境”，2017 年 12 月 20 日。【在线】。可用:<a class="ae jy" href="https://www.digitalocean.com/community/tutorials/how-to-install-python-3-and-set-up-a-local-programming-environment-on-ubuntu-16-04." rel="noopener ugc nofollow" target="_blank">https://www . digital ocean . com/community/tutorials/how-to-install-python-3-and-set-up-a-local-programming-environment-on-Ubuntu-16-04。</a>【2018 年 08 月 01 日接入】。</p><p id="255c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">[3]《Apache Spark 官方文档》，[在线]。可用:<a class="ae jy" href="https://spark.apache.org/docs/latest/." rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/docs/latest/.</a>【2018 年 8 月 5 日访问】。</p><p id="49de" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">[4]《堆栈溢出问答》[在线]。可用:<a class="ae jy" href="https://stackoverflow.com/." rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/.</a>【2018 年 6 月 1 日获取】。</p><p id="da65" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">[5] A. GUPTA，“PySpark 中数据帧操作的完整指南”，2016 年 10 月 23 日。【在线】。可用:<a class="ae jy" href="https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/." rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2016/10/spark-data frame-and-operations/。</a>【2018 年 8 月 14 日进入】。</p></div></div>    
</body>
</html>