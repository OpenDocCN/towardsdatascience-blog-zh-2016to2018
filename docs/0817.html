<html>
<head>
<title>Exploring Activation Functions for Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索神经网络的激活函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-activation-functions-for-neural-networks-73498da59b02?source=collection_archive---------0-----------------------#2017-06-25">https://towardsdatascience.com/exploring-activation-functions-for-neural-networks-73498da59b02?source=collection_archive---------0-----------------------#2017-06-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="b057" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我想更多地关注我们在神经网络中使用的激活函数。为此，我将使用具有不同激活函数的简单全连接神经网络来解决<a class="ae kl" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> MNIST </a>问题。</p><p id="f646" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">MNIST数据是一组大约70000张手写数字的照片，每张照片的尺寸是28x28，并且是黑白的。这意味着我们的输入数据形状是(70000，784)，我们的输出是(70000，10)。</p><p id="6c9a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我将使用一个基本的全连接神经网络，只有一个隐藏层。它看起来像这样:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/839a8550f51efccdf375fdc745f24d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*jYhgQ4I_oFdxgDD-AOgV1w.png"/></div></figure><p id="2f1c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输入层有784个神经元，照片中的每个像素一个，隐藏层有512个神经元，输出层有10个神经元，每个数字一个。</p><p id="4dc0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<code class="fe ku kv kw kx b">keras</code>中，我们可以为每一层使用不同的激活功能。这意味着，在我们的情况下，我们必须决定在隐藏层和输出层使用什么激活函数，在这篇文章中，我将只在隐藏层进行实验，但它也应该与最终层相关。</p><p id="65e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有许多激活函数，我将只讨论基本的:Sigmoid，Tanh和Relu。</p><p id="10f0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，让我们尽量不要使用任何激活功能。你认为会发生什么？下面是代码(我跳过了数据加载部分，你可以在这个<a class="ae kl" href="https://github.com/shudima/notebooks/blob/master/Activation%20Functions.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>里找到完整的代码):</p><pre class="kn ko kp kq gt ky kx kz la aw lb bi"><span id="581b" class="lc ld iq kx b gy le lf l lg lh">model = Sequential()<br/>model.add(Dense(512, input_shape=(784,)))<br/>model.add(Dense(10, activation='softmax'))</span></pre><p id="103d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我说过，784个输入，512个在隐藏层，10个神经元在输出层。在培训之前，我们可以使用<code class="fe ku kv kw kx b">model.summary and model.layers</code>查看网络架构和参数:</p><pre class="kn ko kp kq gt ky kx kz la aw lb bi"><span id="25ff" class="lc ld iq kx b gy le lf l lg lh">Layers (input ==&gt; output)<br/>--------------------------<br/>dense_1 (None, 784) ==&gt; (None, 512)<br/>dense_2 (None, 512) ==&gt; (None, 10)</span><span id="e81c" class="lc ld iq kx b gy li lf l lg lh">Summary<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>dense_1 (Dense)              (None, 512)               401920    <br/>_________________________________________________________________<br/>output (Dense)              (None, 10)                5130      <br/>=================================================================<br/>Total params: 407,050<br/>Trainable params: 407,050<br/>Non-trainable params: 0<br/>_________________________________________________________________<br/>None</span></pre><p id="bcee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">好了，现在我们确定了网络的架构，让我们针对5个时期进行培训:</p><pre class="kn ko kp kq gt ky kx kz la aw lb bi"><span id="349c" class="lc ld iq kx b gy le lf l lg lh">Train on 60000 samples, validate on 10000 samples<br/>Epoch 1/5<br/>60000/60000 [==============================] - 3s - loss: 0.3813 - acc: 0.8901 - val_loss: 0.2985 - val_acc: 0.9178<br/>Epoch 2/5<br/>60000/60000 [==============================] - 3s - loss: 0.3100 - acc: 0.9132 - val_loss: 0.2977 - val_acc: 0.9196<br/>Epoch 3/5<br/>60000/60000 [==============================] - 3s - loss: 0.2965 - acc: 0.9172 - val_loss: 0.2955 - val_acc: 0.9186<br/>Epoch 4/5<br/>60000/60000 [==============================] - 3s - loss: 0.2873 - acc: 0.9209 - val_loss: 0.2857 - val_acc: 0.9245<br/>Epoch 5/5<br/>60000/60000 [==============================] - 3s - loss: 0.2829 - acc: 0.9214 - val_loss: 0.2982 - val_acc: 0.9185</span><span id="b5e7" class="lc ld iq kx b gy li lf l lg lh">Test loss:, 0.299<br/><strong class="kx ir">Test accuracy: 0.918</strong></span></pre><p id="36ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们得到的结果不是很好，在MNIST数据集上91.8%的准确率相当糟糕。当然，你可以说我们需要比5个时期多得多的时期，但是让我们画出损失:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lj"><img src="../Images/e95d904c2292583f93bc204af3158ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j5Y04bszChAUN6ftpQUuaQ.png"/></div></div></figure><p id="1525" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以看到验证损失没有改善，我可以向你保证，即使经过100个时代也不会改善。我们可以尝试不同的技术来防止过度适应，或者让我们的网络更大更智能，以便更好地学习和提高，但让我们只是尝试使用<code class="fe ku kv kw kx b">sigmoid</code>激活功能。</p><p id="2b26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Sigmoid函数看起来像这样:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/a191db5a2938f63f3ccc755d4582a7f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*Noj6FwSMKHV9R_wSbnTKvA.png"/></div></figure><p id="9697" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它将输出压缩为(0，1)区间，并且是非线性的。让我们在网络中使用它:</p><pre class="kn ko kp kq gt ky kx kz la aw lb bi"><span id="cd69" class="lc ld iq kx b gy le lf l lg lh">model = Sequential()<br/>model.add(Dense(512, activation='sigmoid', input_shape=(784,)))<br/>model.add(Dense(10, activation='softmax'))</span></pre><p id="2434" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以看到架构是完全一样的，我们只改变了<code class="fe ku kv kw kx b">Dense</code>层的激活功能。让我们再训练5个纪元:</p><pre class="kn ko kp kq gt ky kx kz la aw lb bi"><span id="e14c" class="lc ld iq kx b gy le lf l lg lh">Train on 60000 samples, validate on 10000 samples<br/>Epoch 1/5<br/>60000/60000 [==============================] - 3s - loss: 0.4224 - acc: 0.8864 - val_loss: 0.2617 - val_acc: 0.9237<br/>Epoch 2/5<br/>60000/60000 [==============================] - 3s - loss: 0.2359 - acc: 0.9310 - val_loss: 0.1989 - val_acc: 0.9409<br/>Epoch 3/5<br/>60000/60000 [==============================] - 3s - loss: 0.1785 - acc: 0.9477 - val_loss: 0.1501 - val_acc: 0.9550<br/>Epoch 4/5<br/>60000/60000 [==============================] - 3s - loss: 0.1379 - acc: 0.9598 - val_loss: 0.1272 - val_acc: 0.9629<br/>Epoch 5/5<br/>60000/60000 [==============================] - 3s - loss: 0.1116 - acc: 0.9673 - val_loss: 0.1131 - val_acc: 0.9668<br/><br/>Test loss: 0.113<br/><strong class="kx ir">Test accuracy: 0.967</strong></span></pre><p id="d1c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那好多了。为了理解为什么，让我们回忆一下我们的神经元是什么样子的:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/f789fb2e395a3a46f5168b10dd5a8de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*lqqZ5v486yJfnP_7MQatAA.png"/></div></figure><p id="5f35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<code class="fe ku kv kw kx b">x</code>是输入，<code class="fe ku kv kw kx b">w</code>是权重，<code class="fe ku kv kw kx b">b</code>是偏差。可以看到，这只是输入与权重和偏差的线性组合。即使在叠加了很多之后，我们仍然可以把它表示成一个线性方程。这意味着，它类似于一个完全没有隐藏层的网络，对于任何数量的隐藏层都是如此。！).我们将添加一些层到我们的第一个网络，看看会发生什么。看起来是这样的:</p><pre class="kn ko kp kq gt ky kx kz la aw lb bi"><span id="2e34" class="lc ld iq kx b gy le lf l lg lh">model = Sequential()<br/>model.add(Dense(512, input_shape=(784,)))</span><span id="1b7f" class="lc ld iq kx b gy li lf l lg lh">for i in range(5):<br/>    model.add(Dense(512))</span><span id="4c82" class="lc ld iq kx b gy li lf l lg lh">model.add(Dense(10, activation='softmax'))</span></pre><p id="2408" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是网络的样子:</p><pre class="kn ko kp kq gt ky kx kz la aw lb bi"><span id="44fd" class="lc ld iq kx b gy le lf l lg lh">dense_1 (None, 784) ==&gt; (None, 512)<br/>dense_2 (None, 512) ==&gt; (None, 512)<br/>dense_3 (None, 512) ==&gt; (None, 512)<br/>dense_4 (None, 512) ==&gt; (None, 512)<br/>dense_5 (None, 512) ==&gt; (None, 512)<br/>dense_6 (None, 512) ==&gt; (None, 10)<br/><br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>dense_1 (Dense)             (None, 512)               401920    <br/>_________________________________________________________________<br/>dense_2 (Dense)             (None, 512)               262656    <br/>_________________________________________________________________<br/>dense_3 (Dense)             (None, 512)               262656    <br/>_________________________________________________________________<br/>dense_4 (Dense)             (None, 512)               262656    <br/>_________________________________________________________________<br/>dense_5 (Dense)             (None, 512)               262656    <br/>_________________________________________________________________<br/>dense_16 (Dense)             (None, 10)                5130      <br/>=================================================================<br/>Total params: 1,720,330<br/>Trainable params: 1,720,330<br/>Non-trainable params: 0<br/>_________________________________________________________________<br/>None</span></pre><p id="eb0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些是训练5个时期后的结果:</p><pre class="kn ko kp kq gt ky kx kz la aw lb bi"><span id="748d" class="lc ld iq kx b gy le lf l lg lh">Train on 60000 samples, validate on 10000 samples<br/>Epoch 1/5<br/>60000/60000 [==============================] - 17s - loss: 1.3217 - acc: 0.7310 - val_loss: 0.7553 - val_acc: 0.7928<br/>Epoch 2/5<br/>60000/60000 [==============================] - 16s - loss: 0.5304 - acc: 0.8425 - val_loss: 0.4121 - val_acc: 0.8787<br/>Epoch 3/5<br/>60000/60000 [==============================] - 15s - loss: 0.4325 - acc: 0.8724 - val_loss: 0.3683 - val_acc: 0.9005<br/>Epoch 4/5<br/>60000/60000 [==============================] - 16s - loss: 0.3936 - acc: 0.8852 - val_loss: 0.3638 - val_acc: 0.8953<br/>Epoch 5/5<br/>60000/60000 [==============================] - 16s - loss: 0.3712 - acc: 0.8945 - val_loss: 0.4163 - val_acc: 0.8767<br/><br/>Test loss: 0.416<br/><strong class="kx ir">Test accuracy: 0.877</strong></span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lq"><img src="../Images/ddf9138e16d23bab08fa3c6c29975ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8b_sQGBowcgxZWwAvsipbA.png"/></div></div></figure><p id="011a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这很糟糕。我们可以看到网络无法学习我们想要的东西。这是因为没有非线性，我们的网络只是一个线性分类器，不能获得非线性关系。</p><p id="a055" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一方面，<code class="fe ku kv kw kx b">sigmoid</code>是一个非线性函数，我们不能将其表示为我们输入的线性组合。这给我们的网络带来了非线性，使它能够学习非线性关系。让我们再次尝试训练5个隐藏层网络，这次使用<code class="fe ku kv kw kx b">sigmoid</code>激活:</p><pre class="kn ko kp kq gt ky kx kz la aw lb bi"><span id="5b4b" class="lc ld iq kx b gy le lf l lg lh">Train on 60000 samples, validate on 10000 samples<br/>Epoch 1/5<br/>60000/60000 [==============================] - 16s - loss: 0.8012 - acc: 0.7228 - val_loss: 0.3798 - val_acc: 0.8949<br/>Epoch 2/5<br/>60000/60000 [==============================] - 15s - loss: 0.3078 - acc: 0.9131 - val_loss: 0.2642 - val_acc: 0.9264<br/>Epoch 3/5<br/>60000/60000 [==============================] - 15s - loss: 0.2031 - acc: 0.9419 - val_loss: 0.2095 - val_acc: 0.9408<br/>Epoch 4/5<br/>60000/60000 [==============================] - 15s - loss: 0.1545 - acc: 0.9544 - val_loss: 0.2434 - val_acc: 0.9282<br/>Epoch 5/5<br/>60000/60000 [==============================] - 15s - loss: 0.1236 - acc: 0.9633 - val_loss: 0.1504 - val_acc: 0.9548<br/><br/>Test loss: 0.15<br/><strong class="kx ir">Test accuracy: 0.955</strong></span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lr"><img src="../Images/e8a18fd5013550118a22bde21a24ecb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fDgApDPMHswjjE6iLG0ftg.png"/></div></div></figure><p id="d4f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还是那句话，好多了。我们可能过拟合，但我们得到了一个显着的性能提升只是通过使用激活函数。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="0c88" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Sigmoid很棒，它有许多积极的属性，如非线性、<a class="ae kl" href="https://en.wikipedia.org/wiki/Differentiable_function" rel="noopener ugc nofollow" target="_blank">可微性</a>和(0，1)范围为我们提供了返回值的概率，这很好，但它也有缺点。当我们使用反向传播时，我们必须将输出的导数反向传播回我们的第一个权重，换句话说，我们希望将最终输出值中的分类/回归误差传递回整个网络。这意味着我们应该派生我们的层并更新权重。sigmoid的问题是，它的导数是这样的:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lz"><img src="../Images/505eca49c4366d8be139e91727ea4173.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iOAQr-iRgHoVpFNSWYV8Iw.png"/></div></div></figure><p id="7a52" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以看到导数的最大值非常小(0.25)，这意味着我们只会将一小部分误差传递给前面的层。这可能会导致我们的网络学习缓慢(我所说的缓慢是指我们需要更多的数据或时期，而不是计算时间)。</p><p id="c4d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了解决这个问题，我们可以使用<code class="fe ku kv kw kx b">Tanh</code>函数，如下所示:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/bc4bbfb4718d0ee4021e0dc6128e9e9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*TLcyVwbgfrNm3XCWW4rFzg.png"/></div></figure><p id="c981" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe ku kv kw kx b">tanh</code>函数也是非线性和可微的。它的输出在(-1，1)范围内，没有(0，1)范围好，但是对于隐藏层来说还是可以的。最后，它的最大导数是一个很好的值，因为现在我们可以更好地传递误差。</p><p id="378a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要使用<code class="fe ku kv kw kx b">tanh</code>激活功能，我们只需改变<code class="fe ku kv kw kx b">Dense</code>层的<code class="fe ku kv kw kx b">activation</code>属性:</p><pre class="kn ko kp kq gt ky kx kz la aw lb bi"><span id="709e" class="lc ld iq kx b gy le lf l lg lh">model = Sequential()<br/>model.add(Dense(512, activation=’tanh’, input_shape=(784,)))<br/>model.add(Dense(10, activation=’softmax’))</span></pre><p id="6d16" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还是那句话，网络架构是一样的，只是激活不同。让我们训练5个纪元:</p><pre class="kn ko kp kq gt ky kx kz la aw lb bi"><span id="ae6a" class="lc ld iq kx b gy le lf l lg lh">Train on 60000 samples, validate on 10000 samples<br/>Epoch 1/5<br/>60000/60000 [==============================] - 5s - loss: 0.3333 - acc: 0.9006 - val_loss: 0.2106 - val_acc: 0.9383<br/>Epoch 2/5<br/>60000/60000 [==============================] - 3s - loss: 0.1754 - acc: 0.9489 - val_loss: 0.1485 - val_acc: 0.9567<br/>Epoch 3/5<br/>60000/60000 [==============================] - 3s - loss: 0.1165 - acc: 0.9657 - val_loss: 0.1082 - val_acc: 0.9670<br/>Epoch 4/5<br/>60000/60000 [==============================] - 3s - loss: 0.0843 - acc: 0.9750 - val_loss: 0.0920 - val_acc: 0.9717<br/>Epoch 5/5<br/>60000/60000 [==============================] - 3s - loss: 0.0653 - acc: 0.9806 - val_loss: 0.0730 - val_acc: 0.9782<br/><br/>Test loss: 0.073<br/><strong class="kx ir">Test accuracy: 0.978</strong></span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mb"><img src="../Images/d6ebe8951ecb1c3eec6b52fb9e85cb44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pk3__-goc0MitOSDARW51g.png"/></div></div></figure><p id="9cf5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">太好了！仅通过使用不同的激活函数，我们就将测试精度提高了1%以上。</p><p id="8f2e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们能做得更好吗？似乎在大多数情况下，我们可以使用relu激活功能。Relu长这样:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/27c21d355b737d9f88ab506da2f6c5c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*hBlI5xsJnSpfQY2i_-iHzQ.png"/></div></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi md"><img src="../Images/4f112be5cb35b0286b6ae7d4e58c9660.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*yDTcsgBV3pcUvXFVwE8kZQ.png"/></div></figure><p id="e14f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个激活函数的范围是(0，inf)，在零点不可微(这个有解)。关于<code class="fe ku kv kw kx b">relu</code>最好的事情是它的梯度是<strong class="jp ir">总是</strong>等于<code class="fe ku kv kw kx b">1</code>，这样我们可以在反向传播过程中通过网络传递最大数量的误差。</p><p id="b826" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们训练一下，看看结果:</p><pre class="kn ko kp kq gt ky kx kz la aw lb bi"><span id="b764" class="lc ld iq kx b gy le lf l lg lh">Train on 60000 samples, validate on 10000 samples<br/>Epoch 1/5<br/>60000/60000 [==============================] - 5s - loss: 0.2553 - acc: 0.9263 - val_loss: 0.1505 - val_acc: 0.9516<br/>Epoch 2/5<br/>60000/60000 [==============================] - 3s - loss: 0.1041 - acc: 0.9693 - val_loss: 0.0920 - val_acc: 0.9719<br/>Epoch 3/5<br/>60000/60000 [==============================] - 3s - loss: 0.0690 - acc: 0.9790 - val_loss: 0.0833 - val_acc: 0.9744<br/>Epoch 4/5<br/>60000/60000 [==============================] - 4s - loss: 0.0493 - acc: 0.9844 - val_loss: 0.0715 - val_acc: 0.9781<br/>Epoch 5/5<br/>60000/60000 [==============================] - 3s - loss: 0.0376 - acc: 0.9885 - val_loss: 0.0645 - val_acc: 0.9823<br/><br/>Test loss: 0.064<br/><strong class="kx ir">Test accuracy: 0.982</strong></span></pre><p id="e8ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">到目前为止，我们取得了最好的成绩。98.2%这是一个不错的结果，我们只使用了一个隐藏层。</p><p id="07ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">需要注意的是，这里没有<strong class="jp ir">最好的</strong>激活功能。在许多情况下，一个可能比另一个更好，但在另一些情况下会更差。</p><p id="e6d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一个重要的注意事项是，使用不同的激活函数不影响我们的网络可以学习什么，只影响多快(它需要多少数据/时期)。这是我们尝试过的所有激活函数的曲线图，但这次是在一个更长的训练周期内。你可以看到所有的激活函数最终都达到了98%的准确率。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi me"><img src="../Images/89a0ae30c96c50c3ad88e4063a851017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odnLF9WKqB5BtJhUdQyO_Q.png"/></div></div></figure><p id="ae12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">希望你喜欢这篇文章。你可以在这里找到代码<a class="ae kl" href="https://github.com/shudima/notebooks/blob/master/Activation%20Functions.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>