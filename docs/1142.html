<html>
<head>
<title>Using scikit-learn to find bullies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 sci kit-学会发现恶霸</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-scikit-learn-to-find-bullies-c47a1045d92f?source=collection_archive---------14-----------------------#2017-08-02">https://towardsdatascience.com/using-scikit-learn-to-find-bullies-c47a1045d92f?source=collection_archive---------14-----------------------#2017-08-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="cb2c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">到目前为止，我所做的大部分工作<a class="ae kl" href="https://medium.com/@gabrieltseng/latest" rel="noopener"/>都是关于神经网络的。然而，这些有一个明显的缺点:它们需要大量的数据。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/da9213e3ee2cc22249e3cb3dea660ff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oMowgSbkMWhjYz2k50G9vA.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">The loss of my Quora NLP neural network. The fact that validation loss is much greater than training loss is indicative of overfitting, and this happens quite early in the training.</figcaption></figure><p id="7a9f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我训练一个自然语言处理神经网络来<a class="ae kl" href="https://medium.com/towards-data-science/natural-language-processing-with-quora-9737b40700c8" rel="noopener">识别相似的 Quora 问题</a>时，这一点非常清楚。即使有 300，000 个数据点可用于训练网络，它仍然没有花很长时间就开始过度拟合。</p><p id="2e7b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本文中，我将使用一个小得多的数据集来练习分类。我将使用各种工具来完成这项工作，包括<a class="ae kl" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"> SpaCy </a>(一种自然语言处理工具)，以及不同的<a class="ae kl" href="http://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>算法。</p><p id="8059" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个过程中，我将探索我所使用的算法到底是做什么的。</p><p id="8376" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">内容:</p><ol class=""><li id="6afb" class="lc ld iq jp b jq jr ju jv jy le kc lf kg lg kk lh li lj lk bi translated">问题定义和设置</li><li id="0b7c" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated">用 scikit-learn 算法求解<br/> a. <a class="ae kl" href="#1afe" rel="noopener ugc nofollow">逻辑回归</a> <br/> b. <a class="ae kl" href="#ba80" rel="noopener ugc nofollow">随机森林</a> <br/> c. <a class="ae kl" href="#27f5" rel="noopener ugc nofollow">支持向量机</a></li><li id="9aab" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated">结论</li></ol></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h1 id="8ab4" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">问题定义和设置</h1><p id="9918" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated"><a class="ae kl" href="https://github.com/GabrielTseng/LearningDataScience/blob/master/Natural_Language_Processing/Detecting_Bullies/1-Making_GloVe_vectors.ipynb" rel="noopener ugc nofollow" target="_blank">链接到代码</a></p><p id="7681" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了找到一个小数据集来玩，我找到了一个很久以前的 Kaggle 比赛:<a class="ae kl" href="https://www.kaggle.com/c/detecting-insults-in-social-commentary" rel="noopener ugc nofollow" target="_blank">检测社会评论中的侮辱</a>。训练集有 3000 个数据点，比我上次解决的自然语言处理挑战小 100 倍。</p><p id="dab5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">挑战:</strong>确定一条评论是否会被认为是对对话中另一位参与者的侮辱。</p><p id="561d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些评论来自评论网站、留言板等，以 csv 文件的形式提供。</p><p id="1bde" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是一个非侮辱性评论的例子:</p><pre class="kn ko kp kq gt na nb nc nd aw ne bi"><span id="15b0" class="nf ly iq nb b gy ng nh l ni nj">"Celebrity Big Brother IS a mistake."</span></pre><p id="2950" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">和一个侮辱性的评论:</p><pre class="kn ko kp kq gt na nb nc nd aw ne bi"><span id="5380" class="nf ly iq nb b gy ng nh l ni nj">"You are stuck on stupid obviously...give me a break and don\'t vote.\\nmoron"</span></pre><h2 id="7d37" class="nf ly iq bd lz nk nl dn md nm nn dp mh jy no np ml kc nq nr mp kg ns nt mt nu bi translated">清理数据</h2><p id="e6f6" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">正如上面的例子所示，清理句子是必要的。我将使用单词嵌入来量化单词，所以我想在句子中分离出单词，没有任何诸如换行符或撇号之类的无用信息。</p><p id="8c21" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了做到这一点，我把注释中的换行符都去掉了(<code class="fe nv nw nx nb b">'\\n'</code>在数据中特别常见，所以我专门做了一点去掉)，只保留字母。</p><p id="0f25" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这样做将上面的注释更改为:</p><pre class="kn ko kp kq gt na nb nc nd aw ne bi"><span id="e632" class="nf ly iq nb b gy ng nh l ni nj">You are stuck on stupid obviously give me a break and don t vote  moron</span></pre><h2 id="91ff" class="nf ly iq bd lz nk nl dn md nm nn dp mh jy no np ml kc nq nr mp kg ns nt mt nu bi translated">符号化</h2><p id="a498" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated"><a class="ae kl" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"> SpaCy </a>非常擅长文字处理，尤其是非常擅长将数据符号化。这意味着它善于识别不同单词在句子中的位置(这很有用，因为它们不总是由空格分隔)。</p><p id="82ae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">SpaCy 的另一个非常酷的部分是，它根据<a class="ae kl" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套词嵌入</a>(我在这里探索<a class="ae kl" href="https://medium.com/towards-data-science/clustering-and-collaborative-filtering-using-word-embeddings-56ee60f0575d" rel="noopener"/>)自动为这些令牌分配 300 维向量。因此，对我来说，获取一个句子，并从中生成一个嵌入矩阵非常容易。</p><p id="55e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，使用嵌入从句子创建矩阵的直观方法不适用于小数据集。如果我在训练一个神经网络，我的方法只是将所有嵌入向量附加在一起:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ny"><img src="../Images/ccbbacdbac87f1d4db0cf187346aa6fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1G8HpQ7bfUXLkMsocshLdg.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">The intuitive approach to generating a matrix using word embeddings. Each word has its own 300 dimensional embedding. They are all appended together to create a ‘sentence matrix’.</figcaption></figure><p id="0bf9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这样做的问题是，随着句子长度的增加，特征的数量(即矩阵的大小)会激增。该数据集中的平均句子长度为<code class="fe nv nw nx nb b">33</code>个单词；这将产生一个大小为<code class="fe nv nw nx nb b">(33, 300)</code>的矩阵，有 9900 个元素。</p><p id="fc50" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">鉴于我的 3000 训练点，这是乞求超负荷。</p><p id="7dd4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我的解决方案是找到矩阵中每个元素的平均值，因此我最终得到了每个句子的 300 维“平均值”向量:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nz"><img src="../Images/40ba4cd090fd6b03415bb73787a1fd48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O-P9vQSnNtkT_PbULgiS1g.png"/></div></div></figure><p id="6e0a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这意味着每个输入将有 300 个特征；这要合理得多。</p><h1 id="1afe" class="lx ly iq bd lz ma oa mc md me ob mg mh mi oc mk ml mm od mo mp mq oe ms mt mu bi translated">使用 scikit 求解-学习</h1><p id="cc44" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated"><a class="ae kl" href="https://github.com/GabrielTseng/LearningDataScience/blob/master/Natural_Language_Processing/Detecting_Bullies/3-Scikit_learn_algorithms.ipynb" rel="noopener ugc nofollow" target="_blank">链接至代码</a></p><p id="c867" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本次竞赛的指标是<a class="ae kl" href="https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it" rel="noopener ugc nofollow" target="_blank"> AUC ROC </a>。粗略地说，AUC ROC 分数可以解释为积极分类项目实际上是积极的可能性有多大，以及消极分类项目是消极的可能性有多大。</p><p id="d49e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这比简单的准确性要好，因为它也考虑了数据集中的偏差。例如，如果我的评论中有 95%是侮辱性的，那么一个将每个评论都归类为侮辱性的分类器将有 95%的准确率，但它将是一个无用的分类器。AUC ROC 避免了这一点。</p><p id="b320" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑到这一点，现在让我们分别研究每一种算法。</p><h2 id="ae07" class="nf ly iq bd lz nk nl dn md nm nn dp mh jy no np ml kc nq nr mp kg ns nt mt nu bi translated">逻辑回归</h2><p id="136b" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated"><strong class="jp ir">它是如何工作的？</strong>该分类器将为我的输入特征中的每个变量定义系数(因此，因为我的输入是 300 维变量，所以定义了 300 个系数)。这就定义了一个函数:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi of"><img src="../Images/0000622420b3598d270306a57fc9c4dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*YpfIulHQ3kJvtDMVM_qBPw.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">This relationship does not have to be linear; different x terms could be multiplied together, for instance. However, <a class="ae kl" href="https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf" rel="noopener ugc nofollow" target="_blank">LIBLINEAR</a>, the library used by scikit-learn, does assume a linear relationship.</figcaption></figure><p id="a0a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后训练这些系数，使得如果输入特征是侮辱，则输出为 1，否则输出为 0。通常使用 sigmoid 函数来简化最后一步:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi og"><img src="../Images/8330e36d91ecccce7065235d09fd03be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*Al5bJBDZiFXMmx4b4E_xFw.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">This function is called the sigmoid function, and restricts the output to between 1 and 0, which is ideal for classification, where something either is an insult (1) or is not (0). Particularly, it allows f(X) to get very large or small if it is confident in a classification, as the function is <a class="ae kl" href="https://www.wikiwand.com/en/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">asymptotic.</a></figcaption></figure><p id="4226" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它做得怎么样？微调这个模型非常容易，因为它只有一个真正可以改变的参数:C，或者正则化强度的倒数(基本上，C 控制系数可以变化的程度。这防止了数据的过度拟合)。</p><p id="904f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">逻辑回归得出以下训练曲线:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/e5225ef1614ca574ea856c5647d66418.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*PN260aOmiXsQZL8j9DcoIA.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">These plots show the success of the models on a training set and cross validation set (the cross validation set was made with 20% of the data) as the training set grew from just 100 samples to all the training data available (to 80% of the data). The size of the cross validation set remains fixed. Making these plots of model success against training set size is useful to uncover the performance of the model, and <a class="ae kl" href="https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted" rel="noopener ugc nofollow" target="_blank">to identify under/overfitting</a>.</figcaption></figure><p id="d0f8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种算法的简单性解释了为什么它训练起来如此之快。它的成功表明，取句子中单词向量的平均值是捕捉句子中情感的高效方式，因为它线性地映射到一个句子是否令人不快。</p><h2 id="ba80" class="nf ly iq bd lz nk nl dn md nm nn dp mh jy no np ml kc nq nr mp kg ns nt mt nu bi translated">随机森林</h2><p id="cb22" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated"><strong class="jp ir">它是如何工作的？为了理解随机森林，理解决策树是有帮助的。<a class="ae kl" href="http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>在这方面做得很好，我不会试图重复，但非常广泛:给定一些数据集(即下面的一些工作机会)，信息被用来将数据分成越来越小的子集，直到它被分类为“接受”或“拒绝”(在我的例子中，或者是“侮辱”或“不侮辱”)。更好的数据被用在树的更高层——所以在下面的例子中，薪水将是关于一份工作是否应该被接受或拒绝的最具指示性的指标。</strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/5c6d11d8ff6e9b3156926b045eb2b4ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*xzF10JmR3K0rnZ8jtIHI_g.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Credit: <a class="ae kl" href="http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/" rel="noopener ugc nofollow" target="_blank">http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/</a></figcaption></figure><p id="86d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随机森林(直观地)通过将数据随机分成(许多)子集来使用许多决策树。然后，在组合结果以生成结论之前，它对每个子集训练决策树。</p><p id="accc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">怎么办到的？训练随机森林时要调整的重要参数是决策树的数量(大约 100 是一个好的起点)和每个数据子集中的特征数量(这里，建议使用特征数量的平方根-大约 17，300 个特征)。</p><p id="4dec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这对我来说证明是最成功的，产生了下面的训练曲线:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi oj"><img src="../Images/6bee439c054537bbc2885464d0da619f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qRFEROuJwX2nj5AiIMkJuA.png"/></div></div></figure><p id="0f16" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里发生了一些严重的过度拟合。训练集的 AUC-ROC 得分接近 1，而交叉验证集的最终得分为 0.87。</p><p id="0f13" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当考虑随机森林如何工作时，这确实是有意义的；绝大多数决策树将会是垃圾(并且将会过拟合)。然而，它们应该互相抵消，留下几棵树，这些树可以归纳到交叉验证集(然后是未来的数据)。</p><h2 id="27f5" class="nf ly iq bd lz nk nl dn md nm nn dp mh jy no np ml kc nq nr mp kg ns nt mt nu bi translated">支持向量机</h2><p id="1eec" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated"><strong class="jp ir">它是如何工作的？</strong>逻辑回归使用线性系数来定义函数，然后使用该函数来预测数据的类别。</p><p id="73a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">支持向量机不是线性系数，而是将数据的每个维度的距离与某个标志进行比较。这定义了一个新的特征空间，然后可以用它来预测数据的类别。</p><p id="0031" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这有点令人困惑，所以让我们考虑一维数据(想象一下，如果不是长度为 300 的输入向量，而是长度为 1):</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ok"><img src="../Images/f4d0396fa34467d9fd1f6b13b3c153ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X6DiCvdg5h2k4WkMNa85DQ.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">The sigma defines L1’s ‘circle of influence’. If it is larger, then F1 will be big even if D is large (if X1 and L1 are further apart). If it is smaller, then F1 will vanish if D gets large.</figcaption></figure><p id="cdcc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我的特征 X1 现在已经转换为一个特征，然后我可以在一个方程中使用它，就像逻辑回归一样:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi gj"><img src="../Images/67eb76d413beb6c09614900012d2d538.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YKJaV8Oq6UIvZbOBTm6Urw.png"/></div></div></figure><p id="96ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这使我能够以比简单的线性逻辑回归更复杂的方式分离数据，从而获得更好的性能。</p><p id="4a13" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它是怎么做的？与逻辑回归一样，最重要的训练超参数是正则化常数 c。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ol"><img src="../Images/f38c1e6fddc149432139f447e8161bf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dfDhB6GIkMnMEinycrJ2XA.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Note: SVC stand for support vector classification</figcaption></figure><p id="546f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最终 AUC ROC 为 0.89，是迄今为止最好的分类器。然而，因为它需要定义一个全新的特征空间来进行分类，所以它也比其他算法慢得多，在这个数据集上训练需要 14.8 秒(相比之下，逻辑回归需要 421 毫秒)。</p><h1 id="9460" class="lx ly iq bd lz ma oa mc md me ob mg mh mi oc mk ml mm od mo mp mq oe ms mt mu bi translated"><strong class="ak">算法对比</strong></h1><p id="f774" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">这些都是怎么做的？</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi om"><img src="../Images/8db82a687190709a29e917769b53374c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JUeC9-Du0bLsuX756ZUc4w.png"/></div></div></figure><p id="4cc1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">SVM 是最好的，但是定义新的特征空间导致显著的计算成本，并且它比逻辑回归花费 35 倍的时间来训练(对于 0.01 AUC ROC 的相对小的改进)。</p><h1 id="2bb8" class="lx ly iq bd lz ma oa mc md me ob mg mh mi oc mk ml mm od mo mp mq oe ms mt mu bi translated">结论</h1><p id="b769" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">从这里可以得到一些启示:</p><ol class=""><li id="27ba" class="lc ld iq jp b jq jr ju jv jy le kc lf kg lg kk lh li lj lk bi translated">我一直惊讶于单词嵌入有多棒，尤其是考虑到<a class="ae kl" href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/" rel="noopener ugc nofollow" target="_blank">它们是如何产生的</a>。2012 年在 Kaggle 上发起这个比赛的时候(单词嵌入之前)，获胜分数是 0.84249。单词嵌入让我不费吹灰之力就把这个分数提高到了 0.89。</li><li id="9c1a" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated">除了准确性，还有其他指标需要考虑。SVM 和逻辑回归之间的训练时间差异的重要性给我留下了深刻的印象，尽管 SVM 最终更好，但它可能不是最好的算法。</li><li id="adf8" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated">所有这些算法都被设计用来处理扔给它们的任何东西，因此相当健壮。考虑到这些算法处理问题的不同，它们的表现非常相似；AUC ROC 评分最大差距只有 0.019——ka ggle 比赛第 1 名和第 10 名的差距。</li></ol></div></div>    
</body>
</html>