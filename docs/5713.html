<html>
<head>
<title>Recurrent Neural Networks by Example in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的递归神经网络实例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470?source=collection_archive---------0-----------------------#2018-11-05">https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470?source=collection_archive---------0-----------------------#2018-11-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/95c304363ff3ad0283d796d47e498ddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oT5oL9eO5BF9qKcerVD2qg.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">(<a class="ae jd" href="https://www.pexels.com/photo/agriculture-alternative-energy-clouds-countryside-414837/" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><div class=""/><div class=""><h2 id="f377" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">利用递归神经网络撰写专利摘要</h2></div><p id="6e40" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当我第一次尝试研究递归神经网络时，我犯了一个错误，那就是试图先学习 LSTMs 和 GRUs 之类的东西背后的理论。在看了几天令人沮丧的线性代数方程后，我在用 Python 进行深度学习的<a class="ae jd" href="https://www.manning.com/books/deep-learning-with-python" rel="noopener ugc nofollow" target="_blank"><em class="lr"/></a><em class="lr"/>中看到了下面这段话</p><blockquote class="ls lt lu"><p id="8981" class="kv kw lr kx b ky kz kh la lb lc kk ld lv lf lg lh lw lj lk ll lx ln lo lp lq ij bi translated">总之，你不需要了解 LSTM 细胞的具体架构的一切；作为一个人类，理解它不应该是你的工作。请记住 LSTM 细胞的作用:允许过去的信息在以后被重新注入。</p></blockquote><p id="b140" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是库喀拉斯(Francois Chollet)的作者，深度学习方面的专家，他告诉我，我不需要理解基础水平的所有东西！我意识到我的错误是从理论的底层开始，而不是试图建立一个递归神经网络。</p><blockquote class="ls lt lu"><p id="497c" class="kv kw lr kx b ky kz kh la lb lc kk ld lv lf lg lh lw lj lk ll lx ln lo lp lq ij bi translated">此后不久，我改变了策略，决定尝试学习数据科学技术的最有效方法:找到问题并解决它！</p></blockquote><p id="903a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种<a class="ae jd" href="https://course.fast.ai/about.html" rel="noopener ugc nofollow" target="_blank">自上而下的方法</a>是指在  <em class="lr">之前学习<em class="lr">如何</em> <strong class="kx jh"> <em class="lr">实现</em> </strong> <em class="lr">一种方法</em> <strong class="kx jh"> <em class="lr">回溯并覆盖</em> <strong class="kx jh"> <em class="lr">理论</em> </strong>。通过这种方式，我能够在前进的道路上找到我需要知道的东西，当我回来研究这些概念时，我有了一个框架，我可以将每个想法放入其中。在这种心态下，我决定不再担心细节，完成一个递归神经网络项目。</strong></em></p><p id="956a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本文介绍了如何在 Keras 中构建和使用递归神经网络来撰写专利摘要。这篇文章没有什么理论，但是当你完成这个项目的时候，你会发现你在这个过程中得到了你需要知道的东西。最终结果是，你可以构建一个有用的应用程序，并弄清楚自然语言处理的深度学习方法是如何工作的。</p><p id="a471" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">完整的代码可以从 GitHub 上的一系列<a class="ae jd" href="https://github.com/WillKoehrsen/recurrent-neural-networks" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本中获得。我还提供了所有的</a><a class="ae jd" href="https://github.com/WillKoehrsen/recurrent-neural-networks/tree/master/models" rel="noopener ugc nofollow" target="_blank">预训练模型</a>，这样你就不用自己花几个小时训练它们了！为了尽可能快地开始并研究模型，参见<a class="ae jd" href="https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Quick%20Start%20to%20Recurrent%20Neural%20Networks.ipynb" rel="noopener ugc nofollow" target="_blank">循环神经网络快速入门</a>，为了更深入的解释，参见<a class="ae jd" href="https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Deep%20Dive%20into%20Recurrent%20Neural%20Networks.ipynb" rel="noopener ugc nofollow" target="_blank">深入循环神经网络</a>。</p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="cb4d" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">递归神经网络</h1><p id="8bc5" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">在开始实现之前，至少理解一些基础知识是有帮助的。在高层次上，一个<a class="ae jd" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">递归神经网络</a> (RNN)处理序列——无论是每日股票价格、句子还是传感器测量——一次一个元素，同时保留序列中先前出现的内容的<em class="lr">内存</em>(称为状态)。</p><p id="fd88" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">循环</em>表示当前时间步的输出成为下一个时间步的输入。对于序列中的每个元素，模型不仅考虑当前的输入，还会考虑它对前面元素的记忆。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/56c59e33cb6a82add37d8041eda2fa7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*KljWrINqItHR6ng05ASR8w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Overview of RNN (<a class="ae jd" href="https://www.manning.com/books/deep-learning-with-python" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="7654" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种记忆使网络能够按顺序学习<em class="lr">长期依赖性</em>，这意味着它可以在进行预测时考虑整个上下文，无论是句子中的下一个单词、情感分类还是下一次温度测量。RNN 是为了模仿人类处理序列的方式而设计的:在形成反应时，我们会考虑整个句子，而不是单词本身。例如，考虑下面的句子:</p><p id="f4c4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">"在乐队热身的前 15 分钟，音乐会很无聊，但随后就变得非常令人兴奋。"</p><p id="68b4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一个孤立地考虑单词的机器学习模型——比如一个<a class="ae jd" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">单词包模型</a>——可能会得出这个句子是否定的结论。相比之下，RNN 人应该能够看到“但是”和“非常令人兴奋”这些词，并意识到句子从否定变成了肯定，因为它已经查看了整个序列。阅读一个完整的序列给我们一个处理其意义的环境，一个编码在循环神经网络中的概念。</p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><p id="66bf" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">RNN 的核心是由记忆细胞构成的一层。目前最受欢迎的细胞是<a class="ae jd" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">长短期记忆</a> (LSTM)，它保持细胞状态和进位，以确保信号(梯度的<a class="ae jd" href="https://stats.stackexchange.com/questions/185639/how-does-lstm-prevent-the-vanishing-gradient-problem" rel="noopener ugc nofollow" target="_blank">形式的信息)在序列处理时不会丢失。在每个时间步，LSTM 考虑当前字、进位和单元状态。</a></p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/7df125b3e7d7618bddabcc8355ff61c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*esTGDR3kcDLaTEHKCBedTQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">LSTM (Long Short Term Memory) Cell (<a class="ae jd" href="https://www.manning.com/books/deep-learning-with-python" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="fe56" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">LSTM 有 3 个不同的门和权重向量:有一个“忘记”门用于丢弃不相关的信息；一个“输入”门用于处理当前输入，一个“输出”门用于在每个时间步产生预测。然而，正如 Chollet 指出的，试图给细胞中的每一个元素赋予特定的含义是徒劳的。</p><p id="bf6c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每个单元元件的功能最终由在训练期间学习的参数(权重)决定。随意标记每个细胞部分，但它不是有效使用的必要条件！回想一下，用于<a class="ae jd" href="https://machinelearningmastery.com/sequence-prediction-problems-learning-lstm-recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank">序列学习</a>的递归神经网络的好处是它保持整个序列的记忆，防止先前的信息丢失。</p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h2 id="dfe9" class="ni mg jg bd mh nj nk dn ml nl nm dp mp le nn no mr li np nq mt lm nr ns mv nt bi translated">问题定式化</h2><p id="6cd3" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">有几种方法我们可以制定的任务，训练 RNN 写文本，在这种情况下，专利摘要。但是，我们将选择将其训练为多对一序列映射器。也就是说，我们输入一系列单词，然后训练模型来预测下一个单词。在被传递到 LSTM 层之前，单词将被映射到整数，然后使用嵌入矩阵(预训练的或可训练的)被映射到向量。</p><p id="0b0b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当我们去写一个新的专利时，我们传入一个单词的起始序列，对下一个单词进行预测，更新输入序列，进行另一个预测，将这个单词添加到序列中，然后继续我们想要生成的单词。</p><p id="fbdf" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该方法的步骤概述如下:</p><ol class=""><li id="4c8a" class="nu nv jg kx b ky kz lb lc le nw li nx lm ny lq nz oa ob oc bi translated">将摘要从字符串列表转换为整数列表(序列)</li><li id="90d0" class="nu nv jg kx b ky od lb oe le of li og lm oh lq nz oa ob oc bi translated">根据序列创建要素和标注</li><li id="597a" class="nu nv jg kx b ky od lb oe le of li og lm oh lq nz oa ob oc bi translated">使用嵌入层、LSTM 层和密集层构建 LSTM 模型</li><li id="fc7e" class="nu nv jg kx b ky od lb oe le of li og lm oh lq nz oa ob oc bi translated">加载预先训练的嵌入</li><li id="77fb" class="nu nv jg kx b ky od lb oe le of li og lm oh lq nz oa ob oc bi translated">训练模型以预测下一步工作</li><li id="073b" class="nu nv jg kx b ky od lb oe le of li og lm oh lq nz oa ob oc bi translated">通过传入起始序列进行预测</li></ol><p id="c345" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请记住，这只是问题的一种表述:我们还可以使用字符级模型，或者对序列中的每个单词进行预测。正如机器学习中的许多概念一样，没有一个正确的答案，但这种方法在实践中效果很好。</p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="563f" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">数据准备</h1><p id="2bc1" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">即使神经网络具有强大的表示能力，获得高质量、干净的数据集也是至关重要的。这个项目的原始数据来自<a class="ae jd" href="http://www.patentsview.org/querydev/" rel="noopener ugc nofollow" target="_blank"> USPTO PatentsView </a>，在这里你可以搜索在美国申请的任何专利的信息。我搜索了“神经网络”这个词，并下载了由此产生的专利摘要——总共 3500 篇。我发现最好在一个狭窄的主题上进行训练，但也可以尝试不同的专利。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/d24fd984b0643d2ebbe722e3af3255de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pAEoYnnufAvTLuvo0TEZ3A.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Patent Abstract Data</figcaption></figure><p id="1e1e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将从专利摘要作为字符串列表开始。我们模型的主要数据准备步骤是:</p><ol class=""><li id="11f2" class="nu nv jg kx b ky kz lb lc le nw li nx lm ny lq nz oa ob oc bi translated">删除标点符号，将字符串拆分成单个单词的列表</li><li id="ab26" class="nu nv jg kx b ky od lb oe le of li og lm oh lq nz oa ob oc bi translated">将单个单词转换成整数</li></ol><p id="8648" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这两个步骤都可以使用<a class="ae jd" href="https://keras.io/preprocessing/text/#tokenizer" rel="noopener ugc nofollow" target="_blank"> Keras </a> <code class="fe oj ok ol om b"><a class="ae jd" href="https://keras.io/preprocessing/text/#tokenizer" rel="noopener ugc nofollow" target="_blank">Tokenizer</a></code>类来完成。默认情况下，这会删除所有标点符号，小写单词，然后将单词转换为整数的<code class="fe oj ok ol om b">sequences</code>。一个<code class="fe oj ok ol om b">Tokenizer</code>首先是一个字符串列表上的<code class="fe oj ok ol om b">fit</code>，然后将这个列表转换成一个整数列表的列表。下面演示了这一点:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/cbd09645e1c1e5cf5cfb63c1cbff4a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*j9tc6cDGzHfBP6WS2ZGeug.png"/></div></div></figure><p id="b990" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第一个单元的输出显示原始摘要，第二个单元的输出显示标记化序列。每个抽象现在都表示为整数。</p><p id="dad2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以使用经过训练的记号赋予器的<code class="fe oj ok ol om b">idx_word</code>属性来计算这些整数的含义:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oo"><img src="../Images/4dcde0de639038b80e972ebfdbad6520.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*5tQCBYgLcH8-kAhLTsh05A.png"/></div></div></figure><p id="a0df" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你仔细观察，你会注意到<code class="fe oj ok ol om b">Tokenizer</code>去掉了所有的标点符号，所有的单词都变成了小写。如果我们使用这些设置，那么神经网络将不会学习正确的英语！我们可以通过将过滤器更改为<code class="fe oj ok ol om b">Tokenizer</code>来调整这一点，以便不删除标点符号。</p><pre class="nd ne nf ng gt op om oq or aw os bi"><span id="87ad" class="ni mg jg om b gy ot ou l ov ow"># Don't remove punctuation or uppercase<br/>tokenizer = Tokenizer(num_words=None, <br/>                     filters='#$%&amp;()*+-&lt;=&gt;@[\\]^_`{|}~\t\n',<br/>                     lower = False, split = ' ')</span></pre><p id="4a02" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">查看不同实现的笔记本，但是，当我们使用预先训练的嵌入时，我们必须删除大写字母，因为嵌入中没有小写字母。当训练我们自己的嵌入时，我们不必担心这一点，因为模型将学习小写和大写的不同表示。</p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="d804" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">功能和标签</h1><p id="3183" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">前一步将所有摘要转换成整数序列。下一步是创建一个有监督的机器学习问题，用它来训练网络。有许多方法可以为文本生成设置递归神经网络任务，但我们将使用以下方法:</p><blockquote class="ls lt lu"><p id="e285" class="kv kw lr kx b ky kz kh la lb lc kk ld lv lf lg lh lw lj lk ll lx ln lo lp lq ij bi translated">给网络一个单词序列，训练它预测下一个单词。</p></blockquote><p id="e40e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">单词数作为参数留下；在这里显示的例子中，我们将使用 50，这意味着我们给我们的网络 50 个单词，并训练它预测第 51 个单词。训练网络的其他方法是让它预测序列中每一点的下一个单词——对每个输入单词进行预测，而不是对整个序列进行一次预测——或者使用单个字符训练模型。这里使用的实现不一定是最佳的——没有公认的最佳解决方案——<em class="lr">但是它工作得很好</em>！</p><p id="a066" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">创建特征和标签相对简单，对于每个抽象(用整数表示)，我们创建多组特征和标签。我们使用前 50 个单词作为特征，第 51 个单词作为标签，然后使用单词 2-51 作为特征，预测第 52 个单词，依此类推。这给了我们更多的训练数据，这是有益的，因为网络的<a class="ae jd" href="https://research.google.com/pubs/archive/35179.pdf" rel="noopener ugc nofollow" target="_blank">性能与它在训练期间看到的数据量</a>成比例。</p><p id="c5f7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">创建要素和标注的实现如下:</p><figure class="nd ne nf ng gt is"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="fe7a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些特征以形状<code class="fe oj ok ol om b">(296866, 50)</code>结束，这意味着我们有将近 300，000 个序列，每个序列有 50 个标记。在递归神经网络的语言中，每个序列有 50 个<em class="lr">时间步</em>，每个时间步有 1 个特征。</p><p id="c318" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以将标签保留为整数，但是当标签被一次性编码时，神经网络能够最有效地训练。我们可以使用下面的代码非常快速地用<code class="fe oj ok ol om b">numpy</code>对标签进行一次性编码:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/c180a6c71b149583c10a3673cf136f70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*INCF26TfjyH51lAhB8yC0g.png"/></div></figure><p id="b0c1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了找到与<code class="fe oj ok ol om b">label_array</code>中的一行相对应的单词，我们使用:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/0b31b7b64b1d6fd67525f13cc8d352ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*jvr7zW3rql-ekRe-Vjz4Mg.png"/></div></figure><p id="e76b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在正确格式化我们所有的特性和标签之后，我们希望将它们分成一个训练集和一个验证集(详见笔记本)。这里很重要的一点是同时改变特性和标签，这样相同的摘要就不会出现在同一个集合中。</p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="6730" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">构建递归神经网络</h1><p id="fe6b" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">Keras 是一个不可思议的库:它允许我们用几行可理解的 Python 代码构建最先进的模型。虽然<a class="ae jd" href="https://deepsense.ai/keras-or-pytorch/" rel="noopener ugc nofollow" target="_blank">其他神经网络库可能更快或更灵活</a>，但在开发时间和易用性方面，没有什么能胜过 Keras。</p><p id="b65a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面是一个简单 LSTM 的代码，并附有解释:</p><figure class="nd ne nf ng gt is"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="6d1e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们正在使用 Keras <code class="fe oj ok ol om b">Sequential</code> API，这意味着我们一次构建一层网络。这些层如下所示:</p><ul class=""><li id="19af" class="nu nv jg kx b ky kz lb lc le nw li nx lm ny lq pb oa ob oc bi translated">一个将每个输入单词映射到 100 维向量的<code class="fe oj ok ol om b">Embedding</code>。嵌入可以使用我们在<code class="fe oj ok ol om b">weights</code>参数中提供的预训练权重(一秒钟内更多)。如果我们不想更新嵌入，可以设置<code class="fe oj ok ol om b">False</code>。</li><li id="d914" class="nu nv jg kx b ky od lb oe le of li og lm oh lq pb oa ob oc bi translated">一个<code class="fe oj ok ol om b">Masking</code>层，用于屏蔽任何没有预训练嵌入的单词，这些单词将被表示为全零。训练嵌入时不应使用该层。</li><li id="2d07" class="nu nv jg kx b ky od lb oe le of li og lm oh lq pb oa ob oc bi translated">网络的核心:一层<code class="fe oj ok ol om b">LSTM </code>单元，带有<a class="ae jd" href="https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/" rel="noopener ugc nofollow" target="_blank">脱落以防止过拟合</a>。由于我们只使用一个 LSTM 层，它<em class="lr">不会</em>返回序列，对于使用两个或更多层，请确保返回序列。</li><li id="05d0" class="nu nv jg kx b ky od lb oe le of li og lm oh lq pb oa ob oc bi translated">激活<code class="fe oj ok ol om b">relu</code>的全连接<code class="fe oj ok ol om b">Dense</code>层。这为网络增加了额外的表示能力。</li><li id="0f0b" class="nu nv jg kx b ky od lb oe le of li og lm oh lq pb oa ob oc bi translated">一个<code class="fe oj ok ol om b">Dropout</code>层，用于防止过度拟合训练数据。</li><li id="5371" class="nu nv jg kx b ky od lb oe le of li og lm oh lq pb oa ob oc bi translated">一个<code class="fe oj ok ol om b">Dense</code>全连接输出层。这为使用<code class="fe oj ok ol om b">softmax</code>激活的 vocab 中的每个单词产生一个概率。</li></ul><p id="ae17" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该模型使用<code class="fe oj ok ol om b">Adam</code>优化器(随机梯度下降的变体)进行编译，并使用<code class="fe oj ok ol om b">categorical_crossentropy</code>损失进行训练。在训练期间，网络将试图通过调整可训练参数(权重)来最小化日志损失。与往常一样，使用<a class="ae jd" href="http://neuralnetworksanddeeplearning.com/chap2.html" rel="noopener ugc nofollow" target="_blank">反向传播</a>计算参数的梯度，并用优化器更新。因为我们正在使用 Keras，所以我们不用担心这是如何在幕后发生的,,只需要正确设置网络。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/f2098cb896b35608f6230cde4bce6daf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*MAy6t2V08M-M5ZKB_9oRsQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">LSTM network layout.</figcaption></figure><p id="696a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在不更新嵌入的情况下，网络中需要训练的参数要少得多。<a class="ae jd" href="https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/" rel="noopener ugc nofollow" target="_blank">对</a> <code class="fe oj ok ol om b"><a class="ae jd" href="https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/" rel="noopener ugc nofollow" target="_blank">LSTM</a></code> <a class="ae jd" href="https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/" rel="noopener ugc nofollow" target="_blank">层的输入是</a> <code class="fe oj ok ol om b"><a class="ae jd" href="https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/" rel="noopener ugc nofollow" target="_blank">(None, 50, 100)</a></code> <a class="ae jd" href="https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/" rel="noopener ugc nofollow" target="_blank">这意味着</a>对于每一批(第一维)，每个序列有 50 个时间步长(字)，每个时间步长在嵌入后有 100 个特征。LSTM 图层的输入总是具有<code class="fe oj ok ol om b">(batch_size, timesteps, features)</code>形状。</p><p id="99c3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">构建这个网络有很多方法，笔记本中还介绍了其他几种方法。例如，我们可以使用两个相互堆叠的<code class="fe oj ok ol om b">LSTM</code>层，一个处理来自两个方向的序列的<code class="fe oj ok ol om b">Bidirectional LSTM</code>层，或者更多的<code class="fe oj ok ol om b">Dense</code>层。我发现上面的设置运行良好。</p><h2 id="6b3a" class="ni mg jg bd mh nj nk dn ml nl nm dp mp le nn no mr li np nq mt lm nr ns mv nt bi translated">预训练嵌入</h2><p id="0aff" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">一旦网络建立起来，我们仍然需要为它提供预先训练好的单词嵌入。你可以在网上找到许多在不同语料库(大量文本)上训练过的嵌入。我们将使用的是斯坦福提供的<a class="ae jd" href="https://nlp.stanford.edu/data/" rel="noopener ugc nofollow" target="_blank">，有 100、200 或 300 种尺寸(我们将坚持 100)。这些嵌入来自</a><a class="ae jd" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe(单词表示的全局向量)</a>算法，并在维基百科上进行训练。</p><p id="44af" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">即使预训练的嵌入包含 400，000 个单词，我们的 vocab 中也包含一些单词。当我们用嵌入来表示这些单词时，它们将具有全零的 100 维向量。这个问题可以通过训练我们自己的嵌入或者通过将<code class="fe oj ok ol om b">Embedding</code>层的<code class="fe oj ok ol om b">trainable</code>参数设置为<code class="fe oj ok ol om b">True</code>(并移除<code class="fe oj ok ol om b">Masking</code>层)来解决。</p><p id="2451" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以从磁盘中快速加载预训练的嵌入，并使用以下代码创建一个嵌入矩阵:</p><figure class="nd ne nf ng gt is"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="7b23" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是给 vocab 中的每个单词分配一个 100 维的向量。如果单词没有预先训练的嵌入，那么这个向量将全为零。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/3ccaa2d6ec6a0764137b017b92d4b246.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*re5ejOCTVKlwmxu7xA4n2g.png"/></div></figure><p id="9920" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了探索嵌入，我们可以使用余弦相似度来在嵌入空间中找到与给定查询词最接近的词:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/b50aa4a77dfb6b753a45029816fdab64.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*nml4qX3uOtk5NO-6aVU54g.png"/></div></figure><p id="1790" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/neural-network-embeddings-explained-4d028e6f0526">嵌入是被学习的</a>，这意味着表征专门应用于一个任务。当使用预先训练的嵌入时，我们希望嵌入所学习的任务足够接近我们的任务，所以嵌入是有意义的。如果这些嵌入是在 tweets 上训练的，我们可能不会期望它们工作得很好，但是因为它们是在 Wikipedia 数据上训练的，它们应该普遍适用于一系列语言处理任务。</p><p id="44b8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果您有大量的数据和计算机时间，通常最好学习自己的特定任务的嵌入。在笔记本中，我采用了两种方法，学习到的嵌入表现稍好。</p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="eb0c" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">训练模型</h1><p id="dc5f" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">随着训练和验证数据的准备、网络的构建和嵌入的加载，我们几乎已经为模型学习如何编写专利摘要做好了准备。然而，在训练神经网络时，最好使用<a class="ae jd" href="https://keras.io/callbacks/" rel="noopener ugc nofollow" target="_blank">模型检查点和 Keras 回调形式的早期停止</a>:</p><ul class=""><li id="e211" class="nu nv jg kx b ky kz lb lc le nw li nx lm ny lq pb oa ob oc bi translated">模型检查点:在磁盘上保存最佳模型(通过验证损失来衡量),以便使用最佳模型</li><li id="7ca5" class="nu nv jg kx b ky od lb oe le of li og lm oh lq pb oa ob oc bi translated">提前停止:当验证损失不再减少时，停止训练</li></ul><p id="7dda" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用<a class="ae jd" href="https://stats.stackexchange.com/questions/231061/how-to-use-early-stopping-properly-for-training-deep-neural-network" rel="noopener ugc nofollow" target="_blank">提前停止</a>意味着我们不会过度适应训练数据，也不会浪费时间训练那些不会提高性能的额外时期。模型检查点意味着我们可以访问最好的模型，如果我们的训练被中断 1000 个纪元，我们也不会失去所有的进展！</p><figure class="nd ne nf ng gt is"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="5052" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，可以使用以下代码训练该模型:</p><figure class="nd ne nf ng gt is"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="58f1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在一个<a class="ae jd" href="https://aws.amazon.com/ec2/instance-types/p2/" rel="noopener ugc nofollow" target="_blank"> Amazon p2.xlarge 实例</a>(0.90 美元/小时的预订)上，完成这个任务需要 1 个多小时。一旦训练完成，我们可以加载回最佳保存的模型，并评估验证数据的最终时间。</p><pre class="nd ne nf ng gt op om oq or aw os bi"><span id="6f4d" class="ni mg jg om b gy ot ou l ov ow">from keras import load_model</span><span id="4767" class="ni mg jg om b gy pf ou l ov ow"># Load in model and evaluate on validation data<br/>model = load_model('../models/model.h5')<br/>model.evaluate(X_valid, y_valid)</span></pre><p id="b855" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">总体而言，使用预训练单词嵌入的模型实现了 23.9%的验证准确率。考虑到作为一个人，我发现很难预测这些摘要中的下一个词，这很好！对最常见的单词(“the”)的简单猜测产生了大约 8%的准确度。笔记本中所有型号的指标如下所示:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pg"><img src="../Images/0b0e05a3e2a293c8facc4529728303d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I1Cjf7ZATHDz5A0UxpobQA.png"/></div></div></figure><p id="c8f3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最佳模型使用预先训练的嵌入和如上所示的相同架构。我鼓励任何人尝试用不同的模式训练！</p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="a144" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">专利摘要生成</h1><p id="0659" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">当然，虽然高指标很好，但重要的是网络是否能产生合理的专利摘要。使用最佳模型，我们可以探索模型生成能力。如果你想在你自己的硬件上运行这个，你可以在这里找到<a class="ae jd" href="https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Exploring%20Model%20Results.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>，在 GitHub 上找到<a class="ae jd" href="https://github.com/WillKoehrsen/recurrent-neural-networks/tree/master/models" rel="noopener ugc nofollow" target="_blank">预训练模型</a>。</p><p id="a430" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了产生输出，我们用从专利摘要中选择的随机序列作为网络的种子，让它预测下一个单词，将预测添加到序列中，并继续预测我们想要的任意多个单词。一些结果如下所示:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ph"><img src="../Images/2d5b9eb2b47fcb9fcc02cacbb4fb7700.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kQlyQ45s2Turn925Qo4Qrw.png"/></div></div></figure><p id="4c43" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">输出的一个重要参数是<a class="ae jd" href="https://medium.com/machine-learning-at-petiteprogrammer/sampling-strategies-for-recurrent-neural-networks-9aea02a6616f" rel="noopener">预测</a>的<em class="lr">差异</em>。我们不是使用概率最高的预测单词，而是将多样性注入到预测中，然后选择概率与更多样的预测成比例的下一个单词。多样性太高，生成的输出开始看起来是随机的，但太低，网络会进入输出的递归循环。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pi"><img src="../Images/1d4a07496f0cc9b69510040bf48d36eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oX-d7QmH5sAAhyUuEZTn5Q.png"/></div></div></figure><p id="196f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">输出还不算太差！有时很难确定哪些是计算机生成的，哪些是机器生成的。这部分是由于专利摘要的<a class="ae jd" href="http://www.wipo.int/standards/en/pdf/03-12-a.pdf" rel="noopener ugc nofollow" target="_blank">本质，大多数时候，它们听起来不像是人类写的。</a></p><p id="b72b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">网络的另一个用途是用我们自己的启动序列来播种它。我们可以使用任何我们想要的文本，看看网络会把它带到哪里:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pj"><img src="../Images/60d5636e7f8c3d79399d2c36e0032f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RzwRgUxCh9z2NuMltAoPFg.png"/></div></div></figure><p id="f956" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">同样，结果并不完全可信，但它们确实很像英语。</p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h2 id="c16d" class="ni mg jg bd mh nj nk dn ml nl nm dp mp le nn no mr li np nq mt lm nr ns mv nt bi translated">人还是机器？</h2><p id="f1b3" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">作为递归神经网络的最终测试，我创建了一个游戏来猜测是模型还是人产生了输出。这是第一个例子，其中两个选项来自计算机，一个来自人类:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pk"><img src="../Images/5dadb1d8599027e920dad13a220ff056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ujviMus1Gb1LGVLEEjXmFw.png"/></div></div></figure><p id="f748" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你的猜测是什么？答案是第<em class="lr">秒</em>是一个人写的实际摘要(嗯，它是摘要中的实际内容。我不确定这些摘要是人写的)。这里还有一个:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pl"><img src="../Images/ac7a5c546979168f703f96ae7bd303d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-v864Qni74z-su4zmJQcNA.png"/></div></div></figure><p id="9b7e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这一次<em class="lr">第三部</em>有了一个有血有肉的作家。</p><p id="7270" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以使用额外的步骤来解释该模型，例如发现哪些神经元随着不同的输入序列而变亮。我们还可以查看学习到的嵌入(或者用<a class="ae jd" href="https://projector.tensorflow.org" rel="noopener ugc nofollow" target="_blank">投影工具</a>可视化它们)。我们将在另一个时间讨论这些话题，并得出结论，我们现在知道如何实现一个递归神经网络来有效地模仿人类文本。</p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="86f7" class="mf mg jg bd mh mi mj mk ml mm mn mo mp km mq kn mr kp ms kq mt ks mu kt mv mw bi translated">结论</h1><p id="98eb" class="pw-post-body-paragraph kv kw jg kx b ky mx kh la lb my kk ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">认识到递归神经网络没有语言理解的概念是很重要的。它实际上是一台非常复杂的模式识别机器。然而，与马尔可夫链或频率分析等方法不同，rnn 基于序列中元素的<em class="lr">排序进行预测。从哲学角度来看，<a class="ae jd" href="https://bigthink.com/endless-innovation/humans-are-the-worlds-best-pattern-recognition-machines-but-for-how-long" rel="noopener ugc nofollow" target="_blank">你可能会认为人类只是极端的模式识别机器</a>，因此递归神经网络只是像人类机器一样工作。</em></p><p id="531a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">递归神经网络的用途远远不止文本生成，还包括<a class="ae jd" href="https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/" rel="noopener ugc nofollow" target="_blank">机器翻译</a>、<a class="ae jd" href="https://cs.stanford.edu/people/karpathy/sfmltalk.pdf" rel="noopener ugc nofollow" target="_blank">图像字幕</a>和<a class="ae jd" href="https://arxiv.org/ftp/arxiv/papers/1506/1506.04891.pdf" rel="noopener ugc nofollow" target="_blank">作者身份识别</a>。虽然我们在这里讨论的这个应用程序不会取代任何人类，但可以想象，随着更多的训练数据和更大的模型，神经网络将能够合成新的、合理的专利摘要。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pm"><img src="../Images/e69f7e224e2353881fbe385d2cad972a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r0SdCL5x2-ufHTWpfrx2EQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">A Bi-Directional LSTM Cell (<a class="ae jd" href="https://developer.nvidia.com/discover/lstm" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="a39b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">人们很容易陷入复杂技术背后的细节或理论，但学习数据科学工具的更有效方法是<a class="ae jd" href="https://github.com/DOsinga/deep_learning_cookbook" rel="noopener ugc nofollow" target="_blank">深入研究并构建应用</a>。一旦你知道了一项技术的能力以及它在实践中是如何工作的，你就可以随时回头去了解这个理论。我们大多数人不会设计神经网络，但学习如何有效地使用它们是值得的。这意味着收起书本，打破键盘，编写你自己的网络。</p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><p id="77f8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一如既往，我欢迎反馈和建设性的批评。可以通过推特<a class="ae jd" href="http://twitter.com/@koehrsen_will" rel="noopener ugc nofollow" target="_blank"> @koehrsen_will </a>或者通过我的网站<a class="ae jd" href="https://willk.online" rel="noopener ugc nofollow" target="_blank"> willk.online </a>找到我。</p></div></div>    
</body>
</html>