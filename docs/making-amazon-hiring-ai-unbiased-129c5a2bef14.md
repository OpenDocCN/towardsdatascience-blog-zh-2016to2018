# 让亚马逊雇佣 AI 不带偏见

> 原文：<https://towardsdatascience.com/making-amazon-hiring-ai-unbiased-129c5a2bef14?source=collection_archive---------17----------------------->

亚马逊招聘算法中性别偏见的[新闻](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)传遍了互联网，这为机器学习模型的可解释性主题开辟了一条新线索。让我给你讲一下这个故事的背景。亚马逊拥有至少 575700 名员工。如果员工的平均任期为 3 年，他们需要每年招聘(191900 +人数增加)。如果每 5 个面试的候选人中有 1 个被选中，每 3 份简历中有 1 个被选中，那么即使人数保持不变，他们每年也需要检查 191900*3*5 = 2878500 份简历。这些数字——3 和 5——会因不同的情况而有所不同，因为送货员比工程师更容易雇用，但我们不要让这个计算变得不必要的复杂。重点是衡量这个数字可以有多大，以及在这个数字上花费了多少精力和资源。

如果你曾经参加过面试，你会同意简历筛选有多无聊——尤其是如果这是你的工作。这是一项基于重复模式的工作——这是人工智能非常擅长的。因此，对于像亚马逊这样的创新型大公司来说，理解自己的招聘实践并用算法复制它是完全有意义的。由于简历和工作描述是文本数据，我们需要利用 NLP(自然语言处理)。

如果我必须自己制作算法，我会使用这条管道，这可能也是亚马逊所做的。

*   预处理简历文本
*   [用 TF-IDF 或](http://ml-dl.com/recommendation-algorithms-part-1/) [BM25](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables) 对文本进行矢量化
*   为物流和工程中重复性很高的入门级配置文件训练一个监督分类器。如果数据足够，我们也可以对非入门级配置文件进行测试。分类器可以是诸如朴素贝叶斯、随机森林或深度学习序列模型之类的任何东西，并且用于分类的类别被选择和不被选择
*   预测新简历被选中的概率
*   过滤超过截止概率如 0.8 的恢复
*   根据概率选择面试的前 x 个概况，其中 x 取决于我们想要聘用的候选人数量和过去的转换率

另一种方法是通过 Lucene/Elasticsearch 对简历和职位描述进行相似性匹配，并选择具有截止相似性得分的前 k 名结果。最高的结果确保了与 JD 的匹配，而不是他们与角色的适合程度，因此这种方法不是很合适。

# 问题是

现在让我们深入了解一下新闻:他们的新招聘引擎不喜欢女性。美国顶级科技公司尚未消除招聘中的性别差距，这种差距在软件开发人员等技术人员中最为明显，男性人数远远超过女性。亚马逊的实验性招聘引擎遵循同样的模式，学会惩罚包含“女性”一词的简历，直到公司发现这个问题。

目前人们对新闻的看法是:

1.  人们的第一反应是人工智能有缺陷。
2.  AI 只会和数据一样有偏差。因此，艾透露，亚马逊的招聘人员可能偏向男性。
3.  亚马逊是一家勇于披露其模式缺陷的公司。大多数公司不会这么做。

# 问题的解决方案

现在我想讨论的是如何使算法无偏的部分。问题是出现在女性简历中的词的重要性较低，因为这些词在精选的简历中出现得较少。亚马逊的系统会惩罚包含“女性”一词的简历，比如“女子象棋俱乐部队长”并且降级了两所女子大学的毕业生* 。还可能存在与种族词汇相关的问题。

由于性别和种族词汇不是一个人技能的指标，我们可以将这些词汇映射到一个常见的令牌，如 AAA。所以现在*男子棋社队长和女子棋社队长*都映射到了 *AAA 的棋社队长。*所以如果 *AAA 的象棋俱乐部队长*入选候选人，男性和女性的简历都会对这些词给予同等的重视。此外，这不仅仅是一个词的男性或女性。在完成矢量化的同时，我们还创建了双词和三词特征，在本例中，它们将是“*AAA ' s chess”*和“*AAA ' s chess club”*，这在之前包含单词 men 和 women 时会有所不同。

因此，我们所需要的是在矢量化之前的**偏差消除文本预处理**步骤，在该步骤中，我们将性别/种族单词映射到一个公共标记。可以通过 HR 的观察或从[列表](http://myenglishgrammar.com/list-20-gender.html)中收集此类单词的列表(并非此列表中的所有单词都有用)。在我看来，这个练习和实验并没有证明人工智能有缺陷，而是揭示了一个常识，即人工智能和数据一样好，如果数据没有准备好，它需要处理。

# 事后思考

令人难过的是，他们解决了偏见，但放弃了该项目，正如文章提到的那样——“亚马逊编辑了程序，使它们对这些特定术语保持中立。但这并不能保证机器不会设计出其他可能被证明具有歧视性的候选人分类方法。”像所有的研究一样，人工智能本质上也是迭代的。亚马逊花了相当多的时间来制作算法，现在这个缺陷被发现并纠正了，它导致了一个更好的算法。只有通过这些循环的改进，我们才有希望实现一个近乎完美的无偏算法。我不知道亚马逊为什么关闭它。

文章还提到，包含“已执行”和“已捕获”等词的简历得分异常高。驯服算法需要对矢量化和分类算法有深入的理解。当 TF-IDF/BM25 在简历中看到一个非常不寻常的词时，它会造成巨大的破坏。罕见词具有高 IDF 值，因此 TF-IDF 值可能变大。分类算法也可以给这些导致奇怪结果的不寻常的单词很高的权重。必须通过文本探索、模型特征重要性和用于解释训练的 ML 模型的算法来找出这样的词。一旦发现，可以手动或通过某种逻辑或通过保持较高的最小文档频率值，将它们从矢量化过程中移除。这有助于减少特征(单词)的数量，并有助于解决过度拟合问题。但这也可能会从模型中删除好的特征，从而降低数据科学家所关注的模型的准确性。

亚马逊擅长的推荐算法也存在类似问题。理想情况下，数据集应该是巨大的+变化的，并且算法应该被健壮地测试。当训练数据较少时，问题就出现了，因此过度拟合和偏差开始起作用。消除这种情况的唯一方法是拥有一个庞大的数据集，该数据集受到其自身雇佣(选定/未选定的候选人)数据的约束。我们需要估计我们可能需要多少数据，以及可能需要多少年来收集。如果所需的年数很长或不确定，关闭项目是有意义的。人们可能会认为人工智能失败了，但这可能是一个数据问题，这就是为什么亚马逊可能会暂时关闭它。还记得为什么几年前深度学习突然开始工作了吗？访问大量的标记数据，更好的计算和算法的改进。

> 我的看法是，亚马逊可能不仅发现了自己模式的缺陷，还发现了从事人力资源技术的其他公司模式的缺陷。这将导致未来几天更好的人力资源解决方案。

最后，更不用说，随着人工智能越来越多地应用于现实世界的问题，机器学习模型的可解释性变得至关重要。

通过评论或者通过 LinkedIn 让我知道你的想法。

* https://www . Reuters . com/article/us-Amazon-com-jobs-automation-insight/Amazon-scraps-secret-ai-recruiting-tool-that-show-bias-against-women-iduskcn 1 MK 08g

*原载于 2018 年 10 月 12 日*[*【ml-dl.com】*](http://ml-dl.com/making-amazon-hiring-ai-unbiased/)*。*