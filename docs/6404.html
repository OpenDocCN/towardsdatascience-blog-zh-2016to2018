<html>
<head>
<title>Named Entity Recognition (NER) with keras and tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 keras 和 tensorflow 的命名实体识别(NER)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede?source=collection_archive---------1-----------------------#2018-12-12">https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede?source=collection_archive---------1-----------------------#2018-12-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="deaf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过应用最先进的深度学习方法满足行业需求</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/93ecea56ccaf3c5c1baa29daf3d2cff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hHAsAlD6xZrjTQoOvccl6w.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">photo credit: pexels</figcaption></figure><p id="7169" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">几年前，当我在一家初创公司做软件工程实习生时，我在一个招聘网站上看到了一个新功能。该应用程序能够识别和解析简历中的重要信息，如电子邮件地址、电话号码、学位头衔等。我开始和我们的团队讨论可能的方法，我们决定用 python 建立一个基于规则的解析器来解析简历的不同部分。在花了一些时间开发解析器之后，我们意识到答案可能不是基于规则的工具。我们开始在谷歌上搜索这是如何做到的，我们遇到了术语<strong class="la iu">自然语言处理(NLP) </strong>以及更具体的与<strong class="la iu">机器学习相关的<strong class="la iu">命名实体识别(NER) </strong>。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/e9421e5286bfd22c17209f389e98eeb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8LOMipM-fmszClg-AwATkQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">photo credit: meenavyas</figcaption></figure><p id="8cd6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">NER 是一种信息提取技术，用于识别和分类文本中的命名实体。这些实体可以是预定义的和通用的，如地点名称、组织、时间等，也可以是非常具体的，如简历中的例子。 NER 在业务中有各种各样的使用案例。我认为 gmail 应用了 NER，当你写邮件时，如果你在邮件中提到时间或附上文件，gmail 会设置日历通知或提醒你附上文件，以防你发送没有附件的邮件。NER 的其他应用包括:从<strong class="la iu">法律、金融和医疗文档中提取重要的命名实体，</strong>为<strong class="la iu">新闻提供者分类内容，</strong>改进<strong class="la iu">搜索算法</strong>等等。在本文的其余部分，我们将简短介绍解决 NER 问题的不同方法，然后我们将开始编写最先进的方法。下面是苏沃洛对 NER 更详细的介绍。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/8df0e7142ca98f5bbfdb94a655f9b9e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yDB7u06I1yJCL5y1N1-tSw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">photo credit: pexels</figcaption></figure><h1 id="48eb" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated"><strong class="ak">接近 NER </strong></h1><ul class=""><li id="c5db" class="mp mq it la b lb mr le ms lh mt ll mu lp mv lt mw mx my mz bi translated"><strong class="la iu">经典方法</strong>:大多基于规则。这里有一个链接，链接到 Sentdex 的一个很短很棒的<a class="ae lv" href="https://www.youtube.com/watch?v=LFXsG7fueyk" rel="noopener ugc nofollow" target="_blank">视频</a>，它在 python 中为 NER 使用了 NLTK 包。</li><li id="b834" class="mp mq it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><strong class="la iu">机器学习方法</strong>:这一类主要有两种方法:<strong class="la iu"> A- </strong>把问题当做多类分类，其中命名实体就是我们的标签，这样我们就可以应用不同的分类算法。这里的问题是<strong class="la iu">识别和标注命名实体需要彻底理解句子的上下文和其中单词标签的序列，而这种方法忽略了这一点。<strong class="la iu"> B- </strong>这一类的另一种方法是<strong class="la iu">条件随机场(CRF)模型。它是一个概率图形模型，可用于模拟顺序数据，如句子中单词的标签。</strong>关于 CRF 在 python 中的更多细节和完整实现，请参见<a class="ae lv" href="https://www.depends-on-the-definition.com/named-entity-recognition-conditional-random-fields-python/" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> Tobias 的</strong> </a>文章。CRF 模型能够捕获序列中当前和先前标签的特征，但是它不能理解前向标签的上下文；这个缺点加上与训练 CRF 模型相关的额外的特征工程，使得它不太适合于工业应用。</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/65fb286f4a35703522035eb372a309e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D1EucoWgQRzqyVh-Tbeqow.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">photo credit: abajournal</figcaption></figure><ul class=""><li id="3cd4" class="mp mq it la b lb lc le lf lh ng ll nh lp ni lt mw mx my mz bi translated"><strong class="la iu">深度学习方法:</strong></li></ul><p id="d02c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在讨论 NER 的深度学习方法(最先进的)的细节之前，我们需要分析适当和清晰的指标来评估我们模型的性能。在不同迭代(时期)中训练神经网络时，通常使用准确度作为评估度量。但是，在 NER 案例中，我们可能会处理重要的金融、医疗或法律文档，准确识别这些文档中的命名实体决定了模型的成功与否。换句话说，<strong class="la iu">在 NER 任务中，误报和漏报都有商业成本。</strong>因此，我们评估模型的主要指标将是 F1 分数，因为我们需要在精确度和召回率之间取得平衡。</p><p id="3738" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">建立高性能深度学习方法的另一个重要策略是，考虑到文本是一种序列数据格式，了解哪种类型的神经网络最适合处理 NER 问题。是的，你猜对了…长短期记忆(LSTM)。此<a class="ae lv" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">链接</a>中关于 LSTMs 的更多详情。但不是任何类型的 LSTM，我们需要使用双向 LSTM，因为使用标准 LSTM 进行预测只会考虑文本序列中的“过去”信息。对 NER 来说，由于上下文在序列中涵盖了过去和未来的标签，我们需要将过去和未来的信息都考虑在内。<strong class="la iu">双向 LSTM 是两个 lstm 的组合——一个从“右到左”向前运行，一个从“左到右”向后运行。</strong></p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/54d7dbbf3db570464da559a2dceefa2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UNwyJyLQT4g7xB7EXNu0NA.jpeg"/></div></div></figure><p id="1db0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将通过参考实际的研究论文来快速浏览四种不同的最新方法的架构，然后我们将继续实施精度最高的方法。</p><ol class=""><li id="eb8b" class="mp mq it la b lb lc le lf lh ng ll nh lp ni lt nr mx my mz bi translated"><a class="ae lv" href="https://arxiv.org/pdf/1508.01991v1.pdf" rel="noopener ugc nofollow" target="_blank">T5】双向 LSTM-CRFT7<strong class="la iu">:</strong></a></li></ol><p id="b766" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">更多细节和 keras 中的<a class="ae lv" href="https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/" rel="noopener ugc nofollow" target="_blank">实现</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/4f2837595979ae3e4317b160baa80f2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*aVIiy1DZosJlNfGHk9jPLA.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">from the paper(Bidirectional LSTM-CRF Models for Sequence Tagging)</figcaption></figure><p id="b6b7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lv" href="https://arxiv.org/pdf/1511.08308.pdf" rel="noopener ugc nofollow" target="_blank"> 2。<strong class="la iu">双向 LSTM-CNN</strong></a><strong class="la iu">:</strong></p><p id="a025" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">更多细节和 keras 中的<a class="ae lv" href="https://www.depends-on-the-definition.com/lstm-with-char-embeddings-for-ner/" rel="noopener ugc nofollow" target="_blank">实现</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ba0e69575aa736e4d6a59998422eab70.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*7P0SUJRt0Yjf4H0prYW0SA.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">from the paper( Named Entity Recognition with Bidirectional LSTM-CNNs)</figcaption></figure><p id="75b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lv" href="https://arxiv.org/pdf/1603.01354.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> 3。双向 LSTM-CNN-CRF:</strong></a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/5925c2a88a190a999d4ed2a0b6abd2e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*TLokHRrDuQ22IlSi8cRPdQ.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">from the paper (End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF)</figcaption></figure><p id="6645" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4.ELMo(从语言模型嵌入):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/97e160a2d569efd601c27c3b7dff6f5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1L2nhU82D0EGU5Lqc2KQdA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae lv" href="https://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">Jay Alamma</a>r</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/9ebfc6f002bb4d409c2bf30d22842c4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XrcN5xtMY2CcH3vgVZ3QdA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae lv" href="https://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">Jay Alammar</a></figcaption></figure><p id="c3ea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最近的一篇<a class="ae lv" href="https://arxiv.org/pdf/1802.05365.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>(深度语境化单词表示)介绍了一种新类型的深度语境化单词表示，它模拟了单词使用的复杂特征(例如，句法和语义)，以及这些使用如何在语言语境中变化(例如，模拟一词多义)。新方法(ELMo)有三个重要的代表<a class="ae lv" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank"/>:</p><p id="b2f6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">1.<em class="nx">上下文</em>:每个单词的表示取决于使用它的整个上下文。</p><p id="edfc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nx"> 2。深度</em>:单词表示结合了深度预训练神经网络的所有层。</p><p id="19ea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nx"> 3。基于字符的</em> : ELMo 表示完全基于字符，允许网络使用形态学线索为训练中看不到的词汇外标记形成健壮的表示。</p><p id="478e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lv" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank"> ELMo </a>对语言有很好的理解，因为它是在大规模数据集上训练的，ELMo 嵌入是在 10 亿词<a class="ae lv" href="https://tfhub.dev/google/elmo/2" rel="noopener ugc nofollow" target="_blank">基准</a>上训练的。这种训练被称为双向语言模型(biLM)，它可以从过去学习，并预测像句子一样的单词序列中的下一个单词。让我们看看如何实现这种方法。我们将使用来自 kaggle 的数据集。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="c88e" class="od ly it nz b gy oe of l og oh">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>plt.style.use("ggplot")</span><span id="2e08" class="od ly it nz b gy oi of l og oh">data = pd.read_csv("ner_dataset.csv", encoding="latin1")<br/>data = data.drop(['POS'], axis =1)<br/>data = data.fillna(method="ffill")<br/>data.tail(12)</span><span id="a44d" class="od ly it nz b gy oi of l og oh">words = set(list(data['Word'].values))<br/>words.add('PADword')<br/>n_words = len(words)<br/>n_words</span><span id="84a2" class="od ly it nz b gy oi of l og oh"><strong class="nz iu">35179</strong></span><span id="6527" class="od ly it nz b gy oi of l og oh">tags = list(set(data["Tag"].values))<br/>n_tags = len(tags)<br/>n_tags</span><span id="3c04" class="od ly it nz b gy oi of l og oh"><strong class="nz iu">17</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/f1737c85fca089d99b166fc6fa328a8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*y9Vf8fyAiAFvFmyiinkpIw.jpeg"/></div></figure><p id="3032" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的数据集中有 47958 个句子，35179 个不同的单词和 17 个不同的命名实体(标签)。</p><p id="c8de" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们来看看句子长度在数据集中的分布:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="ad5a" class="od ly it nz b gy oe of l og oh">class SentenceGetter(object):<br/>    <br/>    def __init__(self, data):<br/>        self.n_sent = 1<br/>        self.data = data<br/>        self.empty = False<br/>        agg_func = lambda s: [(w, t) for w, t in zip(s["Word"].values.tolist(),s["Tag"].values.tolist())]<br/>        self.grouped = self.data.groupby("Sentence #").apply(agg_func)<br/>        self.sentences = [s for s in self.grouped]<br/>    <br/>    def get_next(self):<br/>        try:<br/>            s = self.grouped["Sentence: {}".format(self.n_sent)]<br/>            self.n_sent += 1<br/>            return s<br/>        except:<br/>            return None</span></pre><p id="1fc3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个类负责将每个带有命名实体(标签)的句子转换成一个元组列表[(单词，命名实体)，…]</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="3bcd" class="od ly it nz b gy oe of l og oh">getter = SentenceGetter(data)<br/>sent = getter.get_next()<br/>print(sent)</span><span id="f9c9" class="od ly it nz b gy oi of l og oh"><strong class="nz iu">[('Thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have', 'O'), ('marched', 'O'), ('through', 'O'), ('London', 'B-geo'), ('to', 'O'), ('protest', 'O'), ('the', 'O'), ('war', 'O'), ('in', 'O'), ('Iraq', 'B-geo'), ('and', 'O'), ('demand', 'O'), ('the', 'O'), ('withdrawal', 'O'), ('of', 'O'), ('British', 'B-gpe'), ('troops', 'O'), ('from', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]</strong></span><span id="0e98" class="od ly it nz b gy oi of l og oh">sentences = getter.sentences<br/>print(len(sentences))</span><span id="fe62" class="od ly it nz b gy oi of l og oh"><strong class="nz iu">47959</strong></span><span id="9d94" class="od ly it nz b gy oi of l og oh">largest_sen = max(len(sen) for sen in sentences)<br/>print('biggest sentence has {} words'.format(largest_sen))</span><span id="740e" class="od ly it nz b gy oi of l og oh"><strong class="nz iu">biggest sentence has 104 words</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/57f76821b548fab456b7b72480059ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*WevEdPVcrh7Ch9IcwSYjeQ.jpeg"/></div></figure><p id="86a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以最长的句子有 140 个单词，我们可以看到几乎所有的句子都少于 60 个单词。</p><p id="3b88" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种方法最大的好处之一是我们不需要任何特征工程；我们所需要的是句子和它的标记词，剩下的工作由 ELMo embeddeds 继续进行。为了将我们的句子输入到 LSTM 网络中，它们都需要一样大。看分布图，我们可以把所有句子的长度设置为<strong class="la iu"> 50 </strong>并为空格添加一个通用词；这个过程叫做<strong class="la iu">填充</strong>。(50 是个好数字的另一个原因是我的笔记本电脑无法处理更长的句子)。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="4a75" class="od ly it nz b gy oe of l og oh">max_len = 50<br/>X = [[w[0]for w in s] for s in sentences]<br/>new_X = []<br/>for seq in X:<br/>    new_seq = []<br/>    for i in range(max_len):<br/>        try:<br/>            new_seq.append(seq[i])<br/>        except:<br/>            new_seq.append("PADword")<br/>    new_X.append(new_seq)<br/>new_X[15]</span><span id="0036" class="od ly it nz b gy oi of l og oh"><strong class="nz iu">['Israeli','officials','say','Prime','Minister','Ariel',<br/> 'Sharon', 'will','undergo','a', 'medical','procedure','Thursday',<br/> 'to','close','a','tiny','hole','in','his','heart','discovered',<br/> 'during','treatment', 'for','a', 'minor', 'stroke', 'suffered', 'last', 'month', '.', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword',<br/> 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword',<br/> 'PADword', 'PADword']</strong></span></pre><p id="7f0a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这同样适用于命名实体，但我们这次需要将标签映射到数字:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="238b" class="od ly it nz b gy oe of l og oh">from keras.preprocessing.sequence import pad_sequences</span><span id="4d05" class="od ly it nz b gy oi of l og oh">tags2index = {t:i for i,t in enumerate(tags)}<br/>y = [[tags2index[w[1]] for w in s] for s in sentences]<br/>y = pad_sequences(maxlen=max_len, sequences=y, padding="post", value=tags2index["O"])<br/>y[15]</span><span id="c440" class="od ly it nz b gy oi of l og oh"><strong class="nz iu">array([4, 7, 7, 0, 1, 1, 1, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7,7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,7, 7, 7, 7, 7, 7])</strong></span></pre><p id="48ee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，我们将数据分为训练集和测试集，然后导入 tensorflow Hub(一个用于发布、发现和消费机器学习模型的可重用部分的库)来加载 ELMo 嵌入功能和 keras，以开始构建我们的网络。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="e26c" class="od ly it nz b gy oe of l og oh">from sklearn.model_selection import train_test_split<br/>import tensorflow as tf<br/>import tensorflow_hub as hub<br/>from keras import backend as K</span><span id="177d" class="od ly it nz b gy oi of l og oh">X_tr, X_te, y_tr, y_te = train_test_split(new_X, y, test_size=0.1, random_state=2018)</span><span id="822a" class="od ly it nz b gy oi of l og oh">sess = tf.Session()<br/>K.set_session(sess)<br/>elmo_model = hub.Module("<a class="ae lv" href="https://tfhub.dev/google/elmo/2" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/elmo/2</a>", trainable=True)<br/>sess.run(tf.global_variables_initializer())<br/>sess.run(tf.tables_initializer())</span></pre><p id="251d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第一次在代码块上运行需要一些时间，因为 ELMo 差不多有 400 MB。接下来，我们使用一个函数将我们的句子转换为 ELMo 嵌入:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="7ab6" class="od ly it nz b gy oe of l og oh">batch_size = 32<br/>def ElmoEmbedding(x):<br/>    return elmo_model(inputs={"tokens": tf.squeeze(tf.cast(x,    tf.string)),"sequence_len": tf.constant(batch_size*[max_len])<br/>                     },<br/>                      signature="tokens",<br/>                      as_dict=True)["elmo"]</span></pre><p id="03ac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们建立我们的神经网络:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="73dd" class="od ly it nz b gy oe of l og oh">from keras.models import Model, Input<br/>from keras.layers.merge import add<br/>from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda</span><span id="2531" class="od ly it nz b gy oi of l og oh">input_text = Input(shape=(max_len,), dtype=tf.string)<br/>embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(input_text)<br/>x = Bidirectional(LSTM(units=512, return_sequences=True,<br/>                       recurrent_dropout=0.2, dropout=0.2))(embedding)<br/>x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,<br/>                           recurrent_dropout=0.2, dropout=0.2))(x)<br/>x = add([x, x_rnn])  # residual connection to the first biLSTM<br/>out = TimeDistributed(Dense(n_tags, activation="softmax"))(x)</span><span id="160e" class="od ly it nz b gy oi of l og oh">model = Model(input_text, out)<br/>model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])</span></pre><p id="b042" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于我们的批处理大小为 32，因此必须以 32 的倍数为单位向网络提供数据:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="0fe4" class="od ly it nz b gy oe of l og oh">X_tr, X_val = X_tr[:1213*batch_size], X_tr[-135*batch_size:]<br/>y_tr, y_val = y_tr[:1213*batch_size], y_tr[-135*batch_size:]<br/>y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)<br/>y_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)</span><span id="c369" class="od ly it nz b gy oi of l og oh">history = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_val), y_val),batch_size=batch_size, epochs=3, verbose=1)</span><span id="ac6c" class="od ly it nz b gy oi of l og oh"><strong class="nz iu">Train on 38816 samples, validate on 4320 samples<br/>Epoch 1/3<br/>38816/38816 [==============================] - 834s 21ms/step - loss: 0.0625 - acc: 0.9818 - val_loss: 0.0449 - val_acc: 0.9861<br/>Epoch 2/3<br/>38816/38816 [==============================] - 833s 21ms/step - loss: 0.0405 - acc: 0.9869 - val_loss: 0.0417 - val_acc: 0.9868<br/>Epoch 3/3<br/>38816/38816 [==============================] - 831s 21ms/step - loss: 0.0336 - acc: 0.9886 - val_loss: 0.0406 - val_acc: 0.9873</strong></span></pre><p id="d60d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最初的目标是调整参数以实现更高的精度，但我的笔记本电脑无法处理超过 3 个时期和大于 32 的批量或增加测试规模。我在 Geforce GTX 1060 上运行 keras，花了将近 45 分钟来训练这 3 个纪元，如果你有更好的 GPU，可以通过改变一些参数来尝试一下。</p><p id="27b4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">0.9873 的验证准确性是一个很好的分数，但是我们没有兴趣用准确性指标来评估我们的模型。让我们看看如何获得精确度、召回率和 F1 分数:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="3463" class="od ly it nz b gy oe of l og oh">from seqeval.metrics import precision_score, recall_score, f1_score, classification_report</span><span id="922d" class="od ly it nz b gy oi of l og oh">X_te = X_te[:149*batch_size]<br/>test_pred = model.predict(np.array(X_te), verbose=1)</span><span id="6d0a" class="od ly it nz b gy oi of l og oh"><strong class="nz iu">4768/4768 [==============================] - 64s 13ms/step</strong></span><span id="b476" class="od ly it nz b gy oi of l og oh">idx2tag = {i: w for w, i in tags2index.items()}</span><span id="bc73" class="od ly it nz b gy oi of l og oh">def pred2label(pred):<br/>    out = []<br/>    for pred_i in pred:<br/>        out_i = []<br/>        for p in pred_i:<br/>            p_i = np.argmax(p)<br/>            out_i.append(idx2tag[p_i].replace("PADword", "O"))<br/>        out.append(out_i)<br/>    return out</span><span id="a3ae" class="od ly it nz b gy oi of l og oh">def test2label(pred):<br/>    out = []<br/>    for pred_i in pred:<br/>        out_i = []<br/>        for p in pred_i:<br/>            out_i.append(idx2tag[p].replace("PADword", "O"))<br/>        out.append(out_i)<br/>    return out<br/>    <br/>pred_labels = pred2label(test_pred)<br/>test_labels = test2label(y_te[:149*32])</span><span id="c0d6" class="od ly it nz b gy oi of l og oh">print(classification_report(test_labels, pred_labels))</span><span id="5448" class="od ly it nz b gy oi of l og oh"><strong class="nz iu">               precision   recall  f1-score   support<br/><br/>        org       0.69      0.66      0.68      2061<br/>        tim       0.88      0.84      0.86      2148<br/>        gpe       0.95      0.93      0.94      1591<br/>        per       0.75      0.80      0.77      1677<br/>        geo       0.85      0.89      0.87      3720<br/>        art       0.23      0.14      0.18        49<br/>        eve       0.33      0.33      0.33        33<br/>        nat       0.47      0.36      0.41        22<br/><br/>avg / total       0.82      0.82      0.82     11301</strong></span></pre><p id="c1c4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">0.82 的 F1 成绩是一个很突出的成绩。它击败了本节开始时提到的所有其他三种深度学习方法，并且它可以很容易地被业界采用。</p><p id="d18c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，让我们看看我们的预测是什么样的:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="6be5" class="od ly it nz b gy oe of l og oh">i = 390<br/>p = model.predict(np.array(X_te[i:i+batch_size]))[0]<br/>p = np.argmax(p, axis=-1)<br/>print("{:15} {:5}: ({})".format("Word", "Pred", "True"))<br/>print("="*30)<br/>for w, true, pred in zip(X_te[i], y_te[i], p):<br/>    if w != "__PAD__":<br/>        print("{:15}:{:5} ({})".format(w, tags[pred], tags[true]))</span><span id="9873" class="od ly it nz b gy oi of l og oh"><strong class="nz iu">Word            Pred : (True)<br/>==============================<br/>Citing         :O     (O)<br/>a              :O     (O)<br/>draft          :O     (O)<br/>report         :O     (O)<br/>from           :O     (O)<br/>the            :O     (O)<br/>U.S.           :B-org (B-org)<br/>Government     :I-org (I-org)<br/>Accountability :I-org (O)<br/>office         :O     (O)<br/>,              :O     (O)<br/>The            :B-org (B-org)<br/>New            :I-org (I-org)<br/>York           :I-org (I-org)<br/>Times          :I-org (I-org)<br/>said           :O     (O)<br/>Saturday       :B-tim (B-tim)<br/>the            :O     (O)<br/>losses         :O     (O)<br/>amount         :O     (O)<br/>to             :O     (O)<br/>between        :O     (O)<br/>1,00,000       :O     (O)<br/>and            :O     (O)<br/>3,00,000       :O     (O)<br/>barrels        :O     (O)<br/>a              :O     (O)<br/>day            :O     (O)<br/>of             :O     (O)<br/>Iraq           :B-geo (B-geo)<br/>'s             :O     (O)<br/>declared       :O     (O)<br/>oil            :O     (O)<br/>production     :O     (O)<br/>over           :O     (O)<br/>the            :O     (O)<br/>past           :B-tim (B-tim)<br/>four           :I-tim (I-tim)<br/>years          :O     (O)<br/>.              :O     (O)<br/>PADword        :O     (O)<br/>PADword        :O     (O)<br/>PADword        :O     (O)<br/>PADword        :O     (O)<br/>PADword        :O     (O)<br/>PADword        :O     (O)<br/>PADword        :O     (O)<br/>PADword        :O     (O)<br/>PADword        :O     (O)<br/>PADword        :O     (O)</strong></span></pre><p id="6031" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">像往常一样，代码和 jupyter 笔记本在我的<a class="ae lv" href="https://github.com/nxs5899/Named-Entity-Recognition_DeepLearning-keras" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> Github </strong> </a>上可用。</p><p id="bf1b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">非常感谢您的提问和评论。</p><p id="03b8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">参考资料:</p><ol class=""><li id="6116" class="mp mq it la b lb lc le lf lh ng ll nh lp ni lt nr mx my mz bi translated"><a class="ae lv" href="https://www.depends-on-the-definition.com/named-entity-recognition-with-residual-lstm-and-elmo/" rel="noopener ugc nofollow" target="_blank">https://www . depends-on-the-definition . com/named-entity-recognition-with-residual-lstm-and-elmo/</a></li><li id="51bf" class="mp mq it la b lb na le nb lh nc ll nd lp ne lt nr mx my mz bi translated"><a class="ae lv" href="http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/" rel="noopener ugc nofollow" target="_blank">http://www . wild ml . com/2016/08/rnns-in-tensor flow-a-practical-guide-and-documentated-features/</a></li><li id="6003" class="mp mq it la b lb na le nb lh nc ll nd lp ne lt nr mx my mz bi translated"><a class="ae lv" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank">https://allennlp.org/elmo</a></li><li id="5301" class="mp mq it la b lb na le nb lh nc ll nd lp ne lt nr mx my mz bi translated"><a class="ae lv" href="https://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">https://jalammar.github.io/illustrated-bert/</a></li></ol></div></div>    
</body>
</html>